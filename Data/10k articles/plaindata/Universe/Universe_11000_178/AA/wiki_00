{"id": "3775426", "url": "https://en.wikipedia.org/wiki?curid=3775426", "title": "1991 Perfect Storm", "text": "1991 Perfect Storm\n\nThe 1991 Perfect Storm, also known as The No-Name Storm (especially in the years immediately after it took place) and the Halloween Gale/Storm, was a nor'easter that absorbed Hurricane Grace and ultimately evolved back into a small unnamed hurricane late in its life cycle. The initial area of low pressure developed off Atlantic Canada on October 29. Forced southward by a ridge to its north, it reached its peak intensity as a large and powerful cyclone. The storm lashed the east coast of the United States with high waves and coastal flooding before turning to the southwest and weakening. Moving over warmer waters, the system transitioned into a subtropical cyclone before becoming a tropical storm. It executed a loop off the Mid-Atlantic states and turned toward the northeast. On November 1, the system evolved into a full-fledged hurricane, with peak sustained winds of 75 miles per hour (120 km/h), although the National Hurricane Center left it unnamed to avoid confusion amid media interest in the precursor extratropical storm. It later received the name \"the Perfect Storm\" (playing off the common expression) after a conversation between Boston National Weather Service forecaster Robert Case and author Sebastian Junger. The system was the twelfth and final tropical cyclone, the eighth tropical storm, and fourth hurricane in the 1991 Atlantic hurricane season. The tropical system weakened, striking Nova Scotia as a tropical storm before dissipating.\n\nDamage from the storm totaled over $200 million (1991 USD) and the death toll was thirteen. Most of the damage occurred while the storm was extratropical, after waves up to struck the coastline from Canada to Florida and southeastward to Puerto Rico. In Massachusetts, where damage was heaviest, over 100 homes were destroyed or severely damaged. To the north, more than 100 homes were affected in Maine, including the vacation home of then-President George H. W. Bush. More than 38,000 people were left without power, and along the coast high waves inundated roads and buildings. In portions of New England, the damage was worse than that caused by Hurricane Bob two months earlier.\n\nAside from tidal flooding along rivers, the storm's effects were primarily concentrated along the coast. A buoy off the coast of Nova Scotia reported a wave height of , the highest ever recorded in the province's offshore waters. In the middle of the storm, the fishing vessel \"Andrea Gail\" sank, killing her crew of six and inspiring the book, and later movie, \"The Perfect Storm\". Off the shore of New York's Long Island, an Air National Guard helicopter ran out of fuel and crashed; four members of its crew were rescued and one was killed. Two people died after their boat sank off Staten Island. High waves swept two people to their deaths, one in Rhode Island and one in Puerto Rico, and another person was blown off a bridge to his death. The tropical cyclone that formed late in the storm's duration caused little impact, limited to power outages and slick roads; one person was killed in Newfoundland from a traffic accident related to the storm.\n\nThe Perfect Storm originated from a cold front that exited the east coast of the United States. On October 28, the front spawned an extratropical low to the east of Nova Scotia. Around that time, a ridge extended from the Appalachian Mountains northeastward to Greenland, with a strong high pressure center over eastern Canada. The blocking ridge forced the extratropical low to track toward the southeast and later to the west. Hurricane Grace was swept aloft by its cold front into the warm conveyor belt circulation of the deep cyclone on October 29. The cyclone significantly strengthened as a result of the temperature contrast between the cold air to the northwest and the warmth and humidity from the remnants of Hurricane Grace. The low pressure system continued deepening as it drifted toward the United States. It had an unusual retrograde motion for a nor'easter, beginning a set of meteorological circumstances that occur only once every 50 to 100 years. Most nor'easters affect New England from the southwest.\nWhile situated about south of Halifax, Nova Scotia, the storm attained its peak intensity with winds of up to . The nor'easter reached peak intensity at approximately 12:00 UTC on October 30 with its lowest pressure of 972 millibars. The interaction between the extratropical storm and the high pressure system to its north created a significant pressure gradient, which created large waves and strong winds. Between the southern New England coast and the storm's center, the gradient was . A buoy located south of Halifax reported a wave height of on October 30. This became the highest recorded wave height on the Scotian Shelf, which is the oceanic shelf off the coast of Nova Scotia. East of Cape Cod, a NOAA buoy located at reported maximum sustained winds of with gusts to , and a significant wave height (average height of the highest one-third of all waves) of around 15:00 UTC on October 30. Another buoy, located at , reported maximum sustained winds of with gusts to and a significant wave height of near 00:00 UTC on October 31.\n\nUpon peaking in intensity, the nor'easter turned southward and gradually weakened; by November 1, its pressure had risen to . The low moved over warm waters of the Gulf Stream, where bands of convection around the center began to organize. Around this time, the system attained subtropical characteristics. On November 1, while the storm was moving in a counter-clockwise loop, a tropical cyclone had been identified at the center of the larger low. (Although these conditions are rare, Hurricane Karl during 1980 formed within a larger non-tropical weather system.)\n\nBy around 14:00 UTC on November 1, an eye feature was forming, and the tropical cyclone reached its peak intensity with maximum sustained winds of ; these estimates, combined with reports from an Air Force Reserve Unit flight into the storm and confirmation that a warm-core center was present, indicated that the system had become a Category 1 hurricane on the Saffir–Simpson Hurricane Scale. The hurricane accelerated toward the northeast and quickly weakened back into a tropical storm. It made landfall near Halifax, Nova Scotia, at 14:00 UTC on November 2, with sustained winds of . While the storm was approaching the coast, weather radars depicted curved rainbands on the western side of the system. After crossing over Prince Edward Island, the storm fully dissipated late on November 2.\n\nFor several days, weather models forecast the development of a significant storm off New England. However, the models were inadequate in forecasting coastal conditions, which in one instance failed to provide adequate warning. In addition, a post-storm assessment found an insufficient number of observation sites along the coast. On October 27, the Ocean Prediction Center noted that a \"dangerous storm\" would form within 36 hours, with its wording emphasizing the unusual nature of the storm. The National Weather Service likewise issued warnings for the potential storm, providing information to emergency service offices as well as the media. The public however was skeptical and did not recognize the threat. The timely warnings ultimately lowered the death toll; whereas the Perfect Storm caused 13 deaths, the blizzard of 1978 killed 99 people, and the 1938 New England hurricane killed 564 people.\n\nFrom Massachusetts to Maine, thousands of people evacuated their homes and sought shelter. A state of emergency was declared for nine counties in Massachusetts, including Suffolk County, as well as two in Maine. In North Carolina, the National Weather Service offices in Hatteras and Raleigh first issued a heavy surf advisory on October 27, more than eight hours before the first reports of high waves. That same day, a coastal flood watch and later a warning was issued, along with a gale warning. The Hatteras NWS office ultimately released 19 coastal flood statements, as well as media reports explaining the threat from the wind and waves, and a state of emergency was declared for Dare County, North Carolina. The warnings and lead times in the region were described as \"very good\".\n\nIn Canada, the threat from the storm prompted the cancellation of ferry service from Bar Harbor, Maine, to Yarmouth, Nova Scotia, as well as from Nova Scotia to Prince Edward Island and between Nova Scotia and Newfoundland.\n\nIn its tropical cyclone report on the hurricane, the National Hurricane Center only referred to the system as \"Unnamed Hurricane\". The Natural Disaster Survey Report called the storm \"The Halloween Nor'easter of 1991\". The \"perfect storm\" moniker was coined by author and journalist Sebastian Junger after a conversation with NWS Boston Deputy Meteorologist Robert Case in which Case described the convergence of weather conditions as being \"perfect\" for the formation of such a storm. Other National Weather Service offices were tasked with issuing warnings for this storm in lieu of the typical NHC advisories. The OPC posted warnings on the unnamed hurricane in its \"High Seas Forecasts\". The National Weather Service State Forecast Office in Boston issued \"Offshore Marine Forecasts\" for the storm. Local NWS offices along the East coast covered the storm in their \"Coastal Waters Forecasts\".\n\nBeginning in 1950, the National Hurricane Center named officially recognized tropical storms and hurricanes. The unnamed hurricane was reported to have met all the criteria for a tropical cyclone, but it was purposefully left unnamed. This was done to avoid confusion among the media and the public, who were focusing on the damage from the initial nor'easter, as the hurricane itself was not expected to pose a major threat to land. It was the eighth nameable storm of the 1991 Atlantic hurricane season. Had the system been named instead, it would have received the name \"Henri\", which was the next name on the 1991 list after Grace.\n\nThe Halloween Storm of 1991 left significant damage along the east coast of the United States, primarily in Massachusetts and southern New Jersey. Across seven states, damage totaled over $200 million (1991 USD). Over a three-day period, the storm lashed the northeastern United States with high waves, causing damage to beachfront properties from North Carolina to Maine. The coastal flooding damaged or destroyed hundreds of homes and businesses and closed roads and airports. In addition, high winds left about 38,000 people without power. The total without power was much less than for Hurricane Bob two months prior, and was fairly low due to little rainfall and the general lack of leaves on trees. Overall there were thirteen confirmed deaths, including six on board \"Andrea Gail\", a swordfishing boat. The vessel departed Gloucester, Massachusetts, for the waters off Nova Scotia. After encountering high seas in the middle of the storm, the vessel made its last radio contact late on October 28, about northeast of Sable Island. \"Andrea Gail\" sank while returning to Gloucester, her debris washing ashore over the subsequent weeks. The crew of six was presumed killed after a Coast Guard search was unable to find them. The storm and the boat's sinking became the center-piece for Sebastian Junger's best-selling non-fiction book \"The Perfect Storm\" (1997), which was adapted to a major Hollywood film in 2000 as \"The Perfect Storm\" starring George Clooney.\nDespite the storm's severity, it was neither the costliest nor the strongest to affect the northeastern United States. It was weakening as it made its closest approach to land, and the highest tides occurred during the neap tide, which is the time when tide ranges are minimal. The worst of the storm effects stayed offshore. A buoy northeast of Nantucket, which was west of \"Andrea Gail\" last known position, recorded a rise in wave height in 10 hours while the extratropical storm was still rapidly intensifying. Two buoys near the Massachusetts coast observed record wave heights, and one observed a record wind report. The United States Coast Guard rescued 25 people at sea at the height of the storm, including 13 people from Long Island Sound. A New York Air National Guard helicopter of the 106th Air Rescue Wing ditched during the storm, south of Montauk, New York, after it was unable to refuel in flight and ran out of fuel. After the helicopter had attempted a rescue in the midst of the storm, an 84-person crew on the Coast Guard Cutter \"Tamaroa\" arrived and rescued four members of the crew of five after six hours in hypothermic waters. The survivors were pilots Dave Ruvola and Graham Bushor, flight engineer Jim Miolli, and pararescue jumper John Spillane. The fifth member, pararescue jumper Rick Smith, was never found. They were all featured on the show I Shouldn't be Alive.\n\nFollowing the storm's damage, President George H. W. Bush declared five counties in Maine, seven counties in Massachusetts, and Rockingham County, New Hampshire to be disaster areas. The declaration allowed for the affected residents to apply for low-interest repair loans. New Jersey governor Jim Florio requested a declaration for portions of the coastline, but the request was denied because of the funding needs of other disasters, such as Hurricane Hugo, Hurricane Bob, and the 1989 Loma Prieta earthquake. The American Red Cross opened service centers in four locations in Massachusetts to assist the storm victims by providing food, clothing, medicine, and shelter. The agency deployed five vehicles carrying cleanup units and food, and allocated $1.4 million to provide assistance to 3,000 families.\n\nAlong the Massachusetts coastline, the storm produced wave heights on top of a high tide. In Boston, the highest tide was , which was only lower than the record from the blizzard of 1978. High waves on top of the storm tide reached about . The storm produced heavy rainfall in southeastern Massachusetts, peaking at . Coastal floods closed several roads, forcing hundreds of people to evacuate. In addition to the high tides, the storm produced strong winds; Chatham recorded a gust of . Damage was worst from Cape Ann in northeastern Massachusetts to Nantucket, with over 100 homes destroyed or severely damaged at Marshfield, North Beach, and Brant Point. There were two injuries in the state, although there were no fatalities. Across Massachusetts, damage totaled in the hundreds of millions of dollars.\nElsewhere in New England, waves up to reached as far north as Maine, along with tides that were above normal. Significant flooding was reported in that state, along with high winds that left areas without power. A total of 49 houses were severely damaged, 2 were destroyed, and overall more than 100 were affected. In Kennebunkport, the storm blew out windows and flooded the vacation home of then-President George H. W. Bush. The home sustained significant damage to its first floor. In Portland, tides were above normal, among the ten highest tides since record-keeping began in 1914. Along the coast, damage was worse than that caused by Hurricane Bob two months prior. Across Maine, the storm left $7.9 million (1991 USD) in damage, mostly in York County. More than half of the damage total was from property damage, with the remainder to transportation, seawalls, and public facilities. Although there were no deaths, there were two injuries in the state. In neighboring New Hampshire, coastal flooding affected several towns, destroying two homes. The storm destroyed three boats and damaged a lighthouse. High waves destroyed or swept away over 50,000 lobster traps, representing $2 million in losses (1991 USD). Damage was estimated at $5.6 million (1991 USD). Further west, high winds and coastal flooding lashed the Rhode Island and Connecticut coasts, killing a man in Narragansett, Rhode Island. Winds reached in Newport, Rhode Island, causing power outages.\n\nOff the coast of Atlantic Canada, the storm produced very high waves, flooding a ship near Sable Island and stranding another ship. Along the coast, the waves wrecked three small boats near Tiverton, Nova Scotia, as well as nine boats in Torbay, Newfoundland and Labrador. In Nova Scotia, where the storm made landfall, precipitation reached 1.18 in (30 mm), and 20,000 people in Pictou County were left without power. The storm also caused widespread power outages in Newfoundland from its high winds, which reached 68 mph (110 km/h) near St. Lawrence. There were at least 35 traffic accidents, one fatal, in Grand Falls-Windsor due to slick roads. Prior to the storm's formation, there was a record 4.4 in (116 mm) of snowfall across Newfoundland. The storm caused no significant damage in Canada, other than these traffic accidents.\n\nIn New York and northern New Jersey, the storm system left the most coastal damage since the 1944 Great Atlantic hurricane. Numerous boats were damaged or destroyed, killing two people off Staten Island. High winds swept a man off a bridge, killing him. High waves flooded the beach at Coney Island. In Sea Bright, New Jersey, waves washed over a seawall, forcing 200 people to evacuate. Further inland, the Hudson, Passaic, and Hackensack rivers experienced tidal flooding. Outside Massachusetts, damage was heaviest in southern New Jersey, where the cost was estimated at $75 million (1991 USD). Across the area, tide heights reached their highest since the 1944 hurricane, leaving severe coastal and back bay flooding and closing many roads. The storm caused significant beach erosion, with 500,000 cubic yards (382,000 cubic meters) lost in Avalon, as well as $10 million damage to the beach in Cape May. The presence of a dune system mitigated the erosion in some areas. There was damage to the Atlantic City Boardwalk. Fire Island National Seashore was affected, washing away an entire row of waterfront houses in towns like Fair Harbor. Following the storm, there was a moratorium on clamming in the state's bays, due to contaminated waters. Along the Delmarva Peninsula and Virginia Beach, there was widespread water damage to homes, including ten affected houses in Sandbridge Beach, Virginia. Tides in Ocean City, Maryland, reached a record height of , while elsewhere the tides were similar to the Ash Wednesday Storm of 1962.\n\nIn North Carolina along the Outer Banks, high waves were initially caused by Hurricane Grace and later its interaction with a high pressure system. This produced gale-force winds and waves in the town of Duck. Later, the extratropical predecessor to the unnamed hurricane produced additional high waves, causing oceanfront flooding from Cape Hatteras through the northern portions of Currituck County. Flooding was first reported on October 28, when the ocean covered a portion of North Carolina Highway 12 north of Rodanthe; the route is the primary thoroughfare in the Outer Banks. Nags Head, Kitty Hawk, and Kill Devil Hills had large portions covered with water for several blocks away from the beach. The resultant flooding damaged 525 houses and 28 businesses and destroyed two motels and a few homes. Damage was estimated at $6.7 million (1991 USD). Farther south, the storm left 14 people injured in Florida. There was minor beach erosion and flooding, which damaged two houses and destroyed the pier at Lake Worth. In some locations, beaches gained additional sand from the wave action. Two people went missing off Daytona Beach after their boat lost power. High waves destroyed a portion of State Road A1A. Damage in the state was estimated at $3 million (1991 USD). High waves also affected Bermuda, the Bahamas, and the Dominican Republic. In Puerto Rico, waves of affected the island's north coast, which prompted 32 people to seek shelter. The waves swept a person off a large rock to his death.\n"}
{"id": "28706352", "url": "https://en.wikipedia.org/wiki?curid=28706352", "title": "AOS Checklist of North American Birds", "text": "AOS Checklist of North American Birds\n\nThe AOS Checklist of North American Birds is a checklist of the bird species found in North and Middle America which is now maintained by the American Ornithological Society (AOS). The checklist was originally published by the AOS's predecessor, the American Ornithologists' Union (AOU). The Union merged with the Cooper Ornithological Society in 2016 to form the American Ornithological Society. The checklist was first published in 1886; the seventh edition of the checklist was published in 1998 and is now updated every year by an open-access article published in the \"Ibis\". Seven editions and 54 supplements (minor updates) have been published in the last 127 years. According to Joel Asaph Allen, the Codes of Nomenclature set out in the first edition of the Checklist \"later became the basis of the International Code of Zoological Nomenclature, framed on essentially the same lines and departing from it in no essential respect, except in point of brevity, through omission of adequate illustrations of the rules, and thereby rendering necessary the issuance of official 'Opinions' to clear up obscure points.\"\n\nThe current version of the checklist is available online from the website of the American Ornithological Society. The list is updated regularly by the AOS's North American Classification Committee (NACC), which considers proposals related to classification and nomenclature.\n"}
{"id": "23230502", "url": "https://en.wikipedia.org/wiki?curid=23230502", "title": "Ali Akbar Salehi", "text": "Ali Akbar Salehi\n\nAli Akbar Salehi (, ; born 24 March 1949) is an Iranian academic, diplomat and the head of Atomic Energy Organization of Iran. He served as head of AEOI from 2009 to 2010 and was appointed to the post for a second time on 16 August 2013. Before his appointment of his current position, he was foreign affairs minister from 2010 to 2013. He was also the Iranian representative in the International Atomic Energy Agency from 1998 to 2003.\n\nSalehi was born in Karbala, Iraq, on 24 March 1949. He received a bachelor of science degree in physics from the American University of Beirut in 1971 and a PhD in nuclear engineering from the Massachusetts Institute of Technology in 1977. Salehi is fluent in English and Arabic, in addition to his native Persian.\n\nSalehi is full professor and was chancellor of the Sharif University of Technology and a member of the Academy of Sciences of Iran and the International Centre for Theoretical Physics in Italy. He served as the chancellor of the Sharif University of Technology from 1982 to 1985 and once again from 1989 to 1993. While chancellor, Salehi was involved in an attempt to obtain dual-use technologies from a European supplier, according to David Albright of the Institute for Science and International Security, citing some 1,600 telex documents from the 1990s. He was also chancellor of Imam Khomeini International University for two years (1988–1989).\nAn ISIS (Institute for Science and International Security) report claims the Physics Research Center acted as a front in the late 1980s and early 1990s to obtain illicit nuclear technologies. ISIS claims that, as head of Sharif University, was aware of purchases.\n\nSalehi was appointed as permanent representative of Iran to International Atomic Energy Agency by the then president Mohammad Khatami on 13 March 1997 and remained in the post until 22 August 2005. On 18 December 2003, Salehi signed the Additional Protocol to the safeguard agreement, on behalf of Iran. He was replaced by Pirooz Hosseini.\n\nSalehi was deputy secretary-general of the Organisation of the Islamic Conference under Ekmeleddin İhsanoğlu from 2007 to 2009. He resigned on 16 July 2009 when then President Mahmoud Ahmadinejad appointed Salehi as the new head of Iran's Atomic Energy Organization, replacing Gholam Reza Aghazadeh who resigned on 10 July. Salehi resigned from office on 23 January 2011 when Ahmadinejad nominated him as foreign minister.\n\nOn 13 December 2010, Ahmadinejad dismissed Manouchehr Mottaki for unknown reasons and appointed Salehi in an acting capacity. On 23 January 2011, Ahmadinejad nominated Salehi to become foreign minister. The Iranian Parliament voted him on 30 January and he became the foreign minister of Iran, gaining 146 positive votes. The European Union and the Treasury of the United Kingdom had put Salehi into the sanction list as an asset freeze target on 18 November 2009 due to his previous involvement in Iran's nuclear programme. The EU waived this designation when he became foreign minister in 2010. His term as foreign minister ended on 15 August 2013 when Mohammad Javad Zarif took the position in the elected President Hassan Rouhani's government.\nA day after, Rouhani appointed Salehi as head of Atomic Energy Organization for a second time on 16 August 2013. Salehi replaced Fereydoon Abbasi in the post.\n\nAs the head of the AEOI when Iran was facing increased scrutiny in light of International Atomic Energy Agency findings, Salehi was designated for financial sanctions and travel restrictions by the European Union and the United Kingdom.\nSalehi and Ernest Moniz joined 2015 Geneva Iran and P5+1 nuclear talks to discuss more about technical aspects of Iran nuclear program.\nSalehi has been selected among the ten people who mattered the year 2015 by Nature magazine because of his role in nuclear talks.\n\n"}
{"id": "14349730", "url": "https://en.wikipedia.org/wiki?curid=14349730", "title": "Alpha-Parinaric acid", "text": "Alpha-Parinaric acid\n\nα-Parinaric acid is a conjugated polyunsaturated fatty acid. Discovered by Tsujimoto and Koyanagi in 1933, it contains 18 carbon atoms and 4 conjugated double bonds. The repeating single bond-double bond structure of α-parinaric acid distinguishes it structurally and chemically from the usual \"methylene-interrupted\" arrangement of polyunsaturated fatty acids that have double-bonds and single bonds separated by a methylene unit (−CH−). Because of the fluorescent properties conferred by the alternating double bonds, α-parinaric acid is commonly used as a molecular probe in the study of biomembranes.\n\nα-Parinaric acid occurs naturally in the seeds of the makita tree (\"Parinari laurina\"), a tree found in Fiji and other Pacific islands. Makita seeds contain about 46% α-parinaric acid, 34% α-eleostearic acid as major components, with lesser amounts of saturated fatty acids, oleic acid and linoleic acid. α-Parinaric acid is also found in the seed oil of \"Impatiens balsamina\", a member of the family \"Balsaminaceae\". The major fatty acids of \"Impatiens balsamina\" are 4.7% palmitic acid, 5.8% stearic acid, 2.8% arachidic acid, 18.3% oleic acid, 9.2% linoleic acid, 30.1% linolenic acid and 29.1% α-parinaric acid. It is also present in the fungus \"Clavulina cristata\", and the plant \"Sebastiana brasiliensis\" (family Euphorbiaceae).\n\nThe biochemical mechanism by which α-parinaric acid is formed in the plant \"Impatiens balsamina\" was elaborated using techniques of molecular biology. The enzyme responsible for the creation of the conjugated double bonds was identified using expressed sequence tags, and called a \"conjugase\". This enzyme is related to the family of fatty acid desaturase enzymes responsible for putting double bonds into fatty acids.\n\nα-Parinaric acid may be synthesized chemically using α-linoleic acid as a starting compound. This synthesis enables the transformation of 1,4,7-octatriene methylene-interrupted \"cis\" double bonds of naturally occurring polyunsaturated fatty acids to 1,3,5,7-octatetraenes in high yield. More recently (2008), Lee et al. reported a simple and efficient chemical synthesis using a modular design method called iterative cross-coupling.\n\nBoth the alpha and beta (all \"trans\") isomers of parinaric acid are used as molecular probes of lipid-lipid interactions, by monitoring phase transitions in bilayer lipid membranes. α-Parinaric acid was shown to integrate normally into the phospholipid bilayer of mammalian cells, nervous tissue, with minimal effects on the biophysical properties of the membrane. Molecular interactions with neighboring membrane lipids will affect the fluorescence of α-parinaric acid in predictable ways, and the subsequent subtle changes in energy intensities may be measured spectroscopically.\n\nResearchers have put α-parinaric to good use in the study of membrane biophysics. For example, it was used to help establish the existence of a \"fluidity gradient\" across the membrane bilayer of some tumor cells ― the inner monolayer of the membrane is less fluid than the outer monolayer.\n\nα-Parinaric acid is also used as a chromophore to study interactions between membrane proteins and lipids. Because of the similarity of α-parinaric acid to normal membrane lipids, it has minimal perturbing influence. By measuring shifts in the absorption spectrum, enhancement of α-parinaric acid fluorescence, induced circular dichroism, and energy transfer between tryptophan amino acids in the protein and the bound chromophore, information may be gleaned about the molecular interactions between protein and lipid. For example, this technique is used to investigate how fatty acids bind to serum albumin (a highly abundant blood protein), lipid transport processes including structural characterization of lipoproteins, and phospholipid-transfer proteins.\n\nThe concentrations of fatty acids in blood serum or plasma can be measured using α-parinaric acid, which will compete for binding sites on serum albumin.\n\nα-Parinaric acid has been used to study the hydrophobicity and foaming characteristics of food proteins, as well as the foam stability of beer. In this latter research, α-parinaric acid was used in a fluorescent assay to assess the lipid–binding potential of the proteins in the beer, as these proteins help protect beer from foam–reducing medium– and long–chain fatty acids.\n\nα-Parinaric acid is cytotoxic to human leukemia cells in cell culture at concentrations of 5 μM or less, by sensitizing the tumor cells to lipid peroxidation, the process where free radicals react with electrons from cell membrane lipids, resulting in cell damage. It is similarly cytotoxic to malignant gliomas grown in cell culture. Normal (non-tumorous) astrocytes grown in culture are far less sensitive to the cytotoxic effects of α-parinaric acid. This preferential toxicity towards tumor cells is due to a differential regulation of c-Jun N-terminal kinase, and forkhead transcription factors between malignant and normal cells.\n"}
{"id": "2183025", "url": "https://en.wikipedia.org/wiki?curid=2183025", "title": "Aluminide", "text": "Aluminide\n\nAn aluminide is a compound that has aluminium with more electropositive elements. Since aluminium is near the nonmetals on the periodic table, it can bond with metals differently from other metals. The properties of an aluminide would be intermediate between a metal alloy and an ionic compound.\n\n\nSee for a list.\n"}
{"id": "25433224", "url": "https://en.wikipedia.org/wiki?curid=25433224", "title": "Bringing Back the Bluestones", "text": "Bringing Back the Bluestones\n\nBringing Back the Bluestones is a stage play by Derek Webb about a pressure group from Pembrokeshire campaigning to have the Stonehenge bluestones returned to Wales. It followed the fortunes of Roy Brown as he sets up the group, Carreg Las (Welsh for \"Blue Stone\"), and campaigns for the return of the stones to the Preseli Hills.\n\nThe play began as a short radio play by Derek Webb, but was developed into a stage play in 2002. The original production caused a sensation in Wales when the \"Western Mail\" (a leading Welsh newspaper) carried an article reporting the premise the play was based on as true. It was alleged that Carreg Las had argued that since Greece wanted the Elgin Marbles back, Wales should be able to lay claim to the Bluestones. The \"Western Mail\" ran a front page article, a leader column comment, and devoted a full colour half page to the story. Eminent historians and politicians were consulted. A spokesperson for English Heritage, which looks after Stonehenge, reportedly \"almost choked when asked about the possibility of dismantling Stonehenge\". The story was subsequently taken up by other newspapers and radio stations throughout Wales and in Wiltshire. However, it turned out that the creation of Carreg Las was a spoof designed to generate publicity for the play. \n\nIn 2004, a few years after the first production, \"British Archaeology \" carried the following story: \nThe story was also reported by the \"Milford Mercury\" and the BBC.\n\nThe play was rewritten in 2010 for a performance by Fluellen Theatre Company at the Swansea Grand Theatre, as part of their Lunchtime Theatre programme. This new 1-act version subsequently transferred to deValence in Tenby and on to Theatr Gwaun in Fishguard where it attracted record audiences. The main character in the play, Roy Brown, has subsequently been developed in a further comedy entitled \"Roy Brown: Untitled\" in which Roy sees modern art as a way of making quick money and turns out a number of \"artworks\" in quick succession. The play was premiered in 2010 at Swansea's Grand Theatre. Webb is the author of several other plays which have been premiered at the theatre. His most recent play was commemorating the 100th anniversary of the first flight from the UK to Ireland by Denys Corbett Wilson in April 1912. \n\n"}
{"id": "4065640", "url": "https://en.wikipedia.org/wiki?curid=4065640", "title": "CRC Oil Storage Depot", "text": "CRC Oil Storage Depot\n\nCRC Oil Storage Depot was one of five oil terminals in Hong Kong and owned by China Resources Petroleum Company Limited (CRC).\n\n\n"}
{"id": "13705455", "url": "https://en.wikipedia.org/wiki?curid=13705455", "title": "Capital Power Income", "text": "Capital Power Income\n\nCapital Power Income L.P. was a limited partnership that owned power generation assets in Canada and the United States.\n\nFounded as TransCanada Power L.P. the company was renamed EPCOR Power L.P. after TransCanada Corporation sold its interest in the partnership to EPCOR Utilities Incorporated on September 1, 2005. It was then renamed on November 5, 2009 to reflect the transfer of ownership from EPCOR Utilities to its newly created spin-off Capital Power Corporation.\n\nOn November 7, 2011 Atlantic Power Corporation completed it plan of arrangement to acquire the company.\n\n\n"}
{"id": "42562743", "url": "https://en.wikipedia.org/wiki?curid=42562743", "title": "Coal Industry Commission Act 1919", "text": "Coal Industry Commission Act 1919\n\nThe Coal Industry Commission Act 1919 (c 1) was an Act of Parliament of the United Kingdom, which set up a commission, led by Mr Justice Sankey (and so known as the \"Sankey Commission\"), to consider joint management or nationalisation of the coal mines. It also considered the issues of working conditions, wage and hours.\n\nA Royal Commission, led by Sir John Sankey, was called to examine the future of the mining industry. Leo Chiozza Money, Sidney Webb and R.H. Tawney were the three economists on the commission, all broadly favourable to the miners. Others were appointed from business and the trade unions.\n\nNo agreement was reached and, when the commission reported in June 1919, it offered four separate approaches ranging from full nationalisation to untrammelled private ownership.\n\n\n\n"}
{"id": "50041574", "url": "https://en.wikipedia.org/wiki?curid=50041574", "title": "Coal industry in Wales", "text": "Coal industry in Wales\n\nThe coal industry in Wales has played an important role in the Industrial Revolution in Wales. Coal mining expanded in the eighteenth century to provide fuel for the blast furnaces of the iron and copper industries that were expanding in South Wales. The industry had reached large proportions by the end of that century, and then further expanded to supply steam-coal for the steam vessels that were beginning to trade around the world. The Cardiff Coal Exchange set the world price for steam-coal and Cardiff became a major coal-exporting port. The South Wales Coalfield was at its peak in 1913 and was one of the largest coalfields in the world. It remained the largest coalfield in Britain until 1925. The supply of coal dwindled, and pits closed in spite of a UK-wide strike against closures. The last deep mine in Wales, Tower Colliery, closed in 2008, after thirteen years as a co-operative owned by its miners. \n\nThe South Wales Coalfield was not the only coal mining area of the country. There was a sizeable industry in Flintshire and Denbighshire in northeast Wales, and coal was also mined in Anglesey.\n\nThe South Wales Coalfield extends from parts of Pembrokeshire and Carmarthenshire in the west, through Swansea, Neath Port Talbot, Bridgend County Borough, Rhondda Cynon Taf, Merthyr Tydfil, Caerphilly County Borough and Blaenau Gwent to Torfaen in the east. The rocks comprising this area were laid down during the Upper Carboniferous era. At that time warm seas invaded much of southern and northeastern Wales, coral reefs flourished and were laid down as limestone deposits. In South Wales particularly, extensive swamps developed where tree-size clubmosses and ferns grew. The decay of this vegetation as it died formed peat which became buried over the ensuing millennia by other sediments. Over long periods of time, the peat was consolidated and converted by the pressure of overlying layers into seams of coal. Although thinner than the original peat layers, some of the coal deposits in South Wales are of great thickness.\n\nThe North Wales Coalfield is divided into two parts, the Flintshire Coalfield to the north and the nearly contiguous Denbighshire Coalfield to the south. The Flintshire Coalfield extends from the Point of Ayr in the north, through Connah's Quay to Caergwrle in the south. It also extends under the Dee Estuary to the Neston area of the Wirral Peninsula. The Denbighshire Coalfield extends from near Caergwrle in the north, to Wrexham, Ruabon, Rhosllannerchrugog and Chirk in the south, a small part extending into Shropshire in the Oswestry area.\n\nCoal is the most abundant fossil fuel in the earth's crust and has a long history of being exploited. Archaeological evidence shows that it was burned in funeral pyres in Wales during the Bronze Age and cinders have been found in Roman settlements in Britain. Originally the coal would have come from outcropping coal seams, and from lumps of sea coal gathered from the foreshore. By the seventeenth century, coal was being dug from shallow deposits for local use and by the beginning of the eighteenth century, a trade in coal was developing along the coast from such areas as Pembrokeshire, Llanelli and Swansea Bay. As surface deposits were exhausted by opencast mining, shallow pits were dug, and later bell pits, with more coal being extracted from short galleries. Soon, horizontal shafts were being dug into the hillsides and barrows manoeuvred along wooden tracks.\n\nDuring the Industrial Revolution, Wales was at the forefront of the development of new technologies for the mining industry. These innovations included the use of water power for winding gear, the provision of mine ventilation, the use of steam engines for both winding and pumping and the use of underground tramways and canals for transport. Welsh mine-owners pioneered the use of horse-drawn and later steam railways to transport coal to the docks. The transfer from wood to coal as fuel that occurred during the Industrial Revolution was heavily influenced by Welsh innovations.\n\nThe land under which coal could be found was generally in private ownership. John Crichton-Stuart, 2nd Marquess of Bute was a large landowner in South Wales and developed the coal and iron industries in Glamorganshire in the nineteenth century. Agriculture ceased to be the main source of employment in the county as mining and other industries came to the fore, and he transformed his South Wales estates into a major industrial enterprise. He commissioned surveys in 1817 and again in 1823 to 1824 which showed huge reserves of coal under his land. He sold some outlying parts of his estate in order to purchase other potentially more-productive areas, and claimed rights to minerals under certain common lands through his feudal titles.\n\nLord Bute was one of the main forces behind the development of Cardiff Docks for the export of coal and iron from South Wales. By 1840, the network of canals and railways enabled 4.5 million tons of coal to be mined and transported. Of this, about half was used by the steel industry, about 750,000 tons went for export and the rest was used for other industrial and domestic use. By 1854, coal production had nearly doubled, and 2.5 million tons was being exported.\n\nIn North Wales, the Flintshire manors of Ewloe, Hopedale, and Mostyn and the Denbighshire manor of Brymbo were reported to be making profits from trading coal during the 14th and 15th centuries. By 1593, coal was being exported from ports on the Dee Estuary. The trade developed swiftly and by 1616, the principle collieries were at Bagillt, Englefield, Leaderbrook, Mostyn, Uphfytton and Wepre. Most mines were horizontal adits or shallow bell pits, though a few were becoming sufficiently large to have accumulations of water and ventilation problems.\n"}
{"id": "48520204", "url": "https://en.wikipedia.org/wiki?curid=48520204", "title": "Computational anatomy", "text": "Computational anatomy\n\nComputational anatomy is an interdisciplinary field of biology focused on quantitative investigation and modelling of anatomical shapes variability. It involves the development and application of mathematical, statistical and data-analytical methods for modelling and simulation of biological structures.\n\nThe field is broadly defined and includes foundations in anatomy, applied mathematics and pure mathematics, machine learning, computational mechanics, computational science, biological imaging, neuroscience, physics, probability, and statistics; it also has strong connections with fluid mechanics and geometric mechanics. Additionally, it complements newer, interdisciplinary fields like bioinformatics and neuroinformatics in the sense that its interpretation uses metadata derived from the original sensor imaging modalities (of which Magnetic Resonance Imaging is one example). It focuses on the anatomical structures being imaged, rather than the medical imaging devices. It is similar in spirit to the history of Computational linguistics, a discipline that focuses on the linguistic structures rather than the sensor acting as the transmission and communication medium(s).\n\nIn computational anatomy, the diffeomorphism group is used to study different coordinate systems via coordinate transformations as generated via the Lagrangian and Eulerian velocities of flow in formula_1. The flows between coordinates in Computational anatomy are constrained to be geodesic flows satisfying the principle of least action for the Kinetic energy of the flow. The kinetic energy is defined through a Sobolev smoothness norm with strictly more than two generalized, square-integrable derivatives for each component of the flow velocity, which guarantees that the flows in formula_2 are diffeomorphisms. \nIt also implies that the diffeomorphic shape momentum taken pointwise satisfying the Euler-Lagrange equation for geodesics is determined by its neighbors through spatial derivatives on the velocity field. This separates the discipline from the case of incompressible fluids for which momentum is a pointwise function of velocity. Computational anatomy intersects the study of Riemannian manifolds and nonlinear global analysis, where groups of diffeomorphisms are the central focus. Emerging high-dimensional theories of shape are central to many studies in Computational anatomy, as are questions emerging from the fledgling field of shape statistics.\nThe metric structures in Computational anatomy are related in spirit to morphometrics, with the distinction that Computational anatomy focuses on an infinite-dimensional space of coordinate systems transformed by a diffeomorphism, hence the central use of the terminology , the metric space study of coordinate systems via diffeomorphisms.\n\nAt Computational anatomy's heart is the comparison of shape by recognizing in one shape the other. This connects it to D'Arcy Wentworth Thompson's developments On Growth and Form which has led to scientific explanations of morphogenesis, the process by which patterns are formed in Biology. Albrecht Durer's Four Books on Human Proportion were arguably the earliest works on Computational anatomy. The efforts of Noam Chomsky in his pioneering of Computational Linguistics inspired the original formulation of Computational anatomy as a generative model of shape and form from exemplars acted upon via transformations.\n\nDue to the availability of dense 3D measurements via technologies such as magnetic resonance imaging (MRI), Computational anatomy has emerged as a subfield of medical imaging and bioengineering for extracting anatomical coordinate systems at the morphome scale in 3D. The spirit of this discipline shares strong overlap with areas such as computer vision and kinematics of rigid bodies, where objects are studied by analysing the groups responsible for the movement in question. Computational anatomy departs from computer vision with its focus on rigid motions, as the infinite-dimensional diffeomorphism group is central to the analysis of Biological shapes. It is a branch of the image analysis and pattern theory school at Brown University pioneered by Ulf Grenander. In Grenander's general Metric Pattern Theory, making spaces of patterns into a metric space is one of the fundamental operations since being able to cluster and recognize anatomical configurations often requires a metric of close and far between shapes. The diffeomorphometry metric of Computational anatomy measures how far two diffeomorphic changes of coordinates are from each other, which in turn induces a metric on the shapes and images indexed to them. The models of metric pattern theory, in particular group action on the orbit of shapes and forms is a central tool to the formal definitions in Computational anatomy.\n\nComputational anatomy is the study of shape and form at the morphome or gross anatomy millimeter, or morphology scale, focusing on the study of sub-manifolds of formula_3 points, curves surfaces and subvolumes of human anatomy.\nAn early modern computational neuro-anatomist was David Van Essen performing some of the early physical unfoldings of the human brain based on printing of a human cortex and cutting. Jean Talairach's publication of Tailarach coordinates is an important milestone at the morphome scale demonstrating the fundamental basis of local coordinate systems in studying neuroanatomy and therefore the clear link to charts of differential geometry. Concurrently, virtual mapping in Computational anatomy across high resolution dense image coordinates was already happening in Ruzena Bajcy's and Fred Bookstein's earliest developments based on Computed axial tomography and Magnetic resonance imagery.\nThe earliest introduction of the use of flows of diffeomorphisms for transformation of coordinate systems in image analysis and medical imaging was by Christensen, Joshi, Miller, and Rabbitt.\n\nThe first formalization of Computational Anatomy as an orbit of exemplar templates under diffeomorphism group action was in the original lecture given by Grenander and Miller with that title in May 1997 at the 50th Anniversary of the Division of Applied Mathematics at Brown University, and subsequent publication. This was the basis for the strong departure from much of the previous work on advanced methods for spatial normalization and image registration which were historically built on notions of addition and basis expansion. The structure preserving transformations central to the modern field of Computational Anatomy, homeomorphisms and diffeomorphisms carry smooth submanifolds smoothly. They are generated via Lagrangian and Eulerian flows which satisfy a law of composition of functions forming the group property, but are not additive.\n\nThe original model of Computational anatomy was as the triple, formula_4 the group formula_5, the orbit of shapes and forms formula_6, and the probability laws formula_7 which encode the variations of the objects in the orbit. The template or collection of templates are elements in the orbit formula_8 of shapes.\n\nThe Lagrangian and Hamiltonian formulations of the equations of motion of Computational Anatomy took off post 1997 with several pivotal meetings including the 1997 Luminy meeting organized by the Azencott school at Ecole-Normale Cachan on the \"Mathematics of Shape Recognition\" and the 1998 Trimestre at Institute Henri Poincaré organized by David Mumford \"Questions Mathématiques en Traitement du Signal et de l'Image\" which catalyzed the Hopkins-Brown-ENS Cachan groups and subsequent developments and connections of Computational anatomy to developments in global analysis.\n\nThe developments in Computational Anatomy included the establishment of the Sobelev smoothness conditions on the diffeomorphometry metric to insure existence of solutions of variational problems in the space of diffeomorphisms, the derivation of the Euler-Lagrange equations characterizing geodesics through the group and associated conservation laws, the demonstration of the metric properties of the right invariant metric, the demonstration that the Euler-Lagrange equations have a well-posed initial value problem with unique solutions for all time, and with the first results on sectional curvatures for the diffeomorphometry metric in landmarked spaces. Following the Los Alamos meeting in 2002, Joshi's original large deformation singular \"Landmark\" solutions in Computational anatomy were connected to peaked \"Solitons\" or \"Peakons\" as solutions for the Camassa-Holm equation. Subsequently, connections were made between Computational anatomy's Euler-Lagrange equations for momentum densities for the right-invariant metric satisfying Sobolev smoothness to Vladimir Arnold's characterization of the Euler equation for incompressible flows as describing geodesics in the group of volume preserving diffeomorphisms. The first algorithms, generally termed LDDMM for large deformation diffeomorphic mapping for computing connections between landmarks in volumes and spherical manifolds, curves, currents and surfaces, volumes, tensors, varifolds, and time-series have followed.\n\nThese contributions of Computational anatomy to the global analysis associated to the infinite dimensional manifolds of subgroups of the diffeomorphism group is far from trivial. The original idea of doing differential geometry, curvature and geodesics on infinite dimensional manifolds goes back to Bernhard Riemann's Habilitation (Ueber die Hypothesen, welche der Geometrie zu Grunde liegen); the key modern book laying the foundations of such ideas in global analysis are from Michor.\n\nThe applications within Medical Imaging of Computational Anatomy continued to flourish after two organized meetings at the Institute for Pure and Applied Mathematics conferences at University of California, Los Angeles. Computational anatomy has been useful in creating accurate models of the atrophy of the human brain at the morphome scale, as well as Cardiac templates, as well as in modeling biological systems. Since the late 1990s, computational anatomy has become an important part of developing emerging technologies for the field of medical imaging. Digital atlases are a fundamental part of modern Medical-school education and in neuroimaging research at the morphome scale. Atlas based methods and virtual textbooks which accommodate variations as in deformable templates are at the center of many neuro-image analysis platforms including Freesurfer, FSL, MRIStudio, SPM. Diffeomorphic registration, introduced in the 90's, is now an important player with existing codes bases organized around ANTS,<ref name=\"stnava/ANTs\"></ref> DARTEL, DEMONS, LDDMM,<ref name=\"NITRC: LDDMM: Tool/Resource Info\"></ref> StationaryLDDMM, FastLDDMM, are examples of actively used computational codes for constructing correspondences between coordinate systems based on sparse features and dense images. Voxel-based morphometry(VBM) is an important technology built on many of these principles.\n\nThe model of human anatomy is a deformable template, an orbit of exemplars under group action. Deformable template models have been central to Grenander's Metric Pattern theory, accounting for typicality via templates, and accounting for variability via transformation of the template. An orbit under group action as the representation of the deformable template is a classic formulation from differential geometry. The space of shapes are denoted formula_9, with the group formula_10 with law of composition formula_11; the action of the group on shapes is denoted formula_12, where the action of the group formula_13 is defined to satisfy \n\nThe orbit formula_15 of the template becomes the space of all shapes, formula_16, being homogenous under the action of the elements of formula_17.\nThe orbit model of computational anatomy is an abstract algebra - to be compared to linear algebra- since the groups act nonlinearly on the shapes. This is a generalization of the classical models of linear algebra, in which the set of finite dimensional formula_18 vectors are replaced by the finite-dimensional anatomical submanifolds (points, curves, surfaces and volumes) and images of them, and the formula_19 matrices of linear algebra are replaced by coordinate transformations based on linear and affine groups and the more general high-dimensional diffeomorphism groups.\n\nThe central objects are shapes or forms in Computational anatomy, one set of examples being the 0,1,2,3-dimensional submanifolds of formula_20, a second set of examples being images generated via medical imaging such as via magnetic resonance imaging (MRI) and functional magnetic resonance imaging. The 0-dimensional manifolds are landmarks or fiducial points; 1-dimensional manifolds are curves such as sulcul and gyral curves in the brain; 2-dimensional manifolds correspond to boundaries of substructures in anatomy such as the subcortical structures of the midbrain or the gyral surface of the neocortex; subvolumes correspond to subregions of the human body, the heart, the thalamus, the kidney.\n\nThe landmarks formula_21 are a collections of points with no other structure, delineating important fiducials within human shape and form (see associated landmarked image).\nThe sub-manifold shapes such as surfaces formula_22 are collections of points modeled as parametrized by a local chart or immersion formula_23, formula_24 (see Figure showing shapes as mesh surfaces).\nThe images such as MR images or DTI images formula_25, and are dense functions\nformula_26 are scalars, vectors, and matrices (see Figure showing scalar image).\n\nGroups and group actions are familiar to the Engineering community with the universal popularization and standardization of linear algebra as a basic model for analyzing signals and systems in mechanical engineering, electrical engineering and applied mathematics. In linear algebra the matrix groups (matrices with inverses) are the central structure, with group action defined by the usual definition of formula_27 as an formula_28 matrix, acting on formula_29 as formula_30 vectors; the orbit in linear-algebra is the set of formula_31-vectors given by formula_32, which is a group action of the matrices through the orbit of formula_33.\n\nThe central group in Computational anatomy defined on volumes in formula_1 are the diffeomorphisms formula_35 which are mappings with 3-components formula_36, law of composition of functions formula_37, with inverse formula_38.\n\nMost popular are scalar images, formula_39, with action on the right via the inverse.\n\nFor sub-manifolds formula_22, parametrized by a chart or immersion formula_42, the diffeomorphic action the flow of the position\n\nSeveral group actions in computational anatomy have been defined.\n\nFor the study of rigid body kinematics, the low-dimensional matrix Lie groups have been the central focus. The matrix groups are low-dimensional mappings, which are diffeomorphisms that provide one-to-one correspondences between coordinate systems, with a smooth inverse. The matrix group of rotations and scales can be generated via a closed form finite-dimensional matrices which are solution of simple ordinary differential equations with solutions given by the matrix exponential.\n\nFor the study of deformable shape in Computational anatomy, a more general diffeomorphism group has been the group of choice, which is the infinite dimensional analogue. The high-dimensional differeomorphism groups used in Computational Anatomy are generated via smooth flows formula_44 which satisfy the Lagrangian and Eulerian specification of the flow fields as first introduced in., satisfying the ordinary differential equation: \n\nwith formula_45 the vector fields on formula_46 termed the Eulerian velocity of the particles at position formula_47 of the flow. The vector fields are functions in a function space, modelled as a smooth Hilbert space of high-dimension, with the Jacobian of the flow formula_48 a high-dimensional field in a function space as well, rather than a low-dimensional matrix as in the matrix groups. Flows were first introduced for large deformations in image matching; formula_49 is the instantaneous velocity of particle formula_50 at time formula_51 .\n\nThe inverse formula_52 required for the group is defined on the Eulerian vector-field with advective inverse flow\n\nThe group of diffeomorphisms is very big. To ensure smooth flows of diffeomorphisms avoiding shock-like solutions for the inverse, the vector fields must be at least 1-time continuously differentiable in space. For diffeomorphisms on formula_46, vector fields are modelled as elements of the Hilbert space formula_54 using the Sobolev embedding theorems so that each element has strictly greater than 2 generalized square-integrable spatial derivatives (thus formula_55 is sufficient), yielding 1-time continuously differentiable functions.\n\nThe diffeomorphism group are flows with vector fields absolutely integrable in Sobolev norm:\nwhere\nformula_56 \nwith the linear operator formula_57 mapping to the dual space formula_58, with the integral calculated by integration by parts when formula_59 is a generalized function in the dual space.\nthe images are denoted with the orbit as formula_60 and metric formula_61.\n\nIn classical mechanics the evolution of physical systems is described by solutions to the Euler–Lagrange equations associated to the Least-action principle of Hamilton. This is a standard way, for example of obtaining Newton's laws of motion of free particles. More generally, the Euler-Lagrange equations can be derived for systems of generalized coordinates. The Euler-Lagrange equation in Computational anatomy describes the geodesic shortest path flows between coordinate systems of the diffeomorphism metric. In Computational anatomy the generalized coordinates are the flow of the diffeomorphism and its Lagrangian velocity formula_62, the two related via the Eulerian velocity formula_63. \nHamilton's principle for generating the Euler-Lagrange equation requires the action integral on the Lagrangian given by \nthe Lagrangian is given by the kinetic energy:\n\nIn computational anatomy, formula_64 was first called the Eulerian or diffeomorphic shape momentum since when integrated against Eulerian velocity formula_65 gives energy density, and since there is a conservation of diffeomorphic shape momentum which holds. The operator formula_27 is the generalized moment of inertia or inertial operator.\n\nClassical calculation of the Euler-Lagrange equation from Hamilton's principle requires the perturbation of the Lagrangian on the vector field in the kinetic energy with respect to first order perturbation of the flow. This requires adjustment by the Lie bracket of vector field, given by operator formula_67 which involves the Jacobian given by \nDefining the adjoint formula_69 then the first order variation gives the Eulerian shape momentum formula_59 satisfying the generalized equation:\nmeaning for all smooth formula_71\n\nComputational anatomy is the study of the motions of submanifolds, points, curves, surfaces and volumes.\nMomentum associated to points, curves and surfaces are all singular, implying the momentum is concentrated on subsets of formula_20 which are dimension formula_74 in Lebesgue measure. In such cases, the energy is still well defined formula_75 since although formula_76 is a generalized function, the vector fields are smooth and the Eulerian momentum is understood via its action on smooth functions. The perfect illustration of this is even when it is a superposition of delta-diracs, the velocity of the coordinates in the entire volume move smoothly.The Euler-Lagrange equation () on diffeomorphisms for generalized functions formula_77 was derived in. In Riemannian Metric and Lie-Bracket Interpretation of the Euler-Lagrange Equation on Geodesics derivations are provided in terms of the adjoint operator and the Lie bracket for the group of diffeomorphisms. It has come to be called EPDiff equation for diffeomorphisms connecting to the Euler-Poincare method having been studied in the context of the inertial operator formula_78 for incompressible, divergence free, fluids.\n\nFor the momentum density case formula_79, then Euler–Lagrange equation has a classical solution:The Euler-Lagrange equation on diffeomorphisms, classically defined for momentum densities first appeared in for medical image analysis.\n\nIn Medical imaging and Computational anatomy, positioning and coordinatizing shapes are fundamental operations; the system for positioning anatomical coordinates and shapes built on the metric and the Euler-Lagrange equation a geodesic positioning system as first explicated in Miller Trouve and Younes.\nSolving the geodesic from the initial condition formula_80 is termed the Riemannian-exponential, a mapping formula_81 at identity to the group.\n\nThe Riemannian exponential satisfies formula_82 for initial condition formula_83, vector field dynamics formula_84,\n\n\nComputing the flow formula_80 onto coordinates Riemannian logarithm, mapping formula_92 at identity from formula_93 to vector field formula_94;\n\nformula_95\n\nExtended to the entire group they become\n\nformula_96 ; formula_97 .\n\nThese are inverses of each other for unique solutions of Logarithm; the first is called geodesic positioning, the latter geodesic coordinates (see Exponential map, Riemannian geometry for the finite dimensional version).The geodesic metric is a local flattening of the Riemannian coordinate system (see figure). \n\nIn Computational anatomy the diffeomorphisms are used to push the coordinate systems, and the vector fields are used\nas the control within the\nanatomical orbit or morphological space. The model is that of a dynamical system, the flow of coordinates formula_98 and the control the vector field formula_99 related via formula_100 The Hamiltonian view\n\nThis function is the extended Hamiltonian. The Pontryagin maximum principle gives the optimizing vector field which determines the geodesic flow satisfying formula_105 as well as the reduced Hamiltonian \nThe Lagrange multiplier in its action as a linear form has its own inner product of the canonical momentum acting on the velocity of the flow which is dependent on the shape, e.g. for landmarks a sum, for surfaces a surface integral, and. for volumes it is a volume integral with respect to formula_107 on formula_1. In all cases the Greens kernels carry weights which are the canonical momentum evolving according to an ordinary differential equation which corresponds to EL but is the geodesic reparameterization in canonical momentum. The optimizing vector field is given by\nwith dynamics of canonical momentum reparameterizing the vector field along the geodesic \n\nWhereas the vector fields are extended across the entire background space of formula_1, the geodesic flows associated to the submanifolds has Eulerian shape momentum which evolves as a generalized function formula_111 concentrated to the submanifolds. For landmarks the geodesics have Eulerian shape momentum which are a superposition of delta distributions travelling with the finite numbers of particles; the diffeomorphic flow of coordinates have velocities in the range of weighted Green's Kernels. For surfaces, the momentum is a surface integral of delta distributions travelling with the surface.\n\nThe geodesics connecting coordinate systems satisfying have stationarity of the Lagrangian. The Hamiltonian is given by the extremum along the path formula_112, formula_113, equalling the and is stationary along . Defining the geodesic velocity at the identity formula_114, then along the geodesic\n\nIn Computational anatomy the submanifolds are pointsets, curves, surfaces and subvolumes which are the basic primitives. The geodesic flows between the submanifolds determine the distance, and form the basic measuring and transporting tools of diffeomorphometry. At formula_120 the geodesic has vector field formula_123 determined by the conjugate momentum and the Green's kernel of the inertial operator defining the Eulerian momentum formula_124. The metric distance between coordinate systems connected via the geodesic determined by the induced distance between identity and group element:\n\nGiven the least-action there is a natural definition of momentum associated to generalized coordinates; the quantity acting against velocity gives energy. The field has studied two forms, the momentum associated to the Eulerian vector field termed Eulerian diffeomorphic shape momentum, and the momentum associated to the initial coordinates or canonical coordinates termed canonical diffeomorphic shape momentum. Each has a conservation law.The conservation of momentum goes hand in hand with the . In Computational anatomy, formula_64 is the Eulerian Momentum since when integrated against Eulerian velocity formula_65 gives energy density; operator formula_27 the generalized moment of inertia or inertial operator which acting on the Eulerian velocity gives momentum which is conserved along the geodesic: \n\nConservation of Eulerian shape momentum was shown in and follows from ; conservation of canonical momentum was shown in\n^T p_t</math>: \n\nLDDMM matching based on the entire tensor matrix\nhas group action becomes formula_130 transformed eigenvectors\n\nThe variational problem matching onto the principal eigenvector or the matrix is described\nLDDMM Tensor Image Matching.\n\nHigh angular resolution diffusion imaging (HARDI) addresses the well-known limitation of DTI, that is, DTI can only reveal one dominant fiber orientation at each location. HARDI measures diffusion along formula_132 uniformly distributed directions on the sphere and can characterize more complex fiber geometries. HARDI can be used to reconstruct an orientation distribution function (ODF) that characterizes the angular profile of the diffusion probability density function of water molecules. The ODF is a function defined on a unit sphere, formula_133.\n\nDense LDDMM ODF matching takes the HARDI data as ODF at each voxel and solves the LDDMM variational problem in the space of ODF. In the field of information geometry, the space of ODF forms a Riemannian manifold with the Fisher-Rao metric. For the purpose of LDDMM ODF mapping, the square-root representation is chosen because it is one of the most efficient representations found to date as the various Riemannian operations, such as geodesics, exponential maps, and logarithm maps, are available in closed form. In the following, denote square-root ODF (formula_134) as formula_135, where formula_135 is non-negative to ensure uniqueness and formula_137. The variational problem for matching assumes that two ODF volumes can be generated from one to another via flows of diffeomorphisms formula_138, which are solutions of ordinary differential equations \nformula_139 starting from\nthe identity map formula_140. Denote the action of the diffeomorphism on template as formula_141, formula_142, formula_143 are respectively the coordinates of the unit sphere, formula_144 and the image domain, with the target indexed similarly, formula_145,formula_142,formula_143.\n\nThe group action of the diffeomorphism on the template is given according to\nwhere formula_149 is the Jacobian of the affined transformed ODF and is defined as\nformula_150\nThis group action of diffeomorphisms on ODF reorients the ODF and reflects changes in both the magnitude of formula_151 and the sampling directions of formula_152 due to affine transformation. It guarantees that the volume fraction of fibers oriented toward a small patch must remain the same after the patch is transformed. \nThe LDDMM variational problem is defined as\n\nwhere the logarithm of formula_154 is defined as\nwhere formula_156 is the normal dot product between points in the sphere under the formula_157 metric.\n\nThis LDDMM-ODF mapping algorithm has been widely used to study brain white matter degeneration in aging, Alzheimer's disease, and vascular dementia. The brain white matter atlas generated based on ODF is constructed via Bayesian estimation. Regression analysis on ODF is developed in the ODF manifold space in.\n\nThe principle mode of variation represented by the orbit model is change of coordinates. For setting in which pairs of images are not related by diffeomorphisms but have photometric variation or image variation not represented by the template, active appearance modelling has been introduced, originally by Edwards-Cootes-Taylor and in 3D medical imaging in.\nIn the context of Computational Anatomy in which metrics on the anatomical orbit has been studied, metamorphosis for modelling structures such as tumors and photometric changes which are not resident in the template was introduced in for Magnetic Resonance image models, with many subsequent developments extending the metamorphosis framework.\n\nFor image matching the image metamorphosis framework enlarges the action so that formula_158 with action formula_159. In this setting metamorphosis combines both the diffeomorphic coordinate system transformation of Computational Anatomy as well as the early morphing technologies which only faded or modified the photometric or image intensity alone.\n\nThen the matching problem takes a form with equality boundary conditions:\n\nTransforming coordinate systems based on Landmark point or fiducial marker features dates back to Bookstein's early work on small deformation spline methods for interpolating correspondences defined by fiducial points to the two-dimensional or three-dimensional background space in which the fiducials are defined. Large deformation landmark methods came on in the late 90's. The above Figure depicts a series of landmarks associated three brain structures, the amygdala, entorhinal cortex, and hippocampus.\n\nMatching geometrical objects like unlabelled point distributions, curves or surfaces is another common problem in Computational Anatomy. Even in the discrete setting where these are commonly given as vertices with meshes, there are no predetermined correspondences between points as opposed to the situation of landmarks described above. From the theoretical point of view, while any submanifold formula_161 in formula_20, formula_163 can be parameterized in local charts formula_164, all reparametrizations of these charts give geometrically the same manifold. Therefore, early on in Computational anatomy, investigators have identified the necessity of parametrization invariant representations. One indispensable requirement is that the end-point matching term between two submanifolds is itself independent of their parametrizations. This can be achieved via concepts and methods borrowed from Geometric measure theory, in particular currents and varifolds which have been used extensively for curve and surface matching.\n\nDenoted the landmarked shape formula_165 with endpoint formula_166, the variational problem becomes\n\n</math>|}}The geodesic Eulerian momentum is a generalized function formula_167, supported on the landmarked set in the variational problem.The endpoint condition with conservation implies the initial momentum at the identity of the group:\n\nThe iterative algorithm \nfor large deformation diffeomorphic metric mapping for landmarks is given.\n\nGlaunes and co-workers first introduced diffeomorphic matching of pointsets in the general setting of matching distributions. As opposed to landmarks, this includes in particular the situation of weighted point clouds with no predefined correspondences and possibly different cardinalities. The template and target discrete point clouds are represented as two weighted sums of Diracs formula_169 and formula_170 living in the space of signed measures of formula_171. The space is equipped with a Hilbert metric obtained from a real positive kernel formula_172 on formula_171, giving the following norm:\n\nThe matching problem between a template and target point cloud may be then formulated using this kernel metric for the endpoint matching term:\n\nwhere formula_176 is the distribution transported by the deformation.\n\nIn the one dimensional case, a curve in 3D can be represented by an embedding formula_177, and the group action of \"Diff\" becomes formula_178. However, the correspondence between curves and embeddings is not one to one as the any reparametrization formula_179, for formula_180 a diffeomorphism of the interval [0,1], represents geometrically the same curve. In order to preserve this invariance in the end-point matching term, several extensions of the previous 0-dimensional measure matching approach can be considered.\n\nIn the situation of oriented curves, currents give an efficient setting to construct invariant matching terms. In such representation, curves are interpreted as elements of a functional space dual to the space vector fields, and compared through kernel norms on these spaces. Matching of two curves formula_181 and formula_182 writes eventually as the variational problem\n\nwith the endpoint term formula_184 is obtained from the norm\n\nthe derivative formula_186 being the tangent vector to the curve and formula_187 a given matrix kernel of formula_20. Such expressions are invariant to any positive reparametrizations of formula_181 and formula_190, and thus still depend on the orientation of the two curves.\n\nVarifold is an alternative to currents when orientation becomes an issue as for instance in situations involving multiple bundles of curves for which no \"consistent\" orientation may be defined. Varifolds directly extend 0-dimensional measures by adding an extra tangent space direction to the position of points, leading to represent curves as measures on the product of formula_20 and the Grassmannian of all straight lines in formula_20. The matching problem between two curves then consists in replacing the endpoint matching term by formula_193 with varifold norms of the form:\n\nwhere formula_195 is the non-oriented line directed by tangent vector formula_186 and formula_197 two scalar kernels respectively on formula_2 and the Grassmannian. Due to the inherent non-oriented nature of the Grassmannian representation, such expressions are invariant to positive and negative reparametrizations.\n\nSurface matching share many similarities with the case of curves. Surfaces in formula_20 are parametrized in local charts by embeddings formula_200, with all reparametrizations formula_201 with formula_202 a diffeomorphism of U being equivalent geometrically. Currents and varifolds can be also used to formalize surface matching.\n\nOriented surfaces can be represented as 2-currents which are dual to differential 2-forms. In formula_20, one can further identify 2-forms with vector fields through the standard wedge product of 3D vectors. In that setting, surface matching writes again:\n\nwith the endpoint term formula_184 given through the norm\n\nwith formula_207 the normal vector to the surface parametrized by formula_208.\n\nThis surface mapping algorithm has been validated for brain cortical surfaces against CARET and FreeSurfer. LDDMM mapping for multiscale surfaces is discussed in.\n\nFor non-orientable or non-oriented surfaces, the varifold framework is often more adequate. Identifying the parametric surface formula_208 with a varifold formula_210 in the space of measures on the product of formula_20 and the Grassmannian, one simply replaces the previous current metric formula_212 by:\n\nwhere formula_214 is the (non-oriented) line directed by the normal vector to the surface.\n\nThere are many settings in which there are a series of measurements, a time-series to which the underlying\ncoordinate systems will be matched and flowed onto. This occurs for example\nin the dynamic growth and atrophy models and motion tracking such as have been explored in\nAn observed time sequence is given and the goal is to infer the time flow of geometric change of coordinates carrying the exemplars or templars through the period of observations.\n\nThe generic time-series matching problem considers the series of times is formula_215. The flow optimizes at the series of costs formula_216 giving optimization problems of the form\n\nThere have been at least three solutions offered thus far, piecewise geodesic, principal geodesic and splines.\n\nThe random orbit model of Computational Anatomy first appeared in modelling the change in coordinates associated to the randomness of the group acting on the templates, which induces the randomness on the source of images in the anatomical orbit of shapes and forms and resulting observations through the medical imaging devices. Such a random orbit model in which randomness on the group induces randomness on the images was examined for the Special Euclidean Group for object recognition in.\n\nDepicted in the figure is a depiction of the random orbits around each exemplar, formula_218, generated by randomizing the flow by generating the initial tangent space vector field at the identity formula_94, and then generating random object formula_220.\n\nThe random orbit model induces the prior on shapes and images formula_221 conditioned on a particular atlas formula_222. For this the generative model generates the mean field formula_223 as a random change in coordinates of the template according to formula_224, where the diffeomorphic change in coordinates is generated randomly via the geodesic flows. The prior on random transformations formula_225 on formula_226 is induced by the flow formula_227, with formula_228 constructed as a Gaussian random field prior formula_229. The density on the random observables at the output of the sensor formula_230 are given by\nformula_231\n\nShown in the Figure on the right the cartoon orbit, are a random spray of the subcortical manifolds generated by randomizing the vector fields formula_118 supported over the submanifolds.\n\nThe central statistical model of Computational Anatomy in the context of medical imaging has been the source-channel model of Shannon theory; the source is the deformable template of images formula_233, the channel outputs are the imaging sensors with observables formula_234 (see Figure).\n\nSee The Bayesian model of computational anatomy for discussions (i) MAP estimation with multiple atlases, (ii)\nMAP segmentation with multiple atlases, MAP estimation of templates from populations.\n\nShape in computational anatomy is a local theory, indexing shapes and structures to templates to which they are bijectively mapped. Statistical shape in Computational Anatomy is the empirical study of diffeomorphic correspondences between populations and common template coordinate systems. This is a strong departure from Procrustes Analyses and shape theories pioneered by David G. Kendall in that the central group of Kendall's theories are the finite-dimensional Lie groups, whereas the theories of shape in Computational Anatomy have focused on the diffeomorphism group, which to first order via the Jacobian can be thought of as a field–thus infinite dimensional–of low-dimensional Lie groups of scale and rotations. \n\nThe random orbit model provides the natural setting to understand empirical shape and shape statistics within Computational anatomy since the non-linearity of the induced probability law on anatomical shapes and forms \nformula_6 is induced via the reduction to the vector fields formula_236 at the tangent space at the identity of the diffeomorphism group. The successive flow of the Euler equation induces the random space of shapes and forms formula_237.\n\nPerforming empirical statistics on this tangent space at the identity is the natural way for inducing probability laws on the statistics of shape. Since both the vector fields and the Eulerian momentum formula_119 are in a Hilbert space the natural model is one of a Gaussian random field, so that given test function formula_239, then the inner-products with the test functions are Gaussian distributed with mean and covariance.\n\nThis is depicted in the accompanying figure where sub-cortical brain structures are depicted in a two-dimensional coordinate system based on inner products of their initial vector fields that generate them from the template is shown in a 2-dimensional span of the Hilbert space.\n\nThe study of shape and statistics in populations are local theories, indexing shapes and structures to templates to which they are bijectively mapped. Statistical shape is then the study of diffeomorphic correspondences relative to the template. A core operation is the generation of templates from populations, estimating a shape that is matched to the population. There are several important methods for generating templates including methods based on Frechet averaging, and statistical approaches based on the expectation-maximization algorithm and the Bayes Random orbit models of Computational anatomy. Shown in the accompanying figure is a subcortical template reconstruction from the population of MRI subjects.\n\nSoftware suites containing a variety of diffeomorphic mapping algorithms include the following:\n\n\n\n"}
{"id": "8082", "url": "https://en.wikipedia.org/wiki?curid=8082", "title": "Diamond", "text": "Diamond\n\nDiamond is a solid form of the element carbon with its atoms arranged in a crystal structure called diamond cubic. At room temperature and pressure, another solid form of carbon known as graphite is the chemically stable form, but diamond almost never converts to it. Diamond has the highest hardness and thermal conductivity of any natural material, properties that are utilized in major industrial applications such as cutting and polishing tools. They are also the reason that diamond anvil cells can subject materials to pressures found deep in the Earth.\n\nBecause the arrangement of atoms in diamond is extremely rigid, few types of impurity can contaminate it (two exceptions being boron and nitrogen). Small numbers of defects or impurities (about one per million of lattice atoms) color diamond blue (boron), yellow (nitrogen), brown (defects), green (radiation exposure), purple, pink, orange or red. Diamond also has relatively high optical dispersion (ability to disperse light of different colors).\n\nMost natural diamonds have ages between 1 billion and 3.5 billion years. Most were formed at depths between in the Earth's mantle, although a few have come from as deep as . Under high pressure and temperature, carbon-containing fluids dissolved minerals and replaced them with diamonds. Much more recently (tens to hundreds of million years ago), they were carried to the surface in volcanic eruptions and deposited in igneous rocks known as kimberlites and lamproites.\n\nSynthetic diamonds can be grown from high-purity carbon under high pressures and temperatures or from hydrocarbon gas by chemical vapor deposition (CVD). Imitation diamonds can also be made out of materials such as cubic zirconia and silicon carbide. Natural, synthetic and imitation diamonds are most commonly distinguished using optical techniques or thermal conductivity measurements.\n\nDiamond is a solid form of pure carbon with its atoms arranged in a crystal. Solid carbon comes in different forms known as allotropes depending on the type of chemical bond. The two most common allotropes of pure carbon are diamond and graphite. In graphite the bonds are sp orbital hybrids and the atoms form in planes with each bound to three nearest neighbors 120 degrees apart. In diamond they are sp and the atoms form tetrahedra with each bound to four nearest neighbors. Tetrahedra are rigid, the bonds are strong, and of all known substances diamond has the greatest number of atoms per unit volume, which is why it is both the hardest and the least compressible. It also has a high density, ranging from 3150 to 3530 kilograms per cubic metre (over 3 times the density of water) in natural diamonds and in pure diamond. In graphite, the bonds between nearest neighbors are even stronger but the bonds between planes are weak, so the planes can easily slip past each other. Thus, graphite is much softer than diamond. However, the stronger bonds make graphite less flammable.\n\nDiamonds have been adapted for many uses because of the material's exceptional physical characteristics. Most notable are its extreme hardness and thermal conductivity (900–), as well as wide bandgap and high optical dispersion. Diamond's ignition point is 720– in oxygen and 850– in air. \n\nThe equilibrium pressure and temperature conditions for a transition between graphite and diamond is well established theoretically and experimentally. The pressure changes linearly between at and at (the diamond/graphite/liquid triple point).\nHowever, the phases have a wide region about this line where they can coexist. At normal temperature and pressure, and , the stable phase of carbon is graphite, but diamond is metastable and its rate of conversion to graphite is negligible. However, at temperatures above about , diamond rapidly converts to graphite. Rapid conversion of graphite to diamond requires pressures well above the equilibrium line: at , a pressure of is needed.\n\nAbove the triple point, the melting point of diamond increases slowly with increasing pressure; but at pressures of hundreds of GPa, it decreases. At high pressures, silicon and germanium have a BC8 body-centered cubic crystal structure, and a similar structure is predicted for carbon at high pressures. At , the transition is predicted to occur at . \nThe most common crystal structure of diamond is called diamond cubic. It is formed of unit cells (see the figure) stacked together. Although there are 18 atoms in the figure, each corner atom is shared by eight unit cells and each atom in the center of a face is shared by two, so there are a total of eight atoms per unit cell. Each side of the unit cell is 3.57 angstroms in length.\n\nA diamond cubic lattice can be thought of as two interpenetrating face-centered cubic lattices with one displaced by 1/4 of the diagonal along a cubic cell, or as one lattice with two atoms associated with each lattice point. Looked at from a crystallographic direction, it is formed of layers stacked in a repeating ABCABC ... pattern. Diamonds can also form an ABAB ... structure, which is known as hexagonal diamond or lonsdaleite, but this is far less common and is formed under different conditions from cubic carbon.\n\nDiamonds occur most often as euhedral or rounded octahedra and twinned octahedra known as \"macles\". As diamond's crystal structure has a cubic arrangement of the atoms, they have many facets that belong to a cube, octahedron, rhombicosidodecahedron, tetrakis hexahedron or disdyakis dodecahedron. The crystals can have rounded off and unexpressive edges and can be elongated. Diamonds (especially those with rounded crystal faces) are commonly found coated in \"nyf\", an opaque gum-like skin.\n\nSome diamonds have opaque fibers. They are referred to as \"opaque\" if the fibers grow from a clear substrate or \"fibrous\" if they occupy the entire crystal. Their colors range from yellow to green or gray, sometimes with cloud-like white to gray impurities. Their most common shape is cuboidal, but they can also form octahedra, dodecahedra, macles or combined shapes. The structure is the result of numerous impurities with sizes between 1 and 5 microns. These diamonds probably formed in kimberlite magma and sampled the volatiles.\n\nDiamonds can also form polycrystalline aggregates. There have been attempts to classify them into groups with names such as boart, ballas, stewartite and framesite, but there is no widely accepted set of criteria. Carbonado, a type in which the diamond grains were sintered (fused without melting by the application of heat and pressure), is black in color and tougher than single crystal diamond. It has never been observed in a volcanic rock. There are many theories for its origin, including formation in a star, but no consensus.\n\nDiamond is the hardest known natural material on both the Vickers scale and the Mohs scale. Diamond's great hardness relative to other materials has been known since antiquity, and is the source of its name.\n\nDiamond hardness depends on its purity, crystalline perfection and orientation: hardness is higher for flawless, pure crystals oriented to the <111> direction (along the longest diagonal of the cubic diamond lattice). Therefore, whereas it might be possible to scratch some diamonds with other materials, such as boron nitride, the hardest diamonds can only be scratched by other diamonds and nanocrystalline diamond aggregates.\n\nThe hardness of diamond contributes to its suitability as a gemstone. Because it can only be scratched by other diamonds, it maintains its polish extremely well. Unlike many other gems, it is well-suited to daily wear because of its resistance to scratching—perhaps contributing to its popularity as the preferred gem in engagement or wedding rings, which are often worn every day.\n\nThe hardest natural diamonds mostly originate from the Copeton and Bingara fields located in the New England area in New South Wales, Australia. These diamonds are generally small, perfect to semiperfect octahedra, and are used to polish other diamonds. Their hardness is associated with the crystal growth form, which is single-stage crystal growth. Most other diamonds show more evidence of multiple growth stages, which produce inclusions, flaws, and defect planes in the crystal lattice, all of which affect their hardness. It is possible to treat regular diamonds under a combination of high pressure and high temperature to produce diamonds that are harder than the diamonds used in hardness gauges.\n\nSomewhat related to hardness is another mechanical property \"toughness\", which is a material's ability to resist breakage from forceful impact. The toughness of natural diamond has been measured as 7.5–10 MPa·m. This value is good compared to other ceramic materials, but poor compared to most engineering materials such as engineering alloys, which typically exhibit toughnesses over 100 MPa·m. As with any material, the macroscopic geometry of a diamond contributes to its resistance to breakage. Diamond has a cleavage plane and is therefore more fragile in some orientations than others. Diamond cutters use this attribute to cleave some stones, prior to faceting. \"Impact toughness\" is one of the main indexes to measure the quality of synthetic industrial diamonds.\n\nDiamond has compressive yield strength of 130–140GPa. This exceptionally high value, along with the hardness and transparency of diamond, are the reasons that diamond anvil cells are the main tool for high pressure experiments. These anvils have reached pressures of . Much higher pressures may be possible with nanocrystalline diamonds.\n\nOther specialized applications also exist or are being developed, including use as semiconductors: some blue diamonds are natural semiconductors, in contrast to most diamonds, which are excellent electrical insulators. The conductivity and blue color originate from boron impurity. Boron substitutes for carbon atoms in the diamond lattice, donating a hole into the valence band.\n\nSubstantial conductivity is commonly observed in nominally undoped diamond grown by chemical vapor deposition. This conductivity is associated with hydrogen-related species adsorbed at the surface, and it can be removed by annealing or other surface treatments.\n\nDiamonds are naturally lipophilic and hydrophobic, which means the diamonds' surface cannot be wet by water, but can be easily wet and stuck by oil. This property can be utilized to extract diamonds using oil when making synthetic diamonds. However, when diamond surfaces are chemically modified with certain ions, they are expected to become so hydrophilic that they can stabilize multiple layers of water ice at human body temperature.\n\nThe surface of diamonds is partially oxidized. The oxidized surface can be reduced by heat treatment under hydrogen flow. That is to say, this heat treatment partially removes oxygen-containing functional groups. But diamonds (spC) are unstable against high temperature (above about ) under atmospheric pressure. The structure gradually changes into spC above this temperature. Thus, diamonds should be reduced under this temperature.\n\nDiamonds are not very reactive. Under room temperature diamonds do not react with any chemical reagents including strong acids and bases. A diamond's surface can only be oxidized at temperatures above about in air. Diamond also reacts with fluorine gas above about .\n\nDiamond has a wide bandgap of corresponding to the deep ultraviolet wavelength of 225 nanometers. This means that pure diamond should transmit visible light and appear as a clear colorless crystal. Colors in diamond originate from lattice defects and impurities. The diamond crystal lattice is exceptionally strong, and only atoms of nitrogen, boron and hydrogen can be introduced into diamond during the growth at significant concentrations (up to atomic percents). Transition metals nickel and cobalt, which are commonly used for growth of synthetic diamond by high-pressure high-temperature techniques, have been detected in diamond as individual atoms; the maximum concentration is 0.01% for nickel and even less for cobalt. Virtually any element can be introduced to diamond by ion implantation.\n\nNitrogen is by far the most common impurity found in gem diamonds and is responsible for the yellow and brown color in diamonds. Boron is responsible for the blue color. Color in diamond has two additional sources: irradiation (usually by alpha particles), that causes the color in green diamonds, and plastic deformation of the diamond crystal lattice. Plastic deformation is the cause of color in some brown and perhaps pink and red diamonds. In order of increasing rarity, yellow diamond is followed by brown, colorless, then by blue, green, black, pink, orange, purple, and red. \"Black\", or Carbonado, diamonds are not truly black, but rather contain numerous dark inclusions that give the gems their dark appearance. Colored diamonds contain impurities or structural defects that cause the coloration, while pure or nearly pure diamonds are transparent and colorless. Most diamond impurities replace a carbon atom in the crystal lattice, known as a carbon flaw. The most common impurity, nitrogen, causes a slight to intense yellow coloration depending upon the type and concentration of nitrogen present. The Gemological Institute of America (GIA) classifies low saturation yellow and brown diamonds as diamonds in the \"normal color range\", and applies a grading scale from \"D\" (colorless) to \"Z\" (light yellow). Diamonds of a different color, such as blue, are called \"fancy colored\" diamonds and fall under a different grading scale.\n\nIn 2008, the Wittelsbach Diamond, a blue diamond once belonging to the King of Spain, fetched over US$24 million at a Christie's auction. In May 2009, a blue diamond fetched the highest price per carat ever paid for a diamond when it was sold at auction for 10.5 million Swiss francs (6.97 million euros, or US$9.5 million at the time). That record was, however, beaten the same year: a vivid pink diamond was sold for $10.8 million in Hong Kong on December 1, 2009.\n\nDiamonds can be identified by their high thermal conductivity. Their high refractive index is also indicative, but other materials have similar refractivity. Diamonds cut glass, but this does not positively identify a diamond because other materials, such as quartz, also lie above glass on the Mohs scale and can also cut it. Diamonds can scratch other diamonds, but this can result in damage to one or both stones. Hardness tests are infrequently used in practical gemology because of their potentially destructive nature. The extreme hardness and high value of diamond means that gems are typically polished slowly, using painstaking traditional techniques and greater attention to detail than is the case with most other gemstones; these tend to result in extremely flat, highly polished facets with exceptionally sharp facet edges. Diamonds also possess an extremely high refractive index and fairly high dispersion. Taken together, these factors affect the overall appearance of a polished diamond and most diamantaires still rely upon skilled use of a loupe (magnifying glass) to identify diamonds \"by eye\".\n\nDiamonds are extremely rare, with concentrations of at most parts per billion in source rock. Before the 20th century, most diamonds were found in alluvial deposits. Loose diamonds are also found along existing and ancient shorelines, where they tend to accumulate because of their size and density. Rarely, they have been found in glacial till (notably in Wisconsin and Indiana), but these deposits are not of commercial quality. These types of deposit were derived from localized igneous intrusions through weathering and transport by wind or water.\n\nMost diamonds come from the Earth's mantle, and most of this section discusses those diamonds. However, there are other sources. Some blocks of the crust, or terranes, have been buried deep enough as the crust thickened so they experienced ultra-high-pressure metamorphism. These have evenly distributed \"microdiamonds\" that show no sign of transport by magma. In addition, when meteorites strike the ground, the shock wave can produce high enough temperatures and pressures for \"microdiamonds\" and \"nanodiamonds\" to form. Impact-type microdiamonds can be used as an indicator of ancient impact craters. Popigai crater in Russia may have the world's largest diamond deposit, estimated at trillions of carats, and formed by an asteroid impact.\n\nA common misconception is that diamonds are formed from highly compressed coal. Coal is formed from buried prehistoric plants, and most diamonds that have been dated are far older than the first land plants. It is possible that diamonds can form from coal in subduction zones, but diamonds formed in this way are rare, and the carbon source is more likely carbonate rocks and organic carbon in sediments, rather than coal.\n\nDiamonds are far from evenly distributed over the Earth. A rule of thumb known as Clifford's rule states that they are almost always found in kimberlites on the oldest part of cratons, the stable cores of continents with typical ages of 2.5 billion years or more. However, there are exceptions. The Argyle diamond mine in Australia, the largest producer of diamonds by weight in the world, is located in a \"mobile belt\", also known as an \"orogenic belt\", a weaker zone surrounding the central craton that has undergone compressional tectonics. Instead of kimberlite, the host rock is lamproite. Lamproites with diamonds that are not economically viable are also found in the United States, India and Australia. In addition, diamonds in the Wawa belt of the Superior province in Canada and microdiamonds in the island arc of Japan are found in a type of rock called lamprophyre.\n\nKimberlites can be found in narrow (1–4 meters) dikes and sills, and in pipes with diameters that range from about 75 meters to 1.5 kilometers. Fresh rock is dark bluish green to greenish gray, but after exposure rapidly turns brown and crumbles. It is hybrid rock with a chaotic mixture of small minerals and rock fragments (clasts) up to the size of watermelons. They are a mixture of xenocrysts and xenoliths (minerals and rocks carried up from the lower crust and mantle), pieces of surface rock, altered minerals such as serpentine, and new minerals that crystallized during the eruption. The texture varies with depth. The composition forms a continuum with carbonatites, but the latter have too much oxygen for carbon to exist in a pure form. Instead, it is locked up in the mineral calcite (CaCO).\n\nAll three of the diamond-bearing rocks (kimberlite, lamproite and lamprophyre) lack certain minerals (melilite and kalsilite) that are incompatible with diamond formation. In kimberlite, olivine is large and conspicuous, while lamproite has Ti-phlogopite and lamprophyre has biotite and amphibole. They are all derived from magma types that erupt rapidly from small amounts of melt, are rich in volatiles and magnesium oxide, and are less oxidizing than more common mantle melts such as basalt. These characteristics allow the melts to carry diamonds to the surface before they dissolve.\n\nKimberlite pipes can be difficult to find. They weather quickly (within a few years after exposure) and tend to have lower topographic relief than surrounding rock. If they are visible in outcrops, the diamonds are never visible because they are so rare. In any case, kimberlites are often covered with vegetation, sediments, soils or lakes. In modern searches, geophysical methods such as aeromagnetic surveys, electrical resistivity and gravimetry, help identify promising regions to explore. This is aided by isotopic dating and modeling of the geological history. Then surveyors must go to the area and collect samples, looking for kimberlite fragments or \"indicator minerals\". The latter have compositions that reflect the conditions where diamonds form, such as extreme melt depletion or high pressures in eclogites. However, indicator minerals can be misleading; a better approach is geothermobarometry, where the compositions of minerals are analyzed as if they were in equilibrium with mantle minerals.\n\nFinding kimberlites requires persistence, and only a small fraction contain diamonds that are commercially viable. The only major discoveries since about 1980 have been in Canada. Since existing mines have lifetimes of as little as 25 years, there could be a shortage of new diamonds in the future.\n\nDiamonds are dated by analyzing inclusions using the decay of radioactive isotopes. Depending on the elemental abundances, one can look at the decay of rubidium to strontium, samarium to neodymium, uranium to lead, argon-40 to argon-39, or rhenium to osmium. Those found in kimberlites have ages ranging from 1 to 3.5 billion years, and there can be multiple ages in the same kimberlite, indicating multiple episodes of diamond formation. The kimberlites themselves are much younger. Most of them have ages between tens of millions and 300 million years old, although there are some older exceptions (Argyle, Premier and Wawa). Thus, the kimberlites formed independently of the diamonds and served only to transport them to the surface. Kimberlites are also much younger than the cratons they have erupted through. The reason for the lack of older kimberlites is unknown, but it suggests there was some change in mantle chemistry or tectonics. No kimberlite has erupted in human history.\n\nMost gem-quality diamonds come from depths of 150 to 250 kilometers in the lithosphere. Such depths occur below cratons in \"mantle keels\", the thickest part of the lithosphere. These regions have high enough pressure and temperature to allow diamonds to form and they are not convecting, so diamonds can be stored for billions of years until a kimberlite eruption samples them.\n\nHost rocks in a mantle keel include harzburgite and lherzolite, two type of peridotite. The most dominant rock type in the upper mantle, peridotite is an igneous rock consisting mostly of the minerals olivine and pyroxene; it is low in silica and high in magnesium. However, diamonds in peridotite rarely survive the trip to the surface. Another common source that does keep diamonds intact is eclogite, a metamorphic rock that typically forms from basalt as an oceanic plate plunges into the mantle at a subduction zone.\n\nA smaller fraction of diamonds (about 150 have been studied) come from depths of 330–660 kilometers, a region that includes the transition zone. They formed in eclogite but are distinguished from diamonds of shallower origin by inclusions of majorite (a form of garnet with excess silicon). A similar proportion of diamonds comes from the lower mantle at depths between 660 and 800 kilometers.\n\nDiamond is thermodynamically stable at high pressures and temperatures, with the phase transition from graphite occurring at greater temperatures as the pressure increases. Thus, underneath continents it becomes stable at temperatures of 950 degrees Celsius and pressures of 4.5 gigapascals, corresponding to depths of 150 kilometers or greater. In subduction zones, which are colder, it becomes stable at temperatures of 800 degrees C and pressures of 3.5 gigapascals. At depths greater than 240 km, iron-nickel metal phases are present and carbon is likely to be either dissolved in them or in the form of carbides. Thus, the deeper origin of some diamonds may reflect unusual growth environments.\n\nIn 2018 the first known natural samples of a phase of ice called Ice VII were found as inclusions in diamond samples. The inclusions formed at depths between 400 and 800 kilometers, straddling the upper and lower mantle, and provide evidence for water-rich fluid at these depths.\n\nThe amount of carbon in the mantle is not well constrained, but its concentration is estimated at 0.5 to 1 parts per thousand. It has two stable isotopes, C and C, in a ratio of approximately 99:1 by mass. This ratio has a wide range in meteorites, which implies that it was probably also broad in the early Earth. It can also be altered by surface processes like photosynthesis. The fraction is generally compared to a standard sample using a ratio δC expressed in parts per thousand. Common rocks from the mantle such as basalts, carbonatites and kimberlites have ratios between -8 and -2. On the surface, organic sediments have an average of -25 while carbonates have an average of 0.\n\nPopulations of diamonds from different sources have distributions of δC that vary markedly. Peridotitic diamonds are mostly within the typical mantle range; eclogitic diamonds have values from -40 to +3, although the peak of the distribution is in the mantle range. This variability implies that they are not formed from carbon that is \"primordial\" (having resided in the mantle since the Earth formed). Instead, they are the result of tectonic processes, although (given the ages of diamonds) not necessarily the same tectonic processes that act in the present.\n\nDiamonds in the mantle form through a \"metasomatic\" process where a C-O-H-N-S fluid or melt dissolves minerals in a rock and replaces them with new minerals. (The vague term C-O-H-N-S is commonly used because the exact composition is not known.) Diamonds form from this fluid either by reduction of oxidized carbon (e.g., CO or CO) or oxidation of a reduced phase such as methane.\n\nUsing probes such as polarized light, photoluminescence and cathodoluminescence, a series of growth zones can be identified in diamonds. The characteristic pattern in diamonds from the lithosphere involves a nearly concentric series of zones with very thin oscillations in luminescence and alternating episodes where the carbon is resorbed by the fluid and then grown again. Diamonds from below the lithosphere have a more irregular, almost polycrystalline texture, reflecting the higher temperatures and pressures as well as the transport of the diamonds by convection.\n\nGeological evidence supports a model in which kimberlite magma rose at 4–20 meters per second, creating an upward path by hydraulic fracturing of the rock. As the pressure decreases, a vapor phase exsolves from the magma, and this helps to keep the magma fluid. At the surface, the initial eruption explodes out through fissures at high speeds (over 200 meters per second). Then, at lower pressures, the rock is eroded, forming a pipe and producing fragmented rock (breccia). As the eruption wanes, there is pyroclastic phase and then metamorphism and hydration produces serpentinites.\n\nAlthough diamonds on Earth are rare, they are very common in space. In meteorites, about 3 percent of the carbon is in the form of nanodiamonds, having diameters of a few nanometers. Sufficiently small diamonds can form in the cold of space because their lower surface energy makes them more stable than graphite. The isotopic signatures of some nanodiamonds indicate they were formed outside the Solar System in stars.\n\nHigh pressure experiments predict that large quantities of diamonds condense from methane into a \"diamond rain\" on the ice giant planets Uranus and Neptune. Some extrasolar planets may be almost entirely composed of diamond.\n\nDiamonds may exist in carbon-rich stars, particularly white dwarfs. One theory for the origin of carbonado, the toughest form of diamond, is that it originated in a white dwarf or supernova. Diamonds formed in stars may have been the first minerals.\n\nThe most familiar uses of diamonds today are as gemstones used for adornment, and as industrial abrasives for cutting hard materials. The markets for gem-grade and industrial-grade diamonds value diamonds differently.\n\nThe dispersion of white light into spectral colors is the primary gemological characteristic of gem diamonds. In the 20th century, experts in gemology developed methods of grading diamonds and other gemstones based on the characteristics most important to their value as a gem. Four characteristics, known informally as the \"four Cs\", are now commonly used as the basic descriptors of diamonds: these are its mass in \"carats\" (a carat begin equal to 0.2 grams), \"cut\" (quality of the cut is graded according to proportions, symmetry and polish), \"color\" (how close to white or colorless; for fancy diamonds how intense is its hue), and \"clarity\" (how free is it from inclusions). A large, flawless diamond is known as a paragon.\n\nA large trade in gem-grade diamonds exists. Although most gem-grade diamonds are sold newly polished, there is a well-established market for resale of polished diamonds (e.g. pawnbroking, auctions, second-hand jewelry stores, diamantaires, bourses, etc.). One hallmark of the trade in gem-quality diamonds is its remarkable concentration: wholesale trade and diamond cutting is limited to just a few locations; in 2003, 92% of the world's diamonds were cut and polished in Surat, India. Other important centers of diamond cutting and trading are the Antwerp diamond district in Belgium, where the International Gemological Institute is based, London, the Diamond District in New York City, the Diamond Exchange District in Tel Aviv, and Amsterdam. One contributory factor is the geological nature of diamond deposits: several large primary kimberlite-pipe mines each account for significant portions of market share (such as the Jwaneng mine in Botswana, which is a single large-pit mine that can produce between of diamonds per year). Secondary alluvial diamond deposits, on the other hand, tend to be fragmented amongst many different operators because they can be dispersed over many hundreds of square kilometers (e.g., alluvial deposits in Brazil).\n\nThe production and distribution of diamonds is largely consolidated in the hands of a few key players, and concentrated in traditional diamond trading centers, the most important being Antwerp, where 80% of all rough diamonds, 50% of all cut diamonds and more than 50% of all rough, cut and industrial diamonds combined are handled. This makes Antwerp a de facto \"world diamond capital\". The city of Antwerp also hosts the Antwerpsche Diamantkring, created in 1929 to become the first and biggest diamond bourse dedicated to rough diamonds. Another important diamond center is New York City, where almost 80% of the world's diamonds are sold, including auction sales.\n\nThe De Beers company, as the world's largest diamond mining company, holds a dominant position in the industry, and has done so since soon after its founding in 1888 by the British imperialist Cecil Rhodes. De Beers is currently the world's largest operator of diamond production facilities (mines) and distribution channels for gem-quality diamonds. The Diamond Trading Company (DTC) is a subsidiary of De Beers and markets rough diamonds from De Beers-operated mines. De Beers and its subsidiaries own mines that produce some 40% of annual world diamond production. For most of the 20th century over 80% of the world's rough diamonds passed through De Beers, but by 2001–2009 the figure had decreased to around 45%, and by 2013 the company's market share had further decreased to around 38% in value terms and even less by volume. De Beers sold off the vast majority of its diamond stockpile in the late 1990s – early 2000s and the remainder largely represents working stock (diamonds that are being sorted before sale). This was well documented in the press but remains little known to the general public.\n\nAs a part of reducing its influence, De Beers withdrew from purchasing diamonds on the open market in 1999 and ceased, at the end of 2008, purchasing Russian diamonds mined by the largest Russian diamond company Alrosa. As of January 2011, De Beers states that it only sells diamonds from the following four countries: Botswana, Namibia, South Africa and Canada. Alrosa had to suspend their sales in October 2008 due to the global energy crisis, but the company reported that it had resumed selling rough diamonds on the open market by October 2009. Apart from Alrosa, other important diamond mining companies include BHP Billiton, which is the world's largest mining company; Rio Tinto Group, the owner of the Argyle (100%), Diavik (60%), and Murowa (78%) diamond mines; and Petra Diamonds, the owner of several major diamond mines in Africa.\n\nFurther down the supply chain, members of The World Federation of Diamond Bourses (WFDB) act as a medium for wholesale diamond exchange, trading both polished and rough diamonds. The WFDB consists of independent diamond bourses in major cutting centers such as Tel Aviv, Antwerp, Johannesburg and other cities across the USA, Europe and Asia. In 2000, the WFDB and The International Diamond Manufacturers Association established the World Diamond Council to prevent the trading of diamonds used to fund war and inhumane acts. WFDB's additional activities include sponsoring the World Diamond Congress every two years, as well as the establishment of the \"International Diamond Council\" (IDC) to oversee diamond grading.\n\nOnce purchased by Sightholders (which is a trademark term referring to the companies that have a three-year supply contract with DTC), diamonds are cut and polished in preparation for sale as gemstones ('industrial' stones are regarded as a by-product of the gemstone market; they are used for abrasives). The cutting and polishing of rough diamonds is a specialized skill that is concentrated in a limited number of locations worldwide. Traditional diamond cutting centers are Antwerp, Amsterdam, Johannesburg, New York City, and Tel Aviv. Recently, diamond cutting centers have been established in China, India, Thailand, Namibia and Botswana. Cutting centers with lower cost of labor, notably Surat in Gujarat, India, handle a larger number of smaller carat diamonds, while smaller quantities of larger or more valuable diamonds are more likely to be handled in Europe or North America. The recent expansion of this industry in India, employing low cost labor, has allowed smaller diamonds to be prepared as gems in greater quantities than was previously economically feasible.\n\nDiamonds prepared as gemstones are sold on diamond exchanges called \"bourses\". There are 28 registered diamond bourses in the world. Bourses are the final tightly controlled step in the diamond supply chain; wholesalers and even retailers are able to buy relatively small lots of diamonds at the bourses, after which they are prepared for final sale to the consumer. Diamonds can be sold already set in jewelry, or sold unset (\"loose\"). According to the Rio Tinto Group, in 2002 the diamonds produced and released to the market were valued at US$9 billion as rough diamonds, US$14 billion after being cut and polished, US$28 billion in wholesale diamond jewelry, and US$57 billion in retail sales.\n\nMined rough diamonds are converted into gems through a multi-step process called \"cutting\".\nDiamonds are extremely hard, but also brittle and can be split up by a single blow. Therefore, diamond cutting is traditionally considered as a delicate procedure requiring skills, scientific knowledge, tools and experience. Its final goal is to produce a faceted jewel where the specific angles between the facets would optimize the diamond luster, that is dispersion of white light, whereas the number and area of facets would determine the weight of the final product. The weight reduction upon cutting is significant and can be of the order of 50%. Several possible shapes are considered, but the final decision is often determined not only by scientific, but also practical considerations. For example, the diamond might be intended for display or for wear, in a ring or a necklace, singled or surrounded by other gems of certain color and shape. Some of them may be considered as classical, such as round, pear, marquise, oval, hearts and arrows diamonds, etc. Some of them are special, produced by certain companies, for example, Phoenix, Cushion, Sole Mio diamonds, etc.\n\nThe most time-consuming part of the cutting is the preliminary analysis of the rough stone. It needs to address a large number of issues, bears much responsibility, and therefore can last years in case of unique diamonds. The following issues are considered:\n\nAfter initial cutting, the diamond is shaped in numerous stages of polishing. Unlike cutting, which is a responsible but quick operation, polishing removes material by gradual erosion and is extremely time consuming. The associated technique is well developed; it is considered as a routine and can be performed by technicians. After polishing, the diamond is reexamined for possible flaws, either remaining or induced by the process. Those flaws are concealed through various diamond enhancement techniques, such as repolishing, crack filling, or clever arrangement of the stone in the jewelry. Remaining non-diamond inclusions are removed through laser drilling and filling of the voids produced.\n\nMarketing has significantly affected the image of diamond as a valuable commodity.\n\nN. W. Ayer & Son, the advertising firm retained by De Beers in the mid-20th century, succeeded in reviving the American diamond market. And the firm created new markets in countries where no diamond tradition had existed before. N. W. Ayer's marketing included product placement, advertising focused on the diamond product itself rather than the De Beers brand, and associations with celebrities and royalty. Without advertising the De Beers brand, De Beers was advertising its competitors' diamond products as well, but this was not a concern as De Beers dominated the diamond market throughout the 20th century. De Beers' market share dipped temporarily to 2nd place in the global market below Alrosa in the aftermath of the global economic crisis of 2008, down to less than 29% in terms of carats mined, rather than sold. The campaign lasted for decades but was effectively discontinued by early 2011. De Beers still advertises diamonds, but the advertising now mostly promotes its own brands, or licensed product lines, rather than completely \"generic\" diamond products. The campaign was perhaps best captured by the slogan \"a diamond is forever\". This slogan is now being used by De Beers Diamond Jewelers, a jewelry firm which is a 50%/50% joint venture between the De Beers mining company and LVMH, the luxury goods conglomerate.\n\nBrown-colored diamonds constituted a significant part of the diamond production, and were predominantly used for industrial purposes. They were seen as worthless for jewelry (not even being assessed on the diamond color scale). After the development of Argyle diamond mine in Australia in 1986, and marketing, brown diamonds have become acceptable gems. The change was mostly due to the numbers: the Argyle mine, with its of diamonds per year, makes about one-third of global production of natural diamonds; 80% of Argyle diamonds are brown.\n\nIndustrial diamonds are valued mostly for their hardness and thermal conductivity, making many of the gemological characteristics of diamonds, such as the 4 Cs, irrelevant for most applications. 80% of mined diamonds (equal to about annually) are unsuitable for use as gemstones and are used industrially. In addition to mined diamonds, synthetic diamonds found industrial applications almost immediately after their invention in the 1950s; another of synthetic diamond is produced annually for industrial use (in 2004; in 2014 it is , 90% of which is produced in China). Approximately 90% of diamond grinding grit is currently of synthetic origin.\n\nThe boundary between gem-quality diamonds and industrial diamonds is poorly defined and partly depends on market conditions (for example, if demand for polished diamonds is high, some lower-grade stones will be polished into low-quality or small gemstones rather than being sold for industrial use). Within the category of industrial diamonds, there is a sub-category comprising the lowest-quality, mostly opaque stones, which are known as bort.\n\nIndustrial use of diamonds has historically been associated with their hardness, which makes diamond the ideal material for cutting and grinding tools. As the hardest known naturally occurring material, diamond can be used to polish, cut, or wear away any material, including other diamonds. Common industrial applications of this property include diamond-tipped drill bits and saws, and the use of diamond powder as an abrasive. Less expensive industrial-grade diamonds, known as bort, with more flaws and poorer color than gems, are used for such purposes. Diamond is not suitable for machining ferrous alloys at high speeds, as carbon is soluble in iron at the high temperatures created by high-speed machining, leading to greatly increased wear on diamond tools compared to alternatives.\n\nSpecialized applications include use in laboratories as containment for high-pressure experiments (see diamond anvil cell), high-performance bearings, and limited use in specialized windows. With the continuing advances being made in the production of synthetic diamonds, future applications are becoming feasible. The high thermal conductivity of diamond makes it suitable as a heat sink for integrated circuits in electronics.\n\nApproximately of diamonds are mined annually, with a total value of nearly US$9 billion, and about are synthesized annually.\n\nRoughly 49% of diamonds originate from Central and Southern Africa, although significant sources of the mineral have been discovered in Canada, India, Russia, Brazil, and Australia. They are mined from kimberlite and lamproite volcanic pipes, which can bring diamond crystals, originating from deep within the Earth where high pressures and temperatures enable them to form, to the surface. The mining and distribution of natural diamonds are subjects of frequent controversy such as concerns over the sale of \"blood diamonds\" or \"conflict diamonds\" by African paramilitary groups. The diamond supply chain is controlled by a limited number of powerful businesses, and is also highly concentrated in a small number of locations around the world.\n\nOnly a very small fraction of the diamond ore consists of actual diamonds. The ore is crushed, during which care is required not to destroy larger diamonds, and then sorted by density. Today, diamonds are located in the diamond-rich density fraction with the help of X-ray fluorescence, after which the final sorting steps are done by hand. Before the use of X-rays became commonplace, the separation was done with grease belts; diamonds have a stronger tendency to stick to grease than the other minerals in the ore.\n\nHistorically, diamonds were found only in alluvial deposits in Guntur and Krishna district of the Krishna River delta in Southern India. India led the world in diamond production from the time of their discovery in approximately the 9th century BC to the mid-18th century AD, but the commercial potential of these sources had been exhausted by the late 18th century and at that time India was eclipsed by Brazil where the first non-Indian diamonds were found in 1725. Currently, one of the most prominent Indian mines is located at Panna.\n\nDiamond extraction from primary deposits (kimberlites and lamproites) started in the 1870s after the discovery of the Diamond Fields in South Africa.\nProduction has increased over time and now an accumulated total of have been mined since that date. Twenty percent of that amount has been mined in the last five years, and during the last 10 years, nine new mines have started production; four more are waiting to be opened soon. Most of these mines are located in Canada, Zimbabwe, Angola, and one in Russia.\n\nIn the U.S., diamonds have been found in Arkansas, Colorado, New Mexico, Wyoming, and Montana. In 2004, the discovery of a microscopic diamond in the U.S. led to the January 2008 bulk-sampling of kimberlite pipes in a remote part of Montana. The Crater of Diamonds State Park in Arkansas is open to the public, and is the only mine in the world where members of the public can dig for diamonds.\n\nToday, most commercially viable diamond deposits are in Russia (mostly in Sakha Republic, for example Mir pipe and Udachnaya pipe), Botswana, Australia (Northern and Western Australia) and the Democratic Republic of the Congo.\nIn 2005, Russia produced almost one-fifth of the global diamond output, according to the British Geological Survey. Australia boasts the richest diamantiferous pipe, with production from the Argyle diamond mine reaching peak levels of 42 metric tons per year in the 1990s.\nThere are also commercial deposits being actively mined in the Northwest Territories of Canada and Brazil.\nDiamond prospectors continue to search the globe for diamond-bearing kimberlite and lamproite pipes.\n\nIn some of the more politically unstable central African and west African countries, revolutionary groups have taken control of diamond mines, using proceeds from diamond sales to finance their operations. Diamonds sold through this process are known as \"conflict diamonds\" or \"blood diamonds\".\n\nIn response to public concerns that their diamond purchases were contributing to war and human rights abuses in central and western Africa, the United Nations, the diamond industry and diamond-trading nations introduced the Kimberley Process in 2002. The Kimberley Process aims to ensure that conflict diamonds do not become intermixed with the diamonds not controlled by such rebel groups. This is done by requiring diamond-producing countries to provide proof that the money they make from selling the diamonds is not used to fund criminal or revolutionary activities. Although the Kimberley Process has been moderately successful in limiting the number of conflict diamonds entering the market, some still find their way in. According to the International Diamond Manufacturers Association, conflict diamonds constitute 2–3% of all diamonds traded. Two major flaws still hinder the effectiveness of the Kimberley Process: (1) the relative ease of smuggling diamonds across African borders, and (2) the violent nature of diamond mining in nations that are not in a technical state of war and whose diamonds are therefore considered \"clean\".\n\nThe Canadian Government has set up a body known as the Canadian Diamond Code of Conduct to help authenticate Canadian diamonds. This is a stringent tracking system of diamonds and helps protect the \"conflict free\" label of Canadian diamonds.\n\nSynthetic diamonds are diamonds manufactured in a laboratory, as opposed to diamonds mined from the Earth. The gemological and industrial uses of diamond have created a large demand for rough stones. This demand has been satisfied in large part by synthetic diamonds, which have been manufactured by various processes for more than half a century. However, in recent years it has become possible to produce gem-quality synthetic diamonds of significant size. It is possible to make colorless synthetic gemstones that, on a molecular level, are identical to natural stones and so visually similar that only a gemologist with special equipment can tell the difference.\n\nThe majority of commercially available synthetic diamonds are yellow and are produced by so-called \"high-pressure high-temperature\" (HPHT) processes. The yellow color is caused by nitrogen impurities. Other colors may also be reproduced such as blue, green or pink, which are a result of the addition of boron or from irradiation after synthesis.\nAnother popular method of growing synthetic diamond is chemical vapor deposition (CVD). The growth occurs under low pressure (below atmospheric pressure). It involves feeding a mixture of gases (typically 1 to 99 methane to hydrogen) into a chamber and splitting them to chemically active radicals in a plasma ignited by microwaves, hot filament, arc discharge, welding torch or laser. This method is mostly used for coatings, but can also produce single crystals several millimeters in size (see picture).\n\nAs of 2010, nearly all 5,000 million carats (1,000 tonnes) of synthetic diamonds produced per year are for industrial use. Around 50% of the 133 million carats of natural diamonds mined per year end up in industrial use. Mining companies' expenses average $40 to $60 per carat for natural colorless diamonds, while synthetic manufacturers' expenses average $2,500 per carat for synthetic, gem-quality colorless diamonds. However, a purchaser is more likely to encounter a synthetic when looking for a fancy-colored diamond because nearly all synthetic diamonds are fancy-colored, while only 0.01% of natural diamonds are.\n\nA diamond simulant is a non-diamond material that is used to simulate the appearance of a diamond, and may be referred to as diamante. Cubic zirconia is the most common. The gemstone moissanite (silicon carbide) can be treated as a diamond simulant, though more costly to produce than cubic zirconia. Both are produced synthetically.\n\nDiamond enhancements are specific treatments performed on natural or synthetic diamonds (usually those already cut and polished into a gem), which are designed to better the gemological characteristics of the stone in one or more ways. These include laser drilling to remove inclusions, application of sealants to fill cracks, treatments to improve a white diamond's color grade, and treatments to give fancy color to a white diamond.\n\nCoatings are increasingly used to give a diamond simulant such as cubic zirconia a more \"diamond-like\" appearance. One such substance is diamond-like carbon—an amorphous carbonaceous material that has some physical properties similar to those of the diamond. Advertising suggests that such a coating would transfer some of these diamond-like properties to the coated stone, hence enhancing the diamond simulant. Techniques such as Raman spectroscopy should easily identify such a treatment.\n\nEarly diamond identification tests included a scratch test relying on the superior hardness of diamond. This test is destructive, as a diamond can scratch another diamond, and is rarely used nowadays. Instead, diamond identification relies on its superior thermal conductivity. Electronic thermal probes are widely used in the gemological centers to separate diamonds from their imitations. These probes consist of a pair of battery-powered thermistors mounted in a fine copper tip. One thermistor functions as a heating device while the other measures the temperature of the copper tip: if the stone being tested is a diamond, it will conduct the tip's thermal energy rapidly enough to produce a measurable temperature drop. This test takes about 2–3 seconds.\n\nWhereas the thermal probe can separate diamonds from most of their simulants, distinguishing between various types of diamond, for example synthetic or natural, irradiated or non-irradiated, etc., requires more advanced, optical techniques. Those techniques are also used for some diamonds simulants, such as silicon carbide, which pass the thermal conductivity test. Optical techniques can distinguish between natural diamonds and synthetic diamonds. They can also identify the vast majority of treated natural diamonds. \"Perfect\" crystals (at the atomic lattice level) have never been found, so both natural and synthetic diamonds always possess characteristic imperfections, arising from the circumstances of their crystal growth, that allow them to be distinguished from each other.\n\nLaboratories use techniques such as spectroscopy, microscopy and luminescence under shortwave ultraviolet light to determine a diamond's origin. They also use specially made instruments to aid them in the identification process. Two screening instruments are the \"DiamondSure\" and the \"DiamondView\", both produced by the DTC and marketed by the GIA.\n\nSeveral methods for identifying synthetic diamonds can be performed, depending on the method of production and the color of the diamond. CVD diamonds can usually be identified by an orange fluorescence. D-J colored diamonds can be screened through the Swiss Gemmological Institute's Diamond Spotter. Stones in the D-Z color range can be examined through the DiamondSure UV/visible spectrometer, a tool developed by De Beers. Similarly, natural diamonds usually have minor imperfections and flaws, such as inclusions of foreign material, that are not seen in synthetic diamonds.\n\nScreening devices based on diamond type detection can be used to make a distinction between diamonds that are certainly natural and diamonds that are potentially synthetic. Those potentially synthetic diamonds require more investigation in a specialized lab. Examples of commercial screening devices are D-Screen (WTOCD / HRD Antwerp) and Alpha Diamond Analyzer (Bruker / HRD Antwerp).\n\nOccasionally large thefts of diamonds take place. In February 2013 armed robbers carried out a raid at Brussels Airport and escaped with gems estimated to be worth $50m (£32m; 37m euros). The gang broke through a perimeter fence and raided the cargo hold of a Swiss-bound plane. The gang have since been arrested and large amounts of cash and diamonds recovered.\n\nThe identification of stolen diamonds presents a set of difficult problems. Rough diamonds will have a distinctive shape depending on whether their source is a mine or from an alluvial environment such as a beach or river—alluvial diamonds have smoother surfaces than those that have been mined. Determining the provenance of cut and polished stones is much more complex.\n\nThe Kimberley Process was developed to monitor the trade in rough diamonds and prevent their being used to fund violence. Before exporting, rough diamonds are certificated by the government of the country of origin. Some countries, such as Venezuela, are not party to the agreement. The Kimberley Process does not apply to local sales of rough diamonds within a country.\n\nDiamonds may be etched by laser with marks invisible to the naked eye. Lazare Kaplan, a US-based company, developed this method. However, whatever is marked on a diamond can readily be removed.\n\nThe name \"diamond\" is derived from the ancient Greek \"αδάμας\" \"(adámas\"), \"proper\", \"unalterable\", \"unbreakable\", \"untamed\", from ἀ- (a-), \"un-\" + \"δαμάω\" (\"damáō\"), \"I overpower\", \"I tame\". Diamonds are thought to have been first recognized and mined in India, where significant alluvial deposits of the stone could be found many centuries ago along the rivers Penner, Krishna and Godavari. Diamonds have been known in India for at least 3,000 years but most likely 6,000 years.\n\nDiamonds have been treasured as gemstones since their use as religious icons in ancient India. Their usage in engraving tools also dates to early human history. The popularity of diamonds has risen since the 19th century because of increased supply, improved cutting and polishing techniques, growth in the world economy, and innovative and successful advertising campaigns.\n\nIn 1772, the French scientist Antoine Lavoisier used a lens to concentrate the rays of the sun on a diamond in an atmosphere of oxygen, and showed that the only product of the combustion was carbon dioxide, proving that diamond is composed of carbon. Later in 1797, the English chemist Smithson Tennant repeated and expanded that experiment. By demonstrating that burning diamond and graphite releases the same amount of gas, he established the chemical equivalence of these substances.\n\n\n\n"}
{"id": "22693937", "url": "https://en.wikipedia.org/wiki?curid=22693937", "title": "EDELWEISS", "text": "EDELWEISS\n\nEDELWEISS ( Expérience pour DEtecter Les WIMPs En Site Souterrain) is a dark matter search experiment located at the Modane Underground Laboratory in France. The experiment uses cryogenic detectors, measuring both the phonon and ionization signals produced by particle interactions in germanium crystals. This technique allows nuclear recoils events to be distinguished from electron recoil events.\n\nThe EURECA project is a proposed future dark matter experiment, which will involve researchers from EDELWEISS and the CRESST dark matter search.\n\nDark matter is material which does not emit or absorb light. Measurements of the rotation curves of spiral galaxies suggest it makes up the majority of the mass of galaxies; and precision measurements of the cosmic microwave background radiation suggest it accounts for a significant fraction of the density of the Universe.\n\nA possible explanation of dark matter comes from particle physics. WIMP (Weakly Interacting Massive Particle) is a general term for hypothetical particles which interact only through the weak nuclear and gravitational force. This theory suggests our galaxy is surrounded by a dark halo of such particles. EDELWEISS is one of a number of dark matter search experiments aiming to directly detect WIMP dark matter, by detecting the elastic scattering of a WIMP off an atom within a particle detector. As the interaction rate is so low, this requires sensitive detectors, good background discrimination, and a deep underground site (to reduce the background from cosmic rays).\n\nEDELWEISS is located in the Modane underground laboratory, in the Fréjus road tunnel between France and Italy, below 1800m of rock. A 20 cm lead shield reduces the gamma background, and a polyethylene shield reduces the neutron flux. All materials close to the detectors are screened for radiopurity. A dilution refrigerator is used to cool the detectors, built in the opposite orientation to most instruments with the detectors at the top and the refrigeration mechanism below.\n\nEDELWEISS uses high purity germanium cryogenic bolometers cooled to 20 milliKelvin above absolute zero. The phonon and ionization signals produced by a particle interaction are measured. This allows background events to be rejected as nuclear recoils events (produced by WIMP or neutron interactions) produce much less ionization than electron recoil events (produced by alpha, beta and gamma radiation). The detectors are similar to those used by the CDMS experiment. Simultaneous detection of ionization and heat with semiconductors at low temperature was an original idea by Lawrence M. Krauss, Mark Srednicki and Frank Wilczek.\n\nA major limitation of early detectors was the problem of surface events. Due to incomplete charge collection, a particle interaction near the surface of the crystal gave no ionization signal, so electron recoils near the surface could be mistaken for nuclear recoils. To avoid this, the collaboration developed new detectors with interdigitised electrodes. Different voltages are applied to a series of electrodes so the direction of electric field is different near the surface of the crystal, allowing over 99.5% of surface events to be rejected.\n\nThe results from the first phase of the experiment (EDELWEISS I) were published in 2005, excluding WIMP dark matter with an interaction cross-section above (at ≈85 GeV). \n\nEDELWEISS-II ran 2009–10 with 4 kg of detectors (for a total effective exposure of 384 kg·d) limiting high mass \nand low mass WIMPs, \nand axions. A cross-section of is excluded at 90% C.L. for WIMP mass of 85 GeV. (Just above projected CDMS results in Fig A.)\n\nEDELWEISS-III, under construction, will have 40 detectors. EURECA design work will continue as EDELWEISS-III runs into 2015, operating after 2017.\n\nEDELWEISS is a collaboration of the following member institutions:\n\n\n\n\n"}
{"id": "25128729", "url": "https://en.wikipedia.org/wiki?curid=25128729", "title": "EFluor Nanocrystal", "text": "EFluor Nanocrystal\n\neFluor nanocrystals are a class of fluorophores made of semiconductor quantum dots. The nanocrystals can be provided as either primary amine, carboxylate, or non-functional groups on the surface, allowing conjugation to biomolecules of a researcher's choice. The nanocrystals can be conjugated to primary antibodies which are used for flow cytometry, immunohistochemistry, microarrays, in vivo imaging and microscopy.\n\nThe optical emission properties of eFluor Nanocrystals are primarily dictated by their size, as discussed in the next section. There are at least two aspects to consider when discussing the \"size\" of a quantum dot: the physical size of the semiconductor structure, and the size of the entire quantum dot moiety including the associated ligands and hydrophilic coating. The size of the semiconductor structure is tabulated below, and reflects the diameter of the spherical quantum dot without ligands. eFluor Nanocrystals are rendered water-dispersable with a patented poly-ethylene glycol (PEG) lipid layer that functions as both a protective hydrophilic coating around the quantum dot, as well as reducing non-specific binding By dynamic light scattering measurements, the hydrodynamic radius of all eFluor Nanocrystals ranges from 10–13nm.\n\nQuantum dots are unique fluorophores relative to organic dyes, like fluorescein or rhodamine because they are composed of semiconductor metals, instead of a π-conjugated carbon-bonding framework. With organic dyes, the length of the π-conjugated framework (quantum confinement), as well as side-groups (electron donating/withdrawing or halogens) tend to dictate the absorption and emission spectra of the molecule. Semiconductor quantum dots also work on the concept of quantum confinement, (often referred to as \"Particle in a Box\" theory) where an exciton is formed inside the crystal lattice by an incident photon of higher energy. The electron and hole of the exciton have an interaction energy that is tuned by changing the physical size of the quantum dot. The absorption and emission colors are tuned such that smaller quantum dots confine the exciton into a tighter physical space and increase the energy. Alternatively, a larger quantum dot confines the exciton into a larger physical space, lowering the interaction energy of the electron and hole, and decreasing the energy of the system. As shown in the table above, the diameter of the CdSe quantum dots is related to the emission energy such that the smaller quantum dots emit photons toward the blue wavelength range (higher energy) and the larger quantum dots emit photons toward the red wavelength range (lower energy.)\n\nTo the right are representative absorption (blue) and emission (red) spectra for the eFluor-605 nanocrystal. The absorption spectrum of nanocrystals displays a number of peaks overlaid on background that rises exponentially toward the ultraviolet, where the lowest energy absorption peak arises from the 1S-1S transition, and has been correlated to the physical size of the quantum dot. Generally referred to as the \"1st exciton,\" and is the primary absorption characteristic used to determine both size and concentration for most quantum dots.\n\nThe photoluminescence spectra of quantum dots are also unique relative to organic dyes in that they are typically Gaussian-shaped curves with no red-tailing to the spectrum. The width of the photoluminescence peak represents the heterogeneity in size dispersion of the quantum dots, where a large size dispersion will lead to broad emission peaks, and tight size-dispersion will lead to narrow emission peaks, often quantified by the full width at half maximum (FWHM) value. eFluor Nanocrystals are specified at ≤30nm FWHM for the CdSe nanocrystals, and ≤70nm FWHM for the InGaP eFluor 700 nanocrystals.\n"}
{"id": "1303480", "url": "https://en.wikipedia.org/wiki?curid=1303480", "title": "Electrostriction", "text": "Electrostriction\n\nElectrostriction (cf. magnetostriction) is a property of all electrical non-conductors, or dielectrics, that causes them to change their shape under the application of an electric field.\n\nElectrostriction is a property of all dielectric materials, and is caused by a slight displacement of ions in the crystal lattice upon being exposed to an external electric field. Positive ions will be displaced in the direction of the field, while negative ions will be displaced in the opposite direction. This displacement will accumulate throughout the bulk material and result in an overall strain (elongation) in the direction of the field. The thickness will be reduced in the orthogonal directions characterized by Poisson's ratio. All insulating materials consisting of more than one type of atom will be ionic to some extent due to the difference of electronegativity of the atoms, and therefore exhibit electrostriction.\n\nThe resulting strain (ratio of deformation to the original dimension) is proportional to the square of the polarization. Reversal of the electric field does not reverse the direction of the deformation.\n\nMore formally, the electrostriction coefficient is a fourth rank tensor (formula_1), relating the second-order strain tensor (formula_2) and the first-order electric polarization density (formula_3).\n\nformula_4\n\nThe related piezoelectric effect occurs only in a particular class of dielectrics. Electrostriction applies to all crystal symmetries, while the piezoelectric effect only applies to the 20 piezoelectric point groups. Electrostriction is a quadratic effect, unlike piezoelectricity, which is a linear effect.\n\nAlthough all dielectrics exhibit some electrostriction, certain engineered ceramics, known as relaxor ferroelectrics, have extraordinarily high electrostrictive constants. The most commonly used are\n\nElectrostriction can produce a strain of 0.1% at a field strength of 2 million volts per meter (2 MV/m) for the material called PMN-15 (TRS website listed in the references below). The effect appears to be quadratic at low field strengths (up to 0.3 MV/m) and roughly linear after that, up to a maximum field strength of 4 MV/m . Therefore, devices made of such materials are normally operated around a bias voltage in order to behave nearly linearly. This will probably cause deformations to lead to a change of electric charge, but this is unconfirmed.\n\n\n\n"}
{"id": "49599350", "url": "https://en.wikipedia.org/wiki?curid=49599350", "title": "Epoxydocosapentaenoic acid", "text": "Epoxydocosapentaenoic acid\n\nEpoxide docosapentaenoic acids (epoxydocosapentaenoic acids, EDPs, or EpDPEs) are metabolites of the 22-carbon straight-chain omega-3 fatty acid, docosahexaenoic acid (DHA). Cell types that express certain cytochrome P450 (CYP) epoxygenases metabolize polyunsaturated fatty acid's (PUFAs) by converting one of their double bonds to an epoxide. In the best known of these metabolic pathways, cellular CYP epoxygenases metabolize the 20-carbon straight-chain omega-6 fatty acid, arachidonic acid, to epoxyeicosatrienoic acids (EETs); another CYP epoxygenase pathway metabolizes the 20-carbon omega-3 fatty acid, eicosapentaenoic acid (EPA), to epoxyeicosatetraenoic acids (EEQs). CYP epoxygenases similarly convert various other PUFAs to epoxides (see epoxygenase) These epoxide metabolites have a variety of activities. However, essentially all of them are rapidly converted to their corresponding, but in general far less active, Vicinal (chemistry) dihydroxy fatty acids by ubiquitous cellular Soluble epoxide hydrolase (sEH; also termed Epoxide hydrolase 2). Consequently, these epoxides, including EDPs, operate as short-lived signaling agents that regulate the function of their parent or nearby cells. The particular feature of EDPs (and EEQs) distinguishing them from EETs is that they derive from omega-3 fatty acids and are suggested to be responsible for some of the beneficial effects attributed to omega-3 fatty acids and omega-3-rich foods such as fish oil.\n\nEDPs are epoxide eicosapentaenoic acid metabolites of DHA. DHA has 6 cis (see Cis–trans isomerism) Double bonds each one of which is located between carbons 4-5, 7-8, 10-11, 13-14, 16-17, or 19-20. Cytochrome P450 epoxygenases attack any one of these double bounds to form a respective docosapentaenoic acid (DPA) epoxide regioisomer (see Structural isomer, section on position isomerism (regioisomerism)). A given epoxygenase may therefore convert DHA to 4,5-EDP (i.e. 4,5-epoxy-7\"Z\",10\"Z\",13\"Z\",16\"Z\",19\"Z\"-DPA), 7,8-EDP (i.e. 7,8-epoxy-4\"Z\",10\"Z\",13\"Z\",16\"Z\",19\"Z\"-DPA), 10,11-EDP (i.e. 10,11-epoxy-4\"Z\",7\"Z\",13\"Z\",16\"Z\",19\"Z\"-DPA), 13,14-EDP (i.e. 13,14-epoxy-4\"Z\",7\"Z\",10\"Z\",16\"Z\",19\"Z\"-DPA), 16,17-EDP (i.e. 16,17-epoxy-4\"Z\",7\"Z\",10\"Z\",13\"Z\",19\"Z\"-DPA, or 19,20-EDP (i.e. 19,20-epoxy-4\"Z\", 7\"Z\",10\"Z\",13\"Z\",16\"Z\"-DPA. The epoxygenase enzymes generally form both \"R\"/\"S\" enantiomers at each former double bound position; for example, cytochrome P450 epoxidases attack DHA at the 16,17-double bond position to form two epoxide enantiomers, 16\"R\",17\"S\"-EDP and 16\"S\",17\"S\"-EDP. The 4,5-EDP metabolite is unstable and generally not detected among the EDP formed by cells.\n\nEnzymes of the cytochrome P450 (CYP) superfamily that are classified as epoxygenases based on their ability to metabolize PUFA, particularly arachidonic acid, to epoxides include: CYP1A, CYP2B, CYP2C, CYP2E, CYP2J, and within the CYP3A subfamily, CYP3A4. In humans, CYP2C8, CYP2C9, CYP2C19, CYP2J2, and possibly CYP2S1 isoforms appear to be the principal epoxygenases responsible for metabolizing arachidonic acid to EETs (see Epoxyeicosatrienoic acid#Production). In general, these same CYP epoxygenases also metabolize DHA to EDP (as well as EPA to EEQ; CYP2S1 has not yet been tested for DHA-metabolizing ability), doing so at rates that are often greater than their rates in metabolizing arachidonic acid to EETs; that is, DHA (and EPA) appear to be preferred over arachidonic acid as substrates for many of the CYP epoxygenases. CYP1A1, CYP1A2, CYP2C18, CYP2E1, CYP4A11, CYP4F8, and CYP4F12 also metabolize DHA to EDPs. CYP2C8, CYP2C18, CYP2E1, CYP2J2, VYP4A11, CYP4F8, and CYP4F12 preferentially attack the terminal omega-3 double bond that distinguishes DHA from omega-6 fatty acids and therefore metabolize DHA principally to 19,20-EDP isomers while CYP2C19 metabolizes DHA to 7,8-EDP, 10,11-EDP, and 19,20-EDP isomers CYP2J2 metabolizes DHA to EPAs, principally 19,20-EPA, at twice the rate that it metabolizes arachidonic acid to EETs. In addition to the cited CYP's, CYP4A11, CYP4F8, CYP4F12, CYP1A1, CYP1A2, and CYP2E1, which are classified as CYP monooxygenase rather than CYP epoxygeanses because they metablize arachidonic acid to monohydroxy eicosatetraenoic acids (see 20-Hydroxyeicosatetraenoic acid), i.e. 19-hydroxyeicosatetraenoic acid and/or 20-hydroxyeicosatetranoic acid, take on epoxygease activity in converting DHA primarily to 19,20-EDP isomers (see epoxyeicosatrienoic acid). The CYP450 epoxygenases capable of metabolizing DHA to EDPs are wildly distributed in organs and tissues such as the liver, kidney, heart, lung, pancreas, intestine, blood vessels, blood leukocytes, and brain. These tissues are known to metabolize arachidonic acid to EETs; it has been shown or is presumed that they also metabolize DHA to EPD's.\n\nThe EDPs are commonly made by the stimulation of specific cell types by the same mechanisms which produce EETs (see Epoxyeicosatrienoic acid). That is, cell stimulation causes DHA to be released from the \"sn-2\" position of their membrane-bound cellular phospholipid pools through the action of a Phospholipase A2-type enzyme and the subsequent attack of the released DHA by CYP450 epoxidases. It is notable that the consumption of omega-3 fatty acid-rich diets dramatically raises the serum and tissue levels of EDPs and EEQs in animals as well as humans. Indeed, this rise in EDP (and EEQ) levels in humans is by far the most prominent change in the profile of PUFA metabolites caused by dietary omega-3 fatty acids and, it is suggested, may be responsible for at least some of the beneficial effects ascribed to dietary omega-3 fatty acids.\n\nSimilar to EETs (see Epoxyeicosatrienoic acid), EDPs are rapidly metabolized in cells by a cytosolic soluble epoxide hydrolase (sEH, also termed Epoxide hydrolase 2 [EC 3.2.2.10.]) to form their corresponding Vicinal (chemistry) diol dihydroxyeicosapentaenoic acids. Thus, sEH converts 19,20-EDP to 19,10-dihdroxydocosapentaenoic acid (DPA), 16,17-EDP to 16,17-dihydroxy-DPA, 13,14-EDP to 13,14-dihydroxy-DPA, 10,11-EDP to 10,11-dihydroxy-DPA, and 7,8-EDP to 7,8-dihydroxy-EDP; 4,5-EDP is unstable and therefore generally not detected in cells. The dihydroxy-EDP products, like their epoxy precursors, are enantiomer mixtures; for instance, sEH converts 16,17-EDP to a mixture of 16(\"S\"),17(\"R\")-dihydroxy-DPA and 16(\"R\"),1y(\"S\")-dihydroxy-DPA. These dihydroxy-DPAs typically are far less active than their epoxide precursors. The sEH pathway acts rapidly and is by far the predominant pathway of EDP inactivation; its operation causes EDPs to function as short-lived mediators whose actions are limited to their parent and nearby cells, i.e. they are autocrine and paracrine signaling agents, respectively.\n\nIn addition to the sEH pathway, EDPs, similar to the EETs, may be acylated into phospholipids in an Acylation-like reaction; this pathway may serve to limit the action of EETs or store them for future release. Finally, again similar to the EETs, EDPs are subject to inactivation by being further metabolized b Beta oxidation.\n\nEDPs have not be studied nearly as well as the EETs. This is particularly the case for animal studies into their potential clinical significance. In comparison to a selection of the many activities attributed to the EETs (see Epoxyeicosatrienoic acid), animal studies reported to date find that certain EDPs (16,17-EDP and 19,20-EDP have been most often examined) are: 1) more potent than EETs in decreasing hypertension and pain perception; 2) more potent than or at least equal in potency to the EETs in suppressing inflammation; and 3) act oppositely from the EETs in that EDPs inhibit angiogenesis, endothelial cell migration, endothelial cell proliferation, and the growth and metastasis of human breast and prostate cancer cell lines whereas EETs have stimulatory effects in each of these systems. As indicated in the Metabolism section, consumption of omega-3 fatty acid-rich diets dramatically raises the serum and tissue levels of EDPs and EEQs in animals as well as humans and in humans is by far the most prominent change in the profile of PUFA metabolites caused by dietary omega-3 fatty acids. Hence, the metabolism of DHA to EDPs (and EPA to EEQs) may be responsible for at least some of the beneficial effects ascribed to dietary omega-3 fatty acids.\n"}
{"id": "34250567", "url": "https://en.wikipedia.org/wiki?curid=34250567", "title": "Erin Energy Corporation", "text": "Erin Energy Corporation\n\nErin Energy Corporation () is a United States based company involved in exploration, development and production of oil and gas. It was formerly known as CAMAC Energy. Its principal assets are in Africa comprising nine licenses across 4 countries, with current production and exploratory projects offshore of Nigeria, as well as exploration licenses offshore of Ghana, Kenya, and Gambia, and onshore in Kenya. in addition, through its Pacific Asia Petroleum subsidiaries, it has operations in China.\n\nThe Chairman and CEO is African-American businessman, Dr Kase Lukman Lawal. John Hofmeister, former president of Shell Oil Company, is on the board and will become the Chairman in 2016.\n\nIt was founded in 2005 as CAMAC Energy and has offices in Houston, Texas, Beijing, China, and Lagos, Nigeria. \n\nErin Energy is listed on the NYSE Amex Equities. \n\nOn 4 August 2014 the company announced it had discovered 4 new oil and gas reservoirs in its Oyo-8 development well located offshore Nigeria. Erin had already drilled Oyo-7 in October 2013. The company said in its 4 August 2014 statement it was one step closer to bringing the two wells into production.\n\nIn July 2018, it was announced that Erin Energy went into liquidation.\n\n"}
{"id": "544333", "url": "https://en.wikipedia.org/wiki?curid=544333", "title": "Eugene Mallove", "text": "Eugene Mallove\n\nEugene Franklin Mallove (June 9, 1947 – May 14, 2004) was an American scientist, science writer, editor, and publisher of \"Infinite Energy\" magazine, and founder of the nonprofit organization New Energy Foundation. He was a proponent of cold fusion, and a supporter of its research and related exploratory alternative energy topics, several of which are sometimes characterised as \"fringe science\".\n\nMallove authored \"Fire from Ice\", a book detailing the 1989 report of tabletop cold fusion from Stanley Pons and Martin Fleischmann at the University of Utah. Among other things, the book claims the team did produce \"greater-than-unity\" output energy in an experiment successfully replicated on several occasions, but that the results were suppressed through an organized campaign of ridicule from mainstream physicists, including those studying controlled thermonuclear fusion, trying to protect their research and funding.\n\nMallove was murdered in 2004 while cleaning out his former childhood home, which had been rented out. Three people have been arrested and charged in connection with the killing; two were convicted of first-degree manslaughter and murder, in 2012 and 2014; the third pleaded guilty to obstruction of justice in 2015.\n\nEugene Franklin Mallove was born on June 9, 1947 to Gladys (nee' Alexander) and Mitchell Mallove. He grew up in Norwich, Connecticut and graduated from the Norwich Free Academy in 1965. From an early age, he showed great interest in science and especially astronomy. While in Boston, he met Joanne Smith, who was a student at Boston University. On September 9, 1970, Gene and Joanne married. They had two children, Kimberlyn, born in 1974, and Ethan, born in 1979.\n\nEugene Mallove held a BS (1969) and MS degree (1970) in aeronautical and astronautical engineering from MIT and a ScD degree (1975) in environmental health sciences from Harvard University. He had worked for technology engineering firms such as Hughes Research Laboratories, the Analytic Science Corporation, and MIT's Lincoln Laboratory, and he consulted in research and development of new energies.\n\nIn 1981, he and Gregory Matloff wrote a classic paper about using solar sails to reach Alpha Centauri, the nearest star to our sun. They calculated that the trip would take several hundred years and that the ship would have to withstand accelerations of 60 g. They wrote several papers on that and other proposed methods of space travel, such as laser propulsion, the Bussard ramjet, and exotic fuels that could give very high power.\nMallove taught science journalism at MIT and Boston University and was chief science writer at MIT's news office, a position he left as part of a dispute with the school over cold fusion.\n\nHe was a science writer and broadcaster with the Voice of America radio service and author of three science books: \"The Quickening Universe: Cosmic Evolution and Human Destiny\" (1987, St. Martin’s Press), \"The Starflight Handbook: A Pioneer’s Guide to Interstellar Travel\" (1989, John Wiley & Sons, with co-author Gregory Matloff), and \"Fire from Ice: Searching for the Truth Behind the Cold Fusion Furor\" (1991, John Wiley & Sons). He also published articles for numerous magazines and newspapers.\n\nMallove was a member of the Aurora Biophysics Research Institute (ABRI), one of the founders of the International Society of the Friends of Aetherometry, a member of its Organizing Committee, a co-inventor of the HYBORAC technology and one of the main evaluators of ABRI technologies.\n\nHis alternative energy research included studying the reproduction of Wilhelm Reich's Orgone Motor by Dr. Paulo Correa and Alexandra Correa, as well as the evolution of heat in the Reich-Einstein experiment. He was among the scientists and engineers who claimed to have confirmed the output of excess electric energy from tuned pulsed plasmas in vacuum arc discharges.\n\nMallove's combative stance against what he saw as the hypocrisy of mainstream science gave him a high-profile. Among other things, he was a frequent guest on the American radio program \"Coast to Coast AM\".\n\nIn 1992, Mallove was a consultant on the ERR (Electromagnetic Radiation Receiver) project at the Noah’s Ark Research Facility in the Philippines. He is also credited as a \"cold fusion technical consultant\", for providing advice to the producers of the movie \"The Saint\" from 1997, with a plot revolving around cold fusion formulas.\n\nEugene Mallove was a notable proponent and supporter of research into cold fusion. He authored the book \"Fire from Ice\", which details the 1989 report of table-top cold fusion from Stanley Pons and Martin Fleischmann at the University of Utah. The book claims the team did produce \"greater-than-unity\" output energy in an experiment that was successfully replicated on several occasions. Mallove claims that the results were suppressed through an organized campaign of ridicule from mainstream physicists.\n\nEugene Mallove was killed on May 14, 2004 in Norwich, Connecticut, while cleaning a recently vacated rental property owned by his parents, the home he grew up in. The nature of Mallove's work led to some conspiracy theories regarding the homicide, but police suspected robbery as the motive.\n\nIn 2005, two local men were arrested in connection with the killing. The case proceeded slowly and the charges against the two men were finally dismissed on November 6, 2008.\n\nOn February 11, 2009, the State of Connecticut announced a $50,000 reward leading to the arrest and conviction of the person or persons responsible for the murder. On April 2, 2010, the police made two arrests in connection with the murder and said that more arrests were expected.\nOn May 22, 2011, a state prosecutor said that they were charging a third person in connection with the killing. Court testimony indicated that Mallove may have been killed by an evicted tenant who was angry about belongings being disposed of during the clearout.\n\nOn April 20, 2012, the Norwich Bulletin stated that: \"An ongoing murder trial came to an abrupt halt Friday when Chad Schaffer, of Norwich, decided to accept an offer of 16 years in prison, pleading guilty to the lesser charge of first-degree manslaughter in the 2004 beating death of Eugene Mallove.\" Mallove had just evicted Schaffer's parents, and he was cleaning the evicted house when Schaffer arrived and confronted him.\n\nA third individual was arraigned on November 21, 2013.\n\nMozelle Brown was convicted of Mallove's murder in October 2014 and on January 6, 2015 was sentenced to 58 years in prison. Schaffer's girlfriend, Candace Foster, testified against Brown and Schaffer, and pleaded guilty to a charge of hindering prosecution and tampering with evidence.\n\n\n\n"}
{"id": "29686999", "url": "https://en.wikipedia.org/wiki?curid=29686999", "title": "Feliciano dos Santos", "text": "Feliciano dos Santos\n\nFeliciano dos Santos is a Mozambican musician and environmentalist, hailing from the Niassa Province. He was awarded the Goldman Environmental Prize in 2008, for his use of music to promote the need of improving water and sanitation infrastructure in the Niassa region.\n"}
{"id": "1653453", "url": "https://en.wikipedia.org/wiki?curid=1653453", "title": "Fermion doubling", "text": "Fermion doubling\n\nThe fermion doubling problem is a problem that is encountered when naively trying to put fermionic fields on a lattice. It consists in the appearance of spurious states, such that one ends up having 2 fermionic particles (with \"d\" the number of discretized dimensions) for each original fermion. In order to solve this problem, several strategies are in use, such as Wilson fermions and staggered fermions.\n\nThe action of a free Dirac fermion in \"d\" dimensions, of mass \"m\", and in the continuum (i.e. without discretization) is commonly given as\nHere, the Feynman slash notation was used to write\nwhere γ are the gamma matrices. When this action is discretized on a cubic lattice, the fermion field ψ(\"x\") is replaced with a discretized version ψ, where \"x\" now denotes the lattice site. The derivative is replaced by the finite difference. The resulting action is now:\nwhere \"a\" is the lattice spacing and formula_4 is the vector of length \"a\" in the μ direction. When one computes the inverse fermion propagator in momentum space, one readily finds:\nDue to the finite lattice spacing the momenta \"p\" have to be inside the (first) Brillouin zone, which is typically taken to be the interval [−/\"a\",+/\"a\"].\n\nWhen simply taking the limit \"a\" → 0 in the above inverse propagator, one recovers the correct continuum result. However, when instead expanding this expression around a value of \"p\" where one or more of the components are at the corners of the Brillouin zone (i.e. equal to /\"a\"), one finds the same continuum form again, although the sign in front of the gamma matrix can change. This means that, when one of the components of the momentum is near /\"a\", the discretized fermion field will again behave like a continuum fermion. This can happen with all \"d\" components of the momentum, leading to —with the original fermion with momentum near the origin included— 2 different \"tastes\" (in analogy to flavor).\n\nNielsen and Ninomiya proved a theorem stating that a local, real, free fermion lattice action, having chiral and translational invariance, necessarily has fermion doubling. The only way to get rid of the doublers is by violating one of the presuppositions of the theorem —for example:\n\n\nNotes\nReferences\n"}
{"id": "20307059", "url": "https://en.wikipedia.org/wiki?curid=20307059", "title": "Fuel Manager", "text": "Fuel Manager\n\nFuel Manager is a marine fuel management system used to monitor and report fuel usage, with the aim of reducing vessel operator's fuel cost and the harmful emissions. Fuel Manager is developed by Marorka in Iceland. Fuel Manager was released in 2007.\n\nFuel Manager monitors the propulsion system on-board different types of vessels. It puts operating and environmental parameters in an energy management context. Fuel Manager uses a sophisticated measurement approach to deliver reliable fuel performance information to vessel operators. \n"}
{"id": "28652193", "url": "https://en.wikipedia.org/wiki?curid=28652193", "title": "Green leaf volatiles", "text": "Green leaf volatiles\n\nGreen leaf volatiles (commonly abbreviated as GLV) are volatile organic compounds that are released when plants suffer tissue damage. Specifically, they include aldehydes, esters, and alcohols of 6-carbon compounds released after wounding. These compounds are very quickly produced and emitted, and are used by nearly every green plant. Plants constantly release GLVs, but un-stressed plants release them in much smaller amounts. Some of these chemicals act as signaling compounds between either plants of the same species, of other species, or even vastly different lifeforms like insects. Some, although not necessarily all, of these chemicals act essentially as plant pheromones. GLVs also have antimicrobial properties to prevent infection at the site of injury.\n\nGLVs are used in both plant-plant and plant-insect interactions. They usually serve as a warning signal of oncoming causes of tissue damage. When a plant is attacked, it emits GLVs into the environment through the air. Undamaged neighboring plants perceive these GLV signals and activate the expression of genes related to the plants defense mechanisms. This allows the plant emitting the GLVs and the neighboring plants to enter a primed state. In this primed state plants activate their defenses systems more quickly and in a stronger concentration. The amount of GLVs that a wounded plant emits is directly related to the severity of the injury, so the concentration of GLVs in the atmosphere and the frequency of exposure both play a role in neighboring plants successfully entering a primed state.\n\nIn plant-insect interactions, GLVs are used as a form of defense. They alert predators to the locations of herbivores that are preying on the plant and causing tissue damage. For example, a study done by Northwestern University found that parasitic wasps are more attracted to plants that are emitting GLVs due to wounding from herbivores than to plants that are emitting GLVs due to mechanical damage. In order to determine if plants are capable of recognizing and distinguishing between GLVs, a study was done at University of California Davis where researchers exposed plants to GLVs emitted by a mechanically damaged tomato plant, and to GLVs emitted by a tomato plant that had been damaged by herbivores. Researchers observed a difference in the plants reaction, showing an increase in the proteins related to defense mechanisms for the plant exposed to the herbivore GLVs. This supports the theory that plants are able to distinguish between different GLVs, and react differently depending on which signal they receive.\n\nOther benefits of Green Leaf Volatiles are that at the site of damage, GLVs are released in high concentrations and act as antimicrobial agents to make the plant more resistant to bacterial or fungal infections. To study the anti-fungal properties of GLVs, researchers at the University of Arizona influenced how plants expressed HPL, the main enzyme of GLV synthesis. Scientists compared the rates of fungal spore growth in HPL over-expressing and HPL silencing mutants to the wild type plants. Results from the study showed lower rates of fungal growth and higher GLV emissions on the HPL over-expressing mutants, while the HPL silencing mutants showed higher rates of fungal growth and lower GLV emissions, which supports the hypothesis that GLVs have antimicrobial properties.\n\nThe antimicrobial properties of GLVs have also been part of an evolutionary arms race that raise questions for scientists. During an infection, plants emit GLVs to act as microbial agents, but bacteria and viruses have adapted to use these GLVs to their own benefit. The most common example of this is found in the red raspberry. When the red raspberry plant is infected, the virus influences it to produce more GLVs, which attract the red raspberry aphid. These GLVs cause more aphids to come and to feed on the plant for longer, giving the virus better chances of being ingested and spread more widely. Researchers are now trying to determine whether under infectious conditions plants emit GLVs for their benefit, or if bacteria and viruses induce the release of these compounds for their own benefit. Studies in this area have been inconclusive and contradictory.\n"}
{"id": "38486945", "url": "https://en.wikipedia.org/wiki?curid=38486945", "title": "Juliet Davenport", "text": "Juliet Davenport\n\nJuliet Davenport OBE (born 1968) is a British businesswoman. She founded and is the chief executive of Good Energy, a leading renewable energy company in the United Kingdom.\n\nDavenport was born in Haslemere, Surrey, in 1968.\n\nShe read physics as an undergraduate at Merton College, Oxford before taking a master's degree in economics and environmental economics at Birkbeck, University of London. She also worked for a year at the European Commission on European energy policy and at the European Parliament on carbon taxation.\n\nDavenport began working with Energy for Sustainable Development, an environmental consultancy. While there, she ran technology models and analysed policies on renewable energy from countries around Europe.\n\nIn 1999 Davenport set up Unit[e], a subsidiary of the Monkton Group, of which she later became CEO. In 2003, Unit[e] was renamed Good Energy. The company has won several awards, including being twice named a Sunday Times Best Green Company, Wiltshire Wildlife Trust’s Outstanding Contribution to the Environment 2009 and The Observer’s Ethical Award for best online retail initiative. In 2012, Davenport was named as PLUS CEO of the year.\n\nDavenport was appointed an Officer of the Order of the British Empire in the New Year Honours List 2013.\n\nDavenport is married to Mark Shorrock and has a daughter and a stepdaughter.\n"}
{"id": "1862478", "url": "https://en.wikipedia.org/wiki?curid=1862478", "title": "Kawęczyn Heat Plant", "text": "Kawęczyn Heat Plant\n\nThe Kawęczyn Heat Plant is a coal-fired heat plant at osiedle Kawęczyn in Rembertów district of Warsaw, Poland. It was operated by Vattenfall but their Polish operations were taken over by Polish energy company PGNiG in 2012.\n\nThe heat plant has an installed thermal capacity of 512 MW. It has one high flue gas stack, which is one of Poland's tallest free standing structures.\n\n\n"}
{"id": "25244412", "url": "https://en.wikipedia.org/wiki?curid=25244412", "title": "List of U.S. states by electricity production from renewable sources", "text": "List of U.S. states by electricity production from renewable sources\n\nThe information used to calculate values is from the Electric Power Monthly, February 2015 and 2014 published by the U.S. Energy Information Administration. Renewable generation does not include amounts for 'rooftop solar'. Only utility scale generation from solar sources is included.\n\nSeveral states, including Texas have substantially increased non hydro generation due to wind and solar additions.\n\nVermont jumps to the top as a result of the closure of Vermont Yankee.\n\n"}
{"id": "191490", "url": "https://en.wikipedia.org/wiki?curid=191490", "title": "Machine tool", "text": "Machine tool\n\nA machine tool is a machine for shaping or machining metal or other rigid materials, usually by cutting, boring, grinding, shearing, or other forms of deformation. Machine tools employ some sort of tool that does the cutting or shaping. All machine tools have some means of constraining the workpiece and provide a guided movement of the parts of the machine. Thus the relative movement between the workpiece and the cutting tool (which is called the toolpath) is controlled or constrained by the machine to at least some extent, rather than being entirely \"offhand\" or \"freehand\".\n\nThe precise definition of the term \"machine tool\" varies among users, as discussed below. While all machine tools are \"machines that help people to make things\", not all factory machines are machine tools.\n\nToday machine tools are typically powered other than by human muscle (e.g., electrically, hydraulically, or via line shaft), used to make manufactured parts (components) in various ways that include cutting or certain other kinds of deformation.\n\nWith their inherent precision, machine tools enabled the economical production of interchangeable parts.\n\nMany historians of technology consider that true machine tools were born when the toolpath first became guided by the machine itself in some way, at least to some extent, so that direct, freehand human guidance of the toolpath (with hands, feet, or mouth) was no longer the only guidance used in the cutting or forming process. In this view of the definition, the term, arising at a time when all tools up till then had been hand tools, simply provided a label for \"tools that were machines instead of hand tools\". Early lathes, those prior to the late medieval period, and modern woodworking lathes and potter's wheels may or may not fall under this definition, depending on how one views the headstock spindle itself; but the earliest historical records of a lathe with direct mechanical control \"of the cutting tool's path\" are of a screw-cutting lathe dating to about 1483. This lathe \"produced screw threads out of wood and employed a true compound slide rest\".\n\nThe mechanical toolpath guidance grew out of various root concepts:\n\n\nAbstractly programmable toolpath guidance began with mechanical solutions, such as in musical box cams and Jacquard looms. The convergence of programmable mechanical control with machine tool toolpath control was delayed many decades, in part because the programmable control methods of musical boxes and looms lacked the rigidity for machine tool toolpaths. Later, electromechanical solutions (such as servos) and soon electronic solutions (including computers) were added, leading to numerical control and computer numerical control.\n\nWhen considering the difference between freehand toolpaths and machine-constrained toolpaths, the concepts of accuracy and precision, efficiency, and productivity become important in understanding \"why\" the machine-constrained option adds value. \n\nMatter-Additive, Matter-Preserving, and Matter-Subtractive \"Manufacturing\" can proceed in 16 ways: The work may be held in a hand or a clamp; the tool may be held in a hand (the other hand) or a clamp; the power can come from the hand(s) holding the tool and/or the work, or from some external source, including a foot treadle by the same worker, or a motor without limitation; and the control can come from the hand(s) holding the tool and/or the work, or from some other source, including computer numerical control. With two choices for each of four parameters, the types are enumerated to sixteen types of Manufacturing, where Matter-Additive might mean painting on canvas as readily as it might mean 3D printing under computer control, Matter-Preserving might mean forging at the coal fire as readily as stamping license plates, and Matter-Subtracting might mean casually whittling a pencil point as readily as it might mean precision grinding the final form of a laser deposited turbine blade.\n\nHumans are generally quite talented in their freehand movements; the drawings, paintings, and sculptures of artists such as Michelangelo or Leonardo da Vinci, and of countless other talented people, show that human freehand toolpath has great potential. The value that machine tools added to these human talents is in the areas of rigidity (constraining the toolpath despite thousands of newtons (pounds) of force fighting against the constraint), accuracy and precision, efficiency, and productivity. With a machine tool, toolpaths that no human muscle could constrain can be constrained; and toolpaths that are technically possible with freehand methods, but would require tremendous time and skill to execute, can instead be executed quickly and easily, even by people with little freehand talent (because the machine takes care of it). The latter aspect of machine tools is often referred to by historians of technology as \"building the skill into the tool\", in contrast to the toolpath-constraining skill being in the \"person\" who wields the tool. As an example, it is \"physically possible\" to make interchangeable screws, bolts, and nuts entirely with freehand toolpaths. But it is \"economically practical\" to make them only with machine tools.\n\nIn the 1930s, the U.S. National Bureau of Economic Research (NBER) referenced the definition of a machine tool as \"any machine operating by other than hand power which employs a tool to work on metal\".\n\nThe narrowest colloquial sense of the term reserves it only for machines that perform metal cutting—in other words, the many kinds of [conventional] machining and grinding. These processes are a type of deformation that produces swarf. However, economists use a slightly broader sense that also includes metal deformation of other types that squeeze the metal into shape without cutting off swarf, such as rolling, stamping with dies, shearing, swaging, riveting, and others. Thus presses are usually included in the economic definition of machine tools. For example, this is the breadth of definition used by Max Holland in his history of Burgmaster and Houdaille, which is also a history of the machine tool industry in general from the 1940s through the 1980s; he was reflecting the sense of the term used by Houdaille itself and other firms in the industry. Many reports on machine tool export and import and similar economic topics use this broader definition.\n\nThe colloquial sense implying [conventional] metal cutting is also growing obsolete because of changing technology over the decades. The many more recently developed processes labeled \"machining\", such as electrical discharge machining, electrochemical machining, electron beam machining, photochemical machining, and ultrasonic machining, or even plasma cutting and water jet cutting, are often performed by machines that could most logically be called machine tools. In addition, some of the newly developed additive manufacturing processes, which are not about cutting away material but rather about adding it, are done by machines that are likely to end up labeled, in some cases, as machine tools. In fact, machine tool builders are already developing machines that include both subtractive and additive manufacturing in one work envelope, and retrofits of existing machines are underway.\n\nThe natural language use of the terms varies, with subtle connotative boundaries. Many speakers resist using the term \"machine tool\" to refer to woodworking machinery (joiners, table saws, routing stations, and so on), but it is difficult to maintain any true logical dividing line, and therefore many speakers accept a broad definition. It is common to hear machinists refer to their machine tools simply as \"machines\". Usually the mass noun \"machinery\" encompasses them, but sometimes it is used to imply only those machines that are being excluded from the definition of \"machine tool\". This is why the machines in a food-processing plant, such as conveyors, mixers, vessels, dividers, and so on, may be labeled \"machinery\", while the machines in the factory's tool and die department are instead called \"machine tools\" in contradistinction. As for the 1930s NBER definition quoted above, one could argue that its specificity to metal is obsolete, as it is quite common today for particular lathes, milling machines, and machining centers (definitely machine tools) to work exclusively on plastic cutting jobs throughout their whole working lifespan. Thus the NBER definition above could be expanded to say \"which employs a tool to work on metal \"or other materials of high hardness\"\". And its specificity to \"operating by other than hand power\" is also problematic, as machine tools can be powered by people if appropriately set up, such as with a treadle (for a lathe) or a hand lever (for a shaper). Hand-powered shapers are clearly \"the 'same thing' as shapers with electric motors except smaller\", and it is trivial to power a micro lathe with a hand-cranked belt pulley instead of an electric motor. Thus one can question whether power source is truly a key distinguishing concept; but for economics purposes, the NBER's definition made sense, because most of the commercial value of the existence of machine tools comes about via those that are powered by electricity, hydraulics, and so on. Such are the vagaries of natural language and controlled vocabulary, both of which have their places in the business world.\n\nForerunners of machine tools included bow drills and potter's wheels, which had existed in ancient Egypt prior to 2500 BC, and lathes, known to have existed in multiple regions of Europe since at least 1000 to 500 BC. But it was not until the later Middle Ages and the Age of Enlightenment that the modern concept of a machine tool—a class of machines used as tools in the making of metal parts, and incorporating machine-guided toolpath—began to evolve. Clockmakers of the Middle Ages and renaissance men such as Leonardo da Vinci helped expand humans' technological milieu toward the preconditions for industrial machine tools. During the 18th and 19th centuries, and even in many cases in the 20th, the builders of machine tools tended to be the same people who would then use them to produce the end products (manufactured goods). However, from these roots also evolved an industry of machine tool builders as we define them today, meaning people who specialize in building machine tools for sale to others.\n\nHistorians of machine tools often focus on a handful of major industries that most spurred machine tool development. In order of historical emergence, they have been firearms (small arms and artillery); clocks; textile machinery; steam engines (stationary, marine, rail, and otherwise) (the story of how Watt's need for an accurate cylinder spurred Boulton's boring machine is discussed by Roe); sewing machines; bicycles; automobiles; and aircraft. Others could be included in this list as well, but they tend to be connected with the root causes already listed. For example, rolling-element bearings are an industry of themselves, but this industry's main drivers of development were the vehicles already listed—trains, bicycles, automobiles, and aircraft; and other industries, such as tractors, farm implements, and tanks, borrowed heavily from those same parent industries.\n\nMachine tools filled a need created by textile machinery during the Industrial Revolution in England in the middle to late 1700s. Until that time machinery was made mostly from wood, often including gearing and shafts. The increase in mechanization required more metal parts, which were usually made of cast iron or wrought iron. Cast iron could be cast in molds for larger parts, such as engine cylinders and gears, but was difficult to work with a file and could not be hammered. Red hot wrought iron could be hammered into shapes. Room temperature wrought iron was worked with a file and chisel and could be made into gears and other complex parts; however, hand working lacked precision and was a slow and expensive process.\n\nJames Watt was unable to have an accurately bored cylinder for his first steam engine, trying for several years until John Wilkinson invented a suitable boring machine in 1774, boring Boulton & Watt's first commercial engine in 1776.\n\nThe advance in the accuracy of machine tools can be traced to Henry Maudslay and refined by Joseph Whitworth. That Maudslay had established the manufacture and use of master plane gages in his shop (Maudslay & Field) located on Westminster Road south of the Thames River in London about 1809, was attested to by James Nasmyth who was employed by Maudslay in 1829 and Nasmyth documented their use in his autobiography.\n\nThe process by which the master plane gages were produced dates back to antiquity but was refined to an unprecedented degree in the Maudslay shop. The process begins with three square plates each given an identification (ex., 1,2 and 3). The first step is to rub plates 1 and 2 together with a marking medium (called bluing today) revealing the high spots which would be removed by hand scraping with a steel scraper, until no irregularities were visible. This would not produce true plane surfaces but a \"ball and socket\" concave-concave and convex-convex fit, as this mechanical fit, like two perfect planes, can slide over each other and reveal no high spots. The rubbing and marking are repeated after rotating 2 relative to 1 by 90 degrees to eliminate concave-convex \"potato-chip\" curvature. Next, plate number 3 is compared and scraped to conform to plate number 1 in the same two trials. In this manner plates number 2 and 3 would be identical. Next plates number 2 and 3 would be checked against each other to determine what condition existed, either both plates were \"balls\" or \"sockets\" or \"chips\" or a combination. These would then be scraped until no high spots existed and then compared to plate number 1. Repeating this process of comparing and scraping the three plates could produce plane surfaces accurate to within millionths of an inch (the thickness of the marking medium).\n\nThe traditional method of producing the surface gages used an abrasive powder rubbed between the plates to remove the high spots, but it was Whitworth who contributed the refinement of replacing the grinding with hand scraping. Sometime after 1825 Whitworth went to work for Maudslay and it was there that Whitworth perfected the hand scraping of master surface plane gages. In his paper presented to the British Association for the Advancement of Science at Glasgow in 1840, Whitworth pointed out the inherent inaccuracy of grinding due to no control and thus unequal distribution of the abrasive material between the plates which would produce uneven removal of material from the plates.\n\nWith the creation of master plane gages of such high accuracy, all critical components of machine tools (i.e., guiding surfaces such as machine ways) could then be compared against them and scraped to the desired accuracy.\nThe first machine tools offered for sale (i.e., commercially available) were constructed by Matthew Murray in England around 1800. Others, such as Henry Maudslay, James Nasmyth, and Joseph Whitworth, soon followed the path of expanding their entrepreneurship from manufactured end products and millwright work into the realm of building machine tools for sale.\nImportant early machine tools included the slide rest lathe, screw-cutting lathe, turret lathe, milling machine, pattern tracing lathe, shaper, and metal planer, which were all in use before 1840. With these machine tools the decades-old objective of producing interchangeable parts was finally realized. An important early example of something now taken for granted was the standardization of screw fasteners such as nuts and bolts. Before about the beginning of the 19th century, these were used in pairs, and even screws of the same machine were generally not interchangeable. Methods were developed to cut screw thread to a greater precision than that of the feed screw in the lathe being used. This led to the bar length standards of the 19th and early 20th centuries.\n\nAmerican production of machine tools was a critical factor in the Allies' victory in World War II. Production of machine tools tripled in the United States in the war. No war was more industrialized than World War II, and it has been written that the war was won as much by machine shops as by machine guns.\n\nThe production of machine tools is concentrated in about 10 countries worldwide: China, Japan, Germany, Italy, South Korea, Taiwan, Switzerland, USA, Austria, Spain and a few others. Machine tool innovation continues in several public and private research centers worldwide.\n\n“all the turning of the iron for the cotton machinery built by Mr. Slater was done with hand chisels or tools in lathes turned by cranks with hand power”. David Wilkinson\nMachine tools can be powered from a variety of sources. Human and animal power (via cranks, treadles, treadmills, or treadwheels) were used in the past, as was water power (via water wheel); however, following the development of high-pressure steam engines in the mid 19th century, factories increasingly used steam power. Factories also used hydraulic and pneumatic power. Many small workshops continued to use water, human and animal power until electrification after 1900.\n\nToday most machine tools are powered by electricity; however, hydraulic and pneumatic power are sometimes used, but this is uncommon.\n\nMachine tools can be operated manually, or under automatic control. Early machines used flywheels to stabilize their motion and had complex systems of gears and levers to control the machine and the piece being worked on. Soon after World War II, the numerical control (NC) machine was developed. NC machines used a series of numbers punched on paper tape or punched cards to control their motion. In the 1960s, computers were added to give even more flexibility to the process. Such machines became known as computerized numerical control (CNC) machines. NC and CNC machines could precisely repeat sequences over and over, and could produce much more complex pieces than even the most skilled tool operators.\n\nBefore long, the machines could automatically change the specific cutting and shaping tools that were being used. For example, a drill machine might contain a magazine with a variety of drill bits for producing holes of various sizes. Previously, either machine operators would usually have to manually change the bit or move the work piece to another station to perform these different operations. The next logical step was to combine several different machine tools together, all under computer control. These are known as machining centers, and have dramatically changed the way parts are made.\n\nExamples of machine tools are:\n\n\nWhen fabricating or shaping parts, several techniques are used to remove unwanted metal. Among these are:\n\n\nOther techniques are used to \"add\" desired material. Devices that fabricate components by selective \"addition\" of material are called rapid prototyping machines.\n\nThe worldwide market for machine tools was approximately $81 billion in production in 2014 according to a survey by market research firm Gardner Research. The largest producer of machine tools was China with $23.8 billion of production followed by Germany and Japan at neck and neck with $12.9 billion and $12.88 billion respectively. South Korea and Italy rounded out the top 5 producers with revenue of $5.6 billion and $5 billion respectively.\n\n\n"}
{"id": "14270007", "url": "https://en.wikipedia.org/wiki?curid=14270007", "title": "Midwestern Greenhouse Gas Reduction Accord", "text": "Midwestern Greenhouse Gas Reduction Accord\n\nThe Midwestern Greenhouse Gas Reduction Accord (Midwestern Accord) is a regional agreement by six governors of states in the US Midwest who are members of the Midwestern Governors Association (MGA), and the premier of one Canadian province, whose purpose is to reduce greenhouse gas emissions to combat climate change. The accord has been inactive since March 2010, when an advisory group presented a plan for action to the association with a scheduled implementation date of January 2012. Signatories to the accord are the US states of Minnesota, Wisconsin, Illinois, Iowa, Michigan, Kansas, and the Canadian Province of Manitoba. Observers of the accord are Indiana, Ohio, and South Dakota, as well as the Canadian Province of Ontario.\n\nWhile the Midwest has intensive manufacturing and agriculture sectors, making it the most coal-dependent region in North America, it also has significant renewable energy resources and is particularly vulnerable to the climate change caused by burning coal and other fossil fuels. \n\nThe Midwestern Accord was the fourth tier of the MGA Energy Security and Climate Stewardship Summit Platform, signed on November 15, 2007. It established the Midwestern Greenhouse Gas Reduction Program, which aimed to:\n\nThrough the Midwestern Accord, the governors agreed to establish a Midwestern greenhouse gas reduction program to reduce greenhouse gas emissions in their states, as well as a working group to provide recommendations regarding the implementation of the accord. In June 2009, the Midwestern Greenhouse Gas Reduction Accord Advisory Group finalized its draft recommendations. In March 2010 the advisory group presented a plan to the MGA that called for implementation beginning in January 2012. No further action was taken, as leadership in several of the states switched positions on climate policy.\n\nIn July 2014, accord member Kansas and observers Indiana, South Dakota, and Ohio joined a lawsuit opposing the EPA Clean Power Plan, federal climate regulations which could be met by implementation of the accord.\n\n\n"}
{"id": "3938286", "url": "https://en.wikipedia.org/wiki?curid=3938286", "title": "Mullard 5-10", "text": "Mullard 5-10\n\nThe Mullard 5-10 was a circuit for a valve amplifier designed by the British valve company, Mullard in 1954 at the Mullard Applications Research Laboratory (ARL) in Mitcham Surrey UK, part of the New Road factory complex, to take advantage of their particular products. The circuit was first published in Practical Wireless magazine.\n\nThe amplifier featured five valves and an output of 10 watts - hence '5-10'. Of those valves, one was a full-wave rectifier (an EZ80 or EZ81), one was a pre-amplifier pentode EF86 and one a double-triode ECC83 as phase splitter. The power amplification was handled by a pair of EL84 working in push-pull configuration.\n\nThe frequency response of the circuit was from 40Hz to 20,000Hz with less than 0.2% THD. \n\nIn 1959 Mullard published its famous booklet \"Mullard Circuits for Audio Amplifiers\" covering a range of amplifier and pre-amplifier circuits using valves developed within the Receiving Valve Development Department at the New Road factory.\n\nThe circuit design of the Mullard 5-10, together with the recommended Partridge output transformers, was famous for its unique sound reproduction and many variations of this amplifier (including Mullard's own 20-watt version, the Mullard 5-20 using the EL34) were in widespread use until the end of the valve era; similar designs are still manufactured as expensive equipment for valve audiophiles.\n\n"}
{"id": "2149148", "url": "https://en.wikipedia.org/wiki?curid=2149148", "title": "Municipal solid waste", "text": "Municipal solid waste\n\nMunicipal solid waste (MSW), commonly known as trash or garbage in the United States and rubbish in Britain, is a waste type consisting of everyday items that are discarded by the public. \"Garbage\" can also refer specifically to food waste, as in a garbage disposal; the two are sometimes collected separately.\n\nIn the European Union, the semantic definition is 'mixed municipal waste,' given waste code 20 03 01 in the European Waste Catalog. Although the waste may originate from a number of sources that has nothing to do with a municipality, the traditional role of municipalities in collecting and managing these kinds of waste have produced the particular etymology 'municipal.'\n\nThe composition of municipal solid waste varies greatly from municipality to municipality, and it changes significantly with time. In municipalities which have a well developed waste recycling system, the waste stream mainly consists of intractable wastes such as plastic film and non-recyclable packaging materials. At the start of the 20th century, the majority of domestic waste (53%) in the UK consisted of coal ash from open fires.\nIn developed areas without significant recycling activity it predominantly includes food wastes, market wastes, yard wastes, plastic containers and product packaging materials, and other miscellaneous solid wastes from residential, commercial, institutional, and industrial sources. Most definitions of municipal solid waste do not include industrial wastes, agricultural wastes, medical waste, radioactive waste or sewage sludge. Waste collection is performed by the municipality within a given area. The term \"residual waste\" relates to waste left from household sources containing materials that have not been separated out or sent for reprocessing. Waste can be classified in several ways but the following list represents a typical classification:\n\n\nThe municipal solid waste industry has four components: recycling, composting, disposal, and waste-to-energy via incineration. There is no single approach that can be applied to the management of all waste streams, therefore the Environmental Protection Agency, a U.S. federal government agency, developed a hierarchy ranking strategy for municipal solid waste. The Waste Management Hierarchy is made up of four levels ordered from most preferred to least preferred methods based on their environmental soundness: Source reduction and reuse; recycling or composting; energy recovery; treatment and disposal.\n\nThe functional element of collection includes not only the gathering of solid waste and recyclable materials, but also the transport of these materials, after collection, to the location where the collection vehicle is emptied. This location may be a materials processing facility, a transfer station or a landfill disposal site.\n\nWaste handling and separation involves activities associated with waste management until the waste is placed in storage containers for collection. Handling also encompasses the movement of loaded containers to the point of collection. Separating different types of waste components is an important step in the handling and storage of solid waste at the source.\n\nThe types of means and facilities that are now used for the recovery of waste materials that have been separated at the source include curbside ('kerbside' in the UK) collection, drop-off and buy-back centers. The separation and processing of wastes that have been separated at the source and the separation of commingled wastes usually occur at a materials recovery facility, transfer stations, combustion facilities and treatment plants.\n\nThis element involves two main steps. First, the waste is transferred from a smaller collection vehicle to larger transport equipment. The waste is then transported, usually over long distances, to a processing or disposal site.\n\nToday, the disposal of wastes by land filling or land spreading is the ultimate fate of all solid wastes, whether they are residential wastes collected and transported directly to a landfill site, residual materials from materials recovery facilities (MRFs), residue from the combustion of solid waste, compost, or other substances from various solid waste processing facilities. A modern sanitary landfill is not a dump; it is an engineered facility used for disposing of solid wastes on land without creating nuisances or hazards to public health or safety, such as the problems of insects and the contamination of ground water.\n\nIn the recent years environmental organizations, such as Freegle or Freecycle Network, have been gaining popularity for their online reuse networks. These networks provide a worldwide online registry of unwanted items that would otherwise be thrown away, for individuals and nonprofits to reuse or recycle. Therefore, this free Internet-based service reduces landfill pollution and promotes the gift economy.\n\nLandfills are created by land dumping. Land dumping methods vary, most commonly it involves the mass dumping of waste into a designated area, usually a hole or sidehill. After the waste is dumped, it is then compacted by large machines. When the dumping cell is full, it is then \"sealed\" with a plastic sheet and covered in several feet of dirt. This is the primary method of dumping in the United States because of the low cost and abundance of unused land in North America. Landfills pose the threat of pollution, and can intoxicate ground water. The signs of pollution are effectively masked by disposal companies and it is often hard to see any evidence. Usually landfills are surrounded by large walls or fences hiding the mounds of debris. Large amounts of chemical odor eliminating agent are sprayed in the air surrounding landfills to hide the evidence of the rotting waste inside the plant.\n\nMunicipal solid waste can be used to generate energy. Several technologies have been developed that make the processing of MSW for energy generation cleaner and more economical than ever before, including landfill gas capture, combustion, pyrolysis, gasification, and plasma arc \"gasification\". While older waste incineration plants emitted a lot of pollutants, recent regulatory changes and new technologies have significantly reduced this concern. United States Environmental Protection Agency (EPA) regulations in 1995 and 2000 under the Clean Air Act have succeeded in reducing emissions of dioxins from waste-to-energy facilities by more than 99 percent below 1990 levels, while mercury emissions have been reduced by over 90 percent. The EPA noted these improvements in 2003, citing waste-to-energy as a power source \"with less environmental impact than almost any other source of electricity\".\n\n\n"}
{"id": "9591814", "url": "https://en.wikipedia.org/wiki?curid=9591814", "title": "Munjandie", "text": "Munjandie\n\nA munjandie is an obsolete unit of mass in India approximately equal to 4 grains (0.259 g). After metrication in the mid-20th century, the unit became obsolete. \n\n"}
{"id": "13292154", "url": "https://en.wikipedia.org/wiki?curid=13292154", "title": "Myristoleic acid", "text": "Myristoleic acid\n\nMyristoleic acid, or 9-tetradecenoic acid, is an omega-5 fatty acid. It is biosynthesized from myristic acid by the enzyme Stearoyl-CoA desaturase-1, but it is uncommon in nature. One of the major sources of this fatty acid is the seed oil from plants of the family Myristicaceae, comprising up to 30 per cent of the oil in some species. It is a constituent of \"Serenoa\" or Saw palmetto, and appears to have activity against LNCaP prostate-cancer cells. It also comprises 1.89% of the fats from the fruit of the durian species \"Durio graveolens\".\n\n"}
{"id": "21100282", "url": "https://en.wikipedia.org/wiki?curid=21100282", "title": "Naval Weapons Industrial Reserve Plant, Bedford", "text": "Naval Weapons Industrial Reserve Plant, Bedford\n\nNaval Weapons Industrial Reserve Plant, Bedford (NWIRP) was a government-owned, contractor-operated (GOCO) facility which had the mission of designing, fabricating, and testing prototype weapons and equipment from 1952 until December 2000, located in Bedford, Massachusetts It is located just north of Hanscom Air Force Base. It is currently a Superfund site undergoing environmental cleanup.\n"}
{"id": "21485", "url": "https://en.wikipedia.org/wiki?curid=21485", "title": "Neutrino", "text": "Neutrino\n\nA neutrino ( or ) (denoted by the Greek letter ν) is a fermion (an elementary particle with half-integer spin) that interacts only via the weak subatomic force and gravity. The mass of the neutrino is much smaller than that of the other known elementary particles. Although only differences of squares of the three mass values are known as of 2016, cosmological observations imply that the sum of the three masses must be less than one millionth that of the electron. The neutrino is so named because it is electrically neutral and because its rest mass is so small (\"-ino\") that it was long thought to be zero. The weak force has a very short range, gravity is extremely weak on the subatomic scale, and neutrinos, as leptons, do not participate in the strong interaction. Thus, neutrinos typically pass through normal matter unimpeded and undetected.\n\nWeak interactions create neutrinos in one of three leptonic flavors: electron neutrinos muon neutrinos (), or tau neutrinos (), in association with the corresponding charged lepton. Although neutrinos were long believed to be massless, it is now known that there are three discrete neutrino masses with different tiny values, but they do not correspond uniquely to the three flavors. A neutrino created with a specific flavor is in an associated specific quantum superposition of all three mass states. As a result, neutrinos oscillate between different flavors in flight. For example, an electron neutrino produced in a beta decay reaction may interact in a distant detector as a muon or tau neutrino.\n\nFor each neutrino, there also exists a corresponding antiparticle, called an \"antineutrino\", which also has half-integer spin and no electric charge. They are distinguished from the neutrinos by having opposite signs of lepton number and chirality. To conserve total lepton number, in nuclear beta decay, electron neutrinos appear together with only positrons (anti-electrons) or electron-antineutrinos, and electron antineutrinos with electrons or electron neutrinos.\n\nNeutrinos are created by various radioactive decays, including in beta decay of atomic nuclei or hadrons, nuclear reactions such as those that take place in the core of a star or artificially in nuclear reactors, nuclear bombs or particle accelerators, during a supernova, in the spin-down of a neutron star, or when accelerated particle beams or cosmic rays strike atoms. The majority of neutrinos in the vicinity of the Earth are from nuclear reactions in the Sun. In the vicinity of the Earth, about 65 billion () solar neutrinos per second pass through every square centimeter perpendicular to the direction of the Sun.\n\nFor study, neutrinos can be created artificially with nuclear reactors and particle accelerators. There is intense research activity involving neutrinos, with goals that include the determination of the three neutrino mass values, the measurement of the degree of CP violation in the leptonic sector (leading to leptogenesis); and searches for evidence of physics beyond the Standard Model of particle physics, such as neutrinoless double beta decay, which would be evidence for violation of lepton number conservation. Neutrinos can also be used for tomography of the interior of the earth.\n\nThe neutrino was postulated first by Wolfgang Pauli in 1930 to explain how beta decay could conserve energy, momentum, and angular momentum (spin). In contrast to Niels Bohr, who proposed a statistical version of the conservation laws to explain the observed continuous energy spectra in beta decay, Pauli hypothesized an undetected particle that he called a \"neutron\", using the same \"-on\" ending employed for naming both the proton and the electron. He considered that the new particle was emitted from the nucleus together with the electron or beta particle in the process of beta decay.\n\nJames Chadwick discovered a much more massive neutral nuclear particle in 1932 and named it a neutron also, leaving two kinds of particles with the same name. Earlier (in 1930) Pauli had used the term \"neutron\" for both the neutral particle that conserved energy in beta decay, and a presumed neutral particle in the nucleus; initially he did not consider these two neutral particles as distinct from each other. The word \"neutrino\" entered the scientific vocabulary through Enrico Fermi, who used it during a conference in Paris in July 1932 and at the Solvay Conference in October 1933, where Pauli also employed it. The name (the Italian equivalent of \"little neutral one\") was jokingly coined by Edoardo Amaldi during a conversation with Fermi at the Institute of Physics of via Panisperna in Rome, in order to distinguish this light neutral particle from Chadwick's heavy neutron.\n\nIn Fermi's theory of beta decay, Chadwick's large neutral particle could decay to a proton, electron, and the smaller neutral particle (now called an \"electron antineutrino\"):\n\nFermi's paper, written in 1934, unified Pauli's neutrino with Paul Dirac's positron and Werner Heisenberg's neutron–proton model and gave a solid theoretical basis for future experimental work. The journal \"Nature\" rejected Fermi's paper, saying that the theory was \"too remote from reality\". He submitted the paper to an Italian journal, which accepted it, but the general lack of interest in his theory at that early date caused him to switch to experimental physics.\n\nBy 1934 there was experimental evidence against Bohr's idea that energy conservation is invalid for beta decay: At the Solvay conference of that year, measurements of the energy spectra of beta particles (electrons) were reported, showing that there is a strict limit on the energy of electrons from each type of beta decay. Such a limit is not expected if the conservation of energy is invalid, in which case any amount of energy would be statistically available in at least a few decays. The natural explanation of the beta decay spectrum as first measured in 1934 was that only a limited (and conserved) amount of energy was available, and a new particle was sometimes taking a varying fraction of this limited energy, leaving the rest for the beta particle. Pauli made use of the occasion to publicly emphasize that the still-undetected \"neutrino\" must be an actual particle.\n\nIn 1942, Wang Ganchang first proposed the use of beta capture to experimentally detect neutrinos. In the 20 July 1956 issue of \"Science\", Clyde Cowan, Frederick Reines, F. B. Harrison, H. W. Kruse, and A. D. McGuire published confirmation that they had detected the neutrino, a result that was rewarded almost forty years later with the 1995 Nobel Prize.\n\nIn this experiment, now known as the Cowan–Reines neutrino experiment, antineutrinos created in a nuclear reactor by beta decay reacted with protons to produce neutrons and positrons:\n\nThe positron quickly finds an electron, and they annihilate each other. The two resulting gamma rays (γ) are detectable. The neutron can be detected by its capture on an appropriate nucleus, releasing a gamma ray. The coincidence of both events – positron annihilation and neutron capture – gives a unique signature of an antineutrino interaction.\n\nIn February 1965, the first neutrino found in nature was identified in one of South Africa's gold mines by a group which included Friedel Sellschop. The experiment was performed in a specially prepared chamber at a depth of 3 km in the ERPM mine near Boksburg. A plaque in the main building commemorates the discovery. The experiments also implemented a primitive neutrino astronomy and looked at issues of neutrino physics and weak interactions.\n\nThe antineutrino discovered by Cowan and Reines is the antiparticle of the electron neutrino.\n\nIn 1962, Leon M. Lederman, Melvin Schwartz and Jack Steinberger showed that more than one type of neutrino exists by first detecting interactions of the muon neutrino (already hypothesised with the name \"neutretto\"), which earned them the 1988 Nobel Prize in Physics.\n\nWhen the third type of lepton, the tau, was discovered in 1975 at the Stanford Linear Accelerator Center, it too was expected to have an associated neutrino (the tau neutrino). First evidence for this third neutrino type came from the observation of missing energy and momentum in tau decays analogous to the beta decay leading to the discovery of the electron neutrino. The first detection of tau neutrino interactions was announced in 2000 by the DONUT collaboration at Fermilab; its existence had already been inferred by both theoretical consistency and experimental data from the Large Electron–Positron Collider.\n\nIn the 1960s, the now-famous Homestake experiment made the first measurement of the flux of electron neutrinos arriving from the core of the Sun and found a value that was between one third and one half the number predicted by the Standard Solar Model. This discrepancy, which became known as the solar neutrino problem, remained unresolved for some thirty years, while possible problems with both the experiment and the solar model were investigated, but none could be found. Eventually it was realized that both were correct, but rather it was the neutrinos themselves that were far more interesting than expected. It was postulated that the three neutrinos had nonzero and slightly but indistinguishably different masses, and could therefore oscillate into undetectable flavors on their flight to the Earth. This hypothesis was investigated by a new series of experiments, thereby opening a new major field of research that still continues. Eventual confirmation of the phenomenon of neutrino oscillation led to two Nobel prizes, to Raymond Davis, Jr., who conceived and led the Homestake experiment, and to Art McDonald, who led the SNO experiment, which could detect all of the neutrino flavors and found no deficit.\n\nA practical method for investigating neutrino oscillations was first suggested by Bruno Pontecorvo in 1957 using an analogy with kaon oscillations; over the subsequent 10 years he developed the mathematical formalism and the modern formulation of vacuum oscillations. In 1985 Stanislav Mikheyev and Alexei Smirnov (expanding on 1978 work by Lincoln Wolfenstein) noted that flavor oscillations can be modified when neutrinos propagate through matter. This so-called Mikheyev–Smirnov–Wolfenstein effect (MSW effect) is important to understand because many neutrinos emitted by fusion in the Sun pass through the dense matter in the solar core (where essentially all solar fusion takes place) on their way to detectors on Earth.\n\nStarting in 1998, experiments began to show that solar and atmospheric neutrinos change flavors (see Super-Kamiokande and Sudbury Neutrino Observatory). This resolved the solar neutrino problem: the electron neutrinos produced in the Sun had partly changed into other flavors which the experiments could not detect.\n\nAlthough individual experiments, such as the set of solar neutrino experiments, are consistent with non-oscillatory mechanisms of neutrino flavor conversion, taken altogether, neutrino experiments imply the existence of neutrino oscillations. Especially relevant in this context are the reactor experiment KamLAND and the accelerator experiments such as MINOS. The KamLAND experiment has indeed identified oscillations as the neutrino flavor conversion mechanism involved in the solar electron neutrinos. Similarly MINOS confirms the oscillation of atmospheric neutrinos and gives a better determination of the mass squared splitting. Takaaki Kajita of Japan and Arthur B. McDonald of Canada received the 2015 Nobel Prize for Physics for their landmark finding, theoretical and experimental, that neutrinos can change flavors.\n\nRaymond Davis, Jr. and Masatoshi Koshiba were jointly awarded the 2002 Nobel Prize in Physics. Both conducted pioneering work on solar neutrino detection, and Koshiba's work also resulted in the first real-time observation of neutrinos from the SN 1987A supernova in the nearby Large Magellanic Cloud. These efforts marked the beginning of neutrino astronomy.\nSN 1987A represents the only verified detection of neutrinos from a supernova.\n\nThe neutrino has half-integer spin (½ ħ) and is therefore a fermion. Also being leptons, neutrinos have been observed to interact through only the weak force, although it is assumed that they also interact gravitationally.\n\nWeak interactions create neutrinos in one of three leptonic flavors: electron neutrinos (), muon neutrinos (), or tau neutrinos (), in association with the corresponding electron, muon, and tau charged leptons, respectively.\n\nAlthough neutrinos were long believed to be massless, it is now known that there are also three discrete neutrino masses, but they don't correspond uniquely to the three flavors. Although only differences of squares of the three mass values are known as of 2016, experiments have shown that these masses are tiny in magnitude. From cosmological measurements, it has been calculated that the sum of the three neutrino masses must be less than one millionth that of the electron.\n\nMore formally, neutrino flavor eigenstates are not the same as the neutrino mass eigenstates (simply labelled 1, 2, 3). As of 2016, it is not known which of these three is the heaviest. In analogy with the mass hierarchy of the charged leptons, the configuration with mass 2 being lighter than mass 3 is conventionally called the \"normal hierarchy\", while in the \"inverted hierarchy\", the opposite would hold. Several major experimental efforts are underway to help establish which is correct.\n\nA neutrino created in a specific flavor eigenstate is in an associated specific quantum superposition of all three mass eigenstates. This is possible because the three masses differ so little that they cannot be experimentally distinguished within any practical flight path, due to the uncertainty principle. The proportion of each mass state in the produced pure flavor state has been found to depend strongly on that flavor. The relationship between flavor and mass eigenstates is encoded in the PMNS matrix. Experiments have established values for the elements of this matrix.\n\nThe existence of a neutrino mass allows the possibility of a tiny neutrino magnetic moment, in which case neutrinos could interact electromagnetically as well; no such interaction has been discovered.\n\nNeutrinos oscillate between different flavors in flight. For example, an electron neutrino produced in a beta decay reaction may interact in a distant detector as a muon or tau neutrino, as defined by the flavor of the charged lepton produced in the detector. This oscillation occurs because the three mass state components of the produced flavor travel at slightly different speeds, so that their quantum mechanical wave packets develop relative phase shifts that change how they combine to produce a varying superposition of three flavors. Each flavor component thereby oscillates sinusoidally as the neutrino travels, with the flavors varying in relative strengths. The relative flavor proportions when the neutrino interacts represent the relative probabilities for that flavor of interaction to produce the corresponding flavor of charged lepton.\n\nThere are other possibilities in which neutrino could oscillate even if they were massless. If Lorentz symmetry were not an exact symmetry, neutrinos could experience Lorentz-violating oscillations.\n\nNeutrinos traveling through matter, in general, undergo a process analogous to light traveling through a transparent material. This process is not directly observable because it does not produce ionizing radiation, but gives rise to the MSW effect. Only a small fraction of the neutrino's energy is transferred to the material.\n\nFor each neutrino, there also exists a corresponding antiparticle, called an \"antineutrino\", which also has no electric charge and half-integer spin. They are distinguished from the neutrinos by having opposite signs of lepton number and opposite chirality. As of 2016, no evidence has been found for any other difference. In all observations so far of leptonic processes (despite extensive and continuing searches for exceptions), there is no overall change in lepton number; for example, if total lepton number is zero in the initial state, electron neutrinos appear in the final state together with only positrons (anti-electrons) or electron-antineutrinos, and electron antineutrinos with electrons or electron neutrinos.\n\nAntineutrinos are produced in nuclear beta decay together with a beta particle, in which, e.g., a neutron decays into a proton, electron, and antineutrino. All antineutrinos observed thus far possess right-handed helicity (i.e. only one of the two possible spin states has ever been seen), while neutrinos are left-handed. Nevertheless, as neutrinos have mass, their helicity is frame-dependent, so it is the related frame-independent property of chirality that is relevant here.\n\nAntineutrinos were first detected as a result of their interaction with protons in a large tank of water. This was installed next to a nuclear reactor as a controllable source of the antineutrinos (See: Cowan–Reines neutrino experiment).\nResearchers around the world have begun to investigate the possibility of using antineutrinos for reactor monitoring in the context of preventing the proliferation of nuclear weapons.\n\nBecause antineutrinos and neutrinos are neutral particles, it is possible that they are the same particle. Particles that have this property are known as Majorana particles, after the Italian physicist Ettore Majorana who first proposed the concept. For the case of neutrinos this theory has gained popularity as it can be used, in combination with the seesaw mechanism, to explain why neutrino masses are so small compared to those of the other elementary particles, such as electrons or quarks. Majorana neutrinos have the property that the neutrino and antineutrino could be distinguished only by chirality; what experiments observe as a difference between the neutrino and antineutrino could simply be due to one particle with two possible chiralities.\n\nIt is not yet known whether neutrinos are Majorana or Dirac particles; it is possible to test this property experimentally. For example, if neutrinos are indeed Majorana particles, then lepton-number violating processes such as neutrinoless double beta decay would be allowed, while they would not if neutrinos are Dirac particles. Several experiments have been and are being conducted to search for this process, e.g. GERDA. and SNO+. The cosmic neutrino background is also a probe of whether neutrinos are Majorana particles, since there should be a different number of cosmic neutrinos detected in either the Dirac or Majorana case.\n\nNeutrinos can interact with a nucleus, changing it to another nucleus. This process is used in radiochemical neutrino detectors. In this case, the energy levels and spin states within the target nucleus have to be taken into account to estimate the probability for an interaction. In general the interaction probability increases with the number of neutrons and protons within a nucleus.\n\nIt is very hard to uniquely identify neutrino interactions among the natural background of radioactivity. For this reason, in early experiments a special reaction channel was chosen to facilitate the identification: the interaction of an antineutrino with one of the hydrogen nuclei in the water molecules. A hydrogen nucleus is a single proton, so simultaneous nuclear interactions, which would occur within a heavier nucleus, don't need to be considered for the detection experiment. Within a cubic metre of water placed right outside a nuclear reactor, only relatively few such interactions can be recorded, but the setup is now used for measuring the reactor's plutonium production rate.\n\nVery much like neutrons do in nuclear reactors, neutrinos can induce fission reactions within heavy nuclei. So far, this reaction has not been measured in a laboratory, but is predicted to happen within stars and supernovae. The process affects the abundance of isotopes seen in the universe. Neutrino fission of deuterium nuclei has been observed in the Sudbury Neutrino Observatory, which uses a heavy water detector.\n\nObservations of the cosmic microwave background suggest that neutrinos do not interact with themselves.\n\nThere are three known types (\"flavors\") of neutrinos: electron neutrino , muon neutrino and tau neutrino , named after their partner leptons in the Standard Model (see table at right). The current best measurement of the number of neutrino types comes from observing the decay of the Z boson. This particle can decay into any light neutrino and its antineutrino, and the more types of light neutrinos available, the shorter the lifetime of the Z boson. Measurements of the Z lifetime have shown that the number of light neutrino flavors that couple to the Z is 3. The correspondence between the six quarks in the Standard Model and the six leptons, among them the three neutrinos, suggests to physicists' intuition that there should be exactly three types of neutrino. Proof that there are only three kinds of neutrinos remains an elusive goal of particle physics.\n\nThere are several active research areas involving the neutrino. Some are concerned with testing predictions of neutrino behavior. Other research is focused on measurement of unknown properties of neutrinos, especially their masses and CP violation, as they cannot be predicted with existing theories.\n\nInternational scientific collaborations install large neutrino detectors near nuclear reactors or in neutrino beams from particle accelerators to better constrain the neutrino masses and the values for the magnitude and rates of oscillations between neutrino flavors. These experiments are thereby searching for the existence of CP violation in the neutrino sector; that is, whether or not the laws of physics treat neutrinos and antineutrinos differently.\n\nThe KATRIN experiment in Germany has begun to acquire data in June 2018 to determine the value of the mass of the electron neutrino, with other approaches to this problem in the planning stages.\n\nOn 19 July 2013, the results from the T2K experiment presented at the European Physical Society Conference on High Energy Physics in Stockholm, Sweden, confirmed neutrino oscillation theory.\n\nDespite their tiny masses, neutrinos are so numerous that their gravitational force can influence other matter in the universe.\n\nThe three known neutrino flavors are the only established elementary particle candidates for dark matter, specifically hot dark matter, although that possibility appears to be largely ruled out by observations of the cosmic microwave background. If heavier sterile neutrinos exist, they might serve as warm dark matter, which still seems plausible.\n\nOther efforts search for evidence of a sterile neutrino – a fourth neutrino flavor that does not interact with matter like the three known neutrino flavors. The possibility of \"sterile\" neutrinos is unaffected by the Z-boson decay measurements described above: If their mass is greater than half the Z-boson's mass, they would not be a decay product. Therefore, heavy sterile neutrinos would have a mass of at least 45.6 GeV.\n\nThe existence of such particles is in fact hinted by experimental data from the LSND experiment. On the other hand, the currently running MiniBooNE experiment suggested that sterile neutrinos are not required to explain the experimental data, although the latest research into this area is on-going and anomalies in the MiniBooNE data may allow for exotic neutrino types, including sterile neutrinos. A recent re-analysis of reference electron spectra data from the Institut Laue-Langevin has also hinted at a fourth, sterile neutrino.\n\nAccording to an analysis published in 2010, data from the Wilkinson Microwave Anisotropy Probe of the cosmic background radiation is compatible with either three or four types of neutrinos.\n\nAnother hypothesis concerns \"neutrinoless double-beta decay\", which, if it exists, would violate lepton number conservation and imply a minuscule splitting (or difference) between the physical masses of what are conventionally called a “neutrino” and its corresponding “antineutrino” having the opposite sign in its lepton number. Searches for this mechanism are underway but have not yet found strong evidenced for it. If they were to, then what are now called antineutrinos could not be true antiparticles. The resulting \"six\" distinct neutrinos would have no distinct antiparticle partner. Cosmic ray neutrino experiments detect neutrinos from space to study both the nature of neutrinos and the cosmic sources producing them.\n\nBefore neutrinos were found to oscillate, they were generally assumed to be massless, propagating at the speed of light. According to the theory of special relativity, the question of neutrino velocity is closely related to their mass: if neutrinos are massless, they must travel at the speed of light, and if they have mass they cannot reach the speed of light. Due to their tiny mass, the predicted speed is extremely close to the speed of light in all experiments, and current detectors are not sensitive to the expected difference.\n\nAlso some Lorentz-violating variants of quantum gravity might allow faster-than-light neutrinos. A comprehensive framework for Lorentz violations is the Standard-Model Extension (SME).\n\nIn the early 1980s, first measurements of neutrino speed were done using pulsed pion beams (produced by pulsed proton beams hitting a target). The pions decayed producing neutrinos, and the neutrino interactions observed within a time window in a detector at a distance were consistent with the speed of light. This measurement was repeated in 2007 using the MINOS detectors, which found the speed of neutrinos to be, at the 99% confidence level, in the range between and . The central value of 1.000051\"c\" is higher than the speed of light but is also consistent with a velocity of exactly \"c\" or even slightly less. This measurement set an upper bound on the mass of the muon neutrino of at 99% confidence. After the detectors for the project were upgraded in 2012, MINOS refined their initial result and found agreement with the speed of light, with the difference in the arrival time of neutrinos and light of −0.0006% (±0.0012%).\n\nA similar observation was made, on a much larger scale, with supernova 1987A (SN 1987A). 10 MeV antineutrinos from the supernova were detected within a time window that was consistent with the speed of light for the neutrinos. So far, all measurements of neutrino speed have been consistent with the speed of light.\n\nIn September 2011, the OPERA collaboration released calculations showing velocities of 17 GeV and 28 GeV neutrinos exceeding the speed of light in their experiments (see Faster-than-light neutrino anomaly). In November 2011, OPERA repeated its experiment with changes so that the speed could be determined individually for each detected neutrino. The results showed the same faster-than-light speed. In February 2012, reports came out that the results may have been caused by a loose fiber optic cable attached to one of the atomic clocks which measured the departure and arrival times of the neutrinos. An independent recreation of the experiment in the same laboratory by ICARUS found no discernible difference between the speed of a neutrino and the speed of light.\n\nIn June 2012, CERN announced that new measurements conducted by all four Gran Sasso experiments (OPERA, ICARUS, Borexino and LVD) found agreement between the speed of light and the speed of neutrinos, finally refuting the initial OPERA claim.\n\nThe Standard Model of particle physics assumed that neutrinos are massless. The experimentally established phenomenon of neutrino oscillation, which mixes neutrino flavour states with neutrino mass states (analogously to CKM mixing), requires neutrinos to have nonzero masses. Massive neutrinos were originally conceived by Bruno Pontecorvo in the 1950s. Enhancing the basic framework to accommodate their mass is straightforward by adding a right-handed Lagrangian.\n\nProviding for neutrino mass can be done in two ways, and some proposals use both:\n\nThe strongest upper limit on the masses of neutrinos comes from cosmology: the Big Bang model predicts that there is a fixed ratio between the number of neutrinos and the number of photons in the cosmic microwave background. If the total energy of all three types of neutrinos exceeded an average of per neutrino, there would be so much mass in the universe that it would collapse. This limit can be circumvented by assuming that the neutrino is unstable, but there are limits within the Standard Model that make this difficult. A much more stringent constraint comes from a careful analysis of cosmological data, such as the cosmic microwave background radiation, galaxy surveys, and the Lyman-alpha forest. These indicate that the summed masses of the three neutrinos must be less than .\n\nThe Nobel prize in Physics 2015 was awarded to both Takaaki Kajita and Arthur B. McDonald for their experimental discovery of neutrino oscillations, which demonstrates that neutrinos have mass.\n\nIn 1998, research results at the Super-Kamiokande neutrino detector determined that neutrinos can oscillate from one flavor to another, which requires that they must have a nonzero mass. While this shows that neutrinos have mass, the absolute neutrino mass scale is still not known. This is because neutrino oscillations are sensitive only to the difference in the squares of the masses. The best estimate of the difference in the squares of the masses of mass eigenstates 1 and 2 was published by KamLAND in 2005: |Δ\"m\"| = . In 2006, the MINOS experiment measured oscillations from an intense muon neutrino beam, determining the difference in the squares of the masses between neutrino mass eigenstates 2 and 3. The initial results indicate |Δ\"m\"| = 0.0027 eV, consistent with previous results from Super-Kamiokande. Since |Δ\"m\"| is the difference of two squared masses, at least one of them has to have a value which is at least the square root of this value. Thus, there exists at least one neutrino mass eigenstate with a mass of at least .\n\nIn 2009, lensing data of a galaxy cluster were analyzed to predict a neutrino mass of about . This surprisingly high value requires that the three neutrino masses be nearly equal, with neutrino oscillations on the order of \"milli\" electron-Volts. In 2016 this was updated to a mass of . It predicts 3 sterile neutrinos of the same mass, stems with the Planck dark matter fraction and the non-observation of neutrinoless double beta decay. The masses lie below the Mainz-Troitsk upper bound of for the electron antineutrino. The latter is being tested since June 2018 in the KATRIN experiment, that searches for a mass between and .\n\nA number of efforts are under way to directly determine the absolute neutrino mass scale in laboratory experiments. The methods applied involve nuclear beta decay (KATRIN and MARE).\n\nOn 31 May 2010, OPERA researchers observed the first tau neutrino candidate event in a muon neutrino beam, the first time this transformation in neutrinos had been observed, providing further evidence that they have mass.\n\nIn July 2010, the 3-D MegaZ DR7 galaxy survey reported that they had measured a limit of the combined mass of the three neutrino varieties to be less than . A tighter upper bound yet for this sum of masses, , was reported in March 2013 by the Planck collaboration, whereas a February 2014 result estimates the sum as 0.320 ± 0.081 eV based on discrepancies between the cosmological consequences implied by Planck's detailed measurements of the Cosmic Microwave Background and predictions arising from observing other phenomena, combined with the assumption that neutrinos are responsible for the observed weaker gravitational lensing than would be expected from massless neutrinos.\n\nIf the neutrino is a Majorana particle, the mass may be calculated by finding the half-life of neutrinoless double-beta decay of certain nuclei. The current lowest upper limit on the Majorana mass of the neutrino has been set by KamLAND-Zen: 0.060–0.161 eV.\n\nStandard Model neutrinos are fundamental point-like particles, without any width or volume. Since the neutrino is an elementary particle it does not have a size in the same sense as everyday objects. An effective size can be defined using their electroweak cross section (apparent size in electroweak interaction). The characteristic areas for the electroweak interaction are measured in units called nanobarns (nb) which are 10 cm² or 10 m², roughly the area of a disc a little more than 0.3 attometer in diameter, or about 1 billionth of the size of a uranium nucleus. The electron neutrino cross section is 3.2  nanobarns the muon neutrino cross section is 1.7 nanobarns, and the tau neutrino 1.0 nanobarn. These scattering cross sections depend on no other properties than the masses of the corresponding charged leptons. This size is relevant only to the probability of scattering. Properties associated with conventional \"size\" are absent: neutrinos cannot be condensed to form a separate uniform substance and they have no minimal distance between them.\n\nExperimental results show that (nearly) all produced and observed neutrinos have left-handed helicities (spins antiparallel to momenta), and all antineutrinos have right-handed helicities, within the margin of error. In the massless limit, it means that only one of two possible chiralities is observed for either particle. These are the only chiralities included in the Standard Model of particle interactions.\n\nIt is possible that their counterparts (right-handed neutrinos and left-handed antineutrinos) simply do not exist. If they do, their properties are substantially different from observable neutrinos and antineutrinos. It is theorized that they are either very heavy (on the order of GUT scale—see \"Seesaw mechanism\"), do not participate in weak interaction (so-called \"sterile neutrinos\"), or both.\n\nThe existence of nonzero neutrino masses somewhat complicates the situation. Neutrinos are produced in weak interactions as chirality eigenstates. Chirality of a massive particle is not a constant of motion; helicity is, but the chirality operator does not share eigenstates with the helicity operator. Free neutrinos propagate as mixtures of left- and right-handed helicity states, with mixing amplitudes on the order of \"m\"/\"E\". This does not significantly affect the experiments, because neutrinos involved are nearly always ultrarelativistic, and thus mixing amplitudes are vanishingly small. Effectively, they travel so quickly and time passes so slowly in their rest-frames that they do not have enough time to change over any observable path. For example, most solar neutrinos have energies on the order of –, so the fraction of neutrinos with \"wrong\" helicity among them cannot exceed .\n\nAn unexpected series of experimental results for the rate of decay of heavy highly charged radioactive ions circulating in a storage ring has provoked theoretical activity in an effort to find a convincing explanation. The rates of weak decay of two radioactive species with half lives of about 40 s and 200 s are found to have a significant oscillatory modulation, with a period of about 7 s.\nThe observed phenomenon is known as the GSI anomaly, as the storage ring is a facility at the GSI Helmholtz Centre for Heavy Ion Research in Darmstadt Germany. As the decay process produces an electron neutrino, some of the proposed explanations for the observed oscillation rate invoke neutrino properties. Initial ideas related to flavour oscillation were met with skepticism. A more recent proposal involves mass differences between neutrino mass eigenstates.\n\nNuclear reactors are the major source of human-generated neutrinos. The majority of energy in a nuclear reactor is generated by fission (the four main fissile isotopes in nuclear reactors are , , and ), the resultant neutron-rich daughter nuclides rapidly undergo additional beta decays, each converting one neutron to a proton and an electron and releasing an electron antineutrino (). Including these subsequent decays, the average nuclear fission releases about of energy, of which roughly 95.5% is retained in the core as heat, and roughly 4.5% (or about ) is radiated away as antineutrinos. For a typical nuclear reactor with a thermal power of , the total power production from fissioning atoms is actually , of which is radiated away as antineutrino radiation and never appears in the engineering. This is to say, of fission energy is \"lost\" from this reactor and does not appear as heat available to run turbines, since antineutrinos penetrate all building materials practically without interaction.\n\nThe antineutrino energy spectrum depends on the degree to which the fuel is burned (plutonium-239 fission antineutrinos on average have slightly more energy than those from uranium-235 fission), but in general, the \"detectable\" antineutrinos from fission have a peak energy between about 3.5 and , with a maximum energy of about . There is no established experimental method to measure the flux of low-energy antineutrinos. Only antineutrinos with an energy above threshold of can trigger inverse beta decay and thus be unambiguously identified (see below). An estimated 3% of all antineutrinos from a nuclear reactor carry an energy above this threshold. Thus, an average nuclear power plant may generate over antineutrinos per second above this threshold, but also a much larger number (97%/3% ≈ 30 times this number) below the energy threshold, which cannot be seen with present detector technology. The ND280 detector has been proposed as a viable safeguard unit.\n\nSome particle accelerators have been used to make neutrino beams. The technique is to collide protons with a fixed target, producing charged pions or kaons. These unstable particles are then magnetically focused into a long tunnel where they decay while in flight. Because of the relativistic boost of the decaying particle, the neutrinos are produced as a beam rather than isotropically. Efforts to construct an accelerator facility where neutrinos are produced through muon decays are ongoing. Such a setup is generally known as a \"neutrino factory\".\n\nNuclear weapons also produce very large quantities of neutrinos. Fred Reines and Clyde Cowan considered the detection of neutrinos from a bomb prior to their search for reactor neutrinos; a fission reactor was recommended as a better alternative by Los Alamos physics division leader J.M.B. Kellogg. Fission weapons produce antineutrinos (from the fission process), and fusion weapons produce both neutrinos (from the fusion process) and antineutrinos (from the initiating fission explosion).\n\nNeutrinos are produced together with the natural background radiation. In particular, the decay chains of and isotopes, as well as, include beta decays which emit antineutrinos. These so-called geoneutrinos can provide valuable information on the Earth's interior. A first indication for geoneutrinos was found by the KamLAND experiment in 2005, updated results have been presented by KamLAND and Borexino. The main background in the geoneutrino measurements are the antineutrinos coming from reactors.\nAtmospheric neutrinos result from the interaction of cosmic rays with atomic nuclei in the Earth's atmosphere, creating showers of particles, many of which are unstable and produce neutrinos when they decay. A collaboration of particle physicists from Tata Institute of Fundamental Research (India), Osaka City University (Japan) and Durham University (UK) recorded the first cosmic ray neutrino interaction in an underground laboratory in Kolar Gold Fields in India in 1965.\n\nSolar neutrinos originate from the nuclear fusion powering the Sun and other stars.\nThe details of the operation of the Sun are explained by the Standard Solar Model. In short: when four protons fuse to become one helium nucleus, two of them have to convert into neutrons, and each such conversion releases one electron neutrino.\n\nThe Sun sends enormous numbers of neutrinos in all directions. Each second, about 65 billion () solar neutrinos pass through every square centimeter on the part of the Earth orthogonal to the direction of the Sun. Since neutrinos are insignificantly absorbed by the mass of the Earth, the surface area on the side of the Earth opposite the Sun receives about the same number of neutrinos as the side facing the Sun.\n\nIn 1966, Colgate and White\ncalculated that neutrinos carry away most of the gravitational energy released by the collapse of massive stars, events now categorized as Type Ib and Ic and Type II supernovae. When such stars collapse, matter densities at the core become so high () that the degeneracy of electrons is not enough to prevent protons and electrons from combining to form a neutron and an electron neutrino. A second and more important neutrino source is the thermal energy (100 billion kelvins) of the newly formed neutron core, which is dissipated via the formation of neutrino–antineutrino pairs of all flavors.\n\nColgate and White's theory of supernova neutrino production was confirmed in 1987, when neutrinos from Supernova 1987A were detected. The water-based detectors Kamiokande II and IMB detected 11 and 8 antineutrinos (lepton number = −1) of thermal origin, respectively, while the scintillator-based Baksan detector found 5 neutrinos (lepton number = +1) of either thermal or electron-capture origin, in a burst less than 13 seconds long. The neutrino signal from the supernova arrived at earth several hours before the arrival of the first electromagnetic radiation, as expected from the evident fact that the latter emerges along with the shock wave. The exceptionally feeble interaction with normal matter allowed the neutrinos to pass through the churning mass of the exploding star, while the electromagnetic photons were slowed.\n\nBecause neutrinos interact so little with matter, it is thought that a supernova's neutrino emissions carry information about the innermost regions of the explosion. Much of the \"visible\" light comes from the decay of radioactive elements produced by the supernova shock wave, and even light from the explosion itself is scattered by dense and turbulent gases, and thus delayed. The neutrino burst is expected to reach Earth before any electromagnetic waves, including visible light, gamma rays, or radio waves. The exact time delay of the electromagnetic waves' arrivals depends on the velocity of the shock wave and on the thickness of the outer layer of the star. For a Type II supernova, astronomers expect the neutrino flood to be released seconds after the stellar core collapse, while the first electromagnetic signal may emerge hours later, after the explosion shock wave has had time to reach the surface of the star. The Supernova Early Warning System project uses a network of neutrino detectors to monitor the sky for candidate supernova events; the neutrino signal will provide a useful advance warning of a star exploding in the Milky Way.\n\nAlthough neutrinos pass through the outer gases of a supernova without scattering, they provide information about the deeper supernova core with evidence that here, even neutrinos scatter to a significant extent. In a supernova core the densities are those of a neutron star (which is expected to be formed in this type of supernova), becoming large enough to influence the duration of the neutrino signal by delaying some neutrinos. The 13 second-long neutrino signal from SN 1987A lasted far longer than it would take for unimpeded neutrinos to cross through the neutrino-generating core of a supernova, expected to be only 3200 kilometers in diameter for SN 1987A.\n\nThe number of neutrinos counted was also consistent with a total neutrino energy of , which was estimated to be nearly all of the total energy of the supernova.\n\nFor an average supernova, approximately 10 (an octodecillion) neutrinos are released, but the actual number detected at a terrestrial detector formula_1 will be far smaller, at the level of\n\nformula_2,\n\nwhere formula_3 is the mass of the detector (with e.g. Super Kamiokande having a mass of 50 kton) and formula_4 is the distance to the supernova. Hence in practice it will only be possible to detect neutrino bursts from supernovae within or nearby the Milky Way (our own galaxy). In addition to the detection of neutrinos from individual supernovae, it should also be possible to detect the diffuse supernova neutrino background, which originates from all supernovae in the Universe.\n\nThe energy of supernova neutrinos ranges from a few to several tens of MeV. The sites where cosmic rays are accelerated are expected to produce neutrinos that are at least one million times more energetic, produced from turbulent gaseous environments left over by supernova explosions: the supernova remnants. The origin of the cosmic rays was attributed to supernovas by Walter Baade and Fritz Zwicky; this hypothesis was refined by Vitaly L. Ginzburg and Sergei I. Syrovatsky who attributed the origin to supernova remnants, and supported their claim by the crucial remark, that the cosmic ray losses of the Milky Way is compensated, if the efficiency of acceleration in supernova remnants is about 10 percent. Ginzburg and Syrovatskii's hypothesis is supported by the specific mechanism of \"shock wave acceleration\" happening in supernova remnants, which is consistent with the original theoretical picture drawn by Enrico Fermi, and is receiving support from observational data. The very-high-energy neutrinos are still to be seen, but this branch of neutrino astronomy is just in its infancy. The main existing or forthcoming experiments that aim at observing very-high-energy neutrinos from our galaxy are Baikal, AMANDA, IceCube, ANTARES, NEMO and Nestor. Related information is provided by very-high-energy gamma ray observatories, such as VERITAS, HESS and MAGIC. Indeed, the collisions of cosmic rays are supposed to produce charged pions, whose decay give the neutrinos, and also neutral pions, whose decay give gamma rays: the environment of a supernova remnant is transparent to both types of radiation.\n\nStill-higher-energy neutrinos, resulting from the interactions of extragalactic cosmic rays, could be observed with the Pierre Auger Observatory or with the dedicated experiment named ANITA.\n\nIt is thought that, just like the cosmic microwave background radiation left over from the Big Bang, there is a background of low-energy neutrinos in our Universe. In the 1980s it was proposed that these may be the explanation for the dark matter thought to exist in the universe. Neutrinos have one important advantage over most other dark matter candidates: it is known that they exist. This idea also has serious problems.\n\nFrom particle experiments, it is known that neutrinos are very light. This means that they easily move at speeds close to the speed of light. For this reason, dark matter made from neutrinos is termed \"hot dark matter\". The problem is that being fast moving, the neutrinos would tend to have spread out evenly in the universe before cosmological expansion made them cold enough to congregate in clumps. This would cause the part of dark matter made of neutrinos to be smeared out and unable to cause the large galactic structures that we see.\n\nThese same galaxies and groups of galaxies appear to be surrounded by dark matter that is not fast enough to escape from those galaxies. Presumably this matter provided the gravitational nucleus for formation. This implies that neutrinos cannot make up a significant part of the total amount of dark matter.\n\nFrom cosmological arguments, relic background neutrinos are estimated to have density of 56 of each type per cubic centimeter and temperature () if they are massless, much colder if their mass exceeds . Although their density is quite high, they have not yet been observed in the laboratory, as their energy is below thresholds of most detection methods, and due to extremely low neutrino interaction cross-sections at sub-eV energies. In contrast, boron-8 solar neutrinos—which are emitted with a higher energy—have been detected definitively despite having a space density that is lower than that of relic neutrinos by some 6 orders of magnitude.\n\nNeutrinos cannot be detected directly, because they do not ionize the materials they are passing through (they do not carry electric charge and other proposed effects, like the MSW effect, do not produce traceable radiation). A unique reaction to identify antineutrinos, sometimes referred to as inverse beta decay, as applied by Reines and Cowan (see below), requires a very large detector to detect a significant number of neutrinos. All detection methods require the neutrinos to carry a minimum threshold energy. So far, there is no detection method for low-energy neutrinos, in the sense that potential neutrino interactions (for example by the MSW effect) cannot be uniquely distinguished from other causes. Neutrino detectors are often built underground to isolate the detector from cosmic rays and other background radiation.\n\nAntineutrinos were first detected in the 1950s near a nuclear reactor. Reines and Cowan used two targets containing a solution of cadmium chloride in water. Two scintillation detectors were placed next to the cadmium targets. Antineutrinos with an energy above the threshold of caused charged current interactions with the protons in the water, producing positrons and neutrons. This is very much like decay, where energy is used to convert a proton into a neutron, a positron () and an electron neutrino () is emitted:\n\nFrom known decay:\n\nIn the Cowan and Reines experiment, instead of an outgoing neutrino, you have an incoming antineutrino () from a nuclear reactor:\n\nThe resulting positron annihilation with electrons in the detector material created photons with an energy of about . Pairs of photons in coincidence could be detected by the two scintillation detectors above and below the target. The neutrons were captured by cadmium nuclei resulting in gamma rays of about that were detected a few microseconds after the photons from a positron annihilation event.\n\nSince then, various detection methods have been used. Super Kamiokande is a large volume of water surrounded by photomultiplier tubes that watch for the Cherenkov radiation emitted when an incoming neutrino creates an electron or muon in the water. The Sudbury Neutrino Observatory is similar, but uses heavy water as the detecting medium, which uses the same effects, but also allows the additional reaction any-flavor neutrino photo-dissociation of deuterium, resulting in a free neutron which is then detected from gamma radiation after chlorine-capture. Other detectors have consisted of large volumes of chlorine or gallium which are periodically checked for excesses of argon or germanium, respectively, which are created by electron-neutrinos interacting with the original substance. MINOS used a solid plastic scintillator coupled to photomultiplier tubes, while Borexino uses a liquid pseudocumene scintillator also watched by photomultiplier tubes and the NOνA detector uses liquid scintillator watched by avalanche photodiodes. The IceCube Neutrino Observatory uses of the Antarctic ice sheet near the south pole with photomultiplier tubes distributed throughout the volume.\nThe University of Liverpool ND280 detector employs the novel use of gadolinium encased light detectors in a temperature controlled magnetic field capturing double light pulse events. The T2K experiment developed the technology and practical experiments were successful in both Japan and at Wylfa power station.\n\nNeutrinos' low mass and neutral charge mean they interact exceedingly weakly with other particles and fields. This feature of weak interaction interests scientists because it means neutrinos can be used to probe environments that other radiation (such as light or radio waves) cannot penetrate.\n\nUsing neutrinos as a probe was first proposed in the mid-20th century as a way to detect conditions at the core of the Sun. The solar core cannot be imaged directly because electromagnetic radiation (such as light) is diffused by the great amount and density of matter surrounding the core. On the other hand, neutrinos pass through the Sun with few interactions. Whereas photons emitted from the solar core may require 40,000 years to diffuse to the outer layers of the Sun, neutrinos generated in stellar fusion reactions at the core cross this distance practically unimpeded at nearly the speed of light.\n\nNeutrinos are also useful for probing astrophysical sources beyond the Solar System because they are the only known particles that are not significantly attenuated by their travel through the interstellar medium. Optical photons can be obscured or diffused by dust, gas, and background radiation. High-energy cosmic rays, in the form of swift protons and atomic nuclei, are unable to travel more than about 100 megaparsecs due to the Greisen–Zatsepin–Kuzmin limit (GZK cutoff). Neutrinos, in contrast, can travel even greater distances barely attenuated.\n\nThe galactic core of the Milky Way is fully obscured by dense gas and numerous bright objects. Neutrinos produced in the galactic core might be measurable by Earth-based neutrino telescopes.\n\nAnother important use of the neutrino is in the observation of supernovae, the explosions that end the lives of highly massive stars. The core collapse phase of a supernova is an extremely dense and energetic event. It is so dense that no known particles are able to escape the advancing core front except for neutrinos. Consequently, supernovae are known to release approximately 99% of their radiant energy in a short (10-second) burst of neutrinos. These neutrinos are a very useful probe for core collapse studies.\n\nThe rest mass of the neutrino is an important test of cosmological and astrophysical theories (see \"Dark matter\"). The neutrino's significance in probing cosmological phenomena is as great as any other method, and is thus a major focus of study in astrophysical communities.\n\nThe study of neutrinos is important in particle physics because neutrinos typically have the lowest mass, and hence are examples of the lowest-energy particles theorized in extensions of the Standard Model of particle physics.\n\nIn November 2012, American scientists used a particle accelerator to send a coherent neutrino message through 780 feet of rock. This marks the first use of neutrinos for communication, and future research may permit binary neutrino messages to be sent immense distances through even the densest materials, such as the Earth's core.\n\nIn July 2018, the IceCube Neutrino Observatory announced that they have traced an extremely-high-energy neutrino that hit their Antarctica-based research station in September 2017 back to its point of origin in the blazar TXS 0506 +056 located 3.7 billion light-years away in the direction of the constellation Orion. This is the first time that a neutrino detector has been used to locate an object in space and that a source of cosmic rays has been identified.\n\n\n\n"}
{"id": "49958243", "url": "https://en.wikipedia.org/wiki?curid=49958243", "title": "Nigeria gully erosion crisis", "text": "Nigeria gully erosion crisis\n\nThe Nigeria gully erosion crisis has been ongoing since before 1980, and affects communities large and small. It is an ecological, environmental, economic, and humanitarian disaster resulting in land degradation, loss of lives, and properties worth millions of dollars. The estimated number of gullies in the country is at 3,000. Gullies and areas exposed to erosion in Southeastern Nigeria tripled from about 1.33% (1,021 km) in 1976 to about 3.7% (2,820 km) in 2006 making the region the most affected region in the country.\n\nGullies are majorly caused by surface runoff, the erosion occurs, notably, in gullies, which grow wider and deeper with each rainfall. Many of the gullies have become ravines, which can be dozens or hundreds of feet deep. Some natural processes can induce gullies, such as high amounts of rainfall, poor soil infiltration and unfavorable catchment shape.\n\nHowever, in Nigeria, the causes of gully formation can be largely attributed to human activities such as:\nThough concentrated in several towns and states in the Southeastern part of the country, the crisis affects all Nigerians indirectly. Homes and structures routinely collapse, as the gullies expand with each rainy season. Unchecked, the phenomenon will eventually transform the region into a badland.\n\nImpacts caused by this environmental menace includes:\n\nIn 2010, President Goodluck Ebele Jonathan made a request to the World Bank Nigeria office for assistance in addressing the challenges of gully erosion, emerging Land degradation and environmental insecurity in the country. This request resulted in the formation of the Nigeria Erosion and Watershed Management Project (NEWMAP), an eight (8) year multi-sectoral project aimed at addressing gully erosion in the Southern Nigeria and land degradation in the Northern Nigeria. \n\nBefore this intervention, several interventions failed to address the challenge of gully erosion mainly because of inefficiencies in designing proper structures and lack of commitment by the locals to take action. \n\nHowever, in some areas like Madona gully site, Awhum community Enugu State, and Okudu community Imo State, some sustainable interventions base on good practices were employed. These interventions addressed the menace of gully erosion in those areas.\n"}
{"id": "13802132", "url": "https://en.wikipedia.org/wiki?curid=13802132", "title": "Olga Tsepilova", "text": "Olga Tsepilova\n\nOlga Tsepilova (born 1958) is a Russian sociologist and senior research fellow at the Russian Academy of Sciences.\n\nShe has been studying the social consequences of pollution in Russia, especially in closed nuclear zones like the closed city Ozyorsk in the southern Urals, the site of the infamous Mayak nuclear facility. These studies were not acclaimed by the Federal Security Service (FSB), which has accused her of engaging in espionage.\n\nTsepilova appeared on Time Magazine's list of \"Heroes of the Environment\" October 2007.\n"}
{"id": "24573244", "url": "https://en.wikipedia.org/wiki?curid=24573244", "title": "Pasa Wildlife Reserve", "text": "Pasa Wildlife Reserve\n\nPasa Wildlife Reserve is a wildlife reserve of Burma. It is located in the mountainous area of the Loi La Range near Tachileik in the eastern part of Shan State, close to the border with Thailand and Laos. There is illegal trade with Tiger parts in this area.\n\n"}
{"id": "37522827", "url": "https://en.wikipedia.org/wiki?curid=37522827", "title": "Pipe recovery operations", "text": "Pipe recovery operations\n\nPipe recovery is a specific wireline operation used in the oil and gas industry, when the drill string becomes stuck downhole. Stuck pipe prevents the drill rig from continuing operations. This results in costly downtime, ranging anywhere from $10,000-1,000,000 per day of downtime, therefore it is critical to resolve the problem as quickly as possible. Pipe recovery is the process by which the location of the stuck pipe is identified, and the free pipe is separated from the stuck pipe. This allows fishing tools to subsequently be run down hole to latch onto and remove the stuck pipe.\n\nThe geological formation downhole occasionally has a significantly lower pressure than the drilling fluid being used. When the pipe string comes into contact with the exposed formation the difference in pressure will cause the pipe to be sucked against the formation. If the rig is able to circulate drilling fluid back to the surface that is often a good indication of differentially stuck pipe. One technique for freeing the stuck pipe, or avoiding the issue to begin with, is to rotate the pipe string while pulling out of the hole.\nKey seating occurs when the drill string becomes off-centered in the wellbore, and the pipe collars become caught on a deviation in the wellbore. If the rig is able to move the drill string freely downhole, but every time the drill string is pulled upward it becomes stuck at the same point, then it is likely that the pipe is caught in a key seat.\nAn unstable formation can result in a cave in. The collapse of the formation can pin the pipe inside the wellbore preventing its movement.\nThis can be the result of objects, i.e. slips or pipe wrenches, being dropped down the hole lodging against the BHA.\n\nIn areas with a high concentration of oil wells, it is not unusual for wells to communicate through the formation for distances of up to mile. During a fracing operation thousands of gallons of fluid and sand are pumped down one well to open up the formation surrounding that wellbore. Often large amounts of that proppant and fluid will travel through the formation and into a nearby well. This sand can lodge on top of a packer or coil tubing in the well sticking the pipe. \nTubing in production wells is often exposed to a number of highly corrosive chemicals, such as HS. This corrosion can deteriorate the tubing to the point that it separates from the wellhead causing the tubing to fall downhole. The impact of several thousands of pounds of tubing on the bottom of the hole can severely damage the tubing, causing kinks or a corkscrew effect in the tubing, making it difficult to retrieve out of the hole. Sand coming in through holes in the casing or a malfunctioning production packer can also cause tubing to become stuck in production wells.\n\nThe term free point is used to describe the delineating point between the stuck pipe and the free pipe in a pipe string. Every joint of pipe above the free point is free, meaning it can rotate freely and be moved in and out of the hole, provided it was not attached to the remaining joints of stuck pipe below the free point. A good way to visualize this is to hold a piece of string with your left hand. With your right hand grab the bottom third of that string. The 2/3rds of the string above your right hand would be considered free, since you can move the string however you may like. The section of string inside and below your right hand is stuck, since no matter what you do to the free string it will not affect the string in or below your right hand.\n\nThe pipe stretch free point estimate is a good starting point for identifying the free point.\n\nThe following tools are operated using a wireline truck.\n\n The traditional freepoint tool is an electromechanical tool designed to measure the amount of torque or stretch of a given length of tubing, drill pipe, or casing. The traditional freepoint tool uses either bow springs or magnets to anchor itself inside the pipe. After obtaining an estimate of the free point by using the pipe stretch estimate technique, the traditional freepoint tool is run in the hole to 1000 feet above the estimated stuck point. The tool is anchored in place. Stretch and or torque is then applied to the pipe. This allows the pipe recovery engineer to obtain a baseline reading of the free pipe. This will give him a starting point to compare his later freepoint readings to. The tool is then run roughly 500 feet past the estimated stuck point. Stretch and torque are applied, and readings are taken. If the tool indicates that the pipe is stuck at that point the tool is pulled uphole and readings are taken again. By applying the bracketing technique, the pipe recovery engineer is quickly able to identify the exact point that the pipe is free.\n\nThe Halliburton Freepoint Tool is based around the magnetorestrictive property of steel. This principle states that when torque or stretch is applied to free pipe, the magnetization will change. Stuck pipe will have no change in magnetization. There is a magnet on the bottom of the tool that creates a small magnetic field. There are four co-planar orthogonal multi-axis high sensitivity magnetometers located above the magnet. The magnetometers measure the change in the magnetization of the pipe. The pipe is set at neutral weight, then the tool is run downhole logging the entire pipe string. Once it is at the bottom of the string, torque or stretch is applied to the pipe. The tool is then pulled uphole logging the entire string. The tool will detect differences in the magnetization of the pipe, thereby indicating free and stuck sections of pipe.\n\nBacking off pipe, is an industry term which means unscrewing the pipe at a desired depth downhole.\n\nOnce the free point of a stuck pipe string is determined, the stringshot back-off service can be used to remove the free portion from the well. The string-shot is a metal rod wrapped in explosive primer cord. The back-off procedure applies left-hand torque to the stuck pipe string. The CCL is used to help position the string shot at the predetermined pipe joint, and then the string shot is detonated. The explosion produces a similar effect as an intense hammer blow and allows the joint to be unscrewed at the proper connection. Several factors such as pipe size, weight and condition, back-off depth, mud or borehole fluid weight and well temperature are carefully considered when making up the proper string-shot assembly.\n\nManual back offs are the least preferred technique for removing free pipe from the hole since there is very little control. When performing a manual back off you take the desired back off depth(ft.) and multiply it by the weight of one foot of pipe. This will give you a rough estimate of the neutral weight of the pipe at the desired depth. By setting the pipe at neutral weight at the desired depth you are reducing any tension on the threads at the desired depth, thereby increasing your odds of the pipe unscrewing there. The driller will pickup the string to the back off neutral weight, and then left hand torque is applied until the pipe unscrews. There is no guarantee that the pipe will unscrew at the desired point making the manual back off the last resort for backing off pipe.\n\nChemical cutters use a propellant to generate pressure forcing the chemical, usually Bromine Trifluoride, through a catalyst. The resulting chemical reaction is expelled through the severing head of the cutter at a high temperature and pressure, which cuts the wall of the tubing. The resulting cut is a very smooth cut that does not require any dressing before further pipe recovery operations can take place.\nJet cutters use a circular-shaped charge to produce the cutting action. Jet cutters are capable of severing pipe despite significant downhole pressure. This makes them an ideal choice for extremely deep wells, greater than 20,000 feet deep. They typically leave a flare on the severed pipe string. This flare must be removed, typically by using a mill, before further fishing operations can take place.\nRadial cutting torches use a mixture of powdered metals contained inside the torch body, those metals burn at a very high temperature on ignition by the gas generator. The resultant molten plasma is then ejected through the radial graphic ceramic nozzle and onto the target tubing. The result is a clean, non flared cut. The highly energized plasma is capable of overcoming nearly any wellbore condition, and has a cutting success rate of 77%. The RCT does not contain explosives; this greatly reduces transportation costs and logistical problems.\n\nThe Drill Collar Severing Tool is often used to separate heavy weight drill pipe or drill collars. The DCST contains an explosive charge at either end of the tool; both charges are detonated simultaneously. The explosive shock waves meet in the center of the tool and combine to produce a very high energy wave capable of cutting through the thickest of pipe. The severed pipe is typically split and deformed, requiring milling.\n\n"}
{"id": "3539638", "url": "https://en.wikipedia.org/wiki?curid=3539638", "title": "Plataforma Nunca Máis", "text": "Plataforma Nunca Máis\n\nNunca Máis (, ) is the name and slogan of a popular movement in Galicia (Spain) formed in response to the Prestige ship environmental disaster in 2002. The movement's banner is based on the Galician flag, but with a blue diagonal on a black field, rather than a blue diagonal on a white field. The movement describes itself as representing a broad swathe of civil society.\n\n\"Nunca Máis\" organized a number of demonstrations to ensure the official recognition of Galicia as a catastrophe zone and the immediate resource base to repair the economic, social, environmental and health problems resulting from the disaster. It also called for the set in of motion of disaster prevention systems, in order to avoid disasters like this from occurring. The demonstrations were attended by thousands on successive days.\n\nOn August 11, 2006, the \"Plataforma Nunca Máis\" announced their reactivation to fight against the wave of forest fires that had been started throughout Galicia. A number of rallies in the Galician capital, Santiago de Compostela, were organised, with large numbers attending.\n\n\"Nunca Máis\" acts as a co-ordinator of other associations, namely cultural, civic and ecologist associations. In this fashion, \"Nunca Máis\" could be considered as of being made up of a series of other associations, despite having its own directive board.\n\n\n\n"}
{"id": "25361397", "url": "https://en.wikipedia.org/wiki?curid=25361397", "title": "PopOffsets", "text": "PopOffsets\n\nPop\"Offsets\" was a web-based, carbon offsetting project of Population Matters, (formerly known as the Optimum Population Trust). In 2018 the project was superseded by Empower to Plan, a crowdfunding project for supporting family planning providers.\n\nFounded in 1991, Population Matters is a leading UK campaigning charity concerned with the effects of current world population growth on the natural environment, particularly regarding climate change. It advocates for population stabilisation through improved access to rights-based family planning schemes. Population Matters commissioned a research paper entitled \"“Fewer Emitters, Lower Emissions, Less Cost”\" which indicates that addressing unmet needs for family planning is the most cost-effective way of reducing CO emissions and climate change – possibly less than one third of the cost of other technological fixes – without any environmental downsides. They estimate that every $7 (£4) spent on family planning saves one tonne of CO. A similar reduction would require a $13 (£8) investment in tree planting, $24 (£15) in wind power, $41 (£31) in solar energy and $92 (£56) in hybrid vehicle technology.\n\nLaunched on 3 December 2009, just days before the Copenhagen Climate Summit, PopOffsets was the first and only carbon offsetting scheme worldwide investing its funds in rights-based family planning schemes, health programs, relationship and sex education. Population Matters considers that investing in family planning services is vital for environmental sustainability, with particular regards to women who have little or no access to these services. Every year there are 80 million unintended pregnancies in the world, which could be averted by making sex education and family planning services accessible to everyone. Details of PopOffsets were circulated to every British MP.\n\nThe PopOffsets web-project offered a step-by-step guide enabling contributors to make online donations to Population Matters, in order to support rights-based family planning schemes. Contributors could calculate their estimated annual amount of CO emissions in their country of residence, using a “CO calculator”. The calculator divided the total amount of the chosen country’s CO emissions by the number of its citizens. The contributor could then decide to offset his/her annual CO emissions or to opt for a “one-off” amount, by determining the exact amount of CO that he/she is wishing to offset.\n\nThe funds gathered from online donations were distributed between rights-based family planning schemes, contraceptive services and supplies, sex and relationship education and health programs. Population Matters aims to use the majority of its funds to meet the needs of over 200 million women worldwide who do not have access to family planning services.\n\nThe launch of the Pop Offset project on 3rd December 2009 received international media coverage from both print and broadcast media outlets, including in The Guardian, The Times, The Sunday Express, The Telegraph, Reuters, BBC and Channel 4.\n\nThe PopOffsets program received some criticism mostly on the grounds that populations of developing countries, although bearing high birth rates, are not responsible for creating high levels of CO. Population Matters, however, stressed that reduction of CO outputs by the wealthiest is a vital component of any strategy. Note that the replacement Empower to Plan scheme has no carbon offsetting component. \n\n\n"}
{"id": "37539281", "url": "https://en.wikipedia.org/wiki?curid=37539281", "title": "Power pool", "text": "Power pool\n\nPower pooling is used to balance electrical load over a larger network (electrical grid) than a single utility. It is a mechanism for interchange of power between two and more utilities which provide or generate electricity For exchange of power between two utilities there is an interchange agreement which is signed by them, but signing up an interchange agreement between each pair of utilities within a system can be a difficult task where several large utilities are interconnected. Thus, it is more advantageous to form a power pool with a single agreement that all join. That agreement provides established terms and conditions for pool members and is generally more complex than a bilateral agreement.\n\nIn one model, the power pool, formed by the utilities, has a control dispatch office from where the pool is administered. All the tasks regarding interchange of power and the settlement of disputes are assigned to the pool administrator.\n\nThe formation of power pools provide the following potential advantages:\n\nThe formation of a power pool is associated with a number of problems and constraints. These include:\n\nPower pooling is very important for extending energy control over a large area served by multiple utilities.\n\n\n"}
{"id": "28691929", "url": "https://en.wikipedia.org/wiki?curid=28691929", "title": "Protective relay", "text": "Protective relay\n\nIn electrical engineering, a protective relay is a relay device designed to trip a circuit breaker when a fault is detected. The first protective relays were electromagnetic devices, relying on coils operating on moving parts to provide detection of abnormal operating conditions such as over-current, over-voltage, reverse power flow, over-frequency, and under-frequency. \n\nMicroprocessor-based digital protection relays now emulate the original devices, as well as providing types of protection and supervision impractical with electromechanical relays. Electromechanical relays provide only rudimentary indication of the location and origin of a fault. In many cases a single microprocessor relay provides functions that would take two or more electromechanical devices. By combining several functions in one case, numerical relays also save capital cost and maintenance cost over electromechanical relays. However, due to their very long life span, tens of thousands of these \"silent sentinels\" are still protecting transmission lines and electrical apparatus all over the world. Important transmission lines and generators have cubicles dedicated to protection, with many individual electromechanical devices, or one or two microprocessor relays.\n\nThe theory and application of these protective devices is an important part of the education of a power engineer who specializes in power system protection. The need to act quickly to protect circuits and equipment as well as the general public often requires protective relays to respond and trip a breaker within a few thousandths of a second. In some instances these clearance times are prescribed in legislation or operating rules. A maintenance or testing program is used to determine the performance and availability of protection systems.\n\nBased on the end application and applicable legislation, various standards such as ANSI C37.90, IEC255-4, IEC60255-3, and IAC govern the response time of the relay to the fault conditions that may occur.\n\nElectromechanical protective relays operate by either magnetic attraction, or magnetic induction. Unlike switching type electromechanical relays with fixed and usually ill-defined operating voltage thresholds and operating times, protective relays have well-established, selectable, and adjustable time and current (or other operating parameter) operating characteristics. Protection relays may use arrays of induction disks, shaded-pole, magnets, operating and restraint coils, solenoid-type operators, telephone-relay contacts, and phase-shifting networks.\n\nProtective relays can also be classified by the type of measurement they make. A protective relay may respond to the magnitude of a quantity such as voltage or current. Induction relays can respond to the product of two quantities in two field coils, which could for example represent the power in a circuit.\n\n\"It is not practical to make a relay that develops a torque equal to the quotient of two a.c. quantities. This, however is not important; the only significant condition for a relay is its setting and the setting can be made to correspond to a ratio regardless of the component values over a wide range.\"\n\nSeveral operating coils can be used to provide \"bias\" to the relay, allowing the sensitivity of response in one circuit to be controlled by another. Various combinations of \"operate torque\" and \"restraint torque\" can be produced in the relay.\n\nBy use of a permanent magnet in the magnetic circuit, a relay can be made to respond to current in one direction differently from in another. Such polarized relays are used on direct-current circuits to detect, for example, reverse current into a generator. These relays can be made bistable, maintaining a contact closed with no coil current and requiring reverse current to reset. For AC circuits, the principle is extended with a polarizing winding connected to a reference voltage source.\n\nLightweight contacts make for sensitive relays that operate quickly, but small contacts can't carry or break heavy currents. Often the measuring relay will trigger auxiliary telephone-type armature relays.\n\nIn a large installation of electromechanical relays, it would be difficult to determine which device originated the signal that tripped the circuit. This information is useful to operating personnel to determine the likely cause of the fault and to prevent its re-occurrence. Relays may be fitted with a \"target\" or \"flag\" unit, which is released when the relay operates, to display a distinctive colored signal when the relay has tripped.\n\nElectromechanical relays can be classified into several different types as follows:\n\n\"Armature\"-type relays have a pivoted lever supported on a hinge or knife-edge pivot, which carries a moving contact. These relays may work on either alternating or direct current, but for alternating current, a shading coil on the pole is used to maintain contact force throughout the alternating current cycle. Because the air gap between the fixed coil and the moving armature becomes much smaller when the relay has operated, the current required to maintain the relay closed is much smaller than the current to first operate it. The \"returning ratio\" or \"differential\" is the measure of how much the current must be reduced to reset the relay.\n\nA variant application of the attraction principle is the plunger-type or solenoid operator. A reed relay is another example of the attraction principle.\n\n\"Moving coil\" meters use a loop of wire turns in a stationary magnet, similar to a galvanometer but with a contact lever instead of a pointer. These can be made with very high sensitivity. Another type of moving coil suspends the coil from two conductive ligaments, allowing very long travel of the coil.\n\n\"Induction\" disk meters work by inducing currents in a disk that is free to rotate; the rotary motion of the disk operates a contact. Induction relays require alternating current; if two or more coils are used, they must be at the same frequency otherwise no net operating force is produced. These electromagnetic relays use the induction principle discovered by Galileo Ferraris in the late 19th century. The magnetic system in induction disc overcurrent relays is designed to detect overcurrents in a power system and operate with a pre-determined time delay when certain overcurrent limits have been reached. In order to operate, the magnetic system in the relays produces torque that acts on a metal disc to make contact, according to the following basic current/torque equation:\n\nformula_1\n\nWhere formula_2 and formula_3 are the two fluxes and formula_4 is the phase angle between the fluxes\n\nThe following important conclusions can be drawn from the above equation.\n\nThe relay's primary winding is supplied from the power systems current transformer via a plug bridge, which is called the plug setting multiplier (psm). Usually seven equally spaced tappings or operating bands determine the relays sensitivity. The primary winding is located on the upper electromagnet. The secondary winding has connections on the upper electromagnet that are energised from the primary winding and connected to the lower electromagnet. Once the upper and lower electromagnets are energised they produce eddy currents that are induced onto the metal disc and flow through the flux paths. This relationship of eddy currents and fluxes creates torque proportional to the input current of the primary winding, due to the two flux paths being out of phase by 90°.\n\nIn an overcurrent condition, a value of current will be reached that overcomes the control spring pressure on the spindle and the braking magnet, causing the metal disc to rotate towards the fixed contact. This initial movement of the disc is also held off to a critical positive value of current by small slots that are often cut into the side of the disc. The time taken for rotation to make the contacts is not only dependent on current but also the spindle backstop position, known as the time multiplier (tm). The time multiplier is divided into 10 linear divisions of the full rotation time.\n\nProviding the relay is free from dirt, the metal disc and the spindle with its contact will reach the fixed contact, thus sending a signal to trip and isolate the circuit, within its designed time and current specifications. Drop off current of the relay is much lower than its operating value, and once reached the relay will be reset in a reverse motion by the pressure of the control spring governed by the braking magnet.\n\nApplication of electronic amplifiers to protective relays was described as early as 1928, using vacuum tube amplifiers and continued up to 1956. Devices using electron tubes were studied but never applied as commercial products, because of the limitations of vacuum tube amplifiers. A relatively large standby current is required to maintain the tube filament temperature; inconvenient high voltages are required for the circuits, and vacuum tube amplifiers had difficulty with incorrect operation due to noise disturbances.\n\nStatic relays have no or few moving parts, and became practical with the introduction of the transistor. Measuring elements of static relays have been successfully and economically built up from diodes, zener diodes, avalanche diodes, unijunction transistors, p-n-p and n-p-n bipolar transistors, field effect transistors or their combinations. Static relays offer the advantage of higher sensitivity than purely electromechanical relays, because power to operate output contacts is derived from a separate supply, not from the signal circuits. Static relays eliminated or reduced contact bounce, and could provide fast operation, long life and low maintenance.\n\nDigital protective relays were in their infancy during the late 1960s. An experimental digital protection system was tested in the lab and in the field in the early 1970s. Unlike the relays mentioned above, digital protective relays have two main parts: hardware and software. The world's first commercially available digital protective relay was introduced to the power industry in 1984. In spite of the developments of complex algorithms for implementing protection functions the microprocessor based-relays marketed in the 1980s did not incorporate them.\nA microprocessor-based digital protection relay can replace the functions of many discrete electromechanical instruments. These relays convert voltage and currents to digital form and process the resulting measurements using a microprocessor. The digital relay can emulate functions of many discrete electromechanical relays in one device, simplifying protection design and maintenance. Each digital relay can run self-test routines to confirm its readiness and alarm if a fault is detected. Digital relays can also provide functions such as communications (SCADA) interface, monitoring of contact inputs, metering, waveform analysis, and other useful features. Digital relays can, for example, store multiple sets of protection parameters, which allows the behavior of the relay to be changed during maintenance of attached equipment. Digital relays also can provide protection strategies impossible to implement with electromechanical relays. This is particularly so in long distance high voltage or multi-terminal circuits or in lines that are series or shunt compensated They also offer benefits in self-testing and communication to supervisory control systems.\nThe distinction between digital and numerical protection relay rests on points of fine technical detail, and is rarely found in areas other than Protection. Numerical relays are the product of the advances in technology from digital relays. Generally, there are several different types of numerical protection relays. Each type, however, shares a similar architecture, thus enabling designers to build an entire system solution that is based on a relatively small number of flexible components. They use high speed processors executing appropriate algorithms. Most numerical relays are also multifunctional and have multiple setting groups each often with tens or hundreds of settings.\n\nThe various protective functions available on a given relay are denoted by standard ANSI device numbers. For example, a relay including function 51 would be a timed overcurrent protective relay.\n\nAn overcurrent relay is a type of protective relay which operates when the load current exceeds a pickup value. The ANSI device number is 50 for an instantaneous over current (IOC) or a Definite Time Overcurrent (DTOC). In a typical application the over current relay is connected to a current transformer and calibrated to operate at or above a specific current level. When the relay operates, one or more contacts will operate and energize to trip (open) a circuit breaker. The Definite Time Overcurrent Relay has been used extensively in the United Kingdom but its inherent issue of operating slower for faults closer to the source led to the development of the IDMT relay. \n\nThe inverse definite minimum time (IDMT) protective relays were developed to overcome the shortcomings of the Definite Time Overcurrent Relays.\n\nIf the source impedance remains constant and the fault current changes appreciably as we move away from the relay then it is advantageous to use IDMT overcurrent protection to achieve high speed protection over a large section of the protected circuit. However, if the source impedance is significantly larger than the feeder impedance then the characteristic of the IDMT relay cannot be exploited and DTOC may be utilized. Secondly if the source impedance varies and becomes weaker with less generation during light loads then this leads to slower clearance time hence negating the purpose of the IDMT relay.\n\nIEC standard 60255-151 specifies the IDMT relay curves as shown below. The four curves in Table 1 are derived from the now withdrawn British Standard BS 142. The other five, in Table 2, are derived from the ANSI standard C37.112.\n\nWhile it is more common to use IDMT relays for current protection it is possible to utilize IDMT mode of operation for voltage protection. It is possible to program customised curves in some protective relays and other manufacturers have special curves specific to their relays. Some numerical relays can be used to provide inverse time overvoltage protection or negative sequence overcurrent protection.\n\nI = is the ratio of the fault current to the relay setting current or a Plug Setting Multiplier. \"Plug\" is a reference from the electromechanical relay era and were available in discrete steps. TD is the Time Dial setting.\n\nformula_5\n\nThe above equations result in a \"family\" of curves as a result of using different time multiplier setting (TMS) settings. It is evident from the relay characteristic equations that a larger TMS will result in a slower clearance time for a given PMS (I) value.\n\nDistance relays differ in principle from other forms of protection in that their performance is not governed by the magnitude of the current or voltage in the protected circuit but rather on the ratio of these two quantities. Distance relays are actually double actuating quantity relays with one coil energized by voltage and other coil by current. The current element produces a positive or pick up torque while the voltage element produces a negative or reset torque. The relay operates only when the V/I ratio falls below a predetermined value(or set value). During a fault on the transmission line the fault current increases and the voltage at the fault point decreases. The V/I \nratio is measured at the location of CTs and PTs. The voltage at the PT location depends on the distance between the PT and the fault. If the measured voltage is lesser, that means the fault is nearer and vice versa. Hence the protection called Distance relay. The load flowing through the line appears as an impedance to the relay and sufficiently large loads (as impedance is inversely proportional to the load) can lead to a trip of the relay even in the absence of a fault.\n\nA differential scheme acts on the difference between current entering a protected zone (which may be a bus bar, generator, transformer or other apparatus) and the current leaving that zone. A fault outside the zone gives the same fault current at the entry and exit of the zone, but faults within the zone show up as a difference in current. \n\n\"The differential protection is 100% selective and therefore only responds to faults within its protected zone. The boundary of the protected zone is uniquely defined by the location of the current transformers. Time grading with other protection systems is therefore not required, allowing for tripping without additional delay. Differential protection is therefore suited as fast main protection for all important plant items.\" \n\nDifferential protection can be used to provide protection for zones with multiple terminals and can be used to protect lines, generators, motors, transformers, and other electrical plant.\n\nCurrent transformers in a differential scheme must be chosen to have near-identical response to high overcurrents. If a \"through fault\" results in one set of current transformers saturating before another, the zone differential protection will see a false \"operate\" current and may false trip. \n\nGFCI (ground fault circuit interrupter) circuit breakers combine overcurrent protection and differential protection (non-adjustable) in standard, commonly available modules.\n\nA directional relay uses an additional polarizing source of voltage or current to determine the direction of a fault. Directional elements respond to the phase shift between a polarizing quantity and an operate quantity. The fault can be located upstream or downstream of the relay's location, allowing appropriate protective devices to be operated inside or outside of the zone of protection.\n\nA synchronism checking relay provides a contact closure when the frequency and phase of two sources are similar to within some tolerance margin. A \"synch check\" relay is often applied where two power systems are interconnected, such as at a switchyard connecting two power grids, or at a generator circuit breaker to ensure the generator is synchronized to the system before connecting it.\n\nThe relays can also be classified on the type of power source that they use to work.\n\n"}
{"id": "22297769", "url": "https://en.wikipedia.org/wiki?curid=22297769", "title": "Renewable Energy Certificates Registry", "text": "Renewable Energy Certificates Registry\n\nThe Renewable Energy Certificates Registry (REC-registry) is an internet-based registry system established by the Australian Renewable Energy (Electricity) Act 2000 (the Act), and maintained by the Clean Energy Regulator.\n\nThe REC-registry:\n\nPrior to 1 January 2011, the primary mechanism in the renewable energy target (RET) was the renewable energy certificate (REC). From 1 January 2011 RECs were split into small-scale technology certificates (STCs) and large-scale generation certificates (LGCs). RECs are still used as a general term covering both STCs and LGCs. \n\nAll certificates must be created in the REC-registry before they can be bought, sold, traded or surrendered. Participants must hold an account and be registered as a REC-registry user to create, view or transfer certificates. \n\n"}
{"id": "1762389", "url": "https://en.wikipedia.org/wiki?curid=1762389", "title": "Royalex", "text": "Royalex\n\nRoyalex is a composite material, comprising an outer layer of vinyl and hard acrylonitrile butadiene styrene plastic (ABS) and an inner layer of ABS foam. The layers are bonded by heat treatment. It is used for manufacture of durable, mid-priced canoes.\n\nRoyalex is basically a board comprising a 'sandwich' of standard rigid ABS sheets in the core of which is an ABS layer designed to foam/expand during the heating stage prior to vacuum forming.\n\nThus Royalex can be designed to form virtually any finished thickness to suit the end application. One just adds extra sheets.\n\nSince ABS has a poor resistance to UV a thin outer layer of UV resistant vinyl is generally included. This can be self-coloured or painted.\n\nABS is a copolymer of rubber and thermoplastic, Acrylonitrile, butadiene, styrene. It was developed at US Rubber Co's Chicago plant by Bob Pooley and was simply called Royalite. US Rubber was later renamed Uniroyal - a blanket name to encapsulate their worldwide enterprises, which previously had separate names.\n\nLater Bob Pooley and associates continued work on Royalite to introduce a foaming/expandable element. This was accomplished at the Uniroyal plant in Warsaw, Indiana. The resulting product was named Royalex to recognise the expanding core.\n\nAlthough extremely versatile as well as being very tough, the material was expensive compared to available alternatives. Even so, many industries were attracted to evaluate Royalex. Items produced included Caravans (vacuum formed in two halves) the top was a mirror image of the base so that only one forming tool was required. White Motors introduced a limited run of tractor cabs. A company was formed to produce a repro of the famous 'Coffin nosed' Cord Deusenberg.\nGene Bordinat, Chief Designer at Ford designed a concept car in Royalex in 1965. This resulted in the 'Ford XP Bordinat Cobra'.\n\nThe writer brought both of these cars to London, England to show at the International Plastics Exhibition and for exposure to the then extensive automotive producers.\n\nRoyalex was extremely tough but if hit hard enough could be dented. If this happened such dents could be recovered by applying a hand held hot air gun which re-expanded the core to blow out the dent.\n\nSadly the cost, as well as the need to accost a different build technique when using Royalex, mitigated against its acceptance in large scale production and eventually simple canoe bodies became the main product. An additional reason was that following the breakup of the Uniroyal empire, product specialists were lost and there was no longer a team to drive the concept.\n\nRoyalex was developed by Uniroyal in the 1970s.\n\nIn 2000 the Spartech Corporation took over the Uniroyal Royalex Manufacturing Division and secured the rights to manufacture Royalex at their factory in Warsaw, Indiana.\n\nIn 2013, plastics company PolyOne, of Avon Lake, Ohio purchased Spartech, and decided to shut down Royalex production due to its low volume. The last sheets of Royalex were shipped from the factory in December, 2013. Production will be shut down in April 2014.\n\nIf another manufacturer does not emerge, existing Royalex canoes in retail outlets may be sold out by 2015. Equivalent materials for canoemakers do not exist as of January 2014.\n\nT-Formex, an equivalent to Royalex, has been developed and is manufactured by the canoe manufacturer Esquif. Canoes in the material are available from Esquif and several other manufacturers, including Swift and Wenonah.\n\nThe best known use of Royalex is for the manufacture of canoe hulls. Royalex is lighter, more resistant to UV damage from sunlight, is more rigid, and has greater structural memory than non-composite plastics used for this purpose, such as polyethylene. It is also quieter, faster, and more user friendly in cold or hot conditions than aluminium, described by canoeist Kent Ford described as \"noisy, heavy and hot.\" Royalex canoes are, however, more expensive than aluminium canoes or canoes made from traditionally molded or roto-molded polyethylene hulls. \"Royalex was soft, quiet and slippery on rocks, and not too heavy if you kept the gunwales light,” according to Kent Ford.\n\nRoyalex is heavier than fiber-reinforced composites, such as fiberglass, Kevlar, or graphite, and therefore less suited for high-performance paddling. However, Royalex is cheaper than Kevlar or carbon fiber, with better durability on rocky rivers. Royalex canoes are lighter than polyethylene canoes.\n\n"}
{"id": "55050801", "url": "https://en.wikipedia.org/wiki?curid=55050801", "title": "Ruwais Refinery", "text": "Ruwais Refinery\n\nThe Ruwais Refinery, located in Al Ruwais (Abu Dhabi), and owned by Takreer, the refining arm of ADNOC, is the largest oil refinery in the UAE with a capacity of 400,000 bpd. This is more than twice the size of ADNOC's other Jebel Ali refinery. The Ruwais refinery was initially commissioned in 1981, with a capacity of 120,000 bpd then expanded.\n"}
{"id": "36699703", "url": "https://en.wikipedia.org/wiki?curid=36699703", "title": "Shah wa Arus Dam", "text": "Shah wa Arus Dam\n\nThe Shah wa Arus Dam is a 75m high concrete gravity dam under construction on the Shakardara River, located 22 km away from Kabul City, Afghanistan. The dam with a 60m wide three-bay spillway is mainly geared towards provision of irrigation water as well as flood control and reliable year-round water supply to the city of Kabul as well as the district of Shakardara. The bidding process deadline was on 24 March 2014 to build the Shah-wa-Arus irrigation and hydropower project, which includes a rockfill dam, intake, powerhouse, pickup weir with balancing reservoir and head regulators, and right and left side main canals. Tablieh and Parhoon Tarh JV Company was chosen as the main contractor for the project in a Design-Build contract in which the design services are provided by Alborz Sazeh Company.\n\nPresident Ghani accompanied by ministers of water and energy and economy minister, national Security Council advisor , Kabul Governor and members of the National Assembly opened the construction work on the dam on 19 June 2015. The Dam will cost $41 million, with 81 meters height at the rocky base and about 330 000 cubic meters of concrete, would be the first Roller-compacted concrete (R.C.C) that will be implemented within 55 months. The dam will produce 1.2 Megawatts of power and irrigate 2700 hectares of land in addition to supplying drinking water for the residents of Kabul city. The project is an important scheme that will create jobs for 500 jobless Afghans during its completion process. The Project is located near the village of Nazim Khel on Shakardara River North West of Kabul Province.\n\nThe Shah-wa-Arus Multipurpose Dam Project has the following major components:\n\n\n"}
{"id": "22679533", "url": "https://en.wikipedia.org/wiki?curid=22679533", "title": "Sigma Gamma Epsilon", "text": "Sigma Gamma Epsilon\n\nThe Society of Sigma Gamma Epsilon (ΣΓΕ) is a national honor society to recognize scholarship in the earth sciences founded in 1915 at the University of Kansas. It has chartered more than 200 chapters at colleges and universities across the United States.\n\nThe Society was established to recognize scholarship and professionalism in the Earth Sciences. It has for its objectives the scholastic, scientific, and professional advancement of its members and the extension of relations of friendship and assistance among colleges and universities which are devoted to the advancement of the Earth Sciences.\n\nThe Society was founded on March 30, 1915, at The University of Kansas. Nearly 200 chapters throughout the United States have been installed since 1915. Government of the Society is by student members and the ultimate legislative authority is vested in a National Convention held every two years. It is composed of one student delegate from each chapter and the seven national officers who are faculty members.\n\n"}
{"id": "3883844", "url": "https://en.wikipedia.org/wiki?curid=3883844", "title": "Solid wood", "text": "Solid wood\n\nSolid wood is a term most commonly used to distinguish between ordinary lumber and engineered wood, but it also refers to structures that do not have hollow spaces. Engineered wood products are manufactured by binding together wood strands, fibers, or veneers with adhesives to form a composite material. Engineered wood includes plywood, oriented strand board (OSB) and fiberboard. The fact that a product is made from solid wood is often touted in advertisements. However, using solid wood has advantages and disadvantages. \n\nPerhaps the greatest advantage of solid wood is that the wood is the same all the way through, so repairs are relatively easy. Repairs to veneer are much more difficult and sometimes impossible. \n\nSolid wood furniture is strong enough to easily satisfy all furniture applications, and it can last for centuries. Society is still questioning whether furniture made of plywood (often made from pine) can do the same. Plywood and other engineered wood products used to make furniture are typically covered with a veneer such as Cherry. A definition of solid wood may be seen at :http://www.definitions.net/definition/solid%20wood\n\nIt is common today for furniture manufacturers and retailers to advertise such veneered plywood furniture as made of \"wood solids with cherry veneers\". Most customers believe that to mean solid planks of less expensive woods such as poplar, etc., with expensive woods such as cherry used for veneers. However, \"wood solids\" is a term of art. The \"wood solids\" are simply plywood, or another engineered wood product.\n\nIn the USA, the Federal Trade Commission doesn't allow furniture to be advertised as made of \"solid wood\" unless all exposed surfaces are in fact solid wood. Solid wood is expensive. Engineered wood (often advertised as wood solids) is not.\n\nOne of the most frequently made hollow wood structures are hollow core doors. Hollow core doors are much lighter than solid wood doors, cheaper and are easier to install. However, sound travels more freely through them, which can be a problem if the house is noisy or the occupants desire a lot of privacy. Also, hollow core doors should not be used as doors to the outside because they can more easily be broken open by robbers. Solid wood doors are slightly more fire resistant because the fire has to burn through more material, however, using a steel door will increase fire resistance by a much larger margin.\n"}
{"id": "27624318", "url": "https://en.wikipedia.org/wiki?curid=27624318", "title": "Stone of the Pregnant Woman", "text": "Stone of the Pregnant Woman\n\nThe Stone of the Pregnant Woman () or Stone of the South is a Roman monolith in Baalbek (ancient Heliopolis), Lebanon. Together with another ancient stone block nearby, it is among the largest monoliths ever quarried. The two building blocks were presumably intended for the nearby Roman temple complex, possibly as an addition to the so-called trilithon, and are characterised by a monolithic gigantism that was unparallelled in antiquity.\n\nThere are multiple stories behind the name. One says the monolith is named after a pregnant woman who tricked the people of Baalbek into believing that she knew how to move the giant stone if only they would feed her until she gave birth. Others say the name comes from the legends that pregnant jinn were assigned the task of cutting and moving the stone, while others say that the name reflects the belief that a woman who touches the stone experiences an increase in fertility.\n\nThe stone block still lies in the ancient quarry at a distance of 900 m from the Heliopolis temple complex. In 1996, a geodetic team of the Austrian city of Linz conducted topographical measurements at the site which aimed at establishing the exact dimensions of the two monoliths and their possible use in the construction of the gigantic Jupiter temple. According to their calculations, the block weighs 1,000.12 t, thus practically confirming older estimations such as that of Jean-Pierre Adam.\n\nThe rectangular stone block is:\n\nA second ancient monolith was discovered in the same quarry in the 1990s. With its weight estimated at 1242 t, it surpasses even the dimension of the \"Stone of the Pregnant Woman\".\n\nThese are dimensions of the rectangular stone block, assuming that its shape is consistent in its still-buried parts:\n\nA third ancient monolith was discovered in the same quarry in 2014 by the German Archaeological Institute. Its weight is estimated at around 1650 t, making it the largest stone ever carved by human hands.\n\nIt measures:\n\n\n"}
{"id": "44537900", "url": "https://en.wikipedia.org/wiki?curid=44537900", "title": "Water associated fraction", "text": "Water associated fraction\n\nThe water associated fraction (WAF), sometimes termed the water-soluble fraction (W.S.F.), is the solution of low molecular mass hydrocarbons naturally released from petroleum hydrocarbon mixtures in contact with water. Although generally regarded as hydrophobic, many petroleum hydrocarbons are soluble in water to a limited extent. This combination often also contains less soluble, higher molecular mass components, and more soluble products of chemical and biological degradation.\n\nLow molecular mass compounds account for much of the toxic nature of hydrocarbon spills. In particular, benzene, toluene, ethyl benzene and the xylenes (BTEX) are of great environmental interest due to their availability to organisms. This availability, also influenced by volatility and reactivity, impacts on biodegradation and bioremediation in water and soil environments, with even dissolved components within pore water considered bioavailable.\n\nThe WAF is found in greatest concentration in close proximity to the bulk phase of hydrocarbons, the progress of which is often limited by physical containment measures such as booms. The dissolved components of petroleum mixtures such as crude oil can become subject to the transport mechanisms of the bulk aqueous phase. Source identification of these can therefore become problematic without the visual indications usually expected with petroleum hydrocarbon spills. However, after relatively short periods of exposure, the chemical profile of the original oils is still largely intact, allowing chemical analysis to provide identification and discriminate between different petroleum sources.\n\nIn freshwater aquatic environments, dissolution is the greatest physical weathering process after evaporation. Under the same conditions, the rate of dissolution is between 0.01% and 1% of the rate of evaporation for alkanes and aromatic compounds. Once dissolved, these components are more available to organisms and therefore susceptible to biodegradation processes and experience increased rates of photochemical and chemical degradation. These components represent some of the most toxic oil ingredients because of their increased bioavailability, with reduction in toxicity occurring on emulsification or absorption to colloids which restrict availability to organisms.\n"}
{"id": "38109766", "url": "https://en.wikipedia.org/wiki?curid=38109766", "title": "Water cycle management", "text": "Water cycle management\n\nWater cycle management relates to all planning, strategy development, operational and tactical decisions to optimize the water cycle to satisfy human or environmental objectives. It is used in different organizations to mean management of a sub-set of the whole water cycle. In Australia, it has been adopted to encompass the fresh water to sewage disposal cycle. Elsewhere, it refers to a more holistic view of the water cycle.\n\nMany scientific and engineering disciplines may be involved in water cycle management:\n"}
