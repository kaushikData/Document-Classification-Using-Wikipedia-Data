{"id": "7878791", "url": "https://en.wikipedia.org/wiki?curid=7878791", "title": "Argutite", "text": "Argutite\n\nArgutite (GeO) is a rare germanium oxide mineral. It is a member of the rutile group.\n\nIt was first described for an occurrence in the Argut deposit, central Pyrenees, Haute-Garonne, France in 1983. The type locality is within a zinc ore deposit within lower Paleozoic sedimentary rocks that have undergone metamorphism. Associated minerals include sphalerite, cassiterite, siderite and briartite.\n\n"}
{"id": "1970718", "url": "https://en.wikipedia.org/wiki?curid=1970718", "title": "Chemotropism", "text": "Chemotropism\n\nThe growth of a plant part due to chemical stimulus is known as chemotropism\n\"'Chemotropism\" is growth of organisms such as bacteria and plants, navigated by chemical stimulus from outside of the organism or organism's part. \nThe response of the organism or organism part is termed ‘positive’ if the growth is towards the stimulus, or ‘negative’ if the growth is away from the stimulus.\n\nAn example of chemo-tropic movement can be seen during the growth of the pollen tube, where growth is always towards the ovules. It can be also written that conversion of flower into fruit is an example of chemotropism.\n\nFertilization of flowers by pollen is achieved because the ovary releases chemicals that produce a positive chemo-tropic response from the developing pollen tube.\n\nAn example of positive and negative chemotropism is shown by a plant’s roots; the roots grow towards useful minerals displaying positive chemotropism, and grow away from harmful acids displaying negative chemotropism.\n\nAnother example of chemotropic movement includes the growth of individual neuronal cell axons in response to extracellular signals. These signals guide the developing axon to innervate the correct target tissue. The neuronal growth cones are guided by gradients of chemoattractant molecules emanating from their intermediate or final targets. There is evidence that the axons of peripheral neurons are guided by chemotropism and the directed growth of some central axons is also a chemo-tropic response, it remains to be determined whether chemotropism also operates in the central nervous system. Evidence of chemotropism has also been noted in neuronal regeneration, where chemotropic substances guide the ganglionic neurites towards the degenerated neuronal stump.\n\nThe addition of atmospheric nitrogen, also called nitrogen fixation, is an example of chemotropism.\n\nChemotropism is different from Chemotaxis, the major difference being that chemotropism is related to growth, while chemotaxis is related to locomotion.\n\n"}
{"id": "2649013", "url": "https://en.wikipedia.org/wiki?curid=2649013", "title": "DIII-D (tokamak)", "text": "DIII-D (tokamak)\n\nDIII-D is a tokamak that has been operated since the late 1980s by General Atomics (GA) in San Diego, USA, for the U.S. Department of Energy. The DIII-D National Fusion Facility is part of the ongoing effort to achieve magnetically confined fusion. The mission of the DIII-D Research Program is to establish the scientific basis for the optimization of the tokamak approach to fusion energy production.\n\nDIII-D was built on the basis of the earlier Doublet III, the third in a series of machines built at GA to experiment with tokamaks having non-circular plasma cross sections. This work demonstrated that certain shapes strongly suppressed a variety of instabilities in the plasma, which led to much higher plasma pressure and performance. DIII-D is so-named because the plasma is shaped like the letter D, a shaping that is now widely used on modern designs, and has led to the class of machines known as \"advanced tokamaks.\" Advanced tokamaks are characterized by operation at high plasma β through strong plasma shaping, active control of various plasma instabilities, and achievement of steady-state current and pressure profiles that produce high energy confinement for high fusion gain (ratio of fusion power to heating power).\n\nDIII-D is one of two large magnetic fusion experiments in the U.S. (the other being NSTX-U at PPPL) supported by the U.S. Department of Energy Office of Science. The program is focusing on R&D for pursuing steady-state advanced tokamak operation and supporting design and operation of the ITER experiment now under construction in France. ITER is designed to demonstrate a self-sustained burning plasma that will produce 10 times as much energy from fusion reactions as it requires for heating.\n\nThe DIII-D research program is a large international collaboration, with over 600 users participating from more than 100 institutions. General Atomics operates the San Diego-based facility for the United States Department of Energy through the Office of Fusion Energy Sciences.\n\nResearch in DIII-D aims to elucidate the basic physics processes that govern the behavior of a hot magnetized plasma, and to establish a scientific basis for future burning plasma devices such as ITER. Ultimately, the goal is to use this understanding to develop an economically attractive a fusion power plant.\n\nThe tokamak consists of a toroidal vacuum chamber surrounded by magnetic field coils which contain and shape the plasma. The plasma is created by applying a voltage to generate a large electrical current (more than one million amperes) in the chamber. The plasma is heated to temperatures ten times hotter than that of the sun by a combination of high-power neutral beams and microwaves. The plasma conditions are measured using instrumentation based on intense lasers, microwaves, and other precision plasma diagnostics.\n\nIn May 1974, AEC selected General Atomics to build the Doublet III magnetic fusion experiment based on the success of earlier Doublet I and II magnetic confinement experiments. In Feb 1978, the Doublet III fusion experiment achieved its first operation with plasma at General Atomics. The machine was later upgraded and renamed DIII-D in 1986. \nThe DIII-D program achieved several milestones in fusion development, including the highest plasma β (ratio of plasma pressure to magnetic pressure) ever achieved at the time (early 1980s) and the highest neutron flux (fusion rate) ever achieved at the time (early 1990s).\n\n\n"}
{"id": "389950", "url": "https://en.wikipedia.org/wiki?curid=389950", "title": "Diesel locomotive", "text": "Diesel locomotive\n\nA diesel locomotive is a type of railway locomotive in which the prime mover is a diesel engine. Several types of diesel locomotive have been developed, differing mainly in the means by which mechanical power is conveyed to the driving wheels.\n\nEarly internal combusition locomotives and railcars used kerosene and gasoline as their fuel. Dr. Rudolf Diesel patented his first compression ignition engine in 1898, and steady improvements in the design of diesel engines reduced their physical size and improved their power-to-weight ratio to a point where one could be mounted in a locomotive. Internal combustion engines only operate efficiently within a limited torque range, and while low power gasoline engines can be coupled to a mechanical transmission, the more powerful diesel engines required the development of new forms of transmission. \n\nThe first successful diesel engines used diesel–electric transmissions, and by 1925 a small number of diesel locomotives of were in service in the United States. In 1930, Armstrong Whitworth of the United Kingdom delivered two locomotives using Sulzer-designed engines to Buenos Aires Great Southern Railway of Argentina. In 1933, diesel-electric technology developed by Maybach was used propel the DRG Class SVT 877, a high speed intercity two-car set, and went into series production with other streamlined car sets in Germany starting in 1935. In the USA, diesel-electric propulsion was brought to high speed mainline passenger service in late 1934, largely through the research and development efforts of General Motors from 1930–34 and advances in lightweight carbody design by the Budd Company.\n\nThe economic recovery from the Second World War saw the widespread adoption of diesel locomotives in many countries. They offered greater flexibility and performance than steam locomotives, as well as substantially lower operating and maintenance costs. Diesel–hydraulic transmissions were intrdouced in the 1950s, but from the 1970s onwards diesel–electric transmission has dominated.\n\nThe earliest recorded example of the use of an internal combustion engine in a railway locomotive is the prototype designed by William Dent Priestman, which was examined by Sir William Thomson in 1888 who described it as a \"[Priestman oil engine] mounted upon a truck which is worked on a temporary line of rails to show the adaptation of a petroleum engine for locomotive purposes.\". In 1894, a two axle machine built by Priestman Brothers was used on the Hull Docks. In 1896 an oil-engined railway locomotive was built for the Royal Arsenal, Woolwich, England, in 1896, using an engine designed by Herbert Akroyd Stuart. It was not, strictly, a diesel because it used a hot bulb engine (also known as a semi-diesel) but it was the precursor of the diesel.\n\nFollowing the expiration of Dr. Rudolf Diesel's patent in 1912, his engine design was successfully applied to marine propulsion and stationary applications. However, the massiveness and poor power-to-weight ratio of these early engines made them unsuitable for propelling land-based vehicles. Therefore, the engine's potential as a railroad prime mover was not initially recognized. This changed as development reduced the size and weight of the engine.\n\nIn 1906, Rudolf Diesel, Adolf Klose and the steam and diesel engine manufacturer Gebrüder Sulzer founded Diesel-Sulzer-Klose GmbH to manufacture diesel-powered locomotives. Sulzer had been manufacturing Diesel engines since 1898. The Prussian State Railways ordered a diesel locomotive from the company in 1909, and after test runs between Winterthur and Romanshorn the diesel–mechanical locomotive was delivered in Berlin in September 1912. The world's first diesel-powered locomotive was operated in the summer of 1912 on the Winterthur–Romanshorn railroad in Switzerland, but was not a commercial success. During further test runs in 1913 several problems were found. After the First World War broke out in 1914, all further trials were stopped. The locomotive weight was 95 tonnes and the power was 883 kW with a maximum speed of 100 km/h. Small numbers of prototype diesel locomotives were produced in a number of countries through the mid-1920s.\n\nAdolphus Busch purchased the American manufacturing rights for the diesel engine in 1898 but never applied this new form of power to transportation. He founded the Busch-Sulzer company in 1911.\nOnly limited success was achieved in the early twentieth century with internal combustion engined railcars, owing in part to difficulties with mechanical drive systems.\n\nGeneral Electric (GE) entered the railcar market in the early twentieth century, as Thomas Edison possessed a patent on the electric locomotive, his design actually being a type of electrically propelled railcar. GE built its first electric locomotive prototype in 1895. However, high electrification costs caused GE to turn its attention to internal combustion power to provide electricity for electric railcars. Problems related to co-coordinating the prime mover and electric motor were immediately encountered, primarily due to limitations of the Ward Leonard current control system that had been chosen.\n\nA significant breakthrough occurred in 1914, when Hermann Lemp, a GE electrical engineer, developed and patented a reliable direct current electrical control system (subsequent improvements were also patented by Lemp). Lemp's design used a single lever to control both engine and generator in a coordinated fashion, and was the prototype for all internal combustion–electric drive control systems.\n\nIn 1917–18, GE produced three experimental diesel–electric locomotives using Lemp's control design, the first known to be built in the United States. Following this development, the 1923 Kaufman Act banned steam locomotives from New York City because of severe pollution problems. The response to this law was to electrify high-traffic rail lines. However, electrification was uneconomical to apply to lower-traffic areas.\n\nThe first regular use of diesel–electric locomotives was in switching (shunter) applications, which were more forgiving than mainline applications of the limitations of contemporary diesel technology and where the idling economy of diesel relative to steam would be most beneficial. GE entered a collaboration with the American Locomotive Company (ALCO) and Ingersoll-Rand (the \"AGEIR\" consortium) in 1924 to produce a prototype \"boxcab\" locomotive delivered in July 1925. This locomotive demonstrated that the diesel–electric power unit could provide many of the benefits of an electric locomotive without the railroad having to bear the sizeable expense of electrification. The unit successfully demonstrated, in switching and local freight and passenger service, on ten railroads and three industrial lines. Westinghouse Electric and Baldwin collaborated to build switching locomotives starting in 1929. However, the Great Depression curtailed demand for Westinghouse's electrical equipment, and they stopped building locomotives internally, opting to supply electrical parts instead.\n\nIn June 1925, Baldwin Locomotive Works outshopped a prototype diesel–electric locomotive for \"special uses\" (such as for runs where water for steam locomotives was scarce) using electrical equipment from Westinghouse Electric Company. Its twin-engine design was not successful, and the unit was scrapped after a short testing and demonstration period. Industry sources were beginning to suggest “the outstanding advantages of this new form of motive power”. In 1929, the Canadian National Railways became the first North American railway to use diesels in mainline service with two units, 9000 and 9001, from Westinghouse. However, these early diesels proved expensive and unreliable, with their high cost of acquisition relative to steam unable to be realized in operating cost savings as they were frequently out of service. It would be another five years before diesel-electric propulsion would be successfully used in mainline service, and nearly ten years before it would show real potential to fully replace steam. \n\nBefore diesel power could make inroads into mainline service, the limitations of diesel engines circa 1930 - low power-to-weight ratios and narrow output range - had to be overcome. A major effort to overcome those limitations was launched by General Motors after they moved into the diesel field with their acquisition of the Winton Engine Company, a major manufacturer of diesel engines for marine and stationary applications, in 1930. Supported by the General Motors Research Division, GM's Winton Engine Corporation sought to develop diesel engines suitable for high speed mobile use. The first milestone in that effort was delivery in early 1934 of the Winton 201A, a two-stroke, Roots-blown, uniflow-scavenged, unit-injected diesel engine that could deliver the required performance for a fast, lightweight passenger train. The second milestone, and the one that got American railroads moving towards diesel, was the 1938 delivery of GM's Model 567 engine that was designed specifically for locomotive use, bringing a fivefold increase in life of some mechanical parts and showing its potential for meeting the rigors of freight service.\n\nDiesel–electric railroad locomotion entered mainline service when the Burlington Railroad and Union Pacific used custom-built diesel \"streamliners\" to haul passengers, starting in late 1934.Burlington's \"Zephyr\" trainsets evolved from articulated three car sets with 600 hp power cars in 1934 and early 1935, to the \"Denver Zephyr\" semi-articulated ten car trainsets pulled by cab-booster power sets introduced in late 1936. Union Pacific started diesel streamliner service between Chicago and Portland Oregon in June 1935, and in the following year would add Los Angeles and Oakland California, and Denver Colorado to the destinations of diesel streamliners out of Chicago. The Burlington and Union Pacific streamliners were built by the Budd Company and the Pullman-Standard Company, respectively, using the new Winton engines and power train systems designed by GM's Electro-Motive Corporation. EMC's experimental 1800 hp B-B locomotives of 1935 demonstrated the multiple-unit control systems used for the cab/booster sets and the twin engine format used with the later \"Zephyr\" power units. Both of those features would be used in EMC's later production model locomotives. The lightweight diesel streamliners of the mid-1930s demonstrated the advantages of diesel for passenger service with breakthrough schedule times, but diesel locomotive power would not fully come of age until regular series production of mainline diesel locomotives commenced and it was shown suitable for full-size passenger and freight service.\n\nFollowing their 1925 prototype, he AGEIR consortium produced 25 more units of \"60 ton\" AGEIR boxcab switching locomotives between 1925 and 1928 for several New York City railroads, making them the first series-produced diesel locomotives. The consortium also produced seven twin-engine \"100 ton\" boxcabs and one hybrid trolley/battery unit with a diesel-driven charging circuit. ALCO acquired the McIntosh & Seymour Engine Company in 1929 and entered series production of and single-cab switcher units in 1931. ALCO would be the pre-eminent builder of switch engines through the mid-1930s and would adapt the basic switcher design to produce versatile and highly successful, albeit relatively low powered, road locomotives. \n\nGM, seeing the success of the custom streamliners, sought to expand the market for diesel power by producing standardized locomotives under their Electro-Motive Corporation. In 1936 EMC's new factory started production of switch engines. In 1937 the factory started producing their new E series streamlined passenger locomotives, which would be upgraded with more reliable purpose-built engines in 1938. Seeing the performance and reliability of the new 567 model engine in passenger locomotives, EMC was eager to demonstrate diesel's viability in freight service.\n\nFollowing the successful 1939 tour of EMC's FT demonstrator freight locomotive set, the stage was set for dieselization of American railroads. In 1941 ALCO-GE introduced the RS-1 road-switcher that occupied its own market niche while EMD's F series locomotives were sought for mainline freight service. The US entry into World War II slowed conversion to diesel; the War Production Board put a halt to building new passenger equipment and gave naval uses priority for diesel engine production. During the petroleum crisis of 1942-43, coal-fired steam had the advantage of not using fuel that was in critically short supply. EMD was later allowed to increase production of its FT locomotives and ALCO-GE was allowed to produce a limited number of DL-109 road locomotives, but most in the locomotive business were restricted to making switch engines and steam locomotives. \n\nIn the early postwar era, EMD dominated the market for mainline locomotives with their E and F series locomotives. ALCO-GE in the late 1940s produced switchers and road-switchers that were successful in the short-haul market. However, EMD launched their GP series road-switcher locomotives in 1949, which displaced all other locomotives in the freight market including their own F series locomotives. GE subsequently dissolved its partnership with ALCO and would emerge as EMD's main competitor in the early 1960s, eventually taking the top position in the locomotive market from EMD. \n\nEarly diesel–electric locomotives in the United States used direct current (DC) traction motors, but alternating current (AC) motors came into widespread use in the 1990s, starting with the Electro-Motive SD70MAC in 1993 and followed by the General Electric's AC4400CW in 1994 and AC6000CW in 1995.\n\nIn 1914, world's first functional diesel–electric railcars were produced for the \"Königlich-Sächsische Staatseisenbahnen\" (Royal Saxon State Railways) by Waggonfabrik Rastatt with electric equipment from Brown, Boveri & Cie and diesel engines from Swiss Sulzer AG. They were classified as DET 1 and DET 2 (). Due to shortage of petrol products during World War I, they remained unused for regular service in Germany. In 1922, they were sold to Swiss Compagnie du Chemin de fer Régional du Val-de-Travers (), where they were used in regular service up to the electrification of the line in 1944. Afterwards, the company kept them in service as boosters till 1965.\n\nFiat claims a first Italian diesel–electric locomotive built in 1922, but little detail is available. A Fiat-TIBB diesel–locomotive \"A\", of 440CV, is reported to have entered service on the Ferrovie Calabro Lucane in southern Italy in 1926, following trials in 1924-5.\n\nIn 1924, two diesel–electric locomotives were taken in service by the Soviet railways, almost at one time:\n\n\nIn 1935, Krauss-Maffei, MAN and Voith built the first diesel–hydraulic locomotive, called V 140, in Germany. The German railways (DRG) being very pleased with the performance of that engine, diesel–hydraulics became the mainstream in diesel locomotives in Germany. Serial production of diesel locomotives in Germany began after World War II.\n\nIn many railway stations and industrial compounds, steam shunters had to be kept hot during lots of breaks between scattered short tasks. Therefore, diesel traction became economical for shunting before it became economical for hauling trains. The construction of diesel shunters began in 1920 in France, in 1925 in Denmark, in 1926 in the Netherlands, and in 1927 in Germany. After few years of testing, hundreds of units were produced within a decade.\n\nDiesel-powered or \"oil-engined\" railcars, generally diesel–mechanical, were developed by various European manufacturers in the 1930s, e.g. by William Beardmore and Company for the Canadian National Railways (the Beardmore Tornado engine was subsequently used in the R101 airship). Some of those series for regional traffic were begun with gasoline motors and then continued with diesel motors, such as Hungarian BC (The class code doesn't tell anything but \"railmotor with 2nd and 3rd class seats\".), 128 cars built 1926 – 1937, or German Wismar railbuses (57 cars 1932 – 1941). In France, the first diesel railcar was Renault VH, 115 units produced 1933/34.\nIn Italy, after 6 Gasoline cars since 1931, Fiat and Breda built a lot of diesel railmotors, more than 110 from 1933 to 1938 and 390 from 1940 to 1953, Class 772 known as \"Littorina\", and Class ALn 900.\n\nIn the 1930s, streamlined highspeed diesel railcars were developed in several countries:\n\nIn 1945, a batch of 30 Baldwin diesel–electric locomotives, Baldwin 0-6-6-0 1000, was delivered from the United States to the railways of the Soviet Union.\n\nIn 1947, the London Midland & Scottish Railway introduced the first of a pair of Co-Co diesel–electric locomotives (later British Rail Class D16/1) for regular use in the United Kingdom, although British manufacturers such as Armstrong Whitworth had been exporting diesel locomotives since 1930. Fleet deliveries to British Railways, of other designs such as Class 20 and Class 31, began in 1957.\n\nSeries production of diesel locomotives in Italy began in the mid-1950s. Generally, diesel traction in Italy was of less importance than in other countries, as it was amongst the most advanced countries in electrification of the main lines and, as a result of Italian geography, even on many domestic connections freight transport over sea is cheaper than rail transport.\n\nIn Japan, starting in the 1920s, some petrol-electric railcars were produced. The first diesel–electric traction and the first air-streamed vehicles on Japanese rails were the two DMU3s of class Kiha 43000 (キハ43000系) \nJapan's first series of diesel locomotives was class DD50 (国鉄DD50形デ), twin locomotives, developed since 1950 and in service since 1953.\n\nOne of the first home developed diesel vehicles of China was the DMU Dongfeng (东风), produced in 1958 by CSR Sifang.\nSeries production of China's first diesel locomotive class, the DFH 1, began in 1964 following construction of a prototype in 1959.\n\nThe Trans-Australian Railway built 1912 to 1917 by Commonwealth Railways (CR) passes through 2000 km of waterless (or salt watered) desert terrain unsuitable for steam locomotives. The original engineer Henry Deane envisaged diesel operation to overcome such problems. Some have suggested that the CR worked with the South Australian Railways to trial diesel traction. However, the technology was not developed enough to be reliable.\n\nAs in Europe, the usage of internal combustion engines advanced more readily in self-propelled railcars than in locomotives.\n\nUnlike steam engines, internal combustion engines require a transmission to power the wheels. The engine must be allowed to continue to run when the locomotive is stopped.\n\nA diesel–mechanical locomotive uses a mechanical transmission in a fashion similar to that employed in most road vehicles. This type of transmission is generally limited to low-powered, low speed shunting (switching) locomotives, lightweight multiple units and self-propelled railcars.\nThe mechanical transmissions used for railroad propulsion are generally more complex and much more robust than standard-road versions. There is usually a fluid coupling interposed between the engine and gearbox, and the gearbox is often of the epicyclic (planetary) type to permit shifting while under load. Various systems have been devised to minimise the break in transmission during gear changing; e.g., the S.S.S. (synchro-self-shifting) gearbox used by Hudswell Clarke.\n\nDiesel–mechanical propulsion is limited by the difficulty of building a reasonably sized transmission capable of coping with the power and torque required to move a heavy train. A number of attempts to use diesel–mechanical propulsion in high power applications have been made (e.g., the British Rail 10100 locomotive), although none have proved successful in the end.\n\nIn a diesel–electric locomotive, the diesel engine drives either an electrical DC generator (generally, less than net for traction), or an electrical AC alternator-rectifier (generally net or more for traction), the output of which provides power to the traction motors that drive the locomotive. There is no mechanical connection between the diesel engine and the wheels.\n\nThe important components of diesel–electric propulsion are the diesel engine (also known as the prime mover), the main generator/alternator-rectifier, traction motors (usually with four or six axles), and a control system consisting of the engine governor and electrical or electronic components, including switchgear, rectifiers and other components, which control or modify the electrical supply to the traction motors. In the most elementary case, the generator may be directly connected to the motors with only very simple switchgear.\n\nOriginally, the traction motors and generator were DC machines. Following the development of high-capacity silicon rectifiers in the 1960s, the DC generator was replaced by an alternator using a diode bridge to convert its output to DC. This advance greatly improved locomotive reliability and decreased generator maintenance costs by elimination of the commutator and brushes in the generator. Elimination of the brushes and commutator, in turn, disposed of the possibility of a particularly destructive type of event referred to as a flashover, which could result in immediate generator failure and, in some cases, start an engine room fire.\n\nCurrent North American practice is for four axles for high-speed passenger or \"time\" freight, or for six axles for lower-speed or \"manifest\" freight. The most modern units on \"time\" freight service tend to have six axles underneath the frame. Unlike those in \"manifest\" service, \"time\" freight units will have only four of the axles connected to traction motors, with the other two as idler axles for weight distribution.\n\nIn the late 1980s, the development of high-power variable-voltage/variable-frequency (VVVF) drives, or \"traction inverters,\" has allowed the use of polyphase AC traction motors, thus also eliminating the motor commutator and brushes. The result is a more efficient and reliable drive that requires relatively little maintenance and is better able to cope with overload conditions that often destroyed the older types of motors.\n\nA diesel–electric locomotive's power output is independent of road speed, as long as the unit's generator current and voltage limits are not exceeded. Therefore, the unit's ability to develop tractive effort (also referred to as drawbar pull or tractive force, which is what actually propels the train) will tend to inversely vary with speed within these limits. (See power curve below). Maintaining acceptable operating parameters was one of the principal design considerations that had to be solved in early diesel–electric locomotive development and, ultimately, led to the complex control systems in place on modern units.\n\nThe prime mover's power output is primarily determined by its rotational speed (RPM) and fuel rate, which are regulated by a governor or similar mechanism. The governor is designed to react to both the throttle setting, as determined by the engine driver and the speed at which the prime mover is running.\n\nLocomotive power output, and thus speed, is typically controlled by the engine driver using a stepped or \"notched\" throttle that produces binary-like electrical signals corresponding to throttle position. This basic design lends itself well to multiple unit (MU) operation by producing discrete conditions that assure that all units in a consist respond in the same way to throttle position. Binary encoding also helps to minimize the number of trainlines (electrical connections) that are required to pass signals from unit to unit. For example, only four trainlines are required to encode all possible throttle positions.\n\nNorth American locomotives, such as those built by EMD or General Electric, have nine throttle positions, one idle and eight power (as well as an emergency stop position that shuts down the prime mover). Many UK-built locomotives have a ten-position throttle. The power positions are often referred to by locomotive crews as \"run 3\" or \"notch 3\", depending upon the throttle setting.\n\nIn older locomotives, the throttle mechanism was ratcheted so that it was not possible to advance more than one power position at a time. The engine driver could not, for example, pull the throttle from notch 2 to notch 4 without stopping at notch 3. This feature was intended to prevent rough train handling due to abrupt power increases caused by rapid throttle motion (\"throttle stripping,\" an operating rules violation on many railroads). Modern locomotives no longer have this restriction, as their control systems are able to smoothly modulate power and avoid sudden changes in train loading regardless of how the engine driver operates the controls.\n\nWhen the throttle is in the idle position, the prime mover will be receiving minimal fuel, causing it to idle at low RPM. In addition, the traction motors will not be connected to the main generator and the generator's field windings will not be excited (energized) — the generator will not produce electricity with no excitation. Therefore, the locomotive will be in \"neutral\". Conceptually, this is the same as placing an automobile's transmission into neutral while the engine is running.\n\nTo set the locomotive in motion, the reverser control handle is placed into the correct position (forward or reverse), the brake is released and the throttle is moved to the run 1 position (the first power notch). An experienced engine driver can accomplish these steps in a coordinated fashion that will result in a nearly imperceptible start. The positioning of the reverser and movement of the throttle together is conceptually like shifting an automobile's automatic transmission into gear while the engine is idling\n\nPlacing the throttle into the first power position will cause the traction motors to be connected to the main generator and the latter's field coils to be excited. With excitation applied, the main generator will deliver electricity to the traction motors, resulting in motion. If the locomotive is running \"light\" (that is, not coupled to the rest of a train) and is not on an ascending grade, it will easily accelerate. On the other hand, if a long train is being started, the locomotive may stall as soon as some of the slack has been taken up, as the drag imposed by the train will exceed the tractive force being developed. An experienced engine driver will be able to recognize an incipient stall and will gradually advance the throttle as required to maintain the pace of acceleration.\n\nAs the throttle is moved to higher power notches, the fuel rate to the prime mover will increase, resulting in a corresponding increase in RPM and horsepower output. At the same time, main generator field excitation will be proportionally increased to absorb the higher power. This will translate into increased electrical output to the traction motors, with a corresponding increase in tractive force. Eventually, depending on the requirements of the train's schedule, the engine driver will have moved the throttle to the position of maximum power and will maintain it there until the train has accelerated to the desired speed.\n\nAs will be seen in the following discussion, the propulsion system is designed to produce maximum traction motor torque at start-up, which explains why modern locomotives are capable of starting trains weighing in excess of 15,000 tons, even on ascending grades. Current technology allows a locomotive to develop as much as 30 percent of its loaded driver weight in tractive force, amounting to some of drawbar pull for a large, six-axle freight (goods) unit. In fact, a consist of such units can produce more than enough drawbar pull at start-up to damage or derail cars (if on a curve) or break couplers (the latter being referred to in North American railroad slang as \"jerking a lung\"). Therefore, it is incumbent upon the engine driver to carefully monitor the amount of power being applied at start-up to avoid damage. In particular, \"jerking a lung\" could be a calamitous matter if it were to occur on an ascending grade, except that the safety inherent in the correct operation of automatic train brakes installed in wagons today, prevents runaway trains by automatically applying the wagon brakes when train line air pressure drops.\n\nA locomotive's control system is designed so that the main generator electrical power output is matched to any given engine speed. Given the innate characteristics of traction motors, as well as the way in which the motors are connected to the main generator, the generator will produce high current and low voltage at low locomotive speeds, gradually changing to low current and high voltage as the locomotive accelerates. Therefore, the net power produced by the locomotive will remain constant for any given throttle setting (\"see power curve graph for notch 8\").\n\nIn older designs, the prime mover's governor and a companion device, the load regulator, play a central role in the control system. The governor has two external inputs: requested engine speed, determined by the engine driver's throttle setting, and actual engine speed (feedback). The governor has two external control outputs: fuel injector setting, which determines the engine fuel rate, and load regulator position, which affects main generator excitation. The governor also incorporates a separate overspeed protective mechanism that will immediately cut off the fuel supply to the injectors and sound an alarm in the cab in the event the prime mover exceeds a defined RPM. Not all of these inputs and outputs are necessarily electrical.\n\nThe load regulator is essentially a large potentiometer that controls the main generator power output by varying its field excitation and hence the degree of loading applied to the engine. The load regulator's job is relatively complex, because although the prime mover's power output is proportional to RPM and fuel rate, the main generator's output is not (which characteristic was not correctly handled by the Ward Leonard elevator- and hoist-type drive system that was initially tried in early locomotives). Instead, a quite complex electro-hydraulic Woodward governor was employed. Today, this important function would be performed by the Engine control unit, itself being a part of the Locomotive control unit.\n\nAs the load on the engine changes, its rotational speed will also change. This is detected by the governor through a change in the engine speed feedback signal. The net effect is to adjust both the fuel rate and the load regulator position so that engine RPM and torque (and thus power output) will remain constant for any given throttle setting, regardless of actual road speed.\n\nIn newer designs controlled by a “traction computer,” each engine speed step is allotted an appropriate power output, or “kW reference”, in software. The computer compares this value with actual main generator power output, or “kW feedback”, calculated from traction motor current and main generator voltage feedback values. The computer adjusts the feedback value to match the reference value by controlling the excitation of the main generator, as described above. The governor still has control of engine speed, but the load regulator no longer plays a central role in this type of control system. However, the load regulator is retained as a “back-up” in case of engine overload. Modern locomotives fitted with electronic fuel injection (EFI) may have no mechanical governor; however a “virtual” load regulator and governor are retained with computer modules.\n\nTraction motor performance is controlled either by varying the DC voltage output of the main generator, for DC motors, or by varying the frequency and voltage output of the VVVF for AC motors. With DC motors, various connection combinations are utilized to adapt the drive to varying operating conditions.\n\nAt standstill, main generator output is initially low voltage/high current, often in excess of 1000 amperes per motor at full power. When the locomotive is at or near standstill, current flow will be limited only by the DC resistance of the motor windings and interconnecting circuitry, as well as the capacity of the main generator itself. Torque in a series-wound motor is approximately proportional to the square of the current. Hence, the traction motors will produce their highest torque, causing the locomotive to develop maximum tractive effort, enabling it to overcome the inertia of the train. This effect is analogous to what happens in an automobile automatic transmission at start-up, where it is in first gear and thus producing maximum torque multiplication.\n\nAs the locomotive accelerates, the now-rotating motor armatures will start to generate a counter-electromotive force (back EMF, meaning the motors are also trying to act as generators), which will oppose the output of the main generator and cause traction motor current to decrease. Main generator voltage will correspondingly increase in an attempt to maintain motor power, but will eventually reach a plateau. At this point, the locomotive will essentially cease to accelerate, unless on a downgrade. Since this plateau will usually be reached at a speed substantially less than the maximum that may be desired, something must be done to change the drive characteristics to allow continued acceleration. This change is referred to as \"transition,\" a process that is analogous to shifting gears in an automobile.\n\nTransition methods include:\n\nBoth methods may also be combined, to increase the operating speed range.\n\nIn older locomotives, it was necessary for the engine driver to manually execute transition by use of a separate control. As an aid to performing transition at the right time, the load meter (an indicator that shows the engine driver how much current is being drawn by the traction motors) was calibrated to indicate at which points forward or backward transition should take place. Automatic transition was subsequently developed to produce better operating efficiency and to protect the main generator and traction motors from overloading from improper transition.\n\nModern locomotives incorporate traction \"inverters\", AC to DC, with the capability to deliver 1,200 volts (earlier traction \"generators\", DC to DC, had the capability to deliver only 600 volts). This improvement was accomplished largely through improvements in silicon diode technology. With the capability to deliver 1,200 volts to the traction motors, the necessity for \"transition\" was eliminated.\n\nA common option on diesel–electric locomotives is dynamic (rheostatic) braking.\n\nDynamic braking takes advantage of the fact that the traction motor armatures are always rotating when the locomotive is in motion and that a motor can be made to act as a generator by separately exciting the field winding. When dynamic braking is utilized, the traction control circuits are configured as follows:\n\nThe aggregate effect of the above is to cause each traction motor to generate electric power and dissipate it as heat in the dynamic braking grid. A fan connected across the grid provides forced-air cooling. Consequently, the fan is powered by the output of the traction motors and will tend to run faster and produce more airflow as more energy is applied to the grid.\n\nUltimately, the source of the energy dissipated in the dynamic braking grid is the motion of the locomotive as imparted to the traction motor armatures. Therefore, the traction motors impose drag and the locomotive acts as a brake. As speed decreases, the braking effect decays and usually becomes ineffective below approximately 16 km/h (10 mph), depending on the gear ratio between the traction motors and axles.\n\nDynamic braking is particularly beneficial when operating in mountainous regions; where there is always the danger of a runaway due to overheated friction brakes during descent. In such cases, dynamic brakes are usually applied in conjunction with the air brakes, the combined effect being referred to as blended braking. The use of blended braking can also assist in keeping the slack in a long train stretched as it crests a grade, helping to prevent a \"run-in\", an abrupt bunching of train slack that can cause a derailment. Blended braking is also commonly used with commuter trains to reduce wear and tear on the mechanical brakes that is a natural result of the numerous stops such trains typically make during a run.\n\nThese special locomotives can operate as an electric locomotive or as a diesel locomotive. The Long Island Rail Road, Metro-North Railroad and New Jersey Transit Rail Operations operate dual-mode diesel–electric/third-rail (catenary on NJTransit) locomotives between non-electrified territory and New York City because of a local law banning diesel-powered locomotives in Manhattan tunnels. For the same reason, Amtrak operates a fleet of dual-mode locomotives in the New York area. British Rail operated dual diesel–electric/electric locomotives designed to run primarily as electric locomotives with reduced power available when running on diesel power. This allowed railway yards to remain un-electrified, as the third rail power system is extremely hazardous in a yard area.\n\nDiesel–hydraulic locomotives use one or more torque converters, in combination with gears, with a mechanical final drive to convey the power from the diesel engine to the wheels.\n\nHydraulic drive systems using a hydrostatic hydraulic drive system have been applied to rail use. Modern examples included shunting locomotives by CMI Group (Belgium), 4 to 12 tonne narrow gauge industrial locomotives by Atlas Copco subsidiary GIA. Hydrostatic drives are also utilised in railway maintenance machines (tampers, rail grinders).\n\nApplication of hydrostatic transmissions is generally limited to small shunting locomotives and rail maintenance equipment, as well as being used for non-tractive applications in diesel engines such as drives for traction motor fans.\n\nHydrokinetic transmission (also called hydrodynamic transmission) uses a torque converter. A torque converter consists of three main parts, two of which rotate, and one (the stator) that has a lock preventing backwards rotation and adding output torque by redirecting the oil flow at low output RPM. All three main parts are sealed in an oil-filled housing. To match engine speed to load speed over the entire speed range of a locomotive some additional method is required to give sufficient range. One method is to follow the torque converter with a mechanical gearbox which switches ratios automatically, similar to an automatic transmission on a car. Another method is to provide several torque converters each with a range of variability covering part of the total required; all the torque converters are mechanically connected all the time, and the appropriate one for the speed range required is selected by filling it with oil and draining the others. The filling and draining is carried out with the transmission under load, and results in very smooth range changes with no break in the transmitted power.\n\nDiesel–hydraulic locomotives are less efficient than diesel–electrics. The first-generation BR diesel hydraulics were significantly less efficient (c. 65%) than diesel electrics (c. 80%) — moreover initial versions were found in many countries to be mechanically more complicated and more likely to break down. Hydraulic transmission for locomotives was developed in Germany. There is still debate over the relative merits of hydraulic vs. electrical transmission systems: advantages claimed for hydraulic systems include lower weight, high reliability, and lower capital cost.\n\nBy the 21st century, for diesel locomotive traction worldwide the majority of countries used diesel–electric designs, with diesel hydraulic designs not found in use outside Germany and Japan, and some neighbouring states, where it is used in designs for freight work.\n\nIn Germany and Finland, diesel–hydraulic systems have achieved high reliability in operation. In the UK the diesel–hydraulic principle gained a poor reputation due to the poor durability and reliability of the Maybach Mekydro hydraulic transmission. Argument continues over the relative reliability of hydraulic systems, with questions over whether data has been manipulated to favour local suppliers over non-German ones.\n\nDiesel–hydraulic drive is common in multiple units, with various transmission designs used including Voith torque converters, and fluid couplings in combination with mechanical gearing.\n\nThe majority of British Rail's second generation passenger DMU stock used hydraulic transmission. In the 21st century designs using hydraulic transmission include Bombardier Turbostar, Talent, RegioSwinger families; diesel engined versions of Siemens Desiro platform, and the Stadler Regio-Shuttle.\n\nDiesel–hydraulic locomotives have a smaller market share than those with diesel electric transmission - the main worldwide user of main-line hydraulic transmissions was the Federal Republic of Germany, with designs including the 1950s DB class V 200, and the 1960 and 1970s DB Class V 160 family. British Rail introduced a number of diesel hydraulic designs during it 1955 Modernisation Plan, initially license built versions of German designs (see ). In Spain RENFE used high power to weight ratio twin engined German designs to haul high speed trains from the 1960s to 1990s. (see RENFE Classes 340, 350, 352, 353, 354)\n\nOther main-line locomotives of the post war period included the 1950s GMD GMDH-1 experimental locomotives; the Henschel & Son built South African Class 61-000; in the 1960s Southern Pacific bought 18 Krauss-Maffei KM ML-4000 diesel–hydraulic locomotives. The Denver & Rio Grande Western also bought three, all of which were later sold to SP.\n\nIn Finland, over 200 Finnish-built VR class Dv12 and Dr14 diesel–hydraulics with Voith transmissions have been continuously used since the early 1960s. All units of Dr14 class and most units of Dv12 class are still in service. VR has abandoned some weak-conditioned units of 2700 series Dv12s.\n\nIn the 21st century series production standard gauge diesel–hydraulic designs include the Voith Gravita, ordered by Deutsche Bahn, and the Vossloh G2000, G1206 and G1700 designs, all manufactured in Germany for freight use.\n\nSteam-diesel hybrid locomotives can use steam generated from a boiler or diesel to power a piston engine. The \"Cristiani Compressed Steam System\" used a diesel engine to power a compressor to drive and recirculate steam produced by a boiler; effectively using steam as the power transmission medium, with the diesel engine being the prime mover\n\nThe diesel-pneumatic locomotive was of interest in the 1930s because it offered the possibility of converting existing steam locomotives to diesel operation. The frame and cylinders of the steam locomotive would be retained and the boiler would be replaced by a diesel engine driving an air compressor. The problem was low thermal efficiency because of the large amount of energy wasted as heat in the air compressor. Attempts were made to compensate for this by using the diesel exhaust to re-heat the compressed air but these had limited success. A German proposal of 1929 did result in a prototype but a similar British proposal of 1932, to use an LNER Class R1 locomotive, never got beyond the design stage.\n\nMost diesel locomotives are capable of multiple unit operation (MU) as a means of increasing horsepower and tractive effort when hauling heavy trains. All North American locomotives, including export models, use a standardized AAR electrical control system interconnected by a 27-pin jumper cable between the units. For UK-built locomotives, a number of incompatible control systems are used, but the most common is the Blue Star system, which is electro-pneumatic and fitted to most early diesel classes. A small number of types, typically higher-powered locomotives intended for passenger only work, do not have multiple control systems. In all cases, the electrical control connections made common to all units in a consist are referred to as trainlines. The result is that all locomotives in a consist behave as one in response to the engine driver's control movements.\n\nThe ability to couple diesel–electric locomotives in an MU fashion was first introduced in the EMD FT four-unit demonstrator that toured the United States in 1939. At the time, American railroad work rules required that each operating locomotive in a train had to have on board a full crew. EMD circumvented that requirement by coupling the individual units of the demonstrator with drawbars instead of conventional knuckle couplers and declaring the combination to be a single locomotive. Electrical interconnections were made so one engine driver could operate the entire consist from the head-end unit. Later on, work rules were amended and the semi-permanent coupling of units with drawbars was eliminated in favour of couplers, as servicing had proved to be somewhat cumbersome owing to the total length of the consist (about 200 feet or nearly 61 meters).\n\nIn mountainous regions, it is common to interpose helper locomotives in the middle of the train, both to provide the extra power needed to ascend a grade and to limit the amount of stress applied to the draft gear of the car coupled to the head-end power. The helper units in such distributed power configurations are controlled from the lead unit's cab through coded radio signals. Although this is technically not an MU configuration, the behaviour is the same as with physically interconnected units.\n\nCab arrangements vary by builder and operator. Practice in the U.S. has traditionally been for a cab at one end of the locomotive with limited visibility if the locomotive is not operated cab forward. This is not usually a problem as U.S. locomotives are usually operated in pairs, or threes, and arranged so that a cab is at each end of each set. European practice is usually for a cab at each end of the locomotive as trains are usually light enough to operate with one locomotive. Early U.S. practice was to add power units without cabs (booster or B units) and the arrangement was often A-B, A-A, A-B-A, A-B-B, or A-B-B-A where A was a unit with a cab. Center cabs were sometimes used for switch locomotives.\n\nIn North American railroading, a cow-calf set is a pair of switcher-type locomotives: one (the cow) equipped with a driving cab, the other (the calf) without a cab, and controlled from the cow through cables. Cow-calf sets are used in heavy switching and hump yard service. Some are radio controlled without an operating engineer present in the cab. This arrangement is also known as master-slave. Where two connected units were present, EMD called these TR-2s (approximately ); where three units, TR-3s (approximately ).\n\nCow-calves have largely disappeared as these engine combinations exceeded their economic lifetimes many years ago.\n\nPresent North American practice is to pair two GP40-2 or SD40-2 road switchers, often nearly worn-out and very soon ready for rebuilding or scrapping, and to utilize these for so-called \"transfer\" uses, for which the TR-2, TR-3 and TR-4 engines were originally intended, hence the designation TR, for \"transfer\".\n\nOccasionally, the second unit may have its prime-mover and traction alternator removed and replaced by concrete or steel ballast and the power for traction obtained from the master unit. As a 16-cylinder prime-mover generally weighs in the range, and a traction alternator generally weighs in the range, this would mean that would be needed for ballast.\n\nA pair of fully capable \"Dash 2\" units would be rated . A \"Dash 2\" pair where only one had a prime-mover/alternator would be rated , with all power provided by master, but the combination benefits from the tractive effort provided by the slave as engines in transfer service are seldom called upon to provide much less on a continuous basis.\n\nA standard diesel locomotive presents a very low fire risk but “flame proofing” can reduce the risk even further. This involves fitting a water-filled box to the exhaust pipe to quench any red-hot carbon particles that may be emitted. Other precautions may include a fully insulated electrical system (neither side earthed to the frame) and all electric wiring enclosed in conduit.\n\nThe flameproof diesel locomotive has replaced the fireless steam locomotive in areas of high fire risk such as oil refineries and ammunition dumps. Preserved examples of flameproof diesel locomotives include:\nLatest development of the \"Flameproof Diesel Vehicle Applied New Exhaust Gas Dry Type Treatment System” does not need the water supply.\n\nThe lights fitted to diesel locomotives vary from country to country. North American locomotives are fitted with two headlights (for safety in case one malfunctions) and a pair of ditch lights. The latter are fitted low down at the front and are designed to make the locomotive easily visible as it approaches a grade crossing. Older locomotives may be fitted with a Gyralite or Mars Light instead of the ditch lights.\n\nAlthough diesel locomotives generally emit less sulphur dioxide, a major pollutant to the environment, and greenhouse gases than steam locomotives, they are not completely clean in that respect. Furthermore, like other diesel powered vehicles, they emit nitrogen oxides and fine particles, which are a risk to public health. In fact, in this last respect diesel locomotives may perform worse than steam locomotives.\n\nFor years, it was thought by American government scientists who measure air pollution that diesel locomotive engines were relatively clean and emitted far less health-threatening emissions than those of diesel trucks or other vehicles; however, the scientists discovered that because they used faulty estimates of the amount of fuel consumed by diesel locomotives, they grossly understated the amount of pollution generated annually (In Europe, where most major railways have been electrified, there is less concern). After revising their calculations, they concluded that the annual emissions of nitrogen oxide, a major ingredient in smog and acid rain, and soot would be by 2030 nearly twice what they originally assumed.\n\nThis would mean that diesel locomotives would be releasing more than 800,000 tons of nitrogen oxide and 25,000 tons of soot every year within a quarter of a century, in contrast to the EPA's previous projections of 480,000 tons of nitrogen dioxide and 12,000 tons of soot. Since this was discovered, to reduce the effects of the diesel locomotive on humans (who are breathing the noxious emissions) and on plants and animals, it is considered practical to install traps in the diesel engines to reduce pollution levels and other forms (e.g., use of biodiesel).\n\nDiesel locomotive pollution has been of particular concern in the city of Chicago. The \"Chicago Tribune\" reported levels of diesel soot inside locomotives leaving Chicago at levels hundreds of times above what is normally found on streets outside. Residents of several neighborhoods are most likely exposed to diesel emissions at levels several times higher than the national average for urban areas.\n\nIn 2008, the United States Environmental Protection Agency (EPA) mandated regulations requiring all new or refurbished diesel locomotives to meet Tier II pollution standards that slash the amount of allowable soot by 90% and require an 80% reduction in nitrogen oxide emissions. \"See\" List of low emissions locomotives.\n\nOther technologies that are being deployed to reduce locomotive emissions and fuel consumption include \"Genset\" switching locomotives and hybrid Green Goat designs. Genset locomotives use multiple smaller high-speed diesel engines and generators (generator sets), rather than a single medium-speed diesel engine and a single generator. Because of the cost of developing clean engines, these smaller high-speed engines are based on already developed truck engines. Green Goats are a type of hybrid switching locomotive utilizing a small diesel engine and a large bank of rechargeable batteries. Switching locomotives are of particular concern as they typically operate in a limited area, often in or near urban centers, and spend much of their time idling. Both designs reduce pollution below EPA Tier II standards and cut or eliminate emissions during idle.\n\nAs diesel locomotives advanced, the cost of manufacturing and operating them dropped, and they became cheaper to own and operate than steam locomotives. In North America, steam locomotives were custom-made for specific railway routes, so economies of scale were difficult to achieve. Though more complex to produce with exacting manufacturing tolerances ( for diesel, compared with for steam), diesel locomotive parts were easier to mass produce. Baldwin Locomotive Works offered almost five hundred steam models in its heyday, while EMD offered fewer than ten diesel varieties.. In the United Kingdom, British Railways built steam locomotives to standard designs from 1951 onwards. These included standard, interchangeable parts, and were cheaper to produce than the diesel locomotives then available. The capital cost per drawbar horse power was £13 6s (steam), £65 (diesel), £69 7s (turbine) and £17 13s (electric).\n\nDiesel locomotives offer significant operating advantages over steam locomotives. They can safely be operated by one person, making them ideal for switching/shunting duties in yards (although for safety reasons many main-line diesel locomotives continue to have 2-man crews: an engineer and a conductor/switchman) and the operating environment is much more attractive, being quieter, fully weatherproof and without the dirt and heat that is an inevitable part of operating a steam locomotive. Diesel locomotives can be worked in multiple with a single crew controlling multiple locomotives in a single train — something not practical with steam locomotives. This brought greater efficiencies to the operator, as individual locomotives could be relatively low-powered for use as a single unit on light duties but marshaled together to provide the power needed on a heavy train. With steam traction a single very powerful and expensive locomotive was required for the heaviest trains or the operator resorted to double heading with multiple locomotives and crews, a method which was also expensive and brought with it its own operating difficulties.\n\nDiesel engines can be started and stopped almost instantly, meaning that a diesel locomotive has the potential to incur no fuel costs when not being used. However, it is still the practice of large North American railroads to use straight water as a coolant in diesel engines instead of coolants that incorporate anti-freezing properties; this results in diesel locomotives being left idling when parked in cold climates instead of being completely shut down. A diesel engine can be left idling unattended for hours or even days, especially since practically every diesel engine used in locomotives has systems that automatically shut the engine down if problems such as a loss of oil pressure or coolant loss occur. IAutomatic start/stop systems are available which monitor coolant and engine temperatures. When he unit is close to having its coolant freeze, the system restarts the diesel engine to warm the coolant and other systems.\n\nSteam locomotives require intensive maintenance, lubrication, and cleaning before, during, and after use. Preparing and firing a steam locomotive for use from cold can take many hours. They can be kept in readiness between uses with a low fire, but this requires regular stoking and frequent attention to maintain the level of water in the boiler. This may be necessary to prevent the water in the boiler freezing in cold climates, so long as the water supply itself is not frozen.\n\nThe maintenance and operational costs of steam locomotives were much higher than diesels. Annual maintenance costs for steam locomotives accounted for 25% of the initial purchase price. Spare parts were cast from wooden masters for specific locomotives. The sheer number of unique steam locomotives meant that there was no feasible way for spare-part inventories to be maintained. With diesel locomotives spare parts could be mass-produced and held in stock ready for use and many parts and sub-assemblies could be standardized across an operator's fleet using different models of locomotive from the same builder. Modern diesel locomotives are designed to allow the power assemblies the be replaced, which greatly reduces the time that a locomotive is out of revenue-generating service when it requires maintenance.\n\nSteam engines required large quantities of coal and water, which were expensive variable operating costs. Further, the thermal efficiency of steam was considerably less than that of diesel engines. Diesel's theoretical studies demonstrated potential thermal efficiencies for a compression ignition engine of 36% (compared with 6–10% for steam), and an 1897 one-cylinder prototype operated at a remarkable 26% efficiency.\n\nHowever, one study published in 1959 suggested that many of the comparisons between diesel and steam locomotives were made unfairly mostly because diesels were newer. After painstaking analysis of financial records and technological progress, the author found that if research had continued on steam technology instead of diesel, there would be negligible financial benefit in converting to diesel locomotion.\n\nBy the mid-1960s, diesel locomotives had effectively replaced steam locomotives where electric traction was not in use. Attempts to develop advanced steam technology continue in the 21st century but have not made a significant impact.\n\n\n"}
{"id": "31873472", "url": "https://en.wikipedia.org/wiki?curid=31873472", "title": "Domsjö Fabriker", "text": "Domsjö Fabriker\n\nDomsjö Fabriker is a Swedish refinery located in Örnsköldsvik that converts raw forest materials into specialty cellulose, lignin, and bio-ethanol. In 2000 it was spun off from the forest company Mo och Domsjö AB (MoDo), of which it had been a part since the early 20th century, and sold to a private consortium. On April 18, 2011, it was announced that India's Aditya Birla Group had acquired the company for US$340 million.\n\n\n"}
{"id": "14035173", "url": "https://en.wikipedia.org/wiki?curid=14035173", "title": "Energi 1", "text": "Energi 1\n\nEnergi 1 is a power company that serves Røyken, Ski, Enebakk and Nesodden in Norway. It provides the power grid in the municipality, with a total of 34,000 customers, as well as selling electricity through the subsidiary Energi 1 Kraftsalg Follo AS. Formerly the company was owned by the municipalities it serves.\n"}
{"id": "55074325", "url": "https://en.wikipedia.org/wiki?curid=55074325", "title": "Energy policy of Ecuador", "text": "Energy policy of Ecuador\n\nEnergy policy in Ecuador is driven by its need for energy security as a developing country as well as its conservation efforts. Despite past and ongoing attempts to take charge in energy sustainability (as with the now defunct Yasuni-ITT initiative), oil production and exportation still supports its small $5,853 GDP/capita economy at an average of 549,000 barrels/day in 2016.\n\nThe push and pull between energy independence/nationalism and appeasement of conservationist groups (representing the concerns of environmentalists and indigenous groups) has been evident in the country’s shifting stance on renewable energies and fossil fuels.\n\nCurrently, the state is in charge of all domestic activities regarding the refining and distribution of oil and oil products. The state-owned company, Petroecuador, oversees and executes all related operations. The country is also seeing the construction of a new heavy crude refinery compound, operated by Refinery of the Pacific Eloy Alfaro but funded and built by Pertroecuador. Despite Ecuador’s large oil production, its main source of electricity is hydropower -for the year 2015, 13,096 GWh of electricity came from hydropower facilities as opposed to oil’s 8,919 GWh. The Ecuadorian government has also passed legislation incentivizing the growth of renewable energy markets. ; one example of such policy is the feed-in tariff, which is a contract guaranteeing agents investing in renewable technology a competitive return on investment.\n\nIn 2016, oil made up 30% of Ecuador's total exports at a value $5.05 billion; it ranked as the 55th largest oil exporter in the world. Despite having three small refineries in the Esmeraldas, Shushufundi, and La Libertad sites operated by Petroecuador, the country lacks the ability to refine heavy crude. As such, Ecuador is a net importer of refined fuels, buying mainly from the United States. \n\nThe Refinery of the Pacific Eloy Alfaro (RDP), a petrochemical company with a compound currently under development, would allow Ecuador to refine up to 300,000 barrels/day once fully operational. The company is owned by Petroecuador, Petroleos de Venezuela SA, and the China National Petroleum Corporation. The project is estimated to cost $12 billion and return $3 billion/year that would otherwise be spent on refined fuels. It is being constructed in El Aromo, in the coastal province of Manabi. Completion of the refinery would take place in two phases, the first yielding a processing capacity of 200,000 barrels/day and the second adding another 100,000 barrels/day. \n\nThe necessary access roads and central camp were completed in 2014. In 2016, the company announced the completion of the La Esperanza Multipurpose Aqueduct, a prerequisite project that would supply the refinery with water necessary for its processes. Despite a current investment sum of $1.23 billion from Petroecuador, the venture has seen financing setback. On August 21, 2017, the minister of hydrocarbons, Carlos Pérez García, announced that the government would not contribute any more funds towards the project and instead turn towards foreign investment. The project has become a point of contention with civilians, advocacy groups, and nationalist political groups due to its heavy reliance on Chinese capital and its contradiction of \"sumak kawsay\", \"good living\" in Quechua. Sumak kawsay describes the state's harmony with its different ethnic cultures and natural environments. \n\nEcuador’s 20th constitution (established in 2008) mentions multiple forms of sovereignty, including energy sovereignty. Self-sufficiency is one of the pillars of energy sovereignty as discussed by former President Rafael Correa’s administration. The administration called for the following objectives:\n\n1.) an increase in the capacity for extracting natural resources\n\n2.) a decrease in the imports of processed fuels\n\n3.) a shift in electricity generation towards hydropower facilities. \n\nAnother pillar of energy sovereignty previously discussed by the administration is the defense of natural resources against international energy companies.\n\nThe change in energy imports does not reflect the original proposal. Since 2008, Ecuador has more than doubled its imports of gasoline/diesel (1.494M tonnes/year to 3.175M tonnes/year) while less than halving its imports of fuel oil (531K tonnes/year to 286K tonnes/year). The contribution of hydropower to electricity, however, does meet the originally proposed plan. It has risen since 2008 (back then at 11,294 GWh), while the electricity generated from oil has decreased (from 5,554 GWh). Ecuador has also increased oil production and exportation. In July 2017, the country announced it would keep increasing exports, publicly breaking rank with OPEC. The organization had promised to lower overall barrel production as global oil prices plummeted. \n"}
{"id": "36771515", "url": "https://en.wikipedia.org/wiki?curid=36771515", "title": "Fluorescence intermittency in colloidal nanocrystals", "text": "Fluorescence intermittency in colloidal nanocrystals\n\nBlinking colloidal nanocrystals is a phenomenon observed during studies of single colloidal nanocrystals that show that they randomly turn their photoluminescence on and off even under continuous light illumination.\nThis has also been described as luminescence intermittency.\nSimilar behavior has been observed in crystals made of other materials. For example, porous silicon also exhibits this affect.\n\nColloidal nanocrystals are a new class of optical materials that essentially constitute a new form of matter that can be considered as \"artificial atoms.\" Like atoms, they have discrete optical energy spectra that are tunable over a wide range of wavelengths. The desired behavior and transmission directly correlates to their size. To change the emitted wavelength, the crystal is grown larger or smaller. Their electronic and optical properties can be controlled by this method. For example, to change the emission from one visible wavelength to another simply use a larger or smaller grown crystal. However, this process would not be effective in conventional semiconductors such as gallium arsenide.\n\nThe nanocrystal size controls a widely tunable absorption band resulting in widely tunable emission spectra. This tunability combined with the optical stability of nanocrystals and the great chemical flexibility in the nanocrystal growth have resulted in the widespread nanocrystal applications in use today. Practical device applications range from low-threshold lasers to solar cells and biological imaging and tracking. Producing a specific type of luminescence known as photoluminescence nanocrystals show quite high quantum efficiency of up to 70% at room temperature. The missing 30% efficiency turns out to be an intrinsic property of nanocrystals.\n\nStudies of single colloidal nanocrystals show that they randomly turn their photoluminescence on and off even under continuous light illumination.\nThis tends to hinder progress for engineers and scientists who study single colloidal nanocrystals and try to use their fluorescent properties for biological imaging or lasing.\n\nThe blinking in nanocrystals was first reported in 1996. The discovery was unexpected. The consensus is that blinking happens because illuminated nanocrystals can be charged (or ionized), and then neutralized. Under normal conditions when nanocrystal is neutral, a photon excites an electron-hole pair, which then recombines, emitting another photon and leading to photoluminescence. This process is called radiative recombination. If however, the nanocrystal is charged, the extra carrier triggers a process called non-radiative Auger recombination, where exciton energy is transferred to an extra electron or hole. Auger recombination occurs orders of magnitude faster than the radiative recombination. So photoluminescence is almost entirely suppressed in charged nanocrystals. Scientists still do not fully understand the origin of the charging and neutralization process. One of the photoexcited carriers (the electron or the hole) must be ejected from the nanocrystal. At some later time, the ejected charge returns to the nanocrystal (restoring charge neutrality and therefore radiative recombination). The details of how these processes occur still are not understood.\n\nResearchers are attempting to eliminate the problem of blinking nanocrystals. One common solution is to suppress nanocrystal ionization. This could be done, for example, by growing a very thick semiconductor shell around the nanocrystal core. However, blinking was reduced, not eliminated, because the fundamental processes responsible for blinking - the non-radiative Auger recombination- were still present.\n\nOne method of study attempts to characterize the blinking behavior by studying single crystals or single quantum dots. A powerful microscope is employed along with video equipment. Another method uses ensembles or large quantities of quantum dots and develops statistical information.\n\n"}
{"id": "222313", "url": "https://en.wikipedia.org/wiki?curid=222313", "title": "Formation evaluation", "text": "Formation evaluation\n\nIn petroleum exploration and development, formation evaluation is used to determine the ability of a borehole to produce petroleum. Essentially, it is the process of \"recognizing a commercial well when you drill one\".\n\nModern rotary drilling usually uses a heavy mud as a lubricant and as a means of producing a confining pressure against the formation face in the borehole, preventing blowouts. Only in rare and catastrophic cases, do oil and gas wells \"come in\" with a fountain of gushing oil. In real life, that is a \"blowout\"—and usually also a financial and environmental disaster. But controlling blowouts has drawbacks—mud filtrate soaks into the formation around the borehole and a mud cake plasters the sides of the hole. These factors obscure the possible presence of oil or gas in even very porous formations. Further complicating the problem is the widespread occurrence of small amounts of petroleum in the rocks of many sedimentary provinces. In fact, if a sedimentary province is absolutely barren of traces of petroleum, it is not feasible to continue drilling there.\n\nThe formation evaluation problem is a matter of answering two questions:\n\nIt is complicated by the impossibility of directly examining the formation. It is, in short, the problem of looking at the formation \"indirectly\".\n\nTools to detect oil and gas have been evolving for over a century. The simplest and most direct tool is well cuttings examination. Some older oilmen ground the cuttings between their teeth and tasted to see if crude oil was present. Today, a wellsite geologist or mudlogger uses a low powered stereoscopic microscope to determine the lithology of the formation being drilled and to estimate porosity and possible oil staining. A portable ultraviolet light chamber or \"Spook Box\" is used to examine the cuttings for fluorescence. Fluorescence can be an indication of crude oil staining, or of the presence of fluorescent minerals. They can be differentiated by placing the cuttings in a solvent filled watchglass or dimple dish. The solvent is usually carbon tetrachlorethane. Crude oil dissolves and then redeposits as a fluorescent ring when the solvent evaporates. The written strip chart recording of these examinations is called a sample log or mudlog.\n\nWell cuttings examination is a learned skill. During drilling, chips of rock, usually less than about 1/8 inch (6 mm) across, are cut from the bottom of the hole by the bit. Mud, jetting out of holes in the bit under high pressure, washes the cuttings away and up the hole. During their trip to the surface they may circulate around the turning drillpipe, mix with cuttings falling back down the hole, mix with fragments caving from the hole walls and mix with cuttings travelling faster and slower in the same upward direction. They then are screened out of the mudstream by the shale shaker and fall on a pile at its base. Determining the type of rock being drilled at any one time is a matter of knowing the 'lag time' between a chip being cut by the bit and the time it reaches the surface where it is then examined by the wellsite geologist (or mudlogger as they are sometimes called). A sample of the cuttings taken at the proper time will contain the current cuttings in a mixture of previously drilled material. Recognizing them can be very difficult at times, for example after a \"bit trip\" when a couple of miles of drill pipe has been extracted and returned to the hole in order to replace a dull bit. At such a time there is a flood of foreign material knocked from the borehole walls (cavings), making the mudloggers task all the more difficult.\n\nOne way to get more detailed samples of a formation is by coring. Two techniques commonly used at present. The first is the \"whole core\", a cylinder of rock, usually about 3\" to 4\" in diameter and up to to long. It is cut with a \"core barrel\", a hollow pipe tipped with a ring-shaped diamond chip-studded bit that can cut a plug and bring it to the surface. Often the plug breaks while drilling, usually in shales or fractures and the core barrel jams, slowly grinding the rocks in front of it to powder. This signals the driller to give up on getting a full length core and to pull up the pipe.\n\nTaking a full core is an expensive operation that usually stops or slows drilling for at least the better part of a day. A full core can be invaluable for later reservoir evaluation. Once a section of well has been drilled, there is, of course, no way to core it without drilling another well.\n\nAnother, cheaper, technique for obtaining samples of the formation is \"Sidewall Coring\". One type of sidewall cores is percussion cores. In this method, a steel cylinder—a coring gun—has hollow-point steel bullets mounted along its sides and moored to the gun by short steel cables. The coring gun is lowered to the bottom of the interval of interest and the bullets are fired individually as the gun is pulled up the hole. The mooring cables ideally pull the hollow bullets and the enclosed plug of formation loose and the gun carries them to the surface. Advantages of this technique are low cost and the ability to sample the formation after it has been drilled. Disadvantages are possible non-recovery because of lost or misfired bullets and a slight uncertainty about the sample depth. Sidewall cores are often shot \"on the run\" without stopping at each core point because of the danger of differential sticking. Most service company personnel are skilled enough to minimize this problem, but it can be significant if depth accuracy is important.\n\nA second method of sidewall coring is rotary sidewall cores. In this method, a circular-saw assembly is lowered to the zone of interest on a wireline, and the core is sawed out. Dozens of cores may be taken this way in one run. This method is roughly 20 times as expensive as percussion cores, but yields a much better sample.\n\nA serious problem with cores is the change they undergo as they are brought to the surface. It might seem that cuttings and cores are very direct samples but the problem is whether the formation at depth will produce oil or gas. Sidewall cores are deformed and compacted and fractured by the bullet impact. Most full cores from any significant depth expand and fracture as they are brought to the surface and removed from the core barrel. Both types of core can be invaded or even flushed by mud, making the evaluation of formation fluids difficult. The formation analyst has to remember that all tools give indirect data.\n\nMud logging (or Wellsite Geology) is a well logging process in which drilling mud and drill bit cuttings from the formation are evaluated during drilling and their properties recorded on a strip chart as a visual analytical tool and stratigraphic cross sectional representation of the well. The drilling mud which is analyzed for hydrocarbon gases, by use of a gas chromatograph, contains drill bit cuttings which are visually evaluated by a mudlogger and then described in the mud log. The total gas, chromatograph record, lithological sample, pore pressure, shale density,D-exponent, etc. (all lagged parameters because they are circulated up to the surface from the bit) are plotted along with surface parameters such as rate of penetration (ROP), Weight On Bit (WOB),rotation per minute etc. on the mudlog which serve as a tool for the mudlogger, drilling engineers, mud engineers, and other service personnel charged with drilling and producing the well.\n\nThe oil and gas industry uses wireline logging to obtain a continuous record of a formation's rock properties. Wireline logging can be defined as being \"The acquisition and analysis of geophysical data performed as a function of well bore depth, together with the provision of related services.\" Note that \"wireline logging\" and \"mud logging\" are not the same, yet are closely linked through the integration of the data sets. The measurements are made referenced to \"TAH\" - True Along Hole depth: these and the associated analysis can then be used to infer further properties, such as hydrocarbon saturation and formation pressure, and to make further drilling and production decisions.\n\nWireline logging is performed by lowering a 'logging tool' - or a string of one or more instruments - on the end of a wireline into an oil well (or borehole) and recording petrophysical properties using a variety of sensors. Logging tools developed over the years measure the natural gamma ray, electrical, acoustic, stimulated radioactive responses, electromagnetic, nuclear magnetic resonance, pressure and other properties of the rocks and their contained fluids. For this article, they are broadly broken down by the main property that they respond to.\n\nThe data itself is recorded either at surface (real-time mode), or in the hole (memory mode) to an electronic data format and then either a printed record or electronic presentation called a \"well log\" is provided to the client, along with an electronic copy of the raw data. Well logging operations can either be performed during the drilling process (see Logging While Drilling), to provide real-time information about the formations being penetrated by the borehole, or once the well has reached Total Depth and the whole depth of the borehole can be logged.\n\nReal-time data is recorded directly against measured cable depth. Memory data is recorded against time, and then depth data is simultaneously measured against time. The two data sets are then merged using the common time base to create an instrument response versus depth log. Memory recorded depth can also be corrected in exactly the same way as real-time corrections are made, so there should be no difference in the attainable TAH accuracy.\n\nThe measured cable depth can be derived from a number of different measurements, but is usually either recorded based on a calibrated wheel counter, or (more accurately) using magnetic marks which provide calibrated increments of cable length. The measurements made must then be corrected for elastic stretch and temperature.[1]\n\nThere are many types of wireline logs and they can be categorized either by their function or by the technology that they use. \"Open hole logs\" are run before the oil or gas well is lined with pipe or cased. \"Cased hole logs\" are run after the well is lined with casing or production pipe.[2]\n\nWireline logs can be divided into broad categories based on the physical properties measured.\n\nIn 1928, the Schlumberger brothers in France developed the workhorse of all formation evaluation tools: the electric log. Electric logs have been improved to a high degree of precision and sophistication since that time, but the basic principle has not changed. Most underground formations contain water, often salt water, in their pores. The resistance to electric current of the total formation—rock and fluids—around the borehole is proportional to the sum of the volumetric proportions of mineral grains and conductive water-filled pore space. If the pores are partially filled with gas or oil, which are resistant to the passage of electric current, the bulk formation resistance is higher than for water filled pores. For the sake of a convenient comparison from measurement to measurement, the electrical logging tools measure the resistance of a cubic meter of formation. This measurement is called \"resistivity\".\n\nModern resistivity logging tools fall into two categories, Laterolog and Induction, with various commercial names, depending on the company providing the logging services.\n\nLaterolog tools send an electric current from an electrode on the sonde directly into the formation. The return electrodes are located either on surface or on the sonde itself. Complex arrays of electrodes on the sonde (guard electrodes) focus the current into the formation and prevent current lines from fanning out or flowing directly to the return electrode through the borehole fluid. Most tools vary the voltage at the main electrode in order to maintain a constant current intensity. This voltage is therefore proportional to the resistivity of the formation. Because current must flow from the sonde to the formation, these tools only work with conductive borehole fluid. Actually, since the resistivity of the mud is measured in series with the resistivity of the formation, laterolog tools give best results when mud resistivity is low with respect to formation resistivity, i.e., in salty mud.\nInduction logs use an electric coil in the sonde to generate an alternating current loop in the formation by induction. This is the same physical principle as is used in electric transformers. The alternating current loop, in turn, induces a current in a receiving coil located elsewhere on the sonde. The amount of current in the receiving coil is proportional to the intensity of current loop, hence to the conductivity (reciprocal of resistivity) of the formation. Multiple transmitting and receiving coils are used to focus formation current loops both radially (depth of investigation) and axially (vertical resolution). Until the late 80’s, the workhorse of induction logging has been the 6FF40 sonde which is made up of six coils with a nominal spacing of . Since the 90’s all major logging companies use so-called array induction tools. These comprise a single transmitting coil and a large number of receiving coils. Radial and axial focusing is performed by software rather than by the physical layout of coils. Since the formation current flows in circular loops around the logging tool, mud resistivity is measured in parallel with formation resistivity. Induction tools therefore give best results when mud resistivity is high with respect to formation resistivity, i.e., fresh mud or non-conductive fluid. In oil-base mud, which is non conductive, induction logging is the only option available.\n\nUntil the late 1950s electric logs, mud logs and sample logs comprised most of the oilman's armamentarium. Logging tools to measure porosity and permeability began to be used at that time. The first was the microlog. This was a miniature electric log with two sets of electrodes. One measured the formation resistivity about 1/2\" deep and the other about 1\"-2\" deep. The purpose of this seemingly pointless measurement was to detect permeability. Permeable sections of a borehole wall develop a thick layer of mudcake during drilling. Mud liquids, called filtrate, soak into the formation, leaving the mud solids behind to -ideally- seal the wall and stop the filtrate \"invasion\" or soaking. The short depth electrode of the microlog sees mudcake in permeable sections. The deeper 1\" electrode sees filtrate invaded formation. In nonpermeable sections both tools read alike and the traces fall on top of each other on the stripchart log. In permeable sections they separate.\n\nAlso in the late 1950s porosity measuring logs were being developed. The two main types are: nuclear porosity logs and sonic logs.\n\nThe two main nuclear porosity logs are the Density and the Neutron log.\n\nDensity logging tools contain a caesium-137 gamma ray source which irradiates the formation with 662 keV gamma rays. These gamma rays interact with electrons in the formation through Compton scattering and lose energy. Once the energy of the gamma ray has fallen below 100 keV, photolectric absorption dominates: gamma rays are eventually absorbed by the formation. The amount of energy loss by Compton scattering is related to the number electrons per unit volume of formation. Since for most elements of interest (below Z = 20) the ratio of atomic weight, A, to atomic number, Z, is close to 2, gamma ray energy loss is related to the amount of matter per unit volume, i.e., formation density.\n\nA gamma ray detector located some distance from the source, detects surviving gamma rays and sorts them into several energy windows. The number of high-energy gamma rays is controlled by compton scattering, hence by formation density. The number of low-energy gamma rays is controlled by photoelectric absorption, which is directly related to the average atomic number, Z, of the formation, hence to lithology. Modern density logging tools include two or three detectors, which allow compensation for some borehole effects, in particular for the presence of mud cake between the tool and the formation.\n\nSince there is a large contrast between the density of the minerals in the formation and the density of pore fluids, porosity can easily be derived from measured formation bulk density if both mineral and fluid densities are known.\n\nNeutron porosity logging tools contain an americium-beryllium neutron source, which irradiates the formation with neutrons. These neutrons lose energy through elastic collisions with nuclei in the formation. Once their energy has decreased to thermal level, they diffuse randomly away from the source and are ultimately absorbed by a nucleus. Hydrogen atoms have essentially the same mass as the neutron; therefore hydrogen is the main contributor to the slowing down of neutrons. A detector at some distance from the source records the number of neutron reaching this point. Neutrons that have been slowed down to thermal level have a high probability of being absorbed by the formation before reaching the detector. The neutron counting rate is therefore inversely related to the amount of hydrogen in the formation. Since hydrogen is mostly present in pore fluids (water, hydrocarbons) the count rate can be converted into apparent porosity. Modern neutron logging tools usually include two detectors to compensate for some borehole effects. Porosity is derived from the ratio of count rates at these two detectors rather than from count rates at a single detector.\n\nThe combination of neutron and density logs takes advantage of the fact that lithology has opposite effects on these two porosity measurements. The average of neutron and density porosity values is usually close to the true porosity, regardless of lithology. Another advantage of this combination is the \"gas effect.\" Gas, being less dense than liquids, translates into a density-derived porosity that is too high. Gas, on the other hand, has much less hydrogen per unit volume than liquids: neutron-derived porosity, which is based on the amount of hydrogen, is too low. If both logs are displayed on compatible scales, they overlay each other in liquid-filled clean formations and are widely separated in gas-filled formations.\n\nSonic logs use a pinger and microphone arrangement to measure the velocity of sound in the formation from one end of the sonde to the other. For a given type of rock, acoustic velocity varies indirectly with porosity. If the velocity of sound through solid rock is taken as a measurement of 0% porosity, a slower velocity is an indication of a higher porosity that is usually filled with formation water with a slower sonic velocity.\n\nBoth sonic and density-neutron logs give porosity as their primary information. Sonic logs read farther away from the borehole so they are more useful where sections of the borehole are caved. Because they read deeper, they also tend to average more formation than the density-neutron logs do. Modern sonic configurations with pingers and microphones at both ends of the log, combined with computer analysis, minimize the averaging somewhat. Averaging is an advantage when the formation is being evaluated for seismic parameters, a different area of formation evaluation. A special log, the Long Spaced Sonic, is sometimes used for this purpose. Seismic signals (a single undulation of a sound wave in the earth) average together tens to hundreds of feet of formation, so an averaged sonic log is more directly comparable to a seismic waveform.\n\nDensity-neutron logs read the formation within about four to seven inches (178 mm) of the borehole wall. This is an advantage in resolving thin beds. It is a disadvantage when the hole is badly caved. Corrections can be made automatically if the cave is no more than a few inches deep. A caliper arm on the sonde measures the profile of the borehole and a correction is calculated and incorporated in the porosity reading. However, if the cave is much more than four inches deep, the density-neutron log is reading little more than drilling mud.\n\nThere are two other tools, the SP log and the Gamma Ray log, one or both of which are almost always used in wireline logging. Their output is usually presented along with the electric and porosity logs described above. They are indispensable as additional guides to the nature of the rock around the borehole.\n\nThe SP log, known variously as a \"Spontaneous Potential\", \"Self Potential\" or \"Shale Potential\" log is a voltmeter measurement of the voltage or electrical potential difference between the mud in the hole at a particular depth and a copper ground stake driven into the surface of the earth a short distance from the borehole. A salinity difference between the drilling mud and the formation water acts as a natural battery and will cause several voltage effects. This \"battery\" causes a movement of charged ions between the hole and the formation water where there is enough permeability in the rock. The most important voltage is set up as a permeable formation permits ion movement, reducing the voltage between the formation water and the mud. Sections of the borehole where this occurs then have a voltage difference with other nonpermeable sections where ion movement is restricted. Vertical ion movement in the mud column occurs much more slowly because the mud is not circulating while the drill pipe is out of the hole. The copper surface stake provides a reference point against which the SP voltage is measured for each part of the borehole. There can also be several other minor voltages, due for example to mud filtrate streaming into the formation under the effect of an overbalanced mud system. This flow carries ions and is a voltage generating current. These other voltages are secondary in importance to the voltage resulting from the salinity contrast between mud and formation water.\n\nThe nuances of the SP log are still being researched. In theory, almost all porous rocks contain water. Some pores are completely filled with water. Others have a thin layer of water molecules wetting the surface of the rock, with gas or oil filling the rest of the pore. In sandstones and porous limestones there is a continuous layer of water throughout the formation. If there is even a little permeability to water, ions can move through the rock and decrease the voltage difference with the mud nearby. Shales do not allow water or ion movement. Although they may have a large water content, it is bound to the surface of the flat clay crystals comprising the shale. Thus mud opposite shale sections maintains its voltage difference with the surrounding rock. As the SP logging tool is drawn up the hole it measures the voltage difference between the reference stake and the mud opposite shale and sandstone or limestone sections. The resulting log curve reflects the permeability of the rocks and, indirectly, their lithology. SP curves degrade over time, as the ions diffuse up and down the mud column. It also can suffer from stray voltages caused by other logging tools that are run with it. Older, simpler logs often have better SP curves than more modern logs for this reason. With experience in an area, a good SP curve can even allow a skilled interpreter to infer sedimentary environments such as deltas, point bars or offshore tidal deposits.\n\nThe gamma ray log is a measurement of naturally occurring gamma radiation from the borehole walls. Sandstones are usually nonradioactive quartz and limestones are nonradioactive calcite. Shales however, are naturally radioactive due to potassium isotopes in clays, and adsorbed uranium and thorium. Thus the presence or absence of gamma rays in a borehole is an indication of the amount of shale or clay in the surrounding formation. The gamma ray log is useful in holes drilled with air or with oil based muds, as these wells have no SP voltage. Even in water-based muds, the gamma ray and SP logs are often run together. They comprise a check on each other and can indicate unusual shale sections which may either not be radioactive, or may have an abnormal ionic chemistry. The gamma ray log is also useful to detect coal beds, which, depending on the local geology, can have either low radiation levels, or high radiation levels due to adsorption of uranium. In addition, the gamma ray log will work inside a steel casing, making it essential when a cased well must be evaluated.\n\nThe immediate questions that have to be answered in deciding to complete a well or to plug and abandon (P&A) it are:\n\nThe elementary approach to answering these questions uses the Archie Equation.\n"}
{"id": "24812968", "url": "https://en.wikipedia.org/wiki?curid=24812968", "title": "Gazprom Transgaz Belarus", "text": "Gazprom Transgaz Belarus\n\nGazprom Transgaz Belarus (former name: Beltransgaz) is a natural gas infrastructure and transportation company of Belarus. It operates the main natural gas transit pipelines through Belarus—Northern Lights and Yamal–Europe. Beltransgaz was founded in 1992 on the bases of Zapadtransgaz, a company responsible for the gas transit through Belarus. The company is owned by the Russian gas company Gazprom.\n"}
{"id": "35806664", "url": "https://en.wikipedia.org/wiki?curid=35806664", "title": "Hayrake table", "text": "Hayrake table\n\nA hayrake table is a distinct pattern of table produced as part of the English Arts and Crafts movement in the early part of the twentieth century.\n\nIts distinctive feature is the arrangement of the lower stretcher between the legs as a double-ended Y-shape. The shape of each end, and their joinery, was based on traditional English craft woodworking and the construction of wooden hay rakes. The stretcher is not merely a simple Y shape, but its junction is braced by a T-shaped joint, as was needed for the work of a rake. Some modern reproductions simplify this to a plain Y, abandoning the design's original roots.\n\nThese tables are best known as the work of Ernest Gimson and his associates the Barnsley brothers and Peter Waals at their Daneway workshops in Sapperton, Gloucestershire. Other Arts and Crafts designers of the period also produced them, particularly those in the Cotswolds such as Gordon Russell\n\nTimber used in their construction was, as for other Arts and Crafts work, locally-grown English hardwoods. Most were produced in oak although some, like the original hay rakes, were made in ash.\n\nThe design varies between makers, mostly in its details. Gimson's tables are considered the finest and the canonical example of the design. Their edges are heavily chamfered, a typically Gimson feature, which is derived from the finishing of the original agricultural tools. This chamfer also has the practical benefit for a table stretcher of reduced wear from feet on an otherwise sharp edge. Gimson's distinctive use of gentle stopped chamfers evokes the framing of Gloucestershire wagons. Gimson's tables also have their edges finished with bands of chip carving or sometimes with inlaid bands of light holly and dark bog oak. Peter Waals produced the tables for some time after the death of Gimson and, as with many of his pieces, updated their Arts and Crafts detailing to follow the post-war fashions of Modernism and Art Deco.\n\nAlthough less well-known than some other iconic Arts and Crafts pieces, the hayrake table remains a popular design to this day. They are produced both commercially and as plans for hobbyists.\n\n"}
{"id": "40921257", "url": "https://en.wikipedia.org/wiki?curid=40921257", "title": "Heneicosylic acid", "text": "Heneicosylic acid\n\nHeneicosylic acid, or \"heneicosanoic acid\", is a 21-carbon long-chain saturated fatty acid with the chemical formula CH(CH)COOH. It has shown relevance in the production of foams, paints, and related viscous materials.\n\n"}
{"id": "6927432", "url": "https://en.wikipedia.org/wiki?curid=6927432", "title": "Heteroazeotrope", "text": "Heteroazeotrope\n\nA heteroazeotrope is an azeotrope where the vapour phase coexists with two liquid phases.\nSketch of a T-x/y equilibrium curve of a typical heteroazeotropic mixture\n\n\nHeterogeneous distillation means that during the distillation the liquid phase of the mixture is immiscible.\nIn this case on the plates can be two liquid phases and the top vapour condensate splits in two liquid phases, which can be separated in a decanter.\nThe simplest case of continuous heteroazeotropic distillation is the separation of a binary heterogeneous azeotropic mixture. In this case the system contains two columns and a decanter. The fresh feed (A-B) is added into the first column. (The feed may also be added into the decanter directly or into the second column depending on the composition of the mixture). From the decanter the A-rich phase is withdrawn as reflux into the first column while the B-rich phase is withdrawn as reflux into the second column. This mean the first column produces \"A\" and the second column produces \"B\" as a bottoms product. In the industry the butanol-water mixture is separated with this technique.\n\nAt the previous case the binary system forms already a heterogeneous azeotrope. The other application of the heteroazeotropic distillation is the separation of a binary system (A-B) forming a homogeneous azeotrope. In this case an entrainer or solvent is added to the mixture in order to form an heteroazeotrope with one or both of the components in order to help the separation of the original A-B mixture.\n\nBatch heteroazeotropic distillation is an efficient method for the separation of azeotropic and\nlow relative volatility (low α) mixtures. A third component (entrainer, E) is added to the\nbinary A-B mixture, which makes the separation of A and B possible. The entrainer forms a\nheteroazeotrope with at least one (and preferably with only one (selective entrainer)) of the\noriginal components.\nThe main parts of the conventional batch distillation columns are the following:\n- pot (include reboiler)\n- column\n- condenser to condense the top vapour\n- product receivers\n- (entrainer fed)\nIn case of the heteroazeotropic distillation the equipment is completed with a decanter, where the two liquid phases are split.\n\nThree different cases are possible for the addition of the entrainer:\n\n1, Batch Addition of the Entrainer: The total quantity of the entrainer is added to the charge before the start of the procedure.\n2, Continuous Entrainer Feeding: The total quantity of the entrainer is introduced continuously to the column.\n\n3, Mixed Addition of the Entrainer: The combination of the batch addition and continuous feeding of the entrainer. We added one part of the entrainer to the charge before the start of the distillation and the other part continuously during distillation.\n\nIn the last years the batch heteroazeotropic distillation has come into prominenece so several studies have been published. The heteroazeotropic batch distillation was investigated by feasibility studies, rigorous simulation calculations and laboratory experiments. Feasibility analysis is conducted in Modla et al. and Rodriguez-Donis et al. for the separation of low-relative-volatility and azeotropic mixtures by heterogeneous batch distillation in a batch rectifier. Rodriguez-Donis et al. were the first to provide the entrainer selection rules. The feasibility methods was extended and modified by Rodriguez-Donis et al., Rodriguez-Donis et al., (2005), Skouras et al., and Lang and Modla. Varga applied these feasibility studies in her thesis. Experimental result was published by Rodriguez-Donis et al., Xu and Wand, Van Kaam and others.\n\n"}
{"id": "29690231", "url": "https://en.wikipedia.org/wiki?curid=29690231", "title": "Hybtonite", "text": "Hybtonite\n\nHybtonite is trademark of Amroy Europe Oy for carbon nanoepoxy resins.\nIt is a family of composite resins reinforced with carbon nanotubes (CNTs).\n\nThe material and the manufacturing method were originally developed in the Nanoscience Center of University of Jyväskylä during the years 2002 to 2004.\nUltrasound is used to disperse the nanotubes and to create radicals at the ends of CNT molecules.\nCNTs can then chemically react with epoxy resin or other material forming strong covalent bonds.\nThis results in a more durable hybrid composite structure that is between 20 and 30% stronger (with only 0.5% CNT contents) than a conventional reinforced plastic.\n\nThe manufacturing process allows controlling the material properties such as electrical conductivity, thermal conductivity and viscosity.\nDifferent forms of hybtonite are available for different purposes such as laminating (glass fiber, carbon fiber), epoxy paints and glues.\n\nThe first application areas for hybtonite have been in field of wind turbines, marine applications and sports gear.\n\nOn January 2006, Montreal Hybtonite hockey stick \"Nitro\" was voted number one Nano product in the world at Nanotech 2006 trade show in Tokyo, Japan.\n\nOn December 2009, Amroy received Frost & Sullivan European Technology Innovation Award for its work on hybtonite.\n"}
{"id": "5292003", "url": "https://en.wikipedia.org/wiki?curid=5292003", "title": "Indium(III) oxide", "text": "Indium(III) oxide\n\nIndium(III) oxide (InO) is a chemical compound, an amphoteric oxide of indium.\n\nAmorphous indium oxide is insoluble in water but soluble in acids, whereas crystalline indium oxide is insoluble in both water and acids. The crystalline form exist in two phases, the cubic (bixbyite type) and rhombohedral (corundum type). Both phases have a band gap of about 3 eV. The parameters of the cubic phase are listed in the infobox. The rhombohedral phase is produced at high temperatures and pressures or when using non-equilibrium growth methods. It has a space group Rc No. 167, Pearson symbol hR30, a = 0.5487 nm, b = 0.5487 nm, c = 0.57818 nm, Z = 6 and calculated density 7.31 g/cm.\n\nThin films of chromium-doped indium oxide (InCrO) are a magnetic semiconductor displaying high-temperature ferromagnetism, single-phase crystal structure, and semiconductor behavior with high concentration of charge carriers. It has possible applications in spintronics as a material for spin injectors.\n\nThin polycrystalline films of indium oxide doped with Zn are highly conductive (conductivity ~10 S/m) and even superconductive at helium temperatures. The superconducting transition temperature T depends on the doping and film structure and is below 3.3 K.\n\nBulk samples can be prepared by heating indium(III) hydroxide or the nitrate, carbonate or sulfate. \nThin films of indium oxide can be prepared by sputtering of indium target in argon/oxygen atmosphere. They can be used as diffusion barriers (\"barrier metals\") in semiconductors, e.g. to inhibit diffusion between aluminium and silicon.\n\nMonocrystalline nanowires were synthetized from indium oxide by laser ablation, allowing precise diameter control down to 10 nm. Field effect transistors were fabricated from those. Indium oxide nanowires can serve as sensitive and specific redox protein sensors. Sol-gel method is another way to prepare the nanowires.\n\nIndium oxide can serve as a semiconductor material, forming heterojunctions with p-InP, n-GaAs, n-Si, and other materials. A layer of indium oxide on a silicon substrate can be deposited from an indium trichloride solution, a method useful for manufacture of solar cells.\n\nWhen heated to 700 °C Indium(III) oxide forms InO, (called indium(I) oxide or indium suboxide), at 2000 °C it decomposes.\nIt is soluble in acids but not in alkali.\nWith ammonia at high temperature indium nitride is formed \nWith KO and indium metal the compound KInO containing tetrahedral InO ions was prepared.\nReacting with a range of metal trioxides produced perovskites for example:\n\nIndium oxide is used in some types of batteries, thin film infrared reflectors transparent for visible light (hot mirrors), some optical coatings, and some antistatic coatings. In combination with tin dioxide, indium oxide forms indium tin oxide (also called tin doped indium oxide or ITO), a material used for transparent conductive coatings.\n\nIn semiconductors, indium oxide can be used as an n-type semiconductor used as a resistive element in integrated circuits.\n\nIn histology, indium oxide is used as a part of some stain formulations.\n\n"}
{"id": "1049625", "url": "https://en.wikipedia.org/wiki?curid=1049625", "title": "International Petroleum Exchange", "text": "International Petroleum Exchange\n\nThe International Petroleum Exchange, now ICE Futures (since 2005-04-7), based in London, was one of the world's largest energy futures and options exchanges. Its flagship commodity, \"Brent Crude\" was a world benchmark for oil prices, but the exchange also handled futures contracts and options on fuel oil, natural gas, electricity (baseload and peakload), coal contracts and, as of 22 April 2005, carbon emission allowances with the European Climate Exchange (ECX).\n\nThe IPE was acquired by the Intercontinental Exchange in 2001. The IPE was an open outcry exchange until 7 April 2005, when its name was changed to ICE Futures and all trading was shifted onto an electronic trading platform.\n\nUntil the 1970s, the price of oil was relatively stable with production largely controlled by the biggest oil companies. During that decade two oil price shocks led to continued price volatility in the market; short-term physical markets evolved, and the need to hedge emerged.\n\nA group of energy and futures companies founded the IPE in 1980, and the first contract, for gas oil futures, was launched the following year. In June 1988, the IPE launched Brent Crude futures. \n\nSince its inception, oil futures and latterly options have been traded in pits on the trading floor using the open outcry system. As business volumes have grown, the IPE has moved location several times to accommodate new pits and more traders.\n\nThe Exchange has experienced incremental growth, year-on-year for most of its history. Complexity, but also efficiency have increased as new trading instruments such as swaps, futures, and options have been developed.\n\nSince 1997, the ICE Futures has expanded its offerings from Brent Crude and Gas Oil to include Natural Gas (1997), Electricity (2004), and ECX carbon financial instruments (2005). These expansions have allowed ICE Futures to offer a wider range of energy products. More advanced transactions are also now possible, due to cross- and multi-product transactions, which eliminate the need to use multiple markets or an adviser.\n\n\n"}
{"id": "14951", "url": "https://en.wikipedia.org/wiki?curid=14951", "title": "Ionic bonding", "text": "Ionic bonding\n\nIonic bonding is a type of chemical bonding that involves the electrostatic attraction between oppositely charged ions, and is the primary interaction occurring in ionic compounds. It is one of the main bonds along with Covalent bond and Metallic bonding. Ions are atoms that have gained one or more electrons (known as anions, which are negatively charged) and atoms that have lost one or more electrons (known as cations, which are positively charged). This transfer of electrons is known as electrovalence in contrast to covalence. In the simplest case, the cation is a metal atom and the anion is a nonmetal atom, but these ions can be of a more complex nature, e.g. molecular ions like or . In simpler words, an ionic bond is the transfer of electrons from a metal to a non-metal in order to obtain a full valence shell for both atoms.\n\nIt is important to recognize that \"clean\" ionic bonding – in which one atom or molecule completely transfers an electron to another cannot exist: all ionic compounds have some degree of covalent bonding, or electron sharing. Thus, the term \"ionic bonding\" is given when the ionic character is greater than the covalent character – that is, a bond in which a large electronegativity difference exists between the two atoms, causing the bonding to be more polar (ionic) than in covalent bonding where electrons are shared more equally. Bonds with partially ionic and partially covalent character are called polar covalent bonds. \n\nIonic compounds conduct electricity when molten or in solution, typically as a solid. Ionic compounds generally have a high melting point, depending on the charge of the ions they consist of. The higher the charges the stronger the cohesive forces and the higher the melting point. They also tend to be soluble in water; the stronger the cohesive forces, the lower the solubility. \n\nAtoms that have an almost full or almost empty valence shell tend to be very reactive. Atoms that are strongly electronegative (as is the case with halogens) often have only one or two empty orbitals in their valence shell, and frequently bond with other molecules or gain electrons to form anions. Atoms that are weakly electronegative (such as alkali metals) have relatively few valence electrons, which can easily be shared with atoms that are strongly electronegative. As a result, weakly electronegative atoms tend to distort their electron cloud and form cations.\n\nIonic bonding can result from a redox reaction when atoms of an element (usually metal), whose ionization energy is low, give some of their electrons to achieve a stable electron configuration. In doing so, cations are formed. An atom of another element (usually nonmetal) with greater electron affinity accepts the electron(s) to attain a stable electron configuration, and after accepting electron(s) an atom becomes an anion. Typically, the stable electron configuration is one of the noble gases for elements in the s-block and the p-block, and particular stable electron configurations for d-block and f-block elements. The electrostatic attraction between the anions and cations leads to the formation of a solid with a crystallographic lattice in which the ions are stacked in an alternating fashion. In such a lattice, it is usually not possible to distinguish discrete molecular units, so that the compounds formed are not molecular in nature. However, the ions themselves can be complex and form molecular ions like the acetate anion or the ammonium cation.\n\nFor example, common table salt is sodium chloride. When sodium (Na) and chlorine (Cl) are combined, the sodium atoms each lose an electron, forming cations (Na), and the chlorine atoms each gain an electron to form anions (Cl). These ions are then attracted to each other in a 1:1 ratio to form sodium chloride (NaCl).\n\nHowever, to maintain charge neutrality, strict ratios between anions and cations are observed so that ionic compounds, in general, obey the rules of stoichiometry despite not being molecular compounds. For compounds that are transitional to the alloys and possess mixed ionic and metallic bonding, this may not be the case anymore. Many sulfides, e.g., do form non-stoichiometric compounds.\n\nMany ionic compounds are referred to as salts as they can also be formed by the neutralization reaction of an Arrhenius base like NaOH with an Arrhenius acid like HCl\n\nThe salt NaCl is then said to consist of the acid rest Cl and the base rest Na.\n\nThe removal of electrons from the cation is endothermic, raising the system's overall energy. There may also be energy changes associated with breaking of existing bonds or the addition of more than one electron to form anions. However, the action of the anion's accepting the cation's valence electrons and the subsequent attraction of the ions to each other releases (lattice) energy and, thus, lowers the overall energy of the system.\n\nIonic bonding will occur only if the overall energy change for the reaction is favorable. In general, the reaction is exothermic, but, e.g., the formation of mercuric oxide (HgO) is endothermic. The charge of the resulting ions is a major factor in the strength of ionic bonding, e.g. a salt CA is held together by electrostatic forces roughly four times weaker than CA according to Coulombs law, where C and A represent a generic cation and anion respectively. The sizes of the ions and the particular packing of the lattice are ignored in this rather simplistic argument.\n\nIonic compounds in the solid state form lattice structures. The two principal factors in determining the form of the lattice are the relative charges of the ions and their relative sizes. Some structures are adopted by a number of compounds; for example, the structure of the rock salt sodium chloride is also adopted by many alkali halides, and binary oxides such as magnesium oxide. Pauling's rules provide guidelines for predicting and rationalizing the crystal structures of ionic crystals\n\nFor a solid crystalline ionic compound the enthalpy change in forming the solid from gaseous ions is termed the lattice energy.\nThe experimental value for the lattice energy can be determined using the Born–Haber cycle. It can also be calculated (predicted) using the Born–Landé equation as the sum of the electrostatic potential energy, calculated by summing interactions between cations and anions, and a short-range repulsive potential energy term. The electrostatic potential can be expressed in terms of the interionic separation and a constant (Madelung constant) that takes account of the geometry of the crystal. The further away from the nucleus the weaker the shield. The Born-Landé equation gives a reasonable fit to the lattice energy of, e.g., sodium chloride, where the calculated (predicted) value is −756 kJ/mol, which compares to −787 kJ/mol using the Born–Haber cycle.\n\nIons in crystal lattices of purely ionic compounds are spherical; however, if the positive ion is small and/or highly charged, it will distort the electron cloud of the negative ion, an effect summarised in Fajans' rules. This polarization of the negative ion leads to a build-up of extra charge density between the two nuclei, that is, to partial covalency. Larger negative ions are more easily polarized, but the effect is usually important only when positive ions with charges of 3+ (e.g., Al) are involved. However, 2+ ions (Be) or even 1+ (Li) show some polarizing power because their sizes are so small (e.g., LiI is ionic but has some covalent bonding present). Note that this is not the ionic polarization effect that refers to displacement of ions in the lattice due to the application of an electric field.\n\nIn ionic bonding, the atoms are bound by attraction of oppositely charged ions, whereas, in covalent bonding, atoms are bound by sharing electrons to attain stable electron configurations. In covalent bonding, the molecular geometry around each atom is determined by valence shell electron pair repulsion VSEPR rules, whereas, in ionic materials, the geometry follows maximum packing rules. One could say that covalent bonding is more \"directional\" in the sense that the energy penalty for not adhering to the optimum bond angles is large, whereas ionic bonding has no such penalty. There are no shared electron pairs to repel each other, the ions should simply be packed as efficiently as possible. This often leads to much higher coordination numbers. In NaCl, each ion has 6 bonds and all bond angles are 90°. In CsCl the coordination number is 8. By comparison carbon typically has a maximum of four bonds.\n\nPurely ionic bonding cannot exist, as the proximity of the entities involved in the bonding allows some degree of sharing electron density between them. Therefore, all ionic bonding has some covalent character. Thus, bonding is considered ionic where the ionic character is greater than the covalent character. The larger the difference in electronegativity between the two types of atoms involved in the bonding, the more ionic (polar) it is. Bonds with partially ionic and partially covalent character are called polar covalent bonds. For example, Na–Cl and Mg–O interactions have a few percent covalency, while Si–O bonds are usually ~50% ionic and ~50% covalent. Pauling estimated that an electronegativity difference of 1.7 (on the Pauling scale) corresponds to 50% ionic character, so that a difference greater than 1.7 corresponds to a bond which is predominantly ionic.\nIonic character in covalent bonds can be directly measured for atoms having quadrupolar nuclei (H, N, Br, Cl or I). These nuclei are generally objects of NQR nuclear quadrupole resonance and NMR nuclear magnetic resonance studies. Interactions between the nuclear quadrupole moments \"Q\" and the electric field gradients (EFG) are characterized via the nuclear quadrupole coupling constants\nwhere the \"eq\" term corresponds to the principal component of the EFG tensor and \"e\" is the elementary charge. In turn, the electric field gradient opens the way to description of bonding modes in molecules when the QCC values are accurately determined by NMR or NQR methods. \n\nIn general, when ionic bonding occurs in the solid (or liquid) state, it is not possible to talk about a single \"ionic bond\" between two individual atoms, because the cohesive forces that keep the lattice together are of a more collective nature. This is quite different in the case of covalent bonding, where we can often speak of a distinct bond localized between two particular atoms. However, even if ionic bonding is combined with some covalency, the result is \"not\" necessarily discrete bonds of a localized character. In such cases, the resulting bonding often requires description in terms of a band structure consisting of gigantic molecular orbitals spanning the entire crystal. Thus, the bonding in the solid often retains its collective rather than localized nature. When the difference in electronegativity is decreased, the bonding may then lead to a semiconductor, a semimetal or eventually a metallic conductor with metallic bonding.\n\n\n"}
{"id": "22712197", "url": "https://en.wikipedia.org/wiki?curid=22712197", "title": "John Stephens Graham", "text": "John Stephens Graham\n\nJohn Stephens Graham (August 4, 1905 – October 20, 1976) was a Washington, D.C. attorney and political appointee. He was an Assistant Secretary of the Treasury, and commissioners for the Internal Revenue Service and Atomic Energy Commission.\n\nGraham was born August 4, 1905 in Reading, Massachusetts, son of Joseph L. Graham, a R.J. Reynolds Tobacco Company executive, and Margaret Nowell Graham, an artist. His older sister was Katherine G. Howard, an Eisenhower administration official. He was a cousin of Margaret Mitchell, the author of Gone With the Wind.\n\nGraham graduated from the University of North Carolina at Chapel Hill and attended Harvard Law School before graduating from University of Virginia School of Law with close friend Frank Wisner.\n\nDuring World War II, Graham served in the United States Navy.\n\nGraham served as Assistant Secretary of the Treasury during the second term of President Harry S. Truman and Secretary of the Treasury John Wesley Snyder. He served as the 30th Commissioner of Internal Revenue from November 19, 1952 until January 19, 1953.\n\nAfter Dwight D. Eisenhower became president in 1953, Graham became a financial and business consultant in Washington, D.C. until 1956, when he served as national treasurer for Volunteers for Stevenson, the campaign to elect Adlai Stevenson President of the United States, against incumbent President Eisenhower.\n\nOn September 12, 1957, when Graham was 51, he was appointed as a commissioner of the U.S. Atomic Energy Commission by Eisenhower, and as a delegate to the International Atomic Energy Agency. The President, along with partisan Lewis Strauss, both Republicans, appointed Graham, a Democrat, to fill out John von Neumann's term following Neumann's death. This was done as a show of conciliation between the President and the Joint Committee Graham served as a commissioner on the Commission until June 30, 1962.\n\nHe married Elizabeth Foster Breckinridge (1911–2005), daughter of Henry S. Breckinridge and Ruth Bradley Woodman Breckinridge. Elizabeth's father was the United States Assistant Secretary of War under Woodrow Wilson, and was a member of the prominent Breckinridge family. She was born in Monterey, Pennsylvania, grew up in Washington, D.C. and Bethesda, Maryland, and was a 1933 graduate of Vassar College. She was a tutor, teacher and founder of an after-school program, Tuesday School.\n\nGraham and his wife lived in Winston-Salem, N.C. before moving to Washington, D.C. in 1942 where Graham served in the Navy. The couple had four daughters:\n\n\nGraham died in October 20, 1976 in Washington, D.C. His wife, Elizabeth, lived until October 25, 2005, when she died following a heart attack.\n\n\"Subnotes\"\n\n"}
{"id": "2061865", "url": "https://en.wikipedia.org/wiki?curid=2061865", "title": "Kelvin–Voigt material", "text": "Kelvin–Voigt material\n\nA Kelvin–Voigt material, also called a Voigt material, is a viscoelastic material having the properties both of elasticity and viscosity. It is named after the British physicist and engineer Lord Kelvin and after German physicist Woldemar Voigt.\n\nThe Kelvin–Voigt model, also called the Voigt model, can be represented by a purely viscous damper and purely elastic spring connected in parallel as shown in the picture.\n\nIf we connect these two elements in series we get a model of a Maxwell material.\n\nSince the two components of the model are arranged in parallel, the strains in each component are identical:\n\nwhere the subscript D indicates the stress–strain in the damper and the subscript S indicates the stress–strain in the spring. Similarly, the total stress will be the sum of the stress in each component:\n\nFrom these equations we get that in a Kelvin–Voigt material, stress σ, strain ε and their rates of change with respect to time \"t\" are governed by equations of the form:\n\nor, in dot notation:\n\nwhere \"E\" is a modulus of elasticity and formula_5 is the viscosity. The equation can be applied either to the shear stress or normal stress of a material.\n\nIf we suddenly apply some constant stress formula_6 to Kelvin–Voigt material, then the deformations would approach the deformation for the pure elastic material formula_7 with the difference decaying exponentially:\n\nwhere \"t\" is time and formula_9 the rate of relaxation formula_10. Conversely, the value formula_11 is known as the retardation time.\n\nIf we would free the material at time formula_12, then the elastic element would retard the material back until the deformation becomes zero. The retardation obeys the following equation:\n\nThe picture shows the dependence of the dimensionless deformation formula_14 \non dimensionless time formula_15. In the picture the stress on the material is loaded at time formula_16, and released at the later dimensionless time formula_17.\nSince all the deformation is reversible (though not suddenly) the Kelvin–Voigt material is a solid.\n\nThe Voigt model predicts creep more realistically than the Maxwell model, because in the infinite time limit the strain approaches a constant:\n\nwhile a Maxwell model predicts a linear relationship between strain and time, which is most often not the case. Although the Kelvin–Voigt model is effective for predicting creep, it is not good at describing the relaxation behavior after the stress load is removed.\n\nThe complex dynamic modulus of the Kelvin–Voigt material is given by:\n\nThus, the real and imaginary components of the dynamic modulus are:\n\nNote that formula_22 is constant, while formula_23 is directly proportional to frequency (where the apparent viscosity, formula_5, is the constant of proportionality).\n\n\n"}
{"id": "330970", "url": "https://en.wikipedia.org/wiki?curid=330970", "title": "Ken Saro-Wiwa", "text": "Ken Saro-Wiwa\n\nKenule Beeson \"Ken\" Saro-Wiwa (10 October 1941 – 10 November 1995) was a Nigerian writer, television producer, environmental activist, and winner of the Right Livelihood Award and the Goldman Environmental Prize. Saro-Wiwa was a member of the Ogoni people, an ethnic minority in Nigeria whose homeland, Ogoniland, in the Niger Delta has been targeted for crude oil extraction since the 1950s and which has suffered extreme environmental damage from decades of indiscriminate petroleum waste dumping. Initially as spokesperson, and then as president, of the Movement for the Survival of the Ogoni People (MOSOP), Saro-Wiwa led a nonviolent campaign against environmental degradation of the land and waters of Ogoniland by the operations of the multinational petroleum industry, especially the Royal Dutch Shell company. He was also an outspoken critic of the Nigerian government, which he viewed as reluctant to enforce environmental regulations on the foreign petroleum companies operating in the area.\n\nAt the peak of his non-violent campaign, he was tried by a special military tribunal for allegedly masterminding the gruesome murder of Ogoni chiefs at a pro-government meeting, and hanged in 1995 by the military dictatorship of General Sani Abacha. His execution provoked international outrage and resulted in Nigeria's suspension from the Commonwealth of Nations for over three years.\n\nBorn Kenule Tsaro-Wiwa, the son of Jim Wiwa, a forest ranger and his third wife, Widu. He changed his name to Saro-Wiwa after the Nigerian Civil war. His father's hometown is the village of Bori, Ogoniland whose residents speak the Khana dialect of Ogoni language. Saro-Wiwa spent his childhood in an Anglican home and eventually proved himself to be an excellent student; he received primary education at a Native Authority school in Bori, then attended secondary school at Government College Umuahia. A distinguished student, Saro-Wiwa was captain of the table tennis team and amassed school prizes in history and English. At Umuahia, he was the only student from Ogoni land but fellow students were mandated to speak English which made Saro-Wiwa feel Nigerian; he embraced the language as a useful way to communicate to a larger audience at home and abroad. On completion of secondary education, he obtained a scholarship to study English at the University of Ibadan. At Ibadan, he plunged into academic and cultural interests, he won departmental prizes in 1963 and 1965 and worked for a drama troupe. The travelling drama troupe performed in Kano, Benin Ilorin and Lagos and collaborated with the Nottingham Playhouse theatre group that included a young Judi Dench. He briefly became a teaching assistant at the University of Lagos and later at University of Nigeria, Nsukka. Saro-Wiwa was an African literature lecturer in Nsukka when the Civil war broke out, he supported the Federal Government and had to leave the region for his hometown of Bori. On his journey to Port-Harcourt, he witnessed the multitudes of refugees returning to the East, a scene he described as a \"sorry sight to see\". Three days after his arrival, nearby Bonny was liberated by federal troops. He and his family then stayed in Bonny, he travelled back to Lagos and took a position at the University of Lagos which did not last long as he was called back to Bonny.\n\nHe called back to become the Civilian Administrator for the port city of Bonny in the Niger Delta and during the Nigerian Civil War positioned himself as an Ogoni leader dedicated to the Federal cause. He followed his job as an administrator with an appointment as a commissioner in the old Rivers State. His best known novel, \"\", tells the story of a naive village boy recruited to the army during the Nigerian Civil War of 1967 to 1970, and intimates the political corruption and patronage in Nigeria's military regime of the time. Saro-Wiwa's war diaries, \"On a Darkling Plain\", document his experience during the war. He was also a successful businessman and television producer. His satirical television series, \"Basi & Company\", was wildly popular, with an estimated audience of 30 million.\n\nIn the early 1970s, Saro-Wiwa served as the Regional Commissioner for Education in the Rivers State Cabinet, but was dismissed in 1973 because of his support for Ogoni autonomy. In the late 1970s, he established a number of successful business ventures in retail and real estate, and during the 1980s concentrated primarily on his writing, journalism and television production. In 1977, he became involved in the political arena running as the candidate to represent Ogoni in the Constituent Assembly. Saro-Wiwa lost the election in a narrow margin. It was during this time he had a fall out with his friend Edwards Kobani.\n\nHis intellectual work was interrupted in 1987 when he re-entered the political scene, appointed by the newly installed dictator Ibrahim Babangida to aid the country's transition to democracy. But Saro-Wiwa soon resigned because he felt Babangida's supposed plans for a return to democracy were disingenuous. Saro-Wiwa's sentiments were proven correct in the coming years, as Babangida failed to relinquish power. In 1993, Babangida annulled Nigeria's general elections that would have transferred power to a civilian government, sparking mass civil unrest and eventually forcing him to step down, at least officially, that same year.\n\nSaro-Wiwa's works include TV, drama and prose writing. His earlier works from 1970s to 1980s are mostly satirical displays that portrays a counter-image of Nigerian society but his later writings were more inspired by political dimensions such as environmental and social justice than satire.\n\n\"Transistor Radio\", one of his best known plays was written for a revue during his university days at Ibadan but still resonated well with Nigerian society and was adapted into a television series. Some of his works drew inspiration from the play. In 1972, a radio version of the play was produced and in 1985, he produced, Basi and Company, a successful screen adaption of the play. Saro-Wiwa included the play in \"Four Farcical Plays\" and \" Basi and Company: Four Television Plays\". \"Basi and company\", an adaptation of \"Transistor Radio\" ran on television from 1985 to 1990. A farcical comedy, the show chronicles city life and is anchored by the protagonist, Basi, a resourceful and street wise character looking for ways to achieve his goal of obtaining millions which always ends to become an illusive mission.\n\nIn 1985, the Biafran Civil War novel \"Sozaboy\" was published. The protagonist's language was written in nonstandard English or what Saro-Wiwa called \"Rotten English\", a hybrid language of pidgin English, standard English and broken English.\n\nIn 1990, Saro-Wiwa began devoting most of his time to human rights and environmental causes, particularly in Ogoniland. He was one of the earliest members of the Movement for the Survival of the Ogoni People (MOSOP), which advocated for the rights of the Ogoni people. The Ogoni Bill of Rights, written by MOSOP, set out the movement's demands, including increased autonomy for the Ogoni people, a fair share of the proceeds of oil extraction, and remediation of environmental damage to Ogoni lands. In particular, MOSOP struggled against the degradation of Ogoni lands by Royal Dutch Shell.\n\nIn 1992, Saro-Wiwa was imprisoned for several months, without trial, by the Nigerian military government.\n\nSaro-Wiwa was Vice Chair of the Unrepresented Nations and Peoples Organization (UNPO) General Assembly from 1993 to 1995. UNPO is an international, nonviolent, and democratic organisation (of which MOSOP is a member). Its members are indigenous peoples, minorities, and unrecognised or occupied territories who have joined together to protect and promote their human and cultural rights, to preserve their environments and to find nonviolent solutions to conflicts which affect them.\n\nIn January 1993, MOSOP organised peaceful marches of around 300,000 Ogoni people – more than half of the Ogoni population – through four Ogoni urban centres, drawing international attention to their people's plight. The same year the Nigerian government occupied the region militarily.\n\nSaro-Wiwa was arrested again and detained by Nigerian authorities in June 1993 but was released after a month.\nOn 21 May 1994 four Ogoni chiefs (all on the conservative side of a schism within MOSOP over strategy) were brutally murdered. Saro-Wiwa had been denied entry to Ogoniland on the day of the murders, but he was arrested and accused of incitement to them. He denied the charges but was imprisoned for over a year before being found guilty and sentenced to death by a specially convened tribunal. The same happened to eight other MOSOP leaders who, along with Saro-Wiwa, became known as the Ogoni Nine.\n\nSome of the defendants' lawyers resigned in protest against the alleged rigging of the trial by the Abacha regime. The resignations left the defendants to their own means against the tribunal, which continued to bring witnesses to testify against Saro-Wiwa and his peers. Many of these supposed witnesses later admitted that they had been bribed by the Nigerian government to support the criminal allegations. At least two witnesses who testified that Saro-Wiwa was involved in the murders of the Ogoni elders later recanted, stating that they had been bribed with money and offers of jobs with Shell to give false testimony, in the presence of Shell's lawyer.\n\nThe trial was widely criticised by human rights organisations and, half a year later, Ken Saro-Wiwa received the Right Livelihood Award for his courage, as well as the Goldman Environmental Prize.\n\nOn 10 November 1995, Saro-Wiwa and the rest of the Ogoni Nine were killed by hanging by military personnel. They were buried in Port Harcourt Cemetery.\n\nIn his 1989 short story \"Africa Kills Her Sun\", Saro-Wiwa in a resigned, melancholic mood, foreshadowed his own execution.\n\nBeginning in 1996, the Center for Constitutional Rights (CCR), EarthRights International (ERI), Paul Hoffman of Schonbrun, DeSimone, Seplow, Harris & Hoffman and other human rights attorneys have brought a series of cases to hold Shell accountable for alleged human rights violations in Nigeria, including summary execution, crimes against humanity, torture, inhumane treatment and arbitrary arrest and detention. The lawsuits are brought against Royal Dutch Shell and Brian Anderson, the head of its Nigerian operation.\n\nThe cases were brought under the Alien Tort Statute, a 1978 statute giving non-US citizens the right to file suits in US courts for international human rights violations, and the Torture Victim Protection Act, which allows individuals to seek damages in the US for torture or extrajudicial killing, regardless of where the violations take place.\n\nThe United States District Court for the Southern District of New York set a trial date of June 2009. On 9 June 2009 Shell agreed to an out-of-court settlement of US$15.5 million to victims' families. However, the company denied any liability for the deaths, stating that the payment was part of a reconciliation process. In a statement given after the settlement, Shell suggested that the money was being provided to the relatives of Saro-Wiwa and the eight other victims, to cover the legal costs of the case and also in recognition of the events that took place in the region. Some of the funding is also expected to be used to set up a development trust for the Ogoni people, who inhabit the Niger Delta region of Nigeria. The settlement was made just days before the trial, which had been brought by Ken Saro-Wiwa's son, was due to begin in New York.\n\nSaro-Wiwa's death provoked international outrage and the immediate suspension of Nigeria from the Commonwealth of Nations, as well as the calling back of many foreign diplomats for consultation. The United States and other countries considered imposing economic sanctions. Other tributes to him include:\n\n\n\n\n\nA collection of handwritten letters by Ken Saro-Wiwa were donated to Maynooth University by Sister Majella McCarron, also in the collection are 27 poems, recordings of visits and meetings with family and friends after Saro-Wiwa's death, a collection of photographs and other documents.\n\nThe letters are now in the Digital Repository of Ireland (DRI).\n\nThe Ken Saro-Wiwa Archive is housed in Special Collections at Maynooth University.\n\n\n\nSaro-Wiwa and his wife Maria had five children, who grew up with their mother in the United Kingdom while their father remained in Nigeria. They include Ken Wiwa and Noo Saro-Wiwa, both journalists and writers, and Noo's twin Zina Saro-Wiwa, a journalist and filmmaker. In addition, Saro-Wiwa had two daughters (Singto & Adele) with another woman. He also had another son, Kwame Saro-Wiwa, who was only 1 year old when his father was executed.\n\n\n\n\n"}
{"id": "6358425", "url": "https://en.wikipedia.org/wiki?curid=6358425", "title": "Lamma Power Station", "text": "Lamma Power Station\n\nLamma Power Station (), informally known as Lamma Island Power Station, is a coal and gas-fired power station in Po Lo Tsui, Lamma Island, Hong Kong. With the installed capacity of 3,736 MW, the power station is the second largest coal-fired power station in Hong Kong after Castle Peak Power Station.\n\nBuilt in 1982 (with several later additions) for Hongkong Electric, the station provides power to Hong Kong Island and Lamma Island. By mid-2006, the total installed capacity of the power station was 3,736 MW.\n\n\n"}
{"id": "1178438", "url": "https://en.wikipedia.org/wiki?curid=1178438", "title": "Large eddy simulation", "text": "Large eddy simulation\n\nLarge eddy simulation (LES) is a mathematical model for turbulence used in computational fluid dynamics. It was initially proposed in 1963 by Joseph Smagorinsky to simulate atmospheric air currents, and first explored by Deardorff (1970). LES is currently applied in a wide variety of engineering applications, including combustion, acoustics, and simulations of the atmospheric boundary layer.\n\nThe simulation of turbulent flows by numerically solving the Navier–Stokes equations requires resolving a very wide range of time and length scales, all of which affect the flow field. Such a resolution can be achieved with direct numerical simulation (DNS), but DNS is computationally expensive, and its cost prohibits simulation of practical engineering systems with complex geometry or flow configurations, such as turbulent jets, pumps, vehicles, and landing gear.\n\nThe principal idea behind LES is to reduce the computational cost by ignoring the smallest length scales, which are the most computationally expensive to resolve, via low-pass filtering of the Navier–Stokes equations. Such a low-pass filtering, which can be viewed as a time- and spatial-averaging, effectively removes small-scale information from the numerical solution. This information is not irrelevant, however, and its effect on the flow field must be modeled, a task which is an active area of research for problems in which small-scales can play an important role, such as near-wall flows \n, reacting flows, and multiphase flows.\n\nAn LES filter can be applied to a spatial and temporal field formula_1 and perform a spatial filtering operation, a temporal filtering operation, or both. The filtered field, denoted with a bar, is defined as:\n\nwhere formula_3 is the filter convolution kernel. This can also be written as:\n\nThe filter kernel formula_3 has an associated cutoff length scale formula_6 and cutoff time scale formula_7. Scales smaller than these are eliminated from formula_8. Using the above filter definition, any field formula_9 may be split up into a filtered and sub-filtered (denoted with a prime) portion, as\n\nIt is important to note that the large eddy simulation filtering operation does not satisfy the properties of a Reynolds operator.\n\nThe governing equations of LES are obtained by filtering the partial differential equations governing the flow field formula_11. There are differences between the incompressible and compressible LES governing equations, which lead to the definition of a new filtering operation.\n\nFor incompressible flow, the continuity equation and Navier–Stokes equations are filtered, yielding the filtered incompressible continuity equation,\n\nand the filtered Navier–Stokes equations,\n\nwhere formula_14 is the filtered pressure field and formula_15 is the rate-of-strain tensor. The nonlinear filtered advection term formula_16 is the chief cause of difficulty in LES modeling. It requires knowledge of the unfiltered velocity field, which is unknown, so it must be modeled. The analysis that follows illustrates the difficulty caused by the nonlinearity, namely, that it causes interaction between large and small scales, preventing separation of scales.\n\nThe filtered advection term can be split up, following Leonard (1974), as:\n\nwhere formula_18 is the residual stress tensor, so that the filtered Navier Stokes equations become\n\nwith the residual stress tensor formula_18 grouping all unclosed terms. Leonard decomposed this stress tensor as formula_21 and provided physical interpretations for each term. formula_22, the Leonard tensor, represents interactions among large scales, formula_23, the Reynolds stress-like term, represents interactions among the sub-filter scales (SFS), and formula_24, the Clark tensor, represents cross-scale interactions between large and small scales. Modeling the unclosed term formula_18 is the task of SFS models (also referred to as sub-grid scale, or SGS, models). This is made challenging by the fact that the sub-filter scale stress tensor formula_18 must account for interactions among all scales, including filtered scales with unfiltered scales.\n\nThe filtered governing equation for a passive scalar formula_9, such as mixture fraction or temperature, can be written as\n\nwhere formula_29 is the diffusive flux of formula_9, and formula_31 is the sub-filter stress tensor for the scalar formula_9. The filtered diffusive flux formula_33 is unclosed, unless a particular form is assumed for it (e.g. a gradient diffusion model formula_34). formula_31 is defined analogously to formula_18,\n\nand can similarly be split up into contributions from interactions between various scales. This sub-filter tensor also requires a sub-filter model.\n\nUsing Einstein notation, the Navier–Stokes equations for an incompressible fluid in Cartesian coordinates are\n\nFiltering the momentum equation results in\n\nIf we assume that filtering and differentiation commute, then\n\nThis equation models the changes in time of the filtered variables formula_42. Since the unfiltered variables formula_43 are not known, it is impossible to directly calculate formula_44. However, the quantity formula_45 is known. A substitution is made:\n\nLet formula_47. The resulting set of equations are the LES equations:\n\nFor the governing equations of compressible flow, each equation, starting with the conservation of mass, is filtered. This gives:\n\nwhich results in an additional sub-filter term. However, it is desirable to avoid having to model the sub-filter scales of the mass conservation equation. For this reason, Favre proposed a density-weighted filtering operation, called Favre filtering, defined for an arbitrary quantity formula_9 as:\n\nwhich, in the limit of incompressibility, becomes the normal filtering operation. This makes the conservation of mass equation:\n\nThis concept can then be extended to write the Favre-filtered momentum equation for compressible flow. Following Vreman:\n\nwhere formula_54 is the shear stress tensor, given for a Newtonian fluid by:\n\nand the term formula_56 represents a sub-filter viscous contribution from evaluating the viscosity formula_57 using the Favre-filtered temperature formula_58. The subgrid stress tensor for the Favre-filtered momentum field is given by\n\nBy analogy, the Leonard decomposition may also be written for the residual stress tensor for a filtered triple product formula_60. The triple product can be rewritten using the Favre filtering operator as formula_61, which is an unclosed term (it requires knowledge of the fields formula_9 and formula_63, when only the fields formula_64 and formula_65 are known). It can be broken up in a manner analogous to formula_16 above, which results in a sub-filter stress tensor formula_67. This sub-filter term can be split up into contributions from three types of interactions: the Leondard tensor formula_22, representing interactions among resolved scales; the Clark tensor formula_24, representing interactions between resolved and unresolved scales; and the Reynolds tensor formula_23, which represents interactions among unresolved scales.\n\nIn addition to the filtered mass and momentum equations, filtering the kinetic energy equation can provide additional insight. The kinetic energy field can be filtered to yield the total filtered kinetic energy:\n\nand the total filtered kinetic energy can be decomposed into two terms: the kinetic energy of the filtered velocity field formula_72,\n\nand the residual kinetic energy formula_74,\n\nsuch that formula_76.\n\nThe conservation equation for formula_72 can be obtained by multiplying the filtered momentum transport equation by formula_78 to yield:\n\nwhere formula_80 is the dissipation of kinetic energy of the filtered velocity field by viscous stress, and formula_81 represents the sub-filter scale (SFS) dissipation of kinetic energy.\n\nThe terms on the left-hand side represent transport, and the terms on the right-hand side are sink terms that dissipate kinetic energy.\n\nThe formula_82 SFS dissipation term is of particular interest, since it represents the transfer of energy from large resolved scales to small unresolved scales. On average, formula_82 transfers energy from large to small scales. However, instantaneously formula_82 can be positive \"or\" negative, meaning it can also act as a source term for formula_72, the kinetic energy of the filtered velocity field. The transfer of energy from unresolved to resolved scales is called backscatter (and likewise the transfer of energy from resolved to unresolved scales is called forward-scatter).\n\nLarge eddy simulation involves the solution to the discrete filtered governing equations using computational fluid dynamics. LES resolves scales from the domain size formula_86 down to the filter size formula_6, and as such a substantial portion of high wave number turbulent fluctuations must be resolved. This requires either high-order numerical schemes, or fine grid resolution if low-order numerical schemes are used. Chapter 13 of Pope addresses the question of how fine a grid resolution formula_88 is needed to resolve a filtered velocity field formula_89. Ghosal found that for low-order discretization schemes, such as those used in finite volume methods, the truncation error can be the same order as the subfilter scale contributions, unless the filter width formula_6 is considerably larger than the grid spacing formula_88. While even-order schemes have truncation error, they are non-dissipative, and because subfilter scale models are dissipative, even-order schemes will not affect the subfilter scale model contributions as strongly as dissipative schemes.\n\nThe filtering operation in large eddy simulation can be implicit or explicit. Implicit filtering recognizes that the subfilter scale model will dissipate in the same manner as many numerical schemes. In this way, the grid, or the numerical discretization scheme, can be assumed to be the LES low-pass filter. While this takes full advantage of the grid resolution, and eliminates the computational cost of calculating a subfilter scale model term, it is difficult to determine the shape of the LES filter that is associated with some numerical issues. Additionally, truncation error can also become an issue.\n\nIn explicit filtering, an LES filter is applied to the discretized Navier–Stokes equations, providing a well-defined filter shape and reducing the truncation error. However, explicit filtering requires a finer grid than implicit filtering, and the computational cost increases with formula_92. Chapter 8 of Sagaut (2006) covers LES numerics in greater detail.\n\nInlet boundary conditions affect the accuracy of LES significantly, and the treatment of inlet conditions for LES is a complicated problem. Theoretically, a good boundary condition for LES should contain the following features: \n\n(1) providing accurate information of flow characteristics, i.e. velocity and turbulence; \n\n(2) satisfying the Navier-Stokes equations and other physics;\n\n(3) being easy to implement and adjust to different cases. \n\nCurrently, methods of generating inlet conditions for LES are broadly divided into two categories classified by Tabor et al.: \n\nThe first method for generating turbulent inlets is to synthesize them according to particular cases, such as Fourier techniques, principle orthogonal decomposition (POD) and vortex methods. The synthesis techniques attempt to construct turbulent field at inlets that have suitable turbulence-like properties and make it easy to specify parameters of the turbulence, such as turbulent kinetic energy and turbulent dissipation rate. In addition, inlet conditions generated by using random numbers are computationally inexpensive. However, one serious drawback exists in the method. The synthesized turbulence does not satisfy the physical structure of fluid flow governed by Navier-Stokes equations. \n\nThe second method involves a separate and precursor calculation to generate a turbulent database which can be introduced into the main computation at the inlets. The database (sometimes named as ‘library’) can be generated in a number of ways, such as cyclic domains, pre-prepared library, and internal mapping. However, the method of generating turbulent inflow by precursor simulations requires large calculation capacity. \n\nResearchers examining the application of various types of synthetic and precursor calculations have found that the more realistic the inlet turbulence, the more accurate LES predicts results.\n\nTo discuss the modeling of unresolved scales, first the unresolved scales must be classified. They fall into two groups: resolved sub-filter scales (SFS), and sub-grid scales(SGS).\n\nThe resolved sub-filter scales represent the scales with wave numbers larger than the cutoff wave number formula_93, but whose effects are dampened by the filter. Resolved sub-filter scales only exist when filters non-local in wave-space are used (such as a box or Gaussian filter). These resolved sub-filter scales must be modeled using filter reconstruction.\n\nSub-grid scales are any scales that are smaller than the cutoff filter width formula_6. The form of the SGS model depends on the filter implementation. As mentioned in the Numerical methods for LES section, if implicit LES is considered, no SGS model is implemented and the numerical effects of the discretization are assumed to mimic the physics of the unresolved turbulent motions.\n\nWithout a universally valid description of turbulence, empirical information must be utilized when constructing and applying SGS models, supplemented with fundamental physical constraints such as Galilean invariance\nTwo classes of SGS models exist; the first class is functional models and the second class is structural models. Some models may be categorized as both.\n\nFunctional models are simpler than structural models, focusing only on dissipating energy at a rate that is physically correct. These are based on an artificial eddy viscosity approach, where the effects of turbulence are lumped into a turbulent viscosity. The approach treats dissipation of kinetic energy at sub-grid scales as analogous to molecular diffusion. In this case, the deviatoric part of formula_95 is modeled as:\n\nwhere formula_97 is the turbulent eddy viscosity and formula_98 is the rate-of-strain tensor.\n\nBased on dimensional analysis, the eddy viscosity must have units of formula_99. Most eddy viscosity SGS models model the eddy viscosity as the product of a characteristic length scale and a characteristic velocity scale.\n\nThe first SGS model developed was the Smagorinsky–Lilly SGS model, which was developed by Smagorinsky and used in the first LES simulation by Deardorff. It models the eddy viscosity as:\n\nwhere formula_101 is the grid size and formula_102 is a constant.\n\nThis method assumes that the energy production and dissipation of the small scales are in equilibrium - that is, formula_103.\n\nGermano et al. identified a number of studies using the Smagorinsky model that each found different values for the Smagorinsky constant formula_102 for different flow configurations. In an attempt to formulate a more universal approach to SGS models, Germano et al. proposed a dynamic Smagorinsky model, which utilized two filters: a grid LES filter, denoted formula_105, and a test LES filter, denoted formula_106. In this case, the resolved turbulent stress tensor formula_107 is defined as\n\nwhich is also called the Germano identity. The quantity formula_109 is the residual stress tensor for the test filter scale, and formula_110 is the residual stress tensor for the grid filter, then test filtered.\n\nformula_107 represents the contribution to the SGS stresses by length scales smaller than the test filter width formula_112 but larger than the grid filter width formula_113. The dynamic model then finds the coefficient that best complies with the Germano identity.\nHowever, since the identity is a tensorial equation, it is overdetermined (five equations for one unknown), prompting Lilly\n\nto propose a minimum least-square error method that leads to an equation for formula_102:\n\nwhere\n\nHowever, this procedure was numerically unstable since the numerator could become negative and large fluctuations in formula_102 were often observed. Hence, additional averaging of the error in the minimization is often employed, leading to:\n\nThis has made the dynamic model more stable and making the method more widely applicable. Inherent in the procedure is the assumption that the coefficient formula_102 is invariant of scale (see review\n). The averaging can be a spatial averaging over directions of statistical homogeneity (e.g. volume for homogeneous turbulence or wall-parallel planes\nfor channel flow as originally used in Germano et al.), or time following Lagrangian fluid trajectories\n\n"}
{"id": "25754563", "url": "https://en.wikipedia.org/wiki?curid=25754563", "title": "List of Danish wind turbine manufacturers", "text": "List of Danish wind turbine manufacturers\n\nList of Danish wind turbine manufacturers.\n\n\n\n"}
{"id": "1973352", "url": "https://en.wikipedia.org/wiki?curid=1973352", "title": "Low birth weight", "text": "Low birth weight\n\nLow birth weight (LBW) is defined by the World Health Organization as a birth weight of a\ninfant of 2,499 g or less, regardless of gestational age. Subcategories include very low birth weight (VLBW), which is less than 1500 g (3 pounds 5 ounces), and extremely low birth weight (ELBW), which is less than 1000 g (2 pounds 3 ounces). Normal weight at term delivery is 2500–4200 g (5 pounds 8 ounces – 9 pounds 4 ounces).\n\nLBW is either caused by preterm birth (that is, a low gestational age at birth, commonly defined as younger than 37 weeks of gestation) or the infant being small for gestational age (that is, a slow prenatal growth rate), or a combination of both.\n\nIn general, risk factors in the mother that may contribute to low birth weight include young ages, multiple pregnancies, previous LBW infants, poor nutrition, heart disease or hypertension, untreated coeliac disease, drug addiction, alcohol abuse, and insufficient prenatal care. Environmental risk factors include smoking, lead exposure, and other types of air pollutions.\n\nFour different pathways have been identified that can result in preterm birth and have considerable evidence: precocious fetal endocrine activation, uterine overdistension, decidual bleeding, and intrauterine inflammation/infection. From a practical point a number of factors have been identified that are associated with preterm birth, however, an association does not establish causality.\n\nBeing small for gestational age can be constitutional, that is, without an underlying pathological cause, or it can be secondary to intrauterine growth restriction, which, in turn, can be secondary to many possible factors. For example, babies with congenital anomalies or chromosomal abnormalities are often associated with LBW. Problems with the placenta can prevent it from providing adequate oxygen and nutrients to the fetus. Infections during pregnancy that affect the fetus, such as rubella, cytomegalovirus, toxoplasmosis, and syphilis, may also affect the baby's weight.\n\nWhile active maternal tobacco smoking has well established adverse perinatal outcomes such as LBW, that mothers who smoke during pregnancy are twice as likely to give birth to low-birth weight infants. Review on the effects of passive maternal smoking, also called environmental tobacco exposure (ETS), demonstrated that increased risks of infants with LBW were more likely to be expected in ETS-exposed mothers.\n\nRegarding environmental toxins in pregnancy, elevated blood lead levels in pregnant women, even those well below 10 ug/dL can cause miscarriage, premature birth, and LBW in the offspring. With 10 ug/dL as the Centers for Disease Control and Prevention's “level of concern”, this cut-off value really needs to arise more attentions and implementations in the future.\n\nThe combustion products of solid fuel in developing countries can cause many adverse health issues in people. Because a majority of pregnant women in developing countries, where rate of LBW is high, are heavily exposed to indoor air pollution, increased relative risk translates into substantial population attributable risk of 21% of LBW.\n\nOne environmental exposure which has been found to increase the risk of low birth weight is particulate matter, a component of ambient air pollution. Because particulate matter is composed of extremely small particles, even nonvisible levels can be inhaled and present harm to the fetus. Particulate matter exposure can cause inflammation, oxidative stress, endocrine disruption, and impaired oxygen transport access to the placenta, all of which are mechanisms for heightening the risk of low birth weight. To reduce exposure to particulate matter, pregnant women can monitor the EPA’s Air Quality Index and take personal precautionary measures such as reducing outdoor activity on low quality days, avoiding high-traffic roads/intersections, and/or wearing personal protective equipment (i.e., facial mask of industrial design). Indoor exposure to particulate matter can also be reduced through adequate ventilation, as well as use of clean heating and cooking methods.\n\nA correlation between maternal exposure to CO and low birth weight has been reported that the effect on birth weight of increased ambient CO was as large as the effect of the mother smoking a pack of cigarettes per day during pregnancy. \nIt has been revealed that adverse reproductive effects (e.g., risk for LBW) were correlated with maternal exposure to air pollution combustion emissions in Eastern Europe and North America.\nMercury is a known toxic heavy metal that can harm fetal growth and health, and there has been evidence showing that exposure to mercury (via consumption of large oily fish) during pregnancy may be related to higher risks of LBW in the offspring.\n\nIt was revealed that, exposure of pregnant women to airplane noise was found to be associated with low birth weight. Aircraft noise exposure caused adverse effects on fetal growth leading to low birth weight and preterm infants.\n\nLow birthweight, pre-term birth and pre-eclampsia have been associated with maternal periodontitis exposure. But the strength of the observed associations is inconsistent and vary according to the population studied, the means of periodontal assessment and the periodontal disease classification employed. However the best is that the risk of low birth weight can be reduced with very simple therapy. Treatment of periodontal disease during gestation period is safe and reduction in inflammatory burden reduces the risk of preterm birth as well as low birth weight.\n\nLBW is closely associated with fetal and Perinatal mortality and Morbidity, inhibited growth and cognitive development, and chronic diseases later in life. At the population level, the proportion of babies with a LBW is an indicator of a multifaceted public-health problem that includes long-term maternal malnutrition, ill health, hard work and poor health care in pregnancy. On an individual basis, LBW is an important predictor of newborn health and survival and is associated with higher risk of infant and childhood mortality.\n\nLow birth weight constitutes as sixty to eighty percent of the infant mortality rate in developing countries. Infant mortality due to low birth weight is usually directly causal, stemming from other medical complications such as preterm birth, poor maternal nutritional status, lack of prenatal care, maternal sickness during pregnancy, and an unhygienic home environment. According to an analysis by University of Oregon, reduced brain volume in children is also tied to low birth-weight.\n\nA study by the Agency for Healthcare Research and Quality (AHRQ) found that of the 3.8 million births that occurred in the United States in 2011, approximately 6.1% (231,900) were diagnosed with low birth weight (<2,500 g). Approximately 49,300 newborns (1.3%) weighed less than 1,500 grams (VLBW). Infants born at low birth weight are at a higher risk for developing neonatal infection.\n"}
{"id": "11131199", "url": "https://en.wikipedia.org/wiki?curid=11131199", "title": "MERLIN reactor", "text": "MERLIN reactor\n\nMERLIN reactor was a 10MWt pool-type research reactor at Aldermaston Court, Aldermaston, Berkshire, England which operated from 6 November 1959 until 1962.\n\nIt was privately owned and operated by Associated Electrical Industries. It was opened by Prince Philip on 6 November 1959. The head of the reactor was Dr Alan James Salmon, then aged 36 who had worked for AEI since leaving the RAF and had spent two years at the Atomic Energy Research Establishment studying reactor design before joining AEI Aldermaston.\n\n"}
{"id": "7320339", "url": "https://en.wikipedia.org/wiki?curid=7320339", "title": "Nitrogen rejection unit", "text": "Nitrogen rejection unit\n\nA nitrogen rejection unit (NRU) selectively removes nitrogen from a gas. The name can be applied to any system that removes nitrogen from natural gas.\n\nFor high flow-rate applications, typically >15 MMSCFD, cryogenic processing is the norm. This is a distillation process which utilizes the different volatilities of methane (boiling point of −161.6 °C) and nitrogen (boiling point of −195.69 °C) to achieve separation. In this process, a system of compression and distillation columns drastically reduces the temperature of the gas mixture to a point where methane is liquified and the nitrogen is not. For smaller applications, a series of heat exchangers may be used as an alternative to distillation columns.\n\nFor smaller volumes of gas, a system utilizing Pressure Swing Adsorption (PSA) is a more typical method of separation. In PSA, methane and nitrogen can be separated by using an adsorbent with an aperture size very close to the molecular diameter of the larger species, in this case methane (3.8 angstroms). This means nitrogen is able to diffuse through the adsorbent, filling adsorption sites, whilst methane is not. This results in a purified natural gas stream that fits pipeline specifications. The adsorbent can then be regenerated, leaving a highly pure nitrogen stream. PSA is a flexible method for nitrogen rejection, being applied to both small and large flow rates.\n\nThe operating conditions of various PSA units are quite variable. Depending on the vendor, high degrees of pretreatment of the gas stream (removal of water vapor and heavy hydrocarbons) may be necessary for the system to operate optimally and without damage to the adsorbent material. Moreover, the degree of hydrocarbon recoveries (75% vs 95%) and purities can vary considerably. The economic viability of any PSA unit will be highly dependent on such factors.\n\nAn estimated 25% of the US natural gas reserves contain unacceptably large quantities of nitrogen. Nitrogen is inert and lowers the energy value per volume of natural gas. It also takes up capacity in pipelines that could be used for valuable methane.\n\nPipeline specifications for nitrogen are extremely variable, though \"no more than 4% nitrogen\" seems to be the most typical specification.\n\n\n"}
{"id": "41783067", "url": "https://en.wikipedia.org/wiki?curid=41783067", "title": "ONE Gas", "text": "ONE Gas\n\nONE Gas, Inc. is a stand-alone, 100 percent regulated, publicly traded natural gas utility and is one of the largest natural gas utilities in the United States. \n\nONE Gas provides natural gas distribution services to more than 2 million customers in Oklahoma, Kansas and Texas. Headquartered in Tulsa, Oklahoma, it comprises three operating companies–Oklahoma Natural Gas, Kansas Gas Service, and Texas Gas Service. Its service territory covers most of Oklahoma, much of the eastern half of Kansas, and several disparate portions of Texas.\n\nIts largest natural gas distribution markets by customer count are Oklahoma City and Tulsa, Okla.; Kansas City, Wichita and Topeka, Kan.; and Austin and El Paso, Texas. ONE Gas serves residential, commercial, industrial, transportation and wholesale customers in all three states. It is the largest natural gas distributor in Oklahoma and Kansas, and the third-largest in Texas, in terms of customer count.\n\nONE Gas was founded in February 2014 when ONEOK spun off its distribution subsidiaries. In effect, ONEOK was spinning out the core of the original Oklahoma Natural Gas Company, which was founded in 1906 and changed its name to ONEOK in 1980. \n\n"}
{"id": "27921554", "url": "https://en.wikipedia.org/wiki?curid=27921554", "title": "Omnishale process", "text": "Omnishale process\n\nOmnishale process (also known as the \"Petro Probe process\") is an \"in situ\" shale oil extraction technology to convert kerogen in oil shale to shale oil. This process is classified as an externally generated hot gas technology. The technology is developed by General Synfuels International, a subsidiary of Earth Search Sciences.\n\nThe Omnishale shale oil extraction \"in situ\" technology was invented by Ron McQueen and developed and tested by Petro Probe and General Synfuels International. On 11 October 2005, a subsidiary of Earth Search Sciences Petro Probe acquired an unlimited license to use Omnishale shale oil extraction technology developed by General Synfuels International, Inc. On 15 August 2008, Earth Search Sciences acquired General Synfuels International and oil shale activities of Petro Probe were transferred to General Synfuels International. In 2009, General Synfuels International started a cooperation with Anadarko Petroleum. On 30 March 2010, General Synfuels International started construction of the test plant on the Anadarko-controlled test site near Rock Springs, Wyoming, in the Green River Formation. In April 2010, a $10 million Patriot's Oil Shale Technology Fund L.P. was formed to complete funding of the Phase 1 construction of the test shale oil plant.\n\nThe Omnishale shale oil extraction \"in situ\" technology of Earth Search Sciences was invented by Ron McQueen and developed by Petro Probe and General Synfuels International. In this process, holes are drilled into the oil shale formation and a processing inlet conduit is placed within holes. Pressurized air super heated by an above-ground combustor is directed into the oil shale formation through the inlet conduit. As a result, the kerogen in oil shale is heated and converted to a gaseous state.\n"}
{"id": "49885908", "url": "https://en.wikipedia.org/wiki?curid=49885908", "title": "Open VOGEL", "text": "Open VOGEL\n\nOpen VOGEL is an open source computer program intended for the simulation of aerodynamic problems through the Unsteady Vortex Lattice Method and first order singularity panels (vortex rings, flat doublet panels and flat source/sink panels).\nThe code has been fully developed in the .NET framework using Visual Studio Express and it is published under General Public License, GPLv3.\n\nThe Open VOGEL framework consists in a library of models (which includes slender surfaces, fuselages and other components), a set of winforms-based GUI tools, a calculation core with generalized definitions based in potential flow and a main frame that manages all the components.\n\nThe novel feature of Open VOGEL is that users can add their own models to the library, and expand that way the general capabilities of the software or adapt it for a specific purpose. In that sense, the software acts as a basic framework for research applications. \nAny university, research facility or individual in the world is authorized to take the source code, improve it and release it under GPLv3 for further use in the aeronautical community.\n\nThe idea behind this way of working is that those who use the software for practical applications can directly benefit from what researchers are achieving, so that their results can be directly put into practice.\n\nOne of the key features of Open VOGEL is that it provides built-in tools intended to facilitate the creation of geometrical models. Each component is defined in a parametric way, and the software resolves the geometry to generate a suitable mesh optimized for the calculation core.\n\nOpen VOGEL currently allows three types of models: slender lifting surfaces, closed fuselages and jet engine nacelles (tubular cylinders).\n\nSlender surfaces are bounded lattices specifically intended to simulate wings. The actual model can be viewed as a paper sheet, but their behavior approaches very well to that of thin wings. Open VOGEL employs this kind of surfaces because they require less than a half of the number of panels required to generate a full 3D model, while they achieve very good results in the prediction of the main air-loads (lift, induced drag and parasitic drag through an experimental polar curve).\n\nOpen VOGEL lets designers construct wings by shaping a set of adjacent quadrilateral macro-panels. The software contains a built-in tool to manage the main geometrical parameters (such as dihedral, sweepback, profile and twisting) of each of these macro-panels. This allows users to recreate a very wide variety of models.\n\nLifting surfaces are modeled with an structured grid characterized by a constant number of chord-wise panels. The user is allowed to refine the mesh by changing the number of chord-wise and span-wise panels.\nWhen lifting surfaces are introduced to the calculation model, they are modeled as slender panels. This means that Neumann's boundary conditions (BC) are imposed, and that the lift becomes dependent on the change in tangent velocity across the panel.\n\nFuselages in Open VOGEL are lofted surfaces generated from a collection of parallel cross-sections distributed in a longitudinal direction. The resulting mesh is generated by connecting nodal points uniformly distributed around the outline of several virtual cross sections, which are computed by linear interpolation between the user-defined cross-sections.\n\nOne or more lifting surfaces can be attached to a fuselage by means of features called \"anchors\". An anchor is a set of slender panels that belong to the fuselage and serve as a bridge between the root of a wing and the panels of the fuselage. Anchors are needed to provide a transition in the circulation of both surfaces, i.e., to properly model a physical connection between both of them. Without anchors, the gap between the two surfaces would leak, creating regions of very high airspeed and a consequent wrong prediction of the air-loads.\n\nThe orientation and position of fuselages can be changed as with any other surface, but if one or more lifting surfaces are attached to it, the anchors will only work properly if the fuselage is kept in the original position and orientation (no translation nor rotation applied).\nAnchoring two or more surfaces to a fuselage is only possible when the surfaces are sufficiently staggered so that they don't share any plane transverse to the fuselage. This limitation is a pure consequence of the meshing technique. When meshing a fuselage, each anchored lifting surface is assigned to a longitudinal chunk of fuselage that is meshed apart, so that the number of longitudinal panels in that longitudinal band matches the number of chord-wise panels of the wing. This method has been represented in the picture here beside.\n\nFan ducts (or jet-engine nacelles) can also be added to the model. In the calculation core these surfaces are managed as slender surfaces, with the exception that drag is not computed.\n\nBecause Open VOGEL is based in the Unsteady Vortex Lattice Method, it can simulate steady and unsteady problems. The unsteady solver is not only used to simulate gusts, but it is the core of the aeroelastic module. By linking a structural FE model with the UVLM, Open VOGEL can also simulate the unsteady transit of flexible wings and the deformation of wings in the steady state.\n\nOpen VOGEL relies on an object-oriented multi-threading calculation core (CC). The aerodynamic part of the CC (the ACC) is based in hierarchical structure of classes that begins with the definition of \"vortex rings\", which in turn, can be assembled together as bricks to form a \"lattice\".\n\nLattices can be of two types: \"bounded\" or \"free\". Bounded lattices represent all solid boundaries where boundary conditions have to be imposed, while free lattices represent the wakes shed by the bounded lattices. Because of this parent-child relation, each bounded lattice contains a stack of wakes that can be automatically shed by its parent bounded lattice based on information about the primitive shedding edges.\n\nBoundary conditions are imposed at vortex ring level. Vortex rings can be triangular or quadrilateral, but most importantly, they can be slender or solid. On slender rings, Neumann boundary conditions are imposed, while on solid rings, Dirichlet boundary conditions are imposed. Imposing one BC or the other implies calculating the local velocity or the local velocity potential.\n\nThe calculation of the velocity and the velocity potential at a given point is encapsulated inside the class definition of the vortex rings. The method that is actually implemented differ from the topology of the inducing ring (triangular or quadrilateral). Each lattice exposes then a method that allows the same calculation at lattice level by running over all contained rings.\n\nThe ACC will impose the boundary conditions by solving a system of linear equations. In a rigid model-problem, it will build a unique matrix of coefficients, generate the LU decomposition, and reuse that same matrices to find the circulation in the rings at every time step. For aeroelastic simulations, however, because the relative position of the rings is permanently changing due to the deformation of the lattices, the CC will recalculate the influence matrix and LU decomposition at each time step.\n\nThe right hand side of the system of equations containing the free-velocity and source-potential terms, is only updated at each time for unsteady problems.\n\nBecause Open VOGEL deals with several types of panels and boundary conditions, the math and the implemented algorithms are a bit more complex than those required when only working with vortex rings.\n\nThe basic problem when dealing with vortex rings is to find out the circulation associated to the rings where the non-penetration boundary condition has to be imposed (the bounded lattices). Because the velocity associated to a vortex ring at a given point depends linearly on the circulation of that ring, a system of linear equations can be written, so that when solved, it will provide the value of the local circulations that will cancel the normal velocity at each control point. The matrix problem for this situation can be found in the article about the vortex lattice method.\n\nNow, when closed bodies such as fuselages need to be modeled, the total potential inside the body is required to equal a given constant value (typically chosen as zero).\nThis is a different kind of problem that requires, first of all, calculating the potential of sink/source and doublet panels at a given point located immediately under the surface of the body, and secondly, arranging the new system of equations in the global matrix problem.\n\nThe calculation of the velocity potential associated to constant sink/source panels and vortex rings (constant doublets) at a given point in space is quite complex. Open VOGEL reduces the complexity level by projecting the quadrilateral panels in its normal direction, so that they can be represented by a flat panel (triangular panels do not need to be projected since they are always flat). The calculation of the velocity potential under such situation can be found in this reference. Of course, this method has some associated leakage, which depends on how well panels are approach by their flat counterpart.\n\nWhen working with sink/source panels, the intensity vector formula_1 associated to them is not an unknown. In fact, when the internal potential is targeted to zero, their intensity depends on the value of the normal free air-stream velocity:\n\nformula_2\n\nTherefore, the unit potential corresponding to each one of them at each control point can be calculated once and stored in a source/sink potential matrix formula_3. This matrix is then reused to compute the total sink/source potential when necessary as follows:\n\nformula_4\n\nThe unknowns of the Dirichlet problem are the doublet intensities of each panel surrounding the body, represented here by vector formula_5. Similarly to the sources, we can write a matrix of unit-intensity potential associated to them:\n\nformula_6\n\nFinally, the potential associated to the wakes needs to be added. Because wakes are constantly changing their shape, in place of building a matrix, we rather include their potential as a summation of individual contributions:\n\nformula_7\n\nBecause wakes are shed from lifting surfaces, their potential is only associated to constant doublet panels of known intensity. When wake panels are far enough from the body, they can be approached by point doublets, which have much simpler potential functions. This solution is known as \"the far field potential\".\n\nThe Dirichlet problem is then represented by the next system or linear equations\n\nformula_8\n\nwhich needs to be assembled and solved together with the Neumann problem in a global matrix problem.\n\nWhen the velocity or velocity-potential has to be calculated at all control points in a bounded lattice, a multi-threading operation is started: for each control point in the targeting lattice, all rings from all bounded lattices and associated wakes are run over in an isolated thread. This multi-threading system allows running several vortex rings simultaneously, what considerably reduces the calculation time in multiple-cores machines. The reduction of time is proportional to the number of processors in the machine.\n\nThe next lines of code show how the parallelization works.\n\nThe whole code is located in a GitHub repository.\n\nOpen VOGEL counts with an integrated aeroelastic module that allows treating the interaction between a beam-based structure and a slender surface.\nThis interaction is simulated by dynamic mode decomposition and numerical integration in the time-domain.\n"}
{"id": "15456253", "url": "https://en.wikipedia.org/wiki?curid=15456253", "title": "Paint sealant", "text": "Paint sealant\n\nA paint sealant is a sealant that protects cars from ultraviolet rays and acid rain. Paint sealants protect cars' finishes, and can make cars shiny. There are synthetic sealants and carnauba waxes.\n\nSalt may be a factor that many protective sealants do not defend against, and thus salt water may break down protective layers to get at the metal and corrode it.\n\nPaint sealant works by filling into the pores and irregular surface of the body thereby creating a smooth finish on top. The way it helps is it denies a sticking surface to foreign substance and they come off the car easily without further damaging the car surface\n"}
{"id": "44537209", "url": "https://en.wikipedia.org/wiki?curid=44537209", "title": "Pitch-based carbon fiber", "text": "Pitch-based carbon fiber\n\nCarbon fiber is often time produced using two main methods: through the use of Polyacrylonitrile (PAN) and from pitch. Pitch is a viscoelastic material that is composed of aromatic hydrocarbons. Pitch is produced via the distillation of carbon-based materials, such as plants, crude oil, and coal. Pitch is isotropic, but can be made anisotropic through the use of heat treatments. However, the most important in carbon fiber production is mesophase pitch due to the ability to melt spin anisotropic mesophase pitch without filament breakage.\n\nThe mesophase pitch forms a thermotropic crystal, which allows the pitch to become organized and form linear chains without the use of tension. Mesophase pitch is made by polymerizing isotropic pitch to a higher molecular weight. The melting point for the mesophase pitch is roughly 300 °C. An advantage in the production of Pitch carbon fibers over PAN carbon fibers is that Pitch carbon fibers do not require constant tension on the fibers at all processing stages. Pitch based carbon fibers have been found to be more sheet-like in their crystal structure, as opposed to PAN based carbon fibers, which are more granular.\n\nThere are four main steps in the production of carbon fiber from pitch 1) melt spinning 2) oxidization/precarbonization 3) carbonization and 4) graphitization.\n\n1) Melt spinning is the method of forming fibers through the rapid cooling of a melt; due to the fast rates of cooling, the mesophase pitch is able to become highly oriented. Mesophase pitch can be melt spun, but because of its flow characteristics the process can be difficult. The viscosity of mesophase pitch is more sensitive to temperature than other melt-spun materials. Therefore, during the creation of pitch based fibers the temperature and heat transfer rate must be carefully controlled.\n\n2) Oxidization/Precarbonization is used in order to cross-link the fibers to the point where they cannot be melted or fused together. This step is extremely important because it produces fibers that are stable at the high temperatures of carbonization and graphitization; otherwise, the fibers would fail in those steps of the process.\n\n3) Carbonization is the process removing all nonorganic elements. In the case of carbon fibers, all elements except for carbon are removed. This is achieved by heating the fibers to high temperatures in an environment without oxygen. This step removes all impurities from the fibers and leaves crystalline carbon structures. These structures are mostly hexagonal in shape and are composed of entirely carbon.\n\n4) Graphitization is the process of treating the fibers at high temperatures in order to improve the alignment and orientation of the crystalline regions along the fiber direction [1,8]. Having the crystalline regions aligned, stacked, and oriented along the fiber direction increases the overall strength of the carbon fiber.\n\nThe high strength of carbon fiber can be attributed to these four main processes. Having high levels of crystalline regions allows the fibers to withstand high levels of stress. These crystalline regions are formed via the melt spinning process; the crystals are stiff areas that do not deform when an external stress is applied. Orienting and aligning these crystalline regions gives further strength to the fibers, specifically if the orientation is along the fiber axis. Carbonization and graphitization are the two processes responsible for this alignment of the crystalline regions. Pitch based carbon fiber is lower in strength than fiberglass; however, it has a very high elastic modulus.\n\nPitch-based carbon fibers have various end uses due to their high strength. These fibers are used within the aerospace industry due to their high modulus and higher price. These fibers could be used within the automotive and sports industries, but the cheaper PAN based carbon fibers provide a high enough strength for these applications.\n"}
{"id": "54598531", "url": "https://en.wikipedia.org/wiki?curid=54598531", "title": "Pitch bearing", "text": "Pitch bearing\n\nThe pitch bearing, also named blade bearing, is a component of modern wind turbines which connect the rotor hub and the rotor blade. The bearing allows the required oscillation to control the loads and power of the wind turbine. The pitch system brings the blade to the desired position by adapting the aerodynamic angle of attack. The pitch system is also used for emergency breaks of the turbine system.\n\nMostly large rolling element bearing are used as pitch bearings. The bearing is subjected to high bending moments, radial and axial loads in both directions. Therefore, the rolling elements for state of the art wind turbines are ball bearings, which are used in a double rowed four-point contact. This means each raceway carries on two points, and in sum four points are carrying. Other possible options are different arrangements of the rolling elements or multirow cylindrical roller bearings. Pitch bearing of modern wind turbines can reach diameters of more than 4 meters.\n\nThe load and operating situation of pitch bearings are for rolling element bearings comparatively unfavorable. The bearings are exposed to high loads and small reciprocating movements created by the pitch system or vibrations from the wind profile. The small reciprocating movements between rolling elements and raceway can lead to wear phenomena like false brinelling and fretting corrosion. Furthermore, the high loads can lead to truncation of the contact ellipse. Due to the small reciprocating movements calculation methods to estimate the bearing service life and the friction torque are not usable for pitch bearings. Newer controlling concepts of pitch control, like individual pitch control, will lead to smaller and more frequent oscillation amplitudes which could favor false brinelling and fretting corrosion.\n"}
{"id": "644662", "url": "https://en.wikipedia.org/wiki?curid=644662", "title": "Pixel density", "text": "Pixel density\n\nPixels per inch (ppi) or pixels per centimeter (ppcm) are measurements of the pixel density (resolution) of an electronic image device, such as a computer monitor or television display, or image digitizing device such as a camera or image scanner. Horizontal and vertical density are usually the same, as most devices have square pixels, but differ on devices that have non-square pixels.\n\nPixels per inch (or pixels per centimeter) can also describe the resolution, in pixels, of an image file. A 100×100 pixel image printed in a 1 inch square has a resolution of 100 pixels per inch. Used this way, the measurement is meaningful when printing an image. It has become commonplace to refer to PPI as DPI, even though PPI refers to input resolution. Industry standard, good quality photographs usually require 300 pixels per inch, at 100% size, when printed onto coated paper stock, using a printing screen of 150 lines per inch (lpi). This delivers a quality factor of 2, which is optimum. The lowest acceptable quality factor is considered 1.5, which equates to printing a 225 ppi image using a 150 lpi screen onto coated paper. \n\nScreen frequency is determined by the type of paper the image is printed on. An absorbent paper surface, uncoated recycled paper for instance, lets ink droplets spread (dot gain)—so requires a more open printing screen. Input resolution can therefore be reduced to minimize file size without loss in quality, as long as the quality factor of 2 is maintained. This is easily determined by doubling the line frequency. For example, printing on an uncoated paper stock often limits printing screen frequency to no more than 120 lpi, therefore, a quality factor of 2 is achieved with images of 240 ppi.\n\nThe PPI of a computer display is related to the size of the display in inches and the total number of pixels in the horizontal and vertical directions. This measurement is often referred to as dots per inch, though that measurement more accurately refers to the resolution of a computer printer.\n\nFor example, a 15-inch (38 cm) display whose dimensions work out to 12 inches (30.48 cm) wide by 9 inches (22.86 cm) high, capable of a maximum 1024×768 (or XGA) pixel resolution, can display around 85 PPI in both the horizontal and vertical directions. This figure is determined by dividing the width (or height) of the display area in pixels by the width (or height) of the display area in inches. It is possible for a display to have different horizontal and vertical PPI measurements (e.g., a typical 4:3 ratio CRT monitor showing a 1280×1024 mode computer display at maximum size, which is a 5:4 ratio, not quite the same as 4:3). The apparent PPI of a monitor depends upon the screen resolution (that is, the number of pixels) and the size of the screen in use; a monitor in 800×600 mode has a lower PPI than does the same monitor in a 1024×768 or 1280×960 mode.\n\nThe dot pitch of a computer display determines the absolute limit of possible pixel density.\nTypical circa-2000 cathode ray tube or LCD computer displays range from 67 to 130 PPI, though desktop monitors have exceeded 200 PPI and contemporary small-screen mobile devices often exceed 300 PPI, sometimes by a wide margin.\n\nIn January 2008, Kopin Corporation announced a 0.44 inch (1.12 cm) SVGA LCD with a pixel density of 2272 PPI (each pixel only 11.25μm). In 2011 they followed this up with a 3760 DPI 0.21” diagonal VGA colour display. The manufacturer says they designed the LCD to be optically magnified, as in high-resolution eyewear devices.\n\nHolography applications demand even greater pixel density, as higher pixel density produces a larger image size and wider viewing angle. Spatial light modulators can reduce pixel pitch to 2.5 μm, giving a pixel density of 10,160 PPI.\n\nSome observations indicate that the unaided human generally can't differentiate detail beyond 300 PPI. However, this figure depends both on the distance between viewer and image, and the viewer’s visual acuity. The human eye also responds in a different way to a bright, evenly lit interactive display from how it does to prints on paper.\n\nHigh pixel density display technologies would make supersampled antialiasing obsolete, enable true WYSIWYG graphics and, potentially enable a practical “paperless office” era. For perspective, such a device at 15 inch (38 cm) screen size would have to display more than four Full HD screens (or WQUXGA resolution).\n\nDevelopment of a display with ~900 ppi allows for three pixels with 16-bit color to act as sub-pixels to form a \"pixel cluster\". These pixel clusters act as regular pixels at ~300 ppi to produce a 48-bit color display.\n\nThe PPI pixel density specification of a display is also useful for calibrating a monitor with a printer. Software can use the PPI measurement to display a document at \"actual size\" on the screen.\n\nTheoretically, PPI can be calculated from knowing the diagonal size of the screen in inches and the resolution in pixels (width and height). This can be done in two steps:\n\n1. Calculate diagonal resolution in pixels using the Pythagorean theorem:\n\n2. Calculate PPI:\n\nwhere\n\nFor example, :\nNote that these calculations may not be very precise. Frequently, screens advertised as “X inch screen” can have their real physical dimensions of viewable area differ, for example:\n\nCamera manufacturers often quote view screens in 'number of dots'. This is not the same as the number of pixels, because there are 3 'dots' per pixel – red, green and blue. For example, the Canon 50d is quoted as having 920,000 dots. This translates as 307,200 pixels (x3 = 921,600 dots). Thus the screen is 640×480 pixels.\n\nThis must be taken into account when working out the PPI. Using the above calculations requires the screen's dimensions, but other methods require the total pixels, not total dots. 'Dots' and 'pixels' are often confused in reviews and specs when viewing information about digital cameras specifically.\n\n\"PPI\" or \"pixel density\" may also describe image scanner resolution. In this context, PPI is synonymous with samples per inch. In digital photography, pixel density is the number of pixels divided by the area of the sensor. A typical DSLR, circa 2013, has 1–6.2 MP/cm; a typical compact has 20–70 MP/cm.\n\nFor example, Sony Alpha SLT-A58 has 20.1 megapixels on an APS-C sensor having 6.2 MP/cm since a compact camera like Sony Cyber-shot DSC-HX50V has 20.4 megapixels on an 1/2.3\" sensor having 70 MP/cm. The professional camera has a lower PPI than a compact camera, because it has larger photodiodes due to having far larger sensors.\n\nSmartphones use small displays, but modern smartphone displays have a larger PPI rating, such as the Samsung Galaxy S7 with a quad HD display at 577 PPI, Fujitsu F-02G with a quad HD display at 564 PPI, the LG G6 with quad HD display at 564 PPI or – XHDPI or Oppo Find 7 with 534 PPI on 5.5\" display – XXHDPI (see section below). Sony's Xperia XZ Premium has a 4K display with a pixel density of 807 PPI, the highest of any smartphone as of 2017.\n\nThe Google Android developer documentation groups displays by their approximate pixel densities into the following categories:\n\nThe digital publishing industry primarily uses \"pixels per inch\" but sometimes \"pixels per centimeter\" is used or a conversion factor is given.\n\nThe PNG image file format only allows the meter as the unit for pixel density.\n\nThe following table show how pixel density is supported by popular image file formats. In the second column, \"length\" refers to horizontal and vertical size in inches, centimeters, etc., whereas \"pixel\" refers only to the number of pixels found along the horizontal and vertical dimension. The cell colors used do not indicate how feature-rich a certain image file format is, but what density support can be expected of a certain image file format. Often-used image file formats that do not support pixel density are added for counter-example purposes.\n\nEven though image manipulation software can optionally set density for some image file formats, not many other software uses density information when displaying images. Web browsers, for example, ignore any density information. Named pixel densities is used mainly for browsers and mobile apps. As the table shows, support for density information in image file formats varies enormously and should be used with great care in a controlled context.\n\n<nowiki>*</nowiki> Support in SVG differs. The standard supports the floats pixelUnitToMillimeterX, pixelUnitToMillimeterY, screenPixelToMillimeterX and screenPixelToMillimeterY for use in CSS2. Inkscape SVG supports density for PNG export only inkscape:export-xdpi and inkscape:export-ydpi. Adobe stores it even differently.\n\n"}
{"id": "12657052", "url": "https://en.wikipedia.org/wiki?curid=12657052", "title": "Post stall", "text": "Post stall\n\nIn aircraft flight dynamics, post stall is a flight condition in which the aircraft velocity has become lower than stall speed, in order to overcome stalling in the aircraft. In this state —below the low-velocity limit of the flight domain— the wings no longer provide enough lift to compensate the aircraft weight. Some aircraft, such as fighter jets, have enough engine thrust to maintain flight at those speeds. Normal flight controls, such as ailerons and elevators, may then no longer be sufficient to maintain full control of the aircraft.\n\n"}
{"id": "17676837", "url": "https://en.wikipedia.org/wiki?curid=17676837", "title": "Power flash", "text": "Power flash\n\nA power flash is a flash of light caused by arcing electrical discharges from damaged electrical equipment, most often severed power lines. They are often caused by strong winds, especially those from tropical cyclones and tornadoes, and occasionally by intense downbursts and derechoes. Storm spotters and meteorologists use these flashes to spot tornadoes which would otherwise be invisible due to rain or darkness. They can be distinguished from lightning by the fact that they originate at ground level, the blue color of the flash, and depending on distance, the sound of high-voltage lines shorting out.\n\n"}
{"id": "3782792", "url": "https://en.wikipedia.org/wiki?curid=3782792", "title": "Pseudogap", "text": "Pseudogap\n\nIn condensed matter physics, a pseudogap describes a state where the Fermi surface of a material possesses a partial energy gap, for example, a band structure state where the Fermi surface is gapped only at certain points. The term pseudogap was coined by Nevill Mott in 1968 to indicate a minimum in the density of states at the Fermi level, \"N(E)\", resulting from Coulomb repulsion between electrons in the same atom, a band gap in a disordered material or a combination of these. In the modern context pseudogap is a term from the field of high-temperature superconductivity which refers to an energy range (normally near the Fermi level) which has very few states associated with it. This is very similar to a true 'gap', which is an energy range that contains no allowed states. Such gaps open up, for example, when electrons interact with the lattice. The pseudogap phenomenon is observed in a region of the phase diagram generic to cuprate high-temperature superconductors, existing in underdoped specimens at temperatures above the superconducting transition temperature.\n\nOnly certain electrons 'see' this gap. The gap, which should be associated with an insulating state, only exists for electrons traveling parallel to the copper-oxygen bonds. Electrons traveling at 45° to this bond can move freely throughout the crystal. The Fermi surface therefore consists of Fermi arcs forming pockets centered on the corner of the Brillouin zone. In the pseudogap phase these arcs gradually disappear as the temperature is lowered until only four points on the diagonals of the Brillouin zone remain ungapped.\n\nOn one hand, this could indicate a completely new electronic phase which consumes available states, leaving only a few to pair up and superconduct. On the other hand, the similarity between this partial gap and that in the superconducting state could indicate that the pseudogap results from preformed Cooper pairs.\n\nRecently a pseudogap state has also been reported in strongly disordered conventional superconductors such as TiN, NbN, or granular aluminum.\n\nA pseudogap can be seen with several different experimental methods. One of the first observations was in NMR measurements of YBaCuO by H. Alloul \"et al.\" and by specific heat measurements by Loram \"et al.\" The pseudogap is also apparent in ARPES (Angle Resolved Photoemission Spectroscopy) and STM (Scanning tunneling microscope) data, which can measure the density of states of the electrons in a material.\n\nThe origin of the pseudogap is controversial and still subject to debate in the condensed matter community. Two main interpretations are emerging:\n\n1. The scenario of preformed pairs\nIn this scenario, electrons form pairs at a temperature \"T*\" that can be much larger than the critical temperature \"T\" where superconductivity appears. Values of \"T*\" of the order of 300 K have been measured in underdoped cuprates where \"T\" is about 80 K. The superconductivity does not appear at \"T*\" because large phase fluctuations of the pairing field cannot order at this temperature. The pseudogap is then produced by incoherent fluctuations of the pairing field. The pseudogap is a normal state precursor of the superconducting gap due to local, dynamic pairing correlations. This point of view is supported by a quantitative approach of the attractive pairing model to specific heat experiments. \n2. The scenario of a non-superconductivity-related pseudogap\nIn this class of scenarios, many different possible origins have been put forward, such as the formation of electronic stripes, antiferromagnetic ordering, or other exotic order parameters competing with superconductivity.\n\n"}
{"id": "53457", "url": "https://en.wikipedia.org/wiki?curid=53457", "title": "Quinoa", "text": "Quinoa\n\nQuinoa (\"Chenopodium quinoa\"; ( or , from Quechua ' or ') is a flowering plant in the amaranth family. It is a herbaceous annual plant grown as a grain crop primarily for its edible seeds. Quinoa is not a grass, but rather a pseudocereal botanically related to spinach and amaranth (\"Amaranthus\" spp.).\n\nQuinoa provides protein, dietary fiber, B vitamins, and dietary minerals in rich amounts above those of wheat, corn, rice, or oats. It is gluten-free. After harvest, the seeds are processed to remove the bitter-tasting outer seed coat.\n\nQuinoa originated in the Andean region of northwestern South America, and was domesticated 3,000 to 4,000 years ago for human consumption in the Lake Titicaca basin of Peru and Bolivia, although archaeological evidence shows livestock uses 5,200 to 7,000 years ago.\n\nQuinoa was first domesticated by Andean peoples around 3,000 to 4,000 years ago. It has been an important staple in the Andean cultures, where the plant is indigenous, but relatively obscure to the rest of the world. The Incas, who held the crop to be sacred, referred to it as \"chisoya mama\" or \"mother of all grains\", and it was the Inca emperor who would traditionally sow the first seeds of the season using \"golden implements\".\n\nDuring the Spanish conquest of South America, the colonists scorned it as \"food for Indians\", and suppressed its cultivation, due to its status within indigenous religious ceremonies. The conquistadors forbade quinoa cultivation at one point, and the Incas were forced to grow wheat instead.\nThe United Nations General Assembly declared 2013 as the \"International Year of Quinoa\" <ref name=\"UN Resolution 66/221. International Year of Quinoa, 2013\"></ref> in recognition of the ancestral practices of the Andean people, who have preserved it as a food for present and future generations, through knowledge and practices of living in harmony with nature. The objective was to draw the world’s attention to the role that quinoa could play in providing food security, nutrition and poverty eradication in support of achieving Millennium Development Goals. Some academic commentary emphasised, however, that quinoa production could have ecological and social drawbacks in its native regions, and that these problems needed to be tackled.\n\n\"Chenopodium quinoa\" is a dicotyledonous annual plant, usually about high. It has broad, generally powdery, hairy, lobed leaves, normally arranged alternately. The woody central stem is branched or unbranched depending on the variety and may be green, red or purple. The flowering panicles arise from the top of the plant or from leaf axils along the stem. Each panicle has a central axis from which a secondary axis emerges either with flowers (amaranthiform) or bearing a tertiary axis carrying the flowers (glomeruliform). The green hypogynous flowers have a simple perianth and are generally self-fertilizing. The fruits (seeds) are about in diameter and of various colors—from white to red or black, depending on the cultivar.\n\n\"Chenopodium quinoa\" is believed to have been domesticated in the Peruvian Andes from wild or weed populations of the same species. There are non-cultivated quinoa plants (\"Chenopodium quinoa\" var. \"melanospermum\") that grow in the area it is cultivated; these may either be related to wild predecessors, or they could be descendants of cultivated plants.\n\nIn their natural state, the seeds have a coating which contains bitter-tasting saponins, making them unpalatable. Most of the grain sold commercially has been processed to remove this coating. This bitterness has beneficial effects during cultivation, as it deters birds and therefore, the plant requires minimal protection. The genetic control of bitterness involves quantitative inheritance. Although lowering the saponin content through selective breeding to produce sweeter, more palatable varieties is complicated by ≈10% cross-pollination, it is a major goal of quinoa breeding programs, which may include genetic engineering.\n\nThe toxicity category rating of the saponins in quinoa treats them as mild eye and respiratory irritants and as a low gastrointestinal irritant. In South America, the saponins have many uses, including their use as a detergent for clothing and washing, and as a folk medicine antiseptic for skin injuries.\n\nAdditionally, high levels of oxalic acid are in the leaves and stems of all species of the genus \"Chenopodium\", and in the related genera of the family Amaranthaceae. The risks associated with quinoa are minimal, provided those parts are properly prepared and the leaves are not eaten to excess.\n\nRaw, uncooked quinoa is 13% water, 64% carbohydrates, 14% protein, and 6% fat. Nutritional evaluations indicate that a serving of raw quinoa seeds is a rich source (20% or higher of the Daily Value, DV) of protein, dietary fiber, several B vitamins, including 46% DV for folate, and the dietary minerals magnesium, phosphorus, and manganese.\n\nAfter cooking, which is the typical preparation for eating the seeds, quinoa is 72% water, 21% carbohydrates, 4% protein, and 2% fat. In a serving, \"cooked\" quinoa provides 120 calories and is an excellent source of manganese and phosphorus (30% and 22% DV, respectively), and a moderate source (10-19% DV) of dietary fiber, folate, and the dietary minerals, iron, zinc, and magnesium.\n\nQuinoa is gluten-free. Because of the high concentration of protein, ease of use, versatility in preparation, and potential for greatly increased yields in controlled environments, it has been selected as an experimental crop in NASA's Controlled Ecological Life Support System for long-duration human occupied space flights.\n\nThe plant's growth is highly variable due to the number of different subspecies, varieties and landraces (domesticated plants or animals adapted to the environment in which they originated). However, it is generally undemanding and altitude-hardy; it is grown from coastal regions to over in the Andes near the equator, with most of the cultivars being grown between and . Depending on the variety, optimal growing conditions are in cool climates with temperatures that vary between during the night to near during the day. Some cultivars can withstand lower temperatures without damage. Light frosts normally do not affect the plants at any stage of development, except during flowering. Midsummer frosts during flowering, a frequent occurrence in the Andes, lead to sterilization of the pollen. Rainfall requirements are highly variable between the different cultivars, ranging from during the growing season. Growth is optimal with well-distributed rainfall during early growth and no rain during seed maturation and harvesting.\n\nQuinoa has been cultivated in the United States, primarily in the high elevation San Luis Valley of Colorado where it was introduced in 1983. In this high-altitude desert valley, maximum summer temperatures rarely exceed and night temperatures are about . Due to the short growing season, North American cultivation requires short-maturity varieties, typically of Bolivian origin.\n\nSeveral countries within Europe, including France, England, The Netherlands, Belgium, Germany and Spain, have successfully grown quinoa on a commercial scale. As of 2015, within the UK, crops have been grown to scale and mechanically harvested in September.\n\nQuinoa plants do best in sandy, well-drained soils with a low nutrient content, moderate salinity, and a soil pH of 6 to 8.5. The seedbed must be well prepared and drained to avoid waterlogging.\n\nYields are maximised when N per hectare are available. The addition of phosphorus does not improve yield. In eastern North America, it is susceptible to a leaf miner that may reduce crop success. (It also affects the common weed and close relative \"Chenopodium album\", but \"C. album\" is much more resistant.)\n\nThe genome of quinoa was sequenced in 2017 by researchers at King Abdullah University of Science and Technology in Saudi Arabia. Through genetic engineering, the plant is being modified to have higher crop yield, improved tolerance to heat and biotic stress, and greater sweetness through saponin inhibition.\n\nTraditionally, quinoa grain is harvested by hand, and only rarely by machine, because the extreme variability of the maturity period of most Quinoa cultivars complicates mechanization. Harvest needs to be precisely timed to avoid high seed losses from shattering, and different panicles on the same plant mature at different times. The crop yield in the Andean region (often around 3 t/ha up to 5 t/ha) is comparable to wheat yields. In the United States, varieties have been selected for uniformity of maturity and are mechanically harvested using conventional small grain combines.\n\nThe plants are allowed to stand until the stalks and seeds have dried out and the grain has reached a moisture content below 10%.\nHandling involves threshing the seedheads from the chaff and winnowing the seed to remove the husk. Before storage, the seeds need to be dried in order to avoid germination. Dry seeds can be stored raw until being washed or mechanically processed to remove the pericarp to eliminate the bitter layer containing saponins. The seeds must be dried again before being stored and sold in stores.\n\nSince the early 21st century when quinoa became more commonly consumed in North America, Europe, and Australasia where it was not typically grown, the crop value increased. Between 2006 and 2013, quinoa crop prices tripled. In 2011, the average price was US $3,115 per tonne with some varieties selling as high as $8,000 per tonne. This compares with wheat prices of $9 per bushel (about US $340 per tonne), making wheat about 10% of the value of quinoa. The resulting effect on traditional production regions in Peru and Bolivia also influenced new commercial quinoa production elsewhere in the world, such as the United States. By 2017, quinoa was being cultivated in some 50 countries.\n\nIn 2016, world production of quinoa was 148,720 tonnes, led by Peru and Bolivia with 97% of the total combined (table).\n\nRising quinoa prices over the period 2006 to 2017 may have reduced affordability of traditional consumers to consume quinoa. However, a 2016 study using Peru's Encuesta Nacional de Hogares found that during 2004-2013 rising quinoa prices led to net economic benefits for producers, and other commentary has suggested similar conclusions, including for women specifically. It has also been suggested that as quinoa producers rise above subsistence-level income, they switch their own consumption to Western processed foods which are often less healthy than a traditional, quinoa-based diet, whether because quinoa is held to be worth too much to keep for oneself and one's family, or because processed foods have higher status despite their poorer nutritional value. Efforts are being made in some areas to distribute quinoa more widely and ensure that farming and poorer populations have access to it and have an understanding of its nutritional importance, including use in free school breakfasts and government provisions distributed to pregnant and nursing women in need.\n\nIn terms of wider social consequences, research on traditional producers in Bolivia has emphasised a complex picture. The degree to which individual producers benefit from the global quinoa boom depends on its mode of production, for example through producer associations and co-operatives such as the Asociación Nacional de Productores de Quinua (founded in the 1970s), contracting through vertically-integrated private firms, or wage labour. State regulation and enforcement is also important. It has promoted a shift to cash-cropping among some farmers and a shift toward subsistence production among others, while enabling many urban refugees to return to working the land, outcomes with complex and varied social effects.\n\nThe growth of quinoa consumption in nonindigenous regions has raised concerns over food security, such as unsustainably intensive farming of the crop, expansion of farming into ecologically fragile ecosystems, threatening both the sustainability of producer agriculture and the biodiversity of quinoa.\n\nWorld demand for quinoa is sometimes presented in the media particularly as being caused by rising veganism, but academic commentary has noted that promoting meat consumption as an ethical alternative to eating quinoa is generally inconsistent with achieving a sustainable world food supply.\n\nQuinoa is used in the Jewish community as a substitute for the leavened grains that are forbidden during the Passover holiday. Several kosher certification organizations refuse to certify it as being kosher for Passover, citing reasons including its resemblance to prohibited grains or fear of cross-contamination of the product from nearby fields of prohibited grain or during packaging. However, in December 2013 the Orthodox Union, the world's largest kosher certification agency, announced it would begin certifying quinoa as kosher for Passover.\n\n\n"}
{"id": "33034775", "url": "https://en.wikipedia.org/wiki?curid=33034775", "title": "Rashba effect", "text": "Rashba effect\n\nThe Rashba effect, also called Bychkov-Rashba effect, is a momentum-dependent splitting of spin bands in bulk crystals and low-dimensional condensed matter systems (such as heterostructures and surface states) similar to the splitting of particles and anti-particles in the Dirac Hamiltonian. The splitting is a combined effect of spin–orbit interaction and asymmetry of the crystal potential, in particular in the direction perpendicular to the two-dimensional plane (as applied to surfaces and heterostructures). This effect is named in honour of Emmanuel Rashba, who discovered it with Valentin I. Sheka in 1959 for three-dimensional systems and afterward with \nYurii A. Bychkov in 1984 for two-dimensional systems. Both the Rashba and Dresselhaus effects are concepts of the PhySH Physics Subject Headlines scheme.\n\nRemarkably, this effect can drive a wide variety of novel physical phenomena, especially operating electron spins by electric fields, even when it is a small correction to the band structure of the two-dimensional metallic state. An examples of a physical phenomena that can be explained by Rashba model is the anisotropic magnetoresistance (AMR).\n\nAdditionally, superconductors with large Rashba splitting are suggested as possible realizations of the elusive Fulde-Ferrell-Larkin-Ovchinnikov (FFLO) state, Majorana fermions and topological p-wave superconductors.\n\nLately, a momentum dependent pseudospin-orbit coupling has been realized in cold atom systems.\n\nThe Rashba effect is most easily seen in the simple model Hamiltonian known as the Rashba Hamiltonian \nwhere formula_2 is the Rashba coupling, formula_3 is the momentum and formula_4 is the Pauli matrix vector.\nThis is nothing but a two-dimensional version of the Dirac Hamiltonian (with a 90 degrees rotation of the spins).\n\nThe Rashba model in solids can be derived in the framework of the k·p perturbation theory or from the point of view of a tight binding approximation. However, the specifics of these methods are considered tedious and many prefer an intuitive toy model that gives qualitatively the same physics (quantitatively it gives a poor estimation of the coupling formula_5). Here we will introduce the intuitive toy model approach followed by a sketch of a more accurate derivation.\n\nThe Rashba effect is a direct result of inversion symmetry breaking in the direction perpendicular to the two-dimensional plane. Therefore, let us add to the Hamiltonian a term that breaks this symmetry in the form of an electric field\nDue to relativistic corrections an electron moving with velocity v in the electric field will experience an effective magnetic field B\nwhere formula_8 is the speed of light. This magnetic field couples to the electron spin\nwhere formula_10 is the electron magnetic moment.\n\nWithin this toy model, the Rashba Hamiltonian is given by\nwhere formula_12. However, while this \"toy model\" is superficially convincing, the Ehrenfest theorem seems to suggest that since the electronic motion in the formula_13 direction is that of a bound state that confines it to the 2D surface, the time-averaged electric field (i.e., including that of the potential that binds it to the 2D surface) that the electron experiences must be zero! When applied to the toy model, this argument seems to rule out the Rashba effect (and caused much controversy prior to its experimental confirmation), but turns out to be subtly-incorrect when applied to a more realistic model.. While the above naive derivation provides correct analytical form of the Rashba Hamiltonian, it is inconsistent because the effect comes from mixing energy bands (interband matrix elements) rather from intraband term of the naive model. Consistent approach explains large magnitude of the effect that includes in the denominator instead of the Dirac gap of formula_14 of the order of MeV combination of splittings the energy bands in a crystal that are about eV, see next section.\n\nIn this section we will sketch a method to estimate the coupling constant formula_2 from microscopics using a tight-binding model. Typically, the itinerant electrons that form the two-dimensional electron gas (2DEG) originate in atomic s and p orbitals. For the sake of simplicity let's consider holes in the formula_16 band. In this picture electrons fill all the formula_17 states except for a few holes near the formula_18 point.\n\nThe necessary ingredients to get Rashba splitting are atomic spin-orbit coupling\nand an asymmetric potential in the direction perpendicular to the 2D surface\n\nThe main effect of the symmetry breaking potential is to open a band gap formula_21 between the isotropic formula_16 and the formula_23, formula_24 bands. The secondary effect of this potential is that it hybridizes the formula_16 with the formula_23 and formula_24 bands. This hybridization can be understood within a tight-binding approximation. The hopping element from a formula_16 state at site formula_29 with spin formula_30 to a formula_31 or formula_32 state at site j with spin formula_33 is given by\n\nwhere formula_35 is the total Hamiltonian. In the absence of a symmetry breaking field, i.e. formula_36, the hopping element vanishes due to symmetry. However, if formula_37 then the hopping element is finite. For example, the nearest neighbor hopping element is \nwhere formula_39 stands for unit distance in the formula_40 direction respectively and formula_41 is Kronecker's delta.\n\nThe Rashba effect can be understood as a second order perturbation theory in which a spin-up hole, for example, jumps from a formula_42 state to a formula_43 with amplitude formula_44 then uses the spin–orbit coupling to flip spin and go back down to the formula_45 with amplitude formula_46. \nNote that overall the hole hopped one site and flipped spin.\nThe energy denominator in this perturbative picture is of course formula_21 such that all together we have\nwhere formula_49 is the interionic distance. This result is typically several orders of magnitude larger than the naive result derived in the previous section.\n\n\"Spintronics\" - Electronic devices are based on the ability to manipulate the electrons position by means of electric fields. Similarly, devices can be based on the manipulation of the spin degree of freedom. The Rashba effect allows to manipulate the spin by the same means, that is, without the aid of a magnetic field. Such devices have many advantages over their electronic counterparts.\n\n\"Topological quantum computation\" - Lately it has been suggested that the Rashba effect can be used to realize a p-wave superconductor. Such a superconductor has very special edge-states which are known as Majorana bound states. The non-locality immunizes them to local scattering and henceforth they are predicted to have long coherence times. Decoherence is one of the largest barriers on the way to realize a full scale quantum computer and these immune states are therefore considered good candidates for a quantum bit.\n\nDiscovery of giant Rashba effect in bulk crystals such as BiTeI and ferroelectric GeTe and in a number of low-dimensional systems bears a promise of creating devices operating electrons spins at nanoscale and possessing short operational times.\n\nThe Rashba spin-orbit coupling is typical for systems with uniaxial symmetry, e.g., for hexagonal crystals of CdS and CdSe for which it was originally found and perovskites, and also for heterostructures where it develops as a result of a symmetry breaking field in the direction perpendicular to the 2D surface. All these systems lack inversion symmetry. A similar effect, known as the Dresselhaus spin orbit coupling arises in cubic crystals of AB type lacking inversion symmetry and in quantum wells manufactured from them.\n\n\n"}
{"id": "24770667", "url": "https://en.wikipedia.org/wiki?curid=24770667", "title": "Reversible hydrogen electrode", "text": "Reversible hydrogen electrode\n\nA reversible hydrogen electrode (RHE) is a reference electrode, more specifically a subtype of the standard hydrogen electrodes, for electrochemical processes. Unlike the standard hydrogen electrode, its measured potential does not change with the pH, so it can be directly used in the electrolyte.\n\nThe name refers to the fact that the electrode is in the actual electrolyte solution and not separated by a salt bridge. The hydrogen ion concentration is therefore not 1, but corresponds to that of the electrolyte solution; in this way we can achieve a stable potential with a changing pH value. \nThe potential of the RHE correlates to the pH value:\n\nIn general, for hydrogen electrodes in which the reaction:\n\nexpires, the following dependence of the equilibrium potential , hydrogen pressure formula_2 and the activity formula_3 of the hydronium ions:\n\nHere formula_5 is the standard reduction potential (this is by definition equal to zero), R is the universal gas constant, T the absolute temperature and F is the Faraday constant.\n\nSurges occur in the electrolysis of water which means that the required cell voltage due to kinetic inhibition is higher than the equilibrium potential. The voltage increases with increasing current density at the electrodes. The measurement of equilibrium potentials is therefore possible without power.\n\nThe reversible hydrogen electrode is a fairly practical and reproducible electrode \"standard.\" The term refers to a hydrogen electrode immersed in the electrolyte solution actually used.\n\nThe benefit of that electrode is that no salt bridge is needed:\n\n"}
{"id": "22942834", "url": "https://en.wikipedia.org/wiki?curid=22942834", "title": "Simple cycle peaking power plants", "text": "Simple cycle peaking power plants\n\nSimple cycle peaking power plants, also known as peaker plants, are power plants that generally run only when there is a high demand, known as peak demand, for electricity.\n"}
{"id": "28508232", "url": "https://en.wikipedia.org/wiki?curid=28508232", "title": "Sinchi Amazonic Institute of Scientific Research", "text": "Sinchi Amazonic Institute of Scientific Research\n\nThe Sinchi Amazonic Institute of Scientific Research () is a non-profit research institute of the Government of Colombia charged with carrying out scientific investigations on matters relating to the Amazon Rainforest, the Amazon River and the Amazon Region of Colombia for its better understanding and protection. The word \"sinchi\", is a word in Quechua that means \"strong\" or \"fierce\".\n"}
{"id": "9836249", "url": "https://en.wikipedia.org/wiki?curid=9836249", "title": "Société Beninoise de Gaz", "text": "Société Beninoise de Gaz\n\nSociété Beninoise de Gaz (SoBeGaz) is a natural gas company of Benin.\n"}
{"id": "50022129", "url": "https://en.wikipedia.org/wiki?curid=50022129", "title": "Solar furnace of Uzbekistan", "text": "Solar furnace of Uzbekistan\n\nThe solar furnace of Uzbekistan was built in 1981, and is located 45 kilometers away from Tashkent city. The furnace is the largest in Asia. It uses a curved mirror, or an array of mirrors, acting as a parabolic reflector, which can reach temperatures of up to 3,000 degrees Celsius. The solar furnace of Uzbekistan can be visited by anyone who is interested in solar energy and wants to see its distinctive construction.\n\nThe heat which is produced by the solar furnace is considered to be very clean, without any pollutants. There are different ways of using this energy, such as hydrogen fuel production, foundry applications and high temperature testing. The solar furnace of Uzbekistan is sometimes called the Sun Institute of Uzbekistan. The furnace is a complex optical and mechanical construction, with 63 flat mirrors automatically controlled to track the sun in unison and redirect the solar thermal energy towards the crucible. The furnace was first opened in May 1981, and it is located 1100 meters above sea level. The furnace covers a huge area in the mountains, and consists of 4 complex subdivisions, which are: the main building of “Solar furnace of Uzbekistan”, heliostatic field, concentrator and manufacturing tower. The solar furnace of Uzbekistan was ready for use in 6 years, which means it was built between the years of 1981 and 1987. The place for the solar furnace of Uzbekistan was chosen carefully, because the sun shines there for 270 days a year. The small solar furnace at the complex has the diameter of 2 meters. The heliostatic field of the solar furnace of Uzbekistan currently consists of about 62 heliostats which are installed in a staggered order. The field uses 12090 mirrors in total, and Is the largest concentrator in the world, with an area of 1849 square meters. The concentrator at the furnace uses 10,700 mirrors, and the southern part of the concentrator is covered with special sunscreens. The solar furnace of Uzbekistan is controlled by employees from the laboratory on the 6th floor, and the observation ground is located on the highest spot.\n\nThe solar furnace of Uzbekistan is located in Tashkent region, Parkent city, Republic of Uzbekistan.\n\n\n"}
{"id": "13167850", "url": "https://en.wikipedia.org/wiki?curid=13167850", "title": "Staebler–Wronski effect", "text": "Staebler–Wronski effect\n\nThe Staebler–Wronski Effect (SWE) refers to light-induced metastable changes in the properties of hydrogenated amorphous silicon.\n\nThe defect density of hydrogenated amorphous silicon (a-Si:H) increases with light exposure, causing an increase in the recombination current and reducing the efficiency of the conversion of sunlight into electricity.\n\nIt was discovered by David L. Staebler and Christopher R. Wronski in 1977. They showed that the dark current and photoconductivity of hydrogenated amorphous silicon can be reduced significantly by prolonged illumination with intense light. However, on heating the samples to above 150 °C, they could reverse the effect.\n\n\nThe exact nature and cause of the Staebler–Wronski effect is still not well known. Nanocrystalline silicon suffers less from the Staebler–Wronski effect than amorphous silicon, suggesting that the disorder in the amorphous silicon Si network plays a major role. Other properties that could play a role are hydrogen concentration and its complex bonding mechanism, as well as the concentration of impurities.\n\nHistorically, the most favored model has been the hydrogen bond switching model. It proposes that an electron-hole pair formed by the incident light may recombine near a weak Si–Si bond, releasing energy sufficient to break the bond. A neighbouring H atom then forms a new bond with one of the Si atoms, leaving a dangling bond. These dangling bonds can trap electron-hole pairs, thus reducing the current that can pass through. However, new experimental evidence is casting doubt on this model. More recently, the \"H collision model\" proposed that two spatially separated recombination events cause emission of mobile hydrogen from Si–H bonds to form two dangling bonds, with a metastable paired H state binding the hydrogen atoms at a distant site.\n\nThe efficiency of an amorphous silicon solar cell typically drops during the first six months of operation. This drop may be in the range from 10% up to 30% depending on the material quality and device design. Most of this loss comes in the fill factor of the cell. After this initial drop, the effect reaches an equilibrium and causes little further degradation. The equilibrium level shifts with operating temperature so that performance of modules tend to recover some in the summer months and drop again in the winter months. Most commercially available a-Si modules have SWE degradation in the 10–15% range and suppliers typically specify efficiency based on performance after the SWE degradation has stabilized. In a typical amorphous silicon solar cell the efficiency is reduced by up to 30% in the first 6 months as a result of the Staebler–Wronski effect, and the fill factor falls from over 0.7 to about 0.6. This light induced degradation is the major disadvantage of amorphous silicon as a photovoltaic material.\n\n"}
{"id": "7861587", "url": "https://en.wikipedia.org/wiki?curid=7861587", "title": "Stand-alone power system", "text": "Stand-alone power system\n\nA stand-alone power system (SAPS or SPS), also known as remote area power supply (RAPS), is an off-the-grid electricity system for locations that are not fitted with an electricity distribution system. Typical SAPS include one or more methods of electricity generation, energy storage, and regulation.\n\nElectricity is typically generated by one or more of the following methods:\n\nStorage is typically implemented as a battery bank, but other solutions exist including fuel cells. Power drawn directly from the battery will be direct current extra low voltage (DC ELV), and this is used especially for lighting as well as for DC appliances. An inverter is used to generate AC low voltage, which more typical appliances can be used with.\n\nStand-alone photovoltaic power systems are independent of the utility grid and may use solar panels only or may be used in conjunction with a diesel generator, a wind turbine or batteries.\n\nThe two types of stand-alone photovoltaic power systems are direct-coupled system without batteries and stand alone system with batteries.\n\nThe basic model of a direct coupled system consists of a solar panel connected directly to a dc load. As there are no battery banks in this setup, energy is not stored and hence it is capable of powering common appliances like fans, pumps etc. only during the day. MPPTs are generally used to efficiently utilize the Sun's energy especially for electrical loads like positive-displacement water pumps. Impedance matching is also considered as a design criterion in direct-coupled systems.\n\nIn stand-alone photovoltaic power systems, the electrical energy produced by the photovoltaic panels cannot always be used directly. As the demand from the load does not always equal the solar panel capacity, battery banks are generally used.\nThe primary functions of a storage battery in a stand-alone PV system are:\n\nThe \"hybrid power plant\" is a complete electrical power supply system that can be easily configured to meet a broad range of remote power needs. There are three basic elements to the system - the power source, the battery, and the power management center. Sources for hybrid power include wind turbines, diesel engine generators, thermoelectric generators and solar PV systems. The battery allows autonomous operation by compensating for the difference between power production and use. The power management center regulates power production from each of the sources, controls power use by classifying loads, and protects the battery from service extremes.\n\nMonitoring photovoltaic systems can provide useful information about their operation and what should be done to improve performance, but if the data are not reported properly, the effort is wasted. To be helpful, a monitoring report must provide information on the relevant aspects of the operation in terms that are easily understood by a third party. Appropriate performance parameters need to be selected, and their values consistently updated with each new issue of the report. In some cases it may be beneficial to monitor the performance of individual components in order to refine and improve system performance, or be alerted to loss of performance in time for preventative action. For example, monitoring battery charge/discharge profiles will signal when replacement is due before downtime from system failure is experienced.\n\nIEC has provided a set of monitoring standards called the \"Standard for Photovoltaic system performance monitoring\" (IEC 61724). It focusses on the photovoltaic system's electrical performance and it does not address hybrids or prescribe a method for ensuring that performance assessments are equitable.\n\nPerformance assessment involves:\n\nThe wide range of load related problems identified are classified into the following types:\n\n"}
{"id": "11309798", "url": "https://en.wikipedia.org/wiki?curid=11309798", "title": "Third-brush dynamo", "text": "Third-brush dynamo\n\nA third brush dynamo was a type of dynamo, an electrical generator, formerly used for battery charging on motor vehicles. It was superseded, first by a two-brush dynamo equipped with an external voltage regulator, and later by an alternator. \n\nAs the name implies, the machine had three brushes in contact with the commutator. One was earthed to the frame of the vehicle and another was connected (through a reverse-current cut-out) to the live terminal of the vehicle's battery. The third was connected to the field winding of the dynamo. The other end of the field winding was connected to a switch which could be adjusted (by inserting or removing resistance) to give \"low\" or \"high\" charge. This switch was sometimes combined with the vehicle's light switch so that switching on the headlights simultaneously put the dynamo in high charge mode.\n\nThe third brush dynamo had the advantage of simplicity but, by modern standards, it gave poor voltage regulation. This led to short battery life as a result of over-charging or under-charging.\n\n\n"}
{"id": "45198788", "url": "https://en.wikipedia.org/wiki?curid=45198788", "title": "Vasile Adam", "text": "Vasile Adam\n\nVasile Adam (born October 10, 1956 in the Soviet Union), in little town, Nisporeni) is Moldovan woodcarver known for furniture-making and icons.\n\nHis works are mostly traditional woodworking. Artist has a workshop (from 1996) in his native town in which he provide integrated courses of the woodcarving. Also Vasile Adam works predominantly for private clients. His creations and projects are spread among national collections as well as international ones.\n\nMajor works of the artist was donated to chapel of Romanian People's Salvation Cross and to monastery 'Ciuflea' located in Chisinau.\n\nIn his career, the woodworker tried different techniques and traditions in art, including sculpture. One of his stone creations is a monument to the Soviet heroes, who have died during the World War II. The monument rises over the German town Templin.\n\nAdam is a member of Handicraftsmen's union of Moldova and Academy of Traditional Arts (Sibiu, Romania)\n\nRomanian People's Salvation Cross\n\n"}
{"id": "37872732", "url": "https://en.wikipedia.org/wiki?curid=37872732", "title": "Vestas V164", "text": "Vestas V164\n\nThe Vestas V164 is a three-bladed offshore wind turbine, produced by Vestas, with a nameplate capacity of up to 10 megawatts, a world record. \nVestas revealed the V164's design in 2011 with the first prototype unit operated at Østerild in northern Denmark in January 2014. The first industrial units were installed in 2016 at Burbo Bank, off the west coast of the United Kingdom.\n\nSince 2014 this offshore turbine has had the largest power generation capacity, with diameter of rotor and swept area . \nEach blade weighs 33-35 tonnes.\n\nOriginally called the Vestas V164-7.0MW, at 7.0 MW, the output was increased to 8.0 MW, later to 9.0 MW. \nIn 2017 the turbine capacity was upgraded to 9.5 MW.\nThe next largest wind turbines and competitors to the V164 are the Siemens Wind Power SWT-8.0-154 and Adwen AD 8-180 offshore turbines with a rated capacity of 8 MW. \nThe Enercon E-126 turbine is rated up to 7.58 MW, but only installed onshore.\n\nStarting November 2013, a prototype was installed at Østerild test station. \nThe bottom tower sections weighs over 200 tonnes and is 24 meters long and 7 meters in diameter. \nThe nacelle weighs 390 tonnes. \nThe turbine weighs 1,300 tonnes and the foundation weighs 4,000 tonnes. \nThe total height is . It became operational in January 2014. \nLater that year favourable winds allowed it to sustain its rated 8 MW power for 24 hours for a record one-day production of 192 MWh. \nIn 2017 the 9 MW version did the same for a new one-day production record of 216 MWh.\n\nAt the September 2018 Global Wind Summit, MHI Vestas announced the V164-10.0 MW, the world's first double digit offshore wind turbine. The increase in performance was achieved through \"a small design change to enhance airflow and increase cooling in the converter\". It is not expected to be available for commercial installation until 2021.\n\nThe model was shifted from prototype to production in 2014, when DONG Energy ordered 32 turbines (256 MW) for the extension of the 90 MW Burbo Bank Offshore Wind Farm. The nacelles were produced at the former Odense Steel Shipyard, while the blades are made at Vestas' Isle of Wight facilities. Assembly took place in Belfast. Installation began in 2016 and the wind farm was commissioned in April 2017.\n\nIn December 2016, Norther N.V. (Eneco/ Elicio) announced that MHI Vestas Offshore Wind will provide 44 x V164-8.4 MW (totalling approximately 370 MW) wind turbines to Belgium’s largest offshore wind project, located in the Belgian North Sea, approximately 23 km off the coast of Zeebrugge.\n\nIn April 2018, Parkwind and MHI Vestas announced that MHI Vestas Offshore Wind will provide 23 x V164-9.5 MW wind turbines. Both companies attribute the fast installation timetable, set for late 2019, to the industrialisation of offshore wind in Belgium.\n\nIn April 2016, two turbines were inaugurated in Måde, each providing 8 MW power for a total of 16 MW. These turbines are series 0, i.e. a pre-mass-production model that may allow for further improvements in the design.\n\nIn 2014, Danish Energy Agency announced that MHI Vestas Offshore Wind will provide 49 x V164-8.3 MW (totalling 406.7 MW) wind turbines to the farm Horns Rev 3, located in the Danish North Sea, approximately 40 km from Esbjerg.\n\n"}
{"id": "52182274", "url": "https://en.wikipedia.org/wiki?curid=52182274", "title": "Vortex Bladeless", "text": "Vortex Bladeless\n\nVortex Bladeless S.L is a Spanish tech startup that developed a multi-patented wind turbine without blades. In 2014, Vortex Bladeless won The South Summit Award in the category of energy and industry for the best project.\n\nVortex Bladeless S.L is a wind energy Spanish startup that was founded in 2013 by David Yáñez, David Suriol and Raúl Martín, exclusively dedicated to the development and marketing of Vortex. This multi-patented wind turbine without blades is able to capture the kinetic wind energy by 'vortex shedding' and transform it into electricity. The idea emerged in 2002 when David Yáñez, the inventor, saw a video of the Tacoma Narrows Bridge disaster and led him to the idea of a bladeless wind turbine. This new technology seeks to overcome issues related to traditional wind turbines such as maintenance, amortization, noise, environmental impact, logistics, and visual aspects. In April 2015, Vortex relocated to Boston and formed a coalition with representatives from Harvard University, IDEO, TerraForm Power and Dat Venture. The improvement of its product has been funded by Repsol Foundation Grant, Spanish Angels Investors and a loan from the Spanish government. Vortex also launched a crowdfunding campaign on June 1, 2015 to fund part of its commercialization. Currently, the company's focus is on the development of small wind products, with mass power generation devices planned for the future.\n\nVortex's innovation comes from its unusual shape, where a fiberglass and carbon fiber mast oscillates in the wind taking advantage of the vortex shedding effect. At the bottom of the mast a carbon fiber rod moves inside a linear alternator that generates the electricity, with no moving parts in contact. Vortex has a small carbon footprint, is noiseless, has low center of gravity and allows for small foundation dimensions, so more generators can be placed in an area, at twice the density of traditional turbines.\n\nVortex Bladeless is working three different products. The main characteristics of the three products are:\nAtlantis and Mini models are planned to be introduced for private homes in developing countries, or small constructions like radio antennas with their first field tests in Avila, Spain. And by 2018, with help of crowd funding, the deployment of the Vortex Grand.\n\nVortex Bladeless has won The Latin American Entrepreneurship Award in 2011, Fondo de Emprendedores Fundación Repsol Award in 2012, The South Summit Award in 2014 (in the category of energy and industry), ‘Caja de Ingenieros’ Entrepreneurs Award in 2014 and “Renovable del Año 2016“ Award of the 10th 'Abulenses' phase of Energy, in 2016.\n\n"}
{"id": "31379840", "url": "https://en.wikipedia.org/wiki?curid=31379840", "title": "Waist-to-height ratio", "text": "Waist-to-height ratio\n\nA person's waist-to-height ratio (WHtR), also called waist-to-stature ratio (WSR), is defined as their waist circumference divided by their height, both measured in the same units. The WHtR is a measure of the distribution of body fat. Higher values of WHtR indicate higher risk of obesity-related cardiovascular diseases; it is correlated with abdominal obesity.\n\nA 2010 study that followed 11,000 subjects for up to eight years concluded that WHtR is a much better measure of the risk of heart attack, stroke or death than the more widely used body mass index. However, a 2011 study that followed 60,000 participants for up to 13 years found that waist–hip ratio (when adjusted for BMI) was a better predictor of ischemic heart disease mortality than WHtR.\n\nConversely, WHtR was not a predictor for new-onset diabetes melitus in at least one study.\n\nA WHtR of over 0.5 is critical and signifies an increased risk; a 2010 systematic review of published studies concluded that \"WHtR may be advantageous because it avoids the need for age-, sex- and ethnic-specific boundary values\". For people under 40 the critical value is 0.5, for people aged 40–50 the critical value is between 0.5 and 0.6, and for people over 50 the critical values start at 0.6. \nAs a comparative, the following table categorises the boundaries of persons in terms of health:\n\n\n"}
