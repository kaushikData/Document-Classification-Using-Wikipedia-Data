{"id": "43080768", "url": "https://en.wikipedia.org/wiki?curid=43080768", "title": "10-Hydroxydecanoic acid", "text": "10-Hydroxydecanoic acid\n\n10-Hydroxydecanic acid is a specialized saturated fatty acid that is a minor constituent of royal jelly. It was scientifically discovered in 1957.\n\n"}
{"id": "39528194", "url": "https://en.wikipedia.org/wiki?curid=39528194", "title": "2013 Southern Vietnam and Cambodia blackout", "text": "2013 Southern Vietnam and Cambodia blackout\n\nThe 2013 Southern Vietnam and Cambodia blackout was a power outage in the southern region of Vietnam on 22 May 2013, affecting millions of people.\n\nPower went off at around 14:14 pm (UTC+7) on 22 May 2013.\n\nA careless move of a truck deployed to plant trees in New Bình Dương City urban area is the direct cause for the massive power outage in the southern region of Vietnam. When moving a tree on Wednesday afternoon, the truck let the tree bump onto a line in the national power grid (500 KV).\n\nThis incident caused a massive blackout in 22 provinces and cities in the South of Vietnam. In fact, there has not been any kind of property insurance for these assets in case of unexpected massive and long outage. It even caused the electricity cut for the capital city of Cambodia.\n\n"}
{"id": "45312839", "url": "https://en.wikipedia.org/wiki?curid=45312839", "title": "Aplocheilus parvus", "text": "Aplocheilus parvus\n\nAplocheilus parvus, the dwarf panchax, is a species of killifish native to India and Sri Lanka. This species grows to a length of . Its natural habitats are sheltered fresh and brackish water tanks, small streams and rivulets overgrown with vegetation. They are rarely use as an aquarium fish. It is often misidentified as \"Aplocheilus panchax\" or as \"Aplocheilus blockii\".\n"}
{"id": "1762067", "url": "https://en.wikipedia.org/wiki?curid=1762067", "title": "Baruch Plan", "text": "Baruch Plan\n\nThe Baruch Plan was a proposal by the United States government, written largely by Bernard Baruch but based on the Acheson–Lilienthal Report, to the United Nations Atomic Energy Commission (UNAEC) during its first meeting in June 1946. The United States, Great Britain and Canada called for an international organization to regulate atomic energy and President Truman responded by asking Undersecretary of State Dean Acheson and David E. Lilienthal to draw up a plan.\n\nThe plan proposed to:\n\n\nThe US agreed to turn over all of its weapons on the condition that all other countries pledge not to produce them and agree to an adequate system of inspection. The Soviets rejected this plan on the grounds that the United Nations was dominated by the United States and its allies in Western Europe, and could therefore not be trusted to exercise authority over atomic weaponry in an evenhanded manner (Nationalist China, a UN Security Council member with veto privileges, was anti-communist and aligned with the US at this time). The USSR insisted that America eliminate its own nuclear weapons before considering proposals for a system of controls and inspections.\n\nAlthough the Soviets showed increased interest in the cause of arms control after they became a nuclear power in 1949, and particularly after the death of Stalin in 1953, the issue of the Soviet Union submitting to international inspection was always a thorny one upon which many attempts at nuclear arms control were stalled. Crucially, the Baruch Plan suggested that none of the permanent members of the United Nations Security Council would be able to veto a decision to punish culprits. In presenting his plan to the United Nations, Baruch stated:\n\nThe Baruch Plan was not agreed to by the Soviet Union, and though debate on the matter continued until 1948, it was not seriously advanced later than the end of 1947. The USSR was, at the time of the negotiations, pursuing their own atomic bomb project, and the United States was continuing its own weapons development and production. With the failure of the plan, both nations embarked on programs of weapons development, innovation, production, and testing as part of the overall nuclear arms race of the Cold War.\n\nBertrand Russell urged control of nuclear weapons in the 1940s and early 1950s to avoid the likelihood of a general nuclear war, and felt hopeful when the Baruch Proposal was made. In late 1948 he suggested that \"the remedy might be the threat of immediate war by the United States on Russia for the purpose of forcing nuclear disarmament on her.\" Later he thought less well of the Baruch Proposal as \"Congress insisted upon the insertion of clauses which it was known that the Russians would not accept.\"\n\nIn his 1961 book \"Has Man a Future?\", Russell described the Baruch plan as follows:\nThe Baruch Plan is often questioned on whether it was a legitimate effort to achieve global cooperation on nuclear control.\n\n"}
{"id": "4327884", "url": "https://en.wikipedia.org/wiki?curid=4327884", "title": "Bethe ansatz", "text": "Bethe ansatz\n\nIn physics, the Bethe ansatz is an ansatz method for finding the exact solutions of certain one-dimensional quantum many-body models. It was invented by Hans Bethe in 1931 to find the exact eigenvalues and eigenvectors of the one-dimensional antiferromagnetic Heisenberg model Hamiltonian. Since then the method has been extended to other models in one dimension: Bose gas, Hubbard model, etc.\n\nIn the framework of many-body quantum mechanics, models solvable by the Bethe ansatz can be contrasted with free fermion models. One can say that the dynamics of a free model is one-body reducible: the many-body wave function for fermions (bosons) is the anti-symmetrized (symmetrized) product of one-body wave functions. Models solvable by the Bethe ansatz are not free: the two-body sector has a non-trivial scattering matrix, which in general depends on the momenta.\n\nOn the other hand, the dynamics of the models solvable by the Bethe ansatz is two-body reducible: the many-body scattering matrix is a product of two-body scattering matrices. Many-body collision happen as a sequence of two-body collisions and the many-body wave function can be represented in a form which contains only elements from two-body wave functions. The many-body scattering matrix is equal to the product of pairwise scattering matrices.\n\nThe Yang-Baxter equation guarantees the consistency. The Pauli exclusion principle is valid for models solvable by the Bethe ansatz, even for models of interacting bosons.\n\nThe ground state is a Fermi sphere. Periodic boundary conditions lead to the Bethe ansatz equations. In logarithmic form the Bethe ansatz equations can be generated by the Yang action. The square of the norm of Bethe wave function is equal to the determinant of the matrix of second derivatives of the Yang action. The recently developed algebraic Bethe ansatz led to essential progress, stating that\n\nThe exact solutions of the so-called \"s-d\" model (by P.B. Wiegmann in 1980 and independently by N. Andrei, also in 1980) and the Anderson model (by P.B. Wiegmann in 1981, and by N. Kawakami and A. Okiji in 1981) are also both based on the Bethe ansatz. There exist multi-channel generalizations of these two models also amenable to exact solutions (by N. Andrei and C. Destri and by C.J. Bolech and N. Andrei). Recently several models solvable by Bethe ansatz were realized experimentally in solid states and optical lattices. An important role in the theoretical description of these experiments was played by Jean-Sébastien Caux and Alexei Tsvelik.\n\n\n"}
{"id": "8046828", "url": "https://en.wikipedia.org/wiki?curid=8046828", "title": "Challenge X", "text": "Challenge X\n\nChallenge X: Crossover to Sustainable Mobility, known more commonly as Challenge X, is a four-year competition series amongst engineering students from across North America. Its purpose is to advance vehicle technology towards a goal of sustainable mobility.\n\nThe headline sponsors of the competition are The U.S. Department of Energy (DOE) and General Motors (GM), and the competition series is managed by Argonne National Laboratories (Argonne). Team evaluation, technical and logistical support is also provided by Argonne. The 2006 – 2007 academic year is the third year of the competition.\n\nA selection process was opened to all accredited engineering programs in the United States and Canada in 2003. Of those schools who submitted proposals to the competition organizers, 17 were chosen as Challenge X participants.\n\nChallenge X is a continuation of the long history of student vehicle competitions sponsored by the DOE since 1987 (Challenge X a). The majority of the competitions involve high level sponsors from both the government and the automotive industry. Challenge X is the latest in these series of competitions. The goal of the competition is to guide students through the engineering design process used at many automotive manufacturers, while developing and exploring various advanced technologies which can be used to reduce energy consumption and vehicle emissions. The vice president for GM's Powertrain Division, Tom Stephens described the competition by saying:This is the first time that a student competition has emphasized the importance of fuel choice in achieving sustainability mobility. Challenge X provides the student teams an opportunity to take an open-minded well-to-wheels approach to all the issues involved in energy efficiency and emissions – including the fuel source, the propulsion system and the vehicle's real world utility and consumer appeal (Foster).\nWhen applied properly these technologies lead to more sustainable vehicle solutions. As automotive consumers have increasingly purchased large family sized sport utility vehicles over the past decade, the competition vehicle for Challenge X is the Chevrolet Equinox, a GM a mid-size crossover SUV . Student teams are required to maintain the stock vehicle's performance, safety, utility, and consumer acceptability all while attempting to increase its fuel economy and decrease its emissions.\n\nDuring the first year of the competition each team focused on vehicle simulation, computerized powertrain simulations, and the ever-present engineering give and take present in early vehicle design. This intensive modeling of each teams proposed vehicle architecture is designed to guide the teams in making decisions about vehicle hardware and components. For example, students must compare the advantages and disadvantages of many components that are able to perform the same function, and when students find that there is not an existing component available to perform a specified function, they must determine if they have to time and/or resources to develop such a component. At the end of the 2004 – 2005 academic year, the first years competition was held in Milford, Michigan. The focus of the year one competition was the development of an alternative fuel architecture and the support of models to show a reduction in overall petroleum consumption and a decrease in vehicle emissions. Teams were also judged on technical papers and presentations submitted to competition organizers.\n\nThe University of Waterloo from Ontario, Canada received first place (Sharp). The Waterloo team's design is a series fuel cell hybrid that uses a Hydrogenics PEM fuel cell engine with a Cobasys 288 V NiMH battery and two Ballard 67 kW ACI electric drives (Challenge X c). Second place was awarded to the University of Akron from Akron, Ohio (Sharp) with their design of a through-the-road parallel hybrid with a 1.9 L Volkswagen TDI engine running on biodiesel (B-20), a Siemens 21 kW ACI motor, and a Johnson Controls 165 V battery pack (Challenge X c). Third place was awarded to Ohio State University from Columbus, Ohio (Sharp) with their design, also a through-the-road parallel biodiesel hybrid, utilizing a Panasonic NiMH battery and a 1.9 L Fiat 110 kW CIDI engine (Challenge X c). Fourth place went to Virginia Tech (Sharp). Overall, all 17 teams met the minimum goals for the year one competition and were awarded with the donation of a 2005 Chevrolet Equinox to their team.\n\nThe second year of the competition was focused on teams implementing their chosen architecture into their project vehicle. After completing year one, each team was required to continue with their proposed architecture and any major changes to their design, or to the vehicle structure or safety systems had to be approved through a detailed waiver process where the reasons for and ability to make such changes while meeting competition goals are explained to competition organizers. Vehicle integration was the main focus of year two, where teams were required to have a working vehicle by the time of the year end competition. A strong focus was placed on powertrain development and a demonstration of energy use and emissions goals as defined by each team in their year one reports. Each team was also judged extensively on vehicle performance categories such as greenhouse gas impact, total well-to-wheels fuel economy, emissions, towing capacity, acceleration, off-road performance, and consumer acceptability. Furthermore, each team was required to submit a technical paper and perform an oral presentation (Challenge X d).\n\nThe year two competition was hosted at the GM Desert Proving Grounds in Mesa, Arizona. Top honors in the year two competition went to the first place team Virginia Tech from Blacksburg, Virginia (Gurski). Virginia Tech’s competition entry, the REV (Renewable Energy Vehicle, the Larsen Special Edition) is a split parallel hybrid using a Ballard Ranger 67 kW ACI rear traction motor, an 8 kW MES belted alternator starter, and a Saab 2.0 LFlex Fuel SI engine running on E85 – an ethanol/gas blend. Virginia Tech also won first place in lowest well-to-wheels petroleum energy usage, with an overall reduction of petroleum consumption of 74 percent over the stock vehicle, best written technical reports, lowest regulated tailpipe emissions, and lowest time braking and handling (Challenge X b). Second place was awarded to the University of Wisconsin–Madison Hybrid Vehicle Team from Madison, Wisconsin (Challenge X b) with their Moovada, a through-the-road parallel biodiesel electric running with the 1.9 L Fiat 110-kilowatt CIDI engine (Challenge X c). Mississippi State University from Starkville, Mississippi came in third place overall (Challenge X b) also with a through-the-road parallel biodiesel electric running with the 1.9 L Fiat 110 kW CIDI engine, and a Ballard Ranger 67 kW ACI rear traction motor (Challenge X c). Fourth place went to Ohio State University (Challenge X b).\n\nYear Three required further refinement of the vehicle with the goal of delivering a “showroom” vehicle that addresses the requirements of consumers. At the conclusion of years one through three, team vehicles were judged in areas such as towing capacity, acceleration, ride and handling, noise and vibration, greenhouse gas impact, total well-to-wheels fuel economy, and consumer acceptability.\n\nThe competition is currently in year four for the 2007-2008 academic year. The fourth year of the program will give students the opportunity to further implement innovative technologies in their vehicles, such as telematics, that will help meet consumer demands for safety, security and convenience. The final year will also allow teams to focus on developing and implementing marketing plans for their vehicles to promote the goals of the Challenge X program. The year end competition will be a road rally.\n\n\n\n\n\n\n\n\n\n"}
{"id": "10526566", "url": "https://en.wikipedia.org/wiki?curid=10526566", "title": "ChemAxon", "text": "ChemAxon\n\nChemAxon () is a cheminformatics and bioinformatics software development company specializing in cloud based, end user solutions, back end platforms and consultancy services for chemical and biological research. Headquartered in Budapest, Hungary with 121 employees (as of November 2018). The company also operates business and consultancy offices globally in Cambridge, MA, San Diego, CA, and in Prague. ChemAxon supports its customers via exclusive distributors in China, India, Japan, South Korea, Singapore, and Australia. ChemAxon provides solutions, platforms, applications, and consultancy services for handling chemical and biological entities for the pharmaceutical, biotechnology, new materials, fine-, petro- and agrochemical, food and cosmetics industries. ChemAxon supports academic institutions through special software licensing programs for students, teachers, academic researchers, and high school curriculums. Tools and software solutions are offered to academic research groups wishing to integrate cheminformatic functionalities into open website platforms via web hosting services.\n\nChemAxon Products include tools for visualization and drawing of molecules, chemical database searching and management, and for drug discovery. Products are licensed free of charge for academic use.\n\nChemAxon’s desktop applications include Marvin which is a free chemistry software for drawing and visualizing chemical structures,\n\nInstant JChem, a desktop application for end user scientists; JChem for Excel which integrates the structure handling capabilities of JChem and Marvin within a Microsoft Excel environment.\n\nThe software can be used to predict p\"K\" values and logP values.\n\nThe company developed Markush structure storage and search capabilities (without enumeration), with Markush structures from Thomson Reuters Derwent World Patents Index (DWPISM) database.\n\nPearson Education uses ChemAxon's JChem, MarvinSketch, and MarvinView as the chemistry tools in many of Pearson MasteringChemistry courses.\n\nMolecule characterization data in the form of a simplified molecular-input line-entry system (SMILES) string can be uploaded into the Marvin software.\n\n"}
{"id": "3451700", "url": "https://en.wikipedia.org/wiki?curid=3451700", "title": "Cnidoscolus aconitifolius", "text": "Cnidoscolus aconitifolius\n\nCnidoscolus aconitifolius, commonly known as chaya or tree spinach, is a large, fast-growing leafy perennial shrub that is believed to have originated in the Yucatán Peninsula of Mexico. \nThe specific epithet, \"aconitifolius\", means \"\"Aconitum\"-like leaves\". \nIt has succulent stems which exude a milky sap when cut. It can grow to be 6 meters tall, but is usually pruned to about 2 m for easier leaf harvest. It is a popular leaf vegetable in Mexican and Central American cuisines, similar to spinach. The leaves should be cooked before being eaten, as the raw leaves contain a high content of toxic hydrocyanic acid. Up to 5 raw leaves can be eaten a day. To be safely eaten, the required cooking time is 5–15 minutes.\n\n\"Cnidoscolus aconitifolius\" subsp. \"aconitifolius\" is found from northern Mexico to Guatemala and cultivated as far south as Peru, while \"Cnidoscolus aconitifolius\" subsp. \"polyanthus\" is restricted a small area in western Mexico.\n\nPlants in the Chayamansa Group (syn. \"Cnidoscolus chayamansa\") are the most widely cultivated, because they lack stinging hairs on the leaves. It is divided into four cultivars based on leaf morphology: 'Chayamansa' (most common), 'Estrella', 'Picuda', and 'Redonda'.\n\nChaya is easy to grow, a tender perennial in the US, and suffers little insect damage. It is tolerant of heavy rain and has some drought tolerance. Propagation is normally by woody stem cuttings about 6-12 inches long, as seeds are produced only rarely. Early growth is slow as roots are slow to develop on the cuttings, so leaves are not harvested until the second year. Chaya leaves can be harvested continuously as long as no more than 50% of the leaves are removed from the plant, which guarantees healthy new plant growth.\n\nA USDA study in Puerto Rico reported that higher yields of greens could be obtained with chaya than any other vegetable they had studied. In another study chaya leaves were found to contain substantially greater amounts of nutrients than spinach leaves.\n\nSome varieties have stinging hairs and require gloves for harvesting. Cooking destroys the stinging hairs. Chaya is one of the most productive green vegetables.\n\nChaya is a good source of protein, vitamins, calcium, and iron; and is also a rich source of antioxidants. However, raw chaya leaves are toxic as they contain a glucoside that can release toxic cyanide. Cooking is essential prior to consumption to inactivate the toxic components; in this chaya is similar to cassava, which also contains toxic hydrocyanic glycosides and must be cooked before being eaten.\nYoung chaya leaves and the thick, tender stem tips are cut and boiled as a spinach. It is a tasty vegetable, and is exceptionally high in protein, calcium, iron, and vitamin A. In fact, levels of chaya leaf nutrients are two- to threefold greater than any other land-based leafy green vegetable. Chaya leaves have a possible antidiabetic effect.\n\nTraditionally leaves are immersed and simmered for 20 minutes and then served with oil or butter. Cooking for 20 minutes or more will render the leaves safe to eat. The stock or liquid the leaves are cooked in can also safely be consumed as the cyanide is volatilized as hydrogen cyanide (HCN) during cooking. Cooking in aluminum cookware can result in a toxic broth, causing diarrhea.\n\n"}
{"id": "13916697", "url": "https://en.wikipedia.org/wiki?curid=13916697", "title": "Components of crude oil", "text": "Components of crude oil\n\nThe Components of Crude Oil\n\nCrude oil is essentially a mixture of many different hydrocarbons, all of varying lengths and complexities. In order to separate the individual components that make up the raw natural resource, the crude oil must be fractionally distilled so that chemical components can be removed one at a time according to their boiling points. \na) Light Distillates: \ni) Naphtha - Made into gasoline/petrochemicals\nii) Methane Pentane \nb) Middle distillates\nThe components of crude oil are petrol, tar, oil, dissolved gases and kerosene also known as petroleum.\nc) Residue\n"}
{"id": "1486044", "url": "https://en.wikipedia.org/wiki?curid=1486044", "title": "Cow dung", "text": "Cow dung\n\nCow dung, also known as cow pats, cow pies or cow manure, is the waste product of bovine animal species. These species include domestic cattle (\"cows\"), bison (\"buffalo\"), yak, and water buffalo. Cow dung is the undigested residue of plant matter which has passed through the animal's gut. The resultant faecal matter is rich in minerals. Color ranges from greenish to blackish, often darkening soon after exposure to air.\n\nCow dung, which is usually a dark brown color is often used as manure (agricultural fertilizer). If not recycled into the soil by species such as earthworms and dung beetles, cow dung can dry out and remain on the pasture, creating an area of grazing land which is unpalatable to livestock.\n\nIn many parts of the developing world, and in the past in mountain regions of Europe, caked and dried cow dung is used as fuel.\n\nDung may also be collected and used to produce biogas to generate electricity and heat. The gas is rich in methane and is used in rural areas of India and Pakistan and elsewhere to provide a renewable and stable source of electricity.\n\nIn central Africa, Maasai villages have burned cow dung inside to repel mosquitos. In cold places, cow dung is used to line the walls of rustic houses as a cheap thermal insulator. Most of villagers in India spray fresh cow dung mixed with water in front of the houses to repel insects. It is also dried into cake like shapes and used as replacement for firewood.\n\nCow dung is also an optional ingredient in the manufacture of adobe mud brick housing depending on the availability of materials at hand.\n\nA deposit of cow dung is referred to in American English as a \"cow chip,\" or less commonly \"cow pie,\" and in British English as a \"cowpat\". When dry, it is used in the practice of \"cow chip throwing\" popularized in Beaver, Oklahoma in 1970. On April 21, 2001 Robert Deevers of Elgin, Oklahoma, set the record for cow chip throwing with a distance of .\n\nCow dung is also used in Hindu religious fire yajna as an important ingredient..\n\nCurrently cow dung also used as a making Panchgavya ( combination of five elements ) its used as a agriculture purpose.\n\nCow dung provides food for a wide range of animal and fungus species, which break it down and recycle it into the food chain and into the soil.\n\nIn areas where cattle (or other mammals with similar dung) are not native, there are often also no native species which can break down their dung, and this can lead to infestations of pests such as flies and parasitic worms. In Australia, dung beetles from elsewhere have been introduced to help recycle the cattle dung back into the soil. (see the Australian Dung Beetle Project and Dr. George Bornemissza).\n\nCattle have a natural aversion to feeding around their own dung. This can lead to the formation of taller ungrazed patches of heavily fertilized sward. These habitat patches, termed \"islets\", can be beneficial for many grassland arthropods, including spiders (Araneae) and bugs (Hemiptera). They have an important function in maintaining biodiversity in heavily utilized pastures.\n\nA \"buffalo chip\", also called a \"meadow muffin\", is the name for a large, flat, dried piece of dung deposited by the American bison. Well dried buffalo chips were among the few things that could be collected and burned on the prairie and were used by the Plains Indians, settlers and pioneers, and homesteaders as a source of cooking heat and warmth.\n\nBison dung is sometimes referred to by the name \"nik-nik\". This word is a borrowing from the Sioux language (which probably originally borrowed it from a northern source). In modern Sioux, \"nik-nik\" can refer to the feces of any bovine, including domestic cattle. It has also come to be used, especially in Lakota, to refer to lies or broken promises, analogously to the vulgar English term \"bullshit\" as a figure of speech.\n\n"}
{"id": "266412", "url": "https://en.wikipedia.org/wiki?curid=266412", "title": "Cutting fluid", "text": "Cutting fluid\n\nCutting fluid is a type of coolant and lubricant designed specifically for metalworking processes, such as machining and stamping. There are various kinds of cutting fluids, which include oils, oil-water emulsions, pastes, gels, aerosols (mists), and air or other gases. They may be made from petroleum distillates, animal fats, plant oils, water and air, or other raw ingredients. Depending on context and on which type of cutting fluid is being considered, it may be referred to as cutting fluid, cutting oil, cutting compound, coolant, or lubricant.\n\nMost metalworking and machining processes can benefit from the use of cutting fluid, depending on workpiece material. Common exceptions to this are cast iron and brass, which may be machined dry (though this is not true of all brasses, and any machining of brass will likely benefit from the presence of a cutting fluid).\n\nThe properties that are sought after in a good cutting fluid are the ability to:\n\nMetal cutting generates heat due to friction and energy lost deforming the material. The surrounding air has low thermal conductivity (conducts heat poorly) meaning it is a poor coolant. Ambient air cooling is sometimes adequate for light cuts and low duty cycles typical of maintenance, repair and operations (MRO) or hobbyist work. Production work requires heavy cutting over long time periods and typically produces more heat than air cooling can remove. Rather than pausing production while the tool cools, using liquid coolant removes significantly more heat more rapidly, and can also speed cutting and reduce friction and tool wear.\n\nHowever, it is not just the tool which heats up but also the work surface. Excessive temperature in the tool or work surface can ruin the temper of both, soften either to the point of uselessness or failure, burn adjacent material, create unwanted thermal expansion or lead to unwanted chemical reactions such as oxidation.\n\nBesides cooling, cutting fluids also aid the cutting process by lubricating the interface between the tool's cutting edge and the chip. By preventing friction at this interface, some of the heat generation is prevented. This lubrication also helps prevent the chips from being welded onto the tool, which would interfere with subsequent cutting.\n\nExtreme pressure additives are often added to cutting fluids to further reduce tool wear.\n\nEvery conceivable method of applying cutting fluid (e.g., flooding, spraying, dripping, misting, brushing) can be used, with the best choice depending on the application and the equipment available. For many metal cutting applications the ideal has long been high-pressure, high-volume pumping to force a stream of liquid (usually an oil-water emulsion) directly into the tool-chip interface, with walls around the machine to contain the splatter and a sump to catch, filter, and recirculate the fluid. This type of system is commonly employed, especially in manufacturing. It is often not a practical option for MRO or hobbyist metalcutting, where smaller, simpler machine tools are used. Fortunately it is also not necessary in those applications, where heavy cuts, aggressive speeds and feeds, and constant, all-day cutting are not vital.\n\nAs technology continually advances, the flooding paradigm is no longer always the clear winner. It has been complemented since the 2000s by new permutations of liquid, aerosol, and gas delivery, such as minimum quantity lubrication and through-the-tool-tip cryogenic cooling (detailed below).\n\nThrough-tool coolant systems, also known as through-spindle coolant systems, are systems plumbed to deliver coolant through passages inside the spindle and through the tool, directly to the cutting interface. Many of these are also high-pressure coolant systems, in which the operating pressure can be hundreds to several thousand psi (1 to 30 MPa)—pressures comparable to those used in hydraulic circuits. High-pressure through-spindle coolant systems require rotary unions that can withstand these pressures. Drill bits and endmills tailored for this use have small holes at the lips where the coolant shoots out. Various types of gun drills also use similar arrangements.\n\nThere are generally three types of liquids: mineral, semi-synthetic, and synthetic. Semi-synthetic and synthetic cutting fluids represent attempts to combine the best properties of oil with the best properties of water by suspending emulsified oil in a water base. These properties include: rust inhibition, tolerance of a wide range of water hardness (maintaining pH stability around 9 to 10), ability to work with many metals, resist thermal breakdown, and environmental safety.\n\nWater is a good conductor of heat but has drawbacks as a cutting fluid. It boils easily, promotes rusting of machine parts, and does not lubricate well. Therefore, other ingredients are necessary to create an optimal cutting fluid.\n\nMineral oils, which are petroleum-based, first saw use in cutting applications in the late 19th century. These vary from the thick, dark, sulfur-rich cutting oils used in heavy industry to light, clear oils.\n\nSemi-synthetic coolants, also called \"soluble oil\", are an emulsion or microemulsion of water with mineral oil. These began to see use in the 1930s. A typical CNC machine tool usually uses emulsified coolant, which consists of a small amount of oil emulsified into a larger amount of water through the use of a detergent.\n\nSynthetic coolants originated in the late 1950s and are usually water-based.\n\nThe official technique to measure oil concentration in cutting fluid samples is manual titration: 100ml of the fluid under test is titrated with a 0.5M HCl solution to an endpoint of pH 4 and the volume of titrant used to reach the endpoint is used to calculate the oil concentration. This technique is accurate and not affected by fluid contamination, but needs to be performed by trained personnel in a laboratory environment. A hand-held refractometer is the industrial standard used to determine the mix ratio of water-soluble coolants that estimates oil concentration from the sample refractive index measured in the Brix scale. The refractometer allows for in situ measurements of oil concentration within industrial plants. However, contamination of the sample reduces the accuracy of the measure. Other techniques are used to measure the oil concentration in cutting fluids, such as measure of the fluid viscosity, density, and ultrasound speed. Other test equipment is used to determine such properties as acidity and conductivity.\n\nOthers include:\n\nCutting fluid may also take the form of a paste or gel when used for some applications, in particular hand operations such as drilling and tapping. In sawing metal with a bandsaw, it is common to periodically run a stick of paste against the blade. This product is similar in form factor to lipstick or beeswax. It comes in a cardboard tube, which gets slowly consumed with each application.\n\nSome cutting fluids are used in aerosol (mist) form (air with tiny droplets of liquid scattered throughout). The main problems with mists have been that they are rather bad for the workers, who have to breathe the surrounding mist-tainted air, and that they sometimes don't even work very well. Both of those problems come from the imprecise delivery that often puts the mist everywhere and all the time except at the cutting interface, during the cut—the one place and time where it's wanted. However, a newer form of aerosol delivery, (minimum quantity of lubricant), avoids both of those problems. The delivery of the aerosol is directly through the flutes of the tool (it arrives directly through or around the insert itself—an ideal type of cutting fluid delivery that traditionally has been unavailable outside of a few contexts such as gun drilling or expensive, state-of-the-art liquid delivery in production milling). MQL's aerosol is delivered in such a precisely targeted way (with respect to both location and timing) that the net effect seems almost like dry machining from the operators' perspective. The chips generally seem like dry-machined chips, requiring no draining, and the air is so clean that machining cells can be stationed closer to inspection and assembly than before. MQL doesn't provide much cooling in the sense of heat transfer, but its well-targeted lubricating action prevents some of the heat from being generated in the first place, which helps to explain its success.\n\nCarbon dioxide (chemical formula CO) is also used as a coolant. In this application pressurized liquid CO is allowed to expand and this is accompanied by a drop in temperature, enough to cause a change of phase into a solid. These solid crystals are redirected into the cut zone by either external nozzles or through-the-spindle delivery, to provide temperature controlled cooling of the cutting tool and work piece.\n\nAmbient air, of course, was the original machining coolant. Compressed air, supplied through pipes and hoses from an air compressor and discharged from a nozzle aimed at the tool, is sometimes a useful coolant. The force of the decompressing air stream blows chips away, and the decompression itself has a slight degree of cooling action. The net result is that the heat of the machining cut is carried away a bit better than by ambient air alone. Sometimes liquids are added to the air stream to form a mist (mist coolant systems, described above).\n\nLiquid nitrogen, supplied in pressurized steel bottles, is sometimes used in similar fashion. In this case, boiling is enough to provide a powerful refrigerating effect. For years this has been done (in limited applications) by flooding the work zone. Since 2005, this mode of coolant has been applied in a manner comparable to MQL (with through-the-spindle and through-the-tool-tip delivery). This refrigerates the body and tips of the tool to such a degree that it acts as a \"thermal sponge\", sucking up the heat from the tool–chip interface. This new type of nitrogen cooling is still under patent. Tool life has been increased by a factor of 10 in the milling of tough metals such as titanium and inconel.\n\nAlternatively, using airflow combined with a quick evaporating substance (ex. alcohol, water etc.) can be used as an effective coolant when handling hot pieces that cannot be cooled by alternate methods.\n\n\nCutting fluids present some mechanisms for causing illness or injury in workers. Occupational exposure is associated with increases in cardiovascular disease. These mechanisms are based on the external (skin) or internal contact involved in machining work, including touching the parts and tooling; being splattered or splashed by the fluid; or having mist settle on the skin or enter the mouth and nose in the normal course of breathing.\n\nThe mechanisms include the chemical toxicity or physical irritating ability of:\n\nThe toxicity or irritating ability is usually not high, but it is sometimes enough to cause problems for the skin or for the tissues of the respiratory tract or alimentary tract (e.g., the mouth, larynx, esophagus, trachea, or lungs).\n\nSome of the diagnoses that can result from the mechanisms explained above include irritant contact dermatitis; allergic contact dermatitis; occupational acne; tracheitis; esophagitis; bronchitis; asthma; allergy; hypersensitivity pneumonitis (HP); and worsening of pre-existing respiratory problems.\n\nSafer cutting fluid formulations provide a resistance to tramp oils, allowing improved filtration separation without removing the base additive package. Room ventilation, splash guards on machines, and personal protective equipment (PPE) (such as safety glasses, respirator masks, and gloves) can mitigate hazards related to cutting fluids. Additionally, Skimmers may be used to remove tramp oil from the surface of cutting fluid, which prevents the growth of micro-organisms.\n\nBacterial growth is predominant in petroleum-based cutting fluids. Tramp oil along with human hair or skin oil are some of the debris during cutting which accumulates and forms a layer on the top of the liquid; anaerobic bacteria proliferate due to a number of factors. An early sign of the need for replacement is the \"Monday-morning smell\" (due to lack of usage from Friday to Monday). Antiseptics are sometimes added to the fluid to kill bacteria. Such use must be balanced against whether the antiseptics will harm the cutting performance, workers' health, or the environment. Maintaining as low a fluid temperature as practical will slow the growth of microorganisms.\n\nCutting fluids degrade over time due to contaminants entering the lubrication system. A common type of degradation is the formation of \"tramp oil\", also known as \"sump oil\", which is unwanted oil that has mixed with cutting fluid. It originates as lubrication oil that seeps out from the slideways and washes into the coolant mixture, as the protective film with which a steel supplier coats bar stock to prevent rusting, or as hydraulic oil leaks. In extreme cases it can be seen as a film or skin on the surface of the coolant or as floating drops of oil.\n\nSkimmers are used to separate the tramp oil from the coolant. These are typically slowly rotating vertical discs that are partially submerged below the coolant level in the main reservoir. As the disc rotates the tramp oil clings to each side of the disc to be scraped off by two wipers, before the disc passes back through the coolant. The wipers are in the form of a channel that then redirects the tramp oil to a container where it is collected for disposal. Floating weir skimmers are also used in these situation where temperature or the amount of oil on the water becomes excessive.\n\nSince the introduction of CNC additives, the tramp oil in these systems can be managed more effectively through a continuous separation effect. The tramp oil accumulation separates from the aqueous or oil based coolant and can be easily removed with an absorbent.\n\nOld, used cutting fluid must be disposed of when it is fetid or chemically degraded and has lost its usefulness. As with used motor oil or other wastes, its impact on the environment should be mitigated. Legislation and regulation specify how this mitigation should be achieved. Modern cutting fluid disposal involves techniques such as ultrafiltration using polymeric or ceramic membranes which concentrates the suspended and emulsified oil phase.\n\nChip handling and coolant management are interrelated. Over the decades they have been improved, to the point that many metalworking operations now use engineered solutions for the overall cycle of collecting, separating, and recycling both chips and coolant. For example, the chips are graded by size and type, tramp metals (such as bolts and scrap parts) are separated out, the coolant is centrifuged off the chips (which are then dried for further handling), and so on.\n\n\n"}
{"id": "12429065", "url": "https://en.wikipedia.org/wiki?curid=12429065", "title": "DSF Refractories &amp; Minerals Ltd", "text": "DSF Refractories &amp; Minerals Ltd\n\nDSF Refractories and Minerals Limited is the last major British producer of refactories, specializing in specialist shaped and unshaped refractory products. DSF is located in Friden within the Derbyshire's Peak National Park.\n\nDSF also a processor and supplier of crushed and graded refractory raw materials for other applications such as coloured road-stone.\n\nDSF was established in 1993 following the receivership of DSF Refractories Ltd and its parent company BH-F.\n\nBritish Hartford-Fairmont Ltd 'BH-F' had owned the company since 1987. They acquired DSF to support their business in the glass industry from West Group International 'WGI' who had as West Gas Improvement formed the company in 1892 as Derbyshire Silica Firebrick Ltd to support WGI's gas development business by the manufacture of refractories for use in the production of town gas. By the 1960s, the business diversified into supplying other industries including steel, glass and cement.\n\ntang ina\n"}
{"id": "39124732", "url": "https://en.wikipedia.org/wiki?curid=39124732", "title": "Design for lean manufacturing", "text": "Design for lean manufacturing\n\nDesign for lean manufacturing is a process for applying lean concepts to the design phase of a system, such as a complex product or process. The term describes methods of design in lean manufacturing companies as part of the study of Japanese industry by the Massachusetts Institute of Technology. At the time of the study, the Japanese automakers were outperforming the American counterparts in speed, resources used in design, and design quality. Conventional mass-production design focuses primarily on product functions and manufacturing costs; however,design for lean manufacturing systematically widens the design equation to include all factors that will determine a product's success across its entire value stream and life-cycle. One goal is to reduce waste and maximize value, and other goals include improving the quality of the design and the reducing the time to achieve the final solution. The method has been used in architecture, healthcare, product development, processes design, information technology systems, and even to create lean business models. It relies on the definition and optimization of values coupled with the prevention of wastes before they enter the system. Design for lean manufacturing is system design.\n\nNot to be confused with \"Lean Design\" (copyrighted and patented by Munro & Associates, of Michigan), design for lean manufacturing builds on the set of principles that emerged from design for the customer value and design for manufacturability. Since some lean tools are used in the practice of design for lean manufacturing, it borrows the first word in its name from lean manufacturing as exemplified by the Toyota Production System. Design for lean manufacturing was first coined by Womack, Jones, and Roos after studying the differences between conventional development at American automotive companies and lean methods at Japanese automobile producers. While lean manufacturing focuses on optimization of the production stream and removal of wastes (commonly referred to as muda, mura, and muri) once the value stream has been created, Lean Design ® (Munro & Associates)concerns itself with methods and techniques to create a lean solution from the start, resulting in more value and fewer wastes across the value stream. Lean design ® seeks to optimize the development process through rapid learning cycles to build and test multiple concepts early. Managing the knowledge value stream, systematic problem solving with analysis of the trade-offs between various design options, and solutions generated from ideas filtered by systematic innovation methods are viewed as methods within the lean design process.\n\nDesign for lean manufacturing is based on the premise that product and process design is an ongoing activity and not a one-time activity; therefore design for lean manufacturing should be viewed as a long term strategy for an organization. Design for lean manufacturing must be sustainable and holistic unlike other lean manufacturing or Six Sigma approaches that either tackle only a part of the problem or tackle the problem for a short period of time. Design for lean manufacturing also relates to system thinking as it considers all aspects (or the full circle) and takes the system conditions into consideration when designing products and services, delivering them according to customer needs. ® (Munro & Associates) drives prevention of waste by adopting a systematic process to improve the design phase during development. An organizational focus is required for the implementation of Lean Design ® principles, which includes efficient and sustainable design team. Initial studies of the Japanese approach to design for lean manufacturing noted four principles; leadership of projects by a shusa (or project boss), tightly knit teams, communication on all of the difficult design trade-offs, and simultaneous development between engineering and manufacturing. Further study showed additional depth to the principles, citing 13 principles specific to the Toyota design for lean manufacturing methods in product and process development in the areas of process, skilled people, and tools and technology. As the practice of design for lean manufacturing has expanded in its depth and breadth of application, additional principles have been integrated into the method.\n\n\nTo be successful, a corporate wide design for lean manufacturing implementation typically includes the following dimensions:\n\nWhen the dimensions are fully deployed in an organization, design for lean manufacturing enhances the performance levels with respect to design and innovation. Shingo assessments measure lean implementations in all parts of the organization, including the design methodology. The Shingo Prize for Excellence in Manufacturing is given annually for operational excellence in North America. Using design for lean manufacturing practices helps organizations move toward Shingo excellence.\n\n"}
{"id": "2099657", "url": "https://en.wikipedia.org/wiki?curid=2099657", "title": "Electro-Harmonix", "text": "Electro-Harmonix\n\nElectro-Harmonix is a New York-based company that makes high-end electronic audio processors and sells rebranded vacuum tubes. The company was founded by Mike Matthews in 1968. It is best known for a series of popular guitar effects pedals introduced in the 1970s and 1990s. Unknown to most people, EH also made a line of guitars in the 70's.\n\nDuring the mid-1970s, Electro Harmonix had established itself as a pioneer and leading manufacturer of guitar effects pedals. Electro-Harmonix was the first company to introduce, manufacture, and market affordable state-of-the art \"stomp-boxes\" for guitarist and bassists, such as the first stomp-box flanger (Electric Mistress), the first analog echo/delay unit with no moving parts (Memory Man), the first guitar synthesizer in pedal form (Micro Synthesizer), and the first tube-amp distortion simulator (Hot Tubes). In 1980, Electro-Harmonix also designed and marketed one of the first digital delay/looper pedals (16-Second Digital Delay) and a line of guitars in the 70's.\n\nElectro-Harmonix was founded by rhythm and blues keyboard player Mike Matthews in October 1968 in New York City with $1,000. He took a job as a salesman for IBM in 1967, but shortly afterwards, in partnership with Bill Berko, an audio repairman who claimed to have his own custom circuit for a fuzz pedal, he jobbed construction of the new pedal to a contracting house and began distributing the pedals under a deal with the Guild Guitar Company. Fuzzboxes were in demand following a trail of hits involving their sound, including \"(I Can't Get No) Satisfaction\" by The Rolling Stones two years before, and recent popularization of Jimi Hendrix. The latter connection resulted in the pedals being branded the 'Foxey Lady'. In addition several low priced models of acoustic guitars were sold.\n\nFollowing the departure of his partner, Matthews was introduced to inventor and electric engineer Robert Myer through IBM colleagues. Together they designed a circuit to create a distortion-free sustain. A simple line booster used by Myers in testing to preamplify the guitar's signal was also manufactured from 1969 as the Linear Power Booster (LPB-1), and has continued production in present day.\n\nThe Axis fuzz pedal, also sold under the name 'Foxey Lady' for the Guild guitar company, and LPB-1 Linear Power Booster were the first products in 1969. The LPB-1 massively boosted a guitar signal to provide gain by clipping the signal, resulting in a raw distorted sound, full of sustain and harmonics. Several similar devices, which sold well, followed, such as the Treble Booster and Bass Booster.\nThe Mike Matthews Freedom Amp, a portable guitar amp powered by 40 \"D\" batteries, was popular in many venues that lacked an A/C power source.\n\nElectro-Harmonix stopped making pedals in the mid-1980s, and in the early 1990s started selling vacuum tubes re-branded with its name for guitar amplifiers, which it had also been making since the 1970s. However, due to demand and the high prices guitarists were paying for old 1970s pedals on the vintage market, it reissued the more popular old pedals in the mid-1990s, including the Big Muff Pi and Small Stone. In 2002 it started designing new pedals to add to its range. Company policy was that all reissued effects remained as close as possible to the original, vintage designs; however, casings, knobs and especially the old-fashioned mini-jack power plug were not up to later standards. In 2006 the smaller and more standardized \"micro\" and \"nano\" effect lines using surface-mount circuit components were introduced. Circuit board manufacture was outsourced, and the pedals assembled in New York.\n\n Electro-Harmonix produces pedals with many different types of sound manipulation suitable for guitar, bass, vocal, keyboard, and other instruments. It also sells rebranded vacuum tubes carrying the Electro-Harmonix brand name.\n\nIn 1969 Bob Myer and Mike Matthews designed the Big Muff Pi, a fuzzbox that added a bass-heavy sustain to any guitar sound. It is described by the company as \"the finest harmonic distortion-sustain device developed to date\". Originally this was intended to be a pedal that would mimic the fuzz tones of Jimi Hendrix and other guitarists at the time, but the result was a mix of a fuzz and distortion pedal with a very heavy sound. It also made small amps sound much better and allowed distortion at any volume.\n\nThe pedal sold well and was used by Carlos Santana, Pink Floyd's David Gilmour, Alex Lifeson of Rush and, later, Metallica's bassist Cliff Burton, The Jesus and Mary Chain, and in the 1990s KoRn's rhythm guitarist Munky, Vicente Freitas, Jack White of The White Stripes, J Mascis of Dinosaur Jr., The Edge of U2, and Billy Corgan (on The Smashing Pumpkins landmark album, \"Siamese Dream\"). The band Mudhoney titled their debut EP \"Superfuzz Bigmuff\".\n\nAlthough the first Big Muff production date was for many years cited as 1971, the first version of the Big Muff was actually sold in 1969 as a hand-made \"perf board\" version. A production version with an etched PCB board was made in early 1970. Mike Matthews was friends with Jimi Hendrix and claims Jimi bought one from Manny's Music in New York, shortly after they were released and had one in the Electric Lady Studios shortly before Jimi's death in 1970. Several variations of the Big Muff Pi followed throughout the 1970s. Electro-Harmonix produced a reissue assembled in New York City; until 2009 it produced a version made by Sovtek in Russia which provided a slightly different tone. The Bass Big Muff replaced the Russian version.\n\nSeveral other variations (some of which are not actually Big Muffs) of the pedal were in production , including the Metal Muff (intended to achieve the higher gain Metal guitar sound), the Double Muff, which incorporates the original Muff Fuzz circuit, twice in series with a single overdrive control for each circuit, providing the user either with a cascaded 'Double Muff' sound or the original Muff Fuzz circuit, the Little Big Muff, a smaller version, and a variation in circuit, of the NYC Big Muff, which produces yet another variation in sound, and the Big Muff with Tone Wicker, which is similar to the 2008 revision NYC Big Muff, with two added features: a tone bypass switch allowing you to bypass the tone control and a switch that adjusts the frequency of three high frequency filters in the circuit.\n\nIn 2018, Electro-Harmonix released three vintage Big Muff re-issues the Green Russian Big Muff, the Op-Amp Big Muff, and the Triangle Big Muff\n\nElectro-Harmonix often produces a range of pedals based on a single effect, and then combines two or more into higher end units. For instance, the Epitome combines the MIcro POG, Stereo Electric Mistress, and Holy Grail Plus into one effect unit.\n\nThe widely used Small Stone phase shifter is a 4-stage phaser designed by David Cockerell, whom Electro-Harmonix hired from his former employer EMS. The phased sounds of French composer Jean-Michel Jarre depended heavily on the Small Stone unit. It was reissued years later by EHX and a smaller version of the pedal was eventually introduced in a 'Nano' casing (officially called the \"Small Stone (Nano Chassis)\").\n\nThe Small Clone chorus is a very popular chorus pedal, used by Kurt Cobain, both live and in studio. Like the Small Stone, it is issued in both the standard size and two different smaller versions (the Nano Clone is based on the Clone Theory circuit, while the neo clone is the standard).\n\nThe Electric Mistress is an analog flanger. It had first been sold in 1976 and was by that the first flanger in pedal format. The Deluxe version has been reissued and is still in production, although in 2015, a new Deluxe Electric Mistress was introduced in the company's smaller \"XO\" casing. As well, there are two digital recreations called NEO Mistress and Stereo Electric Mistress. Except for the very first blue/red version the Electric Mistress featured a \"Filter Matrix mode\" which allowed the user to freeze it at any point in the flange, offering distinctive chime-like tones. On the Neo and Stereo Mistress, this is achieved at a certain setting on the \"rate\" knob. Notable users include David Gilmour, Todd Rundgren, Alex Lifeson, Robin Trower, Andy Summers of The Police, J Mascis of Dinosaur Jr. and ex-Red Hot Chili Peppers guitarist John Frusciante.\n\nThe Flanger Hoax pedal is a more advanced unit, allowing further control of the various parameters of phaser, flanger and chorusing effects.\n\nThe Polychorus allows highly adjustable chorus, flanger, filter matrix, and slapback echo effects. Notable users include Cobain (i.e. Radio Friendly Unit Shifter), Adrian Belew, and more recently Ryan Jarman of The Cribs.\n\nElectro-Harmonix's 'XO' line added the Stereo Polyphase, an analog optical envelope- and LFO-controlled phase shifter.\n\nElectro-Harmonix also manufactures delay pedals, including the Deluxe Memory Man, Stereo Memory Man with Hazarai, and #1 Echo. They also produce the '2880' pedal, which allows complex looping and multi-track overdubbing. The #1 Echo provides basic digital echo capability, while the Deluxe Memory Man provides more control over length, repeats, etc. The Deluxe Memory Man also includes built-in chorusing and vibrato effects. The digital Stereo Memory Man with Hazarai (distinct from the Analog Deluxe Memory Man) also includes reverse echo effect and looping/overdubbing. The Memory Toy and Memory Boy delay pedals are essentially smaller budget versions of the Deluxe Memory Man. The Memory Man effects pedal was used by Edge from the band U2 to record the songs \"I Will Follow\" and \"Sunday Bloody Sunday\". One of the singles from the band Deerhunter's 2010 album Halcyon Digest was named \"Memory Boy\".\n\nThe Holy Grail, Holy Grail Plus, Holier Grail (discontinued), Holiest Grail (discontinued), Oceans 11, and Cathedral pedals produce reverberation. These cover a range of capability, including reverb length, room simulation, etc. The company's Holy Stain multi-effects pedal also includes two different types of reverb.\n\nTremolo and vibrato are included as well, in both solid-state and vacuum tube options. These are available in the Stereo Pulsar (solid-state) and Wiggler (tube) pedals.\n\nAlso available are a series of pitch modulation pedals. These include the Micro Synthesizer (for bass or guitar), HOG (Harmonic Octave Generator), POG (Polyphonic Octave Generator, released in 2005), POG 2 (2009), Micro POG (in an XO casing), Nano POG, Octave Multiplexer, and Pitch Fork.\n\nThe POG line of pedals has been used extensively by several prominent 2000s-era rock guitarists, including Jack White (of The White Stripes, The Raconteurs, etc.), and Josh Homme (of Queens of the Stone Age, Them Crooked Vultures, etc.).\n\nElectro-Harmonix offers several pedals for envelope/equalization modulation. Amongst them are the Bassballs (appropriately named for its intended use with bass guitars), Doctor Q and the Q-Tron. Another pedal of note was the vacuum tube-powered Black Finger Compressor which adds distortion-free sustain to the sound and which appeared in the mid 1970s. The solid-state White Finger followed.\n\nIn 1995, Electro-Harmonix owner Mike Matthews commissioned Mike Beigel, former owner of Musitronics Corp. and inventor of the Mu-tron III envelope filter, to design a new envelope filter using the same analog circuitry as the original Mu-tron III, thus keeping the sound as close to the original as possible while adding new features to bring the effect into the new millennium. The pedal featured the same controls as the Mu-tron III and incorporated a \"Boost\" feature, which activates an internal pre-amp and changes the function of the gain knob giving the Q-Tron a sound almost identical to the Mu-tron III. Another feature added to Q-Tron was an effects loop switch and attack response switch. Units with these features are called the Q-Tron+. A smaller more compact version, the Mini Q-Tron, is also available, as well as an even smaller version, the Micro Q-Tron. Electro-Harmonix also currently produces a modulated envelope filter, the Blurst\n\nAdditionally, Electro-Harmonix produces several other pedals.\n\nThese include the Graphic Fuzz (a fuzzbox which includes an EQ section), the Frequency Analyzer (which creates ring modulation) and the Voice Box, a vocoder. The Voice Box has been included in a series of demonstration videos produced by Jack Conte.\n\nElectro-Harmonix has also made a few small power amp pedals for use as a simple guitar amplifier. The EHX 22 Caliber was a 22 watt solid state pedal capable of driving either an 8 ohm or 16 ohm speaker cabinet. It has been discontinued. The 22 Caliber was replaced in the lineup by the EHX 44 Magnum, a similar pedal capable of driving the same speaker load, but at a 44 watt output.\n\nSeveral pedals produced in the decades prior have also been discontinued, many of which are still in high demand for their unique sound.\n\nThese guitars were only available from EH for a very short time in 1974. They were available through a special offer for $87.50 with the purchase of $50 or more in certain scratch-n-dent EH products. The list price was $187.50. They didn't buy out a warehouse and put their name on them. These are the model names and descriptions: EH-7010 EH acoustic guitar (mahogany back and sides), EH-7020 EH acoustic guitar (D-28 copy, rosewood back and sides), and EH-7030 EH acoustic guitar (D-41 copy, rosewood back and sides, pearloid binding and inlay, 3 piece back).\nThey bought them from Moridaira/ Morris Guitar who at the time were the best guitar maker in Japan, making guitars for Fender and many big companies. \nThey put the EH on the guitars. The guitar, marked with the brand \"Brody\" is another Japanese-made acoustic that EH had made for them in the 70's. Mike Matthews has stated that Brody was his mother's maiden name. Unlike the other EH guitar, this one is of a lesser quality.\n\n\n"}
{"id": "38463447", "url": "https://en.wikipedia.org/wiki?curid=38463447", "title": "Energy in Guinea", "text": "Energy in Guinea\n\nThree primary energy sources make up the energy mix in Guinea: fossil biomass, oil and hydropower. Biomass (firewood and charcoal) makes the largest contribution in primary energy consumption. It is locally produced, while Guinea imports all the petroleum products it needs. The potential for hydroelectric power generation is high, but largely untapped. Electricity is not available to a high percentage of Guineans, especially in rural areas, and service is intermittent, even in the capital city of Conakry.\n\nIn 1995, firewood was by far the greatest source of energy, accounting for 85%. In 2008, biomass accounted for 89%. According to a 2012 International Monetary Fund paper, over 74% of households use firewood for cooking. 23% use charcoal.\n\n\"Electricité Nationale de Guinée\" (National Electricity Company of Guinea) is responsible for all production and distribution of electricity in the country. However, service is poor; even households in Conakry are served less than 12 hours a day. According to \"The World Factbook\", as of 2013, only 53% of urban areas and 11% of rural areas had access to electricity, leaving 8.7 million people without it. There is also a sharp east-west divide: west of the Ouré-Kaba-Tougué axis, nearly 30% had electricity, but that figure dropped to barely over 5% to the east.\n\nIn 2013, electricity production was an estimated 971 million kWh. In 2012, an estimated 67.8% of the electricity was obtained from fossil fuel and the remainder from hydroelectric plants. The country has considerable hydropower potential - about 6000 MW or 19,300 GWh annually - but taps only a small percentage of it.\n\nThe country is currently engaged in interconnection projects such as the sub-regional \"Organisation pour la mise en valeur du fleuve Sénégal\" (Sénégal River Basin Development Organization), \"Organisation pour la mise en valeur du fleuve Gambie\" (Gambia River Basin Development Organization) and West African Power Pool.\n\nThe country has no known reserves. It imported an estimated 9,089 bbl/day in 2012.\n\nThe estimated 2012 national consumption was 903 million kWh. Consumption per individual was less than the equivalent of half a ton of petroleum, broken down into 80% from biomass, 18% from hydrocarbons and 2% from electricity.\n\n"}
{"id": "56976", "url": "https://en.wikipedia.org/wiki?curid=56976", "title": "Federal Energy Regulatory Commission", "text": "Federal Energy Regulatory Commission\n\nThe Federal Energy Regulatory Commission (FERC) is the United States federal agency that regulates the transmission and wholesale sale of electricity and natural gas in interstate commerce and regulates the transportation of oil by pipeline in interstate commerce. FERC also reviews proposals to build interstate natural gas pipelines, natural gas storage projects, and liquefied natural gas (LNG) terminals, in addition to licensing non-federal hydropower projects.\n\nFERC is composed of five commissioners who are nominated by the U.S. President and confirmed by the U.S. Senate. There may be no more than three commissioners of one political party serving on the commission at any given time.\n\nThe Federal Power Commission (FPC), which preceded FERC, was established by Congress in 1920 to allow cabinet members to coordinate federal hydropower development.\n\nIn 1935, the FPC was transformed into an independent regulatory agency with five members nominated by the President and confirmed by the Senate. The FPC was authorized to regulate both hydropower and interstate electricity.\n\nIn 1938, the Natural Gas Act gave FPC jurisdiction over interstate natural gas pipelines and wholesale sales. In 1942, this jurisdiction was expanded to cover the licensing of more natural gas facilities. In 1954, the Supreme Court decision in \"Phillips Petroleum Co. v. Wisconsin\" extended FPC jurisdiction over all wellhead sales of natural gas in interstate commerce.\n\nIn response to the 1973 oil crisis, Congress passed the Department of Energy Organization Act in 1977, to consolidate various energy-related agencies into a Department of Energy. Congress insisted that a separate independent regulatory body be retained, and the FPC was renamed the Federal Energy Regulatory Commission (FERC), preserving its independent status within the Department. FERC was also given added responsibility to hear appeals of DOE oil price control determinations and to conduct all \"on the record\" hearings for DOE. As a result, DOE does not have any administrative law judges. As a further protection, when the Department of Energy proposes a rule, it must refer the proposal to FERC, and FERC can take over the proceeding if FERC determines that the rulemaking \"may significantly affect\" matters in its jurisdiction. The DOE Act also transferred the regulation of interstate oil pipelines from the Interstate Commerce Commission to FERC. However, the FERC lost some jurisdiction over the imports and exports of gas and electricity.\n\nIn 1978, FERC was given additional responsibilities for harmonizing the regulation of wellhead gas sales in both the intrastate and interstate markets. FERC also administered a program to foster new cogeneration and small power production under the Public Utilities Regulatory Policy Act of 1978, which was passed as part of the National Energy Act of 1978. The National Energy Act included the Natural Gas Policy Act, which reduced the scope of federal price regulation, to bring greater competition to both the natural gas and electric industry.\n\nIn 1989, Congress ended federal regulation of wellhead natural gas prices, with the passage of the Natural Gas Wellhead Decontrol Act of 1989.\n\nIn 1996, FERC issued Order 888, which spurred the creation of regional transmission organizations in the United States. This would impact existing electric power pools by rebranding themselves as independent transmission operators. Electric utilities in some regions began to spin off their generation units as separate companies that would compete in a wholesale electric market administered by the RTOs.\n\nThe Energy Policy Act of 2005 expanded FERC's authority to protect the reliability and cybersecurity of the bulk power system through the establishment and enforcement of mandatory standards, as well as greatly expanding FERC authority to impose civil penalties on entities that manipulate the electricity and natural gas markets. The Energy Policy Act of 2005 gave FERC additional responsibilities as outlined in FERC's top priorities and updated strategic plan.\n\nIn 2010, FERC issued Order 1000, which required RTOs to create regional transmission plans and identify transmission needs based on public policy. Cost allocation reforms were included, possibly to reduce barriers faced by nonincumbent transmission developers.\n\nThe responsibilities of FERC include the following:\n\n\nFERC is an independent regulatory agency within the United States Department of Energy. The President and Congress do not generally review FERC decisions, but the decisions are reviewable by the federal courts. FERC is self-funding, in that Congress sets its budget through annual and supplemental appropriations and FERC is authorized to raise revenue to reimburse the United States Treasury for its appropriations, through annual charges to the natural gas, oil, and electric industries it regulates.\n\nFERC is independent of the Department of Energy because FERC activities \"shall not be subject to further view by the Secretary [of Energy] or any officer or employee of the Department\". The Department of Energy can, however, participate in FERC proceedings as a third party.\n\nFERC is composed of up to five commissioners who are appointed by the President and confirmed by the Senate. The President appoints one of the commissioners to be the chairman of FERC, the administrative head of the agency. FERC is a bipartisan body; no more than three commissioners may be of the same political party.\n\nFERC has promoted voluntary formation of Regional Transmission Organizations (RTOs) and Independent System Operators (ISOs) to eliminate the potential for undue discrimination in access to the electric grid; regional and interregional transmission planning and cost allocation through the landmark Order No. 1000.\n\nFERC investigated the alleged manipulation of electricity market by Enron and other energy companies, and their role in the California electricity crisis. FERC has collected more than $6.3 billion from California electric market participants through settlements. Since passage of the Energy Policy Act of 2005, FERC has imposed, through settlements and orders, more than $1 billion in civil penalties and disgorgement of unjust profits to address violations of its anti-market manipulation and other rules.\n\nFERC regulates approximately 1,600 hydroelectric projects in the U.S. It is largely responsible for permitting construction of a large network of interstate natural gas pipelines. FERC also works closely with the United States Coast Guard to review the safety, security, and environmental impacts of proposed LNG terminals and associated shipping.\n\nThe Commissioners are:\n\nFERC has been subject to criticism and increasing acts of activism by people from communities affected by Commission decisions approving pipeline and related projects. They contend that FERC \"blithely greenlights too many pipelines, export terminals and other gas infrastructure\" and that FERC's structure in which it recovers its annual operating costs directly from the entities it regulates creates bias in favor of the issuance of pipeline certificates. Some of these critics have disrupted several regular open meetings of the Commission, and they staged two, week-long blockades of the Commission's headquarters in Washington, D.C., to make their points. \"Pipelines are facing unprecedented opposition,\" Commissioner LaFleur remarked to the National Press Club in a 2015 speech. \"We have a situation here.\"\n\nFERC's decisions in these cases are often upheld by the courts. In a July 1, 2014, decision, \"No Gas Pipeline v. Federal Energy Regulatory Commission\", the United States Court of Appeals for the District of Columbia Circuit (D.C. Circuit) said that pipeline applicants are not likely to pursue many certificates that are hopeless. \"The fact that they generally succeed in choosing to expend their resources on applications that serve their own financial interests does not mean that an agency which recognizes merit in such applications is biased,\" the court said. Others have directly disputed FERC's critics by pointing out that \"FERC is a creature of law. It follows a careful administrative path to regulate only a portion of natural gas such as interstate pipelines and LNG import and export terminals. That regulation includes extensive environmental review, driven by many federal laws enacted by Congress, signed by the president, and reviewed and upheld by the U.S. Supreme Court. If the agency were to adopt the path [suggested by these critics], FERC's decisions would routinely be overturned by the federal courts.\"\n\nThe United States District Court for the District of Columbia also dismissed a case involving allegations of structural bias on the part of FERC. The plaintiffs contended that the Omnibus Budget Act of 1986 funding mechanism requires the Commission to recover its budget through proportional charges on regulated entities, therefore making FERC biased in favor of the industry from which it gets its funding. But in an order issued March 22, 2017, the court said the plain language of the statute indicates that FERC does not have control over its own budget. \"The Commission's budget cannot be increased by approving pipelines; rather, [the statute] requires the Commission to make adjustments to 'eliminate any overrecovery or underrecovery.' If Plaintiffs are unhappy with Congress's chosen appropriations to the Commission…, Plaintiffs' recourse lies with their legislative representatives.\" \n\nIn New Jersey, the FERC approval of the PennEast Pipeline was met with widespread criticism by environmental groups who called the decision highly partisan. \"FERC has once again demonstrated its tremendous bias for, and partnership with, the pipeline industry,\" said Maya van Rossum, leader of the Delaware Riverkeeper Network. Doug O'Malley, president of Environment New Jersey, called the FERC approval of the pipeline a \"disaster.\" David Pringle, state campaign director of Clean Water Action and 2018 Congressional candidate, suggested the FERC was serving a partisan interest over the interests of the people of New Jersey, suggesting \"The FERC needs to remember it works for the people of the United States not PennEast.\"\n\nThese criticisms were unfounded as the D.C. Circuit Court of Appeals on July 10, 2018, rejected the Delaware Riverkeeper Network and Maya Van Rossum’s claim that FERC has an incentive to award pipeline certificates because it collects its operating expenses from regulated parties. Upholding a lower court ruling, the D.C. Circuit also rejected the Delaware Riverkeeper Network’s challenge to FERC’s use of tolling orders to meet its statutory deadlines for acting on rehearing applications. \n\nHowever, the D.C. Circuit has provided additional guidance concerning Commission procedures, stating that in one case FERC failed to consider the cumulative environmental impact of four projects that had been separately proposed by the same pipeline. The D.C. Circuit held that the projects were not financially independent and were \"a single pipeline\" that was \"linear and physically interdependent,\" so the cumulative environmental impacts should have been considered concurrently. Subsequently, in a separate decision, the D.C. Circuit sustained the Commission's conduct of separate environmental assessments when it clarified that the \"critical\" factor was that all of the pipeline's projects were either under construction or pending before FERC for environmental review at the same time, noting that the projects lacked temporal overlap. Furthermore, in another case, the D.C. Circuit sustained the Commission's use of a separate environmental assessment when it reasoned that the projects in dispute were \"unrelated\" and did not depend on one another for their justification. This guidance has allowed FERC to address additional claims of improper segmentation. \n\nFERC's leaders have stressed many times since the onset of the increased activism that the proper way to oppose a proposed new infrastructure project is by participating in the related proceeding by submitting comments and participating in public comment sessions, site visits and scoping meetings, since FERC decisions can be appealed up to the Supreme Court.\n\nThere are regions of the country where the state public utility commission and the FERC regulated Regional Transmission Organization operate in identical footprints (e.g., New York). Where this occurs, state policy makers and FERC frequently clash as to the extent of federal power and influence within the state. \n\nThe planning and siting of public policy and renewable power plants and merchant transmission lines can be contentious because the planning process must proceed through both entities. For example in New York, any large (>20 MW for the NYISO and >25 MW for the state Siting Committee) generation or merchant transmission facility must proceed through both the planning process of the NYISO - which operates on a two-year cycle at minimum with an inclusive class year pool of new projects evaluated simultaneously - and the siting process of the state Board on Electric Siting and the Environment. Prior to the formation of the NYISO, the planning process was mostly determined by the state siting board (although the utilities' power pool might have had their own closed door planning session) and large generation projects were developed by the utilities themselves.. This dual planning process provides an opportunity for other market participants to drag out the process legally, not including the other state and/or federal environmental, trade (if an international connection with Canada is requested) and local certification and regulation processes that need to be met. \n\nThis controversy similarly applies to various electric wholesale-market issues within the RTO, i.e., when a state public utility commission asserts that its retail ratepayers (under state regulation) will be impacted by wholesale-market stakeholder decisions and reforms (under federal-level regulation). In contrast, prior to the formation of the NYISO in 1999 in New York, wholesale energy prices were set within a utility's state rate case proceeding. Examples of contentious issues in New York include the NYISO's development of buyer-side mitigation (price floors) in its capacity market, proxy peaking-unit specifications during the demand-curve reset (that helps set capacity market prices), and the state's granting of zero-emissions credits to wholesale-market participating nuclear power plants.\n\n\n\n\n"}
{"id": "155650", "url": "https://en.wikipedia.org/wiki?curid=155650", "title": "Fluoride", "text": "Fluoride\n\nFluoride () is an inorganic, monatomic anion with the chemical formula (also written ), whose salts are typically white or colorless. Fluoride salts typically have distinctive bitter tastes, and are odorless. Its salts and minerals are important chemical reagents and industrial chemicals, mainly used in the production of hydrogen fluoride for fluorocarbons. Fluoride is classified as a weak base since it only partially associates in solution, but concentrated fluoride is corrosive and can attack the skin.\n\nFluoride is the simplest fluorine anion. In terms of charge and size, the fluoride ion resembles the hydroxide ion. Fluoride ions occur on earth in several minerals, particularly fluorite, but are present only in trace quantities in bodies of water in nature.\n\nFluorides include compounds that contain both ionic fluoride and those where fluoride does not dissociate. The nomenclature does not distinguish these situations. For example, sulfur hexafluoride and carbon tetrafluoride are not sources of fluoride ions under ordinary conditions.\n\nThe systematic name \"fluoride\", the valid IUPAC name, is determined according to the additive nomenclature. However, the name \"fluoride\" is also used in compositional IUPAC nomenclature which does not take the nature of bonding involved into account. \n\"Fluoride\" is also used non-systematically, to describe compounds which release fluoride upon dissolving. Hydrogen fluoride is itself an example of a non-systematic name of this nature. However, it is also a trivial name, and the preferred IUPAC name for \"fluorane\".\n\nFluorine is estimated to be the 13th most abundant element in the earth's crust and is widely dispersed in nature, almost entirely in the form of fluorides. Many minerals are known, but of paramount commercial importance is fluorite (CaF), which is roughly 49% fluoride by mass. The soft, colorful mineral is found worldwide.\n\nFluoride is naturally present at low concentration in most fresh and saltwater sources and may also be present in rainwater. Seawater fluoride levels are usually in the range of 0.86 to 1.4 mg/L, and average 1.1 mg/L (milligrams per litre). For comparison, chloride concentration in seawater is about 19 g/L. The low concentration of fluoride reflects the insolubility of the alkaline earth fluorides, e.g., CaF.\n\nConcentrations in fresh water vary more significantly. Surface water such as rivers or lakes generally contains between 0.01–0.3 ppm. Groundwater (well water) concentrations vary even more, depending on the presence of local fluoride-containing minerals. For example, natural levels of under 0.05 mg/L have been detected in parts of Canada but up to 8 mg/L in parts of China; in general levels rarely exceed 10 mg/litre \n\nFluoride can be present in rain, with its concentration increasing significantly upon exposure to volcanic activity or atmospheric pollution derived from burning fossil fuels or other sorts of industry.\n\nAll vegetation contains some fluoride, which is absorbed from soil and water. Some plants concentrate fluoride from their environment more than others. All tea leaves contain fluoride; however, mature leaves contain as much as 10 to 20 times the fluoride levels of young leaves from the same plant.\n\nFluoride can act as a base. It can combine with a proton ():\n\nThis neutralization reaction forms hydrogen fluoride (HF), the conjugate acid of fluoride.\n\nIn aqueous solution, fluoride has a p\"K\" value of 10.8. It is therefore a weak base, and tends to remain as the fluoride ion rather than generating a substantial amount of hydrogen fluoride. That is, the following equilibrium favours the left-hand side in water:\n\nHowever, upon prolonged contact with moisture, soluble fluoride salts will decompose to their respective hydroxides or oxides, as the hydrogen fluoride escapes. Fluoride is distinct in this regard among the halides. The identity of the solvent can have a dramatic effect on the equilibrium shifting it to the right-hand side, greatly increasing the rate of decomposition.\n\nSalts containing fluoride are numerous and adopt myriad structures. Typically the fluoride anion is surrounded by four or six cations, as is typical for other halides. Sodium fluoride and sodium chloride adopt the same structure. For compounds containing more than one fluoride per cation, the structures often deviate from those of the chlorides, as illustrated by the main fluoride mineral fluorite (CaF) where the Ca ions are surrounded by eight F centers. In CaCl, each Ca ion is surrounded by six Cl centers. The difluorides of the transition metals often adopt the rutile structure whereas the dichlorides have cadmium chloride structures.\n\nUpon treatment with a standard acid, fluoride salts convert to hydrogen fluoride and metal salts. With strong acids, it can be doubly protonated to give . Oxidation of fluoride gives fluorine. Solutions of inorganic fluorides in water contain F and bifluoride . Few inorganic fluorides are soluble in water without undergoing significant hydrolysis. In terms of its reactivity, fluoride differs significantly from chloride and other halides, and is more strongly solvated in protic solvents due to its smaller radius/charge ratio. Its closest chemical relative is hydroxide, since both have similar geometries.\n\nWhen relatively unsolvated, for example in nonprotic solvents, fluoride anions are called \"naked\". Naked fluoride is a very strong Lewis base, it is easily reacted with Lewis acids, forming strong adducts. Naked fluoride salts have been prepared as tetramethylammonium fluoride, tetramethylphosphonium fluoride, and tetrabutylammonium fluoride has been reported. Many so-called naked fluoride sources are in fact bifluoride salts.\n\nAt physiological pHs, hydrogen fluoride is usually fully ionised to fluoride. In biochemistry, fluoride and hydrogen fluoride are equivalent. Fluorine, in the form of fluoride, is considered to be a micronutrient for human health, necessary to prevent dental cavities, and to promote healthy bone growth. The tea plant (\"Camellia sinensis\" L.) is a known accumulator of fluorine compounds, released upon forming infusions such as the common beverage. The fluorine compounds decompose into products including fluoride ions. Fluoride is the most bioavailable form of fluorine, and as such, tea is potentially a vehicle for fluoride dosing. Approximately, 50% of absorbed fluoride is excreted renally with a twenty-four-hour period. The remainder can be retained in the oral cavity, and lower digestive tract. Fasting dramatically increases the rate of fluoride absorption to near 100%, from a 60% to 80% when taken with food. Per a 2013 study, it was found that consumption of one litre of tea a day, can potentially supply the daily recommended intake of 4 mg per day. Some lower quality brands can supply up to a 120% of this amount. Fasting can increase this to 150%. The study indicates that tea drinking communities are at an increased risk of dental and skeletal fluorosis, in the case where water fluoridation is in effect. Fluoride ion in low doses in the mouth reduces tooth decay. For this reason, it is used in toothpaste and water fluoridation. At much higher doses and frequent exposure, fluoride causes health complications and can be toxic.\n\nFluoride salts and hydrofluoric acid are the main fluorides of industrial value. Compounds with C-F bonds fall into the realm of organofluorine chemistry. The main uses of fluoride, in terms of volume, are in the production of cryolite, NaAlF. It is used in aluminium smelting. Formerly, it was mined, but now it is derived from hydrogen fluoride. Fluorite is used on a large scale to separate slag in steel-making. Mined fluorite (CaF) is a commodity chemical used in steel-making.\n\nHydrofluoric acid and its anhydrous form, hydrogen fluoride, is also used in the production of fluorocarbons. Hydrofluoric acid has a variety of specialized applications, including its ability to dissolve glass.\n\nFluoride-containing compounds, such as sodium fluoride or sodium monofluorophosphate are used in topical and systemic fluoride therapy for preventing tooth decay. They are used for water fluoridation and in many products associated with oral hygiene. Originally, sodium fluoride was used to fluoridate water; hexafluorosilicic acid (HSiF) and its salt sodium hexafluorosilicate (NaSiF) are more commonly used additives, especially in the United States. The fluoridation of water is known to prevent tooth decay and is considered by the U.S. Centers for Disease Control and Prevention as \"one of 10 great public health achievements of the 20th century\". In some countries where large, centralized water systems are uncommon, fluoride is delivered to the populace by fluoridating table salt. For the method of action for cavity prevention, see Fluoride therapy. Fluoridation of water has its critics (see Water fluoridation controversy). Fluoridated toothpaste is in common use, but is only effective at concentrations above 1,000 ppm, as is common in North America and Europe.\n\nFluoride salts are commonly used in biological assay processing to inhibit the activity of phosphatases, such as serine/threonine phosphatases. Fluoride mimics the nucleophilic hydroxide ion in these enzymes' active sites. Beryllium fluoride and aluminium fluoride are also used as phosphatase inhibitors, since these compounds are structural mimics of the phosphate group and can act as analogues of the transition state of the reaction.\n\nThe U.S. Institute of Medicine (IOM) updated Estimated Average Requirements (EARs) and Recommended Dietary Allowances (RDAs) for some minerals in 1997. Where there was not sufficient information to establish EARs and RDAs, an estimate designated Adequate Intake (AI) was used instead. AIs are typically matched to actual average consumption, with the assumption that there appears to be a need, and that need is met by what people consume. The current AI for women 19 years and older is 3.0 mg/day (includes pregnancy and lactation). The AI for men is 4.0 mg/day. The AI for children ages 1–18 increases from 0.7 to 3.0 mg/day. The major known risk of fluoride deficiency appears to be an increased risk of bacteria-caused tooth cavities. As for safety, the IOM sets Tolerable upper intake levels (ULs) for vitamins and minerals when evidence is sufficient. In the case of fluoride the UL is 10 mg/day. Collectively the EARs, RDAs, AIs and ULs are referred to as Dietary Reference Intakes (DRIs).\n\nThe European Food Safety Authority (EFSA) refers to the collective set of information as Dietary Reference Values, with Population Reference Intake (PRI) instead of RDA, and Average Requirement instead of EAR. AI and UL defined the same as in United States. For women ages 18 and older the AI is set at 2.9 mg/day (includes pregnancy and lactation). For men the value is 3.4 mg/day. For children ages 1–17 years the AIs increase with age from 0.6 to 3.2 mg/day. These AIs are comparable to the U.S. AIs. The EFSA reviewed safety evidence and set an adult UL at 7.0 mg/day (lower for children).\n\nFor U.S. food and dietary supplement labeling purposes the amount of a vitamin or mineral in a serving is expressed as a percent of Daily Value (%DV). Although there is information to set Adequate Intake, fluoride does not have a Daily Value and is not required to be shown on food labels.\n\nDaily intakes of fluoride can vary significantly according to the various sources of exposure. Values ranging from 0.46 to 3.6–5.4 mg/day have been reported in several studies (IPCS, 1984). In areas where water is fluoridated this can be expected to be a significant source of fluoride, however fluoride is also naturally present in virtually all foods and beverages at wide range of concentrations. The maximum safe daily consumption of fluoride is 10 mg/day for an adult (U.S.) or 7 mg/day (European Union).\n\nThe upper limit of fluoride intake from all sources (fluoridated water, food, beverages, fluoride dental products and dietary fluoride supplements) is set at 0.10 mg/kg/day for infants, toddlers, and children through to 8 years old. For older children and adults, who are no longer at risk for dental fluorosis, the upper limit of fluoride is set at 10 mg/day regardless of weight.\n\nAccording to the U.S. Department of Agriculture, the Dietary Reference Intakes, which is the \"highest level of daily nutrient intake that is likely to pose no risk of adverse health effects\" specify 10 mg/day for most people, corresponding to 10 L of fluoridated water with no risk. For infants and young children the values are smaller, ranging from 0.7 mg/d for infants to 2.2 mg/d. Water and food sources of fluoride include community water fluoridation, seafood, tea, and gelatin.\n\nSoluble fluoride salts, of which sodium fluoride is the most common, are toxic, and have resulted in both accidental and self-inflicted deaths from acute poisoning. The lethal dose for most adult humans is estimated at 5 to 10 g (which is equivalent to 32 to 64 mg/kg elemental fluoride/kg body weight). A case of a fatal poisoning of an adult with 4 grams of sodium fluoride is documented, and a dose of 120 g sodium fluoride has been survived. For sodium fluorosilicate (NaSiF), the median lethal dose (LD) orally in rats is 0.125 g/kg, corresponding to 12.5 g for a 100 kg adult.\n\nTreatment may involve oral administration of dilute calcium hydroxide or calcium chloride to prevent further absorption, and injection of calcium gluconate to increase the calcium levels in the blood. Hydrogen fluoride is more dangerous than salts such as NaF because it is corrosive and volatile, and can result in fatal exposure through inhalation or upon contact with the skin; calcium gluconate gel is the usual antidote.\n\nIn the higher doses used to treat osteoporosis, sodium fluoride can cause pain in the legs and incomplete stress fractures when the doses are too high; it also irritates the stomach, sometimes so severely as to cause ulcers. Slow-release and enteric-coated versions of sodium fluoride do not have gastric side effects in any significant way, and have milder and less frequent complications in the bones. In the lower doses used for water fluoridation, the only clear adverse effect is dental fluorosis, which can alter the appearance of children's teeth during tooth development; this is mostly mild and is unlikely to represent any real effect on aesthetic appearance or on public health. Fluoride was known to enhance the measurement of bone mineral density at the lumbar spine, but it was not effective for vertebral fractures and provoked more non vertebral fractures.\n\nA popular urban myth claims that the Nazis used fluoride in concentration camps, but there is no historical evidence to prove this claim.\n\nIn areas that have naturally occurring high levels of fluoride in groundwater which is used for drinking water, both dental and skeletal fluorosis can be prevalent and severe.\n\nAround one-third of the human population drinks water from groundwater resources. Of this, about 10%, approximately three hundred million people, obtains water from groundwater resources that are heavily contaminated with arsenic or fluoride. These trace elements derive mainly from minerals. Maps are available of locations of potential problematic wells.\n\nConcentrated fluoride solutions are corrosive. Gloves made of nitrile rubber are worn when handling fluoride compounds. The hazards of solutions of fluoride salts depend on the concentration. In the presence of strong acids, fluoride salts release hydrogen fluoride, which is corrosive, especially toward glass.\n\nOrganic and inorganic anions are produced from fluoride, including:\n\n\n"}
{"id": "47230730", "url": "https://en.wikipedia.org/wiki?curid=47230730", "title": "Forestry Research Institute of Nigeria", "text": "Forestry Research Institute of Nigeria\n\nForestry Research Institute of Nigeria (FRIN) was established as Federal Department of Forestry Research in 1954. The Institute’s Decree 35 of 1973 and order establishing Research Institute of 1977 changed the status of the Department to an institute being supervised by the Federal Ministry of Environment, but the only Research Institute of the Ministry. For the purpose of developing the nation's education, Forestry Research Institute of Nigeria have a subsidiary college name Federal College of Forestry located in Ibadan charged with training and developing forestry and agricultural practice. The Federal College of Forestry, Ibadan (FEDCOFOR) has six specialized research departments (each having various specialized sections), three support departments. The Federal College Of Forestry, Ibadan is a monotechnic offering both National Diploma (ND) and Higher National Diploma (HND).\n\nHer Head Office located in Jericho Ibadan.\n"}
{"id": "30941678", "url": "https://en.wikipedia.org/wiki?curid=30941678", "title": "Gaffney, Cline &amp; Associates", "text": "Gaffney, Cline &amp; Associates\n\nGaffney, Cline & Associates is a petroleum consulting company headquartered in Alton, the United Kingdom, with offices in London, Houston, Buenos Aires, Singapore, Dubai and Sydney.\n\nGaffney, Cline & Associates (GCA) was founded in 1962. Since 2008, it is a subsidiary of Baker Hughes, a GE Company.\n\nOne of the notable functions of Gaffney Cline, along with similar petroleum industry consultants such as DeGolyer and MacNaughton, Netherland, Sewell, RPS-APA and Ryder Scott, is to provide third party verification and/or valuation of oil and natural gas reserves - typically 1P proven reserves, but also 2P probable reserves and 3P possible reserves - for company annual reports and SEC filing 20-F. In addition to commercial and technical studies in the oil and gas sector, GCA has practice areas in Gas/LNG and Value Assurance.\n\nD. Nathan Meehan, Ph.D, P.E. is President of Gaffney, Cline & Associates and Vice-President of Baker Hughes.\n\n\n"}
{"id": "32903628", "url": "https://en.wikipedia.org/wiki?curid=32903628", "title": "Granule (geology)", "text": "Granule (geology)\n\nA granule is a clast of rock with a particle size of 2 to 4 millimetres based on the Krumbein phi scale of sedimentology. Granules are generally considered to be larger than sand (0.0625 to 2 millimetres diameter) and smaller than pebbles (4 to 64 millimetres diameter). A rock made predominantly of granules is termed a granule conglomerate.\n\n"}
{"id": "28351640", "url": "https://en.wikipedia.org/wiki?curid=28351640", "title": "Hoggin", "text": "Hoggin\n\nHoggin is a compactable groundcover that is composed of a mixture of clay, gravel, and sand or granite dust that produces a buff-coloured bound surface. It is more commonly seen in the south of England and at National Trust properties. The material is aesthetically suited to older properties and is lower maintenance than gravel alone since it does not need regular raking. Once laid, the surface is somewhat permeable to water and therefore does not easily hold puddles or generate rapid surface runoff. The material is increasingly being used at domestic properties as a low cost and environmentally friendly alternative to concrete and block paving in paths and driveways.\n\nA compacted sub-base of larger crushed stone is often laid prior to the top layer of hoggin, especially if the area to be covered is soft ground, or prone to puddling. The larger rocks provide a firm base for the hoggin, and improved drainage.\n\n"}
{"id": "2887212", "url": "https://en.wikipedia.org/wiki?curid=2887212", "title": "Hydroxysultaine", "text": "Hydroxysultaine\n\nHydroxysultaines are chemical compounds used in high-foaming shampoos, bath products and shower gels especially in conjunction with ether sulfates and alkyl sulfates. They are also used in industrial applications where high, stable foam is required. Chemically, hydroxysultaines are zwitterionic, typically containing covalently linked positive and negative ions.\n\nHydroxysultaine is prepared industrially by the reaction of sodium bisulfite with epichlorohydrin to give the sodium salt (sodium 1-chloro-2-hydroxypropane sulfonate). This is similar to the synthesis of isethionate, which is also used as a 'head-group' in surfactants. It is typically combined with the rest of the surfactant molecule via a Menshutkin reaction with a tertiary amine.\n\nHydroxysultaines are also compatible with cationic surfactants and are stable over a wide pH range in soft or hard water. In addition to being used as a surfactant, hydroxysultaines are often used as antistatic agents.\n\nExamples include:\n"}
{"id": "40816967", "url": "https://en.wikipedia.org/wiki?curid=40816967", "title": "Iron tetraboride", "text": "Iron tetraboride\n\nIron tetraboride (FeB) is a superhard superconductor (T < 3K) consisting of iron and boron. Iron tetraboride does not occur in nature and can be created synthetically. Its molecular structure was predicted using computer models.\n\n\n\n"}
{"id": "49462636", "url": "https://en.wikipedia.org/wiki?curid=49462636", "title": "Italian oil drilling referendum, 2016", "text": "Italian oil drilling referendum, 2016\n\nA referendum on oil and natural gas drilling was held in Italy on 17 April 2016. The referendum was on the proposed repealing of a law that allows gas and oil drilling concessions extracting hydrocarbon within 12 nautical miles of the Italian coast to be prolonged until the exhaustion of the useful life of the fields. \n\nAlthough 86% voted in favour of repealing the law, the turnout of 31% was below the majority threshold required to validate the result.\n\nIt was the first referendum requested by at least five Regional Councils in the history of the Italian Republic: all 66 previous referendum questions since 1974 were called after the collection of signatures.\n\nThe search for hydrocarbon liquids and/or gases in the Italian sea is possible - with some restrictions for the coastal and environmental protection - only in certain \"marine areas\" identified by the Italian Parliament or by the Ministry of Economic Development. From 2013, new drilling is prohibited in the Tyrrhenian Sea, in the marine protected areas and in the waters within 12 nautical miles from the coast; however, the concessions approved before 2013 may continue until all of the resources are extracted.\n\nItaly authorized a total of 79 offshore platforms: 31 are located over 12 miles from and 48 within 12 miles.\n\nWithin 12 miles, 9 concessions are authorized (with 39 platforms) and their permits have expired and they have asked for an extension: if the \"yes\" vote wins in the referendum, the 9 expired concessions can not be extended. During 2015 those installations extracted about 622 million cubic meters of natural gas (equivalent to 9% of the national production and 1.1% of total consumption in 2014).\n\nUnder the 12 mile limit, there are 17 other concessions expiring between 2017 and 2027, which in 2015 extracted 1.21 billion cubic meters of gas (17.6% of national production and 2.1 % of national consumption in 2014) and 500,000 tons of oil (about 9.1% of national production and 0.8% of consumption in 2014). These concessions, in the event of a victory for 'yes' in the referendum, will not be extended after 2027.\n\nThe referendum was proposed by several regional governments after the national government passed a law allowing drilling concessions to last until oilfields or gasfields are empty. On 19 January 2016 the Constitutional Court approved the referendum. On 12 February the Five Star Movement asked President Sergio Mattarella to delay the referendum until June to allow it to be held alongside local elections in order to raise turnout and save money.\n"}
{"id": "16619", "url": "https://en.wikipedia.org/wiki?curid=16619", "title": "Kilogram", "text": "Kilogram\n\nThe kilogram or kilogramme (symbol: kg) is the base unit of mass in the International System of Units (SI). Until 20 May 2019, it remains defined by a platinum alloy cylinder, the \"International Prototype Kilogram\" (informally \"Le Grand K\" or IPK), manufactured in 1889, and carefully stored in Saint-Cloud, a suburb of Paris. After 20 May, it will be defined in terms of fundamental physical constants. \n\nThe kilogram was originally defined as the mass of a litre (cubic decimetre) of water. That was an inconvenient quantity to precisely replicate, so in 1799 a platinum artefact was fashioned to define the kilogram. That artefact, and the later IPK, have been the standard of the unit of mass for the metric system ever since.\n\nIn spite of best efforts to maintain it, the IPK has diverged from its replicas by approximately 50 micrograms since their manufacture late in the 19th century. This led to efforts to develop measurement technology precise enough to allow replacing the kilogram artifact with a definition based directly on physical phenomena, a process which is scheduled to finally take place in 2019. \n\nThe new definition is based on invariant constants of nature, in particular the Planck constant which will change to being defined rather than measured, thereby fixing the value of the kilogram in terms of the second and the metre, and eliminating the need for the IPK. The new definition was approved by the General Conference on Weights and Measures (CGPM) on 16 November 2018. The Planck constant relates a light particle’s energy, and hence mass, to its frequency. The new definition only became possible when instruments were devised to measure the Planck constant with sufficient accuracy based on the IPK definition of the kilogram.\n\nThe gram, 1/1000 of a kilogram, was provisionally defined in 1795 as the mass of one cubic centimetre of water at the melting point of ice.\nThe final kilogram, manufactured as a prototype in 1799 and from which the International Prototype Kilogram (IPK) was derived in 1875, had a mass equal to the mass of 1 dm of water under atmospheric pressure and at the temperature of its maximum density, which is approximately 4 °C.\n\nThe kilogram is the only named SI unit with an SI prefix (\"kilo\") as part of its name. Until the 2019 redefinition of SI base units, it was also the last SI unit that was still directly defined by an artefact rather than a fundamental physical property that could be independently reproduced in different laboratories. Three other base units (cd, A, mol) and 17 derived units (N, Pa, J, W, C, V, F, Ω, S, Wb, T, H, kat, Gy, Sv, lm, lx) in the SI system are defined in relation to the kilogram, and thus its stability is important. The definitions of only eight other named SI units do not depend on the kilogram: those of temperature (K, °C), time and frequency (s, Hz, Bq), length (m), and angle (rad, sr).\n\nThe IPK is rarely used or handled. Copies of the IPK kept by national metrology laboratories around the world were compared with the IPK in 1889, 1948, and 1989 to provide traceability of measurements of mass anywhere in the world back to the IPK.\n\nThe International Prototype Kilogram was commissioned by the General Conference on Weights and Measures (CGPM) under the authority of the Metre Convention (1875), and in the custody of the International Bureau of Weights and Measures (BIPM) who hold it on behalf of the CGPM. After the International Prototype Kilogram had been found to vary in mass over time relative to its reproductions, the International Committee for Weights and Measures (CIPM) recommended in 2005 that the kilogram be redefined in terms of a fundamental constant of nature. At its 2011 meeting, the CGPM agreed in principle that the kilogram should be redefined in terms of the Planck constant, \"h\". The decision was originally deferred until 2014; in 2014 it was deferred again until the next meeting. CIPM has proposed revised definitions of the SI base units, for consideration at the 26th CGPM. The formal vote, which took place on 16 November 2018, approved the change, with the new definitions coming into force on 20 May 2019. The accepted redefinition defines the Planck Constant as exactly , thereby defining the kilogram in terms of the second and the metre. Since the metre is defined as a time fraction of the speed of light in vacuum, then the kilogram is defined in terms of the time only.\n\nThe avoirdupois (or \"international\") pound, used in both the imperial and US customary systems, is now defined in terms of the kilogram. Other traditional units of weight and mass around the world are now also defined in terms of the kilogram, making the kilogram the primary standard for virtually all units of mass on Earth.\n\nThe word \"kilogramme\" or \"kilogram\" is derived from the French , which itself was a learned coinage, prefixing the Greek stem of \"a thousand\" to , a Late Latin term for \"a small weight\", itself from Greek . \nThe word was written into French law in 1795, in the \"Decree of 18 Germinal\",\nwhich revised the older system of units introduced by the French National Convention in 1793, where the had been defined as weight () of a cubic centimetre of water, equal to 1/1000 of a . In the decree of 1795, the term thus replaced , and replaced .\n\nThe French spelling was adopted in Great Britain when the word was used for the first time in English in 1795, with the spelling \"kilogram\" being adopted in the United States. In the United Kingdom both spellings are used, with \"kilogram\" having become by far the more common. UK law regulating the units to be used when trading by weight or measure does not prevent the use of either spelling.\n\nIn the 19th century the French word , a shortening of , was imported into the English language where it has been used to mean both kilogram and kilometre. While \"kilo\" is acceptable in many generalist texts, for example \"The Economist\", its use is typically considered inappropriate in certain applications including scientific, technical and legal writing, where authors should adhere strictly to SI nomenclature. When the United States Congress gave the metric system legal status in 1866, it permitted the use of the word \"kilo\" as an alternative to the word \"kilogram\", but in 1990 revoked the status of the word \"kilo\".\n\nDuring the 19th century, the standard system of metric units was the centimetre–gram–second system of units, treating the gram as the fundamental unit of mass and the \"kilogram\" simply as a derived unit. \nIn 1901, however, following the discoveries by James Clerk Maxwell to the effect that electric measurements could not be explained in terms of the three fundamental units of length, mass and time, Giovanni Giorgi proposed a new standard system that would include a fourth fundamental unit to measure quantities in electromagnetism.\nIn 1935 this was adopted by the IEC as the \"Giorgi system\", now also known as MKS system,\nand in 1946 the CIPM approved a proposal to adopt the ampere as the electromagnetic unit of the \"MKSA system\".\nIn 1948 the CGPM commissioned the CIPM \"to make recommendations for a single practical system of units of measurement, suitable for adoption by all countries adhering to the Metre Convention\". This led to the launch of SI in 1960 and the subsequent publication of the \"SI Brochure\", which stated that \"It is not permissible to use abbreviations for unit symbols or unit names ...\".\nThe CGS and MKS systems co-existed during much of the early-to-mid 20th century, but as a result of the decision to adopt the \"Giorgi system\" as the international system of units in 1960, the kilogram is now the SI base unit for mass, while the definition of the gram is derived from that of the kilogram.\n\nThe kilogram is a unit of mass, a property corresponding to the common perception of how \"heavy\" an object is. Mass is an \"inertial\" property; that is, it is related to the tendency of an object at rest to remain at rest, or if in motion to remain in motion at a constant velocity, unless acted upon by a force.\n\nWhile the weight of an object is dependent on the strength of the local gravitational field, the mass of an object is independent of gravity, as mass is a measure of the quantity of matter. Accordingly, for astronauts in microgravity, no effort is required to hold objects off the cabin floor; they are \"weightless\". However, since objects in microgravity still retain their mass and inertia, an astronaut must exert ten times as much force to accelerate a 10kilogram object at the same rate as a 1kilogram object.\n\nBecause at any given point on Earth the weight of an object is proportional to its mass, the mass of an object in kilograms is usually measured by comparing its weight to the weight of a standard mass, whose mass is known in kilograms, using a device called a weighing scale. The ratio of the force of gravity on the two objects, measured by the scale, is equal to the ratio of their masses.\n\nOn April 7, 1795, the gram was decreed in France to be \"the absolute weight of a volume of pure water equal to the cube of the hundredth part of the metre, and at the temperature of melting ice\".\n\nSince trade and commerce typically involve items significantly more massive than one gram, and since a mass standard made of water would be inconvenient and unstable, the regulation of commerce necessitated the manufacture of a \"practical realization\" of the water-based definition of mass. Accordingly, a provisional mass standard was made as a single-piece, metallic artifact one thousand times as massive as the gram—the kilogram.\n\nAt the same time, work was commissioned to precisely determine the mass of a cubic decimetre (one litre) of water. Although the decreed definition of the kilogram specified water at 0°C—its highly stable \"temperature\" point—the French chemist Louis Lefèvre-Gineau and the Italian naturalist Giovanni Fabbroni after several years of research chose to redefine the standard in 1799 to water's most stable \"density\" point: the temperature at which water reaches maximum density, which was measured at the time as 4°C.\nThey concluded that one cubic decimetre of water at its maximum density was equal to 99.9265% of the target mass of the provisional kilogram standard made four years earlier. That same year, 1799, an all-platinum kilogram prototype was fabricated with the objective that it would equal, as close as was scientifically feasible for the day, the mass of one cubic decimetre of water at 4°C. The prototype was presented to the Archives of the Republic in June and on December 10, 1799, the prototype was formally ratified as the \"kilogramme des Archives\" (Kilogram of the Archives) and the kilogram was defined as being equal to its mass. This standard stood for the next 90 years.\n\nSince 1889 the magnitude of the kilogram has been defined as the mass of an object called the \"International Prototype of the Kilogram\", often referred to in the professional metrology world as the \"IPK\". The IPK is made of a platinum alloy known as \"Pt10Ir\", which is 90% platinum and 10% iridium (by mass) and is machined into a right-circular cylinder (height = diameter) of about 39millimetres to minimize its surface area. The addition of 10% iridium improved upon the all-platinum Kilogram of the Archives by greatly increasing hardness while still retaining platinum's many virtues: extreme resistance to oxidation, extremely high density (almost twice as dense as lead and more than 21 times as dense as water), satisfactory electrical and thermal conductivities, and low magnetic susceptibility. The IPK and its six sister copies are stored at the International Bureau of Weights and Measures (known by its French-language initials BIPM) in an environmentally monitored safe in the lower vault located in the basement of the BIPM's Pavillon de Breteuil in Saint-Cloud on the outskirts of Paris (see \"External images\", below, for photographs). Three independently controlled keys are required to open the vault. Official copies of the IPK were made available to other nations to serve as their national standards. These are compared to the IPK roughly every 40 years, thereby providing traceability of local measurements back to the IPK.\n\nThe Metre Convention was signed on May 20, 1875 and further formalized the metric system (a predecessor to the SI), quickly leading to the production of the IPK. The IPK is one of three cylinders made in 1879 by Johnson Matthey, which continues to manufacture nearly all of the national prototypes today. In 1883, the mass of the IPK was found to be indistinguishable from that of the \"Kilogramme des Archives\" made eighty-four years prior, and was formally ratified as \"the\" kilogram by the 1st CGPM in 1889.\n\nModern measurements of Vienna Standard Mean Ocean Water, which is pure distilled water with an isotopic composition representative of the average of the world's oceans, show that it has a density of at its point of maximum density (3.984 °C) under one standard atmosphere (101 325 Pa or 760 torr) of pressure. Thus, a cubic decimetre of water at its point of maximum density is only 25 parts per million less massive than the IPK; that is to say, the 25 milligram difference shows that the scientists over years ago managed to make the mass of the Kilogram of the Archives equal that of a cubic decimetre of water at 4 °C, with a margin of error \"at most\" within the mass of a single excess grain of rice.\n\nThe various copies of the international prototype kilogram are given the following designations in the literature:\n\n\nBy definition, the error in the measured value of the IPK's mass is exactly zero; the mass of the IPK \"is\" the kilogram. However, any changes in the IPK's mass over time can be deduced by comparing its mass to that of its official copies stored throughout the world, a rarely undertaken process called \"periodic verification\". The only three verifications occurred in 1889, 1948, and 1989. For instance, the US owns four 10%iridium (Pt10Ir) kilogram standards, two of which, K4 and K20, are from the original batch of 40 replicas delivered in 1884. The K20 prototype was designated as the primary national standard of mass for the US. Both of these, as well as those from other nations, are periodically returned to the BIPM for verification. Great care is exercised when transporting prototypes. In 1984, the K4 and K20 prototypes were hand-carried in the passenger section of separate commercial airliners.\n\nNote that none of the replicas has a mass precisely equal to that of the IPK; their masses are calibrated and documented as offset values. For instance, K20, the US's primary standard, originally had an official mass of (micrograms) in 1889; that is to say, K20 was 39μg less than the IPK. A verification performed in 1948 showed a mass of . The latest verification performed in 1989 shows a mass precisely identical to its original 1889 value. Quite unlike transient variations such as this, the US's check standard, K4, has persistently declined in mass relative to the IPK—and for an identifiable reason: check standards are used much more often than primary standards and are prone to scratches and other wear. K4 was originally delivered with an official mass of in 1889, but as of 1989 was officially calibrated at and ten years later was Over a period of 110 years, K4 lost 41μg relative to the IPK.\n\nBeyond the simple wear that check standards can experience, the mass of even the carefully stored national prototypes can drift relative to the IPK for a variety of reasons, some known and some unknown. Since the IPK and its replicas are stored in air (albeit under two or more nested bell jars), they gain mass through adsorption of atmospheric contamination onto their surfaces. Accordingly, they are cleaned in a process the BIPM developed between 1939 and 1946 known as \"the BIPM cleaning method\" that comprises firmly rubbing with a chamois soaked in equal parts ether and ethanol, followed by steam cleaning with bi-distilled water, and allowing the prototypes to settle for days before verification. Before the BIPM's published report in 1994 detailing the relative change in mass of the prototypes, different standard bodies used different techniques to clean their prototypes. The NIST's practice before then was to soak and rinse its two prototypes first in benzene, then in ethanol, and to then clean them with a jet of bi-distilled water steam. Cleaning the prototypes removes between 5 and 60μg of contamination depending largely on the time elapsed since the last cleaning. Further, a second cleaning can remove up to 10μg more. After cleaning—even when they are stored under their bell jars—the IPK and its replicas immediately begin gaining mass again. The BIPM even developed a model of this gain and concluded that it averaged 1.11μg per month for the first 3 months after cleaning and then decreased to an average of about 1μg per year thereafter. Since check standards like K4 are not cleaned for routine calibrations of other mass standards—a precaution to minimize the potential for wear and handling damage—the BIPM's model of time-dependent mass gain has been used as an \"after cleaning\" correction factor.\nBecause the first forty official copies are made of the same alloy as the IPK and are stored under similar conditions, periodic verifications using a large number of replicas—especially the national primary standards, which are rarely used—can convincingly demonstrate the stability of the IPK. What has become clear after the third periodic verification performed between 1988 and 1992 is that masses of the entire worldwide ensemble of prototypes have been slowly but inexorably diverging from each other. It is also clear that the mass of the IPK lost perhaps 50μg over the last century, and possibly significantly more, in comparison to its official copies. The reason for this drift has eluded physicists who have dedicated their careers to the SI unit of mass. No plausible mechanism has been proposed to explain either a steady decrease in the mass of the IPK, or an increase in that of its replicas dispersed throughout the world. Moreover, there are no technical means available to determine whether or not the entire worldwide ensemble of prototypes suffers from even greater long-term trends upwards or downwards because their mass \"relative to an invariant of nature is unknown at a level below 1000μg over a period of 100 or even 50 years\". Given the lack of data identifying which of the world's kilogram prototypes has been most stable in absolute terms, it is equally valid to state that the first batch of replicas has, as a group, gained an average of about 25μg over one hundred years in comparison to the IPK.\n\nWhat \"is\" known specifically about the IPK is that it exhibits a short-term instability of about 30μg over a period of about a month in its after-cleaned mass. The precise reason for this short-term instability is not understood but is thought to entail surface effects: microscopic differences between the prototypes' polished surfaces, possibly aggravated by hydrogen absorption due to catalysis of the volatile organic compounds that slowly deposit onto the prototypes as well as the hydrocarbon-based solvents used to clean them.\n\nIt has been possible to rule out many explanations of the observed divergences in the masses of the world's prototypes proposed by scientists and the general public. The BIPM's FAQ explains, for example, that the divergence is dependent on the amount of time elapsed between measurements and not dependent on the number of times the prototype or its copies have been cleaned or possible changes in gravity or environment. Reports published in 2013 by Peter Cumpson of Newcastle University based on the X-ray photoelectron spectroscopy of samples that were stored alongside various prototype kilograms suggested that one source of the divergence between the various prototypes could be traced to mercury that had been absorbed by the prototypes being in the proximity of mercury-based instruments. The IPK has been stored within centimetres of a mercury thermometer since at least as far back as the late 1980s. In this Newcastle University work six platinum weights made in the nineteenth century were all found to have mercury at the surface, the most contaminated of which had the equivalent of 250μg of mercury when scaled to the surface area of a kilogram prototype.\n\nScientists are seeing far greater variability in the prototypes than previously believed. The increasing divergence in the masses of the world's prototypes and the short-term instability in the IPK has prompted research into improved methods to obtain a smooth surface finish using diamond turning on newly manufactured replicas and was one of the reasons that led to the redefinition of the Kilogram. See ', below.\n\nThe stability of the IPK is crucial because the kilogram underpins much of the SI system of measurement as it is currently defined and structured. For instance, the newton is defined as the force necessary to accelerate one kilogram at one metre per second squared. If the mass of the IPK were to change slightly then the newton would also change proportionally. In turn, the pascal, the SI unit of pressure, is defined in terms of the newton. This chain of dependency follows to many other SI units of measure. For instance, the joule, the SI unit of energy, is defined as that expended when a force of one newton acts through one metre. Next to be affected is the SI unit of power, the watt, which is one joule per second. The ampere too is defined relative to the newton.\n\nWith the magnitude of the primary units of electricity thus determined by the kilogram, so too follow many others, namely the coulomb, volt, tesla, and weber. Even units used in the measure of light would be affected; the candela—following the change in the would in turn affect the lumen andlux.\n\nBecause the magnitude of many of the units comprising the SI system of measurement is ultimately defined by the mass of a -year-old, golf-ball-sized piece of metal, the quality of the IPK must be diligently protected to preserve the integrity of the SI system. Yet, despite the best stewardship, the average mass of the worldwide ensemble of prototypes and the mass of the IPK have likely diverged another μg since the third periodic verification years ago. Further, the world's national metrology laboratories must wait for the fourth periodic verification to confirm whether the historical trendspersisted.\n\nFortunately, \"definitions\" of the SI units are quite different from their \"practical realizations\". For instance, the metre is \"defined\" as the distance light travels in a vacuum during a time interval of of a second. However, the metre's \"practical realization\" typically takes the form of a helium–neon laser, and the metre's length is \"delineated\"—not defined—as wavelengths of light from this laser. Now suppose that the official measurement of the second was found to have drifted by a few parts per billion (it is actually extremely stable with a reproducibility of a few parts in 10). \nThere would be no automatic effect on the metre because the second—and thus the metre's length—is abstracted via the laser comprising the metre's practical realization. Scientists performing metre calibrations would simply continue to measure out the same number of laser wavelengths until an agreement was reached to do otherwise. \nThe same is true with regard to the real-world dependency on the kilogram: if the mass of the IPK was found to have changed slightly, there would be no automatic effect upon the other units of measure because their practical realizations provide an insulating layer of abstraction. Any discrepancy would eventually have to be reconciled though, because the virtue of the SI system is its precise mathematical and logical harmony amongst its units. If the IPK's value were definitively proven to have changed, one solution would be to simply redefine the kilogram as being equal to the mass of the IPK plus an offset value, similarly to what is currently done with its replicas; e.g., \"the kilogram is equal to the mass of the (equivalent to 42μg).\n\nThe long-term solution to this problem, however, is to liberate the SI system's dependency on the IPK by developing a practical realization of the kilogram that can be reproduced in different laboratories by following a written specification. The units of measure in such a practical realization would have their magnitudes precisely defined and expressed in terms of fundamental physical constants. While major portions of the SI system would still be based on the kilogram, the kilogram would in turn be based on invariant, universal constants of nature. Much work towards that end is ongoing, though no alternative has yet achieved the uncertainty of 20 parts per billion (~20μg) required to improve upon the IPK.\n\nThe International Committee for Weights and Measures (CIPM) approved a proposed redefinition of SI base units in November 2018 that defines the kilogram by defining the Planck constant to be exactly . This approach effectively defines the kilogram in terms of the second and the metre, and will take effect in 2019.\n\nPrior to the redefinition the kilogram, and several other SI units based on the kilogram, were defined by a man-made metal \"artefact\": the \"Kilogram des Archives\" from 1799 to 1889, and the \"International Prototype Kilogram\" from 1889 onward. \n\nIn 1960, the metre, previously similarly having been defined with reference to a single platinum-iridium bar with two marks on it, was redefined in terms of an invariant physical constant (the wavelength of a particular emission of light emitted by krypton, and later the speed of light) so that the standard can be independently reproduced in different laboratories by following a written specification. \n\nAt the 94th Meeting of the International Committee for Weights and Measures (CIPM) in 2005, it was recommended that the same be done with the kilogram.\n\nIn October 2010, the CIPM voted to submit a resolution for consideration at the General Conference on Weights and Measures (CGPM), to \"take note of an intention\" that the kilogram be defined in terms of the Planck constant, \"h\" (which has dimensions of energy times time) together with other physical constants. This resolution was accepted by the 24th conference of the CGPM in October 2011 and further discussed at the 25th conference in 2014. Although the Committee recognised that significant progress had been made, they concluded that the data did not yet appear sufficiently robust to adopt the revised definition, and that work should continue to enable the adoption at the 26th meeting, scheduled for 2018. Such a definition would theoretically permit any apparatus that was capable of delineating the kilogram in terms of the Planck constant to be used as long as it possessed sufficient precision, accuracy and stability. The Kibble balance (discussed below) is one way do this.\n\nAs part of this project, a variety of very different technologies and approaches were considered and explored over many years. They too are covered below. Some of these now-abandoned approaches were based on equipment and procedures that would have enabled the reproducible production of new, kilogram-mass prototypes on demand (albeit with extraordinary effort) using measurement techniques and material properties that are ultimately based on, or traceable to, physical constants. Others were based on devices that measured either the acceleration or weight of hand-tuned kilogram test masses and which expressed their magnitudes in electrical terms via special components that permit traceability to physical constants. All approaches depend on converting a weight measurement to a mass, and therefore require the precise measurement of the strength of gravity in laboratories. All approaches would have precisely fixed one or more constants of nature at a defined value.\n\nThe Kibble balance (known as a \"watt balance\" before 2016) is essentially a single-pan weighing scale that measures the electric power necessary to oppose the weight of a kilogram test mass as it is pulled by Earth's gravity. It is a variation of an ampere balance, with an extra calibration step that eliminates the effect of geometry. The electric potential in the Kibble balance is delineated by a Josephson voltage standard, which allows voltage to be linked to an invariant constant of nature with extremely high precision and stability. Its circuit resistance is calibrated against a quantum Hall effect resistance standard.\n\nThe Kibble balance requires extremely precise measurement of the local gravitational acceleration \"g\" in the laboratory, using a gravimeter. For instance when the elevation of the centre of the gravimeter differs from that of the nearby test mass in the Kibble balance, the NIST compensates for Earth's gravity gradient of 309μGal per metre, which affects the weight of a one-kilogram test mass by about 316μg/m.\n\nIn April 2007, the NIST's implementation of the Kibble balance demonstrated a combined relative standard uncertainty (CRSU) of 36μg. The UK's National Physical Laboratory's Kibble balance demonstrated a CRSU of 70.3μg in 2007. That Kibble balance was disassembled and shipped in 2009 to Canada's Institute for National Measurement Standards (part of the National Research Council), where research and development with the device could continue.\n\nGravity and the nature of the Kibble balance, which oscillates test masses up and down against the local gravitational acceleration \"g\", are exploited so that mechanical power is compared against electrical power, which is the square of voltage divided by electrical resistance. However, \"g\" varies significantly—by nearly 1%—depending on where on the Earth's surface the measurement is made (see \"Earth's gravity\"). There are also slight seasonal variations in \"g\" at a location due to changes in underground water tables, and larger semimonthly and diurnal changes due to tidal distortions in the Earth's shape caused by the Moon and the Sun. Although \"g\" would not be a term in the \"definition\" of the kilogram, it would be crucial in the process of measurement of the kilogram when relating energy to power. Accordingly, \"g\" must be measured with at least as much precision and accuracy as are the other terms, so measurements of \"g\" must also be traceable to fundamental constants of nature. For the most precise work in mass metrology, \"g\" is measured using dropping-mass absolute gravimeters that contain an iodine-stabilized helium–neon laser interferometer. The fringe-signal, frequency-sweep output from the interferometer is measured with a rubidium atomic clock. Since this type of dropping-mass gravimeter derives its accuracy and stability from the constancy of the speed of light as well as the innate properties of helium, neon, and rubidium atoms, the 'gravity' term in the delineation of an all-electronic kilogram is also measured in terms of invariants of nature—and with very high precision. For instance, in the basement of the NIST's Gaithersburg facility in 2009, when measuring the gravity acting upon Pt10Ir test masses (which are denser, smaller, and have a slightly lower center of gravity inside the Kibble balance than stainless steel masses), the measured value was typically within 8 ppb of .\n\nThe virtue of electronic realizations like the Kibble balance is that the definition and dissemination of the kilogram would no longer be dependent upon the stability of kilogram prototypes, which must be very carefully handled and stored. It would free physicists from the need to rely on assumptions about the stability of those prototypes. Instead, hand-tuned, close-approximation mass standards would simply be weighed and documented as being equal to one kilogram plus an offset value. With the Kibble balance, while the kilogram would be \"delineated\" in electrical and gravity terms, all of which are traceable to invariants of nature; it would be \"defined\" in a manner that is directly traceable to three fundamental constants of nature. The Planck constant defines the kilogram in terms of the second and the metre. By fixing the Planck constant, the \"definition\" of the kilogram would in addition depend only on the \"definitions\" of the second and the metre. The definition of the second depends on a single defined physical constant: the ground state hyperfine splitting frequency of the caesium 133 atom Δ\"ν\"(Cs). The metre depends on the second and on an additional defined physical constant: the speed of light \"c\". Once the kilogram is redefined in this manner, physical objects such as the IPK will no longer be part of the definition, but will instead become \"transfer standards\".\n\nScales like the Kibble balance also permit more flexibility in choosing materials with especially desirable properties for mass standards. For instance, Pt10Ir could continue to be used so that the specific gravity of newly produced mass standards would be the same as existing national primary and check standards (≈21.55g/ml). This would reduce the relative uncertainty when making mass comparisons in air. Alternatively, entirely different materials and constructions could be explored with the objective of producing mass standards with greater stability. For instance, osmium-iridium alloys could be investigated if platinum's propensity to absorb hydrogen (due to catalysis of VOCs and hydrocarbon-based cleaning solvents) and atmospheric mercury proved to be sources of instability. Also, vapor-deposited, protective ceramic coatings like nitrides could be investigated for their suitability for chemically isolating these new alloys.\n\nThe challenge with Kibble balances is not only in reducing their uncertainty, but also in making them truly \"practical\" realizations of the kilogram. Nearly every aspect of Kibble balances and their support equipment requires such extraordinarily precise and accurate, state-of-the-art technology that—unlike a device like an atomic clock—few countries would currently choose to fund their operation. For instance, the NIST's Kibble balance used four resistance standards in 2007, each of which was rotated through the Kibble balance every two to six weeks after being calibrated in a different part of NIST headquarters facility in Gaithersburg, Maryland. It was found that simply moving the resistance standards down the hall to the Kibble balance after calibration altered their values 10ppb (equivalent to 10μg) or more. Present-day technology is insufficient to permit stable operation of Kibble balances between even biannual calibrations. When the new definition takes effect, it is likely there will only be a few—at most—Kibble balances initially operating in the world.\n\nSeveral alternative approaches to redefining the kilogram that were fundamentally different from the Kibble balance were explored to varying degrees, with some abandoned. The Avogadro project, in particular, was important for the 2018 redefinition decision because it provided an accurate measurement of the Planck constant that was consistent with and independent of the Kibble balance method. The alternative approaches included:\n\nAnother Avogadro constant-based approach, known as the International Avogadro Coordination's \"Avogadro project\", would define and delineate the kilogram as a 93.6mm diameter sphere of silicon atoms. Silicon was chosen because a commercial infrastructure with mature processes for creating defect-free, ultra-pure monocrystalline silicon already exists to service the semiconductor industry. To make a practical realization of the kilogram, a silicon boule (a rod-like, single-crystal ingot) would be produced. Its isotopic composition would be measured with a mass spectrometer to determine its average relative atomic mass. The boule would be cut, ground, and polished into spheres. The size of a select sphere would be measured using optical interferometry to an uncertainty of about 0.3nm on the radius—roughly a single atomic layer. The precise lattice spacing between the atoms in its crystal structure (≈192pm) would be measured using a scanning X-ray interferometer. This permits its atomic spacing to be determined with an uncertainty of only three parts per billion. With the size of the sphere, its average atomic mass, and its atomic spacing known, the required sphere diameter can be calculated with sufficient precision and low uncertainty to enable it to be finish-polished to a target mass of one kilogram.\n\nExperiments are being performed on the Avogadro Project's silicon spheres to determine whether their masses are most stable when stored in a vacuum, a partial vacuum, or ambient pressure. However, no technical means currently exist to prove a long-term stability any better than that of the IPK's, because the most sensitive and accurate measurements of mass are made with dual-pan balances like the BIPM's FB2 flexure-strip balance (see \", below). Balances can only compare the mass of a silicon sphere to that of a reference mass. Given the latest understanding of the lack of long-term mass stability with the IPK and its replicas, there is no known, perfectly stable mass artefact to compare against. Single-pan scales, which measure weight relative to an invariant of nature, are not precise to the necessary long-term uncertainty of 10–20 parts per billion. Another issue to be overcome is that silicon oxidizes and forms a thin layer (equivalent to silicon atoms deep) of silicon dioxide (quartz) and silicon monoxide. This layer slightly increases the mass of the sphere, an effect that must be accounted for when polishing the sphere to its finished size. Oxidation is not an issue with platinum and iridium, both of which are noble metals that are roughly as cathodic as oxygen and therefore don't oxidize unless coaxed to do so in the laboratory. The presence of the thin oxide layer on a silicon-sphere mass prototype places additional restrictions on the procedures that might be suitable to clean it to avoid changing the layer's thickness or oxide stoichiometry.\n\nAll silicon-based approaches would fix the Avogadro constant but vary in the details of the definition of the kilogram. One approach would use silicon with all three of its natural isotopes present. About 7.78% of silicon comprises the two heavier isotopes: Si and Si. As described in \" above, this method would \"define\" the magnitude of the kilogram in terms of a certain number of C atoms by fixing the Avogadro constant; the silicon sphere would be the \"practical realization\". This approach could accurately delineate the magnitude of the kilogram because the masses of the three silicon nuclides relative to C are known with great precision (relative uncertainties of 1ppb or better). An alternative method for creating a silicon sphere-based kilogram proposes to use isotopic separation techniques to enrich the silicon until it is nearly pure Si, which has a relative atomic mass of . With this approach, the Avogadro constant would not only be fixed, but so too would the atomic mass of Si. As such, the definition of the kilogram would be decoupled from C and the kilogram would instead be defined as atoms of Si (≈ fixed moles of Si atoms). Physicists could elect to define the kilogram in terms of Si even when kilogram prototypes are made of natural silicon (all three isotopes present). Even with a kilogram definition based on theoretically pure Si, a silicon-sphere prototype made of only nearly pure Si would necessarily deviate slightly from the defined number of moles of silicon to compensate for various chemical and isotopic impurities as well as the effect of surface oxides.\n\nThough not offering a practical realization, this definition would precisely define the magnitude of the kilogram in terms of a certain number of carbon12 atoms. Carbon12 (C) is an isotope of carbon. The mole is currently defined as \"the quantity of entities (elementary particles like atoms or molecules) equal to the number of atoms in 12 grams of carbon12\". Thus, the current definition of the mole requires that moles ( mol) of C has a mass of precisely one kilogram. The number of atoms in a mole, a quantity known as the Avogadro constant, is experimentally determined, and the current best estimate of its value is This new definition of the kilogram proposed to fix the Avogadro constant at precisely with the kilogram being defined as \"the mass equal to that of atoms of C\".\n\nThe accuracy of the measured value of the Avogadro constant is currently limited by the uncertainty in the value of the Planck constant. That relative standard uncertainty has been 50parts per billion (ppb) since 2006. By fixing the Avogadro constant, the practical effect of this proposal would be that the uncertainty in the mass of a C atom—and the magnitude of the kilogram—could be no better than the current 50ppb uncertainty in the Planck constant. Under this proposal, the magnitude of the kilogram would be subject to future refinement as improved measurements of the value of the Planck constant become available; electronic realizations of the kilogram would be recalibrated as required. Conversely, an electronic \"definition\" of the kilogram (see \"\", below), which would precisely fix the Planck constant, would continue to allow moles of C to have a mass of precisely one kilogram but the number of atoms comprising a mole (the Avogadro constant) would continue to be subject to future refinement.\n\nA variation on a C-based definition proposes to define the Avogadro constant as being precisely (≈) atoms. An imaginary realization of a 12-gram mass prototype would be a cube of C atoms measuring precisely atoms across on a side. With this proposal, the kilogram would be defined as \"the mass equal to × atoms of C.\"\n\nAnother Avogadro-based approach, ion accumulation, since abandoned, would have defined and delineated the kilogram by precisely creating new metal prototypes on demand. It would have done so by accumulating gold or bismuth ions (atoms stripped of an electron) and counting them by measuring the electric current required to neutralize the ions. Gold (Au) and bismuth (Bi) were chosen because they can be safely handled and have the two highest atomic masses among the mononuclidic elements that are stable (gold) or effectively so (bismuth). See also \"Table of nuclides\".\n\nWith a gold-based definition of the kilogram for instance, the relative atomic mass of gold could have been fixed as precisely , from the current value of . As with a definition based upon carbon12, the Avogadro constant would also have been fixed. The kilogram would then have been defined as \"the mass equal to that of precisely atoms of gold\" (precisely 3,057,443,620,887,933,963,384,315 atoms of gold or about fixed moles).\n\nIn 2003, German experiments with gold at a current of only demonstrated a relative uncertainty of 1.5%. Follow-on experiments using bismuth ions and a current of 30mA were expected to accumulate a mass of 30g in six days and to have a relative uncertainty of better than 1 ppm. Ultimately, ionaccumulation approaches proved to be unsuitable. Measurements required months and the data proved too erratic for the technique to be considered a viable future replacement to the IPK.\n\nAmong the many technical challenges of the ion-deposition apparatus was obtaining a sufficiently high ion current (mass deposition rate) while simultaneously decelerating the ions so they could all deposit onto a target electrode embedded in a balance pan. Experiments with gold showed the ions had to be decelerated to very low energies to avoid sputtering effects—a phenomenon whereby ions that had already been counted ricochet off the target electrode or even dislodged atoms that had already been deposited. The deposited mass fraction in the 2003 German experiments only approached very close to 100% at ion energies of less than around (<1km/s for gold).\n\nIf the kilogram had been defined as a precise quantity of gold or bismuth atoms deposited with an electric current, not only would the Avogadro constant and the atomic mass of gold or bismuth have to have been precisely fixed, but also the value of the elementary charge (\"e\"), likely to (from the currently recommended value of ). Doing so would have effectively defined the ampere as a flow of electrons per second past a fixed point in an electric circuit. The SI unit of mass would have been fully defined by having precisely fixed the values of the Avogadro constant and elementary charge, and by exploiting the fact that the atomic masses of bismuth and gold atoms are invariant, universal constants of nature.\n\nBeyond the slowness of making a new mass standard and the poor reproducibility, there were other intrinsic shortcomings to the ionaccumulation approach that proved to be formidable obstacles to ion-accumulation-based techniques becoming a practical realization. The apparatus necessarily required that the deposition chamber have an integral balance system to enable the convenient calibration of a reasonable quantity of transfer standards relative to any single internal ion-deposited prototype. Furthermore, the mass prototypes produced by ion deposition techniques would have been nothing like the freestanding platinum-iridium prototypes currently in use; they would have been deposited onto—and become part of—an electrode imbedded into one pan of a special balance integrated into the device. Moreover, the ion-deposited mass wouldn't have had a hard, highly polished surface that can be vigorously cleaned like those of current prototypes. Gold, while dense and a noble metal (resistant to oxidation and the formation of other compounds), is extremely soft so an internal gold prototype would have to be kept well isolated and scrupulously clean to avoid contamination and the potential of wear from having to remove the contamination. Bismuth, which is an inexpensive metal used in low-temperature solders, slowly oxidizes when exposed to room-temperature air and forms other chemical compounds and so would not have produced stable reference masses unless it was continually maintained in a vacuum or inert atmosphere.\n\nThis approach would define the kilogram as \"the mass which would be accelerated at precisely when subjected to the per-metre force between two straight parallel conductors of infinite length, of negligible circular cross section, placed one metre apart in vacuum, through which flow a constant current of elementary charges per second\".\n\nEffectively, this would define the kilogram as a derivative of the ampere rather than the present relationship, which defines the ampere as a derivative of the kilogram. This redefinition of the kilogram would specify elementary charge (\"e\") as precisely coulomb rather than the current recommended value of It would necessarily follow that the ampere (one coulomb per second) would also become an electric current of this precise quantity of elementary charges per second passing a given point in an electric circuit.\nThe virtue of a practical realization based upon this definition is that unlike the Kibble balance and other scale-based methods, all of which require the careful characterization of gravity in the laboratory, this method delineates the magnitude of the kilogram directly in the very terms that define the nature of mass: acceleration due to an applied force. Unfortunately, it is extremely difficult to develop a practical realization based upon accelerating masses. Experiments over a period of years in Japan with a superconducting, 30g mass supported by diamagnetic levitation never achieved an uncertainty better than ten parts per million. Magnetic hysteresis was one of the limiting issues. Other groups performed similar research that used different techniques to levitate the mass.\n\nBecause SI prefixes may not be concatenated (serially linked) within the name or symbol for a unit of measure, SI prefixes are used with the \"gram\", not the kilogram, which already has a prefix as part of its name. For instance, one-millionth of a kilogram is 1mg (one milligram), not 1μkg (one microkilogram).\n\n\n"}
{"id": "1573422", "url": "https://en.wikipedia.org/wiki?curid=1573422", "title": "Leachate", "text": "Leachate\n\nA leachate is any liquid that, in the course of passing through matter, extracts soluble or suspended solids, or any other component of the material through which it has passed.\n\nLeachate is a widely used term in the environmental sciences where it has the specific meaning of a liquid that has dissolved or entrained environmentally harmful substances that may then enter the environment. It is most commonly used in the context of land-filling of putrescible or industrial waste.\n\nIn the narrow environmental context leachate is therefore any liquid material that drains from land or stockpiled material and contains significantly elevated concentrations of undesirable material derived from the material that it has passed through.\n\nLeachate from a landfill varies widely in composition depending on the age of the landfill and the type of waste that it contains. It usually contains both dissolved and suspended material. The generation of leachate is caused principally by precipitation percolating through waste deposited in a landfill. Once in contact with decomposing solid waste, the percolating water becomes contaminated, and if it then flows out of the waste material it is termed leachate. Additional leachate volume is produced during this decomposition of carbonaceous material producing a wide range of other materials including methane, carbon dioxide and a complex mixture of organic acids, aldehydes, alcohols and simple sugars.\n\nThe risks of leachate generation can be mitigated by properly designed and engineered landfill sites, such as those that are constructed on geologically impermeable materials or sites that use impermeable liners made of geomembranes or engineered clay. The use of linings is now mandatory within the United States, Australia and the European Union except where the waste is deemed inert. In addition, most toxic and difficult materials are now specifically excluded from landfilling. However, despite much stricter statutory controls, leachates from modern sites are often found to contain a range of contaminants stemming from illegal activity or legally discarded household and domestic products.\n\nWhen water percolates through waste, it promotes and assists the process of decomposition by bacteria and fungi. These processes in turn release by-products of decomposition and rapidly use up any available oxygen, creating an anoxic environment. In actively decomposing waste, the temperature rises and the pH falls rapidly with the result that many metal ions that are relatively insoluble at neutral pH become dissolved in the developing leachate. The decomposition processes themselves release more water, which adds to the volume of leachate. Leachate also reacts with materials that are not prone to decomposition themselves, such as fire ash, cement-based building materials and gypsum-based materials changing the chemical composition. In sites with large volumes of building waste, especially those containing gypsum plaster, the reaction of leachate with the gypsum can generate large volumes of hydrogen sulfide, which may be released in the leachate and may also form a large component of the landfill gas.\n\nIn a landfill that receives a mixture of municipal, commercial, and mixed industrial waste but excludes significant amounts of concentrated chemical waste, landfill leachate may be characterized as a water-based solution of four groups of contaminants: dissolved organic matter (alcohols, acids, aldehydes, short chain sugars etc.), inorganic macro components (common cations and anions including sulfate, chloride, iron, aluminium, zinc and ammonia), heavy metals (Pb, Ni, Cu, Hg), and xenobiotic organic compounds such as halogenated organics, (PCBs, dioxins, etc.).\n\nThe physical appearance of leachate when it emerges from a typical landfill site is a strongly odoured black-, yellow- or orange-coloured cloudy liquid. The smell is acidic and offensive and may be very pervasive because of hydrogen-, nitrogen- and sulfur-rich organic species such as mercaptans.\n\nIn older landfills and those with no membrane between the waste and the underlying geology, leachate is free to leave the waste and flow directly into the groundwater. In such cases, high concentrations of leachate are often found in nearby springs and flushes. As leachate first emerges it can be black in colour, anoxic, and possibly effervescent, with dissolved and entrained gases. As it becomes oxygenated it tends to turn brown or yellow because of the presence of iron salts in solution and in suspension. It also quickly develops a bacterial flora often comprising substantial growths of \"Sphaerotilus natans\".\n\nIn the UK, in the late 1960s, central Government policy was to ensure new landfill sites were being chosen with permeable underlying geological strata to avoid the build-up of leachate. This policy was dubbed \"dilute and disperse\". However, following a number of cases where this policy was seen to be failing, and an exposee in \"The Sunday Times\" of serious environmental damage being caused by inappropriate disposal of industrial wastes, both policy and the law were changed. The Deposit of Poisonous Wastes Act 1972, together with The 1974 Local Government Act, made local government responsible for waste disposal and for the enforcement of environmental standards regarding waste disposal. \n\nProposed landfill locations also had to be justified not only by geography but also scientifically. Many European countries decided to select landfill sites in groundwater-free clay geological conditions or to require that the site have an engineered lining. In the wake of European advancements, the United States increased its development of leachate retaining and collection systems. This quickly led from lining in principle to the use of multiple lining layers in all landfills (excepting those truly inert).\n\nThe primary criterion for design of the leachate system is that all leachate be collected and removed from the landfill at a rate sufficient to prevent an unacceptable hydraulic head occurring at any point over the lining system.\n\nThere are many components to a collection system including pumps, manholes, discharge lines and liquid level monitors. However, there are four main components which govern the overall efficiency of the system. These four elements are liners, filters, pumps and sumps.\n\nNatural and synthetic liners may be utilized as both a collection device and as a means for isolating leachate within the fill to protect the soil and groundwater below. The chief concern is the ability of a liner to maintain integrity and impermeability over the life of the landfill. Subsurface water monitoring, leachate collection, and clay liners are commonly included in the design and construction of a waste landfill. To effectively serve the purpose of containing leachate in a landfill, a liner system must possess a number of physical properties. The liner must have high tensile strength, flexibility, and elongation without failure. It is also important that the liner resist abrasion, puncture, and chemical degradation by leachate. Lastly, the liner must withstand temperature variation and be black (to resist UV light), easily installed, and economical.\n\nThere are several types of liners used in leachate control and collection. These types include geomembranes, geosynthetic clay liners, geotextiles, geogrids, geonets, and geocomposites. Each style of liner has specific uses and abilities. Geomembranes are used to provide a barrier between mobile polluting substances released from wastes and the groundwater. In the closing of landfills, geomembranes are used to provide a low-permeability cover barrier to prevent the intrusion of rain water. Geosynthetic clay liners (GCLs) are fabricated by distributing sodium bentonite in a uniform thickness between woven and non-woven geotextiles. Sodium bentonite has a low permeability, which makes GCLs a suitable alternative to clay liners in a composite liner system. Geotextiles are used as separation between two different types of soils to prevent contamination of the lower layer by the upper layer. Geotextiles also act as a cushion to protect synthetic layers against puncture from underlying and overlaying rocks. Geogrids are structural synthetic materials used in slope veneer stability to create stability for cover soils over synthetic liners or as soil reinforcement in steep slopes. Geonets are synthetic drainage materials that are often used in lieu of sand and gravel. Radz can take of drainage sand, thus increasing the landfill space for waste. Geocomposites are a combination of synthetic materials that are ordinarily used singly. A common type of geocomposite is a geonet that is heat-bonded to two layers of geotextile, one on each side. The geocomposite serves as a filter and drainage medium.\n\nGeosynthetic clay liners are a type of combination liner. One advantage to using a geosynthetic clay liner (GCL) is the ability to order exact amounts of the liner. Ordering precise amounts from the manufacturer prevents surplus and over-spending. Another advantage to GCLs is that the liner can be used in areas without an adequate clay source. On the other hand, GCLs are heavy and cumbersome, and their installation is very labor-intensive. In addition to being arduous and difficult under normal conditions, installation can be cancelled during damp conditions because the bentonite would absorb the moisture, making the job even more burdensome and tedious.\n\nThe leachate drainage system is responsible for the collection and transport of the leachate collected inside the liner. The pipe dimensions, type, and layout must all be planned with the weight and pressure of waste, and transport vehicles in mind. The pipes are located on the floor of the cell. Above the network lies an enormous amount of weight and pressure. To support this, the pipes can either be flexible or rigid, but the joints to connect the pipes yield better results if the connections are flexible. An alternative to placing the collection system underneath the waste is to position the conduits in trenches or above grade.\n\nThe collection pipe network of a leachate collection system drains, collects, and transports leachate through the drainage layer to a collection sump where it is removed for treatment or disposal. The pipes also serve as drains within the drainage layer to minimize the mounding of leachate in the layer. These pipes are designed with cuts that are inclined to 120 degrees, preventing entry of solid particles.\n\nThe filter layer is used above the drainage layer in leachate collection. There are two types of filters typically used in engineering practices: granular and geotextile. Granular filters consist of one or more soil layers or multiple layers having a coarser gradation in the direction of the seepage than the soil to be protected.\n\nAs liquid enters the landfill cell, it moves down the filter, passes through the pipe network, and rests in the sump. As collection systems are planned, the number, location, and size of the sumps are vital to an efficient operation. When designing sumps, the amount of leachate and liquid expected is the foremost concern. Areas in which rainfall is higher than average typically have larger sumps. A further criterion for sump planning is accounting for the pump capacity. The relationship of pump capacity and sump size is inverse. If the pump capacity is low, the volume of the sump should be larger than average. It is critical for the volume of the sump to be able to store the expected leachate between pumping cycles. This relationship helps maintain a healthy operation. Sump pumps can function with preset phase times. If the flow is not predictable, a predetermined leachate height level can automatically switch the system on. \n\nOther conditions for sump planning are maintenance and pump drawdown. Collection pipes typically convey the leachate by gravity to one or more sumps, depending upon the size of the area drained. Leachate collected in the sump is removed by pumping to a vehicle, to a holding facility for subsequent vehicle pickup, or to an on-site treatment facility. Sump dimensions are governed by the amount of leachate to be stored, pump capacity, and minimum pump drawdown. The volume of the sump must be sufficient to hold the maximum amount of leachate anticipated between pump cycles, plus an additional volume equal to the minimum pump drawdown volume. Sump size should also consider dimensional requirements for conducting maintenance and inspection activities. Sump pumps may operate with preset cycling times or, if leachate flow is less predictable, the pump may be automatically switched on when the leachate reaches a predetermined level.\n\nMore modern landfills in the developed world have some form of membrane separating the waste from the surrounding ground, and in such sites there is often a leachate collection series of pipes laid on the membrane to convey the leachate to a collection or treatment location. An example of a treatment system with only minor membrane use is the Nantmel Landfill Site.\n\nAll membranes are porous to a limited extent so that, over time, low volumes of leachate will cross the membrane. The design of landfill membranes is at such low volumes that they should never have a measurable adverse impact on the quality of the receiving groundwater. A more significant risk may be the failure or abandonment of the leachate collection system. Such systems are prone to internal failure as landfills suffer large internal movements as waste decomposes unevenly and thus buckles and distorts pipes. If a leachate collection system fails, leachate levels will slowly build in a site and may even over-top the containing membrane and flow out into the environment. Rising leachate levels can also wet waste masses that have previously been dry, triggering further active decomposition and leachate generation. Thus, what appears to be a stabilised and inactive site can become re-activated and restart significant gas production and exhibit significant changes in finished ground levels.\n\nOne method of leachate management that was more common in uncontained sites was leachate re-circulation, in which leachate was collected and re-injected into the waste mass. This process greatly accelerated decomposition and therefore gas production and had the impact of converting some leachate volume into landfill gas and reducing the overall volume of leachate for disposal. However, it also tended to increase substantially the concentrations of contaminant materials, making it a more difficult waste to treat.\n\nThe most common method of handling collected leachate is on-site treatment. When treating leachate on-site, the leachate is pumped from the sump into the treatment tanks. The leachate may then be mixed with chemical reagents to modify the pH and to coagulate and settle solids and to reduce the concentration of hazardous matter. Traditional treatment involved a modified form of activated sludge to substantially reduce the dissolved organic content. Nutrient imbalance can cause difficulties in maintaining an effective biological treatment stage. The treated liquor is rarely of sufficient quality to be released to the environment and may be tankered or piped to a local sewage treatment facility; the decision depends on the age of the landfill and on the limit of water quality that must be achieved after treatment. With high conductivity, leachate is hard to treat with biological treatment or chemical treatment.\n\nTreatment with reverse osmosis is also limited, resulting in low recoveries and fouling of the RO membranes. Reverse osmosis applicability is limited by conductivity, organics, and scaling inorganic elements such as CaSO4, Si, and Ba.\n\nIn some older landfills, leachate was directed to the sewers, but this can cause a number of problems. Toxic metals from leachate passing through the sewage treatment plant concentrate in the sewage sludge, making it difficult or dangerous to dispose of the sludge without incurring a risk to the environment. In Europe, regulations and controls have improved in recent decades, and toxic wastes are now no longer permitted to be disposed of in the Municipal Solid Waste landfills, and in most developed countries the metals problem has diminished. Paradoxically, however, as sewage treatment plant discharges are being improved throughout Europe and many other countries, the plant operators are finding that leachates are difficult waste streams to treat. This is because leachates contain very high ammoniacal nitrogen concentrations, are usually very acidic, are often anoxic and, if received in large volumes relative to the incoming sewage flow, lack the Phosphorus needed to prevent nutrient starvation for the biological communities that perform the sewage treatment processes. The result is that leachates are a difficult-to-treat waste stream. \n\nHowever, within ageing municipal solid waste landfills, this may not be a problem as the pH returns close to neutral after the initial stage of acidogenic leachate decomposition. Many sewer undertakers limit maximum ammoniacal nitrogen concentration in their sewers to 250 mg/l to protect sewer maintenance workers, as the WHO's maximum occupational safety limit would be exceeded at above pH 9 to 10, which is often the highest pH allowed in sewer discharges.\n\nMany older leachate streams also contained a variety of synthetic organic species and their decomposition products, some of which had the potential to be acutely damaging to the environment.\n\nThe risks from waste leachate are due to its high organic contaminant concentrations and high concentration of ammonia. Pathogenic microorganisms that might be present in it are often cited as the most important, but pathogenic organism counts reduce rapidly with time in the landfill, so this only applies to the freshest leachate. Toxic substances may, however, be present in variable concentrations, and their presence is related to the nature of the waste deposited.\nMost landfills containing organic material will produce methane, some of which dissolves in the leachate. This could, in theory, be released in poorly ventilated areas in the treatment plant. All plants in Europe must now be assessed under the EU ATEX Directive and zoned where explosion risks are identified to prevent future accidents. The most important requirement is the prevention of the discharge of dissolved methane from untreated leachate into public sewers, and most sewage treatment authorities limit the permissible discharge concentration of dissolved methane to 0.14 mg/l, or 1/10 of the lower explosive limit. This entails methane stripping from the leachate.\n\nThe greatest environmental risks occur in the discharges from older sites constructed before modern engineering standards became mandatory and also from sites in the developing world where modern standards have not been applied. There are also substantial risks from illegal sites and ad-hoc sites used by organizations outside the law to dispose of waste materials. Leachate streams running directly into the aquatic environment have both an acute and chronic impact on the environment, which may be very severe and can severely diminish bio-diversity and greatly reduce populations of sensitive species. Where toxic metals and organics are present this can lead to chronic toxin accumulation in both local and far distant populations. Rivers impacted by leachate are often yellow in appearance and often support severe overgrowths of sewage fungus.\n\nThe contemporary research in the field of assessment techniques and remedial technology of environmental issues originating from landfill leachate has been reviewed in an article published in Critical Reviews in Environmental Science and Technology journal .\n\nLeachate collection systems can experience many problems including clogging with mud or silt. Bioclogging can be exacerbated by the growth of micro-organisms in the conduit. The conditions in leachate collection systems are ideal for micro-organisms to multiply. Chemical reactions in the leachate may also cause clogging through generation of solid residues. The chemical composition of leachate can weaken pipe walls, which may then fail.\n\nLeachate can also be produced from land that was contaminated by chemicals or toxic materials used in industrial activities such as factories, mines or storage sites. Composting sites in areas of high rainfall also produce leachate.\n\nLeachate is also associated with stockpiled coal and with waste materials from metal ore mining and other rock extraction processes, especially those in which sulfide containing materials are exposed to air producing sulfuric acid, often with elevated metal concentrations.\n\nIn the context of civil engineering (more specifically reinforced concrete design), leachate refers to the effluent of pavement wash-off (that may include melting snow and ice with salt) that permeates through the cement paste onto the surface of the steel reinforcement, thereby catalyzing its oxidation and degradation. Leachates can be genotoxic in nature.\n"}
{"id": "2662177", "url": "https://en.wikipedia.org/wiki?curid=2662177", "title": "N-Oxoammonium salt", "text": "N-Oxoammonium salt\n\n\"N\"-Oxoammonium salts in organic chemistry are a class of organic compounds sharing a functional group with the general structure RRN=O X where X is the counterion. They are isoelectronic with carbonyls and structurally related to aldoximes (hydroxylamines) and aminoxyl (nitroxide) radicals, with which they can interconvert via a series of redox steps.\n\nThe \"N\"-oxoammonium salts are used for oxidation of alcohols to carbonyl groups, as well as other forms of oxoammonium-catalyzed oxidations. The stable radical TEMPO reacts via its \"N\"-oxoammonium salt.\n\n"}
{"id": "31414368", "url": "https://en.wikipedia.org/wiki?curid=31414368", "title": "Nuclear Institute for Food and Agriculture", "text": "Nuclear Institute for Food and Agriculture\n\nThe Nuclear Institute for Food and Agriculture, known as NIFA, is one of four agriculture and food irradiation research institute managed by the Pakistan Atomic Energy Commission. The institute is tasked to carry out research in Crop production and protection, soil fertility, water management and conservation and value addition of food resources, employing nuclear and other contemporary techniques.\n\nNIFA was the brainchild of Ishrat Hussain Usmani, bureaucrat and chairman of the Pakistan Atomic Energy Commission, however due to economic difficulties, the plans were not carried out until the 1980s. In 1982, Munir Ahmad Khan led the establishment of the institute and its first director was Abdul Rashid who revolutionized the institute.\n\nThe NIFA administers cobalt-60 radiation source, Laser absorption spectrometer and Atomic Absorption Spectrophotometry, Near-infrared spectrometer and Ultraviolet–visible spectroscopy.\n\nA library was opened in 1990, and recently, the institute has acquired 75 acres of land at CHASNUPP-I site.\n\n"}
{"id": "30060175", "url": "https://en.wikipedia.org/wiki?curid=30060175", "title": "Okinawa Electric Power Company", "text": "Okinawa Electric Power Company\n\n, OEPC or for short, is an electric utility with its exclusive operational area of Okinawa Prefecture, Japan. It is the smallest by electricity sales among Japan’s ten regional power utilities, indeed, its electricity sales is approximately ⁄ of that of The Tokyo Electric Power Company, though it is the largest by revenue among companies headquartered in Okinawa.\n"}
{"id": "331884", "url": "https://en.wikipedia.org/wiki?curid=331884", "title": "Particle horizon", "text": "Particle horizon\n\nThe particle horizon (also called the cosmological horizon, the comoving horizon (in Dodelson's text), or the cosmic light horizon) is the maximum distance from which particles could have traveled to the observer in the age of the universe. Much like the concept of a terrestrial horizon, it represents the boundary between the observable and the unobservable regions of the universe, so its distance at the present epoch defines the size of the observable universe. Due to the expansion of the universe it is not simply the age of the universe times the speed of light (approximately 13.8 billion light-years), but rather the speed of light times the conformal time. The existence, properties, and significance of a cosmological horizon depend on the particular cosmological model.\n\nIn terms of comoving distance, the particle horizon is equal to the conformal time formula_1 that has passed since the Big Bang, times the speed of light formula_2. In general, the conformal time at a certain time formula_3 is given by\n\nwhere formula_5 is the scale factor of the Friedmann–Lemaître–Robertson–Walker metric, and we have taken the Big Bang to be at formula_6. By convention, a subscript 0 indicates \"today\" so that the conformal time today formula_7. Note that the conformal time is not the age of the universe. Rather, the conformal time is the amount of time it would take a photon to travel from where we are located to the furthest observable distance provided the universe ceased expanding. As such, formula_8 is not a physically meaningful time (this much time has not yet actually passed), though, as we will see, the particle horizon with which it is associated is a conceptually meaningful distance.\n\nThe particle horizon recedes constantly as time passes and the conformal time grows. As such, the observed size of the universe always increases. Since proper distance at a given time is just comoving distance times the scale factor (with comoving distance normally defined to be equal to proper distance at the present time, so formula_9 at present), the proper distance to the particle horizon at time formula_3 is given by\n\nand for today formula_12\n\nIn this section we consider the FLRW cosmological model. In that context, the universe can be approximated as composed by non-interacting constituents, each one being a perfect fluid with density formula_14, partial pressure formula_15 and state equation formula_16, such that they add up to the total density formula_17 and total pressure formula_18. Let us now define the following functions:\n\n\nAny function with a zero subscript denote the function evaluated at the present time formula_25 (or equivalently formula_26). The last term can be taken to be formula_27 including the curvature state equation. It can be proved that the Hubble function is given by\n\nwhere formula_29. Notice that the addition ranges over all possible partial constituents and in particular there can be countably infinitely many. With this notation we have:\n\nwhere formula_31 is the largest formula_32 (possibly infinite). The evolution of the particle horizon for an expanding universe (formula_33) is:\n\nwhere formula_2 is the speed of light and can be taken to be formula_27 (natural units). Notice that the derivative is made with respect to the FLRW-time formula_3, while the functions are evaluated at the redshift formula_23 which are related as stated before. We have an analogous but slightly different result for event horizon.\n\nThe concept of a particle horizon can be used to illustrate the famous horizon problem, which is an unresolved issue associated with the Big Bang model. Extrapolating back to the time of recombination when the cosmic microwave background (CMB) was emitted, we obtain a particle horizon of about\n\nwhich corresponds to a proper size at that time of:\n\nSince we observe the CMB to be emitted essentially from our particle horizon (formula_39), our expectation is that parts of the cosmic microwave background (CMB) that are separated by about a fraction of a great circle across the sky of\n\n(an angular size of formula_40) should be out of causal contact with each other. That the entire CMB is in thermal equilibrium and approximates a blackbody so well is therefore not explained by the standard explanations about the way the expansion of the universe proceeds. The most popular resolution to this problem is cosmic inflation.\n\n"}
{"id": "56439506", "url": "https://en.wikipedia.org/wiki?curid=56439506", "title": "Plug-in electric vehicles in Europe", "text": "Plug-in electric vehicles in Europe\n\nThe adoption of plug-in electric vehicles in Europe is actively supported by the European Union and several European government have set public subsidies and other non-financial incentives to promote their widespread adoption. More than one million plug-in electric passenger cars and vans have been registered in Europe by June 2018, the second largest stock after China, with Norway as the leading market with almost 275,000 units. , France ranks second with almost 158,000, followed the UK with over 137,000 units.\n\n, France ranked as the largest European market for light-duty electric commercial vehicles or utility vans, accounting for nearly half of all vans sold in the European Union. The French market share of all-electric utility vans reached a market share of 1.22% of new vans registered in 2014, and 1.30% in 2015. Denmark is the second largest European market, with an 8.5% market share of all vans sold in the country in 2015. Most of the van sold in the Danish market are plug-in hybrids, accounting for almost all of the plug-in hybrid van sales across the EU.\n\nElectrification of transport (electromobility) figures prominently in the Green Car Initiative (GCI), included in the European Economic Recovery Plan. DG TREN is supporting a large European \"electromobility\" project on EVs and related infrastructure with a total budget of around as part of the Green Car Initiative.\n\nThere are measures to promote efficient vehicles in the Directive 2009/33/EC of the European Parliament and of the Council of 23 April 2009 on the promotion of clean and energy-efficient road transport vehicles and in the Directive 2006/32/EC of the European Parliament and of the Council of 5 April 2006 on energy end-use efficiency and energy services.\n\nAs of April 2011, 15 of the 27 European Union member states provide tax incentives for electrically chargeable vehicles, which includes all Western European countries plus the Czech Republic and Romania. Also 17 countries levy carbon dioxide related taxes on passenger cars as a disincentive. The incentives consist of tax reductions and exemptions, as well as of bonus payments for buyers of PEVs, hybrid vehicles, and some alternative fuel vehicles.\n\nMore than one million plug-in electric passenger cars and vans have been registered in Europe by June 2018, making the region the world's second largest after China. Of these, more than 212,000 light-duty vehicles were registered in Europe in 2016 (22.5%), and over 306,000 in 2017 (32.4%). The European stock of light-duty plug-in vehicles includes 501,798 battery electric cars and vans , of which, almost 150,000 (29.7%) were registered in 2017.\n\nCumulative sales of light-duty plug-in electric vehicles in Europe passed the 500,000 unit milestone in May 2016. Norway passed the 100,000th registered plug-in unit milestone in April 2016, France passed the same milestone in September 2016, and the Netherlands in November 2016. The UK achieved the 100,000 unit mark in March 2017.\n\n, European sales of plug-in cars and vans are led by Norway with over 200,000 units registered, followed by France with almost 158,000, and the UK with over 137,000 units. Norway was the top selling plug-in country market in Europe in 2016 and 2017. The other top selling European markets in terms of cumulative registrations are Germany (129,200), the Netherlands (121,500) and Sweden (50,000). The plug-in segment market share achieved a record 1.74% of new car registrations in 2017. As a result of its rapid growth during the first half of 2018, Germany, Europe’s biggest car market, is set to overtake Norway by the end of the year in terms of total annual sales.\n\nA total of 1,614 all-electric cars and 1,305 light-utility vehicles were sold in 2010. Sales jumped from 2,919 units in 2010 to 13,779 in 2011, consisting of 11,271 pure electric cars and 2,508 commercial vans. In addition, over 300 plug-in hybrids were sold in 2011, mainly Opel Amperas. Light-duty plug-in vehicle sales totaled 34,333 units in 2012, consisting of 24,713 all-electric cars and vans, and 9,620 plug-in hybrids. The Opel/Vauxhall Ampera plug-in hybrid was Europe's top selling plug-in electric car in 2012 with 5,268 units, closely followed by the all-electric Nissan Leaf with 5,210 units.\n\nThe plug-in segment sales more than double to 71,943 units in 2013. Pure electric passenger and light commercial vehicles sales increased by 63.9% to 40,496 units. In addition, a total of 31,477 extended-range cars and plug-in hybrids were sold in 2013. Registrations reached 104,746 light-duty plug-in electric vehicles in 2014, up 45.6% from 2013. A total of 65,199 pure electric cars and light-utility vehicles were registered in Europe in 2014, up 60.9% from 2013. All-electric passenger cars represented 87% of the European all-electric segment registrations. Extended-range cars and plug-in hybrid registrations totaled 39,547 units in 2014, up 25.8% from 2013.\n\nDuring 2013 took place a surge in sales of plug-in hybrids in the European market, particularly in the Netherlands, with 20,164 PHEVs registered during the year. Out of the 71,943 highway-capable plug-in electric passenger cars and utility vans sold in the region during 2013, plug-in hybrids totaled 31,447 units, representing 44% of the plug-in electric vehicle segment sales that year. This trend continued in 2014. Plug-in hybrids represented almost 30% of the plug-in electric drive sales during the first six months of 2014, and with the exception of the Nissan Leaf, sales of the previous European best selling models fell significantly, while recently introduced models captured a significant share of the segment sales, with the Mitsubishi Outlander P-HEV, Tesla Model S, BMW i3, Renault Zoe, Volkswagen e-Up!, and the Volvo V60 Plug-in Hybrid ranking among the top ten best selling models.\n\nIn 2014 Norway was the top selling country in the light-duty all-electric market segment, with 18,649 passenger cars and utility vans registered, more than doubling its 2013 sales. France ranked second with 15,046 units registered, followed by Germany with 8,804 units, the UK with 7,730 units, and the Netherlands with 3,585 car and vans registrations. In the plug-in hybrid segment, the Netherlands was the top selling country in 2014 with 12,425 passenger cars registered, followed by the UK with 7,821, Germany with 4,527, and Sweden 3,432 units. Five European countries achieved plug-in electric car sales with a market share higher than 1% of new car sales in 2014, Norway (13.84%), the Netherlands (3.87%), Iceland (2.71%), Estonia (1.57%), and Sweden (1.53%).\n\nIn 2013 the top selling plug-in was the Leaf with 11,120 units sold, followed by the Outlander P-HEV with 8,197 units. The Mitsubishi Outlander plug-in hybrid was the top selling plug-in electric vehicle in Europe in 2014 with 19,853 units sold, surpassing of the Nissan Leaf (14,658), which fell to second place. Ranking third was the Renault Zoe with 11,231 units.\n\nFor a second year running, the Mitsubishi’s Outlander P-HEV was the top selling plug-in electric car in Europe with 31,214 units sold in 2015, up 57% from 2014. The Renault Zoe ranked second among plug-in electric cars, with 18,727 registrations, and surpassed the Nissan Leaf to become best selling pure electric car in Europe in 2015. Ranking next were the Volkswagen Golf GTE plug-in hybrid (17,300), followed by the all-electric Tesla Model S (15,515) and the Nissan Leaf (15,455), the BMW i3, including its REx variant, (12,047), and the Audi A3 e-tron plug-in hybrid (11,791).\n\nThe Netherlands was the top selling country in the European light-duty plug-in electric market segment, with 43,971 passenger cars and utility vans registered in 2015. Norway ranked second with 34,455 units registered, followed by the UK with 28,188 units, France with 27,701 car and vans registrations, and Germany with 23,464 plug-in cars. Eight European countries achieved plug-in electric car sales with a market share higher than 1% of new car sales in 2015, Norway (22.4%), the Netherlands (9.7%), Iceland (2.9%), Sweden (2.6%), Denmark (2.3%), Switzerland (2.0%), France (1.2%) and the UK (1.1%). , almost 25% of the European plug-in stock was registered in the Nordic countries, with over 100,000 units registered. In 2015, combined registrations in the four countries were up 91% from 2014.\n\nFor the first time in the region, in 2015 plug-in hybrids (95,140) outsold all-electric cars (89,640) in the passenger car segment, however, when light-duty plug-in utility vehicles are accounted for, the all-electric segment totaled 97,687 registrations in 2015, up 65,199 in 2014, and ahead of the plug-in hybrid segment. Also in 2015, the European market share of plug-in electric cars passed the 1% mark for the first time, with a 1.41% share of new car sales that year. This trend continue during 2016. Since April 2016 plug-in hybrids have outsold all-electric cars, and the gap has continued to widen. Accounting for passenger plug-in car sales in Western Europe between January and July 2016, plug-in hybrids captured almost 54% of the region's plug-in market sales. During 2016 the all-electric car segment ended with a market share of 0.57% of new car sales, while plug-in hybrids reached a market share of 0.73%.\n\nEuropean sales of plug-in electric cars passed 200,000 units for the first time in 2016. The plug-in segment achieved a market share of 1.3% of total new car sales in 2016. Norway was the top selling plug-in car country in Europe in 2016 with 45,492 plug-in cars and vans registered, followed by the UK with about 36,907 units, France with 33,774, Germany with 25,154, the Netherlands with 24,645, and Sweden with 13,454. France was the top selling market in the light-duty all-electric segment with 27,307 units registered, up 23% from 2015. The plug-in car segment of ten European countries achieved a market share of new car sales above 1%, led by Norway with 29.1%, followed by the Netherlands with 6.4%, 6.4 %, Sweden with 3.5%, and Switzerland with 1.8%.\n\nThe Renault Zoe was the best-selling all-electric car in Europe in 2016 with 21,735 units delivered, and also topped European sales in the broader plug-in electric car segment, ahead of the Outlander P-HEV, the top selling plug-in in the previous two years. The Mitsubishi Outlander PHEV with 21,446 units sold was the second best-selling plug-in car, followed by the Nissan Leaf with 18,718. The Outlander PHEV has been Europe's best-selling plug-in hybrid vehicle for four years in a row, from 2013 to 2016. The top selling all-electric commercial van was the Nissan e-NV200 with 4,319 units registered.\n\nRegistrations totaled 306,143 units in 2017, of which, 149,086 units (48.6%) were all-electric cars and vans. The segment market share achieved a record 1.74% in 2017. Accounting for new registrations of plug-in passenger cars, Norway was Europe's top selling country in 2017 with 62,313 units, followed by Germany with 54,617, which more than doubled in 2017 and moved ahead of French and the British markets for the first time ever. Ranking next were the UK with 47,298, France with 36,835, and Sweden with 19,678 units. Norway also led the all-electric car segment with 33,025 new units registered, up 36.3% from 2016, and the UK led the plug-in hybrid car segment with 31,154 registrations, up 25.1% from 2016.\n\nIn 2017, sales in the Netherlands fell by 51.7% from 2016 due to changes in tax rules, and as a result, it was overtook by both Sweden (+48.4%) and Belgium (+59.2%). Denmark was the only other significant plug-in car market with weaker sales in 2017, down 30.1% from 2017 with the fall also due to a change in taxes. In addition to Germany, plug-in car sales also doubled in Spain (+104.6%) and Portugal (+121.2%), and sales also increased sales significantly in Italy (+71.2%).\n\nAmong all-electric cars, the top selling model was the Renault Zoe with 31,302 units, followed by the Nissan Leaf with 17,293. Combined sales of BMW i3 pure electric and REx models totaled 20,855 units, making the i3 Europe's second best selling plug-in car in 2017 after the Zoe. The best selling plug-in hybrids were the Outlander P-HEV with 19,189 units, the VW Passat GTE with 13,599 and the Mercedes Benz GLC 350e with 11,249.\n\n, the Mitsubishi Outlander P-HEV continues to rank as the all-time top selling plug-in electric car in the region with 100,097 units delivered since its launch in 2013, followed by the Renault Zoe with 91,927 units, the Nissan Leaf with 84,947 units, The Renault Kangoo Z.E. is the all-time top selling all-electric utility van with 29,150 units sold through December 2017.\n\nNorway is the country with the largest electric vehicle ownership per capita in the world. The government's target of 50,000 all-electric cars on Norwegian roads was reached in April 2015, more than two years earlier than expected, thanks to the successful policies implemented to promote electric vehicle adoption that include fiscal and non-monetary incentives. The milestone of 100,000 light-duty plug-in electric vehicles registered was achieved in April 2016. \n\n, the stock of light-duty plug-in electric vehicles registered in Norway totaled more than 200,000 units, including used imports, making Norway the European country with the largest stock of plug-in cars and vans, and the fourth largest in the world. Norway was the top selling plug-in country market in Europe in 2016, surpassing the Netherlands, Europe’s top market in 2015. Registrations achieved a market share of 29.1% of all new passenger cars registered in 2016. Again in 2017, Norway was Europe's top selling plug-in country. The segment market share rose to 39.2% in 2017.\n\nThe Norwegian fleet of plug-in electric cars is one of the cleanest in the world because 98% of the electricity generated in the country comes from hydropower. In March 2014, Norway became the first country where one in every 100 registered passenger cars was a plug-in electric. The plug-in car market penetration reached 5% at the end of 2016, and, , plug-in electric cars were 10% of all passenger cars on Norwegian roads.\n\nAlso the Norwegian plug-in electric vehicle market share of new car sales is the highest in the world. In January 2017, the electrified passenger car segment for the first time ever surpassed combined sales of cars with conventional diesel or gasoline engines. Sales of plug-in hybrids, all-electric cars and conventional hybrids achieved a combined market share of 51.4% of new car sales that month. In September 2018, the combined market share of the plug-in car segment reached 60.2% of new car registrations, becoming the world's highest-ever monthly market share for the plug-in passenger car segment in Norway, and in any country. Accounting for conventional hybrids, the electrified segment achieved a record 71.5% market share in September 2018.\n\nAlso, Norway was the first country in the world to have all-electric cars topping the new car sales monthly ranking. The Tesla Model S has been the top selling new car four times, and the Nissan Leaf has topped the monthly new car sales ranking twice. In March 2014 the Tesla Model S also broke the 28-year-old record for monthly sales of a single model regardless of its power source, surpassing the previous record set in May 1986.\n\n, the stock of light-duty plug-in electric vehicles registered in France totaled 149,797 plug-in cars and electric utility vans delivered since 2010, , and accounting for registrations since 2010, the plug-in electric stock consisted consisted of 92,256 all-electric passenger cars, 25,269 all-electric utility vans, and 32,272 plug-in hybrids. The stock of light-duty plug-in electric vehicles registered in France passed the 100,000 unit milestone in October 2016. Until December 2016, the country ranked as the third largest plug-in market in Europe after Norway and the Netherlands, and the world's sixth.\n\n, France is the country with the world's largest market for light-duty electric commercial vehicles or utility vans. Nearly half of the vans sold in the European Union are sold in France as a result of a national purchase incentive scheme, which French companies have embraced. The market share of all-electric utility vans reached a market share of 1.22% of new vans registered in 2014, and 1.30% in 2015.\n\nMore than 137,000 light-duty plug-in electric vehicles have been registered in the UK up until December 2017, including about 5,100 plug-in commercial vans. Since the launch of the Plug-In Car Grant in January 2011, a total of 127,509 eligible cars have been registered through December 2017, and, , the number of claims made through the Plug-in Van Grant scheme totaled 2,938 units since the launch of the scheme in 2012. \n\nBefore the introduction of series production plug-in vehicles, a total of 1,096 all-electric vehicles were registered in the UK between 2006 and December 2010. Before 2011, the G-Wiz, a heavy quadricycle, listed as the top-selling electric car for several years. \n\nUntil December 2016, the UK had the fourth largest European stock of light-duty plug-in vehicles. The UK ranked in 2016 as the second best-selling European market after Norway. In 2017, the British market led the European plug-in hybrid segment with 31,154 registrations, up 25.1% from 2016. Plug-in electric car registrations in 2017 achieved a record market share of 1.86% of new car sales.\n\n, a total of 129,246 plug-in electric cars have been registered in Germany since 2010. The country is the largest passenger car market in Europe, however, , ranked as the fifth largest plug-in market in Europe. About 80% of the plug-in cars registered in the country through September 2016 were registered since January 2014. The official German definition of electric vehicles changed at the beginning of 2013, before that, official statistics only registered all-electric vehicles because plug-in hybrids were accounted together with conventional hybrids. As a result, the registrations figures for 2012 and older do not account for total new plug-in electric car registrations.\n\nA record of 54,492 plug-in cars were registered in 2017, up 217% from 2016, and the plug-in car segment achieved a market share of 1.58% in 2017. The surge in sales allowed Germany to rank as Europe's second best selling market in 2017, and the country moved ahead of French and the British markets for the first time ever.\n\n, there were 121,542 highway legal light-duty plug-in electric vehicles registered in the Netherlands, consisting of 98,217 range-extended and plug-in hybrids, 21,115 pure electric cars, and 2,210 all-electric light utility vans. When buses, trucks, motorcycles, quadricycles and tricycles are accounted for, the Dutch plug-in electric-drive fleet climbs to 123,499 units. The country's electric vehicle stock reaches 165,886 units when fuel cell electric vehicles (43), mopeds (4,376), electric bicycles (37,652), and microcars (316) are accounted for. A distinct feature of the Dutch plug-in market is dominance of plug-in hybrids, which represented 81% of the country's stock of passenger plug-in electric cars and vans registered at the end of December 2017. , the Netherlands had the second largest plug-in market concentration per capita in the world after Norway.\n\nThe Netherlands listed as the world's third best-selling country market for light-duty plug-in vehicles in 2015, and, until December 2015, the Netherlands had Europe's largest fleet light-duty plug-in vehicles. Plug-in sales fell sharply during 2016, and as a result, the Netherlands listed as the third largest European plug-in market, after being surpassed by both Norway and France during 2016. The stock of light-duty plug-in electric vehicles registered in the Netherlands achieved the 100,000 unit milestone in November 2016. Due to changes in tax rules, the plug-in market share declined from 9.9% in 2015, to 6.7% in 2016, and fell to 2.6% in 2017.\n\n, a total of 50,304 light-duty plug-in electric vehicles have been registered since 2011, consisting of 36,405 plug-in hybrids, 12,223 all-electric cars and 1,676 all-electric utility vans. The Swedish plug-in electric market is dominated by plug-in hybrids, representing 72.4% of the Swedish light-duty plug-in electric vehicle registrations through December 2017. Sweden has ranked among the world's top ten best-selling plug-in markets for two years running, 2015 and 2016, listed in both years as the ninth largest country market. , the Swedish stock of plug-in cars and vans is the sixth largest in Europe. The plug-in passenger car segment achieved a record market share of 5.2% of new registrations in 2017.\n\nIn September 2011 the Swedish government approved a program, effective starting in January 2012, to provide a subsidy of per car for the purchase of 5,000 electric cars and other \"super green cars\" with ultra-low carbon emissions, defined as those with emissions below 50 grams of carbon dioxide () per km. After renewing appropriations for the super green car rebate several times, from 2016, only zero emissions cars are entitled to receive the full premium, while other super green cars, plug-in hybrids, receive half premium. Registrations of super clean cars increased five-fold in July 2014 driven by the end of the quota of 5,000 new cars eligible for the super clean car subsidy. \n\n"}
{"id": "24862", "url": "https://en.wikipedia.org/wiki?curid=24862", "title": "Preservative", "text": "Preservative\n\nA preservative is a substance or a chemical that is added to products such as food, beverages, pharmaceutical drugs, paints, biological samples, cosmetics, wood, and many other products to prevent decomposition by microbial growth or by undesirable chemical changes. In general, preservation is implemented in two modes, chemical and physical. Chemical preservation entails adding chemical compounds to the product. Physical preservation entails processes such as refrigeration or drying. Preservative food additives reduce the risk of foodborne infections, decrease microbial spoilage, and preserve fresh attributes and nutritional quality. Some physical techniques for food preservation include dehydration, UV-C radiation, freeze-drying, and refrigeration. Chemical preservation and physical preservation techniques are sometimes combined.\n\nAntimicrobial preservatives prevent degradation by bacteria. This method is the most traditional and ancient type of preserving—ancient methods such as pickling and adding honey prevent microorganism growth by modifying the pH level. The most commonly used antimicrobial preservative is lactic acid. Common antimicrobial preservatives are presented in the table. Nitrates and nitrites are also antimicrobial. The detailed mechanism of these chemical compounds range from inhibiting growth of the bacteria to the inhibition of specific enzymes.\n\nThe oxidation process spoils most food, especially those with a high fat content. Fats quickly turn rancid when exposed to oxygen. Antioxidants prevent or inhibit the oxidation process. The most common antioxidant additives are ascorbic acid (vitamin C ) and ascorbates. Thus, antioxidants are commonly added to oils, cheese, and chips. Other antioxidants include the phenol derivatives BHA, BHT, TBHQ and propyl gallate. These agents suppress the formation of hydroperoxides. Other preservatives include ethanol and methylchloroisothiazolinone.\nA variety of agents are added to sequester (deactivate) metal ions that otherwise catalyze the oxidation of fats. Common sequestering agents are disodium EDTA, citric acid (and citrates), tartaric acid, and lecithin.\n\nCitric and ascorbic acids target enzymes that degrade fruits and vegetables, e.g., mono/polyphenol oxidase which turns surfaces of cut apples and potatoes brown. Ascorbic acid and tocopherol, which are vitamins, are common preservatives. Smoking entails exposing food to a variety of phenols, which are antioxidants. Natural preservatives include rosemary and oregano extract, hops, salt, sugar, vinegar, alcohol, diatomaceous earth and castor oil.\n\nTraditional preservatives, such as sodium benzoate have raised health concerns in the past. Benzoate was shown in a study to cause hypersensitivity in some asthma sufferers. This has caused reexamination of natural preservatives which occur in vegetables.\n\nPreservatives have been used since prehistoric times. Smoked meat for example has phenols and other chemicals that delay spoilage. The preservation of foods has evolved greatly over the centuries and has been instrumental in increasing food security. The use of preservatives other than traditional oils, salts, paints, etc. in food began in the late 19th century, but was not widespread until the 20th century.\n\nThe use of food preservatives varies greatly depending on the country. Many developing countries that do not have strong governments to regulate food additives face either harmful levels of preservatives in foods or a complete avoidance of foods that are considered unnatural or foreign. These countries have also proven useful in case studies surrounding chemical preservatives, as they have been only recently introduced. In urban slums of highly populated countries, the knowledge about contents of food tends to be extremely low, despite consumption of these imported foods.\n\nIn ancient times the sun and wind naturally dried out foods. Middle Eastern and Oriental cultures started drying foods in 1,200 B.C. in the sun. The Romans used a lot of dry fruit. In the Middle Ages, the people made “still houses” where fruits, vegetables, and herbs were could dry out in climates that did not have strong sunlight for drying. Sometimes fires were made to create heat to dry foods. Drying prevents yeasts and bread molds (\"Rhizopus\") from growing by removing moisture so bacteria cannot grow.\n\nCellars, caves, and cool streams were used for freezing. American estates had ice houses built to store ice and food on the ice. Icehouse was then converted to an “icebox”. Icebox was converted in the 1800s to mechanical refrigeration. Clarence Birdseye found in the 1800s that freezing meats and vegetables at a low temperature made them taste better.\n\nFermenting was discovered when a few grains of barley were left in the rain and turned into beer. Microorganisms ferment the starch-derived sugars into alcohols. This is also how fruits are fermented into wine and cabbage into Kimchi or sauerkraut. Anthropologists believe that as early as 10,000 B.C people began to settle and grow barley. They began to make beer and believed that it was a gift from gods. It was used to preserve foods and to create more nutritious foods from less desirable ingredients. Vitamins are produced through fermentation by microorganisms making the end product more nutritious.\n\nPickling occurs when foods are placed in a container with vinegar or another acid. It is thought that pickling came about when people used to place food in wine or beer to preserve it due to them having a low pH. Containers had to be stoneware or glass (vinegar will dissolve metal from pots). After the food was eaten, the pickling brine had other uses. Romans would make a concentrated pickle sauce called “garum”. It was very concentrated and the dish that it would be used in would only need a few drops to get the fish taste. Due to new foods arriving from Europe in the 16th century, food preservation increased. Ketchup originated from Europe as an oriental fish brine and when it made it to America, sugar was added. Pickling sauces were soon part of many recipes such as chutneys, relish, piccalilli, mustard, and ketchup when different spices were added to them.\n\nThe beginning of curing was done through dehydration. Salting was used by early cultures to help desiccate foods. Many different salts were used from different places such as rock salt, sea salt, spiced salt, etc.. People began to experiment and found in the 1800s that some salts gave meat an appealing red color instead of the grey that they were used to. During their experimenting in the 1920s they realized this mixture of salts were nitrates (saltpeter) that prevented Clostridium botulinum growth.\n\nThe early cultures also used honey or sugar as a preservative. Greece used a quince and honey mixture with a slight amount of drying and then tightly packed into jars. The Romans used the same technique but instead cooked the honey and quince mixture to make a solid texture. The Indian and Oriental traders brought sugarcane to the northern climates where housewives were then able to learn to make preservatives by heating fruit with the sugarcane.\n\nCanning started in 1790 from a French confectioner, Nicolas Appert, when he found that by applying heat to food in sealed glass bottles, the food is free from spoilage. Appert’s ideas were tried by the French Navy with meat, vegetables, fruit, and milk in 1806. An Englishman, Peter Durand decided to use Appert’s method on tin cans in 1810. Even though Appert found a method that worked, he did not understand why it worked because many believed that the lack of air caused the preservation. In 1864 Louis Pasteur linked food spoilage/illness to microorganisms. Different foods are placed into jars or cans and heated to a microorganism and enzyme inactivating temperature. They are then cooled forming a vacuum seal which prevents microorganisms from contaminating the foods.\n\nPublic awareness of food preservatives is uneven. Americans have a perception that food-borne illnesses happen more often in other countries. This may be true, but the occurrence of illnesses, hospitalizations, and deaths are still high. It is estimated by the Center for Disease Control (CDC) that each year there are 76 million illnesses, 325,000 hospitalizations, and 5,000 deaths linked to food-borne illness.\n\nThe increasing demand for ready-to-eat fresh food products has led to challenges for food distributors regarding the safety and quality of their foods. Artificial preservatives meet some of these challenges by preserving freshness for longer periods of time, but these preservatives can cause negative side-effects as well. Sodium nitrite is a preservative used in lunch meats, hams, sausages, hot dogs, and bacon to prevent botulism. It serves the important function of controlling the bacteria that cause botulism, but sodium nitrite can react with proteins, or during cooking at high heats, to form carcinogenic N-nitrosamines. It has also been linked to cancer in lab animals. The commonly used sodium benzoate has been found to extend the shelf life of bottled tomato paste to 40 weeks without loss of quality. However, it can form the carcinogen benzene when combined with vitamin C. Many food manufacturers have reformed their products to eliminate this combination, but a risk still exists. Consumption of sodium benzoate may also cause hyperactivity. For over 30 years, there has been a debate about whether or not preservatives and other food additives can cause hyperactivity. Studies have found that there may be increases in hyperactivity amongst children who consume artificial colorings and benzoate preservatives and who are already genetically predisposed to hyperactivity, but these studies were not entirely conclusive. Hyperactivity only increased moderately, and it was not determined if the preservatives, colorings, or a combination of the two were responsible for the increase.\n"}
{"id": "21475026", "url": "https://en.wikipedia.org/wiki?curid=21475026", "title": "Riffgat", "text": "Riffgat\n\nRiffgat (also known as Borkum Riffgat and OWP Riffgat) is an offshore wind farm to the north-west of the German island of Borkum and north of the eponymous shipping channel in the southern North Sea. The wind turbines are built across an area of . It consists of 30 turbines with a total capacity of 113 megawatt (MW), and is expected to generate enough electricity for 112,000 households.\n\nEarly 2011, the Dutch government stated that the wind farm was partly in Dutch territory and protested against the issuing of construction licenses by the German government.\n\nBetween November 2015 and April 2016, transmission problems prevented Riffgat from exporting power.\n\n\n"}
{"id": "46801975", "url": "https://en.wikipedia.org/wiki?curid=46801975", "title": "Saint-Gobain SEFPRO", "text": "Saint-Gobain SEFPRO\n\nSaint-Gobain SEFPRO (Sintered and Electrofused Products), founded in 1929, produces refractories for the glass industry. The company consists of plants, sales offices and Research and Development Centers employing over 2200 people across four continents, with headquarters in Le Pontet, Vaucluse, France. The group belongs to the ‘Innovative Materials’ division of the Saint-Gobain group.\n\nThe first electro-fused product was invented and developed in the 1920s by the laboratory of Corning Glass Works, the American glassmaker partnered with Hartford Empire Co. Production began at Corhart Refractories Co., a Corning subsidiary, in 1927.\n\nIn 1929, a joint venture between Corning and Saint-Gobain, another leading glassmaker, established ‘L’Electro-Réfractaire\" in Modane, France, with their principal product the Corhart Standard. By 1940, the company was producing 2000 tons of the product per year. In 1942, a new, even more hard-wearing product was introduced: Corhart ZAC. New, pure raw materials were used, such as Zircon, Alumina and Silica (later known as AZS). During the Second World War, the Modane factory was destroyed. In 1947, a new production site was selected in Le Pontet, located in the department of Vaucluse in France, some 350 km from the original location. This would later become the group's headquarters. Workers from the Modane plant relocated to Vaucluse to work in the new factory.\n\nThe choice of location was due, in part, to the proximity of necessary raw materials, such as the purest white bauxite in Europe, alumina provided by the company Péchiney in Gardanne, and the silica sand of the Mont Ventoux region. It was also in close proximity to the port city of Marseille and the Rhone, valuable for its hydro-electric energy.\n\nIn 1952, a Research and Development center was established next to the Le Pontet factory. The company focused on innovation and was granted multiple patents from the 1960s onwards.\n\nIn 1973, \"L’Electro-Réfractaires\" merged with the Société Générale de Produits Réfractaires, based in Vénissieux, France, to form the \"Société Européenne des Produits Réfractaires\" (SEPR). The group finally became Saint-Gobain SEFPRO in 2001.\n\n\nThe site has served as the SEFPRO headquarters since its founding in 1947, replacing the original plant in Modane that was destroyed in 1944 by bombings during World War II. The site in Le Pontet, located in the department of Vaucluse in France, covers 25 hectares of land. Its principal products are standard and electrical resistive High Zirconia; unshaped products; alpha-beta and beta Alumina; \"ER\" glass contact AZS range; and cruciform, of which it is the unique producer worldwide. The plant has received the quality certifications ISO 9001 and ISO 14001.\n\nThe company employs around 700 people and serves markets and customers both local and global.\n\nThe SEPR Italia site was founded in 1960 and acquired by the Electro-Réfractaire group, known as SEFPRO since 2001, in 1971. The company employs 145 people.\n\nThe site is located in Mezzocorona in the Trentino province of Italy. Its specialties are glass contact and superstructure; serving a global market including fused-cast expendables and WR\n\nThe plant is certified ISO 9001, ISO 14001 and OHSAS 18001.\n\nSEPR Beijing is a subsidiary of the global refractory group Saint-Gobain SEFPRO. Originally named ZPER upon its acquisition in 1991, it was renamed in 2001 when a new plant in Changping, China was opened. The Beijing Changping plant is now a critical part of the SEFPRO worldwide network's electrofused product supply, serving glass manufacturers across Asian, European and North American markets and employing 500 people. It measures 74 000 m² and specializes in Container, Flat and Specialty Glass markets, with a focus on glass contact, superstructure and furnace end-zone.\n\nSaint-Gobain TM K.K was founded in 1996 as ‘Toshiba Monofrax’, a 49/51 joint-venture between Saint-Gobain and Toshiba Ceramics in Japan. In 2003, Toshiba Ceramics sold its 51% stake in the company to NEG (40%) and Saint-Gobain (11%) and the company was renamed Saint-Gobain TM K.K. The company specializes in producing fused cast products and serves the Japanese and more generally the Asian glass markets, employing around 200 people.The plant in Kozaki, Japan, covers 80,000 m² and specializes in Float, Displays, Special Glasses, Containers and Tableware. It has gained the ISO 9001, ISO 14001 and OSHMS safety certificates.\n\nSEPR Refractories India was created in 2002 when a plant dedicated to fused-cast products was acquired from Carborundum Universal (CUMI) and merged with a sintered-product plant. The plants, located in Perundurai and Palakkad, now serve customers worldwide, from South East Asia to America and employ 550 people.\n\nThe total Palakkad plants’ area covers 16700m² and serves a wide range of glass industry markets, from Container; Flat and Specialty Glass to Tableware and Fiberglass. The plants follow universal SEFPRO standards, with the sintered product plant's technology modelled directly on the Savoie Réfractaires sintered plant in France.\n\nThe plants have gained the safety certifications ISO 9001, ISO 4001, ISO 18000 and OSHAS.\n\nSavoie Réfractaires is a subsidiary of the global refractory group Saint-Gobain SEFPRO, created in 1985 through the acquisition of a plant in Vénissieux, France. The , established in 1899, began activity at its Vénissieux plant in 1932. However, in 1961 there was a split between the company's Electrode and Refractory divisions, and S.G.P.R (Société Générale des Produits Réfractaires) was founded. A merger between SGPR and refractory company \"L’Eléctro-Réfractaires\" forms (Société Européenne des Produits Réfractaires) in 1973. The Provins site was founded in 1925 and was, over the years, a subsidiary of la Société Electro Réfractaires, SEPR, and Lafarge Réfractaires as companies were restructured, bought and sold. It was finally acquired again by SEPR (now Saint-Gobain SEFPRO) in 1985.\n\nIn 1985, SEPR created 'Savoie Réfractaires' by taking over the Vénissieux plant, which was merged with Provins Réfractaires in 1990 to diversify the company's product base. The plants were chosen for their specialization in sintered materials for the glass industry: Vénissieux serving the Insulation Fiber and Container industry, and Provins serving the Container glass industry. They both now serve global markets and employ 170 people: 120 in Vénissieux and 50 in Provins. Both plants have been awarded the ISO 9001 quality certification certificate. Vénissieux was additionally awarded the ISO 14001 certification in 2010.\n\nValoref is a subsidiary of the global refractory group Saint-Gobain SEFPRO. Valoref recycles used refractories from glass furnace repairs and transforms them into secondary raw materials for the refractory industry. It is certified ISO 9001 and ISO 14001.\n\nCorhart Refractories Buckhannon is a subsidiary of the global refractory group Saint-Gobain SEFPRO. Located in Buckhannon, WV, USA, Corhart Refractories was founded in 1960 as a subsidiary of Corning Glass Works. In 1985 the President, Robert Ayotte, acquired Corhart through a leveraged buyout, and then went on to sell the company to Saint-Gobain two years later. The plant's specialties include the manufacturing of refractories destined for Reinforcement, Insulation Fiberglass, Specialty Glass and Container Glass, serving a global market and employing 170 people. The plant has ISO 9001 and ISO 14001 safety certifications.\n\nLinyi China is a subsidiary of the global refractory group Saint-Gobain SEFPRO, founded in 2005. The plant is located in Linyi, in the province of Shandong in China. Its customer scope includes clients across Asia and it employs around 70 people. Its principal markets are Reinforcement Fiber and Specialty Glass, and its main applications are isostatically pressed Chromic Oxide and Zircon. The plant holds the ISO 9001 and ISO 14001 safety certifications.\n\nCreated in 1968, the Vinhedo plant in Brazil joined SEPR in 1996 and today has over 270 employees. The factory's principal market is expendables.\n\nSaint-Gobain C.R.E.E (Centre de Recherches et d’Etudes Européen) is the European Research and Development Centre of the global group Saint-Gobain, established in 1996. It belongs to the High Performance Materials division.\n\nLocated in Cavaillon in the department of Vaucluse, France, the center employs around 230 researchers and technicians. Its main work focuses on the development of advanced ceramic products.\n\nSGRS (Saint-Gobain Research Shanghai) is the Asian research and development centre of the global group Saint-Gobain, located in the Minhang Development Zone of Shanghai. It belongs to the High Performance Materials division of Saint-Gobain, specializing in a wide range of applications.\n\nThe centre was opened in September 2007, and extended in September 2013 with the addition of a new building.\n\nThe Saint-Gobain Northborough Research and Development center is Saint-Gobain's North American research and development center. Located in Northborough, MA, USA, the center employs 330 people of 27 different nationalities, including 200 scientists and engineers, making it Saint-Gobain's biggest research centre globally.\n\nBetween 2002 and 2009, the center expanded from 100 to over 300 employees, extended the campus by 5,700m² and opened a new energy efficient building to accommodate its expanding projects that was inaugurated by the CEO (Mr. PA DeChalendar) and the Governor of Massachusetts, Mr. Deval Patrick in July 2009. The new building was designed by the architect Shepley Bulfinch Richardson & Abbott, and its construction involved the use of Saint-Gobain's own building products.\n\nThe SEFPRO group specializes in refractory products and services for the glass industry, serving markets like Container glass, Flat glass, E-glass, Wool Fiberglass, Reinforcement Fiberglass and Special glass. It produces fused cast AZS, high zirconia, alumina, sintered refractories, unshaped refractory materials, cruciform and expendables amongst others.\n"}
{"id": "4836999", "url": "https://en.wikipedia.org/wiki?curid=4836999", "title": "Sandon tornado", "text": "Sandon tornado\n\nThe Sandon tornado struck the township of Sandon, Victoria, Australia on 13 November 1976. The tornado is likely to have been an F3 on the Fujita scale. It killed two people when their parked car was picked up and lifted around 9 metres off the ground and thrown 100 metres into a ditch.\n\n\n"}
{"id": "12063182", "url": "https://en.wikipedia.org/wiki?curid=12063182", "title": "Solar Energy Industries Association", "text": "Solar Energy Industries Association\n\nThe Solar Energy Industries Association (SEIA), established in 1974, is a national non-profit trade association of the solar-energy industry in the United States.\n\nSEIA is a 501(c)6 non-profit trade association. SEIA's sister organization, The Solar Foundation, a 501(c)3 non-profit charitable organization, oversees policy-driven research and develops education & outreach programs to promote the further development of solar energy in the U.S.\n\nThe association supports the extension of a 30 percent federal solar investment tax credit for eight years.\n\nWith the recent high flux of green jobs in the solar industry, SEIA maintains a resource for those looking for solar jobs. The Harvard Business Review that the solar industry could absorb all of the jobs lost to the coal industry as it shutters. Solar already employs more people than the entire US coal industry.\n\n\n\n\n\n\n\n\n"}
{"id": "763138", "url": "https://en.wikipedia.org/wiki?curid=763138", "title": "Sonic weapon", "text": "Sonic weapon\n\nSonic and ultrasonic weapons (USW) are weapons of various types that use sound to injure, incapacitate, or kill an opponent. Some sonic weapons are currently in limited use or in research and development by military and police forces. Some of these weapons have been described as sonic bullets, sonic grenades, sonic mines, or sonic cannons. Some make a focused beam of sound or ultrasound; some make an area field of sound.\n\nExtremely high-power sound waves can disrupt or destroy the eardrums of a target and cause severe pain or disorientation. This is usually sufficient to incapacitate a person. Less powerful sound waves can cause humans to experience nausea or discomfort. The use of these frequencies to incapacitate persons has occurred both in anti-citizen special operation and crowd control settings.\n\nThe possibility of a device that produces frequency that causes vibration of the eyeballs—and therefore distortion of vision—was suggested by paranormal researcher Vic Tandy in the 1990s while attempting to demystify a \"haunting\" in his laboratory in Coventry. This \"spook\" was characterised by a feeling of unease and vague glimpses of a grey apparition. Some detective work implicated a newly installed extractor fan that, Tandy found, was generating infrasound of 18.9 Hz, 0.3 Hz, and 9 Hz.\n\nA long-range acoustic device has been used by the crew of the cruise ship \"Seabourn Spirit\" to deter pirates who chased and attacked the ship. More commonly this device and others of similar design have been used to disperse protesters and rioters in crowd control efforts. A similar system is called a \"magnetic acoustic device\" \"The mosquito\". 'Mobile' sonic devices have been used in the United Kingdom to deter teenagers from lingering around shops in target areas. The device works by emitting an ultra-high frequency blast (around 19–20 kHz) that teenagers or people under approximately 20 are susceptible to and find uncomfortable. Age-related hearing loss apparently prevents the ultra-high pitch sound from causing a nuisance to those in their late twenties and above, though this is wholly dependent on a young person's past exposure to high sound pressure levels.\n\nHigh-amplitude sound of a specific pattern at a frequency close to the sensitivity peak of human hearing (2–3 kHz) is used as a burglar deterrent.\n\nSome police forces have used sound cannons against protesters, for example during the 2009 G20 Pittsburgh summit, the 2014 Ferguson unrest, the 2016 Dakota Access Pipeline protest in North Dakota, among others.\n\nIt has been reported that \"sonic attacks\" may have taken place in Cuba in 2016 and 2017, leading to health problems, including hearing loss, in US and Canadian government employees at the US and Canadian embassies in Havana. Some events as reported would seem to violate the laws of physics, and it has been suggested that they are in fact an example of mass psychogenic illness. Cuban investigators have reportedly dismissed claims that sonic weapons have been used against American diplomats, describing them as \"science fiction\".\nThe US issued an alert to its employees in China after it learned of a possible \"sonic attack\" on a US citizen working there. The employee complained of symptoms from late 2017 through April 2018 , then returned to the US and was examined by a doctor who said the person had suffered a mild traumatic brain injury (MTBI). The US diplomat likened it to the incident which they say happened in Cuba. In early June, 2018 several employees were evacuated from their work in the U.S. Consulate offices in China and sent back to Pennsylvania for testing and diagnosis after sharing similar concerns.\n\nStudies have found that exposure to high intensity ultrasound at frequencies from 700 kHz to 3.6 MHz can cause lung and intestinal damage in mice. Heart rate patterns following vibroacoustic stimulation has resulted in serious negative consequences such as atrial flutter and bradycardia.\n\nThe extra-aural (unrelated to hearing) bioeffects on various internal organs and the central nervous system included auditory shifts, vibrotactile sensitivity change, muscle contraction, cardiovascular function change, central nervous system effects, vestibular (inner ear) effects, and chest wall/lung tissue effects. Researchers found that low frequency sonar exposure could result in significant cavitations, hypothermia, and tissue shearing. No follow up experiments were recommended. Tests performed on mice show the threshold for both lung and liver damage occurs at about 184 dB. Damage increases rapidly as intensity is increased. The American Institute of Ultrasound in Medicine (AIUM) has stated that there have been no proven biological effects associated with an unfocused sound beam with intensities below 100 mW/cm² SPTA or focused sound beams below an intensity level of 1 mW/cm² SPTA.\n\nNoise-induced neurologic disturbances in scuba divers exposed to continuous low frequency tones for durations longer than 15 minutes has involved in some cases the development of immediate and long-term problems affecting brain tissue. The symptoms resembled those of individuals who had suffered minor head injuries. One theory for a causal mechanism is that the prolonged sound exposure resulted in enough mechanical strain to brain tissue to induce an encephalopathy. Divers and aquatic mammals may also suffer lung and sinus injuries from high intensity, low frequency sound. This is due to the ease with which low frequency sound passes from water into a body, but not into any pockets of gas in the body, which reflect the sound due to mismatched acoustic impedance.\n\n\n"}
{"id": "30412642", "url": "https://en.wikipedia.org/wiki?curid=30412642", "title": "Sven Peek", "text": "Sven Peek\n\nSven Peek is an environmentalist and activist from Durban, South Africa. He was awarded the Goldman Environmental Prize in 1998, for his efforts on improvement of pollution problems in the region of South Durban.\n"}
{"id": "2265144", "url": "https://en.wikipedia.org/wiki?curid=2265144", "title": "Syneresis (chemistry)", "text": "Syneresis (chemistry)\n\nSyneresis (also spelled 'synæresis' or 'synaeresis'), in chemistry, is the extraction or expulsion of a liquid from a gel, as when serum drains from a contracting clot of blood. Another example of syneresis is the collection of whey on the surface of yogurt. Syneresis can also be observed when the amount of diluent in a swollen polymer exceeds the solubility limit as the temperature changes. A household example of this is the counter intuitive expulsion of water from dry gelatin when the temperature increases. Syneresis has also been proposed as the mechanism of formation of the amorphous silicate composing the frustule of diatoms.\nIn the processing of dairy milk, for example during cheese making, syneresis is the formation of the curd due to the sudden removal of the hydrophilic macropeptides, which causes an imbalance in intermolecular forces. Bonds between hydrophobic sites start to develop and are enforced by calcium bonds which form as the water molecules in the micelles start to leave the structure. This process is usually referred to as the phase of coagulation and syneresis. The splitting of the bond between residues 105 and 106 in the κ-casein molecule is often called the primary phase of the rennet action, while the phase of coagulation and syneresis is referred to as the secondary phase.\n\nIn cooking, syneresis is the sudden release of moisture contained within protein molecules, usually caused by excessive heat, which over-hardens the protein shell. Moisture inside expands upon heating. The hard protein shell pops, expelling the moisture.\n\nThis process is what changes juicy rare steak to dry steak when well-done. It creates weeping in scrambled eggs, with dry protein curd swimming in released moisture. It causes emulsified sauces, such as hollandaise, to \"break\". It creates unsightly moisture pockets within baked custard dishes such as flan or crème brûlée.\n\nIn dentistry, syneresis is the expulsion of water or other liquid molecules from dental impression materials (alginate for example) after an impression has been taken. Due to this process, the impression shrinks a little and therefore its size is no longer accurate. For this reason, many dental impression companies strongly recommend to pour the dental cast as soon as possible to prevent distortion of the dimension of the teeth and objects in the impression.\n\nThe opposite process of syneresis is imbibition, meaning, a material that absorbs water molecules from the surrounding. Alginate is also an example of imbibition since if soaked in water, it will absorb it.\n\n"}
{"id": "29863182", "url": "https://en.wikipedia.org/wiki?curid=29863182", "title": "Taishan Nuclear Power Plant", "text": "Taishan Nuclear Power Plant\n\nThe Taishan Nuclear Power Plant () is under construction in Chixizhen, Taishan, Guangdong province, China.\nThe plant features two of Areva's European Pressurized Reactor (EPR).\nIt was planned to go online in 2013 and to be the third site to house EPR units.\nHowever, the start of operation has been postponed to late 2018. Also, delays in other EPR construction sites mean that Taishan will probably be the first nuclear power plant with an operational EPR unit. One of the two reactors, Taishan 1, came online and was connected to the grid in August 2018.\nThe project is owned by Guangdong Taishan Nuclear Power Joint Venture Company Limited (TNPC), which is 70% owned by China Guangdong Nuclear Power Group (CGNPC) and 30% by Électricité de France (EDF).\n\nEach generator delivers 1750 MWe (Nameplate capacity); generators of that size are the largest single-piece electrical generators. \nThe 495 tonne generator stator is built by Dongfang Electric. \nOf the 1750 MWe gross delivered, around 90 MWe will be used by plant systems such as the large pumps that circulate cooling water, leaving 1660 MWe net for supply to the grid.\n\nOn August 26, 2008, excavation work began.\nThe first concrete for the first unit was poured in October 2009.\nConstruction of each unit was planned to take 46 months, significantly faster and cheaper than the first two EPRs in Finland and France. These plans have proved elusive as start up has been repeatedly delayed. In February 2017 after 88 months of construction, CGNPC announced that completion of the reactors would be delayed until the second half of 2017 and the first half of 2018. \n\nIn spite of this announcement, completion of the plant was delayed. In December 2017, Hong Kong media reported that a boiler had cracked during testing, and that welding on the component was considered \"problematic\". Neither the nuclear plant's operators nor the manufacturer of the affected component responded to the news agency's request for comment. The boiler was later found to be a deareator, which removes dissolved oxygen from water by heating it.\n\nIn January 2018 commissioning was rescheduled, with commercial operation expected in 2018.\n\nOn April 9, 2018, the Official Letter of Approving the Initial Fuel Loading of the first unit of the Taishan Nuclear Power Plant was issued by the National Nuclear Safety Administration (NNSA). Taishan Unit 1 began fuel loading at 18:18 on April 10, marking the beginning of fuel loading of the first reactor using the third-generation nuclear power technology EPR.\n\nFirst criticality was achieved at Taishan Unit 1 on June 6, 2018. On June 29, 2018, Taishan 1 was connected to the grid.\n\n"}
{"id": "4654321", "url": "https://en.wikipedia.org/wiki?curid=4654321", "title": "Tokyo tanks", "text": "Tokyo tanks\n\nTokyo tanks were internally mounted self-sealing fuel tanks used in the Boeing B-17 Flying Fortress and Consolidated B-24 Liberator bombers during World War II. Although nicknamed \"Tokyo\" tanks to dramatically illustrate the significant range they added to the B-17 (approximately 40% greater with combat weights), it was also an exaggeration in that no B-17 ever had the range to bomb Japan from any base in World War II.\n\nThese fuel tanks consisted of eighteen removable rubberized-compound containers, called cells, installed inside the main wings of the airplane, nine to each side. The main wings of the B-17 consisted of an \"inboard wing\" structural panel mounted to the fuselage and mounting the engines and flaps, and an \"outboard wing\" structural panel joined to the inboard wing and carrying only the ailerons. The Tokyo tanks were installed on either side of the joint (a load-bearing point) where the two wing portions were connected. Five cells, totalling capacity, sat side by side in the outboard wing and were joined by a fuel line to the main tank delivering fuel to the outermost engine. The sixth cell was located in the space where the wing sections joined, with the remaining three cells located side-by-side within the inboard wing, and these four cells delivered of fuel to the feeder tank for the inboard engine. The same arrangement was repeated on the opposite wing. The Tokyo tanks added of fuel to the carried in the six regular wing tanks and the that could be carried in a tank that could be mounted in the bomb bay, for a combined total of .\n\nAll B-17F aircraft built by Boeing from Block 80, by Douglas from Block 25, and by Vega from Block 30 were equipped with Tokyo tanks, and the entire run of B-17Gs had Tokyo tanks. These B-17s with factory-mounted Tokyo tanks were first introduced to the Eighth Air Force in England in April 1943 with the arrival of the 94th and 95th Bomb Groups. In June 1943 aircraft that were so equipped began to appear in greater numbers as replacement aircraft, and from the beginning of July, 1943, all replacement aircraft that did not have the tanks were equipped before issue.\n\nAlthough the tanks were removable, this could only be done by first removing the outboard wing panel, and so was not a routine maintenance task. A drawback to the tanks was that there was no means of measuring remaining fuel quantity within the cells. Fuel was moved from the cells to the engine tanks by opening control valves within the bomb bay so that the fuel drained by gravity. Although the tanks were specified as self-sealing, vapor buildup within partially drained tanks made them explosive hazards in combat.\n\n"}
{"id": "60875", "url": "https://en.wikipedia.org/wiki?curid=60875", "title": "Triboluminescence", "text": "Triboluminescence\n\nTriboluminescence is an optical phenomenon in which light is generated through the breaking of chemical bonds in a material when it is pulled apart, ripped, scratched, crushed, or rubbed (see tribology). The phenomenon is not fully understood, but appears to be caused by the separation and reunification of electrical charges. The term comes from the Greek τρίβειν (\"to rub\"; see tribology) and the Latin \"lumen\" (light). Triboluminescence can be observed when breaking sugar crystals and peeling adhesive tapes.\n\n\"Triboluminescence\" is often used as a synonym for \"fractoluminescence\" (a term sometimes used when referring only to light emitted from fractured crystals). Triboluminescence differs from piezoluminescence in that a piezoluminescent material emits light when it is deformed, as opposed to broken. These are examples of mechanoluminescence, which is luminescence resulting from any mechanical action on a solid.\n\nThe Uncompahgre Ute Indians from Central Colorado are one of the first documented groups of people in the world credited with the application of mechanoluminescence involving the use of quartz crystals to generate light. The Ute constructed special ceremonial rattles made from buffalo rawhide which they filled with clear quartz crystals collected from the mountains of Colorado and Utah. When the rattles were shaken at night during ceremonies, the friction and mechanical stress of the quartz crystals impacting together produced flashes of light visible through the translucent buffalo hide.\n\nThe first recorded observation is attributed to English scholar Francis Bacon when he recorded in his 1620 \"Novum Organum\" that \"It is well known that all sugar, whether candied or plain, if it be hard, will sparkle when broken or scraped in the dark.\" The scientist Robert Boyle also reported on some of his work on triboluminescence in 1663. In the late 1790s, sugar production began to produce more refined sugar crystals. These crystals were formed into a large solid cone for transport and sale. This solid cone of sugar had to be broken into usable chunks using a device known as sugar nips. People began to notice that as sugar was \"nipped\" in low light, tiny bursts of light were visible.\n\nA historically important instance of triboluminescence occurred in Paris in 1675. Astronomer Jean-Felix Picard observed that his barometer was glowing in the dark as he carried it. His barometer consisted of a glass tube that was partially filled with mercury. Whenever the mercury slid down the glass tube, the empty space above the mercury would glow. While investigating this phenomenon, researchers discovered that static electricity could cause low-pressure air to glow. This discovery revealed the possibility of electric lighting.\n\nMaterials scientists have not yet arrived at a full understanding of the effect, but the current theory of triboluminescence — based upon crystallographic, spectroscopic, and other experimental evidence — is that upon fracture of asymmetrical materials, charge is separated. When the charges recombine, the electrical discharge ionizes the surrounding air, causing a flash of light. Research further suggests that crystals which display triboluminescence must lack symmetry (thus being anisotropic in order to permit charge separation) and be poor conductors. However, there are substances which break this rule, and which do not possess asymmetry, yet display triboluminescence anyway, such as hexakis(antipyrine)terbium iodide. It is thought that these materials contain impurities, which make the substance locally asymmetric.\n\nThe biological phenomenon of triboluminescence is conditioned by recombination of free radicals during mechanical activation.\n\nA diamond may begin to glow while being rubbed. This occasionally happens to diamonds while a facet is being ground or the diamond is being sawn during the cutting process. Diamonds may fluoresce blue or red. Some other minerals, such as quartz, are triboluminescent, emitting light when rubbed together.\nOrdinary Pressure-sensitive tape (\"Scotch tape\") displays a glowing line where the end of the tape is being pulled away from the roll. In 1953, Soviet scientists first observed that unpeeling a roll of tape in a vacuum produced X-rays. The mechanism of X-ray generation was studied further in 2008. Similar X-Ray emissions have also been observed with metals.\n\nAlso, when sugar crystals are crushed, tiny electrical fields are created, separating positive and negative charges that then create sparks while trying to reunite. Wint-O-Green Life Savers work especially well for creating such sparks, because wintergreen oil (methyl salicylate) is fluorescent and converts ultraviolet light into blue light.\n\nTriboluminescence is a biological phenomenon observed in mechanical deformation and contact electrization of epidermal surface of osseous and soft tissues, at chewing food, at friction in joints of vertebrae, during sexual intercourse, and during blood circulation.\n\n\"Fractoluminescence\" is often used as a synonym for triboluminescence. It is the emission of light from the fracture (rather than rubbing) of a crystal, but fracturing often occurs with rubbing. Depending upon the atomic and molecular composition of the crystal, when the crystal fractures a charge separation can occur making one side of the fractured crystal positively charged and the other side negatively charged. Like in triboluminescence, if the charge separation results in a large enough electric potential, a discharge across the gap and through the bath gas between the interfaces can occur. The potential at which this occurs depends upon the dielectric properties of the bath gas.\n\nThe emission of electromagnetic radiation (EMR) during plastic deformation and crack propagation in metals and rocks have been studied. The EMR emissions from metals and alloys have also been explored and confirmed. Molotskii presented a dislocation mechanism for this type of EMR emissions. Recently, Srilakshmi and Misra reported an additional phenomenon of secondary EMR during plastic deformation and crack propagation in uncoated and metal-coated metals and alloys.\n\nEMR during the micro-plastic deformation and crack propagation from several metals and alloys and transient magnetic field generation during necking in ferromagnetic metals were reported by Misra (1973–75), which have been confirmed and explored by several researchers. Tudik and Valuev (1980) were able to measure the EMR frequency during tensile fracture of iron and aluminum in the region 1014¬¬ Hz by using photomultipliers. Srilakshmi and Misra (2005a) also reported an additional phenomenon of secondary electromagnetic radiation in uncoated and metal-coated metals and alloys. If a solid material is subjected to stresses of large amplitudes, which can cause plastic deformation and fracture, emissions such as thermal, acoustic, ions, exo-emissions occur. With the discovery of new materials and advancement in instrumentation to measure effects of EMR, crack formation and fracture; the EMR emissions effect becomes important.\n\nIn a moderate vacuum, peeling tape generated x-rays sufficient to x-ray a human finger.\n\nThe study of deformation is essential for the development of new materials. Deformation in metals depends on temperature, type of stress applied, strain rate, oxidation and corrosion. Deformation induced EMR can be divided into three categories: effects in ionic crystal materials; effects in rocks and granites; and, effects in metals and alloys. EMR emission depends on the orientation of the grains in individual crystals since material properties are different in differing directions. Amplitude of EMR pulse increases as long as the crack continues to grow as new atomic bonds are broken and it leads to EMR. Pulse starts to decay when crack halts. Observations from experiments showed that emitted EMR signals contain mixed frequency components.\n\nMost widely tensile test method is used to characterize the mechanical properties of materials. From any complete tensile test record, one can obtain important information about the material's elastic properties, the character and extent of plastic deformation, yield and tensile strengths and toughness. The information which can be obtained from one test justifies the extensive use of tensile test in engineering materials research. Therefore, investigations of EMR emissions are mainly based on the tensile test of the specimens.\nFrom experiments, it can be shown that tensile crack formation excites more intensive EMR than shear cracking, increasing the elasticity, strength and loading rate during uniaxial loading increases amplitude. Poissons ratio is a key parameter for EMR characterization during triaxial compression. If the poissions ratio is lower, it is harder for the material to strain transversally and hence higher is the probability of new fractures. Mechanism of plastic deformation is very important for safe operation of any component under dynamic conditions.\n\nThis EMR can be utilized in developing sensors/smart materials. This technique can be implemented in powder metallurgy technique also. EMR is one of these emissions which accompany large deformation. If an element can be identified which gives maximum EMR response with minimum mechanical stimulus then it can be incorporated into main material and thus set new trends in the development of smart material. The deformation induced EMR can serve as a strong tool for failure detection and prevention.\n\nOrel V.E. invented the device to measure EMR whole blood and lymphocytes in laboratory diagnostics.\n\n\n"}
{"id": "34088955", "url": "https://en.wikipedia.org/wiki?curid=34088955", "title": "Tropical Atlantic Variability", "text": "Tropical Atlantic Variability\n\nThe Tropical Atlantic Variability (TAV) is influenced by internal interaction and external effects. TAV can be discussed in different time scales: seasonal (annual cycle) and interannual.\n\nSeasonal variability is the dominant time scale of TAV, which is due to the seasonal march of the sun. This seasonal variability is related to the movement of intertropical convergence zone (ITCZ),which is the convergent zone of trade winds from south and north near the equator. It has strong vertical convection resulting to a redundant participation band and weak winds. The mean location of the ITCZ over the Atlantic Ocean is 5~10 degrees north of the geographical equator. All this asymmetric of ITCZ is the ultimate cause of the annual cycle in equatorial sea surface temperature (SST) in Atlantic by maintaining southerly cross-equatorial winds that intensify in boreal summer/fall and relax in boreal spring. From March to April, during which the temperature of the equator reaches maximum, winds are weakest and the sun shines directly over the equator. So, SST is uniformly warm near the equator, which makes the ITCZ really sensitive to even small disturbance of SST and explains the relaxation of cross-equator winds in spring. From July to September, during which the temperature of the equator reaches its minimum, ITCZ reaches its northernmost location, which explains the intensification of cross-equator winds. It can be seen that the process of cooling takes 3 months while that of warming takes 7 months, which are asymmetric. This seasonal asymmetry is due to influence of seasonal continental monsoon. Because of the narrow width of Atlantic, continental monsoon has a much more important influence on its variation compared to the wide Pacific Ocean, whose leading factor is air-sea interaction.\n\nFor interannual, there is one mode called Atlantic Nino, of which periodicity varies. During the Atlantic Nino event, eastern area of Atlantic will appear warm SST anomalies accompanying a relaxation of trade winds. This mechanism, which is known as Bjerknes Feedback, is similar to El Nino/Southern Oscillation( ENSO).\n\n“On interannual and longer timescales, no single mode seems to dominate. Instead, several mechanisms are responsible for tropical Atlantic variability. On the equator, both observational and modeling studies indicate that there is a Bjerknes-type air-sea coupled mode arising from the interaction of the equatorial zonal SST gradient, ITCZ convection, zonal wind, and thermocline depth.”\n"}
{"id": "718110", "url": "https://en.wikipedia.org/wiki?curid=718110", "title": "Tropical garden", "text": "Tropical garden\n\nA tropical garden features tropical plants and requires good rainfall or a decent irrigation or sprinkler system for watering. These gardens typically need fertilizer and heavy mulching.\n\nTropical gardens are no longer exclusive to tropical areas. Many gardeners in colder climates are adopting the tropical garden design, which is possible through careful choice of plants and flowers. Main features include plants with very large leaves, vegetation that builds in height towards the back of the garden, creating a dense garden. Large plants and small trees hang over the garden, leaving sunlight to hit the ground directly.\n\nThe following are some examples of tropical plants to be used in tropical gardens or as indoor plants.\n\nHoyas are known as the “Wax Flowers” due to their texture and almost unreal appearance. They need relatively warm and humid conditions, but depending on the specific variety of Hoya, they can endure heavy rainfall for some months during the wet seasons just as they may also be exposed to long dry periods. They are commonly used indoors. They need indirect light, but the amount or required light depends on the variety. They need small amounts of water and the soil is to be kept slightly dry since a very wet soil can end up killing this kind of plant. They can be grown in pots that are not too big or in hanging baskets. They grow better if kept at a minimum temperature of 15 degree Celsius (59 Fahrenheit).\n\nContrary to common belief, growing banana trees is not that difficult and allow people to enjoy their own bananas. Also these plants can be used as windbreaks. They need fertile soils, large mulch and organic matter, large amounts of nitrogen and potassium, warm temperature, high humidity, large amounts of water, and shelter from other banana plants. Banana plants are not to be exposed to strong winds and extreme weather conditions (too hot or too cold weather) with an ideal temperature being 26-30 degree Celsius (78-86 Fahrenheit). They stop growing below 14 degree Celsius (57 Fahrenheit). Banana rhizomes are planted upright and their roots have to be well covered with soil.\n\nAmong the many species of birds of paradise is the Strelitzia reginae, an indigenous plant to South Africa and South America. As a tropical plant, it grows in warm, humid climates. This exotic, colorful plant, with evergreen leaves resembles a bird's beak. It is an outdoor plant provided the weather is not too cold. Otherwise, it is better to keep it in a pot indoors. It needs rich soil as well as sun or partial shade. Another requirement for this plant is a good drainage.\n\nFerns are commonly used to give gardens a great foliage. Most ferns are easy to take care of. They basically cannot be exposed to direct sunlight for long periods of time. Moreover, their soil should always be moist. One way to keep ferns moist is by misting them.\n\nPapaya trees should be planted where they can be kept warm and free from wind and freeze. They need enough water to support their leaves, but it should not be exceed as to a root rod. They also need a well-drained soil.\n\nNight temperatures between 12- 18 degree Celsius (55-65 Fahrenheit), and day temperatures between 23 -26 degree Celsius ( 75-80 Fahrenheit) are fine temperatures for orchids. Orchids need sufficient amounts of light as well as humidity. However, there are many varieties of orchids and each of them features specific care needs.\n\nA tropical garden is one of the most difficult gardens to build, or maintain, it becomes more difficult the more your local climate differs from the natural habitat of your plants. Key to a healthy tropical garden are lots of light and lots of water. The large leaves that feature in tropical plants require the soil to be humid at all times, so irrigation might be a must-have for some gardens. Over-watering can kill your plants as well, as it will cause the roots to rot.\n\nA tropical plant that is not cold-hardy should be brought indoors during the winter and returned to the garden for the summer.\n\nTropical plants that work well in non-tropical climates include;\n\nThere are a number of attractions that feature a tropical garden including;\nNorway: *Flor og Fjære -Stavanger, Norway\nAustralia: *Alma Park Zoo - Kuranda, Queensland \nAustralia: *Kuranda Scenic Railway - Dakabin, Queensland\nCanada: *Colasanti's Tropical Gardens Kingsville, Ontario\nUSA: *Cedarvale Botanic Garden and Restaurant - Davis, Oklahoma\nUSA: *Reiman Gardens -Ames, Iowa\nThailand: Dokmai Garden, Chiang Mai. \n"}
{"id": "25159096", "url": "https://en.wikipedia.org/wiki?curid=25159096", "title": "University of Dayton Research Institute", "text": "University of Dayton Research Institute\n\nThe University of Dayton Research Institute is the professional research arm of the University of Dayton in Dayton, Ohio. In fiscal year 2018, UD was ranked first among all colleges in the nation for federally sponsored materials research, according to statistics released by the National Science Foundation. In Ohio, UD is ranked first among nonprofit institutions for research sponsored by the Department of Defense.\n\nThe University of Dayton Research Institute (UDRI) employs 570 full-time research, technical and administrative staff. In 2018, UDRI performed $150 million in sponsored research. UDRI is nationally recognized for its research in materials, structures, sensors and autonomous systems, energy and sustainment technologies. Established as the research arm of the University of Dayton in 1956, UDRI broke the $2 billion mark in sponsored research in 2016.\n\n"}
{"id": "13791073", "url": "https://en.wikipedia.org/wiki?curid=13791073", "title": "Uno-X", "text": "Uno-X\n\nUno-X is a chain of unmanned fuel stations throughout Norway and Denmark. It is operated as the low-cost section of YX Energi. The chain was originally created as a low cost chain in Denmark in the late 1950s. The rights to use the name in Sweden were sold to Britain's Burmah Oil. In 1991 Norsk Hydro acquired the 330 outlets of the Danish operation and five years later it bought the Swedish Uno-X chain from Burmah.\n\nAfter Norsk Hydro merged its operations in Denmark and Norway with Texaco, the chain was repositioned on wholly unmanned sites under a new yellow, black and red logo, and now has around 110 stations in Norway and 200 stations in Denmark. The Danish chain includes the architecturally well known Skovshoved Petrol Station designed in 1936 by Arne Jacobsen.\n\nFollowing Norsk Hydro's acquisition by Statoil, the 250 stations in Sweden were sold in December 2009 to St1 and the Norwegian and Danish operations to the retail group Reitangruppen.\n\nUno-X Hydrogen plans to build 20 hydrogen stations before 2020, each capable of dispensing 200 kg of hydrogen per day, and 100 kg in 3 hours.\n\n"}
{"id": "24691187", "url": "https://en.wikipedia.org/wiki?curid=24691187", "title": "Vehicle electrification", "text": "Vehicle electrification\n\nVehicle electrification is a topic that covers many aspects of electrification in vehicles. It may cover vehicles with electrical means of propulsion, as well as electricity playing a role in their functionality. Voltages vary widely between applications. \n\nIn general, vehicle electrification refers to efforts to design mild to full hybrid vehicle to full electric vehicles as well as converting some of the non-electrical vehicle systems like hydraulic suspension-systems to smart electro-magnetic suspensions, or All-Wheel-Drive (AWD) to eAWD where AWD is automatically engaged only under certain conditions (like certain low vehicle speed range) to leave more juice for full electric vehicle range as opposed to AWD all the time which sips more juices from battery. \n\nThis section will cover common voltages for different vehicle applications.\n\nAutomobiles engineered prior to the 1940s had 6-volt systems in them. However, 6-volts is a common voltage for single 3-cell deep-cycle lead-acid batteries for applications such as golf carts, though they have 24 volts or 36 volts for their electric motors.\n\n12 volt systems are by far the most common voltage system for vehicles in general. Primarily the voltage was devised to overcome limitations of old 6-volt systems as, by the 1950s starting motors needed to become more powerful, but needed more power with less current as well as a way to cut down on wire gauge as more equipment came along and put a greater load on the system. Over time, 12 volt systems evolved to have mainstream niche fields like car audio which has spanned across\n\n24 volt systems are common in military and commercial truck applications. It is also a secondary voltage system for switch-mode power supplies with parallel-to-series relays for facilitating engine starting on diesel engines as they require more compression than regular gas.\n\n"}
{"id": "3988333", "url": "https://en.wikipedia.org/wiki?curid=3988333", "title": "Zirconolite", "text": "Zirconolite\n\nZirconolite is a mineral, calcium zirconium titanate; formula CaZrTiO. Some examples of the mineral may also contain thorium, uranium, cerium, niobium and iron; the presence of thorium or uranium would make the mineral radioactive. It is black or brown in color.\n"}
