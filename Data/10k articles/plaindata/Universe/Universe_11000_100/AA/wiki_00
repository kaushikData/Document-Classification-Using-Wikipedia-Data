{"id": "41964318", "url": "https://en.wikipedia.org/wiki?curid=41964318", "title": "1934 Swissair Tuttlingen accident", "text": "1934 Swissair Tuttlingen accident\n\nThe 1934 Swissair Tuttlingen accident occurred on 27 July 1934 when a Swissair Curtiss AT-32C Condor II aircraft crashed near Tuttlingen, Germany while flying through a thunderstorm, killing all 12 people on board. It was the worst air crash in 1934 and Swissair's first aviation accident since its foundation in 1931.\nThe aircraft involved in the accident, registered CH-170, was a Curtiss AT-32C Condor II, a variant of the standard T-32 developed specifically for Swiss flag carrier Swissair, which was its only operator. CH-170 had entered service on 28 March 1934 and, by the time of the accident, had only been in service for four months. The cabin was configured with seating for up to 15 people.\n\nThe aircraft's flight attendant, Nelly Diener, also known as the \"Engel der Lüfte\" (\"Angel of the Skies\"), is notable for being Europe's first air stewardess. She had been working for Swissair since 1 May 1934. The other two crew members were the pilot, Armin Mühlematter, and the radio navigator, Hans Daschinger. On the accident flight, there were nine passengers aboard.\n\nThe aircraft departed Zurich for Berlin, with stopovers in Stuttgart and Leipzig. Shortly after crossing the Swiss-German border, the aircraft, cruising at an altitude of about 3,000 meters, encountered a thunderstorm, and while flying through it, the right wing eventually broke off. This resulted in an immediate loss of control and the aircraft plummeted into a forest near Tuttlingen, exploding into flames on impact.\n\nInvestigators found that oscillations in the wing had caused a stress fracture, the severity of which was exacerbated by the violent weather conditions in which the aircraft was flying. German investigators, however, determined that one fracture formed in the wing and engine mount structure due to defective construction and welding techniques in conjunction with the engine vibrations, while a second fracture resulted from the force of the turbulence in the storm.\n"}
{"id": "9480763", "url": "https://en.wikipedia.org/wiki?curid=9480763", "title": "Argon flash", "text": "Argon flash\n\nArgon flash, also known as argon bomb, argon flash bomb, argon candle, and argon light source, is a single-use source of very short and extremely bright flashes of light. The light is generated by a shock wave in argon or, less commonly, other noble gas. The shock wave is usually produced by an explosion. Argon flash devices are almost exclusively used for photographing explosions and shock waves.\n\nAlthough krypton and xenon can be also used; argon is favorable because of its low cost.\n\nThe light generated by an explosion is produced primarily by compression heating of the surrounding air. Replacement of the air with a noble gas considerably increases the light output; with molecular gases, the energy is consumed partially by dissociation and other processes, while noble gases are monatomic and can only undergo ionization; the ionized gas then produces the light. The low specific heat capacity of noble gases allows heating to higher temperatures, yielding brighter emission. Flashtubes are filled with noble gases for the same reason.\n\nTypical argon flash devices consist of an argon-filled cardboard or plastic tube with a transparent window on one end and an explosive charge on the other end. Many explosives can be used; Composition B, PETN, RDX, and plastic bonded explosives are just a few examples.\n\nThe device consists of a vessel filled with argon and a solid explosive charge. The explosion generates a shock wave, which heats the gas to very high temperature (over 10 K; published values vary between 15,000 K to 30,000 K with the best values around 25,000 K). The gas becomes incandescent and emits a flash of intense visible and ultraviolet black body radiation. The emission for the temperature range is highest between 97–193 nm, but usually only the visible and near-ultraviolet ranges are exploited.\n\nTo achieve emission, the layer of at least one or two optical depths of the gas has to be compressed to sufficient temperature. The light intensity rises to full magnitude in about 0.1 microsecond. For about 0.5 microsecond the shock wave front instabilities are sufficient to create significant striations in the produced light; this effect diminishes as the thickness of the compressed layer increases. Only about 75 micrometers thick layer of the gas is responsible for the light emission. The shock wave reflects after reaching the window at the end of the tube; this yields a short increase of light intensity. The intensity then fades \n\nThe amount of explosive can control the intensity of the shock wave and therefore of the flash. The intensity of the flash can be increased and its duration decreased by reflecting the shock wave by a suitable obstacle; a foil or a curved glass can be used. The duration of the flash is about as long as the explosion itself, depending on the construction of the lamp, between 0.1 and 100 microseconds. The duration is dependent on the length of the shockwave path through the gas, which is propoportional to the length of the tube; it was shown that each centimeter of the path of shock wave through the argon medium is equivalent to 2 microseconds.\n\nArgon flash is a standard procedure for high speed photography, especially for photographing explosions, or less commonly for use in high altitude test vehicles. The photography of explosions and shock waves is made easy by the fact that the detonation of the argon flash lamp charge can be accurately timed relative to the test specimen explosion and the light intensity can overpower the light generated by the explosion itself. The formation of shock waves during explosions of shaped charges can be imaged this way.\n\nAs the amount of released radiant energy is fairly high, significant heating of the illuminated object can occur. Especially in case of high explosives this has to be taken into account.\n\nSuper Radiant Light (SRL) sources are an alternative to argon flash. An electron beam source delivers brief and intense pulse of electrons to suitable crystals (e.g. cadmium sulfide). Flash times in nanosecond to picosecond range are achievable. Pulsed lasers are another alternative.\n"}
{"id": "29592212", "url": "https://en.wikipedia.org/wiki?curid=29592212", "title": "BSI PAS 2060", "text": "BSI PAS 2060\n\nPAS 2060 is a specification detailing how to demonstrate carbon neutrality produced and published by the British Standards Institution.\n\nThe British Standards Institution announced the development of the PAS 2060 Standard for Carbon Neutrality in October 2009 with the objective of increasing transparency of carbon neutrality claims by providing a common definition and recognized method of achieving carbon neutral status.\n\nThe steering group was made up of a number of public and private organisations and associations, including the Association of British Certification Bodies, Aviva, BREEAM (BRE Environmental Assessment Method), Carbon Clear, The Carbon Neutral Company, EcoAct, The Carbon Trust, The Co-operative Group, Department of Energy and Climate Change, Eurostar, Future Conversations (Concrete Centre), Good Energy, Institute of Environmental Management and Assessment, Marks and Spencer, Oxford Brookes University and UPM-Kymmene.\n\nPAS 2060 Specification for the demonstration of carbon neutrality was launched in June 2010.\n\nThe specification defines a consistent set of measures and requirements for entities (e.g. organisations, governments, communities, families, individuals) to demonstrate carbon neutrality for a product, service, organisation, community, event or building.\n\nThe carbon footprint measurements should include 100% of Scope 1 and Scope 2 emissions, plus all Scope 3 emissions that contribute more than 1% of the total footprint. \n\nThe entity must develop a Carbon Management Plan which contains a public commitment to carbon neutrality and outlines the following major aspects of the reduction strategy: a time scale, specific targets for reductions, the planned means of achieving reductions and how residual emissions will be offset.\n\nPAS 2060 requires that the total amount of carbon emissions at the end of a reduction period be offset by high-quality, certified carbon credits which meet the following criteria: \n\n• From one of the PAS 2060 approved schemes (for example the Clean Development Mechanism, Joint Implementation or Verified Carbon Standard)\n\n• Genuinely additional (i.e. reductions that would not have happened anyway)\n\n• Verified by an independent third party to ensure that emission reductions are permanent, avoid leakage (so that emissions are not increased in another area as a result of the project reductions) and are not double counted\n\n• Retired after a maximum of 12 months to a credible registry.\n\nPAS 2060 requires a standard-compliant declaration of achievement of neutrality through a set of Qualifying Explanatory Statements and public disclosure of all the documentation that supports the carbon neutrality claim.\n\nThe standard stipulates three types of validation of the achievement of neutrality: self validation, other party validation and third party independent validation. Other party validation occurs when the methodology and data has been audited and verified by an external organisation; third party independent validation occurs when it is verification is by an agent registered with UKAS.\n\n"}
{"id": "3159063", "url": "https://en.wikipedia.org/wiki?curid=3159063", "title": "Battery eliminator", "text": "Battery eliminator\n\nA battery eliminator is a device powered by an electrical source other than a battery, which then converts the source to a suitable DC voltage that may be used by a second device designed to be powered by batteries. A battery eliminator eliminates the need to replace batteries but may remove the advantage of portability. A battery eliminator is also effective in replacing obsolete battery designs.\n\nSome examples of battery eliminators include the nine volt mains power supply, the size and shape of a PP9 battery, originally intended to replace the battery in portable radios in the 1960s. A solar panel providing power for a portable appliance may also be considered a battery eliminator. The term is also sometimes used as a misnomer when using a bigger battery for more runtime when branching out a power supply to wired electrical equipment using DC input.\n\nEarly commercial battery eliminators were produced by the Edward S. Rogers, Sr. company in 1925 as a complement to his line of \"batteryless\" radio receivers. Another early producer of battery eliminators was the Galvin Manufacturing Corporation (later known as Motorola) which was opened on September 25, 1928 by Paul Galvin and his brother Joseph E. Galvin, to build battery eliminators for radio receivers installed in automobiles.\n\nThe first car radio receivers were based on vacuum tube technology which required two or three different voltages to function:\n\nBatteries designed for these portable vacuum tube receivers were a combination of several different battery types and sizes, combined in a single package and intended to slowly wear out at about the same rate. The battery typically connected to the radio via a specially shaped four- or five-pin connector, keyed so that the plug must be inserted correctly. A battery eliminator would take the typical 6-volt or 12-volt DC power from a car battery and transform it into the required LT and HT needed to power the vacuum tubes in a car radio. Without a battery eliminator, it was necessary to occasionally replace the battery pack in the vacuum-tube car radio.\n\nTripod-mounted cameras in photography studios often use a battery eliminator to avoid having to interrupt lengthy shooting sessions to replace batteries. The use of the tripod can also interfere with access to the battery hatch.\n\n"}
{"id": "30210534", "url": "https://en.wikipedia.org/wiki?curid=30210534", "title": "Beta-Eleostearic acid", "text": "Beta-Eleostearic acid\n\nβ-Eleostearic acid, or (9\"E\",11\"E\",13\"E\")-octadeca-9,11,13-trienoic acid, is an organic compound with formula or HC-(-CH-)(-CH=CH-)-(-CH-)-(C=O)OH. It is the all-\"trans\" conformational isomer of octadecatrienoic acid.\n\n"}
{"id": "29744058", "url": "https://en.wikipedia.org/wiki?curid=29744058", "title": "Bottle recycling", "text": "Bottle recycling\n\nBottles are able to be recycled and this is generally a positive option. Bottles are collected via kerbside collection or returned using a bottle deposit system. Bottlerecycle.org/ reveals that just 14% of all plastic packaging is recycled globally PET bottles production is predicted to grow by about 5% a year. Currently just over half of plastic bottles are recycled globally About 1 million plastic bottles are bought around the world every minute and only about 50% are recycled.\n\nGlass bottles are fully recyclable, either to be reused as bottle or to be melted and reused as glass, resulting in a saving of energy and raw materials.\n\nRecycling one glass bottle can save enough energy to power a computer for 25 minutes. \n\nPET bottles are mostly recycled as a raw material. In many countries, PET plastics are coded with the resin identification code number \"1\" inside the universal recycling symbol, usually located on the bottom of the container.\n\nHDPE is commonly used in bottles, particularly bottles (or jugs) of milk. Recycling code 2 is applicable. In the US, only about 30-35% of HDPE bottles are recycled.\n\nContainer deposit legislation are laws passed by city, state, provincial, or national governments. They require a deposit on bottles to be collected when sold and reimbursed when returned.\n\nIn May 2018 the Israeli Ministry Impose EUR 12 m Fine on Bottle Manufacturers and Importers that Didn't Meet Collection Targets\n\nMany potential factors are involved in environmental comparisons of returnable vs non-returnable systems. Researchers have often used life cycle analysis methodologies to balance the many diverse considerations. Often the comparisons show benefits and problems with all alternatives. It helps provide a objective view of a complex subject.\nReuse of bottles requires a reverse logistics system, cleaning and, sanitizing bottles, and an effective Quality Management System. A key factor with glass milk bottles is the number of cycles of uses to be expected. Breakage, contamination, or other loss reduces the benefits of returnables. A key factor with one-way recyclables is the recycling rate: In the US, only about 30-35% of HDPE bottles are recycled.\n\n\nhttp://www.sviva.gov.il/English/ResourcesandServices/NewsAndEvents/NewsAndMessageDover/Pages/2018/05-May/Ministry-May-Impose-NIS-50m-Fine-on-Bottle-Manufacturers-and-Importers-that-Didn't-Meet-Plastic-Bottle-Collection-Targets.aspx\n\n"}
{"id": "16398870", "url": "https://en.wikipedia.org/wiki?curid=16398870", "title": "Burundi Ministry of Energy and Mines", "text": "Burundi Ministry of Energy and Mines\n\nThe Burundi Ministry of Energy and Mines is responsible for monitoring Mining in Burundi.\n\n"}
{"id": "26758911", "url": "https://en.wikipedia.org/wiki?curid=26758911", "title": "Carbon dioxide reforming", "text": "Carbon dioxide reforming\n\nCarbon dioxide reformation (also known as dry reformation) is a method of producing synthesis gas (mixtures of hydrogen and carbon monoxide) from the reaction of carbon dioxide with hydrocarbons such as methane. Synthesis gas is conventionally produced via the steam reforming reaction. In recent years, increased concerns on the contribution of greenhouse gases to global warming have increased interest in the replacement of steam as reactant with carbon dioxide.\n\nThe dry reforming reaction may be represented by:\nThus, two greenhouse gases are consumed and useful chemical building blocks, hydrogen and carbon monoxide, are produced. A challenge to the commercialization of this process is that the hydrogen that is produced tends to react with the carbon dioxide. For example, the following reaction typically proceeds with a lower activation energy than the dry reforming reaction itself:\nTypical catalysts are noble metals, Ni or Ni alloys. In addition, a group of researchers in China investigated the use of activated carbon as an alternative catalyst.\n"}
{"id": "7393417", "url": "https://en.wikipedia.org/wiki?curid=7393417", "title": "Challicum Hills Wind Farm", "text": "Challicum Hills Wind Farm\n\nChallicum Hills Wind Farm is a wind farm encompassed by 35 (1.5 MW) NEG NM 64 wind turbines, with a total generating capacity of 52.5 MW of electricity. The wind farm is near Ararat in western Victoria, Australia. The power station was commissioned in August 2003 and is in a long term Power Purchase Agreement (PPA) with Origin Energy.\n\nThe wind farm is owned and operated by Pacific Hydro. Part of the profits flow into a community fund that supports local projects.\n\n\n"}
{"id": "40742435", "url": "https://en.wikipedia.org/wiki?curid=40742435", "title": "Cierzo", "text": "Cierzo\n\nThe cierzo is a strong, dry and usually cold wind that blows from the North or Northwest through the regions of Aragon, La Rioja and Navarra in the Ebro valley in Spain. It takes place when there is an anticyclone in the Bay of Biscay and an low-pressure area in the Mediterranean Sea.\n\nIt is known since ancient times, and its name stems from the Latin word \"circius\", which probably came from an Iberian word. Cato the Elder described it in the 2nd century BC as \"a wind that fills your mouth and tumbles waggons and armed men.\"\nIt reaches a speed of more than 100 km/h several times each year. Its maximum recorded speed has been 160 km/h in July 1956. It is more usual in autumn and winter, when larger pressure gradients take place, but a small pressure difference along the Ebro valley is sufficient to initiate a cierzo wind in any season.\n\nThe cierzo wind is similar to the mistral of the Rhone valley in France or the bora in the Balcans.\n"}
{"id": "32240436", "url": "https://en.wikipedia.org/wiki?curid=32240436", "title": "Climate Change Capital", "text": "Climate Change Capital\n\nClimate Change Capital is a private asset management and advisory group founded in 2003 by Lionel Fretz and James Cameron to support efforts to develop solutions to climate change and resource depletion.\n\nThe company established an advisory group in 2004 to provide financial, strategic and policy advice to energy-intensive industries, financial institutions, cleantech companies and governments.\n\nThe asset management business, which was established in 2005, includes a carbon finance fund that invests in emission reduction projects, predominantly in developing countries, a private equity fund that invests in late stage technology and services companies headquartered in Europe and a property fund that buys commercial green buildings or retrofits existing commercial properties in the United Kingdom.\n\nThe company's think tank was established in 2009 to promote discussion of how capital can be deployed to mitigate and adapt to climate change.\n\nIn 2006, the group launched the world's largest private sector carbon fund.\n\nIn April 2012, Bunge Ltd, a global agribusiness and food company founded in 1818, acquired Climate Change Capital Group Limited.\n\nClimate Change Capital’s chairman is James Cameron, a member of General Electric's ecomagination board, a trustee member of the UK Green Building Council and the Carbon Disclosure Project. Climate Change Capital's CEO is Eric Alsembach who also serves as the managing director of the Bunge Asset Management group.\n\n"}
{"id": "58602846", "url": "https://en.wikipedia.org/wiki?curid=58602846", "title": "Céline Bœhm", "text": "Céline Bœhm\n\nCéline Bœhm is a Professor of Particle Physics at the University of Sydney. She works on astroparticle physics and dark matter.\n\nBœhm studied fundamental physics at the Pierre and Marie Curie University, graduating in 1997. She joined École Polytechnique, where she was ranked first in the year for a Masters in engineering in 1998. She earned the highest distinction for a postgraduate diploma in theoretical physics. She completed her PhD at the École normale supérieure in Paris in 2001, working with . She worked on supersymmetry and was the first to predict the 4-body decay of the stop particle. She studied light scalar top quark and supersymmetric dark matter. She looked at collisional damping, which considers the impact of dark matter and standard model particles with the cosmic microwave background.\n\nIn 2001 Bœhm joined Joseph Silk at the University of Oxford. Here she worked on light dark matter particles which couple to light Z′ bosons. She proposed new candidates for scalar dark matter, in the form of heavy fermions or light gauge bosons. When the SPI spectrometer onboard INTEGRAL identified a 511 keV line in the Galactic Center, Bœhm predicted that this could have been the signature of dark matter. She has continued to search for new signatures of dark matter, including examining the GeV excess in the Fermi Gamma-ray Space Telescope data. In 2004 Bœhm joined the Laboratoire d'Annecy-le-Vieux de Physique Théorique, where she was promoted to senior lecturer in 2008. She was awarded the Centre national de la recherche scientifique Bronze Medal.\n\nShe looked at the analysis of the CoGeNT direct detection method, and found that it could have suffered from a large background. In 2015 Boehm was nominated as Fellow of the Institute of Physics. She is the Principal investigator of the \"Theia\" mission, a space observatory which will allow Bœhm and her team to test the dark matter predictions that arise due to the Lambda-CDM model.\n\nBoehm was made an Emmy Noether Fellow at the Perimeter Institute for Theoretical Physics in 2016, where she continued to work on dark matter. That year, she was promoted to Professor in the Institute for Particle Physics Phenomenology at Durham University. She gave a TED talk, \"The Invisible is All What Matters\", at Durham in 2017. Alongside her work in astroparticle physics, she works on non-crystallographic Coxeter groups. She led the dark matter working package of the Euclid Consortium. In 2017 Bœhm spent two months as a visiting professor at Columbia University, as well as working at the Paris Observatory. She proposed using circular polarisation to study dark matter and neutrinos. She joined the University of Sydney as Head of School for physics in 2018. Bœhm has written for The Conversation. She has taken part in Pint of Science. \n"}
{"id": "48004054", "url": "https://en.wikipedia.org/wiki?curid=48004054", "title": "Donald Mackay Medal", "text": "Donald Mackay Medal\n\nDr. Donald Mackay was deputy Director of the Ross Institute at the London School of Hygiene and Tropical Medicine. He worked for many years in tropical occupational health, especially on the tea plantations of South Asia. He died in 1981.\n\nThe Donald Mackay Medal is awarded in his honor for outstanding work in tropical health, especially relating to improvements in the health of rural or urban workers in the tropics.\n\nThe award criteria are determined by the :\n\nThe medal is awarded annually, by the Royal Society of Tropical Medicine and Hygiene in even-numbered years by the American Society of Tropical Medicine and Hygiene in odd-numbered years. It was first awarded in 1990.\n\n"}
{"id": "4092733", "url": "https://en.wikipedia.org/wiki?curid=4092733", "title": "Dust collector", "text": "Dust collector\n\nA dust collector is a system used to enhance the quality of air released from industrial and commercial processes by collecting dust and other impurities from air or gas. Designed to handle high-volume dust loads, a dust collector system consists of a blower, dust filter, a filter-cleaning system, and a dust receptacle or dust removal system. It is distinguished from air purifiers, which use disposable filters to remove dust.\n\nThe father of the dust collector was Wilhelm Beth from Lübeck. In 1921, he patented three filter designs that he had pioneered to remove dust from air.\n\nDust collectors are used in many processes to either recover valuable granular solid or powder from process streams, or to remove granular solid pollutants from exhaust gases prior to venting to the atmosphere. Dust collection is an online process for collecting any process-generated dust from the source point on a continuous basis. Dust collectors may be of single unit construction, or a collection of devices used to separate particulate matter from the process air. They are often used as an air pollution control device to maintain or improve air quality.\n\nMist collectors remove particulate matter in the form of fine liquid droplets from the air. They are often used for the collection of metal working fluids, and coolant or oil mists. Mist collectors are often used to improve or maintain the quality of air in the workplace environment.\n\nFume and smoke collectors are used to remove sub-micrometer-size particulates from the air. They effectively reduce or eliminate particulate matter and gas streams from many industrial processes such as welding, rubber and plastic processing, high speed machining with coolants, tempering, and quenching.\n\nFive main types of industrial dust collectors are:\n\nInertial separators separate dust from gas streams using a combination of forces, such as centrifugal, gravitational, and inertial. These forces move the dust to an area where the forces exerted by the gas stream are minimal. The separated dust is moved by gravity into a hopper, where it is temporarily stored.\n\nThe three primary types of inertial separators are:\n\nNeither settling chambers nor baffle chambers are commonly used in the minerals processing industry. However, their principles of operation are often incorporated into the design of more efficient dust collectors.\n\nA settling chamber consists of a large box installed in the ductwork. The increase of cross section area at the chamber reduces the speed of the dust-filled airstream and heavier particles settle out.\nSettling chambers are simple in design and can be manufactured from almost any material. However, they are seldom used as primary dust collectors because of their large space requirements and low efficiency. A practical use is as precleaners for more efficient collect.\nAdvantages: 1) simple construction and low cost 2) maintenance free 3) collects particles without need of water. Disadvantages: 1) low efficiency 2) large space required.\n\nBaffle chambers use a fixed baffle plate that causes the conveying gas stream to make a sudden change of direction. Large-diameter particles do not follow the gas stream but continue into a dead air space and settle. Baffle chambers are used as precleaners\n\nCentrifugal collectors use cyclonic action to separate dust particles from the gas stream. In a typical cyclone, the dust gas stream enters at an angle and is spun rapidly. The centrifugal force created by the circular flow throws the dust particles toward the wall of the cyclone. After striking the wall, these particles fall into a hopper located underneath.\n\nThe most common types of centrifugal, or inertial, collectors in use today are:\n\nSingle-cyclone separators create a dual vortex to separate coarse from fine dust. The main vortex spirals downward and carries most of the coarser dust particles. The inner vortex, created near the bottom of the cyclone, spirals upward and carries finer dust particles.\n\nMultiple-cyclone separators consist of a number of small-diameter cyclones, operating in parallel and having a common gas inlet and outlet, as shown in the figure, and operate on the same principle as single cyclone separators—creating an outer downward vortex and an ascending inner vortex.\n\nMultiple-cyclone separators remove more dust than single cyclone separators because the individual cyclones have a greater length and smaller diameter. The longer length provides longer residence time while the smaller diameter creates greater centrifugal force. These two factors result in better separation of dust particulates. The pressure drop of multiple-cyclone separators collectors is higher than that of single-cyclone separators, requiring more energy to clean the same amount of air. A single-chamber cyclone separator of the same volume is more economical, but doesn't remove as much dust.\n\nCyclone separators are found in all types of power and industrial applications, including pulp and paper plants, cement plants, steel mills, petroleum coke plants, metallurgical plants, saw mills and other kinds of facilities that process dust.\n\nThis type of cyclone uses a secondary air flow, injected into the cyclone to accomplish several things. The secondary air flow increases the speed of the cyclonic action making the separator more efficient; it intercepts the particulate before it reaches the interior walls of the unit; and it forces the separated particulate toward the collection area. The secondary air flow protects the separator from particulate abrasion and allows the separator to be installed horizontally because gravity is not depended upon to move the separated particulate downward.\n\nCommonly known as baghouses, fabric collectors use filtration to separate dust particulates from dusty gases. They are one of the most efficient and cost-effective types of dust collectors available, and can achieve a collection efficiency of more than 99% for very fine particulates.\n\nDust-laden gases enter the baghouse and pass through fabric bags that act as filters. The bags can be of woven or felted cotton, synthetic, or glass-fiber material in either a tube or envelope shape.\n\nTo ensure the filter bags have a long usage life they are commonly coated with a filter enhancer (pre-coat). The use of chemically inert limestone (calcium carbonate) is most common as it maximises efficiency of dust collection (including fly ash) via formation of what is called a dustcake or coating on the surface of the filter media. This not only traps fine particulates but also provides protection for the bag itself from moisture, and oily or sticky particulates which can bind the filter media. Without a pre-coat the filter bag allows fine particulates to bleed through the bag filter system, especially during start-up, as the bag can only do part of the filtration leaving the finer parts to the filter enhancer dustcake.\n\nFabric filters generally have the following parts:\n\nBaghouses are characterized by their cleaning method.\n\nA rod connecting to the bag is powered by a motor. This provides motion to remove caked-on particles. The speed and motion of the shaking depends on the design of the bag and composition of the particulate matter. Generally shaking is horizontal. The top of the bag is closed and the bottom is open. When shaken, the dust collected on the inside of the bag is freed. During the cleaning process, no dirty gas flows through a bag while the bag is being cleaned. This redirection of air flow illustrates why baghouses must be compartmentalized.\n\nAir flow gives the bag structure. Dirty air flows through the bag from the inside, allowing dust to collect on the interior surface. During cleaning, gas flow is restricted from a specific compartment. Without the flowing air, the bags relax. The cylindrical bag contains rings that prevent it from completely collapsing under the pressure of the air. A fan blows clean air in the reverse direction. The relaxation and reverse air flow cause the dust cake to crumble and release into the hopper. Upon the completion of the cleaning process, dirty air flow continues and the bag regains its shape.\n\nThis type of baghouse cleaning (also known as pressure-jet cleaning) is the most common. A high pressure blast of air is used to remove dust from the bag. The blast enters the top of the bag tube, temporarily ceasing the flow of dirty air. The shock of air causes a wave of expansion to travel down the fabric. The flexing of the bag shatters and discharges the dust cake.\nThe air burst is about 0.1 second and it takes about 0.5 seconds for the shock wave to travel down the length of the bag. Due to its rapid release, the blast of air does not interfere with contaminated gas flow. Therefore, pulse-jet baghouses can operate continuously and are not usually compartmentalized.\nThe blast of compressed air must be powerful enough to ensure that the shock wave will travel the entire length of the bag and fracture the dust cake.\nThe efficiency of the cleaning system allows the unit to have a much higher gas to cloth ratio (or volumetric throughput of gas per unit area of filter) than shaking and reverse air bag filters. This kind of filter thus requires a smaller area to admit the same volume of air.\n\nThe least common type of cleaning method is sonic. Shaking is achieved by sonic vibration. A sound generator produces a low frequency sound that causes the bags to vibrate. Sonic cleaning is commonly combined with another method of cleaning to ensure thorough cleaning.\n\nCartridge collectors use perforated metal cartridges that contain a pleated, nonwoven filtering media, as opposed to woven or felt bags used in baghouses. The pleated design allows for a greater total filtering surface area than in a conventional bag of the same diameter, The greater filtering area results in a reduced air to media ratio, pressure drop, and overall collector size.\n\nCartridge collectors are available in single use or continuous duty designs. In single-use collectors, the dirty cartridges are changed and collected dirt is removed while the collector is off. In the continuous duty design, the cartridges are cleaned by the conventional pulse-jet cleaning system.\n\nDust collectors that use liquid are known as wet scrubbers. In these systems, the scrubbing liquid (usually water) comes into contact with a gas stream containing dust particles. Greater contact of the gas and liquid streams yields higher dust removal efficiency.\n\nThere is a large variety of wet scrubbers; however, all have one of three basic configurations:\n\n1. Gas-humidification - The gas-humidification process agglomerates fine particles, increasing the bulk, making collection easier.\n\n2. Gas-liquid contact - This is one of the most important factors affecting collection efficiency. The particle and droplet come into contact by four primary mechanisms:\n\n3. Gas-liquid separation - Regardless of the contact mechanism used, as much liquid and dust as possible must be removed. Once contact is made, dust particulates and water droplets combine to form agglomerates. As the agglomerates grow larger, they settle into a collector.\n\nThe \"cleaned\" gases are normally passed through a mist eliminator (demister pads) to remove water droplets from the gas stream. The dirty water from the scrubber system is either cleaned and discharged or recycled to the scrubber. Dust is removed from the scrubber in a clarification unit or a drag chain tank. In both systems solid material settles on the bottom of the tank. A drag chain system removes the sludge and deposits in into a dumpster or stockpile.\n\nSpray-tower scrubber wet scrubbers may be categorized by pressure drop as follows:\n\n\nDue to the large number of commercial scrubbers available, it is not possible to describe each individual type here. However, the following sections provide examples of typical scrubbers in each category.\n\nIn the simple, gravity-spray-tower scrubber, liquid droplets formed by liquid atomized in spray nozzles fall through rising exhaust gases. Dirty water is drained at the bottom.\n\nThese scrubbers operated at pressure drops of 1 to 2 in. water gauge (¼ to ½ kPa) and are approximately 70% efficient on 10 µm particles. Their efficiency is poor below 10 µm. However, they are capable of treating relatively high dust concentrations without becoming plugged.\n\nWet cyclones use centrifugal force to spin the dust particles (similar to a cyclone), and throw the particulates upon the collector's wetted walls. Water introduced from the top to wet the cyclone walls carries these particles away. The wetted walls also prevent dust reentrainment.\n\nPressure drops for these collectors range from 2 to 8 in. water (½ to 2 kPa), and the collection efficiency is good for 5 μm particles and above.\n\nPacked-bed scrubbers consist of beds of packing elements, such as coke, broken rock, rings, saddles, or other manufactured elements. The packing breaks down the liquid flow into a high-surface-area film so that the dusty gas streams passing through the bed achieve maximum contact with the liquid film and become deposited on the surfaces of the packing elements. These scrubbers have a good collection efficiency for respirable dust.\n\nThree types of packed-bed scrubbers are:\n\nEfficiency can be greatly increased by minimizing target size, i.e., using 0.003 in. (0.076 mm) diameter stainless steel wire and increasing gas velocity to more than 1,800 ft/min (9.14 m/s).\n\nVenturi scrubbers consist of a venturi-shaped inlet and separator. The dust-laden gases venturi scrubber enter through the venturi and are accelerated to speeds between 12,000 and 36,000 ft/min (60.97-182.83 m/s). These high-gas velocities immediately atomize the coarse water spray, which is injected radially into the venturi throat, into fine droplets. High energy and extreme turbulence promote collision between water droplets and dust particulates in the throat. The agglomeration process between particle and droplet continues in the diverging section of the venturi. The large agglomerates formed in the venturi are then removed by an inertial separator.\n\nVenturi scrubbers achieve very high collection efficiencies for respirable dust. Since efficiency of a venturi scrubber depends on pressure drop, some manufacturers supply a variable-throat venturi to maintain pressure drop with varying gas flows.\n\nElectrostatic precipitators use electrostatic forces to separate dust particles from exhaust gases. A number of high-voltage, direct-current discharge electrodes are placed between grounded collecting electrodes. The contaminated gases flow through the passage formed by the discharge and collecting electrodes. Electrostatic precipitators operate on the same principle as home \"Ionic\" air purifiers.\n\nThe airborne particles receive a negative charge as they pass through the ionized field between the electrodes. These charged particles are then attracted to a grounded or positively charged electrode and adhere to it.\n\nThe collected material on the electrodes is removed by rapping or vibrating the collecting electrodes either continuously or at a predetermined interval. Cleaning a precipitator can usually be done without interrupting the airflow.\n\nThe four main components of all electrostatic precipitators are:\n\nThe following factors affect the efficiency of electrostatic precipitators:\n\nThere are two main types of precipitators:\n\nDescribed below is the high-voltage, single-stage precipitator, which is widely used in minerals processing operations. The low-voltage, two-stage precipitator is generally used for filtration in air-conditioning systems.\n\nThe majority of electrostatic precipitators installed are the plate type. Particles are collected on flat, parallel surfaces that are 8 to 12 in. (20 to 30 cm) apart, with a series of discharge electrodes spaced along the centerline of two adjacent plates. The contaminated gases pass through the passage between the plates, and the particles become charged and adhere to the collection plates. Collected particles are usually removed by rapping the plates and deposited in bins or hoppers at the base of the precipitator.\n\nTubular precipitators consist of cylindrical collection electrodes with discharge electrodes located on the axis of the cylinder. The contaminated gases flow around the discharge electrode and up through the inside of the cylinders. The charged particles are collected on the grounded walls of the cylinder. The collected dust is removed from the bottom of the cylinder.\n\nTubular precipitators are often used for mist or fog collection or for adhesive, sticky, radioactive, or extremely toxic materials.\n\nUnlike central collectors, unit collectors control contamination at its source. They are small and self-contained, consisting of a fan and some form of dust collector. They are suitable for isolated, portable, or frequently moved dust-producing operations, such as bins and silos or remote belt-conveyor transfer points. Advantages of unit collectors include small space requirements, the return of collected dust to main material flow, and low initial cost. However, their dust-holding and storage capacities, servicing facilities, and maintenance periods have been sacrificed.\n\nA number of designs are available, with capacities ranging from 200 to 2,000 ft³/min (90 to 900 L/s). There are two main types of unit collectors:\n\nFabric collectors are frequently used in minerals processing operations because they provide high collection efficiency and uninterrupted exhaust airflow between cleaning cycles. Cyclone collectors are used when coarser dust is generated, as in woodworking, metal grinding, or machining.\n\nThe following points should be considered when selecting a unit collector:\n\nUse of unit collectors may not be appropriate if the dust-producing operations are located in an area where central exhaust systems would be practical. Dust removal and servicing requirements are expensive for many unit collectors and are more likely to be neglected than those for a single, large collector.\n\nDust collectors vary widely in design, operation, effectiveness, space requirements, construction, and capital, operating, and maintenance costs. Each type has advantages and disadvantages. However, the selection of a dust collector should be based on the following general factors:\n\n\nThe fan and motor system supplies mechanical energy to move contaminated air from the dust-producing source to a dust collector.\n\nThere are two main kinds of industrial fans:\n\n\nCentrifugal fans consist of a wheel or a rotor mounted on a shaft that rotates in a scroll-shaped housing. Air enters at the eye of the rotor, makes a right-angle turn, and is forced through the blades of the rotor by centrifugal force into the scroll-shaped housing. The centrifugal force imparts static pressure to the air. The diverging shape of the scroll also converts a portion of the velocity pressure into static pressure.\n\nThere are three main types of centrifugal fans:\n\n\nAxial-flow fans are used in systems that have low resistance levels. These fans move the air parallel to the fan's axis of rotation. The screw-like action of the propellers moves the air in a straight-through parallel path, causing a helical flow pattern.\n\nThe three main kinds of axial fans are:\n\n\nFan selection\n\nWhen selecting a fan, the following points should be considered:\n\n\nFan Rating Tables\n\nAfter the above information is collected, the actual selection of fan size and speed is usually made from a rating table published by the fan manufacturer. This table is known as a multirating table, and it shows the complete range of capacities for a particular size of fan.\n\nPoints to note:\n\n\nFan installation\nTypical fan discharge conditions\nFan ratings for volume and static pressure, as described in the multirating tables, are based on the tests conducted under ideal conditions. Often, field installation creates airflow problems that reduce the fan's air delivery. The following points should be considered when installing the fan:\n\n\nElectric motors are used to supply the necessary energy to drive the fan.\n\nIntegral-horsepower electric motors are normally three-phase, alternating-current motors. Fractional-horsepower electric motors are normally single-phase, alternating-current motors and are used when less than is required. Since most dust collection systems require motors with more than , only integral-horsepower motors are discussed here.\n\nThe two most common types of integral-horsepower motors used in dust collection systems are:\n\n\nSquirrel-cage and wound-rotor motors are further classified according to the type of enclosure they use to protect their interior windings. These enclosures fall into two broad categories:\n\nDrip-proof and splash-proof motors are open motors. They provide varying degrees of protection; however, they should not be used where the air contains substances that might be harmful to the interior of the motor.\n\nTotally enclosed motors are weather-protected with the windings enclosed. These enclosures prevent free exchange of air between the inside and the outside, but they are not airtight.\n\nTotally enclosed, fan-cooled (TEFC) motors are another kind of totally enclosed motor. These motors are the most commonly used motors in dust collection systems. They have an integral-cooling fan outside the enclosure, but within the protective shield, that directs air over the enclosure.\n\nBoth open and totally enclosed motors are available in explosion-proof and dust-ignition-proof models to protect against explosion and fire in hazardous environments.\n\nMotors are selected to provide sufficient power to operate fans over the full range of process conditions (temperature and flow rate).\nDust collectors can be configured into one of five common types:\n\nImportant parameters in specifying dust collectors include airflow the velocity of the air stream created by the vacuum producer; system power, the power of the system motor, usually specified in horsepower; storage capacity for dust and particles, and minimum particle size filtered by the unit. Other considerations when choosing a dust collection system include the temperature, moisture content, and the possibility of combustion of the dust being collected.\n\nSystems for fine removal may only contain a single filtration system (such as a filter bag or cartridge). However, most units utilize a primary and secondary separation/filtration system. In many cases the heat or moisture content of dust can negatively affect the filter media of a baghouse or cartridge dust collector. A cyclone separator or dryer may be placed before these units to reduce heat or moisture content before reaching the filters. Furthermore, some units may have third and fourth stage filtration. All separation and filtration systems used within the unit should be specified.\nA baghouse is an air pollution abatement device used to trap particulate by filtering gas streams through large fabric bags. They are typically made of glass fibers or fabric.\nA cyclone separator is an apparatus for the separation, by centrifugal means, of fine particles suspended in air or gas.\nElectrostatic precipitators are a type of air cleaner, which charges particles of dust by passing dust-laden air through a strong (50-100 kV) electrostatic field. This causes the particles to be attracted to oppositely charged plates so that they can be removed from the air stream.\n\nAn impinger system is a device in which particles are removed by impacting the aerosol particles into a liquid. Modular media type units combine a variety of specific filter modules in one unit. These systems can provide solutions to many air contaminant problems. A typical system incorporates a series of disposable or cleanable pre-filters, a disposable vee-bag or cartridge filter. HEPA or carbon final filter modules can also be added. Various models are available, including free-hanging or ducted installations, vertical or horizontal mounting, and fixed or portable configurations. Filter cartridges are made out of a variety of synthetic fibers and are capable of collecting sub-micrometre particles without creating an excessive pressure drop in the system. Filter cartridges require periodic cleaning.\n\nA wet scrubber, or venturi scrubber, is similar to a cyclone but it has an orifice unit that sprays water into the vortex in the cyclone section, collecting all of the dust in a slurry system. The water media can be recirculated and reused to continue to filter the air. Eventually the solids must be removed from the water stream and disposed of.\n\nOnline cleaning – automatically timed filter cleaning which allows for continuous, uninterrupted dust collector operation for heavy dust operations.\n\nOffline cleaning – filter cleaning accomplished during dust collector shut down. Practical whenever the dust loading in each dust collector cycle does not exceed the filter capacity. Allows for maximum effectiveness in dislodging and disposing of dust.\n\nOn-demand cleaning – filter cleaning initiated automatically when the filter is fully loaded, as determined by a specified drop in pressure across the media surface.\n\nReverse-pulse/Reverse-jet cleaning – Filter cleaning method which delivers blasts of compressed air from the clean side of the filter to dislodge the accumulated dust cake.\n\nImpact/Rapper cleaning – Filter cleaning method in which high-velocity compressed air forced through a flexible tube results in an arbitrary rapping of the filter to dislodge the dust cake. Especially effective when the dust is extremely fine or sticky.\n\n\n"}
{"id": "40278004", "url": "https://en.wikipedia.org/wiki?curid=40278004", "title": "EDP Brasil", "text": "EDP Brasil\n\nEDP Brasil is one of the largest Brazilian electric utility company, it is a subsidiary of EDP - Energias de Portugal in Brazil. The company was founded in 1996 in São Paulo.\n\nThrough its subsidiaries the company generates, distributes and sells electric energy in nine Brazilian states and. In the generation segment, representing 2,3 GW of installed capacity. In the distribution segment, the group operates in two states (São Paulo and Espírito Santo) and serves a total of 3.1 million customers. In the commercialization segment, EDP operates in the free contracting environment both in the concession areas of our distributors as well as in other concession areas and in renewable energy segment the company operates wind and hydro electric plants with an installed generation capacity of 2.381 MW.\n\nIt is the third largest non-state electricity commercialization company in Brazil, the fifth largest non-state generation and the fourth largest non-state in energy distribution.\n\nOn listing its capital in July 2005, it signed up to the São Paulo Stock Exchange’s (BM&F Bovespa) Novo Mercado.\n\nFor the seventh consecutive year, the company is a component of BM&F Bovespa's Corporate Sustainability Index (ISE).\n\n"}
{"id": "21409647", "url": "https://en.wikipedia.org/wiki?curid=21409647", "title": "Eaton BladeUPS", "text": "Eaton BladeUPS\n\nThe Eaton BladeUPS is a modular three-phase UPS system consisting of individual 6U 12 kW UPS units which can be paralleled together to create up to a 60 kW N+1 redundant UPS. A feature of the BladeUPS is that the 6U cabinet houses both the UPS electronics and batteries; other modular systems house them separately. The high power density and 6U form factor of the BladeUPS are targeted at the growing power demands of IT servers and equipment, especially blade servers.\n\nThe BladeUPS does not fall into a defined category for UPS type as it is a hybrid of a line interactive and online architecture, which Eaton calls Double Conversion on Demand. During normal operation the BladeUPS operates as a line interactive UPS allowing it to be energy efficient. When power conditions fluctuate outside preset standards, the unit switches into online mode (also known as double conversion). Software upgrades by Eaton have allowed BladeUPS owners to defeat this hybrid architecture and force the unit to operate entirely in online (double conversion) mode.\n\nBladeUPS ships with Eaton’s LanSafe software which allows for UPS monitoring and commands. The BladeUPS is compatible with Eaton’s Power Xpert and FORESEER software which provide additional monitoring and functionality.\n\nUp to four extended battery modules can be connected to each BladeUPS providing additional runtime.\n\nBladeUPS was named a 2007 searchdatacenter.com Gold Product of the Year.\n\n"}
{"id": "9773374", "url": "https://en.wikipedia.org/wiki?curid=9773374", "title": "Energy Saving Trust", "text": "Energy Saving Trust\n\nEnergy Saving Trust (EST) is a British organization devoted to promoting energy efficiency, energy conservation, and the sustainable use of energy, thereby reducing carbon dioxide emissions and helping to prevent man-made climate change. It was founded in the United Kingdom as a government-sponsored initiative in 1992, following the global Earth Summit. \n\nEnergy Saving Trust is an independent, not-for-profit organization funded by the government and the private sector. It is a social enterprise, and also has a charitable foundation. The EST has regional offices in England, and country offices in Wales, Northern Ireland, and Scotland. It maintains a comprehensive website, and a network of numerous local advice centres. \n\nThe Energy Saving Trust was formally established in November 1992. It was formed, as a public-private partnership, in response both to the director-general of Ofgas's 1991 proposal to increase energy efficiency in natural gas use, and to the global June 1992 Earth Summit call to reduce greenhouse gas emissions and prevent global warming and climate change. In the wake of energy-supplier privatisation in the UK, the EST was also specifically formed as an instrument to ensure energy conservation and carbon-emission reduction in a free-market environment. The structure, scope, nature, and funding of EST's activities and programmes have varied over the years due to governmental policy changes; however its primary focus – on consumers and households – has remained the same. It is the largest provider of energy-saving advice, and has effected significant and measurable savings of energy, money, and carbon.\n\nEST's main goals are to achieve the sustainable use of energy and to cut carbon dioxide emissions. It acts as a bridge between consumers, government, trade, businesses, third sector organisations, local authorities, and the energy market. The EST's target audience is consumers, local authorities, energy companies, and policy makers. Among other activities, they provide: \n\nThe EST provides grants and free advice to the public to help reduce energy use, energy bills, and greenhouse gas emissions. \n\nFor individuals, Energy Saving Trust provides information and advice on subjects including:\n\n\nFor organisations, Energy Saving Trust provides numerous services including:\n\n\n\n"}
{"id": "3857694", "url": "https://en.wikipedia.org/wiki?curid=3857694", "title": "Firefighting foam", "text": "Firefighting foam\n\nFirefighting foam is a foam used for fire suppression. Its role is to cool the fire and to coat the fuel, preventing its contact with oxygen, resulting in suppression of the combustion. Fire-fighting foam was invented by the Russian engineer and chemist Aleksandr Loran in 1902.\n\nThe surfactants used must produce foam in concentration of less than 1%. Other components of fire-retardant foams are organic solvents (e.g., trimethyl-trimethylene glycol and hexylene glycol), foam stabilizers (e.g., lauryl alcohol), and corrosion inhibitors.\n\n\nClass A foams were developed in mid-1980s for fighting wildfires. Class A foams lower the surface tension of the water, which assists in the wetting and saturation of Class A fuels with water. This aids fire suppression and can prevent reignition. Favorable experiences led to its acceptance for fighting other types of class A fires, including structure fires.\n\nClass B foams are designed for class B fires—flammable liquids. The use of class A foam on a class B fire may yield unexpected results, as class A foams are not designed to contain the explosive vapors produced by flammable liquids. Class B foams have two major subtypes.\n\nSynthetic foams are based on synthetic surfactants. They provide better flow and spreading over the surface of hydrocarbon-based liquids, for faster knockdown of flames. They have limited post-fire security and are toxic groundwater contaminants.\n\nProtein foams contain natural proteins as the foaming agents. Unlike synthetic foams, protein foams are bio-degradable. They flow and spread slower, but provide a foam blanket that is more heat-resistant and more durable.\n\nProtein foams include regular protein foam (P), fluoroprotein foam (FP), film-forming fluoroprotein (FFFP), alcohol-resistant fluoroprotein foam (AR-FP), and alcohol-resistant film-forming fluoroprotein (AR-FFFP).\n\nProtein foam from non-animal sources is preferred, because of the possible threats of biological contaminants such as prions.\n\nEvery type of foam has its application. High-expansion foams are used when an enclosed space, such as a basement or hangar, must be quickly filled. Low-expansion foams are used on burning spills. AFFF is best for spills of jet fuels, FFFP is better for cases where the burning fuel can form deeper pools, and AR-AFFF is suitable for burning alcohols. The most flexibility is achieved by AR-AFFF or AR-FFFP. AR-AFFF must be used in areas where gasolines are blended with oxygenates, since the alcohols prevent the formation of the film between the FFFP foam and the gasoline, breaking down the foam, rendering the FFFP foam virtually useless.\n\nWater has long been a universal agent for suppressing fires, but is not best in all cases. For example, water is typically ineffective on oil fires, and can be dangerous. Fire-fighting foams were a development for extinguishing oil fires.\n\nIn 1902, a method of extinguishing flammable liquid fires by blanketing them with foam was introduced by Russian engineer and chemist Aleksandr Loran. Loran was a teacher in a school in Baku, the center of the Russian oil industry at that time. Impressed by large, difficult-to-extinguish oil fires that he had seen there, Loran tried to find a liquid substance that could deal effectively with them. He invented fire-fighting foam, which was successfully tested in experiments in 1902 and 1903. In 1904 Loran patented his invention, and developed the first foam extinguisher the same year.\n\nThe original foam was a mixture of two powders and water produced in a foam generator. It was called chemical foam because of the chemical action to create it. In general, the powders used were sodium bicarbonate and aluminium sulfate, with small amounts of saponin or liquorice added to stabilise the bubbles. Hand-held foam extinguishers used the same two chemicals in solution. To actuate the extinguisher, a seal was broken and the unit inverted, allowing the liquids to mix and react. Chemical foam is a stable solution of small bubbles containing carbon dioxide with lower density than oil or water, and exhibits persistence for covering flat surfaces. Because it is lighter than the burning liquid, it flows freely over the liquid surface and extinguishes the fire by a smothering (removal/prevention of oxygen) action. Chemical foam is considered obsolete today because of the many containers of powder required, even for small fires.\n\nIn the 1940s, Percy Lavon Julian developed an improved type of foam called Aerofoam. Using mechanical action, a liquid protein-based concentrate, made from soy protein, was mixed with water in either a proportioner or an aerating nozzle to form air bubbles with the free-flowing action. Its expansion ratio and ease of handling made it popular. Protein foam is easily contaminated by some flammable liquids, so care should be used so that the foam is applied only above the burning liquid. Protein foam has slow knockdown characteristics, but it is economical for post-fire security.\n\nIn the early 1950s, high-expansion foam was conceived by Herbert Eisner in England at the Safety in Mines Research Establishment (now the Health & Safety Laboratory) to fight coal mine fires. Will B. Jamison, a Pennsylvania Mining Engineer, read about the proposed foam in 1952, requested more information about the idea. He proceeded to work with the US Bureau of Mines on the idea, testing 400 formulas until a suitable compound was found. In 1964, Walter Kidde & Company (now Kidde) bought the patents for high expansion foam.\n\nIn the 1960s, National Foam, Inc. developed fluoroprotein foam. Its active agent is a fluorinated surfactant that provides an oil-rejecting property to prevent contamination. In general, it is better than protein foam because its longer blanket life provides better safety when entry is required for rescue. Fluoroprotein foam has fast knockdown characteristics and it can also be used together with dry chemicals that destroy protein foam.\n\nIn the mid-1960s, the US Navy developed aqueous film-forming foam (AFFF). This synthetic foam has a low viscosity and spreads rapidly across the surface of most hydrocarbon fuels. A water film forms beneath the foam, which cools the liquid fuel, stopping the formation of flammable vapors. This provides dramatic fire knockdown, an important factor in crash rescue fire fighting.\n\nIn the early 1970s, National Foam, Inc. invented Alcohol-Resistant AFFF technology. AR-AFFF is a synthetic foam developed for both hydrocarbon and polar-solvent materials. Polar solvents are combustible liquids that destroy conventional fire-fighting foam. These solvents extract the water contained in the foam, breaking down the foam blanket. Hence, these fuels require an alcohol- or polar-solvent-resistant foam. Alcohol-resistant foam must be bounced off of a surface and allowed to flow down and over the liquid to form its membrane, compared to standard AFFF that can be sprayed directly onto the fire.\n\nIn 1993, Pyrocool Technologies Inc. acquired the patent rights to a wetting agent with superior cooling properties that is effective on Class A, Class B, Class D as well as pressurized and 3-dimensional fires involving both hydro carbon based fuels and polar solvents such as alcohol and ethanol. The wetting agent is marketed under the name of Pyrocool. Pyrocool Technologies Inc. was awarded the 1998 Presidential Green Chemistry Award by the USEPA. Carol Browner, the USEPA Administrator in 1998, described Pyrocool as the “Technology for the Third Millennium: The Development and Commercial Introduction of an Environmentally Responsible Fire Extinguishment and Cooling Agent”.\n\nIn 2010, Orchidee International of France developed the first FFHPF, the highest performing fluorine-free foam. The foam has achieved a 97% degradability rating and is currently marketed by Orchidee International under the brand name \"BluFoam\". The foam is used at 3% both on hydrocarbon and polar solvent fires.\n\nStudies have shown that PFOS is a persistent, bioaccumulative, and toxic pollutant. It was added to Annex B of the Stockholm Convention on Persistent Organic Pollutants in May 2009. Regulations in the United States, Canada, European Union, Australia, and Japan have banned the new production of PFOS-based products, including fire fighting\nfoams. 3M phased out production of PFOS in 2002 due to toxicity concerns.\n\nOne study, published in 2015, found that firefighters were more likely to have fluorinated surfactants in their blood stream. In 2016, the United States Air Force paid $4.3 million for a water treatment system for residents downstream of Peterson Air Force Base in Colorado.\n\nIn the United States, discharges of AFFF to surface waters are regulated by the U.S. Environmental Protection Agency (EPA) and Department of Defense, pursuant to the Clean Water Act.\n\nIn Australia, in 2015 a public safety announcement was issued by the New South Wales Environment Protection Authority following a water source contamination near RAAF Base Williamtown. Surface water, ground water and fish were reported to contain chemicals from firefighting foams that had been released by the local Royal Australia Air Force base prior to training protocol changes in 2008. The residents of the area were advised to not consume any bore water, in addition to eggs and seafood from fauna exposed to the contaminated water. The discovery led to the banning of all forms of fishing in the waters of Fullerton Cove until the beginning of October 2016.\n\nIn December 2017, New Zealand's Minister for the Environment announced that higher than acceptable levels of PFOS and PFOA were found in groundwater at two Royal New Zealand Air Force bases, thought to be from historic use of firefighting foam containing the substances. Residents residing near the airbases were told to drink bottled water until more extensive testing could be carried out.\n\n\n"}
{"id": "22090661", "url": "https://en.wikipedia.org/wiki?curid=22090661", "title": "Gas Technology Institute", "text": "Gas Technology Institute\n\nThe Gas Technology Institute is an American non-profit research and development organization which develops, demonstrates, and licenses new energy technologies for private and public clients, with a particular focus on the natural gas industry. GTI is located in Des Plaines, Illinois.\n\nThe Institute of Gas Technology was founded in 1941 at the Illinois Institute of Technology (IIT).\n\nThe Gas Research Institute was founded in 1976 by the federal government. It was funded by a tax on interstate shipments of natural gas\n\nIn 2000, the tax that funded the Gas Research Institute was phased out. The Institute of Gas Technology and the Gas Research Institute combined to form the Gas Technology Institute.\n\nIn 2006, the IIT building that formerly housed the institute became home to Shimer College, which took up residence there after moving from Waukegan.\n\n"}
{"id": "24536794", "url": "https://en.wikipedia.org/wiki?curid=24536794", "title": "Gell-Mann–Okubo mass formula", "text": "Gell-Mann–Okubo mass formula\n\nIn physics, the Gell-Mann–Okubo mass formula provides a sum rule for the masses of hadrons within a specific multiplet, determined by their isospin (\"I\") and strangeness (or alternatively, hypercharge)\nwhere \"a\", \"a\", and \"a\" are free parameters.\n\nThe rule was first formulated by Murray Gell-Mann in 1961 and independently proposed by Susumu Okubo in 1962. Isospin and hypercharge are generated by SU(3), which can be represented by eight hermitian and traceless matrices corresponding to the \"components\" of isospin and hypercharge. Six of the matrices correspond to flavor change, and the final two correspond to the third-component of isospin projection, and hypercharge.\n\nThe mass formula was obtained by considering the representations of the Lie algebra su(3). In particular, the meson octet corresponds to the root system of the adjoint representation. However, the simplest, lowest-dimensional representation of su(3) is the fundamental representation, which is three-dimensional, and is now understood to describe the approximate flavor symmetry of the three quarks \"u\", \"d\", and \"s\". Thus, the discovery of not only an su(3) symmetry, but also of this workable formula for the mass spectrum was one of the earliest indicators for the existence of quarks.\n\nThe formula is underlain by the \"octet enhancement hypothesis\", which ascribes dominance of SU(3) breaking to the hypercharge generator of SU(3), \nformula_2,\nand, in modern terms, the relatively higher mass of the strange quark. An elegant abstract derivation of it is available in Ch. 1.3.5 and 1.4 of S. Coleman's text.\n\nThis formula is \"phenomenological\", describing an approximate relation between meson and baryon masses, and has been superseded as theoretical work in quantum chromodynamics advances, notably chiral perturbation theory.\n\nUsing the values of relevant \"I\" and \"S\" for baryons, the Gell-Mann–Okubo formula can be rewritten for the baryon octet,\nwhere \"N\", Λ, Σ, and Ξ represent the average mass of corresponding baryons. Using the current mass of baryons, this yields:\nand\nmeaning that the Gell-Mann–Okubo formula reproduces the mass of octet baryons within ~0.5% of measured values.\n\nFor the baryon decuplet, the Gell-Mann–Okubo formula can be rewritten as the \"equal-spacing\" rule\nwhere Δ, Σ, Ξ, and Ω represent the average mass of corresponding baryons.\n\nThe baryon decuplet formula famously allowed Gell-Mann to predict the mass of the then undiscovered Ω.\n\nThe same mass relation can be found for the meson octet,\nUsing the current mass of mesons, this yields\nand\n\nBecause of this large discrepancy, several people attempted to find a way to understand the failure of the GMO formula in mesons, when it worked so well in baryons. In particular, people noticed that using the square of the average masses yielded much better results:\nThis now yields\nand\nwhich fall within 5% of each other.\n\nFor a while, the GMO formula involving the square of masses was simply an empirical relationship; but later a justification for using the square of masses was found in the context of chiral perturbation theory, just for pseudoscalar mesons, since these are the pseudogoldstone bosons of dynamically broken chiral symmetry, and, as such, obey Dashen's mass formula. Other, mesons, such as vector ones, need no squaring for the GMO formula to work.\n\n\nThe following book contains most (if not all) historical papers on the Eightfold Way and related topics, including the Gell-Mann–Okubo mass formula.\n"}
{"id": "1483799", "url": "https://en.wikipedia.org/wiki?curid=1483799", "title": "Geometrical frustration", "text": "Geometrical frustration\n\nIn condensed matter physics, the term geometrical frustration (or in short: frustration) refers to a phenomenon, where atoms tend to stick to non-trivial positions or where, on a regular crystal lattice, conflicting inter-atomic forces (each one favoring rather simple, but different structures) lead to quite complex structures. As a consequence of the frustration in the geometry or in the forces, a plenitude of distinct ground states may result at zero temperature, and usual thermal ordering may be suppressed at higher temperatures. Much studied examples are amorphous materials, glasses, or dilute magnets.\n\nThe term \"frustration\", in the context of magnetic systems, has been introduced by Gerard Toulouse (1977). Indeed, frustrated magnetic systems had been studied even before. Early work includes a study of the Ising model on a triangular lattice with nearest-neighbor spins coupled antiferromagnetically, by G. H. Wannier, published in 1950. Related features occur in magnets with \"competing interactions\", where both ferromagnetic as well as antiferromagnetic couplings between pairs of spins or magnetic moments are present, with the type of interaction depending on the separation distance of the spins. In that case commensurability, such as helical spin arrangements may result, as had been discussed originally, especially, by A. Yoshimori, T. A. Kaplan, R. J. Elliott, and others, starting in 1959, to describe experimental findings on rare-earth metals. A renewed interest in such spin systems with frustrated or competing interactions arose about two decades later, beginning in the 1970s, in the context of spin glasses and spatially modulated magnetic superstructures. In spin glasses, frustration is augmented by stochastic disorder in the interactions, as may occur, experimentally, in non-stoichiometric magnetic alloys. Carefully analyzed spin models with frustration include the Sherrington-Kirkpatrick model, describing spin glasses, and the ANNNI model, describing commensurability magnetic superstructures.\n\nGeometrical frustration is an important feature in magnetism, where it stems from the relative arrangement of spins. A simple 2D example is shown in Figure 1. Three magnetic ions reside on the corners of a triangle with antiferromagnetic interactions between them; the energy is minimized when each spin is aligned opposite to neighbors. Once the first two spins align antiparallel, the third one is \"frustrated\" because its two possible orientations, up and down, give the same energy. The third spin cannot simultaneously minimize its interactions with both of the other two. Since this effect occurs for each spin, the ground state is sixfold degenerate. Only the two states where all spins are up or down have more energy.\n\nSimilarly in three dimensions, four spins arranged in a tetrahedron (Figure 2) may experience geometric frustration. If there is an antiferromagnetic interaction between spins, then it is not possible to arrange the spins so that all interactions between spins are antiparallel. There are six nearest-neighbor interactions, four of which are antiparallel and thus favourable, but two of which (between 1 and 2, and between 3 and 4) are unfavourable. It is impossible to have all interactions favourable, and the system is frustrated.\n\nGeometrical frustration is also possible if the spins are arranged in a non-collinear way. If we consider a tetrahedron with a spin on each vertex pointing along the \"easy axis\" (that is, directly towards or away from the centre of the tetrahedron), then it is possible to arrange the four spins so that there is no net spin (Figure 3). This is exactly equivalent to having an antiferromagnetic interaction between each pair of spins, so in this case there is no geometrical frustration. With these axes, geometric frustration arises if there is a ferromagnetic interaction between neighbours, where energy is minimized by parallel spins. The best possible arrangement is shown in Figure 4, with two spins pointing towards the centre and two pointing away. The net magnetic moment points upwards, maximising ferromagnetic interactions in this direction, but left and right vectors cancel out (i.e. are antiferromagnetically aligned), as do forwards and backwards. There are three different equivalent arrangements with two spins out and two in, so the ground state is three-fold degenerate.\n\nThe mathematical definition is simple (and analogous to the so-called Wilson loop in Quantum chromodynamics): One considers for example expressions (\"total energies\" or \"Hamiltonians\") of the form\n\nwhere \"G\" is the graph considered, whereas the quantities are the so-called \"exchange energies\" between nearest-neighbours, which (in the energy units considered) assume the values ±1 (mathematically, this is a signed graph), while the are inner products of scalar or vectorial spins or pseudo-spins. If the graph \"G\" has quadratic or triangular faces \"P\", the so-called \"plaquette variables\" \"P\", \"loop-products\" of the following kind, appear:\n\nwhich are also called \"frustration products\". One has to perform a sum over these products, summed over all plaquettes. The result for a single plaquette is either +1 or −1. In the last-mentioned case the plaquette is \"geometrically frustrated\".\n\nIt can be shown that the result has a simple gauge invariance: it does \"not\" change – nor do other measurable quantities, e.g. the \"total energy\" formula_4 – even if locally the exchange integrals and the spins are simultaneously modified as follows:\nHere the numbers \"ε\" and \"ε\" are arbitrary signs, i.e. +1 or −1, so that the modified structure may look totally random.\n\nAlthough most previous and current research on frustration focuses on spin systems, the phenomenon was first studied in ordinary ice. In 1936 Giauque and Stout published \"The Entropy of Water and the Third Law of Thermodynamics. Heat Capacity of Ice from 15 K to 273 K\", reporting calorimeter measurements on water through the freezing and vaporization transitions up to the high temperature gas phase. The entropy was calculated by integrating the heat capacity and adding the latent heat contributions; the low temperature measurements were extrapolated to zero, using Debye’s then recently derived formula. The resulting entropy, \"S\" = 44.28 cal/(K·mol) = 185.3 J/(mol·K) was compared to the theoretical result from statistical mechanics of an ideal gas, \"S\" = 45.10 cal/(K·mol) = 188.7 J/(mol·K). The two values differ by \"S\" = 0.82 ± 0.05 cal/(K·mol) = 3.4 J/(mol·K). This result was then explained by Linus Pauling to an excellent approximation, who showed that ice possesses a finite entropy (estimated as 0.81 cal/(K·mol) or 3.4 J/(mol·K)) at zero temperature due to the configurational disorder intrinsic to the protons in ice.\n\nIn the hexagonal or cubic ice phase the oxygen ions form a tetrahedral structure with an O–O bond length 2.76 Å (276 pm), while the O–H bond length measures only 0.96 Å (96 pm). Every oxygen (white) ion is surrounded by four hydrogen ions (black) and each hydrogen ion is surrounded by 2 oxygen ions, as shown in Figure 5. Maintaining the internal HO molecule structure, the minimum energy position of a proton is not half-way between two adjacent oxygen ions. There are two equivalent positions a hydrogen may occupy on the line of the O–O bond, a far and a near position. Thus a rule leads to the frustration of positions of the proton for a ground state configuration: for each oxygen two of the neighboring protons must reside in the far position and two of them in the near position, so-called ‘ice rules’. Pauling proposed that the open tetrahedral structure of ice affords many equivalent states satisfying the ice rules.\n\nPauling went on to compute the configurational entropy in the following way: consider one mole of ice, consisting of \"N\" O and 2\"N\" protons. Each O–O bond has two positions for a proton, leading to 2 possible configurations. However, among the 16 possible configurations associated with each oxygen, only 6 are energetically favorable, maintaining the HO molecule constraint. Then an upper bound of the numbers that the ground state can take is estimated as \"Ω\" < 2(). Correspondingly the configurational entropy \"S\" = \"k\"ln(\"Ω\") = \"Nk\"ln() = 0.81 cal/(K·mol) = 3.4 J/(mol·K) is in amazing agreement with the missing entropy measured by Giauque and Stout.\n\nAlthough Pauling’s calculation neglected both the global constraint on the number of protons and the local constraint arising from closed loops on the Wurtzite lattice, the estimate was subsequently shown to be of excellent accuracy.\n\nA mathematically analogous situation to the degeneracy in water ice is found in the spin ices. A common spin ice structure is shown in Figure 6 in the cubic pyrochlore structure with one magnetic atom or ion residing on each of the four corners. Due to the strong crystal field in the material, each of the magnetic ions can be represented by an Ising ground state doublet with a large moment. This suggests a picture of Ising spins residing on the corner-sharing tetrahedral lattice with spins fixed along the local quantization axis, the <111> cubic axes, which coincide with the lines connecting each tetrahedral vertex to the center. Every tetrahedral cell must have two spins pointing in and two pointing out in order to minimize the energy. Currently the spin ice model has been approximately realized by real materials, most notably the rare earth pyrochlores HoTiO, DyTiO, and HoSnO. These materials all show nonzero residual entropy at low temperature.\n\nThe spin ice model is only one subdivision of frustrated systems. The word frustration was initially introduced to describe a system’s inability to simultaneously minimize the competing interaction energy between its components. In general frustration is caused either by competing interactions due to site disorder (see also the \"Villain model\" or by lattice structure such as in the triangular, face-centered cubic (fcc), hexagonal-close-packed, tetrahedron, pyrochlore and kagome lattices with antiferromagnetic interaction. So frustration is divided into two categories: the first corresponds to the spin glass, which has both disorder in structure and frustration in spin; the second is the geometrical frustration with an ordered lattice structure and frustration of spin. The frustration of a spin glass is understood within the framework of the RKKY model, in which the interaction property, either ferromagnetic or anti-ferromagnetic, is dependent on the distance of the two magnetic ions. Due to the lattice disorder in the spin glass, one spin of interest and its nearest neighbors could be at different distances and have a different interaction property, which thus leads to different preferred alignment of the spin.\n\nWith the help of lithography techniques, it is possible to fabricate sub-micrometer size magnetic islands whose geometric arrangement reproduces the frustration found in naturally occurring spin ice materials. Recently R. F. Wang et al. reported the discovery of an artificial geometrically frustrated magnet composed of arrays of lithographically fabricated single-domain ferromagnetic islands. These islands are manually arranged to create a two-dimensional analog to spin ice. The magnetic moments of the ordered ‘spin’ islands were imaged with magnetic force microscopy (MFM) and then the local accommodation of frustration was thoroughly studied. In their previous work on a square lattice of frustrated magnets, they observed both ice-like short-range correlations and the absence of long-range correlations, just like in the spin ice at low temperature. These results solidify the uncharted ground on which the real physics of frustration can be visualized and modeled by these artificial geometrically frustrated magnets, and inspires further research activity.\n\nThese artificially frustrated ferromagnets can exhibit unique magnetic properties when studying their global response to an external field using Magneto-Optical Kerr Effect. In particular, a non-monotonic angular dependence of the square lattice coercivity is found to be related to disorder in the artificial spin ice system.\n\nAnother type of geometrical frustration arises from the propagation of a local order. A main question that a condensed matter physicist faces is to explain the stability of a solid.\n\nIt is sometimes possible to establish some local rules, of chemical nature, which lead to low energy configurations and therefore govern structural and chemical order. This is not generally the case and often the local order defined by local interactions cannot propagate freely, leading to geometric frustration. A common feature of all these systems is that, even with simple local rules, they present a large set of, often complex, structural realizations. Geometric frustration plays a role in fields of condensed matter, ranging from clusters and amorphous solids to complex fluids.\n\nThe general method of approach to resolve these complications follows two steps. First, the constraint of perfect space-filling is relaxed by allowing for space curvature. An ideal, unfrustrated, structure is defined in this curved space. Then, specific distortions are applied to this ideal template in order to embed it into three dimensional Euclidean space. The final structure is a mixture of ordered regions, where the local order is similar to that of the template, and defects arising from the embedding. Among the possible defects, disclinations play an important role.\nTwo-dimensional examples are helpful in order to get some understanding about the origin of the competition between local rules and geometry in the large. Consider first an arrangement of identical discs (a model for a hypothetical two-dimensional metal) on a plane; we suppose that the interaction between discs is isotropic and locally tends to arrange the disks in the densest way as possible. The best arrangement for three disks is trivially an equilateral triangle with the disk centers located at the triangle vertices. The study of the long range structure can therefore be reduced to that of plane tilings with equilateral triangles. A well known solution is provided by the triangular tiling with a total compatibility between the local and global rules: the system is said to be \"unfrustrated\".\n\nBut now, the interaction energy is supposed to be at a minimum when atoms sit on the vertices of a regular pentagon. Trying to propagate in the long range a packing of these pentagons sharing edges (atomic bonds) and vertices (atoms) is impossible. This is due to the impossibility of tiling a plane with regular pentagons, simply because the pentagon vertex angle does not divide 2. Three such pentagons can easily fit at a common vertex, but a gap remains between two edges. It is this kind of discrepancy which is called \"geometric frustration\". There is one way to overcome this difficulty. Let the surface to be tiled be free of any presupposed topology, and let us build the tiling with a strict application of the local interaction rule. In this simple example, we observe that the surface inherits the topology of a sphere and so receives a curvature. The final structure, here a pentagonal dodecahedron, allows for a perfect propagation of the pentagonal order. It is called an \"ideal\" (defect-free) model for the considered structure.\n\nThe stability of metals is a longstanding question of solid state physics, which can only be understood in the quantum mechanical framework by properly taking into account the interaction between the positively charged ions and the valence and conduction electrons. It is nevertheless possible to use a very simplified picture of metallic bonding and only keeps an isotropic type of interactions, leading to structures which can be represented as densely packed spheres. And indeed the crystalline simple metal structures are often either close packed face centered cubic (fcc) or hexagonal close packing (hcp) lattices. Up to some extent amorphous metals and quasicrystals can also be modeled by close packing of spheres. The local atomic order is well modeled by a close packing of tetrahedra, leading to an imperfect icosahedral order.\n\nA regular tetrahedron is the densest configuration for the packing of four equal spheres. The dense random packing of hard spheres problem can thus be mapped on the tetrahedral packing problem. It is a practical exercise to try to pack table tennis balls in order to form only tetrahedral configurations. One starts with four balls arranged as a perfect tetrahedron, and try to add new spheres, while forming new tetrahedra. The next solution, with five balls, is trivially two tetrahedra sharing a common face; note that already with this solution, the fcc structure, which contains individual tetrahedral holes, does not show such a configuration (the tetrahedra share edges, not faces). With six balls, three regular tetrahedra are built, and the cluster is incompatible with all compact crystalline structures (fcc and hcp). Adding a seventh sphere gives a new cluster consisting in two \"axial\" balls touching each other and five others touching the latter two balls, the outer shape being an almost regular pentagonal bi-pyramid. However, we are facing now a real packing problem, analogous to the one encountered above with the pentagonal tiling in two dimensions. The dihedral angle of a tetrahedron is not commensurable with 2; consequently, a hole remains between two faces of neighboring tetrahedra. As a consequence, a perfect tiling of the Euclidean space R is impossible with regular tetrahedra. The frustration has a topological character: it is impossible to fill Euclidean space with tetrahedra, even severely distorted, if we impose that a constant number of tetrahedra (here five) share a common edge.\n\nThe next step is crucial: the search for an unfrustrated structure by allowing for curvature in the space, in order for the local configurations to propagate identically and without defects throughout the whole space.\n\nTwenty tetrahedra pack with a common vertex in such a way that the twelve outer vertices form an irregular icosahedron. Indeed the icosahedron edge length \"l\" is slightly longer than the circumsphere radius \"r\" (\"l\" ≈ 1.05\"r\"). There is a solution with regular icosahedra if the space is not Euclidean, but spherical. It is the polytope {3,3,5}, using the Schläfli notation, also known as the 600-cell.\n\nThere are one hundred and twenty vertices which all belong to the hypersphere \"S\" with radius equal to the golden ratio (\"φ\" = ) if the edges are of unit length. The six hundred cells are regular tetrahedra grouped by five around a common edge and by twenty around a common vertex. This structure is called a polytope (see Coxeter) which is the general name in higher dimension in the series containing polygons and polyhedra. Even if this structure is embedded in four dimensions, it has been considered as a three dimensional (curved) manifold. This point is conceptually important for the following reason. The ideal models that have been introduced in the curved Space are three dimensional curved templates. They look locally as three dimensional Euclidean models. So, the {3,3,5} polytope, which is a tiling by tetrahedra, provides a very dense atomic structure if atoms are located on its vertices. It is therefore naturally used as a template for amorphous metals, but one should not forget that it is at the price of successive idealizations.\n\n"}
{"id": "31979588", "url": "https://en.wikipedia.org/wiki?curid=31979588", "title": "Ghana National Petroleum Corporation", "text": "Ghana National Petroleum Corporation\n\nThe Ghana National Petroleum Corporation (GNPC) is the state agency responsible for the exploration, licensing, and distribution of petroleum-related activities in Ghana.\n\nThe corporation was established in 1983 to replace the Petroleum Department which was an agency under the Ministry of Fuel and Power. The department was responsible for the importation of crude oil and petroleum products for the Ghanaian economy. The mandate for oil exploration was held to be the Technical Directorate of the Ministry of Fuel and Power and the Geological Survey Department. The corporation was created to promote the Government of Ghana's objective of supplying adequate and reliable petroleum for the country and the discovery of crude oil in the country's territories.\n\nThe corporation was established as a state-owned company with the statutory backing of PNDC Laws 64 and 84. The laws mandated the corporation to engage in exploration, production and disposal of petroleum products. The laws also established the legal structure that informed the corporation in contractual agreements between the Government of Ghana and private oil exploration companies. In 1987, the Petroleum Income Tax Law were added to the corporation's mandate to permit it to tax various petroleum products for consumption. The law was the PNDC Law 188.\n\nThe corporation has amongst it functions to promote petroleum exploration activities, to appraise existing petroleum discoveries, to ensure that Ghana benefit the most from the development of the country's petroleum resources. The corporation promotes the training of Ghanaians in petroleum related activities and ensure environmental protection in all petroleum related activities.\n\nThe corporation announced in 2007 that the Petroleum Agreement signed in 2004 between Ghana represented by the Ministry of Energy and the Corporation, on the one hand, and Kosmos Energy HC and its partners had yielded in the discovery of oil in commercial quantities. Production of crude oil begun in December 2010. The country's oil fields are known as Jubilee fields and is situated at Cape Three Points in the Western region of the country. The corporation has controlling stake in all the oil wells that produce crude oil. In April 2011, GNPC confirmed that it had an initial 10% interest in a new oil discovery at the Paradise prospect offshore Ghana. The discovery was announced in April 2011 by Hess Corporation, a New York-based oil company. In June 2011, the corporation lifted 135,675 metric tonnes of Jubilee crude oil from the FPSO Kwame Nkrumah. This represented crude oil on behalf of the Ghana Group, comprising the Government of Ghana (GoG) and GNPC. It represented the first lifting of crude oil from the Jubilee fields. The barrels lifted comprised accumulated government royalties of 37,557 metric tonnes and accumulated GNPC’s 13.75 per cent participating interest entitlement of 98,119 metric tonnes.\n\n"}
{"id": "20999825", "url": "https://en.wikipedia.org/wiki?curid=20999825", "title": "Hama Wing", "text": "Hama Wing\n\nThe wind turbine is a Vestas Wind Systems A/S Model V80-2.0MW with a rotor diameter of and a tall tower. At the highest point of rotation the turbine is tall, higher than the Cosmo Clock 21 Ferris wheel in Yokohama. The turbine has an output of 1,980 kW (enough power for 860 homes), and its purpose is to publicize environmentally friendly alternative energy sources while providing electricity to the Minato Mirai 21 district.\n\n"}
{"id": "13256", "url": "https://en.wikipedia.org/wiki?curid=13256", "title": "Helium", "text": "Helium\n\nHelium (from ) is a chemical element with symbol He and atomic number 2. It is a colorless, odorless, tasteless, non-toxic, inert, monatomic gas, the first in the noble gas group in the periodic table. Its boiling point is the lowest among all the elements. After hydrogen, helium is the second lightest and second most abundant element in the observable universe, being present at about 24% of the total elemental mass, which is more than 12 times the mass of all the heavier elements combined. Its abundance is similar to this figure in the Sun and in Jupiter. This is due to the very high nuclear binding energy (per nucleon) of helium-4 with respect to the next three elements after helium. This helium-4 binding energy also accounts for why it is a product of both nuclear fusion and radioactive decay. Most helium in the universe is helium-4, the vast majority of which was formed during the Big Bang. Large amounts of new helium are being created by nuclear fusion of hydrogen in stars.\n\nHelium is named for the Greek Titan of the Sun, Helios. It was first detected as an unknown yellow spectral line signature in sunlight during a solar eclipse in 1868 by Georges Rayet, Captain C. T. Haig, Norman R. Pogson, and Lieutenant John Herschel, and was subsequently confirmed by French astronomer Jules Janssen. Janssen is often jointly credited with detecting the element along with Norman Lockyer. Janssen recorded the helium spectral line during the solar eclipse of 1868 while Lockyer observed it from Britain. Lockyer was the first to propose that the line was due to a new element, which he named. The formal discovery of the element was made in 1895 by two Swedish chemists, Per Teodor Cleve and Nils Abraham Langlet, who found helium emanating from the uranium ore cleveite. In 1903, large reserves of helium were found in natural gas fields in parts of the United States, which is by far the largest supplier of the gas today.\n\nLiquid helium is used in cryogenics (its largest single use, absorbing about a quarter of production), particularly in the cooling of superconducting magnets, with the main commercial application being in MRI scanners. Helium's other industrial uses—as a pressurizing and purge gas, as a protective atmosphere for arc welding and in processes such as growing crystals to make silicon wafers—account for half of the gas produced. A well-known but minor use is as a lifting gas in balloons and airships. As with any gas whose density differs from that of air, inhaling a small volume of helium temporarily changes the timbre and quality of the human voice. In scientific research, the behavior of the two fluid phases of helium-4 (helium I and helium II) is important to researchers studying quantum mechanics (in particular the property of superfluidity) and to those looking at the phenomena, such as superconductivity, produced in matter near absolute zero.\n\nOn Earth it is relatively rare—5.2 ppm by volume in the atmosphere. Most terrestrial helium present today is created by the natural radioactive decay of heavy radioactive elements (thorium and uranium, although there are other examples), as the alpha particles emitted by such decays consist of helium-4 nuclei. This radiogenic helium is trapped with natural gas in concentrations as great as 7% by volume, from which it is extracted commercially by a low-temperature separation process called fractional distillation. Previously, terrestrial helium—a non-renewable resource, because once released into the atmosphere it readily escapes into space—was thought to be in increasingly short supply. However, recent studies suggest that helium produced deep in the earth by radioactive decay can collect in natural gas reserves in larger than expected quantities, in some cases having been released by volcanic activity.\n\nThe first evidence of helium was observed on August 18, 1868, as a bright yellow line with a wavelength of 587.49 nanometers in the spectrum of the chromosphere of the Sun. The line was detected by French astronomer Jules Janssen during a total solar eclipse in Guntur, India. This line was initially assumed to be sodium. On October 20 of the same year, English astronomer Norman Lockyer observed a yellow line in the solar spectrum, which he named the D because it was near the known D and D Fraunhofer line lines of sodium. He concluded that it was caused by an element in the Sun unknown on Earth. Lockyer and English chemist Edward Frankland named the element with the Greek word for the Sun, ἥλιος (\"helios\").\n\nIn 1881, Italian physicist Luigi Palmieri detected helium on Earth for the first time through its D spectral line, when he analyzed a material that had been sublimated during a recent eruption of Mount Vesuvius.\n\nOn March 26, 1895, Scottish chemist Sir William Ramsay isolated helium on Earth by treating the mineral cleveite (a variety of uraninite with at least 10% rare earth elements) with mineral acids. Ramsay was looking for argon but, after separating nitrogen and oxygen from the gas liberated by sulfuric acid, he noticed a bright yellow line that matched the D line observed in the spectrum of the Sun. These samples were identified as helium by Lockyer and British physicist William Crookes. It was independently isolated from cleveite in the same year by chemists Per Teodor Cleve and Abraham Langlet in Uppsala, Sweden, who collected enough of the gas to accurately determine its atomic weight. Helium was also isolated by the American geochemist William Francis Hillebrand prior to Ramsay's discovery when he noticed unusual spectral lines while testing a sample of the mineral uraninite. Hillebrand, however, attributed the lines to nitrogen. His letter of congratulations to Ramsay offers an interesting case of discovery and near-discovery in science.\n\nIn 1907, Ernest Rutherford and Thomas Royds demonstrated that alpha particles are helium nuclei by allowing the particles to penetrate the thin glass wall of an evacuated tube, then creating a discharge in the tube to study the spectrum of the new gas inside. In 1908, helium was first liquefied by Dutch physicist Heike Kamerlingh Onnes by cooling the gas to less than one kelvin. He tried to solidify it by further reducing the temperature but failed because helium does not solidify at atmospheric pressure. Onnes' student Willem Hendrik Keesom was eventually able to solidify 1 cm of helium in 1926 by applying additional external pressure.\n\nIn 1913, Niels Bohr published his \"trilogy\" on atomic structure that included a reconsideration of the Pickering–Fowler series as central evidence in support of his model of the atom. This series is named for Edward Charles Pickering, who in 1896 published observations of previously unknown lines in the spectrum of the star ζ Puppis (these are now known to occur with Wolf–Rayet and other hot stars). Pickering attributed the observation (lines at 4551, 5411, and 10123 Å) to a new form of hydrogen with half-integer transition levels. In 1912, Alfred Fowler managed to produce similar lines from a hydrogen-helium mixture, and supported Pickering's conclusion as to their origin. Bohr's model does not allow for half-integer transitions (nor does quantum mechanics) and Bohr concluded that Pickering and Fowler were wrong, and instead assigned these spectral lines to ionised helium, He. Fowler was initially skeptical but was ultimately convinced that Bohr was correct, and by 1915 \"spectroscopists had transferred [the Pickering–Fowler series] definitively [from hydrogen] to helium.\" Bohr's theoretical work on the Pickering series had demonstrated the need for \"a re-examination of problems that seemed already to have been solved within classical theories\" and provided important confirmation for his atomic theory.\n\nIn 1938, Russian physicist Pyotr Leonidovich Kapitsa discovered that helium-4 has almost no viscosity at temperatures near absolute zero, a phenomenon now called superfluidity. This phenomenon is related to Bose–Einstein condensation. In 1972, the same phenomenon was observed in helium-3, but at temperatures much closer to absolute zero, by American physicists Douglas D. Osheroff, David M. Lee, and Robert C. Richardson. The phenomenon in helium-3 is thought to be related to pairing of helium-3 fermions to make bosons, in analogy to Cooper pairs of electrons producing superconductivity.\n\nAfter an oil drilling operation in 1903 in Dexter, Kansas, produced a gas geyser that would not burn, Kansas state geologist Erasmus Haworth collected samples of the escaping gas and took them back to the University of Kansas at Lawrence where, with the help of chemists Hamilton Cady and David McFarland, he discovered that the gas consisted of, by volume, 72% nitrogen, 15% methane (a combustible percentage only with sufficient oxygen), 1% hydrogen, and 12% an unidentifiable gas. With further analysis, Cady and McFarland discovered that 1.84% of the gas sample was helium. This showed that despite its overall rarity on Earth, helium was concentrated in large quantities under the American Great Plains, available for extraction as a byproduct of natural gas.\n\nThis enabled the United States to become the world's leading supplier of helium. Following a suggestion by Sir Richard Threlfall, the United States Navy sponsored three small experimental helium plants during World War I. The goal was to supply barrage balloons with the non-flammable, lighter-than-air gas. A total of of 92% helium was produced in the program even though less than a cubic meter of the gas had previously been obtained. Some of this gas was used in the world's first helium-filled airship, the U.S. Navy's C-7, which flew its maiden voyage from Hampton Roads, Virginia, to Bolling Field in Washington, D.C., on December 1, 1921, nearly two years before the Navy's first \"rigid\" helium-filled airship, the Naval Aircraft Factory-built \"USS Shenandoah\", flew in September 1923.\n\nAlthough the extraction process, using low-temperature gas liquefaction, was not developed in time to be significant during World War I, production continued. Helium was primarily used as a lifting gas in lighter-than-air craft. During World War II, the demand increased for helium for lifting gas and for shielded arc welding. The helium mass spectrometer was also vital in the atomic bomb Manhattan Project.\n\nThe government of the United States set up the National Helium Reserve in 1925 at Amarillo, Texas, with the goal of supplying military airships in time of war and commercial airships in peacetime. Because of the Helium Act of 1925, which banned the export of scarce helium on which the US then had a production monopoly, together with the prohibitive cost of the gas, the Hindenburg, like all German Zeppelins, was forced to use hydrogen as the lift gas. The helium market after World War II was depressed but the reserve was expanded in the 1950s to ensure a supply of liquid helium as a coolant to create oxygen/hydrogen rocket fuel (among other uses) during the Space Race and Cold War. Helium use in the United States in 1965 was more than eight times the peak wartime consumption.\n\nAfter the \"Helium Acts Amendments of 1960\" (Public Law 86–777), the U.S. Bureau of Mines arranged for five private plants to recover helium from natural gas. For this \"helium conservation\" program, the Bureau built a pipeline from Bushton, Kansas, to connect those plants with the government's partially depleted Cliffside gas field near Amarillo, Texas. This helium-nitrogen mixture was injected and stored in the Cliffside gas field until needed, at which time it was further purified.\n\nBy 1995, a billion cubic meters of the gas had been collected and the reserve was US$1.4 billion in debt, prompting the Congress of the United States in 1996 to phase out the reserve. The resulting Helium Privatization Act of 1996 (Public Law 104–273) directed the United States Department of the Interior to empty the reserve, with sales starting by 2005.\n\nHelium produced between 1930 and 1945 was about 98.3% pure (2% nitrogen), which was adequate for airships. In 1945, a small amount of 99.9% helium was produced for welding use. By 1949, commercial quantities of Grade A 99.95% helium were available.\n\nFor many years, the United States produced more than 90% of commercially usable helium in the world, while extraction plants in Canada, Poland, Russia, and other nations produced the remainder. In the mid-1990s, a new plant in Arzew, Algeria, producing 17 million cubic meters (600 million cubic feet) began operation, with enough production to cover all of Europe's demand. Meanwhile, by 2000, the consumption of helium within the U.S. had risen to more than 15 million kg per year. In 2004–2006, additional plants in Ras Laffan, Qatar, and Skikda, Algeria were built. Algeria quickly became the second leading producer of helium. Through this time, both helium consumption and the costs of producing helium increased. From 2002 to 2007 helium prices doubled.\n\n, the United States National Helium Reserve accounted for 30 percent of the world's helium. The reserve was expected to run out of helium in 2018. Despite that, a proposed bill in the United States Senate would allow the reserve to continue to sell the gas. Other large reserves were in the Hugoton in Kansas, United States, and nearby gas fields of Kansas and the panhandles of Texas and Oklahoma. New helium plants were scheduled to open in 2012 in Qatar, Russia, and the US state of Wyoming, but they were not expected to ease the shortage.\n\nIn 2013, Qatar started up the world's largest helium unit, although the 2017 Qatar diplomatic crisis severely affected helium production there. 2014 was widely acknowledged to be a year of over-supply in the helium business, following years of renowned shortages. Nasdaq reported (2015) that for Air Products, an international corporation that sells gases for industrial use, helium volumes remain under economic pressure due to feedstock supply constraints.\n\nIn the perspective of quantum mechanics, helium is the second simplest atom to model, following the hydrogen atom. Helium is composed of two electrons in atomic orbitals surrounding a nucleus containing two protons and (usually) two neutrons. As in Newtonian mechanics, no system that consists of more than two particles can be solved with an exact analytical mathematical approach (see 3-body problem) and helium is no exception. Thus, numerical mathematical methods are required, even to solve the system of one nucleus and two electrons. Such computational chemistry methods have been used to create a quantum mechanical picture of helium electron binding which is accurate to within < 2% of the correct value, in a few computational steps. Such models show that each electron in helium partly screens the nucleus from the other, so that the effective nuclear charge \"Z\" which each electron sees, is about 1.69 units, not the 2 charges of a classic \"bare\" helium nucleus.\n\nThe nucleus of the helium-4 atom is identical with an alpha particle. High-energy electron-scattering experiments show its charge to decrease exponentially from a maximum at a central point, exactly as does the charge density of helium's own electron cloud. This symmetry reflects similar underlying physics: the pair of neutrons and the pair of protons in helium's nucleus obey the same quantum mechanical rules as do helium's pair of electrons (although the nuclear particles are subject to a different nuclear binding potential), so that all these fermions fully occupy 1s orbitals in pairs, none of them possessing orbital angular momentum, and each cancelling the other's intrinsic spin. Adding another of any of these particles would require angular momentum and would release substantially less energy (in fact, no nucleus with five nucleons is stable). This arrangement is thus energetically extremely stable for all these particles, and this stability accounts for many crucial facts regarding helium in nature.\n\nFor example, the stability and low energy of the electron cloud state in helium accounts for the element's chemical inertness, and also the lack of interaction of helium atoms with each other, producing the lowest melting and boiling points of all the elements.\n\nIn a similar way, the particular energetic stability of the helium-4 nucleus, produced by similar effects, accounts for the ease of helium-4 production in atomic reactions that involve either heavy-particle emission or fusion. Some stable helium-3 (2 protons and 1 neutron) is produced in fusion reactions from hydrogen, but it is a very small fraction compared to the highly favorable helium-4.\nThe unusual stability of the helium-4 nucleus is also important cosmologically: it explains the fact that in the first few minutes after the Big Bang, as the \"soup\" of free protons and neutrons which had initially been created in about 6:1 ratio cooled to the point that nuclear binding was possible, almost all first compound atomic nuclei to form were helium-4 nuclei. So tight was helium-4 binding that helium-4 production consumed nearly all of the free neutrons in a few minutes, before they could beta-decay, and also leaving few to form heavier atoms such as lithium, beryllium, or boron. Helium-4 nuclear binding per nucleon is stronger than in any of these elements (see nucleogenesis and binding energy) and thus, once helium had been formed, no energetic drive was available to make elements 3, 4 and 5. It was barely energetically favorable for helium to fuse into the next element with a lower energy per nucleon, carbon. However, due to lack of intermediate elements, this process requires three helium nuclei striking each other nearly simultaneously (see triple alpha process). There was thus no time for significant carbon to be formed in the few minutes after the Big Bang, before the early expanding universe cooled to the temperature and pressure point where helium fusion to carbon was no longer possible. This left the early universe with a very similar ratio of hydrogen/helium as is observed today (3 parts hydrogen to 1 part helium-4 by mass), with nearly all the neutrons in the universe trapped in helium-4.\n\nAll heavier elements (including those necessary for rocky planets like the Earth, and for carbon-based or other life) have thus been created since the Big Bang in stars which were hot enough to fuse helium itself. All elements other than hydrogen and helium today account for only 2% of the mass of atomic matter in the universe. Helium-4, by contrast, makes up about 23% of the universe's ordinary matter—nearly all the ordinary matter that is not hydrogen.\n\nHelium is the second least reactive noble gas after neon, and thus the second least reactive of all elements. It is chemically inert and monatomic in all standard conditions. Because of helium's relatively low molar (atomic) mass, its thermal conductivity, specific heat, and sound speed in the gas phase are all greater than any other gas except hydrogen. For these reasons and the small size of helium monatomic molecules, helium diffuses through solids at a rate three times that of air and around 65% that of hydrogen.\n\nHelium is the least water-soluble monatomic gas, and one of the least water-soluble of any gas (CF, SF, and CF have lower mole fraction solubilities: 0.3802, 0.4394, and 0.2372 x/10, respectively, versus helium's 0.70797 x/10), and helium's index of refraction is closer to unity than that of any other gas. Helium has a negative Joule–Thomson coefficient at normal ambient temperatures, meaning it heats up when allowed to freely expand. Only below its Joule–Thomson inversion temperature (of about 32 to 50 K at 1 atmosphere) does it cool upon free expansion. Once precooled below this temperature, helium can be liquefied through expansion cooling.\n\nMost extraterrestrial helium is found in a plasma state, with properties quite different from those of atomic helium. In a plasma, helium's electrons are not bound to its nucleus, resulting in very high electrical conductivity, even when the gas is only partially ionized. The charged particles are highly influenced by magnetic and electric fields. For example, in the solar wind together with ionized hydrogen, the particles interact with the Earth's magnetosphere, giving rise to Birkeland currents and the aurora.\n\nUnlike any other element, helium will remain liquid down to absolute zero at normal pressures. This is a direct effect of quantum mechanics: specifically, the zero point energy of the system is too high to allow freezing. Solid helium requires a temperature of 1–1.5 K (about −272 °C or −457 °F) at about 25 bar (2.5 MPa) of pressure. It is often hard to distinguish solid from liquid helium since the refractive index of the two phases are nearly the same. The solid has a sharp melting point and has a crystalline structure, but it is highly compressible; applying pressure in a laboratory can decrease its volume by more than 30%. With a bulk modulus of about 27 MPa it is ~100 times more compressible than water. Solid helium has a density of at 1.15 K and 66 atm; the projected density at 0 K and 25 bar (2.5 MPa) is . At higher temperatures, helium will solidify with sufficient pressure. At room temperature, this requires about 114,000 atm.\n\nBelow its boiling point of 4.22 kelvins and above the lambda point of 2.1768 kelvins, the isotope helium-4 exists in a normal colorless liquid state, called \"helium I\". Like other cryogenic liquids, helium I boils when it is heated and contracts when its temperature is lowered. Below the lambda point, however, helium does not boil, and it expands as the temperature is lowered further. \n\nHelium I has a gas-like index of refraction of 1.026 which makes its surface so hard to see that floats of Styrofoam are often used to show where the surface is. This colorless liquid has a very low viscosity and a density of 0.145–0.125 g/mL (between about 0 and 4 K), which is only one-fourth the value expected from classical physics. Quantum mechanics is needed to explain this property and thus both states of liquid helium (helium I and helium II) are called \"quantum fluids\", meaning they display atomic properties on a macroscopic scale. This may be an effect of its boiling point being so close to absolute zero, preventing random molecular motion (thermal energy) from masking the atomic properties.\n\nLiquid helium below its lambda point (called \"helium II\") exhibits very unusual characteristics. Due to its high thermal conductivity, when it boils, it does not bubble but rather evaporates directly from its surface. Helium-3 also has a superfluid phase, but only at much lower temperatures; as a result, less is known about the properties of the isotope.\nHelium II is a superfluid, a quantum mechanical state (see: macroscopic quantum phenomena) of matter with strange properties. For example, when it flows through capillaries as thin as 10 to 10 m it has no measurable viscosity. However, when measurements were done between two moving discs, a viscosity comparable to that of gaseous helium was observed. Current theory explains this using the \"two-fluid model\" for helium II. In this model, liquid helium below the lambda point is viewed as containing a proportion of helium atoms in a ground state, which are superfluid and flow with exactly zero viscosity, and a proportion of helium atoms in an excited state, which behave more like an ordinary fluid.\n\nIn the \"fountain effect\", a chamber is constructed which is connected to a reservoir of helium II by a sintered disc through which superfluid helium leaks easily but through which non-superfluid helium cannot pass. If the interior of the container is heated, the superfluid helium changes to non-superfluid helium. In order to maintain the equilibrium fraction of superfluid helium, superfluid helium leaks through and increases the pressure, causing liquid to fountain out of the container.\n\nThe thermal conductivity of helium II is greater than that of any other known substance, a million times that of helium I and several hundred times that of copper. This is because heat conduction occurs by an exceptional quantum mechanism. Most materials that conduct heat well have a valence band of free electrons which serve to transfer the heat. Helium II has no such valence band but nevertheless conducts heat well. The flow of heat is governed by equations that are similar to the wave equation used to characterize sound propagation in air. When heat is introduced, it moves at 20 meters per second at 1.8 K through helium II as waves in a phenomenon known as \"second sound\".\n\nHelium II also exhibits a creeping effect. When a surface extends past the level of helium II, the helium II moves along the surface, against the force of gravity. Helium II will escape from a vessel that is not sealed by creeping along the sides until it reaches a warmer region where it evaporates. It moves in a 30 nm-thick film regardless of surface material. This film is called a Rollin film and is named after the man who first characterized this trait, Bernard V. Rollin. As a result of this creeping behavior and helium II's ability to leak rapidly through tiny openings, it is very difficult to confine liquid helium. Unless the container is carefully constructed, the helium II will creep along the surfaces and through valves until it reaches somewhere warmer, where it will evaporate. Waves propagating across a Rollin film are governed by the same equation as gravity waves in shallow water, but rather than gravity, the restoring force is the van der Waals force. These waves are known as \"third sound\".\n\nThere are nine known isotopes of helium, but only helium-3 and helium-4 are stable. In the Earth's atmosphere, one atom is for every million that are . Unlike most elements, helium's isotopic abundance varies greatly by origin, due to the different formation processes. The most common isotope, helium-4, is produced on Earth by alpha decay of heavier radioactive elements; the alpha particles that emerge are fully ionized helium-4 nuclei. Helium-4 is an unusually stable nucleus because its nucleons are arranged into complete shells. It was also formed in enormous quantities during Big Bang nucleosynthesis.\n\nHelium-3 is present on Earth only in trace amounts. Most of it has been present since Earth's formation, though some falls to Earth trapped in cosmic dust. Trace amounts are also produced by the beta decay of tritium. Rocks from the Earth's crust have isotope ratios varying by as much as a factor of ten, and these ratios can be used to investigate the origin of rocks and the composition of the Earth's mantle. is much more abundant in stars as a product of nuclear fusion. Thus in the interstellar medium, the proportion of to is about 100 times higher than on Earth. Extraplanetary material, such as lunar and asteroid regolith, have trace amounts of helium-3 from being bombarded by solar winds. The Moon's surface contains helium-3 at concentrations on the order of 10 ppb, much higher than the approximately 5 ppt found in the Earth's atmosphere. A number of people, starting with Gerald Kulcinski in 1986, have proposed to explore the moon, mine lunar regolith, and use the helium-3 for fusion.\n\nLiquid helium-4 can be cooled to about 1 kelvin using evaporative cooling in a 1-K pot. Similar cooling of helium-3, which has a lower boiling point, can achieve about in a helium-3 refrigerator. Equal mixtures of liquid and below separate into two immiscible phases due to their dissimilarity (they follow different quantum statistics: helium-4 atoms are bosons while helium-3 atoms are fermions). Dilution refrigerators use this immiscibility to achieve temperatures of a few millikelvins.\n\nIt is possible to produce exotic helium isotopes, which rapidly decay into other substances. The shortest-lived heavy helium isotope is helium-5 with a half-life of . Helium-6 decays by emitting a beta particle and has a half-life of 0.8 second. Helium-7 also emits a beta particle as well as a gamma ray. Helium-7 and helium-8 are created in certain nuclear reactions. Helium-6 and helium-8 are known to exhibit a nuclear halo.\n\nHelium has a valence of zero and is chemically unreactive under all normal conditions. It is an electrical insulator unless ionized. As with the other noble gases, helium has metastable energy levels that allow it to remain ionized in an electrical discharge with a voltage below its ionization potential. Helium can form unstable compounds, known as excimers, with tungsten, iodine, fluorine, sulfur, and phosphorus when it is subjected to a glow discharge, to electron bombardment, or reduced to plasma by other means. The molecular compounds HeNe, HgHe, and WHe, and the molecular ions , , , and have been created this way. HeH is also stable in its ground state, but is extremely reactive—it is the strongest Brønsted acid known, and therefore can exist only in isolation, as it will protonate any molecule or counteranion it contacts. This technique has also produced the neutral molecule He, which has a large number of band systems, and HgHe, which is apparently held together only by polarization forces.\n\nVan der Waals compounds of helium can also be formed with cryogenic helium gas and atoms of some other substance, such as LiHe and He.\n\nTheoretically, other true compounds may be possible, such as helium fluorohydride (HHeF) which would be analogous to HArF, discovered in 2000. Calculations show that two new compounds containing a helium-oxygen bond could be stable. Two new molecular species, predicted using theory, CsFHeO and N(CH)FHeO, are derivatives of a metastable FHeO anion first theorized in 2005 by a group from Taiwan. If confirmed by experiment, the only remaining element with no known stable compounds would be neon.\n\nHelium atoms have been inserted into the hollow carbon cage molecules (the fullerenes) by heating under high pressure. The endohedral fullerene molecules formed are stable at high temperatures. When chemical derivatives of these fullerenes are formed, the helium stays inside. If helium-3 is used, it can be readily observed by helium nuclear magnetic resonance spectroscopy. Many fullerenes containing helium-3 have been reported. Although the helium atoms are not attached by covalent or ionic bonds, these substances have distinct properties and a definite composition, like all stoichiometric chemical compounds.\n\nUnder high pressures helium can form compounds with various other elements. Helium-nitrogen clathrate (He(N)) crystals have been grown at room temperature at pressures ca. 10 GPa in a diamond anvil cell. The insulating electride NaHe has been shown to be thermodynamically stable at pressures above 113 GPa. It has a fluorite structure.\n\nAlthough it is rare on Earth, helium is the second most abundant element in the known Universe (after hydrogen), constituting 23% of its baryonic mass. The vast majority of helium was formed by Big Bang nucleosynthesis one to three minutes after the Big Bang. As such, measurements of its abundance contribute to cosmological models. In stars, it is formed by the nuclear fusion of hydrogen in proton-proton chain reactions and the CNO cycle, part of stellar nucleosynthesis.\n\nIn the Earth's atmosphere, the concentration of helium by volume is only 5.2 parts per million. The concentration is low and fairly constant despite the continuous production of new helium because most helium in the Earth's atmosphere escapes into space by several processes. In the Earth's heterosphere, a part of the upper atmosphere, helium and other lighter gases are the most abundant elements.\n\nMost helium on Earth is a result of radioactive decay. Helium is found in large amounts in minerals of uranium and thorium, including cleveite, pitchblende, carnotite and monazite, because they emit alpha particles (helium nuclei, He) to which electrons immediately combine as soon as the particle is stopped by the rock. In this way an estimated 3000 metric tons of helium are generated per year throughout the lithosphere. In the Earth's crust, the concentration of helium is 8 parts per billion. In seawater, the concentration is only 4 parts per trillion. There are also small amounts in mineral springs, volcanic gas, and meteoric iron. Because helium is trapped in the subsurface under conditions that also trap natural gas, the greatest natural concentrations of helium on the planet are found in natural gas, from which most commercial helium is extracted. The concentration varies in a broad range from a few ppm to more than 7% in a small gas field in San Juan County, New Mexico.\n\nFor large-scale use, helium is extracted by fractional distillation from natural gas, which can contain as much as 7% helium. Since helium has a lower boiling point than any other element, low temperature and high pressure are used to liquefy nearly all the other gases (mostly nitrogen and methane). The resulting crude helium gas is purified by successive exposures to lowering temperatures, in which almost all of the remaining nitrogen and other gases are precipitated out of the gaseous mixture. Activated charcoal is used as a final purification step, usually resulting in 99.995% pure Grade-A helium. The principal impurity in Grade-A helium is neon. In a final production step, most of the helium that is produced is liquefied via a cryogenic process. This is necessary for applications requiring liquid helium and also allows helium suppliers to reduce the cost of long distance transportation, as the largest liquid helium containers have more than five times the capacity of the largest gaseous helium tube trailers.\n\nIn 2008, approximately 169 million standard cubic meters (SCM) of helium were extracted from natural gas or withdrawn from helium reserves with approximately 78% from the United States, 10% from Algeria, and most of the remainder from Russia, Poland and Qatar. By 2013, increases in helium production in Qatar (under the company RasGas managed by Air Liquide) had increased Qatar's fraction of world helium production to 25%, and made it the second largest exporter after the United States.\nAn estimated deposit of helium was found in Tanzania in 2016.\n\nIn the United States, most helium is extracted from natural gas of the Hugoton and nearby gas fields in Kansas, Oklahoma, and the Panhandle Field in Texas. Much of this gas was once sent by pipeline to the National Helium Reserve, but since 2005 this reserve is being depleted and sold off, and is expected to be largely depleted by 2021, under the October 2013 \"Responsible Helium Administration and Stewardship Act\" (H.R. 527).\n\nDiffusion of crude natural gas through special semipermeable membranes and other barriers is another method to recover and purify helium. In 1996, the U.S. had \"proven\" helium reserves, in such gas well complexes, of about 147 billion standard cubic feet (4.2 billion SCM). At rates of use at that time (72 million SCM per year in the U.S.; see pie chart below) this would have been enough helium for about 58 years of U.S. use, and less than this (perhaps 80% of the time) at world use rates, although factors in saving and processing impact effective reserve numbers.\n\nHelium must be extracted from natural gas because it is present in air at only a fraction of that of neon, yet the demand for it is far higher. It is estimated that if all neon production were retooled to save helium, that 0.1% of the world's helium demands would be satisfied. Similarly, only 1% of the world's helium demands could be satisfied by re-tooling all air distillation plants. Helium can be synthesized by bombardment of lithium or boron with high-velocity protons, or by bombardment of lithium with deuterons, but these processes are a completely uneconomical method of production.\n\nHelium is commercially available in either liquid or gaseous form. As a liquid, it can be supplied in small insulated containers called dewars which hold as much as 1,000 liters of helium, or in large ISO containers which have nominal capacities as large as 42 m (around 11,000 U.S. gallons). In gaseous form, small quantities of helium are supplied in high-pressure cylinders holding as much as 8 m (approx. 282 standard cubic feet), while large quantities of high-pressure gas are supplied in tube trailers which have capacities of as much as 4,860 m (approx. 172,000 standard cubic feet).\n\nAccording to helium conservationists like Nobel laureate physicist Robert Coleman Richardson, writing in 2010, the free market price of helium has contributed to \"wasteful\" usage (e.g. for helium balloons). Prices in the 2000s had been lowered by the decision of the U.S. Congress to sell off the country's large helium stockpile by 2015. According to Richardson, the price needed to be multiplied by 20 to eliminate the excessive wasting of helium. In their book, the \"Future of helium as a natural resource\" (Routledge, 2012), Nuttall, Clarke & Glowacki (2012) also proposed to create an International Helium Agency (IHA) to build a sustainable market for this precious commodity.\n\nWhile balloons are perhaps the best known use of helium, they are a minor part of all helium use. Helium is used for many purposes that require some of its unique properties, such as its low boiling point, low density, low solubility, high thermal conductivity, or inertness. Of the 2014 world helium total production of about 32 million kg (180 million standard cubic meters) helium per year, the largest use (about 32% of the total in 2014) is in cryogenic applications, most of which involves cooling the superconducting magnets in medical MRI scanners and NMR spectrometers. Other major uses were pressurizing and purging systems, welding, maintenance of controlled atmospheres, and leak detection. Other uses by category were relatively minor fractions.\n\nHelium is used as a protective gas in growing silicon and germanium crystals, in titanium and zirconium production, and in gas chromatography, because it is inert. Because of its inertness, thermally and calorically perfect nature, high speed of sound, and high value of the heat capacity ratio, it is also useful in supersonic wind tunnels and impulse facilities.\n\nHelium is used as a shielding gas in arc welding processes on materials that at welding temperatures are contaminated and weakened by air or nitrogen. A number of inert shielding gases are used in gas tungsten arc welding, but helium is used instead of cheaper argon especially for welding materials that have higher heat conductivity, like aluminium or copper.\n\nOne industrial application for helium is leak detection. Because helium diffuses through solids three times faster than air, it is used as a tracer gas to detect leaks in high-vacuum equipment (such as cryogenic tanks) and high-pressure containers. The tested object is placed in a chamber, which is then evacuated and filled with helium. The helium that escapes through the leaks is detected by a sensitive device (helium mass spectrometer), even at the leak rates as small as 10 mbar·L/s (10 Pa·m/s). The measurement procedure is normally automatic and is called helium integral test. A simpler procedure is to fill the tested object with helium and to manually search for leaks with a hand-held device.\n\nHelium leaks through cracks should not be confused with gas permeation through a bulk material. While helium has documented permeation constants (thus a calculable permeation rate) through glasses, ceramics, and synthetic materials, inert gases such as helium will not permeate most bulk metals.\n\nBecause it is lighter than air, airships and balloons are inflated with helium for lift. While hydrogen gas is more buoyant, and escapes permeating through a membrane at a lower rate, helium has the advantage of being non-flammable, and indeed fire-retardant. Another minor use is in rocketry, where helium is used as an ullage medium to displace fuel and oxidizers in storage tanks and to condense hydrogen and oxygen to make rocket fuel. It is also used to purge fuel and oxidizer from ground support equipment prior to launch and to pre-cool liquid hydrogen in space vehicles. For example, the Saturn V rocket used in the Apollo program needed about 370,000 m (13 million cubic feet) of helium to launch.\n\nHelium as a breathing gas has no narcotic properties, so helium mixtures such as trimix, heliox and heliair are used for deep diving to reduce the effects of narcosis, which worsen with increasing depth. As pressure increases with depth, the density of the breathing gas also increases, and the low molecular weight of helium is found to considerably reduce the effort of breathing by lowering the density of the mixture. This reduces the Reynolds number of flow, leading to a reduction of turbulent flow and an increase in laminar flow, which requires less work of breathing. At depths below divers breathing helium–oxygen mixtures begin to experience tremors and a decrease in psychomotor function, symptoms of high-pressure nervous syndrome. This effect may be countered to some extent by adding an amount of narcotic gas such as hydrogen or nitrogen to a helium–oxygen mixture.\n\nHelium–neon lasers, a type of low-powered gas laser producing a red beam, had various practical applications which included barcode readers and laser pointers, before they were almost universally replaced by cheaper diode lasers.\n\nFor its inertness and high thermal conductivity, neutron transparency, and because it does not form radioactive isotopes under reactor conditions, helium is used as a heat-transfer medium in some gas-cooled nuclear reactors.\n\nHelium, mixed with a heavier gas such as xenon, is useful for thermoacoustic refrigeration due to the resulting high heat capacity ratio and low Prandtl number. The inertness of helium has environmental advantages over conventional refrigeration systems which contribute to ozone depletion or global warming.\n\nHelium is also used in some hard disk drives.\n\nThe use of helium reduces the distorting effects of temperature variations in the space between lenses in some telescopes, due to its extremely low index of refraction. This method is especially used in solar telescopes where a vacuum tight telescope tube would be too heavy.\n\nHelium is a commonly used carrier gas for gas chromatography.\n\nThe age of rocks and minerals that contain uranium and thorium can be estimated by measuring the level of helium with a process known as helium dating.\n\nHelium at low temperatures is used in cryogenics, and in certain cryogenics applications. As examples of applications, liquid helium is used to cool certain metals to the extremely low temperatures required for superconductivity, such as in superconducting magnets for magnetic resonance imaging. The Large Hadron Collider at CERN uses 96 metric tons of liquid helium to maintain the temperature at 1.9 kelvin.\n\nWhile chemically inert, helium contamination will impair the operation of microelectromechanical systems such that iPhones may fail.\n\nNeutral helium at standard conditions is non-toxic, plays no biological role and is found in trace amounts in human blood.\nThe speed of sound in helium is nearly three times the speed of sound in air. Because the fundamental frequency of a gas-filled cavity is proportional to the speed of sound in the gas, when helium is inhaled there is a corresponding increase in the resonant frequencies of the vocal tract. The fundamental frequency (sometimes called pitch) does not change, since this is produced by direct vibration of the vocal folds, which is unchanged. However, the higher resonant frequencies cause a change in timbre, resulting in a reedy, duck-like vocal quality. The opposite effect, lowering resonant frequencies, can be obtained by inhaling a dense gas such as sulfur hexafluoride or xenon.\n\nInhaling helium can be dangerous if done to excess, since helium is a simple asphyxiant and so displaces oxygen needed for normal respiration. Fatalities have been recorded, including a youth who suffocated in Vancouver in 2003 and two adults who suffocated in South Florida in 2006. In 1998, an Australian girl (her age is not known) from Victoria fell unconscious and temporarily turned blue after inhaling the entire contents of a party balloon.\nInhaling helium directly from pressurized cylinders or even balloon filling valves is extremely dangerous, as high flow rate and pressure can result in barotrauma, fatally rupturing lung tissue.\n\nDeath caused by helium is rare. The first media-recorded case was that of a 15-year-old girl from Texas who died in 1998 from helium inhalation at a friend's party; the exact type of helium death is unidentified.\n\nIn the United States only two fatalities were reported between 2000 and 2004, including a man who died in North Carolina of barotrauma in 2002. A youth asphyxiated in Vancouver during 2003, and a 27-year-old man in Australia had an embolism after breathing from a cylinder in 2000. Since then two adults asphyxiated in South Florida in 2006, and there were cases in 2009 and 2010, one a Californian youth who was found with a bag over his head, attached to a helium tank, and another teenager in Northern Ireland died of asphyxiation. At Eagle Point, Oregon a teenage girl died in 2012 from barotrauma at a party. A girl from Michigan died from hypoxia later in the year.\n\nOn February 4, 2015 it was revealed that during the recording of their main TV show on January 28, a 12-year-old member (name withheld) of Japanese all-girl singing group 3B Junior suffered from air embolism, losing consciousness and falling in a coma as a result of air bubbles blocking the flow of blood to the brain, after inhaling huge quantities of helium as part of a game. The incident was not made public until a week later. The staff of TV Asahi held an emergency press conference to communicate that the member had been taken to the hospital and is showing signs of rehabilitation such as moving eyes and limbs, but her consciousness has not been sufficiently recovered as of yet. Police have launched an investigation due to a neglect of safety measures.\n\nOn July 13, 2017 CBS News reported that a political operative who reportedly attempted to recover e-mails missing from the Clinton server, Peter W. Smith, \"apparently\" committed suicide in May at a hotel room in Rochester, Minnesota and that his death was recorded as \"asphyxiation due to displacement of oxygen in confined space with helium\". More details followed in the Chicago Tribune.\n\nThe safety issues for cryogenic helium are similar to those of liquid nitrogen; its extremely low temperatures can result in cold burns, and the liquid-to-gas expansion ratio can cause explosions if no pressure-relief devices are installed. Containers of helium gas at 5 to 10 K should be handled as if they contain liquid helium due to the rapid and significant thermal expansion that occurs when helium gas at less than 10 K is warmed to room temperature.\n\nAt high pressures (more than about 20 atm or two MPa), a mixture of helium and oxygen (heliox) can lead to high-pressure nervous syndrome, a sort of reverse-anesthetic effect; adding a small amount of nitrogen to the mixture can alleviate the problem.\n\n\n\nGeneral\n\nMore detail\n\nMiscellaneous\n\nHelium shortage\n"}
{"id": "32895908", "url": "https://en.wikipedia.org/wiki?curid=32895908", "title": "Hipora", "text": "Hipora\n\nHipora is a waterproof and breathable fabric, used as insert in winter, motorcycle and cycling gloves. It is developed by the Korean company Kolon Industries.\n\nHipora consists of a three-layer microporous silicon coating structure. Some types are impregnated with microscopic aluminum flakes to enhance heat retention characteristics.\n\nThe first layer prevents water from passing through. The pores are less than .5 um in diameter.\n\nThe second layer is a honeycomb structure that lets moisture in to let it expel through the first layer.\n\nThe third layer is very dense for added protection against water. This is the layer closest to the skin.\n\nThis technology meets OSHA Bloodborne Pathogens Standard (29CFR 1910.1030).\n\n"}
{"id": "811722", "url": "https://en.wikipedia.org/wiki?curid=811722", "title": "Jhai Foundation", "text": "Jhai Foundation\n\nThe Jhai Foundation is a non-profit organisation working mainly in Laos.\n\nOne of its projects is bringing communication services to rural communities lacking electricity or telephones. To achieve this goal, they have developed the \"Jhai PC and Communication System\", a solid state, low energy consuming computer that can be powered by a foot-crank generator built into a bicycle frame or solar energy and uses a wireless network to provide VoIP and Internet services.\n\nThe JhaiPC runs Linux with a localised version of KDE. The hardware and software design and user documentation for the JhaiPC are completely open and have been released through SourceForge.net.\n\nWith the minimal communication technology provided by the JhaiPC small businesses, such as farmers in Ban Phon Kam, are able to get better prices for their products. Jhai is also introducing organic farming techniques, fair trade marketing and direct sales to these farmers.\n\n"}
{"id": "12951270", "url": "https://en.wikipedia.org/wiki?curid=12951270", "title": "Loren Ferré Rangel", "text": "Loren Ferré Rangel\n\nMaría Lorenza Loren Ferré Rangel is a Trustee of the Conservation Trust of Puerto Rico, appointed jointly by Secretary of the Interior Gail Norton and Governor Aníbal Acevedo Vilá, after serving for several years as Chair of the Trust's Board of Advisors. She is also a Director of the Puerto Rico Center for the New Economy.\n\nA member of the Ferré Rangel family and granddaughter of don Luis A. Ferré, Ms. Ferré holds a B.A. from Holy Cross College, an M.A. and a Graduate Degree in Museum Studies from Boston University and a Design Certificate from the New York School of Interior Design.\n\nProfessionally, she is vice president for new products at El Día, Inc., the Ferré-Rangel family holding company, and developed the City View Plaza office complex in Guaynabo, Puerto Rico.\n\nShe serves as a director of the Center for the New Economy, a San Juan-based think tank and is Vice President of the Ferré-Rangel Family Foundation.\n\n"}
{"id": "19989934", "url": "https://en.wikipedia.org/wiki?curid=19989934", "title": "Lori 1 Wind Farm", "text": "Lori 1 Wind Farm\n\nLori 1 Wind Farm is a wind farm located along the Bazum Mountains at Pushkin Pass in Lori, Armenia. It is the country's only wind farm. The wind farm consists of four 660-kW wind turbines and has a capacity of 2.64 MWe. Completed in December 2005 by the Iranian company Sunir with US$3.2 million funding from Iran, it is owned by the High-Voltage Electric Networks of Armenia. In 2006, the Lori 1 generated only 2.6 GWh of electricity (a yearly average of 296.8 KWe—about 11% of installed capacity).\nThe Armenian and Iranian authorities have agreed to expand the wind farm up to 90 MW.\n"}
{"id": "42699154", "url": "https://en.wikipedia.org/wiki?curid=42699154", "title": "Magnetorheological elastomer", "text": "Magnetorheological elastomer\n\nMagnetorheological elastomers (MREs) (also called magnetosensitive elastomers) are a class of solids that consist of polymeric matrix with embedded micro- or nano-sized ferromagnetic particles such as carbonyl iron. As a result of this composite microstructure, the mechanical properties of these materials can be controlled by the application of magnetic field.\n\nMREs are typically prepared by curing process for polymers. The polymeric material (e.g. silicone rubber) in its liquid state is mixed with iron powder and several other additives to enhance their mechanical properties. The entire mixture is then cured at high temperature. Curing in the presence of a magnetic field causes the iron particles to arrange in chain like structures resulting in an anisotropic material. If magnetic field is not applied, then iron-particles are randomly distributed in the solid resulting in an isotropic material. Recently, an advanced technology, 3D printing has also been used to configure the magnetic particles inside the polymer matrix. \n\nMREs can be classified according to several parameters like: particles type, matrix, structure and distribution of particles:\n\n\n\n\n\nIn order to understand magneto-mechanical behaviour of MREs, theoretical studies need to be performed which couple the theories of electromagnetism with mechanics. Such theories are called theories of magneto-mechanics.\n\nMREs have been used for vibration isolation applications since their stiffness changes within a magnetic field \n\n"}
{"id": "28793951", "url": "https://en.wikipedia.org/wiki?curid=28793951", "title": "Milne-Thomson circle theorem", "text": "Milne-Thomson circle theorem\n\nIn fluid dynamics the Milne-Thomson circle theorem or the circle theorem is a statement giving a new stream function for a fluid flow when a cylinder is placed into that flow. It was named after the English mathematician L. M. Milne-Thomson.\n\nLet formula_1 be the complex potential for a fluid flow, where all singularities of formula_1 lie in formula_3. If a circle formula_4 is placed into that flow, the complex potential for the new flow is given by\n\nwith same singularities as formula_1 in formula_3 and formula_4 is a streamline. On the circle formula_4, formula_10, therefore\n\nConsider a uniform irrotational flow formula_12 with velocity formula_13 flowing in the positive formula_14 direction and place an infinitely long cylinder of radius formula_15 in the flow with the center of the cylinder at the origin. Then formula_16, hence using circle theorem,\n\nrepresents the complex potential of uniform flow over a cylinder.\n\n"}
{"id": "24573198", "url": "https://en.wikipedia.org/wiki?curid=24573198", "title": "Moneyingyi Wetland Sanctuary", "text": "Moneyingyi Wetland Sanctuary\n\nMoneyingyi Wetland Sanctuary or Moyingyi Wetland Wildlife Sanctuary is a wildlife reserve of Burma. It is located in Bago Division. It occupies an area of and was established in 1986.\n\n\n"}
{"id": "1556266", "url": "https://en.wikipedia.org/wiki?curid=1556266", "title": "Nail (unit)", "text": "Nail (unit)\n\nA nail, as a unit of cloth measurement, is generally a sixteenth of a yard or 2 inches (5.715 cm). The nail was apparently named after the practice of hammering brass nails into the counter at shops where cloth was sold. On the other hand, R D Connor, in \"The weights and measures of England\" (p 84) states that the nail was the 16th part of a Roman foot, i.e., digitus or finger, although he provides no reference to support this. Zupko's \"A dictionary of weights and measures for the British Isles\" (p 256) states that the nail was originally the distance from the thumbnail to the joint at the base of the thumb, or alternately, from the end of the middle finger to the second joint.\n\nAn archaic usage of the term \"nail\" is as a sixteenth of a (long) hundredweight for mass, or 1 clove of 7 pound avoirdupois (3.175 kg).\n\nExplanation: Katherine and Petruchio are getting married. At the tailor shop, they examine the wedding dress, which is nearly finished. Petruchio is concerned that it has too many frills, wonders what it will cost, and suspects that he has been cheated. Katherine says she likes it, and complains that Petruchio is making a fool of her. The taylor repeats Katherine's words: Sir, she says you're making a fool of her. This is where Petruchio launches into the above-quoted tirade. \"Monstrous\" may be a double-entendre for cuckold. The half-yard, quarter and nail were divisions of the yard used in cloth measurement.\n"}
{"id": "197129", "url": "https://en.wikipedia.org/wiki?curid=197129", "title": "Nanocrystalline silicon", "text": "Nanocrystalline silicon\n\nNanocrystalline silicon (nc-Si), sometimes also known as microcrystalline silicon (μc-Si), is a form of porous silicon. It is an allotropic form of silicon with paracrystalline structure—is similar to amorphous silicon (a-Si), in that it has an amorphous phase. Where they differ, however, is that nc-Si has small grains of crystalline silicon within the amorphous phase. This is in contrast to polycrystalline silicon (poly-Si) which consists solely of crystalline silicon grains, separated by grain boundaries. The difference comes solely from the grain size of the crystalline grains. Most materials with grains in the micrometre range are actually fine-grained polysilicon, so nanocrystalline silicon is a better term. The term Nanocrystalline silicon refers to a range of materials around the transition region from amorphous to microcrystalline phase in the silicon thin film. The crystalline volume fraction (as measured from Raman spectroscopy) is another criterion to describe the materials in this transition zone.\n\nnc-Si has many useful advantages over a-Si, one being that if grown properly it can have a higher electron mobility, due to the presence of the silicon crystallites. It also shows increased absorption in the red and infrared wavelengths, which make it an important material for use in a-Si solar cells. One of the most important advantages of nanocrystalline silicon, however, is that it has increased stability over a-Si, one of the reasons being because of its lower hydrogen concentration. Although it currently cannot attain the mobility that poly-Si can, it has the advantage over poly-Si that it is easier to fabricate, as it can be deposited using conventional low temperature a-Si deposition techniques, such as PECVD, as opposed to laser annealing or high temperature CVD processes, in the case of poly-Si.\n\nThe main application of this novel material is in the field of silicon thin film solar cells. As nc-Si has about the same bandgap as crystalline silicon, which is ~1.12 eV, it can be combined in thin layers with a-Si, creating a layered, multi-junction cell called a tandem cell. The top cell in a-Si absorbs the visible light and leaves the infrared part of the spectrum for the bottom cell in nanocrystalline Si.\n\nA few companies are on the verge of commercializing silicon inks based on nanocrystalline silicon or on other silicon compounds. The semiconductor industry is also investigating the potential for nanocrystalline silicon, especially in the memory area.\n\nNanocrystalline silicon and small-grained polycrystalline silicon are considered thin-film silicon.\n\n"}
{"id": "51510936", "url": "https://en.wikipedia.org/wiki?curid=51510936", "title": "Palmer-Bowlus Flume", "text": "Palmer-Bowlus Flume\n\nThe Palmer-Bowlus flume, is a class of flumes commonly used to measure the flow of wastewater in sewer pipes and conduits. The Palmer-Bowlus flume has a u-shaped cross-section and was designed to be inserted into, or in line with, pipes and u-channels found in sanitary sewer applications.\n\nAs a long-throated flume, the point of measurement of the Palmer-Bowlus flume is anywhere upstream of the throat ramp greater than D/2 (D=flume size). Montana flume has a single, specified point of measurement in the contracting section at which the level is measured. Unlike most other flumes used for open channel flow measurement, the Palmer-Bowlus flume can be calibrated by theoretical analysis.\n\nThe general design of the flume detailed in ASTM D5390: Standard Test Method for Open-Channel Flow Measurement of Water with Palmer-Bowlus Flumes. It is important to note that unlike the Parshall flume, the standard for the flume does not set out specific sizes and flow rates, but only general characteristics for the class of flume.\n\n18 sizes of Palmer-Bowlus flumes have been developed - in line with the common pipe sizes to which they would be adapted - from 4-inches to 72-inches. In practice, though, it is uncommon to see Palmer-Bowlus flumes greater than 24-inches in size.\n\nUnder average flow conditions, the Palmer-Bowlus flume is accurate to within 3-5%. For lower flow rates - where the depth is low relative to the length of the flume - the accuracy decreases to 5-6%. This error, combined with typical installation / flow meter errors, means that overall site accuracy is somewhat less than other more common flumes.\n\nFlow in the Palmer-Bowlus Flume transitions from a circular bottom section to a raised trapezoidal throat and then back - accelerating sub-critical flow (Fr~0.5) to a supercritical state (Fr>1) to develop the level-to-flow relationship.\n\nThe simplified free-flow discharge can be summarized as\n\nWhere\n\nNote that Palmer-Bowlus flumes are proprietary to each manufacturer / throat configuration. The table presented below is for the most common throat configuration - a trapezoidal ramp - and is simplified for the entire flume flow range. For other throat configurations refer to the manufacturer's flow tables.\n\nFree-Flow – when there is no “back water” to restrict flow through a flume. Only the single depth (primary point of measurement - Ha) needs to be measured to calculate the flow rate. A free flow also induces a hydraulic jump downstream of the flume.\n\nSubmerged Flow – when the water surface downstream of the flume is high enough to restrict flow through a flume, the flume is deemed to be submerged. Submergence transitions for Palmer-Bowlus flumes are quite high - 85-90%. As a result, corrections for submerged flow in Palmer-Bowlus flumes have not been published. As a result, it is important to set the flume so that it does not experience submerged flow conditions. Although commonly thought of as occurring at higher flow rates, It should be noted that submerged flow can exist at any flow level as it is a function of downstream conditions. In natural stream applications, submerged flow is frequently the result of vegetative growth on the downstream channel banks, sedimentation, or subsidence of the flume.\n\nUnlike other flumes - such as the Parshall, the Palmer-Bowlus flumes is typically only fabricated in two materials:\n\n\nFor standard Palmer-Bowlus flumes with the standard trapezoidal throat ramp:\n\n\n"}
{"id": "32062065", "url": "https://en.wikipedia.org/wiki?curid=32062065", "title": "Pittsburgh Water and Sewer Authority", "text": "Pittsburgh Water and Sewer Authority\n\nThe Pittsburgh Water and Sewer Authority (PWSA) is a municipal authority in Pittsburgh, Pennsylvania. It is responsible for water treatment and delivery systems in the city of Pittsburgh, as well as the city's sewer system. In a 2010 report, the authority reported 80,557 drinking water service connections and 107,151 sewage connections. The authority claims to serve approximately 250,000 customers, though it does not report how this number is calculated. (The population of the city in 2010 was 334,704.)\n\nIt is estimated that there are 930 miles of water lines and 7,300 Fire Hydrants.\n\nThe PWSA was created in 1984 to oversee a $200 million capital improvement program focused on Pittsburgh's water treatment and distribution system. This capital improvement program was primarily designed to ensure that the water system would meet various new requirements mandated by federal and state laws pertaining to safe drinking water. \n\nThe largest project undertaken in the initial years of the PWSA was to cover all open water reservoirs, replace them with closed tanks, or else enact another acceptable solution, such as the installation of a membrane filtration plant at the open Highland Reservoir No. 1, which is the focal point of Highland Park.\n\nThe City of Pittsburgh Water Department was absorbed by the PWSA in 1995, and in 1999, the PWSA also assumed the responsibility of operating and maintaining Pittsburgh's sewer system.\n\n"}
{"id": "55615781", "url": "https://en.wikipedia.org/wiki?curid=55615781", "title": "Pocket Power Stations", "text": "Pocket Power Stations\n\nPocket Power Stations were an early commercial use of Gas Turbine engines(Bristol Proteus), by the South Western Electricity Board, to generate electricity for the grid. They were the world's first unmanned electricity generation stations\n"}
{"id": "25231434", "url": "https://en.wikipedia.org/wiki?curid=25231434", "title": "Pumpherston retort", "text": "Pumpherston retort\n\nThe Pumpherston retort (also known as the Bryson retort) was a type of oil-shale retort used in Scotland at the end of 19th and beginning of 20th century. It marked separation of the oil-shale industry from the coal industry as it was designed specifically for oil-shale retorting. The retort is named after Pumpherston town in Scotland, which was one of the major Scottish oil shale areas. The retort was commercialized by Pumpherston Oil Company.\n\nThe Pumpherston retort was invented and patented in 1894 by William Fraser, James Bryson, and James Jones of Pumpherston Oil Company. By 1910, 1,528 Pumpherston retorts were used in Scotland. In addition, the retort was used in Spain and Australia.\n\nThe Pumpherston retort was a high cylindrical vessel containing two main sections. The upper section was made of iron and the lower section was made of fire bricks. The raw oil shale was fed on the top of retort. Shale oil and oil shale gas were distilled at the upper section at the temperature of . At the lower section, the heat rose to and steam was added to produce ammonia. The process required approximately of water equivalent of steam per one ton of oil shale.\n\nThe retort had a 15 ton capacity, and the residence time was 24 hours. It was started up by combustion of coal, but after the process started it was switched to the produced oil shale gas.\n"}
{"id": "23492496", "url": "https://en.wikipedia.org/wiki?curid=23492496", "title": "Rail Splitter Wind Farm", "text": "Rail Splitter Wind Farm\n\nThe Rail Splitter Wind Farm is a 67-turbine wind farm in northern Logan County and southern Tazewell County in the U.S. state of Illinois. The turbines are expected to generate a maximum of 100.5 megawatts of electricity. The wind farm is owned and operated by Horizon Wind Energy. The wind turbines that constitute the farm are centered on the town of Emden on both sides of Interstate 155. The wind farm, constructed in 2008-2009 at a cost of $200.0 million, was dedicated on July 21, 2009.\n"}
{"id": "2222213", "url": "https://en.wikipedia.org/wiki?curid=2222213", "title": "Society for Cryobiology", "text": "Society for Cryobiology\n\nThe Society for Cryobiology is an international scientific society that was founded in 1964. Its objectives are to promote research in low temperature biology, to improve scientific understanding in this field, and to disseminate and aid in the application of this knowledge. The Society also publishes a journal called \"Cryobiology\".\n\nAlthough some members of the Society for Cryobiology encouraged or helped in the development of early-stage cryonics in the 1960s, the Society later on banned cryonicists from becoming members.\n\n"}
{"id": "460626", "url": "https://en.wikipedia.org/wiki?curid=460626", "title": "Spirulina (dietary supplement)", "text": "Spirulina (dietary supplement)\n\nSpirulina represents a biomass of cyanobacteria (blue-green algae) that can be consumed by humans and other animals. The two species are \"Arthrospira platensis\" and \"A. maxima\".\n\nCultivated worldwide, \"Arthrospira\" is used as a dietary supplement or whole food. It is also used as a feed supplement in the aquaculture, aquarium, and poultry industries.\n\nThe species \"A. maxima\" and \"A. plaetensis\" were once classified in the genus \"Spirulina\". The common name, spirulina, refers to the dried biomass of \"A. platensis\", which belongs to photosynthetic bacteria that cover the groups Cyanobacteria and Prochlorophyta. Scientifically, a distinction exists between spirulina and the genus \"Arthrospira\". Species of \"Arthrospira\" have been isolated from alkaline brackish and saline waters in tropical and subtropical regions. Among the various species included in the genus \"Arthrospira\", \"A. platensis\" is the most widely distributed and is mainly found in Africa, but also in Asia. \"A. maxima\" is believed to be found in California and Mexico. The term \"spirulina\" remains in use for historical reasons.\n\n\"Arthrospira\" species are free-floating, filamentous cyanobacteria characterized by cylindrical, multicellular trichomes in an open left-handed helix. They occur naturally in tropical and subtropical lakes with high pH and high concentrations of carbonate and bicarbonate. \"A. platensis\" occurs in Africa, Asia, and South America, whereas \"A. maxima\" is confined to Central America. Most cultivated spirulina is produced in open-channel raceway ponds, with paddle wheels used to agitate the water.\n\nSpirulina thrives at a pH around 8.5 and above, which will get more alkaline, and a temperature around 30 °C (86 °F). They are autotrophic, meaning that they are able to make their own food, and do not need a living energy or organic carbon source. In addition, a nutrient feed for growing it is:\n\nSpirulina was a food source for the Aztecs and other Mesoamericans until the 16th century; the harvest from Lake Texcoco in Mexico and subsequent sale as cakes were described by one of Cortés' soldiers. The Aztecs called it \"tecuitlatl\".\n\nSpirulina was found in abundance at Lake Texcoco by French researchers in the 1960s, but no reference to its use was made by the Aztecs as a daily food source after the 16th century, probably due to the draining of the surrounding lakes for agriculture and urban development. The topic of the Tecuitlalt, which was earlier discovered in 1520, was not mentioned again until 1940, the French phycologist Pierre Dangeard mentioned about a cake called “\"dihe\"”, consumed by Kanembu tribe, African Lake Chad, Kanem (Chad, Africa). Dangeard studied the “\"dihe\"” samples and found that it is like a puree of spring form blue algae. Spirulina has also been traditionally harvested in Chad. It is dried into \"dihé\", which is used to make broths for meals, and also sold in markets. The spirulina is harvested from small lakes and ponds around Lake Chad.\n\nDuring 1964 and 1965, the botanist Jean Leonard confirmed that \"dihe\" is made up of spirulina, and later studied a bloom of algae in a sodium hydroxide production facility. As a result, the first systematic and detailed study of the growth requirements and physiology of spirulina was performed as a basis for establishing large-scale production in the 1970s.\n\nAs an ecologically sound, nutrient-rich, dietary supplement, spirulina is being investigated to address food security and malnutrition, and as dietary support in long-term space flight or Mars missions. Its interest for food security is for lower land and water needs to produce protein and energy than required for livestock as meat sources.\n\nDried spirulina contains 5% water, 24% carbohydrates, 8% fat, and about 60% (51–71%) protein (table).\n\nProvided in its typical supplement form as a dried powder, a 100-g amount of spirulina supplies 290 Calories and is a rich source (20% or more of the Daily Value, DV) of numerous essential nutrients, particularly protein, B vitamins (thiamin and riboflavin, 207% and 306% DV, respectively), and dietary minerals, such as iron (219% DV) and manganese (90% DV) (table). The lipid content of spirulina is 8% by weight (table) providing the fatty acids, gamma-linolenic acid, alpha-linolenic acid, linoleic acid, stearidonic acid, eicosapentaenoic acid (EPA), docosahexaenoic acid (DHA), and arachidonic acid. In contrast to those 2003 estimates (of DHA and EPA each at 2 to 3% of total fatty acids), 2015 research indicated that spirulina products \"contained no detectable omega-3 fatty acids\" (less than 0.1%, including DHA and EPA). An in vitro study reported that different strains of microalgae produced DHA and EPA in substantial amounts.\n\nSpirulina does not contain vitamin B naturally (see table), and spirulina supplements are not considered to be a reliable source of vitamin B, as they contain predominantly pseudovitamin B (Coα-[α-(7-adenyl)]-Coβ-cyanocobamide), which is biologically inactive in humans. In a 2009 position paper on vegetarian diets, the American Dietetic Association stated that spirulina is not a reliable source of active vitamin B. The medical literature similarly advises that spirulina is unsuitable as a source of B.\n\nSpirulina may have adverse interactions when taken with prescription drugs, particularly those affecting the immune system and blood clotting.\n\nSpirulina is a form of cyanobacterium, some of which are known to produce toxins such as microcystins, BMAA, and others. Some spirulina supplements have been found to be contaminated with microcystins, albeit at levels below the limit set by the Oregon Health Department. Microcystins can cause gastrointestinal disturbances, and in the long term, liver damage. The effects of chronic exposure to even very low levels of microcystins are of concern, because of the potential risk of toxicity to several organ systems and possibly cancer.\n\nThese toxic compounds are not produced by spirulina itself, but may occur as a result of contamination of spirulina batches with other toxin-producing blue-green algae. Because spirulina is considered a dietary supplement in the U.S., no active, industry-wide regulation of its production occurs and no enforced safety standards exist for its production or purity. The U.S. National Institutes of Health describes spirulina supplements as \"possibly safe\", provided they are free of microcystin contamination, but \"likely unsafe\" (especially for children) if contaminated. Given the lack of regulatory standards in the U.S., some public-health researchers have raised the concern that consumers cannot be certain that spirulina and other blue-green algae supplements are free of contamination.\n\nHeavy-metal contamination of spirulina supplements has also raised concern. The Chinese State Food and Drug Administration reported that lead, mercury, and arsenic contamination was widespread in spirulina supplements marketed in China. One study reported the presence of lead up to 5.1 ppm in a sample from a commercial supplement.\n\nSpirulina doses of 10 to 19 grams per day over several months have been used safely. Adverse effects may include nausea, diarrhea, fatigue, or headache.\n\nLike all protein-rich foods, spirulina contains the essential amino acid phenylalanine (2.6-4.1 g/100 g), which should be avoided by people who have phenylketonuria, a rare genetic disorder that prevents the body from metabolizing phenylalanine, which then builds up in the brain, causing damage.\n\nSpirulina contaminated with microcystins has various potential toxicity, especially to children, including liver damage, shock and death.\n\nVarious studies on spirulina as an alternative feed for animal and aquaculture were done. Spirulina can be fed up to 10% for poultry and less than 4% for quail. Increase in the spirulina content up to 40g/kg for 16 days in 21-day-old broiler male chicks, resulted in yellow and red coloration of flesh and this may be due to the accumulation of the yellow pigment, zeaxanthin. Pigs and rabbits can receive up to 10% of the feed and increase in the spirulina content in cattle resulted in increase in milk yield and weight. Spirulina as an alternative feedstock and immune booster for big-mouth buffalo, milk fish, cultured striped jack, carp, red sea bream, tilapia, catfish, yellow tail, zebrafish, shrimp, and abalone was established and up to 2% spirulina per day in aquaculture feed can be safely recommended.\n\nAccording to the U.S. National Institutes of Health, scientific evidence is insufficient to recommend spirulina supplementation for any human condition, and more research is needed to clarify whether consumption yields any benefits.\n\nAdministration of spirulina has been investigated as a way to control glucose in people with diabetes, but the European Food Safety Authority rejected those claims in 2013. Spirulina has been studied as a potential nutritional supplement for adults and children affected by HIV, but there was no conclusive effect on risk of death, body weight, or immune response.\n\nIn 1974, the World Health Organization described spirulina as \"an interesting food for multiple reasons, rich in iron and protein, and is able to be administered to children without any risk,\" considering it \"a very suitable food.\" The United Nations established the Intergovernmental Institution for the use of Micro-algae Spirulina Against Malnutrition in 2003.\n\nIn the late 1980s and early 90s, both NASA (CELSS) and the European Space Agency (MELiSSA) proposed spirulina as one of the primary foods to be cultivated during long-term space missions.\n\n\n"}
{"id": "27979", "url": "https://en.wikipedia.org/wiki?curid=27979", "title": "Sunlight", "text": "Sunlight\n\nSunlight is a portion of the electromagnetic radiation given off by the Sun, in particular infrared, visible, and ultraviolet light. On Earth, sunlight is filtered through Earth's atmosphere, and is obvious as daylight when the Sun is above the horizon. When the direct solar radiation is not blocked by clouds, it is experienced as sunshine, a combination of bright light and radiant heat. When it is blocked by clouds or reflects off other objects, it is experienced as diffused light. The World Meteorological Organization uses the term \"sunshine duration\" to mean the cumulative time during which an area receives direct irradiance from the Sun of at least 120 watts per square meter. Other sources indicate an \"Average over the entire earth\" of \"164 Watts per square meter over a 24 hour day\".\n\nThe ultraviolet radiation in sunlight has both positive and negative health effects, as it is both a requisite for vitamin D synthesis and a mutagen.\n\nSunlight takes about 8.3 minutes to reach Earth from the surface of the Sun. A photon starting at the center of the Sun and changing direction every time it encounters a charged particle would take between 10,000 and 170,000 years to get to the surface.\n\nSunlight is a key factor in photosynthesis, the process used by plants and other autotrophic organisms to convert light energy, normally from the Sun, into chemical energy that can be used to synthesize carbohydrates and to fuel the organisms' activities.\n\nResearchers can measure the intensity of sunlight using a sunshine recorder, pyranometer, or pyrheliometer. To calculate the amount of sunlight reaching the ground, both the eccentricity of Earth's elliptic orbit and the attenuation by Earth's atmosphere have to be taken into account. The extraterrestrial solar illuminance (), corrected for the elliptic orbit by using the day number of the year (dn), is given to a good approximation by\n"}
{"id": "14558397", "url": "https://en.wikipedia.org/wiki?curid=14558397", "title": "Surface phonon", "text": "Surface phonon\n\nIn solid state physics, a surface phonon is the quantum of a lattice vibration mode associated with a solid surface. Similar to the ordinary lattice vibrations in a bulk solid (whose quanta are simply called phonons), the nature of surface vibrations depends on details of periodicity and symmetry of a crystal structure. Surface vibrations are however distinct from the bulk vibrations, as they arise from the abrupt termination of a crystal structure at the surface of a solid. Knowledge of surface phonon dispersion gives important information related to the amount of surface relaxation, the existence and distance between an adsorbate and the surface, and information regarding presence, quantity, and type of defects existing on the surface.\n\nIn modern semiconductor research, surface vibrations are of interest as they can couple with electrons and thereby affect the electrical and optical properties of semiconductor devices. They are most relevant for devices where the electronic active area is near a surface, as is the case in two-dimensional electron systems and in quantum dots. As a specific example, the decreasing size of CdSe quantum dots was found to result in increasing frequency of the surface vibration resonance, which can couple with electrons and affect their properties.\n\nTwo methods are used for modeling surface phonons. One is the \"slab method\", which approaches the problem using lattice dynamics for a solid with parallel surfaces, and the other is based on Green's functions. Which of these approaches is employed is based upon what type of information is required from the computation. For broad surface phonon phenomena, the conventional lattice dynamics method can be used; for the study of lattice defects, resonances, or phonon state density, the Green's function method yields more useful results.\n\nSurface phonons are represented by a wave vector along the surface, q, and an energy corresponding to a particular vibrational mode frequency, ω. The surface Brillouin zone (SBZ) for phonons consists of two dimensions, rather than three for bulk. For example, the face centered cubic (100) surface is described by the directions ΓX and ΓM, referring to the [110] direction and [100] direction, respectively. \n\nThe description of the atomic displacements by the harmonic approximation assumes that the force on an atom is a function of its displacement with respect to neighboring atoms, i.e. Hooke's law holds. Higher order anharmonicity terms can be accounted by using perturbative methods. \n\nThe positions are then given by the relation\n\nwhere i is the place where the atom would sit if it were in equilibrium, m is the mass of the atom that should sit at i, α is the direction of its displacement, u is the amount of displacement of the atom from i, and formula_2 are the force constants which come from the crystal potential. \n\nThe solution to this gives the atomic displacement due to the phonon, which is given by\n\nwhere the atomic position i is described by l, m, and κ, which represent the specific atomic layer, l, the particular unit cell it is in, m, and the position of the atom with respect to its own unit cell, κ. The term x(l,m) is the position of the unit cell with respect to some chosen origin.\n\nPhonons can be labeled by the manner in which the vibrations occur. If the vibration occurs lengthwise in the direction of the wave and involves contraction and relaxation of the lattice, the phonon is called a \"longitudinal phonon\". Alternatively, the atoms may vibrate side-to-side, perpendicular to wave propagation direction; this is known as a \"transverse phonon”. In general, transverse vibrations tend to have smaller frequencies than longitudinal vibrations. \n\nThe wavelength of the vibration also lends itself to a second label. \"Acoustic\" branch phonons have a wavelength of vibration that is much bigger than the atomic separation so that the wave travels in the same manner as a sound wave; \"optical\" phonons can be excited by optical radiation in the infrared wavelength or longer. Phonons take on both labels such that transverse acoustic and optical phonons are denoted TA and TO, respectively; likewise, longitudinal acoustic and optical phonons are denoted LA and LO.\n\nThe type of surface phonon can be characterized by its dispersion in relation to the bulk phonon modes of the crystal. Surface phonon mode branches may occur in specific parts of the SBZ or encompass it entirely across. These modes can show up both in the bulk phonon dispersion bands as what is known as a resonance or outside these bands as a pure surface phonon mode. Thus surface phonons can be purely surface existing vibrations, or simply the expression of bulk vibrations in the presence of a surface, known as a surface-excess property.\nA particular mode, the Rayleigh phonon mode, exists across the entire BZ and is known by special characteristics, including a linear frequency versus wave number relation near the SBZ center.\n\nTwo of the more common methods for studying surface phonons are electron energy loss spectroscopy and helium atom scattering.\n\nThe technique of electron energy loss spectroscopy (EELS) is based upon the fact that electron energy decreases upon interaction with matter. Since the interaction of low energy electrons is mainly in the surface, the loss is due to surface phonon scattering, which have an energy range of 10 eV to 1 eV.\n\nIn EELS, an electron of known energy is incident upon the crystal, a phonon of some wave number, q, and frequency, ω, is then created, and the outgoing electron's energy and wave number are measured. If the incident electron energy, E, and wave number, k, are chosen for the experiment and the scattered electron energy, E, and wave number, k, are known by measurement, as well as the angles with respect to the normal for the incident and scattered electrons, θ and θ, then values of q throughout the BZ can be obtained. Energy and momentum for the electron have the following relation,\n\nwhere m is the mass of an electron. Energy and momentum must be conserved, so the following relations must be true of the energy and momentum exchange throughout the encounter:\n\nwhere G is a reciprocal lattice vector that ensures that q falls in the first BZ and the angles θ and θ are measured with respect to the normal to the surface.\n\nThe dispersion is often shown with q given in units of cm, in which 100 cm = 12.41 meV. The electron incident angles for most EELS phonon study chambers can range from 135-θ and 90-θ for θ ranging between 55-65°.\n\nHelium is the best suited atom to be used for surface scattering techniques, as it has a low enough mass that multiple phonon scattering events are unlikely, and its closed valence electron shell makes it inert, unlikely to bond with the surface upon which it impinges. In particular, He is used because this isotope allows for very precise velocity control, important for obtaining maximum resolution in the experiment.\n\nThere are two main techniques used for helium atom scattering studies. One is a so-called time-of-flight measurement which consists of sending pulses of He atoms at the crystal surface and then measuring the scattered atoms after the pulse. The He beam velocity ranges from 644–2037 m/s. The other involves measuring the momentum of the scattered He atoms by a LiF grating monochromator.\n\nIt is important to note that the He nozzle beam source used in many He scattering experiments poses some risk of error, as it adds components to the velocity distributions that can mimic phonon peaks; particularly in time-of-flight measurements, these peaks can look very much like inelastic phonon peaks. Thus, these false peaks have come to be known by the names \"deceptons\" or \"phonions\".\n\nEELS and helium scattering techniques each have their own particular merits that warrant the use of either depending on the sample type, resolution desired, etc. Helium scattering has a higher resolution than EELS, with a resolution of 0.5-1 meV compared to 7 meV. However, He scattering is available only for energy differences, E-E, of less than about 30 meV, while EELS can be used for up to 500 meV.\n\nDuring He scattering, the He atom does not actually penetrate into the material, being scattered only once at the surface; in EELS, the electron can go as deep as a few monolayers, scattering more than once during the course of the interaction. Thus, the resulting data is easier to understand and analyze for He atom scattering than for EELS, since there are no multiple collisions to account for. \n\nHe beams have a capabilities of delivering a beam of higher flux than electrons in EELS, but the detection of electrons is easier than the detection of He atoms. He scattering is also more sensitive to very low frequency vibrations, on the order of 1 meV. This is the reason for its high resolution in comparison to EELS.\n"}
{"id": "201479", "url": "https://en.wikipedia.org/wiki?curid=201479", "title": "Syngas", "text": "Syngas\n\nSyngas, or synthesis gas, is a fuel gas mixture consisting primarily of hydrogen, carbon monoxide, and very often some carbon dioxide. The name comes from its use as intermediates in creating synthetic natural gas (SNG) and for producing ammonia or methanol. Syngas is usually a product of gasification and the main application is electricity generation. Syngas is combustible and often used as a fuel of internal combustion engines. It has less than half the energy density of natural gas.\n\nSyngas can be produced from many sources, including natural gas, coal, biomass, or virtually any hydrocarbon feedstock, by reaction with steam (steam reforming), carbon dioxide (dry reforming) or oxygen (partial oxidation). Syngas is a crucial intermediate resource for production of hydrogen, ammonia, methanol, and synthetic hydrocarbon fuels. Syngas is also used as an intermediate in producing synthetic petroleum for use as a fuel or lubricant via the Fischer–Tropsch process and previously the Mobil methanol to gasoline process.\n\nProduction methods include steam reforming of natural gas or liquid hydrocarbons to produce hydrogen, the gasification of coal, biomass, and in some types of waste-to-energy gasification facilities.\n\nThe chemical composition of syngas varies based on the raw materials and the processes. Syngas produced by coal gasification generally is a mixture of 30 to 60% carbon monoxide, 25 to 30% hydrogen, 5 to 15% carbon dioxide, and 0 to 5% methane. It also contains lesser amount of other gases.\n\nThe main reaction that produces syngas, steam reforming, is an endothermic reaction with 206 kJ/mol methane needed for conversion.\n\nThe first reaction, between incandescent coke and steam, is strongly endothermic, producing carbon monoxide (CO), and hydrogen (water gas in older terminology). When the coke bed has cooled to a temperature at which the endothermic reaction can no longer proceed, the steam is then replaced by a blast of air.\n\nThe second and third reactions then take place, producing an exothermic reaction—forming initially carbon dioxide and raising the temperature of the coke bed—followed by the second endothermic reaction, in which the latter is converted to carbon monoxide, CO. The overall reaction is exothermic, forming \"producer gas\" (older terminology). Steam can then be re-injected, then air etc., to give an endless series of cycles until the coke is finally consumed. Producer gas has a much lower energy value, relative to water gas, due primarily to dilution with atmospheric nitrogen. Pure oxygen can be substituted for air to avoid the dilution effect, producing gas of much higher calorific value.\nWhen used as an intermediate in the large-scale, industrial synthesis of hydrogen (principally used in the production of ammonia), it is also produced from natural gas (via the steam reforming reaction) as follows:\n\nIn order to produce more hydrogen from this mixture, more steam is added and the water gas shift reaction is carried out:\n\nThe hydrogen must be separated from the to be able to use it. This is primarily done by pressure swing adsorption (PSA), amine scrubbing, and membrane reactors.\n\nConversion of biomass to syngas is typically low-yield. The University of Minnesota developed a metal catalyst that reduces the biomass reaction time by up to a factor of 100. The catalyst can be operated at atmospheric pressure and reduces char. The entire process is autothermic and therefore heating is not required.\n\nCO can be split into CO and then combined with hydrogen to form syngas . A method for production of carbon monoxide from carbon dioxide by treating it with microwave radiation is being examined by the solar fuels-project of the Dutch Institute For Fundamental Energy Research. This technique was alleged to have been used during the Cold war in Russian nuclear submarines to allow them to get rid of CO gas without leaving a bubble trail. Publicly available journals published during the Cold War indicate that American submarines used conventional chemical scrubbers to remove CO. Documents released after the sinking of the Kursk, a Cold War era Oscar-class submarine, indicate that potassium superoxide scrubbers were used to remove carbon dioxide on that vessel.\n\nHeat generated by concentrated solar power may be used to drive thermochemical reactions to split carbon dioxide to carbon monoxide or to make hydrogen. Natural gas may be used as a feedstock in a facility that integrates concentrated solar power with a power plant fueled by natural gas augmented by syngas while the sun is shining. The Sunshine-to-Petrol project has developed a device allowing for efficient production using this technique. It is called the Counter-Rotating Ring Receiver Reactor Recuperator, or CR5.\n\nAn airborne wind energy system has been proposed to supply heat to the steam reforming reaction. This avoids burning natural gas for the heat and radically simplifies the steam reformer.\n\nBy employing co-electrolysis, i.e. the electrochemical conversion of steam and carbon dioxide with the use of renewably generated electricity, syngas can be produced in the framework of a -valorization scenario, allowing for a closed carbon cycle.\n\nUse of electricity to extract carbon dioxide from water and then water gas shift to syngas has been trialled by the US Naval Research Lab. This process becomes cost effective if the price of electricity is below $20/MWh.\n\nElectricity generated from renewable sources is also used to process carbon dioxide and water into syngas through the high-temperature electrolysis. This is an attempt to maintain carbon neutral in the generation process. Audi, in partnership with company named Sunfire, opened a pilot plant in November 2014 to generate e-diesel using this process.\n\nCoal gasification processes to create syngas were used for many years to manufacture illuminating gas (coal gas) for gas lighting, cooking and to some extent, heating, before electric lighting and the natural gas infrastructure became widely available. The syngas produced in waste-to-energy gasification facilities can be used to generate electricity.\n\nSyngas that is not methanized typically has a lower heating value of 120 BTU/scf . Untreated syngas can be run in hybrid turbines that allow for greater efficiency because of their lower operating temperatures, and extended part lifetime.\n\nSyngas is used to directly reduce iron ore to sponge iron.\n\nSyngas can be used in the Fischer–Tropsch process to produce diesel, or converted into e.g. methane, methanol, and dimethyl ether in catalytic processes.\n\nIf the syngas is post-treated by cryogenic processing, it should be taken into account that this technology has great difficulty in recovering pure carbon monoxide if relatively large volumes of nitrogen are present due to carbon monoxide and nitrogen having very similar boiling points which are –191.5 °C and –195.79 °C respectively. Certain process technology selectively removes carbon monoxide by complexation/decomplexation of carbon monoxide with cuprous aluminum chloride () dissolved in an organic liquid such as toluene. The purified carbon monoxide can have a purity greater than 99%, which makes it a good feedstock for the chemical industry. The reject gas from the system can contain carbon dioxide, nitrogen, methane, ethane, and hydrogen. The reject gas can be further processed on a pressure swing adsorption system to remove hydrogen, and the hydrogen and carbon monoxide can be recombined in the proper ratio for catalytic methanol production, Fischer-Tropsch diesel, etc. Cryogenic purification, being very energy-intensive, is not well suited to simply making fuel, because of the greatly reduced net energy gain. \n\nSyngas is used to produce methanol as in the following reaction.<chem> CO_2 + 3 H2 ->CH3OH + H2O</chem>\n\nSyngas is used to produce hydrogen for the Haber process.\n\n"}
{"id": "37889609", "url": "https://en.wikipedia.org/wiki?curid=37889609", "title": "The Fruit Hunters", "text": "The Fruit Hunters\n\nThe Fruit Hunters is a 2012 feature documentary film about exotic fruit cultivators and preservationists. It is directed by Yung Chang and co-written by Chang and Mark Slutsky, and inspired by Adam Leith Gollner’s 2008 book of the same name.\n\nIn addition to documentary sequences, the film also uses CGI animation, models and performers to stage real and imagined moments in the history of fruit.\n\nSubjects in the film include actor Bill Pullman, who was not featured in Gollner's book. The filmmakers became aware of his interest in fruit thanks to a 2009 \"New York Times\" profile. The film follows Pullman's efforts to develop a communal orchard near his Hollywood Hills residence.\n\nTwo staff members of Fairchild Tropical Botanic Garden are shown exploring jungles in Asia and South America in search of plants to graft and preserve.\n\n\"The Fruit Hunters\" also features a Honduran scientist trying to find an alternative to the Cavendish banana, an Italian cultivator who studies Renaissance paintings to identify new varieties, and an indigenous guide in Borneo.\n\nThe idea for \"The Fruit Hunters\" was first pitched at a forum at Hot Docs, utilizing footage with Bill Pullman. The National Film Board of Canada and EyeSteelFilm agreed to co-produce the film, which also has funding from the Canadian Broadcasting Corporation, Telefilm Canada and SODEC.\n\nChang, Slutsky and Gollner were friends before working on the film, having once lived in the same building in Montreal.\n\n"}
{"id": "18502540", "url": "https://en.wikipedia.org/wiki?curid=18502540", "title": "The London Accord", "text": "The London Accord\n\nThe London Accord is a collaboration between investment banks, research houses, academics and NGOs to produce free research on climate change for financial investors.\n\nIt is intended as a reference guide for investors in the climate change sector.\n\nThe London Accord is the largest cooperative project in the world on investment opportunities in avoiding climate change (about 7 million UK pounds).\n\nThe London Accord began in 2005, was launched formally in March 2007 and published its first results on 19 December 2007 launching them at a roll out meeting at Mansion House in London. These findings are freely available from its website.\n\nIts main summary of December 2007 said:\n\n\nThe findings of the research carried out show that:\n\nSince 2007 the London Accord has become one of the leading sources for policy-makers of investment research on environmental, social and governance issues. By the end of 2011, over 250 research reports had been released to the public. Funding for the London Accord has come from the City of London Corporation, Z/Yen Group and Gresham College.\n\nDavid Lewis (Lord Mayor) said:\n\n"}
{"id": "6211224", "url": "https://en.wikipedia.org/wiki?curid=6211224", "title": "Thermal cutoff", "text": "Thermal cutoff\n\nA thermal cutoff is an electrical safety device that interrupts electric current when heated to a specific temperature. These devices may be for one-time use or may be reset manually or automatically.\n\nA thermal fuse is a cutoff which uses a one-time fusible link. Unlike a thermal switch which may automatically reset itself when the temperature drops, the thermal fuse is more like an electrical fuse: a single-use device that cannot be reset and must be replaced when it fails or is triggered. A thermal fuse is used when the overheating is a result of a rare occurrence, such as failure requiring repair (which would also replace the fuse) or replacement at the end of service life. \n\nOne mechanism is a small meltable pellet that holds down a spring. When the pellet melts, the spring is released, separating the contacts and breaking the circuit. The Tamura LE series, NEC Sefuse SF series, Microtemp G4A series, and Hosho Elmwood D series, for example, may use alloy pellets that contain copper, beryllium, and silver to melt at a precise temperature.\n\nThermal fuses are usually found in heat-producing electrical appliances such as coffeemakers and hair dryers. They function as safety devices to disconnect the current to the heating element in case of a malfunction (such as a defective thermostat) that would otherwise allow the temperature to rise to dangerous levels, possibly starting a fire. \n\nUnlike electrical fuses or circuit breakers, thermal fuses only react to excessive temperature and not to excessive current (unless the excessive current is sufficient to cause the thermal fuse itself to heat up to the trigger temperature). For example, in a surge protector thermal fuses may be wired in series with the varistors; when the varistors conduct, the fuse heats up and disconnects the power, which eliminates the risk of fire which can occur when the varistors are overloaded.\n\nA thermal switch (sometimes thermal reset or thermal cutout (TCO)) is a device which normally opens at a high temperature (often with a faint \"plink\" sound) and re-closes when the temperature drops. The thermal switch may be a bimetallic strip, often encased in a tubular glass bulb to protect it from dust or short circuit. Another common design uses a bimetallic shallow dome-shaped cap which \"clicks\" to an inside-out inverted cap shape when heated, such as the \"Klixon\" brand of thermal cutouts. \n\nUnlike a thermal fuse, a thermal switch is usually reusable, and is therefore suited to protecting against temporary situations which are common and user-correctable. Thermal switches are used in power supplies in case of overload, and also as thermostats in some heating and cooling systems.\n\nAnother type of thermal switch is a PTC (Positive Temperature Coefficient) thermistor; these thermistors have a \"switch\" temperature at which the resistance suddenly rises rapidly, limiting the current through the circuit. When used in conjunction with a thermistor relay, the PTC will switch off an electrical system at a desired temperature. Typical use is for motor overheat protection.\n\nThermal switches are included in some light fixtures, particularly with recessed lights, where excessive heat is most likely to occur. This may lead to \"cycling\", where a light turns off and back on every few minutes. Flashing incandescent Christmas lights take advantage of this effect. Some flasher bulbs interrupt power when heated, while other twinkle/sparkle mini-bulbs momentarily shunt current around the filament. \n\nThermal switches are part of the normal operation of older fluorescent light fixtures, where they are the major part of the starter module.\n\nGE trademarked the name \"Guardette\" for the thermal protection switches used on their refrigeration compressors.\n\nThermal switches on microprocessors often stop only the fetching of instructions to execute, reducing the clock rate to zero until a lower temperature is reached, while maintaining power to the cache to prevent data loss (although a second switch, with a higher triggering temperature, usually turns off even the cache and forces the computer to reboot). This mitigates the impact of programs resembling power viruses on the processor's longevity, while still accommodating their possible legitimate uses; it can also make overclocking possible with less risk.\n\nSome thermal switches must be reset manually after they have been tripped. This design is used when an automatic and unattended restart would create a hazardous condition, such as sudden startup of a powerful motor without warning. These types of thermal cutouts are usually reset by pressing a push-button by hand or with a special tool.\n\n"}
{"id": "231506", "url": "https://en.wikipedia.org/wiki?curid=231506", "title": "Varicap", "text": "Varicap\n\nIn electronics, a varicap diode, varactor diode, variable capacitance diode, variable reactance diode or tuning diode is a type of diode designed to exploit the voltage-dependent capacitance of a reverse-biased p–n junction.\n\nVaractors are used as voltage-controlled capacitors. They are commonly used in voltage-controlled oscillators, parametric amplifiers, and frequency multipliers. Voltage-controlled oscillators have many applications such as frequency modulation for FM transmitters and phase-locked loops. Phase-locked loops are used for the frequency synthesizers that tune many radios, television sets, and cellular telephones.\n\nThe varicap was developed by the Pacific Semiconductor subsidiary of the Ramo Wooldridge Corporation who received a patent for the device in June 1961. The device name was also trademarked as the \"Varicap\" by TRW Semiconductors, the successor to Pacific Semiconductors, in October 1967. This helps explain the different names for the device as it came into use.\n\nVaractors are operated in a reverse-biased state, so no DC current flows through the device. The amount of reverse bias controls the thickness of the depletion zone and therefore the varactor's junction capacitance. Generally, the depletion region thickness is proportional to the square root of the applied voltage, and capacitance is inversely proportional to the depletion region thickness. Thus, the capacitance is inversely proportional to the square root of applied voltage.\n\nAll diodes exhibit this variable junction capacitance, but varactors are manufactured to exploit the effect and increase the capacitance variation.\n\nThe figure shows an example of a cross section of a varactor with the depletion layer formed of a p–n junction. This depletion layer can also be made of a MOS or a Schottky diode. This is important in CMOS and MMIC technology.\n\nGenerally the use of a varicap diode in a circuit requires connecting it to a tuned circuit, usually in parallel with any existing capacitance or inductance. A DC voltage is applied as reverse bias across the varicap to alter its capacitance. The DC bias voltage must be blocked from entering the tuned circuit. This can be accomplished by placing a DC blocking capacitor with a capacitance about 100 times greater than the maximum capacitance of the varicap diode in series with it and by applying DC from a high impedance source to the node between the varicap cathode and the blocking capacitor as shown in the upper left circuit in the accompanying diagram.\nSince no significant DC current flows in the varicap, the value of the resistor connecting its cathode back to the DC control voltage can be somewhere in the range of 22 kΩ to 150 kΩ and the blocking capacitor somewhere in the range of 5–100 nF. Sometimes, with very high-Q tuned circuits, an inductor is placed in series with the resistor to increase the source impedance of the control voltage so as not to load the tuned circuit and decrease its Q.\n\nAnother common configuration uses two back-to-back (anode to anode) varicap diodes. (See lower left circuit in diagram.) The second varicap effectively replaces the blocking capacitor in the first circuit. This reduces the overall capacitance and the capacitance range by half, but has the advantage of reducing the AC component of voltage across each device and has symmetrical distortion should the AC component possess enough amplitude to bias the varicaps into forward conduction.\n\nWhen designing tuning circuits with varicaps it is usually good practice to maintain the AC component of voltage across the varicap at a minimal level, usually less than 100 mV peak to peak, to prevent changing the diode capacitance too much, which would distort the signal and add harmonics.\n\nA third circuit, at top right in diagram, uses two series-connected varicaps and separate DC and AC signal ground connections. The DC ground is shown as a traditional ground symbol, and the AC ground as an open triangle. Separation of grounds is often done to (i) prevent high-frequency radiation from the low-frequency ground node, and (ii) prevent DC currents in the AC ground node changing bias and operating points of active devices such as varicaps and transistors.\n\nThese circuit configurations are quite common in television tuners and electronically tuned broadcast AM and FM receivers, as well as other communications equipment and industrial equipment. Early varicap diodes usually required a reverse voltage range of 0–33 v to obtain their full capacitance ranges, which were still quite small, approximately 1–10 pF. These types were – and still are – extensively used in television tuners, whose high carrier frequencies require only small changes in capacitance.\n\nIn time, varicap diodes were developed which exhibited large capacitance ranges, 100–500 pF, with relatively small changes in reverse bias: 0–5 V or 0–12 V. These newer devices allow electronically tuned AM broadcast receivers to be realized as well as a multitude of other functions requiring large capacitance changes at lower frequencies, generally below 10 MHz. Some designs of electronic security tag readers used in retail outlets require these high capacitance varicaps in their voltage-controlled oscillators.\n\nThe three leaded devices depicted at the top of the page are generally two common cathode connected varicaps in a single package. In the consumer AM/FM tuner depicted at the right, a single dual-package varicap diode adjusts both the passband of the tank circuit (the main station selector), and the local oscillator with a single varicap for each. This is done to keep costs down – two dual packages could have been used, one for the tank and one for the oscillator, four diodes in all, and this is what was depicted in the application data for the LA1851N AM radio chip. Two lower-capacitance dual varactors used in the FM section (which operates at a frequency about one hundred times greater) are highlighted by red arrows. In this case four diodes are used, via a dual package for the tank / bandpass filter and a dual package for the local oscillator.\n\nSpecial types of varicap diode exhibiting an abrupt change in capacitance can often be found in consumer equipment such as television tuners, which are used to switch radio frequency signal paths. When in the high capacitance state, usually with low or no bias, they present a low impedance path to RF, whereas when reverse biased their capacitance abruptly decreases and their RF impedance increases. Although they are still slightly conductive to the RF path, the attenuation they introduce decreases the unwanted signal to an acceptably low level. They are often used in pairs to switch between two different RF sources such as the VHF and UHF bands in a television tuner by supplying them with complementary bias voltages.\n\nIn some applications, such as harmonic multiplication, a large signal amplitude alternating voltage is applied across a varicap to deliberately vary the capacitance at signal rate to generate higher harmonics, which are extracted through filtering. If a sine wave current of sufficient amplitude is applied driven through a varicap, the resultant voltage gets \"peaked\" into a more triangular shape, and odd harmonics are generated.\n\nThis was one early method used to generate microwave frequencies of moderate power, 1–2 GHz at 1–5 watts, from about 20 watts at a frequency of 3–400 MHz before adequate transistors had been developed to operate at this higher frequency. This technique is still used to generate much higher frequencies, in the 100 GHz – 1 THz range, where even the fastest GaAs transistors are still inadequate.\n\nAll semiconductor junction devices exhibit the effect, so they can be used as varicaps, but their characteristics will not be controlled and can vary widely between batches.\n\nPopular makeshift varicaps include LEDs, 1N400X series rectifier diodes, Schottky rectifiers and various transistors used with their collector-base junctions reverse biased, particularly the 2N2222 and BC547. Reverse biasing the emitter-base junctions of transistors also is quite effective as long as the AC amplitude remains small. Maximum reverse bias voltage is usually between 5 and 7 Volts, before the avalanche process starts conducting. Higher-current devices with greater junction area tend to possess higher capacitance. The Philips BA 102 varicap and a common zener diode, the 1N5408, exhibit similar changes in junction capacitance, with the exception that the BA 102 possesses a \"specified\" set of characteristics in relation to junction capacitance (whereas the 1N5408 does not) and the \"Q\" of the 1N5408 is less.\n\nBefore the development of the varicap, motor driven variable capacitors or saturable-core reactors were used as electrically controllable reactances in the VCOs and filters of equipment like World War II German spectrum analyzers.\n\n\n\n"}
