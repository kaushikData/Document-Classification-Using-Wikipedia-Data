{"id": "52256432", "url": "https://en.wikipedia.org/wiki?curid=52256432", "title": "A. P. B. Sinha", "text": "A. P. B. Sinha\n\nAkhoury Purnendu Bhusan Sinha (born 1928) is an Indian sold state chemist and a former head of the Physical Chemistry Division of the National Chemical Laboratory, Pune. He is known for his theories on semiconductors and his studies on synthesis of manganites. He is an elected fellow of the Indian National Science Academy and the Indian Academy of Sciences. The Council of Scientific and Industrial Research, the apex agency of the Government of India for scientific research, awarded him the Shanti Swarup Bhatnagar Prize for Science and Technology, one of the highest Indian science awards, in 1972, for his contributions to chemical sciences.\n\nA. P. B. Sinha, born on 27 December 1928, joined the University of London from where he secured a PhD in 1954; his thesis was based on solid state chemistry. Later, he served the National Chemical Laboratory, Pune as a director's grade scientist and headed the Physical Chemistry division of the institution. Continuing his researches on solid state chemistry, he studied low mobility semiconductors with respect to its electron transport and crystal distortions caused by electron lattice transitions, switching, magnetic ordering and memory effects. He is known to have synthesized new manganites and reportedly developed a number of solid state products such as thermistors, photocells, magnets and photovoltaic products. Based on his studies on electron lattice interaction, he proposed support theories for the ferroelectricity theory and developed new theories on the \"thermoelectrical power and mobility in semiconductors\". His researches are reported to have widened the understanding of conduction in semiconductors. The body of his literary work is composed of one book, \"Spectroscopy in inorganic chemistry\", chapters to the book, \"A study of the growth and structure of layers of oxides, sulphides and related compounds, with special reference to the effect of temperature\", edited by C. N. R. Rao, and several articles published in peer reviewed journals. His work has been cited by several authors.\n\nSinha has been associated with journals such as \"Bulletin Materials Science\" and \"Indian Journal of Pure Applied Physics\" as a member of their editorial boards. The Council of Scientific and Industrial Research awarded him the Shanti Swarup Bhatnagar Prize, one of the highest Indian science awards, in 1972. He was elected by the Indian Academy of Sciences as their fellow in 1974 before he became an elected fellow of the Indian National Science Academy in 1978. He is also an elected fellow of the Maharashtra Academy of Sciences and a recipient of the Meritorious Invention Award of the National Research Development Corporation which he received in 1978. After his stint at NCL, Sinha migrated to the US and is associated with the Morris Innovative Research.\n\n\n\n\n"}
{"id": "40299274", "url": "https://en.wikipedia.org/wiki?curid=40299274", "title": "American Radiator Company", "text": "American Radiator Company\n\nThe American Radiator Company was established in 1892 by the merger of a number of North American radiator manufacturers. The company expanded in the early 20th century into Europe under the brand National Radiator Company.\n\nIn 1929 it amalgamated with the Standard Sanitary Manufacturing Company to form the American Radiator and Standard Sanitary Corporation.\n\nThe Michigan Radiator & Iron Manufacturing Company was founded in 1888. John B. Dyar manager and owner of the Detroit Metal & Heating Works was the main promoter. Clarence M. Woolley joined the firm in 1887.\n\nThe Detroit Radiator Company was formed in 1882 by Henry C. and Charles C. Hodges.\n\nThe Pierce Steam Heating Company was founded in 1881 by John B. Pierce and Joseph Bond in Buffalo.\n\nThe Standard Radiator Company (Buffalo) was established by Nelson Holland.\n\nThe American Radiator company was formed in 1891/2 from the Detroit Radiator Company, the Michigan Radiator & Iron Manufacturing Company, and the Pierce Steam Heating Company of Buffalo. The company was headed by Joseph Bond, (of Pierce Steam Heating Co.), as President, Charles Hodges, (of Detroit Radiator) as Treasurer, and Clarence Woolley (of Michigan Radiator) as Secretary.\n\nThe company made a profit of $400,000 in its first year, but was subsequently affected by an economic depression (see also Panic of 1893). In 1894, Mr. Woolley convinced the other officers of the company to pay his way to Europe, whereupon he booked the sale of $50,000 worth of cast iron radiators for the Swiss capitol. This was the start of the company's entry into the European market. By the following year, the company had established a branch in London, England. The company began manufacturing in several west European countries, starting in 1898 and continuing into the 1920s.\n\nThe company was successful in the United States and European markets, and attracted the attention of J.P. Morgan. Morgan helped the firm to combine most of the radiator manufactories in the US. In 1899, the company was re-incorporated under the same name, absorbing the \"St. Louis Radiator Manufacturing Company\", and the \"Standard Radiator Manufacturing Company of Buffalo\", and the radiator business of the \"Titusville Iron Company\" (Pennsylvania). After the death of Mr. Bond in 1902, Mr. Woolley, at age 39, succeeded him as President and Chairman of the Board.\n\nThe American Radiator Building was constructed in New York in 1924. In the 1920s the company added several manufacturing plants in the US, as well as expanding a distribution network.\n\nIn 1929 the Standard Sanitary Manufacturing Company consolidated with the American Radiator Company to form the American Radiator and Standard Sanitary Corporation.\n\nIn 1894, following the Panic of 1893 and the consequent fall in demand for its products the company began investigating the potential of Europe as a market, and a sales branch was opened in London. In 1897, the company began investigating manufacturing in Europe, specifically France and Germany, countries with high trade tariffs; a branch was opened in Hamburg, which assembled and machine finished cast radiator parts shipped in knock down form. In 1898, the company acquired the established plant of Louis Courtot, in Dôle, France (Dole, Jura) forming the \"Compagnie Nationale de Radiateurs\". In 1901, the company made the decision to establish a manufacturing plant in Germany, a site was selected in Schoenebeck. Both establishments proved successful and in 1905 the company began planning for a factory in England, and a site was selected in Hull.\n\nBy 1906 the European operations were so successful that on one occasion profits generated exceeded those from the company's American operations. Much of the profit was re-invested in expansion, and the construction of new factories was initiated in Italy (1910, opened 1911) and Austria (1912, opened by 1914).\n\nDuring World War I the plant became in involved in war work: the Dôle plant was requisitioned by the French state, and manufactured shells; the German, Italian, and Austrian plants were also involved in producing munitions for their respective states; the English plant agreed to supply Belgium with hand-grenades, and also undertook contracts for the British state.\n\nThe European operations were incorporated into the American Radiator and Standard Sanitary Corporation in 1929, forming the basis of that company's international operations.\n\nIn the 1880s engineer Louis Courtot developed a central heating system using radiators and boiler, and established a foundry in Dole (Jura). The factory was acquired by the American Radiator Company in 1898, and a new company established as the Compagnie Nationale de Radiateurs, with a capital of 500,000 francs. Courtot became the managing director of the plant. The company had with low wage costs and fair productivity, resulting in a saving compared to importing products. By 1903 the plant employed 180 people.\n\nIn 1905 the company opened a new factory in Dole, and the earlier factory gradually ceased production. The factory became the largest in Dole employing nearly 2000. During the First World War the plant produced 155 and 380mm artillery shells, employing mainly female workers. At the end of the war, the shortage of manpower led to managed immigration of polish workers, with a housing built for their families.\n\nDuring the 1920s new plant were constructed at Clichy-sous-Bois/Aulnay-sous-Bois and Argenteuil. As a consequence, in 1932 the plant in Dole switched to the manufacture of bathroom furniture, including porcelain sanitary ware.\n\nThe Aulnay-sur-Bois plant opened in 1923. The two main factory buildings were built of reinforced concrete by the Limousin company to the designs of Eugène Freyssinet. An extension was built in 1930, also by Limousin. Foundry work (boilers, cast iron baths) was switched to Aulnay.\n\nIn 1929 a factory was opened in Dammarie-lès-Lys; the factory structure was also to the design of Freyssinet/Limousin. From 1931, the factory was used entirely for the manufacture of radiators, with a staff of 750.\n\nIn 1949, after the parent company had become involved in the production of bathroom furniture (see American Radiator and Standard Sanitary Corporation, formed 1929) the company was renamed \"Idéal Standard\".\n\nManufacture of radiators in Aulnay ceased in 1968. The oil crisis negatively affected the company's activities, and the plant in Aulnay closed. The Dammarie plant closed in 1975. In 1975, production of bathroom furniture ended at Dole. A new company \"Société Nouvelle Idéal Standard\" was established under the control of Société Générale de Fonderie (65%) and Société de Dietrich. In 1984 the company came back under the control of American Standard. Bath production using acrylic resin started in 1986.\n\nIn 1996 the company Sanifrance was created by the combination of activities of Idéal Standard, Porcher, Piel and Emafrance as a subsidiary of American Standard. In 2005 Sanifrance became \"Idéal Standard France\", and in 2006 \"Idéal Standard Industrie France\".\n\nIn 1900 the American Radiator Company decided to add a foundry to their existing operations in Germany, which had been assembling and finishing imported radiator parts in Hamburg from the late 1890s. In 1901 \"Nationale Radiator Gesellschaft mbH\" was formed in Berlin as a subsidiary of the American Radiator Company; a factory was established at Schönebeck in 1902. The company was known under the abbreviation NARAG.\n\nAn additional factory was established in Neuss and production of radiators concentrated at Neuss, boilers at Schoenebeck. With the incorporation in 1919 of the Standard Sanitary company into the parent company, its German subsidiaries were also merged into a division of NARAG, adding cast iron baths, brass water fittings, and porcelain bathroom fittings to the company's output.\n\nDuring World War I the factory manufactured shells for the German Empire. During the Second World War the Neuss factory was heavily targeted by bombing campaigns and required rebuilding at the end of the war. The Schoenebeck plant became a subsidiary of Volkswagen in 1944 and was used to manufacture V1 rockets; the factory was supplied with slave labour from the Schoenebeck camp, a sub-camp of the Buchenwald concentration camp, an estimated 200-400 concentration camp prisoners were used at the plant, other employees during the period included forced labour from eastern Europe, and Italy.\n\nThe Schoenebeck facility was in soviet occupied East Germany at the end of the second world war; as a result in 1950 the Neuss site began production of boilers. In 1951 the company was renamed Ideal Standard GmbH. A refrigeration company 'Gesellschaft die Rheinkälte' (Düsseldorf) was acquired in 1955. In the 1960s, during the German economic boom the company expanded, with a sites in Wittlich (radiators), and in Waldbröl und Berlin (boilers, radiators; acquired from the Projahn-Werkes).\n\nAt the end of the 1960s the Wittlich site was concentrated on the production of fittings serving the whole European market; valve production was reduced at Neuss in favour of the Clichy factory in France, and the site in Neuss became focused on ceramic manufacture. In the mid 1970s the company withdrew from the heating business, and the sites in Waldbröl and Berlin shut.\n\nIn 1905 the company's operations in France and Germany were proving successful, and the firm decided to open a factory in England. The company \"National Radiator Company, Ltd.\" was established, and $500,000 was provided for the establishment of a factory. A site in Kingston-upon-Hull was selected for the factory.\n\nConstruction of the factory began in 1906, and the first casting was produced in December. The plant was expanded in 1910. During World War I, much of the production was shifted to the munition production. In 1917 the factory was further expanded, in anticipation of a post-war building boom.\n\nIn 1934 the company was publicly listed as \"Ideal Boilers and Radiators\" (capital £750,000) in order to raise cash for a factory extension. By 1938 a new plant producing vitreous sanitary ware had begun operation.\n\nDuring World War II the plant produced munitions including mortar bombs and grenades, as well as boilers and vitreous china for military use.\n\nIn 1953 the company was renamed \"Ideal Standard\".\n\nIn 1976 the boiler and radiator operations of the plant were acquired by Stelrad Group (Metal Box), whilst the vitreous china (bathroom furniture) operations remained under the control of Ideal Standard. Radiator production was ended at Stelrad's Hull site, and production was focused on boilers. In 1989 the Metal Box company demergered, and MB group formed; Caradon was acquired by MB group becoming MB Caradon in 1989 and the Hull boiler factory became Caradon Ideal in 1993.\n\nIn the 2010s the Ideal Boilers' foundry was closed with the loss of 57 jobs and outsource casting to third parties.\n\nAs of 2014 the boiler plant operated as \"Ideal Boilers\" as part of the Ideal Stelrad group., and the bathroom fittings plant is part of Ideal Standard.\n\nIn 2015 Ideal Boilers was acquired from holding company \"ISG Holdings 1\" by French HVAC business Groupe Atlantic.\n\nIn consequence of the radiator's contribution to the lives, and social history of North Americans, in 2012, the American Radiator Company was inducted into the North American Railway Hall of Fame.\n\n"}
{"id": "38177351", "url": "https://en.wikipedia.org/wiki?curid=38177351", "title": "Attack on Baku", "text": "Attack on Baku\n\nAttack on Baku (German: Anschlag auf Baku) is a 1942 German thriller film directed by Fritz Kirchhoff and starring Willy Fritsch, René Deltgen and Fritz Kampers. The film was intended as anti-British propaganda during the Second World War. It is noted for its set designs by Otto Hunte, who showed a fascination for modern technology in his depiction of the oil town. The film was shot on location in German-allied Romania, and at Babelsberg Studio in Berlin.\n\nAzerbaijan, 1919. the British hope to secure control of the vast oil fields around Baku by launching a series of terrorist attacks on them. Hans Romberg, a German who is working as a security officer, battles with the British chief agent Captain Forbes and his associates.\n\n\n"}
{"id": "194422", "url": "https://en.wikipedia.org/wiki?curid=194422", "title": "Ball lightning", "text": "Ball lightning\n\nBall lightning is an unexplained and potentially dangerous atmospheric electrical phenomenon. The term refers to reports of luminous, spherical objects that vary from pea-sized to several meters in diameter. Though usually associated with thunderstorms, the phenomenon lasts considerably longer than the split-second flash of a lightning bolt. Many early reports claim that the ball eventually explodes, sometimes with fatal consequences, leaving behind the odor of sulfur.\n\nUntil the 1960s, most scientists treated reports of ball lightning skeptically, despite numerous accounts from around the world. Laboratory experiments can produce effects that are visually similar to reports of ball lightning, but how these relate to the natural phenomenon remains unclear.\n\nScientists have proposed many hypotheses about ball lightning over the centuries. Scientific data on natural ball-lightning remains scarce, owing to its infrequency and unpredictability. The presumption of its existence depends on reported public sightings, which have produced somewhat inconsistent findings. Owing to inconsistencies and to the lack of reliable data, the true nature of ball lightning remains unknown. The first ever optical spectrum of what appears to have been a ball-lightning event, published in January 2014, included a video at high frame-rate.\n\nIt has been suggested that ball lightning could be the source of the legends that describe luminous balls, such as the mythological Anchimayen from Argentinean and Chilean Mapuche culture.\n\nIn a 1960 study, 5% of the population of the Earth reported having witnessed ball lightning. Another study analyzed reports of 10,000 cases.\n\nOne early account was reported during the Great Thunderstorm at a church in Widecombe-in-the-Moor, Devon, in England, on 21 October 1638. Four people died and approximately 60 were injured when, during a severe storm, an ball of fire was described as striking and entering the church, nearly destroying it. Large stones from the church walls were hurled into the ground and through large wooden beams. The ball of fire allegedly smashed the pews and many windows, and filled the church with a foul sulphurous odour and dark, thick smoke.\n\nThe ball of fire reportedly divided into two segments, one exiting through a window by smashing it open, the other disappearing somewhere inside the church. The explanation at the time, because of the fire and sulphur smell, was that the ball of fire was \"the devil\" or the \"flames of hell\". Later, some blamed the entire incident on two people who had been playing cards in the pew during the sermon, thereby incurring God's wrath.\n\nIn December 1726 a number of British newspapers printed an extract of a letter from John Howell of the sloop \"Catherine and Mary\":\nOne particularly large example was reported \"on the authority of Dr. Gregory\" in 1749:\nAdmiral Chambers on board the \"Montague\", 4 November 1749, was taking an observation just before noon...he observed a large ball of blue fire about three miles distant from them. They immediately lowered their topsails, but it came up so fast upon them, that, before they could raise the main tack, they observed the ball rise almost perpendicularly, and not above forty or fifty yards from the main chains when it went off with an explosion, as great as if a hundred cannons had been discharged at the same time, leaving behind it a strong sulphurous smell. By this explosion the main top-mast was shattered into pieces and the main mast went down to the keel.\n\nFive men were knocked down and one of them very bruised. Just before the explosion, the ball seemed to be the size of a large mill-stone.\nA 1753 report depicts ball lightning as having been lethal when professor Georg Richmann of Saint Petersburg, Russia, created a kite-flying apparatus similar to Benjamin Franklin's proposal a year earlier. Richmann was attending a meeting of the Academy of Sciences when he heard thunder and ran home with his engraver to capture the event for posterity. While the experiment was under way, ball lightning appeared and travelled down the string, struck Richmann's forehead and killed him. The ball had left a red spot on Richmann's forehead, his shoes were blown open, and his clothing was singed. His engraver was knocked unconscious. The door frame of the room was split and the door was torn from its hinges.\n\nAn English journal reported that during an 1809 storm, three \"balls of fire\" appeared and \"attacked\" the British ship \"HMS Warren Hastings\". The crew watched one ball descend, killing a man on deck and setting the main mast on fire. A crewman went out to retrieve the fallen body and was struck by a second ball, which knocked him back and left him with mild burns. A third man was killed by contact with the third ball. Crew members reported a persistent, sickening sulphur smell afterward.\n\nEbenezer Cobham Brewer, in his 1864 US edition of \"A Guide to the Scientific Knowledge of Things Familiar\", discussed \"globular lightning\". He describes it as slow-moving balls of fire or explosive gas that sometimes fall to the earth or run along the ground during a thunderstorm. He said that the balls sometimes split into smaller balls and may explode \"like a cannon\".\n\nIn his book \"Thunder and Lightning\", translated into English in 1875, French science writer Wilfrid de Fonvielle wrote that there had been about 150 reports of globular lightning:\n\nGlobular lightning seems to be particularly attracted to metals; thus it will seek the railings of balconies, or else water or gas pipes etc, It has no peculiar tint of its own but will appear of any colour as the case may be ... at Coethen in the Duchy of Anhalt it appeared green. M. Colon, Vice-President of the Geological Society of Paris, saw a ball of lightning descend slowly from the sky along the bark of a poplar tree; as soon as it touched the earth it bounced up again, and disappeared without exploding. On 10th of September 1845 a ball of lightning entered the kitchen of a house in the village of Salagnac in the valley of Correze. This ball rolled across without doing any harm to two women and a young man who were here; but on getting into an adjoining stable it exploded and killed a pig which happened to be shut up there, and which, knowing nothing about the wonders of thunder and lightning, dared to smell it in the most rude and unbecoming manner.\n\nThe motion of such balls is far from being very rapid – they have even been observed occasionally to pause in their course, but they are not the less destructive for all that. A ball of lightning which entered the church of Stralsund, on exploding, projected a number of balls which exploded in their turn like shells.\nTsar Nicholas II, the last Emperor of Russia, reported witnessing what he called \"a fiery ball\" while in the company of his grandfather, Tsar Alexander II: \"Once my parents were away,\" recounted the Tsar, \"and I was at the all-night vigil with my grandfather in the small church in Alexandria. During the service there was a powerful thunderstorm, streaks of lightning flashed one after the other, and it seemed as if the peals of thunder would shake even the church and the whole world to its foundations. Suddenly it became quite dark, a blast of wind from the open door blew out the flame of the candles which were lit in front of the iconostasis, there was a long clap of thunder, louder than before, and I suddenly saw a fiery ball flying from the window straight towards the head of the Emperor. The ball (it was of lightning) whirled around the floor, then passed the chandelier and flew out through the door into the park. My heart froze, I glanced at my grandfather – his face was completely calm. He crossed himself just as calmly as he had when the fiery ball had flown near us, and I felt that it was unseemly and not courageous to be frightened as I was. I felt that one had only to look at what was happening and believe in the mercy of God, as he, my grandfather, did. After the ball had passed through the whole church, and suddenly gone out through the door, I again looked at my grandfather. A faint smile was on his face, and he nodded his head at me. My panic disappeared, and from that time I had no more fear of storms.\"\n\nBritish occultist Aleister Crowley reported witnessing what he referred to as \"globular electricity\" during a thunderstorm on Lake Pasquaney in New Hampshire in 1916. He was sheltered in a small cottage when he \"noticed, with what I can only describe as calm amazement, that a dazzling globe of electric fire, apparently between six and twelve inches (15–30 cm) in diameter, was stationary about six inches below and to the right of my right knee. As I looked at it, it exploded with a sharp report quite impossible to confuse with the continuous turmoil of the lightning, thunder and hail, or that of the lashed water and smashed wood which was creating a pandemonium outside the cottage. I felt a very slight shock in the middle of my right hand, which was closer to the globe than any other part of my body.\"\n\nJennison, of the Electronics Laboratory at the University of Kent, described his own observation of ball lightning:\n\nI was seated near the front of the passenger cabin of an all-metal airliner (Eastern Airlines Flight EA 539) on a late night flight from New York to Washington. The aircraft encountered an electrical storm during which it was enveloped in a sudden bright and loud electrical discharge (0005 h EST, March 19, 1963). Some seconds after this a glowing sphere a little more than 20 cm in diameter emerged from the pilot's cabin and passed down the aisle of the aircraft approximately 50 cm from me, maintaining the same height and course for the whole distance over which it could be observed. \n\n\nDescriptions of ball lightning vary widely. It has been described as moving up and down, sideways or in unpredictable trajectories, hovering and moving with or against the wind; attracted to, unaffected by, or repelled from buildings, people, cars and other objects. Some accounts describe it as moving through solid masses of wood or metal without effect, while others describe it as destructive and melting or burning those substances. Its appearance has also been linked to power lines, altitudes of and higher, and during thunderstorms and calm weather. Ball lightning has been described as transparent, translucent, multicolored, evenly lit, radiating flames, filaments or sparks, with shapes that vary between spheres, ovals, tear-drops, rods, or disks.\n\nBall lightning is often erroneously identified as St. Elmo's fire. They are separate and distinct phenomena.\n\nThe balls have been reported to disperse in many different ways, such as suddenly vanishing, gradually dissipating, absorption into an object, \"popping,\" exploding loudly, or even exploding with force, which is sometimes reported as damaging. Accounts also vary on their alleged danger to humans, from lethal to harmless.\n\nA review of the available literature published in 1972 identified the properties of a \"typical\" ball lightning, whilst cautioning against over-reliance on eye-witness accounts:\n\nIn January 2014, scientists from Northwest Normal University in Lanzhou, China, published the results of recordings made in July 2012 of the optical spectrum of what was thought to be natural ball lightning made by chance during the study of ordinary cloud–ground lightning on the Tibetan Plateau. At a distance of , a total of 1.64 seconds of digital video of the ball lightning and its spectrum was made, from the formation of the ball lightning after the ordinary lightning struck the ground, up to the optical decay of the phenomenon. Additional video was recorded by a high-speed (3000 frames/sec) camera, which captured only the last 0.78 seconds of the event, due to its limited recording capacity. Both cameras were equipped with slitless spectrographs. The researchers detected emission lines of neutral atomic silicon, calcium, iron, nitrogen, and oxygen—in contrast with mainly ionized nitrogen emission lines in the spectrum of the parent lightning. The ball lightning traveled horizontally across the video frame at an average speed equivalent of . It had a diameter of and covered a distance of about within those 1.64 s.\n\nOscillations in the light intensity and in the oxygen and nitrogen emission at a frequency of 100 hertz, possibly caused by the electromagnetic field of the 50 Hz high-voltage power transmission line in the vicinity, were observed. From the spectrum, the temperature of the ball lightning was assessed as being lower than the temperature of the parent lightning (<). The observed data are consistent with vaporization of soil as well as with ball lightning's sensitivity to electric fields.\n\nScientists have long attempted to produce ball lightning in laboratory experiments. While some experiments have produced effects that are visually similar to reports of natural ball lightning, it has not yet been determined whether there is any relation.\n\nNikola Tesla reportedly could artificially produce balls and conducted some demonstrations of his ability, but he was truly interested in higher voltages and powers, and remote transmission of power, so the balls he made were just a curiosity.\n\nThe International Committee on Ball Lightning (ICBL) held regular symposia on the subject. A related group uses the generic name \"Unconventional Plasmas\". The last ICBL symposium was tentatively scheduled for July 2012 in San Marcos, Texas but was cancelled due to a lack of submitted abstracts.\n\nOhtsuki and Ofuruton described producing “plasma fireballs” by microwave interference within an air-filled cylindrical cavity fed by a rectangular waveguide using a 2.45 GHz, 5 kW (maximum power) microwave oscillator.\n\nSome scientific groups, including the Max Planck Institute, have reportedly produced a ball lightning-type effect by discharging a high-voltage capacitor in a tank of water.\n\nMany modern experiments involve using a microwave oven to produce small rising glowing balls, often referred to as \"plasma balls\".\nGenerally, the experiments are conducted by placing a lit or recently extinguished match or other small object in a microwave oven. The burnt portion of the object flares up into a large ball of fire, while \"plasma balls\" float near the oven chamber ceiling. Some experiments describe covering the match with an inverted glass jar, which contains both the flame and the balls so that they don't damage the chamber walls. (A glass jar, however, eventually explodes rather than simply causing charred paint or melting metal, as happens to the inside of a microwave.) Experiments by Eli Jerby and Vladimir Dikhtyar in Israel revealed that microwave plasma balls are made up of nanoparticles with an average radius of . The Israeli team demonstrated the phenomenon with copper, salts, water and carbon.\n\nExperiments in 2007 involved shocking silicon wafers with electricity, which vaporizes the silicon and induces oxidation in the vapors. The visual effect can be described as small glowing, sparkling orbs that roll around a surface. Two Brazilian scientists, Antonio Pavão and Gerson Paiva of the Federal University of Pernambuco have reportedly consistently made small long-lasting balls using this method. These experiments stemmed from the theory that ball lightning is actually oxidized silicon vapors \"(see vaporized silicon hypothesis, below)\".\n\nThere is at present no widely accepted explanation for ball lightning. Several hypotheses have been advanced since the phenomenon was brought into the scientific realm by the English physician and electrical researcher William Snow Harris in 1843, and French Academy scientist François Arago in 1855.\n\nThis hypothesis suggests that ball lightning consists of vaporized silicon burning through oxidation. Lightning striking Earth's soil could vaporize the silica contained within it, and somehow separate the oxygen from the silicon dioxide, turning it into pure silicon vapor. As it cools, the silicon could condense into a floating aerosol, bound by its charge, glowing due to the heat of silicon recombining with oxygen. An experimental investigation of this effect, published in 2007, reported producing \"luminous balls with lifetime in the order of seconds\" by evaporating pure silicon with an electric arc. Videos and spectrographs of this experiment have been made available. This hypothesis got significant supportive data in 2014, when the first ever recorded spectra of natural ball lightning were published. The theorized forms of silicon storage in soil include nanoparticles of Si, SiO, and SiC.\nMatthew Francis has dubbed this the \"dirt clod hypothesis\", in which the spectrum of ball lightning shows that it shares chemistry with soil.\n\nIn this model ball lightning is assumed to have a solid, positively charged core. According to this underlying assumption, the core is surrounded by a thin electron layer with a charge nearly equal in magnitude to that of the core. A vacuum exists between the core and the electron layer containing an intense electromagnetic (EM) field, which is reflected and guided by the electron layer. The microwave EM field applies a ponderomotive force (radiation pressure) to the electrons preventing them from falling into the core.\n\nPyotr Kapitsa proposed that ball lightning is a glow discharge driven by microwave radiation that is guided to the ball along lines of ionized air from lightning clouds where it is produced. The ball serves as a resonant microwave cavity, automatically adjusting its radius to the wavelength of the microwave radiation so that resonance is maintained.\n\nThe Handel Maser-Soliton theory of ball lightning hypothesizes that the energy source generating the ball lightning is a large (several cubic kilometers) atmospheric maser. The ball lightning appears as a plasma caviton at the antinodal plane of the microwave radiation from the maser.\n\nJulio Rubinstein, David Finkelstein, and James R. Powell proposed that ball lightning is a detached St. Elmo's fire (1964–1970). St. Elmo's fire arises when a sharp conductor, such as a ship's mast, amplifies the atmospheric electric field to breakdown. For a globe the amplification factor is 3. A free ball of ionized air can amplify the ambient field this much by its own conductivity. When this maintains the ionization, the ball is then a soliton in the flow of atmospheric electricity.\n\nPowell's kinetic theory calculation found that the ball size is set by the second Townsend coefficient (the mean free path of conduction electrons) near breakdown. Wandering glow discharges are found to occur within certain industrial microwave ovens and continue to glow for several seconds after power is shut off. Arcs drawn from high-power low-voltage microwave generators also are found to exhibit afterglow. Powell measured their spectra, and found that the after-glow comes mostly from metastable NO ions, which are long-lived at low temperatures. It occurred in air and in nitrous oxide, which possess such metastable ions, and not in atmospheres of argon, carbon dioxide, or helium, which do not.\n\nThe soliton model of a ball lightning was further developed. It was suggested that a ball lightning is based on spherically symmetric nonlinear oscillations of charged particles in plasma – the analogue of a spatial Langmuir soliton. These oscillations were described in both classical and quantum approaches. It was found that the most intense plasma oscillations occur in the central regions of a ball lightning. It is suggested that bound states of radially oscillating charged particles with oppositely oriented spins – the analogue of Cooper pairs – can appear inside a ball lightning. This phenomenon, in its turn, can lead to a superconducting phase in a ball lightning. The idea of the superconductivity in a ball lightning was considered earlier. The possibility of the existence of a ball lightning with a composite core was also discussed in this model.\n\nPhysicist Domokos Tar suggested the following theory for ball lightning formation based on his ball lightning observation. Lightning strikes perpendicular to the ground, and thunder follows immediately at supersonic speed in the form of shock waves that form an invisible aerodynamic turbulence ring horizontal to the ground. Around the ring, over and under pressure systems rotate the vortex around a circular axis in the cross section of the torus. At the same time, the ring expands concentrically parallel to the ground at low speed.\n\nIn an open space, the vortex fades and finally disappears. If the vortex's expansion is obstructed, and symmetry is broken, the vortex breaks into cyclical form. Still invisible, and due to the central and surface tension-forces, it shrinks to an intermediate state of a cylinder, and finally into a ball. The resulting transformation subsequently becomes visible once the energy is concentrated into the final spherical stage.\n\nThe ball lightning has the same rotational axis as the rotating cylinder. As the vortex has a much smaller vector of energy compared to the overall energy of the reactant sonic shock wave, its vector is likely fractional to the overall reaction. The vortex, during contraction, gives the majority of its energy to form the ball lightning, achieving nominal energy loss.\n\nIn some observations, the ball lightning appeared to have an extremely high energy concentration but this phenomenon hasn't been adequately verified. The present theory concerns only the low energy lightning ball form, with centripetal forces and surface tension. The visibility of the ball lightning can be associated with electroluminescence, a direct result of the triboelectric effect from materials within the area of the reaction. Static discharge from the cylindrical stage imply the existence of contact electrification within the object. The direction of the discharges indicate the cylinder's rotation, and resulting rotational axis of the ball lightning in accordance to the law of laminar flow. If the ball came from the channel, it would have rotated in the opposite direction.\n\nOne theory that may account for the wide spectrum of observational evidence is the idea of combustion inside the low-velocity region of spherical vortex breakdown of a natural vortex (e.g., the 'Hill's spherical vortex').\n\nOleg Meshcheryakov suggests that ball lightning is made of composite nano or submicrometre particles—each particle constituting a battery. A surface discharge shorts these batteries, causing a current that forms the ball. His model is described as an aerosol model that explains all the observable properties and processes of ball lightning.\n\nAnother hypothesis is that some ball lightning is the passage of microscopic primordial black holes through the Earth's atmosphere. This possibility was mentioned parenthetically by Leo Vuyk in 1992 in a patent application and a second patent application in 1996 by Leendert Vuyk. The first detailed scientific analysis was published by Mario Rabinowitz in Astrophysics and Space Science journal in 1999.\n\nThe declassified Project Condign report concludes that buoyant charged plasma formations similar to ball lightning are formed by novel physical, electrical, and magnetic phenomena, and that these charged plasmas are capable of being transported at enormous speeds under the influence and balance of electrical charges in the atmosphere. These plasmas appear to originate due to more than one set of weather and electrically charged conditions, the scientific rationale for which is incomplete or not fully understood. One suggestion is that meteors breaking up in the atmosphere and forming charged plasmas as opposed to burning completely or impacting as meteorites could explain some instances of the phenomena, in addition to other unknown atmospheric events.\n\nCooray and Cooray (2008) stated that the features of hallucinations experienced by patients having epileptic seizures in the occipital lobe are similar to the observed features of ball lightning. The study also showed that the rapidly changing magnetic field of a close lightning flash is strong enough to excite the neurons in the brain. This strengthens the possibility of lightning-induced seizure in the occipital lobe of a person close to a lightning strike, establishing the connection between epileptic hallucination mimicking ball lightning and thunderstorms.\n\nMore recent research with transcranial magnetic stimulation has been shown to give the same hallucination results in the laboratory (termed magnetophosphenes), and these conditions have been shown to occur in nature near lightning strikes.\nThis hypothesis fails to explain observed physical damage caused by ball lightning or simultaneous observation by multiple witnesses. (At the very least, observations would differ substantially.)\n\nTheoretical calculations from University of Innsbruck researchers suggest that the magnetic fields involved in certain types of lightning strikes could potentially induce visual hallucinations resembling ball lightning. Such fields, which are found within close distances to a point in which multiple lightning strikes have occurred over a few seconds, can directly cause the neurons in the visual cortex to fire, resulting in magnetophosphenes (magnetically induced visual hallucinations).\n\nSeward proposes that ball lightning is a spinning plasma toroid or ring. He built a lab that produces lightning level arcs, and by modifying the conditions he produced bright, small balls that mimic ball lightning and persist in atmosphere after the arc ends. Using a high speed camera he was able to show that the bright balls were spinning plasma toroids.\n\nChen was able to derive the physics and found that there is a class of plasma toroids that remain stable with or without an external magnetic containment, a new plasma configuration unlike anything reported elsewhere.\n\nSeward published images of the results of his experiments, along with his method. Included is a report by a farmer of observing a ball lightning event forming in a kitchen and the effects it caused as it moved around the kitchen. This is the only eye witness account of ball lightning forming, then staying in one area, then ending that the author has heard of.\n\nManykin et al. have suggested atmospheric Rydberg matter as an explanation of ball lightning phenomena. Rydberg matter is a condensed form of highly excited atoms in many aspects similar to electron-hole droplets in semiconductors. However, in contrast to electron-hole droplets, Rydberg matter has an extended life-time—as long as hours. This condensed excited state of matter is supported by experiments, mainly of a group led by Holmlid. It is similar to a liquid or solid state of matter with extremely low (gas-like) density. Lumps of atmospheric Rydberg matter can result from condensation of highly excited atoms that form by atmospheric electrical phenomena, mainly due to linear lightning. Stimulated decay of Rydberg matter clouds can, however, take the form of an avalanche, and so appear as an explosion.\n\nNikola Tesla's (1899 December) theorized that the balls were formed inside a gas that was highly rarefied.\n\nOn the formation\n\"A single powerful streamer, breaking out from a well insulated terminal, may easily convey a current of several hundred amperes! No wonder then, that a small mass of air is exploded with an effect similar to that of a bombshell...\"\n\"..let us now assume that such a powerful streamer or spark discharge, in its passage through the air, happens to come upon a vacuous sphere or space formed in the manner described. This space, containing gas highly rarefied, may be just in the act of contracting, at any rate, the intense current, passing through the rarefied gas suddenly raises the same to an extremely high temperature, all the higher as the mass of the gas is very small.\nBut although the gas may have been brought to vivid incandescence, yet its pressure may not be very great. If, upon the sudden passage of the discharge, the pressure of the heated air exceeds that of the air around, the luminous ball or space will expand, but most generally it may not do so. For assume, for instance, that the air in the vacuous space was at one hundredth say, of its normal pressure, which might well be the case, then, since the pressure in the space would be as the absolute temperature of the gas within, it would require a temperature which seems scarcely realizable, to raise the pressure of the rarefied gas to the normal air pressure.\"But how is it when the second discharge and possibly many subsequent ones pass through the rarefied gas? These discharges find the gas already expanded and in a condition to take up much more energy by reason of the properties it acquires through rarefaction. Evidently, the energy consumption in any given part of the path of the streamer or spark discharge is, under otherwise the same conditions, proportionate to the resistance of that part of the path; and since, after the gas has once broken down, the resistance of other parts of the path of the discharge is much smaller than that including the vacuous space, a comparatively very great energy consumption must necessarily take place in this portion of the current path.\"\n\nOn the duration \n\"Here, then, is a mass of gas heated to high incandescence suddenly but not, as before, in a condition to give up heat rapidly. It can not cool down rapidly by expansion, as when the vacuous space was being formed, nor can it give off much heat by convection. To some extent even radiation is diminished. On the contrary, despite the high temperature, it is compelled to confinement in a limited space which is continuously shrinking instead of expanding. All these causes cooperate in maintaining, for a comparatively long period of time, the gas confined in this space at an elevated temperature, in a state of high incandescence, in the case under consideration. Thus it is that the phenomenon of the ball is produced and the same made to persist for a perceptible fraction or interval of time.\"\n\nOn the movement \n\"As might be expected, the incandescent mass of gas in a medium violently agitated, could not possibly remain in the same place but will be, as a rule, carried, in some direction or other, by the currents of the air. Upon little reflection, however, we are led to the conclusion that the ball or incandescent mass, of whatever shape it be, will always move from the place where an explosion occurred first, to some place where such an explosion occurred later. In fact, all observers concur in the opinion that such a fireball moves slowly.\"\n\nOn the explosion \n\"If we interpret the nature of this wonderful phenomenon in this manner, we shall find it quite natural that when such a ball encounters in its course an object, as a piece of organic matter for instance, it will raise the same to a high temperature, thus liberating suddenly a great quantity of gas by evaporating or volatilizing the substance with the result of being itself dissipated or exploded. Obviously, also, it may be expected that the conducting mass of the ball originated as described, and moving through a highly insulating medium, will be likely to be highly electrified, which accords with many of the observations made.\"\n\nSeveral other hypotheses have been proposed to explain ball lightning:\n\n\n"}
{"id": "1706048", "url": "https://en.wikipedia.org/wiki?curid=1706048", "title": "Battery room", "text": "Battery room\n\nA battery room is a room in a facility used to house batteries for backup or uninterruptible power systems. Battery rooms are found in telecommunication central offices, and to provide standby power to computing equipment in datacenters. Batteries provide direct current (DC) electricity, which may be used directly by some types of equipment, or which may be converted to alternating current (AC) by uninterruptible power supply (UPS) equipment. The batteries may provide power for minutes, hours or days depending on the electrical system design, although most commonly the batteries power the UPS during brief electric utility outages lasting only seconds. \n\nBattery rooms were used to segregate the fumes and corrosive chemicals of wet cell batteries (often lead–acid) from the operating equipment; a separate room also allowed better control of temperature and ventilation for the batteries. In 1890 the Western Union central telegraph office in New York City had 20,000 wet cells, mostly primary zinc-copper type, in use.\n\nTelephone system central offices contain large battery systems to provide power for customer telephones, telephone switches, and related apparatus. Terrestrial microwave links, cellular telephone sites, fibre optic apparatus and satellite communications facilities also have standby battery systems, which may be large enough to occupy a separate room in the building. In normal operation power from the local commercial utility operates telecommunication equipment, and batteries provide power if the normal supply is interrupted. These can be sized for the expected full duration of an interruption, or may be required only to provide power while a standby generator set or other emergency power supply is started. \n\nBatteries often used in battery rooms are the flooded lead-acid battery, the valve regulated lead-acid battery or the nickel–cadmium battery. Batteries are installed in groups. Several batteries are wired together in a series circuit forming a group providing DC electric power at 12, 24, 48 or 60 volts (or higher). Usually there are two or more groups of series-connected batteries. These groups of batteries are connected in a parallel circuit. This arrangement allows an individual group of batteries to be taken offline for service or replacement without compromising the availability of uninterruptible power. Generally, the larger the battery room's electrical capacity, the larger the size of each individual battery and the higher the room's DC voltage.\n\nBattery rooms are also found in electric power plants and substations where reliable power is required for operation of switchgear, critical standby systems, and possibly black start of the station. Often batteries for large switchgear line-ups are 125 V or 250 V nominal systems, and feature redundant battery chargers with independent power sources. Separate battery rooms may be provided to protect against loss of the station due to a fire in a battery bank. For stations that are capable of black start, power from the battery system may be required for many purposes including switchgear operations.\n\nVery large utility batteries may be used for grid energy storage.\n\nBattery rooms are found on diesel-electric submarines, where they contain the lead-acid batteries used for undersea propulsion of the vessel. Even nuclear submarines contain large battery rooms as backups to provide maneuvering power if the nuclear reactor is shutdown. Batteries in surface vessels may also be contained in a battery room.\n\nBattery rooms on ocean-going vessels must prevent seawater from contacting battery acid, as this could produce toxic chlorine gas.\nThis is of particular concern on submarines.\n\nSince several types of secondary batteries give off hydrogen if overcharged, ventilation of a battery room is critical to maintain the concentration below the lower explosive limit. The number of air changes per hour required to prevent unsafe accumulation can be calculated from the number of cells and the charging current, given the chemistry of the battery. \n\nThe life span of secondary batteries is reduced at high temperature and the energy storage capacity is reduced at low temperature, so a battery room must have heating or cooling to maintain the proper temperature.\n\nBatteries may contain large quantities of corrosive electrolytes such as sulfuric acid used in lead-acid batteries or caustic potash (aka potassium hydroxide) used in NiCad batteries. Materials of the battery room must resist corrosion and contain any accidental spills. Plant personnel must be protected from spilled electrolyte. In some jurisdictions, large battery systems may contain reportable amounts of sulfuric acid, a concern for fire departments. Battery rooms in industrial and utility installations typically have an eye-wash station or decontamination showers nearby, so that workers who are accidentally splashed with electrolyte can immediately wash it away from the eyes and skin.\n\n\n"}
{"id": "23693464", "url": "https://en.wikipedia.org/wiki?curid=23693464", "title": "Bilibino Nuclear Power Plant", "text": "Bilibino Nuclear Power Plant\n\nThe Bilibino Nuclear Power Plant ( []) is a power plant in Bilibino, Chukotka Autonomous Okrug, Russia. The plant is equipped with four EGP-6 reactors. The plant is the smallest and the northernmost operating nuclear power plant in the world. Plans to begin a shutdown procedure of the plant in 2019 have been announced, and it will be replaced by the floating nuclear power station \"Akademik Lomonosov\".\n\n"}
{"id": "10794747", "url": "https://en.wikipedia.org/wiki?curid=10794747", "title": "Blowing snow", "text": "Blowing snow\n\nBlowing snow is snow lifted from the surface by the wind, at a height of 8 feet (2 metres) or more, that will reduce visibility. Blowing snow can come from falling snow or snow that already accumulated on the ground but is picked up and blown about by strong winds. It is one of the classic requirements for a blizzard. Its METAR code is BLSN. If the snow remains below 8 feet (2 m), it will be called Drifting snow (METAR code DRSN). The snow which is being blown about may deposit as snowdrifts.\n\nThere are 3 ways of producing blowing snow:\n\n"}
{"id": "3764060", "url": "https://en.wikipedia.org/wiki?curid=3764060", "title": "Camber (aerodynamics)", "text": "Camber (aerodynamics)\n\nIn aeronautics and aeronautical engineering, camber is the asymmetry between the two acting surfaces of an aerofoil, with the top surface of a wing (or correspondingly the front surface of a propeller blade) commonly being more convex (\"positive camber\"). An aerofoil that is not cambered is called a \"symmetric aerofoil\". The benefits of cambering were discovered and first utilized by Sir George Cayley in the early 19th century.\n\nCamber is usually designed into an aerofoil to increase the maximum lift coefficient. This minimizes the stalling speed of aircraft using the aerofoil. An aircraft with wings based on a cambered aerofoil will have a lower stalling speed than an aircraft with a similar wing loading and wings based on a symmetric aerofoil.\n\nAn aircraft designer may also reduce the camber of the outboard section of the wings to increase the critical angle of attack (stall angle) at the wing tips. When the wing approaches the stall this will ensure that the wing root stalls before the tip, giving the aircraft resistance to spinning and maintaining aileron effectiveness close to the stall.\n\nSome recent designs use negative camber. One such design is called the supercritical aerofoil. It is used for near-supersonic flight, and produces a higher lift-to-drag ratio at near supersonic flight than traditional aerofoils. Supercritical aerofoils employ a flattened upper surface, highly cambered (curved) aft section, and greater leading-edge radius as compared to traditional aerofoil shapes. These changes delay the onset of wave drag.\n\nBroadly, an aerofoil is said to have positive camber if, as is commonly the case, its upper surface (or in the case of a propeller or turbine blade its forward surface) is the more convex. But camber is a complex property, that can be more fully characterized by an aerofoil's camber line, the curve \"Z(x)\" that is halfway between the upper and lower surfaces, and thickness function \"T(x)\", which describes the thickness of the aerofoil at any given point. Then, the upper and lower surfaces can be defined as follows:\n\nAn aerofoil where the camber line curves back up near the trailing edge is called a reflexed camber aerofoil. Such an aerofoil is useful in certain situations, such as with tailless aircraft, because the moment about the aerodynamic center of the aerofoil can be 0. A camber line for such an aerofoil can be defined as follows (\"note that the lines over the variables indicates that they have been nondimensionalized by dividing through by the chord\"):\n\nAn aerofoil with a reflexed camber line is shown at right. The thickness distribution for a NACA 4-series aerofoil was used, with a 12% thickness ratio. The equation for this thickness distribution is:\n\nWhere \"t\" is the thickness ratio.\n\n\n"}
{"id": "41710046", "url": "https://en.wikipedia.org/wiki?curid=41710046", "title": "Campos Basin oil spill", "text": "Campos Basin oil spill\n\nOn November 7, 2011, a Chevron owned oil well began leaking causing of crude oil to enter the ocean every day. The leak took place in Campos Basin, Brazil off the coast of Rio de Janeiro. At first, Chevron claimed that the leak was most likely due to a seep in the ocean floor but later admitted that they had made a miscalculation. Chevron says that they underestimated the amount pressure that the reservoir would exert on the oil well and says that heavier mud should have been used to seal the well. of oil were spilled over the course of four days until the well was finally sealed.\n\nAfter the oil spill, 18 ships were sent out into the ocean to clean the spill. There was little documented environmental impact, and the oil never reached the shores of Rio. Following the spill, Brazilian prosecutors filed a lawsuit for 40 billion reais ($18 billion), but in September 2013, Chevron and Brazilian officials reached a settlement of 300 million reais ($135 million).\n\n"}
{"id": "35987921", "url": "https://en.wikipedia.org/wiki?curid=35987921", "title": "Caroline Cannon", "text": "Caroline Cannon\n\nCaroline Cannon is an American environmentalist from Point Hope, Alaska. She was awarded the Goldman Environmental Prize in 2012, for her fight for protection of marine ecosystems against pollution from the petroleum industry.\n"}
{"id": "2589713", "url": "https://en.wikipedia.org/wiki?curid=2589713", "title": "Chernobyl disaster", "text": "Chernobyl disaster\n\nThe Chernobyl disaster, also referred to as the Chernobyl accident, was a catastrophic nuclear accident. It occurred on 25–26 April 1986 in the No. 4 light water graphite moderated reactor at the Chernobyl Nuclear Power Plant near the now-abandoned town of Pripyat, in northern Ukrainian Soviet Socialist Republic, Soviet Union, approximately north of Kiev.\n\nThe event occurred during a late-night safety test which simulated a station blackout power-failure, in the course of which safety systems were intentionally turned off. A combination of inherent reactor design flaws and the reactor operators arranging the core in a manner contrary to the checklist for the test, eventually resulted in uncontrolled reaction conditions. Water flashed into steam generating a destructive steam explosion and a subsequent open-air graphite fire. This fire produced considerable updrafts for about nine days. These lofted plumes of fission products into the atmosphere. The estimated radioactive inventory that was released during this very hot fire phase approximately equaled in magnitude the airborne fission products released in the initial destructive explosion.\nThis radioactive material precipitated onto parts of the western USSR and Europe.\n\nDuring the accident, steam-blast effects caused two deaths within the facility: one immediately after the explosion, and the other compounded by a lethal dose of radiation.\nOver the coming days and weeks, 134 servicemen were hospitalized with acute radiation sickness (ARS), of which 28 firemen and employees died in the days-to-months afterward from the radiation effects.\nAdditionally, approximately fourteen radiation induced cancer deaths among this group of 134 hospitalized survivors were to follow within the next ten years (1996).\nAmong the wider population, an excess of 15 childhood thyroid cancer deaths were documented .\nIt will take further time and investigation to definitively determine the elevated relative risk of cancer among the surviving employees, those that were initially hospitalized with ARS, and the population at large.\n\nThe Chernobyl accident is considered the most disastrous nuclear power plant accident in history, both in terms of cost and casualties.\nIt is one of only two nuclear energy accidents classified as a level 7 event (the maximum classification) on the International Nuclear Event Scale, the other being the Fukushima Daiichi nuclear disaster in Japan in 2011. The struggle to safeguard against scenarios which were perceived as having the potential for greater catastrophe, together with later decontamination efforts of the surroundings, ultimately involved over 500,000 workers and cost an estimated 18 billion rubles.\n\nThe remains of the No. 4 reactor building were enclosed in a large cover which was named the \"Object Shelter\", often known as the sarcophagus. The purpose of the structure was to reduce the spread of the remaining radioactive dust and debris from the wreckage and the protection of the wreckage from further weathering.\nThe sarcophagus was finished in December 1986 at a time when what was left of the reactor was entering the cold shut-down phase.\nThe enclosure was not intended as a radiation shield, but was built quickly as occupational safety for the crews of the other undamaged reactors at the power station, with No. 3 continuing to produce electricity up into 2000.\n\nThe accident motivated safety upgrades on all remaining Soviet-designed reactors in the RBMK (Chernobyl No. 4) family, of which eleven continued to power electric grids .\n\nThe disaster began during a systems test on 26 April 1986 at reactor 4 of the Chernobyl plant near Pripyat and in proximity to the administrative border with Belarus and the Dnieper River. There was a sudden and unexpected power surge. When operators attempted an emergency shutdown, a much larger spike in power output occurred. This second spike led to a reactor vessel rupture and a series of steam explosions. These events exposed the graphite moderator of the reactor to air, causing it to ignite. For the next week, the resulting fire sent long plumes of highly radioactive fallout into the atmosphere over an extensive geographical area, including Pripyat. The plumes drifted over large parts of the western Soviet Union and Europe. According to official post-Soviet data, about 60% of the fallout landed in Belarus.\n\nThirty-six hours after the accident, Soviet officials enacted a 10-kilometre exclusion zone, which resulted in the rapid evacuation of 49,000 people primarily from Pripyat, the nearest large population centre. Although not communicated at the time, an immediate evacuation of the town following the accident was not advisable as the road leading out of the town had heavy nuclear fallout hotspots deposited on it. Initially, the town itself was comparatively safe due to the favourable wind direction. Until the winds began to change direction, shelter in place was considered the best safety measure for the town.\n\nDuring the accident the wind changed direction; the fact that the different plumes from the reactor had different ratios of radioisotopes in them indicates that the relative release rates of different elements from the accident site was changing.\n\nAs plumes and subsequent fallout continued to be generated, the evacuation zone was increased from 10 to 30 km about one week after the accident. A further 68,000 persons were evacuated, including from the town of Chernobyl itself. The surveying and detection of isolated fallout hotspots outside this zone over the following year eventually resulted in 135,000 long-term evacuees in total agreeing to be moved. The near tripling in the total number of permanently resettled persons between 1986 and 2000 from the most severely contaminated areas to approximately 350,000 is regarded as largely political in nature, with the majority of the rest evacuated in an effort to redeem loss in trust in the government, which was most common around 1990. Many thousands of these evacuees would have been \"better off staying home.\" Risk analysis in 2007, supported by DNA biomarkers, has determined that the \"people still living unofficially in the abandoned lands around Chernobyl\" have a lower risk of dying as a result of the elevated doses of radiation in the rural areas than \"if they were exposed to the air pollution health risk in a large city such as nearby Kiev.\"\n\nIn 2017 Philip Thomas, Professor of Risk Management at the University of Bristol, used the years of potential life lost metric to conclude that \"Relocation was unjustified for 75% of the 335,000 people relocated after Chernobyl\", finding that just 900 people among the 220,000 relocated during the second evacuation would have lost 3 months of life expectancy by staying home and that \"none should have been asked to leave\". For comparison, Thomas found that the average resident of London, a city of ~8 million, loses 4.5 months of life due to air pollution.\n\nRussia, Ukraine, and Belarus have been burdened with the continuing and substantial decontamination and monthly compensation costs of the Chernobyl accident. Although certain initiatives are legitimate, as Kalman Mizsei, the director of the UN Development Program, noted, \"an industry has been built on this unfortunate event,\" with a \"vast interest in creating a false picture.\"\nThe accident raised the already heightened concerns about fission reactors worldwide, and while most concern was focused on those of the same unusual design, hundreds of disparate electric-power reactor proposals, including those under construction at Chernobyl, reactor No.5 and 6, were eventually cancelled. With the worldwide issue generally being due to the ballooning in costs for new nuclear reactor safety system standards and the legal costs in dealing with the increasingly hostile/anxious public opinion, there was a precipitous drop in the rate of new startups after 1986.\n\nThe accident also raised concerns about the cavalier safety culture in the Soviet nuclear power industry, slowing industry growth and forcing the Soviet government to become less secretive about its procedures. The government coverup of the Chernobyl disaster was a catalyst for glasnost, which \"paved the way for reforms leading to the Soviet collapse\".\n\nOn 26 April 1986, at 01:23 (UTC+3), reactor four suffered a catastrophic power increase, leading to explosions in its core. As the reactor had not been encased by any kind of hard containment vessel, this dispersed large quantities of radioactive isotopes into the atmosphere and caused an open-air fire that increased the emission of radioactive particles carried by the smoke. The accident occurred during an experiment scheduled to test the viability of a potential safety emergency core cooling feature, which required a normal reactor shutdown procedure.\n\nIn steady state operation, a significant fraction (over 6%) of the power from a nuclear reactor is derived not from fission but from the decay heat of its accumulated fission products. This heat continues for some time after the chain reaction is stopped (e.g., following an emergency SCRAM) and active cooling may be required to prevent core damage. RBMK reactors like those at Chernobyl use water as a coolant. Reactor 4 at Chernobyl consisted of about 1,600 individual fuel channels, each of which required coolant flow of 28 metric tons () per hour.\n\nSince cooling pumps require electricity to cool a reactor after a SCRAM, in the event of a power grid failure, Chernobyl's reactors had three backup diesel generators; these could start up in 15 seconds, but took 60–75 seconds to attain full speed and reach the 5.5megawatt (MW) output required to run one main pump.\n\nTo solve this one-minute gapconsidered an unacceptable safety riskit had been theorized that rotational energy from the steam turbine (as it wound down under residual steam pressure) could be used to generate the required electrical power. Analysis indicated that this residual momentum and steam pressure might be sufficient to run the coolant pumps for 45 seconds, bridging the gap between an external power failure and the full availability of the emergency generators.\n\nThis capability still needed to be confirmed experimentally, and previous tests had ended unsuccessfully. An initial test carried out in 1982 indicated that the excitation voltage of the turbine-generator was insufficient; it did not maintain the desired magnetic field after the turbine trip. The system was modified, and the test was repeated in 1984 but again proved unsuccessful. In 1985, the tests were attempted a third time but also yielded negative results. The test procedure would be repeated in 1986, and it was scheduled to take place during the maintenance shutdown of Reactor Four.\n\nThe test focused on the switching sequences of the electrical supplies for the reactor. The test procedure was expected to begin with an automatic emergency shutdown. No detrimental effect on the safety of the reactor was anticipated, so the test programme was not formally coordinated with either the chief designer of the reactor (NIKIET) or the scientific manager. Instead, it was approved only by the director of the plant (and even this approval was not consistent with established procedures).\n\nAccording to the test parameters, the thermal output of the reactor should have been \"no lower\" than 700 MW at the start of the experiment. If test conditions had been as planned, the procedure would almost certainly have been carried out safely; the eventual disaster resulted from attempts to boost the reactor output once the experiment had been started, which was inconsistent with approved procedure.\n\nThe Chernobyl power plant had been in operation for two years without the capability to ride through the first 60–75 seconds of a total loss of electric power, and thus lacked an important safety feature. The station managers presumably wished to correct this at the first opportunity, which may explain why they continued the test even when serious problems arose, and why the requisite approval for the test had not been sought from the Soviet nuclear oversight regulator (even though there was a representative at the complex of four reactors).\n\nThe experimental procedure was intended to run as follows:\n\nThe conditions to run the test were established before the day shift of 25 April 1986. The day-shift workers had been instructed in advance and were familiar with the established procedures. A special team of electrical engineers was present to test the new voltage regulating system. As planned, a gradual reduction in the output of the power unit was begun at 01:06 on 25 April, and the power level had reached 50% of its nominal 3200 MW thermal level by the beginning of the day shift.\n\nAt this point, another regional power station unexpectedly went offline, and the Kiev electrical grid controller requested that the further reduction of Chernobyl's output be postponed, as power was needed to satisfy the peak evening demand. The Chernobyl plant director agreed, and postponed the test. Despite this delay, preparations for the test not affecting the reactor's power were carried out, including the disabling of the emergency core cooling system or ECCS, a passive/active system of core cooling intended to provide water to the core in a loss-of-coolant accident. Given the other events that unfolded, the system would have been of limited use, but its disabling as a \"routine\" step of the test is an illustration of the inherent lack of attention to safety for this test. In addition, had the reactor been shut down for the day as planned, it is possible that more preparation would have been taken in advance of the test.\n\nAt 23:04, the Kiev grid controller allowed the reactor shutdown to resume. This delay had some serious consequences: the day shift had long since departed, the evening shift was also preparing to leave, and the night shift would not take over until midnight, well into the job. According to plan, the test should have been finished during the day shift, and the night shift would only have had to maintain decay heat cooling systems in an otherwise shut-down plant.\n\nThe night shift had very limited time to prepare for and carry out the experiment. A further rapid decrease in the power level from 50% was executed during the shift change-over. Alexander Akimov was chief of the night shift, and Leonid Toptunov was the operator responsible for the reactor's operational regimen, including the movement of the control rods. Toptunov was a young engineer who had worked independently as a senior engineer for approximately three months.\n\nThe test plan called for a gradual decrease in power output from reactor 4 to a thermal level of 700–1000 MW. An output of 700 MW was reached at 00:05 on 26 April. Due to the reactor's production of a fission byproduct, xenon-135, which is a reaction-inhibiting neutron absorber, core power continued to decrease without further operator action—a process known as reactor poisoning. This continuing decrease in power occurred because in \"steady state operation\", xenon-135 is \"burned off\" as quickly as it is created from decaying iodine-135 by absorbing neutrons from the ongoing chain reaction to become highly stable xenon-136. When the reactor power was lowered, previously produced high quantities of iodine-135 decayed into the neutron-absorbing xenon-135 faster than the reduced neutron flux could burn it off. As the reactor power output dropped further, to approximately 500 MW, Toptunov mistakenly inserted the control rods too far—the exact circumstances leading to this are unknown because Akimov died in hospital on 10 May and Toptunov on 14 May. This combination of factors put the reactor into an unintended near-shutdown state, with a power output of 30 MW thermal or less.\n\nThe reactor was now producing 5 percent of the minimum initial power level established as safe for the test. Control-room personnel decided to restore power by disabling the automatic system governing the control rods and manually extracting the majority of the reactor control rods to their upper limits. Several minutes elapsed between their extraction and the point that the power output began to increase and subsequently stabilize at 160–200 MW (thermal), a much smaller value than the planned 700 MW. The rapid reduction in the power during the initial shutdown, and the subsequent operation at a level of less than 200 MW led to increased poisoning of the reactor core by the accumulation of xenon-135. This restricted any further rise of reactor power, and made it necessary to extract additional control rods from the reactor core in order to counteract the poisoning.\n\nThe operation of the reactor at the low power level and high poisoning level was accompanied by unstable core temperature and coolant flow, and possibly by instability of neutron flux, which triggered alarms. The control room received repeated emergency signals regarding the levels in the steam/water separator drums, and large excursions or variations in the flow rate of feed water, as well as from relief valves opened to relieve excess steam into a turbine condenser, and from the neutron power controller. Between 00:35 and 00:45, emergency alarm signals concerning thermal-hydraulic parameters were ignored, apparently to preserve the reactor power level.\n\nWhen the power level of 200 MW was achieved, preparation for the experiment continued. As part of the test plan, extra water pumps were activated at 01:05 on 26 April, increasing the water flow. The increased coolant flow rate through the reactor produced an increase in the inlet coolant temperature of the reactor core (the coolant no longer having sufficient time to release its heat in the turbine and cooling towers), which now more closely approached the nucleate boiling temperature of water, reducing the safety margin.\n\nThe flow exceeded the allowed limit at 01:19, triggering an alarm of low steam pressure in the steam separators. At the same time, the extra water flow lowered the overall core temperature and reduced the existing steam voids in the core and the steam separators. Since water weakly absorbs neutrons (and the higher density of liquid water makes it a better absorber than steam), turning on additional pumps decreased the reactor power further still. The crew responded by turning off two of the circulation pumps to reduce feedwater flow, in an effort to increase steam pressure, and by removing more manual control rods to maintain power.\n\nAll these actions led to an extremely unstable reactor configuration. Nearly all of the control rods were removed manually, including all but 18 of the \"fail-safe\" manually operated rods of the minimal 28 which were intended to remain fully inserted to control the reactor even in the event of a loss of coolant, out of a total 211 control rods. While the emergency SCRAM system that would insert all control rods to shut down the reactor could still be activated manually (through the \"AZ-5\" button), the automated system that could do the same had been disabled to maintain the power level, and many other automated and even passive safety features of the reactor had been bypassed. Further, the reactor coolant pumping had been reduced, which had limited margin so any power excursion would produce boiling, thereby reducing neutron absorption by the water. The reactor was in an unstable configuration that was outside the safe operating envelope established by the designers. If anything pushed it into supercriticality, it was unable to recover automatically.\n\nAt 1:23:04 a.m., the experiment began. Four of the main circulating pumps (MCP) were active; of the eight total, six are normally active during regular operation. The steam to the turbines was shut off, beginning a run-down of the turbine generator. The diesel generators started and sequentially picked up loads; the generators were to have completely picked up the MCPs' power needs by 01:23:43. In the interim, the power for the MCPs was to be supplied by the turbine generator as it coasted down. As the momentum of the turbine generator decreased, so did the power it produced for the pumps. The water flow rate decreased, leading to increased formation of steam voids (bubbles) in the core.\n\nUnlike western Light Water Reactors, the RBMK had a positive void coefficient of reactivity, meaning when water began to boil and produce voids in the coolant, the nuclear chain reaction increases instead of decreasing.\nWith this feature at low reactor power levels, the no.4 RBMK reactor was now primed to embark on a positive feedback loop, in which the formation of steam voids reduced the ability of the liquid water coolant to absorb neutrons, which in turn increased the reactor's power output.\nThis caused yet more water to flash into steam, giving a further power increase. During almost the entire period of the experiment the automatic control system successfully counteracted this positive feedback, inserting control rods into the reactor core to limit the power rise. This system had control of only 12 rods, and nearly all others had been manually retracted.\n\nAt 1:23:40, as recorded by the SKALA centralized control system, a SCRAM (emergency shutdown) of the reactor was initiated. The SCRAM was started when the EPS-5 button (also known as the AZ-5 button) of the reactor emergency protection system was pressed: this engaged the drive mechanism on all control rods to fully insert them, including the manual control rods that had been withdrawn earlier. The reason why the EPS-5 button was pressed is not known, whether it was done as an emergency measure in response to rising temperatures, or simply as a routine method of shutting down the reactor upon completion of the experiment.\n\nThere is a view that the SCRAM may have been ordered as a response to the unexpected rapid power increase, although there is no recorded data proving this. Some have suggested that the button was not manually pressed, that the SCRAM signal was automatically produced by the emergency protection system, but the SKALA registered a manual SCRAM signal. Despite this, the question as to when or even whether the EPS-5 button was pressed has been the subject of debate. There have been assertions that the manual SCRAM was initiated due to the initial rapid power acceleration. Others have suggested that the button was not pressed until the reactor began to self-destruct, while others believe that it happened earlier and in calm conditions.\n\nIn any case, when the EPS-5 button was pressed, the insertion of control rods into the reactor core began. The control rod insertion mechanism moved the rods at 0.4 m/s, so that the rods took 18 to 20 seconds to travel the full height of the core, about 7 metres. A bigger problem was the design of the RBMK control rods, each of which had a graphite neutron moderator rod attached to the end to boost reactor output by displacing water when the control rod section had been fully withdrawn from the reactor. Thus, when a control rod was at maximum extraction, a neutron-moderating graphite extension was centered in the core with a 1.25 m column of water above and below it. Therefore, injecting a control rod downward into the reactor during a SCRAM initially displaced (neutron-absorbing) water in the lower portion of the reactor with (neutron-moderating) graphite on its way out of the core. As a result, an emergency SCRAM initially increased the reaction rate in the lower part of the core as the graphite section of rods moving out of the reactor displaced water coolant. This behaviour was revealed when the initial insertion of control rods in another RBMK reactor at Ignalina Nuclear Power Plant in 1983 induced a power spike, but since the subsequent SCRAM of that reactor was successful, the information was disseminated but deemed of little importance.\n\nA few seconds into the SCRAM, a power spike occurred, and the core overheated, causing some of the fuel rods to fracture, blocking the control rod columns and jamming the control rods at one-third insertion, with the graphite displacers still in the lower part of the core. Within three seconds the reactor output rose above 530 MW.\n\nThe subsequent course of events was not registered by instruments; it is known only as a result of mathematical simulation. Apparently, the power spike caused an increase in fuel temperature and steam buildup, leading to a rapid increase in steam pressure. This caused the fuel cladding to fail, releasing the fuel elements into the coolant, and rupturing the channels in which these elements were located.\n\nThen, according to some estimations, the reactor jumped to around 30,000 MW thermal, ten times the normal operational output. The last reading on the control panel was 33,000 MW. It was not possible to reconstruct the precise sequence of the processes that led to the destruction of the reactor and the power unit building, but a steam explosion, like the explosion of a steam boiler from excess vapour pressure, appears to have been the next event. There is a general understanding that it was explosive steam pressure from the damaged fuel channels escaping into the reactor's exterior cooling structure that caused the explosion that destroyed the reactor casing, tearing off and blasting the upper plate, to which the entire reactor assembly is fastened, through the roof of the reactor building. This is believed to be the first explosion that many heard. This explosion ruptured further fuel channels, as well as severing most of the coolant lines feeding the reactor chamber, and as a result the remaining coolant flashed to steam and escaped the reactor core. The total water loss in combination with a high positive void coefficient further increased the reactor's thermal power.\n\nA second, more powerful explosion occurred about two or three seconds after the first; this explosion dispersed the damaged core and effectively terminated the nuclear chain reaction. This explosion also compromised more of the reactor containment vessel and ejected hot lumps of graphite moderator. The ejected graphite and the demolished channels still in the remains of the reactor vessel caught fire on exposure to air, greatly contributing to the spread of radioactive fallout and the contamination of outlying areas.\n\nAccording to observers outside Unit 4, burning lumps of material and sparks shot into the air above the reactor. Some of them fell onto the roof of the machine hall and started a fire. About 25 percent of the red-hot graphite blocks and overheated material from the fuel channels was ejected. Parts of the graphite blocks and fuel channels were out of the reactor building. As a result of the damage to the building an airflow through the core was established by the high temperature of the core. The air ignited the hot graphite and started a graphite fire.\n\nAfter the larger explosion, a number of employees at the power station went outside to get a clearer view of the extent of the damage. One such survivor, Alexander Yuvchenko, recounts that once he stepped outside and looked up towards the reactor hall, he saw a \"very beautiful\" LASER-like beam of light bluish light caused by the ionization of air that appeared to \"flood up into infinity\".\n\nThere were initially several hypotheses about the nature of the second explosion. One view was that the second explosion was caused by hydrogen, which had been produced either by the overheated steam-zirconium reaction or by the reaction of red-hot graphite with steam that produced hydrogen and carbon monoxide. Another hypothesis, by Checherov, published in 1998, was that the second explosion was a thermal explosion of the reactor as a result of the uncontrollable escape of fast neutrons caused by the complete water loss in the reactor core. A third hypothesis was that the second explosion was another steam explosion. According to this version, the first explosion was a more minor steam explosion in the circulating loop, causing a loss of coolant flow and pressure that in turn caused the water still in the core to flash to steam. This second explosion then did the majority of the damage to the reactor and containment building.\n\nThe force of the second explosion and the ratio of xenon radioisotopes released after the accident (a vital tool in nuclear forensics) indicated to Yuri V. Dubasov in a 2009 publication (suggested before him by Checherov in 1998), that the \"second\" explosion could have been a nuclear power transient resulting from core material melting in the absence of its water coolant and moderator. Dubasov argues that the reactor did not simply undergo a runaway \"delayed\"-supercritical/exponential increase in power into the multi-gigawatt power range, which is somewhat similar to the conditions of a normal reactor coming up to its commercial power level (with the notable exception that Chernobyl's older RBMK reactor design had the largest positive void coefficient of reactivity of any reactor then operating commercially), permitting a dangerous \"positive feedback\"/runaway condition, given the lack of \"inherent\" safety stops when power levels began to increase above the commercial level. Although a positive-feedback power excursion that increased until the reactor disassembled itself by means of its internal energy and external steam explosions is the more accepted explanation for the cause of the explosions, Dubasov argues instead that a runaway \"prompt\" supercriticality occurred, with the internal physics being more similar to the explosion of a fizzled nuclear weapon, and that this failed/fizzle event produced the \"second\" explosion.\n\nThis nuclear fizzle hypothesis, then mostly defended by Dubasov, was examined further in 2017 by retired physicist Lars-Erik De Geer in an analysis that puts the hypothesized fizzle event as the more probable cause of the \"first\" explosion. The more energetic second explosion, which produced the majority of the damage, has been estimated by Dubasov in 2009 as equivalent to 40 billion joules of energy, the equivalent of about ten tons of TNT. Both the 2009 and 2017 analyses argue that the nuclear fizzle event, whether producing the \"second\" or \"first\" explosion, consisted of a \"prompt\" chain reaction (as opposed to the consensus \"delayed\" neutron mediated chain-reaction) that was limited to a small portion of the reactor core, since expected self-disassembly occurs rapidly in fizzle events.\n\nContrary to safety regulations, bitumen, a combustible material, had been used in the construction of the roof of the reactor building and the turbine hall. Ejected material ignited at least five fires on the roof of the adjacent reactor 3, which was still operating. It was imperative to put those fires out and protect the cooling systems of reactor 3. Inside reactor 3, the chief of the night shift, Yuri Bagdasarov, wanted to shut down the reactor immediately, but chief engineer Nikolai Fomin would not allow this. The operators were given respirators and potassium iodide tablets and told to continue working. At 05:00, Bagdasarov made his own decision to shut down the reactor, leaving only those operators there who had to work the emergency cooling systems.\n\nApproximate radiation intensity levels at different locations at Chernobyl reactor site shortly after the explosion are shown in the table below. A dose of 500 roentgens (~5 Sv) delivered over an hour is usually lethal for human beings.\n\nThe radiation levels in the worst-hit areas of the reactor building have been estimated to be 5.6 roentgens per second (R/s), equivalent to more than 20,000 roentgens per hour. A lethal dose is around 500 roentgens (~5 Gy) over 5 hours, so in some areas, unprotected workers received fatal doses in less than a minute. However, a dosimeter capable of measuring up to 1000 R/s was buried in the rubble of a collapsed part of the building, and another one failed when turned on. All remaining dosimeters had limits of 0.001 R/s and therefore read \"off scale\". Thus, the reactor crew could ascertain only that the radiation levels were somewhere above 0.001 R/s (3.6 R/h), while the true levels were much higher in some areas.\n\nBecause of the inaccurate low readings, the reactor crew chief Alexander Akimov assumed that the reactor was intact. The evidence of pieces of graphite and reactor fuel lying around the building was ignored, and the readings of another dosimeter brought in by 04:30 were dismissed under the assumption that the new dosimeter must have been defective. Akimov stayed with his crew in the reactor building until morning, sending members of his crew to try to pump water into the reactor. None of them wore any protective gear. Most, including Akimov, died from radiation exposure within three weeks.\n\nShortly after the accident, firefighters arrived to try to extinguish the fires. First on the scene was a Chernobyl Power Station firefighter brigade under the command of Lieutenant Volodymyr Pravik, who died on 9 May 1986 of acute radiation sickness. They were not told how dangerously radioactive the smoke and the debris were, and may not even have known that the accident was anything more than a regular electrical fire: \"We didn't know it was the reactor. No one had told us.\"\n\nGrigorii Khmel, the driver of one of the fire engines, later described what happened:\n\nAnatoli Zakharov, a fireman stationed in Chernobyl since 1980, offers a different description in 2008:\n\nHe also said:\n\nThe immediate priority was to extinguish fires on the roof of the station and the area around the building containing Reactor No. 4 to protect No. 3 and keep its core cooling systems intact. The fires were extinguished by 5:00, but many firefighters received high doses of radiation. The fire inside reactor 4 continued to burn until 10 May 1986; it is possible that well over half of the graphite burned out.\n\nThe fire was extinguished by a combined effort of helicopters dropping over 5000 metric tons of sand, lead, clay, and neutron-absorbing boron onto the burning reactor and injection of liquid nitrogen. It is now known that virtually none of the neutron absorbers reached the core.\n\nFrom eyewitness accounts of the firefighters involved before they died (as reported on the CBC television series \"Witness\"), one described his experience of the radiation as \"tasting like metal\", and feeling a sensation similar to that of pins and needles all over his face. (This is similar to the description given by Louis Slotin, a Manhattan Project physicist who died days after a fatal radiation overdose from a criticality accident.)\n\nThe explosion and fire threw hot particles of the nuclear fuel and also far more dangerous fission products, radioactive isotopes such as caesium-137, iodine-131, strontium-90 and other radionuclides, into the air: the residents of the surrounding area observed the radioactive cloud on the night of the explosion.\n\nEquipment assembled included remote-controlled bulldozers and robot-carts that could detect radioactivity and carry hot debris. Valery Legasov (first deputy director of the Kurchatov Institute of Atomic Energy in Moscow) said, in 1987: \"But we learned that robots are not the great remedy for everything. Where there was very high radiation, the robot ceased to be a robot—the electronics quit working.\"\n\nWith the exception of the \"fire\" contained inside Reactor 4, which continued to burn for many days.\n\nThe nearby city of Pripyat was not immediately evacuated. The townspeople, in the early hours of the morning, at 01:23 AM local time, went about their usual business, completely oblivious to what had just happened. However, within a few hours of the explosion, dozens of people fell ill. Later, they reported severe headaches and metallic tastes in their mouths, along with uncontrollable fits of coughing and vomiting.\n\nAs the plant was run by authorities in Moscow, the government of Ukraine did not receive prompt information on the accident.\n\nValentyna Shevchenko, then Chairman of the Presidium of Verkhovna Rada Supreme Soviet of the Ukrainian SSR, recalls that Ukraine's acting Minister of Internal Affairs Vasyl Durdynets phoned her at work at 9 am to report current affairs; only at the end of the conversation did he add that there had been a fire at the Chernobyl nuclear power plant, but it was extinguished and everything was fine. When Shevchenko asked \"How are the people?\", he replied that there was nothing to be concerned about: \"Some are celebrating a wedding, others are gardening, and others are fishing in the Pripyat River\".\nShevchenko then spoke over the phone to Volodymyr Shcherbytsky, Head of the Central Committee of the CPU and de facto head of state, who said he anticipated a delegation of the state commission headed by the deputy chairman of the Council of Ministers of USSR.\n\nA commission was set up the same day (26 April) to investigate the accident. It was headed by Valery Legasov, First Deputy Director of the Kurchatov Institute of Atomic Energy, and included leading nuclear specialist Evgeny Velikhov, hydro-meteorologist Yuri Izrael, radiologist Leonid Ilyin and others. They flew to Boryspil International Airport and arrived at the power plant in the evening of 26 April. By that time two people had already died and 52 were hospitalized. The delegation soon had ample evidence that the reactor was destroyed and extremely high levels of radiation had caused a number of cases of radiation exposure. In the early daylight hours of 27 April, approximately 36 hours after the initial blast, they ordered the evacuation of Pripyat. Initially it was decided to evacuate the population for three days; later this was made permanent.\n\nBy 11:00 on 27 April, buses had arrived in Pripyat to start the evacuation. The evacuation began at 14:00. A translated excerpt of the evacuation announcement follows:\n\nTo expedite the evacuation, residents were told to bring only what was necessary, and that they would remain evacuated for approximately three days. As a result, most personal belongings were left behind, and remain there today. By 15:00, 53,000 people were evacuated to various villages of the Kiev region. The next day, talks began for evacuating people from the 10 km zone. Ten days after the accident, the evacuation area was expanded to 30 km (19 mi). This \"exclusion zone\" has remained ever since, although its shape has changed and its size has been expanded.\n\nEvacuation began long before the accident was publicly acknowledged by the Soviet Union. On the morning of 28 April, radiation levels set off alarms at the Forsmark Nuclear Power Plant in Sweden, over from the Chernobyl Plant. Sweden determined that the radiation had originated elsewhere, and the Swedish government contacted the Soviet government, The Soviet government denied being the source of the radiation, until the Swedish government advised the Soviet government that a report was being made to the International Atomic Energy Authority.\n\nAt 21:02 the evening of 28 April, a 20-second announcement was read in the TV news programme \"Vremya\":\n\nThis was the entirety of the announcement of the accident. The Telegraph Agency of the Soviet Union (TASS) then discussed Three Mile Island and other American nuclear accidents, an example of the common Soviet tactic of emphasizing foreign disasters when one occurred in the Soviet Union. The mention of a commission, however, indicated to observers the seriousness of the incident, and subsequent state radio broadcasts were replaced with classical music, which was a common method of preparing the public for an announcement of a tragedy.\n\nAround the same time, ABC News released its report about the disaster.\n\nShevchenko was the first of the Ukrainian state top officials to arrive at the disaster site early on 28 April. There she spoke with members of medical staff and people, who were calm and hopeful that they could soon return to their homes. Shevchenko returned home near midnight, stopping at a radiological checkpoint in Vilcha, one of the first that were set up soon after the accident.\n\nThere was a notification from Moscow that there was no reason to postpone the 1 May International Workers' Day celebrations in Kiev (including the annual parade), but on 30 April a meeting of the Political bureau of the Central Committee of CPU took place to discuss the plan for the upcoming celebration. Scientists were reporting that the radiological background in Kiev city was normal. At the meeting, which was finished at 18:00, it was decided to shorten celebrations from the regular 3.5–4 to under 2 hours.\n\nSeveral buildings in Pripyat were officially kept open after the disaster to be used by workers still involved with the plant. These included the Jupiter Factory which closed in 1996 and the Azure Swimming Pool, used by the liquidators for recreation during the clean-up, which closed in 1998.\n\nTwo floors of bubbler pools beneath the reactor served as a large water reservoir for the emergency cooling pumps and as a pressure suppression system capable of condensing steam in case of a small broken steam pipe; the third floor above them, below the reactor, served as a steam tunnel. The steam released by a broken pipe was supposed to enter the steam tunnel and be led into the pools to bubble through a layer of water. After the disaster, the pools and the basement were flooded because of ruptured cooling water pipes and accumulated firefighting water, and constituted a serious steam explosion risk.\n\nThe smoldering graphite, fuel and other material above, at more than 1200 °C, started to burn through the reactor floor and mixed with molten concrete from the reactor lining, creating corium, a radioactive semi-liquid material comparable to lava. If this mixture had melted through the floor into the pool of water, it was feared it could have created a serious steam explosion that would have ejected more radioactive material from the reactor. It became necessary to drain the pool.\n\nThe bubbler pool could be drained by opening its sluice gates. However, the valves controlling it were underwater, located in a flooded corridor in the basement. So volunteers in wetsuits and respirators (for protection against radioactive aerosols) and equipped with dosimeters, entered the knee-deep radioactive water and managed to open the valves. These were the engineers Alexei Ananenko and Valeri Bezpalov (who knew where the valves were), accompanied by the shift supervisor Boris Baranov. Upon succeeding and emerging from the water, according to many English language news articles, books and the prominent BBC docudrama \"Surviving Disaster – Chernobyl Nuclear\", the three knew it was a suicide-mission and began suffering from radiation sickness and died soon after. Some sources also incorrectly claimed that they died there in the plant. However, research by Andrew Leatherbarrow, author of the 2016 book \"Chernobyl 01:23:40\", determined that the frequently recounted story is a gross exaggeration. Alexei Ananenko continues to work in the nuclear energy industry, and rebuffs the growth of the Chernobyl media sensationalism surrounding him. While Valeri Bezpalov was found to still be alive by Leatherbarrow, the 65-year-old Baranov had lived until 2005 and had died of heart failure.\n\nOnce the bubbler pool gates were opened by the Ananenko team, fire brigade pumps were then used to drain the basement. The operation was not completed until 8 May, after 20,000 metric tons of highly radioactive water were pumped out.\n\nWith the bubbler pool gone, a meltdown was less likely to produce a powerful steam explosion. To do so, the molten core would now have to reach the water table below the reactor. To reduce the likelihood of this, it was decided to freeze the earth beneath the reactor, which would also stabilize the foundations. Using oil drilling equipment, the injection of liquid nitrogen began on 4 May. It was estimated that 25 metric tons of liquid nitrogen per day would be required to keep the soil frozen at −100 °C. This idea was soon scrapped and the bottom room where the cooling system would have been installed was filled with concrete.\n\nIt is likely that intense alpha radiation hydrolysed the water, generating a low-pH hydrogen peroxide (HO) solution akin to an oxidizing acid. Conversion of bubbler pool water to HO is confirmed by the presence in the Chernobyl lavas of studtite and metastudtite, the only minerals that contain peroxide.\n\nThe worst of the radioactive debris was collected inside what was left of the reactor. During clean-up operations, remotely controlled machinery were used to try to move the most highly radioactive debris, but these failed in the harsh conditions. Consequently, the most highly radioactive materials were shoveled by Chernobyl liquidators from the military wearing heavy protective gear (dubbed \"bio-robots\" by the military); these soldiers could only spend a maximum of 40 seconds working on the rooftops of the surrounding buildings because of the extremely high doses of radiation given off by the blocks of graphite and other debris. Though the soldiers were only supposed to perform the role of the \"bio-robot\" a maximum of once, some soldiers reported having done this task five or six times. The reactor itself was covered with bags of sand, lead and boric acid dropped from helicopters: some 5000 metric tons of material were dropped during the week that followed the accident. Historians estimate that about 600 Soviet pilots risked dangerous levels of radiation to fly the thousands of flights needed to cover reactor No. 4 in this attempt to seal off radiation.\n\nAt the time there was still fear that the reactor could re-enter a self-sustaining nuclear chain-reaction and explode again, and a new containment structure was planned to prevent rain entering and triggering such an explosion, and to prevent further release of radioactive material. This was the largest civil engineering task in history, involving a quarter of a million construction workers who all reached their official lifetime limits of radiation. The Ukrainian filmmaker Vladimir Shevchenko captured film footage of an Mi-8 helicopter as its main rotor collided with a nearby construction crane cable, causing the helicopter to fall near the damaged reactor building and killing its four-man crew on 2 October 1986. By December 1986, a large concrete sarcophagus had been erected to seal off the reactor and its contents. A unique \"clean up\" medal was given to the workers.\n\nAlthough many of the radioactive emergency vehicles were buried in trenches, many of the vehicles used by the liquidators, including the helicopters, still remain parked in a field in the Chernobyl area. Scavengers have since removed many functioning, but highly radioactive, parts.\n\nDuring the construction of the sarcophagus, a scientific team re-entered the reactor as part of an investigation dubbed \"Complex Expedition\", to locate and contain nuclear fuel in a way that could not lead to another explosion. These scientists manually collected cold fuel rods, but great heat was still emanating from the core. Rates of radiation in different parts of the building were monitored by drilling holes into the reactor and inserting long metal detector tubes. The scientists were exposed to high levels of radiation and radioactive dust.\n\nAfter six months of investigation, in December 1986, they discovered with the help of a remote camera an intensely radioactive mass in the basement of Unit Four, more than two metres wide, which they called \"the elephant's foot\" for its wrinkled appearance. The mass was composed of melted sand, concrete and a large amount of nuclear fuel that had escaped from the reactor. The concrete beneath the reactor was steaming hot, and was breached by now-solidified lava and spectacular unknown crystalline forms termed chernobylite. It was concluded that there was no further risk of explosion.\n\nLiquidators worked under deplorable conditions, poorly informed and with poor protection. Many, if not most of them, exceeded radiation safety limits. Some exceeded limits by over 100 times—leading to rapid death.\n\nThe official contaminated zones became stage to a massive clean-up effort lasting seven months. The official reason for such early (and dangerous) decontamination efforts, rather than allowing time for natural decay, was that the land must be re-peopled and brought back into cultivation. Indeed, within fifteen months 75% of the land was under cultivation, even though only a third of the evacuated villages were resettled. Defence forces must have done much of the work. Yet this land was of marginal agricultural value. According to historian David Marples, the administration had a psychological purpose for the clean-up: they wished to forestall panic regarding nuclear energy, and even to restart the Chernobyl power station.\n\nThere were two official explanations of the accident.\n\nThe first official explanation of the accident, later acknowledged to be erroneous, was published in August 1986. It effectively placed the blame on the power plant operators. To investigate the causes of the accident the IAEA created a group known as the International Nuclear Safety Advisory Group (INSAG), which in its report of 1986, INSAG-1, on the whole also supported this view, based on the data provided by the Soviets and the oral statements of specialists. In this view, the catastrophic accident was caused by gross violations of operating rules and regulations. \"During preparation and testing of the turbine generator under run-down conditions using the auxiliary load, personnel disconnected a series of technical protection systems and breached the most important operational safety provisions for conducting a technical exercise.\"\n\nThe operator error was probably due to their lack of knowledge of nuclear reactor physics and engineering, as well as lack of experience and training. According to these allegations, at the time of the accident the reactor was being operated with many key safety systems turned off, most notably the Emergency Core Cooling System (ECCS), LAR (Local Automatic control system), and AZ (emergency power reduction system). Personnel had an insufficiently detailed understanding of technical procedures involved with the nuclear reactor, and knowingly ignored regulations to speed test completion.\n\nIn this analysis of the causes of the accident, deficiencies in the reactor design and in the operating regulations that made the accident possible were set aside and mentioned only casually. Serious critical observations covered only general questions and did not address the specific reasons for the accident.\n\nThe following general picture arose from these observations, and several procedural irregularities also helped to make the accident possible, one of which was insufficient communication between the safety officers and the operators in charge of the experiment being run that night.\n\nThe reactor operators disabled safety systems down to the generators, which the test was really about. The main process computer, SKALA, was running in such a way that the main control computer could not shut down the reactor or even reduce power. Normally the computer would have started to insert all of the control rods. The computer would have also started the \"Emergency Core Protection System\" that introduces 24 control rods into the active zone within 2.5 seconds, which is still slow by 1986 standards. All control was transferred from the process computer to the human operators.\n\nOn the subject of the disconnection of safety systems, Valery Legasov said, in 1987, that \"[i]t was like airplane pilots experimenting with the engines in flight\".\n\nThis view is reflected in numerous publications and also artistic works on the theme of the Chernobyl accident that appeared immediately after the accident, and for a long time remained dominant in the public consciousness and in popular publications.\n\nUkraine has declassified a number of KGB documents from the period between 1971 and 1988 related to the Chernobyl plant, mentioning for example previous reports of structural damages caused by negligence during construction of the plant (such as splitting of concrete layers) that were never acted upon. They document over 29 emergency situations in the plant during this period, 8 of which were caused by negligence or poor competence on the part of personnel.\n\nIn 1991 a Commission of the USSR State Committee for the Supervision of Safety in Industry and Nuclear Power reassessed the causes and circumstances of the Chernobyl accident and came to new insights and conclusions. Based on it, in 1992 the IAEA Nuclear Safety Advisory Group (INSAG) published an additional report, INSAG-7, which reviewed \"that part of the INSAG-1 report in which primary attention is given to the reasons for the accident,\" and was included the USSR State Commission report as Appendix I.\n\nIn this INSAG report, most of the earlier accusations against staff for breach of regulations were acknowledged to be either erroneous, based on incorrect information obtained in August 1986, or less relevant. This report reflected a different view of the main reasons for the accident, presented in Appendix I. According to this account, the operators' actions in turning off the Emergency Core Cooling System, interfering with the settings on the protection equipment, and blocking the level and pressure in the separator drum did not contribute to the original cause of the accident and its magnitude, although they may have been a breach of regulations. In fact, turning off the emergency system designed to prevent the two turbine generators from stopping was not a violation of regulations.\n\nHuman factors, however, contributed to the conditions that led to the disaster. These included operating the reactor at a low power level—less than 700 MW—a level documented in the run-down test programme, and operating with a small operational reactivity margin (ORM). The 1986 assertions of Soviet experts notwithstanding, regulations did not prohibit operating the reactor at this low power level.\n\nHowever, regulations did forbid operating the reactor with a small margin of reactivity. Yet \"post-accident studies have shown that the way in which the real role of the ORM is reflected in the Operating Procedures and design documentation for the RBMK-1000 is extremely contradictory\", and furthermore, \"ORM was not treated as an operational safety limit, violation of which could lead to an accident\".\n\nAccording to the INSAG-7 Report, the chief reasons for the accident lie in the peculiarities of physics and in the construction of the reactor. There are two such reasons:\n\nWhile INSAG-1 and INSAG-7 reports both identified operator error as an issue of concern, the INSAG-7 identified that there were numerous other issues that were contributing factors that led to the incident. These contributing factors include:\n\nBoth views were heavily lobbied by different groups, including the reactor's designers, power plant personnel, and the Soviet and Ukrainian governments. According to the IAEA's 1986 analysis, the main cause of the accident was the operators' actions. But according to the IAEA's 1993 revised analysis the main cause was the reactor's design. One reason there were such contradictory viewpoints and so much debate about the causes of the Chernobyl accident was that the primary data covering the disaster, as registered by the instruments and sensors, were not completely published in the official sources.\n\nOnce again, the human factor had to be considered as a major element in causing the accident. INSAG notes that both the operating regulations and staff handled the disabling of the reactor protection easily enough: witness the length of time for which the ECCS was out of service while the reactor was operated at half power. INSAG's view is that it was the operating crew's deviation from the test programme that was mostly to blame. \"Most reprehensibly, unapproved changes in the test procedure were deliberately made on the spot, although the plant was known to be in a very different condition from that intended for the test.\"\n\nAs in the previously released report INSAG-1, close attention is paid in report INSAG-7 to the inadequate (at the moment of the accident) \"culture of safety\" at all levels. Deficiency in the safety culture was inherent not only at the operational stage but also, and to no lesser extent, during activities at other stages in the lifetime of nuclear power plants (including design, engineering, construction, manufacture, and regulation). The poor quality of operating procedures and instructions, and their conflicting character, put a heavy burden on the operating crew, including the chief engineer. \"The accident can be said to have flowed from a deficient safety culture, not only at the Chernobyl plant, but throughout the Soviet design, operating and regulatory organizations for nuclear power that existed at that time.\"\n\nAlthough no informing comparisons can be made between the accident and a strictly air burst-fuzed nuclear detonation, it has still been approximated that about four hundred times more radioactive material was released from Chernobyl than by the atomic bombing of Hiroshima and Nagasaki. By contrast the Chernobyl accident released about one hundredth to one thousandth of the total amount of radioactivity released during the era of nuclear weapons testing at the height of the Cold War, 1950 – 1960s, with the 1/100 to 1/1000 variance due to trying to make comparisons with different spectrums of isotopes released. Approximately 100,000 km² of land was significantly contaminated with fallout, with the worst hit regions being in Belarus, Ukraine and Russia. Slighter levels of contamination were detected over all of Europe except for the Iberian Peninsula.\n\nThe initial evidence that a major release of radioactive material was affecting other countries came not from Soviet sources, but from Sweden. On the morning of 28 April workers at the Forsmark Nuclear Power Plant (approximately from the Chernobyl site) were found to have radioactive particles on their clothes.\n\nIt was Sweden's search for the source of radioactivity, after they had determined there was no leak at the Swedish plant, that at noon on 28 April led to the first hint of a serious nuclear problem in the western Soviet Union. Hence the evacuation of Pripyat on 27 April 36 hours after the initial explosions, was silently completed before the disaster became known outside the Soviet Union. The rise in radiation levels had at that time already been measured in Finland, but a civil service strike delayed the response and publication.\n\nContamination from the Chernobyl accident was scattered irregularly depending on weather conditions, much of it deposited on mountainous regions such as the Alps, the Welsh mountains and the Scottish Highlands, where adiabatic cooling caused radioactive rainfall. The resulting patches of contamination were often highly localized, and water-flows across the ground contributed further to large variations in radioactivity over small areas. Sweden and Norway also received heavy fallout when the contaminated air collided with a cold front, bringing rain.\n\nRain was purposely seeded over 10,000 km of the Belorussian SSR by the Soviet air force to remove radioactive particles from clouds heading toward highly populated areas. Heavy, black-coloured rain fell on the city of Gomel. Reports from Soviet and Western scientists indicate that Belarus received about 60% of the contamination that fell on the former Soviet Union. However, the 2006 TORCH report stated that half of the volatile particles had landed outside Ukraine, Belarus, and Russia. A large area in Russia south of Bryansk was also contaminated, as were parts of northwestern Ukraine. Studies in surrounding countries indicate that over one million people could have been affected by radiation.\n\nRecently published data from a long-term monitoring program (The Korma Report II) shows a decrease in internal radiation exposure of the inhabitants of a region in Belarus close to Gomel. Resettlement may even be possible in prohibited areas provided that people comply with appropriate dietary rules.\n\nIn Western Europe, precautionary measures taken in response to the radiation included seemingly arbitrary regulations banning the importation of certain foods but not others. In France some officials stated that the Chernobyl accident had no adverse effects. Official figures in southern Bavaria in Germany indicated that some wild plant species contained substantial levels of caesium, which were believed to have been passed onto them during their consumption by wild boars, a significant number of which already contained radioactive particles above the allowed level.\n\nLike many other releases of radioactivity into the environment, the Chernobyl release was controlled by the physical and chemical properties of the radioactive elements in the core. Particularly dangerous are the highly radioactive fission products, those with high nuclear decay rates that accumulate in the food chain, such as some of the isotopes of iodine, caesium and strontium. Iodine-131 and caesium-137 are responsible for most of the radiation exposure received by the general population.\n\nDetailed reports on the release of radioisotopes from the site were published in 1989 and 1995, with the latter report updated in 2002.\n\nAt different times after the accident, different isotopes were responsible for the majority of the external dose. The remaining quantity of any radioisotope, and therefore the activity of that isotope, after 7 decay half-lives have passed, is less than 1% of its initial magnitude, and it continues to reduce beyond 0.78% after 7 half-lives to 0.098% remaining after 10 half-lives have passed and so on. (Some radionuclides have decay products that are likewise radioactive, which is not accounted for here.) The release of radioisotopes from the nuclear fuel was largely controlled by their boiling points, and the majority of the radioactivity present in the core was retained in the reactor.\n\nTwo sizes of particles were released: small particles of 0.3 to 1.5 micrometres, each an individually unrecognizable small dust or smog sized particulate matter and larger settling dust sized particles that therefore were quicker to \"fall-out\" of the air, of 10 micrometres in diameter. These larger particles contained about 80% to 90% of the released high boiling point or non-volatile radioisotopes; zirconium-95, niobium-95, lanthanum-140, cerium-144 and the transuranic elements, including neptunium, plutonium and the minor actinides, embedded in a uranium oxide matrix.\n\nThe dose that was calculated is the relative external gamma dose rate for a person standing in the open. The exact dose to a person in the real world who would spend most of their time sleeping indoors in a shelter and then venturing out to consume an internal dose from the inhalation or ingestion of a radioisotope, requires a personnel specific radiation dose reconstruction analysis.\n\nThe Chernobyl nuclear power plant is located next to the Pripyat River, which feeds into the Dnieper reservoir system, one of the largest surface water systems in Europe, which at the time supplied water to Kiev's 2.4 million residents, and was still in spring flood when the accident occurred. The radioactive contamination of aquatic systems therefore became a major problem in the immediate aftermath of the accident. In the most affected areas of Ukraine, levels of radioactivity (particularly from radionuclides I, Cs and Sr) in drinking water caused concern during the weeks and months after the accident, though officially it was stated that all contaminants had settled to the bottom \"in an insoluble phase\" and would not dissolve for 800–1000 years. Guidelines for levels of radioiodine in drinking water were temporarily raised to 3,700 Bq/L, allowing most water to be reported as safe, and a year after the accident it was announced that even the water of the Chernobyl plant's cooling pond was within acceptable norms. Despite this, two months after the disaster the Kiev water supply was abruptly switched from the Dnieper to the Desna River. Meanwhile, massive silt traps were constructed, along with an enormous 30m-deep underground barrier to prevent groundwater from the destroyed reactor entering the Pripyat River.\n\nBio-accumulation of radioactivity in fish resulted in concentrations (both in western Europe and in the former Soviet Union) that in many cases were significantly above guideline maximum levels for consumption. Guideline maximum levels for radiocaesium in fish vary from country to country but are approximately 1000 Bq/kg in the European Union. In the Kiev Reservoir in Ukraine, concentrations in fish were several thousand Bq/kg during the years after the accident.\n\nIn small \"closed\" lakes in Belarus and the Bryansk region of Russia, concentrations in a number of fish species varied from 100 to 60,000 Bq/kg during the period 1990–92. The contamination of fish caused short-term concern in parts of the UK and Germany and in the long term (years rather than months) in the affected areas of Ukraine, Belarus, and Russia as well as in parts of Scandinavia.\n\nGroundwater was not badly affected by the Chernobyl accident since radionuclides with short half-lives decayed away long before they could affect groundwater supplies, and longer-lived radionuclides such as radiocaesium and radiostrontium were adsorbed to surface soils before they could transfer to groundwater. However, significant transfers of radionuclides to groundwater have occurred from waste disposal sites in the exclusion zone around Chernobyl. Although there is a potential for transfer of radionuclides from these disposal sites off-site (i.e. out of the exclusion zone), the IAEA Chernobyl Report argues that this is not significant in comparison to current levels of washout of surface-deposited radioactivity.\n\nAfter the disaster, four square kilometres of pine forest directly downwind of the reactor turned reddish-brown and died, earning the name of the \"Red Forest\". Some animals in the worst-hit areas also died or stopped reproducing. Most domestic animals were removed from the exclusion zone, but horses left on an island in the Pripyat River from the power plant died when their thyroid glands were destroyed by radiation doses of 150–200 Sv. Some cattle on the same island died and those that survived were stunted because of thyroid damage. The next generation appeared to be normal.\n\nA robot sent into the reactor itself has returned with samples of black, melanin-rich radiotrophic fungi that are growing on the reactor's walls.\n\nOf the 440,350 wild boar killed in the 2010 hunting season in Germany, approximately one thousand were found to be contaminated with levels of radiation above the permitted limit of 600 becquerels of Cesium per kilogram, due to residual radioactivity from Chernobyl. While all animal meat contains a natural level of potassium 40 at a similar level of activity, with both wild and farm animals in Italy containing \"415 ± 56 becquerels kg−1 dw\" of that naturally occurring gamma emitter. The cesium contamination issue has historically reached some uniquely isolated and high levels approaching 20,000 Becquerels of Cesium per kilogram in some specific tests, however as it has not been observed in the wild boar population of Fukushima after the 2011 accident. Evidence exists to suggest that the wild German and Ukrainian boar population are in a unique location were they have subsisted on a diet high in plant or fungi sources that biomagnifies or concentrates radio-cesium, with the most well known food source the consumption of the outer shell or wall of the \"deer-truffle\"/Elaphomyces which along with magnifying radio-cesium also magnifies or concentrates natural soil concentrations of Arsenic.\n\nThe Norwegian Agricultural Authority reported that in 2009 a total of 18,000 livestock in Norway needed to be given uncontaminated feed for a period of time before slaughter in order to ensure that their meat was safe for human consumption. This was due to residual radioactivity from Chernobyl in the plants they graze on in the wild during the summer. 1,914 sheep needed to be given uncontaminated feed for a period of time before slaughter during 2012, and these sheep were located in just 18 of Norway's municipalities, a decrease of 17 from the 35 municipalities affected animals were located in during 2011 (117 municipalities were affected during 1986).\n\nThe after-effects of Chernobyl were expected to be seen for a further 100 years, although the severity of the effects would decline over that period. Scientists report this is due to radioactive caesium-137 isotopes being taken up by fungi such as \"Cortinarius caperatus\" which is in turn eaten by sheep whilst grazing.\n\nThe United Kingdom was forced to restrict the movement of sheep from upland areas when radioactive caesium-137 fell across parts of Northern Ireland, Wales, Scotland and northern England. In the immediate aftermath of the disaster in 1986, a total of 4,225,000 sheep had their movement restricted across a total of 9,700 farms, in order to prevent contaminated meat entering the human food chain. The number of sheep and the number of farms affected has decreased since 1986, Northern Ireland was released from all restrictions in 2000 and by 2009 369 farms containing around 190,000 sheep remained under the restrictions in Wales, Cumbria and northern Scotland. The restrictions applying in Scotland were lifted in 2010, whilst those applying to Wales and Cumbria were lifted during 2012, meaning no farms in the UK remain restricted because of Chernobyl fallout.\n\nThe legislation used to control sheep movement and compensate farmers (farmers were latterly compensated per animal to cover additional costs in holding animals prior to radiation monitoring) was revoked during October and November 2012 by the relevant authorities in the UK.\n\nIn the aftermath of the accident, 237 people suffered from acute radiation sickness, of whom 31 died within the first three months.\n\nIn 2005 the Chernobyl Forum, composed of the IAEA, other UN organizations and the governments of Belarus, Russia and Ukraine, published a report on the radiological environmental and health consequences of the Chernobyl accident.\n\nOn the death toll of the accident, the report states that 28 emergency workers (\"liquidators\") died from acute radiation syndrome, including beta burns, and 15 patients died from thyroid cancer in the following years, and it roughly estimated that cancer deaths caused by Chernobyl may reach a total of about 4,000 among the 5 million persons residing in the contaminated areas. The report projected cancer mortality \"increases of less than one per cent\" (~0.3%) on a time span of 80 years, cautioning that this estimate was \"speculative\" since at this time only a few cancer deaths are linked to the Chernobyl disaster. The report says it is impossible to reliably predict the number of fatal cancers arising from the incident as small differences in assumptions can result in large differences in the estimated health costs. The report says it represents the consensus view of the eight UN organizations.\n\nOf all 66,000 Belarusian emergency workers, by the mid-1990s only 150 (roughly 0.2%) were reported by their government as having died. In contrast, 5,722 casualties were reported among Ukrainian clean-up workers up to the year 1995, by the National Committee for Radiation Protection of the Ukrainian Population.\n\nThe four most harmful radionuclides spread from Chernobyl were iodine-131, caesium-134, caesium-137 and strontium-90, with half-lives of 8.02 days, 2.07 years, 30.2 years and 28.8 years respectively. The iodine was initially viewed with less alarm than the other isotopes, because of its short half-life, but it is highly volatile, and now appears to have travelled furthest and caused the most severe health problems in the short term. Strontium, on the other hand, is the least volatile of the four, and of main concern in the areas near Chernobyl itself. Iodine tends to become concentrated in thyroid and milk glands, leading, among other things, to increased incidence of thyroid cancers. Caesium tends to accumulate in vital organs such as the heart, while strontium accumulates in bones, and may thus be a risk to bone-marrow and lymphocytes. Radiation is most damaging to cells that are actively dividing. In adult mammals cell division is slow, except in hair follicles, skin, bone marrow and the gastrointestinal tract, which is why vomiting and hair loss are common symptoms of acute radiation sickness.\n\nBy the year 2000, the number of Ukrainians claiming to be radiation 'sufferers' (\"poterpili\") and receiving state benefits had jumped to 3.5 million, or 5% of the population. Many of these are populations resettled from contaminated zones, or former or current Chernobyl plant workers. According to IAEA-affiliated scientific bodies, these apparent increases of ill health result partly from economic strains on these countries and poor health-care and nutrition; also, they suggest that increased medical vigilance following the accident has meant that many cases that would previously have gone unnoticed (especially of cancer) are now being registered.\n\nThe World Health Organization states, \"children conceived before or after their father's exposure showed no statistically significant differences in mutation frequencies\". This statistically insignificant increase was also seen by independent researchers analyzing the children of the Chernobyl liquidators.\n\nOn farms in Narodychi Raion of Ukraine it is claimed that in the first four years of the disaster nearly 350 animals were born with gross deformities such as missing or extra limbs, missing eyes, heads or ribs, or deformed skulls; in comparison, only three abnormal births had been registered in the five years prior. The two primary individuals involved with the attempt to suggest that the mutation rate amongst animals was, and continues to be, higher in the Chernobyl zone, are the Anders Moller and Timothy Mousseau group. Apart from continuing to publish experimentally unrepeatable and discredited papers, Mousseau routinely gives talks at the Helen Caldicott organized symposiums for \"Physicians for Social Responsibility\", an anti-nuclear advocacy group, devoted to bring about a \"nuclear free planet\". Moreover, in years past Moller was previously caught and reprimanded for publishing papers that crossed the scientific \"misconduct\"/\"fraud\" line. The duo have more recently attempted to publish meta-analyses in which the primary references they weigh-up, analyze and draw their conclusions from is their own prior papers along with the discredited book \"\".\n\nIn 1996, geneticist colleagues Ronald Chesser and Robert Baker published a paper on the thriving vole population within the exclusion zone, in which the central conclusion of their work was essentially that \"The mutation rate in these animals is hundreds and probably thousands of times greater than normal\", this claim occurred after they had done a comparison of the mitochondrial DNA of the \"Chernobyl voles\" with that of a control group of voles from outside the region. These alarming conclusions led the paper to appear on the front cover of the prestigious journal \"Nature\", however not long after publication Chesser & Baker discovered a fundamental error in their research in which they had incorrectly classified the species of vole, and therefore were comparing the genetics of two entirely different vole species to start with.\n\nFollowing the accident, journalists mistrusted many medical professionals (such as the spokesman from the UK National Radiological Protection Board), and in turn encouraged the public to mistrust them. Throughout the European continent, due to this media-driven framing of the slight contamination and in nations where abortion is legal, many requests for induced abortions, of otherwise normal pregnancies, were obtained out of fears of radiation from Chernobyl, including an excess number of abortions in Denmark in the months following the accident. In Greece, following the accident many obstetricians were unable to resist requests from worried pregnant mothers over fears of radiation. Although it was determined that the effective dose to Greeks would not exceed 1 mSv (100 mrem), a dose much lower than that which could induce embryonic abnormalities or other non-stochastic effects, there was an observed 2500 excess of otherwise wanted pregnancies being terminated, probably out of fear in the mother of radiation risk. A \"slightly\" above the expected number of requested induced abortions occurred in Italy.\n\nWorldwide, an estimated excess of about 150,000 elective abortions may have been performed on otherwise healthy pregnancies out of unfounded fears of radiation from Chernobyl, according to Dr Robert Baker and ultimately a 1987 article published by Linda E. Ketchum in the \"Journal of Nuclear Medicine\" which mentions but does not reference an IAEA source on the matter.\n\nThe available statistical data excludes the Soviet/Ukraine/Belarus abortion rates, as they are presently unavailable. From the available data, an increase in the number of abortions in what were healthy developing human offspring in Denmark occurred in the months following the accident, at a rate of about 400 cases. In Greece, there was an observed 2500 excess of otherwise wanted pregnancies being terminated. In Italy, a \"slightly\" above the expected number of induced abortions occurred, approximately 100.\n\nNo evidence of changes in the prevalence of human deformities/birth congenital anomalies which might be associated with the accident, are apparent in Belarus or the Ukraine, the two republics which had the highest exposure to fallout. In Sweden, and Finland where no increase in abortion rates occurred, it was likewise determined that \"no association between the temporal and spatial variations in radioactivity and variable incidence of congenital malformations [was found].\" A similar null increase in the abortion rate and a healthy baseline situation of no increase in birth defects was determined by assessing the Hungarian Congenital Abnormality Registry, Findings also mirrored in Austria. Larger, \"mainly western European\" data sets approaching a million births in the EUROCAT database, divided into \"exposed\" and control groups were assessed in 1999. As no Chernobyl impacts were detected, the researchers conclude \"in retrospect the widespread fear in the population about the possible effects of exposure on the unborn fetus was not justified\". Despite studies from Germany and Turkey, the only robust evidence of negative pregnancy outcomes that transpired after the accident were these elective abortion indirect effects, in Greece, Denmark, Italy etc., due to the anxieties created.\n\nIn very high doses, it was known at the time that radiation can cause a physiological increase in the rate of pregnancy anomalies, but unlike the dominant linear-no threshold model of radiation and cancer rate increases, it was known, by researchers familiar with both the prior human exposure data and animal testing, that the \"Malformation of organs appears to be a deterministic effect with a threshold dose\" below which, no rate increase is observed. This teratology (birth defects) issue was discussed by Frank Castronovo of the Harvard Medical School in 1999, publishing a detailed review of dose reconstructions and the available pregnancy data following the Chernobyl accident, inclusive of data from Kiev's two largest obstetrics hospitals. Castronovo concludes that \"the lay press with newspaper reporters playing up anecdotal stories of children with birth defects\" is, together with dubious studies that show selection bias, the two primary factors causing the persistent belief that Chernobyl increased the background rate of birth defects. When the vast amount of pregnancy data does not support this perception as no women took part in the most radioactive liquidator operations, no in-utero individuals would have been expected to have received a threshold dose.\n\nIn one small behavioral study in 1998, with low statistical power and limited Multivariate analysis which akin to the widely published Hiroshima and Nagasaki studies, investigated and selected the children; who were in utero during the rapidly dividing and therefore radiosensitive phase of neurogenesis(8 to 16 weeks of gestation), and whose mothers were evacuated from some of the more energetic hot-spot parts of the Chernobyl exclusion zone following the accident. From a random selection of 50 individuals in late-childhood in 1998, a low quality statistically-significant increase in the rate of severe IQ reduction was found, with a threshold of a suggested ~ 0.30 Sv(300 mSv) as a \"thyroid dose\" to the developing human head, for the beginning emergence of cerebral disorder.\n\nThe Chernobyl liquidators, essentially an all-male civil defense emergency workforce, would go on to father normal children, without an increase in developmental anomalies or a statistically significant increase in the frequencies of germline mutations in their progeny. This normality is similarly seen in the children of the survivors of the Goiana accident.\n\nA report by the International Atomic Energy Agency examines the environmental consequences of the accident. The United Nations Scientific Committee on the Effects of Atomic Radiation has estimated a global collective dose of radiation exposure from the accident \"equivalent on average to 21 additional days of world exposure to natural background radiation\"; individual doses were far higher than the global mean among those most exposed, including 530,000 primarily male recovery workers (the Chernobyl liquidators) who averaged an effective dose equivalent to an extra 50 years of typical natural background radiation exposure each.\n\nEstimates of the number of deaths that will eventually result from the accident vary enormously; disparities reflect both the lack of solid scientific data and the different methodologies used to quantify mortality—whether the discussion is confined to specific geographical areas or extends worldwide, and whether the deaths are immediate, short term, or long term.\n\nIn 1994, thirty-one deaths were directly attributed to the accident, all among the reactor staff and emergency workers. As of the 2008 report by the United Nations Scientific Committee on the Effects of Atomic Radiation, and the total number of confirmed deaths from radiation was 64 and was expected to continue to rise.\n\nThe Chernobyl Forum predicts that the eventual death toll could reach 4,000 among those exposed to the \"highest levels of radiation\" (200,000 emergency workers, 116,000 evacuees and 270,000 residents of the most contaminated areas); this figure is a total causal death toll prediction, combining the deaths of approximately 50 emergency workers who died soon after the accident from acute radiation syndrome, 15 children who have died of thyroid cancer and a future predicted total of 3935 deaths from radiation-induced cancer and leukaemia.\n\nIn a peer-reviewed paper in the \"International Journal of Cancer\" in 2006, the authors expanded the discussion on those exposed to all of Europe (but following a different conclusion methodology to the Chernobyl Forum study, which arrived at the total predicted death toll of 4,000 after cancer survival rates were factored in) they stated, without entering into a discussion on deaths, that in terms of total excess cancers attributed to the accident:\n\nTwo anti-nuclear advocacy groups have publicized non-peer-reviewed estimates that include mortality estimates for those who were exposed to even smaller amounts of radiation. The Union of Concerned Scientists (UCS) calculated that, among the hundreds of millions of people exposed worldwide, there will be an eventual 50,000 excess cancer cases, resulting in 25,000 excess cancer deaths, excluding thyroid cancer. However, these calculations are based on a simple linear no-threshold model multiplication and the misapplication of the collective dose, which the International Commission on Radiological Protection (ICRP) states \"should not be done\" as using the collective dose is \"inappropriate to use in risk projections\".\n\nAlong similar lines to the UCS approach, the 2006 TORCH report, commissioned by the European Greens political party, likewise simplistically calculates an eventual 30,000 to 60,000 excess cancer deaths in total, around the globe.\nDue in largest part from the ingestion of contaminated dairy products along with the inhalation of the short-lived and therefore highly radioactive isotope, Iodine-131, the 2005 UN collaborative \"Chernobyl Forum\" revealed thyroid cancer among children to be one of the main health impacts from the Chernobyl accident. In that publication more than 4000 cases were reported, and that there was no evidence of an increase in solid cancers or leukemia. It said that there was an increase in psychological problems among the affected population. Dr Michael Repacholi, manager of WHO's Radiation Program reported that the 4000 cases of thyroid cancer resulted in nine deaths.\n\nAccording to the United Nations Scientific Committee on the Effects of Atomic Radiation, up to the year 2005, an excess of over 6000 cases of thyroid cancer have been reported. That is, over the estimated pre-accident baseline thyroid cancer rate, more than 6000 casual cases of thyroid cancer have been reported in children and adolescents exposed at the time of the accident, a number that is expected to increase. They concluded that there is no other evidence of major health impacts from the radiation exposure.\n\nWell-differentiated thyroid cancers are generally treatable, and when treated the five-year survival rate of thyroid cancer is 96%, and 92% after 30 years. the United Nations Scientific Committee on the Effects of Atomic Radiation had reported 15 deaths from thyroid cancer in 2011. The International Atomic Energy Agency (IAEA) also states that there has been no increase in the rate of birth defects or abnormalities, or solid cancers (such as lung cancer) corroborating the assessments by the UN committee. UNSCEAR raised the possibility of long term genetic defects, pointing to a doubling of radiation-induced minisatellite mutations among children born in 1994. However, the risk of thyroid cancer associated with the Chernobyl accident is still high according to published studies.\n\nThe German affiliate of the ultra-anti-nuclear energy organization, the \"International Physicians for the Prevention of Nuclear War\" suggest that 10,000 people are affected by thyroid cancer as of 2006 and that 50,000 cases are expected in the future.\n\nFred Mettler, a radiation expert at the University of New Mexico, puts the number of worldwide cancer deaths outside the highly contaminated zone at \"perhaps\" 5000, for a total of 9000 Chernobyl-associated fatal cancers, saying \"the number is small (representing a few percent) relative to the normal spontaneous risk of cancer, but the numbers are large in absolute terms\". The same report outlined studies based in data found in the Russian Registry from 1991 to 1998 that suggested that \"of 61,000 Russian workers exposed to an average dose of 107 mSv about 5% of all fatalities that occurred may have been due to radiation exposure.\"\n\nThe report went into depth about the risks to mental health of exaggerated fears about the effects of radiation. According to the IAEA the \"designation of the affected population as \"victims\" rather than \"survivors\" has led them to perceive themselves as helpless, weak and lacking control over their future\". The IAEA says that this may have led to behaviour that has caused further health effects.\n\nFred Mettler commented that 20 years later: \"The population remains largely unsure of what the effects of radiation actually are and retain a sense of foreboding. A number of adolescents and young adults who have been exposed to modest or small amounts of radiation feel that they are somehow fatally flawed and there is no downside to using illicit drugs or having unprotected sex. To reverse such attitudes and behaviours will likely take years although some youth groups have begun programs that have promise.\" In addition, disadvantaged children around Chernobyl suffer from health problems that are attributable not only to the Chernobyl accident, but also to the poor state of post-Soviet health systems.\n\nThe United Nations Scientific Committee on the Effects of Atomic Radiation (UNSCEAR), part of the Chernobyl Forum, have produced their own assessments of the radiation effects. UNSCEAR was set up as a collaboration between various United Nation bodies, including the World Health Organization, after the atomic bomb attacks on Hiroshima and Nagasaki, to assess the long-term effects of radiation on human health.\n\nThe number of potential deaths arising from the Chernobyl disaster is heavily debated. The WHO's prediction of 4000 future cancer deaths in surrounding countries is based on the Linear no-threshold model (LNT), which assumes that the damage inflicted by radiation at low doses is directly proportional to the dose. Radiation epidemiologist Roy Shore contends that estimating health effects in a population from the LNT model \"is not wise because of the uncertainties\".\n\nAccording to the Union of Concerned Scientists the number of excess cancer deaths worldwide (including all contaminated areas) is approximately 27,000 based on the same LNT.\n\nAnother study critical of the Chernobyl Forum report was commissioned by Greenpeace, which asserted that the most recently published figures indicate that in Belarus, Russia and Ukraine the accident could have resulted in 10,000–200,000 additional deaths in the period between 1990 and 2004. The Scientific Secretary of the Chernobyl Forum criticized the report's reliance on non-peer-reviewed locally produced studies. Although most of the study's sources were from peer-reviewed journals, including many Western medical journals, the higher mortality estimates were from non-peer-reviewed sources, while Gregory Härtl (spokesman for the WHO) suggested that the conclusions were motivated by ideology.\n\n\"\" is a 2007 Russian publication that concludes that there were 985,000 premature deaths as a result of the radioactivity released. The results were criticized by M. I. Balonov of the Institute of Radiation Hygiene in St. Petersburg, who described them as biased, drawing from sources which were difficult to independently verify and lacking a proper scientific base. Balanov expressed his opinion that \"the authors unfortunately did not appropriately analyze the content of the Russian-language publications, for example, to separate them into those that contain scientific evidence and those based on hasty impressions and ignorant conclusions\".\n\nAccording to Kenneth Mossman, a Professor of Health Physics and member of the U.S. Nuclear Regulatory Commission advisory committee, the \"LNT philosophy is overly conservative, and low-level radiation may be less dangerous than commonly believed\". Yoshihisa Matsumoto, a radiation biologist at the Tokyo Institute of Technology, cites laboratory experiments on animals to suggest there must be a threshold dose below which DNA repair mechanisms can completely repair any radiation damage. Mossman suggests that the proponents of the current model believe that being conservative is justified due to the uncertainties surrounding low level doses and it is better to have a \"prudent public health policy\".\n\nAnother significant issue is establishing consistent data on which to base the analysis of the impact of the Chernobyl accident. Since 1991 large social and political changes have occurred within the affected regions and these changes have had significant impact on the administration of health care, on socio-economic stability, and the manner in which statistical data is collected. Ronald Chesser, a radiation biologist at Texas Tech University, says that \"the subsequent Soviet collapse, scarce funding, imprecise dosimetry, and difficulties tracking people over the years have limited the number of studies and their reliability\".\n\nIt is difficult to establish the total economic cost of the disaster. According to Mikhail Gorbachev, the Soviet Union spent 18 billion rubles (the equivalent of US$18 billion at that time) on containment and decontamination, virtually bankrupting itself. In Belarus the total cost over 30 years is estimated at US$235 billion (in 2005 dollars). Ongoing costs are well known; in their 2003–2005 report, The Chernobyl Forum stated that between 5% and 7% of government spending in Ukraine is still related to Chernobyl, while in Belarus over $13 billion is thought to have been spent between 1991 and 2003, with 22% of national budget having been Chernobyl-related in 1991, falling to 6% by 2002. Much of the current cost relates to the payment of Chernobyl-related social benefits to some 7 million people across the 3 countries.\n\nA significant economic impact at the time was the removal of of agricultural land and of forest from production. While much of this has been returned to use, agricultural production costs have risen due to the need for special cultivation techniques, fertilizers and additives.\n\nPolitically, the accident gave great significance to the new Soviet policy of glasnost, and helped forge closer Soviet–US relations at the end of the Cold War, through bioscientific cooperation. The disaster also became a key factor in the Union's eventual 1991 dissolution, and a major influence in shaping the new Eastern Europe.\n\nBoth Ukraine and Belarus, in their first months of independence, lowered legal radiation thresholds from the Soviet Union's previous, elevated thresholds (from 35 rems per lifetime under the USSR to 7 rems per lifetime in Ukraine and 0.1 rems per year in Belarus). This required an expansion of territories that were considered contaminated. In Ukraine, over 500,000 people have now been resettled, many of whom have become applicants for medical and other welfare. Ukraine also maintains the destroyed reactor, for which it employs a very large workforce in order to keep individual exposure times low. Many of these workers have since registered disabilities and enrolled for welfare. In Ukraine, the Chernobyl disaster was an icon of the nationalist movement, symbolic of all that was wrong with the Soviet Union, and welfare became a key platform for winning independence. Ukraine has since developed a massive and burdensome welfare system that has become increasingly corrupt and ineffective. It has presented its greatly increased welfare demands since 1991 as a demonstration of its own moral legitimacy, and as an argument for needing foreign aid. Belarus, on the other hand, was politically weak when it gained independence, and looked to Moscow for guidance; in many ways it has returned to the old Soviet policy of secrecy and denial.\n\nFollowing the accident, questions arose about the future of the plant and its eventual fate. All work on the unfinished reactors 5 and 6 was halted three years later. However, the trouble at the Chernobyl plant did not end with the disaster in reactor 4. The damaged reactor was sealed off and of concrete was placed between the disaster site and the operational buildings. The work was managed by Grigoriy Mihaylovich Naginskiy, the deputy chief engineer of Installation and Construction Directorate – 90. The Ukrainian government continued to let the three remaining reactors operate because of an energy shortage in the country.\n\nIn October 1991, a fire broke out in the turbine building of reactor 2; the authorities subsequently declared the reactor damaged beyond repair, and it was taken offline. Reactor 1 was decommissioned in November 1996 as part of a deal between the Ukrainian government and international organizations such as the IAEA to end operations at the plant. On 15 December 2000, then-President Leonid Kuchma personally turned off Reactor 3 in an official ceremony, shutting down the entire site.\n\nThe Chernobyl reactor is now enclosed in a large concrete sarcophagus, which was built quickly to allow continuing operation of the other reactors at the plant.\n\nA New Safe Confinement was to have been built by the end of 2005; however, it has suffered ongoing delays and , when construction finally began, was expected to be completed in 2013. This was delayed again to 2016, the end of the 30-year lifespan of the sarcophagus. The structure was built adjacent to the existing shelter and in November 2016 was slid into place on rails. It is a metal arch high and spanning , to cover both unit 4 and the hastily built 1986 structure. The Chernobyl Shelter Fund, set up in 1997, has received €810 million from international donors and projects to cover this project and previous work. It and the Nuclear Safety Account, also applied to Chernobyl decommissioning, are managed by the European Bank for Reconstruction and Development (EBRD).\n\n, Reactor No. 4 has been covered by the New Safe Confinement that covers the reactor and the unstable \"sarcophagus\". The huge steel arch was moved into place over several weeks, and the completion of this procedure was celebrated with a ceremony at the site, attended by the Ukrainian president, Petro Poroshenko, diplomats and site workers. Unlike the original sarcophagus, the New Safe Confinement is designed to allow the reactor to be safely dismantled using remotely operated equipment.\n\nBy 2002, roughly 15,000 Ukrainian workers were still working within the Zone of Exclusion, maintaining the plant and performing other containment- and research-related tasks, often in dangerous conditions.\nA handful of Ukrainian scientists work inside the sarcophagus, but outsiders are rarely granted access. In 2006 an Australian \"60 Minutes\" team led by reporter Richard Carleton and producer Stephen Rice were allowed to enter the sarcophagus for 15 minutes and film inside the control room.\n\nOn 12 February 2013, a section of the roof of the turbine-building, adjacent to the sarcophagus, collapsed. At first it was assumed that the roof collapsed because of the weight of snow on it. However the amount of snow was not exceptional, and the report of a Ukrainian fact-finding panel concluded that the part collapse of the turbine-building was the result of sloppy repair work and aging of the structure. The report mentioned the possibility that the repaired part of the turbine-building added a larger strain on the total structure than expected, and the braces in the roof were damaged by corrosion and sloppy welding. Experts such as Valentin Kupny, former deputy director of the nuclear plant, did warn that the complex was on the verge of a collapse, leaving the building in an extremely dangerous condition. A proposed reinforcement in 2005 was cancelled by a superior official. After the 12 February incident, radioactivity levels were up to 19 becquerels per cubic meter of air: 12 times normal. The report assumed radioactive materials from inside the structure spread to the surroundings after the roof collapsed. All 225 workers employed by the Chernobyl complex and the French company that is building the new shelter were evacuated shortly after the collapse. According to the managers of the complex, radiation levels around the plant were at normal levels (between 5 and 6 µSv/h) and should not affect workers' health. According to Kupny the situation was underestimated by the Chernobyl nuclear complex managers, and information was kept secret.\n\n, some fuel remained in the reactors at units 1 through 3, most of it in each unit's spent fuel pool, as well as some material in a small spent fuel interim storage facility pond (ISF-1).\n\nIn 1999 a contract was signed for construction of a radioactive waste management facility to store 25,000 used fuel assemblies from units 1–3 and other operational wastes, as well as material from decommissioning units 1–3 (which will be the first RBMK units decommissioned anywhere). The contract included a processing facility able to cut the RBMK fuel assemblies and to put the material in canisters, which were to be filled with inert gas and welded shut.\n\nThe canisters were to be transported to dry storage vaults, where the fuel containers would be enclosed for up to 100 years. This facility, treating 2500 fuel assemblies per year, would be the first of its kind for RBMK fuel. However, after a significant part of the storage structures had been built, technical deficiencies in the concept emerged, and the contract was terminated in 2007. The interim spent fuel storage facility (ISF-2) will now be completed by others by mid-2013.\n\nAnother contract has been let for a liquid radioactive waste treatment plant, to handle some 35,000 cubic meters of low- and intermediate-level liquid wastes at the site. This will need to be solidified and eventually buried along with solid wastes on site.\n\nIn January 2008, the Ukrainian government announced a 4-stage decommissioning plan that incorporates the above waste activities and progresses towards a cleared site.\n\nAccording to official estimates, about 95% of the fuel in Reactor 4 at the time of the accident (about 180 metric tons) remains inside the shelter, with a total radioactivity of nearly 18 million curies (670 PBq). The radioactive material consists of core fragments, dust, and lava-like \"fuel containing materials\" (FCM)—also called \"corium\"—that flowed through the wrecked reactor building before hardening into a ceramic form.\n\nThree different lavas are present in the basement of the reactor building: black, brown, and a porous ceramic. The lava materials are silicate glasses with inclusions of other materials within them. The porous lava is brown lava that dropped into water and thus cooled rapidly.\n\nIt is unclear how long the ceramic form will retard the release of radioactivity. From 1997 to 2002 a series of published papers suggested that the self-irradiation of the lava would convert all 1,200 metric tons into a submicrometre and mobile powder within a few weeks. But it has been reported that the degradation of the lava is likely to be a slow and gradual process rather than sudden and rapid. The same paper states that the loss of uranium from the wrecked reactor is only per year; this low rate of uranium leaching suggests that the lava is resisting its environment. The paper also states that when the shelter is improved, the leaching rate of the lava will decrease.\n\nSome of the surfaces of the lava flows have started to show new uranium minerals such as čejkaite () and uranyl carbonate. However, the level of radioactivity is such that during 100 years, the lava's self irradiation ( α decays per gram and 2 to of β or γ) will fall short of the level required to greatly change the properties of glass (10 α decays per gram and 10 to 10 Gy of β or γ). Also the lava's rate of dissolution in water is very low (10 g·cm·day), suggesting that the lava is unlikely to dissolve in water.\n\nAn area originally extending in all directions from the plant is officially called the \"zone of alienation\". It is largely uninhabited, except for about 300 residents who have refused to leave. The area has largely reverted to forest, and has been overrun by wildlife because of a lack of competition with humans for space and resources. Even today, radiation levels are so high that the workers responsible for rebuilding the sarcophagus are only allowed to work five hours a day for one month before taking 15 days of rest. Ukrainian officials estimated the area would not be safe for human life again for another 20,000 years (although by 2016, 187 local Ukrainians had returned and were living permanently in the zone).\n\nIn 2011 Ukraine opened up the sealed zone around the Chernobyl reactor to tourists who wish to learn more about the tragedy that occurred in 1986. Sergii Mirnyi, a radiation reconnaissance officer at the time of the accident, and now an academic at National University of Kyiv-Mohyla Academy in Kiev, Ukraine, has written about the psychological and physical effects on survivors and visitors, and worked as an advisor to Chernobyl tourism groups.\n\nDuring the dry seasons, a perennial concern is if the forests that have been contaminated by radioactive material catch on fire, then depending on the prevailing atmospheric conditions, the fires could potentially spread the radioactive material further outwards from the exclusion zone in the smoke. In Belarus, the Bellesrad organization is tasked with overseeing the food cultivation and forestry management in the area.\n\nThe Chernobyl Shelter Fund was established in 1997 at the Denver 23rd G8 summit to finance the Shelter Implementation Plan (SIP). The plan calls for transforming the site into an ecologically safe condition by means of stabilization of the sarcophagus followed by construction of a New Safe Confinement (NSC). While the original cost estimate for the SIP was US$768 million, the 2006 estimate was $1.2 billion. The SIP is being managed by a consortium of Bechtel, Battelle, and Électricité de France, and conceptual design for the NSC consists of a movable arch, constructed away from the shelter to avoid high radiation, to be slid over the sarcophagus. The NSC was moved into position in November 2016 and is expected to be completed in late 2017, and is the largest movable structure ever built.\n\nDimensions:\n\nThe United Nations Development Programme has launched in 2003 a specific project called the Chernobyl Recovery and Development Programme (CRDP) for the recovery of the affected areas. The programme was initiated in February 2002 based on the recommendations in the report on Human Consequences of the Chernobyl Nuclear Accident. The main goal of the CRDP's activities is supporting the Government of Ukraine in mitigating long-term social, economic, and ecological consequences of the Chernobyl catastrophe. CRDP works in the four most Chernobyl-affected areas in Ukraine: Kyivska, Zhytomyrska, Chernihivska and Rivnenska.\n\nThe International Project on the Health Effects of the Chernobyl Accident (IPEHCA) was created and received US $20 million, mainly from Japan, in hopes of discovering the main cause of health problems due to I radiation. These funds were divided among Ukraine, Belarus, and Russia, the three main affected countries, for further investigation of health effects. As there was significant corruption in former Soviet countries, most of the foreign aid was given to Russia, and no positive outcome from this money has been demonstrated.\n\nChernobyl Children International (CCI) is a United Nations-accredited, non-profit, international development, medical, and humanitarian organization that works with children, families and communities that continue to be affected by the economic outcome of the Chernobyl accident. The organization's founder and chief executive is Adi Roche. The CCI was founded in 1991 in response to an appeal from Ukrainian and Belarusian doctors for aid. Roche then began organizing 'rest and recuperation' holidays for a few Chernobyl children. Recruiting Irish families who would welcome and care for them, CCI expanded into the United States in 2001.\n\nIt works closely with the Belarusian government, the United Nations, and many thousands of volunteers worldwide to deliver a broad range of economic supports to the children and the wider community. It also acts as an advocate for the rights of those affected by the Chernobyl explosion, and engages in research and outreach activities to encourage the rest of the world to remember the victims and understand the long-term impact on their lives.\n\n\"The Front Veranda\" (1986), a lithograph by Susan Dorothea White in the National Gallery of Australia, exemplifies worldwide awareness of the event. \"Heavy Water: A Film for Chernobyl\" was released by Seventh Art in 2006 to commemorate the disaster through poetry and first-hand accounts. The film secured the Best Short Documentary at Cinequest Film Festival as well as the Rhode Island \"best score\" award along with a screening at Tate Modern.\n\nChernobyl Way is an annual rally run on 26 April by the opposition in Belarus as a remembrance of the Chernobyl disaster.\n\nThe Chernobyl accident attracted a great deal of interest. Because of the distrust that many people (both within and outside the USSR) had in the Soviet authorities, a great deal of debate about the situation at the site occurred in the First World during the early days of the event. Because of defective intelligence based on photographs taken from space, it was thought that unit number three had also suffered a dire accident.\n\nJournalists mistrusted many professionals (such as the spokesman from the UK NRPB), and they in turn encouraged the public to mistrust them.\n\nIn Italy, the Chernobyl accident was reflected in the outcome of the 1987 referendum. As a result of that referendum, Italy began phasing out its nuclear power plants in 1988, a decision that was effectively reversed in 2008. A referendum in 2011 reiterated Italians' strong objections to nuclear power, thus abrogating the government's decision of 2008.\n\nIn Germany, the Chernobyl accident led to the creation of a federal environment ministry, after several states had already created such a post. The minister was given the authority over reactor safety as well, which the current minister still holds . The events are also credited with strengthening the anti-nuclear power movement, which culminated in the decision to end the use of nuclear power that was made by the 1998–2005 Schröder government.\n\n\nExplanatory notes\nCitations\nSources\n\nThe source documents relating to the emergency, published in unofficial sources:\n\n"}
{"id": "38602468", "url": "https://en.wikipedia.org/wiki?curid=38602468", "title": "China-EU Institute for Clean and Renewable Energy", "text": "China-EU Institute for Clean and Renewable Energy\n\nThe China-EU Institute for Clean and Renewable Energy at Huazhong University of Science & Technology (CE-ICARE; ) is an education and research institute located in Wuhan, China, created in July 2010 and hosted in Huazhong University of Science and Technology. ICARE is the third Sino-European institute being created in China after the China-EU International Business School in Shanghai in 1994 (as China-EC Management Institute) and the China-EU School of Law in Beijing in 2008. ICARE project has a 5-year funding from EU and Chinese government.\n\nThe creation of ICARE follows an agreement signed in 2009 by European Commissioner for External Relations Benita Ferrero-Waldner and Chinese Minister for Foreign Trade GAO Hucheng in the frame of the collaboration between China and Europe for environmental protection and climate change mitigation.\n\nICARE mission is to support China in implementing activities in management and technology in order to reduce the consumption of fossil energy and carbon emissions through renewable energies and energy efficiency.\n\nICARE is providing education to university students as well as to Chinese professionals and as the strong will to become the reference institute on energy efficiency and renewable energy in China. ICARE relies on a partner consortium of higher-education institutions composed of 7 European members from 5 countries and 3 Chinese members.\n\n\nIt is a Double master's degree Programme, namely, the master programme of energy science and technology conferred by Huazhong University of Science and Technology (HUST) in China, and the master programme of clean and renewable energy granted by ParisTech (CARE), for students who already have a degree in engineering or in another relevant scientific discipline in areas where there is a strong need for additional qualified workforce in China. Courses are taught in English by European and Chinese teachers, all experts in their fields. Main subjects are Solar Energy, Wind Energy, Biomass, Geothermal, Hydrogen and Energy Storage and Energy Efficiency. There is a six-month training period in laboratories in China or in Europe which allows the students to put into practice skills and knowledge acquired during courses. Presently 160 students are following the Master courses. First class was graduated (Master from ParisTech and Master from HuaZhong University) on March 15, 2013.\n\nTo meet the needs of domestic and international companies for professional training of their staff (decision makers, engineers, etc.) on CRE, ICARE aims at developing a VT platform. Tailored professional programmes are developed by European partners or associated partners. First sessions were held during three days in January 2013 on Photovoltaic energy technologies.\n\nA Research Platform (RP) is developed to facilitate exchange of Ph.D. students between European and Chinese universities and co-supervision of research activities. RP is also a contact facilitator between European professors, during their teaching stays in ICARE, and Chinese professors to meet and develop common research projects. Every year top specialists in CREN fields are delivering conferences at ICARE.\n\n"}
{"id": "57950894", "url": "https://en.wikipedia.org/wiki?curid=57950894", "title": "Composite boson", "text": "Composite boson\n\nA composite boson is a bound state of Fermions such that the combination gives a boson. Some examples are: Cooper pairs, mesons, Superfluid helium, Bose–Einstein condensates, Atomic bosons, and many other composite quantum states like Fermionic condensates. A composite particle containing an even number of fermions is a boson, since it has integer spin. These composite particle states have a symmetric wave function upon exchange of any pair of particles. The wave function is given by the permanent of single particle states for the non interacting case.\n\n"}
{"id": "7958880", "url": "https://en.wikipedia.org/wiki?curid=7958880", "title": "D'Alembert–Euler condition", "text": "D'Alembert–Euler condition\n\nIn mathematics and physics, especially the study of mechanics and fluid dynamics, the d'Alembert-Euler condition is a requirement that the streaklines of a flow are irrotational. Let x = x(X,\"t\") be the coordinates of the point x into which X is carried at time \"t\" by a (fluid) flow. Let formula_1 be the second material derivative of x. Then the d'Alembert-Euler condition is:\n\nThe d'Alembert-Euler condition is named for Jean le Rond d'Alembert and Leonhard Euler who independently first described its use in the mid-18th century. It is not to be confused with the Cauchy–Riemann conditions.\n\n"}
{"id": "18205941", "url": "https://en.wikipedia.org/wiki?curid=18205941", "title": "Danish Organisation for Renewable Energy", "text": "Danish Organisation for Renewable Energy\n\nSustainableEnergy (VedvarendeEnergi) which was called Danish Organisation for Renewable Energy (OVE) until 2010, is a non-governmental, non-profit, membership based association. SustainableEnergy was founded in 1975 based on a popular movement for renewable energy in Denmark with close relationship to the anti-nuclear movement (OOA), which had an immense popular backup.\n\nSustainableEnergy’s aim is to work for a resource- and environment-conscious energy policy through grassroots initiatives to reach 100% renewable energy supply in Denmark by 2030.\n\nSustainableEnergy has a strong engagement:\n\nMembers: - Danish individuals who are users of renewable energy, as well as of groups, high schools, and companies working for renewable energy in Denmark. A special category of the members are the local Energy and Environment Offices, local popular information centers. In total, SustainableEnergy has about 2500 members.\n\nDecisions: - SustainableEnergy's main decisions are made at the yearly General Assembly of the members and the bimonthly Board Meetings. The Members of the Board is elected at the General Assembly. The Chairman is elected by the Board.\n\nInformation: - The members are informed through a bimonthly magazine, in several subject-specific meetings, and a web site.\n\nPolitical Lobbying\n\nInformation Dissemination\n\nSustainableEnergy has gone into co-operation with many other organisations interested in energy in Denmark. Among others:\n\n\nSustainableEnergy plays a significant role in international networking among NGOs. SustainableEnergy is member of:\n\n\n\n"}
{"id": "40163", "url": "https://en.wikipedia.org/wiki?curid=40163", "title": "Darmstadtium", "text": "Darmstadtium\n\nDarmstadtium is a synthetic chemical element with symbol Ds and atomic number 110. It is an extremely radioactive synthetic element. The most stable known isotope, darmstadtium-281, has a half-life of approximately 10 seconds. Darmstadtium was first created in 1994 by the GSI Helmholtz Centre for Heavy Ion Research near the city of Darmstadt, Germany, after which it was named.\n\nIn the periodic table, it is a d-block transactinide element. It is a member of the 7th period and is placed in the group 10 elements, although no chemical experiments have yet been carried out to confirm that it behaves as the heavier homologue to platinum in group 10 as the eighth member of the 6d series of transition metals. Darmstadtium is calculated to have similar properties to its lighter homologues, nickel, palladium, and platinum.\n\nDarmstadtium was first created on November 9, 1994, at the Institute for Heavy Ion Research (Gesellschaft für Schwerionenforschung, GSI) in Darmstadt, Germany, by Peter Armbruster and Gottfried Münzenberg, under the direction of Sigurd Hofmann. The team bombarded a lead-208 target with accelerated nuclei of nickel-62 in a heavy ion accelerator and detected a single atom of the isotope darmstadtium-269:\n\nIn the same series of experiments, the same team also carried out the reaction using heavier nickel-64 ions. During two runs, 9 atoms of Ds were convincingly detected by correlation with known daughter decay properties:\n\nPrior to this, there had been failed synthesis attempts in 1986–7 at the Joint Institute for Nuclear Research in Dubna (then in the Soviet Union) and in 1990 at the GSI; a 1995 attempt at the Lawrence Berkeley National Laboratory resulted in signs suggesting but not pointing conclusively at the discovery of a new isotope Ds formed in the bombardment of Bi with Co, and a similarly inconclusive 1994 attempt at the JINR showed signs of Ds being produced from Pu and S. Each team proposed its own name for element 110: the American team proposed \"hahnium\" after Otto Hahn in an attempt to resolve the situation on element 105 (which they had long been suggesting this name for), the Russian team proposed \"becquerelium\" after Henri Becquerel, and the German team proposed \"darmstadtium\" after Darmstadt, the location of their institute. The IUPAC/IUPAP Joint Working Party (JWP) recognised the GSI team as discoverers in their 2001 report, giving them the right to suggest a name for the element.\n\nUsing Mendeleev's nomenclature for unnamed and undiscovered elements, darmstadtium should be known as \"eka-platinum\". In 1979, IUPAC published recommendations according to which the element was to be called \"ununnilium\" (with the corresponding symbol of \"Uun\"), a systematic element name as a placeholder, until the element was discovered (and the discovery then confirmed) and a permanent name was decided on. Although widely used in the chemical community on all levels, from chemistry classrooms to advanced textbooks, the recommendations were mostly ignored among scientists in the field, who called it \"element 110\", with the symbol of \"E110\", \"(110)\" or even simply \"110\".\n\nIn 1996, the Russian team proposed the name \"becquerelium\" after Henri Becquerel. The American team in 1997 proposed the name \"hahnium\" after Otto Hahn (previously this name had been used for element 105).\n\nThe name \"darmstadtium\" (Ds) was suggested by the GSI team in honor of the city of Darmstadt, where the element was discovered. The GSI team originally also considered naming the element \"wixhausium\", after the suburb of Darmstadt known as Wixhausen where the element was discovered, but eventually decided on \"darmstadtium\". The new name was officially recommended by IUPAC on August 16, 2003.\n\nDarmstadtium has no stable or naturally occurring isotopes. Several radioactive isotopes have been synthesized in the laboratory, either by fusing two atoms or by observing the decay of heavier elements. Nine different isotopes of darmstadtium have been reported with atomic masses 267, 269–271, 273, 277, and 279–281, although darmstadtium-267 and darmstadtium-280 are unconfirmed. Two darmstadtium isotopes, darmstadtium-270 and darmstadtium-271, have known metastable states. Most of these decay predominantly through alpha decay, but some undergo spontaneous fission.\n\nAll darmstadtium isotopes are extremely unstable and radioactive; in general, the heavier isotopes are more stable than the lighter. The most stable known darmstadtium isotope, Ds, is also the heaviest known darmstadtium isotope; it has a half-life of 11 seconds. The isotope Ds has a half-life of 0.18 seconds respectively. The remaining six isotopes and two metastable states have half-lives between 1 microsecond and 70 milliseconds. Some unknown isotopes in this region, such as Ds and Ds, are predicted to also have rather long half-lives of a few seconds. Before its discovery, Ds was predicted to have a half-life of around 5 seconds, but it has since been found to have a half-life of only 5.7 milliseconds. Likewise, while Ds was originally predicted to have a half-life of about 11 seconds, 2015 work that may have detected it as the electron-capture daughter of Rg has found for it a half-life of much less than a second, and studies in 2014 and 2016 finding it as the possible alpha daughter of Cn found a half-life of about 6.7 ms.\nThe undiscovered isotope Ds has been predicted to be the most stable towards beta decay; however, no known darmstadtium isotope has been observed to undergo beta decay. Theoretical calculation in a quantum tunneling model reproduces the experimental alpha decay half-life data for the known darmstadtium isotopes. It also predicts that the undiscovered isotope Ds, which has a magic number of neutrons (184), would have an alpha decay half-life on the order of 311 years: exactly the same approach predicts a ~3500-year alpha half-life for the non-magic Ds isotope, however.\n\nDarmstadtium is the eighth member of the 6d series of transition metals. Since copernicium (element 112) has been shown to be a group 12 metal, it is expected that all the elements from 104 to 111 would continue a fourth transition metal series, with darmstadtium as part of the platinum group metals and a noble metal. Calculations on its ionization potentials and atomic and ionic radii are similar to that of its lighter homologue platinum, thus implying that darmstadtium's basic properties will resemble those of the other group 10 elements, nickel, palladium, and platinum.\n\nPrediction of the probable chemical properties of darmstadtium has not received much attention recently. Darmstadtium is expected to be a noble metal. Based on the most stable oxidation states of the lighter group 10 elements, the most stable oxidation states of darmstadtium are predicted to be the +6, +4, and +2 states; however, the neutral state is predicted to be the most stable in aqueous solutions. In comparison, only palladium and platinum are known to show the maximum oxidation state in the group, +6, while the most stable states are +4 and +2 for both nickel and palladium. It is further expected that the maximum oxidation states of elements from bohrium (element 107) to darmstadtium (element 110) may be stable in the gas phase but not in aqueous solution. Darmstadtium hexafluoride (DsF) is predicted to have very similar properties to its lighter homologue platinum hexafluoride (PtF), having very similar electronic structures and ionization potentials. It is also expected to have the same octahedral molecular geometry as PtF. Other predicted darmstadtium compounds are darmstadtium carbide (DsC) and darmstadtium tetrachloride (DsCl), both of which are expected to behave like their lighter homologues. Unlike platinum, which preferentially forms a cyanide complex in its +2 oxidation state, Pt(CN), darmstadtium is expected to preferentially remain in its neutral state and form instead, forming a strong Ds–C bond with some multiple bond character.\n\nDarmstadtium is expected to be a solid under normal conditions and to crystallize in the body-centered cubic structure, unlike its lighter congeners which crystallize in the face-centered cubic structure, because it is expected to have different electron charge densities from them. It should be a very heavy metal with a density of around 34.8 g/cm. In comparison, the densest known element that has had its density measured, osmium, has a density of only 22.61 g/cm. This results from darmstadtium's high atomic weight, the lanthanide and actinide contractions, and relativistic effects, although production of enough darmstadtium to measure this quantity would be impractical, and the sample would quickly decay.\n\nThe outer electron configuration of darmstadtium is calculated to be 6d7s, which obeys the Aufbau principle and does not follow platinum's outer electron configuration of 5d6s. This is due to the relativistic stabilization of the 7s electron pair over the whole seventh period, so that none of the elements from 104 to 112 are expected to have electron configurations violating the Aufbau principle. The atomic radius of darmstadtium is expected to be around 132 pm.\n\nUnambiguous determination of the chemical characteristics of darmstadtium has yet to have been established due to the short half-lives of darmstadtium isotopes and a limited number of likely volatile compounds that could be studied on a very small scale. One of the few darmstadtium compounds that are likely to be sufficiently volatile is darmstadtium hexafluoride (), as its lighter homologue platinum hexafluoride () is volatile above 60 °C and therefore the analogous compound of darmstadtium might also be sufficiently volatile; a volatile octafluoride () might also be possible. For chemical studies to be carried out on a transactinide, at least four atoms must be produced, the half-life of the isotope used must be at least 1 second, and the rate of production must be at least one atom per week. Even though the half-life of Ds, the most stable confirmed darmstadtium isotope, is 11 seconds, long enough to perform chemical studies, another obstacle is the need to increase the rate of production of darmstadtium isotopes and allow experiments to carry on for weeks or months so that statistically significant results can be obtained. Separation and detection must be carried out continuously to separate out the darmstadtium isotopes and have automated systems experiment on the gas-phase and solution chemistry of darmstadtium, as the yields for heavier elements are predicted to be smaller than those for lighter elements; some of the separation techniques used for bohrium and hassium could be reused. However, the experimental chemistry of darmstadtium has not received as much attention as that of the heavier elements from copernicium to livermorium.\n\nThe more neutron-rich darmstadtium isotopes are the most stable and are thus more promising for chemical studies; however, they can only be produced indirectly from the alpha decay of heavier elements. and indirect synthesis methods are not as favourable for chemical studies as direct synthesis methods. The more neutron-rich isotopes Ds and Ds might be produced directly in the reaction between thorium-232 and calcium-48, but the yield is expected to be low. Furthermore, this reaction has already been tested without success, and more recent experiments that have successfully synthesized Ds using indirect methods show that it has a short half-life of 5.7 ms, not long enough to perform chemical studies. The only known darmstadtium isotope with a half-lifelong enough for chemical research is Ds, which would have to be produced as the granddaughter of Fl.\n\n\n"}
{"id": "3456662", "url": "https://en.wikipedia.org/wiki?curid=3456662", "title": "Dazzler (weapon)", "text": "Dazzler (weapon)\n\nA dazzler is a non-lethal weapon which uses intense directed radiation to temporarily disable its target with flash blindness. Targets can include sensors or human vision.\n\nInitially developed for military use, non-military products are becoming available for use in law enforcement and security.\n\nDazzlers emit infrared or invisible light against various electronic sensors, and visible light against humans, when they are intended to cause no long-term damage to eyes. The emitters are usually lasers, making what is termed a \"laser dazzler\". Most of the contemporary systems are man-portable, and operate in either the red (a laser diode) or green (a diode-pumped solid-state laser, DPSS) areas of the electromagnetic spectrum. The green laser is chosen for its unique ability to react with the human eye. The green laser is less harmful to human eyes.\n\nSome searchlights are bright enough to cause permanent or temporary blindness, and they were used to dazzle the crews of bombers during World War II. Whirling Spray was a system of search lights fitted with rotating mirrors which was used to dazzle and confuse pilots attacking the Suez canal. This was developed into the Canal Defence Light, a small mobile tank mounted system intended for use in the Rhine crossings. However, the system was mainly used as conventional searchlights.\n\nHandgun or rifle-mounted lights may also be used to temporarily blind an opponent and are sometimes marketed for that purpose. In both cases the primary purpose is to illuminate the target and their use to disorient is secondary.\n\nThe first reported use of laser dazzlers in combat was possibly by the British, during the Falklands War of 1982, when they were reputedly fitted to various Royal Navy warships to hinder low-level Argentinian air attacks.\n\nAt the end of Operation Desert Storm, F-15E crews observing the Iraqi military's massacre of Kurdish civilians at Chamchamal were forbidden from firing on the attackers, but instead used their lasers as a dazzler weapon. This ultimately proved ineffective in crashing any attack helicopters.\n\nOn 18 May 2006, the U.S. military announced it was using laser dazzlers mounted on M-4 rifles in troops in Iraq as a non-lethal way to stop drivers who fail to stop at checkpoints manned by American soldiers.\n\nOne defense against laser dazzlers are narrowband optical filters tuned to the frequency of the laser. To counter such defense, dazzlers can employ emitters using more than one wavelength, or tunable lasers with wider range of output. Another defense is photochromic materials able to become opaque under high light energy densities. Nonlinear optics techniques are being investigated: e.g. vanadium-doped zinc telluride (ZnTe:V) can be used to form electro-optic power limiters able to selectively block the intense dazzler beam without affecting weaker light from an observed scene.\n\nWeapons designed to cause permanent blindness are banned by the 1995 United Nations Protocol on Blinding Laser Weapons. The dazzler is a non-lethal weapon intended to cause temporary blindness or disorientation and therefore falls outside this protocol.\n\n\n\n\n"}
{"id": "36113327", "url": "https://en.wikipedia.org/wiki?curid=36113327", "title": "Electromagnetic absorbers", "text": "Electromagnetic absorbers\n\nElectromagnetic absorbers are specifically chosen or designed materials that can inhibit the reflection or transmission of electromagnetic radiation. For example, this can be accomplished with materials such as dielectrics combined with metal plates spaced at prescribed intervals or wavelengths. The particular absorption frequencies, thickness, component arrangement and configuration of the materials also determine capabilities and uses. In addition, researchers are studying advanced materials such as metamaterials in hopes of improved performance and diversity of applications. Some intended applications for the new absorbers include emitters, sensors, spatial light modulators, infrared camouflage, wireless communication, and use in thermophotovoltaics.\n\nGenerally, there are two types of absorbers: resonant absorbers and broadband absorbers. The resonant absorbers are frequency-dependent because of the desired resonance of the material at a particular wavelength. Different types of resonant absorbers are the \"Salisbury screen\", the \"Jaumann absorber\", the \"Dallenbach layer\", \"crossed grating absorbers\", and \"circuit analog (CA) absorbers\".\n\nBroadband absorbers are independent of a particular frequency and can therefore be effective across a broad spectrum.\n\n\n"}
{"id": "17328047", "url": "https://en.wikipedia.org/wiki?curid=17328047", "title": "Energy Resources Conservation Board", "text": "Energy Resources Conservation Board\n\nThe Energy Resources Conservation Board (ERCB) was an independent, quasi-judicial agency of the Government of Alberta. It regulated the safe, responsible, and efficient development of Alberta's energy resources: oil, natural gas, oil sands, coal, and pipelines. Led by eight Board members, the ERCB's team of engineers, geologists, technicians, economists, and other professionals served Albertans from thirteen locations across the province.\n\nThe ERCB's mission was to ensure that the discovery, development, and delivery of Alberta's energy resources took place in a manner that was fair, responsible and in the public interest.\n\nThe ERCB adjudicated and regulated matters related to energy within Alberta to ensure that the development, transportation, and monitoring of the province's energy resources were in the public interest. The Board provided this assurance of the public interest through its activities in the application and hearing process, regulation, monitoring, and surveillance and enforcement.\n\nThe information and knowledge responsibility of the Board included the collection, storage, analysis, appraisal, dissemination and stakeholder awareness of information. Open access to information developed awareness, understanding and responsible behavior and allowed the Board and stakeholders to make informed decisions about energy and utility matters. This responsibility would result in the Board discharging its advisory role with respect to matters under the jurisdiction of the Board.\n\nThe Government of Alberta owns about 80% of the province's mineral rights, such as oil, natural gas, coal, and the oil sands. In other words, most resources are owned by the people of Alberta through their government. While private companies can develop these resources, the ERCB was authorized by the government to protect the public's interest relating to the discovery, development, and delivery of these resources. Regulation was needed so that non-renewable resources were produced in a safe, responsible, and efficient manner, without waste.\n\nThe ERCB also ensured that everyone affected by development had a chance to be heard. When conflicts regarding development remained unresolved between companies and landowners, the ERCB worked to settle the issues in a fair and balanced manner.\n\nIn 1996, the Alberta Geological Survey (AGS) joined the ERCB. AGS assisted the ERCB by providing data, information, knowledge and advice about the geology of Alberta.\n\nAlberta's first energy regulatory body was created in 1938. A succession of agencies led to the new ERCB being established 1 January 2008, as a result of the realignment of the Alberta Energy and Utilities Board (EUB) into the ERCB and the Alberta Utilities Commission. The ERCB also includes the Alberta Geological Survey.\n\nIn October 2008, ERCB was named one of Alberta's Top Employers by Mediacorp Canada Inc., which was announced by the Calgary Herald and the Edmonton Journal.\n\nAlberta Energy Regulator is a corporation created by the Responsible Energy Development Act passed on 10 December 2012 and proclaimed on 17 June 2013, in the Alberta Legislature, operating at arm’s length from the Government of Alberta, under an appointed board of directors headed by Chair, Gerry Protti and CEO Jim Ellis, appointed by Energy Minister Ken Hughes. On 17 June 2013, all regulatory functions previously carried out by the Energy Resources Conservation Board were taken over by the Alberta Energy Regulator.\n\nAlberta Energy Regulator is \"100 per cent funded by industry and is authorized to collect funds through an administrative fee levied on oil and gas wells, oil sands mines, and coal mines. The industry-funded model is commonly used by regulatory agencies from various sectors across North America.\" AER has \"an annual budget of more than $165 million, more than \"1000 staff working in 13 locations across Alberta.\" Alberta Energy Regulator \"regulates approximately - 181,000 active wells, 27,800 oil facilities and 20,000 gas facilities, and 405,000 kilometres (km) of pipelines.\" AER also \"considers some 36 800 applications for energy development every year.\"\n\nIn December 2012, the Responsible Energy Development Act passed in the Alberta Legislature. Alberta Energy Regulator is mandated under the Act, to direct and oversee \"the orderly transition from the Energy Resources Conservation Act to the Responsible Energy Development Act. Under this act, the newly formed Alberta Energy Regulator, will \"bring together the regulatory functions from the Energy Resources Conservation Board and the Ministry of Environment and Sustainable Resource Development into a one-stop shop.\" The Alberta Energy Regulator is now \"responsible for all projects from application to reclamation.\" They will respond to project proponents, landowners and industry regarding energy regulations in Alberta. The Alberta Energy Regulator was phased in in June 2013. Responsible Energy Development Act gave the Alberta Energy Regulator \"the authority to administer the Public Lands Act, the Environmental Protection and Enhancement Act and the Water Act, with regards to energy development.\" The Alberta Energy Regulator will enforce environmental laws and issue environmental and water permits, responsibilities formerly the mandate of Alberta Environment.\n\nGerry Protti, appointed by Energy Minister Ken Hughes, on 18 June 2013, as chair of the Alberta Energy Regulator (AER), that will regulate oil, gas and coal development in Alberta, was a former executive with Encana, the founding president of the Canadian Association of Petroleum Producers (CAPP) and spent many years as lobbyist for the Energy Policy Institute of Canada. Jim Ellis, a former deputy minister in environment and energy, was appointed as CEO by the Lieutenant Governor\nin Council.\n\nIn the past the Energy Resources Conservation Board and Alberta Environment conducted investigations differently. Alberta Surface Rights Group, the United Landowners of Alberta, First Nations, farmers and ranchers have expressed concerns about the streamlining of regulatory processes that may benefit oil and gas industries at their expense.\n\nAccording to their brochure the Alberta Energy Regulator \"ensures the safe, efficient, orderly, and environmentally responsible development of hydrocarbon resources over their entire life cycle. This includes allocating and conserving water resources, managing public lands, and protecting the environment while providing economic benefits for all Albertans.\"\n\nThe ERCB regulated the safe, responsible, and efficient development of oil, natural gas, oil sands, and coal, and as well as the pipelines to move the resources to market.\n\nRegulation was done through two core functions: adjudication and regulation, and information and knowledge. ERCB approval must have been given at almost every step of an energy project’s life.\n\nTo maintain its autonomous structure, the ERCB answered directly to the Executive Council (Cabinet) of Alberta through the Minister of Energy, but it made its formal decisions independently in accordance with the six statutes it administers.\n\nThe ERCB was led by a Board of eight people: a Chairman and Board Members. Supporting the Chairman and Board Members was the Executive Committee, and approximately 900 staff who worked in eight main branches:\n\nThis branch, made up of three groups, provided a streamlined approach to processing some 40 000 energy development applications each year. The Facilities Group handled project reviews, audits, and approvals related to new or modified oil and gas facilities, such as wells, pipelines, batteries, and gas plants. The Resource Group dealt with applications and issues related to development and conservation projects for oil, gas, and coal. The Business Operations and Development Group managed the coordination of administrative support, approvals development, planning, objections, and hearings.\n\nThis branch provided technical and operational expertise in the development, application, and enforcement of regulatory requirements for conventional and nonconventional resources. The branch ensured that oil and gas operations are conducted in a safe and responsible manner through incident response, resource conservation, protection of the environment, and industry liability management. Operating from Field Centres across Alberta, field staff inspected construction, operation, and abandonment operations at oil, gas, and oil sands facilities and respond to emergencies and public concerns on a 24-hour basis.\n\nThis branch incorporated several groups. Human Resources provides services and programs to ensure that a competent and committed workforce was in place to achieve ERCB goals and objectives. The Communications Group developed strategic communication, consultation strategies and delivers related media, Web site, and document services to keep staff and stakeholders informed about ERCB activities. Administrative Services provided building, library, and printing services.\n\nThis branch provided revenue and expenditure management and administration of the industry funding levy. In addition, staff coordinated the preparation of the ERCB’s three-year business plan and performance reporting.\n\nThis branch was responsible for ERCB information systems, support, and technological infrastructure, with a focus on new ways to deliver electronic commerce. Another core area was the collection and dissemination of energy resource information, including oil and gas production. This information was also used to determine provincial royalties, well records, regulatory publications, maps, and various energy databases.\n\nThis branch provided a wide range of legal advice and services to the organization, with a focus on procedural fairness and objectivity. Its responsibilities included application and regulatory policy, hearings, proceedings, related internal and external consultations, and the formulation of energy regulations and legislation. The branch administered intervener funding and led a key advisory committee that advises the Board on decisions and policy matters.\n\nThis branch maintained an integrated and current inventory of Alberta’s subsurface energy, mineral, and other resources in a geological framework. It provided knowledge, advice, and forecasts about the states of earth-energy resource development in the context of Alberta’s environment, economy, and society. The branch also developed and supported regulatory processes and best practices to conserve earth-energy resources, maintains environmental quality, assures public safety, and guides informed risk taking in regulatory and policy decisions.\n\nThe Oil Sands Branch had overall responsibility for how the ERCB regulateed oil sands activities in Alberta. The branch comprised the Mineable Oil Sands Group, which looked after oil sands developments that use mining recovery technology as well as bitumen upgrading, and the In Situ Oil Sands Group, which focused on developments using recovery technology involving subsurface or in situ recovery methods. Collaborating with other ERCB branches, the Oil Sands Branch took the lead on processing applications, conducting surveillance and enforcement of approved projects, and carrying out geological assessments as they apply to the oil sands.\n\nIn their 2012 report ECRB cautioned that oil sands operators failed to convert their tailings ponds into deposits suitable for reclamation in a timely fashion, as proposed in their project applications. \"The volume of fluid tailings, and the area required to hold fluid tailings, continued to grow, and the reclamation of tailings ponds was further delayed.\" ECRB follows the industry wide directive, Directive 074, the first of its kind, which sets out the \"industry-wide requirements for tailings management,\" requiring \"operators to commit resources to research, develop, and implement fluid tailings reduction technologies and to commit to tailings management and progressive reclamation as operational priorities that are integrated with mine planning and bitumen production activities.\" The Government of Alberta is setting up a Tailings Management Framework to complement and expand Directive 074's policies to \"ensure that fluid fine tailings are reclaimed as quickly as possible and that current inventories are reduced.\"\n\nOn 12 June 2013 the Regional Municipality of Wood Buffalo after many days of heavy rain, declared a state of emergency. The flood conditions lasted from June 10–18, 2013. It was the first of many communities to do so in Alberta during the 2013 floods. Wood Buffalo authorities organised evacuations from some areas and placed others in boil water advisories as local waterways, such as the Hangingstone River, rose to dangerously high levels.\n\nAn application was a request by a company for ERCB approval—in the form of a licence, order, permit, or approval—for an energy project. Most energy-related projects require ERCB approval. Each year tens of thousands of applications were reviewed and approved by the ERCB.\n\nThe ERCB also played a vital environmental protection role by reviewing flaring permits, oilfield waste disposal facilities, drilling waste practices, and emergency response plans.\n\nERCB approval for a facility or project was considered to be routine if an application was complete, there were no landowner objections, and the company applying had met all technical, safety, public consultation, and environmental requirements. The turnaround time for a complete and well-prepared routine application could be as short as one day.\n\nSome projects required input from other government departments. The ERCB passed such applications to Alberta Environment, which handles distribution to other departments. This \"one-window\" approach meant that applicants did not have to go to each government department for individual review and approval. The general rule was that each government department checks that a specific proposal meets its own regulations and standards and then forwards any deficiencies or concerns to the ERCB via Alberta Environment.\n\nNonroutine applications took more time—weeks, or even months—to process if there were landowner objections, community and environmental concerns, or objections from competing companies. Objections to applications may also have been resolved through facilitation, mediation, or negotiated settlements approved by the Board. However, any unresolved matter or objection related to an application may have proceeded to an ERCB hearing.\n\nERCB. 2011-06. \"ST98-2011 Alberta’s Energy Reserves 2010 and Supply/Demand Outlook 2011–2020\"\n\nERCB. 2011-04. \"Big Reserves, Big Responsibility: Developing Alberta’s Oil Sands\"\n\nERCB. 2009. \"Directive 074: Tailings Performance Criteria and Requirements for Oil Sands Mining Schemes.\" \n\nERCB. 2008. \"Directive 073: Requirements for Inspection and Compliance of Oil Sands Mining and Processing Plant Operations in the Oil Sands Mining Area.\" \n\nAn ERCB hearing was a formal process that provided an important opportunity for different points of view about an energy project to be aired in a fair and orderly forum. A hearing allowed for an open, public testing of technical, environmental, social, and economic evidence from those involved. The process ensured that all relevant arguments for and against the energy facility project are heard.\n\nERCB hearings were held when the ERCB received an objection from a person who may have been directly and adversely affected by a proposed project. Applications filed may have created community concern or a need for more information; however, these matters were often settled through an Appropriate Dispute Resolution (ADR) process. When matters were settled through ADR or there were no public concerns and objections, there was no need for a hearing. The Board would also dismiss objections if the person does not appear to be directly or adversely affected.\n\nThe ERCB maild a Notice of Hearing to inform people and organizations affected by an application about the hearing. The Notice of Hearing may have been published in daily and/or weekly newspapers.\n\nHearing notices were available on the ERCB Web site. Companies involved in large projects usually held an open house to explain their proposed project, answer citizens’ questions, and address the community’s concerns.\n\nThe Notice of Hearing provided interested parties with the following information:\n\nAn ERCB hearing followed a formal process to ensure that everyone had a say:\n\n"}
{"id": "52347214", "url": "https://en.wikipedia.org/wiki?curid=52347214", "title": "Explosive antimony", "text": "Explosive antimony\n\nExplosive antimony is an allotrope of the chemical element antimony that is so sensitive to shocks that it explodes when scratched or subjected to sudden heating. The allotrope was first described in 1855.\n\nChemists form the allotrope though electrolysis of a concentrated solution of antimony trichloride in hydrochloric acid, which forms an amorphous glass. \nThis glass contains significant amounts of halogen impurity at its boundaries. \n\nWhen it explodes the allotrope releases 24 calories of energy per gram. White fumes of antimony trichloride are produced and the elemental antimony reverts to its metallic form.\n"}
{"id": "89918", "url": "https://en.wikipedia.org/wiki?curid=89918", "title": "Field-emission electric propulsion", "text": "Field-emission electric propulsion\n\nField-emission electric propulsion (FEEP) is an advanced electrostatic space propulsion concept, a form of ion thruster, that uses liquid metal (usually either caesium, indium or mercury) as a propellant. A FEEP device consists of an emitter and an accelerator electrode. A potential difference of the order of 10 kV is applied between the two, which generates a strong electric field at the tip of the metal surface. The interplay of electric force and surface tension generates surface instabilities which give rise to Taylor cones on the liquid surface. At sufficiently high values of the applied field, ions are extracted from the cone tip by field evaporation or similar mechanisms, which then are accelerated to high velocities (typically 100 km/s or more).\n\nA separate electron source is required to keep the spacecraft electrically neutral. Due to its very low thrust (in the micronewton to millinewton range), FEEP thrusters are primarily used for microradian, micronewton attitude control on spacecraft, such as in the ESA/NASA LISA Pathfinder scientific spacecraft. \n\nThe FEEP thruster was also slated for installation on Gravity Field and Steady-State Ocean Circulation Explorer spacecraft, but the Gridded ion thruster was used instead.\n\nField emission electric propulsion (FEEP) is an electrostatic propulsion concept based on field ionization of a liquid metal and subsequent acceleration of the ions by a strong electric field. FEEP is currently the object of interest in the scientific community, due to its unique features: sub-μN to mN thrust range, near instantaneous switch on/switch off capability, and high-resolution throttleability (better than one part in 10), which enables accurate thrust modulation in both continuous and pulsed modes. Presently baseline for scientific missions onboard drag-free satellites, this propulsion system has also been proposed for attitude control and orbit maintenance on\ncommercial small satellites and constellations.\n\nThis type of thruster can accelerate a large number of different liquid metals or alloys. The best performance (in terms of thrust efficiency and power-to-thrust ratio) can be obtained using high atomic weight alkali metals, such as cesium and rubidium (133 amu for Cs, 85.5 amu for Rb). These propellants have a low ionization potential (3.87 eV for Cs and 4.16 eV for Rb), low melting point (28.7 °C for Cs and 38.9 °C for Rb) and very good wetting capabilities. These features lead to low power losses due to ionization and heating and the capability to use capillary forces for feeding purposes (i.e. no pressurised tanks nor valves are required). Moreover, alkali metals have the lowest attitude to form ionized droplets or multiply-charged ions, thus leading to the best attainable mass efficiency. The actual thrust is produced by exhausting a beam of mainly singly-ionized cesium or rubidium atoms, produced by field evaporation at the tip of the emitter.\n\nAn accelerating electrode (accelerator) is placed directly in front of the emitter. This electrode consists of a metal (usually stainless steel) plate where two sharp blades are machined. When thrust is required, a strong electric field is generated by the application of a high voltage difference between the emitter and the accelerator. Under this condition, the free surface of the liquid metal enters a regime of local instability, due to the combined effects of the electrostatic force and the surface tension. A series of protruding cusps, or \"Taylor cones\" are thus created. When the electric field reaches a value in the order of 10 V/m, the atoms at the tip of the cusps spontaneously ionize and an ion jet is extracted by the electric field, while the electrons are rejected in the bulk of the liquid. An external source of electrons (neutralizer) provides negative charges to maintain global electrical neutrality of the thruster assembly.\n\nLiquid metal ion sources (LMIS) based on field ionization or field evaporation were introduced in the late '60s and quickly became widespread as simple, cheap ion sources for a number of applications. In particular, the use of LMIS operated on gallium, indium, alkali metals or alloys has been standard practice in secondary ion mass spectrometry (SIMS) since the '70s.\n\nWhile there exist different field emitter configurations, such as the needle, the capillary and slit emitter types, the principle of operation is the same in all cases. In the slit emitter, for example, a liquid metal propellant is fed by capillary forces through a narrow channel. The emitter consists of two identical halves made from stainless steel, and clamped or screwed together. A nickel layer, sputter deposited onto one of the emitter halves, outlines the desired channel contour and determines channel height (a.k.a. slit height, typically 1–2 μm) and channel width (a.k.a. slit length, ranging from 1 mm up to about 7 cm).\n\nThe channel ends at the emitter tip, formed by sharp edges that are located opposite a negative, or accelerator, electrode, and separated by a small gap (about 0.6 mm) from the emitter tip. An extraction voltage is applied between the two electrodes. The emitter carries a positive potential while the accelerator is at negative potential. The electric field being generated between the emitter and accelerator now acts on the liquid metal propellant.\n\nThe narrow slit width not only enables the capillary feed, but, when combined with the sharp channel edges directly opposite the accelerator, also ensures that a high electric field strength is obtained near the slit exit. The liquid metal column, when subjected to this electric field, begins to deform, forming cusps (Taylor cones), which protrude from the surface of the liquid. As the liquid cusps form ever sharper cones due to the action of the electric field, the local electric field strength near these cusps intensifies. Once a local electric field strength of about 10 V/m is reached, electrons are ripped off the metal atoms. These electrons are collected through the liquid metal column by the channel walls, and the positive ions are accelerated away from the liquid through a gap in the negative accelerator electrode by the same electric field that created them.\n\nSlit emitters had been developed to increase the emitting area of the thruster in order to yield higher thrust levels and to avoid the irregular behaviour observed for single emitters. The substantial advantage of slit emitters over stacked needles is in the self-adjusting mechanism governing the formation and redistribution of emission sites on the liquid metal surface according to the operating parameters; in a stacked-needle array, on the contrary, the Taylor cones can only exist on the fixed tips, which pre-configure a geometrical arrangement that can only be consistent with one particular operating condition. \n\nSlit emitters with a wide variety of slit widths have been fabricated; currently, devices with slit widths between 2 mm and 7 cm are available. These devices, spanning a thrust range from 0.1 μN to 2 mN, are operated with cesium or rubidium.\n\nThe miniaturized FEEP module design with a crown-shape emitter to fit into the standard CubeSat chassis was reported in 2017.\n\nThe single-emitter FEEP design of 0.5 mN is commercially available, and its arrayed version development is nearing completion as in 2018.\n\n"}
{"id": "25394605", "url": "https://en.wikipedia.org/wiki?curid=25394605", "title": "Genetically modified virus", "text": "Genetically modified virus\n\nA genetically modified virus is a virus that has gone through genetic modification for various biomedical purposes, agricultural purposes, bio-control and technological purposes. Genetic modification involves the insertion or deletion of genes to improve organisms and is usually obtained with biotechnology.\n\nGenetic modification involves the insertion or deletion of genes into the viral genome to improve organisms and is usually obtained with biotechnology methods. These methods of gene transfer include physically inserting the extra DNA into the nucleus of the intended host with a very small syringe, or with very small particles fired from a gene gun. Other methods exploit natural forms of gene transfer, such as the ability of \"Agrobacterium\" to transfer genetic material to plants, or the ability of lentiviruses to transfer genes to animal cells.\n\nGene therapy uses genetically modified viruses to deliver genes that can cure diseases in human cells.These viruses can deliver DNA or RNA genetic material to the targeted cells. Gene therapy is also used by inactivating mutated genes that are causing the disease using viruses. Viruses that have been used for gene therapy are, adenovirus, lentivirus, retrovirus and the herpes simplex virus. Although gene therapy is still relatively new, it has had some successes. It has been used to treat inherited genetic disorders such as severe combined immunodeficiency. Although some successes, gene therapy is still considered a risky technique and studies are still undergoing to ensure safety and effectiveness.\n\nIn 2004, researchers reported that a genetically modified virus that exploits the selfish behaviour of cancer cells might offer an alternative way of killing tumours. Since then, several researchers have developed genetically modified oncolytic viruses that show promise as treatments for various types of cancer.\n\nIn 2001, it was reported that genetically modified viruses can possibly be used to develop vaccines against diseases such as, AIDS, herpes, dengue fever and viral hepatitis by using a proven safe vaccine virus, such as adenovirus, and modify its genome to have genes that code for immunogenic proteins that can spike the immune systems response to then be able to fight the virus.\n\nIn 2012, US researchers reported that they injected a genetically modified virus into the heart of pigs. This virus inserted into the heart muscles a gene called Tbx18 which enabled heartbeats. The researchers forecast that one day this technique could be used to restore the heartbeat in humans who would otherwise need electronic pacemakers.\n\nA genetically modified version of the lentivirus has been used to treat diseases. The lentivirus is versatile and can be used as a genetically modified virus by using it as a vector for gene therapy.\n\nIn Spain and Portugal, by 2005 rabbits had declined by as much as 95% over 50 years due diseases such as myxomatosis, rabbit haemorrhagic disease and other causes. This in turn caused declines in predators like the Iberian lynx, a critically endangered species. In 2000 Spanish researchers investigated a genetically modified virus which might have protected rabbits in the wild against myxomatosis and rabbit haemorrhagic disease. However, there was concern that such a virus might make its way into wild populations in areas such as Australia and create a population boom. Rabbits in Australia are considered to be such a pest that land owners are legally obliged to control them.\n\nTropical and subtropical regions located in the United States, South America, India, Africa, Mexico, China, and Australia suffered a decline in papaya production due to the papaya ringspot virus (PRSV). To combat this disease PRSV-resistant papaya was genetically engineered through gene technology. PSRV-resistant papayas were engineered by using a protein mediated, RNA-silencing mechanism. This was achieved through an agrobacterium mediated transformation of the papaya. Other methods used to combat the disease are, a gene gun method and post-transcriptional gene silencing technology. Post-transcriptional gene silencing technology is still currently being studied and can possibly be used for treatment of PRSV in the future.\n\nIn 2009, MIT scientists created a genetically modified virus has been used to construct a more environmentally friendly lithium-ion battery. The battery was constructed by genetically engineering different viruses such as, the E4 bacteriophage and the M13 bacteriophage, to be used as a cathode. This was done by editing the genes of the virus that code for the protein coat. The protein coat is edited to coat itself in iron phosphate to be able to adhere to highly conductive carbon-nanotubes. The viruses that have been modified to have a multifunctional protein coat can be used as a nano-structured cathode with causes ionic interactions with cations. Allowing the virus to be used as a small battery. Angela Blecher, the scientist who led the MIT research team on the project, says that the battery is powerful enough to be used as a rechargeable battery, power hybrid electric cars, and a number of personal electronics.\n\nThe National Institute of Health declared a research funding moratorium on select Gain-of-Function virus research in January 2015. Questions about a potential escape of a modified virus from a biosafety lab and the utility of dual-use-technology, dual use research of concern (DURC), prompted the NIH funding policy revision.\n\nA scientist claims she was infected by a genetically modified virus while working for Pfizer. In her federal lawsuit she says she has been intermittently paralyzed by the Pfizer-designed virus. \"McClain, of Deep River, suspects she was inadvertently exposed, through work by a former Pfizer colleague in 2002 or 2003, to an engineered form of the lentivirus, a virus similar to the one that can lead to acquired immune deficiency syndrome, or AIDS.\" The court found that McClain failed to demonstrate that her illness was caused by exposure to the lentivirus, but also that Pfizer violated whistleblower laws.\n"}
{"id": "6732384", "url": "https://en.wikipedia.org/wiki?curid=6732384", "title": "Graphical timeline of the Stelliferous Era", "text": "Graphical timeline of the Stelliferous Era\n\nThis is the timeline of the stelliferous era but also partly charts the primordial era, and charts more of the degenerate era of the heat death scenario.\n\nThe scale is formula_1. Example one million years is formula_2.\n\n"}
{"id": "1399021", "url": "https://en.wikipedia.org/wiki?curid=1399021", "title": "Grimsby Dock Tower", "text": "Grimsby Dock Tower\n\nGrimsby Dock Tower is a hydraulic accumulator tower and a maritime landmark at the entrance to the Royal Dock, Grimsby, in North East Lincolnshire, England. It was completed on 27 March 1852 with the purpose of containing a reservoir at a height of , that was used to provide hydraulic power to power the machinery of the Grimsby Docks. The extreme height of the tower was necessary to achieve sufficient pressure, and as a result of this, the tower can be seen for several miles around, even far inland on the north bank of the River Humber in villages such as Patrington.\n\nThe tower was built to provide water pressure to power the hydraulic machinery (for cranes, lock gates and sluices) at the Grimsby Docks. The tower was built to carry a tank above the ground with a direct feed into the machinery. Small pumps topped up the tank as the hydraulic machinery drew off water. The tower system was brought into use in 1852 working the machinery of the lock gates, dry-docks and fifteen quayside cranes, and also to supply fresh water to ships and the dwelling houses on the dock premises. The water was obtained from a well, in diameter and deep, with a boring of in diameter to the chalk rock in the centre, situated near to where the \"Grimsby Evening Telegraph\" has its present offices. The well was also fed by seven borings of in diameter, at intervals in a length of , which discharged into the well by a brick culvert in diameter.\n\nWater was conveyed from the well to the tower in a cast iron pipe thirteen inches in diameter when it was then forced into the tank by two force pumps, each of ten inches in diameter, worked by a duplicate, horizontal engine of twenty-five horse power. All the engines, pumps and pipes and the whole of the machinery were made by Mr Mitchell of the Perran Foundry, Cornwall.\n\nDuring the building of the tower, Armstrong developed another system using weighted accumulators, which at once was found to have great advantages. The working hydraulic pressure was greater, permitting more compact and cheaper machinery and everything was at ground level. The first such installation was at New Holland dock and pier, actually brought into use more than a year before the Grimsby system.\n\nThe tower was designed by James William Wild who based its appearance on that of the Torre del Mangia on the Palazzo Pubblico in Siena. It was built under the supervision of J. M. Rendel, who was the civil engineer in charge of construction of the Royal Dock.\n\nThe ground floor of the tower was lined with pink, white and blue drapery when Queen Victoria came with Prince Albert to visit the dock and 'open' the tower in October 1854. Her Majesty gave permission for Prince Albert, the Prince of Wales and the Princess Royal to accompany Mr Randel in the hydraulic lift to the gallery running around the tower above the water tank, from which a clear view of Grimsby, Cleethorpes and the mouth of the River Humber is obtained. \n\nThe tower is high, wide at the base, and tapers gradually to below the first projection; its walls are thick and narrow to at the string course under the corbels. The bricks of this plain brick tower were made from clay obtained from excavations in the marsh adjoining the docks, and are set in blue lime mortar. Hoop iron bond is used in the walls to a considerable extent. The foundation of the tower is a solid masonry wall built upon a timber bearer piling. Approximately one million bricks were used in its construction. \n\nLocal legend has it that the tower was built on a foundation of cotton wool; the story likely has its origins in the profitable import of cotton to West Yorkshire which an impetus to the building of the Royal Dock and the tower.\n\nThe Dock Tower continued to provide water for hydraulic working until 1892 when the erection of the hydraulic accumulator tower on the opposite pier approximately to the North West of the Dock Tower took over - water in this structure was pressurised by a 300-tonne weight. Present dock and lock machinery are powered by electric or electro-hydraulic energy. \n\nDuring the Second World War, there were plans to demolish the tower, as it acted as a beacon for German Luftwaffe aircraft heading towards Liverpool. Later, a plaque was placed on the bricks paying tribute to the minesweeper crews of the war.\n\nThe tower's lift is no longer in operation with access to the top via a spiral staircase in one corner of the building. The second balcony is currently used for the transmission and reception of radio signals.\n\nThe tower is Grade I listed and is the subject of a Preservation Order.\n\nThere is a model of the Grimsby Dock Tower entirely constructed of Lego, at Legoland in Windsor. While the tower itself is correct, a large building has been incorrectly added to the base of the tower.\n\n"}
{"id": "3530536", "url": "https://en.wikipedia.org/wiki?curid=3530536", "title": "H-1NF", "text": "H-1NF\n\nH-1NF is the Australian Plasma Fusion Research Facility.\n\nThe \"H-1\" flexible Heliac is a three field-period helical axis stellarator located in the ANU Research School of Physics and Engineering at Canberra, Australia. Optimisation of the H-1 power supplies for low current ripple allows precise control of the ratio of secondary (helical, vertical) coil to primary (poloidal, toroidal) coil currents, resulting in a finely tunable magnetic geometry. Slight variation in the current ratio between shots (plasma discharges) in a sequence corresponds to a high resolution parameter scan through magnetic configurations (i.e.: rotational transform profile, magnetic well). The programmable control system allows for repetition rates of around 30 shots per hour, limited by data acquisition time and magnet cooling time.\n\n\n"}
{"id": "53478734", "url": "https://en.wikipedia.org/wiki?curid=53478734", "title": "HYDAC (company)", "text": "HYDAC (company)\n\nHYDAC is a German company group that specializes in the production and distribution of components and systems as well as services related to hydraulics and fluidics. Hydac is constituted of 15 legal entities, all of which are GmbHs, companies with limited liabilities. CEOs for these companies are Alexander Dieter, Wolfgang Haering and Hartmut Herzog. Subsidiaries exist in more than 10 countries. In 2011, the company had approximately 5,500 employees.\n\nHYDAC was founded in 1963 by Werner Dieter and Ottmar Schön, when an exclusive license for central Europe for a hydraulic accumulator was taken out. The name HYDAC is an abbreviation for \"hydraulic accumulator\".\n\nIn 2015, HYDAC had 9,000 employees, 50 subsidiaries, and 500 distribution and service partners. The company group is headquartered in Sulzbach (Saarland). More than 3000 employees are employed in Sulzbach.\n\nIn 2014, the subsidiary with the highest revenue was HYDAC Technology GmbH with revenues of €445.6 million and 384 employees. In the same year, HYDAC International GmbH and its 466 employees, which is presently providing distribution services to the other subsidiaries of the group, had revenues of €75.9 million.\n\nThe groups subsidiaries are:\n"}
{"id": "58716467", "url": "https://en.wikipedia.org/wiki?curid=58716467", "title": "High Grand Falls Power Station", "text": "High Grand Falls Power Station\n\nThe High Grand Falls Hydroelectric Power Station, also High Grand Falls Dam, is a planned hydroelectric power station across the Tana River that harnesses the energy of the Kibuka Falls, in Kenya. The planned capacity of the power station is . The station is expected to be the most powerful hydroelectric energy source in Kenya.\n\nThe power station would lie at Kibuka Falls, across the Tana River, a in the vicinity of Mwingi National Reserve, at the border between Kitui County and Tharaka-Nithi County, approximately north-east of the city of Nairobi, the country's capital and its largest city. This location is downstream of the Seven Forks Scheme.\n\nThis development is part of the Lamu Port and Lamu-Southern Sudan-Ethiopia Transport Corridor project (LAPSSET). The dam is expected to create a lake with a surface area of and holding of water. An estimated 4,500 families in Kiuti and Tharaka Nithi counties, are expected to be displaced by the new dam. In addition to the planned 693 megawatts of electricity, the dam will provide water for the irrigation of more than of farmland. The dam is also expected to mitigate flooding in the coastal counties during the rainy season.\n\nthe idea to build this dam was conceived in 2009, during the Mwai Kibaki presidency. Following a tendering process, a British entity, \"GBM Engineering Consortium\", based in London, was the only qualifier for the tender. GBM beat six other international construction firms, five of the Chinese. Once initiated, construction is expected to last six years.\n\nThe consortium that is developing the dam and power station, will design, fund, build, own, operate and transfer the project, after recovering their investment, during 20 years of ownership, following commercial commissioning.\n\nThe estimated costs for the dam and power plant is estimated at US$2 billion (KSh200 billion).\n\nThe dam and power station will be developed in phases. The first phase, with generation capacity of , is expected to come online in 2031. The second phase, with capacity of another is expected online in 1932.\n\n\n"}
{"id": "21890436", "url": "https://en.wikipedia.org/wiki?curid=21890436", "title": "Information-led development", "text": "Information-led development\n\nInformation-led development (ILD) is a development strategy whereby a developing country makes as a primary economic policy focus the creation and development of a national information technology (IT) sector with the express aim of relying on this sector as an engine of growth. Notable examples of such countries are India and the Philippines. Increasingly, tools from information-led development are being used for community economic development in both developed countries and emerging markets.\n\nMore recently, a new formulation of ILD has emerged. With origins in community economic development in the United States, the new ILD model describes the use of data to generate actionable information or information solutions to development challenges. Examples of this include the inclusion of non-financial payment obligations in consumer credit files, also known as alternative data, and the use of this information in underwriting, as a means to reduce financial exclusion in the United States, where an estimated 54 million Americans are shut out of mainstream credit access as there is insufficient information about them in their credit files to be scored by a credit scoring model. This variant of ILD was pioneered by PERC, a non-profit policy research organization and development intermediary headquartered in Chapel Hill, North Carolina. Other US-based organizations, including Social Compact and the Local Initiatives Support Corporation, employ variants of ILD, but none has applied this internationally except for PERC.\n\nThis development model is gaining traction in emerging markets such as Colombia and South Africa, where the data is being used to reduce financial exclusion and facilitate credit access as a means to build wealth and form assets. It is also attracting increasing attention from development agencies, including USAID, the International Finance Corporation, the World Bank Group, and the Consultative Group to Assist the Poor.\n\n"}
{"id": "22637432", "url": "https://en.wikipedia.org/wiki?curid=22637432", "title": "John Twidell", "text": "John Twidell\n\nJohn Twidell is Director of the AMSET Centre Ltd. and is a visiting lecturer at the Environmental Change Institute, University of Oxford, and the School of Aeronautics and Engineering, City University, London. He is Editor Emeritus of the academic journal \"Wind Engineering\". He co-edited the 2009 book \"Offshore Wind Power\".\n\nProfessor Twidell previously held the Chair in Renewable Energy at De Montfort University and was Director of the Energy Studies Unit of Strathclyde University. He has served on the Boards of the British Wind Energy Association (now RenewableUK) and the UK Solar Energy Society, on committees of the Institute of Physics and as an adviser to the UK Parliamentary Select Committee on Energy. He has been a long-standing champion of wind energy.\n"}
{"id": "19984175", "url": "https://en.wikipedia.org/wiki?curid=19984175", "title": "Kisigmánd Wind Farm", "text": "Kisigmánd Wind Farm\n\nThe Kisigmánd Wind Farm is a wind power project in Komárom-Esztergom County, Hungary. It is owned and operated by Iberdrola Renovables, and has an output of up to 50 MW of power, which made it the largest wind farm by power generation in Hungary when it was built. It was Iberdrola's first wind farm in Hungary when it began operation in early 2009.\n\n"}
{"id": "8073723", "url": "https://en.wikipedia.org/wiki?curid=8073723", "title": "Korpeje–Kordkuy pipeline", "text": "Korpeje–Kordkuy pipeline\n\nThe Korpeje–Kordkuy pipeline is a long natural gas pipeline from Korpeje field north of Okarem in western Turkmenistan to Kordkuy in Iran. of pipeline run in Turkmenistan while run in Iran.\n\nIn October 1995, National Iranian Oil Company decided to build the pipeline to supply the remote northern part of Iran. The pipeline was built in 1997 and it cost US$190 million. Iran financed 90% of construction costs, which was later paid back by gas deliveries. The capacity of pipeline is per year. It has a diameter of .\n\nThe pipeline was inaugurated on 29 December 1997 by presidents Saparmurat Niyazov and Mohammad Khatami.\n\n"}
{"id": "4274189", "url": "https://en.wikipedia.org/wiki?curid=4274189", "title": "List of glaciers in Norway", "text": "List of glaciers in Norway\n\nThese are the largest glaciers on mainland Norway. However, the 18 largest glaciers in the Kingdom of Norway are on Svalbard, including the second largest glacier in Europe, Austfonna on Nordaustlandet. In total, Norway has around 1,600 glaciers - 900 of these are in North Norway, but 60% of the total glacier area is south of Trøndelag. 1% of mainland Norway is covered by glaciers.\n"}
{"id": "49025113", "url": "https://en.wikipedia.org/wiki?curid=49025113", "title": "March 1998 North American storm complex", "text": "March 1998 North American storm complex\n\nThe March 6–9, 1998 North American storm complex, was a particularly powerful snowstorm originating on March 6. It was the biggest storm to hit Chicago since February 14, 1990, when 9.7 inches fell over two days. This storm dumped varying amounts of snow across the area. Chicago's Southwest Side was also hard hit, with 10.9 inches recorded at Midway International Airport.\n"}
{"id": "3858844", "url": "https://en.wikipedia.org/wiki?curid=3858844", "title": "Morne Trois Pitons National Park", "text": "Morne Trois Pitons National Park\n\nMorne Trois Pitons National Park is a World Heritage Site (since 1997) located in Dominica. This area was established as a national park by the Dominican government in July 1975, the first to be legally established in the country. The National Park is named after its highest mountain, Morne Trois Pitons, meaning mountain of three peaks. The park is a significant area of volcanic activity. Features within the part include the Valley of Desolation, a region of boiling mud ponds and small geysers; the Boiling Lake, Titou Gorge, and Emerald Pool. \n\n"}
{"id": "19984421", "url": "https://en.wikipedia.org/wiki?curid=19984421", "title": "Mosonszolnok Wind Farm", "text": "Mosonszolnok Wind Farm\n\nThe Mosonszolnok Wind Farm is a wind farm in Győr-Moson-Sopron County, Hungary. It has 12 individual wind turbines with a nominal output of around 2 MW which will deliver up to 24 MW of power, enough to power over 9,494 homes, with a capital investment required of approximately US$40 million.\n"}
{"id": "8384818", "url": "https://en.wikipedia.org/wiki?curid=8384818", "title": "Moving shock", "text": "Moving shock\n\nIn fluid dynamics, a moving shock is a shock wave that is travelling through a fluid (often gaseous) medium with a velocity relative to the velocity of the fluid already making up the medium. As such, the normal shock relations require modification to calculate the properties before and after the moving shock. A knowledge of moving shocks is important for studying the phenomena surrounding detonation, among other applications.\n\nTo derive the theoretical equations for a moving shock, one may start by denoting the region in front of the shock as subscript 1, with the subscript 2 defining the region behind the shock. This is shown in the figure, with the shock wave propagating to the right. \nThe velocity of the gas is denoted by \"u\", pressure by \"p\", and the local speed of sound by \"a\".\nThe speed of the shock wave relative to the gas is \"W\", making the total velocity equal to \"u\" + \"W\". \n\nNext, suppose a reference frame is then fixed to the shock so it appears stationary as the gas in regions 1 and 2 move with a velocity relative to it. Redefining region 1 as \"x\" and region 2 as \"y\" leads to the following shock-relative velocities:\n\nWith these shock-relative velocities, the properties of the regions before and after the shock can be defined below introducing the temperature as \"T\", the density as \"ρ\", and the Mach number as \"M\":\n\nIntroducing the heat capacity ratio as \"γ\", the speed of sound, density, and pressure ratios can be derived:\n\nOne must keep in mind that the above equations are for a shock wave moving towards the right. For a shock moving towards the left, the \"x\" and \"y\" subscripts must be switched and:\n\n\n"}
{"id": "3110439", "url": "https://en.wikipedia.org/wiki?curid=3110439", "title": "Omega-9 fatty acid", "text": "Omega-9 fatty acid\n\nOmega-9 fatty acids (ω−9 fatty acids or \"n\"−9 fatty acids) are a family of unsaturated fatty acids which have in common a final carbon–carbon double bond in the omega−9 position; that is, the ninth bond from the methyl end of the fatty acid.\n\nSome omega−9 fatty acids are common components of animal fat and vegetable oil. \nTwo omega−9 fatty acids important in industry are: \n\nUnlike omega-3 fatty acids and omega-6 fatty acid, omega−9 fatty acids are not classed as essential fatty acids (EFA). This is both because they can be created by the human body from unsaturated fat, and are therefore not essential in the diet, and because the lack of an omega−6 double bond keeps them from participating in the reactions that form the eicosanoids.\n\nUnder severe conditions of EFA deprivation, mammals will elongate and desaturate oleic acid to make mead acid, (20:3, \"n\"−9). This has been documented to a lesser extent in one study following vegetarians and semi-vegetarians who followed diets without substantial sources of EFA.\n\n"}
{"id": "898781", "url": "https://en.wikipedia.org/wiki?curid=898781", "title": "Permalloy", "text": "Permalloy\n\nPermalloy is a nickel–iron magnetic alloy, with about 80% nickel and 20% iron content. Invented in 1914 by physicist Gustav Elmen at Bell Telephone Laboratories, it is notable for its very high magnetic permeability, which makes it useful as a magnetic core material in electrical and electronic equipment, and also in magnetic shielding to block magnetic fields. Commercial permalloy alloys typically have relative permeability of around 100,000, compared to several thousand for ordinary steel.\n\nIn addition to high permeability, its other magnetic properties are low coercivity, near zero magnetostriction, and significant anisotropic magnetoresistance. The low magnetostriction is critical for industrial applications, allowing it to be used in thin films where variable stresses would otherwise cause a ruinously large variation in magnetic properties. Permalloy's electrical resistivity can vary as much as 5% depending on the strength and the direction of an applied magnetic field. Permalloys typically have the face centered cubic crystal structure with a lattice constant of approximately 0.355 nm in the vicinity of a nickel concentration of 80%. A disadvantage of permalloy is that it is not very ductile or workable, so applications requiring elaborate shapes, such as magnetic shields, are made of other high permeability alloys such as mu metal. Permalloy is used in transformer laminations and magnetic recording heads.\n\nPermalloy was initially developed in the early 20th century for inductive compensation of telegraph cables. When the first transatlantic submarine telegraph cables were laid in the 1860s, it was found that the long conductors caused distortion which reduced the maximum signalling speed to only 10–12 words per minute. The right conditions for transmitting signals through cables without distortion were first worked out mathematically in 1885 by Oliver Heaviside. It was proposed by Carl Emil Krarup in 1902 in Denmark that the cable could be compensated by wrapping it with iron wire, increasing the inductance and making it a loaded line to reduce distortion. However, iron did not have high enough permeability to compensate a transatlantic-length cable. After a prolonged search, permalloy was discovered in 1914 by Gustav Elmen of Bell Laboratories, who found it had higher permeability than silicon steel. Later, in 1923, he found its permeability could be greatly enhanced by heat treatment. A wrapping of permalloy tape could reportedly increase the signalling speed of a telegraph cable fourfold.\n\nThis method of cable compensation declined in the 1930s, but by World War 2 many other uses for Permalloy were found in the electronics industry.\n\nOther compositions of permalloy are available, designated by a numerical prefix denoting the percentage of nickel in the alloy, for example \"45 permalloy\" means an alloy containing 45% nickel, and 55% iron. \"Molybdenum permalloy\" is an alloy of 81% nickel, 17% iron and 2% molybdenum. The latter was invented at Bell Labs in 1940. At the time, when used in long distance copper telegraph lines, it allowed a tenfold increase in maximum line working speed. Supermalloy, at 79% Ni, 16% Fe, and 5% Mo, is also well known for its high performance as a \"soft\" magnetic material, characterized by high permeability and low coercivity.\n\n\n"}
{"id": "30885530", "url": "https://en.wikipedia.org/wiki?curid=30885530", "title": "Petroleum Product Pricing Regulatory Agency", "text": "Petroleum Product Pricing Regulatory Agency\n\nThe Petroleum Products Pricing Regulatory Agency (PPPRA) is an agency of the government of Nigeria established in 2003 to, among other responsibilities, monitor and regulate the supply and distribution, and determine the prices of petroleum products in Nigeria. Its headquarters is located in Abuja, Nigeria.\n\nThe current Acting Executive Secretary of the Agency is Mrs. Sotonye E. Iyoyo (2016 - date). Previous chief executives are Dr. Oluwole Oluleye (2003 – 2009), Mr. Abiodun Ibikunle (2009 – 2011), Engr. Goody Chike Egbuji (2011 - 2011), Mr. Reginald Stanley (2011 - 2014) and Mr. Farouk Ahmed (2014 - 2016).\n\n\n"}
{"id": "56398", "url": "https://en.wikipedia.org/wiki?curid=56398", "title": "Phase diagram", "text": "Phase diagram\n\nA phase diagram in physical chemistry, engineering, mineralogy, and materials science is a type of chart used to show conditions (pressure, temperature, volume, etc.) at which thermodynamically distinct phases (such as solid, liquid or gaseous states) occur and coexist at equilibrium.\n\nCommon components of a phase diagram are \"lines of equilibrium\" or \"phase boundaries\", which refer to lines that mark conditions under which multiple phases can coexist at equilibrium. Phase transitions occur along lines of equilibrium.\n\nTriple points are points on phase diagrams where lines of equilibrium intersect. Triple points mark conditions at which three different phases can coexist. For example, the water phase diagram has a triple point corresponding to the single temperature and pressure at which solid, liquid, and gaseous water can coexist in a stable equilibrium ( and a partial vapor pressure of ).\n\nThe solidus is the temperature below which the substance is stable in the solid state. The liquidus is the temperature above which the substance is stable in a liquid state. There may be a gap between the solidus and liquidus; within the gap, the substance consists of a mixture of crystals and liquid (like a \"slurry\").\n\nWorking fluids are often categorized by on the basis of the shape of their phase diagram.\n\nThe simplest phase diagrams are pressure–temperature diagrams of a single simple substance, such as water. The axes correspond to the pressure and temperature. The phase diagram shows, in pressure–temperature space, the lines of equilibrium or phase boundaries between the three phases of solid, liquid, and gas.\n\nThe curves on the phase diagram show the points where the free energy (and other derived properties) becomes non-analytic: their derivatives with respect to the coordinates (temperature and pressure in this example) change discontinuously (abruptly). For example, the heat capacity of a container filled with ice will change abruptly as the container is heated past the melting point. The open spaces, where the free energy is analytic, correspond to single phase regions. Single phase regions are separated by lines of non-analytical behavior, where phase transitions occur, which are called phase boundaries.\n\nIn the diagram on the right, the phase boundary between liquid and gas does not continue indefinitely. Instead, it terminates at a point on the phase diagram called the critical point. This reflects the fact that, at extremely high temperatures and pressures, the liquid and gaseous phases become indistinguishable, in what is known as a supercritical fluid. In water, the critical point occurs at around \"T\" = , \"p\" = and \"ρ\" = 356 kg/m³.\n\nThe existence of the liquid–gas critical point reveals a slight ambiguity in labelling the single phase regions. When going from the liquid to the gaseous phase, one usually crosses the phase boundary, but it is possible to choose a path that never crosses the boundary by going to the right of the critical point. Thus, the liquid and gaseous phases can blend continuously into each other. The solid–liquid phase boundary can only end in a critical point if the solid and liquid phases have the same symmetry group.\n\nFor most substances, the solid–liquid phase boundary (or fusion curve) in the phase diagram has a positive slope so that the melting point increases with pressure. This is true whenever the solid phase is denser than the liquid phase. The greater the pressure on a given substance, the closer together the molecules of the substance are brought to each other, which increases the effect of the substance's intermolecular forces. Thus, the substance requires a higher temperature for its molecules to have enough energy to break out of the fixed pattern of the solid phase and enter the liquid phase. A similar concept applies to liquid–gas phase changes. \n\nWater is an exception which has a solid-liquid boundary with negative slope so that the melting point decreases with pressure. This occurs because ice (solid water) is less dense than liquid water, as shown by the fact that ice floats on water. At a molecular level, ice is less dense because it has a more extensive network of hydrogen bonding which requires a greater separation of water molecules. Other exceptions are antimony and bismuth.\n\nThe value of the slope dP/dT is given by the Clapeyron equation for fusion (melting)\nwhere ΔH is the heat of fusion which is always positive, and ΔV is the volume change for fusion. For most substances ΔV is positive so that the slope is positive. However for water and other exceptions, ΔV is negative so that the slope is negative.\n\nIn addition to temperature and pressure, other thermodynamic properties may be graphed in phase diagrams. Examples of such thermodynamic properties include specific volume, specific enthalpy, or specific entropy. For example, single-component graphs of temperature vs. specific entropy (\"T\" vs. \"s\") for water/steam or for a refrigerant are commonly used to illustrate thermodynamic cycles such as a Carnot cycle, Rankine cycle, or vapor-compression refrigeration cycle.\n\nIn a two-dimensional graph, two of the thermodynamic quantities may be shown on the horizontal and vertical axes. Additional thermodynamic quantities may each be illustrated in increments as a series of lines - curved, straight, or a combination of curved and straight. Each of these iso-lines represents the thermodynamic quantity at a certain constant value.\n\nIt is possible to envision three-dimensional (3D) graphs showing three thermodynamic quantities. For example, for a single component, a 3D Cartesian coordinate type graph can show temperature (\"T\") on one axis, pressure (\"p\") on a second axis, and specific volume (\"v\") on a third. Such a 3D graph is sometimes called a \"p\"–\"v\"–\"T\" diagram. The equilibrium conditions are shown as curves on a curved surface in 3D with areas for solid, liquid, and vapor phases and areas where solid and liquid, solid and vapor, or liquid and vapor coexist in equilibrium. A line on the surface called a triple line is where solid, liquid and vapor can all coexist in equilibrium. The critical point remains a point on the surface even on a 3D phase diagram.\n\nFor water, the 3D \"p\"–\"v\"–\"T\" diagram is seen here:\nAn orthographic projection of the 3D \"p\"–\"v\"–\"T\" graph showing pressure and temperature as the vertical and horizontal axes collapses the 3D plot into the standard 2D pressure–temperature diagram. When this is done, the solid–vapor, solid–liquid, and liquid–vapor surfaces collapse into three corresponding curved lines meeting at the triple point, which is the collapsed orthographic projection of the triple line.\n\nOther much more complex types of phase diagrams can be constructed, particularly when more than one pure component is present. In that case, concentration becomes an important variable. Phase diagrams with more than two dimensions can be constructed that show the effect of more than two variables on the phase of a substance. Phase diagrams can use other variables in addition to or in place of temperature, pressure and composition, for example the strength of an applied electrical or magnetic field, and they can also involve substances that take on more than just three states of matter.\n\nOne type of phase diagram plots temperature against the relative concentrations of two substances in a mixture called a \"binary phase diagram\", as shown at right. Such a mixture can be either a solid solution, eutectic or peritectic, among others. These two types of mixtures result in very different graphs. Another type of binary phase diagram is a \"boiling-point diagram\" for a mixture of two components, i. e. chemical compounds. For two particular volatile components at a certain pressure such as atmospheric pressure, a boiling-point diagram shows what vapor (gas) compositions are in equilibrium with given liquid compositions depending on temperature. In a typical binary boiling-point diagram, temperature is plotted on a vertical axis and mixture composition on a horizontal axis.\nA simple example diagram with hypothetical components 1 and 2 in a non-azeotropic mixture is shown at right. The fact that there are two separate curved lines joining the boiling points of the pure components means that the vapor composition is usually not the same as the liquid composition the vapor is in equilibrium with. See Vapor–liquid equilibrium for more information.\n\nIn addition to the above-mentioned types of phase diagrams, there are thousands of other possible combinations. Some of the major features of phase diagrams include congruent points, where a solid phase transforms directly into a liquid. There is also the peritectoid, a point where two solid phases combine into one solid phase during cooling. The inverse of this, when one solid phase transforms into two solid phases during cooling, is called the eutectoid.\n\nA complex phase diagram of great technological importance is that of the iron–carbon system for less than 7% carbon (see steel).\n\nThe x-axis of such a diagram represents the concentration variable of the mixture. As the mixtures are typically far from dilute and their density as a function of temperature is usually unknown, the preferred concentration measure is mole fraction. A volume-based measure like molarity would be inadvisable.\n\nPolymorphic and polyamorphic substances have multiple crystal or amorphous phases, which can be graphed in a similar fashion to solid, liquid, and gas phases.\n\nSome organic materials pass through intermediate states between solid and liquid; these states are called mesophases. Attention has been directed to mesophases because they enable display devices and have become commercially important through the so-called liquid-crystal technology. Phase diagrams are used to describe the occurrence of mesophases.\n\n\n"}
{"id": "51637929", "url": "https://en.wikipedia.org/wiki?curid=51637929", "title": "Piñatex", "text": "Piñatex\n\nPiñatex is a natural leather alternative made from cellulose fibres extracted from pineapple leaves, Piñatex was developed by Dr Carmen Hijosa and first presented at the PhD graduate exhibition at the Royal College of Art, London. Piñatex is manufactured and distributed by Dr Hijosa's company Ananas Anam Ltd.\n\nPiñatex's development began when Dr Hijosa was working as consultant in the leather goods industry in the Philippines in the 1990s. She observed the leather produced there by was poor quality, environmentally unsustainable and bad for the people involved in the industry. Dr Hijosa was by the barong tagalog, a traditional transparent garment worn over shirts in the Philippines made of pineapple fibres. She then spent seven years developing the product through a PhD at the Royal College of Art in London, and joint collaborations with Bangor University in Wales, Northampton Leather Technology Center, Leitat Technological Centre in Spain, alongside NonWoven Philippines Inc. in Manila, and Bonditex S.A., a textile finishing company in Spain.\n\nPiñatex is created by felting the long fibres from pineapple leaves together to create a non-woven substrate. The pineapple industry globally produces 40,000 tonnes of waste pineapple leaves each year, which are usually left to rot or are burned. Approximately 480 leaves (the waste from 16 pineapple plants) are needed to create 1 square metre of material. The material uses the long leaf fibres which are separated by the pineapple farmers for additional income, the leftover biomass from the process can be used as a fertiliser.\n\nBecause Piñatex is produced from a waste product it requires no additional land, water, pesticides or fertilizers. It also avoids the use of toxic chemicals and heavy metals used in animal leather production and has none of the wastage of leather caused by the shape of the animal's skin.\n\nPiñatex is produced in a range of colours and finishes, including a textured surface and a metallic finish. It has been described as having a softer, more pliable, \"leather-like\" texture than other synthetic leathers. It can also be cut, stitched, embossed and embroidered for different design uses. Piñatex is vegan and unlike petroleum based synthetic leather, the base mesh is compostable.\n\nPiñatex is breathable and flexible. It has been used in the manufacture of such products as bags, shoes, wallets, watch bands, and seat covers. The textile is being further developed for use in clothing. Products have been produced by designer Ally Capellino, LIAN & LIV, Time IV Change, ROMBAUT, and Nae; prototypes have been created by Puma and Camper. Bourgeois Boheme, a vegan footwear label, uses Piñatex in their sandals.\n\nIn 2016 Piñatex won the Arts Foundation UK award for Material Innovation and in 2015 Dr Hijosa was finalists of the Cartier Women's Initiative Awards. Piñatex is a PETA certified vegan fashion label.\n\nPineapple leather was highlighted on the first page of, and throughout, L.J.M. Owen's book, \"Egyptian Enigma\". It was the featured fabric on a journal gifted to the main character, Dr. Elizabeth Pimms, by her sister Sam Pimms, an ardent vegetarian.\n"}
{"id": "13442518", "url": "https://en.wikipedia.org/wiki?curid=13442518", "title": "Potbelly stove", "text": "Potbelly stove\n\nA potbelly stove is a cast-iron wood-burning stove, round with a bulge in the middle. The name is derived from the resemblance of the stove to that of a fat man's pot belly. They were designed to heat large spaces and were often found in train stations or one-room schoolhouses. The flat top of the fireplace allowed for cooking of food, or the heating of water.\n\n"}
{"id": "9238407", "url": "https://en.wikipedia.org/wiki?curid=9238407", "title": "Renewable portfolio standard (United States)", "text": "Renewable portfolio standard (United States)\n\nA Renewable Portfolio Standard (RPS) is a regulation that requires the increased production of energy from renewable energy sources, such as wind, solar, biomass, and geothermal. The federal RPS is called the Renewable Electricity Standard (RES).\n\nThe RPS mechanism generally places an obligation on electricity supply companies to produce a specified fraction of their electricity from renewable energy sources. Certified renewable energy generators earn certificates for every unit of electricity they produce and can sell these along with their electricity to supply companies. Supply companies then pass the certificates to some form of regulatory body to demonstrate their compliance with their regulatory obligations. Because it is a market mandate, the RPS relies almost entirely on the private market for its implementation. Unlike feed-in tariffs which guarantee purchase of all renewable energy regardless of cost, RPS programs tend to allow more price competition between different types of renewable energy, but can be limited in competition through eligibility and multipliers for RPS programs. Those supporting the adoption of RPS mechanisms claim that market implementation will result in competition, efficiency, and innovation that will deliver renewable energy at the lowest possible cost, allowing renewable energy to compete with cheaper fossil fuel energy sources.\n\nRPS programs have been adopted in 29 of 50 U.S. states, and the District of Columbia.\n\nOf all the state-based RPS programs in place today, no two are the same. Each has been designed taking into account state-specific policy objectives (e.g. economic growth, diversity of energy supply, environmental concerns), local resource endowment, political considerations, and the capacity to expand renewable energy production. At the most basic level, this gives rise to differing RPS targets and years (e.g. Arizona's 15% by 2025 and Colorado's 30% by 2020). Other factors in program design include resource eligibility, in-state requirements, new build requirements, technology favoritism, lobbying by industry associations and non-profits, groups cost caps, program coverage (IOUs versus Cooperatives and Municipal utilities), cost recovery by utilities, penalties for non-compliance, rules regarding REC creation and trading, and additional non-binding goals. Since RPS programs create a mandate to purchase renewable energy, they create a lucrative captive market of buyers for renewable energy producers who are eligible in a particular state's RPS program to issue RECs. A state may choose to promote new investment in renewable energy generation capacity by not making eligible existing renewable energy such as hydroelectric plants or geothermal energy to qualify under an RPS program.\n\nMany states that have mandatory Renewable Portfolio Standards also have additional voluntary targets either for the total proportion of renewable energy or for a particular technology type.\n\nIn many states, municipalities and cooperatives are exempt from the RPS target, have a lower target, or are required to develop their own targets. Furthermore, in some states such as Minnesota, individual utilities (e.g., Xcel Energy) are singled out for special treatment.\n\nStates with RPS programs have associated renewable energy certificate trading programs. RECs provide a mechanism by which to track the amount of renewable power being sold and to financially reward eligible power producers. For each unit of power that an eligible producer generates, a certificate or credit is issued. These can then be sold either in conjunction with the underlying power or separately to energy supply companies. A market exists for RECs because energy supply companies are required to redeem certificates equal to their obligation under the RPS program. State specific programs or various applications (e.g., WREGIS, M-RETS, NEPOOL GIS) are used to track REC issuance and ownership. These credits can in some programs be 'banked' (for use in future years) or borrowed (to meet current year commitments). There is a great deal of variety among the states in the handling and functioning of RECs and this will be a major issue in integrating state and federal programs.\n\nRPS multipliers are powerful tools for regulators to direct revenue, investment, and job creation to particular types of renewable energy vs a free market of renewable energy. Since the definition of what is renewable energy isn't clear cut, for example, nuclear power, and whether an RPS program should consider environmental damage of a renewable energy source (for example, hydroelectric dams, bird strikes of wind turbines, geothermal earthquakes, solar thermal water use) affects RPS program implementations. A state can use a multiplier as protectionism to local renewable energy generators from out of state renewable generators. Since RECs are regulated at a state level, their ability to be traded over state lines varies.\n\nOver 16 of the approximately 30 states with RPS programs have also established a set-aside for solar energy. This results in the creation and trading of RECs specific to solar known as solar renewable energy certificates (SRECs). With a separate market for SRECs, states are able to ensure that a portion of their renewable energy comes from solar. As a result, states with solar carve-outs, such as New Jersey, have had more success in promoting solar energy through the RPS than states, such as Texas, with a generic REC market or REC multiplier.\n\nEnergy supply companies need to show that they have acquired a particular percentage of their power sales from the designated technology type. Multiple technology types are bundled together in 'tiers' or 'classes' with similar effect. Not all states have set-asides or tiers (some preferring to promote particular technologies through credit multipliers) and each state that groups technologies together in a tier does so differently.\n\nEvery state defines 'renewable' technologies differently. Many states exclude existing renewable facilities from benefiting from an RPS program for the same reason. A state's definition of eligible technologies is also driven by the objectives of the program. Programs designed to promote diversity in generation types may include or promote technologies different from programs designed to achieve environmental goals.\n\nIn a 2011 report published by the Union of Concerned Scientists, Doug Koplow said: \n\nNuclear power should not be eligible for inclusion in a renewable portfolio standard. Nuclear power is an established, mature technology with a long history of government support. Furthermore, nuclear plants are unique in their potential to cause catastrophic damage (due to accidents, sabotage, or terrorism); to produce very long-lived radioactive wastes; and to exacerbate nuclear proliferation.\nThe United Nations classifies a particular subset of presently operating nuclear fission technologies as renewable. Reactors that produce more fissile fuel than they consume - breeder reactors and, eventually, nuclear fusion, are classified within the same category as conventional renewable energy sources, such as solar and falling water.\n\nIn order to motivate compliance, states that have enforceable standards will have penalties for utilities that fail to reach the specified targets. States may choose to set penalty values or make arbitrary penalty amounts when suppliers fail to meet a renewable target. Where specific technologies are promoted through either tiers or set-aside provisions, the penalties for missing these targets are typically separate and higher. Some states have higher penalties for repeat violations and others escalate penalties on a yearly basis according to price indices.\n\nAll states either place caps on the cost of the program or include some form of 'escape clause' whereby the regulatory authority can suspend the program or exempt utilities from meeting its requirements. The need for such measures arises from the difficulties in estimating in advance the actual cost of the RPS program. The realized cost to the utility and the ratepayer is not known until the supply and cost base of renewable power, along with actual demand, is established. However, likely costs can be estimated, and some states appear to have set cost caps low enough that complete RPS requirements could not be fulfilled without a significant decrease in renewables costs.\n\nWith few exceptions, utilities are allowed to recover the additional cost of procuring renewable power. The method by which this can be achieved varies by state. Some states opt for a ratepayer surcharge while others require utilities to include costs in rate base. In some instances, utilities are even able to recover the cost of penalties associated with non-compliance.\n\nHowever, the federal government has discussed enacting a nationwide RPS in the future. Such a policy would establish a common goal for every state in the country, which is less confusing than the state by state table below. If the federal government does pass a national Renewable Portfolio Standard it could be problematic for some states. Since each state has a unique environmental landscape, each state has different abilities when it comes to producing renewable energy. States with less renewable resources available could be penalized for their lack of ability.\n\nRPS mechanisms have tended to be most successful in stimulating new renewable energy capacity in the United States where they have been used in combination with federal Production Tax Credits (PTC). In periods, where PTC have been withdrawn the RPS alone has often proven to be insufficient stimulus to incentivise large volumes of capacity.\n\nPublic Utility Regulatory Policies Act is a law, passed in 1978 by the United States Congress as part of the National Energy Act. It is meant to promote greater use of renewable energy.\n\nIn 2009, the US Congress considered Federal level RPS requirements. The \"American Clean Energy and Security Act\" reported out of committee in July by the Senate Committee on Energy & Natural Resources includes a Renewable Electricity Standard that calls for 3% of U.S. electrical generation to come from non-hydro renewables by 2011–2013. However, the proposed Support Renewable Energy Act died in the 111th Congress.\n\nIn 2007, the Edison Electric Institute, a trade association for America’s investor-owned utilities, reiterated their continuing opposition to a nationwide RPS; among the reasons included were that it conflicts with and preempts existing RES programs passed in many states, it does not adequately consider the uneven distribution of renewable resources across the country, and it creates inequities among utility customers, by specifically exempting all rural electric cooperatives, and government-owned utilities from the RES mandate.\n\nThe American Legislative Exchange Council (ALEC) has drafted the model bill Electricity Freedom Act, which ALEC affiliate representatives are now attempting to roll out in various states and which \"would end requirements for states to derive a specific percentage of their electricity needs from renewable energy sources.\" As a result, of being unable to stop the approval of this model legislation, the American Wind Energy Association and the Solar Energy Industry Association allowed their ALEC membership lapse after one year as members.\n\nDifferent state RPS programs issue a different number of Renewable Energy Credits depending on the generation technology; for example, solar generation counts for twice as much as other renewable sources in Michigan and Virginia.\n\nThe California Renewables Portfolio Standard was created in 2002 under Senate Bill 1078 and further accelerated in 2006 under Senate Bill 107. The bills stipulate that California electricity corporations must expand their renewable portfolio by 1% each year until reaching 20% in 2010. On November 17, 2008, Governor Arnold Schwarzenegger signed executive order S-14-08 which mandated a RPS of 33% by 2020 which sits in addition to the 20% by 2010 order. The target has been extended to 50% by 2030. In September of 2018, Governor Jerry Brown signed legislation increasing the state's requirement to 100% renewable energy by 2045 and increasing the interim target to 60% by 2030.\n\nThe Colorado Renewable Portfolio Standard was updated from 20% to 30% in the 2010 Legislative Session as House Bill 1001. This increase is anticipated to increase solar industry jobs from current (2009) estimated 2,500 to 33,500 by 2020. The updated RPS is also anticipated to create an additional $4.3B (U.S.) in state revenue within the industries.\n\nOn October 6, 2008, Public Act 295 was signed into law in the State of Michigan. This Act, known as the Clean, Renewable and Efficient Energy Act, established a Renewable Energy Standard for the State of Michigan. The Renewable Energy Standard requires Michigan electric providers to achieve a retail supply portfolio that includes at least 10% renewable energy by 2015.\n\nA ballot proposal to raise the standard to 25% renewable energy by 2025 as a constitutional amendment was put to the voters in the November 2012 General Election as Proposal 3. A Proposal To Amend the State Constitution to Establish a Standard for Renewable Energy. The ballot proposal was defeated with over 60% opposing the proposal.\n\nAccording to the State of Michigan, as of March 4, 2013 \"progress toward the first compliance year in 2012 and the 10 percent renewable energy standard in 2015 is going smoothly. Michigan’s electric providers are on track to meet the 10 percent renewable energy requirement. The renewable energy standard is resulting in the development of new renewable capacity and can be credited with the development of over 1,000 MW of new renewable energy projects becoming commercially operational since the Act became law. The weighted average price of renewable energy contracts is $82.54 per MWh which is less than forecasted in REPs.\"\n\nIn 1997 Nevada passed a Renewable Portfolio Standard as part of their 1997 Electric Restructuring Legislation (AB 366) It required any electric providers in the state to acquire actual renewable electric generation or purchase renewable energy credits so that each utility had 1 percent of total consumption in renewables. However, on June 8, 2001, Nevada Governor Kenny Guinn signed SB 372, at the time the country's most aggressive renewable portfolio standard. The law requires that 15 percent of all electricity generated in Nevada be derived from new renewables by the year 2013.\n\nThe Nevada RPS includes double goal. The 2001 revision requires that at least 5 percent of the renewable energy projects must generate electricity from solar energy.\n\nIn June 2005, the Nevada legislature passed a bill during a special legislative session that modified the Nevada RPS (Assembly Bill 03). The bill extends the deadline and raised the requirements of the RPS to 20 percent of sales by 2015.\n\nFlorida<br>On Friday January 9, 2009 the Florida Public Service Commission unanimously agreed to require the state's utilities to generate 20 percent of their power from renewable resources by 2020.<br> This is still not law until the legislature approves.\nThis will drastically change the landscape for renewable energy applications for a state that gets less than 3 percent of its power from renewable energy. The proposal calls for 7 percent renewable energy by January 2013, 12 percent by 2016, 18 percent by 2019 and 20 percent by end of 2020.\n\nIn an April 2008 unanimous vote, the Ohio legislature passed a bill requiring 25 percent of Ohio's energy to be generated from alternative and renewable sources, of which half or 12.5 percent must derive from renewable sources. However, in June 2014, the state froze its RPS at 2.5 percent for two years.\n\nFor Oregon’s three largest utilities (Portland General Electric (PGE), PacifiCorp and the Eugene Water and Electric Board), the standard starts at 5% in 2011, increases to 15% in 2015, 20% in 2020, and 25% in 2025. Other electric utilities in the state, depending on size, have standards of 5% or 10% in 2025. In 2016, the target was raised to 50%, as two companies must supply 50% of Oregon's power as renewable by 2040.\n\nThe Texas Renewable Portfolio Standard was originally created by Senate Bill 7 in 1999. The Texas RPS mandated that utility companies jointly create 2000 new MWs of renewables by 2009 based on their market share. In 2005, Senate Bill 20, increased the state’s RPS requirement to 5,880 MW by 2015, of which, 500 MW must come from non-wind resources. The bill set a goal of 10,000 MW of renewable energy capacity for 2025. The state's installed capacity reached the 10,000 MW target in early 2010, \n15 years ahead of schedule.\n\n\n"}
{"id": "27135340", "url": "https://en.wikipedia.org/wiki?curid=27135340", "title": "Seaweed farming", "text": "Seaweed farming\n\nSeaweed farming is the practice of cultivating and harvesting seaweed. In its simplest form, it consists of the management of naturally found batches. In its most advanced form, it consists of fully controlling the life cycle of the algae. The main food species grown by aquaculture in Japan, China and Korea include \"Gelidium\", \"Pterocladia\", \"Porphyra\", and \"Laminaria\". Seaweed farming has frequently been developed as an alternative to improve economic conditions and to reduce fishing pressure and over exploited fisheries. Seaweeds have been harvested throughout the world as a food source as well as an export commodity for production of agar and carrageenan products.\n\nCultivation of \"gim\" (laver) in Korea is reported in the books from 15th century, such as \"Revised and Augmented Survey of the Geography of Korea\" and \"Geography of Gyeongsang Province\".\n\nSeaweed farming began in Japan as early as 1670 in Tokyo Bay. In autumn of each year, farmers would throw bamboo branches into shallow, muddy water, where the spores of the seaweed would collect. A few weeks later these branches would be moved to a river estuary. The nutrients from the river would help the seaweed to grow.\n\nIn the 1940s, the Japanese improved this method by placing nets of synthetic material tied to bamboo poles. This effectively doubled the production. A cheaper variant of this method is called the \"hibi\" method — simple ropes stretched between bamboo poles.\n\nSeaweed farming has been introduced to several different islands in Indonesia such as Rote island. The Australian government first brought agar seeds to Rote 15 years ago to keep locals out of poverty.\n\nIn the early 1970s there was a recognized demand for seaweed and seaweed products, outstripping supply, and cultivation was viewed as the best means to increase productions.\n\nThe earliest seaweed farming guides in the Philippines recommended cultivation of \"Laminaria\" seaweed and reef flats at approximately one metre's depth at low tide. They also recommended cutting off sea grasses and removing sea urchins prior to farm construction. Seedlings are then tied to monofilament lines and strung between mangrove stakes pounded into the substrate. This off-bottom method is still one of the major methods used today.\n\nThere are new long-line cultivation methods that can be used in deeper water approximately 7 metres in depth. They use floating cultivation lines anchored to the bottom and are the primary methods used in the villages of North Sulawesi, Indonesia.\n\nCultivation of seaweed in Asia is a relatively low-technology business with a high labour requirement. There have been many attempts in various countries to introduce high technology to cultivate detached plants growth in tanks on land in order to reduce labour, but they have yet to attain commercial viability.\n\nSeveral environmental problems can result from seaweed farming. Sometimes seaweed farmers cut down mangroves to use as stakes for their ropes. This, however, negatively affects the farming since it reduces the water quality and mangrove biodiversity due to depletion. Farmers may also sometimes remove eelgrass from their farming areas. This, however, is also discouraged, as it adversely affects water quality.\n\nSeaweed farming helps to preserve coral reefs, by increasing diversity where the algae and seaweed have been introduced and it also provides added niche for local species of fish and invertebrates. Farming may be beneficial by increasing the production of herbivorous fishes and shellfish in the area. reported an increase in Siginid population after the start of extensive farming of eucheuma seaweed in villages in North Sulawesi, Indonesia.\n\nSeaweed culture can also be used to capture, absorb, and eventually incorporate excessive nutrients into living tissue. \"Nutrient bioextraction\" is the preferred term for bioremediation involving cultured plants and animals. Nutrient bioextraction (also called bioharvesting) is the practice of farming and harvesting shellfish and seaweed for the purpose of removing nitrogen and other nutrients from natural water bodies. (See main article Nutrient pollution.)\n\nSeaweed farming can be an actor in biological carbon sequestration.\n\nThe practice of seaweed farming has long since spread beyond Japan. In 1997, it was estimated that 40,000 people in the Philippines made their living through seaweed farming. Cultivation is also common in all of southeast Asia, Canada, Great Britain, Spain, and the United States.\n\nIn Japan alone, annual production value of nori amounts to US$2 billion and is one of the world's most valuable crops produced by aquaculture. The high demand in seaweed production provides plentiful opportunities and work for the local community. A study conducted by the Philippines showed that plots of approximately one hectare can have a net income from eucheuma farming that was 5 to 6 times that of the minimum average wage of an agriculture worker. In the same study, they also saw an increase in seaweed exports from 675 metric tons (MT) in 1967 to 13,191 MT in 1980, which doubled to 28,000 MT by 1988.\n\n"}
{"id": "57875", "url": "https://en.wikipedia.org/wiki?curid=57875", "title": "Soap", "text": "Soap\n\nSoap is the term for a salt of a fatty acid or for a variety of cleansing and lubricating products produced from such a substance. Household uses for soaps include washing, bathing, and other types of housekeeping, where soaps act as surfactants, emulsifying oils to enable them to be carried away by water. In industry, they are used as thickeners, components of some lubricants, and precursors to catalysts.\n\nSince they are salts of fatty acids, soaps have the general formula (RCO)M (R is an alkyl). The major classification of soaps is determined by the identity of M. When M is Na or K, the soaps are called toilet soaps, used for handwashing. Many metal dications (Mg, Ca, and others) give metallic soap. When M is Li, the result is lithium soap (e.g., lithium stearate), which is used in high-performance greases.\n\nSoaps are key components of most lubricating greases and thickeners. Greases are usually emulsions of calcium soap or lithium soap and mineral oil. Many other metallic soaps are also useful, including those of aluminium, sodium, and mixtures thereof. Such soaps are also used as thickeners to increase the viscosity of oils. In ancient times, lubricating greases were made by the addition of lime to olive oil.\n\nMetal soaps are also included in modern artists' oil paints formulations as a rheology modifier.\nMost heavy metal soaps are prepared by neutralization of purified fatty acids:\n\nIn a domestic setting, \"soap\" usually refers to what is technically called a toilet soap, used for household and personal cleaning. When used for cleaning, soap solubilizes particles and grime, which can then be separated from the article being cleaned.\nThe insoluble oil/fat molecules become associated inside micelles, tiny spheres formed from soap molecules with polar hydrophilic (water-attracting) groups on the outside and encasing a lipophilic (fat-attracting) pocket, which shields the oil/fat molecules from the water making it soluble. Anything that is soluble will be washed away with the water.\n\nThe production of toilet soaps usually entails saponification of fats (triglycerides). Triglycerides are vegetable or animal oils and fats. An alkaline solution (often lye or sodium hydroxide) induces saponification whereby the triglyceride fats first hydrolyze into salts of fatty acids. Glycerol (glycerin) is liberated. The glycerin can remain in the soap product as a softening agent, although it is sometimes separated.\n\nThe type of alkali metal used determines the kind of soap product. Sodium soaps, prepared from sodium hydroxide, are firm, whereas potassium soaps, derived from potassium hydroxide, are softer or often liquid. Historically, potassium hydroxide was extracted from the ashes of bracken or other plants. Lithium soaps also tend to be hard. These are used exclusively in greases.\n\nFor making toilet soaps, triglycerides (oils and fats) are derived from coconut, olive, or palm oils, as well as tallow. Triglyceride is the chemical name for the triesters of fatty acids and glycerin. Tallow, \"i.e.,\" rendered beef fat, is the most available triglyceride from animals. Each species offers quite different fatty acid content, resulting in soaps of distinct feel. The seed oils give softer but milder soaps. Soap made from pure olive oil, sometimes called Castile soap or Marseille soap, is reputed for its particular mildness. The term \"Castile\" is also sometimes applied to soaps from a mixture of oils, but a high percentage of olive oil.\n\nThe earliest recorded evidence of the production of soap-like materials dates back to around 2800 BC in ancient Babylon. A formula for soap consisting of water, alkali, and cassia oil was written on a Babylonian clay tablet around 2200 BC.\n\nThe Ebers papyrus (Egypt, 1550 BC) indicates the ancient Egyptians bathed regularly and combined animal and vegetable oils with alkaline salts to create a soap-like substance. Egyptian documents mention a similar substance was used in the preparation of wool for weaving.\n\nIn the reign of Nabonidus (556–539 BC), a recipe for soap consisted of \"uhulu\" [ashes], cypress [oil] and sesame [seed oil] \"for washing the stones for the servant girls\".\n\nThe word \"sapo\", Latin for soap, likely was borrowed from an early Germanic language and is cognate with Latin \"sebum\", \"tallow\". It first appears in Pliny the Elder's account. \"Historia Naturalis\", which discusses the manufacture of soap from tallow and ashes, but the only use he mentions for it is as a pomade for hair; he mentions rather disapprovingly that the men of the Gauls and Germans were more likely to use it than their female counterparts. Aretaeus of Cappadocia, writing in the first century AD, observes among \"Celts, which are men called Gauls, those alkaline substances that are made into balls [...] called \"soap\"\". The Romans' preferred method of cleaning the body was to massage oil into the skin and then scrape away both the oil and any dirt with a strigil. The Gauls used soap made from animal fat.\n\nZosimos of Panopolis, \"circa\" 300 AD, describes soap and soapmaking. Galen describes soap-making using lye and prescribes washing to carry away impurities from the body and clothes. The use of soap for personal cleanliness became increasingly common in the 2nd century A.D. According to Galen, the best soaps were Germanic, and soaps from Gaul were second best.\n\nA detergent similar to soap was manufactured in ancient China from the seeds of Gleditsia sinensis. Another traditional detergent is a mixture of pig pancreas and plant ash called \"Zhu yi zi\". True soap, made of animal fat, did not appear in China until the modern era. Soap-like detergents were not as popular as ointments and creams.\n\nHard toilet soap with a pleasant smell was produced in the Middle East during the Islamic Golden Age, when soap-making became an established industry. Recipes for soap-making are described by Muhammad ibn Zakariya al-Razi (854–925), who also gave a recipe for producing glycerine from olive oil. In the Middle East, soap was produced from the interaction of fatty oils and fats with alkali. In Syria, soap was produced using olive oil together with alkali and lime. Soap was exported from Syria, to other parts of the Muslim world and to Europe.\n\nA 12th-century Islamic document describes the process of soap production. It mentions the key ingredient, alkali, which later becomes crucial to modern chemistry, derived from \"al-qaly\" or \"ashes\".\n\nBy the 13th century, the manufacture of soap in the Islamic world had become virtually industrialized, with sources in Nablus, Fes, Damascus, and Aleppo.\n\nSoapmakers in Naples were members of a guild in the late sixth century (then under the control of the Eastern Roman Empire), and in the eighth century, soap-making was well known in Italy and Spain. The Carolingian capitulary \"De Villis\", dating to around 800, representing the royal will of Charlemagne, mentions soap as being one of the products the stewards of royal estates are to tally. The lands of Medieval Spain were a leading soapmaker by 800, and soapmaking began in the Kingdom of England about 1200. Soapmaking is mentioned both as \"women's work\" and as the produce of \"good workmen\" alongside other necessities, such as the produce of carpenters, blacksmiths, and bakers.\n\nIn Europe, soap in the 9th century was produced from animal fats and had an unpleasant smell. Hard toilet soap with a pleasant smell was later imported from the Middle East.\n\nIn France, by the second half of the 15th century, the semi-industrialized professional manufacture of soap was concentrated in a few centers of Provence—Toulon, Hyères, and Marseille—which supplied the rest of France. In Marseilles, by 1525, production was concentrated in at least two factories, and soap production at Marseille tended to eclipse the other Provençal centers. English manufacture tended to concentrate in London.\n\nFiner soaps were later produced in Europe from the 16th century, using vegetable oils (such as olive oil) as opposed to animal fats. Many of these soaps are still produced, both industrially and by small-scale artisans. Castile soap is a popular example of the vegetable-only soaps derived from the oldest \"white soap\" of Italy.\n\nIndustrially manufactured bar soaps became available in the late 18th century, as advertising campaigns in Europe and America promoted popular awareness of the relationship between cleanliness and health. In modern times, the use of soap has become commonplace in industrialized nations due to a better understanding of the role of hygiene in reducing the population size of pathogenic microorganisms. \n\nUntil the Industrial Revolution, soapmaking was conducted on a small scale and the product was rough. In 1780, James Keir established a chemical works at Tipton, for the manufacture of alkali from the sulfates of potash and soda, to which he afterwards added a soap manufactory. The method of extraction proceeded on a discovery of Keir's. Andrew Pears started making a high-quality, transparent soap in 1807 in London. His son-in-law, Thomas J. Barratt, opened a factory in Isleworth in 1862.\n\nDuring the Restoration era (February 1665 – August 1714) a soap tax was introduced in England, which meant that until the mid-1800s, soap was a luxury, used regularly only by the well-to-do. The soap manufacturing process was closely supervised by revenue officials who made sure that soapmakers' equipment was kept under lock and key when not being supervised. Moreover, soap could not be produced by small makers because of a law which stipulated that soap boilers must manufacture a minimum quantity of one imperial ton at each boiling, which placed the process beyond reach of the average person. The soap trade was boosted and deregulated when the tax was repealed in 1853.\n\nWilliam Gossage produced low-priced, good-quality soap from the 1850s. Robert Spear Hudson began manufacturing a soap powder in 1837, initially by grinding the soap with a mortar and pestle. American manufacturer Benjamin T. Babbitt introduced marketing innovations that included sale of bar soap and distribution of product samples. William Hesketh Lever and his brother, James, bought a small soap works in Warrington in 1886 and founded what is still one of the largest soap businesses, formerly called Lever Brothers and now called Unilever. These soap businesses were among the first to employ large-scale advertising campaigns.\n\nLiquid soap was not invented until the nineteenth century; in 1865, William Shepphard patented a liquid version of soap. In 1898, B.J. Johnson developed a soap derived from palm and olive oils; his company, the B.J. Johnson Soap Company, introduced \"Palmolive\" brand soap that same year. This new brand of soap became popular rapidly, and to such a degree that B.J. Johnson Soap Company changed its name to Palmolive.\n\nIn the early 1900s, other companies began to develop their own liquid soaps. Such products as Pine-Sol and Tide appeared on the market, making the process of cleaning things other than skin, such as clothing, floors, and bathrooms, much easier.\n\nLiquid soap also works better for more traditional or non-machine washing methods, such as using a washboard.\n\nA variety of methods are available for hobbyists to make soap. Most soapmakers use processes where the glycerol remains in the product, and the saponification continues for many days after the soap is poured into molds. The glycerol is left during the hot-process method, but at the high temperature employed, the reaction is practically completed in the kettle, before the soap is poured into molds. This simple and quick process is employed in small factories all over the world.\n\nHandmade soap from the cold process also differs from industrially made soap in that an excess of fat is used, beyond that needed to consume the alkali (in a cold-pour process, this excess fat is called \"superfatting\"), and the glycerol left in acts as a moisturizing agent. However, the glycerine also makes the soap softer. Addition of glycerol and processing of this soap produces glycerin soap. Superfatted soap is more skin-friendly than one without extra fat, although it can leave a \"greasy\" feel. Sometimes, an emollient is added, such as jojoba oil or shea butter. Sand or pumice may be added to produce a scouring soap. The scouring agents serve to remove dead cells from the skin surface being cleaned. This process is called exfoliation. \nTo make antibacterial soap, compounds such as triclosan or triclocarban can be added. There is some concern that use of antibacterial soaps and other products might encourage antibiotic resistance in microorganisms.\n\n\n\n"}
{"id": "36649360", "url": "https://en.wikipedia.org/wiki?curid=36649360", "title": "Solar Academy International", "text": "Solar Academy International\n\nSolar Academy International is a global Solar energy training school network. Solar Academy International's first training center, Ontario Solar Academy, was established in 2009 in Toronto, Ontario, Canada. Entrepreneurs and founders of Solar Academy International, Jacob Travis and David Gower, created the training school in response to demands by solar companies experiencing a shortage in qualified workers within the solar industry, following the surge in interest in solar energy upon the passing of the Ontario Green Energy Act 2009 that led to the creation of the Feed-in Tariff Program. Today, Solar Academy International is the largest training school in Canada, having trained over 700 student graduates, with operations expanding overseas.\n\nThe school's most popular class is a 5-day intensive course in which students are taught the fundamental basics of the solar industry including aspects of design, sales, and installation by NABCEP-certified installers. Instructors have, in addition to sharing a wealth of field experience and knowledge in best practices, been known to take on students for solar projects, offering practical, paid experience to beginners looking to enter the solar industry. The course concludes with a field trip to a LEED-certified Platinum facility with an operational solar PV system. This intensive course also satisfies the education requirements laid out by the North American Board of Certified Energy Practitioners (NABCEP) for the Entry Level Program. Students who have completed the course are eligible to write the NABCEP Entry Level Exam at one of Castle Worldwide's testing facilities by registering through the Academy.\n\nSolar Academy International also recently started offering a multiple month internship program for interested applicants to get both knowledge and experience into the solar industry. The internship is tailored to the intern's strengths and interests and offers flexibility in length and scope.\n\nOn August 27, 2010, the then-named Ontario Solar Academy was the first solar PV training school in Canada to receive the Institute for Sustainable Power Quality (ISPQ) accreditation by the Interstate Renewable Energy Council. ISPQ Accreditation is internationally recognized for having high standards in solar energy education and is awarded to training providers. Enrollment in the course surged upon approval of this accreditation, with many students satisfied with the quality of instruction and comprehensiveness of the introductory course.\n\nCourses in Canada are held at the Franken Solar facilities in Mississauga, Ontario. Past courses have also been held in select cities in the United States. Workshops in Miami and Toronto are slated to take place in December 2014.\n\n"}
{"id": "4434437", "url": "https://en.wikipedia.org/wiki?curid=4434437", "title": "Suzlon", "text": "Suzlon\n\nSuzlon Energy Ltd. is a wind turbine supplier based in Pune, India. It was formerly ranked by MAKE as the world's fifth largest wind turbine supplier. It has since dropped out of the Global top ten rankings (as of 2014) due to extensive losses and inability to repay debts.. \nThe company's website claims to have over 17,000 MW of wind energy capacity installed globally, with operations across 18 countries and a workforce of over 8,000. Despite financial issues, it continues to be a major manufacturer of wind turbines; in 2016, the company posted a profit EBITDA after accruing losses over seven consecutive years.\n\nThe company is listed on the National Stock Exchange of India (NSE:SUZLONEQ) and on the Bombay Stock Exchange (BSE:532667). Though once considered a favourable stock, and a favourite of the stock broker Rakesh Jhunjhunwala, it fell out of favour as the company posted continuous losses. It fell from a high of Rs. 68.75 in 2010 to a low of Rs. 18.5 in 2014, with a single day drop of 10% in September 2014. It continues to trade low at less than Rs. 17 per share in 2018.\n\nSuzlon is a vertically integrated wind power company. It makes and installs wind turbines, and manufactures blades, generators, panels, and towers in-house. It is integrated downstream and delivers turnkey projects through its project management and installation consultancy, and operations & maintenance services. Suzlon has offices, R&D and technology centres, manufacturing facilities and service support centres spread across the globe, with its head office in Pune, India.The company's larger offices, design and R&D teams are located in India, Germany, Denmark and The Netherlands. As per its website, Suzlon has fifteen manufacturing facilities and a workforce of over 8,000 employees globally.\n\nIn 1995, founder Tulsi Tanti was managing a 20-employee textile company. Due to the erratic availability of power locally, and its rising costs, the highest business expenditure after the raw materials was electricity. The cost of electricity also offset any profits made by the company. After providing electricity for his own company, Tanti moved into wind energy production as a way to secure the textile company's energy needs, and founded Suzlon Energy. Suzlon adopted a business model wherein clients would be responsible for 25% of the up-front capital investment and Suzlon would arrange the remaining 75% on loan. Initially, banks were hesitant to fund loans for this model, but by 2008, many Indian banks started financing wind power projects for Suzlon clients.\n\nIn 2001, Tanti sold off the textile business; Suzlon is still actively run by Tulsi Tanti, now in the role of Chairman, Suzlon Group.\n\nIn 2003, Suzlon got its first order in USA from DanMar & Associates to supply 24 turbines in southwestern Minnesota. Also in 2003 Suzlon set up an office in Beijing.\n\nSuzlon Rotor Corporation in 2006 began producing the blades in Pipestone, Minnesota in the United States. Among its clients is Wind Capital Group.\n\nIn the year 2006, Suzlon reached a definitive agreement for acquisition of Belgium firm Hansen Transmissions, specializing in gearboxes for wind turbines, for $565 million. In 2007, the company purchased a controlling stake in Germany's Senvion (then operating as REpower Systems) which valued the firm at US$1.6 billion.\n\nIn June 2007, Suzlon had signed a contract with Edison Mission Energy (EME) of US for delivery of 150 wind turbines of 2.1 megawatts in 2008 and a similar volume to be delivered in 2009. EME had an option not to purchase the 150 turbines due to be delivered in 2009, which it has chosen to exercise.\n\nIn November 2009, the company decided to sell 35% stake of Hansen for $370 million as part of its debt restructuring program, through placing new shares. It appointed Bank of America Merrill Lynch and Morgan Stanley as the managers and book runners for the same.\n\nIn January 2011, Suzlon received an order worth US$1.28 billion for building 1000 megawatts of wind energy projects from the Indian branch of the Lord Swaraj Paul-owned Caparo Energy Ltd.\n\nIn May 2011, Suzlon announced returning to profitability after the financial crisis of 2009\nIn October 2011, Suzlon sold its remaining 26.06% stake in Hansen Transmissions International NV to ZF Friedrichshafen AG for .\n\nIn the same month, it also achieved full control of its German subsidiary REpower Systems (now Senvion) by acquiring the remaining 5% stake held by minority shareholders that resisted the takeover. The takeover was completed through the squeeze-out procedure by paying EUR 63 Million.\nIt has to redeem 500 million worth of FCCB's (foreign currency convertible bonds) in 2012 in tranches of 300 million in June and 200 million in October respectively.\nIn line with the previously announced strategy to dispose of non-critical group assets to reduce long-term debt, Suzlon Chairman said that Suzlon Energy, will sell stake in its China manufacturing unit to China Power New Energy Development Company Limited for 3.4 billion rupees ($60 million).\n\nOn 30 November 2013 the Suzlon Group subsidiary REpower Systems (now Senvion SE) won an Engineering, Procurement and Construction (EPC) contract from Mitsui & Co (Australia) Ltd to deliver 52 wind turbines with a total rated output of 106.6 MW for the Bald Hills wind farm in Victoria, Australia.\n\nAs of August 2014, Suzlon's debt was over 8000 crores. On 22 January 2015, Suzlon announced the sale of Senvion SE, its wholly owned subsidiary, to Centerbridge Partners, a private equity firm in a deal valued at 7200 crores. The deal is expected to ease Suzlon's debt burden. In a further equity infusion, Dilip Shanghvi Family and Associates (DSA), run by Dilip Shanghvi, the founder and managing directory of Sun Pharmaceutical, agreed to purchase a 23 percent stake in Suzlon for a sum of 1800 crores. The deal will see Tanti's holding shrink to 24 percent, but management control will still remain with the Tanti family. Its total borrowings stood at Rs 11430.76 crore in FY16 from Rs 17810.96 crore FY15.\n\nOn 17 January 2017, Suzlon Energy achieved 10,000 megawatts installed wind energy milestone in India. Suzlon's 10,000 MW of wind installation is capable of powering over 5 million households per annum and offsets approximately 21.5 million tonnes of carbon dioxide (CO) emission annually which is equivalent to planting over 1500 million trees.\n\nGlobally, Suzlon has installed over 17000 MW of wind power capacity in 18 countries.\n\nSuzlon crossed 11,000 megawatts of cumulative installations in India. \nSuzlon has cumulatively added over 11000 megawatts of wind power capacity for over 1,700 customers in India across 40 sites in eight States. \nSuzlon accounts for nearly one-third of the country's total wind installations.\n\nIts notable installations in India include:\n\nIn 2012, Suzlon signed an Expression of Interest with the government of Karnataka to develop 2500 MW of wind power in the state between 2012 and 2017.\n\n"}
{"id": "13095366", "url": "https://en.wikipedia.org/wiki?curid=13095366", "title": "True mass", "text": "True mass\n\nThe term true mass is synonymous with the term mass, but is used in astronomy to differentiate the measured mass of a planet from the lower limit of mass usually obtained from radial velocity techniques. Methods used to determine the true mass of a planet include measuring the distance and period of one of its satellites, advanced astrometry techniques that use the motions of other planets in the same star system, combining radial velocity techniques with transit observations (which indicate very low orbital inclinations), and combining radial velocity techniques with stellar parallax measurements (which also determine orbital inclinations).\n\n"}
