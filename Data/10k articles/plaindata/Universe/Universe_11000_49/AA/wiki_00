{"id": "13728020", "url": "https://en.wikipedia.org/wiki?curid=13728020", "title": "5083 aluminium alloy", "text": "5083 aluminium alloy\n\n5083 aluminium alloy is an aluminium alloy with magnesium and traces of manganese and chromium. It is highly resistant to attack by seawater and industrial chemicals.\n\nAlloy 5083 retains exceptional strength after welding. It has the highest strength of the non-heat treatable alloys, but is not recommended for use in temperatures in excess of 65°C.\n\nFor AA 5083 alloy, the stages of porous structure development are substantially identical with that of pure aluminum, although an increase in oxide growth rate and high conductance of the oxide film were observed.\n\nAlloy 5083 is commonly used in:\n\nUses requiring a weldable moderate-strength alloy having good corrosion resistance are commonly utilizing alloy 5083.\n\n"}
{"id": "50660598", "url": "https://en.wikipedia.org/wiki?curid=50660598", "title": "750 GeV diphoton excess", "text": "750 GeV diphoton excess\n\nThe 750 GeV diphoton excess in particle physics was an anomaly in data collected at the Large Hadron Collider (LHC) in 2015, which could have been an indication of a new particle or resonance. The anomaly was absent in data collected in 2016, suggesting that the diphoton excess was a statistical fluctuation. In the interval between the December 2015 and August 2016 results, the anomaly generated considerable interest in the scientific community, including about 500 theoretical studies. The hypothetical particle was denoted by the Greek letter Ϝ (pronounced digamma) in the scientific literature, owing to the decay channel in which the anomaly occurred. The data, however, were always less than five standard deviations (sigma) different from that expected if there was no new particle, and, as such, the anomaly never reached the accepted level of statistical significance required to announce a discovery in particle physics. After the August 2016 results, interest in the anomaly sank as it was considered a statistical fluctuation.\nIndeed, a Bayesian analysis of the anomaly found that whilst data collected in 2015 constituted “substantial” evidence for the digamma on the Jeffreys’ scale, data collected in 2016 combined with that collected in 2015 was evidence against the digamma.\n\nOn December 15, 2015, the ATLAS and CMS collaborations at CERN presented results from the second operational run of the Large Hadron Collider (LHC) at the center of mass energy of 13 TeV, the highest ever achieved in proton-proton collisions. Among the results, the invariant mass distribution of pairs of high-energy photons produced in the collisions showed an excess of events compared to the Standard Model prediction at around 750 GeV/\"c\". The statistical significance of the deviation was reported to be 3.9 and 3.4 standard deviations (locally) respectively for each experiment.\n\nThe excess could have been explained by the production of a new particle (the digamma) with a mass of about 750 GeV/\"c\" that decayed into two photons. The cross-section at 13 TeV centre of mass energy required to explain the excess, multiplied by the branching fraction into two photons, was estimated to be\n\nThis result, while unexpected, was compatible with previous experiments, and in particular with the LHC measurements at a lower centre of mass energy of 8 TeV.\n\nAnalysis of a larger sample of data, collected by ATLAS and CMS in the first half 2016, did not confirm the existence of the Ϝ particle, which indicates that the excess seen in 2015 was a statistical fluctuation.\n\nThe non-observation of the 750 GeV bump in follow-up searches by the ATLAS and CMS experiments had a significant impact on the particle physics community. Despite the initial significance being lower than the discovery threshold of five sigma, many physicists treated the initial excess as tantamount to a discovery, as evidenced by the extreme interest particularly by the theory community, leading to the authorship of over 500 articles. The event highlighted the desire in the community for the LHC to discover a fundamentally new particle, and the difficulties in searching for a signal which is unknown a priori.\n\n"}
{"id": "36594129", "url": "https://en.wikipedia.org/wiki?curid=36594129", "title": "Agha Waqar's water-fuelled car", "text": "Agha Waqar's water-fuelled car\n\nThe water-kit is a technical design of an alleged \"perpetual motion machine\" created by Agha Waqar Ahmed, an inventor based in Khairpur, Sindh, Pakistan. In July 2012, Agha Waqar Ahmed's Water-kit was publicly announced as an apparatus that allows water to be used as a source of fuel in cars. The apparatus consists of a \"water-kit\" that can break water into hydrogen and oxygen molecules through the process of electrolysis of water. Ahmed claims that any car using his kit can run exclusively on water and will yield of travel on a liter of water.\n\nHe has also claimed that his technology can power the entire country with water. His claims have been met with scepticism as well as enthusiasm from Pakistani scientists. Critics have pointed out that in order for the invention to work, it would have to violate the second law of thermodynamics.\n\nAhmed calls his invention \"water-kit\". The kit consists of a cylindrical jar (which holds the water), electrodes, wires to be attached with car's battery and a hose leading to the engine. The kit claims to use electrolysis to convert water into HHO gas, which is then used as fuel in an otherwise unmodified gasoline engine. The kit states that it requires distilled water. Ahmed claims that he has been able to achieve much higher amounts of HHO compared to any other inventor because of his \"undisclosed calculations\". He has applied for a patent from the Government of Pakistan and is waiting for their feedback.\n\nAhmed claims to have done several demonstrations for Pakistani government officials. According to a television interview he has had three meetings with Government's Science and Technology sector as well as a meeting with Pakistan's Atomic energy commission.\nUpon hearing about his invention, the prime minister of Pakistan, Raja Pervaiz Ashraf, formed a committee consisting of three people. The committee members were Khursheed Shah, the minister of religious affairs, Mir Changez Khan Jamali, the federal minister for Science and Technology, and Dr. Asim Hussain, the advisor to the Prime Minister on Petroleum and Natural Resources. Out of the three members, Khursheed Shah has publicly supported Ahmed's invention.\n\nAfter much publicity and media coverage, many government officials came forward to offer support for Ahmed's invention. Along with the aforementioned federal minister for Religious Affairs, two other federal ministers endorsed his work. Sindh Minister for Law and Prisons, Mohammad Ayaz Soomro, called Ahmed a national hero while speaking to a media group. Three other government officials congratulated him, stating that he \"brought a revolution in the world of Science\". However it should be noted that Ahmed has been criticised for not clearly explaining how his invention could work without violating the laws of thermodynamics. Ahmed was invited by Jamil Ahmed Khan, the Pakistani ambassador to UAE, to demonstrate his technology in Abu Dhabi. Abdul Qadeer Khan, a nuclear scientist, has supported the idea of a water-fuelled car; however he has been criticised for his position.\nSamar Mubarakmand was supportive of this idea and seemed enthusiastic at first, but when Agha Waqar was called by Pakistan Television Corporation to take the demo of this Water Kit, he failed to give a demonstration in the live program in his car, claiming he had not fit the Water kit in his car for the TV program. Mubarakmand then told him that in his opinion, calcium carbide is added in water to produce the fuel for the engine. In that case, the calcium carbide is the energy source, not the water.\n\nAtta ur Rahman, a scientist and a former minister of Science and Technology, called for a technical investigation of Ahmed's car by top engineering universities when he appeared on Geo TV's \"Capital Talk\". He told the host, Hamid Mir, that such inventions don't work and if they were to work, they would violate the law of conservation of energy. Ahmed replied to this criticism by stating that he had already made long drives with the car, done demonstrations for politicians and media personalities. He even challenged Rahman to check and verify his water fueled car himself. By the end of the show, everyone agreed that the car should be tested by independent sources. Rahman showed similar concerns in another show by Talat Hussain Syed, \"News Night with Talat\", for Dawn News.\nWhile speaking to Express Tribune, Rahman stated he suspected this invention to be a scam. He repeated that such an invention would have to violate the laws of thermodynamics to work.\n\nOn 11 August 2012, Rahman reported that the vehicle was to be fully inspected by engineers at the National University of Science & Technology, but Ahmed never showed up for the appointment. This caused Rehman to further accuse Ahmed of fraud, stating \"Agha Waqar Ahmad did a disappearing act and never turned up for the appointment. The fraud had been exposed and escape was the only route available to him\".\n\nPervez Hoodbhoy, a nuclear physicist, wrote in The Express Tribune that Ahmed's coverage in the media, along with endorsement by government authorities and scientists, has exposed the ignorance and self-delusion of such entities. He criticized politicians and media for hailing Ahmed as a national hero and for promoting pseudo-scientific theories related to water-fueled cars. He insisted any technology should comply with basic principles of Science before getting any government funding.\nIt is worth mentioning that before Agha Waqar Ahmed, another Pakistani known as Doctor Ghulam Sarwar had claimed in December 2011 that he had invented a car that only runs on water.\n\nSeveral other skeptics have also weighed in on Ahmed's invention, mainly due to its violation of second law of thermodynamics. On 6 August 2012, a physics teacher, Ranomal Malani, challenged Ahmed to demonstrate his kit publicly. If he succeeds, Malani has promised a pay out of five million rupees. Ahmed accepted the challenge.\n\nTalat Hussain, who covered Ahmed's invention on his show, did a follow up show on 3 August 2012 to address the skeptics. His guests consisted of a panel of scientists, a Ph.D student and a reporter for KTN News. Ahmed was notable by his absence. The scientists reiterated the problem of water-kit's incompatibility with the second law of thermodynamics. One of the guests, Javed Ahmed Chatta, said he was present at one of the demonstrations and showed his skepticism; however, his concerns were ignored by the authorities. Another guest, Adeel Ghayur, stated that even if the apparent violation of natural laws was ignored the cost to operate such a car would be impractical.\n\n"}
{"id": "32076138", "url": "https://en.wikipedia.org/wiki?curid=32076138", "title": "Amman East Power Plant", "text": "Amman East Power Plant\n\nAmman East Power Plant is a combined cycle gas-fired power plant in Al Manakher, Jordan. It was the first independent power plant in Jordan.\n\nThe plant is operated by AES Jordan PSC, a subsidiary of AES Corporation and Mitsui & Co. Construction started in 2007 and the plant was commissioned in 2009. The plant has an installed capacity of 380 MW and it cost US$300 million.\n\nIn 2011, it was announced that Qatar Electricity & Water Corporation will buy a stake in the power plant.\n"}
{"id": "57119018", "url": "https://en.wikipedia.org/wiki?curid=57119018", "title": "Barcarena water contamination", "text": "Barcarena water contamination\n\nThe Barcarena water contamination by toxic metal waste was detected in the rivers near Barcarena, Pará, Brazil, on February 16, 2018 by Brazilian authorities. Preliminary investigations indicated that the cause was the leakage of toxic waste from a tailing dam due to excessive rain. The tailing dam is part of the Alunorte aluminium refinery, owned by the Norwegian Norsk Hydro. Additionally, the company's CEO acknowledged improper discharge of rain and surface water into the local rivers. \n"}
{"id": "750807", "url": "https://en.wikipedia.org/wiki?curid=750807", "title": "Brio (company)", "text": "Brio (company)\n\nBRIO is a wooden toy company founded in Sweden. The company was founded in the small town of Boalt, Scania in 1884 by basket maker Ivar Bengtsson. For a long time the company was based in Osby, Scania, in southern Sweden. In 1908 Ivar's three sons took over and founded BRIO, which is an acronym for BRöderna (\"the brothers\") Ivarsson [at] Osby. In 2006 the company moved its headquarters to Malmö.\n\nIn 1984, the company started the BRIO Lekoseum (from Swedish \"leka\", to play), a toy museum featuring the company's products and those of other companies (such as Barbie dolls and Märklin model railways), at the headquarters in Osby. Children can play with many of the toys. Since the late summer of 2014 the museum has been run as an independent foundation, hence the official name is now only Lekoseum.\n\nBRIO is best known for their wooden toy trains, sold in Europe since 1958. Most are non-motorized and suitable for younger children. The cars connect with magnets and are easy to manipulate; in recent years, the range has been extended with battery powered, remote control, and 'intelligent track'-driven engines. BRIO licenses \"Thomas the Tank Engine\" wooden trains in some parts of Europe, but Mattel holds the \"Thomas the Tank Engine\" license in the United States. Many competitors, such as Whittle Shortline, make products that are compatible with BRIO.\n\nBRIO also sells BRIO-Mech construction kits. Long, thin wooden slats with evenly spaced holes are connected together with various fasteners made of colorful plastic. Young children can build sturdy and elaborate constructions.\n\nBRIO also sold \"Theodore Tugboat\" toys. Released were the tugboats, the Dispatcher, Benjamin Bridge, Chester the Container Ship, and Barrington and Bonavista barges. These toys were discontinued in 2000, a year before the show was cancelled.\n\nDuring the 1960s BRIO manufactured dollhouses and dollhouse furniture. Some of this furniture is highly sought after by collectors as it comprises miniature replicas of items by Danish designer Arne Jacobsen. Pieces include the 'egg' chair, the 'series 7' chair, and the 'swan' couch.\n\nAdditionally, the Alga subsidiary was one of five companies who at one time produced the physical skill game \"Crossbows and Catapults\".\n\nPlantoys of Thailand had a joint venture with BRIO in 2001-2002, being present in the Brio catalogs of the time. \nIn 2004 the Swedish investment company Proventus became the major shareholder of Brio with more than 40% of the votes. In the same year BRIO of Sweden moved most of their production to three factories in Guangdong Province, China.\n\nThe \"Financial Statement January - December 2008\" presented on February 17, 2009 speaks of financial problems concerning the company's liquidity. On the 11th of March 2009 the company stated \"BRIO financially reconstructed – shareholders’ equity strengthened by more than SEK 300 million\".\n\nBRIO was acquired by the Ravensburger Group on January 8, 2015.\n\n"}
{"id": "6460996", "url": "https://en.wikipedia.org/wiki?curid=6460996", "title": "Dark earth", "text": "Dark earth\n\nDark earth in archaeology is an archaeological horizon, as much as thick, indicating settlement over long periods of time. The material is high in organic matter, including charcoal, which gives it its characteristic dark colour; it may also contain fragments of pottery, tile, animal bone and other artefacts. It is interpreted as soil enriched with the sooty remains of thatched roofs from houses without chimneys, with other waste materials. In some areas it appears to give the soil added fertility.\nLondon's dark earth was originally called 'black earth' by archaeologists. It was renamed dark earth because of confusion with the \"chernozem\" (black earth soils in Russia; in these, the dark colour is traditionally (not universally) thought to come from humus, rather than soot).\n\nIn the Hebrides, it was customary to remove the thatch from the \"black houses\" every spring, and spread it on the fields as fertilizer, improved by the soot which it retained. On Achill Island, special smoke huts were built in the fields, stone structures with sod roofs. From October to May smoky fires burned inside them, and in spring the sods were spread on the fields. In the Amazon basin, there are very extensive areas of dark soil, known as terra preta, enriched by small particles of soot, and these areas are much more fertile than the natural soil.\n\nIn England, dark earth covers many areas that were built up in the Roman period, especially Londinium. In some cases, it may represent open spaces on the edge of urban centres, but can also be found in more rural settings in and around foci of settlement. In the example of London, deposits underlying the ancient city's dark earth are often dated to between the 2nd to 5th century, the middle and later Roman period. Overlying deposits are frequently dated to the 9th century when Saxon London was repopulated and began to expand. The dark earth shows little evidence of any depositional structure or 'horizons', although tip lines are sometimes recorded.\n\nArchaeologists have debated what London's dark earth layer may indicate about human use of the city. It has been taken as evidence of refuse disposal or gardening \"during\" the Roman period on the site of previous buildings. In this case it might be evidence of a decline of Londinium's population, or of its partial displacement outside the city walls. However, late Roman cemeteries around London do not show a population decline compared with earlier London.\n\nAlternatively, dark earth might have formed only \"after\" abandonment at the end of the Roman period. In this interpretation, dark earth would consist of urban deposits of smoke-impregnated thatch, decayed weeds, timber, and earth floors, reworked and homogenized by worm action, or by agricultural (ploughing), which mixed building materials from the abandoned Roman cities with material deposited later.\n\nIn Sweden, dark earth covering has been found in Uppåkra (in southernmost Sweden, former Denmark), where city-like settlement existed from about the year 1 until 1000 C.E. when the settlement shifted to modern day Lund. Dark earth over has been found in the Viking city of Björkö (today called Birka), in central Sweden, close to modern Stockholm. Dark earth has also been found in Köpingsvik, on the island of Öland close to the southern Sweden east coast.\n\n\n"}
{"id": "23637699", "url": "https://en.wikipedia.org/wiki?curid=23637699", "title": "Energiapolar", "text": "Energiapolar\n\nEnergiapolar Oy is one of the largest electricity sales companies in Finland. The company's main market area is Northern Finland. Energiapolar has 80 000 customers and annual revenue of EUR 60 million. Electricity sales volume was over 900 000 MWh in 2013. Energiapolar solely concentrates on electricity sales and trading while working in close co-operation with the local network companies.\n\nShareholders of Energiapolar are (ownership in %): Rovakaira Oy (34,21 %), Rovaniemen Energia Oy (24,34 %), Torniolaakson Sähkö Oy (17,81 %), Koillis-Lapin Sähkö Oy (15,79 %), Muonion Sähköosuuskunta (3,29 %), Enontekiön Sähkö Oy (2,63 %) and Pellon Sähkö Oy (1.93%).\n"}
{"id": "31044759", "url": "https://en.wikipedia.org/wiki?curid=31044759", "title": "Essential Energy", "text": "Essential Energy\n\nEssential Energy is a state-owned electricity infrastructure company which owns, maintains and operates the electrical distribution networks for much of New South Wales, covering 95 percent of the state geography. It also owns the reticulated water network in Broken Hill through Essential Water, formerly Australian Inland Energy and Water.\n\nEssential Energy was formed from the previously state-owned energy business, Country Energy, when the retail division of the company, along with the Country Energy brand, was sold by the NSW Government in 2011 to Origin Energy.\n\n\n"}
{"id": "584911", "url": "https://en.wikipedia.org/wiki?curid=584911", "title": "External ballistics", "text": "External ballistics\n\nExternal ballistics or exterior ballistics is the part of ballistics that deals with the behavior of a projectile in flight. The projectile may be powered or un-powered, guided or unguided, spin or fin stabilized, flying through an atmosphere or in the vacuum of space, but most certainly flying under the influence of a gravitational field.\n\nGun-launched projectiles may be unpowered, deriving all their velocity from the propellant's ignition until the projectile exits the gun barrel. However, exterior ballistics analysis also deals with the trajectories of rocket-assisted gun-launched projectiles and gun-launched rockets; and rockets that acquire all their trajectory velocity from the interior ballistics of their on-board propulsion system, either a rocket motor or air-breathing engine, both during their boost phase and after motor burnout. External ballistics is also concerned with the free-flight of other projectiles, such as balls, arrows etc.\n\nWhen in flight, the main or major forces acting on the projectile are gravity, drag, and if present, wind; if in powered flight, thrust; and if guided, the forces imparted by the control surfaces.\n\nIn small arms external ballistics applications, gravity imparts a downward acceleration on the projectile, causing it to drop from the line of sight. Drag, or the air resistance, decelerates the projectile with a force proportional to the square of the velocity. Wind makes the projectile deviate from its trajectory. During flight, gravity, drag, and wind have a major impact on the path of the projectile, and must be accounted for when predicting how the projectile will travel.\n\nFor medium to longer ranges and flight times, besides gravity, air resistance and wind, several intermediate or meso variables described in the external factors paragraph have to be taken into account for small arms. Meso variables can become significant for firearms users that have to deal with angled shot scenarios or extended ranges, but are seldom relevant at common hunting and target shooting distances.\n\nFor long to very long small arms target ranges and flight times, minor effects and forces such as the ones described in the long range factors paragraph become important and have to be taken into account. The practical effects of these minor variables are generally irrelevant for most firearms users, since normal group scatter at short and medium ranges prevails over the influence these effects exert on projectile trajectories.\n\nAt extremely long ranges, artillery must fire projectiles along trajectories that are not even approximately straight; they are closer to parabolic, although air resistance affects this. Extreme long range projectiles are subject to significant deflections, depending on circumstances, from the line toward the target; and all external factors and long range factors must be taken into account when aiming. In super gun artillery cases, like the Paris gun, very subtle relativistic effects that are not covered in this article can further refine aiming solutions.\n\nIn the case of ballistic missiles, the altitudes involved have a significant effect as well, with part of the flight taking place in a near-vacuum well above a rotating earth, steadily moving the target from where it was at launch time.\n\nTwo methods can be employed to stabilize non-spherical projectiles during flight:\n\nThe effect of gravity on a projectile in flight is often referred to as projectile drop or bullet drop. It is important to understand the effect of gravity when zeroing the sighting components of a gun. To plan for projectile drop and compensate properly, one must understand parabolic shaped trajectories.\n\nIn order for a projectile to impact any distant target, the barrel must be inclined to a positive elevation angle relative to the target. This is due to the fact that the projectile will begin to respond to the effects of gravity the instant it is free from the mechanical constraints of the bore. The imaginary line down the center axis of the bore and out to infinity is called the line of departure and is the line on which the projectile leaves the barrel. Due to the effects of gravity a projectile can never impact a target higher than the line of departure. When a positively inclined projectile travels downrange, it arcs below the line of departure as it is being deflected off its initial path by gravity. Projectile/Bullet drop is defined as the vertical distance of the projectile below the line of departure from the bore. Even when the line of departure is tilted upward or downward, projectile drop is still defined as the distance between the bullet and the line of departure at any point along the trajectory. Projectile drop does not describe the actual trajectory of the projectile.\nKnowledge of projectile drop however is useful when conducting a direct comparison of two different projectiles regarding the shape of their trajectories, comparing the effects of variables such as velocity and drag behavior.\n\nFor hitting a distant target an appropriate positive elevation angle is required that is achieved by angling the line of sight from the shooter's eye through the centerline of the sighting system downward toward the line of departure. This can be accomplished by simply adjusting the sights down mechanically, or by securing the entire sighting system to a sloped mounting having a known downward slope, or by a combination of both. This procedure has the effect of elevating the muzzle when the barrel must be subsequently raised to align the sights with the target. A projectile leaving a muzzle at a given elevation angle follows a ballistic trajectory whose characteristics are dependent upon various factors such as muzzle velocity, gravity, and aerodynamic drag. This ballistic trajectory is referred to as the bullet path. If the projectile is spin stabilized, aerodynamic forces will also predictably arc the trajectory slightly to the right, if the rifling employs \"right-hand twist.\" Some barrels are cut with left-hand twist, and the bullet will arc to the left, as a result. Therefore, to compensate for this path deviation, the sights also have to be adjusted left or right, respectively. A constant wind also predictably affects the bullet path, pushing it slightly left or right, and a little bit more up and down, depending on the wind direction. The magnitude of these deviations are also affected by whether the bullet is on the upward or downward slope of the trajectory, due to a phenomenon called \"yaw of repose,\" where a spinning bullet tends to steadily and predictably align slightly off center from its point mass trajectory. Nevertheless, each of these trajectory perturbations are predictable once the projectile aerodynamic coefficients are established, through a combination of detailed analytical modeling and test range measurements.\n\nProjectile/bullet path analysis is of great use to shooters because it allows them to establish ballistic tables that will predict how much vertical elevation and horizontal deflection corrections must be applied to the sight line for shots at various known distances. The most detailed ballistic tables are developed for long range artillery and are based on six-degree-of-freedom trajectory analysis, which accounts for aerodynamic behavior along the three axial directions—elevation, range, and deflection—and the three rotational directions—pitch, yaw, and spin. For small arms applications, trajectory modeling can often be simplified to calculations involving only four of these degrees-of-freedom, lumping the effects of pitch, yaw and spin into the effect of a yaw-of-repose to account for trajectory deflection. Once detailed range tables are established, shooters can relatively quickly adjust sights based on the range to target, wind, air temperature and humidity, and other geometric considerations, such as terrain elevation differences.\n\nProjectile path values are determined by both the sight height, or the distance of the line of sight above the bore centerline, and the range at which the sights are zeroed, which in turn determines the elevation angle. A projectile following a ballistic trajectory has both forward and vertical motion. Forward motion is slowed due to air resistance, and in point mass modeling the vertical motion is dependent on a combination of the elevation angle and gravity. Initially, the projectile is rising with respect to the line of sight or the horizontal sighting plane. The projectile eventually reaches its apex (highest point in the trajectory parabola) where the vertical speed component decays to zero under the effect of gravity, and then begins to descend, eventually impacting the earth. The farther the distance to the intended target, the greater the elevation angle and the higher the apex.\n\nThe projectile path crosses the horizontal sighting plane two times. The point closest to the gun occurs while the bullet is climbing through the line of sight and is called the near zero. The second point occurs as the projectile is descending through the line of sight. It is called the far zero and defines the current sight in distance for the gun. Projectile path is described numerically as distances above or below the horizontal sighting plane at various points along the trajectory. This is in contrast to projectile drop which is referenced to the plane containing the line of departure regardless of the elevation angle. Since each of these two parameters uses a different reference datum, significant confusion can result because even though a projectile is tracking well below the line of departure it can still be gaining actual and significant height with respect to the line of sight as well as the surface of the earth in the case of a horizontal or near horizontal shot taken over flat terrain.\n\nKnowledge of the projectile drop and path has some practical uses to shooters even if it does not describe the actual trajectory of the projectile. For example, if the vertical projectile position over a certain range reach is within the vertical height of the target area the shooter wants to hit, the point of aim does not necessarily need to be adjusted over that range; the projectile is considered to have a sufficiently flat point-blank range trajectory for that particular target. Also known as \"battle zero\", maximum point-blank range is also of importance to the military. Soldiers are instructed to fire at any target within this range by simply placing their weapon's sights on the center of mass of the enemy target. Any errors in range estimation are tactically irrelevant, as a well-aimed shot will hit the torso of the enemy soldier. The current trend for elevated sights and higher-velocity cartridges in assault rifles is in part due to a desire to extend the maximum point-blank range, which makes the rifle easier to use.\n\nMathematical models, such as computational fluid dynamics, are used for calculating the effects of drag or air resistance; they are quite complex and not yet completely reliable, but research is ongoing. The most reliable method, therefore, of establishing the necessary projectile aerodynamic properties to properly describe flight trajectories is by empirical measurement.\n\nUse of ballistics tables or ballistics software based on the Mayevski/Siacci method and G1 drag model, introduced in 1881, are the most common method used to work with external ballistics. Projectiles are described by a ballistic coefficient, or BC, which combines the air resistance of the bullet shape (the drag coefficient) and its sectional density (a function of mass and bullet diameter).\n\nThe deceleration due to drag that a projectile with mass \"m\", velocity \"v\", and diameter \"d\" will experience is proportional to 1/BC, 1/\"m\", \"v²\" and \"d²\". The BC gives the ratio of ballistic efficiency compared to the standard G1 projectile, which is a fictitious projectile with a flat base, a length of 3.28 calibers/diameters, and a 2 calibers/diameters radius tangential curve for the point.\nThe G1 standard projectile originates from the \"C\" standard reference projectile defined by the German steel, ammunition and armaments manufacturer Krupp in 1881. The G1 model standard projectile has a BC of 1. The French Gâvre Commission decided to use this projectile as their first reference projectile, giving the G1 name.\n\nSporting bullets, with a calibre \"d\" ranging from 0.177 to 0.50 inches (4.50 to 12.7 mm), have G1 BC’s in the range 0.12 to slightly over 1.00, with 1.00 being the most aerodynamic, and 0.12 being the least. Very-low-drag bullets with BC's ≥ 1.10 can be designed and produced on CNC precision lathes out of mono-metal rods, but they often have to be fired from custom made full bore rifles with special barrels.\n\nSectional density is a very important aspect of a projectile or bullet, and is for a round projectile like a bullet the ratio of frontal surface area (half the bullet diameter squared, times pi) to bullet mass. Since, for a given bullet shape, frontal surface increases as the square of the calibre, and mass increases as the cube of the diameter, then sectional density grows linearly with bore diameter. Since BC combines shape and sectional density, a half scale model of the G1 projectile will have a BC of 0.5, and a quarter scale model will have a BC of 0.25.\n\nSince different projectile shapes will respond differently to changes in velocity (particularly between supersonic and subsonic velocities), a BC provided by a bullet manufacturer will be an average BC that represents the common range of velocities for that bullet. For rifle bullets, this will probably be a supersonic velocity, for pistol bullets it will probably be subsonic. For projectiles that travel through the supersonic, transonic and subsonic flight regimes BC is not well approximated by a single constant, but is considered to be a function \"BC(M)\" of the Mach number M; here M equals the projectile velocity divided by the speed of sound. During the flight of the projectile the M will decrease, and therefore (in most cases) the BC will also decrease.\n\nMost ballistic tables or software takes for granted that one specific drag function correctly describes the drag and hence the flight characteristics of a bullet related to its ballistics coefficient. Those models do not differentiate between wadcutter, flat-based, spitzer, boat-tail, very-low-drag, etc. bullet types or shapes. They assume one invariable drag function as indicated by the published BC.\n\nSeveral drag curve models optimized for several standard projectile shapes are however available. The resulting fixed drag curve models for several standard projectile shapes or types are referred to as the:\n\n\nHow different speed regimes affect .338 calibre rifle bullets can be seen in the .338 Lapua Magnum product brochure which states Doppler radar established G1 BC data. The reason for publishing data like in this brochure is that the Siacci/Mayevski G1 model can not be tuned for the drag behavior of a specific projectile whose shape significantly deviates from the used reference projectile shape. Some ballistic software designers, who based their programs on the Siacci/Mayevski G1 model, give the user the possibility to enter several different G1 BC constants for different speed regimes to calculate ballistic predictions that closer match a bullets flight behavior at longer ranges compared to calculations that use only one BC constant.\n\nThe above example illustrates the central problem fixed drag curve models have. These models will only yield satisfactory accurate predictions as long as the projectile of interest has the same shape as the reference projectile or a shape that closely resembles the reference projectile. Any deviation from the reference projectile shape will result in less accurate predictions. How much a projectile deviates from the applied reference projectile is mathematically expressed by the form factor (\"i\"). The form factor can be used to compare the drag experienced by a projectile of interest to the drag experienced by the employed reference projectile at a given velocity (range). The problem that the actual drag curve of a projectile can significantly deviate from the fixed drag curve of any employed reference projectile systematically limits the traditional drag resistance modeling approach. The relative simplicity however makes that it can be explained to and understood by the general shooting public and hence is also popular amongst ballistic software prediction developers and bullet manufacturers that want to market their products.\n\nAnother attempt at building a ballistic calculator is the model presented in 1980 by Dr. Arthur J. Pejsa. Mr. Pejsa claims on his website that his method was consistently capable of predicting (supersonic) rifle bullet trajectories within 2.5 mm (0.1 in) and bullet velocities within 0.3 m/s (1 ft/s) out to 914 m (1,000 yd) in theory. The Pejsa model is a closed-form solution.\n\nThe Pejsa model can predict a projectile within a given flight regime (for example the supersonic flight regime) with only two velocity measurements, a distance between said velocity measurements, and a slope or deceleration constant factor. The model allows the drag curve to change slopes(true/calibrate) or curvature at three different points. Down range velocity measurement data can be provided around key inflection points allowing for more accurate calculations of the projectile retardation rate, very similar to a Mach vs CD table. The Pejsa model allows the slope factor to be tuned to account for subtle differences in the retardation rate of different bullet shapes and sizes. It ranges from 0.1 (flat-nose bullets) to 0.9 (very-low-drag bullets). If this slope or deceleration constant factor is unknown a default value of 0.5 is used. With the help of test firing measurements the slope constant for a particular bullet/rifle system/shooter combination can be determined. These test firings should preferably be executed at 60% and for extreme long range ballistic predictions also at 80% to 90% of the supersonic range of the projectiles of interest, staying away from erratic transonic effects. With this the Pejsa model can easily be tuned. A practical downside of the Pejsa model is that accurate projectile specific down range velocity measurements to provide these better predictions can not be easily performed by the vast majority of shooting enthusiasts.\n\nAn average retardation coefficient can be calculated for any given slope constant factor if velocity data points are known and distance between said velocity measurements is known. Obviously this is true only within the same flight regime. With velocity actual speed is meant, as velocity is a vector quantity and speed is the magnitude of the velocity vector. Because the power function does not have constant curvature a simple chord average cannot be used. The Pejsa model uses a weighted average retardation coefficient weighted at 0.25 range.The closer velocity is more heavily weighted. The retardation coefficient is measured in feet whereas range is measured in yards hence 0.25 * 3.0 = 0.75, in some places 0.8 rather than 0.75 is used. The 0.8 comes from rounding in order to allow easy entry on hand calculators. Since the Pejsa model does not use a simple chord weighted average, two velocity measurements are used to find the chord average retardation coefficient at midrange between the two velocity measurements points, limiting it to short range accuracy. In order to find the starting retardation coefficient Dr. Pejsa provides two separate equations in his two books. The first involves the power function. The second equation is identical to the one used to find the weighted average at R / 4; add N * (R/2) where R is the range in feet to the chord average retardation coefficient at midrange and where N is the slope constant factor. After the starting retardation coefficient is found the opposite procedure is used in order find the weighted average at R / 4; the starting retardation coefficient minus N * (R/4). In other words, N is used as the slope of the chord line. Dr. Pejsa states that he expanded his drop formula in a power series in order to prove that the weighted average retardation coefficient at R / 4 was a good approximation. For this Dr. Pejsa compared the power series expansion of his drop formula to some other unnamed drop formula’s power expansion to reach his conclusions. The fourth term in both power series matched when the retardation coefficient at 0.25 range was used in Pejsa’s drop formula. The fourth term was also the first term to use N. The higher terms involving N where insignificant and disappeared at N = 0.36, which according to Dr. Pejsa was a lucky coincidence making for an exceedingly accurate linear approximation, especially for N’s around 0.36. If a retardation coefficient function is used exact average values for any N can be obtained because from calculus it is trivial to find the average of any integrable function. Dr. Pejsa states that the retardation coefficient can be modeled by C * V where C is a fitting coefficient which disappears during the derivation of the drop formula and N the slope constant factor.\n\nThe retardation coefficient equals the velocity squared divided by the retardation rate A. Using an average retardation coefficient allows the Pejsa model to be a closed-form expression within a given flight regime.\n\nIn order to allow the use of a G1 ballistic coefficient rather than velocity data Dr. Pejsa provided two reference drag curves. The first reference drag curve is based purely on the Siacci/Mayevski retardation rate function. The second reference drag curve is adjusted to equal the Siacci/Mayevski retardation rate function at a projectile velocity of 2600 fps (792.5 m/s) using a .30-06 Springfield Cartridge, Ball, Caliber .30 M2 rifle spitzer bullet with a slope or deceleration constant factor of 0.5 in the supersonic flight regime. In other flight regimes the second Pejsa reference drag curve model uses slope constant factors of 0.0 or -4.0. These deceleration constant factors can be verified by backing out Pejsa's formulas (the drag curve segments fits the form V / C and the retardation coefficient curve segments fits the form V / (V / C) = C * V where C is a fitting coefficient). The empirical test data Pejsa used to determine the exact shape of his chosen reference drag curve and pre-defined mathematical function that returns the retardation coefficient at a given Mach number was provided by the US military for the Cartridge, Ball, Caliber .30 M2 bullet. The calculation of the retardation coefficient function also involves air density, which Pejsa did not mention explicitly. The Siacci/Mayevski G1 model uses the following deceleration parametrization (60 °F, 30 inHg and 67% humidity, air density ρ = 1.2209 kg/m). Dr. Pejsa suggests using the second drag curve because the Siacci/Mayevski G1 drag curve does not provide a good fit for modern spitzer bullets. To obtain relevant retardation coefficients for optimal long range modeling Dr. Pejsa suggested using accurate projectile specific down range velocity measurement data for a particular projectile to empirically derive the average retardation coefficient rather than using a reference drag curve derived average retardation coefficient. Further he suggested using ammunition with reduced propellant loads to empirically test actual projectile flight behavior at lower velocities. When working with reduced propellant loads utmost care must be taken to avoid dangerous or catastrophic conditions (detonations) with can occur when firing experimental loads in firearms.\n\nAlthough not as well known as the Pejsa model, an additional alternative ballistic model was presented in 1989 by Colonel Duff Manges (U S Army Retired) at the American Defense Preparedness (ADPA) 11th International Ballistic Symposium held at the Brussels Congress Center, Brussels, Belgium, May 9–11, 1989. A paper titled \"Closed Form Trajectory Solutions for Direct Fire Weapons Systems\" appears in the proceedings, Volume 1, Propulsion Dynamics, Launch Dynamics, Flight Dynamics, pages 665-674. Originally conceived to model projectile drag for 120 mm tank gun ammunition, the novel drag coefficient formula has been applied subsequently to ballistic trajectories of center-fired rifle ammunition with results comparable to those claimed for the Pejsa model.\n\nThe Manges model uses a first principles theoretical approach that eschews \"G\" curves and \"ballistic coefficients\" based on the standard G1 and other similarity curves. The theoretical description has three main parts. The first is to develop and solve a formulation of the two dimensional differential equations of motion governing flat trajectories of point mass projectiles by defining mathematically a set of quadratures that permit closed form solutions for the trajectory differential equations of motion. A sequence of successive approximation drag coefficient functions is generated that converge rapidly to actual observed drag data. The vacuum trajectory, simplified aerodynamic, d'Antonio, and Euler drag law models are special cases. The Manges drag law thereby provides a unifying influence with respect to earlier models used to obtain two dimensional closed form solutions to the point-mass equations of motion. The third purpose of this paper is to describe a least squares fitting procedure for obtaining the new drag functions from observed experimental data. The author claims that results show excellent agreement with six degree of freedom numerical calculations for modern tank ammunition and available published firing tables for center-fired rifle ammunition having a wide variety of shapes and sizes.\n\nA Microsoft Excel application has been authored that uses least squares fits of wind tunnel acquired tabular drag coefficients. Alternatively, manufacturer supplied ballistic trajectory data, or Doppler acquired velocity data can be fitted as well to calibrate the model. The Excel application then employs custom macroinstructions to calculate the trajectory variables of interest. A modified 4th order Runge-Kutta integration algorithm is used. Like Pejsa, Colonel Manges claims center-fired rifle accuracies to the nearest one tenth of an inch for bullet position, and nearest foot per second for the projectile velocity.\n\nThe Proceedings of the 11th International Ballistic Symposium are available through the National Defense Industrial Association (NDIA) at the website http://www.ndia.org/Resources/Pages/Publication_Catalog.aspx.\n\nThere are also advanced professional ballistic models like PRODAS available. These are based on six degrees of freedom (6 DoF) calculations. 6 DoF modeling accounts for x, y, and z position in space along with the projectiles pitch, yaw, and roll rates. 6 DoF modeling needs such elaborate data input, knowledge of the employed projectiles and expensive data collection and verification methods that it is impractical for non-professional ballisticians, but not impossible for the curious, computer literate, and mathematically inclined. Semi-empirical aeroprediction models have been developed that reduced extensive test range data on a wide variety of projectile shapes, normalizing dimensional input geometries to calibers; accounting for nose length and radius, body length, and boattail size, and allowing the full set of 6-dof aerodynamic coefficients to be estimated. Early research on spin-stabilized aeroprediction software resulted in the SPINNER computer program. The FINNER aeroprediction code calculates 6-dof inputs for fin stabilized projectiles. Solids modeling software that determines the projectile parameters of mass, center of gravity, axial and transverse moments of inertia necessary for stability analysis are also readily available, and simple to computer program. Finally, algorithms for 6-dof numerical integration suitable to a 4th order Runge-Kutta are readily available. All that is required for the amateur ballistician to investigate the finer analytical details of projectile trajectories, along with bullet nutation and precession behavior, is computer programming determination. Nevertheless, for the small arms enthusiast, aside from academic curiosity, one will discover that being able to predict trajectories to 6-dof accuracy is probably not of practical significance compared to more simplified point mass trajectories based on published bullet ballistic coefficients. 6 DoF is generally used by the aerospace and defense industry and military organizations that study the ballistic behavior of a limited number of (intended) military issue projectiles. Calculated 6 DoF trends can be incorporated as correction tables in more conventional ballistic software applications.\n\nThough 6 DoF modeling and software applications are used by professional well equipped organizations for decades, the computing power restrictions of mobile computing devices like (ruggedized) personal digital assistants, tablet computers or smartphones impaired field use as calculations generally have to be done on the fly. In 2016 the Scandinavian ammunition manufacturer Nammo Lapua Oy released a 6 DoF calculation model based ballistic free software named Lapua Ballistics. The software is distributed as a mobile app only and available for Android and iOS devices. The employed 6 DoF model is however limited to Lapua bullets as a 6 DoF solver needs bullet specific drag coefficient (Cd)/Doppler radar data and geometric dimensions of the projectile(s) of interest. For other bullets the Lapua Ballistics solver is limited to and based on G1 or G7 ballistic coefficients and the Mayevski/Siacci method.\n\nMilitary organizations have developed ballistic models like the NATO Armament Ballistic Kernel (NABK) for fire-control systems for artillery like the SG2 Shareable (Fire Control) Software Suite (S4) from the NATO Army Armaments Group (NAAG).\n\nFor the precise establishment of drag or air resistance effects on projectiles, Doppler radar measurements are required. Weibel 1000e or Infinition BR-1001 Doppler radars are used by governments, professional ballisticians, defence forces and a few ammunition manufacturers to obtain real-world data of the flight behavior of projectiles of their interest. Correctly established state of the art Doppler radar measurements can determine the flight behavior of projectiles as small as airgun pellets in three-dimensional space to within a few millimetres accuracy. The gathered data regarding the projectile deceleration can be derived and expressed in several ways, such as ballistic coefficients (BC) or drag coefficients (C). Because a spinning projectile experiences both precession and nutation about its center of gravity as it flies, further data reduction of doppler radar measurements is required to separate yaw induced drag and lift coefficients from the zero yaw drag coefficient, in order to make measurements fully applicable to 6-dof trajectory analysis.\n\nDoppler radar measurement results for a lathe-turned monolithic solid .50 BMG very-low-drag bullet (Lost River J40 .510-773 grain monolithic solid bullet / twist rate 1:15 in) look like this:\n\nThe initial rise in the BC value is attributed to a projectile's always present yaw and precession out of the bore. The test results were obtained from many shots not just a single shot. The bullet was assigned 1.062 for its BC number by the bullet's manufacturer Lost River Ballistic Technologies.\n\nDoppler radar measurement results for a Lapua GB528 Scenar 19.44 g (300 gr) 8.59 mm (0.338 in) calibre very-low-drag bullet look like this:\n\nThis tested bullet experiences its maximum drag coefficient when entering the transonic flight regime around Mach 1.200.\n\nWith the help of Doppler radar measurements projectile specific drag models can be established that are most useful when shooting at extended ranges where the bullet speed slows to the transonic speed region near the speed of sound. This is where the projectile drag predicted by mathematic modeling can significantly depart from the actual drag experienced by the projectile. Further Doppler radar measurements are used to study subtle in-flight effects of various bullet constructions.\n\nGovernments, professional ballisticians, defence forces and ammunition manufacturers can supplement Doppler radar measurements with measurements gathered by telemetry probes fitted to larger projectiles.\n\nIn general, a pointed projectile will have a better drag coefficient (C) or ballistic coefficient (BC) than a round nosed bullet, and a round nosed bullet will have a better C or BC than a flat point bullet. Large radius curves, resulting in a shallower point angle, will produce lower drags, particularly at supersonic velocities. Hollow point bullets behave much like a flat point of the same point diameter. Projectiles designed for supersonic use often have a slightly tapered base at the rear, called a boat tail, which reduces air resistance in flight. Cannelures, which are recessed rings around the projectile used to crimp the projectile securely into the case, will cause an increase in drag.\n\nAnalytical software was developed by the Ballistics Research Laboratory - later called the Army Research Laboratory - which reduced actual test range data to parametric relationships for projectile drag coefficient prediction. Large caliber artillery also employ drag reduction mechanisms in addition to streamlining geometry. Rocket-assisted projectiles employ a small rocket motor that ignites upon muzzle exit providing additional thrust to overcome aerodynamic drag. Rocket assist is most effective with subsonic artillery projectiles. For supersonic long range artillery, where base drag dominates, base bleed is employed. Base bleed is a form of a gas generator that does not provide significant thrust, but rather fills the low-pressure area behind the projectile with gas, effectively reducing the base drag and the overall projectile drag coefficient.\n\nA projectile fired at supersonic muzzle velocity will at some point slow to approach the speed of sound. At the transonic region (about Mach 1.2–0.8) the centre of pressure (CP) of most non spherical projectiles shifts forward as the projectile decelerates. That CP shift affects the (dynamic) stability of the projectile. If the projectile is not well stabilized, it cannot remain pointing forward through the transonic region (the projectile starts to exhibit an unwanted precession or coning motion called limit cycle yaw that, if not damped out, can eventually end in uncontrollable tumbling along the length axis). However, even if the projectile has sufficient stability (static and dynamic) to be able to fly through the transonic region and stays pointing forward, it is still affected. The erratic and sudden CP shift and (temporary) decrease of dynamic stability can cause significant dispersion (and hence significant accuracy decay), even if the projectile's flight becomes well behaved again when it enters the subsonic region. This makes accurately predicting the ballistic behavior of projectiles in the transonic region very difficult. \n\nBecause of this, marksmen normally restrict themselves to engaging targets close enough that the projectile is still supersonic.\nIn 2015 the American ballistician Bryan Litz introduced the \"Extended Long Range\" concept to define rifle shooting at ranges where supersonic fired (rifle) bullets enter the transonic region. According to Litz, \"Extended Long Range starts whenever the bullet slows to its transonic range. As the bullet slows down to approach Mach 1, it starts to encounter transonic effects, which are more complex and difficult to account for, compared to the supersonic range where the bullet is relatively well-behaved.\"\n\nThe ambient air density has a significant effect on dynamic stability during transonic transition. Though the ambient air density is a variable environmental factor, adverse transonic transition effects can be negated better by a projectile traveling through less dense air, than when traveling through denser air. Projectile or bullet length also affects limit cycle yaw. Longer projectiles experience more limit cycle yaw than shorter projectiles of the same diameter. Another feature of projectile design that has been identified as having an effect on the unwanted limit cycle yaw motion is the chamfer at the base of the projectile. At the very base, or heel of a projectile or bullet, there is a chamfer, or radius. The presence of this radius causes the projectile to fly with greater limit cycle yaw angles. Rifling can also have a subtle effect on limit cycle yaw. In general faster spinning projectiles experience less limit cycle yaw.\n\nTo circumvent the transonic problems encountered by spin-stabilized projectiles, projectiles can theoretically be guided during flight. The Sandia National Laboratories announced in January 2012 it has researched and test-fired 4-inch (102 mm) long prototype dart-like, self-guided bullets for small-caliber, smooth-bore firearms that could hit laser-designated targets at distances of more than a mile (about 1,610 meters or 1760 yards). These projectiles are not spin stabilized and the flight path can be course adjusted with an electromagnetic actuator 30 times per second. The researchers also claim they have video of the bullet radically pitching as it exits the barrel and pitching less as it flies down range, a disputed phenomenon known to long-range firearms experts as “going to sleep”. Because the bullet’s motions settle the longer it is in flight, accuracy improves at longer ranges, Sandia researcher Red Jones said. “Nobody had ever seen that, but we’ve got high-speed video photography that shows that it’s true,” he said. Since Sandia is seeking a private company partner to complete testing of the prototype and bring a guided bullet to the marketplace, the future of this technology remains uncertain.\n\nDue to the practical inability to know in advance and compensate for all the variables of flight, no software simulation, however advanced, will yield predictions that will always perfectly match real world trajectories. It is however possible to obtain predictions that are very close to actual flight behavior.\n\nBallistic prediction computer programs intended for (extreme) long ranges can be evaluated by conducting field tests at the supersonic to subsonic transition range (the last 10 to 20% of the supersonic range of the rifle/cartridge/bullet combination). For a typical .338 Lapua Magnum rifle for example, shooting standard 16.2 gram (250 gr) Lapua Scenar GB488 bullets at 905 m/s (2969 ft/s) muzzle velocity, field testing of the software should be done at ≈ 1200–1300 meters (1312 - 1422 yd) under International Standard Atmosphere sea level conditions (air density ρ = 1.225 kg/m³). To check how well the software predicts the trajectory at shorter to medium range, field tests at 20, 40 and 60% of the supersonic range have to be conducted. At those shorter to medium ranges, transonic problems and hence unbehaved bullet flight should not occur, and the BC is less likely to be transient. Testing the predictive qualities of software at (extreme) long ranges is expensive because it consumes ammunition; the actual muzzle velocity of all shots fired must be measured to be able to make statistically dependable statements. Sample groups of less than 24 shots may not obtain the desired statistically significant confidence interval.\n\nGovernments, professional ballisticians, defence forces and a few ammunition manufacturers use Doppler radars and/or telemetry probes fitted to larger projectiles to obtain precise real world data regarding the flight behavior of the specific projectiles of their interest and thereupon compare the gathered real world data against the predictions calculated by ballistic computer programs. The normal shooting or aerodynamics enthusiast, however, has no access to such expensive professional measurement devices. Authorities and projectile manufacturers are generally reluctant to share the results of Doppler radar tests and the test derived drag coefficients (C) of projectiles with the general public.\n\nIn January 2009 the Scandinavian ammunition manufacturer Nammo/Lapua published Doppler radar test-derived drag coefficient data for most of their rifle projectiles.\nIn 2015 the US ammunition manufacturer Berger Bullets announced the use of Doppler radar in unison with PRODAS 6 DoF software to generate trajectory solutions.\nIn 2016 US ammunition manufacturer Hornady announced the use of Doppler radar derived drag data in software utilizing a modified point mass model to generate trajectory solutions.\nWith the measurement derived C data engineers can create algorithms that utilize both known mathematical ballistic models as well as test specific, tabular data in unison. When used by predictive software like QuickTARGET Unlimited, Lapua Edition, Lapua Ballistics or Hornady 4DOF the Doppler radar test-derived drag coefficient data can be used for more accurate external ballistic predictions.\n\nSome of the Lapua-provided drag coefficient data shows drastic increases in the measured drag around or below the Mach 1 flight velocity region. This behavior was observed for most of the measured small calibre bullets, and not so much for the larger calibre bullets. This implies some (mostly smaller calibre) rifle bullets exhibited more limit cycle yaw (coning and/or tumbling) in the transonic/subsonic flight velocity regime.\nThe information regarding unfavourable transonic/subsonic flight behavior for some of the tested projectiles is important. This is a limiting factor for extended range shooting use, because the effects of limit cycle yaw are not easily predictable and potentially catastrophic for the best ballistic prediction models and software.\n\nPresented C data can not be simply used for every gun-ammunition combination, since it was measured for the barrels, rotational (spin) velocities and ammunition lots the Lapua testers used during their test firings. Variables like differences in rifling (number of grooves, depth, width and other dimensional properties), twist rates and/or muzzle velocities impart different rotational (spin) velocities and rifling marks on projectiles. Changes in such variables and projectile production lot variations can yield different downrange interaction with the air the projectile passes through that can result in (minor) changes in flight behavior. This particular field of external ballistics is currently (2009) not elaborately studied nor well understood.\n\nThe method employed to model and predict external ballistic behavior can yield differing results with increasing range and time of flight. To illustrate this several external ballistic behavior prediction methods for the Lapua Scenar GB528 19.44 g (300 gr) 8.59 mm (0.338 in) calibre very-low-drag rifle bullet with a manufacturer stated G1 ballistic coefficient (BC) of 0.785 fired at 830 m/s (2723 ft/s) muzzle velocity under International Standard Atmosphere sea level conditions (air density ρ = 1.225 kg/m³), Mach 1 = 340.3 m/s, Mach 1.2 = 408.4 m/s), predicted this for the projectile velocity and time of flight from 0 to 3,000 m (0 to 3,281 yd):\n\nThe table shows the Doppler radar test derived drag coefficients (C) prediction method and the 2017 Lapua Ballistics 6 DoF App predictions produce similar results. The 6 DoF modeling estimates bullet stability ((S) and (S)) that gravitates to over-stabilization for ranges over for this bullet. At the total drop predictions deviate 47.5 cm (19.7 in) or 0.20 mil (0.68 moa) at 50° latitude and up to the total drop predictions are within 0.30 mil (1 moa) at 50° latitude. The 2016 Lapua Ballistics 6 DoF App version predictions were even closer to the Doppler radar test predictions.\n\nThe traditional Siacci/Mayevski G1 drag curve model prediction method generally yields more optimistic results compared to the modern Doppler radar test derived drag coefficients (C) prediction method. At range the differences will be hardly noticeable, but at and beyond the differences grow over 10 m/s (32.8 ft/s) projectile velocity and gradually become significant.\nAt range the projectile velocity predictions deviate 25 m/s (82.0 ft/s), which equates to a predicted total drop difference of 125.6 cm (49.4 in) or 0.83 mil (2.87 moa) at 50° latitude.\n\nThe Pejsa drag model closed-form solution prediction method, without slope constant factor fine tuning, yields very similar results in the supersonic flight regime compared to the Doppler radar test derived drag coefficients (C) prediction method. At range the projectile velocity predictions deviate 10 m/s (32.8 ft/s), which equates to a predicted total drop difference of 23.6 cm (9.3 in) or 0.16 mil (0.54 moa) at 50° latitude.\n\nThe G7 drag curve model prediction method (recommended by some manufacturers for very-low-drag shaped rifle bullets) when using a G7 ballistic coefficient (BC) of 0.377 yields very similar results in the supersonic flight regime compared to the Doppler radar test derived drag coefficients (C) prediction method. At range the projectile velocity predictions have their maximum deviation of 10 m/s (32.8 ft/s). The predicted total drop difference at is 0.4 cm (0.16 in) at 50° latitude. The predicted total drop difference at is 45.0 cm (17.7 in), which equates to 0.25 mil (0.86 moa).\n\nDecent prediction models are expected to yield similar results in the supersonic flight regime. The five example models down to all predict supersonic Mach 1.2 projectile velocities and total drop differences within a 51 cm (20.1 in) bandwidth. In the transonic flight regime at the models predict projectile velocities around Mach 1.0 to Mach 1.1 and total drop differences within a much larger 150 cm (59 in) bandwidth.\n\nWind has a range of effects, the first being the effect of making the projectile deviate to the side (horizontal deflection). From a scientific perspective, the \"wind pushing on the side of the projectile\" is not what causes horizontal wind drift. What causes wind drift is drag. Drag makes the projectile turn into the wind, much like a weather vane, keeping the centre of air pressure on its nose. This causes the nose to be cocked (from your perspective) into the wind, the base is cocked (from your perspective) \"downwind.\" So, (again from your perspective), the drag is pushing the projectile downwind in a nose to tail direction.\n\nWind also causes aerodynamic jump which is the vertical component of cross wind deflection caused by lateral (wind) impulses activated during free flight of a projectile or at or very near the muzzle leading to dynamic imbalance. The amount of aerodynamic jump is dependent on cross wind speed, the gyroscopic stability of the bullet at the muzzle and if the barrel twist is clockwise or anti-clockwise. Like the wind direction reversing the twist direction will reverse the aerodynamic jump direction.\n\nA somewhat less obvious effect is caused by head or tailwinds. A headwind will slightly increase the relative velocity of the projectile, and increase drag and the corresponding drop. A tailwind will reduce the drag and the projectile/bullet drop. In the real world, pure head or tailwinds are rare, since wind is seldomly constant in force and direction and normally interacts with the terrain it is blowing over. This often makes ultra long range shooting in head or tailwind conditions difficult.\n\nThe vertical angle (or elevation) of a shot will also affect the trajectory of the shot. Ballistic tables for small calibre projectiles (fired from pistols or rifles) assume a horizontal line of sight between the shooter and target with gravity acting perpendicular to the earth. Therefore, if the shooter-to-target angle is up or down, (the direction of the gravity component does not change with slope direction), then the trajectory curving acceleration due to gravity will actually be less, in proportion to the cosine of the slant angle. As a result, a projectile fired upward or downward, on a so-called \"slant range,\" will over-shoot the same target distance on flat ground. The effect is of sufficient magnitude that hunters must adjust their target hold off accordingly in mountainous terrain. A well known formula for slant range adjustment to horizontal range hold off is known as the Rifleman's rule. The Rifleman's rule and the slightly more complex and less well known Improved Rifleman's rule models produce sufficiently accurate predictions for many small arms applications. Simple prediction models however ignore minor gravity effects when shooting uphill or downhill. The only practical way to compensate for this is to use a ballistic computer program. Besides gravity at very steep angles over long distances, the effect of air density changes the projectile encounters during flight become problematic.\nThe mathematical prediction models available for inclined fire scenarios, depending on the amount and direction (uphill or downhill) of the inclination angle and range, yield varying accuracy expectation levels.\nLess advanced ballistic computer programs predict the same trajectory for uphill and downhill shots at the same vertical angle and range. The more advanced programs factor in the small effect of gravity on uphill and on downhill shots resulting in slightly differing trajectories at the same vertical angle and range. No publicly available ballistic computer program currently (2017) accounts for the complicated phenomena of differing air densities the projectile encounters during flight.\n\nAir pressure, temperature, and humidity variations make up the ambient air density. Humidity has a counter intuitive impact. Since water vapor has a density of 0.8 grams per litre, while dry air averages about 1.225 grams per litre, higher humidity actually decreases the air density, and therefore decreases the drag. (An easy way to remember that water vapor reduces air density is to observe that clouds float.)\n\nGyroscopic drift is an interaction of the bullet's mass and aerodynamics with the atmosphere that it is flying in.\nEven in completely calm air, with no sideways air movement at all, a spin-stabilized projectile will experience a spin-induced sideways component, due to a gyroscopic phenomenon known as \"yaw of repose.\" For a right hand (clockwise) direction of rotation this component will always be to the right. For a left hand (counterclockwise) direction of rotation this component will always be to the left.\nThis is because the projectile's longitudinal axis (its axis of rotation) and the direction of the velocity vector of the center of gravity (CG) deviate by a small angle, which is said to be the equilibrium yaw or the yaw of repose. The magnitude of the yaw of repose angle is typically less than 0.5 degree. Since rotating objects react with an angular velocity vector 90 degrees from the applied torque vector, the bullet's axis of symmetry moves with a component in the vertical plane and a component in the horizontal plane; for right-handed (clockwise) spinning bullets, the bullet's axis of symmetry deflects to the right and a little bit upward with respect to the direction of the velocity vector, as the projectile moves along its ballistic arc. As the result of this small inclination, there is a continuous air stream, which tends to deflect the bullet to the right. Thus the occurrence of the yaw of repose is the reason for the bullet drifting to the right (for right-handed spin) or to the left (for left-handed spin). This means that the bullet is \"skidding\" sideways at any given moment, and thus experiencing a sideways component.\n\nThe following variables affect the magnitude of gyroscopic drift:\n\nDoppler radar measurement results for the gyroscopic drift of several US military and other very-low-drag bullets at 1000 yards (914.4 m) look like this:\nThe table shows that the gyroscopic drift cannot be predicted on weight and diameter alone. In order to make accurate predictions on gyroscopic drift several details about both the external and internal ballistics must be considered. Factors such as the twist rate of the barrel, the velocity of the projectile as it exits the muzzle, barrel harmonics, and atmospheric conditions, all contribute to the path of a projectile.\n\nSpin stabilized projectiles are affected by the Magnus effect, whereby the spin of the bullet creates a force acting either up or down, perpendicular to the sideways vector of the wind.\nIn the simple case of horizontal wind, and a right hand (clockwise) direction of rotation, the Magnus effect induced pressure differences around the bullet cause a downward (wind from the right) or upward (wind from the left) force viewed from the point of firing to act on the projectile, affecting its point of impact. The vertical deflection value tends to be small in comparison with the horizontal wind induced deflection component, but it may nevertheless be significant in winds that exceed 4 m/s (14.4 km/h or 9 mph).\n\nThe Magnus effect has a significant role in bullet stability because the Magnus force does not act upon the bullet's center of gravity, but the center of pressure affecting the \"yaw\" of the bullet. The Magnus effect will act as a \"destabilizing\" force on any bullet with a center of pressure located \"ahead\" of the center of gravity, while conversely acting as a \"stabilizing\" force on any bullet with the center of pressure located \"behind\" the center of gravity. The location of the center of pressure depends on the flow field structure, in other words, depending on whether the bullet is in supersonic, transonic or subsonic flight. What this means in practice depends on the shape and other attributes of the bullet, in any case the Magnus force greatly affects stability because it tries to \"twist\" the bullet along its flight path.\n\nParadoxically, very-low-drag bullets due to their length have a tendency to exhibit greater Magnus destabilizing errors because they have a greater surface area to present to the oncoming air they are travelling through, thereby reducing their aerodynamic efficiency. This subtle effect is one of the reasons why a calculated C or BC based on shape and sectional density is of limited use.\n\nAnother minor cause of drift, which depends on the nose of the projectile being above the trajectory, is the Poisson Effect. This, if it occurs at all, acts in the same direction as the gyroscopic drift and is even less important than the Magnus effect. It supposes that the uptilted nose of the projectile causes an air cushion to build up underneath it. It further supposes that there is an increase of friction between this cushion and the projectile so that the latter, with its spin, will tend to roll off the cushion and move sideways.\n\nThis simple explanation is quite popular. There is, however, no evidence to show that increased pressure means increased friction and unless this is so, there can be no effect. Even if it does exist it must be quite insignificant compared with the gyroscopic and Coriolis drifts.\n\nBoth the Poisson and Magnus Effects will reverse their directions of drift if the nose falls below the trajectory. When the nose is off to one side, as in equilibrium yaw, these effects will make minute alterations in range.\n\nThe Coriolis effect causes Coriolis drift in a direction perpendicular to the Earth's axis; for most locations on Earth and firing directions, this deflection includes horizontal and vertical components. The deflection is to the right of the trajectory in the northern hemisphere, to the left in the southern hemisphere, upward for eastward shots, and downward for westward shots. The vertical Coriolis deflection is also known as the Eötvös effect. Coriolis drift is not an aerodynamic effect; it is a consequence of the rotation of the Earth.\n\nThe magnitude of the Coriolis effect is small. For small arms, the magnitude of the Coriolis effect is generally insignificant (for high powered rifles in the order of about at ), but for ballistic projectiles with long flight times, such as extreme long-range rifle projectiles, artillery, and rockets like intercontinental ballistic missiles, it is a significant factor in calculating the trajectory. The magnitude of the drift depends on the firing and target location, azimuth of firing, projectile velocity and time of flight.\n\nViewed from a non-rotating reference frame (i.e. not one rotating with the Earth) and ignoring the forces of gravity and air resistance, a projectile moves in a straight line. When viewed from a reference frame fixed with respect to the Earth, that straight trajectory appears to curve sideways. The direction of this horizontal curvature is to the right in the northern hemisphere and to the left in the southern hemisphere, and does not depend on the azimuth of the shot. The horizontal curvature is largest at the poles and decreases to zero at the equator.\n\nThe Eötvös effect changes the perceived gravitational pull on a moving object based on the relationship between the direction and velocity of movement and the direction of the Earth's rotation.\n\nThe Eötvös effect is largest at the equator and decreases to zero at the poles. It causes eastward-traveling projectiles to deflect upward, and westward-traveling projectiles to deflect downward. The effect is less pronounced for trajectories in other directions, and is zero for trajectories aimed due north or south. In the case of large changes of momentum, such as a spacecraft being launched into Earth orbit, the effect becomes significant. It contributes to the fastest and most fuel-efficient path to orbit: a launch from the equator that curves to a directly eastward heading.\n\nThough not forces acting on projectile trajectories there are some equipment related factors that influence trajectories. Since these factors can cause otherwise unexplainable external ballistic flight behavior they have to be briefly mentioned.\n\nLateral jump is caused by a slight lateral and rotational movement of a gun barrel at the instant of firing. It has the effect of a small error in bearing. The effect is ignored, since it is small and varies from round to round.\n\nLateral throw-off is caused by mass imbalance in applied spin stabilized projectiles or pressure imbalances during the transitional flight phase when a projectile leaves a gun barrel off axis leading to static imbalance. If present it causes dispersion. The effect is unpredictable, since it is generally small and varies from projectile to projectile, round to round and/or gun barrel to gun barrel.\n\nThe maximum practical range of all small arms and especially high-powered sniper rifles depends mainly on the aerodynamic or ballistic efficiency of the spin stabilised projectiles used. Long-range shooters must also collect relevant information to calculate elevation and windage corrections to be able to achieve first shot strikes at point targets. The data to calculate these fire control corrections has a long list of variables including:\n\nThe ambient air density is at its maximum at Arctic sea level conditions. Cold gunpowder also produces lower pressures and hence lower muzzle velocities than warm powder. This means that the maximum practical range of rifles will be at it shortest at Arctic sea level conditions.\n\nThe ability to hit a point target at great range has a lot to do with the ability to tackle environmental and meteorological factors and a good understanding of exterior ballistics and the limitations of equipment. Without (computer) support and highly accurate laser rangefinders and meteorological measuring equipment as aids to determine ballistic solutions, long-range shooting beyond 1000 m (1100 yd) at unknown ranges becomes guesswork for even the most expert long-range marksmen.\n\n\"Interesting further reading: \"\n\nHere is an example of a ballistic table for a .30 calibre Speer 169 grain (11 g) pointed boat tail match bullet, with a BC of 0.480. It assumes sights 1.5 inches (38 mm) above the bore line, and sights adjusted to result in point of aim and point of impact matching 200 yards (183 m) and 300 yards (274 m) respectively.\n\nThis table demonstrates that, even with a fairly aerodynamic bullet fired at high velocity, the \"bullet drop\" or change in the point of impact is significant. This change in point of impact has two important implications. Firstly, estimating the distance to the target is critical at longer ranges, because the difference in the point of impact between 400 and is 25–32 in (depending on zero), in other words if the shooter estimates that the target is 400 yd away when it is in fact 500 yd away the shot will impact 25–32 in (635–813 mm) below where it was aimed, possibly missing the target completely. Secondly, the rifle should be zeroed to a distance appropriate to the typical range of targets, because the shooter might have to aim so far above the target to compensate for a large bullet drop that he may lose sight of the target completely (for instance being outside the field of view of a telescopic sight). In the example of the rifle zeroed at , the shooter would have to aim 49 in or more than 4 ft (1.2 m) above the point of impact for a target at 500 yd.\n\n\n\nGeneral external ballistics\n\nSmall arms external ballistics\n\nArtillery external ballistics\n"}
{"id": "36056523", "url": "https://en.wikipedia.org/wiki?curid=36056523", "title": "Fraunhofer Institute for Solar Energy Systems", "text": "Fraunhofer Institute for Solar Energy Systems\n\nThe Fraunhofer Institute for Solar Energy Systems ISE (or Fraunhofer ISE) is an institute of the Fraunhofer-Gesellschaft. Located in Freiburg, Germany, The Institute performs applied scientific and engineering research and development for all areas of solar energy. Fraunhofer ISE has three external branches in Germany which carry out work on solar cell and semiconductor material development: the Laboratory and Service Center (LSC) in Gelsenkirchen, the Technology Center of Semiconductor Materials (THM) in Freiberg, and the Fraunhofer Center for Silicon Photovoltaics (CSP) in Halle. Since 2006, Prof. Dr. Eicke R. Weber is the director of Fraunhofer ISE. With over 1,100 employees, Fraunhofer ISE is the largest institute for applied solar energy research in Europe. The 2012 Operational Budget including investments is 74.3 million euro.\n\nFraunhofer ISE was founded in 1981 by Prof. Adolf Goetzberger in Freiburg, Germany. It was the first non-university establishment for applied solar energy research in Europe. The first areas of focus were the fluorescent collector FLUKO, transparent insulation and the initial steps towards high efficiency silicon and III-V solar cells, silicon thin film solar cells and material research.\n\nAlready in 1983, the first fully electronic so-called \"ISE inverter\" was developed for use in autonomous photovoltaic systems. In 1986, the first serial product using fluorescent collectors as a power supply was produced. Within the PV small device program, numerous other successful products were developed. When the clean room was put into operation in 1989, the production of high efficiency solar cells began. In 1998, selective solar absorber coatings, which were developed at Fraunhofer ISE for solar thermal collectors, were put into industrial-scale production.\n\nIn 2011 Fraunhofer ISE celebrated its 30th anniversary. Since its founding, the scientists have received many prestigious prizes and awards for their research results in the field.\n\nThe solar energy research at Fraunhofer ISE establishes the technical prerequisites for an efficient and environmentally friendly energy supply for industrial as well as threshold and developing countries. The institute is committed to moving away from fossil fuels and nuclear power and moving towards a 100% renewable energy supply with the aim of providing affordable solutions for the energy transformation.\n\nTo this purpose, the institute develops materials, components, systems and processes for basic research and beyond. The areas of expertise include the development of production techniques and prototypes, setting up and monitoring demonstration systems and operating indoor and outdoor testing and calibration centers.\n\nThe various areas of research at Fraunhofer ISE are categorized into the following business areas:\n\nEnergy Efficient Buildings At Fraunhofer ISE, energy-efficient buildings are one of the main areas of research. Teaming up with architects, expert planners and the industry, the researchers at Fraunhofer ISE optimize the performance of existing building and develop the buildings of tomorrow with consideration to the economic aspects, the energy efficiency and the user comfort. Through its involvement in the International Energy Agency (IEA), the Institute contributes in the establishment of the international boundary conditions for the realization of these concepts. In this business area, many disciplines come together: from material research and coating development through to component and system development and finally the required tests.\n\nApplied Optics and functional Surfaces Solar energy systems convert solar radiation incident on the earth into thermal, electrical or chemical energy. In order to better transmit, reflect, absorb, filter, redirect or concentrate the incoming radiation, Fraunhofer ISE develops optical components and systems. This business area serves as an interdisciplinary field and serves many areas of solar technology: windows and facades, solar thermal collectors, concentrator systems for photovoltaics and solar power plants as well as photovoltaic module technology.\n\nSolar Thermal Systems This business area covers the markets of low and high temperature applications. Solar thermal collectors and collector systems with flat or evacuated tube collectors find numerous applications in the practice. These include process water and solar heating systems, cooling and ventilation systems and sea water desalination systems. Also façade-integrated collectors are implemented. With linear concentrating collectors, operating temperatures from 150 °C to over 400 °C are achieved. Both trough and parabolic collectors are not only used for solar thermal power production in large power plants, but also in simpler and more cost-effective plants for the production of process heat, process steam and driving heat for absorption chillers.\n\nSilicon Photovoltaics Especially due to the market introduction programs in Japan and Germany, the role of photovoltaics is gaining more and more on importance. More than 85% of the solar cells produced worldwide are based on crystalline silicon. The price-performance ratio, long-term stability and the cost-reduction potential indicate that this top performer in the terrestrial photovoltaic market will retain its place as market leader longer than just the next decade. Fraunhofer ISE's expertise ranges over the entire value chain of crystalline silicon photovoltaics, starting from material development and crystallization, through to solar cell processing and photovoltaic module technology.\n\nPhotovoltaic Modules and Systems Module technology converts solar cells into a robust product for reliable operation in PV power plants. Fraunhofer ISE supports the product development concentrating on increasing efficiencies, reducing costs and achieving the highest reliability. Over and above, the Institute offers its services for quality assurance of modules and power plants.\n\nAlternative Photovoltaic Technologies In addition to silicon photovoltaics, the solar cell research at Fraunhofer ISE also extends to other photovoltaic technologies: With III-V based semiconductors like gallium indium phosphide, aluminum gallium arsenide or gallium arsenide, the highest efficiencies can be reached today. The technology of the dye solar cells has developed well beyond the laboratory stage and organic solar cells are attractive especially due to the expected low manufacturing costs.\n\nRenewable Power Supply The construction of grid-connected systems is the largest market for the photovoltaic branch today. The Institute provides consultancy services for system planning, characterizes solar modules and carries out the technical analysis and performance testing of photovoltaic systems.\n\nOff-grid power supplies also are a focus of the ongoing research at the institute. People living in remote rural areas, the countless number of telecommunication systems, environmental measurement technology as well as portable electronic devices require an autonomous power supply, independent of the grid. Fraunhofer ISE develops renewable energy systems for this purpose.\n\nFraunhofer ISE also performs research in the area of power electronics and controls. This includes inverter development and testing in a modern power electronics laboratory as well as research in the field of energy management including smart metering and smart grids.\n\nIn future, vehicles will run partly or completely on electricity and draw their energy from the grid (electric and plug-in). Fraunhofer ISE is working at the interface between the vehicles and the grid on concepts for an environmentally acceptable power supply and the optimal integration of the vehicles into the electricity grid, including metering and billing systems.\n\nHydrogen Technology In a fuel cell, hydrogen reacts with oxygen and sets useful energy free in the form of electricity and heat. Since hydrogen does not exist in its pure form in nature, it must be produced from one of its many chemical compounds. At Fraunhofer ISE in the area of hydrogen technology, the research focuses on innovative technologies for hydrogen generation and on processes for the highly efficient conversion of hydrogen into electricity and heat using the most modern equipment. Together with partners from industry and science, components and complete fuel cell systems are developed for autonomous, portable as well as mobile applications.\n\nPresently the following certified test labs provide testing and calibration services at the laboratories:\n\nOther service establishments at the laboratories are:\n\nThe Institute is one of the founding members and the Member-in-Charge of the Fraunhofer Energy Alliance, comprising sixteen Fraunhofer institutes with expertise in energy technology and energy research.\n\nThe Institute is a member of the ForschungsVerbund Erneuerbare Energien (FVEE) and the European Renewable Energy Research Centres Agency (EUREC), as well as other alliances.\n\nThe Institute maintains a close cooperation with the Material Research Center of the University of Freiburg, which assists the Institute with fundamental research. Th institute director holds a faculty position at that University as professor of physics and applied sciences.\n\nTo date, seven spin-off companies have been founded from the applied research results at Fraunhofer ISE. Among them are the following:\n\n\nThe laboratory has a staff of 1139, of whom 439 hold permanent positions. (as of 04/2012).\n\nThe research institute has a net floor area of 21,000 m² which contains offices, laboratories and test fields. New labs and office space are presently under construction.\n\nThe 2011 operational budget totaled €61.3 million. Just five percent of the operational budget was basic funding, 90% from German federal funds and 10% from German state funds. About 50% was from contract research with industry; the remainder stemmed from public and other sources. In 2011, annual investments amounted to €7.7 million. (as of 04/2012)\n\n"}
{"id": "396575", "url": "https://en.wikipedia.org/wiki?curid=396575", "title": "Fσ set", "text": "Fσ set\n\nIn mathematics, an F set (said F-sigma set) is a countable union of closed sets. The notation originated in France with F for \"fermé\" (\"French\": closed) and σ for \"somme\" (\"French\": sum, union).\n\nIn metrizable spaces, every open set is an F set. The complement of an F set is a G set. In a metrizable space, any closed set is a G set.\n\nThe union of countably many F sets is an F set, and the intersection of finitely many F sets is an F set. F is the same as formula_1 in the Borel hierarchy.\n\nEach closed set is an F set.\n\nThe set formula_2 of rationals is an F set. The set formula_3 of irrationals is not a F set.\n\nIn a Tychonoff space, each countable set is an F set, because a point formula_4 is closed.\n\nFor example, the set formula_5 of all points formula_6 in the Cartesian plane such that formula_7 is rational is an F set because it can be expressed as the union of all the lines passing through the origin with rational slope:\n\nwhere formula_2, is the set of rational numbers, which is a countable set.\n\n"}
{"id": "2555528", "url": "https://en.wikipedia.org/wiki?curid=2555528", "title": "Gas to liquids", "text": "Gas to liquids\n\nGas to liquids (GTL) is a refinery process to convert natural gas or other gaseous hydrocarbons into longer-chain hydrocarbons, such as gasoline or diesel fuel. Methane-rich gases are converted into liquid synthetic fuels. Two general strategies exist: (i) direct partial combustion of methane to methanol and (ii) Fischer-Tropsch-like processes that convert carbon monoxide and hydrogen into hydrocarbons. Strategy ii is followed by diverse methods to convert the hydrogen-carbon monoxide mixtures to liquids. Direct partial combustion has been demonstrated in nature but not replicated commercially. Technologies reliant on partial combustion have been commercialized mainly in regions where natural gas is inexpensive.\n\nThe motivation for GTL is to produce liquid fuels, which are more readily transported than methane. Methane must be cooled below its critical temperature of -82.3 °C in order to be liquified under pressure. Because of the associated cryogenic apparatus, LNG tankers are expensive, not to mention potentially dangerous. Methanol is a conveniently handled combustable liquid, but its energy density is half of that of gasoline.\nThe Fischer–Tropsch process starts with partial oxidation of methane (natural gas) to carbon dioxide, carbon monoxide, hydrogen gas and water. The ratio of carbon monoxide to hydrogen is adjusted using the water gas shift reaction, while the excess carbon dioxide is removed. Removing the water yields synthesis gas (syngas). Syngas is allowed to react over an iron or cobalt catalyst to produce liquid hydrocarbons, including alcohols.\n\nMethanol is made from methane (natural gas) in a series of three reactions:\n\nThe methanol thus formed may be converted to gasoline by the Mobil process and methanol-to-olefins.\n\nIn the early 1970s, Mobil developed an alternative procedure in which natural gas is converted to syngas, and then methanol. The methanol reacts in the presence of a zeolite catalyst to form alkanes. In terms of mechanism, methanol is partially dehydrated to give dimethyl ether:\nThe mixture of dimethyl ether and methanol is then further dehydrated over a zeolite catalyst such as ZSM-5, which in practice is polymerized and hydrogenated to give a gasoline with hydrocarbons of five or more carbon atoms making up 80% of the fuel by weight. The Mobile MTG process is no longer practiced. A more modern implementation of MTG is the Topsøe integrated gasoline synthesis (TIGS).\n\nMethanol can can be converted to olefins using zeolite and SAPO-based heterogeneous catalysts. Depending on the catalyst pore size, this process can afford either C2 or C3 products, which are important monomers.\n\nA third gas-to-liquids process builds on the MTG technology by converting natural gas-derived syngas into drop-in gasoline and jet fuel via a thermochemical single-loop process.\n\nThe STG+ process follows four principal steps in one continuous process loop. This process consists of four fixed bed reactors in series in which a syngas is converted to synthetic fuels. The steps for producing high-octane synthetic gasoline are as follows:\n\n\nWith methane as the predominant target for GTL, much attention has focused on the three enzymes that process methane. These enzymes support the existence of methanotrophs, microorganisms that metabolize methane as their only source of carbon and energy. Aerobic methanotrophs harbor enzymes that oxygenate of methane to methanol. The relevant enzymes are methane monooxygenases, which are found both in soluble and particulate (i.e. membrane-bound) varieties. They catalyze the oxygenation according to the following stoichiometry:\n\nAnaerobic methanotrophs rely on the bioconversion of methane using the enzymes called methyl-coenzyme M reductases. These organisms effect reverse methanogenesis. Strenuous efforts have been made to elucidate the mechanisms of these methane-converting enzymes, which would enable their catalysis to be replicated in vitro.\n\nBiodiesel can be made from using the microbes Moorella thermoacetica and Yarrowia lipolytica. This process is known as biological gas-to-liquids.\n\nUsing gas-to-liquids processes, refineries can convert some of their gaseous waste products (flare gas) into valuable fuel oils, which can be sold as is or blended only with diesel fuel. The World Bank estimates that over of natural gas are flared or vented annually, an amount worth approximately $30.6 billion, equivalent to 25% of the United States' gas consumption or 30% of the European Union's annual gas consumption, a resource that could be useful using GTL. Gas-to-liquids processes may also be used for the economic extraction of gas deposits in locations where it is not economical to build a pipeline. This process will be increasingly significant as crude oil resources are depleted.\n\nRoyal Dutch Shell produces a diesel from natural gas in a factory in Bintulu, Malaysia. Another Shell GTL facility is the Pearl GTL plant in Qatar, the world's largest GTL facility. SASOL has recently built the Oryx GTL facility in Ras Laffan Industrial City, Qatar and together with Uzbekneftegaz and Petronas builds the Uzbekistan GTL plant. Chevron Corporation, in a joint venture with the Nigerian National Petroleum Corporation is commissioning the Escravos GTL in Nigeria, which uses Sasol technology.\n\nNew generation of GTL technology is being pursued for the conversion of unconventional, remote and problem gas into valuable liquid fuels. GTL plants based on innovative Fischer-Tropsch catalyst have been built by INFRA Technology. Other mainly U.S. companies include Velocys, ENVIA Energy, Waste Management, NRG Energy, Ventech, Velocys, ThyssenKrupp Industrial Solutions, Liberty GTL, Petrobras, and Greenway Innovative Energy, Primus Green Energy, and Compact GTL. and Petronas. Several of these processes have proven themselves with demonstration flights using their jet fuels.\nAnother proposed solution to stranded gas involves use of novel FPSO for offshore conversion of gas to liquids such as methanol, diesel, petrol, synthetic crude, and naphtha.\n\n"}
{"id": "3246247", "url": "https://en.wikipedia.org/wiki?curid=3246247", "title": "Hagen–Poiseuille flow from the Navier–Stokes equations", "text": "Hagen–Poiseuille flow from the Navier–Stokes equations\n\nIn fluid dynamics, the derivation of the Hagen–Poiseuille flow from the Navier–Stokes equations shows how this flow is an exact solution to the Navier–Stokes equations.\n\nThe laminar flow through a pipe of uniform (circular) cross-section is known as Hagen–Poiseuille flow. The equations governing the Hagen–Poiseuille flow can be derived directly from the Navier–Stokes momentum equations in 3D cylindrical coordinates by making the following set of assumptions:\n\n\nThen the angular equation in the momentum equations and the continuity equation are identically satisfied. The first momentum equation reduces to formula_5, i.e., the pressure formula_6 is a function of the axial coordinate formula_7 only. The third momentum equation reduces to:\n\nSince formula_11 needs to be finite at formula_12, formula_13. The no slip boundary condition at the pipe wall requires that formula_14 at formula_15 (radius of the pipe), which yields \n\nThus we have finally the following parabolic velocity profile: \n\nThe maximum velocity occurs at the pipe centerline (formula_18):\n\nThe average velocity can be obtained by integrating over the pipe cross section:\n\nThe Hagen–Poiseuille equation relates the pressure drop formula_21 across a circular pipe of length formula_22 to the\naverage flow velocity in the pipe formula_23 and other parameters. Assuming that the pressure decreases linearly across the length \nof the pipe, we have formula_24 (constant). Substituting this and the expression for formula_25 into the expression for formula_23, and noting that the pipe diameter formula_27, we get: \nRearrangement of this gives the Hagen–Poiseuille equation:\n\n"}
{"id": "45489359", "url": "https://en.wikipedia.org/wiki?curid=45489359", "title": "Henry Seth Taylor steam buggy", "text": "Henry Seth Taylor steam buggy\n\nThe Henry Seth Taylor steam buggy is the first known car built in Canada. It was built by Henry Seth Taylor, a watchmaker and jeweller in Stanstead, Quebec in 1867. It was unveiled at the Stanstead Fall Fair that year. The vehicle was crashed into a creek shortly thereafter.\n\nThe Buggy was modeled after a US-built steam car Taylor had seen in 1864. It was designed by Taylor to have a coal-fired boiler for the carriage. Fully pressurized, the steam was used to move a piston attached to the rear axle, producing forward motion. The vehicle does not have a reverse gear or brakes. It was able to travel at a sustained . The vehicle features thin strips of metal bent around the edge of the wheels in place of rubber tires. It is powered by a two-cylinder boiler mounted behind the driver. Steam is generated in the vertical coal-fired boiler, which is connected by rubber hoses to a six-gallon water tank located between the front wheels. The vehicle lacks a coal box, but has a storage nook for lump coal or wood under the driver’s perch.\n\nThe vehicle weighs . The boiler (as designed) can withstand of pressure. It was the only example built, not having been meant for mass production; rather, it was exhibited as a curiosity at fairs around the area.\n\nThe vehicle lay disassembled in a barn where the wood from the carriage body and wheels had rotted away; it was discovered in the 1950s and was purchased and later restored by American collector Richard Stewart. In 1983, it was purchased from Stewart by the Canada Science and Technology Museum in Ottawa, where it has remained since. The original brass-work around the cylinders and the oil caps were found to be in \"fantastic condition\", but the boiler has been replaced two times.\n\nThe vehicle is featured on a 43-cent stamp issued by Canada Post in 1993, as part of the Historic Land Vehicles series.\n"}
{"id": "49407336", "url": "https://en.wikipedia.org/wiki?curid=49407336", "title": "Institute of Nuclear Energy Research", "text": "Institute of Nuclear Energy Research\n\nThe Institute of Nuclear Energy Research (INER; ) is the agency of the Atomic Energy Council of the Taiwan (ROC) dedicated to the research and development on nuclear safety, nuclear facility decommissioning, radioactive waste treatment and disposal technologies in Taiwan.\n\nINER was established on 9 May 1968.\n\n\n"}
{"id": "12247272", "url": "https://en.wikipedia.org/wiki?curid=12247272", "title": "Kota Damansara Community Forest Park", "text": "Kota Damansara Community Forest Park\n\nThe Kota Damansara Community Forest Park (KDCFP) is a secondary forest located in Selangor, Malaysia. It was part of a defunct Sungai Buloh Forest Reserve gazetted in 1898 which contained of protected primary forest. This made it as the oldest forest reserve in Peninsular Malaysia. The forest reserve was degazetted a number of times and subsequent development brought down the size of the reserve to a .\n\nLogging activities especially has turned the area into a secondary forest. This results in the abundance of pioneer plants as well as rare species of plant such as Begonia aequilateralis. Despite being a secondary forest, a number of primary forest species such as meranti are observable at the park. Furthermore, KDCFP is a dipterocarp forest typically found in tropical countries such as Malaysia. The area is a combination of flat land and low elevation hill.\n\nThe trademark of KDCFP is an artificial lake. The lake was created in early 2000s due to the building of a road that blocked a small stream that flows through the park. This has introduced aquatic flora and fauna attributable to slow-flowing water body to the area.\n\nKota Damansara is a developing area and there are efforts to develop the area. The local residents and the Malaysian Nature Society with aid from the United Nations Development Program through the Small Grants Program are trying to protect the area from further development projects.\n\n\n"}
{"id": "2772805", "url": "https://en.wikipedia.org/wiki?curid=2772805", "title": "Lake Monoun", "text": "Lake Monoun\n\nLake Monoun is a lake in West Province, Cameroon, that lies in the Oku Volcanic Field . On August 15, 1984, the lake exploded in a limnic eruption, which resulted in the release of a large amount of carbon dioxide that killed 37 people. At first, the cause of the deaths was a mystery, and causes such as terrorism were suspected. Further investigation and a similar event two years later at Lake Nyos led to the currently accepted explanation.\n\nSeveral people reported hearing a loud noise on August 15, 1984 around 22:30. A gas cloud reportedly emanated from a crater in the eastern part of the lake. The resulting deaths of residents in a low-lying area are believed to have occurred between 03:00 and dawn. The victims were said to have skin burns, which reports later clarified as \"skin damage\" such as discoloration. Survivors reported that the whitish, smoke-like cloud smelled bitter and acidic. Vegetation was flattened around the eastern part of the lake, probably by a tsunami.\n\nAlthough Lake Monoun is near the center of a volcanic field that includes at least 34 recent craters, the subsequent investigation found that the event was not caused by an eruption or sudden ejection of volcanic gas from the lake. Rather, emission of carbon dioxide in a limnic eruption is thought to be to blame. The cloud's smell and skin damage to victims were not fully explained. Some theories attribute the skin problems to a combination of preexisting conditions and routine postmortem effects like livor mortis, although there is no clear consensus.\n\nAmong the victims were some of the riders in a truck carrying twelve people. The truck's engine stopped working, and the people inside the truck got out and were killed. Two people sitting on top of the truck survived, because their elevated position allowed them to breathe - carbon dioxide is heavier than air (oxygen and nitrogen) which causes it to stay close to the ground.\n\nTwo years later, on 21 August 1986, a similar and even more deadly event occurred at Lake Nyos, about NNW, killing about 1746 people and more than 3,000 livestock. Along with Lake Nyos and Lake Kivu, Lake Monoun is one of three lakes in the world known to have high concentrations of gas dissolved deep below the surface and which have the right conditions for a limnic eruption.\n\nIn 2003, a venting pipe was inserted into the lake, in an effort to prevent the disaster from recurring. Initially a pump was needed to pull water from the bottom, but as carbon dioxide began to come out of solution it created buoyancy in the water in the pipes, allowing the pump to be turned off.\n\nHowever, a study in September 2005 by George Kling and other researchers at the University of Michigan found that gas was not being removed from the lake quickly enough to ensure that the disaster never happens again. Kling recommended the lowering of the existing pipe and the addition of a new one in order to release more carbon dioxide.\n\n\n\n"}
{"id": "42914155", "url": "https://en.wikipedia.org/wiki?curid=42914155", "title": "Lehigh Technologies", "text": "Lehigh Technologies\n\nLehigh Technologies is a specialty materials company that manufactures micronized rubber powders (MRP). Based in Atlanta, Georgia, Lehigh Technologies operates a MRP manufacturing plant at its headquarters, with an annual production capacity of more than 100 million pounds. Lehigh also operates a MRP research and development facility, its Application & Development Center.\n\nLehigh Technologies has been manufacturing MRP at its commercial plant since 2007. According to Lehigh, the company has demonstrated steady growth since it began manufacturing MRP at commercial scale.\n\nIn 2010, Lehigh was selected as a Technology Pioneer by the World Economic Forum (WEF). The awards program recognizes companies from around the world that are involved in the design, development and deployment of new technologies that are set to have a significant impact on business and society.\n\nIn June 2011, Lehigh announced that more than 100 million tires on the road had been manufactured using the company's MRP. In 2014, the company announced that more than 250 million tires produced to date had incorporated MRP. In October 2017, Lehigh Technologies was acquired by Michelin.\n\nIn November 2010, Lehigh expanded its in-house capabilities with the launch of the Application & Development Center (ADC), a dedicated research and development facility staffed by a team of industry professionals. The stated mission of the ADC is to develop new applications for MRP and to accelerate adoption of existing MRP applications by improving understanding of MRP chemistry, providing expert advice for new material formulations that incorporate MRP, and developing new materials based on MRP.\n\nIn January 2014, Lehigh announced an expansion of its ADC that includes the hiring of new technical staff and investment in new mixing and analytical equipment. According to Lehigh, this expansion improves the ADC’s ability to develop functional materials based on MRP.\n\nIn addition to its production facility in Atlanta, Lehigh has developed a network of partners and distributors.\n\nLehigh partnered with Connell Brothers Corporation (CBC), the largest distributor of specialty chemicals and ingredients in Asia-Pacific, to market its MRP in Japan, In 2014, Lehigh announced an expanded partnership with CBC, aimed at broadening industry outreach, supply chain efficiency and technical support in Japan, as well as introducing MRP to South Korea and Southeast Asia.\n\nIn September 2014, Lehigh announced the closing of an $8 million fundraise with JSR Corporation, a $4 billion specialty chemicals company based in Japan, to support the company’s geographic expansion and technology roadmap for MRP in the region.\n\nIn October 2011, Lehigh expanded into Brazil with the appointment of a country manager. In December 2012, Lehigh announced a partnership with Andes Chemical Corporation to market MRP in Central America.\n\nIn September 2012, Lehigh partnered with HERA Holding, a Spanish waste-to-resource company, to market MRP to the tire, consumer, and industrial plastics and coatings industries in Europe.\n\nSince 2008, Lehigh has raised more than $50 million from investors that include Kleiner Perkins Caufield & Byers, Index Ventures, Leaf Clean Energy and NGP Energy Technology Partners.\n\nLehigh has commercialized a proprietary cryogenic turbo mill technology that transforms end-of-life tires and post-industrial rubber materials into micronized rubber powders. Lehigh produces MRP from vulcanized elastomeric material, most often end-of-life tire material. However, it can also be produced from post-industrial nitrile rubber, ethylene propylene diene monomer (EPDM), butyl, and natural rubber compounds.\n\nLehigh's MRPs are available in two product lines, PolyDyne and MicroDyne. Industries incorporating MRP into their products include tire, automotive, construction, industrial components and consumer products. It is also used as an additive in tires, plastics, asphalt, coatings, and sealants. MRP can also be incorporated into prime or recycled grade polypropylene (PP), high-density polyethylene (HDPE) and nylons. Additionally, the incorporation of MRPs in thermoplastic elastomers (TPE) and thermoplastic vulcanizates (TPV) makes it a feasible ingredient for automotive and building and construction applications.\n\nThe particle size distributions of Lehigh's MRP typically range from 830 µm (20 mesh) to 50 µm (300 mesh), and its MRP is free of foreign particulates, such as metal and fiber.\n\nLehigh claims that its cryogenic turbo mill technology represents an evolution over previous post-manufactured rubber technologies. The most common rubber processing technology converts end-of-life tire and post-industrial rubber material into rubber chips that are typically one inch or larger in size. These chips are then used in tire-derived fuel and civil engineering projects. A second-generation processing technology converts end-of-life tire and rubber material into crumb rubber, also known as ground tire rubber (GTR). GTR typically comprises chips between one inch and 30 mesh in size, with the associated fiber and steel mostly removed. This material is used in asphalt, as garden mulch and in playgrounds.\n\nBecause of the material's smaller size, Lehigh markets its micrometer-scale MRP as a third-generation rubber processing technology. MRP's size and composition enables it to be incorporated into advanced and higher-value applications.\n\nLehigh's MRP is used as a compound extender to replace natural rubber and synthetic polymers, as well as to act as a process aid in material production. Lehigh claims that because it replaces commodity-priced rubber- and oil-based feedstocks, MRP can reduce formulation costs. The company has asserted that MRP can offer up to 50 percent cost savings over virgin raw materials in some cases.\n\nBecause it is derived from end-of-life materials, MRP can also improve the sustainability, and in some cases the performance, of the compounds in which it is used. For example, Lehigh has stated that the smaller particle sizes of MRP increase the impact strength of certain plastic compositions. However, in all applications the particle size and loading levels depend on the target application.\n\n\n"}
{"id": "46997023", "url": "https://en.wikipedia.org/wiki?curid=46997023", "title": "Lerwick District Heating and Energy Recovery Plant", "text": "Lerwick District Heating and Energy Recovery Plant\n\nLerwick District Heating and Energy Recovery Plant is a district heating scheme based in Lerwick, Shetland and operated by the Shetland Islands Council. There is an additional input of heat from a private biomass plant. The main offices are located close to the Lerwick Power Station along with a 12 MWh heat accumulator. Heat is not provided from the power station as it is due to be closed.\n\nThe district heating scheme has over 30 km of mains pipes connecting to 1,200 properties. These properties range from hospitals, schools, leisure centre, industry, retail to housing.\n\nThe energy recovery plant burns 22,000 tonnes of waste per year from Shetland, Orkney and offshore and heats water for the district heating scheme.\n\nIt is hoped that heat will be available from the proposed new power station due to be built by 2019. However this cannot be guaranteed as there is a possibility that an interconnector cable, Shetland HVDC Connection will be laid to Shetland from the Scottish Mainland over 100 km away. The power station would still have to be built for security of supply. In the meantime a 2 MW electric powered heat pump to abstract heat from the sea is being planned for 2016. Another possibility being considered is electric boilers powered by wind turbines which could help grid stabilization.\n"}
{"id": "39408689", "url": "https://en.wikipedia.org/wiki?curid=39408689", "title": "Linkou Power Plant", "text": "Linkou Power Plant\n\nThe Linkou Power Plant () is a coal-fired power plant in Linkou District, New Taipei, Taiwan. With the previous total installed capacity of 600 MW, the power plant used to be the smallest coal-fired power plant in Taiwan. The power plant is currently undergoing retrofitting to increase its installed generation capacity to 2.4 GW.\n\nThe power plant began its operation on 18 July 1968 after a successful train run of its first 300 MW giant electric generator which started two weeks before.\n\nOn 1 September 2014, the current two unit generators were decommissioned.\n\nOn 6 October 2016, the plant completed its refurbishment of its old two units and commissioned the new one supercritical unit of 800 MW.\n\nOn 24 March 2017, the second of the 800 MW unit was commissioned.\n\nOne 800 MW ultra supercritical coal-fired unit is currently being built by Mitsubishi Heavy Industries and CTCI Corporation at the plant. The unit is expected to be commissioned in 2019.\n\nLinkou Power Plant is accessible north of Shanbi Station of Taoyuan Metro.\n\n"}
{"id": "412942", "url": "https://en.wikipedia.org/wiki?curid=412942", "title": "Lipopolysaccharide", "text": "Lipopolysaccharide\n\nLipopolysaccharides (LPS), also known as lipoglycans and endotoxins, are large molecules consisting of a lipid and a polysaccharide composed of O-antigen, outer core and inner core joined by a covalent bond; they are found in the outer membrane of Gram-negative bacteria.\n\nThe term \"lipooligosaccharide\" (\"LOS\") is used to refer to a low-molecular-weight form of bacterial lipopolysaccharides.\n\nThe toxic activity of LPS was first discovered and termed \"endotoxin\" by Richard Friedrich Johannes Pfeiffer, who distinguished between exotoxins, which he classified as a toxin that is released by bacteria into the surrounding environment, and endotoxins, which he considered to be a toxin kept \"within\" the bacterial cell and released only after destruction of the bacterial cell wall. Subsequent work showed that release of LPS from gram negative microbes does not necessarily require the destruction of the bacterial cell wall, but rather, LPS is secreted as part of the normal physiological activity of membrane vesicle trafficking in the form of bacterial outer membrane vesicles (OMVs), which may also contain other virulence factors and proteins.\n\nToday, the term 'endotoxin' is mostly used synonymously with LPS, although there are a few endotoxins that are not related to LPS, such as the so-called delta endotoxin proteins secreted by \"Bacillus thuringiensis\".\n\nLPS is the major component of the outer membrane of Gram-negative bacteria, contributing greatly to the structural integrity of the bacteria, and protecting the membrane from certain kinds of chemical attack. LPS also increases the negative charge of the cell membrane and helps stabilize the overall membrane structure. It is of crucial importance to many Gram-negative bacteria, which die if it is mutated or removed; however, it appears that LPS is nonessential in at least some Gram-negative bacteria, such as \"Neisseria meningitidis\", \"Moraxella catarrhalis\", and \"Acinetobacter baumannii\". LPS induces a strong response from normal animal immune systems. It has also been implicated in non-pathogenic aspects of bacterial ecology, including surface adhesion, bacteriophage sensitivity, and interactions with predators such as amoebae.\n\nLPS is required for the proper conformation of Omptin activity; however, smooth LPS will sterically hinder omptins.\n\nIt comprises three parts: \n\nA repetitive glycan polymer contained within an LPS is referred to as the O antigen, O polysaccharide, or O side-chain of the bacteria.\nThe O antigen is attached to the core oligosaccharide, and comprises the outermost domain of the LPS molecule. The composition of the O chain varies from strain to strain. For example, there are over 160 different O antigen structures produced by different \"E. coli\" strains. The presence or absence of O chains determines whether the LPS is considered rough or smooth. Full-length O-chains would render the LPS smooth, whereas the absence or reduction of O-chains would make the LPS rough. Bacteria with rough LPS usually have more penetrable cell membranes to hydrophobic antibiotics, since a rough LPS is more hydrophobic. O antigen is exposed on the very outer surface of the bacterial cell, and, as a consequence, is a target for recognition by host antibodies.\n\nThe Core domain always contains an oligosaccharide component that attaches directly to lipid A and commonly contains sugars such as heptose and 3-Deoxy-D-manno-oct-2-ulosonic acid (also known as KDO, keto-deoxyoctulosonate). The LPS Cores of many bacteria also contain non-carbohydrate components, such as phosphate, amino acids, and ethanolamine substituents.\n\nLipid A is, in normal circumstances, a phosphorylated glucosamine disaccharide decorated with multiple fatty acids. These hydrophobic fatty acid chains anchor the LPS into the bacterial membrane, and the rest of the LPS projects from the cell surface. The lipid A domain is responsible for much of the toxicity of Gram-negative bacteria. When bacterial cells are lysed by the immune system, fragments of membrane containing lipid A are released into the circulation, causing fever, diarrhea, and possible fatal endotoxic shock (also called septic shock). The Lipid A moiety is a very conserved component of the LPS. However Lipid A structure varies among bacterial species and Lipid A structure defines an overall host immune activation.\n\nLipooligosaccharides (LOS) are glycolipids found in the outer membrane of some types of Gram-negative bacteria, such as \"Neisseria\" spp. and \"Haemophilus\" spp. The term is synonymous with the low molecular weight form of bacterial LPS. LOS plays a central role in maintaining the integrity and functionality of the outer membrane of the Gram negative cell envelope.\nLipooligosaccharides play an important role in the pathogenesis of certain bacterial infections because they are capable of acting as immunostimulators and immunomodulators. Furthermore, LOS molecules are responsible for the ability of some bacterial strains to display molecular mimicry and antigenic diversity, aiding in the evasion of host immune defenses and thus contributing to the virulence of these bacterial strains.\n\nChemically, lipooligosaccharides lack O-antigens and possess only a lipid A-based outer membrane-anchoring moiety, and an oligosaccharide core. In the case of \"Neisseria meningitidis\", the lipid A portion of the molecule has a symmetrical structure and the inner core is composed of 3-deoxy-D-manno-2-octulosonic acid (KDO) and heptose (Hep) moieties. The outer core oligosaccharide chain varies depending on the bacterial strain.\nThe term lipooligosaccharide is used to refer to the low molecular weight form of bacterial lipopolysaccharides, which can be categorized into two forms: the high molecular weight (Mr, or smooth) form possesses a high molecular weight, repeating polysaccharide O-chain, while the low molecular weight (low-Mr or rough) form, lacks the O-chain but possesses a short oligosaccharide in its place.\n\nThe making of LPS can be modified in order to present a specific sugar structure. Those can be recognised by either other LPS (which enables to inhibit LPS toxins) or glycosyltransferases that use those sugar structure to add more specific sugars. \nIt has recently been shown that a specific enzyme in the intestine (alkaline phosphatase) can detoxify LPS by removing the two phosphate groups found on LPS carbohydrates. This may function as an adaptive mechanism to help the host manage potentially toxic effects of gram-negative bacteria normally found in the small intestine. A different enzyme may detoxify LPS when it enters, or is produced in, animal tissues. Neutrophils, macrophages, and dendritic cells produce a lipase, acyloxyacyl hydrolase (AOAH), that inactivates LPS by removing the two secondary acyl chains from lipid A. If they are given LPS parenterally, mice that lack AOAH develop high titers of non-specific antibodies, develop prolonged hepatomegaly, and experience prolonged endotoxin tolerance. LPS inactivation may be required for animals to restore homeostasis after parenteral LPS exposure.\n\nLPS acts as the prototypical endotoxin because it binds the CD14/TLR4/MD2 receptor complex in many cell types, but especially in monocytes, dendritic cells, macrophages and B cells, which promotes the secretion of pro-inflammatory cytokines, nitric oxide, and eicosanoids.\n\nAs part of the cellular stress response, superoxide is one of the major reactive oxygen species induced by LPS in various cell types that express TLR (toll-like receptor).\n\nLPS is also an exogenous pyrogen (fever-inducing substance).\n\nBeing of crucial importance to Gram-negative bacteria, these molecules make candidate targets for new antimicrobial agents.\n\nSome researchers doubt reports of generalized toxic effects attributed to all lipopolysaccharides, in particular, for cyanobacteria.\n\nLPS function has been under experimental research for several years due to its role in activating many transcription factors. LPS also produces many types of mediators involved in septic shock. Humans are much more sensitive to LPS than other animals (e.g., mice). A dose of 1 µg/kg induces shock in humans, but mice will tolerate a dose up to a thousand times higher. This may relate to differences in the level of circulating natural antibodies between the two species.\nSaid et al. showed that LPS causes an IL-10-dependent inhibition of CD4 T-cell expansion and function by up-regulating PD-1 levels on monocytes which leads to IL-10 production by monocytes after binding of PD-1 by PD-L1.\n\nEndotoxins are in large part responsible for the dramatic clinical manifestations of infections with pathogenic Gram-negative bacteria, such as \"Neisseria meningitidis\", the pathogens that causes meningococcal disease, including meningococcemia, Waterhouse–Friderichsen syndrome, and meningitis.\n\nBruce Beutler was awarded a portion of the 2011 Nobel Prize in Physiology or Medicine for his work demonstrating that TLR4 is the LPS receptor.\n\nPortions of the LPS from several bacterial strains have been shown to be chemically similar to human host cell surface molecules; the ability of some bacteria to present molecules on their surface which are chemically identical or similar to the surface molecules of some types of host cells is termed molecular mimicry. For example, in \"Neisseria meningitidis\" L2,3,5,7,9, the terminal tetrasaccharide portion of the oligosaccharide (lacto-N-neotetraose) is the same tetrasaccharide as that found in paragloboside, a precursor for ABH glycolipid antigens found on human erythrocytes. In another example, the terminal trisaccharide portion (lactotriaose) of the oligosaccharide from pathogenic \"Neisseria\" spp. LOS is also found in lactoneoseries glycosphingolipids from human cells. Most meningococci from groups B and C, as well as gonococci, have been shown to have this trisaccharide as part of their LOS structure. The presence of these human cell surface 'mimics' may, in addition to acting as a 'camouflage' from the immune system, play a role in the abolishment of immune tolerance when infecting hosts with certain human leukocyte antigen (HLA) genotypes, such as HLA-B35.\n\nRecently, a new study published has discovered that LPS can be sensed directly by hematopoietic stem cells (HSCs) through the bonding with TLR4, causing them to proliferate in reaction to a systemic infection. This response activate the TLR4-TRIF-ROS-p38 signaling within the HSCs and through a sustained TLR4 activation can cause a proliferative stress, leading to impair their competitive repopulating ability. Infection in mice using \"S. typhimurium\" showed similar results, validating the experimental model also in vivo.\n\nO-antigens (the outer carbohydrates) are the most variable portion of the LPS molecule, imparting the antigenic specificity. In contrast, lipid A is the most conserved part. However, lipid A composition also may vary (e.g., in number and nature of acyl chains even within or between genera). Some of these variations may impart antagonistic properties to these LPS. For example, Rhodobacter sphaeroides diphosphoryl lipid A (RsDPLA) is a potent antagonist of LPS in human cells, but is an agonist in hamster and equine cells.\n\nIt has been speculated that conical Lipid A (e.g., from \"E. coli\") are more agonistic, less conical lipid A like those of \"Porphyromonas gingivalis\" may activate a different signal (TLR2 instead of TLR4), and completely cylindrical lipid A like that of \"Rhodobacter sphaeroides\" is antagonistic to TLRs.\n\nLPS gene clusters are highly variable between different strains, subspecies, species of bacterial pathogens of plants and animals.\n\nNormal human blood serum contains anti-LOS antibodies that are bactericidal and patients that have infections caused by serotypically distinct strains possess anti-LOS antibodies that differ in their specificity compared with normal serum. These differences in humoral immune response to different LOS types can be attributed to the structure of the LOS molecule, primarily within the structure of the oligosaccharide portion of the LOS molecule.\nIn \"Neisseria gonorrhoeae\" it has been demonstrated that the antigenicity of LOS molecules can change during an infection due to the ability of these bacteria to synthesize more than one type of LOS, a characteristic known as phase variation. Additionally, \"Neisseria gonorrhoeae\", as well as \"Neisseria meningitidis\" and \"Haemophilus influenzae\", are capable of further modifying their LOS \"in vitro\", for example through sialylation (modification with sialic acid residues), and as a result are able to increase their resistance to complement-mediated killing or even down-regulate complement activation or evade the effects of bactericidal antibodies. Sialylation may also contribute to hindered neutrophil attachment and phagocytosis by immune system cells as well as a reduced oxidative burst. \"Haemophilus somnus\", a pathogen of cattle, has also been shown to display LOS phase variation, a characteristic which may help in the evasion of bovine host immune defenses.\nTaken together, these observations suggest that variations in bacterial surface molecules such as LOS can help the pathogen evade both the humoral (antibody and complement-mediated) and the cell-mediated (killing by neutrophils, for example) host immune defenses.\n\nRecently, it was shown that in addition to TLR4 mediated pathways, certain members of the family of the transient receptor potential ion channels recognize LPS. LPS-mediated activation of TRPA1 was shown in mice and \"Drosophila melanogaster\" flies. At higher concentrations, LPS activates other members of the sensory TRP channel family as well, such as TRPV1, TRPM3 and to some extent TRPM8. \nLPS is recognized by TRPV4 on epithelial cells. TRPV4 activation by LPS was necessary and sufficient to induce nitric oxide production with a bactericidal effect.\n\nThe presence of endotoxins in the blood is called endotoxemia. It can lead to septic shock, if the immune response is severely pronounced.\n\nMoreover, endotoxemia of intestinal origin, especially, at the host-pathogen interface, is considered to be an important factor in the development of alcoholic hepatitis, which is likely to develop on the basis of the small bowel bacterial overgrowth syndrome and an increased intestinal permeability.\n\nLipid A may cause uncontrolled activation of mammalian immune systems with production of inflammatory mediators that may lead to septic shock. This inflammatory reaction is mediated by Toll-like receptor 4 which is responsible for immune system cell activation. Damage to the endothelial layer of blood vessels caused by these inflammatory mediators can lead to capillary leak syndrome, dilation of blood vessels and a decrease in cardiac function and can lead to septic shock. Pronounced complement activation can also be observed later in the course as the bacteria multiply in the blood. High bacterial proliferation triggering destructive endothelial damage can also lead to disseminated intravascular coagulation (DIC) with loss of function of certain internal organs such as the kidneys, adrenal glands and lungs due to compromised blood supply. The skin can show the effects of vascular damage often coupled with depletion of coagulation factors in the form of petechiae, purpura and ecchymoses. The limbs can also be affected, sometimes with devastating consequences such as the development of gangrene, requiring subsequent amputation. Loss of function of the adrenal glands can cause adrenal insufficiency and additional hemorrhage into the adrenals causes Waterhouse-Friderichsen syndrome, both of which can be life-threatening.\nIt has also been reported that gonococcal LOS can cause damage to human fallopian tubes.\n\nThe molecular mimicry of some LOS molecules is thought to cause autoimmune-based host responses, such as flareups of multiple sclerosis. Other examples of bacterial mimicry of host structures via LOS are found with the bacteria \"Helicobacter pylori\" and \"Campylobacter jejuni\", organisms which cause gastrointestinal disease in humans, and \"Haemophilus ducreyi\" which causes chancroid. Certain \"C. jejuni\" LPS serotypes (attributed to certain tetra- and pentasaccharide moieties of the core oligosaccharide) have also been implicated with Guillain–Barré syndrome and a variant of Guillain–Barré called Miller-Fisher syndrome.\n\nEpidemiological studies have previously shown that increased endotoxin load, which can be a result of increased populations of endotoxin producing bacteria in the intestinal tract, is associated with certain obesity-related patient groups. Other studies have shown that purified endotoxin from \"Escherichia coli\" can induce obesity and insulin-resistance when injected into germ-free mouse models. A more recent study has uncovered a potentially contributing role for \"Enterobacter cloacae\" B29 toward obesity and insulin resistance in a human patient. The presumed mechanism for the association of endotoxin with obesity is that endotoxin induces an inflammation-mediated pathway accounting for the observed obesity and insulin resistance. There is a correlation between plasma LPS and insulin resistance.\n\nBacterial genera associated with endotoxin-related obesity effects: \"Escherichia\", \"Enterobacter\"\n\nLipopolysaccharides are frequent contaminants in plasmid DNA prepared from bacteria or proteins expressed from bacteria, and \"must\" be removed from the DNA or protein to avoid contaminating experiments and to avoid toxicity of products manufactured using industrial fermentation.\n\nAlso, ovalbumin is frequently contaminated with endotoxins. Ovalbumin is one of the extensively studied proteins in animal models and also an established model allergen for airway hyper-responsiveness (AHR). Commercially available ovalbumin that is contaminated with LPS can fully activate endothelial cells in an in-vitro assay of the first step of inflammation, and it falsifies research results, as it does not accurately reflect the effect of sole protein antigen on animal physiology.\n\nIn pharmaceutical production, it is necessary to remove all traces of endotoxin from drug product containers, as even small amounts of endotoxin will cause illness in humans. A depyrogenation oven is used for this purpose. Temperatures in excess of 300 °C are required to break down this substance. A defined endotoxin reduction rate is a correlation between time and temperature. Based on primary packaging material as syringes or vials, a glass temperature of 250 °C and a holding time of 30 minutes is typical to achieve a reduction of endotoxin levels by a factor of 1000.\n\nThe standard assay for detecting presence of endotoxin is the Limulus Amebocyte Lysate (LAL) assay, utilizing blood from the Horseshoe crab (\"Limulus polyphemus\"). Very low levels of LPS can cause coagulation of the limulus lysate due to a powerful amplification through an enzymatic cascade. However, due to the dwindling population of horseshoe crabs, and the fact that there are factors that interfere with the LAL assay, efforts have been made to develop alternative assays, with the most promising ones being ELISA tests using a recombinant version of a protein in the LAL assay, Factor C.\n\n"}
{"id": "48716248", "url": "https://en.wikipedia.org/wiki?curid=48716248", "title": "Ministry of Energy and Mineral Resources (Indonesia)", "text": "Ministry of Energy and Mineral Resources (Indonesia)\n\nThe Ministry of Energy and Mineral Resources of the Republic of Indonesia (Indonesian \"Kementerian Energi dan Sumber Daya Mineral Republik Indonesia\") is an Indonesian ministry responsible for providing assist to the President and Vice President in performing government's affairs in the field of energy and mineral resources. The current minister is Ignasius Jonan.\n\n"}
{"id": "38609955", "url": "https://en.wikipedia.org/wiki?curid=38609955", "title": "Molybdenum bronze", "text": "Molybdenum bronze\n\nIn chemistry, molybdenum bronze is a generic name for certain mixed oxides of molybdenum with the generic formula where A may be hydrogen, an alkali metal cation (such as Li, Na, K), and Tl. These compounds form deeply coloured plate-like crystals with a metallic sheen, hence their name. These bronzes derive their metallic character partially occupied 4d-bands. The oxidation state in KMoO are K, O, and Mo. MoO is an insulator, with an unfilled 4d band.\n\nThese compounds have been much studied since the 1980s due to their markedly anisotropic electrical properties, reflecting their layered structure. The electrical resistivity can vary considerably depending on the direction, in some cases by 200:1 or more. They are generally non-stoichiometric compounds. Some are metals and some are semiconductors.\n\nThe first report of a \"molybdenum bronze\" was by Alfred Stavenhagen and E. Engels in 1895. They reported that electrolysis of molten and gave indigo-blue needles with metallic sheen, which they analysed by weight as . The first unambiguous synthesis of alkali molybdenum bronzes was reported only in 1964, by Wold and others. They obtained two potassium bronzes, \"red\" and \"blue\" , by electrolysis of molten + at 550 °C and 560 °C, respectively. Sodium bronzes were also obtained by the same method. It was observed that at a slightly higher temperature (about 575 °C and above) only is obtained.\n\nAnother preparation technique involves crystallization from the melt in a temperature gradient. This report also called attention to the marked anisotropic resistivity of the purple lithium bronze and its metal-to-insulator transition at about 24 K.\n\nHydrogen bronzes were obtained in 1950 by Glemser and Lutz, by ambient-temperature reactions. The hydrogen in these compounds can be replaced by alkali metals by treatment with solutions of the corresponding halides. Reactions are conducted in an autoclave at about 160 °C.\n\nMolybdenum bronzes are classified in three major families:\n\nThe hydrogen molydbdenum bronzes have similar appearances but different compositions:\n\nOther molybdenum bronzes with anomalous electrical properties have been reported, which do not fit in these families. These include \n\n"}
{"id": "57740761", "url": "https://en.wikipedia.org/wiki?curid=57740761", "title": "Museum of Woodworking", "text": "Museum of Woodworking\n\nMuseum of Woodworking ( or Ahşap Eserler Galerisi) is an art museum and gallery dedicated to wood working. It is located in Eskişehir, Turkey.\n\nThe museum and art gallery was established by the Municipality of Odunpazarı in the context of the International Wood Carved Sculptures Festival hosted. It was opened on May 25, 2016. It was situated in the Seramik Park in Odunpazarı district, before it was relocated to its current place at Kurşunlu Mosque and Complex in May 2017. There are 160 wooden articles on display, which were handcrafted for the 3rd International Wodworking Festival hosted 2015 in Turkey, and for the 2016 International Wood Carved Sculptures Festival. The 4th International Wood Carved Sculptures Festival was held at the museum in May 2018 under the motto \"The Voice of The Wood\" ().\n\nThe museum is open to public every day, but Mondays, between 10:00 and 18:00 hours.\n"}
{"id": "49224757", "url": "https://en.wikipedia.org/wiki?curid=49224757", "title": "Musser Lumber Company", "text": "Musser Lumber Company\n\nMusser Lumber Company was a US lumber company, and one of Iowa's pioneer lumber concerns. In its day, it was one of the most complete and modern lumber mills on the Mississippi River, employing about 400 men during the working season, and between 200 and 300 men in the logging end of the business.\n\nThe Musser Lumber Company of Muscatine, Iowa, was incorporated in February, 1881, with a capital stock of $200,000. Peter Musser was elected president; Richard Musser, vice-president; P. M. Musser, secretary and treasurer; and C. R. Fox, yard and planing mill superintendent. This company was an outgrowth of the pioneer lumber firm of R. Musser & Company, which was established by Peter and Richard Musser and Edward Hoch in 1855. These gentlemen began by buying rafts of lumber in the river, which they yarded and sold by retail and Wholesale. They also established a lumber yard at Iowa City in 1856, where Peter Musser resided, it being managed by that gentleman until he went to California, in April, 1864. Hoch continued in the firm for three years until 1858, when he retired, and the firm became R. Musser & Company. In 1864, P. M. Musser bought into the business, without any change of firm name; he assumed charge of the Iowa City yard, which he conducted until 1873, when he and John Porter purchased the interest of R. Musser & Company in the branch yard at Iowa City. They carried on under the firm name of Musser & Porter, with P. M. Musser still retaining his interest in the business in Muscatine. In 1870, having returned from California, Peter Musser joined his brother, Richard, together with C. R. Fox, a former employee, and P. M. Musser, in the organization of the firm of Musser & Company for the purpose of building and operating a saw mill and carrying on the lumber business at Muscatine. In the spring of 1871, they completed their mill at a point on the Mississippi River, since known as Musserville, which was included in the corporate limits of the city of Muscatine, and began the manufacture of lumber. The mill cut about of lumber a year. In 1873, Richard Musser retired from the business, selling his interest to P. M. Musser, and the firm became P. M. Musser & Company. Three years later, Richard Musser renewed his connection with the company, and the firm name of Musser & Company was again adopted.\n\nIn 1877, the company enlarged their mill, putting in improved machinery and increasing its capacity materially. Four years later, the proprietors decided to organize under an act of incorporation, which was done in February, 1881, under the name of the Musser Lumber Company, the incorporators being the original proprietors, R. Musser, P. Musser, P. M. Musser and C. R. Fox of Muscatine, and John Musser of Adamstown, Pennsylvania, and the officers elected being the same as given above. During the same year, the company rebuilt and enlarged the mill, which they fitted with the most modern and improved machinery, making it one of the most complete and capacious saw mills on the Mississippi River, with capacity for manufacturing of lumber, of lath and of shingles for the working season. The power was furnished by two engines, the combined force of which equaled 500 horse-power. In 1882, the company erected a planing mill, detached from the saw mill, which afforded facilities for dressing lumber and the manufacture of flooring, siding and boards. The business of this company was strictly wholesale, and their trade was principally located in the states of Iowa, Nebraska, Kansas, Missouri, Colorado and Dakota. The number of people employed at their mills and yards was about 300.\n\nThe Musser Lumber Company owned large tracts of pine lands along the Chippewa in Wisconsin and in the Minnesota lumber region; their logs were cut by contract and were rafted by the Chippewa Logging Company and the Mississippi River Logging Company, two large lumber corporations, of which the Musser Lumber Company was one of the incorporators. For about 12 years, this company owned and operated its own rafting steamers, it having operated the “Silver Wave” and “Le Clare Belle\" during all that time, and the steamer, “Musser,” for two years. All were sold during the season of 1888 to Capt. S. R. Van Sant. The company’s plant covered an area of thirty-five acres, and the yards were stocked according to the season of the year, in the amount of of lumber and a proportionate amount of lath and shingles. To convey an idea of the growth of the business from the date of its origin with the firm of Hoeh & Musser, it was reported that the amount of business done the first year was about US$100,000, while the 1898 annual business of the company exceeded $750,000. This corporation had its own system of water-works, which was established in 1872, and in which a Duplex pump and Cameron pump, was used. This company also had one of Barry’s Niagara rotary hydrants, which could throw a three-inch stream a distance of about and to a height of , and thus the supply of water and force was equal to emergencies which might arise.\n\nThe Mussers were also the original incorporators of the Mississippi River Logging Company and Chippewa Lumber and Boom Company; they were also largely interested in the Musser-Sauntry Land, Logging and Manufacturing Company of Stillwater, Minnesota, among the largest pine land owners and logging companies in the world.\n\nThe aggregate cut of the mills at Muscatine was reported in 1875 at 38,000,000 feet, with 21,000,000 shingles; that of 1880 at 55,000,000 feet, with 18,000,000 shingles, while that of 1897 reached 110,000,000 feet of lumber, 30,000,000 shingles and 25,000,000 lath.\n\nThe Musser Lumber Company, one of Iowa's pioneer lumber concerns, closed up its business at Muscatine on November 28, 1916. Following a meeting of the directors of the lumber concern, it was decided to dissolve the company and formal announcement of the dissolution was made on that date by P. M. Musser, president and treasurer. The officers of the company up to the time of the dissolution were: P. M. Musser, president and treasurer; William Musser, vice-president, and R. Drew Musser, secretary.\n\n"}
{"id": "13796322", "url": "https://en.wikipedia.org/wiki?curid=13796322", "title": "National Association of Electrical Distributors", "text": "National Association of Electrical Distributors\n\nThe National Association of Electrical Distributors (NAED) is a trade association for the American electrical equipment distribution industry. NAED is a 501(c)6 non-profit organization dedicated to serving and protecting the electrical distribution channel.\n\nNAED provides networking opportunities through approximately 20 meetings & conferences a year, training and research through the NAED Education & Research Foundation, industry information and research through TED Magazine. The year 2008 marks the association's 100th anniversary.\n\nNAED has three membership categories: distributor member, associate member and allied partners. A company must be an electrical distributor and meet NAED membership requirements. To become an NAED associate member, a company must be a manufacturer or value-added reseller. To become an NAED allied partner a company must be a marketing group, service or technology organization and must meet NAED criteria.\n\n\n\nSource: NAED database, April 2007\nNAED also runs an Education and Research Foundation. The Foundation provides information and educational resources for distributors, manufacturers, and customers in the electrical distribution industry.\n\n"}
{"id": "49776733", "url": "https://en.wikipedia.org/wiki?curid=49776733", "title": "Nationalization of the Iranian oil industry", "text": "Nationalization of the Iranian oil industry\n\nThe nationalization of the Iranian oil industry () was a movement in the Iranian parliament (Majlis) to nationalize Iran's oil industry. The legislation was passed on March 15, 1951 and was verified by the Majlis on March 17, 1951. The legislation led to the nationalization of the Anglo-Persian Oil Company (AIOC). The movement was led by Mohammad Mosaddegh, a member of the Majlis for the National Front and future prime minister of Iran. The movement to nationalize the oil industry was the reaction to the following concessions made by Iran to foreign powers: The Reuter concession of 1872, proceeding letter, the 1933 agreement between the Iranian government and AIOC, and the Gas-golshaian contract. According to the political scientist Mark J. Gasiorowski, the oil nationalization movement had two major consequences: the establishment of a democratic government and the pursuit of Iranian national sovereignty.\n\nFrom the time of the discovery of oil in Iran, foreign powers used force and exploited the weakness of the Iranian state to coerce it into concessions which allowed foreign companies to control oil extraction. The nationalization of the oil industry was the response to these foreign interventions. Particularly the following concessions:\n\n\nThe competition to gain more control of the Iranian oil industry increased during World War II when the United Kingdom, the Soviet Union and the United States all became involved in Iranian affairs. When faced with demands from the oil companies of these three countries, the Iranian government announced that the issue would be decided after the war as the economic conditions of the countries were not clear. The government refused all these demands after the war ended.\n\nRahimiyan, the member from Quchan in the 14th Majlis was the first who introduced a plan to nationalize the oil industry. However, this plan was never discussed. On October 23, 1949, at home of Mohammad Mosaddegh and in the presence of twelve experts the National Front (political party) was established. It consisted of various political efforts whose joint objective was the protection of the rights of Iranian oil industry.\n\nMosaddegh (16 June 1882 – 5 March 1967) was an Iranian politician and the leader of the movement to nationalize Iran's oil industry. He was educated in Europe, and joined politics after the Iranian Constitutional Revolution of 1905-1907. He held multiple posts such as member of parliament, governor of the Fars province, finance minister, foreign minister, and prime minister. In the election of the 14th Majlis in 1943, he was elected member for Tehran. Before gaining recognition as the leader of the national oil movement, he played a large role in the trans-Iranian railway project and the re-organization of the courts and the Justice Department.\n\nThe 16th Majlis consisted of some members from National Front such as Mosaddegh. In November 1950, the rejection of the oil supplemental agreement was offered from oil committee of Majlis which was chaired by Mosaddegh. The prime minister at the time, Haj Ali Razmara, opposed the measure. On March 7, 1951, Razmara was murdered by Khalil Tahmasebi, a member of Fada'iyan-e Islam. After the death of Razmara, the Majlis began the process of nationalizing the Iranian oil industry.\n\nOn March 15, 1951, legislation to nationalize the oil industry was passed by the Majlis with a majority of votes. On March 17 the Majlis verified the nationalization of Iran oil industry and the AIOC was nationalized.\n\nIn April, Mosaddegh was selected as prime minister by Shah Mohammad Reza Pahlavi under immense pressure from the Majlis.\n\nIn the aftermath of March 1951, the economic crisis worsened and Iranian oil was not bought by other countries. The Abadan Refinery, at the time one of the largest oil refineries in the world, was closed. The nationalization of the Iranian oil industry continued even through strong opposition from the United States and the United Kingdom.\n\nIn the first year of the nationalization, the only foreign sale of Iranian Oil were 300 barrels to an Italian merchant ship. Foreign oil companies prevented any impacts of the Iranian withdrawal from being felt by consumer countries by increasing output elsewhere. Oil production was expanded by BP and ARAMCO in Saudi Arabia, Kuwait and Iraq. Oil production in the Middle East increased by around 10% annually in 1951, 1952 and 1953. The loss of oil exports had bad effect on the Iranian economy. With oil production decreasing from 242 million barrels in 1950 to 10.6 million barrels in 1952.\n\nIn August 1953, the government of Mosaddegh was overthrown by a military coup detat orchestrated by the United States Central Intelligence Agency and the British Secret Intelligence Service. Mosaddegh was sentenced to three years in prison and then kept under house arrest until his death in 1967.\n\nAfter the coup the Iranian oil crisis ended and the AIOC did not succeed to stop production. The National Iranian oil company as an international consortium was founded and the AIOC was made a member. With the nationalization of the oil industry, British and American political influence continued for years after coup.\n\n"}
{"id": "27956705", "url": "https://en.wikipedia.org/wiki?curid=27956705", "title": "Polymer capacitor", "text": "Polymer capacitor\n\nA polymer capacitor, or more accurately a polymer electrolytic capacitor, is an electrolytic capacitor (e-cap) with a solid electrolyte of a conductive polymer. There are four different types:\n\nPolymer Ta-e-caps are available in rectangular surface-mounted device (SMD) chip style. Polymer Al-e-caps and hybrid polymer Al-e-caps are available in rectangular surface-mounted device (SMD) chip style, in cylindrical SMDs (V-chips) style or as radial leaded versions (single-ended).\n\nPolymer electrolytic capacitors are characterized by particularly low internal equivalent series resistances (ESR) and high ripple current ratings. Their electrical parameters have similar temperature dependence, reliability and service life compared to solid tantalum capacitors, but have a much better temperature dependence and a considerably longer service life than aluminum electrolytic capacitors with non-solid electrolytes. In general polymer e-caps have a higher leakage current rating than the other solid or non-solid electrolytic capacitors.\n\nPolymer electrolytic capacitors are also available in a hybrid construction. The hybrid polymer aluminum electrolytic capacitors combine a solid polymer electrolyte with a liquid electrolyte. These types are characterized by low ESR values but have low leakage currents and are insensitive to transient, however they have a temperature-dependent service life similar to non-solid e-caps.\n\nPolymer electrolytic capacitors are mainly used in power supplies of integrated electronic circuits as buffer, bypass and decoupling capacitors, especially in devices with flat or compact design. Thus they compete with MLCC capacitors, but offer higher capacitance values than MLCC, and they display no microphonic effect (such as class 2 and 3 ceramic capacitors).\n\nAluminum electrolytic capacitors (Al-e-caps) with liquid electrolytes were invented in 1896 by Charles Pollak.\n\nTantalum electrolytic capacitors with solid manganese dioxide (MnO) electrolytes were invented by Bell Laboratories in the early 1950s, as a miniaturized and more reliable low-voltage support capacitor to complement the newly invented transistor, see Tantalum capacitor. The first Ta-e-caps with MnO electrolytes had 10 times better conductivity and a higher ripple current load than earlier types Al-e-caps with liquid electrolyte. Additionally, unlike standard Al-e-caps, the equivalent series resistance (ESR) of Ta-caps is stable in varying temperatures.\n\nDuring the 1970s, the increasing digitization of electronic circuits came with decreasing operating voltages, and increasing switching frequencies and ripple current loads. This had consequences for power supplies and their electrolytic capacitors. Capacitors with lower ESR and lower equivalent series inductance (ESL) for bypass and decoupling capacitors used in power supply lines were needed. see Role of ESR, ESL and capacitance.\n\nA breakthrough came in 1973, with the discovery by A. Heeger and F. Wudl of an organic conductor, the charge-transfer salt TCNQ. TCNQ (7,7,8,8-tetracyanoquinodimethane or N-n-butyl isoquinolinium in combination with TTF (Tetrathiafulvalene)) is a chain molecule of almost perfect one-dimensional structure that has a 10-fold better conductivity along the chains than does MnO, and has a 100-fold better conductivity than non-solid electrolytes.\n\nThe first Al-e-caps to use the charge transfer salt TTF-TCNQ as a solid organic electrolyte was the OS-CON series offered in 1983 from Sanyo. These were wound, cylindrical capacitors with 10x increased electrolyte conductivity compared with MnO\n\nThese capacitors were used in devices for applications that required the lowest possible ESR or highest possible ripple current. One OS-CON e-cap could replace three more bulky \"wet\" e-caps or two Ta-caps. By 1995, the Sanyo OS-CON became the preferred decoupling capacitor for Pentium processor-based IBM personal computers.\nThe Sanyo OS-CON e-cap product line was sold in 2010 to Panasonic. Panasonic then replaced the TCNQ salt with a conducting polymer under the same brand.\n\nThe next step in ESR reduction was the development of conducting polymers by Alan J. Heeger, Alan MacDiarmid and Hideki Shirakawa in 1975. The conductivity of conductive polymers such as polypyrrole (PPy) or PEDOT is better than that of TCNQ by a factor of 100 to 500, and close to the conductivity of metals.\n\nIn 1988 the first polymer electrolyte e-cap, \"APYCAP\" with PPy polymer electrolyte, was launched by the Japanese manufacturer Nitsuko. The product was not successful, in part because it was not available in SMD versions.\n\nIn 1991 Panasonic launched its polymer Al-e-cap series \"SP-Cap\", These e-caps used PPy polymer electrolyte and reached ESR values that were directly comparable to ceramic multilayer capacitors (MLCCs). They were still less expensive than tantalum capacitors and with their flat design useful in compact devices such as laptops and cell phones they competed with tantalum chip capacitors as well.\n\nTantalum electrolytic capacitors with PPy polymer electrolyte cathode followed three years later. In 1993 NEC introduced its SMD polymer Ta-e-caps called \"NeoCap\". In 1997 Sanyo followed with the \"POSCAP\" polymer tantalum chips.\n\nA new conductive polymer for tantalum polymer capacitors was presented by Kemet at the \"1999 Carts\" conference. This capacitor used the newly developed organic conductive polymer PEDT (Poly(3,4-ethylenedioxythiophene)), also known as PEDOT (trade name Baytron®).\n\nTwo years later at the 2001 APEC Conference, Kemet introduced PEDOT polymer aluminum e-caps to the market. PEDOT polymer has a higher temperature stability, and as PEDOT:PSS solution this electrolyte could be inserted only by dipping instead of in-situ polymerization like for PPy which makes the production faster and cheaper. Its AO-Cap series included SMD capacitors with stacked anode in \"D\" size with heights from 1.0 to 4.0 mm, in competition to the Panasonic SP-Caps using PPy at that time.\n\nAround the turn of the millennium hybrid polymer capacitors were developed, which have in addition to the solid polymer electrolyte a liquid electrolyte connecting the polymer layers covering the dielectric layer on the anode and the cathode foil. The non-solid electrolyte provide oxygen for self-healing purposes to reduce the leakage current. In 2001, NIC launched a hybrid polymer e-cap to replace a polymer type at lower price and with lower leakage current. As of 2016 hybrid polymer capacitors are available from multiple manufacturers.\n\nThe predominant application of all electrolytic capacitors is in power supplies. They are used in input and output smoothing capacitors, as decoupling capacitors to circulate the harmonic current in a short loop, as bypass capacitors to shunt AC noise to the ground by bypassing the power supply lines, as backup capacitors to mitigate the drop in line voltage during sudden power demand or as filter capacitor in low-pass filter to reduce switching noises. In these applications, in addition to the size, are the capacitance, the impedance \"Z\", the ESR, and the inductance ESL important electrical characteristics for the functionality of these capacitors in the circuits.\n\nThe change to digital electronic equipment led to the development of switching power supplies with higher frequencies and \"on-board\" DC/DC converter, lower supply voltages and higher supply currents. Capacitors for this applications needed lower ESR values, which at that time with Al-e-caps could only be realized with larger case sizes or by replacement with much more expensive solid Ta-caps.\n\nThe reason how the ESR influences the functionality of an integrated circuit is simple. If the circuit, f. e. a microprocessor, has a sudden power demand, the supply voltage drops by ESL, ESR and capacitance charge loss. Because in case of a sudden current demand the voltage of the power line drops:\n\nFor example:\n\nGiven a supply voltage of 3 V, with a tolerance of 10% (300 mV) and supply current of a maximum of 10 A, a sudden power demand drops the voltage by\n\nThis means that the ESR in a CPU power supply must be less than 30 mΩ, otherwise the circuit malfunctions.\nSimilar rules are valid for capacitance and ESL. The specific capacitance could be increased over the years by higher etched anode foils respectively by smaller and finer tantalum powder grains by a factor of 10 to 15 and could follow the trend of miniaturizing. The ESL challenge has led to the stacked foil versions of polymer Al e-caps. However, for lowering the ESR only the development of new, solid conductive materials, first TCNQ, after that the conductive polymers, which led to the development of the polymer electrolyte capacitors with their very low ESR values, the ESR challenge of digitization of electronic circuits could be accepted.\n\nElectrolytic capacitors use a chemical feature of some special metals, earlier called \"valve metals\", that by anodic oxidation form an insulating oxide layer. By applying a positive voltage to the anode (+) material in an electrolytic bath an oxide barrier layer with a thickness corresponding to the applied voltage can be formed. This oxide layer acts as the dielectric in an e-cap. To increase the capacitors capacitance the anode surface is roughened and so the oxide layer surface also is roughened. To complete a capacitor a counter electrode has to match the rough insulating oxide surface. This is accomplished by the electrolyte, which acts as the cathode (-) electrode of an electrolytic capacitor.\nThe main difference between the polymer capacitors is the anode material and its oxide used as the dielectric:\n\n\nThe properties of the aluminum oxide layer compared with tantalum pentoxide dielectric layer are given in the following table:\n\nEvery e-cap in principle forms a \"plate capacitor\" whose capacitance is an increasing function of the electrode area A, the permittivity ε of the dielectric material and the thickness of the dielectric (d).\nCapacitance is proportional to the product of the area of one plate multiplied by the permittivity and divided by the dielectric thickness.\n\nThe dielectric thickness is in the range of nanometers per volt. On the other hand, the breakdown voltage of these oxide layers is quite high. Using etched or sintered anodes, with their much higher surface area compared to a smooth surface of the same size or volume, e-caps can achieve a high volumetric capacitance. The latest developments in high etched or sintered anodes increases the capacitance value, depending on the rated voltage, by a factor of up to 200 for Al-e-caps or Ta-e-caps compared with smooth anodes.\nBecause the forming voltage defines the oxide thickness, the desired voltage tolerance can be easily produced. Therefore, the volume of a capacitor is defined by the product of capacitance and voltage, the so-called \"CV product\".\n\nComparing the dielectric constants of tantalum and aluminum oxides, Ta2O5 has permittivity approximately 3-fold higher than Al2O3. Ta-caps therefore theoretically can be smaller than Al-caps with the same capacitance and rated voltage. \nFor real tantalum electrolytic capacitors, the oxide layer thicknesses are much thicker than the rated voltage of the capacitor actually requires. This is done for safety reasons to avoid shorts coming from field crystallization. For this reason the real differences of sizes that derive from the different permittivities, are partially ineffective.\n\nThe most important electrical property of an electrolyte in an electrolytic capacitor is its electrical conductivity. The electrolyte forms the counter electrode, of the e-cap, the cathode. The roughened structures of the anode surface continue in the structure of the oxide layer, the dielectric, the cathode must adapt precisely to the roughened structure. With a liquid, as in the conventional \"wet\" e-caps that is easy to achieve. In polymer e-caps in which a solid conductive polymer forms the electrolyte, this is much more difficult to achieve, because its conductivity comes by a chemical process of polymerization. However, the benefits of a solid polymer electrolyte, the significantly lower ESR of the capacitor and the low temperature dependence of the electrical parameters, in many cases justify the additional production steps as well as higher costs.\n\nElectrolytic capacitors with the charge transfer salt tetracyanoquinodimethane TCNQ as electrolyte, formerly produced by Sanyo with the trade name \"OS-CON\", in the true sense of the term \"polymer\" were not \"polymer capacitors\". TCNQ electrolytic capacitors are mentioned here to point out the danger of confusion with 'real' polymer capacitors, which are sold nowadays under the same trade name OS-CON. The original OS-CON capacitors with TCNQ electrolyte sold by the former manufacturer Sanyo has been discontinued with the integration of Sanyo capacitor businesses by Panasonic 2010. Panasonic keep the trade name OS-CON but change the TCNQ electrolyte into a conductive polymer electrolyte (PPy).\n\nElectrolytic capacitors with TCNQ electrolyte are not available anymore.\n\nPolymers are formed by a chemical reaction, polymerization. In this reaction monomers are continuously attached to a growing polymer strand. Usually polymers are electrically insulators, at best, semiconductors. For use as an electrolyte in e-caps, electrical conductive polymers are employed. The conductivity of a polymer is obtained by conjugated double bonds which permit free movement of charge carriers in the doped state. As charge carriers serve electron holes. That means, the conductivity of conducting polymers, which is nearly comparable with metallic conductors, only starts when the polymers are doped oxidatively or reductively.\n\nA polymer electrolyte must be able to penetrate the anode's finest pores to form a complete, homogeneous layer, because only anode oxide sections covered by the electrolyte contribute to the capacitance. For this the precursors of the polymer has to consist of very small base materials that can penetrate even the smallest pores. The size of this precursors are the limiting factor in the size of the pores in the etched aluminum anode foils or of the size of tantalum powder. The rate of polymerization must be controlled for capacitor manufacturing. Too rapid polymerization does not lead to a complete anode coverage, while too slow polymerization increases production costs. Neither the precursors nor the polymer or its residues may attack the anodes oxide chemically or mechanically. The polymer electrolyte must have high stability over a wide temperature range over long times. The polymer film is not only the counter electrode of the e-cap it also protects the dielectric even against external influences such as the direct contact of graphite in this capacitors, which are provided with a cathode contact via graphite and silver.\n\nPolymer e-caps employ either polypyrrole (PPy) or polythiophene (PEDOT or PEDT)\n\nPolypyrrole (PPy) is a conducting polymer formed by oxidative polymerization of pyrrole. A suitable oxidizing agent is iron (III) chloride (FeCl3). Water, methanol, ethanol, acetonitrile and other polar solvents may be used for the synthesis of PPy. As a solid conducting polymer electrolyte It reaches conductivities up to 100 S/m. Polypyrrole was the first conductive polymer used in polymer Al-e-caps as well as in polymer Ta-e-caps.\n\nThe problem with the polymerization of PPy was the rate of polymerization. When pyrrole is mixed with the desired oxidizing agents at room temperature, the polymerization reaction begins immediately. Thus polypyrrole begins to form, before the chemical solution can get into the anode pores. The polymerization rate can be controlled by cryogenic cooling or by electrochemically polymerization.\n\nThe cooling method requires a very great technical effort and is unfavorable for mass production. In the electrochemical polymerization at first an auxiliary electrode layer on the dielectric has to be applied and to be connected to the anode. For this purpose, ionic dopants are added to the basic substances of the polymer, forming a conductive surface layer on the dielectric during the first impregnation. During subsequent impregnation cycles, the in-situ polymerization can be time-controlled by the current flow after applying a voltage between the anode and cathode. With this method a fine and stable polypyrrole film on the dielectric oxide layer of the anode can be realized. However, both methods of in situ polymerization are complex and require multiple repetition polymerization steps that increase manufacturing costs.\n\nThe polypyrrole electrolyte has two fundamental disadvantages. It is toxic in the production of capacitors and becomes unstable at the higher soldering temperatures required for soldering with lead-free solders.\n\nPoly(3,4-ethylenedioxythiophene), abbreviated PEDOT or PEDT is a conducting polymer based on 3,4-ethylenedioxythiophene or EDOT monomer. PEDOT is polarized by the oxidation of EDOT with catalytic amounts of iron (III) sulfate. The re-oxidation of iron is given by Sodium persulfate. Advantages of PEDOT are optical transparency in its conducting state, non toxic, stable up to temperatures of 280 °C and a conductivity up to 500 S/m. Its heat resistance allows polymer capacitors to be manufactured that withstand the higher temperatures required for lead-free soldering. Additional this capacitors have better ESR values as polymer e-caps with PPy electrolyte.\n\nThe difficult methods of in situ polymerization of PEDOT in the anodes of the capacitors initially were at first the same as with polypyrrole. This changed with the development of pre-polymerized dispersions of PEDOT in which the capacitor anodes simple could be dipped and then dried at room temperature. For this purpose, the PEDOT chemicals is added with sodium polystyrene sulfonate (PSS) and dissolved in water. The complete polymer layer on the dielectric is then composed of pre-polymerized particles from the dispersion. These dispersions are known as PEDOT: PSS, trade names Baytron P® and Clevios™, protecting PEDOT's valuable properties.\n\nPEDOT:PSS dispersions are available in different variants. For capacitors with high capacitance values with high-roughened aluminum anode foils or fine-grained tantalum powders, dispersions having very small particle sizes are offered. The average size of these pre-polymerized particles is about 30 nm, small enough to penetrate the finest anode capillaries. Another variant of a PEDOT:PSS dispersion has been developed with larger pre-polymerized particles leading to a relatively thick polymer layer in order to make an enveloping protection of the capacitive cell of rectangular Ta and Al polymer capacitors against mechanical and electrical stress.\n\nWith PEDOT:PSS dispersions produced polymer aluminum electrolytic capacitors are well suited to reach higher rated voltage values of 200 V and 250 V. In addition, the leakage current values of the polymer electrolytic capacitors, which are produced with these dispersions, are significantly lower than for polymer capacitors having in-situ polymerized polymer layers.. Beneath to the better ESR values, higher temperature stability and lower leakage current values, however, the ease of manufacture of polymer capacitors with the pre-polymerized PEDOT:PSS dispersions, which in already only three dips of immersion have an almost complete coverage of the dielectric with a conducting polymer layer. This approach significantly has reduced production costs.\n\nHybrid polymer aluminum electrolytic capacitors combine a coating of the roughened and oxidized aluminum anode structure with a conductive polymer together with a liquid electrolyte. The liquid electrolyte is soaked in the separator (spacer) and achieves with its ion conductivity the electrical contact between the both polymer layers covering the dielectric and on the cathode foil. The liquid electrolyte can supply the oxygen for self-healing processes of the capacitor, which reduces the leakage current, so that values such as in conventional \"wet\" the electrolytic capacitor can be achieved. In addition the safety margin for the required oxide thickness for a desired rated voltage can be reduced.\n\nThe detrimental effects of the liquid electrolyte on ESR and temperature characteristics are relatively low. By using appropriate organic electrolytes and a good sealing of the capacitors a long service life can be achieved.\n\nBased on the used anode metal and the combination of a polymer electrolyte together with a liquid electrolyte, there are three different types:\n\nThese three different types or families, are produced in two different styles,\n\nIn the early 1990s polymer Ta-caps coincided with the emergence of flat devices such as mobile phones and laptops using SMD assembly technology. The rectangular base surface achieves the maximum mounting space, which is not possible with round base surfaces. The sintered cell can be manufactured so that the finished component has a desired height, typically the height of other components. Typical heights range from about 0.8 to 4 mm.\n\nPolymer tantalum electrolytic capacitors are essentially tantalum capacitors in which the electrolyte is a conductive polymer instead of manganese dioxide, see also tantalum capacitor#Materials, production and styles\nTantalum capacitors are manufactured from a powder of relatively pure elemental tantalum metal.\n\nThe powder is compressed around a tantalum wire, the anode connection, to form a “pellet”. This pellet/wire combination is subsequently vacuum sintered at high temperature (typically 1200 to 1800 °C) which produces a mechanically strong anode pellet. During sintering, the powder takes on a sponge-like structure, with all the particles interconnected into a monolithic spatial lattice. This structure is of predictable mechanical strength and density, but is also highly porous, producing a large anode surface area.\n\nThe dielectric layer is then formed over all the tantalum particle surfaces of the anode by the electrochemical process of anodization or forming. To achieve this, the “pellet” is submerged into a very weak solution of acid and DC voltage is applied. The total dielectric thickness is determined by the final voltage applied during the forming process. Thereafter, the oxidized sintered block is impregnated with the precursors of the polymer, to achieve the polymer electrolyte, the counter electrode. This polymerized pellet now is successively dipped into conducting graphite and then silver to provide a good connection to the conducting polymer. This layers achieves the cathode connection of the capacitor. The capacitive cell then is generally molded by a synthetic resin.\n\nPolymer tantalum electrolytic capacitors have ESR values that are approximately only 1/10 of the value of tantalum electrolytic capacitors with manganese dioxide electrolyte of the same size. By a multi-anode technique in which several anode blocks are connected in parallel in one case, the ESR value can be reduced again. The advantage of the multi-anode technology in addition to the very low ESR values is the lower inductance ESL, whereby the capacitors are suitable for higher frequencies.\n\nThe disadvantage of all polymer tantalum capacitors is the higher leakage current, which is approximately by a factor of 10 higher compared to the capacitors with manganese dioxide electrolyte. Polymer SMD Tantalum Electrolytic Capacitors are available up to a size of 7.3x4.3x4.3 mm (length × width × height) with a capacity of 1000 µF at 2.5 V. They cover temperature ranges from −55 °C to +125 °C and are available in rated voltage values from 2.5 to 63 V.\n\nLowering ESR and ESL remains a major research and development objective for all polymer capacitors. Some constructive measures can have also a major impact on the electrical parameters of capacitors. Smaller ESR values can be achieved for example by parallel connection of several conventional capacitor cells in one case. Three parallel capacitors with an ESR of 60 mΩ each have a resulting ESR of 20 mΩ. This technology is called “multi-anode” construction and will be used at polymer tantalum capacitors. In this construction up to six individual anodes in one case are connected. This design is offered as polymer tantalum chip capacitors as well as lower expensive tantalum chip capacitors with MnO electrolyte. Multi-anode polymer tantalum capacitors have ESR values on the single-digit milliohm range.\n\nOne another simple constructive measure changes the parasitic inductance of the capacitor, the ESL. Since the length of the leads inside the capacitor case has a large amount of the total ESL the inductance of the capacitor can be reduced by reducing the length of the internal leads by asymmetric sintering of the anode lead. This technique is called “face-down” construction. Due to the lower ESL of this face-down construction the resonance of the capacitor is shifted to higher frequencies, which take into account the faster load changes of digital circuits with ever-higher switching frequencies.\n\nPolymer tantalum chip capacitors with these new design enhancements, that both the ESR and the ESL decreased reaching properties, approaching ever closer to those of MLCC capacitors.\n\nRectangular polymer Al-caps have one or more layered aluminum anode foils and a conductive polymer electrolyte. The layered anode foils are at one side contacted with each other, this block is anodically oxidized to achieve the dielectric, and the block is impregnated with the precursors of the polymer to achieve the polymer electrolyte, the counter electrode. Like for polymer tantalum capacitors this polymerized block now is successively dipped into conducting graphite and then silver to provide a good connection to the conducting polymer. This layers achieves the cathode connection of the capacitor. The capacitive cell then generally is molded by a synthetic resin.\n\nThe layered anode foils in the rectangular shaped polymer Al-chip-e-caps are electrically parallel connected single capacitors. Thus, the ESR and ESL values are connected in parallel reducing ESR and ESL correspondingly, and allowing them to operate at higher frequencies.\n\nThese rectangular polymer Al-chip-e-caps are available in the \"D\"-case with 7,3x4,3 mm and heights of round 2...4 mm. They provide a competitive alternative to Ta-caps.\n\nComparing mechanical comparable polymer Al-chip-e-caps and polymer Ta-chip-e-caps shows that the different permittivities of aluminum oxide and tantalum pentoxide have little impact on the specific capacity due to different safety margins in oxide layers. Polymer Ta-e-caps use an oxide layer thickness that corresponds to approximately four times the rated voltage, while the polymer Al-e-caps have about twice the rated voltage.\n\nCylindrical polymer aluminum capacitors based on the technique of wound aluminum electrolytic capacitors with liquid electrolytes. They are available only with aluminum as anode material.\n\nThey are intended for larger capacitance values compared to rectangular polymer capacitors. Due to their design, they may vary in height on a given surface mounting area so that larger capacitance values can be achieved by a taller case without increasing the mounting surface. This is primarily useful for printed circuit boards without a height limit.\n\nCylindrical polymer Al-e-caps are made of two aluminum foils, an etched and formed anode and a cathode foil that are mechanically separated by a separator and wound together. The winding is impregnated with the polymer precursors to achieve the polymerized conducting polymer to form cathode the polymer electrode, electrically connected to the cathode foil. The winding then is built into an aluminum case and sealed with a rubber sealing. For the SMD version (Vertical chip= V-chip) the case is provided with a bottom plate.\n\nThe cylindrical polymer Al-e-caps are less expensive than corresponding polymer tantalum capacitors for a given CV value (capacitance × rated voltage). They are available up to a size of 10×13 mm (diameter × height) with a CV value of 3900 µF/2.5 V They can cover temperature ranges from -55 °C to +125 °C and are available in nominal voltage values from 2.5 to 200 V respectively 250 V.\n\nUnlike \"wet\" Al-e-caps, the cases of polymer Al capacitors do not have a vent (notch) in the bottom of the case, since a short circuit does not form gas, which would increase pressure in the case. Therefore, a predetermined breaking point is not required.\n\nHybrid polymer capacitors are available only in the cylindrical style construction thus corresponds to the above-described cylindrical polymer Al-e-caps leaded in the radial (single-ended) design or with a base plate in the SMD version (V-chip). The difference is that the polymer only covers the surface of the roughened structure of the dielectric AlO and the surface of the cathode foil as thin layers. With this especially the high-ohmic parts in the small pores of the anode foil can be made low-ohmic to reduce the capacitors ESR. As electrical connection between both polymer layers serve a liquid electrolyte like in conventional wet Al-e-caps impregnating the separator. The small distance the non-solid electrolyte conduct increases the ESR a little bit, however in fact not dramatically. Advantage of this construction is that the liquid electrolyte in operation delivers the oxygen which is necessary for self-healing of the dielectric layer in the presence of any small defects.\n\nThe current that flows through a small defect results in selective heating, which normally destroys the overlying polymer film, isolating, but not healing, the defect. In hybrid polymer capacitors liquid can flow to the defect, delivering oxygen and healing the dielectric by generating new oxides, decreasing the leakage current. Hybrid polymer Al-e-caps have a much lower leakage current than standard polymer Al-e-caps.\n\nThe polymer electrolyte, the two different anode materials, aluminum and tantalum, together with the different designs led to multiple polymer e-cap families with different specifications. For comparison, the basic parameters of the tantalum electrolytic capacitors with manganese dioxide electrolyte are also listed.\n\nElectrical properties of polymer capacitors can best be compared, using consistent capacitance, rated voltage and dimensions. The values for the ESR and the ripple current are the most important parameters for the use of for polymer capacitors in electronic equipment. The leakage current is significant, because it is higher than that of e-caps with non-polymer electrolytes. The respective values of Ta-e-caps with MnO electrolyte and wet Al-e-caps are included.\n\nAdvantages of polymer e-caps against wet Al-e-caps:\n\nDisadvantages of polymer e-caps against wet Al-e-caps:\n\nAdvantages of hybrid polymer Al-e-caps:\n\nDisadvantage of hybrid polymer Al-e-caps:\n\nAdvantages of polymer Ta and Al-e-caps against MLCCs (ceramic):\n\nThe electrical characteristics of capacitors are harmonized by the international generic specification IEC 60384-1. In this standard, the electrical characteristics of capacitors are described by an idealized series-equivalent circuit with electrical components which model all ohmic losses, capacitive and inductive parameters of electrolytic capacitors:\n\n\nThe capacitance value of polymer electrolytic capacitors depends on measuring frequency and temperature. Electrolytic capacitors with non-solid electrolytes show a broader aberration over frequency and temperature ranges than polymer capacitors.\n\nThe standardized measuring condition for polymer Al-e-caps is an AC measuring method with 0.5 V at a frequency of 100/120 Hz and a temperature of 20 °C. For polymer Ta-e-caps a DC bias voltage of 1.1 to 1.5  V for types with a rated voltage ≤2.5 V, or 2.1 to 2.5 V for types with a rated voltage of >2.5 V, may be applied during the measurement to avoid reverse voltage.\n\nThe capacitance value measured at the frequency of 1 kHz is about 10% less than the 100/120 Hz value. Therefore, the capacitance values of polymer e-caps are not directly comparable and differ from those of film capacitors or ceramic capacitors, whose capacitance is measured at 1 kHz or higher.\n\nThe basic unit of a polymer electrolytic capacitor's capacitance is the microfarad (μF). The capacitance value specified in manufacturers data sheets is called the rated capacitance C or nominal capacitance C. It is given according to IEC 60063 in values corresponding to the E series. These values are specified with a capacitance tolerance in accordance with IEC 60062 preventing overlaps.\nThe actual measured capacitance value must be within the tolerance limits.\n\nReferring to IEC 60384-1, the allowed operating voltage for polymer e-caps is called the \"rated voltage U\". The rated voltage U is the maximum DC voltage or peak pulse voltage that may be applied continuously at any temperature within the rated temperature range T.\n\nThe voltage proof of electrolytic capacitors decreases with increasing temperature. For some applications it is important to use a higher temperature range. Lowering the voltage applied at a higher temperature maintains safety margins. For some capacitor types therefore the IEC standard specifies a \"temperature derated voltage\" for a higher temperature, the \"category voltage U\". The category voltage is the maximum DC voltage or peak pulse voltage that may be applied continuously to a capacitor at any temperature within the category temperature range T. The relation between both voltages and temperatures is given in the picture at right.\n\nApplying a higher voltage than specified may destroy electrolytic capacitors.\n\nApplying a lower voltage may have a positive influence on polymer electrolytic capacitors. For hybrid polymer Al-e-caps a lower applied voltage in some cases can extend the lifetime. For polymer Ta-e-caps lowering the voltage applied increases the reliability and reduces the expected failure rate.\n\nThe relation between rated temperature T and rated voltage U as well as higher category temperature T and derated category voltage U is given in the picture at right.\n\nPolymer e-cap oxide layers are formed for safety reasons at a higher voltage than the rated voltage, called a surge voltage. Therefore, it is allowed to apply a surge voltage for short times and a limited number of cycles.\n\nThe surge voltage indicates the maximum peak voltage value that may be applied during their application for a limited number of cycles. The surge voltage is standardized in IEC 60384-1.\n\nFor polymer Al-e-caps the surge voltage is 1.15 times the rated voltage. For polymer Ta-e-caps the surge voltage can be 1.3 times the rated voltage, rounded off to the nearest volt.\n\nThe surge voltage applied to polymer capacitors may influence the capacitor's failure rate.\n\nTransients are fast and high voltage spikes. Polymer electrolytic capacitors, aluminum as well as tantalum polymer capacitors can not withstand transients or peak voltages higher than surge voltage. Transients for this type of e-caps may destroy the components.\n\nHybrid polymer Al-e-caps are relatively insensitive to high and short- term transient voltages higher than surge voltage, if the frequency and the energy content of the transients are low. This ability depends on rated voltage and component size. Low energy transient voltages lead to a voltage limitation similar to a zener diode An unambiguous and general specification of tolerable transients or peak voltages is not possible. In every case transients arise, the application must be individually assessed.\n\nPolymer electrolytic capacitors, tantalum as well as aluminum polymer capacitors are polarized capacitors and generally requires the anode electrode voltage to be positive relative to the cathode voltage. Nevertheless, they can withstand for short instants a type dependent reverse voltage for a limited number of cycles. A reverse voltage higher than the type-dependent threshold level applied for a long time to the polymer electrolyte capacitor leads to short-circuit and to destruction of the capacitor.\n\nTo minimize the likelihood of a polarized electrolytic being incorrectly inserted into a circuit, polarity has to be very clearly indicated on the case, see the section on \"Polarity marking\" below.\n\nSee also: Electrolytic capacitor#Impedance and Electrolytic capacitor#ESR and dissipation factor tan δ\n\nThe impedance is the complex ratio of the voltage to the current in an AC circuit, and expresses as AC resistance both magnitude and phase at a particular frequency. In the data sheets of polymer electrolyte capacitors only the impedance magnitude \"|Z|\" is specified, and simply written as \"Z\". Regarding the IEC 60384-1 standard, the impedance values of polymer electrolytic capacitors are measured and specified at 100 kHz.\n\nIn the special case of resonance, in which the both reactive resistances \"X\" and \"X\" have the same value (\"X=X\"), the impedance will be determined by only equivalent series resistance \"ESR\", which summarizes all resistive losses of the capacitor. At 100 kHz the impedance and the ESR have nearly the same value for polymer e-caps with capacitance values in the µF range. With frequencies above the resonance the impedance increases again due to \"ESL\" of the capacitor, turning the capacitor into an inductor.\n\nImpedance and ESR, as shown in the curves, as shown in the curves, heavily depend on the used electrolyte. The curves show the progressively lower impedance and ESR values of \"wet\" Al-e-caps and MnO Ta-e-caps, Al/TCNQ and tantalum polymer e-caps. The curve of a ceramic Class 2 MLCC capacitor, with still lower Z and ESR values is also shown, but whose capacitance is voltage-dependent.\n\nAn advantage of the polymer e-caps over non-solid Al-e-caps is low temperature dependence and almost linear curve of the ESR over the specified temperature range. This applies both to polymer tantalum, polymer aluminum, as well as for hybrid polymer aluminum e-caps.\n\nImpedance and ESR are also dependent on design and materials of the capacitors. Cylindrical Al-e-caps with the same capacitance as rectangular Al-e-caps have higher inductance than rectangular Al-e-caps with layered electrodes and therefore they have a lower resonant frequency. This effect is amplified by multi-anode construction, in which individual inductances are reduced by their parallel connection and the \"face-down\" technique.\n\nA \"ripple current\" is the root mean square (RMS) value of a superimposed AC current of any frequency and any waveform of the current curve for continuous operation within the specified temperature range. It arises mainly in power supplies (including switched-mode power supplies) after rectifying an AC voltage and flows as charge and discharge current through the decoupling or smoothing capacitor.\n\nRipple currents generates heat inside the capacitor body. This dissipation power loss \"P\" is caused by \"ESR\" and is the squared value of the effective (RMS) ripple current \"I\".\n\nThis internally generated heat, additional to the ambient temperature and other external heat sources, leads to a higher capacitor body temperature with a temperature difference of \"Δ T\" against the ambient. This heat has to be distributed as thermal losses \"P\" over the capacitor's surface \"A\" and the thermal resistance \"β\" to the ambient.\n\nThis heat is distributed to the ambient by thermal radiation, convection, and thermal conduction. The temperature of the capacitor, which is the net balance between heat produced and distributed, must not exceed the capacitor's maximum specified temperature.\n\nThe ripple current for polymer e-caps is specified as a maximum effective (RMS) value at 100 kHz at upper rated temperature. Non-sinusoidal ripple currents have to be analyzed and separated into their individual single frequencies by means of Fourier analysis and summarized by squared addition to calculate a RMS value.\n"}
{"id": "201689", "url": "https://en.wikipedia.org/wiki?curid=201689", "title": "Polypropylene", "text": "Polypropylene\n\nPolypropylene (PP), also known as polypropene, is a thermoplastic polymer used in a wide variety of applications. It is produced via chain-growth polymerization from the monomer propylene.\n\nPolypropylene belongs to the group of polyolefins and is partially crystalline and non-polar. Its properties are similar to polyethylene, but it is slightly harder and more heat resistant. It is a white, mechanically rugged material and has a high chemical resistance. Polypropylene is the second-most widely produced commodity plastic (after polyethylene) and it is often used in packaging and labeling. In 2013, the global market for polypropylene was about 55 million tonnes.\n\nPhillips Petroleum chemists J. Paul Hogan and Robert Banks first polymerized propylene in 1951. Propylene was first polymerized to a crystalline isotactic polymer by Giulio Natta as well as by the German chemist Karl Rehn in March 1954. This pioneering discovery led to large-scale commercial production of isotactic polypropylene by the Italian firm Montecatini from 1957 onwards. Syndiotactic polypropylene was also first synthesized by Natta and his coworkers.\n\nAfter polyethylene, polypropylene is the most important plastic with revenues expected to exceed US$145 billion by 2019. The sales of this material are forecast to grow at a rate of 5.8% per year until 2021.\n\nPolypropylene is in many aspects similar to polyethylene, especially in solution behaviour and electrical properties. The methyl group improves mechanical properties and thermal resistance, although the chemical resistance decreases. The properties of polypropylene depend on the molecular weight and molecular weight distribution, crystallinity, type and proportion of comonomer (if used) and the isotacticity. In isotactic polypropylene, for example, the methyl groups are oriented on one side of the carbon backbone. This arrangement creates a greater degree of crystallinity and results in a stiffer material that is more resistant to creep than both atactic polypropylene and polyethylene.\n\nThe density of (PP) is between 0.895 and 0.92 g/cm³. Therefore, PP is the commodity plastic with the lowest density. With lower density, moldings parts with lower weight and more parts of a certain mass of plastic can be produced. Unlike polyethylene, crystalline and amorphous regions differ only slightly in their density. However, the density of polyethylene can significantly change with fillers.\n\nThe Young's modulus of PP is between 1300 and 1800 N/mm².\n\nPolypropylene is normally tough and flexible, especially when copolymerized with ethylene. This allows polypropylene to be used as an engineering plastic, competing with materials such as acrylonitrile butadiene styrene (ABS). Polypropylene is reasonably economical.\n\nPolypropylene has good resistance to fatigue.\n\nThe melting point of polypropylene occurs in a range, so the melting point is determined by finding the highest temperature of a differential scanning calorimetry chart. Perfectly isotactic PP has a melting point of . Commercial isotactic PP has a melting point that ranges from , depending on atactic material and crystallinity. Syndiotactic PP with a crystallinity of 30% has a melting point of . Below 0 °C, PP becomes brittle.\n\nThe thermal expansion of polypropylene is very large, but somewhat less than that of polyethylene.\n\nPolypropylene at room temperature is resistant to fats and almost all organic solvents, apart from strong oxidants. Non-oxidizing acids and bases can be stored in containers made of PP. At elevated temperature, PP can be dissolved in nonpolar solvents such as xylene, tetralin and decalin. Due to the tertiary carbon atom PP is chemically less resistant than PE (see Markovnikov rule).\n\nMost commercial polypropylene is isotactic and has an intermediate level of crystallinity between that of low-density polyethylene (LDPE) and high-density polyethylene (HDPE). Isotactic & atactic polypropylene is soluble in P-xylene at 140 °C. Isotactic precipitates when the solution is cooled to 25 °C and atactic portion remains soluble in P-xylene.\n\nThe melt flow rate (MFR) or melt flow index (MFI) is a measure of molecular weight of polypropylene. The measure helps to determine how easily the molten raw material will flow during processing. Polypropylene with higher MFR will fill the plastic mold more easily during the injection or blow-molding production process. As the melt flow increases, however, some physical properties, like impact strength, will decrease.\n\nThere are three general types of polypropylene: homopolymer, random copolymer, and block copolymer. The comonomer is typically used with ethylene. Ethylene-propylene rubber or EPDM added to polypropylene homopolymer increases its low temperature impact strength. Randomly polymerized ethylene monomer added to polypropylene homopolymer decreases the polymer crystallinity, lowers the melting point and makes the polymer more transparent.\n\nThe term tacticity describes for polypropylene how the methyl group is oriented in the polymer chain. Commercial polypropylene is usually isotactic. This article therefore always refers to isotactic polypropylene, unless stated otherwise. The tacticity is usually indicated in percent, using the isotactic index (according to DIN 16774). The index is measured by determining the fraction of the polymer insoluble in boiling heptane. Commercially available polypropylenes usually have an isotactic index between 85 and 95%. The tacticity effects the polymers physical properties. As the methyl group is in isotactic propylene consistently located at the same side, it forces the macromolecule in a helical shape, as also found in starch. An isotactic structure leads to a semi-crystalline polymer. The higher the isotacticity (the isotactic fraction), the greater the crystallinity, and thus also the softening point, rigidity, e-modulus and hardness.\n\nAtactic polypropylene, on the other hand, lacks any regularity which makes it unable to crystallize and amorphous.\n\nIsotactic polypropylene has a high degree of crystallinity, in industrial products 30 - 60%. Syndiothactic polypropylene is slightly less crystalline, atactic PP is amorphous (not crystalline).\n\nIsotactic polypropylene can exist in various crystalline modifications which differ by the molecular arrangement of the polymer chains. The crystalline modifications are categorized into the α-, β- and γ-modification as well as mesomorphic (smectic) forms. The α-modification is predominant in iPP. Such crystals are built from lamellae in the form of folded chains. A characteristic anomaly is that the lamellae are arranged in the so-called \"cross-hatched\" structure. The melting point of α-crystalline regions is given as 185 to 220 °C, the density as 0.936 to 0.946 g·cm. The β-modification is in comparison somewhat less ordered, as a result of which it forms faster and has a lower melting point of 170 to 200 °C. The formation of the β-modification can be promoted by nucleating agents, suitable temperatures and shear stress. The γ-modification is hardly formed under the conditions used in industry and is poorly understood. The mesomorphic modification, however, occurs often in industrial processing, since the plastic is usually cooled quickly. The degree of order of the mesomorphic phase ranges between the crystalline and the amorphous phase, its density is with 0.916 g·cm comparatively. The mesomorphic phase is considered as cause for the transparency in rapidly cooled films (due to low order and small crystallites).\n\nSyndiotactic polypropylene was discovered much later than isotactic PP and could only be prepared by using metallocene catalysts. Syndiotactic PP has a lower melting point, with 161 to 186 °C, depending on the degree of tacticity.\n\nAtactic polypropylene is amorphous and has therefore no crystal structure. Due to its lack of crystallinity, it is readily soluble even at moderate temperatures, which allows to separate it as by-product from isotactic polypropylene by extraction. However, the aPP obtained this way is not completely amorphous but can still contain 15% crystalline parts. Atactic polypropylene can also be produced selectively using metallocene catalysts, atactic polypropyleneproduced this way has a considerably higher molecular weight.\n\nAtactic polypropylene has lower density, melting point and softening temperature than the crystalline types and is tacky and rubber-like at room temperature. It is a colorless, cloudy material and can be used between -15 and +120 °C. Atactic polypropylene is used as a sealant, as an insulating material for automobiles and as an additive to bitumen.\n\nPolypropylene is liable to chain degradation from exposure to temperatures above 100 °C. Oxidation usually occurs at the tertiary carbon centers leading to chain breaking via reaction with oxygen. In external applications, degradation is evidenced by cracks and crazing. It may be protected by the use of various polymer stabilizers, including UV-absorbing additives and anti-oxidants such as phosphites (e.g. tris(2,4-di-tert-butylphenyl)phosphite) and hindered phenols, which prevent polymer degradation.\n\nMicrobial communities isolated from soil samples mixed with starch have been shown to be capable of degrading polypropylene.\nPolypropylene has been reported to degrade while in human body as implantable mesh devices. The degraded material forms a tree bark-like layer at the surface of mesh fibers.\n\nPP can be made translucent when uncolored but is not as readily made transparent as polystyrene, acrylic, or certain other plastics. It is often opaque or colored using pigments.\n\nPolypropylene is produced by the chain-growth polymerization of propene:\n\nThe industrial production processes can be grouped into gas phase polymerization, bulk polymerization and slurry polymerization. All state-of-the-art processes use either gas-phase or bulk reactor systems.\n\n\nThe properties of PP are strongly affected by its tacticity, the orientation of the methyl groups () relative to the methyl groups in neighboring monomer units (see above). The tacticity of polypropylene can be chosen by the choice of an appropriate catalyst.\n\nThe properties of PP are strongly affected by its tacticity, the orientation of the methyl groups ( in the figure) relative to the methyl groups in neighboring monomer units. A Ziegler-Natta catalyst is able to restrict linking of monomer molecules to a specific orientation, either isotactic, when all methyl groups are positioned at the same side with respect to the backbone of the polymer chain, or syndiotactic, when the positions of the methyl groups alternate. Commercially available isotactic polypropylene is made with two types of Ziegler-Natta catalysts. The first group of the catalysts encompasses solid (mostly supported) catalysts and certain types of soluble metallocene catalysts. Such isotactic macromolecules coil into a helical shape; these helices then line up next to one another to form the crystals that give commercial isotactic polypropylene many of its desirable properties.\n\nAnother type of metallocene catalysts produce syndiotactic polypropylene. These macromolecules also coil into helices (of a different type) and crystallize. Atactic polypropylene is an amorphous rubbery material. It can be produced commercially either with a special type of supported Ziegler-Natta catalyst or with some metallocene catalysts.\n\nModern supported Ziegler-Natta catalysts developed for the polymerization of propylene and other 1-alkenes to isotactic polymers usually use as an active ingredient and as a support. The catalysts also contain organic modifiers, either aromatic acid esters and diesters or ethers. These catalysts are activated with special cocatalysts containing an organoaluminum compound such as Al(CH) and the second type of a modifier. The catalysts are differentiated depending on the procedure used for fashioning catalyst particles from MgCl and depending on the type of organic modifiers employed during catalyst preparation and use in polymerization reactions. Two most important technological characteristics of all the supported catalysts are high productivity and a high fraction of the crystalline isotactic polymer they produce at 70–80 °C under standard polymerization conditions. Commercial synthesis of isotactic polypropylene is usually carried out either in the medium of liquid propylene or in gas-phase reactors.\n\nCommercial synthesis of syndiotactic polypropylene is carried out with the use of a special class of metallocene catalysts. They employ bridged bis-metallocene complexes of the type bridge-(Cp)(Cp)ZrCl where the first Cp ligand is the cyclopentadienyl group, the second Cp ligand is the fluorenyl group, and the bridge between the two Cp ligands is -CH-CH-, >SiMe, or >SiPh. These complexes are converted to polymerization catalysts by activating them with a special organoaluminum cocatalyst, methylaluminoxane (MAO).\n\nTraditionally, three manufacturing processes are the most representative ways to produce polypropylene.\n\nHydrocarbon slurry or suspension: Uses a liquid inert hydrocarbon diluent in the reactor to facilitate transfer of propylene to the catalyst, the removal of heat from the system, the deactivation/removal of the catalyst as well as dissolving the atactic polymer. The range of grades that could be produced was very limited. (The technology has fallen into disuse).\n\nBulk slurry (or bulk): Uses liquid propylene instead of liquid inert hydrocarbon diluent. The polymer does not dissolve into a diluent, but rather rides on the liquid propylene. The formed polymer is withdrawn and any unreacted monomer is flashed off.\n\nGas phase: Uses gaseous propylene in contact with the solid catalyst, resulting in a fluidized-bed medium.\n\nMelting process of polypropylene can be achieved via extrusion and molding. Common extrusion methods include production of melt-blown and spun-bond fibers to form long rolls for future conversion into a wide range of useful products, such as face masks, filters, diapers and wipes.\n\nThe most common shaping technique is injection molding, which is used for parts such as cups, cutlery, vials, caps, containers, housewares, and automotive parts such as batteries. The related techniques of blow molding and injection-stretch blow molding are also used, which involve both extrusion and molding.\n\nThe large number of end-use applications for polypropylene are often possible because of the ability to tailor grades with specific molecular properties and additives during its manufacture. For example, antistatic additives can be added to help polypropylene surfaces resist dust and dirt. Many physical finishing techniques can also be used on polypropylene, such as machining. Surface treatments can be applied to polypropylene parts in order to promote adhesion of printing ink and paints.\n\nExpanded Polypropylene (EPP) has been produced through both solid and melt state processing. EPP is manufactured using melt processing with either chemical or physical blowing agents. Expansion of PP in solid state, due to its highly crystalline structure, has not been successful. In this regard, two novel strategies were developed for expansion of PP. It was observed that PP can be expanded to make EPP through controlling its crystalline structure or through blending with other polymers.\n\nWhen polypropylene film is extruded and stretched in both the machine direction and across machine direction it is called \"biaxially oriented polypropylene\". Biaxial orientation increases strength and clarity. BOPP is widely used as a packaging material for packaging products such as snack foods, fresh produce and confectionery. It is easy to coat, print and laminate to give the required appearance and properties for use as a packaging material. This process is normally called converting. It is normally produced in large rolls which are slit on slitting machines into smaller rolls for use on packaging machines.\n\nAs polypropylene is resistant to fatigue, most plastic living hinges, such as those on flip-top bottles, are made from this material. However, it is important to ensure that chain molecules are oriented across the hinge to maximise strength.\n\nPolypropylene is used in the manufacturing piping systems; both ones concerned with high-purity and ones designed for strength and rigidity (e.g. those intended for use in potable plumbing, hydronic heating and cooling, and reclaimed water). This material is often chosen for its resistance to corrosion and chemical leaching, its resilience against most forms of physical damage, including impact and freezing, its environmental benefits, and its ability to be joined by heat fusion rather than gluing.\n\nMany plastic items for medical or laboratory use can be made from polypropylene because it can withstand the heat in an autoclave. Its heat resistance also enables it to be used as the manufacturing material of consumer-grade kettles. Food containers made from it will not melt in the dishwasher, and do not melt during industrial hot filling processes. For this reason, most plastic tubs for dairy products are polypropylene sealed with aluminum foil (both heat-resistant materials). After the product has cooled, the tubs are often given lids made of a less heat-resistant material, such as LDPE or polystyrene. Such containers provide a good hands-on example of the difference in modulus, since the rubbery (softer, more flexible) feeling of LDPE with respect to polypropylene of the same thickness is readily apparent. Rugged, translucent, reusable plastic containers made in a wide variety of shapes and sizes for consumers from various companies such as Rubbermaid and Sterilite are commonly made of polypropylene, although the lids are often made of somewhat more flexible LDPE so they can snap onto the container to close it. Polypropylene can also be made into disposable bottles to contain liquid, powdered, or similar consumer products, although HDPE and polyethylene terephthalate are commonly also used to make bottles. Plastic pails, car batteries, wastebaskets, pharmacy prescription bottles, cooler containers, dishes and pitchers are often made of polypropylene or HDPE, both of which commonly have rather similar appearance, feel, and properties at ambient temperature. A diversity of medical devices are made from PP.\n\nA common application for polypropylene is as biaxially oriented polypropylene (BOPP). These BOPP sheets are used to make a wide variety of materials including clear bags. When polypropylene is biaxially oriented, it becomes crystal clear and serves as an excellent packaging material for artistic and retail products.\n\nPolypropylene, highly colorfast, is widely used in manufacturing carpets, rugs and mats to be used at home.\n\nPolypropylene is widely used in ropes, distinctive because they are light enough to float in water. For equal mass and construction, polypropylene rope is similar in strength to polyester rope. Polypropylene costs less than most other synthetic fibers.\n\nPolypropylene is also used as an alternative to polyvinyl chloride (PVC) as insulation for electrical cables for LSZH cable in low-ventilation environments, primarily tunnels. This is because it emits less smoke and no toxic halogens, which may lead to production of acid in high-temperature conditions.\n\nPolypropylene is also used in particular roofing membranes as the waterproofing top layer of single-ply systems as opposed to modified-bit systems.\n\nPolypropylene is most commonly used for plastic moldings, wherein it is injected into a mold while molten, forming complex shapes at relatively low cost and high volume; examples include bottle tops, bottles, and fittings.\n\nIt can also be produced in sheet form, widely used for the production of stationery folders, packaging, and storage boxes. The wide color range, durability, low cost, and resistance to dirt make it ideal as a protective cover for papers and other materials. It is used in Rubik's Cube stickers because of these characteristics.\n\nThe availability of sheet polypropylene has provided an opportunity for the use of the material by designers. The light-weight, durable, and colorful plastic makes an ideal medium for the creation of light shades, and a number of designs have been developed using interlocking sections to create elaborate designs.\n\nPolypropylene sheets are a popular choice for trading card collectors; these come with pockets (nine for standard-size cards) for the cards to be inserted and are used to protect their condition and are meant to be stored in a binder.\n\nExpanded polypropylene (EPP) is a foam form of polypropylene. EPP has very good impact characteristics due to its low stiffness; this allows EPP to resume its shape after impacts. EPP is extensively used in model aircraft and other radio controlled vehicles by hobbyists. This is mainly due to its ability to absorb impacts, making this an ideal material for RC aircraft for beginners and amateurs.\n\nPolypropylene is used in the manufacture of loudspeaker drive units. Its use was pioneered by engineers at the BBC and the patent rights subsequently purchased by Mission Electronics for use in their Mission Freedom Loudspeaker and Mission 737 Renaissance loudspeaker.\n\nPolypropylene fibres are used as a concrete additive to increase strength and reduce cracking and spalling. In some areas susceptible to earthquake, e.g. California, PP fibers are added with soils to improve the soils strength and damping when constructing the foundation of structures such as buildings, bridges, etc.\n\nPolypropylene is used in polypropylene drums.\n\nIn June 2016, a study showed that a mixture of polypropylene and durable superoleophobic surfaces created by two engineers from Ohio State University can repel liquids such as shampoo and oil. This technology could make it easier to remove all of the liquid contents from polypropylene bottles, particularly those that have high surface tension such as shampoo or oil.\n\nPolypropylene is a major polymer used in nonwovens, with over 50% used for diapers or sanitary products where it is treated to absorb water (hydrophilic) rather than naturally repelling water (hydrophobic). Other interesting non-woven uses include filters for air, gas, and liquids in which the fibers can be formed into sheets or webs that can be pleated to form cartridges or layers that filter in various efficiencies in the 0.5 to 30 micrometre range. Such applications occur in houses as water filters or in air-conditioning-type filters. The high surface-area and naturally oleophilic polypropylene nonwovens are ideal absorbers of oil spills with the familiar floating barriers near oil spills on rivers.\n\nPolypropylene, or 'polypro', has been used for the fabrication of cold-weather base layers, such as long-sleeve shirts or long underwear. Polypropylene is also used in warm-weather clothing, in which it transports sweat away from the skin. Polyester has replaced polypropylene in these applications in the U.S. military, such as in the ECWCS. Although polypropylene clothes are not easily flammable, they can melt, which may result in severe burns if the wearer is involved in an explosion or fire of any kind. Polypropylene undergarments are known for retaining body odors which are then difficult to remove. The current generation of polyester does not have this disadvantage.\n\nSome fashion designers have adapted polypropylene to construct jewelry and other wearable items.\n\nIts most common medical use is in the synthetic, nonabsorbable suture Prolene, manufactured by Ethicon Inc.\n\nPolypropylene has been used in hernia and pelvic organ prolapse repair operations to protect the body from new hernias in the same location. A small patch of the material is placed over the spot of the hernia, below the skin, and is painless and rarely, if ever, rejected by the body. However, a polypropylene mesh will erode the tissue surrounding it over the uncertain period from days to years.\n\nA notable application was as a transvaginal mesh, used to treat vaginal prolapse and concurrent urinary incontinence. Due to the above-mentioned propensity for polypropylene mesh to erode the tissue surrounding it, the FDA has issued several warnings on the use of polypropylene mesh medical kits for certain applications in pelvic organ prolapse, specifically when introduced in close proximity to the vaginal wall due to a continued increase in number of mesh-driven tissue erosions reported by patients over the past few years. On 3 January 2012, the FDA ordered 35 manufacturers of these mesh products to study the side effects of these devices.\n\nVery thin sheets (~2–20 µm) of polypropylene are used as a dielectric within certain high-performance pulse and low-loss RF capacitors.\n\nExpanded polypropylene (EPP) foams is a structural material in hobbyist radio control model aircraft. Unlike expanded polystyrene foam (EPS) which is friable and breaks easily on impact, EPP foam is able to absorb kinetic impacts very well without breaking, retains its original shape, and exhibits memory form characteristics which allow it to return to its original shape in a short amount of time.\n\nWhen the cathedral on Tenerife, La Laguna Cathedral, was repaired in 2002–2014, it turned out that the vaults and dome were in a rather bad condition. Therefore, these parts of the building were demolished, and replaced by constructions in polypropylene. This was reported as the first time this material was used in this scale in buildings.\n\nUnder the trade name Ulstron polypropylene rope is used to manufacture scoop nets for whitebait. It has also been used for sheets of yacht sails.\n\nPolypropylene is recyclable and has the number \"5\" as its resin identification code: \n\nMany objects are made with polypropylene precisely because it is resilient and resistant to most solvents and glues. Also, there are very few glues available specifically for gluing PP. However, solid PP objects not subject to undue flexing can be satisfactorily joined with a two part epoxy glue or using hot-glue guns. Preparation is important and it is often helpful to roughen the surface with a file, emery paper or other abrasive material to provide better anchorage for the glue. Also it is recommended to clean with mineral spirits or similar alcohol prior to gluing to remove any oils or other contamination. Some experimentation may be required. There are also some industrial glues available for PP, but these can be difficult to find, especially in a retail store.\n\nPP can be melted using a speed welding technique. With speed welding, the plastic welder, similar to a soldering iron in appearance and wattage, is fitted with a feed tube for the plastic weld rod. The speed tip heats the rod and the substrate, while at the same time it presses the molten weld rod into position. A bead of softened plastic is laid into the joint, and the parts and weld rod fuse. With polypropylene, the melted welding rod must be \"mixed\" with the semi-melted base material being fabricated or repaired. A speed tip \"gun\" is essentially a soldering iron with a broad, flat tip that can be used to melt the weld joint and filler material to create a bond.\n\nThe advocacy organization Environmental Working Group classifies PP as of low to moderate hazard.\nPP is dope-dyed; no water is used in its dyeing, in contrast with cotton.\n\nLike all organic compounds, polypropylene is combustible. The flash point of a typical composition is 260 °C; autoignition temperature is 388 °C.\n\n"}
{"id": "25738556", "url": "https://en.wikipedia.org/wiki?curid=25738556", "title": "Power Grid Company of Bangladesh", "text": "Power Grid Company of Bangladesh\n\nPower Grid Company of Bangladesh is the sole electric power transmission organization in Bangladesh. It is a government company that owns and operates the power grid in Bangladesh. It is a subsidiary of Power Development Board.\n\n"}
{"id": "3484453", "url": "https://en.wikipedia.org/wiki?curid=3484453", "title": "Power conditioner", "text": "Power conditioner\n\nA power conditioner (also known as a line conditioner or power line conditioner) is a device intended to improve the quality of the power that is delivered to electrical load equipment. The term most often refers to a device that acts in one or more ways to deliver a voltage of the proper level and characteristics to enable load equipment to function properly. In some uses, power conditioner refers to a voltage regulator with at least one other function to improve power quality (e.g. power factor correction, noise suppression, transient impulse protection, etc.)\n\nThe terms \"power conditioning\" and \"power conditioner\" can be misleading, as the word \"power\" here refers to the electricity generally rather than the more technical electric power. \n\nConditioners specifically work to smooth the sinusoidal A.C. wave form and maintain a constant voltage over varying loads.\n\nAn AC power conditioner is the typical power conditioner that provides \"clean\" AC power to sensitive electrical equipment. Usually this is used for home or office applications and has up to 10 or more receptacles or outlets and commonly provides surge protection as well as noise filtering.\n\nPower line conditioners take in power and modify it based on the requirements of the machinery to which they are connected. Attributes to be conditioned are measured with various devices, such as, Phasor measurement units. Voltage spikes are most common during electrical storms or malfunctions in the main power lines. The surge protector stops the flow of electricity from reaching a machine by shutting off the power source.\n\nThe term \"Power Conditioning\" has been difficult to define historically. However, with the advances in power technology and recognition by IEEE, NEMA, and other standards organizations, a new actual engineering definition has now been developed and accepted to provide an accurate depiction of this definition.\n\n\"Power Conditioning\" is the ability to filter the AC line signal provided by the power company.\n\"Power Regulation\" is the ability to take a signal from the local power company, turn it into a DC signal that will run an oscillator, which generates a single frequency sine wave, determined by the local area needs, is fed to the input stage of power amplifier, and is then output as specified as the ideal voltage present at any standard wall outlet.\n\nA good quality power conditioner is designed with internal filter banks to isolate the individual power outlets or receptacles on the power conditioner. This eliminates interference or \"cross-talk\" between components. For example, if the application will be a home theater system, the noise suppression rating listed in the technical specifications of the power conditioner will be very important. This rating is expressed in decibels (db). The higher the db rating, the better the noise suppression. \n\nActive power filters (APF) are filters which can perform the job of harmonic elimination. Active power filters can be used to filter out harmonics in the power system which are significantly below the switching frequency of the filter. The active power filters are used to filter out both higher and lower order harmonics in the power system.\n\nThe main difference between active power filters and passive power filters is that APFs mitigate harmonics by injecting active power with the same frequency but with reverse phase to cancel that harmonic, where passive power filters use combinations of resistors (R), inductors (L) and capacitors (C) and does not require an external power source or active components such as transistors. This difference makes it possible for APFs to mitigate a wide range of harmonics.\n\nThe power conditioner will also have a \"joule\" rating. A joule is a measurement of energy or heat required to sustain one watt for one second, known as a watt second. Since electrical surges are momentary spikes, the joule rating indicates how much electrical energy the suppressor can absorb at once before becoming damaged itself. The higher the joule rating, the greater the protection.\n\nPower conditioners vary in function and size, generally according to their use. Some power conditioners provide minimal voltage regulation while others protect against six or more power quality problems. Units may be small enough to mount on a printed circuit board or large enough to protect an entire factory. \n\nSmall power conditioners are rated in volt-amperes (V·A) while larger units are rated in kilovolt-amperes (kV·A).\n\nIdeally electric power would be supplied as a sine wave with the amplitude and frequency given by national standards (in the case of mains) or system specifications (in the case of a power feed not directly attached to the mains) with an impedance of zero ohms at all frequencies. \n\nNo real life power feed will ever meet this ideal. Deviations may include:\n\n\n\n"}
{"id": "27802151", "url": "https://en.wikipedia.org/wiki?curid=27802151", "title": "Refinaria do Planalto Paulista", "text": "Refinaria do Planalto Paulista\n\nRefinaria de Paulínia or simply REPLAN is a petroleum refinery located in the city of Paulínia in the São Paulo state, in Brazil. REPLAN is the largest refinery of the Brazilian company Petrobras, with a capacity of about , which accounts for about 20% of Brazilian overall petroleum refining capacity. About 80% of the processed petroleum is produced in Brazil, mostly from the Campos Basin.\n\nThe REPLAN refinery was destroyed in the Tom Clancy novel Dead or Alive, by terrorists hoping to undermine a deal between the United States and Petrobras to ship oil to the United States at sub-OPEC prices.\n\n"}
{"id": "317214", "url": "https://en.wikipedia.org/wiki?curid=317214", "title": "Rip cut", "text": "Rip cut\n\nIn woodworking, a rip-cut is a type of cut that severs or divides a piece of wood parallel to the grain. The other typical type of cut is a \"cross-cut\", a cut perpendicular to the grain. Unlike cross-cutting, which shears the wood fibers, a rip saw works more like a series of chisels, lifting off small splinters of wood. The nature of the wood grain requires the shape of the saw teeth to be different thus the need for both rip saws and crosscut saws; however some circular saw blades are \"combination blades\" and can make both types of cuts. A rip cut is the fundamental type of cut made at a sawmill.\n\n\"Rip cut\" comes from \"rip\": to split or saw timber in the direction of the grain, and \"cut\": to divide with a sharp-edged instrument.\nWood may also be split along the grain (riven), but the split will follow the grain and usually not be flat. Knots also prevent riving thus the need for rip cuts. A \"kerf\" is the opening in the wood made by the saw.\n\nTypes of hand saws used to make rip cuts are rip saws, frame saws some of which are whipsaws, and veneer saws.\n\nRip cuts are commonly made with a table saw, but other types of power saws can also be used, including a radial arm saw, band saw, and hand held circular saw. In sawmills the head saw is the first rip-saw a log goes through, which is sometimes a gang-saw, and then the cants may be resawn using other saws and then edged in a edger and sometimes cut to length by a crosscut saw. Also, smaller portable sawmills and chainsaw mills use rip-cuts to produce lumber. Each time a piece of wood is rip cut it takes time and the kerf material turns into sawdust and loses value so the number and width of each rip cut influence the economics of the operation: This gives band saws an advantage over circular saws and chainsaws.\n\nThe types of rip-cuts influence the quality of the lumber. Plain-sawn is the most common type of cut where a log is repeatedly run through a saw and much of the lumber has wood grain nearly parallel to the width of the boards. Quarter sawn and rift-sawn wood is more time consuming and wasteful to produce but is higher quality. \n\nAs a general rule, tools which work well for rip cutting do not work well for crosscutting. Most woodworkers thus have a table saw, which is used for rip cutting, and a separate chop or miter saw, which is used for crosscutting. Crosscut power-saws should never be used for \"ripping\" a board because it is very dangerous. Circular saw blades designed for rip cutting have a smaller number of larger teeth than similar blades designed for cross cutting. There are combination blades for table saws that can be used for ripping and cross cutting but should not be used for non through cuts such as dados and rabbets. If you use a radial arm saw to rip you need a blade with a negative hook angle for the teeth to keep the saw from lifting the board off the saw and kicking back.\n"}
{"id": "54447621", "url": "https://en.wikipedia.org/wiki?curid=54447621", "title": "Roll Out Solar Array", "text": "Roll Out Solar Array\n\nThe Roll Out Solar Array, known as ROSA, is a solar array designed by NASA. This new type of solar array provides much more energy than the old solar arrays. Traditional solar panels used to power satellites can be bulky with heavy panels folded together using mechanical hinges. When launching into space, mass and volume are everything, and ROSA is 20 percent lighter and four times smaller in volume than rigid panel arrays. It is a flexible and rollable solar array like a measuring tape wound on its spool. The new solar array design rolls up to form a compact cylinder for launch with significantly less mass and volume, potentially offering substantial cost savings as well as an increase in power for satellites. Being smaller and lighter than the traditional solar panels, ROSA has a center wing made of a flexible material which support the strings of photovoltaic cells that churn out electrical energy. Both the sides of the wing have a narrow arm that extends through the length of the wing to provide support to the array, called a high strain composite boom. The booms look like split tubes made of a stiff composite material, flattened and rolled up lengthwise.The array does not need any motor to unfurl.This is achieved using the energy stored in the booms that is released as each boom transitions from a coil shape to a straight support arm. Those solar wings are then deployed due to strain energy in rolled booms that are present at the two ends of the structure.\n\nBrian R Spence,and Stephen F White are the first persons to come up with the idea of Roll Out Solar Array on Jan 21st, 2010. They received a patent for this work on April 1 of 2014\n\nNASA tested the ROSA technology in vacuum chambers on earth several years ago, but still decided to test it in space on June 18 of 2017. Over the weekend of June 17–18, 2017, engineers on the ground remotely operated the International Space Station’s robotic Canadarm2 to extract the Roll Out Solar Array (ROSA) experiment from the SpaceX Dragon resupply ship. After the observation the mechanism was not planned to be retrieved back to earth. The solar array unfurled June 18, extending by tensioning booms on both sides of the 1.6-meter-wide wing. NASA decided to conduct continuous tests for a week and observe its consequences. Engineers observed the behavior of the solar array as it was exposing it to extreme temperature swings through the ISS's orbit. Vibrations and oscillations were also mechanically introduced to assay the array’s response to structural loads. Subsequent to the experiments, ground controllers were unable to lock the solar panel in its stowed configuration. The solar array was therefore jettisoned from the International Space Station.\n\nROSA being very compact in size and due to its large power generation capacity it is reliable for future missions and majorly for interplanetary travels which need a huge amount of energy.\n\n\n\n"}
{"id": "4288407", "url": "https://en.wikipedia.org/wiki?curid=4288407", "title": "Royd Moor Wind Farm", "text": "Royd Moor Wind Farm\n\nThe Royd Moor Wind Farm is located at Penistone, Barnsley, South Yorkshire, England, and consists of thirteen 500 kW turbines. The site is located approximately 6 km north west of Penistone, above the A628 trunk road between Barnsley and Manchester. The turbines are set in two parallel, staggered rows of six and seven, on a ridge in hilly fell land lying 320 m above sea level.\n\nThese turbines were completed in 1993 and are 35m in height to the hub and have a rotor diameter of 37m. The maximum output is 6.5 MW, equivalent to the annual energy required to power 3300 homes.\n\nThe original planning application was for a 25-year operation but this has been extended to 30 years. Currently, the farm is due to be decommissioned in 2023.\n\n"}
{"id": "21622042", "url": "https://en.wikipedia.org/wiki?curid=21622042", "title": "Screen heating", "text": "Screen heating\n\nThe method of screening has been linked to the early Egyptians who used the process to separate basic minerals by using a mesh that had equal openings. This idea fell down through the ages and developed into a series of European patents around 1924, when the thought of electrically heating the wire cloth for screens was considered. This was the first attempt at eliminating screen blinding (when the material being processed (clay, dirt, etc.) clogs the holes in the screen) by using heat, and it proved that the surface tension from blinding was reduced when heat was introduced.\n\nThe first commercially available screen heating system was developed in 1947 by F.R. Hannon & Sons (now Hanco International (), and Thomas W. Hannon obtained a patent for his electrically heated screen construction and method . This was just in time, as industries were beginning to expand in coming out of World War II. The first applications of the screen heating system were limited almost exclusively to the clay industry, though it did not take long for other applications, in a variety of industries, to be identified.\n\nAnother significant invention for screen heating was created by Thomas W. Hannon (for Hanco International). This was for screen-related clamp bars (also known as a Wedge Grip), that are used when mounting a screen on frame members. The clamp bars secured a screen to a tensioning rail, which in turn increased the electrical connection between the two components. ()\n\nIn the years following, copper and copper cables were implemented in an attempt to improve the system. Silicon-bonded fiberglass was also added to the insulation of screen boxes. For a short period, flexible shunts were also used, but a major deficiency was found in attaching the stationary transformer to a screen that vibrated.\n\nIn exploring ways to properly attach a transformer to a vibrating screen, Hanco International () pioneered the Flux-Power Screen Heating System . The invention came to be in 1961, created by Thomas W. Hannon (), and it was unlike anything of its time. The design eliminated the use of physical connections as seen in earlier screen heating systems. Omitting the mechanical connection also deletes the possibility of primary winding failures that come with short circuits on the secondary of the transformer while eliminating wear and fatigue on the screen, itself. This design also allowed for the transformer to be encapsulated and protected from the elements and dust.\n\nWhen processing basic mineral deposits (clay, shale, limestone, etc.), Flux-Powered Screen Heating Systems are ideal. Heating the screen lessens surface tension from moisture which is a main reason for stickiness against cloth mediums. Introducing heat allows maximum contact and keeps the screen clear, allowing finer material separations and more precise sizing. This, in turn, adds to efficiency.\n\nFlux-Powered Screen Heating Systems have been used and proven effective in screening fertilizers (in handling phosphoric, potash and sulfate rock), foods, chemicals, and in recovering coal for power.\n\n"}
{"id": "23637288", "url": "https://en.wikipedia.org/wiki?curid=23637288", "title": "Space Exploration Vehicle", "text": "Space Exploration Vehicle\n\nThe Space Exploration Vehicle (SEV) is a modular vehicle concept developed by NASA. It would consist of a pressurized cabin that can be mated either with a wheeled chassis to form a rover for planetary surface exploration (on the Moon and elsewhere) or to a flying platform for open space missions such as servicing satellites and missions to near-Earth asteroids. The concept evolved from the Lunar Electric Rover (LER) concept, which in turn was a development of the Small Pressurized Rover (SPR) concept.\n\nConcept vehicles of the Lunar Electric Rover (and later, the SEV) were tested during the Desert Research and Technology Studies in 2008, 2009, 2010 and 2011. One of the LER concept vehicles took part in the presidential inauguration parade of Barack Obama in 2009. The chassis and structural elements of these concept vehicles were fabricated by Off-Road International. Research and testing continued in 2012 in the Johnson Space Center with a mock-up of a free-flying SEV simulating a mission to an asteroid.\n\nDevelopment of the SEV has continued, producing variants called the Multi-Mission Space Exploration Vehicle (MMSEV) and in 2013 a cabin for a possible lunar lander called the Alternate MMSEV (AMMSEV).\n\nThe SEV is developed together with other projects under the Advanced Explorations Systems Program. The program's budget for FY 2010 was $152.9 million.\n\nThe SEV is the size of a small pickup truck, it has 12 wheels, and can house two astronauts for up to two weeks. The SEV consists of a chassis and cabin module. The SEV will allow the attachment of tools such as cranes, cable reels, backhoes and winches. Designed for 2, this vehicle is capable of supporting 4 in an emergency. With wheels that can pivot 360 degrees, the SEV is able to drive in any direction. Astronauts can enter and exit without space suits directly from an airlock docking hatch, or through a suitport without the need to depressurize the habitat module.\n\nThe pressurized module contains a small bathroom with privacy curtains and a shower head producing a water mist for sponge baths. It also contains cabinets for tools, workbench areas and two crew seats that can fold back into beds.\n\n\n\n\n\n"}
{"id": "32551113", "url": "https://en.wikipedia.org/wiki?curid=32551113", "title": "Sripada Yellampalli project", "text": "Sripada Yellampalli project\n\nSripada Yellampalli project is an irrigation project located at Yellampalli Village, Ramagundam Rural Mandal, between Peddapalli district (Old Karimnagar district- Mancherial District (Old Adilabad district) in Telangana State, India. The project is fourth largest on the Godavari River in Telangana region. It is named after late legislator, D. Sripada Rao.\n\nSripada Yellampalli irrigation project foundation was laid by Chief Minister of Andhra Pradesh, Y. S. Rajasekhara Reddy on 28 July 2004.\n\nThe project is designed to utilize about 63 tmc of water at a cost of Rs. 900 crores in the first phase. In the second phase, about 49.5 tmc would be lifted to the upland regions of Karimnagar, Adilabad, Nizamabad, Warangal and Medak districts. 6 tmc water allotted for NTPC Ramagundam project. After the flood gates are installed, the project could would store about 20 Tmcft of water.\n\nThe project operational requirement is 163 MW power and 469 million KWh of electrical energy annually to pump the water.\nThe project would supply water for NTPC power project reservoir in Ramagundam Mandal in Karimnagar.\nIt is started on 2005 near Mormoor and Yellampalli village at Ramagundam and its mainly supplying drinking water to Ramagundam city and Hyderabad city.\n\n"}
{"id": "61983", "url": "https://en.wikipedia.org/wiki?curid=61983", "title": "Tannin", "text": "Tannin\n\nTannins (or tannoids) are a class of astringent, polyphenolic biomolecules that bind to and precipitate proteins and various other organic compounds including amino acids and alkaloids.\n\nThe term \"tannin\" (from Anglo-Norman \"tanner\", from Medieval Latin \"tannāre\", from \"tannum\", oak bark) refers to the use of oak and other bark in tanning animal hides into leather. By extension, the term \"tannin\" is widely applied to any large polyphenolic compound containing sufficient hydroxyls and other suitable groups (such as carboxyls) to form strong complexes with various macromolecules.\n\nThe tannin compounds are widely distributed in many species of plants, where they play a role in protection from predation, and perhaps also as pesticides, and might help in regulating plant growth. The astringency from the tannins is what causes the dry and puckery feeling in the mouth following the consumption of unripened fruit, red wine or tea. Likewise, the destruction or modification of tannins with time plays an important role when determining harvesting times.\n\nTannins have molecular weights ranging from 500 to over 3,000 (gallic acid esters) and up to 20,000 (proanthocyanidins).\n\nThere are three major classes of tannins: Shown below are the base unit or monomer of the tannin. Particularly in the flavone-derived tannins, the base shown must be (additionally) heavily hydroxylated and polymerized in order to give the high molecular weight polyphenol motif that characterizes tannins. Typically, tannin molecules require at least 12 hydroxyl groups and at least five phenyl groups to function as protein binders.\nOligostilbenoids (oligo- or polystilbenes) are oligomeric forms of stilbenoids and constitute a class of tannins.\n\nPseudo tannins are low molecular weight compounds associated with other compounds. They do not change color during the Goldbeater's skin test, unlike hydrolysable and condensed tannins, and cannot be used as tanning compounds. Some examples of pseudo tannins and their sources are:\n\nEllagic acid, gallic acid, and pyrogallic acid were first discovered by chemist Henri Braconnot in 1831. Julius Löwe was the first person to synthesize ellagic acid by heating gallic acid with arsenic acid or silver oxide. \n\nMaximilian Nierenstein studied natural phenols and tannins found in different plant species. Working with Arthur George Perkin, he prepared ellagic acid from algarobilla and certain other fruits in 1905. He suggested its formation from galloyl-glycine by \"Penicillium\" in 1915. Tannase is an enzyme that Nierenstein used to produce m-digallic acid from gallotannins. He proved the presence of catechin in cocoa beans in 1931. He showed in 1945 that luteic acid, a molecule present in the myrobalanitannin, a tannin found in the fruit of \"Terminalia chebula\", is an intermediary compound in the synthesis of ellagic acid.\n\nAt these times, molecule formulas were determined through combustion analysis. The discovery in 1943 by Martin and Synge of paper chromatography provided for the first time the means of surveying the phenolic constituents of plants and for their separation and identification. There was an explosion of activity in this field after 1945, including prominent work by Edgar Charles Bate-Smith and Tony Swain at Cambridge University.\n\nIn 1966, Edwin Haslam proposed a first comprehensive definition of plant polyphenols based on the earlier proposals of Bate-Smith, Swain and Theodore White, which includes specific structural characteristics common to all phenolics having a tanning property. It is referred to as the White–Bate-Smith–Swain–Haslam (WBSSH) definition.\n\nTannins are distributed in species throughout the plant kingdom. They are commonly found in both gymnosperms as well as angiosperms. Mole studied the distribution of tannin in 180 families of dicotyledons and 44 families of monocotyledons (Cronquist). Most families of dicot contain tannin-free species (tested by their ability to precipitate proteins). The best known families of which all species tested contain tannin are: Aceraceae, Actinidiaceae, Anacardiaceae, Bixaceae, Burseraceae, Combretaceae, Dipterocarpaceae, Ericaceae, Grossulariaceae, Myricaceae for dicot and Najadaceae and Typhaceae in Monocot. To the family of the oak, Fagaceae, 73% of the species tested (N = 22) contain tannin. For those of acacias, Mimosaceae, only 39% of the species tested (N = 28) contain tannin, among Solanaceae rate drops to 6% and 4% for the Asteraceae. Some families like the Boraginaceae, Cucurbitaceae, Papaveraceae contain no tannin-rich species.\n\nThe most abundant polyphenols are the condensed tannins, found in virtually all families of plants, and comprising up to 50% of the dry weight of leaves. Tannins of tropical woods tend to be of a cathetic nature rather than of the gallic type present in temperate woods.\n\nThere may be a loss in the bio-availability of still other tannins in plants due to birds, pests, and other pathogens.\n\nTannins are found in leaf, bud, seed, root, and stem tissues. An example of the location of the tannins in stem tissue is that they are often found in the growth areas of trees, such as the secondary phloem and xylem and the layer between the cortex and epidermis. Tannins may help regulate the growth of these tissues.\n\nIn all vascular plants studied so far, tannins are manufactured by a chloroplast-derived organelle, the tannosome. Tannins are mainly physically located in the vacuoles or surface wax of plants. These storage sites keep tannins active against plant predators, but also keep some tannins from affecting plant metabolism while the plant tissue is alive; it is only after cell breakdown and death that the tannins are active in metabolic effects.\n\nTannins are classified as ergastic substances, i.e., non-protoplasm materials found in cells. Tannins, by definition, precipitate proteins. In this condition, they must be stored in organelles able to withstand the protein precipitation process. Idioblasts are isolated plant cells which differ from neighboring tissues and contain non-living substances. They have various functions such as storage of reserves, excretory materials, pigments, and minerals. They could contain oil, latex, gum, resin or pigments etc. They also can contain tannins. In Japanese persimmon (\"Diospyros kaki\") fruits, tannin is accumulated in the vacuole of tannin cells, which are idioblasts of parenchyma cells in the flesh.\n\nThe convergent evolution of tannin-rich plant communities has occurred on nutrient-poor acidic soils throughout the world. Tannins were once believed to function as anti-herbivore defenses, but more and more ecologists now recognize them as important controllers of decomposition and nitrogen cycling processes. As concern grows about global warming, there is great interest to better understand the role of polyphenols as regulators of carbon cycling, in particular in northern boreal forests.\n\nLeaf litter and other decaying parts of kauri (\"Agathis australis\"), a tree species found in New Zealand, decompose much more slowly than those of most other species. Besides its acidity, the plant also bears substances such as waxes and phenols, most notably tannins, that are harmful to microorganisms.\n\nThe leaching of highly water soluble tannins from decaying vegetation and leaves along a stream may produce what is known as a blackwater river. Water flowing out of bogs has a characteristic brown color from dissolved peat tannins. The presence of tannins (or humic acid) in well water can make it smell bad or taste bitter, but this does not make it unsafe to drink.\n\nTannins leaching from an unprepared driftwood decoration in an aquarium can cause pH lowering and coloring of the water to a tea-like tinge. A way to avoid this is to boil the wood in water several times, discarding the water each time. Using peat as an aquarium substrate can have the same effect. Many hours of boiling the driftwood may need to be followed by many weeks or months of constant soaking and many water changes before the water will stay clear. Adding baking soda to the water to raise its pH level will accelerate the process of leaching, as the more alkaline solution can draw out tannic acid from the wood faster than the pH-neutral water.\n\nSoftwoods, while in general much lower in tannins than hardwoods, are usually not recommended for use in an aquarium so using a hardwood with a very light color, indicating a low tannin content, can be an easy way to avoid tannins. Tannic acid is brown in color, so in general white woods have a low tannin content. Woods with a lot of yellow, red, or brown coloration to them (like cedar, redwood, red oak, etc.) tend to contain a lot of tannin.\n\nThere is no single protocol for extracting tannins from all plant material. The procedures used for tannins are widely variable. It may be that acetone in the extraction solvent increases the total yield by inhibiting interactions between tannins and proteins during extraction or even by breaking hydrogen bonds between tannin-protein complexes.\n\nThere are three groups of methods for the analysis of tannins: precipitation of proteins or alkaloids, reaction with phenolic rings, and depolymerization.\n\nAlkaloids such as caffeine, cinchonine, quinine or strychnine, precipitates polyphenols and tannins. This property can be used in a quantitation method.\n\nWhen goldbeater's skin or ox skin is dipped in HCl, rinsed in water, soaked in the tannin solution for 5 minutes, washed in water, and then treated with 1% FeSO solution, it gives a blue black color if tannin was present.\n\nUse of ferric chloride (FeCl) tests for phenolics in general. Powdered plant leaves of the test plant (1.0 g) are weighed into a beaker and 10 ml of distilled water are added. The mixture is boiled for five minutes. Two drops of 5% FeCl are then added. Production of a greenish precipitate was an indication of the presence of tannins. Alternatively, a portion of the water extract is diluted with distilled water in a ratio of 1:4 and few drops of 10% ferric chloride solution is added. A blue or green color indicates the presence of tannins (Evans, 1989).\n\nThe hide-powder method is used in tannin analysis for leather tannin and the Stiasny method for wood adhesives. Statistical analysis reveals that there is no significant relationship between the results from the hide-powder and the Stiasny methods.\n\n400 mg of sample tannins are dissolved in 100 ml of distilled water. 3 g of slightly chromated hide-powder previously dried in vacuum for 24h over CaCl are added and the mixture stirred for 1 h at ambient temperature. The suspension is filtered without vacuum through a sintered glass filter. The weight gain of the hide-powder expressed as a percentage of the weight of the starting material is equated to the percentage of tannin in the sample.\n\n100 mg of sample tannins are dissolved in 10 ml distilled water. 1 ml of 10M HCl and 2 ml of 37% formaldehyde are added and the mixture heated under reflux for 30 min. The reaction mixture is filtered while hot through a sintered glass filter. The precipitate is washed with hot water (5× 10 ml) and dried over CaCl. The yield of tannin is expressed as a percentage of the weight of the starting material.\n\nThe bark tannins of \"Commiphora angolensis\" have been revealed by the usual color and precipitation reactions and by quantitative determination by the methods of Löwenthal-Procter and of Deijs (formalin-hydrochloric acid method).\n\nColorimetric methods have existed such as the Neubauer-Löwenthal method which uses potassium permanganate as an oxidizing agent and indigo sulfate as an indicator, originally proposed by Löwenthal in 1877. The difficulty is that the establishing of a titer for tannin is not always convenient since it is extremely difficult to obtain the pure tannin. Neubauer proposed to remove this difficulty by establishing the titer not with regard to the tannin but with regard to crystallised oxalic acid, whereby he found that 83 g oxalic acid correspond to 41.20 g tannin. Löwenthal's method has been criticized. For instance, the amount of indigo used is not sufficient to retard noticeably the oxidation of the non-tannins substances. The results obtained by this method are therefore only comparative. A modified method, proposed in 1903 for the quantification of tannins in wine, Feldmann's method, is making use of calcium hypochlorite, instead of potassium permanganate, and indigo sulfate.\n\nStrawberries contain both hydrolyzable and condensed tannins.\n\nMost berries, such as cranberries, and blueberries, contain both hydrolyzable and condensed tannins.\n\nNuts that can be consumed raw, such as hazelnuts, walnuts, and pecans, contain high amounts of tannins. Almonds have a lower content. Tannin concentration in the crude extract of these nuts did not directly translate to the same relationships for the condensed fraction.\n\nCloves, tarragon, cumin, thyme, vanilla, and cinnamon all contain tannins.\n\nMost legumes contain tannins. Red-colored beans contain the most tannins, and white-colored beans have the least. Peanuts without shells have a very low tannin content. Chickpeas (garbanzo beans) have a smaller amount of tannins.\n\nChocolate liquor contains about 6% tannins.\n\nPrincipal human dietary sources of tannins are tea and coffee. Most wines aged in charred oak barrels possess tannins absorbed from the wood. Soils high in clay also contribute to tannins in wine grapes. This concentration gives wine its signature astringency.\n\nCoffee pulp has been found to contain low to trace amounts of tannins.\n\nAlthough citrus fruits do not themselves contain tannins, orange-colored juices often contain food dyes with tannins. Apple juice, grape juices and berry juices are all high in tannins. Sometimes tannins are even added to juices and ciders to create a more astringent feel to the taste.\n\nIn addition to the alpha acids extracted from hops to provide bitterness in beer, condensed tannins are also present. These originate both from the malt and hops. Especially in Germany, trained brewmasters consider the presence of tannins as a flaw. In some styles, the presence of this astringency is acceptable or even desired, as, for example, in a Flanders red ale.\n\nIn lager type beers, the tannins can form a precipitate with specific haze-forming proteins in the beer resulting in turbidity at low temperature. This chill haze can be prevented by removing part of the tannins or part of the haze-forming proteins. Tannins are removed using PVPP, haze-forming proteins by using silica or tannic acid.\n\nTannins have traditionally been considered antinutritional, but it is now known that their beneficial or antinutritional properties depend upon their chemical structure and dosage. The new technologies used to analyze molecular and chemical structures have shown that a division into condensed and hydrolyzable tannins is too simplistic. Recent studies have demonstrated that products containing chestnut tannins included at low dosages (0.15–0.2%) in the diet of chickens may be beneficial.\n\nSome studies suggest that chestnut tannins have positive effects on silage quality in the round bale silages, in particular reducing NPNs (non protein nitrogen) in the lowest wilting level.\n\nImproved fermentability of soya meal nitrogen in the rumen may occur. Studies conducted in 2002 on \"in vitro\" ammonia release and dry matter degradation of soybean meal comparing three different types of tannins (quebracho, acacia and chestnut) demonstrated that chestnut tannins are more efficient in protecting soybean meal from \"in vitro\" degradation by rumen bacteria.\n\nCondensed tannins inhibit herbivore digestion by binding to consumed plant proteins and making them more difficult for animals to digest, and by interfering with protein absorption and digestive enzymes (for more on that topic, see plant defense against herbivory). Many tannin-consuming animals secrete a tannin-binding protein (mucin) in their saliva. Tannin-binding capacity of salivary mucin is directly related to its proline content. Salivary proline-rich proteins (PRPs) are sometimes used to inactivate tannins. One reason is that they inactivate tannins to a greater extent than do dietary proteins resulting in reduced fecal nitrogen losses. PRPs additionally contain non-specific nitrogen and non-essential amino acids making them more convenient than valuable dietary protein.\n\nHistatins, another type of salivary proteins, also precipitate tannins from solution, thus preventing alimentary adsorption.\n\nTannin production began at the beginning of the 19th century with the industrial revolution, to produce tanning material for the need for more leather. Before that time, processes used plant material and were long (up to six months).\n\nThere was a collapse in the vegetable tannin market in the 1950s–1960s, due to the appearance of synthetic tannins, which were invented in response to a scarcity of vegetable tannins during World War II. At that time, many small tannin industry sites closed. Vegetable tannins are estimated to be used for the production of 10–20% of the global leather production.\n\nThe cost of the final product depends on the method used to extract the tannins, in particular the use of solvents, alkali and other chemicals used (for instance glycerin). For large quantities, the most cost-effective method is hot water extraction.\n\nTannic acid is used worldwide as clarifying agent in alcoholic drinks and as aroma ingredient in both alcoholic and soft drinks or juices. Tannins from different botanical origins also find extensive uses in the wine industry.\n\nTannins are an important ingredient in the process of tanning leather. Tanbark from oak, mimosa, chestnut and quebracho tree has traditionally been the primary source of tannery tannin, though inorganic tanning agents are also in use today and account for 90% of the world's leather production.\n\nTannins produce different colors with ferric chloride (either blue, blue black, or green to greenish-black) according to the type of tannin. Iron gall ink is produced by treating a solution of tannins with iron(II) sulfate.\n\nTannin is a component in a type of industrial particleboard adhesive developed jointly by the Tanzania Industrial Research and Development Organization and Forintek Labs Canada. \"Pinus radiata\" tannins has been investigated for the production of wood adhesives.\n\nCondensed tannins, e.g., quebracho tannin, and Hydrolyzable tannins, e.g., chestnut tannin, appear to be able to substitute a high proportion of synthetic phenol in phenol-formaldehyde resins for wood particleboard.\n\nTannins can be used for production of anti-corrosive primer, sold under brand-name \"Nox Primer\" for treatment of rusted steel surfaces prior to painting, rust converter to transform oxidized steel into a smooth sealed surface and rust inhibitor.\n\nThe use of resins made of tannins has been investigated to remove mercury and methylmercury from solution. Immobilized tannins have been tested to recover uranium from seawater.\n\n\n"}
{"id": "42378095", "url": "https://en.wikipedia.org/wiki?curid=42378095", "title": "Ukujima Mega Solar Plant", "text": "Ukujima Mega Solar Plant\n\nThe Ukujima Mega Solar Plant project in Japan would be one of the world's largest photovoltaic solar generating facilities at 480 MW (400 MW). It would be located on the island of Ukujima, off the west coast of Kyushu, and is in a developing stage. The plan suffered a setback in September 2014 when Kyushu Electric announced it would not buy any electricity from any new solar plants. In November 2015, it was announced that Kyushu Electric would allow 400 MW to be connected.\n\n"}
{"id": "7527972", "url": "https://en.wikipedia.org/wiki?curid=7527972", "title": "Unistar", "text": "Unistar\n\nUniStar Nuclear Energy is a wholly owned subsidiary of EDF (Électricité de France), formed in 2007 to develop new nuclear energy facilities in the United States.\n\nUnistar has one project under development, Calvert Cliffs Unit 3 in Maryland - the reference plant for the Areva, U.S. EPR design. The Nuclear Regulatory Commission is currently reviewing UniStar's Combined License (COL) application for this facility. UniStar is also assisting PPL Corporation with the development of their Bell Bend Unit 1 project, which is also a U.S. EPR design.\n\nCreated in 2005 by Constellation Energy and Areva, UniStar Nuclear became UniStar Nuclear Energy in August 2007 - a joint venture by Constellation Energy and EDF. In 2010, EDF acquired 100 percent of UniStar Nuclear Energy.\n\nIn 2008, UniStar also submitted a Combined Operating License Application for a new nuclear unit in Oswego NY, to be named Nine Mile Point Unit 3. UniStar failed to acquire the land rights from Constellation Energy (and other parties) and subsequently terminated the NMP3 project in 2013.\n\nIn February 2014, after significant staff reductions the prior year, UniStar vacated its Baltimore MD office and moved remaining staff to the EDF North America Headquarters in Chevy Chase MD.\n\nUniStar continues to pursue the a COL for the Calvert Cliffs Unit 3 project, however continued delays by AREVA to obtain the U.S. NRC approval of the U.S. EPR design (i.e. Design Certification) have delayed the project.\n\n\n"}
{"id": "16774279", "url": "https://en.wikipedia.org/wiki?curid=16774279", "title": "United States Special Envoy for Eurasian Energy", "text": "United States Special Envoy for Eurasian Energy\n\nThe Special Envoy for Eurasian Energy is a diplomatic position within the United States Department of State. The envoy is \"engage directly with senior European, Central Asian, Russian and other political and business leaders to support the continued development and diversification of the energy sector.\" The position is currently filled by Richard Morningstar.\n\nThe legislation calling for the position was drafted in 2007. In February 2008, responding to questions on the issue at a briefing before the Senate Foreign Relations Committee, Secretary of State Condoleezza Rice reaffirmed that the State Department was looking to appoint a special energy coordinator for the Central Asian and Caspian region. Thomas R. Pickering was reportedly high on the list, but he withdrew himself from consideration in early 2008. On 31 March 2008 the Bush administration named C. Boyden Gray as the first envoy to this position.\n\nOn 14 November 2008, Gray became part of the presidential delegation assigned to attend the Baku Energy Summit in Baku, Azerbaijan. The delegation also included Ambassador to Azerbaijan Anne E. Derse and Secretary of Energy Samuel Bodman. After change of the United States Administration, Gray resigned from this position on 20 January 2009.\n\nRichard L. Morningstar was named to the position on 20 April 2009. He led the United States delegation to the energy conference in Sofia on 24–25 April 2009. On 13 July 2009, Ambassador Morningstar represented the United States at the signing ceremony of the intergovernmental agreement of the Nabucco pipeline. He has strongly opposed the possible participation of Iran in the Nabucco project.\n\n"}
{"id": "32827122", "url": "https://en.wikipedia.org/wiki?curid=32827122", "title": "Urban science", "text": "Urban science\n\nUrban science is an interdisciplinary field that studies diverse urban issues and problems. Based on research findings of various disciplines such as history, economics, sociology, administration, architecture, urban engineering, transportation engineering, landscape architecture, environmental engineering, and geo-informatics, it aims to produce both theoretical and practical knowledge that contributes to understanding and solving the problems of urban issues in contemporary society.\n"}
{"id": "31823380", "url": "https://en.wikipedia.org/wiki?curid=31823380", "title": "Vietnam Atomic Energy Commission", "text": "Vietnam Atomic Energy Commission\n\nVietnam Atomic Energy Commission (abbreviation VAEC) is an agency under The Ministry of Science and Technology of Vietnam government with mission of studying formulation of policies, strategies, planning and plans for atomic energy development in Vietnam; conducting fundamental and applied research on nuclear science and technology, nuclear reactor technology, nuclear reactor fuel and material, radiation protection and nuclear safety, radioactive waste treatment and management technology.\n\n"}
{"id": "1287573", "url": "https://en.wikipedia.org/wiki?curid=1287573", "title": "White gas", "text": "White gas\n\nWhite gas is a common name for a number of flammable substances:\n\n\nWhite gas should not be confused with white spirit, which is more akin to kerosene.\n\n"}
{"id": "34385938", "url": "https://en.wikipedia.org/wiki?curid=34385938", "title": "William Smith (conservationist)", "text": "William Smith (conservationist)\n\nWilliam Walter Smith (14 September 1852 – 3 March 1942) was a New Zealand gardener, naturalist and conservationist. He was born in Hawick, Roxburghshire, Scotland in 1852. He was the initial person in charge of implementing the Scenery Preservation Act of 1903, which led to the creation of scenic and historic reserves.\n"}
{"id": "43539212", "url": "https://en.wikipedia.org/wiki?curid=43539212", "title": "Wooden bullet", "text": "Wooden bullet\n\nWooden bullets are wooden projectiles designed to be fired from a gun. They are intended to be used as non-lethal weapons for crowd control by enforcing \"pain compliance\" at a distance. They have been known to raise large welts or bruises on their targets.\n\nFirst used by British troops in Hong Kong, and then in Northern Ireland, they are still used by some U.S. police forces for crowd control.\n"}
