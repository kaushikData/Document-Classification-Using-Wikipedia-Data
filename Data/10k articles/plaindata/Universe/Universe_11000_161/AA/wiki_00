{"id": "57080098", "url": "https://en.wikipedia.org/wiki?curid=57080098", "title": "Achwa 1 Hydroelectric Power Station", "text": "Achwa 1 Hydroelectric Power Station\n\nThe Achwa 1 Hydroelectric Power Station (A1HPS), also Achwa I Hydroelectric Power Station, is a planned hydroelectric power station in Uganda, with a proposed installed capacity of .\n\nThe facility would be located across the Achwa River, in Gulu District, Northern Uganda. This location is at the border between Gulu District and Pader District, approximately north of the settlement of Aswa. This location is within close proximity of the Achwa 2 Hydroelectric Power Station.\n\nThis is approximately , by road, northeast of Gulu, the largest city in Northern Uganda.\n\nAchwa 1 is a run-of-the-river hydroelectricity project with planned annual output of 274 GWh. This power station is one in a cascade of five power stations planned on the Achwa River totaling . The power generated will be sold to the Uganda Electricity Transmission Company Limited, for integration into the national electricity grid.\n\nThe power generated will be evacuated via the Lira–Gulu–Agago High Voltage Power Line, a 132kV high voltage transmission line, to a substation in Lira, a distance of approximately , where it will be sold to the Uganda Electricity Transmission Company Limited (“UETCL”). Other infrastructure that will be constructed include of service roads, a service road to connect the site to the Gulu-Kitgum Road and a camp for the construction workers.\n\nIn 2016, solicitation for bids to carry out feasibility and environmental impact assessments for this power station, were advertised. The development rights are owned by \"Berkeley Energy\", through its wholly owned Ugandan subsidiary, \"Maji Power Limited.\n\n\n"}
{"id": "12605165", "url": "https://en.wikipedia.org/wiki?curid=12605165", "title": "Arimine Dam", "text": "Arimine Dam\n\nThe is located in Toyama, Toyama Prefecture, Japan built upon the Wada River. The two bends in the middle of the dam is the most defining feature. The Arimine Lake is an artificial lake that was created by construction of the dam.\n\nThe tributaries of the Jōganji River were found to be very attractive for hydroelectric power in the early 20th century due to their high flow and mountainous geography. This caught the attention of the electric utility of the time, the Etchu Electric Power Company, and for the purpose of electricity built a dam on the Wada River. At around the same time the Toyama Prefecture was considering a dam for purposes of flood control and irrigation. Construction of a prefecture managed dam began before World War II but the start of the war halted construction. Later, the Hokuriku Electric Power Company inherited the unfinished dam due to restructuring of the power companies. The plans changed to make the primary purpose of the dam electricity production and was finished in the 50s. It now contributes to electric power as well as irrigation and flood control.\n\n"}
{"id": "24700665", "url": "https://en.wikipedia.org/wiki?curid=24700665", "title": "Arthur Nozik", "text": "Arthur Nozik\n\nArthur Nozik is a researcher at the National Renewable Energy Lab (NREL). He is also a professor at the University of Colorado, which is located in Boulder. He researches semiconductor quantum dots at the National Renewable Energy Laboratory, and is a chemistry professor at the University of Colorado. He also does research for the advancement of solar energy, for which he won the Intergovernmental Renewable Energy Organization (IREO) Award for Science and Technology in 2009.\n\nDr. Arthur Nozik received his bachelor's degree in Chemical Engineering from the Cornell University in 1959, and he earned his PhD from Yale University in 1967. In 1967, he discovered a new transparent conductor (CdSnO)\nThin-Film Devices, which helped develop new applications for solar energy devices. Then he did research on quantization effects in semiconductor quantum dots, for the Allied Chemical Corporation and the American Cyanamid Corporation. He then worked as a group leader of Photoelectrochemistry from 1974 to 1978. He worked in both these places until 1978, when he joined the National Renewable Energy Laboratory (NREL). He has published a little over 150 research papers related to solar cell, quantum dot, semiconductor, silicon solar cells. He has been an editor of The Journal of Physical Chemistry since 1993 and served as Senior Editor. He has reviewed numerous papers for various scientific magazines.\n\n\n"}
{"id": "1000441", "url": "https://en.wikipedia.org/wiki?curid=1000441", "title": "Artificial chemistry", "text": "Artificial chemistry\n\nAn artificial chemistry\n\nis a chemical-like system that usually consists of objects, called molecules, that interact according to rules resembling chemical reaction rules. Artificial chemistries are created and studied in order to understand fundamental properties of chemical systems, including prebiotic evolution, as well as for developing chemical computing systems. Artificial chemistry is a field within computer science wherein chemical reactions—often biochemical ones—are computer-simulated, yielding insights on evolution, self-assembly, and other biochemical phenomena. The field does not use actual chemicals, and should not be confused with either synthetic chemistry or computational chemistry. Rather, bits of information are used to represent the starting molecules, and the end products are examined along with the processes that led to them. The field originated in artificial life but has shown to be a versatile method with applications in many fields such as chemistry, economics, sociology and linguistics.\n\nAn artificial chemistry is defined in general as a triple (S,R,A). In some cases it is sufficient to define it as a tuple (S,I).\n\n\n\n\nArtificial chemistries emerged as a sub-field of artificial life, in particular from strong artificial life. The idea behind this field was that if one wanted to build something alive, it had to be done by a combination of non-living entities. For instance, a cell is itself alive, and yet is a combination of non-living molecules. Artificial chemistry enlists, among others, researchers that believe in an extreme bottom-up approach to artificial life. In artificial life, bits of information were used to represent bacteria or members of a species, each of which moved, multiplied, or died in computer simulations. In artificial chemistry bits of information are used to represent starting molecules capable of reacting with one another. The field has pertained to artificial intelligence by virtue of the fact that, over billions of years, non-living matter evolved into primordial life forms which in turn evolved into intelligent life forms.\n\nThe first reference about Artificial Chemistries come from a Technical paper written by John McCaskill\nWalter Fontana working with Leo Buss then took up the work developing the AlChemy model\nThe model was presented at the second International Conference of Artificial Life.\nIn his first papers he presented the concept of organization, as a set of molecules that is algebraically closed and self-maintaining.\nThis concept was further developed by Dittrich and Speroni di Fenizio into a theory of chemical organizations\n\nTwo main schools of artificial chemistries have been in Japan and Germany.\nIn Japan the main researchers have been Takashi Ikegami\n\nHideaki Suzuki\nand Yasuhiro Suzuki\nIn Germany, it was Wolfgang Banzhaf, who, together with his students Peter Dittrich and Jens Ziegler, developed various artificial chemistry models.\nTheir 2001 paper 'Artificial Chemistries - A Review' became a standard in the field.\nJens Ziegler, as part of his PhD thesis, proved that an artificial chemistry could be used to control a small Khepera robot\nAmong other models, Peter Dittrich developed the Seceder model which is able to explain group formation in society through some simple rules. Since then he became a professor in Jena where he investigates artificial chemistries as a way to define a general theory of constructive dynamical systems.\n\nArtificial Chemistries are often used in the study of protobiology, in trying to bridge the gap between chemistry and biology.\nA further motivation to study artificial chemistries is the interest in constructive dynamical systems. Yasuhiro Suzuki has modeled various systems such as membrane systems, signaling pathways (P53), ecosystems, and enzyme systems by using his method, abstract rewriting system on multisets (ARMS).\n\n\n\n"}
{"id": "9019747", "url": "https://en.wikipedia.org/wiki?curid=9019747", "title": "Bahar (unit)", "text": "Bahar (unit)\n\nBahar (Arabic: ) is an obsolete unit of measurement.\n\n\n"}
{"id": "14176489", "url": "https://en.wikipedia.org/wiki?curid=14176489", "title": "Bible paper", "text": "Bible paper\n\nBible paper, also known as scritta paper, is a thin grade of paper used for printing books which have a large number of pages, such as a dictionary. Technically it is called \"lightweight offset paper\" and is a type of woodfree uncoated paper. This paper grade often contains cotton or linen fibres to increase its strength in spite of its thinness.\n\nIt is used for making Bibles, encyclopedias and dictionaries; as well as some fiction books such as the ones published by the \"Bibliothèque de la Pléiade\". The \"Norton Anthology of English Literature\" is also known for using Bible paper (an essayist from \"The New York Times\" referred to it as \"wispy cigarette paper\").\n\n\n"}
{"id": "7374324", "url": "https://en.wikipedia.org/wiki?curid=7374324", "title": "Canadian Environmental Law Association", "text": "Canadian Environmental Law Association\n\nThe Canadian Environmental Law Association (CELA) is a non-profit, public interest organization established in 1970 to use existing laws to protect the environment and to advocate environmental law reforms. It is also a free legal advisory clinic for the public, and will act at hearings and in courts on behalf of citizens or citizens' groups who are otherwise unable to afford legal assistance. Funded by Legal Aid Ontario, CELA is one of 79 community legal clinics located across Ontario, 15 of which offer services in specialized areas of the law. CELA also undertakes additional educational and law reform projects funded by government and private foundations. CELA was established at the same time as its sister organization the Canadian Institute for Environmental Law and Policy, which does not offer Legal Aid services but focuses on policy related to emerging and neglected environmental issues.\n\n\n"}
{"id": "10592503", "url": "https://en.wikipedia.org/wiki?curid=10592503", "title": "Charging station", "text": "Charging station\n\nAn electric vehicle charging station, also called EV charging station, electric recharging point, charging point, charge point, ECS (Electronic Charging Station) and EVSE (electric vehicle supply equipment), is an element in an infrastructure that supplies electric energy for the recharging of electric vehicles, such as plug-in electric vehicles, including electric cars, neighborhood electric vehicles and plug-in hybrids. At home or work, some electric vehicles have onboard converters that can plug into a standard electrical outlet or a high-capacity appliance outlet. Others either require or can use a charging station that provides electrical conversion, monitoring, or safety functionality. These stations are also needed when traveling, and many support faster charging at higher voltages and currents than are available from residential EVSEs. Public charging stations are typically on-street facilities provided by electric utility companies or located at retail shopping centers and operated by many private companies.\n\nCharging stations provide one or a range of heavy duty or special connectors that conform to the variety of competing standards. Common rapid charging standards include the Combined Charging System, CHAdeMO, and the Tesla Supercharger.\n\n, there were 800,000 electric vehicles and 18,000 charging stations in the United States.\n\nCharging stations fall into four basic contexts:\n\n\nBattery capacity and the capability of handling faster charging are both increasing, and methods of charging have needed to change and improve. New options have also been introduced (on a small scale, including mobile charging stations and charging via inductive charging mats). The differing needs and solutions of various manufacturers has slowed the emergence of standard charging methods, and in 2015, there is a strong recognition of the need for standardization.\n\n, around 50,000 non-residential charging points were deployed in the U.S., Europe, Japan and China. , there are 3,869 CHAdeMO quick chargers deployed around the world, with 1,978 in Japan, 1,181 in Europe and 686 in the United States, 24 in other countries. , Estonia is the first and only country that had completed the deployment of an EV charging network with nationwide coverage, with 165 fast chargers available along highways at a maximum distance of between , and a higher density in urban areas.\n\n, 5,678 public charging stations existed across the United States, with 16,256 public charging points, of which 3,990 were located in California, 1,417 in Texas, and 1,141 in Washington. , about 15,000 charging stations had been installed in Europe.\n\n, Norway, which has the highest electric ownership per capita, had 4,029 charging points and 127 quick charging stations. As part of its commitment to environmental sustainability, the Dutch government initiated a plan to establish over 200 fast (DC) charging stations across the country by 2015. The rollout will be undertaken by Switzerland-based power and automation company ABB and Dutch startup Fastned, and will aim to provide at least one station every 50 kilometres (31 miles) for the Netherlands' 16 million residents. In addition to that, the E-laad foundation installed about 3000 public (slow) charge points since 2009.\n\n, Japan had 1,381 public quick-charge stations, the largest deployment of fast chargers in the world, but only around 300 slow chargers. , China had around 800 public slow charging points, and no fast charging stations. , the country with the highest ratio of quick chargers to electric vehicles (EVSE/EV) was Japan, with a ratio of 0.030, and the Netherlands had the largest ratio of slow EVSE/EV, with more than 0.50, while the U.S had a slow EVSE/EV ratio of 0.20.\n\n, the largest public charging networks in Australia exist in the capital cities of Perth and Melbourne, with around 30 stations (7 kW AC) established in both cities – smaller networks exist in other capital cities.\n\nIn April 2017, YPF, the state-owned oil company of Argentina, reported that it will install 220 fast-load stations for electric vehicles in 110 of its service stations in national territory.\n\nAlthough the rechargeable electric vehicles and equipment can be recharged from a domestic wall socket, a charging station is usually accessible to multiple electric vehicles and has additional current or connection sensing mechanisms to disconnect the power when the EV is not charging.\n\nThere are two main types of safety sensor:\n\nUntil 2013, there was an issue where Blink chargers were overheating and causing damage to both charger and car. The solution employed by the company was to reduce the maximum current.\n\nThe US based SAE defines Level 1 charging as using a standard 120 volt AC house outlet to charge an electric vehicle. This will take a long time to fully charge the car but if only used to commute or travel short distances, a full charge is not needed or can be done overnight.\n\n240 volt AC charging is known as Level 2 charging. Level 2 charging is similar to household appliances such as clothes driers. Level 2 chargers range from chargers installed in consumer garages, to relatively slow public chargers. They can charge an electric car battery in 4–6 hours. Level 2 chargers are often placed at destinations so that drivers can charge their car while at work or shopping. Level 2 home chargers are best for drivers who use their vehicles more often or require more flexibility. In many countries outside of North and South America, this is the standard household voltage.\n\nLevel 3 charging, also known as DC fast charging, supports charging up to 500 volts. The organization CHAdeMO is working to standardize fast chargers. Level 3 chargers use a 480 V plug delivering 62.5 kW (peak power can be as much as 120 kW and is varied across the charge. The Tesla Supercharger is the most ubiquitous in the United States. For a Tesla Model S 75, a supercharger can add around of range in about 30 minutes or a full charge in around 75 minutes. As of April 2018, Tesla reports that they have 1,210 supercharging stations and is continuously expanding the network.\n\nAnother standards organization, The International Electrotechnical Commission, defines charging in \"modes\" (IEC 62196).\n\nThere are three connection \"cases\":\n\nThere are four plug \"types\":\n\nFor Combined Charging System (CCS) DC charging which requires PLC (Powerline Communications), two extra connectors are added at the bottom of Type 1 or Type 2 vehicle inlets and charging plugs to connect high voltage DC charging stations to the battery of the vehicle. These are commonly known as Combo 1 or Combo 2 connectors. The choice of Combo 1 or Combo 2 style inlets is normally standardised on a per-country basis, so that public charging providers do not need to fit cables with both variants. Generally, North America uses Combo 1 style vehicle inlets, most of the rest of the world uses Combo 2 style vehicle inlets for CCS.\n\nThe vehicle is connected to the power grid through standard socket-outlets present in residences, which depending on the country are usually rated at around 10 A. To use mode 1, the electrical installation must comply with the safety regulations and must have an earthing system, a circuit breaker to protect against overload and an earth leakage protection. The sockets have blanking devices to prevent accidental contacts.\n\nThe first limitation is the available power, to avoid risks of:\nThe second limitation is related to the installation's power management.\n\nThe vehicle is connected to the main power grid via household socket-outlets. Charging is done via a single-phase or three-phase network and installation of an earthing cable. A protection device is built into the cable. This solution is more expensive than Mode 1 due to the specificity of the cable.\n\nThe vehicle is connected directly to the electrical network via specific socket and plug and a dedicated circuit. A control and protection function is also installed permanently in the installation. This is the only charging mode that meets the applicable standards regulating electrical installations. It also allows load shedding so that electrical household appliances can be operated during vehicle charging or on the contrary optimise the electric vehicle charging time.\n\nThe electric vehicle is connected to the main power grid through an external charger. Control and protection functions and the vehicle charging cable are installed permanently in the installation.\n\nCharging stations for electric vehicles may not need much new infrastructure in developed countries, less than delivering a new alternative fuel over a new network.\nThe stations can leverage the existing ubiquitous electrical grid and home recharging is an option. For example, polls have shown that more than half of homeowners in the United States have access to a plug to charge their cars. Also most driving is local over short distances which reduces the need for charging mid-trip. In the USA, for example, 78% of commutes are less than round-trip. Nevertheless, longer drives between cities and towns require a network of public charging stations or another method to extend the range of electric vehicles beyond the normal daily commute. One challenge in such infrastructure is the level of demand: an isolated station along a busy highway may see hundreds of customers per hour if every passing electric vehicle has to stop there to complete the trip. In the first half of the 20th century, internal combustion vehicles faced a similar infrastructure problem.\n\nThe charging time depends on the battery capacity and the charging power. In simple terms, the time rate of charge depends on the charging level used, and the charging level depends on the voltage handling of the batteries and charger electronics in the car. The US based SAE defines Level 1 (household 120 VAC) as the slowest, Level 2 (upgraded household 240 VAC) in the middle and Level 3 (super charging, 480 VDC or higher) as the fastest. Level 3 charge time can be as fast as 30 minutes for an 80% charge, although there has been serious industry competition about whose standard should be widely adopted. Charge time can be calculated using the formula: \"Charging Time [h] = Battery Capacity [kWh] / Charging Power [kW]\"\n\nThe battery capacity of a fully charged electric vehicle from electric vehicle automakers (such as Nissan) is about 20 kWh, providing it with an electrical autonomy of about 100 miles. Tesla initially released their Model S with battery capacities of 40 kWh, 60 kWh and 85 kWh with the latter having an estimated range of approximately 480 km; as of January 2018 they have two models, 75 kWh and 100 kWh. Plug in hybrid vehicles have capacity of roughly 3 to 5 kWh, for an electrical autonomy of 20 to 40 kilometers, but the gasoline engine ensures the full autonomy of a conventional vehicle.\n\nAs the electric-only autonomy is still limited, the vehicle has to be charged every two or three days on average. In practice, drivers plug in their vehicles each night, thus starting each day with a full charge.\n\nFor normal charging (up to 7.4 kW), car manufacturers have built a battery charger into the car. A charging cable is used to connect it to the electrical network to supply 230 volt AC current. For quicker charging (22 kW, even 43 kW and more), manufacturers have chosen two solutions:\n\nThe user finds charging an electric vehicle as simple as connecting a normal electrical appliance; however to ensure that this operation takes place in complete safety, the charging system must perform several safety functions and dialogue with the vehicle during connection and charging.\n\nTesla currently gives the owners of its Model S and Model X cars a supercharging credit that gives 400 kWh for free. After that credit is used, drivers using Tesla Superchargers have to pay per kWh. The price ranges from $0.06 to $0.26 per kWh in the United States. The benefit of Tesla superchargers is that they are only usable by Tesla vehicles. Other charging networks are available for non-Tesla vehicles. The Blink network of chargers has both Level 2 and DC Fast Chargers and charges separate rates for members and non members. Their prices range from $0.39 to $0.69 per kWh for members and $0.49 to $0.79 per kWh for non members, depending on location. The ChargePoint network has free chargers and paid chargers that drivers activate with a free membership card. The paid charging stations' prices are based on local rates (similarly to Blink). Other networks use similar payment methods as typical gas stations, in which one pays with cash or a credit card per kWh of electricity.\n\nCurrently charging stations are being installed by public authorities, commercial enterprises and some major employers in order to stimulate the market for vehicles that use alternative fuels to gasoline and diesel fuels. For this reason, most charge stations are currently either provided gratis or accessible to members of certain groups without significant charge (e.g. activated by a free \"membership card\" or by a digital \"day code\").\n\nCharging stations can be found and will be needed where there is on-street parking, at taxi stands, in parking lots (at places of employment, hotels, airports, shopping centers, convenience shops, fast food restaurants, coffeehouses etc.), as well as in the workplaces, in driveways and garages at home. Existing filling stations may also incorporate charging stations. , charging stations have been criticized for being inaccessible, hard to find, out of order, and slow; thus reducing EV expansion. At the same time more gas stations add EV charging stations to meet the increasing demand among EV drivers.\n\nElectric car manufacturers, charging infrastructure providers, and regional governments have entered into many agreements and ventures to promote and provide electric vehicle networks of public charging stations.\n\nThe EV Plug Alliance is an association of 21 European manufacturers which proposes an alternative connecting solution. The project is to impose an IEC norm and to adopt a European standard for the connection solution with sockets and plugs for electric vehicle charging infrastructure. Members (Schneider Electric, Legrand, Scame, Nexans, etc.) argue that the system is safer because they use shutters. General consensus is that the IEC 62196 and IEC 61851-1 already have taken care of safety by making parts non-live when touchable.\n\nThe principal suppliers and manufacturers of charging stations offer a range of options from simple charging posts for roadside use, charging cabinets for covered parking places to fully automated charging stations integrated with power distribution equipment\n\n\nThese companies (among AC slow-charging stations) design and manufacture DC fast charging stations (less than 30 minutes). These systems may offer a restricted charge, stopping at a charge level of 80%, or may change the charging rate to a lower level after a charge level of 80% is reached.\n\nAn operator manages charging stations from one or more manufacturers.\n\n\nReports emerged in late July 2013 of a significant conflict between the companies responsible for the two types of charging plugs. The Japanese-developed CHAdeMO standard is favored by Nissan, Mitsubishi, and Toyota, while the Society of Automotive Engineers’ (SAE) International J1772 Combo standard is backed by FCA, GM, Ford, Volkswagen, and BMW. Both are direct-current quick-charging systems designed to charge the battery of an electric vehicle to 80 percent in approximately 20 minutes, but the two systems are completely incompatible. In light of an ongoing feud between the two groups, experts in the field warned that the momentum of the electric vehicle market will be severely affected. Richard Martin, editorial director for clean technology marketing and consultant firm Navigant Research, stated:\nFast charging, however and whenever it gets built out, is going to be key for the development of a mainstream market for plug-in electric vehicles. The broader conflict between the CHAdeMO and SAE Combo connectors, we see that as a hindrance to the market over the next several years that needs to be worked out.\n\nAs of September 16, 2013, a standard does not exist in Australia for charging connectors. Australia’s first fast-DC charging stations follow the Japanese CHAdeMO standard.\n\nIn the United States, the standard charging station sign is defined in the Federal Highway Administration's \"Manual on Uniform Traffic Control Devices\" (MUTCD) 2009 edition.\n\nIn July 2013, FHWA released interim MUTCD approval for charging station signs located on public roads governed by MUTCD standards.\n\nThere is an open source, public domain European charge station sign proposed.\n\nIn colder areas such as Finland, some northern US states and Canada there already exists some infrastructure for public power outlets provided primarily for use by block heaters and set with circuit breakers that prevent large current draws for other uses. These can sometimes be used to recharge electric vehicles, albeit slowly. In public lots, some such outlets are turned on only when the temperature falls below -20 °C, further limiting their use.\n\nA battery swapping (or switching) station is a place at which a vehicle's discharged battery or battery pack can be immediately swapped for a fully charged one, eliminating the delay involved in waiting for the vehicle's battery to charge.\n\nBattery swapping is common in warehouses using electric forklift trucks. The concept of an exchangeable battery service was first proposed as early as 1896, in order to overcome the limited operating range of electric cars and trucks. It was first put into practice between 1910 and 1924, by Hartford Electric Light Company, through the GeVeCo battery service, and was initially available for electric trucks. The vehicle owner purchased the vehicle, without a battery, from General Vehicle Company (GeVeCo), part-owned by General Electric, and the electricity was purchased from Hartford Electric through the use of an exchangeable battery. Both vehicles and batteries were modified to facilitate a fast battery exchange. The owner paid a variable per-mile charge and a monthly service fee to cover maintenance and storage of the truck. During the period of the service, the vehicles covered more than 6 million miles.\n\nBeginning in 1917, a similar successful service was operated in Chicago for owners of Milburn Electric cars, who also could buy the vehicle without the batteries. A rapid battery replacement system was implemented to keep running 50 electric buses at the 2008 Summer Olympics.\n\nIn recent years, Better Place, Tesla, and Mitsubishi Heavy Industries have been involved with integrating battery switch technology with their electric vehicles to extend driving range. In a battery switch station, the driver does not need to get out of the car while the battery is swapped. Battery swap requires an electric car designed for the \"easy swap\" of batteries. However, electric vehicle manufacturers working on battery switch technology have not standardized on battery access, attachment, dimension, location, or type.\n\nIn 2013, Tesla announced a proprietary charging station service to support owners of Tesla vehicles. A network of Tesla Supercharger stations was supposed to support both battery pack swaps for the Model S, along with the more-widespread fast charging capability for both the Model S and the Tesla Roadster. However, Tesla has abandoned their battery swap initiatives in favor of rapidly expanding fast-charging stations. This decision has driven Tesla to be a market-leader in fast charging stations, amounting to 1,210 stations worldwide, as of April 2018.\n\nThe following benefits are claimed for battery swapping:\n\nThe Better Place network was the first modern commercial deployment of the battery switching model. The Renault Fluence Z.E. was the first electric car enabled with switchable battery technology available for the Better Place network in operation in Israel and Denmark. Better Place used the same technology to swap batteries that F-16 jet fighter aircraft use to load their bombs. Better Place launched its first battery-swapping station in Israel, in Kiryat Ekron, near Rehovot in March 2011. The battery exchange process took five minutes. , about 600 Fluence Z.E.s had been sold in the country. Sales during the first quarter of 2013 improved, with 297 cars sold, bringing the total fleet in Israel close to 900. , there were 17 battery switch stations fully operational in Denmark, enabling customers to drive anywhere across the country in an electric car. Fluence Z.E. sales totaled 198 units through December 2012.\n\nBetter Place filed for bankruptcy in Israel in May 2013. The company's financial difficulties were caused by the high investment required to develop the charging and swapping infrastructure, about million in private capital, and a market penetration significantly lower than originally predicted by Shai Agassi. Fewer than 1,000 Fluence Z.E. cars had been deployed in Israel and only around 400 units in Denmark. Under Better Place's business model, the company owned the batteries, so the court liquidator had to decide what to do with customers who did not have ownership of the battery and risked being left with a useless car.\n\nTesla designed its Model S to allow fast battery swapping. In June 2013, Tesla announced its goal of deploying a battery swapping station in each of its supercharging stations. At a demonstration event, Tesla showed that a battery swap operation with the Model S took just over 90 seconds, about half the time it takes to refill a gasoline-powered car used for comparison purposes during the event.\n\nThe first stations were planned to be deployed along Interstate 5 in California because, according to Tesla, a large number of Model S sedans make the San Francisco-Los Angeles trip regularly. Those stations were to be followed by ones on the Washington, DC to Boston corridor. Elon Musk said the service would be offered for the price of about of gasoline at the current local rate, around to at June 2013 prices. Owners could pick up their battery pack fully charged on the return trip, which was included in the swap fee. Tesla would also offer the option to keep the pack received on the swap and pay the price difference if the battery received was newer, or to receive the original pack back from Tesla for a transport fee. Pricing had not been determined.\n\nIn June 2015, Musk indicated that Tesla was likely to abandon its plans to build a network of swap stations. He told his company's shareholders that, despite inviting all Model S owners in the California area to try out the one existing facility, at Harris Ranch, only four or five people had done so. Consequently, it was unlikely that the concept was worth expanding.\n\n\nGogoro has announced their intention to launch the Gogoro Energy Network in 2015. The network is built on the idea of distributed GoStations which will serve as battery swapping locations for Gogoro's Smartscooters.\n\nBattSwap is a new European start-up with battery swap solution. It has a working prototype covered by seed funding received from European angels. Swap station takes only 30 seconds to make a complete swap and is 10x cheaper than Tesla supercharger to build.\nVoltia (formerly Greenway Operator) designed and runs proprietary battery swapping stations (BSS) in Slovakia for switching the batteries in light commercial vehicles. The stations have been in successful commercial operation since 2012.\n\nVoltia's BSS are drive up/drive in station, with a house for a number of batteries to be charged simultaneously. The structure allows drivers to pull up and, using a hydraulic lift, switch their used battery with a new, fully charged one in under 7 minutes. A computer system notifies drivers where to dock their old battery and which new one to take. It is ideal for companies for whom time is of the essence and time spent recharging is time and money.\nThe batteries come in a variety of sizes (40-90kWh), which offer different useful ranges (160–270 km).\n\n\nThese battery swapping solution have been criticized for being proprietary. By creating a monopoly regarding the ownership of the batteries and the patent protected technologies the companies split up the market and decrease the chances of a wider usage of battery swapping.\n\nRecharging a large battery pack presents a high load on the electrical grid, but this can be scheduled for periods of reduced load or reduced electricity costs. In order to schedule the recharging, either the charging station or the vehicle can communicate with the smart grid. Some plug-in vehicles allow the vehicle operator to control recharging through a web interface or smartphone app. Furthermore, in a vehicle-to-grid scenario the vehicle battery can supply energy to the grid at periods of peak demand. This requires additional communication between the grid, charging station, and vehicle electronics. SAE International is developing a range of standards for energy transfer to and from the grid including SAE J2847/1 \"Communication between Plug-in Vehicles and the Utility Grid\". ISO and IEC are also developing a similar series of standards known as ISO/IEC 15118: \"Road vehicles -- Vehicle to grid communication interface\".\n\nCharging stations are usually connected to the electrical grid, which often means that their electricity originates from fossil-fuel power stations or nuclear power plants. Solar power is also suitable for electric vehicles. Nidec Industrial Solutions has designed a system that can be powered by either the grid or renewable energy sources like PV (50-320 kW). SolarCity is marketing its solar energy systems along with electric car charging installations. The company has announced a partnership with Rabobank to make electric car charging available for free to owners of Tesla vehicles traveling on Highway 101 between San Francisco and Los Angeles. Other cars that can make use of same charging technology are welcome.`\nThe SPARC (Solar Powered Automotive ReCharging Station) uses a single custom fabricated monocrystalline solar panel capable of producing 2.7 kW of peak power to charge pure electric or plug-in hybrid to 80% capacity without drawing electricity from the local grid. Plans for the SPARC include a non-grid tied system as well as redundancy for tying to the grid through a renewable power plan. This supports their claim for net-zero driving of electric vehicles.\n\nThe E-Move Charging Station is equipped with eight monocrystalline solar panels, which can supply 1.76 kWp of solar power. With further refinements, the designers are hoping to generate about 2000 kWh of electricity from the panels over the year.\n\nIn 2012, Urban Green Energy introduced the world's first wind-powered electric vehicle charging station, the Sanya SkyPump. The design features a 4 kW vertical-axis wind turbine paired with a GE WattStation.\n\n"}
{"id": "28250168", "url": "https://en.wikipedia.org/wiki?curid=28250168", "title": "Concentrator photovoltaics", "text": "Concentrator photovoltaics\n\nConcentrator photovoltaics (CPV) (also known as Concentration Photovoltaics) is a photovoltaic technology that generates electricity from sunlight. Contrary to conventional photovoltaic systems, it uses lenses and curved mirrors to focus sunlight onto small, but highly efficient, multi-junction (MJ) solar cells. In addition, CPV systems often use solar trackers and sometimes a cooling system to further increase their efficiency. Ongoing research and development is rapidly improving their competitiveness in the utility-scale segment and in areas of high insolation. This sort of solar technology can be thus used in smaller areas.\n\nSystems using high-concentration photovoltaics (HCPV) especially have the potential to become competitive in the near future. They possess the highest efficiency of all existing PV technologies, and a smaller photovoltaic array also reduces the balance of system costs. Currently, CPV is not used in the PV rooftop segment and is far less common than conventional PV systems. For regions with a high annual direct normal irradiance of 2000 kilowatt-hour (kWh) per square meter or more, the levelized cost of electricity is in the range of $0.08–$0.15 per kWh and installation cost for a 10-megawatt CPV power plant was identified to lie between €1.40–€2.20 (~$1.50-$2.30) per watt-peak (W).\n\nIn 2016, cumulative CPV installations reached 350 megawatts (MW), less than 0.2% of the global installed capacity of 230,000 MW. Commercial HCPV systems reached instantaneous (\"spot\") efficiencies of up to 42% under standard test conditions (with concentration levels above 400) and the International Energy Agency sees potential to increase the efficiency of this technology to 50% by the mid-2020s. As of December 2014, the best lab cell efficiency for concentrator MJ-cells reached 46% (four or more junctions). Under outdoor, operating conditions, CPV module efficiencies have exceeded 33% (\"one third of a sun\"). System-level AC efficiencies are in the range of 25-28%. CPV installations are located in China, the United States, South Africa, Italy and Spain.\n\nHCPV directly competes with concentrated solar power (CSP) as both technologies are suited best for areas with high direct normal irradiance, which are also known as the Sun Belt region in the United States and the Golden Banana in Southern Europe. CPV and CSP are often confused with one another, despite being intrinsically different technologies from the start: CPV uses the photovoltaic effect to directly generate electricity from sunlight, while CSP – often called \"concentrated solar thermal\" – uses the heat from the sun's radiation in order to make steam to drive a turbine, that then produces electricity using a generator. Currently, CSP is more common than CPV.\n\nResearch into concentrator photovoltaics has taken place since the mid 1970s, initially spurred on by the energy shock from a mideast oil embargo. Sandia National Laboratories in Albuquerque, New Mexico was the site for most of the early work, with the first modern-like photovoltaic concentrating system produced there late in the decade. Their first system was a linear-trough concentrator system that used a point focus acrylic Fresnel lens focusing on water-cooled silicon cells and two axis tracking. Cell cooling with a passive heat sink and use of silicone-on-glass Fresnel lenses was demonstrated in 1979 by the Ramón Areces Project at the Institute of Solar Energy of the Technical University of Madrid. The 350 kW SOLERAS project in Saudi Arabia—the largest until many years later—was constructed by Sandia/Martin Marietta in 1981.\n\nResearch and development continued through the 1980s and 1990s without significant industry interest. Improvements in cell efficiency were soon recognized as essential to making the technology economical. However the improvements to Si-based cell technologies used by both concentrators and flat PV failed to favor the system-level economics of CPV. The introduction of III-V Multi-junction solar cells starting in the early 2000s has since provided a clear differentiator. MJ cell efficiencies have improved from 34% (3-junctions) to 46% (4-junctions) at research-scale production levels. A substantial number of multi-MW CPV projects have also been commissioned worldwide since 2010.\n\nModern CPV systems operate most efficiently in highly concentrated sunlight (i.e. concentration levels equivalent to hundreds of suns), as long as the solar cell is kept cool through the use of heat sinks. Diffuse light, which occurs in cloudy and overcast conditions, cannot be highly concentrated using conventional optical components only (i.e. macroscopic lenses and mirrors). Filtered light, which occurs in hazy or polluted conditions, has spectral variations which produce mismatches between the electrical currents generated within the series-connected junctions of spectrally \"tuned\" multi-junction (MJ) photovoltaic cells. These CPV features lead to rapid decreases in power output when atmospheric conditions are less than ideal.\n\nTo produce equal or greater energy per rated watt than conventional PV systems, CPV systems must be located in areas that receive plentiful direct sunlight. This is typically specified as average DNI greater than 5.5-6 kWh/m/day or 2000kWh/m/yr. Otherwise, evaluations of annualized DNI vs. GNI/GHI irradiance data have concluded that conventional PV should still perform better over time than presently available CPV technology in most regions of the world (see for example ).\n\nCPV research and development has been pursued in over 20 countries for more than a decade. The annual CPV-x conference series has served as a primary networking and exchange forum between university, government lab, and industry participants. Government agencies have also continued to encourage a number of specific technology thrusts. \n\nARPA-E announced a first round of R&D funding in late 2015 for the MOSAIC Program (Microscale Optimized Solar-cell Arrays with Integrated Concentration) to further combat the location and expense challenges of existing CPV technology. As stated in the program description: \"MOSAIC projects are grouped into three categories: complete systems that cost effectively integrate micro-CPV for regions such as sunny areas of the U.S. southwest that have high Direct Normal Incident (DNI) solar radiation; complete systems that apply to regions, such as areas of the U.S. Northeast and Midwest, that have low DNI solar radiation or high diffuse solar radiation; and concepts that seek partial solutions to technology challenges.\"\n\nIn Europe the CPVMATCH Program (Concentrating PhotoVoltaic Modules using Advanced Technologies and Cells for Highest efficiencies) aims \"to bring practical performance of HCPV modules closer to theoretical limits\". Efficiency goals achievable by 2019 are identified as 48% for cells and 40% for modules at >800x concentration.\n\nThe Australian Renewable Energy Agency (ARENA) extended its support in 2017 for further commercialization of the HCPV technology developed by Raygen. Their 250kW dense array receivers are the most powerful CPV receivers thus far created, with demonstrated PV efficiency of 40.4% and include usable heat co-generation.\n\nThe design of macroscopic sunlight concentrators for CPV introduces a very specific optical design problem, with features that makes it different from any other optical design. It has to be efficient, suitable for mass production, capable of high concentration, insensitive to manufacturing and mounting inaccuracies, and capable of providing uniform illumination of the cell. All these reasons make nonimaging optics the most suitable for CPV.\n\nFor very low concentrations, the wide acceptance angles of nonimaging optics avoid the need for active solar tracking. For medium and high concentrations, a wide acceptance angle can be seen as a measure of how tolerant the optic is to imperfections in the whole system. It is vital to start with a wide acceptance angle since it must be able to accommodate tracking errors, movements of the system due to wind, imperfectly manufactured optics, imperfectly assembled components, finite stiffness of the supporting structure or its deformation due to aging, among other factors. All of these reduce the initial acceptance angle and, after they are all factored in, the system must still be able to capture the finite angular aperture of sunlight.\n\nAll CPV systems have a concentrating optic and a solar cell. Generally, active solar tracking is necessary. Low-concentration systems often have a simple booster reflector, which can increase solar electric output by over 30% from that of non-concentrator PV systems. Experimental results from such LCPV systems in Canada resulted in energy gains over 40% for prismatic glass and 45% for traditional crystalline silicon PV modules.\n\nSemiconductor properties allow solar cells to operate more efficiently in concentrated light, as long as the cell Junction temperature is kept cool by suitable heat sinks. Efficiency of multi-junction photovoltaic cells developed in research is upward of 44% today, with the potential to approach 50% in the coming years. The theoretical limiting efficiency under concentration approaches 65% for 5 junctions, which is a likely practical maximum.\n\nCPV systems are categorized according to the amount of their solar concentration, measured in \"suns\" (the square of the magnification).\n\nLow concentration PV are systems with a solar concentration of 2–100 suns. For economic reasons, conventional or modified silicon solar cells are typically used, and, at these concentrations, the heat flux is low enough that the cells do not need to be actively cooled. There is now modeling and experimental evidence that standard solar modules do not need any modification, tracking or cooling if the concentration level is low and yet still have increased output of 35% or more.\n\nFrom concentrations of 100 to 300 suns, the CPV systems require two-axis solar tracking and cooling (whether passive or active), which makes them more complex.\n\nHigh concentration photovoltaics (HCPV) systems employ concentrating optics consisting of dish reflectors or fresnel lenses that concentrate sunlight to intensities of 1,000 suns or more. The solar cells require high-capacity heat sinks to prevent thermal destruction and to manage temperature related electrical performance and life expectancy losses. To further exacerbate the concentrated cooling design, the heat sink must be passive, otherwise the power required for active cooling will reduce the overall conversion efficiency and economy. Multi-junction solar cells are currently favored over single junction cells, as they are more efficient and have a lower temperature coefficient (less loss in efficiency with an increase in temperature). The efficiency of both cell types rises with increased concentration; multi-junction efficiency rises faster. Multi-junction solar cells, originally designed for non-concentrating PV on space-based satellites, have been re-designed due to the high-current density encountered with CPV (typically 8 A/cm at 500 suns). Though the cost of multi-junction solar cells is roughly 100 times that of conventional silicon cells of the same area, the small cell area employed makes the relative costs of cells in each system comparable and the system economics favor the multi-junction cells. Multi-junction cell efficiency has now reached 44% in production cells.\n\nThe 44% value given above is for a specific set of conditions known as \"standard test conditions\". These include a specific spectrum, an incident optical power of 850 W/m², and a cell temperature of 25 °C. In a concentrating system, the cell will typically operate under conditions of variable spectrum, lower optical power, and higher temperature. The optics needed to concentrate the light have limited efficiency themselves, in the range of 75–90%. Taking these factors into account, a solar module incorporating a 44% multi-junction cell might deliver a DC efficiency around 36%. Under similar conditions, a crystalline silicon module would deliver an efficiency of less than 18%.\n\nWhen high concentration is needed (500–1000 times), as occurs in the case of high efficiency multi-junction solar cells, it is likely that it will be crucial for commercial success at the system level to achieve such concentration with a sufficient acceptance angle. This allows tolerance in mass production of all components, relaxes the module assembling and system installation, and decreasing the cost of structural elements. Since the main goal of CPV is to make solar energy inexpensive, there can be used only a few surfaces. Decreasing the number of elements and achieving high acceptance angle, can be relaxed optical and mechanical requirements, such as accuracy of the optical surfaces profiles, the module assembling, the installation, the supporting structure, etc. To this end, improvements in sunshape modelling at the system design stage may lead to higher system efficiencies.\n\nConcentrator photovoltaics technology has established its presence in the solar industry over the last several years. The first CPV power plant that exceeded 1 MW-level was commissioned in Spain in 2006. By the end of 2015, the number of CPV power plants around the world accounted for a total installed capacity of 350 MW. Field data collected over the past six years is also starting to benchmark the prospects for long-term system reliability.\n\nThe emerging CPV segment has comprised ~0.1% of the fast-growing utility market for PV installations over the past decade. Unfortunately, by the end of 2015, the near term outlook for CPV industry growth has faded with closure of all of the largest CPV manufacturing facilities: including those of Suncore, Soitec, Amonix, and Solfocus. Nevertheless, the growth outlook for the overall PV industry continues to appear strong.\n\nThe largest CPV power plant currently in operation is of 80 MW capacity located in Golmud, China, hosted by Suncore Photovoltaics.\n\nConcentrator photovoltaics and thermal (CPVT), also sometimes called combined heat and power solar (CHAPS) or hybrid thermal CPV, is a cogeneration or micro cogeneration technology used in the field of concentrator photovoltaics that produces usable heat and electricity within the same system. CPVT at high concentrations of over 100 suns (HCPVT) utilizes similar components as HCPV, including dual-axis tracking and multi-junction photovoltaic cells. A fluid actively cools the integrated thermal–photovoltaic receiver, and simultaneously transports the collected heat.\n\nTypically, one or more receivers and a heat exchanger operate within a closed thermal loop. To maintain efficient overall operation and avoid damage from thermal runaway, the demand for heat from the secondary side of the exchanger must be consistently high. Under such optimal operating conditions, collection efficiencies exceeding 70% (up to ~35% electric, ~40% thermal for HCPVT) are anticipated. Net operating efficiencies may be substantially lower depending on how well a system is engineered to match the demands of the particular thermal application.\n\nThe maximum temperature of CPVT systems is typically too low alone to power a boiler for additional steam-based cogeneration of electricity. Such systems may be economical to power lower temperature applications having a constant high heat demand. The heat may be employed in district heating, water heating and air conditioning, desalination or process heat. For applications having lower or intermittent heat demand, a system may be augmented with a switchable heat dump to the external environment in order to maintain reliable electrical output and safeguard cell life, despite the resulting reduction in net operating efficiency.\n\nHCPVT active cooling enables the use of much higher power thermal–photovoltaic receiver units, generating typically 1–100 kilowatts electric, as compared to HCPV systems that mostly rely upon passive cooling of single ~20W cells. Such high-power receivers utilize dense arrays of cells mounted on a high-efficiency heat sink. Minimizing the number of individual receiver units is a simplification that should ultimately yield improvement in the overall balance of system costs, manufacturability, maintainability/upgradeability, and reliability.\n\nThe maximum operating temperatures (T) of CPVT systems are limited to less than approximately 100–125 °C on account of the intrinsic reliability limitation of their multi-junction PV cells. This contrasts to CSP and other CHP systems which may be designed to function at temperatures in excess of several hundred degrees. More specifically, the multi-junction photovoltaic cells are fabricated from a layering of thin-film having intrinsic lifetimes during CPV operation that rapidly decrease with an Arrhenius-type temperature dependence. The system receiver must therefore provide for highly efficient and uniform cell cooling, where an ideal receiver would provide T ~ T. In addition to material and design limitations in receiver heat-transfer performance, numerous extrinsic factors, such as the frequent system thermal cycling, further reduce the practical T compatible with long system life to below about 80 °C.\n\nThe higher capital costs, lesser standardization, and added engineering & operational complexities (in comparison to zero and low-concentration PV technologies) make demonstrations of system reliability and long-life performance critical challenges for the first generation of CPV and CPVT technologies. Performance certification testing standards (e.g. IEC 62108, UL 8703, IEC 62789, IEC 62670) include stress conditions that may be useful to uncover some predominantly infant and early life (<1–2 year) failure modes at the system, module, and sub-component levels. However, such standardized tests – as typically performed on only a small sampling of units – are generally incapable to evaluate comprehensive long-term (10 to 25 or more years) lifetimes for each unique CPVT system design and application under its broader range of actual operating conditions. Long-life performance of these complex systems is therefore assessed in the field, and is improved through aggressive product development cycles which are guided by the results of accelerated component/system aging, enhanced performance monitoring diagnostics, and failure analysis. Significant growth in the deployment of CPV and CPVT can be anticipated once the long-term performance and reliability concerns are better addressed to build confidence in system bankability.\n\nThe economics of a mature CPVT industry is anticipated to be competitive, despite the large recent cost reductions and gradual efficiency improvements for conventional silicon PV (which can be installed alongside conventional CSP to provide for similar electrical+thermal generation capabilities). CPVT may currently be economical for niche markets having all of the following application characteristics:\n\nUtilization of a power purchase agreement (PPA), government assistance programs, and innovative financing schemes are also helping potential manufacturers and users to mitigate the risks of early CPVT technology adoption.\n\nCPVT equipment offerings ranging from low (LCPVT) to high (HCPVT) concentration are now being deployed by several startup ventures. As such, longer-term viability of the technical and/or business approach being pursued by any individual system provider is typically speculative. Notably, the minimum viable products of startups can vary widely in their attention to reliability engineering. Nevertheless, the following incomplete compilation is offered to assist with the identification of some early industry trends.\n\nLCPVT systems at ~14x concentration using reflective trough concentrators, and receiver pipes clad with silicon cells having dense interconnects, have been assembled by Cogenra with a claimed 75% efficiency (~15-20% electric, 60% thermal). Several such systems are in operation for more than 5 years as of 2015, and similar systems are being produced by Absolicon and Idhelio at 10x and 50x concentration, respectively.\n\nHCPVT offerings at over 700x concentration have more recently emerged, and may be classified into three power tiers. Third tier systems are distributed generators consisting of large arrays of ~20W single-cell receiver/collector units, similar to those previously pioneered by Amonix and SolFocus for HCPV. Second tier systems utilize localized dense-arrays of cells that produce 1-100 kW of electrical power output per receiver/generator unit. First tier systems exceed 100 kW of electrical output and are most aggressive in targeting the utility market.\n\nSeveral HCPVT system providers are listed in the following table. Nearly all are early demonstration systems which have been in service for under 5 years as of 2015. Collected thermal power is typically 1.5x-2x the rated electrical power.\n\n\n"}
{"id": "28989907", "url": "https://en.wikipedia.org/wiki?curid=28989907", "title": "Demographics of Oceania", "text": "Demographics of Oceania\n\nOceania is a region centered on the islands of the tropical Pacific Ocean. Conceptions of what constitutes Oceania vary, with it being defined in various ways, often geopolitically or geographically. In the geopolitical conception used by the United Nations, International Olympic Committee, and many atlases, the Oceanic region includes Australia and the nations of the Pacific from Papua New Guinea east, but not the Malay Archipelago or Indonesian New Guinea. The term is sometimes used more specifically to denote Australasia as a geographic continent, \nor biogeographically as a synonym for either the Australasian ecozone (Wallacea and Australasia) or the Pacific ecozone (Melanesia, Polynesia, and Micronesia apart either from New Zealand or from mainland New Guinea).\n\nAlthough Christmas Island and the Cocos (Keeling) Islands belong to the Commonwealth of Australia and are inhabited, they are nearer Indonesia than the Australian mainland, and are commonly associated with Asia instead of Oceania.\n\nThe demographic table below shows all inhabited states and territories of Oceania. The information in this chart comes from the CIA World Factbook or the United States Department of State, unless noted otherwise or not available (NA); where sources differ, references are included.\n\n"}
{"id": "34298214", "url": "https://en.wikipedia.org/wiki?curid=34298214", "title": "Dragontrail", "text": "Dragontrail\n\nDragontrail, manufactured by Asahi Glass Co., is an alkali-aluminosilicate sheet glass engineered for a combination of thinness, lightness and damage-resistance, similar to Corning's Gorilla Glass. The material's primary properties are its strength, allowing thin glass without fragility; its high scratch resistance; and its hardness with a Vickers hardness test rating of 595 to 673.\n\nTo date, some of the cell-phone models that have incorporated this type of protection are:\n\n\n"}
{"id": "54022239", "url": "https://en.wikipedia.org/wiki?curid=54022239", "title": "Dual graviton", "text": "Dual graviton\n\nIn theoretical physics, the dual graviton is a hypothetical elementary particle that is a dual of the graviton under electric-magnetic duality predicted by some formulations of supergravity in eleven dimensions.\n\nThe dual graviton was first hypothesized in 1980. It was theoretically modeled in 2000s, which was then predicted in eleven-dimensional mathematics of SO(8) supergravity in the framework of electric-magnetic duality. It again emerged in the \"E\" generalized geometry in eleven dimensions, and the E7 generalized vielbeine-geometry in eleven dimensions. While there is no local coupling between graviton and dual graviton, the field introduced by dual graviton may be coupled to a BF model as non-local gravitational fields in extra dimensions.\n\nThe dual formulations of linearized gravity are described by a mixed Young symmetry tensor formula_1, the so-called dual graviton, in any spacetime dimension \"D\" > 4 with the following characters:\nwhere square brackets show antisymmetrization.\n\nFor 5-D spacetime, the spin-2 dual graviton is described by the Curtright field formula_4. The symmetry properties imply that\n\nThe Lagrangian action for the spin-2 dual graviton formula_7 in 5-D spacetime, the Curtright field, becomes\nwhere formula_9 is defined as\nand the gauge symmetry of the Curtright field is\nThe dual Riemann curvature tensor of the dual graviton is defined as follows:\nand the dual Ricci curvature tensor and scalar curvature of the dual graviton become, respectively \nThey fulfill the following Bianchi identities\nwhere formula_16 is the 5-D spacetime metric.\n\nDual gravitons have interaction with topological BF model in \"D\" = 5 through the following Lagrangian action\nwhere\nHere, formula_19 is the curvature form, and formula_20 is the background field.\n\nIn principle, it should similarly be coupled to a BF model of gravity as the linearized Einstein–Hilbert action in \"D\" > 4:\nwhere formula_22 is the determinant of the metric tensor matrix, and formula_23 is the Ricci scalar.\n\nIn similar manner while we define gravitomagnetic and gravitoelectic for the graviton, we can define electric and magnetic fields for the dual graviton. There are the following relation between the gravitoelectic field formula_24 and gravitomagnetic field formula_25 of the graviton formula_26 and the gravitoelectic field formula_27 and gravitomagnetic field formula_28 of the dual graviton formula_29:\nand scalar curvature formula_23 with dual scalar curvature formula_33:\nwhere formula_36 denotes the Hodge dual.\n\nThe free (4,0) conformal gravity in \"D\" = 6 is defined as\nwhere formula_38 is the Weyl tensor in \"D\" = 6. The free (4,0) conformal gravity can be reduced to the graviton in the ordinary space, and the dual graviton in the dual space in \"D\" = 4.\n"}
{"id": "277750", "url": "https://en.wikipedia.org/wiki?curid=277750", "title": "Electric power industry", "text": "Electric power industry\n\nThe electric power industry covers the generation, transmission, distribution and sale of electric power to the general public and industry. The commercial distribution of electric power started in 1882 when electricity was produced for electric lighting. In the 1880s and 1890s, growing economic and safety concerns lead to the regulation of the industry. Once an expensive novelty limited to the most densely populated areas, reliable and economical electric power has become an essential aspect for normal operation of all elements of developed economies.\n\nBy the middle of the 20th century, electricity was seen as a \"natural monopoly\", only efficient if a restricted number of organizations participated in the market; in some areas, vertically-integrated companies provide all stages from generation to retail, and only governmental supervision regulated the rate of return and cost structure.\n\nSince the 1990s, many regions have opened up the generation and distribution of electric power to provide a more competitive electricity market. While such markets can be abusively manipulated with consequent adverse price and reliability impact to consumers, generally competitive production of electrical energy leads to worthwhile improvements in efficiency. However, transmission and distribution are harder problems since returns on investment are not as easy to find.\n\nAlthough electricity had been known to be produced as a result of the chemical reactions that take place in an electrolytic cell since Alessandro Volta developed the voltaic pile in 1800, its production by this means was, and still is, expensive. In 1831, Michael Faraday devised a machine that generated electricity from rotary motion, but it took almost 50 years for the technology to reach a commercially viable stage. In 1878, in the United States, Thomas Edison developed and sold a commercially viable replacement for gas lighting and heating using locally generated and distributed direct current electricity.\n\nThe world's first public electricity supply was provided in late 1881, when the streets of the Surrey town of Godalming in the UK were lit with electric light. This system was powered from a water wheel on the River Wey, which drove a Siemens alternator that supplied a number of arc lamps within the town. This supply scheme also provided electricity to a number of shops and premises to light 34 incandescent Swan light bulbs.\n\nAdditionally, Robert Hammond, in December 1881, demonstrated the new electric light in the Sussex town of Brighton in the UK for a trial period. The ensuing success of this installation enabled Hammond to put this venture on both a commercial and legal footing, as a number of shop owners wanted to use the new electric light. Thus the Hammond Electricity Supply Co. was launched. Whilst the Godalming and Holborn Viaduct Schemes closed after a few years the Brighton Scheme continued on, and supply was in 1887 made available for 24 hours per day.\n\nIn early 1882, Edison opened the world’s first steam-powered electricity generating station at Holborn Viaduct in London, where he had entered into an agreement with the City Corporation for a period of three months to provide street lighting. In time he had supplied a number of local consumers with electric light. The method of supply was direct current (DC).\n\nIt was later on in the year in September 1882 that Edison opened the Pearl Street Power Station in New York City and again it was a DC supply. It was for this reason that the generation was close to or on the consumer's premises as Edison had no means of voltage conversion. The voltage chosen for any electrical system is a compromise. For a given amount of power transmitted, increasing the voltage reduces the current and therefore reduces the required wire thickness. Unfortunately it also increases the danger from direct contact and increases the required insulation thickness. Furthermore, some load types were difficult or impossible to make work with higher voltages. The overall effect was that Edison's system required power stations to be within a mile of the consumers. While this could work in city centres, it would be unable to economically supply suburbs with power.\n\nThe mid to late 1880s saw the introduction of alternating current (AC) systems in Europe and the U.S. AC power had an advantage in that transformers, installed at power stations, could be used to raise the voltage from the generators, and transformers at local substations could reduce voltage to supply loads. Increasing the voltage reduced the current in the transmission and distribution lines and hence the size of conductors and distribution losses. This made it more economical to distribute power over long distances. Generators (such as hydroelectric sites) could be located far from the loads. AC and DC competed for a while, during a period called the War of Currents. The DC system was able to claim slightly greater safety, but this difference was not great enough to overwhelm the enormous technical and economic advantages of alternating current which eventually won out.\nThe AC power system used today developed rapidly, backed by industrialists such as George Westinghouse with Mikhail Dolivo-Dobrovolsky, Galileo Ferraris, Sebastian Ziani de Ferranti, Lucien Gaulard, John Dixon Gibbs, Carl Wilhelm Siemens, William Stanley, Jr., Nikola Tesla, and others contributed to this field.\n\nWhile high-voltage direct current (HVDC) is increasingly being used to transmit large quantities of electricity over long distances or to connect adjacent asynchronous power systems, the bulk of electricity generation, transmission, distribution and retailing takes place using alternating current.\n\nThe business model behind the electric utility has changed over the years playing a vital role in shaping the electricity industry into what it is today; from generation, transmission, distribution, to the final local retailing. This has occurred prominently since the reform of the electricity supply industry in England and Wales in 1990. In some countries, wholesale electricity markets operate, with generators and retailers trading electricity in a similar manner to shares and currency. As deregulation continues further, utilities are driven to sell their assets as the energy market follows in line with the gas market in use of the futures and spot markets and other financial arrangements. Even globalization with foreign purchases are taking place. One such purchase was when the UK’s National Grid, the largest private electric utility in the world, bought New England’s electric system for $3.2 billion. Between 1995 and 1997, seven of the 12 Regional Electric Companies (RECs) in England and Wales were bought by U.S. energy companies. Domestically, local electric and gas firms have merged operations as they saw the advantages of joint affiliation, especially with the reduced cost of joint-metering. Technological advances will take place in the competitive wholesale electric markets, such examples already being utilized include fuel cells used in space flight; aeroderivative gas turbines used in jet aircraft; solar engineering and photovoltaic systems; off-shore wind farms; and the communication advances spawned by the digital world, particularly with microprocessing which aids in monitoring and dispatching.\n\nElectricity is expected to see growing demand in the future. The Information Revolution is highly reliant on electric power. Other growth areas include emerging new electricity-exclusive technologies, developments in space conditioning, industrial processes, and transportation (for example hybrid vehicles, locomotives).\n\nThe electric power industry is commonly split up into four processes. These are electricity generation such as a power station, electric power transmission, electricity distribution and electricity retailing. In many countries, electric power companies own the whole infrastructure from generating stations to transmission and distribution infrastructure. For this reason, electric power is viewed as a natural monopoly. The industry is generally heavily regulated, often with price controls and is frequently government-owned and operated. However, the modern trend has been growing deregulation in at least the latter two processes.\n\nThe nature and state of market reform of the electricity market often determines whether electric companies are able to be involved in just some of these processes without having to own the entire infrastructure, or citizens choose which components of infrastructure to patronise. In countries where electricity provision is deregulated, end-users of electricity may opt for more costly green electricity.\n\nAll forms of electricity generation have positive and negative aspects. Technology will probably eventually declare the most preferred forms, but in a market economy, the options with less overall costs generally will be chosen above other sources. It is not clear yet which form can best meet the necessary energy demands or which process can best solve the demand for electricity. There are indications that renewable energy and distributed generation are becoming more viable in economic terms. A diverse mix of generation sources reduces the risks of electricity price spikes.\n\nThe organization of the electrical sector of a country or region varies depending on the economic system of the country. In some places, all electric power generation, transmission and distribution is provided by a government controlled organization. Other regions have private or investor-owned utility companies, city or municipally owned companies, cooperative companies owned by their own customers, or combinations. Generation, transmission and distribution may be offered by a single company, or different organizations may provide each of these portions of the system.\n\n"}
{"id": "1956903", "url": "https://en.wikipedia.org/wiki?curid=1956903", "title": "Fibrominn", "text": "Fibrominn\n\nFibrominn, located in Benson, Minnesota, is the first power plant in the United States designed to burn poultry litter as its main source of fuel. It is producing 55 megawatts of electric power and burning turkey manure combined with wood chips. All of the energy is purchased by Xcel Energy. The plant was developed by Fibrowatt LLC, part of the Homeland Renewable Energy Group, which was set up by the management team which built the world's first three poultry-litter-fueled power plants (in the UK). Construction began in 2005 and the plant began operating in 2007. Grand opening ceremonies were held on 12 October 2007 and 13 October 2007. The plant is now owned by ContourGlobal, LP.\n\n"}
{"id": "28848995", "url": "https://en.wikipedia.org/wiki?curid=28848995", "title": "GDiesel", "text": "GDiesel\n\nGDiesel is a 100% drop-in alternative Diesel fuel that is manufactured by Reno, Nevada-based Advanced Refining Concepts (ARC). This new type of Diesel results from an innovative way of combining conventional ultra-low-sulfur Diesel and natural gas — hence the \"G\" in the name. It burns far cleaner and significantly improves performance, delivering 10 percent or better improvement in fuel economy, depending on the application.\n\nThe inventor of GDiesel® is Dr. Rudolf W. Gunnerman, who has a 40-year background in the development and marketing of energy- and fuel-related technologies. His last venture, Sulfco, which provided an ultrasound and hydrogen peroxide for efficient desulfurization and also promised cheaper fuel, wound up bankrupt after many years of positive press releases that success was just around the corner. His son Peter is the partner and director of Advanced Refining Concepts. The firm's ClearRefining® process is relatively simple, beginning with the standard ultra-low sulfur Diesel fuel that one would buy at any filling station. This feedstock is first pressurized in a steel tank to less than 10 pounds per square inch (69 kPa) and heated to about 250° F, much lower levels than those required during typical refinery processes. Natural gas, the same material used for cooking and heating, is piped into the tank of Diesel fuel, and the resulting mixture then swirls up and through a wheel-shaped filter wrapped with four different metal catalysts — cobalt, among others.\n\nIn August 2010, GDiesel received formal certification as an “alternative fuel” from the Nevada Division of Environmental Protection.\n\nThe fuel is produced at the company’s McCarran, Nevada, facility in which natural gas is combined with standard Diesel fuel and is then circulated through four different metal catalysts.\n\nPreviously, the fuel was featured in publications such as \"Diesel World\" and \"Motor Trend\".\n\n\n"}
{"id": "11084792", "url": "https://en.wikipedia.org/wiki?curid=11084792", "title": "Haradh", "text": "Haradh\n\nHaradh is a small village in Al-Hasa, Saudi Arabia. Haradh is famous for its oil-rich and gas-rich fields. Several oil plants and a gas plant are located in the Haradh area.\n\nHaradh is located on top of the massive Ghawar Field. Saudi Aramco is operating oil plants that are producing about a million barrels a day\nThere is only one automated teller machine near Nadec factory.\n\nThere is a small airport in Haradh for the exclusive use of Saudi Aramco offering flights for its employees to Al-Hasa and Dammam.\n\n"}
{"id": "12661531", "url": "https://en.wikipedia.org/wiki?curid=12661531", "title": "Hausner ratio", "text": "Hausner ratio\n\nThe Hausner ratio is a number that is correlated to the flowability of a powder or granular material. It is named after the engineer Henry H. Hausner (1900–1995).\n\nThe Hausner ratio is calculated by the formula\n\nwhere formula_2 is the freely settled bulk density of the powder, and formula_3 is the tapped bulk density of the powder. The Hausner ratio is not an absolute property of a material; its value can vary depending on the methodology used to determine it.\n\nThe Hausner ratio is used in a wide variety of industries as an indication of the flowability of a powder. A Hausner ratio greater than 1.25 is considered to be an indication of poor flowability. The Hausner ratio (H) is related to the Carr index (C), another indication of flowability, by the formula formula_4. Both the Hausner ratio and the Carr index are sometimes criticized, despite their relationships to flowability being established empirically, as not having a strong theoretical basis. Use of these measures persists, however, because the equipment required to perform the analysis is relatively cheap and the technique is easy to learn.\n"}
{"id": "3201172", "url": "https://en.wikipedia.org/wiki?curid=3201172", "title": "High Power Electric Propulsion", "text": "High Power Electric Propulsion\n\nHigh Power Electric Propulsion (HiPEP) is a variation of ion thruster for use in nuclear electric propulsion applications. It was ground-tested in 2003 by NASA and was intended for use on the Jupiter Icy Moons Orbiter, which was canceled in 2005.\n\nThe HiPEP thruster differs from earlier ion thrusters because the xenon ions are produced using a combination of microwave and magnetic fields. The ionization is achieved through a process called Electron Cyclotron Resonance (ECR). In ECR, the small number of free electrons present in the neutral gas gyrate around the static magnetic field lines. The injected microwaves' frequency is set to match this gyrofrequency and a resonance is established. Energy is transferred from the right-hand polarized portion of the microwave to the electrons. This energy is then transferred to the bulk gas/plasma via the rare - yet important - collisions between electrons and neutrals. During these collisions, electrons can be knocked free from the neutrals, forming ion-electron pairs. The process is a highly efficient means of creating a plasma in low density gases. Previously the electrons required were provided by a hollow cathode.\n\nThe thruster itself is in the 20-50 kW class, with a specific impulse of 6,000-9,000 seconds, and a propellant throughput capability exceeding 100 kg/kW. The goal of the project, as of June 2003, was to achieve a technology readiness level of 4-5 within 2 years.\n\nThe pre-prototype HiPEP produced 670 mN of thrust at a power level of 39.3 kW using 7.0 mg/s of fuel giving a specific impulse of 9620 s. Downrated to 24.4 kW, the HiPEP used 5.6 mg/s of fuel giving a specific impulse of 8270 s and 460 mN of thrust.\n\n\n"}
{"id": "16087606", "url": "https://en.wikipedia.org/wiki?curid=16087606", "title": "History of steam road vehicles", "text": "History of steam road vehicles\n\nThe history of steam road vehicles comprises the development of vehicles powered by a steam engine for use on land and independent of rails, whether for conventional road use, such as the steam car and steam waggon, or for agricultural or heavy haulage work, such as the traction engine.\n\nThe first experimental vehicles were built in the 17th and 18th century, but it was not until after Richard Trevithick had developed the use of high-pressure steam, around 1800, that mobile steam engines became a practical proposition. The first half of the 19th century saw great progress in steam vehicle design, and by the 1850s it was viable to produce them on a commercial basis. This progress was dampened by legislation which limited or prohibited the use of steam powered vehicles on roads. Nevertheless, the 1880s to the 1920s saw continuing improvements in vehicle technology and manufacturing techniques, and steam road vehicles were developed for many applications. In the 20th century, the rapid development of internal combustion engine technology led to the demise of the steam engine as a source of propulsion of vehicles on a commercial basis, with relatively few remaining in use beyond the Second World War. Many of these vehicles were acquired by enthusiasts for preservation, and numerous examples are still in existence. In the 1960s the air pollution problems in California gave rise to a brief period of interest in developing and studying steam powered vehicles as a possible means of reducing the pollution. Apart from interest by steam enthusiasts, the occasional replica vehicle, and experimental technology no steam vehicles are in production at present.\n\nEarly research on the steam engine before 1700 was closely linked to the quest for self-propelled vehicles and ships; the first practical applications from 1712 were stationary plant working at very low pressure which entailed engines of very large dimensions. The size reduction necessary for road transport meant an increase in steam pressure with all the attendant dangers, due to the inadequate boiler technology of the period. A strong opponent of high pressure steam was James Watt who, along with Matthew Boulton did all he could to dissuade William Murdoch from developing and patenting his steam carriage, built in model form in 1784.\n\nFerdinand Verbiest is suggested to have built what may have been the first steam powered car in about 1672, but very little concrete information on this is known to exist.\n\nDuring the latter part of the 18th century, there were numerous attempts to produce self-propelled steerable vehicles. Many remained in the form of models. Progress was dogged by many problems inherent to road vehicles in general, such as suitable power-plant giving steady rotative motion, suspension, braking, steering, adequate road surfaces, tyres, and vibration-resistant bodywork, among other issues. The extreme complexity of these issues can be said to have hampered progress over more than a hundred years, as much as hostile legislation.\n\nNicolas-Joseph Cugnot's \"machine à feu pour le transport de wagons et surtout de l'artillerie\" (\"fire engine for transporting wagons and especially artillery\") was built from 1769 in two versions for use by the French Army. This was the first steam wagon that was not a toy, and that was known to exist. Cugnot's \"fardier\", a term usually applied to a massive two-wheeled cart for exceptionally heavy loads, was intended to be capable of transporting 4 tonnes (3.9 tons), and of travelling at up to 4 km/h (2.5 mph). The vehicle was of tricycle layout, with two rear wheels and a steerable front wheel controlled by a tiller. There is considerable evidence, from the period, that this vehicle actually ran, making it probably the first to do so; however it remained a short-lived experiment due to inherent instability and the vehicle's failure to meet the Army's specified performance level.\n\nIn 1801, Richard Trevithick constructed an experimental steam-driven vehicle (\"Puffing Devil\") which was equipped with a firebox enclosed within the boiler, with one vertical cylinder, the motion of the single piston being transmitted directly to the driving wheels by means of connecting rods. It was reported as weighing 1520 kg fully loaded, with a speed of 14.5 km/h (9 mph) on the flat. During its first trip it was left unattended and \"self-destructed\". Trevithick soon built the \"London Steam Carriage\" that ran successfully in London in 1803, but the venture failed to attract interest and soon folded up.\n\nIn the context of Trevithick's vehicle, an English writer by the name of \"Mickleham\" in 1822 coined the term \"Steam Engine\":\n\nIn 1805 Oliver Evans built the \"Oruktor Amphibolos\" (literally 'amphibious digger'), a steam-powered, flat-bottomed dredger that he modified to be self-propelled on both land and water. It is widely believed to be the first amphibious vehicle, and the first steam-powered road vehicle to run in the United States. However, no designs for the machine survive, and the only accounts of its achievements come from Evans himself. Later analysis of Evans's descriptions suggests that the 5hp engine was unlikely to have been powerful enough to move the vehicle either on land or water, and that the chosen route for its demonstration would have had the benefit of gravity, river currents and tides to assist with the vehicles' progress. The dredger was not a success, and after a few years lying idle, was dismantled for parts.\n\nMore commercially successful for a time than Trevithick's carriage were the steam carriage services operated in England in the 1830s, principally by Walter Hancock and associates of Sir Goldsworthy Gurney, among others and in Scotland by John Scott Russell. However, the heavy road tolls imposed by the Turnpike Acts discouraged steam road vehicles and for a short time allowed the continued monopoly of horse traction until railway trunk routes became established in the 1840s and '50s.\n\nAlthough engineers developed ingenious steam-powered road vehicles, they did not enjoy the same level of acceptance and expansion as steam power at sea and on the railways in the middle and late 19th century of the \"Age of Steam\".\n\nRansomes built a portable steam engine, that is a farm steam engine on wheels, hauled from farm to farm by horses in 1841. The next year Ransomes automated it and had the engine drive itself to farms\n\nHarsh legislation virtually eliminated mechanically propelled vehicles from the roads of Great Britain for 30 years, the \"Locomotive Act\" of 1861 imposing restrictive speed limits on \"road locomotives\" of 5 mph (8 km/h) in towns and cities, and 10 mph (16 km/h) in the country. In 1865 the \"Locomotives Act\" of that year (the famous Red Flag Act) further reduced the speed limits to 4 mph (6.4 km/h) in the country and just 2 mph (3.2 km/h) in towns and cities, additionally requiring a man bearing a red flag (red lantern during the hours of darkness) to precede every vehicle. At the same time, the act gave local authorities the power to specify the hours during which any such vehicle might use the roads. The sole exceptions were street trams which from 1879 onwards were authorised under licence from the Board of Trade.\n\nIn France the situation was radically different from the extent of the 1861 ministerial ruling formally authorising the circulation of steam vehicles on ordinary roads. Whilst this led to considerable technological advances throughout the 1870s and '80s, steam vehicles nevertheless remained a rarity.\n\nTo an extent competition from the successful railway network reduced the need for steam vehicles. From the 1860s onwards, attention was turned more to the development of various forms of traction engine which could either be used for stationary work such as sawing wood and threshing, or for transporting outsize loads too voluminous to go by rail. Steam trucks were also developed but their use was generally confined to the local distribution of heavy materials such as coal and building materials from railway stations and ports.\n\nIn 1858 Thomas Rickett of Buckingham built the first of several steam carriages. Instead of looking like a carriage, it resembled a small locomotive. It consisted of a steam-engine mounted on three wheels: two large driven rear wheels and one smaller front wheel by which the vehicle was steered. The whole was driven by a chain drive and a maximum speed of twelve miles per hour was reached. The weight of the machine was 1.5 tonnes and somewhat lighter than Rickett's steam carriage.\n\nTwo years later, in 1860, Rickett built a similar but heavier vehicle. This model incorporated spur-gear drive instead of chain. In his final design, resembling a railway locomotive, the cylinders were coupled directly outside the cranks of the driving-axle.\n\nIn 1864, Italian inventor Innocenzo Manzetti built a road-steamer. It had the boiler at the front and a single-cylinder engine.\n\nIn 1867, Canadian jeweller Henry Seth Taylor demonstrated his four-wheeled steam buggy at the Stanstead Fair in Stanstead, Quebec, and again the following year. The basis of the buggy, which he began building in 1865, was a high-wheeled carriage with bracing to support a two-cylinder steam engine mounted on the floor.\n\nAround 1867–1869 in France, a Louis-Guillaume Perreaux commercial steam engine was attached to a Pierre Michaux metal framed velocipede, creating the Michaux-Perreaux steam velocipede. Along with the Roper steam velocipede, it might have been the first motorcycle. The only Michaux-Perreaux steam velocipede made is in the Musée de l'Île-de-France, Sceaux, and was included in The Art of the Motorcycle exhibition in New York in 1998.\n\nSylvester H. Roper drove around Boston, Massachusetts on a steam carriage he invented in 1863. One of his 1863 carriages went to the Henry Ford Museum, where, in 1972, it was the oldest car in the collection. Around 1867–1869 he built a steam velocipede, which may have been the first motorcycle. Roper died in 1896 of heart failure while testing a later version of his steam motorcycle.\n\nH.P. Holt constructed a small road-steamer in 1866. Able to reach a speed of twenty miles per hour on level roads, it had a vertical boiler at the rear and two separate twin cylinder engines, each of which drove one rear wheel by means of a chain and sprocket wheels.\n\nIn 1869, a small three-wheeled vehicle propelled by a horizontal twin cylinder engine which drove the rear axle by spur-gearing; only one rear wheel was driven, the other turning freely on the axle. A vertical fire-tube boiler was mounted at the rear with a polished copper casing over the fire box and chimney; the boiler was enclosed in a mahogany casing. The front wheel was used for steering and the weight was only 19 cwt.\n\n1868 - 1870, John Henry Knight of Farnham built a four-wheeled steam carriage which originally only had a single-cylinder engine.\n\n1869, The road-steamer built by Robert William Thomson of Edinburgh became famous because its wheels were shod with heavy solid rubber tyres. Thomson's first road steamers, manufactured in his own small workshop in Leith, were fitted with three wheels, the small single wheel at the front being directly below the steering wheel. The tyres, which were 125 mm (5\") thick, were corrugated internally and adhered to the wheel by friction. He then turned to T. M. Tennant and Co of Bowershall Iron and Engine Works, Leith for their manufacture, but as they could not keep up with demand in 1870 some of the production was moved to Robey & Co of Lincoln. .\nOver the next two years Robeys built 32 of these vehicles, which were either 8 or 12 horse power versions. A large proportion were exported. These included three to Vienna and others to Ireland, Greece, Moscow, New Zealand, Australia and Chile. A further Thomson steam vehicle was built in 1877, but apart from traction engines, Robeys appear to have discontinued making road steam vehicles until 1904, when they started manufacturing steam road lorries.\n\n1872, a steam-coach by Charles Randolph of Glasgow was in length, weighed four and a half tons, but had a maximum speed of only 6 miles per hour. Two vertical twin-cylinder engines were independent of one another and each drove one of the rear wheels by spur-gearing. The entire vehicle was enclosed and fitted with windows all around, carried six people, and even had two driving mirrors for observing traffic approaching from behind, the earliest recorded instance of such a device.\n\nIn 1875, R. Neville Grenville of Glastonbury constructed a 3-wheeled steam vehicle which travelled a maximum of 15 miles per hour. This vehicle is still in existence, preserved for many years in the Bristol City Museum but since 2012 at the National Motor Museum, Beaulieu.\n\nFrom 1873 to 1883 Amédée Bollée of Le Mans built a series of steam-powered passenger vehicles able to carry 6 to 12 people at speeds up to 60 km/h (38 mph), with such names as \"Rapide\" and \"L'Obeissante\". In his vehicles the boiler was mounted behind the passenger compartment with the engine at the front of the vehicle, driving the differential through a shaft with chain drive to the rear wheels. The driver sat behind the engine and steered by means of a wheel mounted on a vertical shaft. The lay-out more closely resembled much later motor cars than other steam vehicles. \"L'Obeissante\", moreover, in 1873 had independent suspension on all four corners.\n\nIn 1892, painter Joens Cederholm and his brother, André, a blacksmith, designed their first car, a two-seater, introducing a condensor in 1894. It was not a success.\n\nStarting in 1894, David Shearer designed and built the first car in Australia. It was capable of 15 miles per hour on the streets of Adelaide, South Australia. The boiler was his own design, being a horizontal boiler of the 'semi-flash' type. Steering was by a tiller type design and a photograph of the vehicle shows it carrying eight passengers. The news article on the car has a sectional drawing of the design. The car’s first official road trial was in 1899.\n\nThe development by Serpollet of the flash steam boiler brought about the appearance of various diminutive steam tricycles and quadricycles during the late 80s and early 90s, notably by de Dion and Bouton; these successfully competed in long distance races but soon met with stiff competition for public favour from the internal combustion engine cars being developed, notably by Peugeot, that quickly cornered most of the popular market. In the face of the flood of IC cars, proponents of the steam car had to fight a long rear-guard battle that was to last into modern times.\n\nThis American firm bought the patents from the Stanley Brothers and began building their steam buggies from 1898 to 1905. Locomobile Company of America went into building gas cars and\nlasted until the Depression.\n\nIn 1902, Francis E. Stanley (1849–1918) and Freelan O. Stanley formed the Stanley Motor Carriage Company. They made famous models such as the 1906 Stanley Rocket, 1908 Stanley K Raceabout and 1923 Stanley Steam Car.\n\nDuring the Crimean war a traction engine was used to pull multiple open trucks.\n\nIn the 1870s many armies experimented with steam tractors pulling road trains of supply wagons.\n\nBy 1898 steam traction engine trains with up to four wagons were employed in military manoeuvres in England.\n\nIn 1900, John Fowler & Co. provided armoured road trains for use by the British forces in the Second Boer War.\n\nIn 1906 the Land Speed Record was broken by a Stanley steam car, piloted by Fred Marriott, which achieved 127 mph (203 km/h) at Ormond Beach, Florida. This annual week-long \"Speed Week\" was the forerunner of today's Daytona 500. This record was not exceeded by any land vehicle until 1910, and stood as the steam-powered world speed record till 25 August 2009.\n\nAttempts were made to bring more advanced steam cars on the market, the most remarkable being the Doble Steam Car which shortened start-up time very noticeably by incorporating a highly efficient monotube steam generator to heat a much smaller quantity of water along with effective automation of burner and water feed control. By 1923, Doble's steam cars could be started from cold with the turn of a key and driven off in 40 seconds or less.\n\nAbner Doble developed the Doble Ultimax engine for the Paxton Phoenix steam car, built by the Paxton Engineering Division of McCulloch Motors Corporation, Los Angeles. Its sustained maximum power was . The project was eventually dropped in 1954.\n\nSteam cars became less popular after the adoption of the electric starter, which eliminated the need for risky hand cranking to start gasoline-powered cars. The introduction of assembly-line mass production by Henry Ford, which hugely reduced the cost of owning a conventional automobile, was also a strong factor in the steam car's demise as the Model T was both cheap and reliable.\n\nIn 1968, renewed interest was shown, sometimes prompted by newly available techniques. An older idea that was resurrected is to use a water-tube generator with fire around it, as opposed to using firetubes heating a boiler of water. A prototype car was built by Charles J. & Calvin E. Williams of Ambler, Pennsylvania. Other high-performance steam cars were built by Richard J.Smith of Midway City, California, and A.M. and E. Pritchard of Caulfeld, Australia. Companies/organisations as Controlled Steam Dynamics of Mesa, Arizona, General Motors, Thermo-Electron Corp. of Waltham, Massachusetts, and Kinetics Inc, of Sarasota, Florida all built high-performance steam engines in the same period. Bill Lear also started work on a closed circuit steam turbine to power cars and buses, and built a transit bus and converted a Chevrolet Monte Carlo sedan to use this turbine system. It used a proprietary working fluid dubbed Learium, possibly a chlorofluorocarbon similar to DuPont Freon.\n\nIn 1970, a variant of the steam car was made by Wallace L.Minto, which works on Ucon U-113 fluorocarbon as the working fluid (instead of steam) and kerosene, gasoline, or the like as a fuel. The car was called the Minto car.\n\nOn 25 August 2009, a team of British engineers from Hampshire ran their steam powered car \"Inspiration\" at Edwards Air Force Base in the Mojave Desert, and averaged over two runs, driven by Charles Burnett III. The car was long and weighed , built from carbon fibre and aluminium and contained 12 boilers with over of steam tubing.\n\nToday most of the problems facing steam cars have been satisfactorily solved, but not the core problem of the low fuel efficiency achieved by practical road going engines.\n\nThe main upside would be the ability to burn fuels currently wasted, such as garbage, wood scrap, and fuels of low value such as crude oil.\n\nCurrently the re-introduction of any modern steam car project would run up against the problem of a general loss of steam engine culture which would make it difficult to set up an infrastructure of spares and qualified mechanics. It would also be necessary to meet more stringent safety standards and legislation than existed in the heyday of steam-powered road vehicles. The biggest arguments in favour of such a movement would be: greatly reduced pollution by particulates and noxious gases without recourse to filters, silence in operation, and direct drive without a gearbox. However the competition which development of a modern steam-powered vehicle has to consider is not so much from gasoline-powered cars as from electric, hydrogen-powered and hybrid vehicles.\n\nProposed nuclear cars would actually be steam powered with the steam generated from the heat of a nuclear reaction. Nuclear power is probably the best known commercial use of steam in the 21st century, because every nuclear power plant uses steam in a similar manner.\n\n\n"}
{"id": "38575149", "url": "https://en.wikipedia.org/wiki?curid=38575149", "title": "Huaneng Geermu Solar Park", "text": "Huaneng Geermu Solar Park\n\nThe Huaneng Geermu Solar Park is a 50 MWp photovoltaic power station located in the Qinghai Province, in China. Most of it uses fixed tilt arrays, but has a 0.25 MW and a 5 MW single axis tracking section. Phase I is 5 MW, Phase II of the first section is 15 MW. 2 MW are amorphous silicon GS-50 modules from GS Solar rated 50 Watts each. The remaining 18 MW is polycrystalline silicon TW230(28)B modules from Tianwei New Energy PV Module rated 230 Watts each. A second section, called the Huaneng Geermu Phase II Solar Power Generation Project, 30 MWp, includes a 5 MW tracking section. The remaining 25 MW is fixed tilt. 15 MW uses TW235P60-FA2 modules from Tianwei New Energy PV Module, 10 MW uses YL235PT-29b modules from Yingli, and 5 MW uses TSM-235PC05 modules from Trina Solar. All of the modules are rated 235 Watts.\n\n"}
{"id": "9206525", "url": "https://en.wikipedia.org/wiki?curid=9206525", "title": "Hydron (chemistry)", "text": "Hydron (chemistry)\n\nIn chemistry, a hydron is the general name for a cationic form of atomic hydrogen, represented with the symbol . However, this term is avoided and instead \"proton\" is used, which strictly speaking refers to the cation of protium, the most common isotope of hydrogen. The term \"hydron\" includes cations of hydrogen regardless of their isotopic composition: thus it refers collectively to protons (H) for the protium isotope, deuterons (H or D) for the deuterium isotope, and tritons (H or T) for the tritium isotope. Unlike most other ions, the hydron consists only of a bare atomic nucleus.\n\nThe negatively charged counterpart of the hydron is the hydride anion, .\n\nHydron compounds are hydrophilic (ionic) solutes. While displaying a preference for solvents with high relative static permittivity (dielectric constants), they dissolve not only in polar compounds, but also in nonpolar compounds.\n\nThe hydron (a completely free or \"naked\" hydrogen atomic nucleus) is too reactive to occur in many liquids, even though it is sometimes visualized to do so by students of chemistry. A free hydron would react with a molecule of the liquid to form a more complicated cation. Examples are the hydronium ion in water-based acids, and , the unstable cation of fluoroantimonic acid, the strongest superacid. For this reason, in such liquids including liquid acids, hydrons diffuse by contact from one complex cation to another, via the Grotthuss mechanism.\n\nThe hydron ion can incorporate an electron pair from a Lewis base into the molecule by adduction:\nBecause of this capture of the Lewis base (L), the hydron ion has Lewis acidic character. The hydron ion is a strong Lewis acid, able to capture one Lewis base.\n\nThe hydrated form of the hydrogen cation, the hydronium (hydroxonium) ion , is a key object of Arrhenius' definition of acid. Other hydrated forms, the Zundel cation which is formed from a proton and two water molecules, and . The hydron itself is crucial in more general Brønsted–Lowry acid–base theory, which extends the concept of acid–base chemistry beyond aqueous solutions.\n\nOther isotopes of hydrogen are too unstable to be relevant in chemistry.\n\nThe term \"hydron\" is recommended by IUPAC to be used instead of \"proton\" if no distinction is made between the isotopes proton, deuteron and triton, all found in naturally occurring undifferentiated isotope mixtures. The name \"proton\" refers to isotopically pure H.\nOn the other hand, referring to the hydron as simply \"hydrogen ion\" is not recommended because hydrogen anions also exist. \n\nThe term \"hydron\" was defined by IUPAC in 1988.\nTraditionally, the term \"proton\" was and is used in place of \"hydron\".\nThe latter term is generally only used in the context where comparisons between the various isotopes of hydrogen is important (as in the kinetic isotope effect or hydrogen isotopic labeling). Otherwise, referring to hydrons as protons is still considered acceptable, for example in such terms as protonation, deprotonation, proton pump or proton channel. The transfer of in an acid-base reaction is usually referred to as \"proton\" transfer. Acid and bases are referred to as \"proton\" donors and acceptors correspondingly.\n\nHowever, although 99.9844% of natural hydrogen nuclei are protons, the remainder (about 156 per million in sea water) are deuterons (see deuterium). A rare triton also occurs (see tritium).\n\n"}
{"id": "26730261", "url": "https://en.wikipedia.org/wiki?curid=26730261", "title": "Hypertriton", "text": "Hypertriton\n\nA hypertriton is a type of hypernucleus, formed of a proton, a neutron and any hyperon. The name comes from \"hyperon\", which refers to baryons containing strange quarks, and \"triton\", which refers to the nucleus of tritium. Because low-mass hyperons are longer-lived and easier to create than high-mass hyperons, the most common hypertritons are those containing Lambda baryons.\n\nIts antiparticle, the antihypertriton, is formed of an antiproton, an antineutron and any antihyperon. The first one was discovered in March 2010 by the STAR detector of the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National Laboratory.\n\nNormal nuclei are formed only of protons and neutrons. To study them, scientists arrange the various nuclides into a two-dimensional table of nuclides. On one axis is the number of neutrons \"N\", and on the other is the number of protons \"Z\". Because the antihyperon introduces a third component (strangeness), the table becomes three-dimensional.\n\n"}
{"id": "20099425", "url": "https://en.wikipedia.org/wiki?curid=20099425", "title": "Kakanj Power Station", "text": "Kakanj Power Station\n\nKakanj Thermal Power Plant is one of Bosnia and Herzegovina's largest coal-fired power plant having an installed electric capacity of 450 MW and producing around 2.3 billion Kwh of electricity per year. The power plant is operated by Elektroprivreda Bosne i Hercegovine.\nThe chimney of Kakanj Power Plant is 300 metres tall and is one of the tallest man-made objects built in former Yugoslavia.\n\nKakanj has been coal mining area since 1898. Construction of the coal-fired power plant started in 1947. and the first and unit was commissioned in 1956.\n\nConstruction of the second unit started in 1960 and was finished in 1960. Five more units were constructed in the next 28 years. The last one, unit 7, was finished in 1988. A new 300-MW steam unit is in planning and a 100-MW CCGT block may be built to replace the old 32-MW sets.\n\nThermal Power Station Kakanj is now one of the largest generators of electricity in Federation of Bosnia and Herzegovina. It has power generation units with an installed capacity of 578 MW. \nThe power plant has a tall stack, one of the tallest structures in Bosnia and Herzegovina.\n\n"}
{"id": "24095413", "url": "https://en.wikipedia.org/wiki?curid=24095413", "title": "Kuzbassenergo", "text": "Kuzbassenergo\n\nKuzbassenergo OAO (also known as TGK-12) is a Russian joint-stock company specialized in the distribution of energy. The company's headquarters are located in the Russian city of Barnaul, in the Altai Krai region (Southwest Siberia). The company is controlled by Siberian Energy Investments Ltd., a holding company controlled by Andrey Melnichenko's company Donalink Ltd.\n\nAccording to the company's official website, Kuzbassenergo generated 24.906 GWh in 2006.\n\nKuzbassenergo's CEO is Sergey Mikhailov. The company was delisted from the Moscow Exchange in January 2016.\n"}
{"id": "11177052", "url": "https://en.wikipedia.org/wiki?curid=11177052", "title": "Linenfold", "text": "Linenfold\n\nLinenfold (or linen fold) is a simple style of relief carving used to decorate wood panelling with a design \"imitating window tracery\", \"imitating folded linen\" or \"stiffly imitating folded material\". Originally from Flanders, the style became widespread across Northern Europe in the 14th to 16th centuries. The name was applied to the decorative style by antiquarian connoisseurs in the early 19th century; the contemporary name was apparently \"lignum undulatum\" (Latin: \"wavy wood\"), Nathaniel Lloyd pointed out.\n\nWood panelling or wainscoting, almost always made from oak, became popular in Northern Europe from the 14th century, after European carpenters rediscovered the techniques to create frame and panel joinery. The framing technique was used from the 13th century onwards to clad interior walls, to form choir stalls, and to manufacture moveable and semi-moveable furniture, such as chests and presses, and even the back panels of joined chairs. Linenfold was developed as a simple technique to decorate the flat surfaces of the ubiquitous panels thus created.\n\nThe simplest linenfold style is \"parchemin\" (also known as \"parchment fold\"), a low relief carving formed like a sheet of paper or piece of linen folded in half and then spread out with the sharp centered fold running vertically, and the top and bottom running out to the corners of the panel, with something of the appearance of an opened book. This style of linenfold can be created using a plane and a pre-drawn pattern, with a little finishing chisel work required at each end. A stitched embroidered border could be counterfeited by the use of punches. More complicated styles resemble a sheet of fabric that has volute folds back and forth many times. Linenfold might be fielded, visually complete against a flat panel surface and contained within each panel, or it might provide the appearance of a continuous linenfold passing behind the stiles of the framing.\n\nCarving linenfold decoration requires little carpentry skill or training, and it can be mass-produced by a semi-skilled workforce: the creased designs were run with a moulding plane; only top and bottom edges needed to be finished with a chisel. Significantly, linenfold had no prototype in architectural practice: the technique of the moulding plane is not applicable to stone-cutting. Workshops were established by the later 15th century to cater to the developing market for inexpensively \"ceiled\" rooms, supplementing the more complicated, and more expensive, patterns of tracery used in the earliest interior panelling.\n\nRegional variations quickly developed in England, France and Germany. The linenfold of France, Netherlands, and Germany \"is carved with a sharper definition and greater delicacy than was usual in England\", where an early linenfold panelling can be seen in the hall screen at Compton Wynyates. Linenfold started to fall out of fashion as Renaissance styles spread in the 16th century, replaced by fielded panels for simpler work, and more complicated \"Roman\" and higher relief carving, but linenfold continued to be used in less sophisticated surroundings well into the 17th century. In the 19th century, linenfold panelling reappeared in the revivals of the Gothic and Tudor styles.\n\n"}
{"id": "7417226", "url": "https://en.wikipedia.org/wiki?curid=7417226", "title": "Lucretia Edwards", "text": "Lucretia Edwards\n\nLucretia W. Edwards (May 15, 1916 Philadelphia, Pennsylvania – October 12, 2005) was an environmental activist and preservationist from Richmond, California. \n\nEdwards is responsible for thousands of acres being added to the East Bay Regional Parks District and the National Park Service. These include the Miller/Knox Regional Shoreline, Point Pinole Regional Shoreline, and Rosie the Riveter/World War II Home Front National Historical Park, which absorbed Lucretia Edwards Park, named in her honor. She was also responsible for the addition of Winehaven, Point Richmond, East Brother Island Lighthouse, and Point Molate to the National Register of Historic Places.\n\nIn 1989 Representative George Miller recognized her in the congressional record; she was chosen the Woman of the Year for the 11th District by Assembly Person Bob Campbell. The California State Senate and Assembly honored her in a series of special ceremonies along with 101 women, a Women Legislators Caucus-sponsored honor.\n\nShe was married to Tom Edwards and had three children. The family lived in a Point Richmond, California, house for 37 years, where she also died.\n\n"}
{"id": "39607405", "url": "https://en.wikipedia.org/wiki?curid=39607405", "title": "Mcgee v. General Motors Corp.", "text": "Mcgee v. General Motors Corp.\n\nMcGee v. General Motors was a 1998 court case in which the jury awarded plaintiffs Robert and Connie McGee $60 million. The trial revealed hidden information about a General Motors fuel tank design. General Motors (GM) was alleged to have sacrificed vehicle safety measures in favor of additional profit. This case was featured on CNN, \"60 Minutes\", \"The New York Times\", and \"USA Today\".\n\nOn July 3, 1991, Robert and Connie McGee were traveling with their children Kelly and Shane in their 1983 Oldsmobile Cutlass Cruiser. While stopped at a toll booth, the car was hit by a trailer which had disconnected from a truck several lanes away, driven by Curtis Cayton. The impact caused the trailer tongue-head to pierce the car’s gas tank, resulting in a gasoline explosion.\n\nAfter the accident, Shane McGee died as a result of severe burns to 98% of his body. Robert, Connie, and Kelly survived, but needed skin grafts to repair burn damage. Connie had to undergo reconstructive surgery due to extensive burns on her hands and face.\n\nThe McGees settled with the truck driver out of court, but sued General Motors for intentionally failing to provide adequate safety measures on the car.\n\nDuring the trial, testimony from several former General Motors engineers showed that the design of the fuel tank was too thin and placed too close to the ground. Also, internal documents showed that the placement of a metal shield could protect the most vulnerable parts of the gas tank and greatly improve safety, at a cost of $4.50 per vehicle.\n\nThe case was tried at the Broward County Circuit Court in Ft. Lauderdale, FL, and the plaintiffs were represented by Robert W. Kelley, John J. Uustal and Sheldon Schlesinger. General Motors was represented by James Feeney, Bud Kirk, Daniel Gerber, Terrence Russell and Jay Lefkowitz. On May 18, 1998, the jury returned a verdict of $60 million in compensatory damages, and ruled that both General Motors and Curtis Cayton were at fault.\n\n"}
{"id": "24957977", "url": "https://en.wikipedia.org/wiki?curid=24957977", "title": "Metglas", "text": "Metglas\n\nMetglas is a thin amorphous metal alloy ribbon produced by using rapid solidification process of approximately . This rapid solidification creates unique ferromagnetic properties that allows the ribbon to be magnetized and de-magnetized quickly and effectively with very low core losses of approximately 5 mW/kg at 60 Hz and a maximum relative permeability of approximately 1,000,000.\n\nMetglas is based on technology developed at AlliedSignal research facilities in Morristown, New Jersey and Vacuumschmelze in Hanau, Germany. The development of amorphous metals began in 1970. Over the years, many new alloys have been found using the same principles of rapid solidification.\n\nMetglas, also known as metallic glass alloys, differ from traditional metals in that they have a non-crystalline structure and possess unique physical and magnetic properties that combine high permeability, strength and hardness with flexibility and toughness.\n\n"}
{"id": "39868194", "url": "https://en.wikipedia.org/wiki?curid=39868194", "title": "Mikhail L. Surguchev", "text": "Mikhail L. Surguchev\n\nMikhail L. Surguchev (1928—1991) was a prominent petroleum scientist in the USSR. He was widely recognized in Russia and internationally as an expert in reservoir engineering, oil field development, waterflooding, enhancing and improving oil recovery (EOR and IOR) methods.\n\nSurguchev studied at Ishimbai Petroleum Engineering High School, and Kuibyshev Industrial Institute (University).\n\nAfter graduating from the University, Mikhail Surguchev obtained his Doctorate at the State Research Institute Giprovostokneft in Samara, where he developed a blocked inter-contour system for oilfield development by waterflooding. This system was implemented in a number of major oil fields, particularly those in the Volga-Ural and Western Siberia Regions.\n\nIn 1966, Dr. Surguchev was invited to join the All-Union Oil and Gas Scientific Research Institute (VNIIneft) in Moscow, the main research institute of the petroleum industry in the country.\nIn 1971 he was appointed Head of the Department of Enhanced Oil Recovery at the VNIIneft Institute; subsequently becoming the Deputy Director, and ultimately the Director. In his years at this Institute, he guided and took part in a range of theoretical and experimental investigations of EOR processes. He was also the project manager for the development of the giant Samotlor oil field in Western Siberia, and was responsible for initiating a number of improved oil recovery projects in reservoirs situated in the Volga-Urals, Western Siberia, and other parts of the country.\n\nIn 1986, Dr. Surguchev became the Director General of the Inter-branch Scientific Technological Complex “Oil Recovery” in Moscow, a position that he held until his untimely death in 1991. Under his leadership, the complex prospered, and has given considerable support to the industry in improving the technology of oil recovery.\n\nDr. Surguchev’s contribution to the science and technology of oil recovery included more than 200 scientific papers, 18 monographs and 30 patented inventions. A large number of his technical innovations were implemented by the petroleum industry.\n\nDr. Surguchev was elected a member of the USSR Academy of Sciences. He was a Laureate of the Lenin Prize for Scientific and Technical Achievements (1966) and an Honored Scientist of the Russian Federation (1989). He has been awarded the USSR Order of the Red Banner of Labour.\n\nDr. Surguchev was widely known and respected throughout the petroleum industry as a strong advocate for international cooperation and the dissemination of information between scientists and engineers. He was the Vice Chairman of the Executive Board of the World Petroleum Congress, and a very active member of the Steering committee for the European Symposia on Improved Oil Recovery.\n\n\n"}
{"id": "872012", "url": "https://en.wikipedia.org/wiki?curid=872012", "title": "Ministry of the Environment (Japan)", "text": "Ministry of the Environment (Japan)\n\nThe is a Cabinet-level ministry of the government of Japan responsible for global environmental conservation, pollution control, and nature conservation. The ministry was formed in 2001 from the sub-cabinet level Environmental Agency established in 1971. The Minister of the Environment is a member of the Cabinet of Japan and is chosen by the Prime Minister, usually from among members of the Diet.\n\nIn March 2006, the then-Minister of the Environment Yuriko Koike, created a \"furoshiki\" cloth to promote its use in the modern world.\n\nIn August 2011, the Cabinet of Japan approved a plan to establish a new energy watchdog under the Environment Ministry, and the Nuclear Regulation Authority was founded on September 19, 2012.\n\nThe Ministry of the Environment began advocating the Cool Biz campaign in summer 2005 as a means to help reduce electric consumption by limiting use of air conditioning and allowing the wearing of less formal officewear. This idea was proposed by then-Minister of the Environment Yuriko Koike under Prime Minister Junichiro Koizumi.\n\nFollowing the Tōhoku earthquake and tsunami in March 2011, the shut down of many nuclear power plants for safety reasons lead to energy shortages. To conserve energy, the government recommended setting air conditioners at 28 degrees Celsius, switching off computers not in use, and called for shifting work hours to the morning and taking more summer vacation than usual. The government then launched a \"Super Cool Biz\" campaign to encourage workers to wear outfits appropriate for the office yet cool enough to endure the summer heat. Polo shirts and trainers are allowed, while jeans and sandals are also acceptable under certain circumstances. June 1 marked the start of the Environment Ministry's campaign, with full-page newspaper ads and photos of ministry workers smiling rather self-consciously at their desks wearing polo shirts and colorful Okinawa kariyushi shirts. The campaign was repeated in 2012 and 2013.\n\n\n"}
{"id": "32879356", "url": "https://en.wikipedia.org/wiki?curid=32879356", "title": "Mår Hydroelectric Power Station", "text": "Mår Hydroelectric Power Station\n\nThe Mår Power Station is a hydroelectric power station located in Tinn, Telemark, Norway. It operates at an installed capacity of , with an average annual production of about .\n"}
{"id": "6363357", "url": "https://en.wikipedia.org/wiki?curid=6363357", "title": "Nurse log", "text": "Nurse log\n\nA nurse log is a fallen tree which, as it decays, provides ecological facilitation to seedlings. Broader definitions include providing shade or support to other plants. Some of the advantages a nurse log offers to a seedling are: water, moss thickness, leaf litter, mycorrhizae, disease protection, nutrients, and sunlight. Recent research into soil pathogens suggests that in some forest communities, pathogens hostile to a particular tree species appear to gather in the vicinity of that species, and to a degree inhibit seedling growth. Nurse logs may therefore provide some measure of protection from these pathogens, thus promoting greater seedling survivorship.\n\nVarious mechanical and biological processes contribute to the breakdown of lignin in fallen trees, resulting in the formation of niches of increasing size, which tend to fill with forest litter such as soil from spring floods, needles, moss, mushrooms and other flora. Mosses also can cover the outside of a log, hastening its decay and supporting other species as rooting media and by retaining water. Small animals such as various squirrels often perch or roost on nurse logs, adding to the litter by food debris and scat. The decay of this detritus contributes to the formation of a rich humus that provides a seedbed and adequate conditions for germination.\nNurse logs often provide a seedbed to conifers in a temperate rain forest ecosystem.\n\n\n"}
{"id": "20933001", "url": "https://en.wikipedia.org/wiki?curid=20933001", "title": "Oreichthys cosuatis", "text": "Oreichthys cosuatis\n\nOreichthys cosuatis is a small (~3.2\"/8 cm TL) cyprinid fish found in India and Bangladesh. It is also reported from Thailand and Myanmar.\n\nThey are strictly freshwater, and are found in ditches, ponds, streams and canals.\nIn an aquarium, they are generally peaceful, and tend not to bother other fishes, even those much smaller than they are although they are susceptible to be bullied by larger more boisterous fish. They prefer cooler water (76 °F/24.4 °C at most, although higher temps are tolerated in the short term providing oxygen levels do not drop).Gentle water flow is preferred and be sure to use a soft substrate as this fish has sensitive bristles which is uses whilst foraging.\n\n\"Oreichthys cosuatis\" is found in the Ganges and Brahmaputra river systems in India, Bangladesh and Nepal. This fish favours slow flowing areas of rivers with dense vegetation and clear water.\nThis fish reaches around 40mm with the male growing slightly larger than the females,slightly more colourful and they have an extended dorsal fin\n"}
{"id": "19420000", "url": "https://en.wikipedia.org/wiki?curid=19420000", "title": "Oryza glaberrima", "text": "Oryza glaberrima\n\nOryza glaberrima, commonly known as African rice, is one of the two domesticated rice species. It was first domesticated and grown in West Africa, and was brought to the Americas by enslaved West African rice farmers. It is now largely a subsistence crop, rarely sold in markets even in West Africa.\n\nWhile it has been partly replaced by higher-yielding Asian rice, and the number of varieties grown is declining, it persists, making up an estimated 20% of rice grown in West Africa. By comparison to Asian rice, it is hardy, pest-resistant, low-labour, suited to a variety of African conditions, filling, and has a distinct nutty flavour. It is also grown for cultural reasons; for instance, it is sacred to awasena followers among the Jola people, and is a heritage variety in the United States.\n\nCrossbreeding between African and Asian rice is difficult, but there exist some crosses.\n\nHumans have independently domesticated two different rice species. African rice was domesticated from wild African rice, \"Oryza barthii\", while Asian rice (\"Oryza sativa\"), was domesticated from wild Asian rice, \"Oryza rufipogon\".\n\n\"Oryza barthii\" still grows wild in Africa, in a wide variety of open habitats. The Sahara was formerly wetter, with massive paleolakes in what is now the Western Sahara. As the climate dried, the wild rice retreated and probably became increasingly domesticated as it relied on humans for irrigation. Rice growing in deeper, more permanent water became floating rice.\nIt is believed to have been domesticated 2000–3000 years ago in the inland delta of the Upper Niger River, in what is now Mali. It then spread through West Africa. It has also been recorded off the \"east\" coast of Africa, in the Zanzibar Archipelago.\n\nWild rice seedheads shatter, scattering the rice grains to seed the next generation. Domestic rice does not shatter, making the grains easy for humans to gather. A mutation that caused rice not to shatter would probably have been the beginning of domestication.\n\nIbn Baṭṭūṭa recorded rice couscous in the area of present-day Mali in 1350.\n\nIn the late fifteenth and sixteenth centuries, the Portuguese sailed to the Southern Rivers area in West Africa and wrote that the land was rich in rice. \"<nowiki>[T]</nowiki>hey said they found the country covered by vast crops, with many cotton trees and large fields planted in rice … the country looked to them as having the aspect of a pond (i.e., a marais)”. The Portuguese accounts speak of the Falupo Jola, Landuma, Biafada, and Bainik growing rice. André Álvares de Almada wrote about the dike systems used for rice cultivation, from which modern West African rice dike systems are descended.\n\nAfrican rice was brought to the Americas with the transatlantic slave trade, arriving in Brazil probably by the 1550s and in the U.S. in 1784. The seed was carried as provisions on slave ships, and the technology and skills needed to grow it were brought by enslaved rice farmers. Newly imported African slaves were marketed (and sometimes even trained) for their rice-growing skills, as the high price of rice made it a major cash crop. The tolerance of African rice for brackish water meant it could be grown on coastal deltas, as it was in West Africa.\n\nThere are numerous stories about how the rice came to North America, including a slave smuggling grains in her hair and a ship driven in to trade by a storm. African rice is a rare crop in Brazil, Guyana, El Salvador and Panama, but it is still occasionally grown there. There are also native South American rices, which makes it hard to recognize the arrival of African rice in histories.\n\nAsian rice came to West Africa in the late 1800s, and by the late twentieth century had substantially supplanted native African rice. However, African rice was still used in specific, often marginal habitats, and preferred for its taste Farmers may grow African rice to eat and Asian rice to sell, as African rice is not exported.\n\nThe 2007 food price shocks drove efforts to raise rice production. Rice-going regions of Africa are generally net rice importers (partly due to a lack of good local rice-processing capacity) so price increases hurt. Among the efforts to increase yield was the adoption of nerica cultivars, crossbred to specifications from local farmers using African rice varieties provided by local farmers. These were bred during the 1990s and released in the early 21st century. Results so far have been mixed; the nerica varieties are less hardy and more labour-intensive, and effects on real-world yields vary. Subsidies of nerica seeds have also been criticized for encouraging the loss of native varieties and reducing the independence of farmers.\n\nMultiple varieties of African rice are often grown so that the harvest is staggered. In this way, the harvest can be eaten fresh. Freshly harvested rice is moist, and can be puffed in fire, and eaten. Fried rice have browny color when fried this is because of the husk that is green in color when heated turns brown.\n\nAfrican rice can be prepared in much the same way as Asian rice, but has a distinct nutty flavour, for which it is favoured in West Africa. African rice grains are often reddish in colour; some varieties are strongly aromatic, other, like Carolina Gold, are not at all aromatic.\n\nAfrican rice is also used medicinally.\n\nAfrican rice is a tall rice plant, usually under 120 cm but up to five meters for floating varieties, which may also branch and root from higher stem nodes. Generally, African rice has small, pear-shaped grain, reddish bran and green to black hulls, straight, simply-branched panicles, and short, rounded ligules. There are, however, exceptions, and it can be hard to distinguish from Asian rice. For complete certainty, a genetic test can be used.\n\nAfrican rice is well adapted to the African environment. It is drought- and deep-water-resistant, and tolerates fluctuations in water depth, iron toxicity, infertile soils, severe climatic conditions, and human neglect better than Asian rice. Some varieties also mature more quickly, and may be sown directly on higher ground, eliminating the need to transplant seedlings. Most is rain-watered, and the soil is often not cultivated.\n\nAfrican rice has profuse vegetative growth, which smothers weeds. It exhibits better resistance to various rice pests and diseases, such as blast disease, African rice gall midge (\"Orseolia oryzivora\"), parasitic nematodes (\"Heterodera sacchari\" and \"Meloidogyne\" spp.), rice stripe necrosis virus, rice yellow mottle virus, and the parasitic plant \"Striga\".\n\nMost African rices shatter more than Asian rices, possibly because they haven't been domesticated for as long. A few varieties of African rice are as resistant to shattering as shatter-resistant Asian varieties, but most are not; on average, about half of the grains are scattered and lost. This is why yield is lower; when the heads of African rice are bagged before they become ripe, so that the shattered grains are caught in paper bags, the yield of African rice is the same as the yield of Asian rice.\n\nLike other grains, rice may lodge, or fall over, when grain heads are full. African rice's greater height and weaker stems makes it more likely to lodge, although it also lets it survive in deep water, and makes it easier to harvest. African rice tends to elongate rapidly if completely submerged, which is not advantageous in regions prone to short floods, as it weakens the plant.\n\nThe grains of African rice are more brittle than those of Asian rice. The grains are more likely to break during industrial polishing. Broken rice is widely used in West Africa, and some cookbooks from the region will suggest manually breaking the grains for certain recipes,(example) but most broken rice eaten is from Asian rice, about 16% of which is broken in processing.\n\nThe genome of \"O. glaberrima\" has been sequenced, and was published in 2014. This allowed genomic as well as physiological comparison with related species, and identified some effects of some genes.\n\nAfrican and Asian rice do not readily interpollinate, even under controlled conditions, and when they do, the offspring are very rarely fertile. Even the fertile crossbred offspring have low fertility.\n\nCrossbreeding seems to have succeeded in at least one area of Maritime Guinea, as some varieties there show crossbred genes.\n\nMore recently, the nerica cultivars (new rice for Africa) have been developed using green revolution techniques like embryo rescue. Over 3000 crosses were made as part of the NERICA program. Breeding within the species is easier, and there are uncounted numbers of African rice varieties, although the majority may have been lost. A similar crossed variety was bred in the United States in 2011, and work is being done on crosses with Indian rice varieties.\n\nThere are a great many varieties of African rice. In the 1960s older women in Jipalom (a village in the Ziguinchor Region) could unhesitatingly name more than ten varieties of African rice that were no longer planted, besides the half-dozen that were then still being planted. Each woman would plant multiple different varieties, to suite varying microhabitats and to stagger the harvest. A 2006 survey showed that a village typically cultivated 25 varieties of rice; an individual household would on average have 14 varieties and grow four per year; this, however, is down from the seven to nine varieties per woman that was average in previous decades. Women, who are traditionally responsible for the seeds, trade them often over long-distance networks.\n\nVarieties, each with subtypes, include:\n\nThe cultivars the Africa Rice Center calls TOG 12303 and TOG 9300 have low shattering, and thus yields comparable with low-shattering Asian rice varieties.\n\nScientists from the Africa Rice Center managed to cross-breed African rice with Asian rice varieties to produce a group of interspecific cultivars called New Rice for Africa (NERICA).\n\nCarolina Gold is an heirloom cultivar grown in the early United States, sometimes known as golden-seed rice for the colour of its grains.\n\nLong-grain gold-seed rice boasted grains 5/12ths of an inch long (up from 3/8ths of an inch), and was brought to market by planter Joshua John Ward in the 1840s. Despite its popularity, the variety was lost in the American Civil War.\n\nCharleston Gold was released in 2011 and is a crossbreed of Carolina Gold and two \"Oryza sativa\" breeding lines called IR64 (Indica) and IR65610-24-3-6-3-2-3, which raised the yield, shortened the stem, and added an aromatic quality to the rice.\n\n"}
{"id": "7545693", "url": "https://en.wikipedia.org/wiki?curid=7545693", "title": "Pedras de abalar", "text": "Pedras de abalar\n\nThe Pedras de abalar, Galician for \"oscillating stones\", are several large stones in Galicia, Spain, that can easily be moved by a person or the wind. One of these is in Muxía, and is known as the \"Pedra da Barca\". These are large stones that are balanced on a point, so that they can be moved back and forth easily, or even wiggle in response to the wind. These were used at one time to determine the guilt or innocence of those accused of serious crimes. In English, such stones have been called rocking stones, or logan stones.\n\nThe \"Pedras de Abalar\" in Galicia are:\n\nOther pedra de abalar include the \"Pena da Conga\" in Melide, and the \"Castro do Faro\" in O Porriño.\n\n"}
{"id": "824692", "url": "https://en.wikipedia.org/wiki?curid=824692", "title": "Perfluorooctanoic acid", "text": "Perfluorooctanoic acid\n\nPerfluorooctanoic acid (PFOA) (conjugate base perfluorooctanoate) is a perfluorinated carboxylic acid produced and used worldwide as an industrial surfactant in chemical processes and as a material feedstock, and is known as an emerging health concern and subject of regulatory action and voluntary industrial phase-outs. PFOA is considered a surfactant, or fluorosurfactant, due to its chemical structure consisting of a perfluorinated, n-octyl \"tail group\" and a carboxylate \"head group\". The head group can be described as hydrophilic while the fluorocarbon tail is both hydrophobic and lipophobic; The tail group is inert and does not interact strongly with polar or non-polar chemical moieties; the head group is reactive and interacts strongly with polar groups, specifically water. The \"tail\" is hydrophobic due to being non-polar and lipophobic because fluorocarbons are less susceptible to the London dispersion force than hydrocarbons. \n\nPFOA is used for several industrial applications, including carpeting, upholstery, apparel, floor wax, textiles, sealants, and cookware. PFOA serves as a surfactant in the emulsion polymerization of fluoropolymers and as a building block for the synthesis of perfluoroalkyl-substituted compounds, polymers, and polymeric materials. PFOA has been manufactured since the 1940s in industrial quantities. It is also formed by the degradation of precursors such as some fluorotelomers. PFOA is used as a surfactant because it can lower the surface tension of water more than hydrocarbon surfactants while having exceptional stability due to having perfluoroalkyl tail group. The stability of PFOA is desired industrially but is a cause of concern environmentally.\n\nPFOA persists indefinitely in the environment. It is toxic. As it is a suspected carcinogen, various studies have been undertaken, but no link between PFOA and cancer emerged, although it remains suspect and under investigation. PFOA has been detected in the blood of more than 98% of the general US population in the low and sub-parts per billion (ppb) range, and levels are higher in chemical plant employees and surrounding subpopulations. How general populations are exposed to PFOA is not completely understood. PFOA has been detected in industrial waste, stain-resistant carpets, carpet-cleaning liquids, house dust, microwave popcorn bags, water, food, some cookware and PTFE products.\n\nAs a result of a class-action lawsuit and community settlement with DuPont, three epidemiologists conducted studies on the population surrounding a chemical plant that was exposed to PFOA at levels greater than in the general population. The studies concluded that there was probably an association between PFOA exposure and six health outcomes: kidney cancer, testicular cancer, ulcerative colitis, thyroid disease, hypercholesterolemia (high cholesterol), and pregnancy-induced hypertension.\n\nThe primary manufacturer of PFOS, the 3M Company (known as Minnesota Mining and Manufacturing Company from 1902 to 2002), began a production phase-out in 2002 in response to concerns expressed by the United States Environmental Protection Agency (EPA). Eight other companies agreed to gradually phase out the manufacturing of the chemical by 2015.\n\nBy 2014, EPA had listed PFOA (free acid) and PFOS (potassium salt) as emergent contaminants:\n3M (then Minnesota Mining and Manufacturing Company) began producing PFOA by electrochemical fluorination in 1947. Starting in 1951, DuPont purchased PFOA from 3M for use in the manufacturing of specific fluoropolymers—commercially branded as Teflon, but DuPont internally referred to the material as C8.\n\nIn the fall of 2000, lawyer Rob Bilott, a partner at Taft Stettinius & Hollister, won a court order forcing DuPont to share all documentation related to PFOA. This included 110,000 files, consisting of confidential studies and reports conducted by DuPont scientists over decades. By 1993, DuPont understood that \"PFOA caused cancerous testicular, pancreatic and liver tumors in lab animals\" and the company began to investigate alternatives. However, products manufactured with PFOA were such an integral part of DuPont's earnings, $1 billion in annual profit, they chose to continue using PFOA. Billott learned that both \"3M and DuPont had been conducting secret medical studies on PFOA for more than four decades\", and by 1961 DuPont was aware of hepatomegaly in mice fed with PFOA.\n\nIn 1968, organofluorine content was detected in the blood serum of consumers, and in 1976 it was suggested to be PFOA or a related compound such as PFOS.\n\nBilott exposed how DuPont had been knowingly polluting water with PFOAs in Parkersburg, West Virginia since the 1980s. In the 1980s and 1990s, researchers investigated the toxicity of PFOA.\n\nIn 1999, EPA ordered companies to examine the effects of perfluorinated chemicals after receiving data on the global distribution and toxicity of PFOS. For these reasons, and EPA pressure, in May 2000, 3M announced the phaseout of the production of PFOA, PFOS, and PFOS-related products—the company's best-selling repellent. 3M stated that they would have made the same decision regardless of EPA pressure.\n\nBecause of the 3M phaseout, in 2002, DuPont built its own plant in Fayetteville, North Carolina to manufacture the chemical. The chemical has received attention due to litigation from the PFOA-contaminated community around DuPont's Washington Works facility in Washington, West Virginia, along with EPA focus. Research on PFOA has demonstrated ubiquity, animal-based toxicity, and some associations with human health parameters and potential health effects. Additionally, advances in analytical chemistry in recent years have allowed the routine detection of low- and sub-parts per billion levels of PFOA in a variety of substances. In 2013, Gore-Tex eliminated the use of PFOAs in the manufacture of its weatherproof functional fabrics. GenX has been introduced as a replacement for PFOA but it is doubtful whether it is less harmful.\n\nFor his work in the exposure of the contamination, lawyer Rob Bilott received The Right Livelihood Award in 2017. This battle with DuPont is featured in the documentary called The Devil We Know, which premiered at the Sundance Film Festival in 2018.\n\nPFOA has two main synthesis routes, electrochemical fluorination (ECF) and telomerization. The equation below represents the ECF route with hydrofluoric acid reacting with octanoyl chloride, octanoyl chloride.\nThe equation above shows the multiple products of ECF. The target product, F(CF)COF (not represented) is produced as only 10–15% of the total product, while the main products are perfluorinated cyclic ether isomers, including FC-75. To yield PFOA, the perfluorinated acid fluoride is hydrolyzed. The PFOA formed by this method is a mixture of straight chain (78%), terminally branched (13%), and internally branched (9%) molecules, as ECF rearranges the carbon \"tail\" of the acid chloride. ECF also results in production wastes. 3M synthesized ECF PFOA at their Cottage Grove, MN facility from 1947 to 2002 and was the world's largest producer. ECF production continues on a smaller scale in Europe and Asia.\n\nPFOA is also synthesized by the telomerization represented below, where the telogen is the organoiodine compound and the taxogen is the unsaturated tetrafluoroethylene.\nThe product is oxidized by SO to form PFOA. Under reaction conditions, telomers form with varying length chains containing an even number of carbon atoms, as products mostly contain two to six tetrafluoroethylene taxogens. After oxidation, distillation is used to separate PFOA from the other perfluorinated carboxylic acids. The telomerization synthesis of PFOA was pioneered by DuPont, and it is not well suited to the laboratory. PFOA formed by telomerization is completely linear, in contrast to the mixture of structures formed by ECF.\n\nPFOA has widespread applications. In 1976, PFOA was reported as a water and oil repellent \"in fabrics and leather and in the production of floor waxes and waxed papers\"; however, it is believed that paper is no longer treated with perfluorinated compounds, but with fluorotelomers with less than 0.1% PFOA. The compound is also used in \"insulators for electric wires, planar etching of fused silica\", fire fighting foam, and outdoor clothing. As a protonated species, the acid form of PFOA was the most widely used perfluorocarboxylic acid used as a reactive intermediate in the production of fluoroacrylic esters.\nAs a salt, its dominant use is as an emulsifier for the emulsion polymerization of fluoropolymers such as PTFE, polyvinylidene fluoride, and fluoroelastomers. For this use, 3M subsidiary Dyneon has a replacement emulsifer despite DuPont stating PFOA is an \"essential processing aid\". PFOA is used in the production of Gore-Tex as it is PTFE-based. In PTFE processing, PFOA is in aqueous solution and forms micelles that contain tetrafluoroethylene and the growing polymer. PFOA can be used to stabilize fluoropolymer and fluoroelastomer suspensions before further industrial processing and in ion-pair reversed-phase liquid chromatography it can act as an extraction agent. PFOA also finds uses in electronic products and as an industrial fluorosurfactant.\n\nIn a 2009 EPA study of 116 products, purchased between March 2007 and May 2008 and found to contain at least 0.01% fluorine by weight, the concentrations of PFOA were determined. Concentrations shown below range from not detected, or ND, (with the detection limit in parenthesis) to 6750 with concentrations in nanograms of PFOA per gram of sample (parts per billion) unless stated otherwise.\n\nPFOA contaminates every continent. PFOA has been detected in the central Pacific Ocean at low parts per quadrillion ranges, and at low parts per trillion (ppt) levels in coastal waters. Due to the surfactant nature of PFOA, it has been found to concentrate in the top layers of ocean water. PFOA is detected widely in surface waters, and is present in numerous mammals, fish, and bird species. PFOA is in the blood or vital organs of Atlantic salmon, swordfish, striped mullet, gray seals, common cormorants, Alaskan polar bears, brown pelicans, sea turtles, sea eagles, Midwestern bald eagles, California sea lions and Laysan albatrosses on Sand Island, a wildlife refuge on Midway Atoll, in the middle of the North Pacific Ocean, about halfway between North America and Asia. However, wildlife has much less PFOA than humans, unlike PFOS and other longer perfluorinated carboxylic acids; in wildlife, PFOA is not as bioaccumulative as longer perfluorinated carboxylic acids.\n\nMost industrialized nations have average PFOA blood serum levels ranging from 2 to 8 parts per billion; the highest consumer sub-population identified was in Korea—with about 60 parts per billion. In Peru, Vietnam, and Afghanistan blood serum levels have been recorded to be below one part per billion. In 2003–2004 99.7% of Americans had detectable PFOA in their serum with an average of about 4 parts per billion, and concentrations of PFOA in US serum have declined by 25% in recent years. Despite a decrease in PFOA, the longer perfluorinated carboxylic acid PFNA is increasing in the blood of US consumers.\n\nIn 2016, the Environmental Working Group published an analysis of EPA data, and reported that unsafe levels of PFOA and PFOS were found in public water systems serving 5.2 million people in the US. The analysis was based on monitoring reports submitted to EPA by 52 water systems in 19 states and two territories.\n\nPFOA is released directly from industrial sites. For example, the estimate for the DuPont Washington Works facility is a total PFOA emissions of 80,000 pounds (lbs) in 2000 and 1,700 pounds in 2004. A 2006 study, with two of four authors DuPont employees, estimated about 80% of historical perfluorocarboxylate emissions were released to the environment from fluoropolymer manufacture and use. PFOA can be measured in water from industrial sites other than fluorochemical plants. PFOA has also been detected in emissions from the carpet industry, paper and electronics industries. The most important emission sources are carpet and textile protection products, as well as fire-fighting foams.\n\nPFOA can form as a breakdown product from a variety of precursor molecules. In fact, the main products of the fluorotelomer industry, fluorotelomer-based polymers, have been shown to degrade to form PFOA and related compounds, with half-lives of decades, both biotically and by simple abiotic reaction with water. It has been argued that fluorotelomer-based polymers already produced might be major sources of PFOA globally for decades to come. Other precursors that degrade to PFOA include 8:2 fluorotelomer alcohol (F(CF)CHCHOH), polyfluoroalkyl phosphate surfactants (PAPS), and possibly \"N\"-EtFOSE alcohol (F(CF)SON(Et)CHCHOH). When PTFE is degraded by heat (pyrolysis) it can form PFOA as a minor product. The Organisation for Economic Co-operation and Development (OECD) has compiled a list of 615 chemicals that have the potential to break down into perfluorocarboxylic acids (PFCA) including PFOA. However, not all 615 have the potential to break down to form PFOA.\n\nA majority of waste water treatment plants (WWTPs) that have been tested output more PFOA than is input, and this increased output has been attributed to the biodegradation of fluorotelomer alcohols. A current PFOA precursor concern are fluorotelomer-based polymers; fluorotelomer alcohols attached to hydrocarbon backbones via ester linkages may detach and be free to biodegrade to PFOA.\n\nFood, drinking water, outdoor air, indoor air, dust, and food packagings are all implicated as sources of PFOA to people. However, it is unclear which exposure routes dominate because of data gaps. When water is a source, blood levels are approximately 100 times higher than drinking water levels.\n\nPeople who lived in the PFOA-contaminated area around DuPont's Washington Works facility were found to have higher levels of PFOA in their blood from drinking water. The highest PFOA levels in drinking water were found in the Little Hocking water system, with an average concentration of 3.55 parts per billion during 2002–2005. Individuals who drank more tap water, ate locally grown fruits and vegetables, or ate local meat, were all associated with having higher PFOA levels. Residents who used water carbon filter systems had lower PFOA levels.\n\nPFOA is also formed as an unintended byproduct in the production of fluorotelomers and is present in finished goods treated with fluorotelomers, including those intended for food contact. Fluorotelomers are applied to food contact papers because they are lipophobic: they prevent oil from soaking into the paper from fatty foods. Also, fluorotelomers can be metabolized into PFOA. In a U.S. Food and Drug Administration (USFDA) study, lipophobic fluorotelomer-based paper coatings (which can be applied to food contact paper in the concentration range of 0.4%) were found to contain 88,000–160,000 parts per billion PFOA, while microwave popcorn bags contained 6–290 parts per billion PFOA. Toxicologists estimate that microwave popcorn could account for about 20% of the PFOA levels measured in an individual consuming 10 bags a year if 1% of the fluorotelomers are metabolized to PFOA.\n\nIn 2008 as news stories began to raise concerns about PFOA in microwaved popcorn, Dan Turner, DuPont's global public relations chief, said, \"I serve microwave popcorn to my three-year-old.\" Five years later, journalist Peter Laufer wrote to Turner to ask if his child was still eating microwave popcorn. \"I am not going to comment on such a personal inquiry\", Turner replied.\n\nFluorotelomer coatings are used in fast food wrappers, candy wrappers, and pizza box liners. PAPS, a type of paper fluorotelomer coating, and PFOA precursor, is also used in food contact papers.\n\nDespite DuPont's asserting that \"cookware coated with DuPont Teflon non-stick coatings does not contain PFOA\", residual PFOA was also detected in finished PTFE products including PTFE cookware (4–75 parts per billion). However, PFOA levels ranged from undetectable (<1.5) to 4.3 parts per billion in a more recent study. Also, non-stick cookware is heated—which should volatilize PFOA; PTFE products that are not heated, such as PTFE sealant tape, had higher (1800 parts per billion) levels detected. Overall, PTFE cookware is considered an insignificant exposure pathway to PFOA.\n\nPFOA and PFOS were detected in \"very high\" (low parts per million) levels in agricultural fields for grazing beef cattle and crops around Decatur, AL. The approximately 5000 acres of land were fertilized with \"treated municipal sewage sludge, or biosolids\". PFOA was also detected in fodder grass grown in these soils and the blood of the cattle feeding on this grass. The water treatment plant received process wastewater from a nearby perfluorochemical manufacturing plant. 3M says they managed their own wastes, but Daikin America \"discharged process wastewater to the municipal waste treatment plant\". If traced to meat, it would be the first time perfluorochemicals were traced from sludge to food. However, the USDA reported—with a detection limits of 20 parts per billion—non-detectable levels for both PFOA and PFOS in cattle muscle tissue.\n\nIn the United States there are no federal drinking water standards for PFOA or PFOS as of late 2017. EPA began requiring public water systems to monitor for PFOA and PFOS in 2012, and published drinking water health advisories, which are non-regulatory technical documents, in 2016. The lifetime health advisories and health effects support documents assist federal, state, tribal, and local officials and managers of drinking water systems in protecting public health when these chemicals are present in drinking water. The levels of PFOS and PFOA concentrations under which adverse health effects are not anticipated to occur over a lifetime of exposure are 0.07 ppb (70 ppt). EPA has not announced whether it will develop a National Primary Drinking Water Regulation for these contaminants.\n\nIn November 2017 the State of New Jersey announced plans to develop its own drinking water standards for PFOA and PFNA. These standards—14 ppt for PFOA and 13 ppt for PFNA—would be the most stringent regulatory standards in the country.\n\nUsing information gained through a Freedom of Information Act request, in May 2018 it was learned that January 2018 emails between the EPA, the Office of Management and Budget, the Department of Defense, and the Department of Health and Human Services showed an effort to suppress the release of a draft report on the toxicology of PFOS and PFOA done by the Agency for Toxic Substances and Disease Registry. The report found that these chemicals endanger human health at a far lower level than EPA has previously called safe. After media accounts of the effort surfaced, the regional EPA administrator for Colorado denied that EPA had anything to do with suppressing the report. The report was finally released on June 21, 2018.\n\nThe new ATSDR analysis derives provisional Minimal Risk Levels (MRLs) of 3x10 mg/kg/day for PFOA and 2x10 mg/kg/day for PFOS during intermediate exposure.\n\nAn attempt to regulate PFOA in food packaging occurred in the US state of California in 2008. A bill, sponsored by State Senator Ellen Corbett and the Environmental Working Group, was passed in the house and senate that would have banned PFOA, PFOS, and seven or more related fluorinated carbon compounds in food packaging starting in 2010, but the bill was vetoed by Governor Schwarzenegger. The bill would have affected fluorochemical manufacturers outside of the state. Schwarzenegger said the compound should be reviewed by the newly established, and more comprehensive, state program.\n\nFluorotelomer-based products have been shown to degrade to PFOA over periods of decades; \n\nPFOA is a carcinogen, a liver toxicant, a developmental toxicant, and an immune system toxicant, and also exerts hormonal effects including alteration of thyroid hormone levels. Animal studies show developmental toxicity from reduced birth size, physical developmental delays, endocrine disruption, and neonatal mortality. PFOA alters lipid metabolism. It is an agonist of PPARα and is a peroxisome proliferator in rodents contributing to a well understood form of oxidative stress. Humans are considered less susceptible to peroxisome proliferation than rodents. However, PFOA has been found to be a liver carcinogen in rainbow trout via a potential estrogenic mechanism, which may be more relevant to humans.\n\nAn EPA review notes that PFOA has not \"been shown to be mutagenic in a variety of assays\". PFOA has been described as a member of a group of \"classic non-genotoxic carcinogens\". However, a provisional German assessment notes that a 2005 study found PFOA to be genotoxic via a peroxisome proliferation pathway that produced oxygen radicals in HepG2 cells, and a 2006 study demonstrated the induction and suppression of a broad range of genes; therefore, it states that the indirect genotoxic (and thus carcinogenic) potential of PFOA \"cannot\" be dismissed. Criteria have been proposed that would allow PFOA, and other perfluorinated compounds, to be classified as \"weakly non-specific genotoxic\".\n\nPFOA is resistant to degradation by natural processes such as metabolism, hydrolysis, photolysis, or biodegradation and has been found to persist in the environment. PFOA is found in environmental and biological fluids as the anion perfluorooctanoate. PFOA can be absorbed from ingestion and can penetrate skin. The acid headgroup of PFOA enables binding to proteins with fatty acid or hormone substrates such as serum albumin, liver fatty acid-binding protein, and the nuclear receptors PPARα and possibly CAR. \n\nIn animals, PFOA is mainly present in the liver, blood, and kidneys. PFOA does not accumulate in fat tissue, unlike traditional organohalogen persistent organic pollutants. In humans, PFOA has an average elimination half-life of about 3 years. Because of this long half-life, PFOA has the potential to bioaccumulate.\n\nThe levels of PFOA exposure in humans vary widely. While an average American might have 3 or 4 parts per billion of PFOA present in their blood serum, individuals occupationally exposed to PFOA have had blood serum levels over 100,000 parts per billion (100 parts per million or 0.01%) recorded. In a study of individuals living around DuPont's Washington Works plant, those who had no occupational exposure had a median blood serum level of 329 parts per billion while the median of those with occupational exposure was 775 parts per billion. While no amount of PFOA in humans is legally recognized as harmful, DuPont was \"not satisfied\" with data showing their Chinese workers accumulated an average of about 2,250 parts per billion of PFOA in their blood from a starting average of around 50 parts per billion less than a year prior.\n\nIn late 2012, scientists at Emory University compared health risks in workers at a DuPont chemical plant in West Virginia with high PFOA exposure to the risks of the same diseases in other regional DuPont factory workers and in the US population. In comparison with the other DuPont workers, workers at the high-PFOA plant were at roughly three times the risk of dying of mesothelioma or chronic kidney disease, and roughly twice the risk of dying of diabetes mellitus. Workers were at similarly elevated risk for kidney cancer and for non-cancer kidney diseases. In rodents, PFOA concentrates in the kidneys.\n\nSingle cross-sectional studies on consumers have been published noting multiple associations. Blood serum levels of PFOA were associated with an increased time to pregnancy—or \"infertility\"—in a 2009 study. PFOA exposure was associated with decreased semen quality, increased serum alanine aminotransferase levels, and increased occurrence of thyroid disease. In a study of 2003–2004 US samples, a higher (9.8 milligram per deciliter) total cholesterol level was observed when the highest quartile was compared to the lowest. Along with other related compounds, PFOA exposure was associated with an increased risk of attention deficit hyperactivity disorder (ADHD) in a study of US children aged 12–15. In a paper presented at the 2009 annual meeting of the International Society of Environmental Epidemiology, PFOA appeared to act as an endocrine disruptor by a potential mechanism on breast maturation in young girls. A C8 Science Panel status report noted an association between exposure in girls and a later onset of puberty.\n\nPFOA has been associated with signs of reduced fetal growth including lower birth weight. However, other studies have not replicated the lower birth weight finding including a study on the DuPont exposed community. PFOA exposure in the Danish general population was not associated with an increased risk of prostate, bladder, pancreatic, or liver cancer. Maternal PFOA levels were not associated with an offspring's increased risk of hospitalization due to infectious diseases, behavioral and motor coordination problems, or delays in reaching developmental milestones.\n\nIn 2010, the three members of the C8 Science Panel published a review of the epidemiological evidence on PFOA exposure in \"Environmental Health Perspectives\". Insufficient evidence exists to conclude PFOA causes adverse health effects in humans, but consistent evidence exists on associations with higher cholesterol and uric acid. Whether or not these potential effects result in an increase in cardiovascular disease is unknown. Further data on the 69,030 member cohort that is being studied by the panel is scheduled for release through 2012. A 2011 epidemiological study demonstrated ‘‘probable link’’ between PFOA and kidney cancer, testicular cancer, thyroid disease, high cholesterol, pre-eclampsia and ulcerative colitis.\n\nFacial birth defects, an effect observed in rat offspring, occurred with the children of two out of seven female DuPont employees from the Washington Works facility from 1979 to 1981. Bucky Bailey is one of the affected individuals, however, DuPont does not accept any liability from the toxicity of PFOA. While 3M sent DuPont results from a study that showed birth defects to rats administered PFOA and DuPont moved the women out of the Teflon production unit, subsequent animal testing led DuPont to conclude there was no reproductive risk to women, and they were returned to the production unit. However, data released in March 2009 on the community around DuPont's Washington Works plant showed \"a modest, imprecise indication of an elevation in risk … above the 90th percentile … based on 12 cases in the uppermost\ncategory\", which was deemed \"suggestive of a possible relationship\" between PFOA exposure and birth defects.\n\nPFOA was proposed for listing under the Stockholm Convention on Persistent Organic Pollutants.\n\nDuPont has used PFOA for over 50 years at its Washington Works plant. Area residents sued DuPont in August 2001 and claimed DuPont released PFOA in excess of their community guideline of 1 part per billion resulting in lower property values and increased risk of illness. The class was certified by Wood Circuit Court Judge George W. Hill. As part of the settlement, DuPont is paid for blood tests and health surveys of residents believed to be affected. Participants numbered 69,030 in the study, which was reviewed by three epidemiologists—the C8 Science Panel—to determine if any health effects are the likely result of exposure.\n\nOn December 13, 2005, DuPont announced a settlement with the EPA in which DuPont will pay US$10.25 million in fines and an additional US$6.25 million for two supplemental environmental projects without any admission of liability.\n\nOn September 30, 2008, Chief Judge Joseph R. Goodwin of the United States District Court for the Southern District of West Virginia denied the certification of a class of Parkersburg residents exposed to PFOA from DuPont's facility because they did not \"show the common individual injuries needed to certify a class action\". On September 28, 2009, Judge Goodwin dismissed the claims of those residents except for medical monitoring. By 2015, more than three thousand plaintiffs have filed personal-injury lawsuits against DuPont.\n\nIn 2002, a panel of toxicologists, including several from EPA, proposed a level of 150 ppb for drinking water in the PFOA contaminated area around DuPont's Washington Works plant. This level was much higher than any known environmental concentration.\n\nIn July 2004, EPA filed a suit against DuPont alleging \"widespread contamination\" of PFOA near the Parkersburg, West Virginia plant \"at levels exceeding the company's community exposure guidelines;\" the suit also alleged that \"DuPont had—over a 20 year period—repeatedly failed to submit information on adverse effects (in particular, information on liver enzyme alterations and birth defects in offspring of female Parkersburg workers).\"\n\nIn October 2005, a USFDA study was published revealing PFOA and PFOA precursor chemicals in food contact and PTFE products.\n\nOn January 25, 2006, EPA announced a voluntary program with several chemical companies to reduce PFOA and PFOA precursor emissions by the year 2015.\n\nOn February 15, 2005, EPA's Science Advisory Board (SAB) voted to recommended that PFOA should be considered a \"likely human carcinogen\".\n\nOn May 26, 2006, EPA's SAB addressed a letter to Administrator Stephen L. Johnson. Three-quarters of advisers thought the stronger \"likely to be carcinogenic\" descriptor was warranted, in opposition to EPA's own PFOA hazard descriptor of \"suggestive evidence of carcinogenicity, but not sufficient to assess human carcinogenic potential\".\n\nOn November 21, 2006, EPA ordered DuPont to offer alternative drinking water or treatment for public or private water users living near DuPont's Washington Works plant in West Virginia (and in Ohio), if the level of PFOA detected in drinking water is equal to or greater than 0.5 parts per billion. This measure sharply lowered the previous action level of 150 parts per billion that was established in March 2002.\n\nAccording to a May 23, 2007, \"Environmental Science & Technology\" Online article, U.S. Food and Drug Administration research regarding food contact papers as a potential source of PFOA to humans is ongoing.\n\nIn November 2007, the Centers for Disease Control and Prevention (CDC) published data on PFOA concentrations comparing 1999–2000 vs. 2003–2004 NHANES samples.\n\nOn January 15, 2009, EPA set a provisional health advisory level of 0.4 ppb in drinking water.\n\nOn May 19, 2016, EPA lowered the drinking water health advisory level to 0.07 ppb for PFOA and PFOS.\n\nIn 2007, the New Jersey Department of Environmental Protection (NJDEP) issued a preliminary health-based guidance level of 0.04 ppb in drinking water, due to PFOA being found at \"elevated levels in the system's drinking water near DuPont's massive Chambers Works chemical plant\". In November 2017 NJDEP announced plans to develop regulatory drinking water standards for PFOA and PFNA, in the absence of federal standards.\n\nIn 2007, the Minnesota Department of Health lowered its Health Based Value for PFOA in drinking water from 1.0 ppb to 0.5 ppb, where \"the sources are landfilled industrial wastes from a 3M manufacturing plant\".\n\nPFOA contaminated waste was incorporated into soil improver and spread on agricultural land in Germany, leading to PFOA drinking water contamination of up to 0.519 parts per billion. The German Federal Environmental Agency issued guidelines for the sum of PFOA and PFOS concentrations in drinking water: 0.1 parts per billion for precaution and 0.3 parts per billion for a threshold. Residents were found to have a 6–8 factor increase of PFOA serum levels over unexposed Germans, with average PFOA concentrations in the 22–27 parts per billion range. An expert panel concluded that \"concentrations were considered too low to cause overt adverse health effects in the exposed population\".\n\nIn the Netherlands, after questions by members of Parliament, the minister of Environment ordered a study into the potential exposure to PFOA of people living in the vicinity of the DuPont factory in Dordrecht. The report was published in March 2016 and concluded that \"prior to 2002 residents were exposed to levels of PFOA at which health effects could not be ruled out\". As a result of this, the government commissioned several further studies, including blood tests and measurements in drinking water.\n\nPFOA was identified as a PBT substance in the EU in 2013. It was then included in the candidate list of substances of very high concern. In 2017, PFOA, its salts and PFOA-related substances were added to annex XVII (restriction) of the REACH Regulation.\n\nOn August 10, 2016, Australian litigation funder IMF Bentham announced an agreement to fund a class action led by the law firm Gadens against the Australian Department of Defence for economic losses to homeowners, fishers, and farmers resulting from the use of aqueous film forming foam (containing PFOA) at RAAF Base Williamtown.\n\n\n"}
{"id": "47863549", "url": "https://en.wikipedia.org/wiki?curid=47863549", "title": "Powder", "text": "Powder\n\nA powder is a dry, bulk solid composed of a large number of very fine particles that may flow freely when shaken or tilted. Powders are a special sub-class of granular materials, although the terms \"powder\" and \"granular\" are sometimes used to distinguish separate classes of material. In particular, \"powders\" refer to those granular materials that have the finer grain sizes, and that therefore have a greater tendency to form clumps when flowing. \"Granulars\" refers to the coarser granular materials that do not tend to form clumps except when wet.\n\nMany manufactured goods come in powder form, such as flour, sugar, ground coffee, powdered milk, copy machine toner, gunpowder, cosmetic powders, and some pharmaceuticals. In nature, dust, fine sand and snow, volcanic ash, and the top layer of the lunar regolith are also examples.\n\nBecause of their importance to industry, medicine and earth science, powders have been studied in great detail by chemical engineers, mechanical engineers, chemists, physicists, geologists, and researchers in other disciplines.\n\nTypically, a powder can be compacted or loosened into a vastly larger range of bulk densities than can a coarser granular material. When deposited by sprinkling, a powder may be very light and fluffy. When vibrated or compressed it may become very dense and even lose its ability to flow. The bulk density of coarse sand, on the other hand, does not vary over an appreciable range.\n\nThe clumping behavior of a powder arises because of the molecular Van der Waals force that causes individual grains to cling to one another. This force is present not just in powders, but in sand and gravel, too. However, in such coarse granular materials the weight and the inertia of the individual grains are much larger than the very weak Van der Waals forces, and therefore the tiny clinging between grains does not have a dominant effect on the bulk behavior of the material. Only when the grains are very small and lightweight does the Van der Waals force become predominant, causing the material to clump like a powder. The cross-over size between flow conditions and stick conditions can be determined by simple experimentation.\n\nMany other powder behaviors are common to all granular materials. These include segregation, stratification, jamming and unjamming, fragility, loss of kinetic energy, frictional shearing, compaction and Reynolds' dilatancy.\n\nPowders are transported in the atmosphere differently from a coarse granular material. For one thing, tiny particles have little inertia compared to the drag force of the gas that surrounds them, and so they tend to \"go with the flow\" instead of traveling in straight lines. For this reason, powders may be an inhalation hazard. Larger particles cannot weave through the body's defenses in the nose and sinus, but will strike and stick to the mucous membranes. The body then moves the mucous out of the body to expel the particles. The smaller particles on the other hand can travel all the way to the lungs from which they cannot be expelled. Serious and sometimes fatal diseases such as silicosis are a result from working with certain powders without adequate respiratory protection.\n\nAlso, if powder particles are sufficiently small, they may become suspended in the atmosphere for a very long time. Random motion of the air molecules and turbulence provide upward forces that may counteract the downward force of gravity. Coarse granulars, on the other hand, are so heavy that they fall immediately back to the ground. Once disturbed, dust may form huge dust storms that cross continents and oceans before settling back to the surface. This explains why there is relatively little hazardous dust in the natural environment. Once aloft, the dust is very likely to stay aloft until it meets water in the form of rain or a body of water. Then it sticks and is washed downstream to settle as mud deposits in a quiet lake or sea. When geological changes later re-expose these deposits to the atmosphere, they may have already cemented together to become mudstone, a type of rock. For comparison, the Moon has neither wind nor water, and so its regolith contains dust but no mudstone.\n\nThe cohesive forces between the particles tend to resist their becoming airborne, and the motion of wind across the surface is less likely to disturb a low-lying dust particle than a larger sand grain that protrudes higher into the wind. Mechanical agitation such as vehicle traffic, digging or passing herds of animals is more effective than a steady wind at stirring up a powder.\n\nThe aerodynamic properties of powders are often used to transport them in industrial applications. Pneumatic conveying is the transport of powders or grains through a pipe by blowing gas. A gas fluidized bed is a container filled with a powder or granular substance that is \"fluffed up\" by blowing gas upwardly through it. This is used for fluidized bed combustion, chemically reacting the gas with the powder.'\n\nSome powders may be dustier than others. The tendency of a powder to generate particles in the air under a given energy input is called \"dustiness\". It is an important powder property which is relevant to powder aerosolization process. It also has indications for human exposure to aerosolized particles and associated health risks (via skin contacts or inhalation) at workplaces. Various dustiness testing methods have been established in research laboratories, in order to predict powder behaviors during aerosolization. These methods (laboratory setups) allow application of a wide range of energy inputs to powdered materials, which simulates different real-life scenarios.\n\nMany common powders made in industry are combustible; particularly metals or organic materials such as flour. Since powders have a very high surface area, they can combust with explosive force once ignited. Facilities such as flour mills can be vulnerable to such explosions without proper dust mitigation efforts.\n\nSome metals become especially dangerous in powdered form, notably titanium.\n\nA paste or gel might become a powder after it has been thoroughly dried, but is not considered a powder when it is wet because it does not flow freely. Substances like dried clay, although dry bulk solids composed of very fine particles, are not powders unless they are crushed because they have too much cohesion between the grains, and therefore they do not flow freely like a powder. A liquid flows differently than a powder, because a liquid cannot resist any shear stress and therefore it cannot reside at a tilted angle without flowing (that is, it has zero \"angle of repose.\") A powder on the other hand is a solid, not a liquid, because it may support shear stresses and therefore may display an angle of repose.\n\n\n"}
{"id": "35538934", "url": "https://en.wikipedia.org/wiki?curid=35538934", "title": "Saturation dome", "text": "Saturation dome\n\nA saturation dome is a graphical representation of the combination of vapor and gas that is used in thermodynamics. It can be used to find either the pressure or the specific volume as long as one already has at least one of these properties.\n\nA saturation dome uses the projection of a P–\"v\"–T diagram (pressure, specific volume, and temperature) onto the P–\"v\" plane. This gives a P–\"v\" diagram at a constant temperature. The points that create the left-hand side of the dome represent the saturated liquid states, while the points on the right-hand side represent the saturated vapor states (commonly referred to as the “dry” region). On the left-hand side of the dome there is compressed liquid and on the right-hand side there is superheated gas.Within the dome itself, there is a liquid–vapor mixture. This two-phase region is commonly referred to as the “wet” region. The percentage of liquid and vapor can be calculated using vapor quality. The triple state line is where the three phases (solid, liquid, and vapor) exist in equilibrium.\n\nThe point at the very top of the dome is called the critical point. This point is where the saturated liquid and saturated vapor lines meet. Past this point, it is impossible for a liquid–vapor transformation to occur. It is also where the critical temperature and critical pressure meet. Beyond this point, it is also impossible to distinguish between the liquid and vapor phases.\n\nA saturation state is the point where a phase change begins or ends. For example, the saturated liquid line represents the point where any further addition of energy will cause a small portion of the liquid to convert to vapor. Likewise, along the saturated vapor line, any removal of energy will cause some of the vapor to condense back into a liquid, producing a mixture. When a substance reaches the saturated liquid line it is commonly said to be at its boiling point. The temperature will remain constant while it is at constant pressure underneath the saturation dome (boiling water stays at a constant of 212F) until it reaches the saturated vapor line. This line is where the mixture has converted completely to vapor. Further heating of the saturated vapor will result in a superheated vapor state. This is because the vapor will be at a temperature higher than the saturation temperature (212F for water) for a given pressure.\n\nVapor quality refers to the vapor–liquid mixture that is contained underneath the\ndome. This quality is defined as the fraction of the total mixture which is vapor, based on\nmass. A\nfully saturated vapor has a quality of 100% while a saturated liquid has a quality of 0%.\nQuality can be estimated graphically as it is related to the specific volume, or how far horizontally across the dome the point exists. At the saturated liquid state, the specific volume is denoted as \"v\", while at the saturated vapor stage it is denoted as \"v\". \n\nQuality can be calculated by the equation:\n\nformula_1\n"}
{"id": "55326632", "url": "https://en.wikipedia.org/wiki?curid=55326632", "title": "Sonnedix", "text": "Sonnedix\n\nSonnedix Power Holdings Limited (together with its subsidiaries, Sonnedix) is an Independent solar power Power Producer (IPP) with a proven track record in delivering high performance, cost competitive solar photovoltaic plants around the world. \n\nFormed in 2008, Sonnedix has over 700 MW of photovoltaic power plants in operation, as well as several hundred MW under development, in Italy, France, Spain, USA/Puerto Rico, Chile, South Africa and Japan. It is in the top 10 operational European PV Portfolios. \n\nSonnedix is majority owned by institutional investors advised by J P Morgan Asset Management.\n\nwww.sonnedix.com\n\nSonnedix is engaged in the production of renewable energy from solar photovoltaic power at a global level. As of the end of Q1 2018, Sonnedix has a total controlled capacity of 1376.3 MW and 160 PV plants in four continents (North America, South America, Europe and Asia).\n\nOther than the United States, all other countries are locations of Sonnedix's power plants.\n"}
{"id": "23905047", "url": "https://en.wikipedia.org/wiki?curid=23905047", "title": "Southport gas holder", "text": "Southport gas holder\n\nSouthport Gas Holder was once the tallest structure in the northern town of Southport, England for 40 years.\nThe high structure could be seen from miles around, for example from Blackpool and Winter Hill. To some people of the local area it was an instantly recognisable symbol of home coming after being away for weeks.\nIt was built in 1969 in the Blowick area of Southport (1.7 miles from the town centre) – . It acted as a storage unit guaranteeing the town's gas supply. Similar structures were built across the country when town gas was generated from coal and before the construction of a high pressure gas grid.\n\nThe largest tower was decommissioned in January 2008, due to an environmental risk posed by the storage of oil within it. Discussions soon came as it was decommissioned and just weeks later it was decided by National Grid plc, that two out of the three gas holders (including the largest) would be completely demolished as soon as mid-2009. This sparked mixed reviews. Some people thought that the tower was a symbol of Southport, and should not be destroyed. Others however (mostly those who lived directly in the shadow of the tower) disagreed and could not wait to see the back of it. The tower caused problems for local residents such as poor TV reception.\n\nOn Tuesday 14 April, demolition men arrived to start dismantling the 1,100 tonne structure. The second largest crane in Britain came to dismantle the iconic structure and in less than seven weeks it was gone from the Southport skyline forever. The gas holder may be the largest structure the town will ever see.\n\nAfter the destruction of the tower there was a mixed reaction from the local residents, some felt they had lost an icon while others felt the tower was an eyesore, and the removal may even affect house prices. The future of the third holder is still uncertain due to costs of removal.\n'Ground Zero', how some people like to call the site where the gas tower once was, has yet to be developed but it is said it will be either housing and/or for business use. Some people have suggested that there should be something to remember the gas tower, but this idea has not yet been agreed on.\n\n"}
{"id": "889743", "url": "https://en.wikipedia.org/wiki?curid=889743", "title": "Stonehenge Free Festival", "text": "Stonehenge Free Festival\n\nThe Stonehenge Free Festival was a British free festival from 1974 to 1984 held at the prehistoric monument Stonehenge in England during the month of June, and culminating with the summer solstice on or near June 21. It emerged as the major free festival in the calendar after the violent suppression of the Windsor Free Festival in August 1974, with Wally Hope providing the impetus for its founding, and was itself violently suppressed in 1985 in the Battle of the Beanfield, with no free festival held at Stonehenge since although people have been allowed to gather at the stones again for the solstice since 1999. \n\nBy the 1980s, the festival had grown to be a major event, attracting up to 30,000 people in 1984. The festival attendees were viewed as hippies by the wider British public . This, along with the open drug use and sale, contributed to the increase in restrictions on access to Stonehenge, and fences were erected around the stones in 1977. The same year, police resurrected a moribund law against driving over grassland in order to levy fines against festival goers in motorised transport. By 1984 police-festival relations were relaxed with only a nominal police presence required.\nThe festival was a celebration of various alternative cultures. The Tibetan Ukrainian Mountain Troupe, The Tepee People, Circus Normal, the Peace Convoy, New Age Travellers and the Wallys were notable counterculture attendees.\n\nThe stage hosted many bands including Hawkwind, Gong, Doctor and the Medics, Flux of Pink Indians, Buster Blood Vessel, Omega Tribe, Crass, The Selecter, Dexys Midnight Runners, Thompson Twins, Bronz, The Raincoats, The 101ers with Joe Strummer, Jeremy Spencer & the Children of God, Brent Black Music Co-op, Killerhertz, Mournblade, Amazulu, Wishbone Ash, Man, Benjamin Zephaniah, Inner City Unit, Here and Now, Cardiacs, The Enid, Roy Harper, Jimmy Page, Ted Chippington, Zorch and Ozric Tentacles, Vince pie and the crumbs, which all played for free.\n\nThe 1981 list of bands included Red Ice, Selecter, Theatre of Hate, Sugar Minott, Doll by Doll, Thompson Twins, Night Doctor, Merger, Androids of Mu, Deaf Aids, Killerhertz, The Raincoats, Thandoy, Foxes and Rats, ICU Lightning Raiders, Psycho Hampster, Misty in Roots, Andy Allens Future, Inner Visions, Red Beat, Man to Man Triumphant, Stolen Pets, Seeds of Creation, sorcerer ,Coxone Sound System, Black Widow, Here and Now, Hawkwind, Steel and Skin, The Lines, Waiting for Arnold, Play Dead, Cauldron, Lighting by Shoe, Flux of Pink Indians, The Mob, Treatment, Popular History of Signs, The Wystic Mankers, Elsie Steer and Cosmic Dave.\n\n\n\n"}
{"id": "58348348", "url": "https://en.wikipedia.org/wiki?curid=58348348", "title": "The Solar Film", "text": "The Solar Film\n\nThe Solar Film (also known as \"A Short Film on Solar Energy\") is a 1979 short film by Elaine and Saul Bass and produced by Michael Britton.\n\nThis film takes a look at the short history of solar energy, what it is and how can it be used culturally and biologically.\n\nMike Oldfield's \"Tubular Bells\" was used in the film.\n\nThe film was commissioned by Robert Redford who also served as executive producer.\n\n\n"}
{"id": "1645331", "url": "https://en.wikipedia.org/wiki?curid=1645331", "title": "Trigonal pyramidal molecular geometry", "text": "Trigonal pyramidal molecular geometry\n\nIn chemistry, a trigonal pyramid is a molecular geometry with one atom at the apex and three atoms at the corners of a trigonal base, resembling a tetrahedron (not to be confused with the tetrahedral geometry). When all three atoms at the corners are identical, the molecule belongs to point group \"C\". Some molecules and ions with trigonal pyramidal geometry are the pnictogen hydrides (XH), xenon trioxide (XeO), the chlorate ion, , and the sulfite ion, . In organic chemistry, molecules which have a trigonal pyramidal geometry are sometimes described as sp hybridized. The AXE method for VSEPR theory states that the classification is AXE.\n\nThe nitrogen in ammonia has 5 valence electrons and bonds with three hydrogen atoms to complete the octet. This would result in the geometry of a regular tetrahedron with each bond angle equal to cos(−) ≈ 109.5°. However, the three hydrogen atoms are repelled by the electron lone pair in a way that the geometry is distorted to a trigonal pyramid (regular 3-sided pyramid) with bond angles of 107°. In contrast, boron trifluoride is flat, adopting a trigonal planar geometry because the boron does not have a lone pair of electrons. In ammonia the trigonal pyramid undergoes rapid nitrogen inversion.\n\n\n"}
{"id": "1598527", "url": "https://en.wikipedia.org/wiki?curid=1598527", "title": "Tungsram", "text": "Tungsram\n\nTungsram is one of Hungary's largest, oldest, and internationally most prestigious firms, known for light bulbs and electronics. Established in Újpest (today part of Budapest, Hungary) in 1896, it initially produced telephones, wires and switchboards. The name \"Tungsram\" is a portmanteau of \"tungsten\" and \"wolfram\" (the two common names of the metal used for making light bulb filaments).\n\nOn 13 December 1904, Hungarian Sándor Just and Croatian Franjo Hanaman were granted Hungarian patent no. 34541 for the world's first tungsten filament bulb that lasted longer and produced brighter light than a carbon filament. The co-inventors licensed their patent to the company, which came to be named Tungsram after the eponymous tungsten incandescent bulbs, which are still called Tungsram bulbs in many European countries. In 1934, Tungsram incorporated a patent by Imre Bródy for bulbs filled with krypton gas, providing for longer bulb lifetime. During World War I mass production of radio tubes began and became the most profitable division of the company.\n\nBritish Tungsram Radio Works was a subsidiary of Hungarian Tungsram in pre-war days.\n\nIn 1990, General Electric acquired a majority stake in Tungsram and over six years invested $600 million in the venture, thoroughly restructuring every aspect of its operations. To date, this has been the largest manufacturing investment by a U.S. firm in Central and Eastern Europe. Tungsram is today a subsidiary of General Electric and the name is merely retained as a brand.\n\nAs of February 2018, the CEO of GE Hungary, Jörg Bauer agreed to buy GEʼs lighting business in Europe, the Middle East, Africa and Turkey, as well as its global automotive lighting business. The business continues to operate again under the name \"Tungsram Group\".\n\n\n"}
{"id": "3865797", "url": "https://en.wikipedia.org/wiki?curid=3865797", "title": "Uranium tetrafluoride", "text": "Uranium tetrafluoride\n\nUranium tetrafluoride (UF) is a green crystalline solid compound of uranium with an insignificant vapor pressure and very slight solubility in water. Uranium in its tetravalent (uranous) state is very important in different technological processes. In the uranium refining industry it is known as green salt.\n\nUF is generally an intermediate in the conversion of uranium hexafluoride (UF) to either uranium oxides (UO or UO) or uranium metal. It is formed by the reaction of UF with hydrogen gas in a vertical tube-type reactor or by the action of hydrogen fluoride (HF) on uranium dioxide. UF is less stable than the uranium oxides and reacts slowly with moisture at ambient temperature, forming UO and HF, the latter of which is very corrosive; it is thus a less favorable form for long-term disposal. The bulk density of UF varies from about 2.0 g/cm to about 4.5 g/cm depending on the production process and the properties of the starting uranium compounds.\n\nA molten salt reactor design, a type of nuclear reactor where the working fluid is a molten salt, would use UF as the core material. UF is generally chosen over other salts because of the usefulness of the elements without isotope separation, better neutron economy and moderating efficiency, lower vapor pressure and better chemical stability.\n\nLike all uranium salts UF is toxic and thus harmful by inhalation, ingestion and through skin contact. \n\n"}
{"id": "16373117", "url": "https://en.wikipedia.org/wiki?curid=16373117", "title": "Winter Hill air disaster", "text": "Winter Hill air disaster\n\nThe Winter Hill air disaster occurred on 27 February 1958 when the Silver City Airways Bristol 170 Freighter \"G-AICS\", traveling from the Isle of Man to Manchester, England, crashed into Winter Hill (also known as Rivington Moor) several hundred yards away from the Independent Television Authority's Winter Hill transmitting station.\n\nThirty-five people died and seven were injured. The ICAO report states that the accident occurred at 09.15 in the morning. At this time, the weather was so severe that none of the engineers working in the ITA transmitting station were aware of the crash. Several feet of snow hampered rescue efforts, and a snow cat vehicle had to be diverted from the A6 to cut a path for emergency vehicles though the track had been cleared by people using spades by the time it arrived.\n\nThe flight was essentially a charter flight from Ronaldsway Airport in the Isle of Man to Manchester Ringway Airport for a group of mainly Motor Traders to visit the Manchester Exide Battery Factory and Manchester car show.\n\n The Silver City Airways Bristol 170 Freighter \"G-AICS\", call sign \"Charlie Sierra\", was due to fly from Ronaldsway Airport, Ballasalla, on the Isle of Man to an aircraft reporting point at Squire's Gate about from Blackpool. The aircraft was flown by Captain Cairnes at an altitude of due to low cloud and other air traffic in the Manchester area. After gaining clearance from air-traffic control at Manchester Ringway Airport, Captain Cairnes flew \"Charlie Sierra\" inland to Wigan Beacon, a non-directional beacon in the Manchester Zone, which transmitted a recognition signal of \"MYK\" in morse code on a frequency of 316 kHz and a range of about . Due to a number of navigation errors in respect to the radio compass bearings and altitude readings, the Silver City Bristol Freighter crashed near the summit of Winter Hill, five miles (8 km) south-east of Chorley, Lancashire at 9:45 am on 27 February 1958.\n\nThirty-five of the passengers were killed, the majority connected with the Isle of Man motor trade. The three crew were among the seven that survived with injuries.\n\nThe error of the first officer in tuning the radio compass on Oldham Beacon instead of on Wigan Beacon was the probable cause.\n"}
