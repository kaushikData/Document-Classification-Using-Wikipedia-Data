{"id": "24482950", "url": "https://en.wikipedia.org/wiki?curid=24482950", "title": "1979 Turkish Airlines Ankara crash", "text": "1979 Turkish Airlines Ankara crash\n\nThe 1979 Turkish Airlines Ankara crash occurred on December 23, 1979, when a Turkish Airlines Fokker F28 Fellowship 1000 airliner, registration TC-JAT, named \"Trabzon\", on a domestic passenger flight from Samsun Airport to Esenboğa International Airport in Ankara, flew into the side of a hill near the village of Kuyumcuköy in Çubuk district of Ankara Province, north-northeast of the destination airport on approach to landing.\n\nThe crew had deviated from the localizer course while on an ILS approach experiencing severe turbulence.\n\nThe aircraft had four crew and 41 passengers on board. 38 passengers and three crew were killed in the accident.\n\nThe aircraft, a Fokker F28 Fellowship 1000 with two Rolls-Royce RB183-2 \"Spey\" Mk555-15 turbofan jet engines, was built by Fokker with manufacturer serial number 11071, and made its first flight in 1973.\n"}
{"id": "31828619", "url": "https://en.wikipedia.org/wiki?curid=31828619", "title": "Average Service Availability Index", "text": "Average Service Availability Index\n\nThe Average Service Availability Index (ASAI) is a reliability index commonly used by electric power utilities. ASAI is calculated as\n\nformula_1\n\nwhere formula_2 is the number of customers and formula_3 is the annual outage time (in hours) for location formula_4. ASAI can be represented in relation to SAIDI (when the annual SAIDI is given in hours)\n\nformula_5\n\nThe American Society of Architectural Illustrators, ASAI, is an international non-profit organization dedicated to the advancement and recognition of the art, science and profession of architectural illustration. Through communication, education and advocacy, the Society strives to refine and emphasize the role of illustration in the practice and appreciation of architecture.\n\n\n"}
{"id": "3840002", "url": "https://en.wikipedia.org/wiki?curid=3840002", "title": "Bismuth telluride", "text": "Bismuth telluride\n\nBismuth telluride (BiTe) is a gray powder that is a compound of bismuth and tellurium also known as bismuth(III) telluride. It is a semiconductor, which, when alloyed with antimony or selenium, is an efficient thermoelectric material for refrigeration or portable power generation. BiTe is a topological insulator, and thus exhibits thickness-dependent physical properties.\n\nBismuth telluride is a narrow-gap layered semiconductor with a trigonal unit cell. The valence and conduction band structure can be described as a many-ellipsoidal model with 6 constant-energy ellipsoids that are centered on the reflection planes. BiTe cleaves easily along the trigonal axis due to Van der Waals bonding between neighboring tellurium atoms. Due to this, bismuth-telluride-based materials used for power generation or cooling applications must be polycrystalline. Furthermore, the Seebeck coefficient of bulk BiTe becomes compensated around room temperature, forcing the materials used in power-generation devices to be an alloy of bismuth, antimony, tellurium, and selenium.\n\nRecently, researchers have attempted to improve the efficiency of BiTe-based materials by creating structures where one or more dimensions are reduced, such as nanowires or thin films. In one such instance n-type bismuth telluride was shown to have an improved Seebeck coefficient (voltage per unit temperature difference) of −287 μV/K at 54 °C, However, one must realize that Seebeck coefficient and electrical conductivity have a tradeoff: a higher Seebeck coefficient results in decreased carrier concentration and decreased electrical conductivity.\n\nIn another case, researchers report that bismuth telluride has high electrical conductivity of 1.1×10 S·m/m with its very low lattice thermal conductivity of 1.20 W/(m·K), similar to ordinary glass.\n\nBismuth telluride is a well-studied topological insulator. Its physical properties have been shown to change at highly reduced thicknesses, when its conducting surface states are exposed and isolated. These thin samples are obtained through either epitaxy or mechanical exfoliation.\n\nEpitaxial growth methods such as molecular beam epitaxy and metal organic chemical vapor deposition are common methods of obtaining thin BiTe samples. The stoichiometry of samples obtained through such techniques can vary greatly between experiments, so Raman spectroscopy is often used to determine relative purity. However, thin BiTe samples are resistant to Raman spectroscopy due to their low melting point and poor heat dispersion.\n\nThe crystalline structure of BiTe allows for mechanical exfoliation of thin samples by cleaving along the trigonal axis. This process is significantly lower in yield than epitaxial growth, but produces samples without defects or impurities. Similar to extracting graphene from bulk graphite samples, this is done by applying and removing adhesive tape from successively thinner samples. This procedure has been used to obtain BiTe flakes with a thickness of 1 nm. However, this process can leave significant amounts of adhesive residue on a standard Si/SiO substrate, which in turn obscure atomic force microscopy measurements and inhibit the placement of contacts on the substrate for purposes of testing. Common cleaning techniques such as oxygen plasma, boiling acetone and isopropyl alcohol are ineffective in removing residue..\n\nThe mineral form of BiTe is tellurobismuthite which is moderately rare. There are many natural bismuth tellurides of different stoichiometry, as well as compounds of the Bi-Te-S-(Se) system, like BiTeS (tetradymite).\n\nBismuth telluride is prepared by sealing a sample of bismuth and tellurium metal in a quartz tube under vacuum (critical, as an unsealed or leaking sample may explode in a furnace) and heating it to 800 °C in a muffle furnace.\n\n\n"}
{"id": "46735500", "url": "https://en.wikipedia.org/wiki?curid=46735500", "title": "Brad De Losa", "text": "Brad De Losa\n\nBrad De Losa (born 1979) is an Australian fitter who is a champion at forestry sports such as wood-chopping and sawing. In 2015, he set a new world record, cutting through four tree trunks in less than 58 seconds. In May 2017 Brad De Losa managed to win the Stihl Timbersports Champions Trophy for a third year in a row.\n\nDe Losa won his first competition at the age of 16.\n\n"}
{"id": "51538671", "url": "https://en.wikipedia.org/wiki?curid=51538671", "title": "CNP-600", "text": "CNP-600\n\nThe CNP-600 is a pressurized water nuclear reactor developed by the China National Nuclear Corporation (CNNC).\nThe reactor has a gross electrical capacity of 650 MW.\nIt is a generation II reactor based on China's first commercial nuclear reactor design, the CNP-300, and on the M310 reactor design used in Daya Bay Nuclear Power Plant.\n\nThe first CNP-600 unit began operation at Qinshan Nuclear Power Plant in 2002, with other 3 units coming online between 2004 and 2011.\nThere are only 2 CNP-600 reactors currently under construction, both at Changjiang Nuclear Power Plant.\nAn advanced version of the design, the ACP-600, is currently being developed by the CNNC. The new design will have improved safety systems and an extended design life of 60 years.\n\n"}
{"id": "8261070", "url": "https://en.wikipedia.org/wiki?curid=8261070", "title": "COMES", "text": "COMES\n\nThe French Solar Energy Authority (Commissariat à l'Energie Solaire, ComES), a public scientific and industrial entity, was set up in 1978 to promote a comprehensive energy policy based on energy savings, on efficient energy management, and on renewable sources of energy (photovoltaic, solar thermal, wind, hydraulic, biomass). It was supervised by the Ministry for Industry and by the Ministry for Research. When it was discontinued, its duties were taken up by the French Agency for the Environment and Energy Management, ADEME.\n\nThe first Managing Director and Chief Executive of ComES was M. Henry Durand, an engineer.\n\nAs a national agency, COMES defined, financed and evaluated projects using renewable energies. Shortly after this agency was created, its Department of International Affairs was set up (by Jean-Jacques Subrenat, a career diplomat), and became involved in a number of projects, both multilateral and in the context of bilateral relations between France and partner countries.\n\nA new distribution of tasks among public agencies led to the French Solar Energy Authority being discontinued: its tasks were taken over, and expanded, by the Agence de l'Environnement et de la Maîtrise de l'Energie (ADEME) which, compared with its predecessors, has a wider purview which includes the environment.\n"}
{"id": "36742682", "url": "https://en.wikipedia.org/wiki?curid=36742682", "title": "Coal Exchange (London)", "text": "Coal Exchange (London)\n\nThe London Coal Exchange was situated on the north side of Thames Street in the City of London, nearly opposite to Old Billingsgate Market, occupying three different structures from 1770 to 1962. The original coal exchange opened in 1770. A second building from 1805 was replaced by a new purpose-built structure constructed from 1847 to 1849, and opened by Prince Albert on 30 October 1849. This third London coal exchange was one of the first substantial buildings constructed from cast iron, built several years before the hall at the Great Exhibition. It was demolished in 1962 to allow widening of what is now Lower Thames Street despite a campaign by the Victorian Society to save the building. Cast iron decorations from the 1849 Coal Exchange building were selected as the model for the dragon boundary mark for the main entrances to the City of London.\n\nCoal had been imported to London by sea since at least medieval times. A coal exchange was established in 1770 on Thames Street in the City of London, near the site of Smart's Quay and close to Billingsgate Market; the main trades at Billingsgate Dock were fish and coal. The market was established by the main coal merchants as a private body to regulate the trade of coal in the capital, and was controlled by private coal merchants until the old Coal Exchange was bought by the Corporation of London in 1807. A new building had been built in 1805, with a recessed classical portico supported by small Doric pillars and triangular pediment above, with stone steps leading to a quadrangle within. Under the control of the City Corporation, the Coal Exchange became a free and open market, regulated by various Acts of Parliament, including Acts in 1831, 1838 and 1845.\n\nAt this period, London was heated almost entirely by coal. By 1848, approximately 3.5 million tons of coal was being transported each year from the coalfields in Northumberland and Durham to London, with over 12,000 shiploads carried on nearly 3,000 vessels. The coal trade was also an important source of tax revenue for the city. The duty on coal funded Wren's rebuilding of more than fifty city churches and St Paul's Cathedral after the Great Fire of London in 1666, and also funded for other building works in the metropolis, such as the Thames Embankment.\n\nHistorically, coal taxes (metage, payable on each chaldron of 35 bushels or the imperial ton) were charged by the City based on volume measurements. A coal duty of 4 pence was confirmed by James I, with a duty of 8 pence more added under William III and Mary II, and an additional 1 pence added to fund the construction of the new Coal Exchange. From 1831, the City of London charged a duty of 13 pence per imperial ton before coal could be unshipped, and a certificate was sent to the Coal Exchange stating the date of shipment, name and owner of the ship, quantity of coal, where it was mined, and the price paid. The coal trade was dominated by sea transport until 1845, but railway transportation increased in importance in the 1850s and 1860s, with similar quantities carried on the rails, and the coal duty was extended to coal brought within 20 miles of London by any means in 1862. By 1875, five million tons of coal were being brought to London each year by rail and three million tons by sea.\n\nThousands of workmen of various grades were employed to move the coal from the ships to a customer's coal cellar, with \"coal-whippers\" carrying sacks of coal by hand from a ship to a coal merchant's lighter, and then taken onshore by \"coal-backers\". The coal was sorted by \"coal-sifter\" and put into coal-sacks by a \"coal-filler\", then transported by \"coal-waggoners\" and delivered by a \"coal-trimmer\". The work was heavy manual labour, but the trade was essential and workers were reasonably well paid for the period.\n\nIn the Coal Exchange, coal factors acted as agents between sellers and buyers. Factors agreed with coal sellers how much coal is available on a particular day, based on the ships available and the market price, with the market meeting on Mondays, Wednesdays and Fridays from 12 noon to 2:30 pm. All sales were agreed privately, with no public auction, and factors would take a 0.5% commission.\n\nBy 1845, a petition was made to build a new exchange, and the City Clerk of Works, James Bunstone Bunning, produced a design. Construction started in December 1847 and the new Coal Exchange was formally opened by Prince Albert on 30 October 1849. The Lord Mayor and City MP, James Duke, was made a baronet in honour of the occasion.\n\nThe new Coal Exchange was built on the north side of Thames Street, on the east side of its junction with St. Mary-at-Hill, with four floors. A Roman hypocaust was found during the excavation of the building's foundations, part of the Roman house at Billingsgate, and preserved in its basement and is now a scheduled ancient monument.\n\nThe south and west fronts, facing the streets, were built in Italianate style from Portland stone, with four floors, measuring wide and high. At the southwest corner was an unusual high semi-circular portico with Doric columns and entablature, surmounted by a tower of Portland stone high, with a conical roof topped by a gilt ball. Within the tower was a staircase providing access to the upper floors. The ground floor portico provided access to an entrance vestibule leading to a large central circular vaulted hall. The central rotunda was in diameter, with a wooden floor inlaid with a large mariner's compass. The rotunda was covered by a glazed cast iron dome with its centre above the ground, held up by 8 cast iron piers, supported by 32 ribs 42 ft 6in long.\n\nThe dome design was based on that of the Bourse de commerce of Paris by François-Joseph Bélanger and François Brunet, completed in 1811. The dome was decorated with \"Raphaelesque\" encaustic panels by Frederick Sang on a coal-related theme, depicting fossils of ferns, palms and other plants, and images of collieries and mining operations, and views of North Shields, Sunderland, Newcastle upon Tyne and Durham together, with cast iron decorative features. The piers also supported three tiers of cast iron galleries which opened on to offices around the exterior of the building which were occupied by coal factors and other agents and merchants connected with the coal trade.\n\nThe building suffered some damage in the Second World War, and it ceased to be used as a coal exchange after the war when the coal industry was nationalised. It was then used as offices, but the City of London did not proceed with plans to refurbish the building in the 1950s because its demolition had been suggested to allow widening of the road from Blackfriars to the East End, and it became progressively more dilapidated. Nonetheless, in the 1950s, Professor Richard Hitchcock described it as \"the prime city monument of the early Victorian period\". In September 1956, John Betjeman (a founding member of the Victorian Society and a passionate defender of Victorian architecture) gave a speech to the Society for the Protection of Ancient Buildings in the rotunda of the Coal Exchange to argue for its preservation. It became a Grade 2 listed building in 1958. A letter published in \"The Times\" and signed by Walter Gropius, Sigfried Giedion, Josep Lluís Sert and Eduard Sekler described the Coal Exchange as \"a landmark in the history of early iron construction\".\n\nVarious alternatives were proposed. The Georgian Group and the Victorian Society both favoured preservation of the Coal Exchange, even if that meant that the \"very dull, plain and ordinary\" rear parts of the nearby Grade-1 listed Custom House (then the headquarters of HM Customs & Excise) were removed. Others suggested a scheme in which a walkway would be added in arches under the Coal Exchange. There was also a suggestion that the dome could become part of the new Royal School of Music in the Barbican, or shipped to Australia to become part of the National Gallery of Victoria in Melbourne, but funds were not available.\n\nMP Tom Driberg made a speech in an adjournment debate in February 1961, quoting a statement by Sir Mortimer Wheeler published in \"The Times\" the previous day, saying that \"Professor Pevsner has placed the threatened London Coal Exchange among the twelve irreplaceable buildings of 19th century England ... It expresses an era of urban revolution as no other surviving building is capable of doing ... The Coal Exchange is a national monument in the fullest sense of the phrase, and its destruction would be unforgivable.\"\n\nThe Corporation was uncooperative, and declined to allow any respite. One member remarked that \"We cannot spend time on the preservation of a Victorian building\".\nDespite campaigns and protests, it was demolished in November 1962 to make way for a \"vital\" widening of Lower Thames Street. The demolition of the Coal Exchange was described by author Hermione Hobhouse as \"one of the great conservationist horror stories\" and its loss has been compared to the demolition of the Adelphi in 1936 and of Euston Arch shortly before, in 1961. The cleared site was then left empty for 10 years while other land was acquired for the road widening scheme.\n\nCast iron dragons which were mounted on the eaves parapet above the entrance to the Coal Exchange were preserved and were erected as dragon boundary marks in October 1963 in Temple Gardens on Victoria Embankment. Half-sized replicas were erected at the other main entrances to the City, in preference to the more fierce dragon at Temple Bar.\n\n\n"}
{"id": "24580536", "url": "https://en.wikipedia.org/wiki?curid=24580536", "title": "Cobalt", "text": "Cobalt\n\nCobalt is a chemical element with symbol Co and atomic number 27. Like nickel, cobalt is found in the Earth's crust only in chemically combined form, save for small deposits found in alloys of natural meteoric iron. The free element, produced by reductive smelting, is a hard, lustrous, silver-gray metal.\n\nCobalt-based blue pigments (cobalt blue) have been used since ancient times for jewelry and paints, and to impart a distinctive blue tint to glass, but the color was later thought by alchemists to be due to the known metal bismuth. Miners had long used the name \"kobold ore\" (German for \"goblin ore\") for some of the blue-pigment producing minerals; they were so named because they were poor in known metals, and gave poisonous arsenic-containing fumes when smelted. In 1735, such ores were found to be reducible to a new metal (the first discovered since ancient times), and this was ultimately named for the \"kobold\".\n\nToday, some cobalt is produced specifically from one of a number of metallic-lustered ores, such as for example cobaltite (CoAsS). The element is however more usually produced as a by-product of copper and nickel mining. The copper belt in the Democratic Republic of the Congo (DRC) and Zambia yields most of the global cobalt production. The DRC alone accounted for more than 50% of world production in 2016 (123,000 tonnes), according to Natural Resources Canada.\n\nCobalt is primarily used in the manufacture of magnetic, wear-resistant and high-strength alloys. The compounds cobalt silicate and cobalt(II) aluminate (CoAlO, cobalt blue) give a distinctive deep blue color to glass, ceramics, inks, paints and varnishes. Cobalt occurs naturally as only one stable isotope, cobalt-59. Cobalt-60 is a commercially important radioisotope, used as a radioactive tracer and for the production of high energy gamma rays.\n\nCobalt is the active center of a group of coenzymes called cobalamins. vitamin B, the best-known example of the type, is an essential vitamin for all animals. Cobalt in inorganic form is also a micronutrient for bacteria, algae, and fungi.\n\nCobalt is a ferromagnetic metal with a specific gravity of 8.9. The Curie temperature is and the magnetic moment is 1.6–1.7 Bohr magnetons per atom. Cobalt has a relative permeability two-thirds that of iron. Metallic cobalt occurs as two crystallographic structures: hcp and fcc. The ideal transition temperature between the hcp and fcc structures is , but in practice the energy difference between them is so small that random intergrowth of the two is common.\n\nCobalt is a weakly reducing metal that is protected from oxidation by a passivating oxide film. It is attacked by halogens and sulfur. Heating in oxygen produces CoO which loses oxygen at to give the monoxide CoO. The metal reacts with fluorine (F) at 520 K to give CoF; with chlorine (Cl), bromine (Br) and iodine (I), producing equivalent binary halides. It does not react with hydrogen gas (H) or nitrogen gas (N) even when heated, but it does react with boron, carbon, phosphorus, arsenic and sulfur. At ordinary temperatures, it reacts slowly with mineral acids, and very slowly with moist, but not with dry, air.\n\nCommon oxidation states of cobalt include +2 and +3, although compounds with oxidation states ranging from −3 to +5 are also known. A common oxidation state for simple compounds is +2 (cobalt(II)). These salts form the pink-colored metal aquo complex [Co(HO)] in water. Addition of chloride gives the intensely blue . In a borax bead flame test, cobalt shows deep blue in both oxidizing and reducing flames.\n\nSeveral oxides of cobalt are known. Green cobalt(II) oxide (CoO) has rocksalt structure. It is readily oxidized with water and oxygen to brown cobalt(III) hydroxide (Co(OH)). At temperatures of 600–700 °C, CoO oxidizes to the blue cobalt(II,III) oxide (CoO), which has a spinel structure. Black cobalt(III) oxide (CoO) is also known. Cobalt oxides are antiferromagnetic at low temperature: CoO (Néel temperature 291 K) and CoO (Néel temperature: 40 K), which is analogous to magnetite (FeO), with a mixture of +2 and +3 oxidation states.\n\nThe principal chalcogenides of cobalt include the black cobalt(II) sulfides, CoS, which adopts a pyrite-like structure, and cobalt(III) sulfide (CoS).\n\nFour dihalides of cobalt(II) are known: cobalt(II) fluoride (CoF, pink), cobalt(II) chloride (CoCl, blue), cobalt(II) bromide (CoBr, green), cobalt(II) iodide (CoI, blue-black). These halides exist in anhydrous and hydrated forms. Whereas the anhydrous dichloride is blue, the hydrate is red.\n\nThe reduction potential for the reaction + e → is +1.92 V, beyond that for chlorine to chloride, +1.36 V. Consequently, cobalt(III) and chloride would result in the cobalt(III) being reduced to cobalt(II). Because the reduction potential for fluorine to fluoride is so high, +2.87 V, cobalt(III) fluoride is one of the few simple stable cobalt(III) compounds. Cobalt(III) fluoride, which is used in some fluorination reactions, reacts vigorously with water.\nAs for all metals, molecular compounds and polyatomic ions of cobalt are classified as coordination complexes, that is, molecules or ions that contain cobalt linked to several ligands. The principles of electronegativity and hardness–softness of a series of ligands can be used to explain the usual oxidation state of cobalt. For example, Co complexes tend to have ammine ligands. Because phosphorus is softer than nitrogen, phosphine ligands tend to feature the softer Co and Co, an example being tris(triphenylphosphine)cobalt(I) chloride ((P(CH))CoCl). The more electronegative (and harder) oxide and fluoride can stabilize Co and Co derivatives, e.g. caesium hexafluorocobaltate (CsCoF) and potassium percobaltate (KCoO).\n\nAlfred Werner, a Nobel-prize winning pioneer in coordination chemistry, worked with compounds of empirical formula [Co(NH)]Cl. One of the isomers determined was cobalt(III) hexammine chloride. This coordination complex, a typical Werner-type complex, consists of a central cobalt atom coordinated by six ammine orthogonal ligands and three chloride counteranions. Using chelating ethylenediamine ligands in place of ammonia gives tris(ethylenediamine)cobalt(III) chloride ([Co(en)]Cl), which was one of the first coordination complexes to be resolved into optical isomers. The complex exists in the right- and left-handed forms of a \"three-bladed propeller\". This complex was first isolated by Werner as yellow-gold needle-like crystals.\n\nCobaltocene is a structural analog to ferrocene, with cobalt in place of iron. Cobaltocene is much more sensitive to oxidation than ferrocene. Cobalt carbonyl (Co(CO)) is a catalyst in carbonylation and hydrosilylation reactions. Vitamin B (see below) is an organometallic compound found in nature and is the only vitamin that contains a metal atom. An example of an alkylcobalt complex in the otherwise uncommon +4 oxidation state of cobalt is the homoleptic complex (Co(1-norb)), a transition metal-alkyl complex that is notable for its stability to β-hydrogen elimination. The cobalt(III) and cobalt(V) complexes [Li(THF)][Co(1-norb)] and [Co(1-norb)][BF] are also known.\n\nCo is the only stable cobalt isotope and the only isotope that exists naturally on Earth. Twenty-two radioisotopes have been characterized; the most stable, Co has a half-life of 5.2714 years, and Co has a half-life of 271.8 days, Co a half-life of 77.27 days, and Co a half-life of 70.86 days. All the other radioactive isotopes of cobalt have half-lives shorter than 18 hours, and in most cases shorter than 1 second. This element also has 4 meta states, all of which have half-lives shorter than 15 minutes.\n\nThe isotopes of cobalt range in atomic weight from 50 u (Co) to 73 u (Co). The primary decay mode for isotopes with atomic mass unit values less than that of the most abundant stable isotope, Co, is electron capture and the primary mode of decay in isotopes with atomic mass greater than 59 atomic mass units is beta decay. The primary decay products below Co are element 26 (iron) isotopes; above that the decay products are element 28 (nickel) isotopes.\n\nCobalt compounds have been used for centuries to impart a rich blue color to glass, glazes, and ceramics. Cobalt has been detected in Egyptian sculpture, Persian jewelry from the third millennium BC, in the ruins of Pompeii, destroyed in 79 AD, and in China, dating from the Tang dynasty (618–907 AD) and the Ming dynasty (1368–1644 AD).\n\nCobalt has been used to color glass since the Bronze Age. The excavation of the Uluburun shipwreck yielded an ingot of blue glass, cast during the 14th century BC. Blue glass from Egypt was either colored with copper, iron, or cobalt. The oldest cobalt-colored glass is from the eighteenth dynasty of Egypt (1550–1292 BC). The source for the cobalt the Egyptians used is not known.\n\nThe word \"cobalt\" is derived from the German \"\", from \"kobold\" meaning \"goblin\", a superstitious term used for the ore of cobalt by miners. The first attempts to smelt those ores for copper or nickel failed, yielding simply powder (cobalt(II) oxide) instead. Because the primary ores of cobalt always contain arsenic, smelting the ore oxidized the arsenic into the highly toxic and volatile arsenic oxide, adding to the notoriety of the ore.\n\nSwedish chemist Georg Brandt (1694–1768) is credited with discovering cobalt circa 1735, showing it to be a previously unknown element, different from bismuth and other traditional metals. Brandt called it a new \"semi-metal\". He showed that compounds of cobalt metal were the source of the blue color in glass, which previously had been attributed to the bismuth found with cobalt. Cobalt became the first metal to be discovered since the pre-historical period. All other known metals (iron, copper, silver, gold, zinc, mercury, tin, lead and bismuth) had no recorded discoverers.\n\nDuring the 19th century, a significant part of the world's production of cobalt blue (a dye made with cobalt compounds and alumina) and smalt (cobalt glass powdered for use for pigment purposes in ceramics and painting) was carried out at the Norwegian Blaafarveværket. The first mines for the production of smalt in the 16th century were located in Norway, Sweden, Saxony and Hungary. With the discovery of cobalt ore in New Caledonia in 1864, the mining of cobalt in Europe declined. With the discovery of ore deposits in Ontario, Canada in 1904 and the discovery of even larger deposits in the Katanga Province in the Congo in 1914, the mining operations shifted again. When the Shaba conflict started in 1978, the copper mines of Katanga Province nearly stopped production. The impact on the world cobalt economy from this conflict was smaller than expected: cobalt is a rare metal, the pigment is highly toxic, and the industry had already established effective ways for recycling cobalt materials. In some cases, industry was able to change to cobalt-free alternatives.\n\nIn 1938, John Livingood and Glenn T. Seaborg discovered the radioisotope cobalt-60. This isotope was famously used at Columbia University in the 1950s to establish parity violation in radioactive beta decay.\n\nAfter World War II, the US wanted to guarantee the supply of cobalt ore for military uses (as the Germans had been doing) and prospected for cobalt within the U.S. border. An adequate supply of the ore was found in Idaho near Blackbird canyon in the side of a mountain. The firm Calera Mining Company started production at the site.\n\nThe stable form of cobalt is produced in supernovas through the r-process. It comprises 0.0029% of the Earth's crust. Free cobalt (the native metal) is not found on Earth because of the oxygen in the atmosphere and the chlorine in the ocean. Both are abundant enough in the upper layers of the Earth's crust to prevent native metal cobalt from forming. Except as recently delivered in meteoric iron, pure cobalt in native metal form is unknown on Earth. The element has a medium abundance but natural compounds of cobalt are numerous and all amounts of cobalt compounds are found in most rocks, soil, plants, and animals.\n\nIn nature, cobalt is frequently associated with nickel. Both are characteristic components of meteoric iron, though cobalt is much less abundant in iron meteorites than nickel. As with nickel, cobalt in meteoric iron alloys may have been well enough protected from oxygen and moisture to remain as the free (but alloyed) metal, though neither element is seen in that form in the ancient terrestrial crust.\n\nCobalt in compound form occurs in copper and nickel minerals. It is the major metallic component that combines with sulfur and arsenic in the sulfidic cobaltite (CoAsS), safflorite (CoAs), glaucodot ((Co,Fe)AsS), and skutterudite (CoAs) minerals. The mineral cattierite is similar to pyrite and occurs together with vaesite in the copper deposits of Katanga Province. When it reaches the atmosphere, weathering occurs; the sulfide minerals oxidize and form pink erythrite (\"cobalt glance\": Co(AsO)·8HO) and spherocobaltite (CoCO).\n\nCobalt is also a constituent of tobacco smoke. The tobacco plant readily absorbs and accumulates heavy metals like cobalt from the surrounding soil in its leaves. These are subsequently inhaled during tobacco smoking.\n\nThe main ores of cobalt are cobaltite, erythrite, glaucodot and skutterudite (see above), but most cobalt is obtained by reducing the cobalt by-products of nickel and copper mining and smelting.\n\nSince cobalt is generally produced as a by-product, the supply of cobalt depends to a great extent on the economic feasibility of copper and nickel mining in a given market. Demand for cobalt was projected to grow 6% in 2017.\n\nSeveral methods exist to separate cobalt from copper and nickel, depending on the concentration of cobalt and the exact composition of the used ore. One method is froth flotation, in which surfactants bind to different ore components, leading to an enrichment of cobalt ores. Subsequent roasting converts the ores to cobalt sulfate, and the copper and the iron are oxidized to the oxide. Leaching with water extracts the sulfate together with the arsenates. The residues are further leached with sulfuric acid, yielding a solution of copper sulfate. Cobalt can also be leached from the slag of copper smelting.\n\nThe products of the above-mentioned processes are transformed into the cobalt oxide (CoO). This oxide is reduced to metal by the aluminothermic reaction or reduction with carbon in a blast furnace.\n\nThe United States Geological Survey estimates world reserves of cobalt at 7,100,000 metric tons. The Democratic Republic of the Congo (DRC) currently produces 63% of the world’s cobalt. This market share may reach 73% by 2025 if planned expansions by mining producers like Glencore Plc take place as expected. But by 2030, global demand could be 47 times more than it was in 2017, Bloomberg New Energy Finance has estimated.\n\nChanges that Congo made to mining laws in 2002 led did attract investment in Congolese copper and cobalt projects. However Glencore dominates the coltan market in DRC. Its Mutanda mine shipped 24,500 tons of cobalt from its Mutanda mine last year, 40% of Congo DRC’s output and nearly a quarter of global production. T Glencore’s Katanga Mining project is resuming as well and should produce 300,000 tons of copper and 20,000 tons of cobalt by 2019, according to Glencore.\n\nIn 2005, the top producer of cobalt was the copper deposits in the Democratic Republic of the Congo's Katanga Province. Formerly Shaba province, the area had almost 40% of global reserves, reported the British Geological Survey in 2009. By 2015, Democratic Republic of the Congo (DRC) supplied 60% of world cobalt production, 32,000 tons at $20,000 to $26,000 per ton. Recent growth in production could at least partly be due to how low mining production fell during DRC Congo's very violent civil wars in the early 2000s, or to the changes the country made to its Mining Code in 2002 to encourage foreign and multinational investment and which did bring in a number of investors, including Glencore. Three of its current Congo projects \nArtisanal mining supplied 10% to 25% of the DRC production. Some 100,000 cobalt miners in Congo DRC use hand tools to dig hundreds of feet, with little planning and fewer safety measures, say workers and government and NGO officials, as well as Washington Post reporters' observations on visits to isolated mines. The lack of safety precautions frequently causes injuries or death. Mining pollutes the vicinity and exposes local wildlife and indigenous communities to toxic metals thought to cause birth defects and breathing difficulties, according to health officials.\n\nHuman rights activists have alleged, and investigative journalism reported confirmation, that child labor is used in mining cobalt from African artisanal mines. This revelation prompted cell phone maker Apple Inc., on March 3, 2017, to stop buying ore from suppliers such as Zhejiang Huayou Cobalt who source from artisanal mines in the DRC, and begin using only suppliers that are verified to meet its workplace standards.\n\nThe political and ethnic dynamics of the region have in the past caused horrific outbreaks of violence and years of armed conflict and displaced populations. This instability affected the price of cobalt and also created perverse incentives for the combattants in the First and Second Congo Wars to prolong the fighting, since access to diamond mines and other valuable resources helped to finance their military goals—which frequently amounteed to genocide—and also enriched the fighters themselves. While DR Congo has in the 2010s not recently been invaded by neighboring military forces, some of the richest mineral deposits adjoin areas where Tutsis and Hutus still frequently clash, unrest continues although on a smaller scale and refugees still flee outbreaks of violence.\nCobalt extracted from small Congolese artisanal mining endeavors in 2007 supplied a single Chinese company, Congo DongFang International Mining. A subsidiary of Zhejiang Huayou Cobalt, one of the world’s largest cobalt producers, Congo DongFang supplied cobalt to some of the world’s largest battery manufacturers, who produced batteries for ubiquitous products like the Apple iPhones. Corporate pieties about an ethical supply chain were thus met with some incredulity. A number of observers have called for tech corporations and other manufacturers to avoid sourcing conflict metals in Central Africa at all rather than risk enabling the financial exploitation, human rights abuses like kidnappings for unfree labor, environmental devastation and the human toll of violence, poverty and toxic conditions.\n\nThe Mukondo Mountain project, operated by the Central African Mining and Exploration Company (CAMEC) in Katanga Province, may be the richest cobalt reserve in the world. It produced an estimated one third of the total global coval production in 2008. In July 2009, CAMEC announced a long-term agreement to deliver its entire annual production of cobalt concentrate from Mukondo Mountain to Zhejiang Galico Cobalt & Nickel Materials of China.\n\nIn February 2018, global asset management firm AllianceBernstein defined the DRC as economically \"the Saudi Arabia of the electric vehicle age,\" due to its cobalt resources, as essential to the lithium-ion batteries that drive electric vehicles.\n\nOn March 9, 2018, President Joseph Kabila updated the 2002 mining code, increasing royalty charges and declaring cobalt and coltan \"strategic metals\".\n\nIn 2017, some exploration companies were planning to survey old silver and cobalt mines in the area of Cobalt, Ontario where significant deposits are believed to lie. The mayor of Cobalt stated that the people of Cobalt welcomed new mining endeavours and pointed out that the local work force is peaceful and English-speaking, and good infrastructure would allow much easier sourcing of spare parts for the equipment or other supplies than were to be found in conflict-zones.\n\nCobalt has been used in production of high-performance alloys. It can also be used to make rechargeable batteries, and the advent of electric vehicles and their success with consumers probably has a great deal to do with the DRC's soaring production. Other important factors were the 2002 Mining Code, which encouraged investment by foreign and transnational corporations such as Glencore, and the end of the First and Second Congo Wars.\n\nCobalt-based superalloys have historically consumed most of the cobalt produced. The temperature stability of these alloys makes them suitable for turbine blades for gas turbines and aircraft jet engines, although nickel-based single-crystal alloys surpass them in performance. Cobalt-based alloys are also corrosion- and wear-resistant, making them, like titanium, useful for making orthopedic implants that don't wear down over time. The development of wear-resistant cobalt alloys started in the first decade of the 20th century with the stellite alloys, containing chromium with varying quantities of tungsten and carbon. Alloys with chromium and tungsten carbides are very hard and wear-resistant. Special cobalt-chromium-molybdenum alloys like Vitallium are used for prosthetic parts (hip and knee replacements). Cobalt alloys are also used for dental prosthetics as a useful substitute for nickel, which may be allergenic. Some high-speed steels also contain cobalt for increased heat and wear resistance. The special alloys of aluminium, nickel, cobalt and iron, known as Alnico, and of samarium and cobalt (samarium-cobalt magnet) are used in permanent magnets. It is also alloyed with 95% platinum for jewelry, yielding an alloy suitable for fine casting, which is also slightly magnetic.\n\nLithium cobalt oxide (LiCoO) is widely used in lithium-ion battery cathodes. The material is composed of cobalt oxide layers with the lithium intercalated. During discharge, the lithium is released as lithium ions. Nickel-cadmium (NiCd) and nickel metal hydride (NiMH) batteries also include cobalt to improve the oxidation of nickel in the battery. \nTransparency Market Research estimated the global lithium-ion battery market at $30 billion in 2015 and predicted an increase to over US$75 billion by 2024.\n\nAlthough in 2018 most cobalt in batteries was used in a mobile device, a more recent application for cobalt is rechargeable batteries for electric cars. This industry has increased five-fold in its demand for cobalt, which makes it urgent to find new raw materials in more stable areas of the world. Demand is expected to continue or increase as the prevalence of electric vehicles increases. Exploration in 2016–2017 included the area around Cobalt, Ontario, an area where many silver mines ceased operation decades ago.\n\nSince child and slave labor have been repeatedly reported in coltan mining, primarily in the artisanal mines of DR Congo, tech companies seeking an ethical supply chain have faced shortages of this raw material and the price of cobalt metal reached a nine-year high in October 2017, more than US$30 a pound, versus US$10 in late 2015.\n\nSeveral cobalt compounds are oxidation catalysts. Cobalt acetate is used to convert xylene to terephthalic acid, the precursor of the bulk polymer polyethylene terephthalate. Typical catalysts are the cobalt carboxylates (known as cobalt soaps). They are also used in paints, varnishes, and inks as \"drying agents\" through the oxidation of drying oils. The same carboxylates are used to improve the adhesion between steel and rubber in steel-belted radial tires. In addition they are used as accelerators in polyester resin systems.\n\nCobalt-based catalysts are used in reactions involving carbon monoxide. Cobalt is also a catalyst in the Fischer–Tropsch process for the hydrogenation of carbon monoxide into liquid fuels. Hydroformylation of alkenes often uses cobalt octacarbonyl as a catalyst, although it is often replaced by more efficient iridium- and rhodium-based catalysts, e.g. the Cativa process.\n\nThe hydrodesulfurization of petroleum uses a catalyst derived from cobalt and molybdenum. This process helps to clean petroleum of sulfur impurities that interfere with the refining of liquid fuels.\n\nBefore the 19th century, cobalt was predominantly used as a pigment. It has been used since the Middle Ages to make smalt, a blue-colored glass. Smalt is produced by melting a mixture of roasted mineral smaltite, quartz and potassium carbonate, which yields a dark blue silicate glass, which is finely ground after the production. Smalt was widely used to color glass and as pigment for paintings. In 1780, Sven Rinman discovered cobalt green, and in 1802 Louis Jacques Thénard discovered cobalt blue. Cobalt pigments such as cobalt blue (cobalt aluminate), cerulean blue (cobalt(II) stannate), various hues of cobalt green (a mixture of cobalt(II) oxide and zinc oxide), and cobalt violet (cobalt phosphate) are used as artist's pigments because of their superior chromatic stability. Aureolin (cobalt yellow) is now largely replaced by more lightfast yellow pigments.\n\nCobalt-60 (Co-60 or Co) is useful as a gamma-ray source because they can be produced in predictable quantity and high activity by bombarding cobalt with neutrons. It produces gamma rays with energies of 1.17 and 1.33 MeV.\n\nCobalt is used in external beam radiotherapy, sterilization of medical supplies and medical waste, radiation treatment of foods for sterilization (cold pasteurization), industrial radiography (e.g. weld integrity radiographs), density measurements (e.g. concrete density measurements), and tank fill height switches. The metal has the unfortunate property of producing a fine dust, causing problems with radiation protection. Cobalt from radiotherapy machines has been a serious hazard when not discarded properly, and one of the worst radiation contamination accidents in North America occurred in 1984, when a discarded radiotherapy unit containing cobalt-60 was mistakenly disassembled in a junkyard in Juarez, Mexico.\n\nCobalt-60 has a radioactive half-life of 5.27 years. Loss of potency requires periodic replacement of the source in radiotherapy and is one reason why cobalt machines have been largely replaced by linear accelerators in modern radiation therapy. Cobalt-57 (Co-57 or Co) is a cobalt radioisotope most often used in medical tests, as a radiolabel for vitamin B uptake, and for the Schilling test. Cobalt-57 is used as a source in Mössbauer spectroscopy and is one of several possible sources in X-ray fluorescence devices.\n\nNuclear weapon designs could intentionally incorporate Co, some of which would be activated in a nuclear explosion to produce Co. The Co, dispersed as nuclear fallout, is sometimes called a cobalt bomb.\n\n\nCobalt is essential to the metabolism of all animals. It is a key constituent of cobalamin, also known as vitamin B, the primary biological reservoir of cobalt as an ultratrace element. Bacteria in the stomachs of ruminant animals convert cobalt salts into vitamin B, a compound which can only be produced by bacteria or archaea. A minimal presence of cobalt in soils therefore markedly improves the health of grazing animals, and an uptake of 0.20 mg/kg a day is recommended because they have no other source of vitamin B.\n\nIn the early 20th century, during the development of farming on the North Island Volcanic Plateau of New Zealand, cattle suffered from what was termed \"bush sickness\". It was discovered that the volcanic soils lacked the cobalt salts essential for the cattle food chain.\n\nThe \"coast disease\" of sheep in the Ninety Mile Desert of the Southeast of South Australia in the 1930s was found to originate in nutritional deficiencies of trace elements cobalt and copper. The cobalt deficiency was overcome by the development of \"cobalt bullets\", dense pellets of cobalt oxide mixed with clay given orally for lodging in the animal's rumen.\n\nProteins based on cobalamin use corrin to hold the cobalt. Coenzyme B features a reactive C-Co bond that participates in the reactions. In humans, B has two types of alkyl ligand: methyl and adenosyl. MeB promotes methyl (−CH) group transfers. The adenosyl version of B catalyzes rearrangements in which a hydrogen atom is directly transferred between two adjacent atoms with concomitant exchange of the second substituent, X, which may be a carbon atom with substituents, an oxygen atom of an alcohol, or an amine. Methylmalonyl coenzyme A mutase (MUT) converts MMl-CoA to Su-CoA, an important step in the extraction of energy from proteins and fats.\n\nAlthough far less common than other metalloproteins (e.g. those of zinc and iron), other cobaltoproteins are known besides B. These proteins include methionine aminopeptidase 2, an enzyme that occurs in humans and other mammals that does not use the corrin ring of B, but binds cobalt directly. Another non-corrin cobalt enzyme is nitrile hydratase, an enzyme in bacteria that metabolizes nitriles.\n\nCobalt is an essential element for life in minute amounts. The LD value for soluble cobalt salts has been estimated to be between 150 and 500 mg/kg. In the US, the Occupational Safety and Health Administration (OSHA) has designated a permissible exposure limit (PEL) in the workplace as a time-weighted average (TWA) of 0.1 mg/m. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 0.05 mg/m, time-weighted average. The IDLH (immediately dangerous to life and health) value is 20 mg/m.\n\nHowever, chronic cobalt ingestion has caused serious health problems at doses far less than the lethal dose. In 1966, the addition of cobalt compounds to stabilize beer foam in Canada led to a peculiar form of toxin-induced cardiomyopathy, which came to be known as \"beer drinker's cardiomyopathy\".\n\nIt causes respiratory problems when inhaled. It also causes skin problems when touched; after nickel and chromium, cobalt is a major cause of contact dermatitis. These risks are faced by cobalt miners.\n\nCobalt can be effectively absorbed by charred pigs' bones; however, this process is inhibited by copper and zinc, which have greater affinities to bone char.\n\n\n\n"}
{"id": "12207392", "url": "https://en.wikipedia.org/wiki?curid=12207392", "title": "Compact complement topology", "text": "Compact complement topology\n\nIn mathematics, the compact complement topology is a topology defined on the set formula_1 of real numbers, defined by declaring a subset formula_2 open if and only if it is either empty or its complement formula_3 is compact in the standard Euclidean topology on formula_1.\n"}
{"id": "18938271", "url": "https://en.wikipedia.org/wiki?curid=18938271", "title": "Daniel J. Piette", "text": "Daniel J. Piette\n\nDaniel J. Piette (born July 6, 1957) is an American businessperson.\n\nGraduated from the University of Wisconsin, Madison List of University of Wisconsin-Madison people in 1980. From 1980 to 1982 he worked for Exxon as a mining engineer. He then became Customer Support Manager in David P. Cook and Associates from 1982 to 1986 and Vice President, Geosciences for Terra-Mar Resource Information Services from 1986 to 1989. He was then Vice President and General Manager for Landmark Graphics Corporation in the USA, Venezuela, and Singapore from 1989 to 1996, and later became Chief Executive Officer of Bell Geospace. He was CEO of OpenSpirit Corporation in Houston, TX from 2003 to 2011 and orchestrated the sale of the company to TIBCO Software. He served as the President and CEO of Object Reservoir in Houston, Texas from 2011 to 2013, and CEO of TerraSpark from 2013 to 2014.\n\nPiette was a member of the board of directors at Petroleum Geo-Services from 2007 to 2018.\n\n"}
{"id": "14840572", "url": "https://en.wikipedia.org/wiki?curid=14840572", "title": "Digital protective relay", "text": "Digital protective relay\n\nIn utility and industrial electric power transmission and distribution systems, a digital protective relay is a computer-based system with software-based protection algorithms for the detection of electrical faults. Such relays are also termed as microprocessor type protective relays. They are functional replacements for electro-mechanical protective relays and may include many protection functions in one unit, as well as providing metering, communication, and self-test functions.\n\nThe digital protective relay is a protective relay that uses a microprocessor to analyze power system voltages, currents or other process quantities for the purpose of detection of faults in an electric power system or industrial process system. A digital protective relay may also be called a \"numeric protective relay\". It is also called numerical relay.\n\nLow voltage and low current signals (i.e., at the secondary of a voltage transformers and current transformers) are brought into a low pass filter that removes frequency content above about 1/3 of the sampling frequency (a relay A/D converter needs to sample faster than twice per cycle of the highest frequency that it is to monitor). The AC signal is then sampled by the relay's analog to digital converter from 4 to 64 (varies by relay) samples per power system cycle. As a minimum, magnitude of the incoming quantity, commonly using Fourier transform concepts (RMS and some form of averaging) would be used in a simple relay function. More advanced analysis can be used to determine phase angles, power, reactive power, impedance, waveform distortion, and other complex quantities.\n\nOnly the fundamental component is needed for most protection algorithms, unless a high speed algorithm is used that uses subcycle data to monitor for fast changing issues. The sampled data is then passed through a low pass filter that numerically removes the frequency content that is above the fundamental frequency of interest (i.e., nominal system frequency), and uses Fourier transform algorithms to extract the fundamental frequency magnitude and angle.\n\nThe relay analyzes the resultant A/D converter outputs to determine if action is required under its protection algorithm(s). Protection algorithms are a set of logic equations in part designed by the protection engineer, and in part designed by the relay manufacturer.\nThe relay is capable of applying advanced logic. It is capable of analyzing whether the relay should trip or restrain from tripping based on parameters set by the user, compared against many functions of its analogue inputs, relay contact inputs, timing and order of event sequences.\n\nIf a fault condition is detected, output contacts operate to trip the associated circuit breaker(s).\n\nThe logic is user-configurable and can vary from simply changing front panel switches or moving of circuit board jumpers to accessing the relay's internal parameter setting webpage via communications link on another computer hundreds of kilometres away.\n\nThe relay may have an extensive collection of settings, beyond what can be entered via front panel knobs and dials, and these settings are transferred to the relay via an interface with a PC (personal computer), and this same PC interface may be used to collect event reports from the relay.\n\nIn some relays, a short history of the entire sampled data is kept for oscillographic records. The event recording would include some means for the user to see the timing of key logic decisions, relay I/O (input/output) changes, and see, in an oscillographic fashion, at least the fundamental component of the incoming analogue parameters.\n\nDigital/numerical relays provide a front panel display, or display on a terminal through a communication interface. This is used to display relay settings and real-time current/voltage values, etc.\n\nMore complex digital relays will have metering and communication protocol ports, allowing the relay to become an element in a SCADA system. Communication ports may include RS232/RS485 or Ethernet (copper or fibre-optic). Communication languages may include Modbus, DNP3 or IEC61850 protocols.\n\nBy contrast, an electromechanical protective relay converts the voltages and currents to magnetic and electric forces and torques that press against spring tensions in the relay. The tension of the spring and taps on the electromagnetic coils in the relay are the main processes by which a user sets such a relay.\n\nIn a solid-state relay, the incoming voltage and current wave-forms are monitored by analog circuits, not recorded or digitized. The analog values are compared to settings made by the user via potentiometers in the relay, and in some case, taps on transformers.\n\nIn some solid-state relays, a simple microprocessor does some of the relay logic, but the logic is fixed and simple. For instance, in some time overcurrent solid state relays, the incoming AC current is first converted into a small signal AC value, then the AC is fed into a rectifier and filter that converts the AC to a DC value proportionate to the AC waveform. An op-amp and comparator is used to create a DC that rises when a trip point is reached. Then a relatively simple microprocessor does a slow speed A/D conversion of the DC signal, integrates the results to create the time-overcurrent curve response, and trips when the integration rises above a set-point. Though this relay has a microprocessor, it lacks the attributes of a digital/numeric relay, and hence the term \"microprocessor relay\" is not a clear term.\n\nThe digital/numeric relay was invented by Edmund O. Schweitzer, III in the early 1980s. SEL, AREVA, and ABB Group's were early forerunners making some of the early market advances in the arena, but the arena has become crowded today with many manufacturers. In transmission line and generator protection, by the mid-1990s the digital relay had nearly replaced the solid state and electro-mechanical relay in new construction. In distribution applications, the replacement by the digital relay proceeded a bit more slowly. While the great majority of feeder relays in new applications today are digital, the solid state relay still sees some use where simplicity of the application allows for simpler relays, which allows one to avoid the complexity of digital relays.\n\nProtective elements refer to the overall logic surrounding the electrical condition that is being monitored. For instance, a differential element refers to the logic required to monitor two (or more) currents, find their difference, and trip if the difference is beyond certain parameters. The term element and function are quite interchangeable in many instances.\n\nFor simplicity on one-line diagrams, the protection function is usually identified by an ANSI device number. In the era of electromechanical and solid state relays, any one relay could implement only one or two protective functions, so a complete protection system may have many relays on its panel. In a digital/numeric relay, many functions are implemented by the microprocessor programming. Any one numeric relay may implement one or all of these functions.\n\nA listing of device numbers is found at ANSI Device Numbers.\nA summary of some common device numbers seen in digital relays is:\n\n\n"}
{"id": "32310124", "url": "https://en.wikipedia.org/wiki?curid=32310124", "title": "Emulsified fuel", "text": "Emulsified fuel\n\nEmulsified Fuels are emulsions composed of water and a combustible liquid, either oil or a fuel. Emulsions are a particular example of a dispersion comprising a continuous and a dispersed phase. In the case of emulsions both phases are the immiscible liquids, oil and water. Emulsion fuels can be either a microemulsion or an ordinary emulsion (sometimes referred to as macroemulsion, to differentiate them from microemulsions). The essential differences between the two are stability (microemulsions are thermodynamically stable systems, whereas macroemulsions are kinetically stabilized) and particle size distribution (microemulsions are formed spontaneously and have dimensions of 10 to 200 nm, whereas macroemulsions are formed by a shearing process and have dimensions of 100 nm to over 1 micrometer). Microemulsions are isotropic whereas macroemulsions are prone to settling (or creaming) and changes in particle size over time. Both use surfactants (also called emulsifiers) and can be either water-in-oil (invert emulsions), or oil-in-water (regular emulsions) or bicontinuous (also called multiple or complex emulsions).\n\nWater continuous (oil-in-water) emulsified fuels are exemplified by the Orimulsion system and bitumen emulsions. These are often described as a high internal phase emulsions (hipe) because the continuous phase is around 30% of the composition of the fuel it is more usual for the dispersed phase to be the minor component. Water continuous emulsions of very heavy crudes, bitumen are easier to pump than the original fuel, which would require considerable heating and / or dilution with a distilled product (kerosene or light crude) in order to make them easy to handle. Water continuous emulsions of residual fuel, heavy fuel oils etc. which have a calorific value and are used in industrial applications can also be converted to emulsified fuels, thus reducing the need to use cutter fluids and improving the combustion emissions associated with the inferior fuels.\n\nOil continuous (water-in-oil) emulsified fuels are exemplified by diesel (or biodiesel blended fuels) and water emulsions,see: futrolaquapower.com (inactive website) These emulsified fuels were recognized in Europe (France and Italy) and CEN workshop standard was established (CWA 15145:2004). Other types of fuels have been emulsified contain between 5 and 30% water (by mass) in the overall fuel emulsion.\n\nMicroemulsions of fuels have also been prepared. The type of surfactants and quantities required to make these emulsion fuels sets them apart from other commercial emulsion fuels. These are considered where safety issues (e.g. fire prevention; https://www.youtube.com/watch?v=3F8edPltVc8 ) or commercial return justify the extra costs (e.g. enhance oil recovery, surfactant flooding; http://files.sc.akzonobel.com/bulletins/Enhanced%20Oil%20Recovery-Tech-bulletin.pdf ).\n\nThe main advantages to using emulsified fuels instead of the fuel itself are environmental and economic benefits. Addition of water to the diesel process decreases combustion temperatures and lowers NO emissions. An interesting paper (http://www.dieselnet.com/tech/engine_water.html#emu ) compares water injection and emulsified fuels into diesel engines (marine and stationary engines) and discusses the emissions and mechanisms involved. It concludes that emulsified fuels are singularly effective in simultaneously reducing NO and PM emissions. Another paper has examined the effects of EGR and Emulsion Fuels \n\n"}
{"id": "44093312", "url": "https://en.wikipedia.org/wiki?curid=44093312", "title": "Endeavour House", "text": "Endeavour House\n\nEndeavour House is the Suffolk County Council headquarters located in Ipswich, Suffolk, England. Endeavour House was originally owned by the American-based energy and electric company, TXU Corporation. Located on Russell Road Ipswich, England; Endeavour House was constructed between 2001 and 2003 by Bovis Lend Lease (now Lend Lease Project Management & Construction) contractors as well as M&S contractors and was designed by TTSP architects. The building had an original construction cost of £28 million (Suffolk County Council paid £16.75 million for the building) and is one of the most energy efficient office buildings in Europe.\n\nThe American company TXU had originally wanted to build a corporate office in England however in November 2002, 4 months before the building was complete TXU Corporation went into administration. TXU Corporation quickly put the uncompleted building up for sale. In 2003 Suffolk County Council were able to buy the 60% completed building for only £16 million. They made some alterations, the main change being the addition of a 90 desk chamber for 125 councilors on the second floor. The original plan for this was an indoor garden. Despite this the building has adapted well to its new role showing this to be an attractive building that is a good workplace. \n\nSuffolk County Council have energy bills of £422,000 a year, around 75% of this money goes straight back to the council. In late 2017 Babergh and Mid Suffolk District Councils will relocate to Endeavour House to lower costs on the councils. All 83 members of these 2 District Councils will be based in the headquarters for Suffolk County Council.\n\nEndeavour House was situated on the site of the now demolished Russell House. Russell House was once an Eastern Electricity Central Accounting Office but was demolished in 2001 to make way for the first stage of the Ipswich Village Development, Endeavour House.\n\n\nThe main tenant is Suffolk County Council with every department including their NHS (National Health Service (England)) division being located in Endeavour House. The former planning and architecture division, Concertus Design and Property Consultants are also based in the building.\n\nSuffolk CCGs will move into Endeavour house in October 2017.\n\nThe site is in the heart of the Ipswich Village Development and is situated opposite Grafton House (Ipswich Borough Council HQ) located on 8 Russell Road. Portman Road Stadium is situated behind the building. There is limited surface car parking for permit holders with access from Constantine Road. There is also a large multi-storey car park shared with Ipswich Borough Council with access from Constantine Road.\n\nThe five-storey building was designed as a corporate office building for TXU and consisted of some very energy efficient features. The building is an architectural achievement as it has 50,000 photovoltaic cells incorporated into the glass curtain walls, this being the largest installation of any office building in Europe. At peak sunlight these cells provide 57% of the buildings energy demand and also create a dark pattern of the glass. There is also rain collecting reservoirs underground which will recycle the water and pump it around the building to use in toilets.\n\nFeatures of Endeavour House:\n\nThe building has a design suited to a corporate business and is not a regular county council hall with its glass curtain walls, cantilever walkways and offices with windows that look into the concourse. This building is the complete opposite of their old headquarters at the East Suffolk County Hall which consisted of dark corridors circulating a hap hazard floor plan of a building that was built in 1837. So the building had to adapt to a workers who have been settled in a Victorian hall. The building has adapted well and has received good comments. Endeavour House is proof that an office buildings can be externally stylish and adaptable to change.\n\nThe building is split into two with a long concourse leading from the side to side with natural sunlight from a huge glass roof. The floor is pale white limestone with stairs hanging of the edge of each floor over the concourse. The main entrance hall is in the corner of the building where a large angled glass hall, of which allows tons of natural light to flood the building. In the other corner there is a smaller 2 storey cafe nestled into the open glass section, both corners are connected by the central concourse with a continuous glass roof allowing for a very large, modern and bright building.\n\nThe interior is clean and very open planned, each department have a large, sleek open planned office which stretches the length of the building. Managing directors do not have their own office but share work space with the rest of the department, there is however many private meeting rooms per office. Each large open planned office has a small kitchen at the entrance to the office with a sitting area viewing over the concourse. Every office also has a set of toilets adjacent to the entrance.As the building is practically split into 2 separate buildings, there can be at least 2 departments per floor. M&S contractors were called in by Bovis Lend Lease to design and build the timber structural supports to hold the stairs that they also built. The cantilever stairs had to hang onto the concourse.\n"}
{"id": "19220980", "url": "https://en.wikipedia.org/wiki?curid=19220980", "title": "Fermentative hydrogen production", "text": "Fermentative hydrogen production\n\nFermentative hydrogen production is the fermentative conversion of organic substrates to H. Hydrogen produced in this manner is often called biohydrogen. The conversion is effected by bacteria and protozoa, which employ enzymes. Fermentative hydrogen production is one of several anaerobic conversions. \n\nDark fermentation reactions do not require light energy. These are capable of constantly producing hydrogen from organic compounds throughout the day and night. Typically these reactions are coupled to the formation of carbon dioxide or formate. Important reactions that result in hydrogen production start with glucose, which is converted to acetic acid:\nA related reaction gives formate instead of carbon dioxide:\nThese reactions are exegonic by 216 and 209 kcal/mol, respectively.\n\nUsing synthetic biology, bacteria can be genetically altered to enhance this reaction.\n\nPhotofermentation differs from dark fermentation, because it only proceeds in the presence of light. Electrohydrogenesis is used in microbial fuel cells.\n\nFor example photo-fermentation with \"Rhodobacter sphaeroides\" SH2C can be employed to convert small molecular fatty acids into hydrogen.\n\n\"Enterobacter aerogenes\" is an outstanding hydrogen producer. It is an anaerobic facultative and mesophilic bacterium that is able to consume different sugars and in contrast to cultivation of strict anaerobes, no special operation is required to remove all oxygen from the fermenter. \"E. aerogenes\" has a short doubling time and high hydrogen productivity and evolution rate. Furthermore, hydrogen production by this bacterium is not inhibited at high hydrogen partial pressures; however, its yield is lower compared to strict anaerobes like \"Clostridia\". A theoretical maximum of 4 mol H/mol glucose can be produced by strict anaerobic bacteria. Facultative anaerobic bacteria such as \"E. aerogenes\" have a theoretical maximum yield of 2 mol H/mol glucose.\n\n\n"}
{"id": "38869", "url": "https://en.wikipedia.org/wiki?curid=38869", "title": "Flax", "text": "Flax\n\nFlax (\"Linum usitatissimum\"), also known as common flax or linseed, is a member of the genus \"Linum\" in the family Linaceae. It is a food and fiber crop cultivated in cooler regions of the world. The textiles made from flax are known in the Western countries as linen, and traditionally used for bed sheets, underclothes, and table linen. The oil is known as linseed oil. In addition to referring to the plant itself, the word \"flax\" may refer to the unspun fibers of the flax plant. The plant species is known only as a cultivated plant, and appears to have been domesticated just once from the wild species \"Linum bienne\", called pale flax.\n\nSeveral other species in the genus \"Linum\" are similar in appearance to \"L. usitatissimum\", cultivated flax, including some that have similar blue flowers, and others with white, yellow, or red flowers. Some of these are perennial plants, unlike \"L. usitatissimum\", which is an annual plant.\n\nCultivated flax plants grow to tall, with slender stems. The leaves are glaucous green, slender lanceolate, 20–40 mm long, and 3 mm broad.\n\nThe flowers are pure pale blue, 15–25 mm in diameter, with five petals. The fruit is a round, dry capsule 5–9 mm in diameter, containing several glossy brown seeds shaped like an apple pip, 4–7 mm long.\n\nThe earliest evidence of humans using wild flax as a textile comes from the present-day Republic of Georgia, where spun, dyed, and knotted wild flax fibers were found in Dzudzuana Cave and dated to the Upper Paleolithic, 30,000 years ago. Flax was first domesticated in the Fertile Crescent region. Evidence exists of a domesticated oilseed flax with increased seed size by 9,000 years ago from Tell Ramad in Syria. Use of the crop steadily spread, reaching as far as Switzerland and Germany by 5,000 years ago. In China and India, domesticated flax was cultivated also at least 5,000 years ago.\n\nFlax was cultivated extensively in ancient Egypt, where the temple walls had paintings of flowering flax, and mummies were entombed in linen. Egyptian priests wore only linen, as flax was considered a symbol of purity. Phoenicians traded Egyptian linen throughout the Mediterranean and the Romans used it for their sails. As the Roman Empire declined, so did flax production, but Charlemagne revived the crop in the eighth century CE with laws designed to publicize the hygiene of linen textiles and the health of linseed oil. Eventually, Flanders became the major center of the linen industry in the European Middle Ages. In North America, flax was introduced by the colonists and it flourished there, but by the early 20th century, cheap cotton and rising farm wages had caused production of flax to become concentrated in northern Russia, which came to provide 90% of the world's output. Since then, flax has lost its importance as a commercial crop, due to the easy availability of more durable fibres.\n\nFlax is grown for its seeds, which can be ground into a meal or turned into linseed oil, a product used as a nutritional supplement and as an ingredient in many wood-finishing products. Flax is also grown as an ornamental plant in gardens. Moreover, flax fibers are used to make linen. The Latin name of the species, \"usitatissimum,\" means \"most useful\".\n\nFlax fibers taken from the stem of the plant are two to three times as strong as cotton fibers. Additionally, flax fibers are naturally smooth and straight. Europe and North America both depended on flax for plant-based cloth until the 19th century, when cotton overtook flax as the most common plant for making rag-based paper. Flax is grown on the Canadian prairies for linseed oil, which is used as a drying oil in paints and varnishes and in products such as linoleum and printing inks.\n\nLinseed meal, the byproduct of producing linseed oil from flax seeds, is used to feed livestock. It is a protein-rich feed for ruminants, rabbits, and fish.\n\nFlaxseeds occur in two basic varieties/colors: brown and yellow (golden linseeds). Most types of these basic varieties have similar nutritional characteristics and equal numbers of short-chain omega-3 fatty acids. The exception is a type of yellow flax called solin (trade name \"Linola\"), which has a completely different oil profile and is very low in omega-3s.\n\nFlaxseeds produce a vegetable oil known as flaxseed oil or linseed oil, which is one of the oldest commercial oils. It is an edible oil obtained by expeller pressing and sometimes followed by solvent extraction. Solvent-processed flaxseed oil has been used for many centuries as a drying oil in painting and varnishing.\n\nAlthough brown flaxseed varieties may be consumed as readily as the yellow ones, and have been for thousands of years, its better-known uses are in paints, for fiber, and for cattle feed.\n\nA 100-gram portion of ground flaxseed supplies about , 41 g of fat, 28 g of fiber, and 20 g of protein.\n\nFlaxseed sprouts are edible and have a slightly spicy flavor profile. Excessive consumption of flaxseeds with inadequate amounts of water may cause bowel obstruction. In northern India, flaxseed, called \"tisi\" or \"alsi\", is traditionally roasted, powdered, and eaten with boiled rice, a little water, and a little salt. In India, linseed oil is known as \"alsi\" in Hindi and \"javas\" in Marathi. It is mainly used in Savji curries, such as mutton (goat meat) curries.\n\nWhole flaxseeds are chemically stable, but ground flaxseed meal, because of oxidation, may go rancid when left exposed to air at room temperature in as little as one week. Refrigeration and storage in sealed containers will keep ground flaxseed meal for a longer period before it turns rancid. Under conditions similar to those found in commercial bakeries, trained sensory panelists could not detect differences between bread made with freshly ground flaxseed and bread made with flaxseed that had been milled four months ago and stored at room temperature. This shows, if packed immediately without exposure to air and light, milled flaxseed is stable against excessive oxidation when stored for nine months at room temperature, and under warehouse conditions, for 20 months at ambient temperatures.\n\nThree natural phenolic glucosides—secoisolariciresinol diglucoside, p-coumaric acid glucoside, and ferulic acid glucoside—can be found in commercial breads containing flaxseed.\n\nFlax fiber is extracted from the bast beneath the surface of the stem of the flax plant. Flax fiber is soft, lustrous, and flexible; bundles of fiber have the appearance of blonde hair, hence the description \"flaxen\" hair. It is stronger than cotton fiber, but less elastic. The best grades are used for linen fabrics such as damasks, lace, and sheeting. Coarser grades are used for the manufacturing of twine and rope, and historically, for canvas and webbing equipment. Flax fiber is a raw material used in the high-quality paper industry for the use of printed banknotes, laboratory paper(blotting and filter), rolling paper for cigarettes, and tea bags.\n\nThe use of flax fibers dates back tens of thousands of years; linen, a refined textile made from flax fibers, was worn widely by Sumerian priests more than 4,000 years ago. Industrial-scale flax fiber processing existed in antiquity. A Bronze Age factory dedicated to flax processing was discovered in Euonymeia.\n\nFlax mills for spinning flaxen yarn were invented by John Kendrew and Thomas Porthouse of Darlington, England, in 1787. New methods of processing flax have led to renewed interest in the use of flax as an industrial fiber.\n\nIn a 100-gram serving, flaxseed contains high levels (> 19% of the Daily Value, DV) of protein, dietary fiber, several B vitamins, and dietary minerals. Ten grams of flaxseed contains one gram of water-soluble fiber (which lowers blood cholesterol) and three grams of insoluble fiber (which helps prevent constipation). Flax contains hundreds of times more lignans than other plant foods. Flaxseeds are especially rich in thiamine, magnesium, potassium, and phosphorus (DVs above 90%).\n\nAs a percentage of total fat, flaxseeds contain 54% omega-3 fatty acids (mostly ALA), 18% omega-9 fatty acids (oleic acid), and 6% omega-6 fatty acids (linoleic acid); the seeds contain 9% saturated fat, including 5% as palmitic acid. Flaxseed oil contains 53% 18:3 omega-3 fatty acids (mostly ALA) and 13% 18:2 omega-6 fatty acids.\n\nOne study of research published between 1990 and 2008 showed that consuming flaxseed or its derivatives may reduce total and LDL-cholesterol in the blood, with greater benefits in women and those with high cholesterol.\n\nA meta-analysis has shown that consumption of more than 30 g of flaxseed daily for more than 12 weeks reduced body weight, body mass index (BMI), and waist circumference for persons with a BMI greater than 27. Another meta-analysis has shown that consumption of flaxseed for more than 12 weeks produced small reductions in systolic blood pressure and diastolic blood pressure. Flaxseed supplementation showed a small reduction in c-reactive protein (a marker of inflammation) only in persons with a BMI greater than 30.\n\nFlaxseed and its oil have repeatedly been demonstrated to be nontoxic and are generally recognized as safe for human consumption. Like many common foods, flax contains small amounts of cyanogenic glycoside; these are nontoxic when consumed in typical amounts, but may be toxic when consumed in large quantities of such staple foods such as cassava. Typical concentrations (for example, 0.48% in a sample of defatted dehusked flaxseed meal) can be removed by special processing.\n\nThe soils most suitable for flax, besides the alluvial kind, are deep loams containing a large proportion of organic matter. Flax is often found growing just above the waterline in cranberry bogs. Heavy clays are unsuitable, as are soils of a gravelly or dry sandy nature. Farming flax requires few fertilizers or pesticides. Within eight weeks of sowing, the plant can reach in height and grows several centimeters per day under its optimal growth conditions, reaching within 50 days.\n\nIn 2014, world production of flax (linseed) was 2.65 million tonnes, led by Canada with 33% of the global total. Other major producers were Kazakhstan, China, and Russia (table).\n\nFlax is harvested for fiber production after about 100 days, or a month after the plants flower and two weeks after the seed capsules form. The bases of the plants begin to turn yellow. If the plants are still green, the seed will not be useful, and the fiber will be underdeveloped. The fiber degrades once the plants turn brown.\n\nFlax grown for seed is allowed to mature until the seed capsules are yellow and just starting to split; it is then harvested in various ways. A combine harvester may either cut only the heads of the plants, or the whole plant. These are then dried to extract the seed. The amount of weeds in the straw affects its marketability, and this, coupled with market prices, determines whether the farmer chooses to harvest the flax straw. If the flax straw is not harvested, typically, it is burned, since the stalks are quite tough and decompose slowly (\"i.e.\", not in a single season). Formed into windrows from the harvesting process, the straw often clogs up tillage and planting equipment. Flax straw that is not of sufficient quality for fiber uses can be baled to build shelters for farm animals, or sold as biofuel, or removed from the field in the spring.\n\nTwo ways are used to harvest flax fiber, one involving mechanized equipment (combines), and a second method, more manual and targeting maximum fiber length.\n\nFlax for fiber production is usually harvested by a specialized flax harvester. Usually built on the same machine base as a combine, but instead of the cutting head it has a flax puller. The flax plant turned over and is gripped by rubber belts roughly 20–25 cm (8-10\") above ground, to avoid getting grasses and weeds in the flax. The rubber belts then pulls the whole plant out of the ground with the roots so the whole length of the plant fiber can be used. The plants then pass over the machine and is placed on the field crosswise to the harvesters direction of travel. The plants are left in the field for field retting.\n\nThe mature plant can also be cut with mowing equipment, similar to hay harvesting, and raked into windrows. When dried sufficiently, a combine then harvests the seeds similar to wheat or oat harvesting .\n\nThe plant is pulled up with the roots (not cut), so as to increase the fiber length. After this, the flax is allowed to dry, the seeds are removed, and it is then retted. Dependent upon climatic conditions, characteristics of the sown flax and fields, the flax remains on the ground between two weeks and two months for retting. As a result of alternating rain and the sun, an enzymatic action degrades the pectins which bind fibers to the straw. The farmers turn over the straw during retting to evenly rett the stalks. When the straw is retted and sufficiently dry, it is rolled up. It is then stored by farmers before extracting the fibers.\nThreshing is the process of removing the seeds from the rest of the plant. Separating the usable flax fibers from other components requires pulling the stems through a hackle and/or beating the plants to break them.\n\nFlax processing is divided into two parts: the first part is generally done by the farmer, to bring the flax fiber into a fit state for general or common purposes. This can be performed by three machines: one for threshing out the seed, one for breaking and separating the straw (stem) from the fiber, and one for further separating the broken straw and matter from the fiber.\n\nThe second part of the process brings the flax into a state for the very finest purposes, such as lace, cambric, damask, and very fine linen. This second part is performed by a refining machine.\n\nBefore the flax fibers can be spun into linen, they must be separated from the rest of the stalk. The first step in this process is retting, which is the process of rotting away the inner stalk, leaving the outer parts intact. At this point, straw, or coarse outer stem (cortex and epidermis), is still remaining. To remove this, the flax is \"broken\", the straw is broken up into small, short bits, while the actual fiber is left unharmed. Scutching scrapes the outer straw from the fiber. The stems are then pulled through \"hackles\", which act like combs to remove the straw and some shorter fibers out of the long fiber.\n\nSeveral methods are used for retting flax. It can be retted in a pond, stream, field, or tank. When the retting is complete, the bundles of flax feel soft and slimy, and quite a few fibers are standing out from the stalks. When wrapped around a finger, the inner woody part springs away from the fibers. Pond retting is the fastest. It consists of placing the flax in a pool of water which will not evaporate. It generally takes place in a shallow pool which will warm up dramatically in the sun; the process may take from a few days to a few weeks. Pond-retted flax is traditionally considered of lower quality, possibly because the product can become dirty, and is easily over-retted, damaging the fiber. This form of retting also produces quite an odor. Stream retting is similar to pool retting, but the flax is submerged in bundles in a stream or river. This generally takes two or three weeks longer than pond retting, but the end product is less likely to be dirty, does not smell as bad, and because the water is cooler, is less likely to be over-retted. Both pond and stream retting were traditionally used less because they pollute the waters used for the process.\n\nIn field retting, the flax is laid out in a large field, and dew is allowed to collect on it. This process normally takes a month or more, but is generally considered to provide the highest quality flax fibers, and it produces the least pollution.\n\nRetting can also be done in a plastic trash can or any type of water-tight container of wood, concrete, earthenware, or plastic. Metal containers will not work, as an acid is produced when retting, and it would corrode the metal. If the water temperature is kept at , the retting process under these conditions takes 4 or 5 days. If the water is any colder, it takes longer. Scum collects at the top, and an odor is given off the same as in pond retting. 'Enzymatic' retting of flax has been researched as a technique to engineer fibers with specific properties.\n\nDressing the flax is the process of removing the straw from the fibers. Dressing consists of three steps: breaking, scutching, and heckling. The breaking breaks up the straw. Some of the straw is scraped from the fibers in the scutching process, and finally, the fiber is pulled through heckles to remove the last bits of straw.\n\nIn September 2009, Canadian flax exports reportedly had been contaminated by a deregistered genetically modified cultivar called 'Triffid' that had food and feed safety approval in Canada and the U.S., however, Canadian growers and the Flax Council of Canada raised concerns about the marketability of this cultivar in Europe where a zero tolerance policy exists regarding unapproved genetically modified organisms. Subsequently, deregistered in 2010 and never grown commercially in Canada or the U.S., 'Triffid' stores were destroyed, but future exports and further tests at the University of Saskatchewan proved that 'Triffid' persisted among flax crops, possibly affecting future crops. Canadian flaxseed cultivars were reconstituted with 'Triffid'-free seed used to plant the 2014 crop. Laboratories are certified to test for the presence of 'Triffid' at a level of one seed in 10,000.\n\nFlax is the emblem of Northern Ireland and displayed by the Northern Ireland Assembly. In a coronet, it appeared on the reverse of the British one-pound coin to represent Northern Ireland on coins minted in 1986, 1991, and 2014. Flax also represents Northern Ireland on the badge of the Supreme Court of the United Kingdom and on various logos associated with it.\n\nCommon flax is the national flower of Belarus.\n\nIn early versions of the Sleeping Beauty tale, such as \"Sun, Moon, and Talia\" by Giambattista Basile, the princess pricks her finger, not on a spindle, but on a sliver of flax, which later is sucked out by her children conceived as she sleeps.\n"}
{"id": "3428890", "url": "https://en.wikipedia.org/wiki?curid=3428890", "title": "Fuel saving device", "text": "Fuel saving device\n\nFuel saving devices are sold on the aftermarket with claims to improve the fuel economy and/or the exhaust emissions of any purport to optimize ignition, air flow, or fuel flow in some way. An early example of such a device sold with difficult-to-justify claims is the 200 mpg carburetor designed by Canadian inventor Charles Nelson Pogue.\n\nThe US EPA is required by Section 511 of the Motor Vehicle Information and Cost Savings Act to test many of these devices and to provide public reports on their efficacy; the agency finds most devices do not improve fuel economy to any measurable degree, unlike forced induction, water injection (engine), intercooling and other fuel economy devices which have been long proven. Tests by Popular Mechanics magazine also found unproven types of devices yield no measurable improvements in fuel consumption or power, and in some cases actually decrease both power and fuel economy.\n\nOther organizations generally considered reputable, such as the American Automobile Association and Consumer Reports have performed studies with the same result.\n\nOne reason that ineffective fuel saving gadgets are popular is the difficulty of accurately measuring small changes in the fuel economy of a vehicle. This is because of the high level of variance in the fuel consumption of a vehicle under normal driving conditions. Due to selective perception and confirmation bias, the buyer of a device can perceive an improvement where none actually exists. Also, observer-expectancy effect can result in a user subconsciously altering driving habits. These biases can be either positive or negative to the device tested, depending on the biases of the individual. For these reasons, regulatory bodies have developed standardized drive cycles for consistent, accurate testing of vehicle fuel consumption. Where fuel economy does improve after the fitment of a device, it is usually due to the tune-up procedure that is conducted as part of the installation. In older systems with distributor ignitions, device manufacturers would specify timing advance beyond that recommended by the manufacturer, which by itself could boost fuel economy while potentially increasing emissions of some combustion products, at the risk of possible engine damage.\n\nModifying the accessory drive system can increase fuel economy and performance to some extent. Underdrive pulleys modify the amount of engine power that can be drawn by accessory devices. Such alterations to the drive systems for alternators or air conditioning compressors (rather than the power steering pump, for example) can be detrimental to vehicle usability (e.g., by not keeping the battery fully charged), but will not impair safety.\n\nCompounds sold for addition to the vehicle's fuel may include tin, magnesium and platinum. The claimed purpose of these is generally to\nimprove the energy density of the fuel. Additives for addition to the engine oil, sometimes marketed as \"engine treatments\", contain teflon, zinc, or chlorine compounds.\n\nMagnets attached to a vehicle's fuel line have been claimed to improve fuel economy by aligning fuel molecules, but because motor fuels are non-polar, no such alignment or other magnetic effect on the fuel is possible. When tested, typical magnet devices had no effect on vehicle performance or economy.\n\nSome devices claim to improve efficiency by changing the way that liquid fuel is converted to vapor. These include fuel heaters and devices to increase or decrease turbulence in the intake manifold. These do not work on standard vehicles because the principle is already applied to the design of the engine. This method is however integral to making vegetable oil conversions, and similar heavy oil engines, run at all. \n\nDevices have been marketed which bleed a small amount of air into the fuel line before the carburetor. These may improve fuel economy because the engine runs slightly lean as a consequence. However, running leaner than the manufacturer intended can cause overheating, piston damage, loss of maximum power and poor emissions (e.g., higher NOx due to higher combustion temperatures, or, if misfiring occurs, higher HC).\n\nSome electronic devices are marketed as fuel savers. The Fuel Doctor FD-47, for example, plugs into the vehicle's cigarette lighter and displays several LEDs. It is claimed to increase vehicle fuel economy by up to 25% through \"power conditioning of the vehicle's electrical systems\", but \"Consumer Reports\" detected no difference in economy or power in tests on ten separate vehicles, finding that the device did nothing but light up. \"Car and Driver\" magazine found that the device contains nothing but \"a simple circuit board for the LED lights\", and disassembly and circuit analysis reached the same conclusion. The maker disputed claims that the device has no effect, and proposed changes to the \"Consumer Reports\" testing procedure, which when implemented made no difference to the results.\n\nAnother device described as 'electronic' is the 'Electronic Engine Ionizer Fuel Saver'. Testing of this device resulted in a loss of power and an engine compartment fire.\n\nThere are also genuinely useful 'emissions-control defeat devices' that operate by allowing a vehicle's engine to operate outside government-imposed tailpipe emissions parameters. These government standards force factory engines to operate outside their most efficient range of operation. Either engine control units are reprogrammed to operate more efficiently, or sensors that influence the ECU's operation are modified or 'simulated' to cause it to operate in a more efficient manner. Oxygen sensor simulators allow fuel-economy reducing catalytic converters to be removed. Such devices are often sold for \"off-road use only\".\n\nThe reason why most devices are not capable of producing the claimed improvements is based in thermodynamics. This formula expresses the theoretical efficiency of a petrol engine:\n\nwhere η is efficiency, r is the compression ratio, and γ is the ratio of the specific heats of the cylinder gases.\n\nAssuming an ideal engine with no friction, perfect insulation, perfect combustion, a compression ratio of 10:1, and a 'γ' of 1.27 (for gasoline-air combustion), the theoretical efficiency of the engine would be 46%.\n\nFor example, if an automobile typically gets 20 miles<32.19 km> per gallon with a 20% efficient engine that has a 10:1 compression ratio, a carburetor claiming 100MPG would have to increase the efficiency by a factor of 5, to 100%. This is clearly beyond what is theoretically or practically possible. A similar claim of 300MPG for any vehicle would require an engine (in this particular case) that is 300% efficient, which violates the First Law of Thermodynamics.\n\nExtremely efficient vehicle designs capable of achieving 100MPG+ (such as the VW 1l) do not have substantially greater engine efficiency, but instead focus on better aerodynamics, reduced vehicle weight, and using energy that would otherwise be dissipated as heat during braking.\n\nThere is a debunked urban legend about an inventor who creates a 100 mpg (2.35 L/100 km) or even 200 mpg carburetor, but after demonstrating it for the major vehicle manufacturers, the inventor mysteriously disappears. In some versions of the story, he is claimed to have been killed by the government. This fiction is thought to have started after Canadian Charles Nelson Pogue filed in 1930 for such a device, followed by others.\n\nThe popular U.S. television show \"MythBusters\" investigated several fuel-saving devices using gasoline- and diesel-powered fuel-injected cars under controlled circumstances. Fuel line magnets, which supposedly align the fuel molecules so they burn better, were tested and found to make no difference in fuel consumption. The debunked notion that adding acetone to gasoline improves efficiency by making the gasoline burn more completely without damaging the plastic parts of the fuel system was tested, and although there was no apparent damage the fuel system, the vehicle's fuel economy was actually worsened.\n\nThe show tested the hypothesis that a car with a carburetor type gasoline engine can run on hydrogen gas alone, which was confirmed as viable, although the high cost of hydrogen gas as well as storage difficulties currently prohibit widespread adoption. They also tested a device that supposedly produces sufficient hydrogen to power a car by electrolysis (running an electric current through water to split its molecules into hydrogen and oxygen). Although some hydrogen was produced, the amount was minuscule compared to the quantity necessary to run a car for even a few seconds.\n\nThe show also tested a carburetor that, according to its manufacturer, could improve fuel efficiency to 300 miles<482.80 km> per gallon. However, the device actually made the car less fuel efficient. They also determined that a diesel-powered car can run on used cooking oil though they did not check whether it damaged the engine.\n\nThe show noted that out of 104 fuel efficiency devices tested by the EPA, only seven showed any improvement in efficiency, and even then, the improvement was never more than six percent. The show also noted that if any of the devices they tested actually worked to the extent they were supposed to, the episode would have been one of the most legendary hours of television.\n\n\n"}
{"id": "5125429", "url": "https://en.wikipedia.org/wiki?curid=5125429", "title": "Gharo Wind Power Plant", "text": "Gharo Wind Power Plant\n\nGharo Wind Power Plant is an underconstruction wind power generation project being built in the persistently wind-swept corridor of Gharo, Sindh, Pakistan. This will be one of the first wind power projects in Pakistan. Halcrow Pakistan and the Spanish company, based in Barcelona, Taller d'Enginyeria Ambiental SL have compiled the Enivoronmental Impact Assessment for this project.\n\nThe Overseas Private Investment Corporation (OPIC) approved $288 million for financing two wind power projects in Sindh in order to overcome power shortage and promote clean energy in Pakistan. OPIC would help Pakistan diversify its energy production to include important contributions from renewable energy sources.\nThe OPIC credit facility would help build 50 MW Sapphire wind power plant in Gharo-Ketti Bandar wind corridor, designed to generate 133 giga watt hours of emission-free electricity annually. \n\nCurrently, 45 wind power projects of around 3200 MW capacity are under process in Pakistan. \n\n\n"}
{"id": "39586666", "url": "https://en.wikipedia.org/wiki?curid=39586666", "title": "Graphite-like ZnO nanostructures", "text": "Graphite-like ZnO nanostructures\n\nMost of the synthesized Zinc oxide (ZnO) nanostructures in different geometric configurations such as nanowires, nanorods, nanobelts and nanosheets are usually in the wurtzite crystal structure. However, it was found from density functional theory calculations that for ultra-thin films of ZnO, the graphite-like structure was energetically more favourable as compared to the wurtzite structure. The stability of this phase transformation of wurtzite lattice to graphite-like structure of the ZnO film is only limited to the thickness of about several Zn-O layers and was subsequently verified by experiment. Similar phase transition was also observed in ZnO nanowire when it was subjected to uniaxial tensile loading. However, with the use of the first-principles all electron full-potential method, it was observed that the wurtzite to graphite-like phase transformation for ultra-thin ZnO films will not occur in the presence of a significant amount of oxygen vacancies (V) at the Zn-terminated (0001) surface of the thin film. The absence of the structural phase transformation was explained in terms of the Coulomb attraction at the surfaces. The graphitic ZnO thin films are structurally similar to the multilayer of graphite and are expected to have interesting mechanical and electronic properties for potential nanoscale applications. In addition, density functional theory calculations and experimental observations also indicate that the concentration of the V is the highest near the surfaces as compared to the inner parts of the nanostructures. This is due to the lower V defect formation energies in the interior of the nanostructures as compared to their surfaces.\n"}
{"id": "13566934", "url": "https://en.wikipedia.org/wiki?curid=13566934", "title": "Harold La Borde", "text": "Harold La Borde\n\nHarold La Borde was a Trinidadian sailor and adventurer who from 1969 to 1973 circumnavigated the world in his 40-ft ketch \"Hummingbird II\". He was accompanied by his wife, Kwailan, and his five-year-old son Pierre. As the first known Trinidadian sailors to cross the Atlantic and later to circumnavigate, Harold and Kwailan were awarded the nation's highest honour – the Gold Trinity Cross.\n\nHarold was born in Trinidad, West Indies, on June 18th 1933 (died June 2016), of parents with a rich mixture of blood in their veins – French, African, Spanish and Amerindian (Carib). He was educated at a local Roman Catholic school and began his sailing career by building dinghies, in which he taught himself the rudiments of seamanship, and reading any book about deep-sea sailing that he could lay his hands on. Harold La Borde was determined to get a suitable boat, opting to build one himself.\n\nIn his first book, \"An Ocean to Ourselves\" (1962), La Borde tells how he built a 26-foot ketch \"Humming Bird\". Harold and Kwailan, who were married in 1959, made their maiden voyage in the 26-foot vessel, \"Humming Bird\", to England in 1960, together with a friend, Buck Wong Chong. The \"Humming Bird\" was subsequently sold, and says Harold \"is somewhere in Europe.\"\n\nThe La Bordes, always working as a team, took jobs at an Outward Bound school in Nigeria in 1961, after the voyage, but the call of the sea was too strong for the young couple and they returned to Trinidad in 1963, when they started to build the 40-foot ketch \"Humming Bird II\". Their first-born son, Pierre, arrived while work was in progress.\n\nThe boat was completed in three years and, after chartering her out to Americans for a further three years in order to raise sufficient funds, the family set out on 2 February 1969 on the, now historic, voyage that took them around the world. Harold and Kwailan were both awarded their nation's highest award, the Trinity Cross for their seafaring adventure. Their second son, Andre, was born in Auckland, New Zealand, during the voyage.\n\nUpon their return home, the 40-foot \"Humming Bird II\" was purchased by the Trinidad and Tobago Government in 1973, and can be seen in the museum near the lighthouse on South Quay; and according to Harold \"is rotting away there. It is a sad thing, especially when you talk about taking care of historical things.\"\n\nThe La Bordes went on to another circumnavigation voyage via Cape Horn (1984–86) in the \"Humming Bird III\".\n\nHarold La Borde T.C. also wrote a further two books with input from his family, wife Kwailan and sons Pierre and André, \"All Oceans Blue\" (1977), and \"Lonely Oceans South\" (1987).\n\nDocumentary films of their travels were made in conjunction with the Government Film Unit, which were also very professionally put together. After retiring from their respective jobs in Trinidad, the La Bordes ran a small family marina in Trinidad's busiest yachting bay. Harold's full-time job was working on the Humming Bird III, every day while Kwailan finished their autobiography which includes all of their sailing voyages to the present, entitled \"Wind, Sea, and Faith\".\n\nHarold La Borde died on June 12, 2016, leaving behind his wife, sons Pierre and Andre, three grandchildren (Shannon, Arama, and Sanchia), and his brothers Rudy and Hugh.\n\n"}
{"id": "136688", "url": "https://en.wikipedia.org/wiki?curid=136688", "title": "Joseph Swan", "text": "Joseph Swan\n\nSir Joseph Wilson Swan FRS (31 October 1828 – 27 May 1914) was an English physicist, chemist, and inventor. He is known as an independent early developer of a successful incandescent light bulb, and is the person responsible for developing and supplying the first incandescent lights used to illuminate homes and public buildings, including the Savoy Theatre, London, in 1881.\n\nIn 1904, Swan was knighted by King Edward VII, awarded the Royal Society's Hughes Medal, and was made an honorary member of the Pharmaceutical Society. He had received the highest decoration in France, the Legion of Honour, when he visited the 1881 International Exposition of Electricity, Paris. The exhibition included displays of his inventions, and the city was lit with his electric lighting.\n\nSwan was the maternal grandfather of Christopher Morcom, Alan Turing's close friend and first love during their studies at the Sherborne boarding school.\n\nJoseph Wilson Swan was born in 1828 at Pallion Hall in Pallion, in the Parish of Bishopwearmouth, Sunderland, County Durham. His parents were John Swan and Isabella Cameron. Swan was apprenticed for six years to a Sunderland firm of druggists, \"Hudson and Osbaldiston\". However, it is not known if Swan completed his six-year apprenticeship, as both partners subsequently died. He was said to have had an enquiring mind, even as a child. He augmented his education with a fascination of his surroundings, the industry of the area, and reading at Sunderland Library. He attended lectures at the Sunderland Atheneum. Swan subsequently joined Mawson's, a firm of manufacturing chemists in Newcastle upon Tyne, started in the year of Swan's birth by John Mawson (9 September 1819 – 17 December 1867), the husband of his sister, Elizabeth Swan (22 November 1822 – 2 August 1905). In 1846, Swan was offered a partnership at Mawson's. This company subsequently existed as \"Mawson, Swan and Morgan\" until 1973, formerly located on Grey Street in Newcastle upon Tyne, near Grey's Monument. The premises, now occupied by burger chain restaurant Byron, can be identified by a line of Victorian-style electric street lamps in front of the store on Grey Street.\n\nSwan lived at Underhill, Low Fell, Gateshead, a large house on Kells Lane North, where he conducted most of his experiments in the large conservatory. The house was later converted into a private fee paying, grant aided co-educational grammar school named Beaconsfield School. Here, students could still find examples of Swan's original electrical fittings.\n\nIn 1850, Swan began working on a light bulb using carbonised paper filaments in an evacuated glass bulb. By 1860, he was able to demonstrate a working device, and obtained a British patent covering a partial vacuum, carbon filament incandescent lamp. However, the lack of a good vacuum and an adequate electric source resulted in an inefficient light bulb with a short lifetime.\n\nIn 1875, Swan returned to consider the problem of the light bulb with the aid of a better vacuum and a carbonised thread as a filament. The most significant feature of Swan's improved lamp was that there was little residual oxygen in the vacuum tube to ignite the filament, thus allowing the filament to glow almost white-hot without catching fire. However, his filament had low resistance, thus needing heavy copper wires to supply it.\n\nSwan first publicly demonstrated his incandescent carbon lamp at a lecture for the Newcastle upon Tyne Chemical Society on 18 December 1878. However, after burning with a bright light for some minutes in his laboratory, the lamp broke down due to excessive current. On 17 January 1879, this lecture was successfully repeated with the lamp shown in actual operation; Swan had solved the problem of incandescent electric lighting by means of a vacuum lamp. On 3 February 1879, he publicly demonstrated a working lamp to an audience of over seven hundred people in the lecture theatre of the Literary and Philosophical Society of Newcastle upon Tyne, Sir William Armstrong of Cragside presiding. Swan turned his attention to producing a better carbon filament, and the means of attaching its ends. He devised a method of treating cotton to produce \"parchmentised thread\", and obtained British Patent 4933 on 27 November 1880. From that time he began installing light bulbs in homes and landmarks in England.\nHis house, Underhill, Low Fell, Gateshead, was the world's first to have working light bulbs installed. The Lit & Phil Library in Westgate Road, Newcastle, was the first public room lit by electric light during a lecture by Swan on 20 October 1880. In 1881, he founded his own company, The Swan Electric Light Company, and started commercial production.\n\nThe Savoy, a state-of-the-art theatre in the City of Westminster, London, was the first public building in the world lit entirely by electricity. Swan supplied about 1,200 incandescent lamps, powered by an generator on open land near the theatre. The builder of the Savoy, Richard D'Oyly Carte, explained why he had introduced Swan's electric light: \"The greatest drawbacks to the enjoyment of the theatrical performances are, undoubtedly, the foul air and heat which pervade all theatres. As everyone knows, each gas-burner consumes as much oxygen as many people, and causes great heat beside. The incandescent lamps consume no oxygen, and cause no perceptible heat.\" The first generator proved too small to power the whole building, and though the entire front-of-house was electrically lit, the stage was lit by gas until 28 December 1881. At that performance, Carte stepped onstage and broke a glowing lightbulb before the audience to demonstrate the safety of Swan's new technology. On 29 December 1881, \"The Times\" described the electric lighting as superior, visually, to gaslight.\n\nThe first private residence, other than the inventor's, lit by the new incandescent lamp was that of his friend, Sir William Armstrong at Cragside, near Rothbury, Northumberland. Swan personally supervised the installation there in December 1880. Swan had formed \"The Swan Electric Light Company Ltd\" with a factory at Benwell, Newcastle, and had established the first commercial manufacture of incandescent lightbulbs by the beginning of 1881.\n\nSwan's carbon rod lamp and carbon filament lamp, while functional, were still relatively impractical due to low resistance (needing very expensive thick copper wiring) and short running life. While searching for a better filament for his light bulb, Swan inadvertently made another advance. In 1881, he developed and patented a process for squeezing nitrocellulose through holes to form conducting fibres. His newly established company (which by merger eventually became the Edison and Swan United Company) used Swan's cellulose filaments in their bulbs. The textile industry has also used this process.\nThe first ship to use Swan's invention was \"The City of Richmond\", owned by the Inman Line. She was fitted with incandescent lamps in June 1881. The Royal Navy also introduced them to their ships soon after; with HMS \"Inflexible\" having the new lamps installed in the same year. An early employment in engineering was during the digging of the Severn Tunnel, where contractor Thomas Walker installed \"20-candlepower lamps\" in the temporary pilot tunnels.\n\nSwan was one of the early developers of the electric safety lamp for miners, exhibiting his first in Newcastle upon Tyne at the North of England Institute of Mining and Mechanical Engineers on 14 May 1881. This required a wired supply, so the following year, he presented one with a battery and other improved versions followed. By 1886, a lamp with better light output than a flame safety lamp was in production by the Edison-Swan Company. However, it suffered from problems of reliability and was not a success. It took development by others over the next 20 years or so before effective electric lamps were in common use.\n\nIn what are considered to be independent lines of inquiry, Swan's incandescent electric lamp was developed at the same time Thomas Edison was working on his incandescent lamp with Swan's first successful lamp and Edison's lamp both patented in 1879. Edison's goal in developing his lamp was for it to be used as one part of a much larger system: a long-life high-resistance lamp that could be connected in parallel to work economically with the large-scale electric-lighting utility he was creating. Swan's original lamp design, with its low resistance (the lamp could only be used in series) and short life span, was not suited for such an application.\n\nSwan's strong patents in Great Britain led, in 1883, to the two competing companies merging to exploit both Swan's and Edison's inventions, with the establishment of the Edison & Swan United Electric Light Company. Known commonly as \"Ediswan\", the company sold lamps made with a cellulose filament that Swan had invented in 1881, while the Edison Company continued using bamboo filaments outside of Britain. In 1892, General Electric (GE) began exploiting Swan's patents to produce cellulose filaments, until they were replaced in 1904 by a GE developed \"General Electric Metallized\" (GEM) baked cellulose filaments.\n\nIn 1886, Ediswan moved production to a former jute mill at Ponders End, North London. In 1916, Ediswan set up the UK's first radio thermionic valve factory at Ponders End. This area, with nearby Brimsdown subsequently developed as a centre for the manufacture of thermionic valves, cathode ray tubes, etc., and nearby parts of Enfield became an important centre of the electronics industry for much of the 20th century. Ediswan became part of British Thomson-Houston and Associated Electrical Industries (AEI) in the late 1920s.\n\nWhen working with wet photographic plates, Swan noticed that heat increased the sensitivity of the silver bromide emulsion. By 1871, he had devised a method of using dry plates, and substituting nitrocellulose plastic for glass plates, thus initiating the age of convenience in photography. Eight years later, he patented bromide paper, developments of which are still used for black-and-white photographic prints.\n\nIn 1864, Swan patented the transfer process for making carbon prints, a permanent photographic process. By adding the transfer step, Swan was able to easily make photographs with a full tonal range.\n\nIn 1894, Swan was elected a Fellow of the Royal Society (FRS), and in September 1901, he was awarded the honorary degree of Doctor of Science (D.Sc.) from Durham University. In 1904, he was knighted, awarded the Royal Society's Hughes Medal, and made an honorary member of the Pharmaceutical Society. Swan died in 1914 at Warlingham in Surrey.\n\nIn 1945, the London Power Company commemorated Swan by naming a new 1,554 GRT coastal collier SS \"Sir Joseph Swan\".\n\n\n"}
{"id": "1175891", "url": "https://en.wikipedia.org/wiki?curid=1175891", "title": "Knot density", "text": "Knot density\n\nKnot density is a traditional measure for quality of handmade or knotted pile carpets. It refers to the number of knots, or knot count, per unit of surface area - typically either per square inch (kpsi) or per square centimeter (kpsc), but also per decimeter or meter (kpsd or kpsm). Number of knots per unit area is directly proportional to the quality of carpet. Density may vary from 25 to over 1000 kpsi, or 4 to over 155 kpsc, where ≤80 kpsi is poor quality, 120 to 330 kpsi medium to good, and ≥330 kpsi is very good quality. The inverse, knot ratio, is also used to compare characteristics. Knot density = warp×weft while knot ratio = warp/weft. For comparison: 100,000/square meter = 1,000/square decimeter = 65/square inch = 179/\"gereh\".\n\nFor two carpets of the same age, origin, condition and design, the one with the higher number of knots will be the more valuable. Knot density is normally measured in knots per square inch (KPSI) which is simply the number of vertical knots across one inch of carpet multiplied by the number of horizontal knots in the same area. Average knot density varies between region and design. A rug could have a knot density half that of another yet still be more valuable, KPSI is only one measurement of quality and value in Persian carpets.\n\nKnot density is related to and affects or affected by the thickness of the length of the pile and the width of the warp and woof, and also the designs and motifs used and their characteristics and appearance. \"In rugs with a high knot density, curvilinear, elaborate motifs are possible. In those with a low knot density (as well as kilims), simpler, rectilinear, motifs tend to prevail.\" \"A carpet design with a high knot density is better adapted to intricate and curvilinear designs, which of necessity must have a shorter pile length to avoid looking blurry. A carpet with a lesser knot density is better adapted to bold, geometric designs and can utilize a long pile for softer, more reflective surface that appeals to the sense of touch.\"\n\nHand-tying of knots is a very labour-intensive task. An average weaver can tie almost 10,000 knots per day. More difficult patterns with an above-average knot density can only be woven by a skillful weaver, thus increasing the production costs even more. An average weaver may tie 360 knots per hour (1/10 seconds), while 1200 knots approaches the maximum a skilful weaver can tie per hour.\n\nIn the late fifteenth century a \"carpet design revolution\" occurred, made possible by finer yarns, and before this time it is rare to find carpets with ≥120 kpsi but by the next century carpets with three to four times that density were fairly common. For example, the Pazyryk carpet (ca. 400 BCE) is around 234 kpsi and the Ardabil Carpets (ca. 1550 CE) are 300–350 kpsi. A fragment of a silk Mughal carpet in the Metropolitan Museum of Art has a knot density of 2,516 kpsi and a silk Hereke prayer rug (ca. 1970 CE) contains 4,360 symmetric kpsi. However, the rug with the highest knot density is a silk Hereke masterpiece by the Özipeks workshops, having an incredible density of approximately 10,000 kpsi, with a production time of about 15 years. (FN5a - https://new.liveauctioneers.com/item/53198792_probably-the-finest-rug-in-the-world-silk-hereke)\n\nIn Persian, \"reg\" (\"raj\", \"rag\", Persian: \"row, course\") refers to the knots per gereh (Persian: \"knot\"), which refers to a unit of approximately 2.75 inches. \"Dihari\" is a unit of 6,000 knots used to measure production in India.\n\n"}
{"id": "55421339", "url": "https://en.wikipedia.org/wiki?curid=55421339", "title": "Kyambura Hydroelectric Power Station", "text": "Kyambura Hydroelectric Power Station\n\nKyambura Hydroelectric Power Station, commonly referred to as Kyambura Power Station, is a mini hydropower station under construction in Uganda. \n\nThe power station is located in the Kyambura area, across the Kyambura River, in Rubirizi District, approximately , north of the town of Rubirizi, where the district headquarters are located. This is approximately , by road, northwest of Mbarara, the largest city in the Western Region of Uganda.\n\nThe power station is a run-of-river installation with generation capacity of and annual production of 36.7 GWh. The original design had the main intake via a \"headrace tunnel\". In the new design, that has been replaced by a \"headrace canal\". This has reduced project costs and construction time. The budgeted cost of construction is US$24 million.\n\nConstruction began in early 2017 and commissioning is expected in the fourth quarter of 2018.\n\n\n"}
{"id": "5251555", "url": "https://en.wikipedia.org/wiki?curid=5251555", "title": "Liquid Air", "text": "Liquid Air\n\nLiquid Air was the brand name of an unusual automobile planned by Liquid Air Power and Automobile Co. of London in 1899.\n\nThe first factory location was acquired in Boston, Massachusetts in 1899, and its owners claimed that they could construct a car that would run a hundred miles on liquid air. By 1901 the company had gone into receivership. In 1902 the product was demonstrated by its designer, Hans Knudsen, at a show in London; apparently the automobile that was shown was a modified Locomobile steamer. Here it was claimed to run at on of liquid air, sold at a shilling a gallon.\n\n"}
{"id": "55625361", "url": "https://en.wikipedia.org/wiki?curid=55625361", "title": "Lottery competition", "text": "Lottery competition\n\nLottery competition in ecology is a model for how organisms compete. It was first used to describe competition in coral reef fish. Under lottery competition, many offspring compete for a large number of sites (e.g., many fry competing for a few territories, or many seedlings competing for a few treefall gaps). Under lottery competition, one individual is chosen randomly to \"win\" that site (typically becoming an adult soon after), and the \"losers\" typically die off. Thus, in an analogy to a lottery or raffle, every individual has an equal chance of winning (like every ticket has an equal chance of being chosen), and therefore more abundant species are proportionately more likely to win (just as an individual who buys more tickets is more likely to win). Some models generalize this idea by weighting some individuals who are more likely to be chosen (by analogy, this would be like some tickets counting as two tickets instead of one).\n\nLottery competition has been used to in understanding many key ideas in ecology, including the storage effect (species coexist because they are affected differently by environmental variation) and neutral theory (species diversity is maintained because species are competitively equivalent, and extinction rates are slow enough to be offset by speciation and dispersal events).\n"}
{"id": "4976589", "url": "https://en.wikipedia.org/wiki?curid=4976589", "title": "Lumber yard", "text": "Lumber yard\n\nA lumber yard is a location where lumber and wood-related products used in construction and/or home improvement projects are processed or stored. Some lumber yards offer retail sales to consumers, and some of these may also provide services such as the use of planers, saws and other large machines.\n\nGenerally, timber yards are locations where raw logs and other wood or forest products are processed and stored. The terms \"lumber yard\" and \"timber yard\" are sometimes used interchangeably, and timber yards may include additional aspects that lumber yards encompass, and vice versa.\n\nLumber yards sell products made at lumber mills, where customers pick up products at the yard themselves or request that an order be built and delivered to them by the lumber yard. Lumber yards may also sell wood-plastic composites, such as Trex, any other type of construction material or supplies, and general hardware store items. Lumber yards are the primary resources for contractors and homeowners when construction material is needed. Home Depot stores also have lumber sections, but this may not be considered a \"lumber yard\", although it serves the same purpose. Lumber yards use forklift trucks to move the large heavy units of lumber around the yard. Saws may also be available to cut boards to a desired length for customers.\n\nAt timber yards (also sometimes referred to as lumber yards), logs and other forest products are processed and stored. Additional activities include inspection for signs of rot, utilizing stacking techniques to prevent rot from forming, and the removal of bark and spraying of pesticides to prevent insect infestation. Some lumber/timber yards use cranes to move logs, and sprinklers to keep the logs moist.\n\nRansfords, located in Shropshire, England, operates one of the largest timber yards in Europe, and processes 70,000 tonnes of logs annually.\n\nSome lumber yards sell products that are assessed and certified by the Forest Stewardship Council's (FSC), per its standards. One lumber yard in Portland, Oregon limits its product line to only those that meet or exceed FSC's requirements. In 2008, Sustainable Northwest, a nonprofit environmental group in Portland, Oregon, began operations sourcing and purveying environmentally friendly \"green wood\" products. The group specializes in offering wood products that are sourced from trees in non-threatened forests.\n\nIn the United States, the National Fire Protection Association has been documenting incidences of lumber yard fires since 1998. Products at lumber yards are very prone to fires, and electrical problems at lumber yards are a significant causation of fires. In Murray, Utah, after a large lumber yard fire caused approximately US$1 million in damage at the CECO Concrete Construction company, city residents called for companies in the city to take further preventative actions to prevent fires from occurring. In some cities, such as Phoenix, Arizona, city fire protection and prevention codes for lumber yards exist.\n\nIn Sydney, Australia, convicts have worked at a place called \"The Lumber Yard\" as a part of serving their sentences, to facilitate lumber and product production and to provide convicts with skills training. A timber yard that was associated with The Lumber Yard was also in operation, where workers pulled and stacked logs floated downstream on the Lane Cove River. The Lumber Yard was a government lumber yard, and operated as such from circa 1791 to 1834.\n\n\n\n\n"}
{"id": "20478", "url": "https://en.wikipedia.org/wiki?curid=20478", "title": "Magnetopause", "text": "Magnetopause\n\nThe magnetopause is the abrupt boundary between a magnetosphere and the surrounding plasma. For planetary science, the magnetopause is the boundary between the planet’s magnetic field and the solar wind. The location of the magnetopause is determined by the balance between the pressure of the dynamic planetary magnetic field and the dynamic pressure of the solar wind. As the solar wind pressure increases and decreases, the magnetopause moves inward and outward in response. Waves (ripples and flapping motion) along the magnetopause move in the direction of the solar wind flow in response to small-scale variations in the solar wind pressure and to Kelvin–Helmholtz instability.\n\nThe solar wind is supersonic and passes through a bow shock where the direction of flow is changed so that most of the solar wind plasma is deflected to either side of the magnetopause, much like water is deflected before the bow of a ship. The zone of shocked solar wind plasma is the magnetosheath. At Earth and all the other planets with intrinsic magnetic fields, some solar wind plasma succeeds in entering and becoming trapped within the magnetosphere. At Earth, the solar wind plasma which enters the magnetosphere forms the plasma sheet. The amount of solar wind plasma and energy that enters the magnetosphere is regulated by the orientation of the interplanetary magnetic field, which is embedded in the solar wind.\n\nThe Sun and other stars with magnetic fields and stellar winds have a solar magnetopause or heliopause where the stellar environment is bounded by the interstellar environment.\n\nPrior to the age of space exploration, interplanetary space was considered to be a vacuum. The coincidence of the Carrington super flare and the super geomagnetic event of 1859 was evidence that plasma was ejected from the Sun during a flare event. Chapman and Ferraro proposed that a plasma was emitted by the Sun in a burst as part of a flare event which disturbed the planet's magnetic field in a manner known as a geomagnetic storm. The collision frequency of particles in the plasma in the interplanetary medium is very low and the electrical conductivity is so high that it could be approximated to an infinite conductor. A magnetic field in a vacuum cannot penetrate a volume with infinite conductivity. Chapman and Bartels (1940) illustrated this concept by postulating a plate with infinite conductivity placed on the dayside of a planet’s dipole as shown in the schematic. The field lines on the dayside are bent. At low latitudes, the magnetic field lines are pushed inward. At high latitudes, the magnetic field lines are pushed backwards and over the polar regions. The boundary between the region dominated by the planet’s magnetic field (i.e., the magnetosphere) and the plasma in the interplanetary medium is the magnetopause. The configuration equivalent to a flat, infinitely conductive plate is achieved by placing an image dipole (green arrow at left of schematic) at twice the distance from the planet’s dipole to the magnetopause along the planet-Sun line. Since the solar wind is continuously flowing outward, the magnetopause above, below and to the sides of the planet are swept backward into the geomagnetic tail as shown in the artist’s concept. The region (shown in pink in the schematic) which separates field lines from the planet which are pushed inward from those which are pushed backward over the poles is an area of weak magnetic field or day-side cusp. Solar wind particles can enter the planet’s magnetosphere through the cusp region. Because the solar wind exists at all times and not just times of solar flares, the magnetopause is a permanent feature of the space near any planet with a magnetic field.\n\nThe magnetic field lines of the planet’s magnetic field are not stationary. They are continuously joining or merging with magnetic field lines of the interplanetary magnetic field. The joined field lines are swept back over the poles into the planetary magnetic tail. In the tail, the field lines from the planet’s magnetic field are re-joined and start moving toward night-side of the planet. The physics of this process was first explained by Dungey (1961).\n\nIf one assumed that magnetopause was just a boundary between a magnetic field in a vacuum and a plasma with a weak magnetic field embedded in it, then the magnetopause would be defined by electrons and ions penetrating one gyroradius into the magnetic field domain. Since the gyro-motion of electrons and ions is in opposite directions, an electric current flows along the boundary. The actual magnetopause is much more complex.\n\nIf the pressure from particles within the magnetosphere is neglected, it is possible to estimate the distance to the part of the magnetosphere that faces the Sun. The condition governing this position is that the dynamic ram pressure from the solar wind is equal to the magnetic pressure from the Earth's magnetic field.\n\"B\"(\"r\") is the Magnetic field strength of the planet in SI units (\"B\" in T, μ in H/m)\n\nSince the dipole magnetic field strength varies with distance as formula_4 the magnetic field strength can be written as formula_5, where formula_6 is the planet's magnetic moment, expressed in formula_7.\nSolving this equation for r leads to an estimate of the distance\n\nThe distance from Earth to the subsolar magnetopause varies over time due to solar activity, but typical distances range from 6 - 15 Rformula_10. Empirical models using real-time solar wind data can provide a real-time estimate of the magnetopause location. A bow shock stands upstream from the magnetopause. It serves to decelerate and deflect the solar wind flow before it reaches the magnetopause. \n\nResearch on the magnetopause is conducted using the LMN coordinate system (which is set of axes like XYZ). N points normal to the magnetopause outward to the magnetosheath, L lies along the projection of the dipole axis onto the magnetopause (positive northward), and M completes the triad by pointing dawnward.\n\nVenus and Mars do not have a planetary magnetic field and do not have a magnetopause. The solar wind interacts with the planet’s atmosphere and a void is created behind the planet. In the case of the Earth’s moon and other bodies without a magnetic field or atmosphere, the body’s surface interacts with the solar wind and a void is created behind the body.\n\n"}
{"id": "53517667", "url": "https://en.wikipedia.org/wiki?curid=53517667", "title": "Maine Wildlife Management Areas", "text": "Maine Wildlife Management Areas\n\nMaine Wildlife Management Areas (WMAs) are state owned lands managed by the Maine Department of Inland Fisheries and Wildlife. The WMAs comprise approximately 100,000 acres and contain a diverse array of habitats, from wetland flowages critical to waterfowl production to the spruce-fir forests of northern Maine on which Canada Lynx, moose and wintering deer are dependent. Spread geographically throughout all counties of the State the properties are available for a multitude of recreational opportunities, with a focus on hunting, fishing and trapping. The focus on offering these types of recreational opportunities is in line with the funding used to acquire such properties, historically accomplished with funding from Federal Aid to Wildlife Restoration and State bonding approved by voters.\n\n"}
{"id": "39795259", "url": "https://en.wikipedia.org/wiki?curid=39795259", "title": "Mainstream Energy Corporation", "text": "Mainstream Energy Corporation\n\nMainstream Energy Corporation is a solar energy company founded in 2005. It is the parent company of REC Solar, AEE Solar, and SnapNRack. \n"}
{"id": "17094238", "url": "https://en.wikipedia.org/wiki?curid=17094238", "title": "Marina Rikhvanova", "text": "Marina Rikhvanova\n\nMarina Rikhvanova is a Russian ecologist and leader of the Baikal Ecological Wave (BEW) organization which protects Siberia’s Lake Baikal from ecological damage. Lake Baikal, the world’s biggest reservoir of fresh water, is currently under threat from industrial pollution. In 2008, Rikhvanova was awarded the Goldman Environmental Prize.\n\nRikhvanova's concern for the Lake Baikal dates to her years in college, when she wrote a paper on environmental threats posed by a pulp and a paper mill in Baikalsk that had been dumping contaminants into the lake since the 1960s. The mill dumped thousands of tons of pollutants into the lake, including dioxin, which has appeared in Baikal fish and the fat of Baikal seals. When Rikhvanova co-founded a non-governmental organization called Baikal Ecological Wave in 1990, she targeted the mill. Rikhvanova organized demonstrations, petitions and meetings, all aimed at bringing an end to the mill's pollution of Baikal. Last year, Russian authorities ordered the Baikalsk plant to switch to a closed water treatment system that eliminated any wastewater discharge into the lake.\n\nIn 2006, Putin decided to reroute an oil pipeline that would have been built within a half-mile of Baikal, near a fault line. This followed protest rallies by thousands of Russians in the streets of Irkutsk.\n\nRikhvanova has paid a price for her advocacy. Russian police have raided her offices and seized her organization's computers. In 2008, a group of Russian youths attacked a tent camp Rikhvanova had organized to protest a proposed uranium enrichment center in Angarsk, about 50 miles west of Baikal.\n\n"}
{"id": "1387032", "url": "https://en.wikipedia.org/wiki?curid=1387032", "title": "Molding (decorative)", "text": "Molding (decorative)\n\nMoulding (also spelled molding in the United States though usually not within the industry), also known as coving , is a strip of material with various profiles used to cover transitions between surfaces or for decoration. It is traditionally made from solid milled wood or plaster, but may be of plastic or reformed wood. In classical architecture and sculpture, the molding is often carved in marble or other stones.\n\nA \"plain\" moulding has right-angled upper and lower edges. A \"sprung\" moulding has upper and lower edges that bevel towards its rear, allowing mounting between two non-parallel planes (such as a wall and a ceiling), with an open space behind.\n\nDecorative moldings have been made of wood, stone and cement. Recently moldings have been made of extruded PVC and Expanded Polystyrene (EPS) as a core with a cement-based protective coating. Synthetic moldings have environmental, health and safety concerns that were investigated by Doroudiani \"et al\".\n\nCommon moldings include:\n\nAt their simplest, moldings hide and help weather seal natural joints produced in the framing process of building a structure. As decorative elements they are a means of applying light- and dark-shaded stripes to a structural object without having to change the material or apply pigments. Depending on their function they may be primarily a means of hiding or weather-sealing a joint, purely decorative, or some combination of the three. \n\nAs decorative elements the contrast of dark and light areas gives definition to the object. Imagine the vertical surface of a wall lit by sunlight at an angle of about 45 degrees above the wall. Adding a small overhanging horizontal molding to the surface of the wall will introduce a dark horizontal shadow below the molding, called a fillet molding. Adding a vertical fillet to a horizontal surface will create a light vertical shadow. Graded shadows are possible by using moldings in different shapes: the concave \"cavetto\" molding produces a horizontal shadow that is darker at the top and lighter at the bottom; an \"ovolo\" (convex) molding makes a shadow that is lighter at the top and darker at the bottom. Other varieties of concave molding are the \"scotia\" and \"congé\" and other convex moldings the \"echinus\", the \"torus\" and the astragal.\n\nPlacing an ovolo directly above a cavetto forms a smooth \"s\"-shaped curve with vertical ends that is called an \"ogee\" or \"cyma reversa\" molding. Its shadow appears as a band light at the top and bottom but dark in the interior. Similarly, a cavetto above an ovolo forms an \"s\" with horizontal ends, called a \"cyma\" or \"cyma recta\". Its shadow shows two dark bands with a light interior.\n\nTogether the basic elements and their variants form a decorative vocabulary that can be assembled and rearranged in endless combinations. This vocabulary is at the core of both classical architecture and Gothic architecture.\n\n\n"}
{"id": "11238236", "url": "https://en.wikipedia.org/wiki?curid=11238236", "title": "National Hurricane Research Project", "text": "National Hurricane Research Project\n\nThe National Hurricane Research Project (NHRP) was initiated in 1955 by the \nUnited States Weather Bureau in response to the devastating 1954 hurricane season, which \nsaw Hurricane Carol, Hurricane Edna, and Hurricane Hazel bring destruction and \nflooding to New England and the Mid-Atlantic states. \nRobert Simpson, a Weather Bureau meteorologist who had \nparticipated in Air Force hurricane reconnaissance flights as an observer, was appointed as \nthe first director of NHRP and organized the Research Operations Base at Morrison Air Force \nBase (now Palm Beach International Airport) in West Palm Beach, FL in 1956.\n\nDuring the first three years of the Project, scientists used three specially \ninstrumented Air Force Hurricane Hunters aircraft with crews on bailment from the 55th \nWeather Reconnaissance Squadron. They flew missions into Hurricane Greta (1956), \nHurricane Audrey (1957), \nHurricane Daisy (1958), \nand Hurricane Helene (1958) in this initial period, collecting data which delineated the \nstructure and energy budget of hurricanes for the first time.\n\nIn 1959, the Project was moved to Miami and collocated with the Miami hurricane forecast \noffice. Simpson left the Project to complete his doctoral degree and Robert Cecil Gentry \nwas appointed NHRP director. The Department of Commerce leased two DC-6 aircraft and \nreceived a B-57 jet from the Air Force so that NHRP could continue to carry out airborne \nexperiments on hurricanes. The combination of research project, forecast center, and \naircraft facility was dubbed the \"National Hurricane Center\" (NHC). This has caused \nlater confusion, since 1967 the NHC name has been used exclusively by the forecast center. \nHowever the Project and Research Flight Facility (RFF) remained separate entities, with \ntheir own personnel, budgets, and objectives.\n\nDuring the 1960s, while NHRP continued to carry out research flights into \nHurricane Donna (1960), Hurricane Cleo (1964), and Hurricane Betsy (1965), the \nProject also began to create computer models of hurricane circulation, formulated a \nstatistical track program (NHC-64), wrote a manual on hurricane forecasting, and evaluate \nthe accuracy of track forecasts. Starting with Hurricane Esther (1961), NHRP was \nheavily involved with Project STORMFURY, the U. S. Government's experiment in hurricane \nmodification. They seeded Hurricane Beulah in 1963, but had to wait six more years before a \nsuitable candidate storm entered their operational area.\n\nLate in 1964, the Project was renamed the National Hurricane Research Laboratory in \nrecognition of it becoming a permanent institution within the Weather Bureau. This presaged \nthe creation of the Environmental Research Laboratories the next year.\n"}
{"id": "1862857", "url": "https://en.wikipedia.org/wiki?curid=1862857", "title": "Northumberland Wildlife Trust", "text": "Northumberland Wildlife Trust\n\nNorthumberland Wildlife Trust was established in 1971 (following a split from the \"Northumberland & Durham Trust\", established 1962) to help conserve and protect the wildlife of Northumberland, Newcastle upon Tyne and North Tyneside in the UK. The Trust is a charity, and a member of The Wildlife Trusts partnership.\n\nThe Trust was founded by Tony Tynan, and to honour that fact he has been given the title of Founder.\n\nParticular projects where the Trust has a special expertise include Red Squirrel conservation and peatland restoration.\n\nThe Trust has an active team engaged in education activities including projects based in North Northumberland, Blyth Valley district and Newcastle and North Tyneside. It runs activities in which volunteers can participate every Tuesday, Wednesday, Thursday, Friday and alternate Sundays. Volunteers help to maintain the Trust's nature reserves.\n\nThe Trust is a charity and relies heavily on its members to help fund its activities.\n\nThe Trust manages 60 nature reserves including Hauxley and East Chevington on the Northumberland Coast to Whitelee Moor, a 15 square kilometre hill farm on the Scottish Border. The most recent acquisition was the beach at Cresswell. In addition to its nature reserves, the Trust manages Weetslade Country Park on behalf of the Land Restoration Trust and Bakethin Reservoir conservation area on behalf of Northumbrian Water.\n\nNorthumberland Wildlife Trust headquarters are located on one their reserves, St. Nicholas Park, Gosforth. most of the Trust's staff are based here, as are part of the Save Our Squirrels Northumberland.\n\n\nThe full list of reserves can be seen at the Trust's Website.\n\n\n"}
{"id": "47479696", "url": "https://en.wikipedia.org/wiki?curid=47479696", "title": "Oil and Gas Authority", "text": "Oil and Gas Authority\n\nThe Oil and Gas Authority’s role is to regulate, influence and promote the UK oil and gas industry in order to achieve their statutory principal objective of maximising the economic recovery of the UK’s oil and gas resources. Established in April 2015 as an executive agency of the Department for Business, Energy and Industrial Strategy, on 1 October 2016 the OGA was incorporated as a Government Company, with the Secretary of State for Business, Energy and Industrial Strategy the sole shareholder. The OGA's headquarters are in Aberdeen with another office in London, which is also its registered company address.\n\nIn June 2013, the UK government asked Sir Ian Wood of Wood Group to conduct a review into maximising the recovery of oil and gas from the UK Continental Shelf. One of the recommendations of the Wood Review was the creation of an independent economic regulator for the sector. Subsequently the OGA was launched on 1 April 2015 as an executive agency of DECC. The Energy Act 2016 , which received Royal Assent in May 2016, created the legislative framework to formally establish the OGA as a government company, limited by shares under the Companies Act 2006,with the Secretary of State for Business, Energy and Industrial Strategy the sole shareholder. The Energy Act 2016 also provided the OGA with new regulatory powers, including the ability to participate in meetings with operators, to have access to data, provide dispute resolution and introduce a range of sanctions such as enforcement notices and fines of up to £1 million.\n\n"}
{"id": "2150384", "url": "https://en.wikipedia.org/wiki?curid=2150384", "title": "Oxyliquit", "text": "Oxyliquit\n\nAn Oxyliquit, also called liquid air explosive or liquid oxygen explosive, is an explosive material which is a mixture of liquid oxygen (LOX) with a suitable fuel, such as carbon (as lampblack), or an organic chemical (e.g. a mixture of soot and naphthalene), wood meal, or aluminium powder or sponge. It is a class of Sprengel explosives.\n\nOxyliquits have numerous advantages. They are inexpensive to make, can be initiated by a safety fuse, and in case of a misfire, the oxygen evaporates quickly, rendering the charge quite safe in a short period of time. The first large scale deployment took place in 1899 during the building of the Simplon Tunnel, in the form of cartridges filled with diatomaceous earth soaked with petroleum, or an absorbent cork charcoal, dipped in liquid oxygen immediately before use. In another modification, the cartridge is filled with liquid oxygen after placement in the borehole. \n\nOne of the disadvantages of oxyliquits is that, once mixed, they are sensitive to sparks, shock, and heat, in addition to reported cases of spontaneous ignition. The power relative to weight is high, but the density is low, so the brisance is low as well. Ignition by a fuse alone is sometimes unreliable. The charge should be detonated within 5 minutes of soaking, but even after 15 minutes it may be capable of exploding, even though weaker and with production of carbon dioxide. \n\nAn oxyliquit explosive can be made accidentally by spilling liquid oxygen on tarmac during filling high-altitude airplane systems. The pavement then can become sufficiently explosive to be initiated by walking on it, although the oxygen evaporates shortly after spilling.\n\nAt first, liquid air, self-enriched by standing (nitrogen has a lower boiling point and evaporates preferentially) was used, but pure liquid oxygen gives better results.\n\nA mixture of lampblack and liquid oxygen was measured to have detonation velocity of 3,000 m/s, and 4 to 12% more strength than dynamite. However, the flame it makes has too long duration to be safe in possible presence of explosive gases, so oxyliquits found their use mostly in open quarries and strip mining. \nThe explosive properties of these mixtures were discovered in Germany in 1895 by Prof. Carl von Linde, a developer of a successful machine for liquefaction of gases, who named them oxyliquits.\n\nIn 1930, over 3 million pounds of liquid oxygen were used for this purpose in Germany alone, and additional 201,466 lb (91,383 kg) were consumed by British quarries. The accident rate was lower than with conventional explosives. However, the Dewar flasks the LOX was stored in occasionally exploded, which was caused by iron impurities in the activated carbon serving as trace gas absorbent in the insulation vacuum layer in the flask, which caused spontaneous ignition in case of LOX leak into the enclosed space.\n\nUse of oxyliquits during World War II was low, as there was a plentiful supply of nitrates obtained from synthetic ammonia.\n\nDue to the complicated machinery required for manufacture of liquid oxygen, oxyliquit explosives were used mostly only where their consumption was high. In the United States, some such locations were the strip mines in coal mining areas of the Midwest. Its consumption peaked in 1953 with 10,190 tons, but then decreased to zero in 1968, when it was entirely replaced with the cheaper ANFO.\n\nOxyliquit explosive was prepared \"ad hoc\" from sugar and liquid oxygen from an oxygen bottle to blast a hole in a collapsed cave in Stanisław Lem's 1951 novel \"The Astronauts\". The same device was used in Andy Weir's novel \"The Martian\" to cause the intentional depressurization of a spaceship by blasting the airlock door.\n\n"}
{"id": "38475327", "url": "https://en.wikipedia.org/wiki?curid=38475327", "title": "Pantasote", "text": "Pantasote\n\nPantasote is an imitation leather material made by the Pantasote Company, beginning in 1891. It was a durable, relatively inexpensive material used as upholstery and for tents and awnings. \n\n"}
{"id": "34779206", "url": "https://en.wikipedia.org/wiki?curid=34779206", "title": "Photovoltaic power station", "text": "Photovoltaic power station\n\nA photovoltaic power station, also known as a solar park, is a large-scale photovoltaic system (PV system) designed for the supply of merchant power into the electricity grid. They are differentiated from most building-mounted and other decentralised solar power applications because they supply power at the utility level, rather than to a local user or users. They are sometimes also referred to as solar farms or solar ranches, especially when sited in agricultural areas. The generic expression utility-scale solar is sometimes used to describe this type of project.\n\nThe solar power source is via photovoltaic modules that convert light directly to electricity. However, this differs from, and should not be confused with concentrated solar power, the other large-scale solar generation technology, which uses heat to drive a variety of conventional generator systems. Both approaches have their own advantages and disadvantages, but to date, for a variety of reasons, photovoltaic technology has seen much wider use in the field. , PV systems outnumber concentrators by about 40 to 1.\n\nIn some countries, the nameplate capacity of a photovoltaic power stations is rated in megawatt-peak (MW), which refers to the theoretical maximum solar array's DC power output. In other countries, the manufacturer gives the surface and the efficiency. However, Canada, Japan, Spain and some parts of the United States often specify using the converted lower nominal power output in MW; a measure directly comparable to other forms of power generation. A third and less common rating is the mega volt-amperes (MVA). Most solar parks are developed at a scale of at least 1 MW. As at the start of 2017, the world's largest operating photovoltaic power station has a capacity of over 800 megawatts and projects up to 1 gigawatt are planned. As at the end of 2016, about 4,300 projects with a combined capacity of 96 GW were solar farms larger than 4 MW.\n\nMost of the existing large-scale photovoltaic power stations are owned and operated by independent power producers, but the involvement of community- and utility-owned projects is increasing. To date, almost all have been supported at least in part by regulatory incentives such as feed-in tariffs or tax credits, but as levelized costs have fallen significantly in the last decade and grid parity has been reached in an increasing number of markets, it may not be long before external incentives cease to exist.\n\nThe first 1 MW solar park was built by Arco Solar at Lugo near Hesperia, California at the end of 1982, followed in 1984 by a 5.2 MW installation in Carrizo Plain. Both have since been decommissioned, though Carrizo Plain is the site for several large plants now being constructed or planned. The next stage followed the 2004 revisions to the feed-in tariffs in Germany when a substantial volume of solar parks were constructed.\nSeveral hundred installations over 1 MW have been since been installed in Germany, of which more than 50 are over 10 MW. With its introduction of feed-in tariffs in 2008, Spain became briefly the largest market, with some 60 solar parks over 10 MW, but these incentives have since been withdrawn. The USA, China India, France, Canada, and Italy, amongst others, have also become major markets as shown on the list of photovoltaic power stations.\n\nThe largest sites under construction have capacities of hundreds of MW and projects at a scale of 1 GW are being planned.\n\nThe land area required for a desired power output, varies depending on the location, and on the efficiency of the solar modules, the slope of the site and the type of mounting used. Fixed tilt solar arrays using typical modules of about 15% efficiency on horizontal sites, need about 1 hectare/MW in the tropics and this figure rises to over 2 hectares in northern Europe.\n\nBecause of the longer shadow the array casts when tilted at a steeper angle, this area is typically about 10% higher for an adjustable tilt array or a single axis tracker, and 20% higher for a 2-axis tracker, though these figures will vary depending on the latitude and topography.\n\nThe best locations for solar parks in terms of land use are held to be brown field sites, or where there is no other valuable land use. Even in cultivated areas, a significant proportion of the site of a solar farm can also be devoted to other productive uses, such as crop growing or biodiversity.\n\nAgrivoltaics is co-developing the same area of land for both solar photovoltaic power as well as for conventional agriculture. A recent study found that the value of solar generated electricity coupled to shade-tolerant crop production created an over 30% increase in economic value from farms deploying agrivoltaic systems instead of conventional agriculture.\n\nIn some cases several different solar power stations, with separate owners and contractors, are developed on adjacent sites. This can offer the advantage of the projects sharing the cost and risks of project infrastructure such as grid connections and planning approval. Solar farms can also be co-located with wind farms. Sometimes the title 'solar park' is used, rather than an individual solar power station.\n\nSome examples of such solar clusters are the Charanka Solar Park, where there are 17 different generation projects; Neuhardenberg, with eleven plants, and the Golmud solar parks with total reported capacity over 500MW. An extreme example is calling all of the solar farms in the Gujarat state of India a single solar park, the Gujarat Solar Park.\n\nMost Solar parks are ground mounted PV systems, also known as free-field solar power plants. They can either be fixed tilt or use a single axis or dual axis solar tracker. While tracking improves the overall performance, it also increases the system's installation and maintenance cost. A solar inverter converts the array's power output from DC to AC, and connection to the utility grid is made through a high voltage, three phase step up transformer of typically 10 kV and above.\n\nThe solar arrays are the subsystems which convert incoming light into electrical energy. They comprise a multitude of solar modules, mounted on support structures and interconnected to deliver a power output to electronic power conditioning subsystems.\n\nA minority of utility-scale solar parks are configured on buildings and so use building-mounted solar arrays. The majority are 'free field' systems using ground-mounted structures, usually of one of the following types:\n\nMany projects use mounting structures where the solar modules are mounted at a fixed inclination calculated to provide the optimum annual output profile. The modules are normally oriented towards the Equator, at a tilt angle slightly less than the latitude of the site. In some cases, depending on local climatic, topographical or electricity pricing regimes, different tilt angles can be used, or the arrays might be offset from the normal East-West axis to favour morning or evening output.\n\nA variant on this design is the use of arrays, whose tilt angle can be adjusted twice or four times annually to optimise seasonal output. They also require more land area to reduce internal shading at the steeper winter tilt angle. Because the increased output is typically only a few percent, it seldom justifies the increased cost and complexity of this design.\n\nTo maximise the intensity of incoming direct radiation, solar panels should be orientated normal to the sun's rays. To achieve this, arrays can be designed using two-axis trackers, capable of tracking the sun in its daily orbit across the sky, and as its elevation changes throughout the year.\n\nThese arrays need to be spaced out to reduce inter-shading as the sun moves and the array orientations change, so need more land area. They also require more complex mechanisms to maintain the array surface at the required angle. The increased output can be of the order of 30% in locations with high levels of direct radiation, but the increase is lower in temperate climates or those with more significant diffuse radiation, due to overcast conditions. For this reason, dual axis trackers are most commonly used in subtropical regions, and were first deployed at utility scale at the Lugo plant.\n\nA third approach achieves some of the output benefits of tracking, with a lesser penalty in terms of land area, capital and operating cost. This involves tracking the sun in one dimension – in its daily journey across the sky – but not adjusting for the seasons. The angle of the axis is normally horizontal, though some, such as the solar park at Nellis Airforce Base, which have a 20° tilt, incline the axis towards the equator in a north-south orientation – effectively a hybrid between tracking and fixed tilt.\n\nSingle axis tracking systems are aligned along axes roughly North-South. Some use linkages between rows so that the same actuator can adjust the angle of several rows at once.\n\nSolar panels produce direct current (DC) electricity, so solar parks need conversion equipment to convert this to alternating current (AC), which is the form transmitted by the electricity grid. This conversion is done by inverters. To maximise their efficiency, solar power plants also incorporate maximum power point trackers, either within the inverters or as separate units. These devices keep each solar array string close to its peak power point.\n\nThere are two primary alternatives for configuring this conversion equipment; centralised and string inverters, although in some cases individual, or micro-inverters are used. Single inverters allows optimizing the output of each panel, and multiple inverters increases the reliability by limiting the loss of output when an inverter fails.\n\nThese units have relatively high capacity, typically of the order of 1 MW, so they condition the output of a substantial block of solar arrays, up to perhaps in area. Solar parks using centralised inverters are often configured in discrete rectangular blocks, with the related inverter in one corner, or the centre of the block.\n\nString inverters are substantially lower in capacity, of the order of 10 kW, and condition the output of a single array string. This is normally a whole, or part of, a row of solar arrays within the overall plant. String inverters can enhance the efficiency of solar parks, where different parts of the array are experiencing different levels of insolation, for example where arranged at different orientations, or closely packed to minimise site area.\n\nThe system inverters typically provide power output at voltages of the order of 480 V. Electricity grids operate at much higher voltages of the order of tens or hundreds of thousands of volts, so transformers are incorporated to deliver the required output to the grid. Due to the long lead time, the Long Island Solar Farm chose to keep a spare transformer onsite, as transformer failure would have kept the solar farm offline for a long period. Transformers typically have a life of 25 to 75 years, and normally do not require replacement during the life of a photovoltaic power station.\n\nThe performance of a solar park is a function of the climatic conditions, the equipment used and the system configuration. The primary energy input is the global light irradiance in the plane of the solar arrays, and this in turn is a combination of the direct and the diffuse radiation.\n\nA key determinant of the output of the system is the conversion efficiency of the solar modules, which will depend in particular on the type of solar cell used.\n\nThere will be losses between the DC output of the solar modules and the AC power delivered to the grid, due to a wide range of factors such as light absorption losses, mismatch, cable voltage drop, conversion efficiencies, and other parasitic losses. A parameter called the 'performance ratio' has been developed to evaluate the total value of these losses. The performance ratio gives a measure of the output AC power delivered as a proportion of the total DC power which the solar modules should be able to deliver under the ambient climatic conditions. In modern solar parks the performance ratio should typically be in excess of 80%.\n\nEarly photovoltaic systems output decreased as much as 10%/year, but as of 2010 the median degradation rate was 0.5%/year, with modules made after 2000 having a significantly lower degradation rate, so that a system would lose only 12% of its output performance in 25 years. A system using modules which degrade 4%/year will lose 64% of its output during the same period. Many panel makers offer a performance guarantee, typically 90% in ten years and 80% over 25 years. The output of all panels is typically warranteed at plus or minus 3% during the first year of operation.\n\nSolar power plants are developed to deliver merchant electricity into the grid as an alternative to other renewable, fossil or nuclear generating stations.\n\nThe plant owner is an electricity generator. Most solar power plants today are owned by independent power producers (IPP's), though some are held by investor- or community-owned utilities.\n\nSome of these power producers develop their own portfolio of power plants, but most solar parks are initially designed and constructed by specialist project developers. Typically the developer will plan the project, obtain planning and connection consents, and arrange financing for the capital required. The actual construction work is normally contracted to one or more EPC (engineering, procurement and construction) contractors.\n\nMajor milestones in the development of a new photovoltaic power plant are planning consent, grid connection approval, financial close, construction, connection and commissioning. At each stage in the process, the developer will be able to update estimates of the anticipated performance and costs of the plant and the financial returns it should be able to deliver.\n\nPhotovoltaic power stations occupy at least one hectare for each megawatt of rated output, so require a substantial land area; which is subject to planning approval. The chances of obtaining consent, and the related time, cost and conditions, varying from jurisdiction to jurisdiction and location to location. Many planning approvals will also apply conditions on the treatment of the site after the station has been decommissioned in the future. A professional health, safety and environment assessment is usually undertaken during the design of a PV power station in order to ensure the facility is designed and planned in accordance with all HSE regulations.\n\nThe availability, locality and capacity of the connection to the grid is a major consideration in planning a new solar park, and can be a significant contributor to the cost.\n\nMost stations are sited within a few kilometres of a suitable grid connection point. This network needs to be capable of absorbing the output of the solar park when operating at its maximum capacity. The project developer will normally have to absorb the cost of providing power lines to this point and making the connection; in addition often to any costs associated with upgrading the grid, so it can accommodate the output from the plant.\n\nOnce the solar park has been commissioned, the owner usually enters into a contract with a suitable counterparty to undertake operation and maintenance (O&M). In many cases this may be fulfilled by the original EPC contractor.\n\nSolar plants' reliable solid-state systems require minimal maintenance, compared to rotating machinery for example. A major aspect of the O&M contract will be continuous monitoring of the performance of the plant and all of its primary subsystems, which is normally undertaken remotely. This enables performance to be compared with the anticipated output under the climatic conditions actually experienced. It also provides data to enable the scheduling of both rectification and preventive maintenance. A small number of large solar farms use a separate inverter or maximizer for each solar panel, which provide individual performance data that can be monitored. For other solar farms, thermal imaging is a tool that is used to identify non-performing panels for replacement.\n\nA solar park's income derives from the sales of electricity to the grid, and so its output is metered in real-time with readings of its energy output provided, typically on a half-hourly basis, for balancing and settlement within the electricity market.\n\nIncome is affected by the reliability of equipment within the plant and also by the availability of the grid network to which it is exporting. Some connection contracts allow the transmission system operator to constrain the output of a solar park, for example at times of low demand or high availability of other generators. Some countries make statutory provision for priority access to the grid for renewable generators, such as that under the European Renewable Energy Directive.\n\nIn recent years, PV technology has improved its electricity generating efficiency, reduced the installation cost per watt as well as its energy payback time (EPBT), and has reached grid parity in at least 19 different markets by 2014. Photovoltaics is increasingly becoming a viable source of mainstream power. However, prices for PV systems show strong regional variations, much more than solar cells and panels, which tend to be global commodities. In 2013, utility-scale system prices in highly penetrated markets such as China and Germany were significantly lower ($1.40/W) than in the United States ($3.30/W). The IEA explains these discrepancies due to differences in \"soft costs\", which include customer acquisition, permitting, inspection and interconnection, installation labor and financing costs.\n\nSolar generating stations have become progressively cheaper in recent years, and this trend is expected to continue. Meanwhile, traditional electricity generation is becoming progressively more expensive. These trends are expected to lead to a crossover point when the levelised cost of energy from solar parks, historically more expensive, matches the cost of traditional electricity generation. This point is commonly referred to as grid parity.\n\nFor merchant solar power stations, where the electricity is being sold into the electricity transmission network, the levelised cost of solar energy will need to match the wholesale electricity price. This point is sometimes called 'wholesale grid parity' or 'busbar parity'.\n\nSome photovoltaic systems, such as rooftop installations, can supply power directly to an electricity user. In these cases, the installation can be competitive when the output cost matches the price at which the user pays for his electricity consumption. This situation is sometimes called 'retail grid parity', 'socket parity' or 'dynamic grid parity'. Research carried out by UN-Energy in 2012 suggests areas of sunny countries with high electricity prices, such as Italy, Spain and Australia, and areas using diesel generators, have reached retail grid parity.\n\nBecause the point of grid parity has not yet been reached in many parts of the world, solar generating stations need some form of financial incentive to compete for the supply of electricity. Many legislatures around the world have introduced such incentives to support the deployment of solar power stations.\n\nFeed-in tariffs are designated prices which must be paid by utility companies for each kilowatt hour of renewable electricity produced by qualifying generators and fed into the grid. These tariffs normally represent a premium on wholesale electricity prices and offer a guaranteed revenue stream to help the power producer finance the project.\n\nThese standards are obligations on utility companies to source a proportion of their electricity from renewable generators. In most cases, they do not prescribe which technology should be used and the utility is free to select the most appropriate renewable sources.\n\nThere are some exceptions where solar technologies are allocated a proportion of the RPS in what is sometimes referred to as a 'solar set aside'.\n\nSome countries and states adopt less targeted financial incentives, available for a wide range of infrastructure investment, such as the US Department of Energy loan guarantee scheme, which stimulated a number of investments in the solar power plant in 2010 and 2011.\n\nAnother form of indirect incentive which has been used to stimulate investment in solar power plant was tax credits available to investors. In some cases the credits were linked to the energy produced by the installations, such as the Production Tax Credits. In other cases the credits were related to the capital investment such as the Investment Tax Credits\n\nIn addition to free market commercial incentives, some countries and regions have specific programs to support the deployment of solar energy installations.\n\nThe European Union's Renewables Directive sets targets for increasing levels of deployment of renewable energy in all member states. Each has been required to develop a National Renewable Energy Action Plan showing how these targets would be met, and many of these have specific support measures for solar energy deployment. The directive also allows states to develop projects outside their national boundaries, and this may lead to bilateral programs such as the Helios project.\n\nThe Clean Development Mechanism of the UNFCCC is an international programme under which solar generating stations in certain qualifying countries can be supported.\n\nAdditionally many other countries have specific solar energy development programmes. Some examples are India's JNNSM, the Flagship Program in Australia, and similar projects in South Africa and Israel.\n\nThe financial performance of the solar power plant is a function of its income and its costs.\n\nThe electrical output of a solar park will be related to the solar radiation, the capacity of the plant and its performance ratio. The income derived from this electrical output will come primarily from the sale of the electricity, and any incentive payments such as those under Feed-in Tariffs or other support mechanisms.\n\nElectricity prices may vary at different times of day, giving a higher price at times of high demand. This may influence the design of the plant to increase its output at such times.\n\nThe dominant costs of solar power plants are the capital cost, and therefore any associated financing and depreciation. Though operating costs are typically relatively low, especially as no fuel is required, most operators will want to ensure that adequate operation and maintenance cover is available to maximise the availability of the plant and thereby optimise the income to cost ratio.\n\nThe first places to reach grid parity were those with high traditional electricity prices and high levels of solar radiation. Currently, more capacity is being installed in the rooftop than in the utility-scale segment. However, the worldwide distribution of solar parks is expected to change as different regions achieve grid parity. This transition also includes a shift from rooftop towards utility-scale plants, since the focus of new PV deployment has changed from Europe towards the \"Sunbelt markets\" where ground-mounted PV systems are favored.\n\nBecause of the economic background, large-scale systems are presently distributed where the support regimes have been the most consistent, or the most advantageous. Total capacity of worldwide PV plants above 4 MW was assessed by Wiki-Solar as 36 GW in c. 2,300 installations at the end of 2014 and represents about 25 percent of total global PV capacity of 139 GW. The countries which had the most capacity, in descending order, were the United States, China, Germany, India, United Kingdom, Spain, Italy, Canada and South Africa. Activities in the key markets are reviewed individually below.\n\nChina was reported in early 2013 to have overtaken Germany as the nation with the most utility-scale solar capacity. Much of this has been supported by the Clean Development Mechanism.\nThe distribution of power plants around the country is quite broad, with the highest concentration in the Gobi desert and connected to the Northwest China Power Grid.\n\nThe first multi-megawatt plant in Europe was the 4.2 MW community-owned project at Hemau, commissioned in 2003. But it was the revisions to the German feed-in tariffs in 2004, which gave the strongest impetus to the establishment of utility-scale solar power plants. The first to be completed under this programme was the Leipziger Land solar park developed by Geosol. Several dozen plants were built between 2004 and 2011, several of which were at the time the largest in the world. The EEG, the law which establishes Germany’s feed-in tariffs, provides the legislative basis not just for the compensation levels, but other regulatory factors, such as priority access to the grid. The law was amended in 2010 to restrict the use of agricultural land, since which time most solar parks have been built on so-called ‘development land’, such as former military sites. Partly for this reason, the geographic distribution of photovoltaic power plants in Germany is biased towards the former Eastern Germany.\nAs of February 2012, Germany had 1.1 million photovoltaic power plants (most are small kW roof mounted).\n\nIndia has been rising up the leading nations for the installation of utility-scale solar capacity. The Charanka Solar Park in Gujarat was opened officially in April 2012 and was at the time the largest group of solar power plants in the world.\n\nGeographically the states with the largest installed capacity are Telangana, Rajasthan and Andhra Pradesh with over 2 GW of installed solar power capacity each. Rajasthan and Gujarat share the Thar Desert, along with Pakistan. In September 2018 Acme Solar announced that it had commissioned India's cheapest solar power plant, the 200 MW Rajasthan Bhadla solar power park\n\nItaly has a very large number of photovoltaic power plants, the largest of which is the 84 MW Montalto di Castro project.\n\nBy the end of 2017, it was reported that more than 732 MW of solar energy projects had been completed, which contributed to 7% of Jordan's electricity. After having initially set the percentage of renewable energy Jordan aimed to generate by 2020 at 10%, the government announced in 2018 that it sought to beat that figure and aim for 20%. A report by pv magazine described Jordan as the \"Middle East’s solar powerhouse\".\n\nThe majority of the deployment of solar power stations in Spain to date occurred during the boom market of 2007-8.\nThe stations are well distributed around the country, with some concentration in Extremadura, Castile-La Mancha and Murcia.\n\nThe introduction of Feed-in tariffs in the United Kingdom in 2010 stimulated the first wave of utility-scale projects, with c. 20 plants being completed before tariffs were reduced on 1 August 2011 following the 'Fast Track Review'. A second wave of installations was undertaken under the UK's Renewables Obligation, with the total number of plants connected by the end of March 2013 reaching 86. This is reported to have made the UK Europe's best market in the first quarter of 2013.\n\nUK projects were originally concentrated in the South West, but have more recently spread across the South of England and into East Anglia and the Midlands. The first solar park in Wales came on stream in 2011 at Rhosygilwen, north Pembrokeshire. As of June 2014 there were 18 schemes generating more than 5 MW and 34 in planning or construction in Wales.\n\nThe US deployment of photovoltaic power stations is largely concentrated in southwestern states. The Renewable Portfolio Standards in California and surrounding states provide a particular incentive.\nThe volume of projects under construction in early 2013 has led to the forecast that the US will become the leading market.\n\nThe following solar parks were, at the time they became operational, the largest in the world or their continent, or are notable for the reasons given:\nSolar power plants under development are not included here, but may be on this list.\n\n"}
{"id": "11025540", "url": "https://en.wikipedia.org/wiki?curid=11025540", "title": "Piezomagnetism", "text": "Piezomagnetism\n\nPiezomagnetism is a phenomenon observed in some antiferromagnetic crystals. It is characterised by a linear coupling between the system's magnetic polarisation and mechanical strain. In a piezomagnetic, one may induce a spontaneous magnetic moment by applying physical stress, or a physical deformation by applying a magnetic field.\n\nPiezomagnetism differs from the related property of magnetostriction; if an applied magnetic field is reversed in direction, the strain produced changes signs. Additionally, a non-zero piezomagnetic moment can be produced by mechanical strain \"alone\", at zero field - this is not true of magnetostriction. According to IEEE: \"Piezomagnetism is the linear magnetomechanical effect analogous to the linear electromechanical effect of piezoelectricity. Similarly, magnetostriction and electrostriction are analogous second-order effects. These higher-order effects can be represented as effectively first-order when variations in the system parameters are small compared with the initial values of the parameters\".\n\nThe piezomagnetic effect is made possible by an absence of certain symmetry elements in a crystal structure; specifically, symmetry under time reversal forbids the property.\n\nThe first experimental observation of piezomagnetism was made in 1960, in the fluorides of cobalt and manganese.\n\nThe strongest piezomagnet known is uranium dioxide, with magnetoelastic memory switching at magnetic fields near 180,000 Oe.\n"}
{"id": "50806706", "url": "https://en.wikipedia.org/wiki?curid=50806706", "title": "Pillared graphene", "text": "Pillared graphene\n\nPillared graphene is a hybrid carbon, structure consisting of an oriented array of carbon nanotubes connected at each end to a sheet of graphene. It was first described theoretically by George Froudakis and colleagues of the University of Crete in Greece in 2008. Pillared graphene has not yet been synthesised in the laboratory, but it has been suggested that it may have useful electronic properties, or as a hydrogen storage material.\n\n"}
{"id": "57529724", "url": "https://en.wikipedia.org/wiki?curid=57529724", "title": "Puna Geothermal Venture", "text": "Puna Geothermal Venture\n\nThe Puna Geothermal Venture (PGV) is a currently nonoperational geothermal energy power plant on the island of Hawaii, the largest island in the state of Hawaii. The plant was shut down shortly after the start of the May 2018 lower Puna eruption. The eruption caused lava to flow over a power substation, a warehouse and at least three geothermal wells that had been preventatively quenched and capped when lava fountains erupted nearby, eventually also cutting off road access.\n\nPGV was the first and only commercially-productive geothermal electrical plant in Hawaii. Constructed on a site adjacent to failed experimental wells drilled and operated by the Hawaii Geothermal Project in the 1970s and 80s, construction on the generating facility began in 1989 and was completed in 1993. \n\nPrior to the shutdown and lava damage, the plant had an installed generating capacity of 38 MW from six production wells and five injection wells along Kīlauea’s East rift zone, which had provided approximately 25% of the island’s power. Power generated was sold to the Hawaiian Electric Industries (also known as HELCO) to supply the island’s electric power customers.\n\nThe first exploratory geothermal wells in Kapoho were drilled in 1961–62. The effort was spearheaded by wealthy local landowner Richard Lyman following a trip to Japan, where he learned of a project to lay underwater power cables between the country's islands. Upon returning to Hawaii, Lyman promoted geothermal electrical generation in the Puna district, distributed to Oahu and other population centers by underwater electric cables, as a way of developing the then-newly-admitted-state's economy. He found business partners to establish Hawaii Magma Power in 1961 to develop geothermal energy in the region. Several exploratory drills were made before the company determined that there was insufficient geothermal potential, although Lyman claimed to the \"Honolulu Advertiser\" that the company \"had mechanical difficulties and gave up.\"\n\nIn the 1970s, the Hawaii Geothermal Project was formed to conduct research funded by federal and state grants. After scientific studies of the geology and geothermal potential of the area were conducted by HGP, a site for the first well was chosen in 1975. Drilling began without environmental impact studies or a period for public input. At the time, the area around the drill site was very rural and undisturbed, but new subdivisions were planned nearby. The well was occasionally allowed to discharge gas and fluid from the borehole, which created a loud noise and release large volumes of toxins and pollutants into the air. By 1976, three subdivisions nearby had been established and were beginning to see an influx of residents, including twelve families that now lived within one mile of the well. The continued noise pollution and toxic gas discharges began to cause concern and opposition from the new, nearby residents. Nonetheless, funds were secured from the federal and state governments to begin the next step of the project: turning the geothermal energy of the well into electricity. Although HELCO was among the stakeholders in the project, they assured their stockholders that they would not invest the company's capital in the project due to the volcanic and seismic risk at the site.\n\nAn experimental 3 MW generator was completed in 1981 and remained operational throughout the 1980s. However, families continued to build and move into the nearby subdivisions, resulting in several attempts and a failed lawsuit to stop the noise and pollution generated from the site. Since the site was within a high lava hazard zone, the generator was built on skids and housed in a building which included a crane so that the generator could be removed if lava flows threatened the facility. Additionally, the wellhead was housed in a concrete bunker that could be sealed to prevent lava from damaging the wellhead. Despite the opposition and the operating losses incurred by operating and maintaining the generator, there was enough support to keep the generator operating. In April 1989, county officials raised concerns about the condition of the facility following a minor blowout and considered reviewing and possibly revoking the operating permit for the site. After several additional incidents at the site in 1989, the facility was ordered to be shut down, which occurred on December 11. During its life, the HGP generator produced between 15 and 19 million kilowatt hours of electricity annually.\n\nIn 1980, HELCO requested a proposal for 25 MW of geothermal electricity generation. To answer the request, a joint venture was formed the same year between two of Lyman's companies and two other companies to produce geothermal energy on a parcel of land leased from the Lyman family. The joint venture was renamed Puna Geothermal Venture in 1981. Over the following years, PGV drilled three wells, but all suffered from mechanical failures of their well casings and were unusable for production.\n\nIn 1986, PGV secured a contract with HELCO to supply 25 MW of electricity by 1993. PGV promised that they would be using new technology that would result in zero emissions and operate at a drastically lower noise level. In 1989, when the HGP facility was facing growing problems, the PGV proposed facility managed to overcome enough local opposition to be granted a permit from the local planning commission.\n\nThe plant has raised some concerns with local residents as a result of occasional toxic emissions. In 1991, well KS-8 suffered a blowout, causing the state to suspend the permits for the plant. In 2016 the plant’s owners were found to be in violation of U.S. Environmental Protection Agency standards regarding hydrogen sulfide releases and was fined $76,500 for two incidents in 2013. Additional concerns and opposition to the plant have been raised by Native Hawaiians, who view all forms of volcanic activity as manifestations of the goddess Pele. To the Native Hawaiians who revere Pele, geothermal wells and energy production are a desecration of her body and spirit.\n\nIn 2005 during the drilling of the KS-13 well, magma was encountered at a depth of . The borehole had to be redrilled several times as the magma flowed up the borehole, cooling into clear, colorless glass. The magma, at a temperature of approximately 1922°F (1050°C), was dacitic—similar to the granitic rock that forms continents—consisting of approximately two-thirds silica, which contrasts with the dark, iron-rich basaltic rock that forms most of the Hawaiian Islands. It was encountered after drilling through a layer of diorite igneous rock, which suggested to researchers that the magma had chemically separated as it dwelled for a long period of time. As quoted in the journal \"Nature\", the team said it was possibly the first time that \"the actual process of differentiation of continental-type rock from primitive ocean basalt has been observed \"in situ\"\". Magma specialist Bruce Marsh of Johns Hopkins University described the uniqueness of the encounter: \"Before, all we had to deal with were lava flows; but they are the end of a magma's life. They're lying there on the surface, they've de-gassed. It's not the natural habitat. It's the difference between looking at dinosaur bones in a museum and seeing a real, living dinosaur roaming out in the field.\"\n\nPGV had a generating capacity of 25 MW when it opened in 1993, which was expanded to 30 MW in 1995 and 38 MW in 2012. In 2015, HELCO announced that Ormat was selected as the winner of a bid to add 25 MW of geothermal generating capacity in the Puna district. In March 2018, Ormat announced their plan to increase production at the plant by 8 MW—from 38 MW to 46 MW—by 2020.\n\nOn May 3, 2018 earth fissures opened inside and around the Leilani Estates subdivision near the PGV plant, following hundreds of earthquakes over the first two days of May, resulting in concerns of possible toxic hydrogen sulfide gas releases and explosions at the geothermal power facility. According to PGV's vice president of community affairs, PGV began preemptively shutting down equipment and inventorying its stockpile of highly-flammable pentane when the frequency of earthquakes began increasing and the first cracks appeared in Leilani Estates on May 1–2; the plant was taken off-line approximately three hours after it received the first report that lava had begun to emerge from the ground on May 3. All pentane stored at PGV, approximately , were removed by the morning of May 10. \n\nOver the next couple of weeks, the wells were quenched with cold water to stabilize them—where the weight of the cold water was sufficient to prevent steam from rising from the bottom of the well—and allow them to be plugged. One well, KS-14, possibly being super-heated from close proximity to magma, could not be quenched and was filled with a drilling mud in an attempt to stabilize it. The wells were then sealed with metallic plugs, which arrived at the site on May 22, that officials with PGV claim can withstand 2000°F (1100°C) lava. According to Tom Travis of the Hawaii Emergency Management Agency, he researched and was unable to find any precedent for lava overrunning a geothermal well that had been shut-down like the wells at PGV. The PGV team had spoken with scientists in Iceland that have operated wells within lava fields and provided insights into how lava might affect the wellhead of a shutdown well.\n\nLava approached several of the capped wells on May 27. Two of the 11 capped geothermal wells, identified as KS-5 and KS-6, were covered by the lava from fissures 7 and 21 on May 27 and 28. The event was the first time lava had covered a geothermal well. On May 30 a substation and a warehouse containing a drilling rig were overrun and destroyed by molten rock, with the main access road to the facility cut. A third well was covered with lava later during the eruption.\n\nDespite the geothermal plant’s shutdown, Hawaii Electric Light did not expect blackouts on the Big Island to be caused by insufficient power generation as older, diesel-fueled generators were brought on-line. According to PGV's vice president of community affairs, the wells can be put back into operation.\n\nIn June 2018, Snopes.com debunked several fake news and conspiracy theories that were spreading which claimed that 'fracking' activity by PGV caused earthquakes in the vicinity and was the main cause of the 2018 lower Puna eruption:\nPGV is located in the East Rift Zone of the Kīlauea volcano, which forms the Big Island of Hawaii. The geothermal energy reservoir discovered by the Hawaii Geothermal Project in this location is known as the Kapoho Geothermal Reservoir. The geothermal energy potential of the East Rift Zone is estimated to exceed 200 MW. The geothermal reservoir is contained within basaltic rock and relies on the permeability of two major fracture systems. Both fracture systems have large openings, recorded by the drop of drillbits for up to .\n\nThe first and only geothermal energy plant in Hawaii, the plant began generating power in 1993, and is owned by Ormat Technologies which purchased it in 2004.\n\nThere are two power plants in the Puna Complex. The first plant consists of ten Combined cycle Ormat Energy Converters (OEC) made up of ten steam turbines and ten binary turbines. Production of electricity began on April 22, 1993. The second plant consists of two Binary cycle OECs and was placed in service in 2011 and commenced commercial operation in 2012. There is a plan to replace the first plant's ten old OECs with two new OECs which increase the generating capacity from 38 MW to 46 MW.\n\nAs of December 2017, there were six production wells and five injection wells in the Puna Complex.\n\nAs of 2008, there are five active production wells all of which have a surface elevation of above sea level and produce a mixture of steam and brine. The five wells produced an average of 600,000 lb/hour (270,000 kg/hour) of steam and 1,200,000 lb/hour (545,000 kg/hour) of brine. The temperature of fluids emerging from the production wells is approximately 640°F (338°C), which is returned to the injection wells at approximately 300–400°F (150–200°C) after being used to generate electricity.\n\nIn addition to electricity generation, additional uses of the facility have been suggested. A paper presented by Andrea Gill of the Hawaii Deptartment of Business, Economic Development, and Tourism—on behalf of a working group established to consider direct uses of the geothermal energy at PGV and its vicinity—outlined potential direct uses of the fluids at PGV as well as shallow groundwater wells in its vicinity, including fruit and macadamia nut dehydration, aquaculture and greenhouse heating, pasteurization and sterilization, geothermal/health spas, and geothermal heat pumps. At PGV, hot brine after being utilized to generate electricity is approximately 300–400°F (150–200°C) when it is reinjected into the earth; the brine could be tapped for its thermal energy before being reinjected. Water wells under 750 ft (230 m) in the vicinity of PGV have recorded temperatures up to 193°F (89°C).\n\nIn 2005, after a drill hit an uncommonly-observed type of magma, researchers and PGV expressed a desire to turn the borehole into an observatory for scientific studies.\n\n"}
{"id": "33394945", "url": "https://en.wikipedia.org/wiki?curid=33394945", "title": "Rena oil spill", "text": "Rena oil spill\n\nThe \"Rena\" oil spill occurred off the coast of Tauranga in New Zealand. The spill was caused by the grounding of on the Astrolabe Reef. The \"Rena\" was a container ship and cargo vessel owned by the Greek shipping company Costamare Inc., through one of its subsidiary companies, and chartered by the Mediterranean Shipping Company (MSC). The spill has been described as New Zealand's worst maritime environmental disaster.\n\nOn Wednesday, 5 October 2011, at 2:20 AM (Tuesday, 4 October 13:20 UTC) while sailing in clear weather from Napier to Tauranga, at a speed of , \"Rena\" ran aground on the Astrolabe Reef. The ship was carrying 1,368 containers, eight of which contained hazardous materials, as well as 1,700 tonnes of heavy fuel oil and 200 tonnes of marine diesel oil. Initially the ship listed 11 degrees to port, with the front stuck on the reef.\n\nBy Sunday, 9 October 2011, a oil slick threatened wildlife and the area's rich fishing waters.\n\nOil from \"Rena\" began washing ashore at Mount Maunganui beach on 10 October 2011. Bad weather that night had caused the ship to shift further onto the reef, and the crew was evacuated. The shifting of the ship caused further damage, resulting in a further 130 - 350 tonnes of oil leaking.\n\nStrong winds and bad weather on the night of 11 October 2011 caused the ship to list over to starboard 19 degrees; this resulted in between 30 and 70 containers being washed overboard. None of the containers contained hazardous cargo. Containers subsequently began washing ashore on Motiti Island.\n\nOn the afternoon of 12 October 2011, aerial footage showed a large crack in the hull, increasing fears that the ship could break in two and sink. It also showed a container floating in the water surrounded by smoke, suggesting that a chemical reaction was occurring.\n\nOn 13 October 2011, Maritime New Zealand ordered beaches from Mount Maunganui to Maketu Point, including the Maketu Estuary, to be closed to the public. Volunteers were warned that contact with spilled oil could lead to vomiting, nausea and rashes, and local residents were urged to close their windows to limit fumes.\n\nCostamare Shipping, the owners of \"Rena\", apologised to the people of Tauranga, saying they were \"deeply sorry\" for the \"disastrous event.\" Although not legally obligated to do so, the charterer, Mediterranean Shipping Company, promised to help with the cleanup costs.\nOn 14 October 2011, it was reported that the ship's hull had cracked in half, and the bow and stern sections were held together only by internal structures and the reef. Calmer weather meant that preparations could be made to pump out the remainder of the ship's oil, but a change in the wind direction meant that oil was likely to spread as far east as Whakatane and Opotiki.\n\nAlso on 14 October 2011, the Filipino crew of the \"Rena\" left New Zealand \"for their safety\" after a racist backlash against Filipinos in Tauranga.\n\nCalm weather on 15 October 2011, allowed salvage experts to board the vessel and begin preparations to pump the remaining oil to a barge. Platforms were attached to the side of the ship, and pumping began on 16 October 2011. By 17 October only twenty tons of oil had been removed.\n\nOn 16 October a mine-countermeasures team aboard HMNZS \"Manawanui\" began hunting for spilled containers.\n\nAfter October, salvage efforts were focused on removing the ship's cargo before it completely broke apart.\n\nIn January 2012, the \"Rena\" completely broke in half and the stern section slipped off of the reef and eventually sank. A small amount of oil and containers escaped the ship as it broke in half.\n\nBy 23 March 2012, 649 containers of cargo had been recovered and it was thought that only a few tens of tonnes of oil still remained in the ship.\n\nThe incident was not immediately brought to the public eye since it took place on 5 October 2011, the same day the well-known American businessman Steve Jobs died. In addition, media outlets had their hands full with the Occupy Wall Street Movement that was taking place. Information about the oil spill did not surface on major news websites until four days later when CBC News published an article on 9 October 2011. British newspaper \"The Guardian\" then released a piece on the incident on 10 October 2011, followed by the BBC on 11 October 2011.\n\nResidents of Motiti Island voiced their concerns over the effects the oil spill was having on their lifestyles, since they relied on water filled in tanks and seafood from the affected waters for survival. It cost islanders approximately $100 to leave the island for food or water; this expense along with the toxic water and seafood raised concern among the citizens of Motiti Island that their lives might never be the same.\n\nNew Zealand environmental minister, Nick Smith, said that the impact of this spill was the most significant in New Zealand history.\nReporter Karen Barlow of Lateline said that it may not be the biggest ever oil spill, but it could be catastrophic for the pristine waters of the Bay of Plenty.\nWorld Wildlife Fund spokesperson, Bob Zuur, confirmed a major loss of wildlife.\n\n\"The cleanup for the Rena Oil spill will take time,\" said New Zealand scientist Dr. Norm Duke. \"Petroleum oil will naturally break down - but this takes time and oxygenation. So, the longer the oil remains floating at sea - the safer it becomes. And, the rougher the weather - the better also.\" \n\nMaritime New Zealand used the oil dispersant Corexit 9500 to help in the cleanup process. Corexit is known to increase the toxicity of oil. The dispersant was applied for only a week, after results proved inconclusive.\n\n\n\nThe disaster occurred only seven weeks prior to the 2011 general election (which took place on 26 November 2011) and partially affected the campaign. On 14 October it was reported that the disaster had caused a 4% drop in the governing National Party's polling on the \"iPredict\" prediction market.\n\nOn 13 October 2011, the New Zealand Labour Party announced that it would impose a moratorium on deep sea drilling for oil if elected to power.\n\nIn 2012 the government announced a twelve-month environmental recovery plan for the area with an expected cost of NZ$2–3 million (US$1.6M–2.5M as of January 2012).\n\nOn 12 October 2011, the captain of \"Rena\" appeared in the Tauranga District Court charged with operating a vessel causing unnecessary danger or risk to a person or property. He was granted name suppression and remanded on bail. If convicted he faced a fine of up to $10,000 or up to 12 months imprisonment. The ship's second officer, who was responsible for navigation at the time of the accident, was subsequently charged and appeared in court on 13 October.\n\nThe two men, both Filipino, pleaded guilty to 11 charges between them, including attempting to pervert the course of justice (based on alleged alteration of navigational documents after the collision). The sentencing for both men was scheduled for 25 May 2012.\n\nThe (New Zealand) Transport Accident Investigation Commission released an interim report into the grounding on 8 March 2012. The report states only what happened but not why, and does not apportion blame. It states that \"Rena\" arrived at the port of Napier and began unloading cargo, but was forced to stand off in the harbour when a ship with priority booking arrived. \"Rena\" was therefore delayed 13 hours in leaving Napier. On making the run toward Tauranga, the captain was under pressure to make up time and to arrive at the port's pilot station by 3:00 am, and the ship's charts showed that a more direct course than usual had been set that took \"Rena\" toward Astrolabe Reef. An intermittent radar echo first noticed at 2:05 am was ignored when nothing was seen through binoculars, and at 2:14 \"Rena\" struck the reef.\n\nOn 25 May 2012, the captain and navigation officer appeared in Tauranga District Court for sentencing. Each was sentenced to 7 months imprisonment.\n\nThree Bay of Plenty iwi groups have launched a breach of the Treaty of Waitangi case against the Crown. Their key concern is that they were not adequately consulted in regard to the Crown agreeing to receive $10 million as compensation for the remains of the wreck not being removed. The government agreed to the compensation payment when a storm caused the remains of the wreck to fall off the reef onto the seabed and marine demolition experts said it was too dangerous to remove it.\n\nThe second most extreme oil spill New Zealand has experienced since 1990 was the Jody F. Millennium log ship incident. The ship broke free from several of her moorings in Gisborne Harbour due to huge swells on Wednesday 6 February 2002. Tugboats attempted to bring the ship alongside the wharf, but it was then decided to take the ship to sea. A big swell hit the ship which then grounded on the beach. An estimated 25 tonnes of fuel oil leaked out of the ship.\n\n\n"}
{"id": "393148", "url": "https://en.wikipedia.org/wiki?curid=393148", "title": "Roton", "text": "Roton\n\nIn theoretical physics, a roton is an elementary excitation, or quasiparticle, in superfluid helium-4. The dispersion relation of elementary excitations in this superfluid shows a linear increase from the origin, but exhibits first a maximum and then a minimum in energy as the momentum increases. Excitations with momenta in the linear region are called phonons; those with momenta close to the minimum are called rotons. Excitations with momenta near the maximum are sometimes called maxons.\n\nThe term \"roton\" is also used for the quantised eigenmode of a freely rotating molecule.\n\nOriginally, the roton spectrum was phenomenologically introduced by Lev Landau. Currently there exist different models which try to explain the roton spectrum, with different degrees of success and fundamentality. The requirement for any model of this kind is that it must explain not only the shape of the spectrum itself but also other related observables, such as the speed of sound and structure factor of superfluid helium-4. Microwave and Bragg spectroscopy has been conducted on helium to study the roton spectrum.\n\nBose–Einstein condensation of rotons has been also proposed and studied. Its first detection has been reported in 2018.\n\n\n"}
{"id": "38209210", "url": "https://en.wikipedia.org/wiki?curid=38209210", "title": "Solar bus", "text": "Solar bus\n\nA solar bus or solar-charged bus is a bus which is powered exclusively or mainly by solar energy. A solar-powered bus service is referred to as a solar bus service. The use of the term \"solar bus\" normally implies that solar energy is used not only for powering electric equipment on the bus, but also for the propulsion of the vehicle.\n\nExisting solar buses are battery electric buses or (in the case of hybrid solar buses) hybrid buses equipped with batteries which are recharged from solar (or other) power sources; a launch of a solar bus service often goes hand in hand with investments for large-scale installations of stationary solar panels with photovoltaic cells. Similarly as other solar vehicles, many solar buses have photovoltaic cells contained in solar panels on the vehicle's roof which convert the sun's energy directly into electric energy to be used by the engine.\n\nThe introduction of solar buses and other green vehicles for purposes of public transport forms a part of sustainable transport schemes.\n\nThe distinction between a solar-only electric bus and an electro-solar bus is fluid, as the distinction depends on the actual usage: whether the bus is recharged from solar or other power sources.\n\nSolar-only bus services involve recharging the bus from solar energy, usually from solar panel-covered bus station canopies. The concept is similar to that of solar parking plot for cars and bicycles, where vehicles can re-charge while parked. The need for recharging poses constraints on the run and standstill times of the bus. The implementation of a solar bus service benefits from an optimization of over-all requirements for the specific bus service.\n\nElectro-solar buses are powered additionally from electric power transmitted from power plants; hybrid solar buses may be equipped with hybrid engines.\n\nOpen air low-speed electric shuttle sightseeing buses equipped with a solar panel-covered roof are produced in series and are commercially available. According to the producers, the solar panels save energy and prolong the battery life cycle.\n\nThe Tindo solar battery-charged bus (\"Tindo\", Kaurna word for \"sun\") is an experimental battery electric vehicle that operates in Adelaide, Australia. It is the world's first solar bus, operating since 2007. It uses 100% solar power, is equipped with a regenerative braking system and air conditioning and can carry up to 40 persons, 25 of whom are seated. The bus itself is not equipped with solar panels. It receives electric power from a photovoltaic system on Adelaide's central bus station. Hailed as the world's first bus service powered exclusively by solar power, the bus service connects Adelaide City and North Adelaide as part of Adelaide City's sustainable transport agenda. The Tindo is part of the 98A and 98C bus service (until recently known as the Adelaide Connector) which is offered as free public transport. \n\nWithin the Chinese government's program for clean transport sector, China's first solar hybrid buses were put in operation in July 2012 in the city of Qiqihar. Its engine is powered by lithium-ion batteries which are fed by solar panels installed on the bus roof. It is claimed that each bus consumes 0.6 to 0.7 kilowatt-hours of electricity per kilometer and can transport up to 100 persons, and that the use of solar panels prolongs the batteries' lifetime by 35 percent.\n\nAustria's first solar-powered bus was put in operation in the village of Perchtoldsdorf. Its powertrain, operating strategy, and design specification were specifically optimized in view of its planned regular service routes. It has been in trial operation since autumn 2011.\n\nThe tribrid bus is a hybrid electric bus developed by the University of Glamorgan, Wales, for use as student transport between the University’s different campuses. It is powered by hydrogen fuel or solar cells, batteries and ultracapacitors.\n\nThe first Solar Bus in the UK launched in Brighton April 2017. Following a marathon six week effort from hundreds of local people, The Big Lemon and Brighton Energy Coop's joint Solar Bus project has won funding from the M&S Community Energy Fund to cover the roof of The Big Lemon’s bus depot in solar panels to power the new electric buses on clean green renewable energy. The bus was named by one of the Solar Roof partners, Viper IT Solutions and is called \"Om Shanti\".\n\nThe 120 solar panels will generate 30,000kW of electricity – the equivalent of 1.8 million boiled kettles. With no emissions, the Solar Buses will reduce noxious gases in some of Brighton and Hove’s most polluted areas and will power the 52 route between Woodingdean and Brighton on 100% renewable energy.\n\nThe Solar Bus project was one of 199 different applications to the scheme, 125 of which were shortlisted. These were put to public vote for six weeks during September and October and the voting process also included the option to donate to the project via the Crowdfunding platform.\n\nThe Solar Bus project was one of 19 regional winners, with 1549 votes, 170 pledges, and a total of £13,325 raised through crowdfunding, almost half the total amount of £28,798 raised through the scheme nationally. The project will benefit from £12,500 funding from M&S Energy which, together with the £13,325 crowdfunding donations will fund the solar array on the roof of the bus depot.\n\nThe Kayoola Solar Bus is a 35-seater electric solar bus with zero tailpipe emissions, a range of 80 km, with latent range extension from the real-time charging enabled by the roof mounted solar panels. The development of the Kayoola Solar Bus Concept represents the commitment of the Kiira Motors Project to championing the progressive development of local capacity for Vehicle Technology Innovation, a key ingredient for institutionalizing a sustainable Vehicle Manufacturing Industry in Uganda. \n\nThe Solar Buzz is a 14-seater US Electricar bus, made in 1994, that has been repurposed as the world's first truly solar bus in Truth or Consequences, New Mexico in 2011. The Buzz has 2 KW in homemade solar panels on the roof, 40 golf cart batteries, 2 electric motors, and has no tailpipe. The power required to go one mile is the same as the power required to make a pot of coffee: around 700 Wh. The Solar Buzz is an IntraCity bus approved by the state Public Regulation Commission (#56817) in 2015 and is a commercial daytime private shuttle service with a US$3 Fare, operated by Hot Springs Transit, LLC. Hot Springs Transit provides transit service to the 6100 person population of Truth or Consequences.\n\nSolar panels are also used for powering electronic devices of the bus such as heating and air conditioning, even in buses with non-solar-powered engine. In the US, such buses are advertised to meet anti-idling regulations in several states.\n\nRefitting existing non-existing vehicles with photovoltaic panels that feed the original battery with additional electric power has been shown to have the potential for making a substantial contribution to CO2 emission mitigation and to the reduction of pollution. The thus transformed buses are however not solar buses in the strict sense of the word, as they do not use solar energy for propulsion. The use of buses in public transport implies frequent stops with the opening and closing of doors, which influence the way the energy of the battery is used.\n\nIn principle also trolleybuses or other non-autonomous electric buses or alternately powered buses such as fuel cell buses or dual-mode buses could be used for solar bus services, provided the origin of all or most of the energy used for propulsing the bus would be solar energy. In practice however such systems also draw on other sources of energy, at least also other renewable energy sources such as wind energy. An example is that the city of Hamburg, Germany, received the 2011 European Green Capital Award for, among others, its fuel cell bus service that is claimed to be the world's largest hydrogen-powered bus fleet and is intended to use hydrogen generated from solar and wind energy.\n\n"}
{"id": "5935618", "url": "https://en.wikipedia.org/wiki?curid=5935618", "title": "Taken by Storm", "text": "Taken by Storm\n\nTaken By Storm: The Troubled Science, Policy and Politics of Global Warming is a 2002 book about the global warming controversy by Christopher Essex and Ross McKitrick. The authors argue that politicians and others claim far more certainty than is justified by the science. The authors also argue that public policy discussions have abandoned science and resorted to \"ad hominem\" attacks.\n\n\"Taken by Storm\" was one of two runners up for the 2002 Donner Prize for the best book on Canadian public policy.\n\n\n"}
{"id": "28843688", "url": "https://en.wikipedia.org/wiki?curid=28843688", "title": "Tetrakis(3,5-bis(trifluoromethyl)phenyl)borate", "text": "Tetrakis(3,5-bis(trifluoromethyl)phenyl)borate\n\nTetrakis[3,5-bis(trifluoromethyl)phenyl]borate is an anion with chemical formula [{3,5-(CF)CH}B], which is commonly abbreviated as [BAr] as the boron atom (B) is surrounded by four fluorinated aryl (Ar) groups. It is sometimes referred to as \"Kobayashi's anion\" in honour of Hiroshi Kobayashi who led the team that first synthesised it, but more commonly it is affectionately nicknamed \"BARF.\" BARF has a tetrahedral geometry around the central boron atom but each of the four surrounding aryl groups is aromatic and planar. The motivation for its preparation was the search for an anion which coordinates more weakly than the then-available ions hexafluorophosphate, tetrafluoroborate, or perchlorate. Salts of this anion are known as solids and in both aqueous and non-aqueous solutions. BARF can be used in catalytic systems where the active site requires an anion which will not coordinate to the metal centre and interfere with the catalytic cycle, such as in the preparation of polyketones.\n\nThe sodium salt of the BARF ion is abbreviated NaBAr (or sometimes as NaBArF recognising the 24 fluorine atoms in each BARF ion). It was first prepared by a group led by Kobayashi and reported in 1984. A Grignard reagent was prepared by reacting 1-iodo-3,5-bis(trifluoromethyl)benzene with magnesium metal in dry diethyl ether (EtO) to form 3,5-bis(trifluoromethyl)phenylmagnesium iodide, ArMgI where Ar = 3,5-(CF)CH. An ethereal solution of boron trifluoride is then added to this Grignard and the purified salt produced in 84% yield after workup and column chromatography.\n\nA safer synthesis has since been developed, utilising the magnesium-bromine exchange reaction between 1-bromo-3,5-bis(trifluoromethyl)benzene and isopropylmagnesium chloride to generate the required aryl-Grignard reagent, which is then reacted with sodium tetrafluoroborate.\n\nNon-coordinating anions are anions that interact only weakly with cations, a useful property when studying highly electrophilic cations. In coordination chemistry, the term can also be used to refer to anions which are unlikely to bind directly to the metal centre of a complex. Hexafluorophosphate is a non-coordinating anion in both senses of the term. Three widely used non-coordinating anions are hexafluorophosphate, tetrafluoroborate , and perchlorate ; of these, the hexafluorophosphate ion has the least coordinating ability and it is deliberately used for this property. BARF was developed as a new non-coordinating anion in the 1990s, and is far less coordinating than even the hexafluorophosphate anion.\n\nNaBAr can be used in deprotection of acetal or ketal-protected carbonyl compounds. For example, deprotection of 2-phenyl-1,3-dioxolane to benzaldehyde can be achieved in water in five minutes at 30 °C.\n\nBrookhart's acid is the salt of the BARF anion with the diethyl ether oxonium cation, [(EtO)H]BAr. It can be formed from the sodium salt in diethyl ether in the presence of hydrogen chloride as sodium chloride is insoluble in diethyl ether, facilitating cation exchange.\n\nSynthesis of BARF salts with hexa(acetonitrile)metal(II) cations, [M(CHCN)], are known for vanadium, chromium, manganese, iron, cobalt, and nickel.\n"}
{"id": "45273959", "url": "https://en.wikipedia.org/wiki?curid=45273959", "title": "Tidal disruption event", "text": "Tidal disruption event\n\nA tidal disruption event (also known as a tidal disruption flare) is an astronomical phenomenon that occurs when a star gets sufficiently close to a supermassive black hole's event horizon and is pulled apart by the black hole's tidal forces, experiencing spaghettification.\n\nAccording to early papers (see History section), tidal disruption events should be an inevitable consequence of massive black holes activity hidden in galaxy nuclei, whereas later theorists concluded that the resulting explosion or flare of radiation from the accretion of the stellar debris could be a unique signpost for the presence of a dormant black hole in the center of a normal galaxy.\n\nIt was in 1971 that for the first time the theorist John A. Wheeler suggested that the breakup of a star in the ergosphere of a rotating black hole could induce acceleration of the released gas to relativistic speeds by the so-called \"tube of toothpaste effect\". Wheeler succeeded in applying the relativistic generalization of the classical Newtonian tidal disruption problem to the neighborhood of a Schwarzschild or a Kerr black hole (without axial rotation or in rotation, cf. Fishbone (1973) and Mashhoon (1975, 1977)). But these early works restricted their attention to incompressible star models\nand/or to stars penetrating slightly into the Roche radius, thus undergoing only tides of small amplitudes or, at best, only quiescent disruption phenomena (aka the future TDE).\n\nIn 1976 in the \"MNRAS\" astronomers Juhan Frank and Martin F. Rees of the Cambridge Institute of Astronomy evoked for the first time \"the effect of massive black holes on stellar systems \", defining a critical radius under which stars are disturbed and literally sucked up by the black hole, suggesting that it is possible to observe these events in certain galaxies. But at the time, the English researchers did not propose any precise model or simulation.\n\nThis speculative prediction and this lack of theoretical tools aroused the curiosity of Jean-Pierre Luminet and Brandon Carter of the Paris Observatory in the early 1980s who invented the concept of TDE. \nTheir first works were published in 1982 in the journal \"Nature\" and in 1983 in the \"Astronomy & Astrophysics\". The authors had managed to describe the tidal disturbances in the heart of AGNs based on the \"stellar pancake outbreak\" model to use Luminet's expression, a model describing the tide field generated by a \"big black hole\" - let's say supermassive - and the effect they called the \"pancake detonation\" to qualify the radiation outbreak resulting from these disturbances.\n\nThen in 1986, Luminet and Carter published in the journal \"Astrophysical Journal Supplement\" an important article of 29 pages in which they analyzed all the cases of TDE and not only the 10% producing \"spaghettifications\" and other \"pancakes flambées\".\n\nIt was only a decade later, in 1990, that the first TDE-compliant candidates were detected through NASA's \"All Sky\" X-ray survey of NASA's ROSAT satellite. Since then, more than a dozen candidates have been discovered, including more active sources in ultraviolet or visible for a reason that remained mysterious.\n\nFinally, the theory of Luminet and Carter was confirmed by the observation of spectacular eruptions resulting from the accretion of stellar debris by a massive object located in the heart of the AGN (e.g. NGC 5128 or NGC 4438) but also in the heart of the Milky Way (Sgr A *). The TDE theory even explains the superluminous supernova SN 2015L, better known by the code name ASASSN-15lh, a marial supernova that exploded just before being absorbed beneath the horizon of a massive black hole. \n\nToday, all TDEs have been listed in \"The Open TDE Catalog\" run by the Harvard CfA, which has had 87 entries since 1999.\n\nThe theoretical interpretation of the TDEs was finally described in 2004 by Stephanie Komossa of the Max Plank Institute (MPE / MPG) and again recently in 2014. Let us also mention the work of the teams of Suvi Gezari (2006), Geoffrey C. Bower (2011 ) as well as J. Guillochon and E.Ramirez-Ruiz (2015) and Jane Dai et al. (2018).\n\nIn September 2016, a team from the University of Science and Technology of China in Hefei, Anhui, China, announced that, using data from NASA Wide-field Infrared Survey Explorer, a stellar tidal disruption event was observed at a known black hole. Another team at Johns Hopkins University in Baltimore, Maryland, U.S., detected three additional events. In each case, astronomers hypothesized that the astrophysical jet created by the dying star would emit ultraviolet and X-ray radiation, which would be absorbed by dust surrounding the black hole and emitted as infrared radiation. Not only was this infrared emission detected, but they concluded that the delay between the jet's emission of ultraviolet and X-ray radiation and the dust's emission of infrared radiation may be used to estimate the size of the black hole devouring the star.\n\n\n\n"}
{"id": "1296312", "url": "https://en.wikipedia.org/wiki?curid=1296312", "title": "Torpex", "text": "Torpex\n\nTorpex is a secondary explosive, 50% more powerful than TNT by mass. Torpex comprises 42% RDX, 40% TNT and 18% powdered aluminium. It was used in the Second World War from late 1942. The name is short for \"torpedo explosive\", having been originally developed for use in torpedoes. Torpex proved to be particularly useful in underwater munitions because the aluminium component had the effect of making the explosive pulse last longer, which increased the destructive power. Torpex was used only in critical applications, e.g. torpedoes, depth charges, and the Upkeep, Tallboy, and Grand Slam bombs. It was also used in the Operation Aphrodite drones. Torpex has long been superseded by H6 and PBX compositions. It is therefore regarded as obsolete, so Torpex is unlikely to be encountered except in old munitions or unexploded ordnance.\n\nTorpex was developed at the Royal Gunpowder Factory, Waltham Abbey, in the United Kingdom as a more powerful military alternative to TNT. RDX was developed in 1899. Though very stable and serving as the reference point by which the sensitivity of other explosives are judged, it was too expensive for most military applications and reserved for use in the most important products, such as torpedoes. Aluminium powder was also added to the mix to further enhance the effect. Although both RDX and TNT have a negative oxygen balance, the superheated aluminium component tends to contribute primarily by extending the expansion time of the explosive product gases. Beeswax was also added as a phlegmatizing agent, to reduce sensitivity to shock and impact. Later, beeswax was replaced with paraffin wax, and calcium chloride was added as a moisture absorber to reduce the production of hydrogen gas under high humidity.\n\n\n"}
{"id": "24320869", "url": "https://en.wikipedia.org/wiki?curid=24320869", "title": "Triatomic molecule", "text": "Triatomic molecule\n\nTriatomic molecules are molecules composed of three atoms, of either the same or different chemical elements. Examples include HO, CO (pictured) and HCN.\n\nThe vibrational modes of a triatomic molecule can be determined in specific cases.\n\nA symmetric linear molecule ABA can perform:\nIn the previous formulas, \"M\" is the total mass of the molecule, \"m\" and \"m\" are the masses of the elements A and B, \"k\" and \"k\" are the spring constants of the molecule along its axis and perpendicular to it.\n\nHomonuclear triatomic molecules contain three of the same kind of atom.\n\nOzone, O is an example of a triatomic molecule with all atoms the same. Triatomic hydrogen, H, is unstable and breaks up spontaneously. H, the trihydrogen cation is stable by itself and is symmetric.\nHe, the helium trimer is only weakly bound by van der Waals force and is in a Efimov state. Trisulfur (S) is analogous to ozone.\n\nAll triatomic molecules may be classified as possessing either a linear, bent or cyclic geometry.\nLinear triatomic molecules owe their geometry to their sp or spd hybridised central atoms. Well known linear triatomic molecules include Carbon Dioxide (CO) and Hydrogen Cyanide (HCN). \n\nXenon difluoride (XeF) is one of the rare examples of a linear triatomic molecule possessing non-bonded pairs of electrons on the central atom.\n"}
{"id": "23418473", "url": "https://en.wikipedia.org/wiki?curid=23418473", "title": "Types of snow", "text": "Types of snow\n\nTypes of snow can be designated by the shape of its flakes, description of how it is falling, and by how it collects on the ground. A blizzard and snow storm indicate heavy snowfalls over a large area, snow squalls give heavy snowfalls over narrow bands, while flurries are used for the lightest snowfall. Types which fall in the form of a ball, rather than a flake, are known as graupel, with sleet and snow grains as types of graupel. Once on the ground, snow can be categorized as powdery when fluffy, granular when it begins the cycle of melting and refreezing, and crud or eventually ice once it packs down into a dense drift after multiple melting and refreezing cycles. When powdering, snow drifts with the wind or ground blizzard, sometimes to the depth of several metres. After attaching to hillsides, blown snow can evolve into a snow slab, which is an avalanche hazard on steep slopes.\n\nSnowfall's intensity is determined by visibility. When the visibility is over , snow is determined to be light. Moderate snow describes snowfall with visibility restrictions between and . Heavy snowfall describes conditions when visibility is restricted below .\n\nOther terms for falling snow:\n\n\n\n\n\n"}
{"id": "56028871", "url": "https://en.wikipedia.org/wiki?curid=56028871", "title": "Wide-area damping control", "text": "Wide-area damping control\n\nWide-area damping control (WADC) is a class of automatic control systems used to provide stability augmentation to modern electrical power systems known as smart grids. Actuation for the controller is provided via modulation of capable active or reactive power devices throughout the grid. Such actuators are most commonly previously-existing power system devices, such as high-voltage DC (HVDC) transmission lines and static VAR compensators (SVCs) which serve primary purposes not directly related to the WADC application. However, damping may be achieved with the utilization of other devices installed with the express purpose of stability augmentation, including energy storage technologies. Wide-area instability of a large electrical grid unequipped with a WADC is the result of the loss of generator rotor synchronicity and is typically envisioned as a generator (or group of generators) oscillating with an undamped exponential trajectory as the result of insufficient damping torque.\n\nLarge interconnected power systems are susceptible to generator rotor instability, particularly when disparate machine groups are connected to the system through high impedance transmission lines. Previously unaccounted for load growth, transmission lines operating closer to rated capacity, connecting two previously electrically isolated subsystems by a single transmission line, and increased renewable resource penetration increase the possibility of lightly-damped oscillations. While several causes of resonance exist in electrical grids, inter-area oscillations pose the greatest threat to wide-spread breakup leading to substantial power outages. Two main sources of inter-area modes are identified: 1.) two previously electrically isolated systems which are connected by a single (or several parallel) transmission lines or 2.) increased load and generation in an existing system without increased transmission capability. Both of these conditions continue to be imposed on most large interconnected systems transitioning to the smart grid architecture.\n\nRotor instability phenomena may be studied by considering two different disturbance types: small-signal and transient. \"Small-signal stability\" considers an electric grid subject to \"normal\" operating conditions, while \"transient stability\" studies the ability of the system to retain stability in the event of a large disturbance (e.g. transmission line fault). While many different features of the electrical grid impact rotor stability (e.g. transmission line congestion, power system stabilizer (PSS) settings, etc.), the WADC architecture introduces sufficient torque to quell the negative effects of resonant systems.\n\nSmall-signal rotor stability is the ability of a system to retain synchronicity under ambient perturbation. The system is linearizable under such an assumption, facilitating the application of linear system theory for stability assessment and WADC design. The power transferred between two machines serially connected by impedance formula_1 with sending voltage formula_2 and receiving voltage formula_3 is given by formula_4, where formula_5 is the difference in internal rotor angle of the two machines. Note that to deliver additional power to a load with constant impedance with fixed sending and receiving end voltage, angular separation \"must\" increase. Maximum power is transferred between the machines when formula_6; the two generators lose synchronicity for any angle greater than this value. Nominal operating conditions assume formula_7 to ensure a sufficient margin of stability. An ever increasing load with fixed power system equipment (e.g. transmission line impedances constant) force electrical grid to operate closer to unacceptable rotor angle ranges. This has the effect of diminishing safety margins for the continuous operation of the system, warranting the implementation of a WADC.\n\nThe so-called \"swing equation\" provides the differential relationship between accelerating power (i.e. the difference between mechanical power at the shaft and electrical power delivered) and the rotor angle. Considering a turbine spinning at sufficiently high speed that perturbations in rotational velocity may be temporarily ignored, the swing equation (assuming no rotational friction loss) may be expressed thusly: formula_8, where formula_9 is the inertia constant, formula_10 the nominal system frequency in rad/sec (roughly 377 for a 60 Hz system), formula_11 the electrical power, and formula_12 the mechanical power introduced by the prime mover. The swing equation establishes a second-order differential relationship that may be solved analytically or using the equal-area criterion (EAC) for a single-machine infinite bus (SMIB) system. Transient stability considers contingent events of substantial impact on the system that linearization fails to accurately represent the dynamics of interest, including generator trips and transmission line faults. The result of transient analysis provides an indication of whether or not the generators, when perturbed substantially (i.e. allowed to accelerate/decelerate due to power imbalance), will eventually decelerate/accelerate back to an equilibrium point within reason. When dealing with large multi-machine systems, analytical solution is intractable and stability assessment must be transitioned to a nonlinear numerical integration platform.\n\nTo enhance the rotor stability of a modern electrical grid, various methods to provide damping have been considered for WADCs. High-voltage DC transmission lines, power system stabilizers, wind turbines.., and flexible AC transmission equipment are capable of attenuating the effects of resonant inter-area oscillatory behavior. Methods employing PSSs frequently modify the g enerator's local PSS control loop, adding an additional voltage reference term to the automatic voltage regulator (AVR) circuit. Since nominal PSS control design servos based on perturbations in rotational velocity, sensors in addition to a standard rotary encoder are required.\n\nModulation of active power between several coherent generator sets is a common approach to damping inter-area oscillations. The highest capacity and currently viable actuators are high-voltage DC (HVDC) transmission lines. By modulating the active power shared between converter stations, substantial positive impact may be realized by employing such equipment. HVDC is limited by their quantity of operational lines and difficulty to install new units. Grid-scale batteries have been considered for active power modulation, in addition to HVDC. While providing active power (and hence damping torque) in a similar fashion to HVDC, energy storage devices are limited by capacity and expense. Energy storage units are more geographically flexible and easily installed than HVDC, however. Static VAR compensators (SVCs) and other reactive devices are also used as actuators in wide-area damping control \n\nDue to the geographically disparate nature of actuators, sensor suites must maintain time synchronism. While local Caesium atomic clocks offer the highest accuracy time fidelity, GPS technology allows continued synchronicity of measured control feedback signals as they are sent to the aggregated WADC processing center. Without remote sensing capability, the WADC control scheme is severely limited in damping capability. Phasor measurement units (PMUs) are typically preferred to obtain sufficient fidelity in voltage/current angle measurements. These modern sensors provide sufficiently high reporting rate and minimal measurement error required for high-performance control systems. However, latency and the potential for GPS spoofing provide challenges for the implementation of a WADC with PMUs \n\n"}
