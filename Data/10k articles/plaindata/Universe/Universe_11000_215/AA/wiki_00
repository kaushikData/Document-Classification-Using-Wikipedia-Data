{"id": "27248907", "url": "https://en.wikipedia.org/wiki?curid=27248907", "title": "Aircraft gross weight", "text": "Aircraft gross weight\n\nThe aircraft gross weight (also known as the all-up weight (AUW)) is the total aircraft weight at \"any moment\" during the flight or ground operation.\n\nAn aircraft's gross weight will decrease during a flight due to fuel and oil consumption. An aircraft's gross weight may also vary during a flight due to payload dropping or in-flight refuelling.\n\nAt the moment of releasing its brakes, the gross weight of an aircraft is equal to its takeoff weight. During flight, an aircraft's gross weight is referred to as the \"en-route weight\" or \"in-flight weight\".\n\nAn aircraft's gross weight is limited by several weight restrictions in order to avoid overloading its structure or to avoid unacceptable performance or handling qualities while in operation.\n\nAircraft gross weight limits are established during an aircraft's design and certification period and are laid down in the aircraft's type certificate and manufacturer specification documents.\n\nThe absolute maximum weight capabilities of a given aircraft are referred to as the \"structural weight limits\".\nThe structural weight limits are based on aircraft maximum structural capability and define the envelope for the CG charts(both maximum weight and CG limits).\n\nAn aircraft's structural weight capability is typically a function of when the aircraft was manufactured, and in some cases, old aircraft can have their structural weight capability increased by structural modifications.\n\nThe maximum design taxi weight (also known as the maximum design ramp weight (MDRW)) is the maximum weight certificated for aircraft manoeuvring on the ground (taxiing or towing) as limited by aircraft strength and airworthiness requirements.\n\nIs the maximum certificated design weight when the brakes are released for takeoff and is the greatest weight for which compliance with the relevant structural and engineering requirements has been demonstrated by the manufacturer.\n\nThe maximum certificated design weight at which the aircraft meets the appropriate landing certification requirements. It generally depends on the landing gear strength or the landing impact loads on certain parts of the wing structure.\n\nThe MDLW must not exceed the MDTOW.\n\nThe maximum landing weight is typically designed for 10 feet per second (600 feet per minute) sink rate at touch down with no structural damage.\n\nThe maximum certificated design weight of the aircraft less all usable fuel and other specified usable agents (engine injection fluid, and other consumable propulsion agents). It is the maximum weight permitted before usable fuel and other specified usable fluids are loaded in specified sections of the airplane. The MDZFW is limited by strength and airworthiness requirements. At this weight, the subsequent addition of fuel will not result in the aircraft design strength being exceeded. The weight difference between the MDTOW and the MDZFW may be utilised only for the addition of fuel.\n\nMinimum certificated weight for flight as limited by aircraft strength and airworthiness requirements.\n\nAircraft \"authorised\" gross weight limits (also referred to as \"certified\" weight limits) are laid down in the aircraft flight manuals (AFM) and/or associated certificate of airworthiness (C of A). The authorised or permitted limits may be equal to or lower than the structural design weight limits.\n\nThe authorised weight limits that can legally be used by an operator or airline are those listed in the AFM and the weight and balance manual.\n\nThe authorised (or certified) weight limits are chosen by the customer/airline and they are referred to as the \"purchased weights\". An operator may purchase a certified weight below the maximum design weights because many of the airport operating fees are based on the aircraft AFM maximum allowable weight values. An aircraft purchase price is, typically, a function of the certified weight purchased.\n\nMaximum weights established, for each aircraft, by design and certification must not be exceeded during aircraft operation (ramp or taxying, takeoff, en-route flight, approach, and landing) and during aircraft loading (zero fuel conditions, centre of gravity position, and weight distribution).\n\nWeights could be restricted on some type of aircraft depending on the aircraft handling requirements; for example aerobatic aircraft, where certain aerobatic manoeuvres can only be executed with a limited gross weight.\n\nIn addition, the authorised maximum weight limits may be less as limited by centre of gravity, fuel density, and fuel loading limits.\n\nThe maximum taxi weight (MTW) (also known as the maximum ramp weight (MRW) is the maximum weight authorized for maneuvering (taxiing or towing) an aircraft on the ground as limited by aircraft strength and airworthiness requirements. It includes the weight of taxi and run-up fuel for the engines and the APU.\n\nIt is greater than the maximum takeoff weight due to the fuel that will be burned during the taxi and runup operations.\n\nThe difference between the maximum taxi/ramp weight and the maximum take-off weight (maximum taxi fuel allowance) depends on the size of the aircraft, the number of engines, APU operation, and engines/APU fuel consumption, and is typically assumed for 10 to 15 minutes allowance of taxi and run-up operations.\n\nThe maximum takeoff weight (also known as the \"maximum brake-release weight\") is the maximum weight authorised at brake release for takeoff, or at the start of the takeoff roll.\n\nThe maximum takeoff weight is always less than the maximum taxi/ramp weight to allow for fuel burned during taxi by the engines and the APU.\n\nIn operation, the maximum weight for takeoff may be limited to values less than the maximum takeoff weight due to aircraft performance, environmental conditions, airfield characteristics (takeoff field length, altitude), maximum tire speed and brake energy, obstacle clearances, and/or en route and landing weight requirements.\n\nThe maximum weight authorised for normal landing of an aircraft.\n\nThe MLW must not exceed the MTOW.\n\nThe operation landing weight may be limited to a weight lower than the Maximum Landing Weight by the most restrictive of the following requirements:\n\n\nIf the flight has been of short duration, fuel may have to be jettisoned to reduce the landing weight.\n\nOverweight landings require a structural inspection or evaluation of the touch-down loads before the next aircraft operation.\n\nThe maximum permissible weight of the aircraft less all usable fuel and other specified usable agents (engine injection fluid, and other consumable propulsion agents). It is the maximum weight permitted before usable fuel and other specified usable fluids are loaded in specified sections of the airplane.\n\n\n"}
{"id": "44993509", "url": "https://en.wikipedia.org/wiki?curid=44993509", "title": "Alex Epstein (American writer)", "text": "Alex Epstein (American writer)\n\nAlexander Joseph Epstein (; born August 1, 1980) is an American author, energy theorist, and industrial policy pundit. He is the founder and president of the Center for Industrial Progress, a for-profit think tank located in San Diego, California. Epstein is also the \"New York Times\" bestselling author of \"The Moral Case for Fossil Fuels\" (2014), in which he advocates the use of fossil fuels like coal, oil and natural gas, for which he has been criticized. Epstein is an adjunct scholar at the Cato Institute and a former fellow at the Ayn Rand Institute.\n\nEpstein grew up in Chevy Chase, Maryland and attended Montgomery County Public Schools. In childhood his favorite subjects were mathematics and science, and in high school he became interested in politics and humanities. He cites Ayn Rand as his greatest influence, having been especially impressed by her novel \"Atlas Shrugged\". Among his other favorite writers is Thomas Sowell.\n\nFrom 1998 to 2002, Epstein earned a Bachelor of Arts degree in philosophy from Duke University, where he was the editor and publisher of \"The Duke Review\" for two years and where he also studied computer science.\n\nAfter college, Epstein knew that he wanted to be a professional intellectual but also that he did not want to go to graduate school or to work at a university. He became a freelance writer, and two years later joined the Ayn Rand Institute, a non-profit organization in Irvine, California that promotes Ayn Rand's novels and her philosophy of Objectivism. Epstein was a writer and fellow at the Ayn Rand Institute between 2004 and 2011, where he specialized in business issues.\n\nIn 2011, Epstein founded the Center for Industrial Progress (CIP), an advocacy group and think tank whose mission is \"to bring about a new industrial revolution.\"\n\nIn 2012, Epstein debated American environmentalist Bill McKibben while representing CIP at an event held at Duke University.\n\nIn 2013, \"Rolling Stone\" placed Epstein and the Center for Industrial Progress on their list of top Global Warming Deniers. Epstein wrote a rebuttal to the piece in \"Forbes\", in which he refutes \"Rolling Stone\"s characterization of his views and mocks the piece for including scientifically inaccurate information. In his rebuttal, Epstein also criticizes the term global warming \"denier\", which he claims is a smear tactic intended to liken critics of environmentalism to Holocaust deniers.\n\nIn 2014, Epstein and CIP publicly supported the Keystone Pipeline.\n\nIn 2015, \"The Guardian\" published an opinion piece by Jason Wilson critical of Epstein and CIP, stating, \"Epstein's work has been popular and influential on the right because it is a particularly fluent, elaborate form of climate denialism. The CIP [\"sic\"] prides itself on being able to train corporate leaders to 'successfully outmessage \"environmentalists\"'.\" He also criticizes Epstein for being an \"ideologue\" funded by petrochemical billionaires, the Koch brothers, though CIP is for-profit and thus does not receive funds from anyone, including the Kochs.\n\nIn 2016, Epstein testified before the Senate Environment and Public Works Committee at the invitation of the committee's chairman, James Inhofe (R-OK), who has called climate change a \"hoax.\" Epstein suggested that rising carbon dioxide levels \"benefit plants and Americans.\" When questioned by committee member Barbara Boxer as to why Epstein, who self-identifies as a philosopher, was even there, Epstein responded, \"to teach you how to think clearly.\" Boxer replied that she doesn't \"appreciate being lectured by a philosopher and not a scientist.\"\n\nEpstein has contributed to several media outlets regarding climate and energy issues, including \"USA Today\", \"The Wall Street Journal\", and Fox News. Epstein and CIP challenge the belief that the consumption of fossil fuels harms human life, arguing that recent gains in public health and safety were achieved not in spite of mankind's reliance on carbon energy but, in large measure, because of it.\n\n\n"}
{"id": "995719", "url": "https://en.wikipedia.org/wiki?curid=995719", "title": "Burton process", "text": "Burton process\n\nThe Burton process is a thermal cracking process invented by William Merriam Burton and Robert E. Humphreys, each of whom held a Ph.D. in chemistry from Johns Hopkins University. The process they developed is often called the Burton process. More fairly, though, it should be known as the Burton-Humphreys process, since both men played key roles in its development. This issue was settled in court, although the decision gave primary recognition to Burton.\n\nThe process uses the destructive distillation of crude oil heated in a still under pressure. In this revolutionary still, different products emerging from a bubble tower at different temperatures and pressures. Of critical importance, the use of these stills more than doubled the production of gasoline from most kinds of oil. The first large-scale use of these towers began with the decision by Standard Oil of Indiana to build 120 stills for an appropriation of $709,000, authorized in 1911. This decision was taken just as the US Supreme Court ordered the dissolution of the Standard Oil Trust.\n\nThis thermal cracking process was patented on January 7, 1913 (Patent No. 1,049,667). The first thermal cracking method, the Shukhov cracking process, was invented by Vladimir Shukhov (Patent of Russian empire No. 12926 on November 27, 1891). While the Russians contended that the Burton process was essentially a slight modification of the Shukhov process, Americans refused to concede and the Burton-Humphreys patent remained in use. Ultimately, it contributed to the development of petrochemicals.\n\nIn 1937 the Burton process was superseded by catalytic cracking, but it is still in use today to produce diesel.\n\n"}
{"id": "1902819", "url": "https://en.wikipedia.org/wiki?curid=1902819", "title": "Casing hanger", "text": "Casing hanger\n\nIn petroleum production, the casing hanger is that portion of a wellhead assembly which provides support for the casing string when it is lowered into the wellbore. It serves to ensure that the casing is properly located. When the casing string has been run into the wellbore it is hung off, or suspended, by a casing hanger, which rests on a landing shoulder inside the casing spool. Casing hangers must be designed to take the full weight of the casing, and provide a seal between the casing hanger and the spool.\n\nCasing Hangers may also be suspended within the wellhead by means of radial distortion of the wellhead bore e.g. the \"Pos-Grip\" method. \n\nThis is installed to support the individual casing strings in the well. It is the landing base or the casing head. This is usually welded or screwed to the top of the surface casing string. The surface casing serves as a foundation pile for the well which transfers the hanging load to the earth. The casing head is prepared with a bowl into which the slips sit and chuck the casing in place. Most casing heads allow for the pressure readings to be taken on the annulus and provide the means to pump out or into if necessary. The top of the casing string and annulus is usually sealed.\n\nThe most common size of casing hanger is 13-3/8\" with over 155,000 such units installed worldwide in 2014 alone.\n\nThe contraction of the oilfield industry throughout 2015 meant that this figure was reduced somewhat to just under 120,000 units, of which 36,822 were \"Pos-Grip\" casing hangers.\n\n\n"}
{"id": "782162", "url": "https://en.wikipedia.org/wiki?curid=782162", "title": "Cone (topology)", "text": "Cone (topology)\n\nIn topology, especially algebraic topology, the cone \"CX\" of a topological space \"X\" is the quotient space:\n\nof the product of \"X\" with the unit interval \"I\" = [0, 1]. \nIntuitively, this construction makes \"X\" into a cylinder and collapses one end of the cylinder to a point.\n\nIf formula_2 is a compact subspace of Euclidean space, the cone on formula_2 is homeomorphic to the union of segments from formula_2 to any fixed point formula_5 such that these segments intersect only by formula_6 itself. That is, the topological cone agrees with the geometric cone for compact spaces when the latter is defined. However, the topological cone construction is more general.\n\nHere we often use geometric cone (defined in the introduction) instead of the topological one. The considered spaces are compact, so we get the same result up to homeomorphism.\n\nis the curved surface of the solid cone:\n\nAll cones are path-connected since every point can be connected to the vertex point. Furthermore, every cone is contractible to the vertex point by the homotopy\n\nThe cone is used in algebraic topology precisely because it embeds a space as a subspace of a contractible space.\n\nWhen \"X\" is compact and Hausdorff (essentially, when \"X\" can be embedded in Euclidean space), then the cone \"CX\" can be visualized as the collection of lines joining every point of \"X\" to a single point. However, this picture fails when \"X\" is not compact or not Hausdorff, as generally the quotient topology on \"CX\" will be finer than the set of lines joining \"X\" to a point.\n\nThe map formula_9 induces a functor formula_10 on the category of topological spaces Top. If formula_11is a continuous map, then formula_12is defined by formula_13, where square brackets denote equivalence classes.\n\nIf formula_14 is a pointed space, there is a related construction, the reduced cone, given by \n\nwhere we take the basepoint of the reduced cone to be the equivalence class of formula_16. With this definition, the natural inclusion formula_17 becomes a based map. This construction also gives a functor, from the category of pointed spaces to itself.\n\n\n"}
{"id": "7249", "url": "https://en.wikipedia.org/wiki?curid=7249", "title": "Crankshaft", "text": "Crankshaft\n\nA crankshaft—related to \"crank\"—is a mechanical part able to perform a conversion between reciprocating motion and rotational motion. In a reciprocating engine, it translates reciprocating motion of the piston into rotational motion; whereas in a reciprocating compressor, it converts the rotational motion into reciprocating motion. In order to do the conversion between two motions, the crankshaft has \"crank throws\" or \"crankpins\", additional bearing surfaces whose axis is offset from that of the crank, to which the \"big ends\" of the connecting rods from each cylinder attach.\n\nIt is typically connected to a flywheel to reduce the pulsation characteristic of the four-stroke cycle, and sometimes a torsional or vibrational damper at the opposite end, to reduce the torsional vibrations often caused along the length of the crankshaft by the cylinders farthest from the output end acting on the torsional elasticity of the metal.\n\nThe earliest hand-operated cranks appeared in China during the Han Dynasty (202 BC-220 AD). They were used for silk-reeling, hemp-spinning, for the agricultural winnowing fan, in the water-powered flour-sifter, for hydraulic-powered metallurgic bellows, and in the well windlass. The rotary winnowing fan greatly increased the efficiency of separating grain from husks and stalks. However, the potential of the crank of converting circular motion into reciprocal motion never seems to have been fully realized in China, and the crank was typically absent from such machines until the turn of the 20th century.\n\nA crank in the form of an eccentrically-mounted handle of the rotary handmill appeared in 5th century BC Celtiberian Spain and ultimately spread across the Roman Empire. A Roman iron crank dating to the 2nd century AD was excavated in Augusta Raurica, Switzerland. The crank-operated Roman mill is dated to the late 2nd century. \n\nEvidence for the crank combined with a connecting rod appears in the Hierapolis mill, dating to the 3rd century; they are also found in stone sawmills in Roman Syria and Ephesus dating to the 6th century. The pediment of the Hierapolis mill shows a waterwheel fed by a mill race powering via a gear train two frame saws which cut blocks by the way of some kind of connecting rods and cranks. The crank and connecting rod mechanisms of the other two archaeologically-attested sawmills worked without a gear train. Water-powered marble saws in Germany were mentioned by the late 4th century poet Ausonius; about the same time, these mill types seem also to be indicated by Gregory of Nyssa from Anatolia.\n\nA rotary grindstone operated by a crank handle is shown in the Carolingian manuscript \"Utrecht Psalter\"; the pen drawing of around 830 goes back to a late antique original. Cranks used to turn wheels are also depicted or described in various works dating from the tenth to thirteenth centuries. \n\nThe first depictions of the compound crank in the carpenter's brace appear between 1420 and 1430 in northern European artwork. The rapid adoption of the compound crank can be traced in the works of an unknown German engineer writing on the state of military technology during the Hussite Wars: first, the connecting-rod, applied to cranks, reappeared; second, double-compound cranks also began to be equipped with connecting-rods; and third, the flywheel was employed for these cranks to get them over the 'dead-spot'. The concept was much improved by the Italian engineer and writer Roberto Valturio in 1463, who devised a boat with five sets, where the parallel cranks are all joined to a single power source by one connecting-rod, an idea also taken up by his compatriot Italian painter Francesco di Giorgio.\n\nThe crank had become common in Europe by the early 15th century, as seen in the works of the military engineer Konrad Kyeser (1366–after 1405). Devices depicted in Kyeser's \"Bellifortis\" include cranked windlasses for spanning siege crossbows, cranked chain of buckets for water-lifting and cranks fitted to a wheel of bells. Kyeser also equipped the Archimedes' screws for water-raising with a crank handle, an innovation which subsequently replaced the ancient practice of working the pipe by treading. The earliest evidence for the fitting of a well-hoist with cranks is found in a miniature of c. 1425 in the German \"Hausbuch of the Mendel Foundation\".\n\nPisanello painted a piston-pump driven by a water-wheel and operated by two simple cranks and two connecting-rods. The Italian physician Guido da Vigevano (c. 1280−1349), planning for a new crusade, made illustrations for a paddle boat and war carriages that were propelled by manually turned compound cranks and gear wheels. The \"Luttrell Psalter\", dating to around 1340, describes a grindstone which was rotated by two cranks, one at each end of its axle; the geared hand-mill, operated either with one or two cranks, appeared later in the 15th century.\n\nMedieval cranes were occasionally powered by cranks, although more often by windlasses.\n\nThe 15th century also saw the introduction of cranked rack-and-pinion devices, called cranequins, which were fitted to the crossbow's stock as a means of exerting even more force while spanning the missile weapon (see right). In the textile industry, cranked reels for winding skeins of yarn were introduced.\n\nAround 1480, the early medieval rotary grindstone was improved with a treadle and crank mechanism. Cranks mounted on push-carts first appear in a German engraving of 1589. Crankshafts were also described by Leonardo da Vinci (1452–1519) and a Dutch farmer and windmill owner by the name Cornelis Corneliszoon van Uitgeest in 1592. His wind-powered sawmill used a crankshaft to convert a windmill's circular motion into a back-and-forward motion powering the saw. Corneliszoon was granted a patent for his crankshaft in 1597.\n\nFrom the 16th century onwards, evidence of cranks and connecting rods integrated into machine design becomes abundant in the technological treatises of the period: Agostino Ramelli's \"The Diverse and Artifactitious Machines\" of 1588 depicts eighteen examples, a number that rises in the \"Theatrum Machinarum Novum\" by Georg Andreas Böckler to 45 different machines. Cranks were formerly common on some machines in the early 20th century; for example almost all phonographs before the 1930s were powered by clockwork motors wound with cranks. Reciprocating piston engines use cranks to convert the linear piston motion into rotational motion. Internal combustion engines of early 20th century automobiles were usually started with hand cranks, before electric starters came into general use. The 1918 Reo owner's manual describes how to hand crank the automobile:\n\n\nAl-Jazari (1136–1206) described a crank and connecting rod system in a rotating machine in two of his water-raising machines. His twin-cylinder pump incorporated a crankshaft, including both the crank and shaft mechanisms.\n\nLarge engines are usually multicylinder to reduce pulsations from individual firing strokes, with more than one piston attached to a complex crankshaft. Many small engines, such as those found in mopeds or garden machinery, are single cylinder and use only a single piston, simplifying crankshaft design.\n\nA crankshaft is subjected to enormous stresses, potentially equivalent of several tonnes of force. The crankshaft is connected to the fly-wheel (used to smooth out shock and convert energy to torque), the engine block, using bearings on the main journals, and to the pistons via their respective con-rods. An engine loses up to 75% of its generated energy in the form of friction, noise and vibration in the crankcase and piston area. The remaining losses occur in the valvetrain (timing chains, belts, pulleys, camshafts, lobes, valves, seals etc.) heat and blow by.\n\nThe crankshaft has a linear axis about which it rotates, typically with several bearing journals riding on replaceable bearings (the main bearings) held in the engine block. As the crankshaft undergoes a great deal of sideways load from each cylinder in a multicylinder engine, it must be supported by several such bearings, not just one at each end. This was a factor in the rise of V8 engines, with their shorter crankshafts, in preference to straight-8 engines. The long crankshafts of the latter suffered from an unacceptable amount of flex when engine designers began using higher compression ratios and higher rotational speeds. High performance engines often have more main bearings than their lower performance cousins for this reason.\n\nThe distance the axis of the crank throws from the axis of the crankshaft determines the piston stroke measurement, and thus engine displacement. A common way to increase the low-speed torque of an engine is to increase the stroke, sometimes known as \"shaft-stroking.\" This also increases the reciprocating vibration, however, limiting the high speed capability of the engine. In compensation, it improves the low speed operation of the engine, as the longer intake stroke through smaller valve(s) results in greater turbulence and mixing of the intake charge. Most modern high speed production engines are classified as \"over square\" or short-stroke, wherein the stroke is less than the diameter of the cylinder bore. As such, finding the proper balance between shaft-stroking speed and length leads to better results.\n\nThe configuration, meaning the number of pistons and their placement in relation to each other leads to straight, V or flat engines. The same basic engine block can sometimes be used with different crankshafts, however, to alter the firing order. For instance, the 90° V6 engine configuration, in older days sometimes derived by using six cylinders of a V8 engine with a 3 throw crankshaft, produces an engine with an inherent pulsation in the power flow due to the \"gap\" between the firing pulses alternates between short and long pauses because the 90 degree engine block does not correspond to the 120 degree spacing of the crankshaft. The same engine, however, can be made to provide evenly spaced power pulses by using a crankshaft with an individual crank throw for each cylinder, spaced so that the pistons are actually phased 120° apart, as in the GM 3800 engine. While most production V8 engines use four crank throws spaced 90° apart, high-performance V8 engines often use a \"flat\" crankshaft with throws spaced 180° apart, essentially resulting in two straight four engines running on a common crankcase. The difference can be heard as the flat-plane crankshafts result in the engine having a smoother, higher-pitched sound than cross-plane (for example, IRL IndyCar Series compared to NASCAR Sprint Cup Series, or a Ferrari 355 compared to a Chevrolet Corvette). This type of crankshaft was also used on early types of V8 engines. See the main article on crossplane crankshafts.\n\nFor some engines it is necessary to provide counterweights for the reciprocating mass of each piston and connecting rod to improve engine balance. These are typically cast as part of the crankshaft but, occasionally, are bolt-on pieces. While counter weights add a considerable amount of weight to the crankshaft, it provides a smoother running engine and allows higher RPM levels to be reached.\n\nIn some engine configurations, the crankshaft contains direct links between adjacent crank pins, without the usual intermediate main bearing. These links are called \"flying arms\". This arrangement is sometimes used in V6 and V8 engines, as it enables the engine to be designed with different V angles than what would otherwise be required to create an even firing interval, while still using fewer main bearings than would normally be required with a single piston per crankthrow. This arrangement reduces weight and engine length at the expense of less crankshaft rigidity.\n\nSome early aircraft engines were a rotary engine design, where the crankshaft was fixed to the airframe and instead the cylinders rotated with the propeller.\n\nThe radial engine is a reciprocating type internal combustion engine configuration in which the cylinders point outward from a central crankshaft like the spokes of a wheel. It resembles a stylized star when viewed from the front, and is called a \"star engine\" (German Sternmotor, French Moteur en étoile) in some languages. The radial configuration was very commonly used in aircraft engines before turbine engines became predominant.\n\nCrankshafts can be monolithic (made in a single piece) or assembled from several pieces. Monolithic crankshafts are most common, but some smaller and larger engines use assembled crankshafts.\n\nCrankshafts can be forged from a steel bar usually through roll forging or cast in ductile steel. Today more and more manufacturers tend to favor the use of forged crankshafts due to their lighter weight, more compact dimensions and better inherent damping. With forged crankshafts, vanadium microalloyed steels are mostly used as these steels can be air cooled after reaching high strengths without additional heat treatment, with exception to the surface hardening of the bearing surfaces. The low alloy content also makes the material cheaper than high alloy steels. Carbon steels are also used, but these require additional heat treatment to reach the desired properties. Cast iron crankshafts are today mostly found in cheaper production engines (such as those found in the Ford Focus diesel engines) where the loads are lower. Some engines also use cast iron crankshafts for low output versions while the more expensive high output version use forged steel.\n\nCrankshafts can also be machined out of a billet, often a bar of high quality vacuum remelted steel. Though the fiber flow (local inhomogeneities of the material's chemical composition generated during casting) doesn’t follow the shape of the crankshaft (which is undesirable), this is usually not a problem since higher quality steels, which normally are difficult to forge, can be used. These crankshafts tend to be very expensive due to the large amount of material that must be removed with lathes and milling machines, the high material cost, and the additional heat treatment required. However, since no expensive tooling is needed, this production method allows small production runs without high costs.\n\nIn an effort to reduce costs, used crankshafts may also be machined. A good core may often be easily reconditioned by a crankshaft grinding process. Severely damaged crankshafts may also be repaired with a welding operation, prior to grinding, that utilizes a submerged arc welding machine. To accommodate the smaller journal diameters a ground crankshaft has, and possibly an over-sized thrust dimension, undersize engine bearings are used to allow for precise clearances during operation.\n\nMachining or remanufacturing crankshafts are precision machined to exact tolerances with no odd size crankshaft bearings or journals. Thrust surfaces are micro-polished to provide precise surface finishes for smooth engine operation and reduced thrust bearing wear. Every journal is inspected and measured with critical accuracy. After machining, oil holes are chamfered to improve lubrication and every journal polished to a smooth finish for long bearing life. Remanufactured crankshafts are thoroughly cleaned with special emphasis to flushing and brushing out oil passages to remove any contaminants. Remanufacturing a crankshaft typically involves the following steps:\n\nMachine shops soak the crankshafts in a hot tank and power wash the overall shaft. The oil holes are then wire brushed. A magnetic particle inspection checks for cracks. The crankshaft is magnetized and sprayed with an iron oxide powder which, under blacklight conditions, makes any imperfections visible. \n\nThe counterweights are removed, cleaned, and checked for cracks and tightness. The bolts of loose counterweights are replaced. The counterweights are then installed back into the crankshafts, which are inspected for damage. A machinist determines the size and hardness of the journals and mains. \n\nA technician checks the keyway, nose, and bolt holes, then seals the surface for non-conformities. Bolt holes are typically tapped ½” or less on remanufactured crankshafts. The bearings and the straightness of the crankshaft are also inspected, and it must be re-straightened with an industrial straightening machine if not up to OEM standards. The straightening machine determines how many dials are out of line. To re-straighten the shaft, technicians heat the crankshaft to 500-600 degrees. Any more than 700 degrees takes the hardness out of the shaft. \n\nThe magnetic particle inspection is repeated after the crankshaft has been straightened.\n\nThe counterweights and webbing are stamped in proper firing order (alpha if numeric and vice versa). Technicians then stamp the employee ID#, Work Order # and date on #1 rod webbing. Stamping this information on the rod webbing helps keep the quality control process order in case of future issues during the manufacturing process.\n\nTechnicians undercut the rod or journals to eliminate wear before buildup. Further buildup and corrosion is typically prevented via thermal spraying, which involves protrusion of molten particles onto the heated metallic surface, forming a smooth coating interwoven into the structure. Typically, boron alloys are used for this process, as they are very dense, hard, and oxide-free. They prevent against abrasive materials that cause divots, scratches, and cracks, and prevent surface erosion and corrosion.\n\nThe welding process for re-manufactured crankshafts is called submerged arc welding. It is a powdered flux plus a weld which combines to produce a more precise weld. The most common flux powder used is called #1 Flux 2245 HD. This powder eliminates the need for technicians to wear weld masking and reduces the amount of dust by-product.\n\nThe crankshaft is once more heated to 500-600 degrees to relieve structural stress. Its overall straightness is also rechecked. If the re-manufactured crankshaft is out of alignment, then the technician must re-straighten the structure. \n\nCrankshaft grinding involves rough grinding the excess material from the rod or journals. On the rod there are various mains that need to be reground to proper OEM specifications. These rods are spun grind to the next under-size using the pultrusion crankshaft grinding machine. Rod mains are ground inside and outside. \n\nThe technician then performs a finished crankshaft grinding procedure, which is a more precise grind which reaches the correct OEM specifications. Before the technician starts the crankshaft grinding they should see what crankshaft bearings are available and start from there. For example, the OEM specification for a Caterpillar 3306 Rod is 2.9987” – 3.0003”. Top industrial crankshaft grinding technicians always stop at the high end of the tolerance level. \n\nShot peening adds an additional layer of hardness to the re-manufactured crankshaft.\n\nThe counterweights are replaced in proper firing order. Either the new counterweights are installed or the old counterweight bolts are re-tightened.\n\nThe machine shop then determines if the proper rotational balance of the re-manufactured crankshafts is achieved. In the engine, the crankshaft, pistons, and rods are all in a constant rotation. The counterweights are designed to offset the weight of the rod and the pistons. When in motion, the kinetic energy and the sum of all forces should be equal to zero on all moving parts. If the counterweights are imbalanced, it adds additional stress on other components of the engine. The technician must ensure that the internal and external balance of the crankshaft counterweights are properly aligned.\n\nThe technician micro-polishes each of the rebuilt crankshafts by hand. To further refine the crankshaft grinding process the machinist makes the most precise fit by micro-polishing the component with a 600-grit emery cloth. Through micro-polishing and industrial crankshaft grinding, the machine shop achieves the recommended Rockwell hardness and Ra finish (Roughness Parameter).\n\nIndustry standard crankshaft hardness is 40 on the Rockwell hardness scale. A 45-50 rating is what most reputable machine shops try to employ for all remanufactured crankshafts. Typically, hardness can be reduced if the engine is out of oil or the journal is spun incorrectly.\n\nQuality control inspects all of the finished crankshafts mistakes. A typical quality control department uses separate testing and analytical measurement tools from the technicians to ensure accuracy. The remanufactured crankshafts are then rustproofed using Cosmoline and packaged in damage-proof coverings.\n\nMicrofinishing is a method of finishing the surface of the crankshaft in such a manner that microscopic roughness, pitting or cracking is reduced to a smooth, integral surface. Crankshafts fail by fatigue cracking and cracks start at the most highly stressed point in the material, which is the surface. Once a crack has developed, it increases the local stress in the area at its 'V' point, which slowly increases the size of the crack. The objective of microfinishing is to reduce to the smallest number and size any deviation in the surface and thus minimize the opportunity for cracks to develop. Increased requirements on microfinishing has become a common requirement as Industries are moving towards lead-free bearings. Improved performance and longevity are associated with the microfinishing requirements.\n\nThe fatigue strength of crankshafts is usually increased by using a radius at the ends of each main and crankpin bearing. The radius itself reduces the stress in these critical areas, but since the radius in most cases is rolled, this also leaves some compressive residual stress in the surface, which prevents cracks from forming.\n\nMost production crankshafts use induction hardened bearing surfaces, since that method gives good results with low costs. It also allows the crankshaft to be reground without re-hardening. But high performance crankshafts, billet crankshafts in particular, tend to use nitridization instead. Nitridization is slower and thereby more costly, and in addition it puts certain demands on the alloying metals in the steel to be able to create stable nitrides. The advantage of nitridization is that it can be done at low temperatures, it produces a very hard surface, and the process leaves some compressive residual stress in the surface, which is good for fatigue properties. The low temperature during treatment is advantageous in that it doesn’t have any negative effects on the steel, such as annealing. With crankshafts that operate on roller bearings, the use of carburization tends to be favored due to the high Hertzian contact stresses in such an application. Like nitriding, carburization also leaves some compressive residual stresses in the surface.\n\nSome expensive, high performance crankshafts also use heavy-metal counterweights to make the crankshaft more compact. The heavy-metal used is most often a tungsten alloy but depleted uranium has also been used. A cheaper option is to use lead, but compared with tungsten its density is much lower.\n\nThe shaft is subjected to various forces but generally needs to be analysed in two positions. Firstly, failure may occur at the position of maximum bending; this may be at the centre of the crank or at either end. In such a condition the failure is due to bending and the pressure in the cylinder is maximal. Second, the crank may fail due to twisting, so the conrod needs to be checked for shear at the position of maximal twisting. The pressure at this position is the maximal pressure, but only a fraction of maximal pressure.\"\n\nIn a conventional piston-crank arrangement in an engine or compressor, a piston is connected to a crankshaft by a connecting rod. As the piston moves through its stroke, the connecting rod varies its angle to the direction of motion of the piston and as the connecting rod is free to rotate at its connection to both the piston and crankshaft, no torque is transmitted by the connecting rod and forces transmitted by the connecting rod are transmitted along the longitudinal axis of the connecting rod. The force exerted by the piston on the connecting rod results in a reaction force exerted by the connecting rod back on the piston. When the connecting rod makes an angle to the direction of motion of the piston, the reaction force exerted by the connecting rod on the piston has a lateral component. This lateral force pushes the piston sideways against the cylinder wall. As the piston moves within the cylinder, this lateral force causes additional friction between the piston and cylinder wall. Friction accounts for approximately 20% of all losses in an internal combustion engine, of which approximately 50% is due to piston cylinder friction \n\nIn a paired counter-rotating crankshaft arrangement, each piston is connected to two crankshafts so lateral forces due to the angle of the connecting rods cancel each other out. This reduces piston-cylinder friction and therefore fuel consumption. The symmetrical arrangement reduces the requirement for counterweights, reducing overall mass and making it easier for the engine to accelerate and decelerate. It also eliminates engine rocking and torque effects. Several counter-rotating crankshaft arrangements have been patented, for example US2010/0263621. An early example of a counter-rotating crankshaft arrangement is the Lanchester flat-twin engine. \n\n\n\n"}
{"id": "25682420", "url": "https://en.wikipedia.org/wiki?curid=25682420", "title": "December 2000 nor'easter", "text": "December 2000 nor'easter\n\nThe December 2000 nor'easter was a significant winter storm that impacted the Mid-Atlantic and New England regions of the United States around the end of the month. It began as an Alberta clipper that moved southeastward through the central United States and weakened over the Ohio Valley. However, it redeveloped off the coast of North Carolina and moved northward as it intensified. It moved into central Long Island and eventually tracked northward into New England. The storm dropped heavy precipitation throughout the Northeast, especially in northern New Jersey and eastern New York, where snowfall often exceeded . Even so, as it struck on a weekend, its effects were generally minor and mostly limited to travel delays, traffic accidents, and business closures.\n\nThe storm developed as an Alberta clipper-type low pressure area that moved southeastward across the Great Plains and Midwest. Throughout North Dakota, Minnesota and Iowa, moderate snowfall accompanied the system. After weakening over the Ohio Valley, the storm redeveloped off the coast of North Carolina. In the days preceding the event, a cyclone over eastern Canada circulated cold air southward. Computer models indicated the potential for a major storm up to seven days in advance; however, initially, certain forecasts suggested that a separate storm over the southern U.S. would merge with the clipper. Instead, this feature moved out to sea.\n\nAs the secondary storm began to intensify offshore, precipitation rapidly expanded and tracked northward towards southern Virginia, eastern Maryland and Delaware, largely in the form of freezing rain, ice, and snow. From 0200 UTC on December 30 to 1200 UTC, the cyclone intensified by 13 mbar, and continued to deepen for several more hours. Snowfall reached southern New Jersey between 0600 and 0800 UTC, and New York City at around 1000 UTC. Thundersnow developed within heavy bands in some areas. The significant snow was characterized by a sharp western cutoff; for example, in Chester County, accumulations ranged from around 6 inches in the far eastern parts of the county to only an inch along its western border with Lancaster County.\n\nWhile located off the New Jersey coast, the storm stopped strengthening and slowly moved northward. The center was situated near central Long Island at 2100 UTC. Across eastern Long Island and parts of eastern New England, snow mixed with and, in some cases, changed over to rain. The surface low had moved into eastern Connecticut by early on December 31. As it continued to head northeastward, a new center of low pressure developed near Boston and moved towards coastal Maine. The storm system had abated by January 1.\n\nThe storm produced moderate to heavy snowfall from eastern Pennsylvania through New Jersey, New York, and New England, extending as far north as Maine. As much as fell west of the New York City metropolitan region, which generally reported of snow, making it the biggest snowstorm at the time since the North American blizzard of 1996. Washington, D.C. and Baltimore recorded little or no snowfall, while impacted Philadelphia. Eastern New York, especially the Hudson Valley and Catskill Mountains, western Connecticut, western and central Massachusetts, Vermont, New Hampshire and Maine also picked up heavy snowfall. Eastern New England received up to before dry air from the south reduced the duration of the snow.\n\nAbout 30 flights were canceled at the Philadelphia International Airport, and the city declared a snow emergency. Amtrak canceled Metroliner service along the northeast corridor from Washington, D.C. to New York City. Numerous businesses throughout eastern Pennsylvania closed on December 30, although because the storm struck on a weekend, the number of traffic accidents was fairly low. \n\nIn New Jersey, increasing winds caused blowing and drifting of the snow and led to near-blizzard conditions. Behind the storm, very cold and gusty weather lingered. New Jersey Transit shut down bus service in northern portions of the state, and rail lines had 20-minute delays. Most injuries in the state were related to physical strain while shoveling, snow blower accidents or slips and falls. As the snowfall was of a light nature, few trees limbs and electrical wires were downed by the storm. GPU Energy reported only around 5,500 power outages. A countywide state of emergency was declared in Sussex County, as vehicles were sliding off roadways. Several other traffic accidents and delays were reported throughout the state, and in Somerset County, a few roads were closed due to the snow. A Red Cross shelter was opened for residents of a Spotswood trailer park.\n\nIn New York State, the heaviest snow peaked at in Platte Cove, Greene County. Despite 50 flight cancellations at the Albany International Airport and several traffic accidents, no major damage or injuries were reported. Although heavy snow fell in the state, no major damage was reported in Connecticut. In Massachusetts and Rhode Island, high winds, gusting to as high as buffeted the coast.\n\n"}
{"id": "22918955", "url": "https://en.wikipedia.org/wiki?curid=22918955", "title": "Dewatering", "text": "Dewatering\n\nDewatering is the removal of water from solid material or soil by wet classification, centrifugation, filtration, or similar solid-liquid separation processes, such as removal of residual liquid from a filter cake by a filter press as part of various industrial processes.\n\nConstruction dewatering, unwatering, or water control are common terms used to describe removal or draining groundwater or surface water from a riverbed, construction site, caisson, or mine shaft, by pumping or evaporation. On a construction site, this dewatering may be implemented before subsurface excavation for foundations, shoring, or cellar space to lower the water table. This frequently involves the use of submersible \"dewatering\" pumps, centrifugal (\"trash\") pumps, eductors, or application of vacuum to well points.\n\nA deep well typically consists of a borehole fitted with a slotted liner and an electric submersible pump. As water is pumped from a deep well, a hydraulic gradient is formed and water flows into the well forming a cone of depression around the well in which there is little or no water remaining in the pore spaces of the surrounding soil. Deep wells work best in soils with a permeability of k =  m/s to  m/s; the amount of drawdown that a well can achieve is limited only by the size of the fish pump.\n\nDeep wells can be installed in a ring around an excavation to lower the water level and maintain a safe, dry site. Several equations can be used to design deep well dewatering systems, however many of these are based on empirical data and occasionally fail. Practice and experience, along with a firm understanding of the underlying principles of dewatering, are the best tools for designing a successful system. Some dewatering situations \"are so common that they can be designed almost by rule of thumb\".\n\nDeep wells are also used for aquifer testing and for groundwater drainage by wells.\n\nWellpoints are small-diameter (about 50 mm) tubes with slots near the bottom that are inserted into the ground from which water is drawn by a vacuum generated by a dewatering pump. Wellpoints are typically installed at close centers in a line along or around the edge of an excavation. As a vacuum is limited to 0 bar, the height to which water can be drawn is limited to about 6 meters (in practice). Wellpoints can be installed in stages, with the first reducing the water level by up to five meters, and a second stage, installed at a lower level, lowering it further.The water trickling between the deep wells may be collected by a single row of well point at the toe. This method ensures a much thicker width free from seepage forces.\n\nWellpoint spears are generally used to draw out groundwater in sandy soil conditions and are not as effective in clay or rock conditions. Open pumps are sometimes employed instead of spears if the ground conditions contain significant clay or rock content. \n\nThe installation of horizontal dewatering systems is relatively easy. A trencher installs an unperforated pipe followed by a synthetic or organic wrapped perforated pipe. The drain length is determined by the drain diameter, soilconditions and the water table. In general drain lengths of 50 meters is common. After installation of the drainpipe a pump is connected to the drain. After the water table has been lowered, the intended construction can start. After the construction is finished the pumps are stopped, and the water table will rise again. Installation depths up to 6 meters are common.\n\nWhilst engineers can use dewatering to lower a groundwater table, or to drain soils, they can also use the process to control pore pressure in soils and avoid damage to structures by base heave. High pore pressures occur in soils composed of fine silts or clays. Since these soils have a very low permeability, dewatering in a traditional sense (gravity flow into an abstraction well) may prove very costly or even futile. Instead, a vacuum-assisted dewatering scheme, such as ejector wells, or vacuum-sealed deep wells may serve to draw water into a well for abstraction.\n\nhttp://www.cdpwinc.com/applications\n\n"}
{"id": "35567878", "url": "https://en.wikipedia.org/wiki?curid=35567878", "title": "Direction Generale de L'Hydraulique et de l'Electrification Rurales", "text": "Direction Generale de L'Hydraulique et de l'Electrification Rurales\n\nDirection Generale de L'Hydraulique et del'Electrification Rurales (DGHER) is a hydraulic and electricity scheme in Burundi. Also known as Direction Generale de l'Hydraulique et des Energies Rurales, the World Bank has been involved with the funding.\n"}
{"id": "27005970", "url": "https://en.wikipedia.org/wiki?curid=27005970", "title": "Doroodzan Dam", "text": "Doroodzan Dam\n\nDoroodzan Dam is an earthen dam in Fars Province, Iran, about north of Shiraz. Completed in 1974 and built primarily for irrigation water storage, flood control, and municipal water storage, the facility is also a hydroelectric dam with an installed electricity generating capability of 10 MW.\n\n"}
{"id": "41068", "url": "https://en.wikipedia.org/wiki?curid=41068", "title": "Drop (liquid)", "text": "Drop (liquid)\n\nA drop or droplet is a small column of liquid, bounded completely or almost completely by free surfaces. A drop may form when liquid accumulates at the lower end of a tube or other surface boundary, producing a hanging drop called a pendant drop. Drops may also be formed by the condensation of a vapor or by atomization of a larger mass of liquid.\n\nLiquid forms drops because the liquid exhibits surface tension.\n\nA simple way to form a drop is to allow liquid to flow slowly from the lower end of a vertical tube of small diameter. The surface tension of the liquid causes the liquid to hang from the tube, forming a pendant. When the drop exceeds a certain size it is no longer stable and detaches itself. The falling liquid is also a drop held together by surface tension.\n\nSome substances that appear to be solid, can be shown to instead be extremely viscous liquids, because they form drops and display droplet behavior. In the famous pitch drop experiments, pitch - a substance somewhat like solid bitumen - is shown to be a liquid in this way. Pitch in a funnel slowly forms droplets, each droplet taking about 10 years to form and break off.\n\nIn the pendant drop test, a drop of liquid is suspended from the end of a tube by surface tension. The force due to surface tension is proportional to the length of the boundary between the liquid and the tube, with the proportionality constant usually denoted formula_1. Since the length of this boundary is the circumference of the tube, the force due to surface tension is given by\n\nwhere \"d\" is the tube diameter.\n\nThe mass \"m\" of the drop hanging from the end of the tube can be found by equating the force due to gravity (formula_3) with the component of the surface tension in the vertical direction (formula_4) giving the formula\n\nwhere α is the angle of contact with the tube, and \"g\" is the acceleration due to gravity.\n\nThe limit of this formula, as α goes to 90°, gives the maximum weight of a pendant drop for a liquid with a given surface tension, formula_1.\n\nThis relationship is the basis of a convenient method of measuring surface tension, commonly used in the petroleum industry. More sophisticated methods are available to take account of the developing shape of the pendant as the drop grows. These methods are used if the surface tension is unknown.\n\nThe drop adhesion to a solid can be divided into two categories: lateral adhesion and normal adhesion. Lateral adhesion resembles friction (though tribologically lateral adhesion is a more accurate term) and refers to the force required to slide a drop on the surface, namely the force to detach the drop from its position on the surface only to translate it to another position on the surface. Normal adhesion is the adhesion required to detach a drop from the surface in the normal direction, namely the force to cause the drop to fly off from the surface. The measurement of both adhesion forms can be done with the Centrifugal Adhesion Balance (CAB). The CAB uses a combination of centrifugal and gravitational forces to obtain any ratio of lateral and normal forces. For example, it can apply a normal force at zero lateral force for the drop to fly off away from the surface in the normal direction or it can induce a lateral force at zero normal force (simulating zero gravity).\n\nThe term droplet is a diminutive form of 'drop' - and as a guide is typically used for liquid particles of less than 500 µm diameter. In spray application, droplets are usually described by their perceived size (i.e., diameter) whereas the dose (or number of infective particles in the case of biopesticides) is a function of their volume. This increases by a cubic function relative to diameter; thus a 50 µm droplet represents a dose in 65 pl and a 500 µm drop represents a dose in 65 nanolitres.\n\nA droplet with a diameter of 3 mm has a terminal velocity of approximately 8 m/s.\nDrops smaller than in diameter will attain 95% of their terminal velocity within . But above this size the distance to get to terminal velocity increases sharply. An example is a drop with a diameter of that may achieve this at .\n\nDue to the different refractive index of water and air, refraction and reflection occur on the surfaces of raindrops, leading to rainbow formation.\n\nThe major source of sound when a droplet hits a liquid surface is the resonance of excited bubbles trapped underwater. These oscillating bubbles are responsible for most liquid sounds, such as running water or splashes, as they actually consist of many drop-liquid collisions.\n\nReducing the surface tension of a body of liquid makes possible to reduce or prevent noise due to droplets falling into it. This would involve adding soap, detergent or a similar substance to water. The reduced surface tension reduces the noise from dripping.\n\nThe classic shape associated with a drop (with a pointy end in its upper side) comes from the observation of a droplet clinging to a surface. The shape of a drop falling through a gas is actually more or less spherical for drops less than 2 mm in diameter. Larger drops tend to be flatter on the bottom part due to the pressure of the gas they move through. As a result, as drops get larger, a concave depression forms which leads to the eventual breakup of the drop.\n\nRaindrop sizes typically range from 0.5 mm to 4 mm, with size distributions quickly decreasing past diameters larger than 2-2.5 mm.\n\nScientists traditionally thought that the variation in the size of raindrops was due to collisions on the way down to the ground. In 2009 French researchers succeeded in showing that the distribution of sizes is due to the drops' interaction with air, which deforms larger drops and causes them to fragment into smaller drops, effectively limiting the largest raindrops to about 6 mm diameter. However, drops up to 10 mm (equivalent in volume to a sphere of radius 4.5 mm) are theoretically stable and could be levitated in a wind tunnel.\nThe largest recorded raindrop was 8.8 mm in diameter, located at the base of a cumulus congestus cloud in the vicinity of Kwajalein Atoll in July 1999. A raindrop of identical size was detected over northern Brazil in September 1995.\n\nIn medicine, this property is used to create droppers and IV infusion sets which have a standardized diameter, in such a way that 1 millilitre is equivalent to 20 drops. When smaller amounts are necessary (such as paediatrics), microdroppers or paediatric infusion sets are used, in which 1 millilitre = 60 microdrops.\n\n\n"}
{"id": "12535286", "url": "https://en.wikipedia.org/wiki?curid=12535286", "title": "Emsland Nuclear Power Plant", "text": "Emsland Nuclear Power Plant\n\nEmsland Nuclear Power Station is a nuclear reactor located in the district of Emsland, Germany just south of the Lingen Nuclear Power Plant. The reactor has 193 fuel elements totaling a core weight of 103 tons. It is a Konvoi type reactor.\n\nIt is owned by RWE Power AG.\n"}
{"id": "15511029", "url": "https://en.wikipedia.org/wiki?curid=15511029", "title": "Fordite", "text": "Fordite\n\nFordite, also known as Detroit agate or motor agate, is old automotive paint which has hardened sufficiently to be cut and polished. It was formed from the buildup of layers of enamel paint slag on tracks and skids on which cars were hand spray-painted (a now automated process), which have been baked numerous times. In recent times the material has been recycled into jewelry.\n"}
{"id": "18714334", "url": "https://en.wikipedia.org/wiki?curid=18714334", "title": "Galați Power Station", "text": "Galați Power Station\n\nThe Galaţi Power Station is a large thermal power plant located in Galaţi, having 4 generation groups of 100 MW each and 1 unit of 135 MW having a total electricity generation capacity of 535 MW.\n"}
{"id": "31376949", "url": "https://en.wikipedia.org/wiki?curid=31376949", "title": "Greek Atomic Energy Commission", "text": "Greek Atomic Energy Commission\n\nThe Greek Atomic Energy Commission (GAEC) is an independent government agency of Greece which is responsible for atomic safety, development and regulations and for monitoring artificially produced ionizing and non-ionizing radiation. The GAEC was established by act of legislation in 1954. The seven-member board of directors operate under the supervision of the Ministry of Development through the General Secretariat of Research and Technology.\n\n"}
{"id": "147049", "url": "https://en.wikipedia.org/wiki?curid=147049", "title": "ISO 14000", "text": "ISO 14000\n\nISO 14000 is a family of standards related to environmental management that exists to help organizations (a) minimize how their operations (processes, etc.) negatively affect the environment (i.e. cause adverse changes to air, water, or land); (b) comply with applicable laws, regulations, and other environmentally oriented requirements; and (c) continually improve in the above.\n\nISO 14000 is similar to ISO 9000 quality management in that both pertain to the process of how a product is produced, rather than to the product itself. As with ISO 9001, certification is performed by third-party organizations rather than being awarded by ISO directly. The ISO 19011 and ISO 17021 audit standards apply when audits are being performed.\n\nThe requirements of ISO 14001 are an integral part of the European Union's Eco-Management and Audit Scheme (EMAS). EMAS's structure and material are more demanding, mainly concerning performance improvement, legal compliance, and reporting duties. The current version of ISO 14001 is ISO 14001:2015, which was published in September 2015.\n\nIn March 1992, BSI Group published the world's first environmental management systems standard, BS 7750, as part of a response to growing concerns about protecting the environment. Prior to this, environmental management had been part of larger systems such as Responsible Care. BS 7750 supplied the template for the development of the ISO 14000 series in 1996, which has representation from ISO committees all over the world. , more than 300,000 certifications to ISO 14001 can be found in 171 countries.\n\nPrior to the development of the ISO 14000 series, organizations voluntarily constructed their own EMSs, but this made comparisons of environmental effects between companies difficult; therefore, the universal ISO 14000 series was developed. An EMS is defined by ISO as: \"part of the overall management system, that includes organizational structure, planning activities, responsibilities, practices, procedures, processes, and resources for developing, implementing, achieving, and maintaining the environmental policy.\"\n\nThe ISO 14000 family includes most notably the ISO 14001 standard, which represents the core set of standards used by organizations for designing and implementing an effective environmental management system (EMS). Other standards in this series include ISO 14004, which gives additional guidelines for a good EMS, and more specialized standards dealing with specific aspects of environmental management. The major objective of the ISO 14000 series of norms is to provide \"practical tools for companies and organizations of all kinds looking to manage their environmental responsibilities.\"\n\nThe ISO 14000 series is based on a voluntary approach to environmental regulation. The series includes the ISO 14001 standard, which provides guidelines for the establishment or improvement of an EMS. The standard shares many common traits with its predecessor, ISO 9000, the international standard of quality management, which served as a model for its internal structure, and both can be implemented side by side. As with ISO 9000, ISO 14000 acts both as an internal management tool and as a way of demonstrating a company’s environmental commitment to its customers and clients.\n\nISO 14001 defines criteria for an EMS. It does not state requirements for environmental performance but rather maps out a framework that a company or organization can follow to set up an effective EMS. It can be used by any organization that wants to improve resource efficiency, reduce waste, and reduce costs. Using ISO 14001 can provide assurance to company management and employees as well as external stakeholders that environmental impact is being measured and improved. ISO 14001 can also be integrated with other management functions and assists companies in meeting their environmental and economic goals.\n\nISO 14001, like other ISO 14000 standards, is voluntary, with its main aim to assist companies in continually improving their environmental performance and complying with any applicable legislation. The organization sets its own targets and performance measures, and the standard highlights what an organization needs to do to meet those goals, and to monitor and measure the situation. The standard does not focus on measures and goals of environmental performance, but of the organization. The standard can be applied to a variety of levels in the business, from the organizational level down to the product and service level. \n\nISO 14001 is known as a generic management system standard, meaning that it is relevant to any organization seeking to improve and manage resources more effectively. This includes:\n\n\nAll standards are periodically reviewed by ISO to ensure they still meet market requirements. The current version is ISO 14001:2015, and certified organizations were given a three-year transition period to adapt their environmental management system to the new edition of the standard. The new version of ISO 14001 focuses on the improvement of environmental performance rather than the improvement of the management system itself. It also includes several new updates all aimed at making environmental management more comprehensive and relevant to the supply chain. One of the main updates asks organizations to consider environmental impact during the entire life cycle, although there is no requirement to actually complete a life cycle analysis. Additionally, the commitments of top management and the methods of evaluating compliance have also been strengthened. Another significant change linked ISO 14001 to the general management system structure, introduced in 2015, called the High Level Structure. Both ISO 9001 and 14001 use this same structure, making implementation and auditing more uniform. The new standard also requires the holder of the certificate to specify risks and opportunities and how to address them.\n\nThe basic principles of ISO 14001 are based on the well-known Plan-Do-Check-Act (PDCA) cycle.\n\nPrior to implementing ISO 14001, an initial review or gap analysis of the organization's processes and products is recommended, to assist in identifying all elements of the current operation and, if possible, future operations, that may interact with the environment, termed \"environmental aspects.\" Environmental aspects can include both direct, such as those used during manufacturing, and indirect, such as raw materials. This review assists the organization in establishing their environmental objectives, goals, and targets (which should ideally be measurable); helps with the development of control and management procedures and processes; and serves to highlight any relevant legal requirement, which can then be built into the policy.\n\nDuring this stage, the organization identifies the resources required and works out those members of the organization responsible for the EMS' implementation and control. This includes establishing procedures and processes, although only one documented procedure is specifically related to operational control. Other procedures are required to foster better management control over elements such as documentation control, emergency preparedness and response, and the education of employees, to ensure that they can competently implement the necessary processes and record results. Communication and participation across all levels of the organization, especially top management, is a vital part of the implementation phase, with the effectiveness of the EMS being dependent on active involvement from all employees.\n\nDuring the \"check\" stage, performance is monitored and periodically measured to ensure that the organization's environmental targets and objectives are being met. In addition, internal audits are conducted at planned intervals to ascertain whether the EMS meets the user's expectations and whether the processes and procedures are being adequately maintained and monitored.\n\nAfter the checking stage, a management review is conducted to ensure that the objectives of the EMS are being met, the extent to which they are being met, and that communications are being appropriately managed. Additionally, the review evaluates changing circumstances, such as legal requirements, in order to make recommendations for further improvement of the system. These recommendations are incorporated through continual improvement: plans are renewed or new plans are made, and the EMS moves forward.\n\nISO 14001 encourages a company to continually improve its environmental performance. Apart from the obvious – the reduction in actual and possible negative environmental impacts – this is achieved in three ways:\n\n\nOverall, the CI concept expects the organization to gradually move away from merely operational environmental measures towards a more strategic approach on how to deal with environmental challenges.\n\nISO 14001 was developed primarily to assist companies with a framework for better management control, which can result in reducing their environmental impacts. In addition to improvements in performance, organizations can reap a number of economic benefits, including higher conformance with legislative and regulatory requirements by adopting the ISO standard. By minimizing the risk of regulatory and environmental liability fines and improving an organization’s efficiency, benefits can include a reduction in waste, consumption of resources, and operating costs. Secondly, as an internationally recognized standard, businesses operating in multiple locations across the globe can leverage their conformance to ISO 14001, eliminating the need for multiple registrations or certifications. Thirdly, there has been a push in the last decade by consumers for companies to adopt better internal controls, making the incorporation of ISO 14001 a smart approach for the long-term viability of businesses. This can provide them with a competitive advantage against companies that do not adopt the standard (Potoki & Prakash, 2005). This in turn can have a positive impact on a company's asset value (Van der Deldt, 1997). It can lead to improved public perceptions of the business, placing them in a better position to operate in the international marketplace. The use of ISO 14001 can demonstrate an innovative and forward-thinking approach to customers and prospective employees. It can increase a business’s access to new customers and business partners. In some markets it can potentially reduce public liability insurance costs. It can also serve to reduce trade barriers between registered businesses. There is growing interest in including certification to ISO 14001 in tenders for public-private partnerships for infrastructure renewal. Evidence of value in terms of environmental quality and benefit to the taxpayer has been shown in highway projects in Canada.\n\nISO 14001 can be used in whole or in part to help an organization (for-profit or not-for-profit) better manage its relationship with the environment. If all the elements of ISO 14001 are incorporated into the management process, the organization may opt to prove that it has achieved full alignment or conformity with the international standard, ISO 14001, by using one of four recognized options. These are:\n\n\nISO does not control conformity assessment; its mandate is to develop and maintain standards. ISO has a neutral policy on conformity assessment in so much that one option is not better than the next. Each option serves different market needs. The adopting organization decides which option is best for them, in conjunction with their market needs.\n\nOption one is sometimes incorrectly referred to as \"self-certify\" or \"self-certification\". This is not an acceptable reference under ISO terms and definitions, for it can lead to confusion in the market. The user is responsible for making their own determination. \n\nOption two is often referred to as a customer or 2nd-party audit, which is an acceptable market term. \n\nOption three is an independent third-party process by an organization that is based on an engagement activity and delivered by specially trained practitioners. This option was based on an accounting procedure branded as the EnviroReady Report, which was created to help small- and medium-sized organizations. Its development was originally based on the Canadian Handbook for Accountants; it is now based on an international accounting standard.\n\nThe fourth option, certification, is another independent third-party process, which has been widely implemented by all types of organizations. Certification is also known in some countries as registration. Service providers of certification or registration are accredited by national accreditation services such as UKAS in the UK.\n\nIn 2010, the latest EMAS Regulation (EMAS III) entered into force; the scheme is now globally applicable, and includes key performance indicators and a range of further improvements. \n, more than 3,900 organizations and approximately 9,200 sites are EMAS registered.\n\nISO 14001's EMS requirements are similar to those of EMAS. Additional requirements for EMAS include: \n\nThere are many reasons that ISO 14001 should be potentially attractive to supply chain managers, including the use of the voluntary standard to guide the development of integrated systems, its requirement for supply chain members in industries such as automotive and aerospace, the potential of pollution prevention leading to reduced costs of production and higher profits, its alignment with the growing importance of corporate social responsibility, and the possibility that an ISO-registered system may provide firms with a unique environmental resource, capabilities, and benefits that lead to competitive advantage.\nResearch on the supply chain impact of ISO 14001 registration posited that potential positive impacts might include more proactive environmental management, higher levels of communication, higher levels of waste reduction and cost efficiency, better ROI, higher levels of customer relationship management, fewer issues with employee health, and a reduced number of safety incidents. This research concluded that ISO 14001 registration can be leveraged across the supply chain for competitive advantage.\n\n\n\n"}
{"id": "58491912", "url": "https://en.wikipedia.org/wiki?curid=58491912", "title": "List of pipeline accidents in the United States in 2002", "text": "List of pipeline accidents in the United States in 2002\n\nThe following is a list of pipeline accidents in the United States in 2002. It is one of several lists of U.S. pipeline accidents. See also list of natural gas and oil production accidents in the United States.\n\nThis is not a complete list of all pipeline accidents. For natural gas alone, the Pipeline and Hazardous Materials Safety Administration (PHMSA), a United States Department of Transportation agency, has collected data on more than 3,200 accidents deemed serious or significant since 1987.\n\nA \"significant incident\" results in any of the following consequences:\n\nPHMSA and the National Transportation Safety Board (NTSB) post incident data and results of investigations into accidents involving pipelines that carry a variety of products, including natural gas, oil, diesel fuel, gasoline, kerosene, jet fuel, carbon dioxide, and other substances. Occasionally pipelines are repurposed to carry different products.\n\n"}
{"id": "3243489", "url": "https://en.wikipedia.org/wiki?curid=3243489", "title": "Lithium niobate", "text": "Lithium niobate\n\nLithium niobate () is a compound of niobium, lithium, and oxygen. Its single crystals are an important material for optical waveguides, mobile phones, piezoelectric sensors, optical modulators and various other linear and non-linear optical applications. It is a human-made dielectric material that does not exist in nature. Lithium niobate is sometimes referred to by the brand name linobate.\n\nLithium niobate is a colorless solid insoluble in water. It has a trigonal crystal system, which lacks inversion symmetry and displays ferroelectricity, the Pockels effect, the piezoelectric effect, photoelasticity and nonlinear optical polarizability. Lithium niobate has negative uniaxial birefringence which depends slightly on the stoichiometry of the crystal and on temperature. It is transparent for wavelengths between 350 and 5200 nanometers.\n\nLithium niobate can be doped by magnesium oxide, which increases its resistance to optical damage (also known as photorefractive damage) when doped above the optical damage threshold. Other available dopants are , , , , , , , and .\n\nSingle crystals of lithium niobate can be grown using the Czochralski process. After a crystal is grown, it is sliced into wafers of different orientation. Common orientations are Z-cut, X-cut, Y-cut, and cuts with rotated angles of the previous axes.\n\nNanoparticles of lithium niobate and niobium pentoxide can be produced at low temperature. The complete protocol implies a LiH induced reduction of NbCl followed by \"in situ\" spontaneous oxidation into low-valence niobium nano-oxides. These niobium oxides are exposed to air atmosphere resulting in pure NbO. Finally, the stable NbO is converted into lithium niobate LiNbO nanoparticles during the controlled hydrolysis of the LiH excess. Spherical nanoparticles of lithium niobate with a diameter of approximately 10 nm can be prepared by impregnating a mesoporous silica matrix with a mixture of an aqueous solution of LiNO and NHNbO(CO) followed by 10 min heating in an IR furnace.\n\nLithium niobate is used extensively in the telecoms market, e.g. in mobile telephones and optical modulators. It is the material of choice for the manufacture of surface acoustic wave devices. For some uses it can be replaced by lithium tantalate, . Other uses are in laser frequency doubling, nonlinear optics, Pockels cells, optical parametric oscillators, Q-switching devices for lasers, other acousto-optic devices, optical switches for gigahertz frequencies, etc. It is an excellent material for manufacture of optical waveguides. It's also used in the making of optical spatial low-pass (anti-aliasing) filters. \n\nIn the past few years lithium niobate is finding applications as a kind of electrostatic tweezers, an approach known as optoelectronic tweezers as the effect requires light excitation to take place. This effect allows for fine manipulation of micrometer-scale particles with high flexibility since the tweezing action is constrained to the illuminated area. The effect is based on the very high electric fields generated during light exposure (1–100 kV/cm) within the illuminated spot. These intense fields are also finding applications in biophysics and biotechnology, as they can influence living organisms in a variety of ways. For example, iron-doped lithium niobate excited with visible light has been shown to produce cell death in tumoral cell cultures.\n\nPeriodically-poled lithium niobate (PPLN) is a domain-engineered lithium niobate crystal, used mainly for achieving quasi-phase-matching in nonlinear optics. The ferroelectric domains point alternatively to the \"+c\" and the \"−c\" direction, with a period of typically between 5 and 35 µm. The shorter periods of this range are used for second harmonic generation, while the longer ones for optical parametric oscillation. Periodic poling can be achieved by electrical poling with periodically structured electrode. Controlled heating of the crystal can be used to fine-tune phase matching in the medium due to a slight variation of the dispersion with temperature.\n\nPeriodic poling uses the largest value of lithium niobate's nonlinear tensor, d = 27 pm/V. Quasi-phase matching gives maximum efficiencies that are 2/π (64%) of the full d, about 17 pm/V.\n\nOther materials used for periodic poling are wide band gap inorganic crystals like KTP (resulting in periodically poled KTP, PPKTP), lithium tantalate, and some organic materials.\n\nThe periodic poling technique can also be used to form surface nanostructures.\n\nHowever, due to its low photorefractive damage threshold, PPLN only finds limited applications: at very low power levels. MgO-doped lithium niobate is fabricated by periodically-poled method. Periodically-poled MgO-doped lithium niobate (PPMgOLN) therefore expands the application to medium power level.\n\nThe Sellmeier equations for the extraordinary index are used to find the poling period and approximate temperature for quasi-phase matching. Jundt gives\n\nformula_1\n\nvalid from 20 to 250 °C for wavelengths from 0.4 to 5 micrometers, whereas for longer wavelength,\n\nformula_2\n\nwhich is valid for \"T\" = 25 to 180 °C, for wavelengths λ between 2.8 and 4.8 micrometers.\n\nIn these equations \"f\" = (\"T\" − 24.5)(\"T\" + 570.82), λ is in micrometers, and \"T\" is in °C.\n\nMore generally for ordinary and extraordinary index for MgO-doped :\n\nformula_3,\n\nwith:\n\nfor congruent (CLN) and stochiometric (SLN).\n\n"}
{"id": "28849853", "url": "https://en.wikipedia.org/wiki?curid=28849853", "title": "Marthozite", "text": "Marthozite\n\nMarthozite is an orthorhombic mineral that has a general formula of Cu(UO)(SeO)(OH)·7HO. It was named after French mineralogist Aime Marthoz, former Director-general of the Union Miniere du Haut Katanga.\n\nIt is usually found in cavities in selenian (selenium-containing) digenite. It is specifically found in the zones of oxidation of the Musonoi deposit in Katanga, Africa. \n\nMarthozite is orthorhombic, meaning that it has three axes of unequal lengths all orthogonal to each other. Since it is orthorhombic, marthozite is biaxial, meaning that it has three different indices of refraction. Marthozite is anisotropic, which means that it breaks light into one fast ray and one slow ray. Marthozite shows pleochroism from yellowish brown to greenish yellow.\n"}
{"id": "21543926", "url": "https://en.wikipedia.org/wiki?curid=21543926", "title": "Martin Fleischmann", "text": "Martin Fleischmann\n\nMartin Fleischmann FRS (29 March 1927 – 3 August 2012) was a British chemist who worked in electrochemistry. Premature announcement of his cold fusion research with Stanley Pons, regarding excess heat in heavy water, caused a media sensation although they continued their interest and research in cold fusion.\n\nBorn in Karlovy Vary, Czechoslovakia, in 1927. His father was a wealthy lawyer and his mother the daughter of a high-ranking Austrian civil officer. Since his father was of Jewish heritage, Fleischmann's family abandoned a castle of their property and moved to the Netherlands and then to England in 1938, to avoid Nazi persecution. His father died of the complications of injuries received in a Nazi prison and afterwards Fleischmann lived for a period with his mother in a leased cottage in Rustington, Sussex. His early education was obtained at Worthing High School for Boys. After serving in the Czech Airforce Training Unit during the war, he moved to London in order to obtain an undergraduate and postgraduate degrees in chemistry at Imperial College London. His PhD degree was awarded in 1951 under the supervision of Professor Herrington and treated on the diffusion of electrogenerated hydrogen through palladium foils. He met Shelia, his future wife, as a student and remained married to her for 62 years.\n\nFleischmann's professional career was focused almost entirely on fundamental electrochemistry. Fleischmann went on to teach at King's College, Durham University, which in 1963 became the newly established University of Newcastle upon Tyne. In 1967, Fleischmann became Professor of Electrochemistry at the University of Southampton, occupying the Faraday Chair of Chemistry. From 1970 to 1972, he was president of the International Society of Electrochemists. In 1973, together with Patrick J. Hendra and A. James McQuillan, he played an important role in the discovery of Surface Enhanced Raman Scattering effect (SERS) a contribution for which the University of Southampton was awarded a National Chemical Landmark plaque by the Royal Society of Chemistry in 2013, and he developed the ultramicroelectrode in the 1980s. In 1979, he was awarded the medal for electrochemistry and thermodynamics by the Royal Society of London. In 1982 he retired from the University of Southampton. In 1985 he received the Olin Palladium Award from the Electrochemical Society, and in 1986 was elected to the Fellowship of the Royal Society. He retired from teaching in 1983 and was given an honorary professorship at Southampton University.\n\n\nFleischmann confided to Stanley Pons that he might have found what he believed to be a way to create nuclear fusion at room temperatures. From 1983 to 1989, he and Pons spent $100,000 in self-funded experiments at the University of Utah. Fleischmann wanted to publish it first in an obscure journal, and had already spoken with a team that was doing similar work in a different university for a joint publication. The details have not surfaced, but it would seem that the University of Utah wanted to establish priority over the discovery and its patents by making a public announcement before the publication. In an interview with \"60 Minutes\" on 19 April 2009, Fleischmann said that the public announcement was the university's idea, and that he regretted doing it. This decision would later cause heavy criticism against Fleischmann and Pons, being perceived as a breach of how science is usually communicated to other scientists.\n\nOn 23 March 1989 it was finally announced at a press conference as \"a sustained nuclear fusion reaction,\" which was quickly labeled by the press as cold fusion – a result previously thought to be unattainable. On 26 March Fleischmann warned on the \"Wall Street Journal Report\" not to try replications until a published paper was available two weeks later in \"Journal of Electroanalytical Chemistry\", but that did not stop hundreds of scientists who had already started work at their laboratories the moment they heard the news on 23 March, and more often than not they failed to reproduce the effects. Those who failed to reproduce the claim attacked the pair for fraudulent, sloppy, and unethical work;\nincomplete, unreproducible, and inaccurate results; and erroneous interpretations. When the paper was finally published, both electrochemists and physicists called it \"sloppy\" and \"uninformative\", and it was said that, had Fleischmann and Pons waited for the publication of their paper, most of the trouble would have been avoided because scientists would not have gone so far in trying to test their work. Fleischmann and Pons sued an Italian journalist who had published very harsh criticisms against them, but the judge rejected it saying that criticisms were appropriate given the scientists' behaviour, the lack of evidence since the first announcement, and the lack of interest shown by the scientific community, and that they were an expression of the journalist's \"right of reporting\". Fleischmann, Pons and the researchers who believed that they had replicated the effect remained convinced the effect was real, but the general scientific community remains skeptical.\n\nIn 2009, Michael McKubre concluded from his attempt to duplicate the \"Fleischmann-Pons Effect\", that there is \"heat production consistent with nuclear but not chemical energy or known lattice storage effect\". This was an extension of the work done by Miles at the Navy Laboratory (NAWCWD) at China Lake, California (1990-1994).\n\nIn 1992, Fleischmann moved to France with Pons to continue their work at the IMRA laboratory (part of Technova Corporation, a subsidiary of Toyota), but in 1995 he retired and returned to England. He co-authored further papers with researchers from the US Navy and Italian national laboratories (INFN and ENEA), on the subject of cold fusion. In March 2006, \"Solar Energy Limited\" division \"D2Fusion Inc\" announced in a press release that Fleischmann, then 79, would be acting as their senior scientific advisor.\n\nFleischmann died at home in Tisbury, Wiltshire on 3 August 2012, of natural causes. He had suffered from Parkinson's disease, diabetes and heart disease. He was survived by his son Nicholas and his two daughters, Vanessa and Charlotte.\n\nWhile holding the Faraday Chair of Electrochemistry he and Graham Hills established in the late 60s the now renowned Electrochemistry Group of the University of Southampton.\n\nFleischmann produced over 272 scientific papers and book chapters on the field of electrochemistry. He contributed to the fundamental theory of:\n\n\n\n"}
{"id": "40502817", "url": "https://en.wikipedia.org/wiki?curid=40502817", "title": "Maxwell–Jüttner distribution", "text": "Maxwell–Jüttner distribution\n\nIn physics, the Maxwell–Jüttner distribution is the distribution of speeds of particles in a hypothetical gas of relativistic particles. Similar to Maxwell's distribution, the Maxwell–Jüttner distribution considers a classical ideal gas where the particles are dilute and do not significantly interact with each other. The distinction from Maxwell's case is that effects of special relativity are taken into account. In the limit of low temperatures \"T\" much less than \"mc\"/\"k\" (where \"m\" is the mass of the kind of particle making up the gas, \"c\" is the speed of light and \"k\" is Boltzmann's constant), this distribution becomes identical to the Maxwell–Boltzmann distribution.\n\nThe distribution can be attributed to Ferencz Jüttner, who derived it in 1911. It has become known as the Maxwell–Jüttner distribution by analogy to the name Maxwell-Boltzmann distribution that is commonly used to refer to Maxwell's distribution.\n\nAs the gas becomes hotter and \"kT\" approaches or exceeds \"mc\", the probability distribution for formula_1 in this relativistic Maxwellian gas is given by the Maxwell–Jüttner distribution:\n\nwhere formula_3 formula_4 and formula_5 is the modified Bessel function of the second kind.\n\nAlternatively, this can be written in terms of the momentum as\n\nwhere formula_7. The Maxwell–Jüttner equation is covariant, but not manifestly so, and the temperature of the gas does not vary with the gross speed of the gas.\n\nSome limitations of the Maxwell–Jüttner distributions are shared with the classical ideal gas: neglect of interactions, and neglect of quantum effects. An additional limitation (not important in the classical ideal gas) is that the Maxwell–Jüttner distribution neglects antiparticles.\n\nIf particle-antiparticle creation is allowed, then once the thermal energy \"kT\" is a significant fraction of \"mc\", particle-antiparticle creation will occur and begin to increase the number of particles while generating antiparticles (the number of particles is not conserved, but instead the conserved quantity is the difference between particle number and antiparticle number). The resulting thermal distribution will depend on the chemical potential relating to the conserved particle-antiparticle number difference. A further consequence of this is that it becomes necessary to incorporate statistical mechanics for indistinguishable particles, because the occupation probabilities for low kinetic energy states becomes of order unity. For fermions it is necessary to use Fermi–Dirac statistics and the result is analogous to the thermal generation of electron-hole pairs in semiconductors. For bosonic particles, it is necessary to use the Bose–Einstein statistics.\n"}
{"id": "34171909", "url": "https://en.wikipedia.org/wiki?curid=34171909", "title": "Moens–Korteweg equation", "text": "Moens–Korteweg equation\n\nIn biomechanics, the Moens–Korteweg equation models the relationship between wave speed or pulse wave velocity (PWV) and the incremental elastic modulus of the arterial wall or its distensibility. The equation was derived independently by Adriaan Isebree Moens and Diederik Korteweg. It is derived from Newton's second law of motion, using some simplifying assumptions, and reads:\n\nThe Moens–Korteweg equation states that PWV is proportional to the square root of the incremental elastic modulus, (\"E\"), of the vessel wall given constant ratio of wall thickness, \"h\", to vessel radius, \"r\", and blood density, ρ, assuming that the artery wall is isotropic and experiences isovolumetric change with pulse pressure.\n\n"}
{"id": "19648678", "url": "https://en.wikipedia.org/wiki?curid=19648678", "title": "Multiple-Use Sustained-Yield Act of 1960", "text": "Multiple-Use Sustained-Yield Act of 1960\n\nThe Multiple Use - Sustained Yield Act of 1960 (or MUSYA) (Public Law 86-517) is a federal law passed by the United States Congress on June 12, 1960. This law authorizes and directs the Secretary of Agriculture to develop and administer the renewable resources of timber, range, water, recreation and wildlife on the national forests for multiple use and sustained yield of the products and services.\n\nThis is the first law to have the five major uses of national forests contained in one law equally, with no use greater than any other.\n\nBy the 1950s, the national forests no longer held enough resources to meet the growing needs of an increasing population and expanding economy. The U.S. Forest Service had operated within broad authorities since Gifford Pinchot's time as Chief Forester. Now, for the first time the agency had a specific congressional directive which stipulated that timber sales were not in all cases to be the limiting factor.\n\nMUSYA defines the terms \"multiple use\" and \"sustained yield\" as follows:\n\n\nThe 1960 law was amended by the Omnibus Parks and Public Lands Management Act of 1996.\n\n"}
{"id": "15309059", "url": "https://en.wikipedia.org/wiki?curid=15309059", "title": "NaFIRS", "text": "NaFIRS\n\nIn the United Kingdom, the National Fault and Interruption Reporting System maintains statistical information relating both to electrical faults and interruptions to the supply of electricity to the 15 UK Electricity Distribution Network Operators (DNOs).\n\n"}
{"id": "43971349", "url": "https://en.wikipedia.org/wiki?curid=43971349", "title": "North Eastern Electricity Supply Company of Odisha", "text": "North Eastern Electricity Supply Company of Odisha\n\nNorth Eastern Electricity Supply Company of Odisha or NESCO was incorporated as a Public Sector Company of Government of Odisha on November 19, 1997 to carry out the distribution and retail supply business of electricity in the entire North Eastern Odisha, 5 Districts of Odisha; Balasore, Mayurbhanj, Keonjhar, Jajpur, and Bhadrak. Northern Electricity Supply Company of Odisha was incorporated under the Companies Act 1956 and started functioning as a subsidiary of Grid Corporation of Odisha (GRIDCO), a Government of Odisha Power Utility, from November 26, 1998 under Distribution and Retail Supply License.\n\n"}
{"id": "12018886", "url": "https://en.wikipedia.org/wiki?curid=12018886", "title": "Onionskin", "text": "Onionskin\n\nOnionskin or onion skin is a thin, light-weight, strong, often translucent paper. Though not made from onions, it superficially resembles their thin, papery skins. It was usually used with carbon paper for typing duplicates in a typewriter, for permanent records where low bulk was important, or for airmail correspondence. It is typically 25–39 g/m² (9-pound basis weight in US units), and may be white or canary-colored. \n\nIn the typewriter era, onion skin often had a deeply textured cockle finish which allowed for easier erasure of typing mistakes, but other glazed and unglazed finishes were also available then and may be more common today.\n\nOnionskin paper is relatively durable and lightweight due to its high content of cotton fibers. Because of these attributes and its crispness when folding, onionskin paper is one of the best papers to use for toy kites and advanced paper airplanes. Paper airplanes made from onionskin paper tend to fly very well due to their low weight and high integrity once folded.\n\nOnionskin paper has also been regularly used in traditional cel animation. Due to its translucency, it is used as a guide in drawing the frames between key-frames. This is a process that animators refer to as \"in-betweening\". The process of \"onionskinning\" is also used in digital animation where frames are represented by digital layers in a production.\n"}
{"id": "32348051", "url": "https://en.wikipedia.org/wiki?curid=32348051", "title": "Open Fuel Standard Coalition", "text": "Open Fuel Standard Coalition\n\nThe Open Fuel Standard Coalition is a bipartisan group in the United States actively working for passage of H.R. 1687, the \"Open Fuel Standard Act of 2011.\" The OFS Coalition views this legislation as the solution to the current energy crisis by the implementing of alternative energy sources into our fuel transportation market sector, thereby breaking our dependence on foreign oil. Specifically, by implementing H.R. 1687, the Open Fuel Standard Act of 2011 all vehicles sold in the US will have to either operate on mixed fuels, containing 85 percent ethanol, methanol, biodiesel, or any other alternative energy source. \n\nThe need for an open fuel standard could not be any clearer as we have seen the rising price of oil in recent years. The continuing unrest and violence in the Middle East has caused dramatic increases in crude oil that remains strongly controlled by OPEC. Our current gasoline consumption from foreign oil is unsustainable for the future. Additionally, our oil addiction is contributing to the financial support of our enemies abroad and unquestionably aiding terrorist organizations. The promise of freeing our dependence on foreign oil has long been stated, but we can no longer kick the can further down the road. Now, both Republicans and Democrats must work together to implement a sensible bipartisan solution to reinvent and revitalize our energy policies. By the end of the 112th session, we hope to quickly move this legislative through congress and enforce the open fuel standard as a policy priority.\n\nThe open fuel standard will create incentive to promote and invest in the emerging market of ethanol, methanol, biodiesel, and any other alternative fuel. Vehicles with these alternative fuel engines will re-energize our economy and create new jobs here in the U.S. Comparatively, mixed fuels produce less carbon emissions as well as beating the price of oil three to one. The technology and resources are already available for flex-fuel cars, but the availability and access to pumps is scarce. By manufacturing flex-fuel cars, gas stations can create new revenue by adding mixed fuel pumps that offer low cost alternative to gasoline. However, the first step is allowing a demand by opening up the market to producing more flex fuel and alternative fueled cars.\nOur foreign competitors, including China and Brazil have already successfully implemented similar regulations on auto industries selling cars in their countries resulting in tremendous success and new manufacturing plants in both China and Brazil. The U.S. cannot stay competitive as we fall behind in the investments of alternative energy and auto industries take their business abroad.\n\nThe goal of the OFS coalition is to pass H.R. 1687, the \"Open Fuel Standard Act of 2011\" in the 112th United States Congress and generate pressures on the administration and federal departments to take further action. The OFS Coalition is assisting in providing information about the open fuel standard in an effort to educate Members of the United States Congress, while successfully engaging with various groups to promote the tactical purposes of the open fuel standard and draw attention and support of key Members of Congress.\n\nA web site has been created by the Open Fuel Standard Coalition to help keep interested parties informed about the Open Fuel Standard Act. The purpose of OpenFuelStandard.org is to provide information to citizens who want to help the Open Fuel Standard Act become law. \n"}
{"id": "56662209", "url": "https://en.wikipedia.org/wiki?curid=56662209", "title": "Our Power (Scotland)", "text": "Our Power (Scotland)\n\nOur Power is an energy supply company backed by the Scottish government, the first in the UK to operate on a non-profit distribution basis. Serving around 27,000 households and between 800 and 1,000 new customers every month, Our Power is entirely asset-locked; being owned by social housing providers, community organisations and local authorities.\n\nEstablished in 2016 with 53 member housing associations and local authorities, the company broadened its investor-base in late 2017 after raising £4.4m from 301 investors via a social purpose bond. In July 2017, Welsh actor and community activist Michael Sheen visited their Edinburgh headquarters to support the work of the social enterprise.\n\nOur Power now has over 80 employees based in Edinburgh. It works with 67 social housing providers, community organisations and local authorities as members, and provides heat and power to around 21,000 households.\n\nThe company has partnered with the charity Money Advice Scotland to launch a variable tariff for customers on restricted meters, who are unable to switch providers or access tariffs available to single rate meter customers. With the aim of turning a profit by 2020, its fourth year of operation, Our Power has now made its first forays outside Scotland: extending their membership to include a Welsh housing association and nearing the completion of deals that will see it supply one local authority and two community energy initiatives in England.\n\nIn February 2018 Our Power introduced its first UK-wide tariff called +IMPACT, which the company defines as \"100% Green\". It is competitively priced, with the ultimate goal being to \"secure thousands of ethically-minded energy customers\" by the end of the year. Our Power developed the tariff to bring energy costs down for families living in or at risk of fuel poverty.\n\nOur Power has partnered with Mongoose Energy for the purpose of creating 'Our Community Energy,' a renewable energy investment scheme, which directly links renewable generation and not-for-profit ownership. The new community organisation was launched in response to a survey which had found that more than a third of Scots (36%) admitted to living in fuel poverty, suggesting that this was particularly acute in the case of millennials, with over half of those aged 18–34 (56%) directly impacted. The scheme endeavors to utilize the power from two Scottish community-owned wind power projects, so that the companies can offer less expensive electricity to those most in need of cheaper energy. The wind farms located in Pogbie, East Lothian and Brockholes, Berwickshire aspire to raise a sum of £2.9 million through bond and equity offers.\n\n"}
{"id": "20685630", "url": "https://en.wikipedia.org/wiki?curid=20685630", "title": "Peierls transition", "text": "Peierls transition\n\nA Peierls transition or Peierls distortion is a distortion of the periodic lattice of a one-dimensional crystal. Atomic positions oscillate, so that the perfect order of the 1-D crystal is broken.\n\nPeierls' theorem states that \"a one-dimensional equally spaced chain with one electron per ion is unstable\". \nIt was asserted in the 1930s by Rudolf Peierls. It can be proven using a simple model of the potential for an electron in a 1-D crystal with lattice spacing formula_1. The periodicity of the crystal creates energy band gaps in the formula_2 diagram at the edge of the Brillouin zone formula_3 (similar to the result of the Kronig–Penney model, which helps to explain the origin of band gaps in semiconductors). If the ions each contribute one electron, then the band will be half-filled, up to values of formula_4 in the ground state.\n\nImagine a lattice distortion where every other ion moves closer to one neighbor and further away from the other, the unfavourable energy of the long bond between ions is outweighed by the energy gain of the short bond. The period has just doubled from formula_1 to formula_6. In essence, the proof relies on the fact that doubling the period would introduce new band gaps located at multiples of formula_4; see the figure in the right. This would cause small energy savings, based on the distortion of the bands in the vicinity of the new gaps. Approaching formula_4, the distortion due to the introduction of the new band gap will cause the electrons to be at a lower energy than they would be in the perfect crystal. Therefore, this lattice distortion becomes energetically favorable when the energy savings due to the new band gaps outweighs the elastic energy cost of rearranging the ions. Of course, this effect will be noticeable only when the electrons are arranged close to their ground state – in other words, thermal excitation should be minimized. Therefore, the Peierls transition should be seen at low temperature. This is the basic argument for the occurrence of the Peierls transition, sometimes called dimerization.\n\nPeierls’ discovery gained experimental backing during the effort to find new superconducting materials. In 1964, Dr. William Little of the Stanford University Department of Physics theorized that a certain class of polymer chains may experience a high \"T\" superconducting transition. The basis for his assertion was that the lattice distortions that lead to pairing of electrons in the BCS theory of superconductivity could be replaced instead by rearranging the electron density in a series of side chains. This means that now electrons would be responsible for creating the Cooper pairs instead of ions. Because the transition temperature is inversely proportional to the square root of the mass of the charged particle responsible for the distortions, the \"T\" should be improved by a corresponding factor:\n\nThe subscript \"i\" represents \"ion\", while \"e\" represents \"electron\". The predicted benefit in superconducting transition temperature was therefore a factor of about 300.\n\nIn the 1970s, various organic materials such as TTF-TCNQ were synthesized. What was found is that these materials underwent an insulating transition rather than a superconducting one. Eventually it was realized that these were the first experimental observations of the Peierls transition. With the introduction of new band gaps after the lattice becomes distorted, electrons must overcome this new energy barrier in order to become free to conduct. The simple model of the Peierls distortion as a rearrangement of ions in a 1-D chain could describe why these materials became insulators rather than superconductors.\n\nPeierls predicted that the rearrangement of the ion cores in a Peierls transition would produce periodic fluctuations in the electron density. These are commonly called charge density waves, and they are an example of collective charge transport. Several materials systems have verified the existence of these waves. Good candidates are weakly coupled molecular chains, where electrons can move freely along the direction of the chains, but motion is restricted perpendicular to the chains. NbSe and KMoO are two examples in which charge density waves have been observed at relatively high temperatures of 145 K and 180 K respectively.\n\nFurthermore, the 1-D nature of the material causes a breakdown of the Fermi liquid theory for electron behavior. Therefore, a 1-D conductor should behave as a Luttinger liquid instead. A Luttinger liquid is a paramagnetic one-dimensional metal without Landau quasi-particle excitations.\n\n1-D metals have been the subject of much research. Here are a few examples of both theoretical and experimental research efforts to illustrate the broad range of topics:\n\n\n"}
{"id": "42709", "url": "https://en.wikipedia.org/wiki?curid=42709", "title": "Pendulum", "text": "Pendulum\n\nA pendulum is a weight suspended from a pivot so that it can swing freely. When a pendulum is displaced sideways from its resting, equilibrium position, it is subject to a restoring force due to gravity that will accelerate it back toward the equilibrium position. When released, the restoring force acting on the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth. The time for one complete cycle, a left swing and a right swing, is called the period. The period depends on the length of the pendulum and also to a slight degree on the amplitude, the width of the pendulum's swing.\n\nFrom the first scientific investigations of the pendulum around 1602 by Galileo Galilei, the regular motion of pendulums was used for timekeeping, and was the world's most accurate timekeeping technology until the 1930s. The pendulum clock invented by Christian Huygens in 1658 became the world's standard timekeeper, used in homes and offices for 270 years, and achieved accuracy of about one second per year before it was superseded as a time standard by the quartz clock in the 1930s. Pendulums are also used in scientific instruments such as accelerometers and seismometers. Historically they were used as gravimeters to measure the acceleration of gravity in geophysical surveys, and even as a standard of length. The word \"pendulum\" is new Latin, from the Latin \"pendulus\", meaning 'hanging'.\n\nThe \"simple gravity pendulum\" is an idealized mathematical model of a pendulum. This is a weight (or bob) on the end of a massless cord suspended from a pivot, without friction. When given an initial push, it will swing back and forth at a constant amplitude. Real pendulums are subject to friction and air drag, so the amplitude of their swings declines.\n\nThe period of swing of a simple gravity pendulum depends on its length, the local strength of gravity, and to a small extent on the maximum angle that the pendulum swings away from vertical, \"θ\", called the amplitude. It is independent of the mass of the bob. If the amplitude is limited to small swings, the period \"T\" of a simple pendulum, the time taken for a complete cycle, is:\n\nwhere formula_2 is the length of the pendulum and formula_3 is the local acceleration of gravity.\n\nFor small swings the period of swing is approximately the same for different size swings: that is, \"the period is independent of amplitude\". This property, called isochronism, is the reason pendulums are so useful for timekeeping. Successive swings of the pendulum, even if changing in amplitude, take the same amount of time.\n\nFor larger amplitudes, the period increases gradually with amplitude so it is longer than given by equation (1). For example, at an amplitude of \"θ\" = 23° it is 1% larger than given by (1). The period increases asymptotically (to infinity) as \"θ\" approaches 180°, because the value \"θ\" = 180° is an unstable equilibrium point for the pendulum. The true period of an ideal simple gravity pendulum can be written in several different forms (see Pendulum (mathematics) ), one example being the infinite series:\n\nwhere formula_5 is in radians.\n\nThe difference between this true period and the period for small swings (1) above is called the \"circular error\". In the case of a typical grandfather clock whose pendulum has a swing of 6° and thus an amplitude of 3° (0.05 radians), the difference between the true period and the small angle approximation (1) amounts to about 15 seconds per day.\n\nFor small swings the pendulum approximates a harmonic oscillator, and its motion as a function of time, \"t\", is approximately simple harmonic motion:\nwhere formula_7 is a constant value, dependent on initial conditions.\n\nFor real pendulums, the period varies slightly with factors such as the buoyancy and viscous resistance of the air, the mass of the string or rod, the size and shape of the bob and how it is attached to the string, and flexibility and stretching of the string. In precision applications, corrections for these factors may need to be applied to eq. (1) to give the period accurately.\n\nAny swinging rigid body free to rotate about a fixed horizontal axis is called a compound pendulum or physical pendulum. The appropriate equivalent length formula_8 for calculating the period of any such pendulum is the distance\nfrom the pivot to the \"center of oscillation\". This point is located under the center of mass at a distance from the\npivot traditionally called the radius of oscillation, which depends on the mass distribution of the pendulum. If most of the mass is concentrated in a relatively small bob compared to the pendulum length, the center of oscillation is close to the center of mass.\n\nThe radius of oscillation or equivalent length formula_8 of any physical pendulum can be shown to be\nwhere formula_11 is the moment of inertia of the pendulum about the pivot point,\nformula_12 is the mass of the pendulum, and formula_13 is the distance between the pivot point and the center of mass.\nSubstituting this expression in (1) above, the period formula_14 of a compound pendulum is given by\nfor sufficiently small oscillations.\n\nFor example, a rigid uniform rod of length formula_8 pivoted about one end has moment of inertia formula_17.\nThe center of mass is located at the center of the rod, so formula_18 Substituting these values into the above equation gives formula_19. This shows that a rigid rod pendulum has the same period as a simple pendulum of 2/3 its length.\n\nChristiaan Huygens proved in 1673 that the pivot point and the center of oscillation are interchangeable. This means if any pendulum is turned upside down and swung from a pivot located at its previous center of oscillation, it will have the same period as before and the new center of oscillation will be at the old pivot point. In 1817 Henry Kater used this idea to produce a type of reversible pendulum, now known as a Kater pendulum, for improved measurements of the acceleration due to gravity.\n\nOne of the earliest known uses of a pendulum was a 1st-century seismometer device of Han Dynasty Chinese scientist Zhang Heng. Its function was to sway and activate one of a series of levers after being disturbed by the tremor of an earthquake far away. Released by a lever, a small ball would fall out of the urn-shaped device into one of eight metal toad's mouths below, at the eight points of the compass, signifying the direction the earthquake was located.\n\nMany sources claim that the 10th-century Egyptian astronomer Ibn Yunus used a pendulum for time measurement, but this was an error that originated in 1684 with the British historian Edward Bernard.\n\nDuring the Renaissance, large hand-pumped pendulums were used as sources of power for manual reciprocating machines such as saws, bellows, and pumps. Leonardo da Vinci made many drawings of the motion of pendulums, though without realizing its value for timekeeping.\n\nItalian scientist Galileo Galilei was the first to study the properties of pendulums, beginning around 1602. The earliest extant report of his research is contained in a letter to Guido Ubaldo dal Monte, from Padua, dated November 29, 1602. His biographer and student, Vincenzo Viviani, claimed his interest had been sparked around 1582 by the swinging motion of a chandelier in Pisa Cathedral. Galileo discovered the crucial property that makes pendulums useful as timekeepers, called isochronism; the period of the pendulum is approximately independent of the amplitude or width of the swing. He also found that the period is independent of the mass of the bob, and proportional to the square root of the length of the pendulum. He first employed freeswinging pendulums in simple timing applications. His physician friend, Santorio Santorii, invented a device which measured a patient's pulse by the length of a pendulum; the \"pulsilogium\". In 1641 Galileo conceived and dictated to his son Vincenzo a design for a pendulum clock; Vincenzo began construction, but had not completed it when he died in 1649. The pendulum was the first harmonic oscillator used by man.\n\nIn 1656 the Dutch scientist Christiaan Huygens built the first pendulum clock. This was a great improvement over existing mechanical clocks; their best accuracy was improved from around 15 minutes deviation a day to around 15 seconds a day. Pendulums spread over Europe as existing clocks were retrofitted with them.\n\nThe English scientist Robert Hooke studied the conical pendulum around 1666, consisting of a pendulum that is free to swing in two dimensions, with the bob rotating in a circle or ellipse. He used the motions of this device as a model to analyze the orbital motions of the planets. Hooke suggested to Isaac Newton in 1679 that the components of orbital motion consisted of inertial motion along a tangent direction plus an attractive motion in the radial direction. This played a part in Newton's formulation of the law of universal gravitation. Robert Hooke was also responsible for suggesting as early as 1666 that the pendulum could be used to measure the force of gravity.\n\nDuring his expedition to Cayenne, French Guiana in 1671, Jean Richer found that a pendulum clock was minutes per day slower at Cayenne than at Paris. From this he deduced that the force of gravity was lower at Cayenne. In 1687, Isaac Newton in \"Principia Mathematica\" showed that this was because the Earth was not a true sphere but slightly oblate (flattened at the poles) from the effect of centrifugal force due to its rotation, causing gravity to increase with latitude. Portable pendulums began to be taken on voyages to distant lands, as precision gravimeters to measure the acceleration of gravity at different points on Earth, eventually resulting in accurate models of the shape of the Earth.\n\nIn 1673, 17 years after he invented the pendulum clock, Christiaan Huygens published his theory of the pendulum, \"Horologium Oscillatorium sive de motu pendulorum\". Marin Mersenne and René Descartes had discovered around 1636 that the pendulum was not quite isochronous; its period increased somewhat with its amplitude. Huygens analyzed this problem by determining what curve an object must follow to descend by gravity to the same point in the same time interval, regardless of starting point; the so-called \"tautochrone curve\". By a complicated method that was an early use of calculus, he showed this curve was a cycloid, rather than the circular arc of a pendulum, confirming that the pendulum was not isochronous and Galileo's observation of isochronism was accurate only for small swings. Huygens also solved the problem of how to calculate the period of an arbitrarily shaped pendulum (called a \"compound pendulum\"), discovering the \"center of oscillation\", and its interchangeability with the pivot point.\n\nThe existing clock movement, the verge escapement, made pendulums swing in very wide arcs of about 100°. Huygens showed this was a source of inaccuracy, causing the period to vary with amplitude changes caused by small unavoidable variations in the clock's drive force. To make its period isochronous, Huygens mounted cycloidal-shaped metal 'chops' next to the pivots in his clocks, that constrained the suspension cord and forced the pendulum to follow a cycloid arc. This solution didn't prove as practical as simply limiting the pendulum's swing to small angles of a few degrees. The realization that only small swings were isochronous motivated the development of the anchor escapement around 1670, which reduced the pendulum swing in clocks to 4°–6°.\n\nDuring the 18th and 19th century, the pendulum clock's role as the most accurate timekeeper motivated much practical research into improving pendulums. It was found that a major source of error was that the pendulum rod expanded and contracted with changes in ambient temperature, changing the period of swing. This was solved with the invention of temperature compensated pendulums, the mercury pendulum in 1721 and the gridiron pendulum in 1726, reducing errors in precision pendulum clocks to a few seconds per week.\n\nThe accuracy of gravity measurements made with pendulums was limited by the difficulty of finding the location of their center of oscillation. Huygens had discovered in 1673 that a pendulum has the same period when hung from its center of oscillation as when hung from its pivot, and the distance between the two points was equal to the length of a simple gravity pendulum of the same period. In 1818 British Captain Henry Kater invented the reversible Kater's pendulum which used this principle, making possible very accurate measurements of gravity. For the next century the reversible pendulum was the standard method of measuring absolute gravitational acceleration.\n\nIn 1851, Jean Bernard Léon Foucault showed that the plane of oscillation of a pendulum, like a gyroscope, tends to stay constant regardless of the motion of the pivot, and that this could be used to demonstrate the rotation of the Earth. He suspended a pendulum free to swing in two dimensions (later named the Foucault pendulum) from the dome of the Panthéon in Paris. The length of the cord was . Once the pendulum was set in motion, the plane of swing was observed to precess or rotate 360° clockwise in about 32 hours.\nThis was the first demonstration of the Earth's rotation that didn't depend on celestial observations, and a \"pendulum mania\" broke out, as Foucault pendulums were displayed in many cities and attracted large crowds.\n\nAround 1900 low-thermal-expansion materials began to be used for pendulum rods in the highest precision clocks and other instruments, first invar, a nickel steel alloy, and later fused quartz, which made temperature compensation trivial. Precision pendulums were housed in low pressure tanks, which kept the air pressure constant to prevent changes in the period due to changes in buoyancy of the pendulum due to changing atmospheric pressure. The best pendulum clocks achieved accuracy of around a second per year.\n\nThe timekeeping accuracy of the pendulum was exceeded by the quartz crystal oscillator, invented in 1921, and quartz clocks, invented in 1927, replaced pendulum clocks as the world's best timekeepers. Pendulum clocks were used as time standards until World War 2, although the French Time Service continued using them in their official time standard ensemble until 1954. Pendulum gravimeters were superseded by \"free fall\" gravimeters in the 1950s, but pendulum instruments continued to be used into the 1970s.\n\nFor 300 years, from its discovery around 1582 until development of the quartz clock in the 1930s, the pendulum was the world's standard for accurate timekeeping. In addition to clock pendulums, freeswinging seconds pendulums were widely used as precision timers in scientific experiments in the 17th and 18th centuries. Pendulums require great mechanical stability: a length change of only 0.02%, 0.2 mm in a grandfather clock pendulum, will cause an error of a minute per week.\n\nPendulums in clocks (see example at right) are usually made of a weight or bob \"(b)\" suspended by a rod of wood or metal \"(a)\". To reduce air resistance (which accounts for most of the energy loss in precision clocks) the bob is traditionally a smooth disk with a lens-shaped cross section, although in antique clocks it often had carvings or decorations specific to the type of clock. In quality clocks the bob is made as heavy as the suspension can support and the movement can drive, since this improves the regulation of the clock (see Accuracy below). A common weight for seconds pendulum bobs is . Instead of hanging from a pivot, clock pendulums are usually supported by a short straight spring \"(d)\" of flexible metal ribbon. This avoids the friction and 'play' caused by a pivot, and the slight bending force of the spring merely adds to the pendulum's restoring force. A few precision clocks have pivots of 'knife' blades resting on agate plates. The impulses to keep the pendulum swinging are provided by an arm hanging behind the pendulum called the \"crutch\", \"(e)\", which ends in a \"fork\", \"(f)\" whose prongs embrace the pendulum rod. The crutch is pushed back and forth by the clock's escapement, \"(g,h)\".\n\nEach time the pendulum swings through its centre position, it releases one tooth of the \"escape wheel\" \"(g)\". The force of the clock's mainspring or a driving weight hanging from a pulley, transmitted through the clock's gear train, causes the wheel to turn, and a tooth presses against one of the pallets \"(h)\", giving the pendulum a short push. The clock's wheels, geared to the escape wheel, move forward a fixed amount with each pendulum swing, advancing the clock's hands at a steady rate.\n\nThe pendulum always has a means of adjusting the period, usually by an adjustment nut \"(c)\" under the bob which moves it up or down on the rod. Moving the bob up decreases the pendulum's length, causing the pendulum to swing faster and the clock to gain time. Some precision clocks have a small auxiliary adjustment weight on a threaded shaft on the bob, to allow finer adjustment. Some tower clocks and precision clocks use a tray attached near to the midpoint of the pendulum rod, to which small weights can be added or removed. This effectively shifts the centre of oscillation and allows the rate to be adjusted without stopping the clock.\n\nThe pendulum must be suspended from a rigid support. During operation, any elasticity will allow tiny imperceptible swaying motions of the support, which disturbs the clock's period, resulting in error. Pendulum clocks should be attached firmly to a sturdy wall.\n\nThe most common pendulum length in quality clocks, which is always used in grandfather clocks, is the seconds pendulum, about long. In mantel clocks, half-second pendulums, long, or shorter, are used. Only a few large tower clocks use longer pendulums, the 1.5 second pendulum, long, or occasionally the two-second pendulum, which is used in Big Ben.\n\nThe largest source of error in early pendulums was slight changes in length due to thermal expansion and contraction of the pendulum rod with changes in ambient temperature. This was discovered when people noticed that pendulum clocks ran slower in summer, by as much as a minute per week (one of the first was Godefroy Wendelin, as reported by Huygens in 1658). Thermal expansion of pendulum rods was first studied by Jean Picard in 1669. A pendulum with a steel rod will expand by about 11.3 parts per million (ppm) with each degree Celsius increase, causing it to lose about 0.27 seconds per day for every degree Celsius increase in temperature, or 9 seconds per day for a change. Wood rods expand less, losing only about 6 seconds per day for a change, which is why quality clocks often had wooden pendulum rods. The wood had to be varnished to prevent water vapor from getting in, because changes in humidity also affected the length.\n\nThe first device to compensate for this error was the mercury pendulum, invented by George Graham in 1721. The liquid metal mercury expands in volume with temperature. In a mercury pendulum, the pendulum's weight (bob) is a container of mercury. With a temperature rise, the pendulum rod gets longer, but the mercury also expands and its surface level rises slightly in the container, moving its centre of mass closer to the pendulum pivot. By using the correct height of mercury in the container these two effects will cancel, leaving the pendulum's centre of mass, and its period, unchanged with temperature. Its main disadvantage was that when the temperature changed, the rod would come to the new temperature quickly but the mass of mercury might take a day or two to reach the new temperature, causing the rate to deviate during that time. To improve thermal accommodation several thin containers were often used, made of metal. Mercury pendulums were the standard used in precision regulator clocks into the 20th century.\n\nThe most widely used compensated pendulum was the gridiron pendulum, invented in 1726 by John Harrison. This consists of alternating rods of two different metals, one with lower thermal expansion (CTE), steel, and one with higher thermal expansion, zinc or brass. The rods are connected by a frame, as shown in the drawing at the right, so that an increase in length of the zinc rods pushes the bob up, shortening the pendulum. With a temperature increase, the low expansion steel rods make the pendulum longer, while the high expansion zinc rods make it shorter. By making the rods of the correct lengths, the greater expansion of the zinc cancels out the expansion of the steel rods which have a greater combined length, and the pendulum stays the same length with temperature.\n\nZinc-steel gridiron pendulums are made with 5 rods, but the thermal expansion of brass is closer to steel, so brass-steel gridirons usually require 9 rods. Gridiron pendulums adjust to temperature changes faster than mercury pendulums, but scientists found that friction of the rods sliding in their holes in the frame caused gridiron pendulums to adjust in a series of tiny jumps. In high precision clocks this caused the clock's rate to change suddenly with each jump. Later it was found that zinc is subject to creep. For these reasons mercury pendulums were used in the highest precision clocks, but gridirons were used in quality regulator clocks.\n\nGridiron pendulums became so associated with good quality that, to this day, many ordinary clock pendulums have decorative 'fake' gridirons that don't actually have any temperature compensation function.\n\nAround 1900, low thermal expansion materials were developed which could be used as pendulum rods in order to make elaborate temperature compensation unnecessary. These were only used in a few of the highest precision clocks before the pendulum became obsolete as a time standard. In 1896 Charles Édouard Guillaume invented the nickel steel alloy Invar. This has a CTE of around 0.5 µin/(in·°F), resulting in pendulum temperature errors over 71 °F of only 1.3 seconds per day, and this residual error could be compensated to zero with a few centimeters of aluminium under the pendulum bob (this can be seen in the Riefler clock image above). Invar pendulums were first used in 1898 in the Riefler regulator clock which achieved accuracy of 15 milliseconds per day. Suspension springs of Elinvar were used to eliminate temperature variation of the spring's restoring force on the pendulum. Later fused quartz was used which had even lower CTE. These materials are the choice for modern high accuracy pendulums.\n\nThe effect of the surrounding air on a moving pendulum is complex and requires fluid mechanics to calculate precisely, but for most purposes its influence on the period can be accounted for by three effects:\nIncreases in barometric pressure increase a pendulum's period slightly due to the first two effects, by about 0.11 seconds per day per kilopascal (0.37 seconds per day per inch of mercury or 0.015 seconds per day per torr). Researchers using pendulums to measure the acceleration of gravity had to correct the period for the air pressure at the altitude of measurement, computing the equivalent period of a pendulum swinging in vacuum. A pendulum clock was first operated in a constant-pressure tank by Friedrich Tiede in 1865 at the Berlin Observatory, and by 1900 the highest precision clocks were mounted in tanks that were kept at a constant pressure to eliminate changes in atmospheric pressure. Alternatively, in some a small aneroid barometer mechanism attached to the pendulum compensated for this effect.\n\nPendulums are affected by changes in gravitational acceleration, which varies by as much as 0.5% at different locations on Earth, so precision pendulum clocks have to be recalibrated after a move. Even moving a pendulum clock to the top of a tall building can cause it to lose measurable time from the reduction in gravity.\n\nThe timekeeping elements in all clocks, which include pendulums, balance wheels, the quartz crystals used in quartz watches, and even the vibrating atoms in atomic clocks, are in physics called harmonic oscillators. The reason harmonic oscillators are used in clocks is that they vibrate or oscillate at a specific resonant frequency or period and resist oscillating at other rates. However, the resonant frequency is not infinitely 'sharp'. Around the resonant frequency there is a narrow natural band of frequencies (or periods), called the resonance width or bandwidth, where the harmonic oscillator will oscillate. In a clock, the actual frequency of the pendulum may vary randomly within this resonance width in response to disturbances, but at frequencies outside this band, the clock will not function at all.\n\nThe measure of a harmonic oscillator's resistance to disturbances to its oscillation period is a dimensionless parameter called the \"Q\" factor equal to the resonant frequency divided by the resonance width. The higher the \"Q\", the smaller the resonance width, and the more constant the frequency or period of the oscillator for a given disturbance. The reciprocal of the Q is roughly proportional to the limiting accuracy achievable by a harmonic oscillator as a time standard.\n\nThe \"Q\" is related to how long it takes for the oscillations of an oscillator to die out. The \"Q\" of a pendulum can be measured by counting the number of oscillations it takes for the amplitude of the pendulum's swing to decay to 1/\"e\" = 36.8% of its initial swing, and multiplying by 2\"π\".\n\nIn a clock, the pendulum must receive pushes from the clock's movement to keep it swinging, to replace the energy the pendulum loses to friction. These pushes, applied by a mechanism called the escapement, are the main source of disturbance to the pendulum's motion. The \"Q\" is equal to 2\"π\" times the energy stored in the pendulum, divided by the energy lost to friction during each oscillation period, which is the same as the energy added by the escapement each period. It can be seen that the smaller the fraction of the pendulum's energy that is lost to friction, the less energy needs to be added, the less the disturbance from the escapement, the more 'independent' the pendulum is of the clock's mechanism, and the more constant its period is. The \"Q\" of a pendulum is given by:\n\nwhere \"M\" is the mass of the bob, \"ω\" = 2\"π\"/\"T\" is the pendulum's radian frequency of oscillation, and \"Γ\" is the frictional damping force on the pendulum per unit velocity.\n\n\"ω\" is fixed by the pendulum's period, and \"M\" is limited by the load capacity and rigidity of the suspension. So the \"Q\" of clock pendulums is increased by minimizing frictional losses (\"Γ\"). Precision pendulums are suspended on low friction pivots consisting of triangular shaped 'knife' edges resting on agate plates. Around 99% of the energy loss in a freeswinging pendulum is due to air friction, so mounting a pendulum in a vacuum tank can increase the \"Q\", and thus the accuracy, by a factor of 100.\n\nThe \"Q\" of pendulums ranges from several thousand in an ordinary clock to several hundred thousand for precision regulator pendulums swinging in vacuum. A quality home pendulum clock might have a \"Q\" of 10,000 and an accuracy of 10 seconds per month. The most accurate commercially produced pendulum clock was the Shortt-Synchronome free pendulum clock, invented in 1921. Its Invar master pendulum swinging in a vacuum tank had a \"Q\" of 110,000 and an error rate of around a second per year.\n\nTheir Q of 10–10 is one reason why pendulums are more accurate timekeepers than the balance wheels in watches, with \"Q\" around 100–300, but less accurate than the quartz crystals in quartz clocks, with \"Q\" of 10–10.\n\nPendulums (unlike, for example, quartz crystals) have a low enough \"Q\" that the disturbance caused by the impulses to keep them moving is generally the limiting factor on their timekeeping accuracy. Therefore, the design of the escapement, the mechanism that provides these impulses, has a large effect on the accuracy of a clock pendulum. If the impulses given to the pendulum by the escapement each swing could be exactly identical, the response of the pendulum would be identical, and its period would be constant. However, this is not achievable; unavoidable random fluctuations in the force due to friction of the clock's pallets, lubrication variations, and changes in the torque provided by the clock's power source as it runs down, mean that the force of the impulse applied by the escapement varies.\n\nIf these variations in the escapement's force cause changes in the pendulum's width of swing (amplitude), this will cause corresponding slight changes in the period, since (as discussed at top) a pendulum with a finite swing is not quite isochronous. Therefore, the goal of traditional escapement design is to apply the force with the proper profile, and at the correct point in the pendulum's cycle, so force variations have no effect on the pendulum's amplitude. This is called an \"isochronous escapement\".\n\nIn 1826 British astronomer George Airy proved what clockmakers had known for centuries; that the disturbing effect of a drive force on the period of a pendulum is smallest if given as a short impulse as the pendulum passes through its bottom equilibrium position. Specifically, he proved that if a pendulum is driven by an impulse that is symmetrical about its bottom equilibrium position, the pendulum's period will be unaffected by changes in the drive force. The most accurate escapements, such as the deadbeat, approximately satisfy this condition.\n\nThe presence of the acceleration of gravity \"g\" in the periodicity equation (1) for a pendulum means that the local gravitational acceleration of the Earth can be calculated from the period of a pendulum. A pendulum can therefore be used as a gravimeter to measure the local gravity, which varies by over 0.5% across the surface of the Earth. The pendulum in a clock is disturbed by the pushes it receives from the clock movement, so freeswinging pendulums were used, and were the standard instruments of gravimetry up to the 1930s.\n\nThe difference between clock pendulums and gravimeter pendulums is that to measure gravity, the pendulum's length as well as its period has to be measured. The period of freeswinging pendulums could be found to great precision by comparing their swing with a precision clock that had been adjusted to keep correct time by the passage of stars overhead. In the early measurements, a weight on a cord was suspended in front of the clock pendulum, and its length adjusted until the two pendulums swung in exact synchronism. Then the length of the cord was measured. From the length and the period, \"g\" could be calculated from equation (1).\n\nThe seconds pendulum, a pendulum with a period of two seconds so each swing takes one second, was widely used to measure gravity, because its period could be easily measured by comparing it to precision regulator clocks, which all had seconds pendulums. By the late 17th century, the length of the seconds pendulum became the standard measure of the strength of gravitational acceleration at a location. By 1700 its length had been measured with submillimeter accuracy at several cities in Europe. For a seconds pendulum, \"g\" is proportional to its length:\n\n\n\nThe precision of the early gravity measurements above was limited by the difficulty of measuring the length of the pendulum, \"L\" . \"L\" was the length of an idealized simple gravity pendulum (described at top), which has all its mass concentrated in a point at the end of the cord. In 1673 Huygens had shown that the period of a rigid bar pendulum (called a \"compound pendulum\") was equal to the period of a simple pendulum with a length equal to the distance between the pivot point and a point called the center of oscillation, located under the center of gravity, that depends on the mass distribution along the pendulum. But there was no accurate way of determining the center of oscillation in a real pendulum.\n\nTo get around this problem, the early researchers above approximated an ideal simple pendulum as closely as possible by using a metal sphere suspended by a light wire or cord. If the wire was light enough, the center of oscillation was close to the center of gravity of the ball, at its geometric center. This \"ball and wire\" type of pendulum wasn't very accurate, because it didn't swing as a rigid body, and the elasticity of the wire caused its length to change slightly as the pendulum swung.\n\nHowever Huygens had also proved that in any pendulum, the pivot point and the center of oscillation were interchangeable. That is, if a pendulum were turned upside down and hung from its center of oscillation, it would have the same period as it did in the previous position, and the old pivot point would be the new center of oscillation.\n\nBritish physicist and army captain Henry Kater in 1817 realized that Huygens' principle could be used to find the length of a simple pendulum with the same period as a real pendulum. If a pendulum was built with a second adjustable pivot point near the bottom so it could be hung upside down, and the second pivot was adjusted until the periods when hung from both pivots were the same, the second pivot would be at the center of oscillation, and the distance between the two pivots would be the length \"L\" of a simple pendulum with the same period.\n\nKater built a reversible pendulum (shown at right) consisting of a brass bar with two opposing pivots made of short triangular \"knife\" blades \"(a)\" near either end. It could be swung from either pivot, with the knife blades supported on agate plates. Rather than make one pivot adjustable, he attached the pivots a meter apart and instead adjusted the periods with a moveable weight on the pendulum rod \"(b,c)\". In operation, the pendulum is hung in front of a precision clock, and the period timed, then turned upside down and the period timed again. The weight is adjusted with the adjustment screw until the periods are equal. Then putting this period and the distance between the pivots into equation (1) gives the gravitational acceleration \"g\" very accurately.\n\nKater timed the swing of his pendulum using the \"method of coincidences\" and measured the distance between the two pivots with a micrometer. After applying corrections for the finite amplitude of swing, the buoyancy of the bob, the barometric pressure and altitude, and temperature, he obtained a value of 39.13929 inches for the seconds pendulum at London, in vacuum, at sea level, at 62 °F. The largest variation from the mean of his 12 observations was 0.00028 in. representing a precision of gravity measurement of 7×10 (7 mGal or 70 µm/s). Kater's measurement was used as Britain's official standard of length (see below) from 1824 to 1855.\n\nReversible pendulums (known technically as \"convertible\" pendulums) employing Kater's principle were used for absolute gravity measurements into the 1930s.\n\nThe increased accuracy made possible by Kater's pendulum helped make gravimetry a standard part of geodesy. Since the exact location (latitude and longitude) of the 'station' where the gravity measurement was made was necessary, gravity measurements became part of surveying, and pendulums were taken on the great geodetic surveys of the 18th century, particularly the Great Trigonometric Survey of India.\n\n\nRelative pendulum gravimeters were superseded by the simpler LaCoste zero-length spring gravimeter, invented in 1934 by Lucien LaCoste. Absolute (reversible) pendulum gravimeters were replaced in the 1950s by free fall gravimeters, in which a weight is allowed to fall in a vacuum tank and its acceleration is measured by an optical interferometer.\n\nBecause the acceleration of gravity is constant at a given point on Earth, the period of a simple pendulum at a given location depends only on its length. Additionally, gravity varies only slightly at different locations. Almost from the pendulum's discovery until the early 19th century, this property led scientists to suggest using a pendulum of a given period as a standard of length.\n\nUntil the 19th century, countries based their systems of length measurement on prototypes, metal bar primary standards, such as the standard yard in Britain kept at the Houses of Parliament, and the standard \"toise\" in France, kept at Paris. These were vulnerable to damage or destruction over the years, and because of the difficulty of comparing prototypes, the same unit often had different lengths in distant towns, creating opportunities for fraud. During the Enlightenment scientists argued for a length standard that was based on some property of nature that could be determined by measurement, creating an indestructible, universal standard. The period of pendulums could be measured very precisely by timing them with clocks that were set by the stars. A pendulum standard amounted to defining the unit of length by the gravitational force of the Earth, for all intents constant, and the second, which was defined by the rotation rate of the Earth, also constant. The idea was that anyone, anywhere on Earth, could recreate the standard by constructing a pendulum that swung with the defined period and measuring its length.\n\nVirtually all proposals were based on the seconds pendulum, in which each swing (a half period) takes one second, which is about a meter (39 inches) long, because by the late 17th century it had become a standard for measuring gravity (see previous section). By the 18th century its length had been measured with sub-millimeter accuracy at a number of cities in Europe and around the world.\n\nThe initial attraction of the pendulum length standard was that it was believed (by early scientists such as Huygens and Wren) that gravity was constant over the Earth's surface, so a given pendulum had the same period at any point on Earth. So the length of the standard pendulum could be measured at any location, and would not be tied to any given nation or region; it would be a truly democratic, worldwide standard. Although Richer found in 1672 that gravity varies at different points on the globe, the idea of a pendulum length standard remained popular, because it was found that gravity only varies with latitude. Gravitational acceleration increases smoothly from the equator to the poles, due to the oblate shape of the Earth, so at any given latitude (east-west line), gravity was constant enough that the length of a seconds pendulum was the same within the measurement capability of the 18th century. Thus the unit of length could be defined at a given latitude and measured at any point along that latitude. For example, a pendulum standard defined at 45° north latitude, a popular choice, could be measured in parts of France, Italy, Croatia, Serbia, Romania, Russia, Kazakhstan, China, Mongolia, the United States and Canada. In addition, it could be recreated at any location at which the gravitational acceleration had been accurately measured.\n\nBy the mid 19th century, increasingly accurate pendulum measurements by Edward Sabine and Thomas Young revealed that gravity, and thus the length of any pendulum standard, varied measurably with local geologic features such as mountains and dense subsurface rocks. So a pendulum length standard had to be defined at a single point on Earth and could only be measured there. This took much of the appeal from the concept, and efforts to adopt pendulum standards were abandoned.\n\nOne of the first to suggest defining length with a pendulum was Flemish scientist Isaac Beeckman who in 1631 recommended making the seconds pendulum \"the invariable measure for all people at all times in all places\". Marin Mersenne, who first measured the seconds pendulum in 1644, also suggested it. The first official proposal for a pendulum standard was made by the British Royal Society in 1660, advocated by Christiaan Huygens and Ole Rømer, basing it on Mersenne's work, and Huygens in \"Horologium Oscillatorium\" proposed a \"horary foot\" defined as 1/3 of the seconds pendulum. Christopher Wren was another early supporter. The idea of a pendulum standard of length must have been familiar to people as early as 1663, because Samuel Butler satirizes it in \"Hudibras\":\nIn 1671 Jean Picard proposed a pendulum-defined 'universal foot' in his influential \"Mesure de la Terre\". Gabriel Mouton around 1670 suggested defining the \"toise\" either by a seconds pendulum or a minute of terrestrial degree. A plan for a complete system of units based on the pendulum was advanced in 1675 by Italian polymath Tito Livio Burratini. In France in 1747, geographer Charles Marie de la Condamine proposed defining length by a seconds pendulum at the equator; since at this location a pendulum's swing wouldn't be distorted by the Earth's rotation. James Steuart (1780) and George Skene Keith were also supporters.\n\nBy the end of the 18th century, when many nations were reforming their weight and measure systems, the seconds pendulum was the leading choice for a new definition of length, advocated by prominent scientists in several major nations. In 1790, then US Secretary of State Thomas Jefferson proposed to Congress a comprehensive decimalized US 'metric system' based on the seconds pendulum at 38° North latitude, the mean latitude of the United States. No action was taken on this proposal. In Britain the leading advocate of the pendulum was politician John Riggs Miller. When his efforts to promote a joint British–French–American metric system fell through in 1790, he proposed a British system based on the length of the seconds pendulum at London. This standard was adopted in 1824 (below).\n\nIn the discussions leading up to the French adoption of the metric system in 1791, the leading candidate for the definition of the new unit of length, the metre, was the seconds pendulum at 45° North latitude. It was advocated by a group led by French politician Talleyrand and mathematician Antoine Nicolas Caritat de Condorcet. This was one of the three final options considered by the French Academy of Sciences committee. However, on March 19, 1791 the committee instead chose to base the metre on the length of the meridian through Paris. A pendulum definition was rejected because of its variability at different locations, and because it defined length by a unit of time. (However, since 1983 the metre has been officially defined in terms of the length of the second and the speed of light.) A possible additional reason is that the radical French Academy didn't want to base their new system on the second, a traditional and nondecimal unit from the \"ancien regime\".\n\nAlthough not defined by the pendulum, the final length chosen for the metre, 10 of the pole-to-equator meridian arc, was very close to the length of the seconds pendulum (0.9937 m), within 0.63%. Although no reason for this particular choice was given at the time, it was probably to facilitate the use of the seconds pendulum as a secondary standard, as was proposed in the official document. So the modern world's standard unit of length is certainly closely linked historically with the seconds pendulum.\n\nBritain and Denmark appear to be the only nations that (for a short time) based their units of length on the pendulum. In 1821 the Danish inch was defined as 1/38 of the length of the mean solar seconds pendulum at 45° latitude at the meridian of Skagen, at sea level, in vacuum. The British parliament passed the \"Imperial Weights and Measures Act\" in 1824, a reform of the British standard system which declared that if the prototype standard yard was destroyed, it would be recovered by defining the inch so that the length of the solar seconds pendulum at London, at sea level, in a vacuum, at 62 °F was 39.1393 inches. This also became the US standard, since at the time the US used British measures. However, when the prototype yard was lost in the 1834 Houses of Parliament fire, it proved impossible to recreate it accurately from the pendulum definition, and in 1855 Britain repealed the pendulum standard and returned to prototype standards.\n\nA pendulum in which the rod is not vertical but almost horizontal was used in early seismometers for measuring earth tremors. The bob of the pendulum does not move when its mounting does, and the difference in the movements is recorded on a drum chart.\n\nAs first explained by Maximilian Schuler in a 1923 paper, a pendulum whose period exactly equals the orbital period of a hypothetical satellite orbiting just above the surface of the earth (about 84 minutes) will tend to remain pointing at the center of the earth when its support is suddenly displaced. This principle, called Schuler tuning, is used in inertial guidance systems in ships and aircraft that operate on the surface of the Earth. No physical pendulum is used, but the control system that keeps the inertial platform containing the gyroscopes stable is modified so the device acts as though it is attached to such a pendulum, keeping the platform always facing down as the vehicle moves on the curved surface of the Earth.\n\nIn 1665 Huygens made a curious observation about pendulum clocks. Two clocks had been placed on his mantlepiece, and he noted that they had acquired an opposing motion. That is, their pendulums were beating in unison but in the opposite direction; 180° out of phase. Regardless of how the two clocks were started, he found that they would eventually return to this state, thus making the first recorded observation of a coupled oscillator.\n\nThe cause of this behavior was that the two pendulums were affecting each other through slight motions of the supporting mantlepiece. This process is called entrainment or mode locking in physics and is observed in other coupled oscillators. Synchronized pendulums have been used in clocks and were widely used in gravimeters in the early 20th century. Although Huygens only observed out-of-phase synchronization, recent investigations have shown the existence of in-phase synchronization, as well as \"death\" states wherein one or both clocks stops.\n\nPendulum motion appears in religious ceremonies as well. The swinging incense burner called a censer, also known as a thurible, is an example of a pendulum. Pendulums are also seen at many gatherings in eastern Mexico where they mark the turning of the tides on the day which the tides are at their highest point. See also pendulums for divination and dowsing.\n\nPendulums are widely used in science education as an example of a harmonic oscillator, to teach dynamics and oscillatory motion. One use is to demonstrate the law of conservation of energy. A heavy object such as a bowling ball or wrecking ball is attached to a string. The weight is then moved to within a few inches of a volunteer's face, then released and allowed to swing and come back. In most instances, the weight reverses direction and then returns to (almost) the same position as the original release location — \"i.e.\" a small distance from the volunteer's face — thus leaving the volunteer unharmed. On occasion the volunteer is injured if either the volunteer does not stand still or the pendulum is initially released with a push (so that when it returns it surpasses the release position).\n\nIt is claimed that the pendulum was used as an instrument of torture and execution by the Spanish Inquisition in the 18th century. The allegation is contained in the 1826 book \"The history of the Inquisition of Spain \" by the Spanish priest, historian and liberal activist Juan Antonio Llorente. A swinging pendulum whose edge is a knife blade slowly descends toward a bound prisoner until it cuts into his body. This method of torture came to popular consciousness through the 1842 short story \"The Pit and the Pendulum\" by American author Edgar Allan Poe but there is considerable skepticism that it actually was used.\n\nMost knowledgeable sources are skeptical that this torture was ever actually used. The only evidence of its use is one paragraph in the preface to Llorente's 1826 \"History\", relating a second-hand account by a single prisoner released from the Inquisition's Madrid dungeon in 1820, who purportedly described the pendulum torture method. Modern sources point out that due to Jesus' admonition against bloodshed, Inquisitors were only allowed to use torture methods which did not spill blood, and the pendulum method would have violated this stricture. One theory is that Llorente misunderstood the account he heard; the prisoner was actually referring to another common Inquisition torture, the \"strappado\" (garrucha), in which the prisoner has his hands tied behind his back and is hoisted off the floor by a rope tied to his hands. This method was also known as the \"pendulum\". Poe's popular horror tale, and public awareness of the Inquisition's other brutal methods, has kept the myth of this elaborate torture method alive.\n\nThe value of g reflected by the period of a pendulum varies from place to place. The gravitational force varies with distance from the center of the Earth, i.e. with altitude - or because the Earth's shape is oblate, g varies with latitude.\nA more important cause of this reduction in g at the equator is because the equator is spinning at one revolution per day, reducing the gravitational force there.\n\nNote: most of the sources below, including books, can be viewed online through the links given.\n\n"}
{"id": "54305958", "url": "https://en.wikipedia.org/wiki?curid=54305958", "title": "Pentamethylbismuth", "text": "Pentamethylbismuth\n\nPentamethylbismuth (or pentamethylbismuthorane) is an organometalllic compound containing five methyl groups bound to a bismuth atom with formula Bi(CH). It is an example of a hypervalent compound. The molecular shape is trigonal bipyramid.\n\nPentamethylbismuth is produced in a two step process. First, trimethylbismuth is reacted with sulfuryl chloride to yield dichloro trimethylbismuth, which is then reacted with twice as much methyllithium dissolved in ether. The blue solution is cooled to −110 °C to precipitate the solid product.\n\nAt -110 °C, Bi(CH) is a blue-violet solid. The methyl groups are arranged in a trigonal bipyramid, and the bond-lengths of methyl with bismuth are all the same. However, the molecule is not rigid, as can be determined from the nuclear magnetic resonance spectrum that shows all methyl groups are equivalent. It is stable as a solid, but in the gas phase, when heated or in solution decomposes to trimethylbismuth.\nThe colour is unusual for bismuth or other hypervalent pnictide compounds, which are colourless. Calculations show that the colour is due to HOMO-LUMO transition. The HOMO is ligand based, whereas the LUMO is modified by relativistically stabilised bismuth 6s orbitals.\n\nIf excess methyllithium is used in production, an orange hexamethylbismuth salt, LiBi(CH), is formed . This is unusual as very few elements can form bonds with six organic groups.\n"}
{"id": "2167447", "url": "https://en.wikipedia.org/wiki?curid=2167447", "title": "Pentode", "text": "Pentode\n\nA pentode is an electronic device having five active electrodes. The term most commonly applies to a three-grid amplifying vacuum tube (thermionic valve), which was invented by Gilles Holst and Bernhard D.H. Tellegen in 1926. The pentode consists of an evacuated glass envelope containing five electrodes in this order: a cathode heated by a filament, a control grid, a screen grid, a suppressor grid, and a plate (anode). The pentode (called a \"triple-grid amplifier\" in some early literature) was developed from the tetrode tube by the addition of a third grid, the suppressor grid. This served to prevent secondary emission electrons emitted by the plate from reaching the screen grid, which caused instability and parasitic oscillations in the tetrode. The pentode is closely related to the beam tetrode. Pentodes were widely used in industrial and consumer electronic equipment such as radios and televisions until the 1960s, when they were replaced by transistors. Their main use now is in high power industrial applications such as radio transmitters. The obsolete consumer tubes are still used in a few legacy and specialty vacuum tube audio devices.\n\n\nThe simple tetrode or \"screen-grid tube\" offered a larger amplification factor, more power and a higher frequency capability than the earlier triode. However, in the tetrode \"secondary electrons\" knocked out of the anode (plate) by the electrons from the cathode striking it (a process called secondary emission) can flow to the screen grid due to its relatively high potential. This current of electrons leaving the anode reduces the net anode current \"I\". As the anode voltage \"V\" is increased, the electrons from the cathode hit the anode with more energy, knocking out more secondary electrons, increasing this current of electrons leaving the anode. The result is that in the tetrode the anode current \"I\" is found to \"decrease\" with increasing anode voltage \"V\", over part of the characteristic curve. This property (Δ\"V\"/Δ\"I\" < 0) is called negative resistance. It can cause the tetrode to become unstable, leading to parasitic oscillations in the output, called dynatron oscillations in some circumstances.\n\nThe pentode, as introduced by Tellegen, has an additional electrode, or third grid, called the suppressor grid, located between the screen grid and the anode, which solves the problem of secondary emission. The suppressor grid is given a low potential, it is usually either grounded or connected to the cathode. Secondary emission electrons from the anode are repelled by the negative potential on the suppressor grid, so they can't reach the screen grid but return to the anode. The primary electrons from the cathode have a higher kinetic energy, so they can still pass through the suppressor grid and reach the anode.\n\nPentodes, therefore, can have higher current outputs and a wider output voltage swing; the anode/plate can even be at a lower voltage than the screen grid yet still amplify well.\n\n\nPentode tubes were first used in consumer-type radio receivers. A well-known pentode type, the EF50, was designed before the start of World War II, and was extensively used in radar sets and other military electronic equipment. The pentode contributed to the electronic preponderance of the Allies.\n\nThe Colossus computer and the Manchester Baby used large numbers of EF36 pentode tubes. Later on, the 7AK7 tube was expressly developed for use in computer equipment.\n\nAfter World War II, pentodes were widely used in TV receivers, particularly the successor to the EF50, the EF80. Vacuum tubes were replaced by transistors during the 1960s. However, they continue to be used in certain applications, including high-power radio transmitters and (because of their well-known valve sound) in high-end and professional audio applications, microphone preamplifiers and electric guitar amplifiers. Large stockpiles in countries of the former Soviet Union have provided a continuing supply of such devices, some designed for other purposes but adapted to audio use, such as the GU-50 transmitter tube.\n\nA pentode can have its screen grid (grid 2) connected to the anode (plate), in which case it reverts to an ordinary triode with commensurate characteristics (lower anode resistance, lower mu, lower noise, more drive voltage required). The device is then said to be \"triode-strapped\" or \"triode-connected\". This is sometimes provided as an option in audiophile pentode amplifier circuits, to give the sought-after \"sonic qualities\" of a triode power amplifier. A resistor may be included in series with the screen grid to avoid exceeding the screen grid's power or voltage rating, and to prevent local oscillation. Triode-connection is a useful option for audiophiles who wish to avoid the expense of 'true' power triodes.\n\n"}
{"id": "5613213", "url": "https://en.wikipedia.org/wiki?curid=5613213", "title": "Piña", "text": "Piña\n\nPiña is a fiber made from the leaves of a pineapple plant and is commonly used in the Philippines (also known as nanas or nenas in Tagalog). It is sometimes combined with silk or polyester to create a textile fabric. Piña's name comes from the Spanish word \"piña\" which means pineapple.\n\nSince piña is from a leaf, the leaf has to be cut first from the plant. Then the fiber is pulled or split away from the leaf. Most leaf fibers are long and somewhat stiff. Each strand of the piña fiber is hand scraped and is knotted one by one to form a continuous filament to be handwoven and then made into a piña cloth.\n\nKalibo, Aklan, is the main and the oldest manufacturer/weaver of piña cloth in the Philippines which are being exported to various parts of the world most particularly North America, and Europe. Piña weaving is an age-old tradition which was recently revived in the past 20 years. \n\nPineapple silk was considered the queen of Philippine fabrics and is considered the fabric of choice of the Philippine elite. During the 1996 APEC summit held in the Philippines, world leaders donned Barong Tagalog made of piña sourced from Kalibo during the group photo.\n\nProducers include La Herminia Piña Weaving Industry, Malabon Pina Producers and Weavers Association, Reycon's Piña Cloth and Industry, and Rurungan sa Tubod Foundation.\n\nA major use for piña fabric is in the creation of the Barong Tagalog and other formal wear in the Philippines. It is also used for other table linens, bags, mats and other clothing items, or anytime that a lightweight, but stiff and sheer fabric is needed.\n\n\n"}
{"id": "2007252", "url": "https://en.wikipedia.org/wiki?curid=2007252", "title": "Pounce (calligraphy)", "text": "Pounce (calligraphy)\n\nPounce is a fine powder, most often made from powdered cuttlefish bone, that was used both to dry ink and to sprinkle on a rough writing surface to make it smooth enough for writing. This last was certainly needed if the paper came \"unsized\", that is, lacking the thin gelatinous material used to fill the surface of the paper and make it smooth enough for writing with a quill or a steel nib. It was also used to prepare the surface when drafting with Rapidiograph pens on mylar, a common drafting medium in the late twentieth century. \n\nAlthough some people claim that pounce was never added afterwards to dry ink, this probably represents confusion between the two processes of preparing paper and drying the ink after writing. Experiment shows that using pounce does indeed smooth \"unsized\" paper but then does little or nothing to dry the ink after you have written on that prepared paper, and it is clearly the case that pouncing or sanding continued long after properly \"sized\" writing paper came into general use during the nineteenth century. \n\nThe pounce or sand is gently sprinkled all over the writing on the paper. When using a quill or a steel nib, and with inks that are made up to match those typically in use during the 18th or 19th centuries, and provided the pen has been used with the fine strokes typical of handwriting of that period, the handwriting will be sufficiently dry within 10 seconds to allow the paper to be folded without blotting. Gently vibrating the paper whilst the pounce or sand is on it ensures that little or no pounce or sand sticks to the handwriting and excess sand or pounce is shaken off before folding the paper. \n\nIn the 19th century the pounce pots or sanders often had a shallow dish round the top so that pounce or sand could be returned to the pot and reused. The process is very effective for quickly drying ink, and although blotting paper has been available since Tudor times, pounce or sand continued to be used throughout the nineteenth century because it was often cheaper.\n"}
{"id": "1149802", "url": "https://en.wikipedia.org/wiki?curid=1149802", "title": "Rotary converter", "text": "Rotary converter\n\nA rotary converter is a type of electrical machine which acts as a mechanical rectifier, inverter or frequency converter. \n\nRotary converters were used to convert alternating current (AC) to direct current (DC), or DC to AC power, before the advent of chemical or solid state power rectification and inverting. They were commonly used to provide DC power for commercial, industrial and railway electrification from an AC power source.\n\nRotary converters are still used as frequency converters.\n\nThe rotary converter can be thought of as a motor-generator, where the two machines share a single rotating armature and set of field coils. The basic construction of the rotary converter consists of a DC generator (dynamo) with a set of slip rings tapped into its rotor windings at evenly spaced intervals. When a dynamo is spun the electric currents in its rotor windings alternate as it rotates in the magnetic field of the stationary field windings. This alternating current is rectified by means of a commutator which allows DC current to be extracted from the rotor. This principle is taken advantage of by energizing the same rotor windings with AC power which causes the machine to act as a synchronous AC motor. The rotation of the energized coils excites the stationary field windings producing part of the DC current. The other part is AC current from the slip rings which is directly rectified into DC by the commutator. This makes the rotary converter a hybrid dynamo and mechanical rectifier. When used in this way it is referred to as a synchronous rotary converter or simply a synchronous converter. The AC slip rings also allow the machine to act as an alternator. \n\nThe device can be reversed and DC applied to the field and commutator windings to spin the machine and produce AC power. When operated as a DC to AC machine it is referred to as an inverted rotary converter.\n\nOne way to envision what is happening in an AC-to-DC rotary converter is to imagine a rotary reversing switch that is being driven at a speed that is synchronous with the power line. Such a switch could rectify the AC input waveform with no magnetic components at all save those driving the switch. The rotary converter is somewhat more complex than this trivial case because it delivers near-DC rather than the pulsating DC that would result from just the reversing switch, but the analogy may be helpful in understanding how the rotary converter avoids transforming all of the energy from electrical to mechanical and back to electrical.\n\nThe advantage of the rotary converter over the discrete motor-generator set is that the rotary converter avoids converting all of the power flow into mechanical energy and then back into electrical energy; some of the electrical energy instead flows directly from input to output, allowing the rotary converter to be much smaller and lighter than a motor-generator set of an equivalent power-handling capability. The advantages of a motor-generator set include adjustable voltage regulation which can compensate for voltage drop in the supply network; it also provided complete power isolation, harmonics isolation, greater surge and transient protection, and sag (brownout) protection through increased momentum.\n\nIn this first illustration of a single-phase to direct-current rotary converter, it may be used five different ways:\n\nA typical use for an AC/DC converter was for railway electrification, where utility power was supplied as alternating current but the trains were designed to work on direct current. Before the invention of mercury arc rectifiers and high-power semiconductor rectifiers, this conversion could only be accomplished using motor-generators or rotary converters.\n\nAround 1900 most machinery and appliances were operated by DC power which was provided by rotary converter substations for residential, commercial and industrial consumption. Rotary converters provided high current DC power for industrial electrochemical processes such as electroplating. Steel mills needed large amounts of on-site DC power for their main roll drive motors. Similarly, paper mills and printing presses required direct current to start and stop their motors in perfect synchronization to prevent tearing the sheet.\n\nAC to DC synchronous rotary converters were made obsolete by mercury arc rectifiers in the 1930s and later on by semiconductor rectifiers in the 1960s. Some of the original New York City Subway substations using synchronous rotary converters operated until 1999. Compared to the rotary converter, the mercury arc and semiconductor rectifiers did not need daily maintenance, manual synchronizing for parallel operation, nor skilled personnel, and they provided clean DC power. This enabled the new substations to be unmanned, only requiring periodic visits from a technician for inspection and maintenance. \n\nAC replaced DC in most applications and eventually the need for local DC substations diminished along with the need for rotary converters. Many DC customers converted to AC power, and on-site solid-state DC rectifiers were used to power the remaining DC equipment from the AC supply.\n\nThe self-balancing dynamo is of similar construction to the single- and two-phase rotary converter. It was commonly used to create a completely balanced three-wire 120/240-volt DC electrical supply. The AC extracted from the slip rings was fed into a transformer with a single center-tapped winding. The center-tapped winding forms the DC neutral wire. It needed to be driven by a mechanical power source, such as a steam engine, diesel engine, or electric motor. It could be considered a rotary converter used as a double current generator; the AC current was used to balance the DC neutral wire.\n\n"}
{"id": "340302", "url": "https://en.wikipedia.org/wiki?curid=340302", "title": "Slow flight", "text": "Slow flight\n\nSlow flight is a portion of an airplane's performance envelope above the speed at which the plane will stall, but below the aircraft's endurance speed. This part of the performance chart is also known as \"the back side of the power curve\" because when flying in this area, \"more power is required to fly at a speed lower than the minimum drag speed\" and still maintain straight and level flight. A large angle of attack is required in order to maintain the altitude of the aircraft.\n\nAt such low speeds, aircraft flight control surfaces begin to lose their effectiveness. Ailerons, in particular, are susceptible. The rudder remains the most efficient flight surface and the adverse effect of yaw which alters bank angle is useful for altering the direction of the aircraft without the need for aileron inputs. If the ailerons are used excessively it is probable that one wing will stall (due to the increased angle of attack of the wing with the downward aileron) and send the aircraft into a spin. In modern aircraft, flight envelope protection in the aircraft flight control system prevents a pilot from controlling their aircraft into doing this.\n"}
{"id": "30977268", "url": "https://en.wikipedia.org/wiki?curid=30977268", "title": "Sodium ethyl xanthate", "text": "Sodium ethyl xanthate\n\nSodium ethyl xanthate (SEX) is an organosulfur compound with the chemical formula CHCHOCSNa. It is a pale yellow powder, which is usually obtained as the dihydrate. Sodium ethyl xanthate is used in the mining industry as a flotation agent. A closely related potassium ethyl xanthate (KEX) is obtained as the anhydrous salt.\n\nAs with most xanthates, sodium ethyl xanthate can be prepared by treating sodium ethoxide with carbon disulfide:\n\nSodium ethyl xanthate is a pale yellow powder. It is relatively stable in water at high pH if not heated. It rapidly hydrolyses at pH <9 at 25 °C. It is the conjugate base of the unknown strong acid with p\"K\" of 1.6 and p\"K\" estimated as 12.4 for the conjugate base. Sodium ethyl xanthate easily adsorbs on the surface of solid sulfides. \n\nXanthate are susceptible to hydrolysis and oxidation:\n\nThese reactions require acidic conditions.\n\nSodium ethyl xanthate can be identified through optical absorption peaks in the infrared (1179, 1160, 1115, 1085 cm) and ultraviolet (300 nm) ranges. There are at least six chemical detection methods: \n\n\nSodium ethyl xanthate can also be quantified using gravimetry, by weighing the lead xanthate residue obtained after reacting SEX with 10% solution of lead nitrate. There are also several electrochemical detection methods, which can be combined with some of the above chemical techniques.\n\nSodium ethyl xanthate is predominantly used in the mining industry as flotation agent for recovery of metals, such as copper, nickel, silver or gold, as well as solid metal sulfides or oxides from ore slurries. This application was introduced by Cornelius H. Keller in 1925. Other applications include defoliant, herbicide and an additive to rubber to protect it against oxygen and ozone.\n\nThe mechanism of flotation enhancement is as follows. The polar part of xanthate molecule attaches to the ore particles with the non-polar hydrocarbon part sticking out and forming a hydrophobic layer. Then the particles are brought to the water surface by air bubbles. Only a small amount of about 300 g/tonne of ore is required for efficient separation. The efficiency of the hydrophobic action increases, but the selectivity to ore type decreases with increasing length of the hydrocarbon chain in xanthates. The chain is shortest in sodium ethyl xanthate that makes it highly selective to copper, nickel, lead, gold and zinc ores. Aqueous solutions (10%) with pH=7–11 are normally used in the process.\n\nIn 2000, Australia produced up to 10,000 tonnes of sodium ethyl xanthate and imported about 6,000 tonnes, mostly from China. The material produced in Australia is the so-called 'liquid sodium ethyl xanthate' that refers to a 40% aqueous solution of the solid. It is obtained by reacting carbon disulfide with sodium hydroxide and ethanol in a closed process. Its density is 1.2 g/cm and the freezing point is −6 °C.\n\nSodium ethyl xanthate has moderate oral and dermal toxicity in animals and is irritating to eyes and skin. It is especially toxic to aquatic life and therefore its disposal is strictly controlled. Median lethal dose for (male albino mice, oral, 10% solution at pH~11) is 730 mg/kg of body weight, with most deaths occurring in the first day. The most affected organs were the central nervous system, liver and spleen. \n\nSince 1993, sodium ethyl xanthate is classified as a Priority Existing Chemical in Australia, meaning that its manufacture, handling, storage, use or disposal may result in adverse health or environment effects. This decision was justified by the widespread use of the chemical in industry and its decomposition to the toxic and flammable carbon disulfide gas. From two examples of sodium ethyl xanthate spillage in Australia, one resulted in evacuation of 100 people and hospitalization of 6 workers who were exposed to the fumes. In another accident, residents of the spillage area complained of headache, dizziness and nausea. Consequently, during high-risk sodium ethyl xanthate handling operations, workers are required by the Australian regulations to be equipped with protective clothing, anti-static gloves, boots and full-face respirators or self-contained breathing apparatus.\n\n"}
{"id": "547045", "url": "https://en.wikipedia.org/wiki?curid=547045", "title": "Strip-built", "text": "Strip-built\n\nStrip-built is a method of boat building commonly used for canoes and kayaks, but also suitable for larger boats. The process involves securing narrow, flexible strips of wood edge-to-edge around temporary forms.\n\nThese are the most popular among boatbuilders. Some professional builders also offer both kits and finished boats. The canoes are constructed by gluing together 1/4\" x 3/4\" strips of wood over a building jig consisting of station molds that define the shape of the hull. The forms are cut as a series of cross-sections of the final design and set up along a \"strong back\" or other solid base. The strips are shaped with bead and cove router bits. Stripping begins at the sheer line and finishes with \"the football\". The strips are edge-glued to each other, being held in place with nails, staples, or simply clamped to the forms. Once the strips are glued together, and the staples/nails removed, the inside and outside are sanded fair. Fiberglass and epoxy is applied to the canoe inside and out. The fiberglass covering is transparent, waterproof, and allowing the wood strips to be seen. The strips are usually cedar, but can be any type of wood. Contrasting woods are sometimes used as accent strips. The last steps in construction is to install the seats, thwarts, and gunwales. Finally a coat of marine grade polyurethane is applied to protect the wood and epoxy from ultraviolet light.\n\nIn the 1950s, this process for building canoes was adapted from ship/boat building techniques, and refined by a group of Minnesota canoe racers; primarily Eugene Jensen, Irwin C.(Buzzy) Peterson, and Karl Ketter.This process is similarly suited to building kayaks.\n\n"}
{"id": "31612433", "url": "https://en.wikipedia.org/wiki?curid=31612433", "title": "Sunmobile", "text": "Sunmobile\n\nSunmobile was a model of a solar-powered automobile. William G. Cobb of the General Motors Corporation built and demonstrated his 15-inch long model at the 1955 General Motors car show in Chicago on August 31, 1955. The automobile was a futuristic miniature representation to show the possibilities of solar energy. Since it was a miniature model, it could not be driven by a person.\n\nThe name of the 1955 General Motors Motorama car show where Cobb's model car was shown was General Motors Powerama. General Motors at the time said the car that showed futuristic capabilities was not practical because even if the solar cells ran at 100% efficiency they would only produce 12 horse-power, not enough to propel an average automobile of the time.\n\nCobb showed and introduced the field of photovoltaics to a car show that had over 2,000,000 visitors. At the time all automobiles were run by gasoline engines. Cobb's Sunmobile model had 12 selenium photoelectric cells on top of a balsa wood body. These solar cells were connected in series-parallel and converted the sun light directly into electricity, which in turn ran a small low-inertia electric motor. The motor rotated at 2000 r.p.m. and ran on 1.5 volts. The motor's energy in turn was transferred to the model car's drive shaft, which then in turn transferred the energy through its rear axle by a pulley to the car's wheels which moved the car forward.\n\nThe model car balsa wood body was in five sections. It had two center sections that were hollowed out to receive a pine chassis, the electric motor, and the drive mechanism. There were precision gears from the electric motor to the wheels. They had a 3 to 1 ratio, with the smallest gear on the motor and the larger gear on the drive mechanism that propelled the rear wheels. These gears were connected with a ladder-type chain connected to their sprockets. The rubber tires, obtained at a hobby shop, were one and a half inches in diameter. The rear axle was one-eighth inch in diameter. The wiring for the solar cells to the electric motor was by standard No. 20 insulated stranded copper wire. \n\n"}
{"id": "4309963", "url": "https://en.wikipedia.org/wiki?curid=4309963", "title": "Tactile transducer", "text": "Tactile transducer\n\nA tactile transducer or \"bass shaker\" is a device which is made on the principle that low bass frequencies can be felt as well as heard. They can be compared with a common loudspeaker, just that the diaphragm is missing. Instead, another object is used as a diaphragm. A shaker transmits low-frequency vibrations into various surfaces so that they can be felt by people. This is called tactile sound. Tactile transducers may augment or in some cases substitute for a subwoofer. One benefit of tactile transducers is they produce little or no noise, if properly installed, as compared with a subwoofer speaker enclosure.\n\nA bass-shaker is meant to be firmly attached to some surface such as a seat, couch or floor. The shaker houses a small weight which is driven by a voice coil similar to those found in dynamic loudspeakers. The voice-coil is driven by a low-frequency audio signal from an amplifier; common shakers typically handle 25 to 50 watts of amplifier power. The voice coil exerts force on both the weight and the body of the shaker, with the latter forces being transmitted into the mounting surface. Tactile transducers may be used in a home theater, a video gaming chair or controller, a commercial movie theater, or for special effects in an arcade game, amusement park ride or other application.\n\nRelated to bass shakers are a newer type of transducer referred to as linear actuators. These piston-like electromagnetic devices transmit motion in a direct fashion by lifting home theater seating in the vertical plane rather than transferring vibrations (by mounting within a seat, platform or floor). This technology is said to transmit a high-fidelity sound-motion augmentation, whereas \"Shakers\" may require heavy equalization and/or multiple units to approach a realistic effect. \n\nThere are other products which employ hydraulic (long-throw) linear actuators and outboard motion processors for home applications as popularized in \"virtual reality\" rides. These products differ radically from tactile transducers in that they require the manual composition and synchronized playback of motion signals, in addition to the standard soundtrack that the motion is meant to accompany.\n\nVarious designs for tactile transducers have been presented since the 1960s, most of which fall under the \"shaker\" category. Shakers create a vigorous vibration by moving a mass (usually a magnet) which is bolted to a final mass (like a chair or couch). A simple example of this is the vibration available on a common cellphone. Another way of producing tactile sound uses \"linear actuators\", which move furniture (usually up and down), rather than shaking it. The main advantage of linear actuators is that they deliver actual motion (ground excursion), not just vibration.\n\nIn the 2010s, tactile sound transducers have evolved to include higher frequencies and produce higher fidelity. The human tactile frequency range is from 1 Hz, very low frequency such as earthquakes, up to 5 kHz in some hearing impaired individuals. For most individuals 2 to 3 kHz is the upper threshold for tactile reception. These 2010s-era devices must have higher resolution than previous \"shakers\" to produce these frequencies. Most humans have tactile resolution to 2 Hz which is the smallest change in frequency that can be perceived. The primary use for this extended bandwidth is to reproduce the vibratory signature for musical instruments such as violins, guitars, the human voice or sound effects in movies (for example, the speeders in \"Star Wars\"). Also higher frequencies may be used to augment hearing through bone conduction, a consideration for people who have compromised their hearing from exposure to loud music.\n\nTactile sound is often used to increase the realism of an artificial environment. For example, mounting a tactile sound transducer in a chair or couch in a home cinema or video game setup can give more of a sense of \"being there\". For such use, the transducer is often connected to the LFE channel of an A/V receiver. Tactile sound is often used in combination with a subwoofer so that low frequencies can be both felt and heard. To facilitate broadband tactile sound, all channels are summed to provide a full range signal to the transducer amplifier. Graphic equalizers can also be used to further modify the effect.\n\nFor musical performance, drummers will often use a tactile sound transducer mounted on their drum stool so they can \"feel\" themselves playing, rather than using a more conventional stage monitor. The size and power of a stage monitor required to adequately reproduce low frequency drum sounds would be expensive and hard to transport, while a tactile sound transducer can be rather small and require much less power to get the job done. As well, sound engineers may prefer a tactile transducer over a loud, powerful subwoofer monitor cabinet, because a monitor speaker may produce a lot of stage volume.\n\nNicolas Collins describes several tactile transducers, including some wide-range drivers, which are able to transmit a broader frequency spectrum. The composer David Tudor used tactile transducers in his work \"Rainforest\" (1968). He used \"Rolen-Star\" wide-range drivers to create all kinds of different loudspeaker sculptures. The tactile transducers are attached to large objects such as metal buckets and bring these objects in vibration. The vibrations of these sculptures are then picked up by contact microphones and amplified through a common loudspeaker system. More recent examples can be found in the work of Sabrina Schroeder, who places tactile transducers on bass drums. Lynn Pook attaches small tactile transducers on the bodies of the audience members and Carola Bauckholt used in her piece \"Doppelbelichtung\" for violin 12 so-called violin loudspeakers. These consists of violins hanging from the ceiling, each with a small tactile transducer attached to them.\n"}
{"id": "677931", "url": "https://en.wikipedia.org/wiki?curid=677931", "title": "Tandem", "text": "Tandem\n\nTandem, or in tandem, is an arrangement in which a team of machines, animals or people are lined up one behind another, all facing in the same direction.\n\nThe original use of the term in English was in \"tandem harness\", which is used for two or more draft horses, or other draft animals, harnessed in a single line one behind another, as opposed to a pair, harnessed side by side, or a team of several pairs. The tandem harness allows additional animals to provide pulling power for a vehicle designed for a single animal.\n\nThe English word \"tandem\" derives with a word play from the Latin adverb , meaning \"at length\" or \"finally\".\n\nTandem seating may be used on a tandem bicycle where it is alternative to sociable seating. \"Tandem\" can also be used more generally to refer to any group of persons or objects working together, not necessarily in line.\n\nThe Messerschmitt KR200 was an example of a very small automobile that used tandem seating. A tandem arrangement may also be used for cars parked in a residential garage.\n\nIn heavy trucks tandem refers to two closely spaced axles. Legally defined by the distance between the axles (up to in the European Union, in the United States), mechanically there are many configurations. Either or both axles may be powered, and often interact with each other. In the United States both axles are typically powered and equalized, in the European Union one axle is typically unpowered, and can often be adjusted to load, and even raised off the ground, turning a tandem into a single-axle.\n\nThe two seating configurations for trainer, night and all-weather interceptor or attack aircraft are pilot and instructor side by side or in tandem. Usually, the pilot is in front and the instructor behind. In attack helicopters, sometimes the pilot sits in back with the weapons operator in front for better view to aim weapons, as the Bell AH-1 Cobra was a tandem cockpit redesign which produced a much slimmer profile than the Bell UH-1 Iroquois on which it was based. Attack aircraft and all-weather interceptors often use a second crew member to operate avionics such as radar, or as a second pilot. Bombers such as the Convair B-58 Hustler seated three crew members in tandem. A common engineering adaptation is to lengthen the cockpit or fuselage to create a trainer with tandem seating from a single-seater aircraft.\n\nAn alternative configuration is side-by-side seating, which is common in civil aircraft of all sizes, trainers and large military aircraft, but less so in high performance jets and gliders where drag reduction is paramount. The Boeing B-47 Stratojet and Boeing XB-52 bombers used fighter-style tandem seating, but the final B-52 bomber series used a conventional side-by-side cockpit. The Grumman A-6 Intruder, General Dynamics F-111 Aardvark, Sukhoi Su-24 or the Sukhoi Su-34 are examples of combat aircraft that use this configuration. For training aircraft, it has the advantage that pilot and instructor can see each other's actions, allowing the pilot to learn from the instructor and the instructor to correct the student pilot. The tandem configuration has the advantage of being closer to the normal working environment that a fast jet pilot is likely to encounter.\n\nIn some cases, such as the Northrop Grumman EA-6B Prowler, a two-place aircraft can be lengthened into a four-place aircraft. Also, a single seat cockpit can be redesigned into a side-by-side arrangement in the case of the Douglas A-1 Skyraider, TF-102 trainer or the Hawker Hunter training versions.\n\n\n"}
{"id": "22878213", "url": "https://en.wikipedia.org/wiki?curid=22878213", "title": "The Energy and Resources Institute", "text": "The Energy and Resources Institute\n\nThe Energy and Resources Institute (TERI) is a research institute based in New Delhi that conducts research work in the fields of energy, environment and sustainable development. Established in 1974, it was formerly known as Tata Energy and Research Institute. As the scope of its activities widened, it was renamed The Energy and Resources Institute in 2003.\n\nThe origins of TERI lie in Mithapur, a remote town in Gujarat, where a TATA engineer, Darbari Seth, was concerned about the enormous quantities of energy his factory spent on desalination. He proposed the idea of a research institute to tackle the depletion of natural resources and energy scarcity. J. R. D. Tata, chairman of the TATA Group, liked the idea and accepted the proposal. TERI was set up with a modest corpus of 35 million rupees. On the invitation of the then Prime Minister Indira Gandhi, TERI was registered in Delhi in 1974 as the Tata Energy Research Institute.\n\nTERI initially began its operations in the Bombay House, Mumbai, headquarters of Tata. In 1984, it moved to Delhi where it continued to operate out on the rented premises (which included the India International Centre ) for almost a decade. In 1993, the organization set up its permanent base in Darbari Seth Block, named after its founder, in the India Habitat Centre complex located at Lodhi Road, New Delhi. Today TERI has a global presence with many centres in India and abroad.\n\nIn October 2011, Princess Máxima of the Netherlands opened the European headquarters of TERI in Utrecht.\n\nTERI established a research base in Africa to provide technical assistance as well as to facilitate exchange of knowledge amongst the communities in various African states.\n\nIn 2016-17, TERI set up the world's biggest facility for Mycorrhiza production in Gual Pahari, Gurugram,Haryana.\n\nTERI has over 1250 employees, with research professionals from disciplines pertaining to issues of environment and energy. The Institute's present Director General is Dr Ajay Mathur.\n\nThe scope of the organisation's activities includes climate change, energy efficiency, renewable energy, biotechnology, and social transformation.\n\n\n\nTERI Press, TERI's publishing arm releases a plethora of publications out of which some noteworthy publications are :\n\n\nGreen Rating for Integrated Habitat Assessment (GRIHA) was conceived by TERI and developed with Ministry of New and Renewable Energy, is a national rating system for green buildings in India.\n\nTERI School of Advanced Studies was established on 19 August 1998, and was recognised by the University Grants Commission (UGC) as a deemed-to-be University in 1999. Set-up as the TERI School of Advanced Studies in 1998, the institution was subsequently renamed TERI University.\n\nEstablished in January 2015 with a vision to provide sustainability education and help create environmental awareness among children at an early age. Inaugurated by Union Minister for Environment and Forests Prakash Javadekar, the K-12 school is affiliated to CBSE.\n\n"}
{"id": "1793822", "url": "https://en.wikipedia.org/wiki?curid=1793822", "title": "The Riverkeepers", "text": "The Riverkeepers\n\nThe Riverkeepers: Two Activists Fight to Reclaim Our Environment as a Basic Human Right (1997) is an academic work by John Cronin and Robert F. Kennedy, Jr., with the foreword by Al Gore.\n\nNew York, NY : Scribner, c1997, \n\n"}
{"id": "685232", "url": "https://en.wikipedia.org/wiki?curid=685232", "title": "The Wetlands Institute", "text": "The Wetlands Institute\n\nThe Wetlands Institute is a non-profit organization started in 1969 by the executive director of WWF, Herbert Mills. The Wetlands Institute sits on 6,000 acres (24 km²) of protected wetlands in Stone Harbor, New Jersey. It hosts educational tours and courses and is a base for research on wetlands ecology.\n\nIt is home to the annual Wings 'n Water art festival.\n"}
{"id": "2699278", "url": "https://en.wikipedia.org/wiki?curid=2699278", "title": "Trihydrogen cation", "text": "Trihydrogen cation\n\nThe trihydrogen cation, also known as protonated molecular hydrogen or , is one of the most abundant ions in the universe. It is stable in the interstellar medium (ISM) due to the low temperature and low density of interstellar space. The role that plays in the gas-phase chemistry of the ISM is unparalleled by any other molecular ion. The cation is also the simplest triatomic molecule, since its two electrons are the only valence electrons in the system. It is also the simplest example of a three-center two-electron bond system.\n\n was first discovered by J. J. Thomson in 1911. While studying the resultant species of plasma discharges, he discovered something very odd. Using an early form of mass spectrometry, he discovered a large abundance of a molecular ion with a mass-to-charge ratio of 3. He stated that the only two possibilities were C or . Since C would be very unlikely and the signal grew stronger in pure hydrogen gas, he correctly assigned the species as .\n\nThe formation pathway was discovered by Hogness & Lunn in 1925. They also used an early form of mass spectrometry to study hydrogen discharges. They found that as the pressure of hydrogen increased, the amount of increased linearly and the amount of decreased linearly. In addition, there was little H at any pressure. This data suggested the proton exchange formation pathway discussed below.\n\nIn 1961, Martin \"et al.\" first suggested that may be present in interstellar space given the large amount of hydrogen in interstellar space and its reaction pathway was exothermic (~1.5 eV). This led to the suggestion of Watson and Herbst & Klemperer in 1973 that is responsible for the formation of many observed molecular ions.\n\nIt was not until 1980 that the first spectrum of was discovered by Takeshi Oka, which was of the ν fundamental band using a technique called frequency modulation detection. This started the search for interstellar . Emission lines were detected in the late 1980s and early 1990s in the ionospheres of Jupiter, Saturn, and Uranus.\n\nIn 1996, was finally detected in the interstellar medium (ISM) by Geballe & Oka in two molecular interstellar clouds in the sightlines GL2136 and W33A. In 1998, was unexpectedly detected by McCall \"et al.\" in a diffuse interstellar cloud in the sightline Cygnus OB2#12. In 2006 Oka announced that was ubiquitous in interstellar medium, and that the Central Molecular Zone contained a million times the concentration of ISM generally.\n\nThe three hydrogen atoms in the molecule form an equilateral triangle, with a bond length of 0.90 Å on each side. The bonding among the atoms is a three-centre two-electron bond, a delocalized resonance hybrid type of structure. The strength of the bond has been calculated to be around 4.5 eV (104 kcal/mol).\n\nThe main pathway for the production of is by the reaction of and H.\n\nThe concentration of is what limits the rate of this reaction in nature: the only known natural source of it is via ionization of H by a cosmic ray in interstellar space by the ionization of H:\n\nThe cosmic ray has so much energy, it is almost unaffected by the relatively small energy transferred to the hydrogen when ionizing an H molecule. In interstellar clouds, cosmic rays leave behind a trail of , and therefore . In laboratories, is produced by the same mechanism in plasma discharge cells, with the discharge potential providing the energy to ionize the H.\n\nThe information for this section was also from a paper by Eric Herbst. There are many destruction reactions for . The dominant destruction pathway in dense interstellar clouds is by proton transfer with a neutral collision partner. The most likely candidate for a destructive collision partner is the second most abundant molecule in space, CO.\n\nThe significant product of this reaction is HCO, an important molecule for interstellar chemistry. Its strong dipole and high abundance make it easily detectable by radioastronomy. can also react with atomic oxygen to form OH and H.\n\nOH then usually reacts with more H to create further hydrogenated molecules.\n\nAt this point, the reaction between and H is no longer exothermic in interstellar clouds. The most common destruction pathway for is dissociative recombination, yielding four possible sets of products: HO + H, OH + H, OH + 2H, and O + H + H. While water is a possible product of this reaction, it is not a very efficient product. Different experiments have suggested that water is created anywhere from 5–33% of the time. Water formation on grains is still considered the primary source of water in the interstellar medium.\n\nThe most common destruction pathway of in diffuse interstellar clouds is dissociative recombination. This reaction has multiple products. The major product is dissociation into three hydrogen atoms, which occurs roughly 75% of the time. The minor product is H and H, which occurs roughly 25% of the time.\n\nThe most abundant molecule in dense interstellar clouds is H. When a molecule collides with H, stoichiometrically there is no net yield. However, a proton transfer still can take place, which can potentially change the total nuclear spin of the two molecules depending on the nuclear spins of the protons. Two different spin configurations for are possible, called ortho and para. Ortho- has all three proton spins parallel, yielding a total nuclear spin of . Para- has two proton spins parallel while the other is anti-parallel, yielding a total nuclear spin of . Similarly, H also has ortho and para states, with ortho-H having a total nuclear spin 1 and para-H having a total nuclear spin of 0. When an ortho- and a para-H collide, the transferred proton changes the total spins of the molecules, yielding instead a para- and an ortho-H.\n\nThe spectroscopy of is challenging. The pure rotational spectrum is exceedingly weak. Ultraviolet light is too energetic and would dissociate the molecule. Rovibronic (Infrared) spectroscopy provides the ability to observe . Rovibronic spectroscopy is possible with because one of the vibrational modes of , the ν asymmetric bend mode, has a weak transition dipole moment. Since Oka's initial spectrum, over 900 absorption lines have been detected in the infrared region. emission lines have also been found by observing the atmospheres of the Jovian planets. emission lines are found by observing molecular hydrogen and finding a line that cannot be attributed to molecular hydrogen.\n\n has been detected in two types of celestial environments: Jovian planets and interstellar clouds. In Jovian planets, it has been detected in the planet's ionospheres, the region where the Sun's high energy radiation ionizes the particles in the atmosphere. Since there is a high level of H in these atmospheres, this radiation can produce a significant amount of . Also, with a broadband source like the Sun, there is plenty of radiation to pump the to higher energy states from which it can relax by stimulated and spontaneous emission.\n\nThe detection of the first emission lines was reported in 1989 by Drossart \"et al.\", found in the ionosphere of Jupiter. Drossart found a total of 23 lines with a column density of 1.39/cm. Using these lines, they were able to assign a temperature to the of around , which is comparable to temperatures determined from emission lines of other species like H. In 1993, was found in Saturn by Geballe \"et al.\" and in Uranus by Trafton \"et al.\"\n\n was not detected in the interstellar medium until 1996, when Geballe & Oka reported the detection of in two molecular cloud sightlines, GL2136 and W33A. Both sources had temperatures of of about and column densities of about 10/cm. Since then, has been detected in numerous other molecular cloud sightlines, such as AFGL 2136, Mon R2 IRS 3, GCS 3-2, GC IRS 3, and LkHα 101.\n\nUnexpectedly, three lines were detected in 1998 by McCall \"et al.\" in the diffuse cloud sightline of Cyg OB2 No. 12. Before 1998, the density of H was thought to be too low to produce a detectable amount of . McCall detected a temperature of ~ and a column density of ~10/cm, the same column density as Geballe & Oka. Since then, has been detected in many other diffuse cloud sightlines, such as GCS 3-2, GC IRS 3, and ζ Persei.\n\nTo approximate the pathlength of in these clouds, Oka used the steady-state model to determine the predicted number densities in diffuse and dense clouds. As explained above, both diffuse and dense clouds have the same formation mechanism for , but different dominating destruction mechanisms. In dense clouds, proton transfer with CO is the dominating destruction mechanism. This corresponds to a predicted number density of 10 cm in dense clouds.\nIn diffuse clouds, the dominating destruction mechanism is dissociative recombination. This corresponds to a predicted number density of 10/cm in diffuse clouds. Therefore, since column densities for diffuse and dense clouds are roughly the same order of magnitude, diffuse clouds must have a pathlength 100 times greater than that for dense clouds. Therefore, by using as a probe of these clouds, their relative sizes can be determined.\n\n"}
{"id": "55193320", "url": "https://en.wikipedia.org/wiki?curid=55193320", "title": "Tullygarran ogham stones", "text": "Tullygarran ogham stones\n\nThe Tullygarran ogham stones are a pair of ogham stones forming a National Monument located in County Kerry, Ireland.\n\nTullygarran Ogham Stones are located east of Tralee, near to Chute Hall.\n\nThe stones were discovered in 1848 after a storm uncovered an ancient burial ground overlooking Smerwick Bay. Dayrolles Eveleigh-de-Moleyns, 4th Baron Ventry moved them to his home at Chute Hall.\n\nThe stones are:\n"}
{"id": "1412775", "url": "https://en.wikipedia.org/wiki?curid=1412775", "title": "Wicker", "text": "Wicker\n\nWicker is a technique for making products woven from any one of a variety of cane-like materials, a generic name for the materials used in such manufacture, and a term for the items so produced. The word \"wicker\" is believed to be of Scandinavian origin: \"vika\" which means to bend in Swedish, and \"vikker\" meaning willow. Wicker is traditionally made of material of plant origin, such as rattan, willow, reed, and bamboo, but synthetic fibers are now also used. Wicker is light yet sturdy, making it suitable for items that will be moved often like porch and patio furniture. \"Rushwork\" and wickerwork are terms used in England.\n\nWicker has been documented as far back as ancient Egypt, made from indigenous \"reed and swamp grasses.\" Middle-class families could only afford a few pieces, such as small tables. However, archaeologists working on the tombs of the wealthy pharaohs have uncovered a wider variety of wicker items, including \"chests, baskets, wig boxes, and chairs\". Wicker even found use in the Achaemenid Empire on the battlefield, in shields.\n\nThe popularity of wicker passed from ancient Egypt and Persia to ancient Rome. Wicker baskets were used to carry items in Pompeii. Furniture was manufactured out of wicker in the Roman style. It has been proposed that the extensive use of wicker in the Iron Age (1200 BC – 400 AD in Europe) may have influenced the development of the woven patterns used in Celtic art. By the 16th and 17th centuries, wicker was \"quite common\" in European countries like Portugal, Spain, and England.\n\nWicker received a boost during the Age of Exploration, when international sea traders returned from southeast Asia with a species of palm called rattan. Rattan is stronger than traditional European wicker materials, although the rattan stem can be separated so the softer inner core can be used for wicker.\n\nThe 19th century brought immense popularity for wicker in Europe, England, and North America. It was used outdoors as well as indoors. People in the Victorian Era believed it to be more sanitary than upholstered furniture. It was inexpensive, resisted harsh weather and was adaptable to many styles.\n\nIn the United States, Cyrus Wakefield began constructing rattan furniture in the 1850s. He first used rattan that had been offloaded from ships, where it was used as ballast, but as his designs became well-known, he began importing the material himself. Wakefield's company became one of the leading industries in wicker furniture; it later merged with the Heywood Chair Manufacturing Company (a wooden chair company that had invented a mechanical process for weaving wicker seats) to form the Heywood-Wakefield of Gardner, Massachusetts, one of the oldest and most prominent North American wicker manufacturers.\n\nIn recent times, its aesthetic was influenced heavily by the Arts and Crafts movement at the turn of the 20th century.\n\nWicker is still a popular material. Antique wicker products are highly sought after by collectors. Reproductions of furniture and accent pieces are also sold for indoor and outdoor use. (In North America today, \"rattan\" and \"wicker\" are frequently used interchangeably.) Wickerwork is an important industry in Poland, employing hundreds of skilled workers to create goods for export to western Europe.\n\nNatural wicker is well known for its strength and durability, as well as the high level of beauty and comfort that an expert craftsperson can create. Materials used can be any part of a plant, such as the cores of cane or rattan stalks, or whole thicknesses of plants, as with willow switches. Other popular materials include reed and bamboo. Natural wicker requires maintenance to keep it in good shape.\n\nWicker can also be made from synthetic materials, or a combination. In furniture, such as benches, chairs, stools and other seating devices, a frame is typically made of stiffer materials, after which more pliant material is woven into the frame to fill it. In a smaller piece such as a basket, a strengthening frame is not needed so the entire piece is woven from the wicker material.\n\nSynthetic types include paper-wrapped high tensile wire (using the Lloyd Loom process patented in the early 20th century), and plastic or resin. The synthetic wickers are often preferred for outdoor use (\"all-weather wicker\"). The frame material used in these more recent versions includes aluminum.\n\nThe word wicker is from the Middle English \"wiker\", of Scandinavian origin.\n\n\n"}
{"id": "26520744", "url": "https://en.wikipedia.org/wiki?curid=26520744", "title": "Z-tube", "text": "Z-tube\n\nThe Z-tube is an experimental apparatus for measuring the tensile strength of a liquid.\n\nIt consists of a Z-shaped tube with open ends, filled with a liquid, and set on top of a spinning table. If the tube were straight, the liquid would immediately fly out one end or the other of the tube as it began to spin. By bending the ends of the tube back towards the center of rotation, a shift of the liquid away from center will result in the water level in one end of the tube rising and thus increasing the pressure in that end of the tube, and consequently returning the liquid to the center of the tube. By measuring the rotational speed and the distance from the center of rotation to the liquid level in the bent ends of the tube, the pressure reduction inside the tube can be calculated.\n\nNegative pressures, (i.e. less than zero absolute pressure, or in other words, tension) have been reported using water processed to remove dissolved gases. Tensile strengths up to 280 atmospheres have been reported for water in glass.\n"}
