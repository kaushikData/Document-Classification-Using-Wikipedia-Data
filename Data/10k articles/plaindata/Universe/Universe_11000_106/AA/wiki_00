{"id": "56168286", "url": "https://en.wikipedia.org/wiki?curid=56168286", "title": "1958 Mailuu-Suu tailings dam failure", "text": "1958 Mailuu-Suu tailings dam failure\n\nThe 1958 Mailuu-Suu tailings dam failure in the industrial town of Mailuu-Suu, (Kyrgyz: Майлуусуу), Jalal-Abad Region, southern Kyrgyzstan, caused the uncontrolled release of of radioactive waste. \n\nThe event caused a number of direct casualties and widespread environmental damage. It was the single worst incident in a region of arid, mountainous western Kyrgyzstan, with a collection of shuttered Soviet-era uranium mining and processing sites, a legacy of extensive radioactive waste dumps, and a history of flooding and mudslides.\n\nAs of 2017, despite recent remediations funded by the World Bank and others, the treatment of radioactive waste at Mailuu-Suu still poses serious health and safety risks for local residents. \n\nOil was discovered here in the early 1900s. Deposits of radium-bearing barites had been discovered by Alexander Fersman in 1929, during his national mineralogical resources survey for the new Soviet government. Uranium mining began in 1946, organized by the \"Zapadnyi Mining and Chemical Combine\". In addition to mining, two uranium plants would process more than of uranium ore, by ion exchange and alkaline leach, to produce uranium oxide for Soviet atomic bomb projects. The processed ore was both mined locally and imported from elsewhere in the Eastern bloc. \n\nThe town was classified as one of the Soviet government's secret cities, officially known only as \"Mailbox 200\".\n\nUranium mining was halted in 1968. Operations left behind some 23 separate uranium tailings dams and 13 waste rock dumps, poorly designed on unstable hillsides above a town of 20,000 people in an area prone to both landslides and earthquakes, holding a total of material containing radionuclides and heavy metals. No attempt to stabilize or seal the material was done when Soviet mining ceased. \n\nOn April 16, 1958, with mining and processing plants still operational, a combination of poor design, neglect, heavy rainfall and a reported earthquake caused the #7 tailings dam at Mailuu-Suu to fail. About 50% of the entire volume of the dam flowed into the swift Mailuu-Suu River, only downhill from the breach. The waste then spread about downstream across the national border into Uzbekistan then into the heavily populated Fergana Valley. The Mailuu-Suu River is a tributary of the Kara Darya, used for agricultural irrigation in the valley. \n\nSome fatalities, building destruction, and contamination of the flood plain were reported as the direct result of the mudflow. Lack of any public response by officials makes it difficult to identify fatalities from the April 1958 event, especially as distinguished from everyday exposure. \n\nLongterm health effects are more measurable. Grave threats to long-term residents persist, with residents experiencing far higher rates of cancer, goiter, anemia, and other illnesses related to radiological exposure.\n\nMailuu-Suu was found to be one of the 10 most polluted sites in the world in a study published in 2006 by the Blacksmith Institute.\n\nAnnual spring flooding and the lack of maintenance pose a continued threat of further releases of radioactive material. In 1994, a new landslide temporarily dammed the Mailuu-Suu River. In 2002 a flood caused by a mudslide nearly submerged a tailings pit. \n\nThe World Bank approved a US$5 million grant to reclaim the tailings pits in 2004, and approved an additional $1 million grant for the project in 2011. The United Nations Development Programme, and the European Bank for Reconstruction and Development have also funded programs. \n"}
{"id": "20340167", "url": "https://en.wikipedia.org/wiki?curid=20340167", "title": "Advanced Thin Ionization Calorimeter", "text": "Advanced Thin Ionization Calorimeter\n\nThe Advanced Thin Ionization Calorimeter (ATIC) is a balloon-borne instrument flying in the stratosphere over Antarctica to measure the energy and composition of cosmic rays. ATIC was launched from McMurdo Station for the first time in December 2000 and has since completed three successful flights out of four.\n\nThe detector uses the principle of ionization calorimetry: several layers of the scintillator bismuth germanate emit light as they are struck by particles, allowing to calculate the particles' energy. A silicon matrix is used to determine the particles' electrical charge.\n\nThe project is an international collaboration of researchers from Louisiana State University, University of Maryland, College Park, Marshall Space Flight Center, Purple Mountain Observatory in China, Moscow State University in Russia and Max Planck Institute for Solar System Research in Germany. ATIC is supported in the United States by NASA and flights are conducted under the auspices of the Balloon Program Office at Wallops Flight Facility by the staff of the Columbia Scientific Balloon Facility. Antarctic logistics are provided by the National Science Foundation and its contractor Raytheon Polar Services Corporation.\nThe principal investigator for ATIC is John Wefel of Louisiana State University.\n\nIn November 2008, researchers published in \"Nature\" the finding of a surplus of high energy electrons. During a 5-week observatory period in 2000 and 2003, ATIC counted 70 electrons with energies in the range 300-800 GeV; these electrons were in excess of those expected from the galactic background. The source of these electrons is unknown, but it is assumed to be relatively close, no more than about 3000 lightyears away, since high energy electrons rapidly lose energy as they travel through the galactic magnetic field and collide with photons. The electrons could originate from a nearby pulsar or other astrophysical object, but the researchers were not able to identify a fitting object. According to another conjecture, the electrons result from collisions of Dark Matter particles, for example WIMP Kaluza-Klein particles of mass near 620 GeV.\n\nEarlier in the year, the satellite PAMELA had found excess positrons (the antiparticle of the electron) in the cosmic ray signal, also believed to originate from dark matter interactions.\nATIC cannot distinguish between electrons and positrons, so it is possible that the two results are compatible.\n\nOn the other hand, in November 2008 the Milagro experiment reported cosmic ray \"hotspots\" in the sky, possibly supporting astrophysical objects as sources of the surplus electrons. In May 2009, observations by the Fermi space telescope were reported which did not support the spike of high-energy electrons seen by ATIC.\n\n"}
{"id": "9023486", "url": "https://en.wikipedia.org/wiki?curid=9023486", "title": "Aircraft principal axes", "text": "Aircraft principal axes\n\nAn aircraft in flight is free to rotate in three dimensions: \"yaw\", nose left or right about an axis running up and down; \"pitch\", nose up or down about an axis running from wing to wing; and \"roll\", rotation about an axis running from nose to tail. The axes are alternatively designated as \"vertical\", \"transverse\", and \"longitudinal\" respectively. These axes move with the vehicle and rotate relative to the Earth along with the craft. These definitions were analogously applied to spacecraft when the first manned spacecraft were designed in the late 1950s.\n\nThese rotations are produced by torques (or moments) about the principal axes. On an aircraft, these are intentionally produced by means of moving control surfaces, which vary the distribution of the net aerodynamic force about the vehicle's center of gravity. Elevators (moving flaps on the horizontal tail) produce pitch, a rudder on the vertical tail produces yaw, and ailerons (flaps on the wings that move in opposing directions) produce roll. On a spacecraft, the moments are usually produced by a reaction control system consisting of small rocket thrusters used to apply asymmetrical thrust on the vehicle.\n\n\nNormally, these axes are represented by the letters X, Y and Z in order to compare them with some reference frame, usually named x, y, z. Normally, this is made in such a way that the X is used for the longitudinal axis, but there are other possibilities to do it.\n\nThe yaw axis has its origin at the center of gravity and is directed towards the bottom of the aircraft, perpendicular to the wings and to the fuselage reference line. Motion about this axis is called yaw. A positive yawing motion moves the nose of the aircraft to the right. The rudder is the primary control of yaw.\n\nThe term \"yaw\" was originally applied in sailing, and referred to the motion of an unsteady ship rotating about its vertical axis. Its etymology is uncertain.\n\nThe pitch axis (also called transverse or lateral axis) has its origin at the center of gravity and is directed to the right, parallel to a line drawn from wingtip to wingtip. Motion about this axis is called pitch. A positive pitching motion raises the nose of the aircraft and lowers the tail. The elevators are the primary control of pitch.\n\nThe roll axis (or longitudinal axis) has its origin at the center of gravity and is directed forward, parallel to the fuselage reference line. Motion about this axis is called roll. An angular displacement about this axis is called bank. A positive rolling motion lifts the left wing and lowers the right wing. The pilot rolls by increasing the lift on one wing and decreasing it on the other. This changes the bank angle. The ailerons are the primary control of bank. The rudder also has a secondary effect on bank.\n\nThese axes are related to the principal axes of inertia, but are not the same. They are geometrical symmetry axes, regardless of the mass distribution of the aircraft. \n\nIn aeronautical and aerospace engineering intrinsic rotations around these axes are often called Euler angles, but this conflicts with existing usage elsewhere. The calculus behind them is similar to the Frenet–Serret formulas. Performing a rotation in an intrinsic reference frame is equivalent to right-multiplying its characteristic matrix (the matrix that has the vector of the reference frame as columns) by the matrix of the rotation.\n\nThe first aircraft to demonstrate active control about all three axes was the Wright brothers' 1902 glider.\n\n\n"}
{"id": "3611487", "url": "https://en.wikipedia.org/wiki?curid=3611487", "title": "Allomone", "text": "Allomone\n\nAn allomone is any chemical substance produced and released by an individual of one species that affects the behaviour of a member of another species to the benefit of the originator but not the receiver. Production of allomones is a common form of defence, particularly by plant species against insect herbivores.\n\nMany insects have developed ways to defend against these plant defenses (in an evolutionary arms race). One method of adapting to allomones is to develop a positive reaction to them; the allomone then becomes a kairomone. Others alter the allomones to form pheromones or other hormones, and yet others adopt them into their own defensive strategies, for example by regurgitating them when attacked by an insectivorous insect.\n\nA third class of allelochemical (chemical used in interspecific communication), synomones, benefit both the sender and receiver.\n\"Allomone was proposed by Brown and Eisner (Brown, 1968) to denote those substances which convey an advantage upon the emitter. Because Brown and Eisner did not specify whether or not the receiver would benefit, the original definition of allomone includes both substances that benefit the receiver and the emitter, and substances that only benefit the emitter. An example of the first relationship would be a mutualistic relationship, and the latter would be a repellent secretion.\"\nDisrupt growth and development and reduce longevity of adults e.g. toxins or digestibility reducing factors.\n\nDisrupt normal host selection behaviour e.g. Repellents, suppressants, locomotory excitants.\n\n\n\n"}
{"id": "566198", "url": "https://en.wikipedia.org/wiki?curid=566198", "title": "Branding iron", "text": "Branding iron\n\nA branding iron is used for branding, pressing a heated metal shape against an object or livestock with the intention of leaving an identifying mark.\n\nThe history of branding is very much tied to the history of using animals as a commodity. The act of marking livestock with fire-heated marks to identify ownership begins in ancient times with the ancient Egyptians. The process continued throughout the ages, with Romans using the process to brand slaves as well.\n\nIn the English lexicon, the Germanic word \"brand\" originally meant anything hot or burning, such as a \"fire-brand\", a burning stick. By the European Middle Ages it commonly identified the process of burning a mark into a stock animals with thick hides, such as cattle, so as to identify ownership under animus revertendi. In England, the rights of common including the common pasture system meant that cattle could be grazed on certain land with commoner's rights and the cattle were branded to show ownership, often with the commoner's or Lord of the manor's mark. The practice was widespread in most European nations with large cattle grazing regions, including Spain. \n\nWith colonialism, many cattle branding traditions and techniques were spread via the Spanish Empire to South America and to countries of the British Empire including the Americas, Australasia & South Africa where distinct sets of traditions and techniques developed respectively.\n\nIn the Americas these European systems continued with English tradition being used in the New England Colonies and spread outwards with the western expansion of the U.S. The Spanish system evolved from the south with the \"vaquero\" tradition in what today is the southwestern United States and northern Mexico. The branding iron consisted of an iron rod with a simple symbol or mark which was heated in a fire. After the branding iron turned red-hot, the cowhand pressed the branding iron against the hide of the cow. The unique brand meant that cattle owned by multiple owners could then graze freely together on the commons or open range. Drovers or cowboys could then separate the cattle at \"roundup\" time for driving to market.\n\nBranding Irons come in a variety of styles, designed primarily by their method of heating.\n\nThe traditional fire-heated method is still in use today. While they require longer lengths of time to heat, are inconsistent in temperature and all around inferior to more advanced forms of branding, they are inexpensive to produce and purchase. Fire-heated branding irons are used to brand wood, steak, leather, livestock and plastics.\n\nElectric branding irons utilize an electric heating element to heat a branding iron to the desired temperature. Electric branding irons come in many variations from irons designed to brand cattle, irons designed to mark wood and leather and models designed to be placed inside a drill press for the purposes of manufacturing. An Electric Branding Iron's temperature can be controlled by increasing or decreasing the flow of electricity.\n\nPropane Branding Irons use a continuous flow of propane to heat the iron head. They are commonly used where electricity is not available. Utilizing the flow of propane, the temperature can be adjusted for varying branding environments.\n\nA commercially built branding iron heater fired with L.P. gas is a common method of heating several branding irons at once.\n\nIn contrast to traditional hot-iron branding, freeze branding uses a branding iron that has been chilled with a coolant such as dry ice or liquid nitrogen. Rather than burning a scar into the animal, a freeze brand damages the pigment-producing hair cells, causing the animal's hair to grow white where the brand has been applied. To apply a freeze brand, the hair coat of the animal is shaved so that the bare skin is exposed, then the frozen iron is applied to the bare area for a period of time that varies with both the species of animal and the color of its hair coat: Shorter times are used on dark-colored animals, simply causing the hair follicles to lose all color and regrow as white hairs. Longer times are needed on animals with white hair coats, as the brand is applied long enough to permanently stop the hair from growing in the branded area and only epidermis remains.\n\nLivestock branding is perhaps the most prevalent use of a branding iron. Modern use includes gas heating, the traditional fire-heated method, an iron heated by electricity (electric cattle branding iron) or an iron super cooled by dry ice (freeze branding iron). Cattle, horses and other livestock are commonly branded today for the same reason they were in Ancient times, to prove ownership.\n\nWoodworkers will often use Electric or Fire-Heated Branding Irons to leave their maker's mark or company logo. Timber pallets and other timber export packaging is often marked in this way in accordance with ISPM 15 to indicate that the timber has been treated to prevent it carrying pests.\n\nSteak branding irons are used commonly by barbecue enthusiasts and professional chefs to leave a mark indicating how well done a steak is or to identify the chef or grill master.\n\nBranding Irons are used often by makers of horse tack often in place of a steel leather stamp to indicate craftsmanship.\n\n"}
{"id": "40565959", "url": "https://en.wikipedia.org/wiki?curid=40565959", "title": "Cabo Tamar oil spill", "text": "Cabo Tamar oil spill\n\nOn 7 June 1978, the Chilean Oil tanker Cabo Tamar, ran aground at San Vicente Bay, near Talcahuano, Chile and released 12,000 tons of oil (of the 64,000 ton load). \n\n"}
{"id": "40843", "url": "https://en.wikipedia.org/wiki?curid=40843", "title": "Capacitive coupling", "text": "Capacitive coupling\n\nCapacitive coupling is the transfer of energy within an electrical network or between distant networks by means of displacement current between circuit(s) nodes, induced by the electric field. This coupling can have an intentional or accidental effect.\n\nIn its simplest implementation, capacitive coupling is achieved by placing a capacitor between two nodes. Where analysis of many points in a circuit is carried out, the capacitance at each point and between points can be described in a matrix form. \n\nIn analog circuits, a coupling capacitor is used to connect two circuits such that only the AC signal from the first circuit can pass through to the next while DC is blocked. This technique helps to isolate the DC bias settings of the two coupled circuits. Capacitive coupling is also known as \"AC coupling\" and the capacitor used for the purpose is also known as a \"DC-blocking capacitor\".\n\nA coupling capacitor's ability to prevent a DC load from interfering with an AC source is particularly useful in Class A amplifier circuits by preventing a 0 volt input being passed to a transistor with additional resistor biasing; creating continuous amplification.\n\nCapacitive coupling decreases the low frequency gain of a system containing capacitively coupled units. Each coupling capacitor along with the input electrical impedance of the next stage forms a high-pass filter and the sequence of filters results in a cumulative filter with a −3dB frequency that may be higher than those of each individual filter. So for adequate low frequency response, the capacitors used must have high capacitance ratings. They should be high enough that the reactance of each is at most a tenth of the input impedance of each stage, at the lowest frequency of interest. See Impedance bridging.\n\nCoupling capacitors can also introduce nonlinear distortion at low frequencies. This is not an issue at high frequencies because the voltage across the capacitor stays very close to zero. However, if a signal is allowed to pass through the coupling that is low relative to the RC cutoff frequency, voltages can develop across the capacitor, which for some capacitor types results in changes of capacitance, leading to distortion. This is avoided by choosing capacitor types that have low \"voltage coefficient\", and by using large values that put the cutoff frequency far lower than the frequencies of the signal.\n\nThese disadvantages of capacitively coupling DC-biased transistor amplifier circuits are largely minimized in directly coupled designs.\n\nAC coupling is also widely used in digital circuits to transmit digital signals with a zero DC component, known as DC-balanced signals. DC-balanced waveforms are useful in communications systems, since they can be used over AC-coupled electrical connections to avoid voltage imbalance problems and charge accumulation between connected systems or components.\n\nFor this reason, most modern line codes are designed to produce DC-balanced waveforms. The most common classes of DC-balanced line codes are constant-weight codes and paired-disparity codes.\n\nA gimmick loop is a simple type of capacitive coupler: two closely spaced strands of wire. It provides capacitive coupling of a few picofarads between two nodes. Sometimes the wires are twisted together for physical stability.\n\nCapacitive coupling is often unintended, such as the capacitance between two wires or PCB traces that are next to each other. Often one signal can capacitively couple with another and cause what appears to be noise. To reduce coupling, wires or traces are often separated as much as possible, or ground lines or ground planes are run in between signals that might affect each other, so that the lines are capacitively coupled to ground rather than each other. Breadboards are particularly prone to these issues due to the long pieces of metal that line every row creating a several-picofarad capacitor between lines. To prototype high-frequency (10s of MHz) or high-gain analog circuits, often the circuits are built over a ground plane so that the signals couple to ground more than to each other. If a high-gain amplifier's output capacitively couples to its input it often becomes an electronic oscillator.\n\nOne rule of thumb says that drivers should be able to drive 25 pF of capacitance which allows for PCB traces up to 0.30 meters.\n\n\n\n"}
{"id": "11212479", "url": "https://en.wikipedia.org/wiki?curid=11212479", "title": "Containerboard", "text": "Containerboard\n\nContainerboard (CCM or corrugated case material) is a type of paperboard specially manufactured for the production of corrugated board. It includes both linerboard and corrugating medium (or fluting), the two types of paper that make up corrugated board. Since containerboard is made mainly out of natural unbleached wood fibers, it is generally brown, although its shade may vary depending on the type of wood, pulping process, recycling rate and impurities content. For certain boxes that demand good presentation, white bleached pulp or coating is used on the top ply of the linerboard that goes outside the box.\n\nContainerboard is made on modified paper machines that can handle higher grammages.\n\nThe production of containerboard is the highest of all kinds of paper in the world. More than 100 million tons are produced annually. It is made in specialized paper machines from virgin as well as recycled fibers. Linerboard made of virgin pulp is called \"kraftliner\", whereas recycled linerboard is known as \"testliner.\" The corrugating medium may be recycled medium, called \"wellenstoff\" in Europe, or virgin, which is called semichemical medium for the type of pulp used in its production. The borders of these categories becomes blurred when both virgin and recycled fibers are used in making one product. \n\nAt the end of the manufacturing process containerboard is cut into rolls comprising a continuous sheet of paper, which will later be unwound in the corrugator machine while making the corrugated board. A typical roll has a width of , a diameter of , and weighs around .\n\n"}
{"id": "25069616", "url": "https://en.wikipedia.org/wiki?curid=25069616", "title": "Crystal cluster", "text": "Crystal cluster\n\nA crystal cluster is a group of crystals which formed in an open space environment and exhibit euhedral crystal form determined by their internal crystal structure. A cluster of small crystals coating the walls of a cavity are called druse.\n\n"}
{"id": "16405140", "url": "https://en.wikipedia.org/wiki?curid=16405140", "title": "Dimitris Melissanidis", "text": "Dimitris Melissanidis\n\nDimitris Melissanidis (Greek: Δημήτρης Μελισσανίδης) born March 8, 1951 in Nikaia, Greece, is a Greek business shipping magnate and oil tycoon who is one of Greece's most successful businessmen. Dimitris Melissanidis is sometimes described as the largest independent supplier of fuel oil on the planet.\n\nHe is the founder and ex-owner of Aegean Marine Petroleum Network Inc. which is the largest independent fuel supplier in the world. He also owned the second largest Oil company in Greece, Aegean Oil and leader of AEK Athens F.C..\n\nMelissanidis was ranked 98th in 2015 and 97th in 2014 in the world in the Lloyd's List \"Top 100 Most Influential People in the Shipping Industry\" and has been repeatedly acknowledged for his prestigious international shipping contribution. Melissanidis has also been acknowledged for his successful business career on \"Forbes\" magazine.\n\nHe has served as AEK Athens Football Club's thirtieth (30th) and thirty second (32nd) president, serving from 1992–1993 and 1994-1995 respectively. He was also president of AEK during 1998-1999 but ENIC Group was the shareholder.\n\nOn June 7, 2013, with AEK relegated to the Amateur Division because of financial problems, Dimitris Melissanidis became again active for the club as an administrative leader. Later, together with other notable AEK fans and old players, they created the non-profit association \"Union Friends of AEK\" (\"Enosi Filon AEK\") which took the majority stake of the football club.\n\nMelissanidis was born in Nikaia, Greece and was raised in Athens. He is the son of a Pontic refugee, Zoras Melissanidis, active in Pontic affairs, and a deeply respected local political figure prior to his forced move to Athens. Melissanidis began as a businessman in 1975 owning a small driving school in Korydallos area.\n\nMelissanidis was the founder and owner of Aegean Marine Petroleum Network Inc. (AMPNI). He launched the business in 1995. AMPNI is listed the second largest oil company in Greece behind Hellenic Petroleum and the largest independent fuel supplier in the world. Aegean is a large oil and shipping company in Greece and has a global presence in 33 markets, including America, Canada, Mexico, the United Kingdom, France, Belgium, Cyprus, Spain, Portugal, Italy, Denmark, Bulgaria, Russia, Serbia, The Netherlands, Romania, Turkey, Switzerland, South Africa, Jamaica, Trinidad and Tobago, Ghana, Singapore, Morocco, Gibraltar, Fujairah, Cape Verde, Panama, The United Arab Emirates, China, India, Indonesia, Malaysia and The Canary Islands. Sister company, Aegean Oil is also the leading marine fuels physical supplier in Greece.\n\nAMPNI is listed on the New York Stock Exchange.\n\nIn August 2016, Melissanidis sold all the shares of AMPNI for $99.5 million.\n\nMelissanidis was President of AEK Athens F.C. during the 90's.\n\nOn June 7, 2013, Dimitris Melissanidis became again active for AEK, after they declared bankruptcy, in order to save the club and bring them back to success. The company declared bankruptcy and were relegated to Greece's third tier. The club was reported to owe €170 million ($219 million) in taxes and will start the 2013/14 campaign in the Football League 2, the country's third amateur division. Later, together with other notable AEK fans and old players, they created the non-profit association \"Union Friends of AEK\" (\"Enosi Filon AEK\") which took the majority stake of the football club.\n\nSince the arrival and return of Melissandis to the club, AEK has secured major sponsorship deals which will give the club a huge financial boost. Melissanidis has secured a €2.1 million sponsorship deal from OPAP, a €1.5 million contract with Greek Telecommunications company Nova Sports and a €1 million sponsorship deal with Fujitsu.\n\nMelissanidis has unveiled plans also for a new stadium in Nea Filadelfia, where the old stadium of AEK was placed.\n\nOn October 2, 2013 the AEK Athens board, under Dimitris Melissanidis, presented plans for the new stadium to the municipality of Nea Filadelfeia, in order to gain permission to build. A new 4-star UEFA system stadium will be built, seating between 32,500 and 35,000 spectators. The cost of this project is estimated to be around €70-80 million. It is modelled after the Agia Sophia church in Constantinople, as AEK has its roots there. Around 1,500-2,000 new jobs will become available and the neighbourhood is expected to benefit largely from this endeavour. The structural design of the stadium was designed by Italian architect Massimo Majowiecki and the architectural design by the Greek architect Athanasius Kyratsous. Majowiecki is most known for designing the home ground of the Juventus Stadium.\n\nMelissanidis, via his own company Geonama Holdings, reported owned a stake in Emma Delta, a private equity fund of EMMA Capital. Emma Delta acquired 33% stake of Greek betting firm OPAP for €652 million. According to Reuters, OPAP was the European biggest betting firm, as of 2008. However, according to the press release, the 33% stake in Emma Delta was beneficially owned by his son Georgios, whom was nominated into the board of directors of OPAP.. At the signing, Dimitris Melissanidis ended his statement by commenting that \"OPAP is turning a page\".\n\nMelissanidis has social and philanthropic action, mainly in cooperation with the Church of Greece and the Ecumenical Patriarchate of Constantinople. In January 2014, he was honoured with the title of \"Kouropalates\" (\"cura palatii\") by the Ecumenical Patriarch of Constantinople, a title given for the first time since the 15th century.\n\nDimitris Melissanidis has been involved in many controversies over the years. In the 1980s when he ran his first business in Piraeus (a driving school) he received his first bribery conviction and then another bribery conviction for paying off two players in an amateur football league.]\n\nIn 1996, Melissanidis and his business partner, Yiannis Karras, were charged to a prison sentence for oil smuggling, however the charges were dropped and he was not found guilty for oil smuggling.\n\nDuring the time of negotiations with OPAP, Costas Louropoulos, OPAP’s chief executive, felt put “under pressure by Mr Melissanidis in a series of telephone calls. “He insulted me, as on many previous occasions. . . You dare to sign [the Intralot and lottery contracts] and I will take your head off”, Mr Louropoulos quoted Mr Melissanidis as telling him on May 20, 2013”, according to the \"Financial Times\". Also, immediately after the signing of the OPAP deal, the Greek privatization agency chairman, Stelios Stavridis, was dismissed by Greek Finance Minister, Yannis Stournaras, “for ethical reasons”, when news reports emerged that he travelled on Dimitris Melissanidis’ private jet.\n\nFurthermore, in a highly publicized incident, discussed in the Greek Parliament, calling from an ‘Aegean Oil’ listed telephone number, Melissanidis personally bullied with death threats to Lefteris Charalambopoulos, the Greek reporter who published the government report in the left-wing ‘Unfollow’ magazine. The caller self-identified as Dimitris Melissanidis threatened the reporters life repeatedly. Part of what was said by the man self-identified as Dimitris Melissanidis, which was taken down by the reporter, follows: “I could have you killed without having warned you. But I am a man and I’m gonna have you blown up in your sleep. I’ll have you killed, you, your wife, your children, everything you’ve got”.\nWhen the reporter told the caller that he would alert the authorities, he replied: “Screw you and the authorities. I don’t understand anything, I am Melissanidis. You will not be able to sleep. You will not be able to go out, I’ll be your nightmare. Fear of me will haunt you. They will come to your house and blow you up in your sleep. I am used to talking to big journalists. I looked you up and I will tear you down”.\n\nFurthermore, after Melissanidis assumed OPAP's management, AEK FC received a €2.1 million sponsorship deal. Protesting about “unfair competition” due to Melissanidis’ management of both AEK and OPAP, some Greek football teams pulled out of their OPAP sponsorship contracts. Olympiacos F.C. pulled out of its €1.9 million sponsorship deal with OPAP.\n\n"}
{"id": "215909", "url": "https://en.wikipedia.org/wiki?curid=215909", "title": "Electricity market", "text": "Electricity market\n\nIn economic terms, electricity (both power and energy) is a commodity capable of being bought, sold, and traded. An electricity market is a system enabling purchases, through bids to buy; sales, through offers to sell; and short-term trades, generally in the form of financial or obligation swaps. Bids and offers use supply and demand principles to set the price. Long-term trades are contracts similar to power purchase agreements and generally considered private bi-lateral transactions between counterparties.\n\nWholesale transactions (bids and offers) in electricity are typically cleared and settled by the market operator or a special-purpose independent entity charged exclusively with that function. Market operators do not clear trades but often require knowledge of the trade in order to maintain generation and load balance.\nThe commodities within an electric market generally consist of two types: power and energy. Power is the metered net electrical transfer rate at any given moment and is measured in megawatts (MW). Energy is electricity that flows through a metered point for a given period and is measured in megawatt-hours (MWh).\n\nMarkets for energy-related commodities trade net generation output for a number of intervals usually in increments of 5, 15 and 60 minutes. Markets for power-related commodities required and managed by (and paid for by) market operators to ensure reliability, are considered ancillary services and include such names as spinning reserve, non-spinning reserve, operating reserves, responsive reserve, regulation up, regulation down, and installed capacity.\n\nIn addition, for most major operators, there are markets for transmission congestion and electricity derivatives such as electricity futures and options, which are actively traded. These markets developed as a result of the restructuring of electric power systems around the world. This process has often gone on in parallel with the restructuring of natural gas markets.\nOne early introduction of energy market concepts and privatization to electric power systems took place in Chile in the early 1980s, in parallel with other market-oriented reforms associated with the Chicago Boys. The Chilean model was generally perceived as successful in bringing rationality and transparency to power pricing. Argentina improved on the Chilean model by imposing strict limits on market concentration and by improving the structure of payments to units held in reserve to assure system reliability. One of the principal purposes of the introduction of market concepts in Argentina was to privatize existing generation assets (which had fallen into disrepair under the government-owned monopoly, resulting in frequent service interruptions) and to attract capital needed for rehabilitation of those assets and for system expansion. The World Bank was active in introducing a variety of hybrid markets in other Latin American nations, including Peru, Brazil, and Colombia, during the 1990s, with limited success.\n\nA quantum leap in electricity pricing theory occurred in 1988 when four professors at MIT and Boston University (Fred C. Schweppe, Michael C. Caramanis, Richard D. Tabors, and Roger E. Bohn) published a book entitled, \"Spot Pricing of Electricity.\" It presented the concept that prices at each location on a transmission system should reflect the marginal cost of serving one additional unit of demand at that location. It then proposed quantifying these prices by solving a systemwide cost minimization problem while complying with all of the system's operational constraints, such as generator capacity limits, locational loads, line flow limits, etc. using linear programming software. The locational marginal prices then emerged as the shadow prices for relaxing the load limit at each location. \n\nA key event for electricity markets occurred in 1990 when the UK government under Margaret Thatcher privatised the UK electricity supply industry. The process followed by the British was then used as a model (or at least a catalyst) for the restructuring of several other Commonwealth countries, notably the National Electricity Markets of Australia and New Zealand and the Alberta Electricity Market in Canada. \n\nIn the United States the traditional vertically integrated electric utility model with a transmission system designed to serve its own customers worked extremely well for decades. As dependence on a reliable supply of electricity grew and electricity was transported over increasingly greater distances, wide area synchronous grid interconnections developed. Transactions were relatively few and generally scheduled well in advance.\n\nHowever, in the last decade of the 20th century, some US policy makers and academics asserted that the electric power industry would ultimately experience deregulation and independent system operators (ISOs) and regional transmission organizations (RTOs) were established. They were conceived as the way to handle the vastly increased number of transactions that take place in a competitive environment. About a dozen states decided to deregulate but some pulled back following the California electricity crisis of 2000 and 2001.\n\nIn different deregulation processes the institutions and market designs were often very different but many of the underlying concepts were the same. These are: separate the potentially competitive functions of generation and retail from the natural monopoly functions of transmission and distribution; and establish a wholesale electricity market and a retail electricity market. The role of the wholesale market is to allow trading between generators, retailers and other financial intermediaries both for short-term delivery of electricity (see spot price) and for future delivery periods (see forward price).\n\nSome states exempt non investor-owned utilities from some aspects of deregulation such as customer choice of supplier. For example, some of the New England states exempt municipal lighting plants from several aspects of deregulation and these municipal utilities do not have to allow customers to purchase from competitive suppliers. Municipal utilities in these states can also opt to function as vertically-integrated utilities and operate generation assets both inside and outside of their service area to supply their utility customers as well as sell output to the market.\n\nElectricity is by its nature difficult to store and has to be available on demand. Consequently, unlike other products, it is not possible, under normal operating conditions, to keep it in stock, ration it or have customers queue for it. Furthermore, demand and supply vary continuously.\n\nThere is therefore a physical requirement for a controlling agency, the transmission system operator, to coordinate the dispatch of generating units to meet the expected demand of the system across the transmission grid. If there is a mismatch between supply and demand the generators speed up or slow down causing the system frequency (either 50 or 60 hertz) to increase or decrease. If the frequency falls outside a predetermined range the system operator will act to add or remove either generation or load.\n\nThe proportion of electricity lost in transmission and the level of congestion on any particular branch of the network will influence the economic dispatch of the generation units.\n\nMarkets may extend beyond national boundaries.\n\nA wholesale electricity market exists when competing generators offer their electricity output to retailers. The retailers then re-price the electricity and take it to market. While wholesale pricing used to be the exclusive domain of large retail suppliers, increasingly markets like New England are beginning to open up to end-users. Large end-users seeking to cut out unnecessary overhead in their energy costs are beginning to recognize the advantages inherent in such a purchasing move. Consumers buying electricity directly from generators is a relatively recent phenomenon.\n\nBuying wholesale electricity is not without its drawbacks (market uncertainty, membership costs, set up fees, collateral investment, and organization costs, as electricity would need to be bought on a daily basis), however, the larger the end user's electrical load, the greater the benefit and incentive to make the switch.\n\nFor an economically efficient electricity wholesale market to flourish it is essential that a number of criteria are met, namely the existence of a coordinated spot market that has \"bid-based, security-constrained, economic dispatch with nodal prices\". These criteria have been largely adopted in the US, Australia, New Zealand and Singapore.\n\nThe system price in the day-ahead market is, in principle, determined by matching offers from generators to bids from consumers at each node to develop a classic supply and demand equilibrium price, usually on an hourly interval, and is calculated separately for subregions in which the system operator's load flow model indicates that constraints will bind transmission imports.\n\nThe theoretical prices of electricity at each node on the network is a calculated \"shadow price\", in which it is assumed that one additional kilowatt-hour is demanded at the node in question, and the hypothetical incremental cost to the system that would result from the optimized redispatch of available units establishes the hypothetical production cost of the hypothetical kilowatt-hour. This is known as \"locational marginal pricing\" (LMP) or \"nodal pricing\" and is used in some deregulated markets, most notably in the PJM Interconnection, ERCOT, New York, and New England markets in the USA, New Zealand,\nand in Singapore.\n\nIn practice, the LMP algorithm described above is run, incorporating a security-constrained, least-cost dispatch calculation (see below) with supply based on the generators that submitted offers in the day-ahead market, and demand based on bids from load-serving entities draining supplies at the nodes in question.\n\nWhile in theory the LMP concepts are useful and not evidently subject to manipulation, in practice system operators have substantial discretion over LMP results through the ability to classify units as running in \"out-of-merit dispatch\", which are thereby excluded from the LMP calculation. In most systems, units that are dispatched to provide reactive power to support transmission grids are declared to be \"out-of-merit\" (even though these are typically the same units that are located in constrained areas and would otherwise result in scarcity signals). System operators also normally bring units online to hold as \"spinning-reserve\" to protect against sudden outages or unexpectedly rapid ramps in demand, and declare them \"out-of-merit\". The result is often a substantial reduction in clearing price at a time when increasing demand would otherwise result in escalating prices.\n\nResearchers have noted that a variety of factors, including energy price caps set well below the putative scarcity value of energy, the effect of \"out-of-merit\" dispatch, the use of techniques such as voltage reductions during scarcity periods with no corresponding scarcity price signal, etc., results in a \"missing money\" problem. The consequence is that prices paid to suppliers in the \"market\" are substantially below the levels required to stimulate new entry. The markets have therefore been useful in bringing efficiencies to short-term system operations and dispatch, but have been a failure in what was advertised as a principal benefit: stimulating suitable new investment where it is needed, when it is needed.\n\nIn LMP markets, where constraints exist on a transmission network, there is a need for more expensive generation to be dispatched on the downstream side of the constraint. Prices on either side of the constraint separate giving rise to congestion pricing and constraint rentals.\n\nA constraint can be caused when a particular branch of a network reaches its thermal limit or when a potential overload will occur due to a contingent event (e.g., failure of a generator or transformer or a line outage) on another part of the network. The latter is referred to as a \"security constraint\". Transmission systems are operated to allow for continuity of supply even if a contingent event, like the loss of a line, were to occur. This is known as a \"security constrained system\".\n\nIn most systems the algorithm used is a \"DC\" model rather than an \"AC\" model, so constraints and redispatch resulting from thermal limits are identified/predicted, but constraints and redispatch resulting from reactive power deficiencies are not. Some systems take marginal losses into account. The prices in the real-time market are determined by the LMP algorithm described above, balancing supply from available units. This process is carried out for each 5-minute, half-hour or hour (depending on the market) interval at each node on the transmission grid. The hypothetical redispatch calculation that determines the LMP must respect security constraints and the redispatch calculation must leave sufficient margin to maintain system stability in the event of an unplanned outage anywhere on the system. This results in a spot market with \"bid-based, security-constrained, economic dispatch with nodal prices\".\n\nSince the introduction of the market, New Zealand has experienced shortages in 2001 and 2003, high prices all through 2005 and even higher prices and the risk of a severe shortage in 2006 (as of April 2006). These problems arose because New Zealand is at risk from drought due to its high proportion of electricity generated from hydro.\n\nMany established markets do not employ nodal pricing, examples being the UK, EPEX SPOT (most European countries), and Nord Pool Spot (Nordic and Baltic countries).\n\nFinancial risk management is often a high priority for participants in deregulated electricity markets due to the substantial price and volume risks that the markets can exhibit.\nA consequence of the complexity of a wholesale electricity market can be extremely high price volatility at times of peak demand and supply shortages. The particular characteristics of this price risk are highly dependent on the physical fundamentals of the market such as the mix of types of generation plant and relationship between demand and weather patterns. Price risk can be manifest by price \"spikes\" which are hard to predict and price \"steps\" when the underlying fuel or plant position changes for long periods.\n\nVolume risk is often used to denote the phenomenon whereby electricity market participants have uncertain volumes or quantities of consumption or production. For example, a retailer is unable to accurately predict consumer demand for any particular hour more than a few days into the future and a producer is unable to predict the precise time that they will have plant outage or shortages of fuel. A compounding factor is also the common correlation between extreme price and volume events. For example, price spikes frequently occur when some producers have plant outages or when some consumers are in a period of peak consumption. The introduction of substantial amounts of intermittent power sources such as wind energy may affect market prices.\n\nElectricity retailers, who in aggregate buy from the wholesale market, and generators who in aggregate sell to the wholesale market, are exposed to these price and volume effects and to protect themselves from volatility, they will enter into \"hedge contracts\" with each other. The structure of these contracts varies by regional market due to different conventions and market structures. However, the two simplest and most common forms are simple fixed price forward contracts for physical delivery and contracts for differences where the parties agree a strike price for defined time periods. In the case of a contract for difference, if a resulting wholesale price index (as referenced in the contract) in any time period is higher than the \"strike\" price, the generator will refund the difference between the \"strike\" price and the actual price for that period. Similarly a retailer will refund the difference to the generator when the actual price is less than the \"strike price\". The actual price index is sometimes referred to as the \"spot\" or \"pool\" price, depending on the market.\n\nMany other hedging arrangements, such as swing contracts, virtual bidding, Financial Transmission Rights, call options and put options are traded in sophisticated electricity markets. In general they are designed to transfer financial risks between participants.\n\n\nA retail electricity market exists when end-use customers can choose their supplier from competing electricity retailers; one term used in the United States for this type of consumer choice is 'energy choice'. A separate issue for electricity markets is whether or not consumers face real-time pricing (prices based on the variable wholesale price) or a price that is set in some other way, such as average annual costs. In many markets, consumers do not pay based on the real-time price, and hence have no incentive to reduce demand at times of high (wholesale) prices or to shift their demand to other periods. Demand response may use pricing mechanisms or technical solutions to reduce peak demand.\n\nGenerally, electricity retail reform follows from electricity wholesale reform. However, it is possible to have a single electricity generation company and still have retail competition. If a wholesale price can be established at a node on the transmission grid and the electricity quantities at that node can be reconciled, competition for retail customers within the distribution system beyond the node is possible. In the German market, for example, large, vertically integrated utilities compete with one another for customers on a more or less open grid.\n\nAlthough market structures vary, there are some common functions that an electricity retailer has to be able to perform, or enter into a contract for, in order to compete effectively. Failure or incompetence in the execution of one or more of the following has led to some dramatic financial disasters:\n\n\nThe two main areas of weakness have been risk management and billing. In the USA in 2001, California's flawed regulation of retail competition led to the California electricity crisis and left incumbent retailers subject to high spot prices but without the ability to hedge against these (see Manifesto on The California Electricity Crisis). In the UK a retailer, Independent Energy, with a large customer base went bust when it could not collect the money due from customers.\n\nCompetitive retail needs open access to distribution and transmission wires. This in turn requires that prices must be set for both these services. They must also provide appropriate returns to the owners of the wires and encourage efficient location of power plants.\nThere are two types of fees, the access fee and the regular fee. The access fee covers the cost of having and accessing the network of wires available, or the right to use the existing transmission and distribution network. The regular fee reflects the marginal cost of transferring electricity through the existing network of wires.\n\nNew technology is available and has been piloted by the US Department of Energy that may be better suited to real-time market pricing. A potential use of event-driven SOA could be a virtual electricity market where home clothes dryers can bid on the price of the electricity they use in a real-time market pricing system. The real-time market price and control system could turn home electricity customers into active participants in managing the power grid and their monthly utility bills. Customers can set limits on how much they would pay for electricity to run a clothes dryer, for example, and electricity providers willing to transmit power at that price would be alerted over the grid and could sell the electricity to the dryer.\n\nOn one side, consumer devices can bid for power based on how much the owner of the device were willing to pay, set ahead of time by the consumer. On the other side, suppliers can enter bids automatically from their electricity generators, based on how much it would cost to start up and run the generators. Further, the electricity suppliers could perform real-time market analysis to determine return-on-investment for optimizing profitability or reducing end-user cost of goods. The effects of a competitive retail electricity market are mixed across states, but generally appear to lower prices in states with high participation and raise prices in states that have little customer participation.\n\nEvent-driven SOA software could allow homeowners to customize many different types of electricity devices found within their home to a desired level of comfort or economy. The event-driven software could also automatically respond to changing electricity prices, in as little as five-minute intervals. For example, to reduce the home owner's electricity usage in peak periods (when electricity is most expensive), the software could automatically lower the target temperature of the thermostat on the central heating system (in winter) or raise the target temperature of the thermostat on the central cooling system (in summer).\n\nIn the main, experience in the introduction of wholesale and retail competition has been mixed. Many regional markets have achieved some success and the ongoing trend continues to be towards deregulation and introduction of competition. However, in 2000/2001 major failures such as the California electricity crisis and the Enron debacle caused a slow down in the pace of change and in some regions an increase in market regulation and reduction in competition. However, this trend is widely regarded as a temporary one against the longer term trend towards more open and competitive markets.\n\nNotwithstanding the favorable light in which market solutions are viewed conceptually, the \"missing money\" problem has to date proved intractable. If electricity prices were to move to the levels needed to incentivize new merchant (i.e., market-based) transmission and generation, the costs to consumers would be politically difficult.\n\nThe increase in annual costs to consumers in New England alone were calculated at $3 billion during the recent FERC hearings on the NEPOOL market structure. Several mechanisms that are intended to incent new investment where it is most needed by offering enhanced capacity payments (but only in zones where generation is projected to be short) have been proposed for NEPOOL, PJM and NYPOOL, and go under the generic heading of \"locational capacity\" or LICAP (the PJM version is called the \"Reliability Pricing Model\", or \"RPM\"). There is substantial doubt as to whether any of these mechanisms will in fact incent new investment, given the regulatory risk and chronic instability of the market rules in US systems, and there are substantial concerns that the result will instead be to increase revenues to incumbent generators, and costs to consumers, in the constrained areas.\n\nThe capacity mechanism is claimed to be merely a mechanism for subsiding coal.\n\nThe Capacity Market is a part of the British government's Electricity Market Reform package. According to the Department for Business, Energy and Industrial Strategy \"the Capacity Market will ensure security of electricity supply by providing a payment for reliable sources of capacity, alongside their electricity revenues, to ensure they deliver energy when needed. This will encourage the investment we need to replace older power stations and provide backup for more intermittent and inflexible low carbon generation sources\".\n\nTwo Capacity Market Auctions are held each year. The T-4 auction buys capacity to be delivered in four years’ time and the T-1 auction is a top-up auction held just ahead of each delivery year. The following Capacity Market Auction results have been published:\n\n\nThe National Grid 'Guidance document for Capacity Market participants' provides the following definitions:\n\nReserve Capacity Mechanism Review Report\n"}
{"id": "626196", "url": "https://en.wikipedia.org/wiki?curid=626196", "title": "Electromagnetic propulsion", "text": "Electromagnetic propulsion\n\nElectromagnetic propulsion (EMP), is the principle of accelerating an object by the utilization of a flowing electrical current and magnetic fields. The electrical current is used to either create an opposing magnetic field, or to charge a field, which can then be repelled. When a current flows through a conductor in a magnetic field, an electromagnetic force known as a Lorentz force, pushes the conductor in a direction perpendicular to the conductor and the magnetic field. This repulsing force is what causes propulsion in a system designed to take advantage of the phenomenon. The term electromagnetic propulsion (EMP) can be described by its individual components: electromagneticusing electricity to create a magnetic field, and propulsionthe process of propelling something. When a fluid (liquid or gas) is employed as the moving conductor, the propulsion may be termed magnetohydrodynamic drive. One key difference between EMP and propulsion achieved by electric motors is that the electrical energy used for EMP is not used to produce rotational energy for motion; though both use magnetic fields and a flowing electrical current.\n\nThe science of electromagnetic propulsion does not have origins with any one individual and has application in many different fields. The thought of using magnets for propulsion continues to this day and has been dreamed of since at least 1897 when John Munro published his fictional story \"A Trip to Venus\". Current applications can be seen in maglev trains and military railguns. Other applications that remain not widely used or still in development include ion thruster for low orbiting satellites and magnetohydrodynamic drive for ships and submarines.\n\nOne of the first recorded discoveries regarding electromagnetic propulsion was in 1889 when Professor Elihu Thomson made public his work with electromagnetic waves and alternating currents. A few years later Emile Bachelet proposed the idea of a metal carriage levitated in air above the rails in a modern railway, which he showcased in the early 1890s. In the 1960s Eric Roberts Laithwaite developed the linear induction motor, which built upon these principles and introduced the first practical application of electromagnetic propulsion. In 1966 James R. Powell and Gordon Danby patented the superconducting maglev transportation system, and after this engineers around the world raced to create the first high-speed rail. From 1984 to 1995 the first commercial automated maglev system ran in Birmingham. It was a low speed Maglev shuttle that ran from the Birmingham International Airport to the Birmingham International Railway System.\n\nElectromagnetic propulsion is utilized in transportation systems to minimize friction and maximize speed over long distances. This has mainly been implemented in high-speed rail systems that use a linear induction motor to power trains by magnetic currents. It has also been utilized in theme parks to create high-speed roller coasters and water rides.\n\nMaglev\n\nA typical Maglev train costs three cents per passenger mile, or seven cents per ton mile (not including construction costs). This compares to 15 cents per passenger miles for travel by plane and 30 cents for ton mile for travel by intercity trucks. Maglev tracks have high longevity due to minimal friction and an even distribution of weight. Most last for at least 50 years and require little maintenance during this time. Maglev trains are promoted for their energy efficiency since they run on electricity, which can be produced by coal, nuclear, hydro, fusion, wind or solar power without requiring oil. On average most trains travel 483 km/h (300 mph) and use 0.4 megajoules per passenger mile. Using a 20 mi/gallon car with 1.8 people as a comparison, travel by car is typically 97 km/h (60 mph) and uses 4 megajoules per passenger mile. The carbon dioxide emissions are based upon the method of electrical production and fuel use. Many renewable electrical production methods generate little or no carbon dioxide during production (although carbon dioxide may be released during manufacture of the components, e.g. the steel used in wind turbines). The running of the train is significantly quieter than other trains, trucks or airplanes.\nAssembly: Linear Induction Motor\nA linear induction motor consists of two parts: the primary coil assembly and the reaction plate. The primary coil assembly consists of phase windings surrounded by steel laminations, and includes a thermal sensor within a thermal epoxy. The reaction plate consists of a 3.2 mm (0.125 inch) thick aluminum or copper plate bonded to a 6.4 mm (0.25 inch) thick cold rolled steel sheet. There is an air gap between these two parts that creates the frictionless property an electromagnetic propulsion system encompasses. Functioning of a linear induction motor begins with an AC force that is supplied to the coil windings within the primary coil assembly. This creates a traveling magnetic field that induces a current in the reaction plate, which then creates its own magnetic field. The magnetic fields in the primary coil assembly and reaction plate alternate, which generates force and direct linear motion.\n\nThere are multiple applications for EMP technologies in the field of aerospace. Many of these applications are conceptual as of now, however, there are also several applications that range from near term to next century. One of such applications is the use of EMP to control fine adjustments of orbiting satellites. One of these particular systems is based on the direct interactions of the vehicle's own electromagnetic field and the magnetic field of the Earth. The thrust force may be thought of as an electrodynamic force of interaction of the electric current inside its conductors with the applied natural field of the Earth. To attain a greater force of interaction, the magnetic field must be propagated further from the flight craft. The advantages of such systems is the very precise and instantaneous control over the thrust force. In addition, the expected electrical efficiencies are far greater than those of current chemical rockets that attain propulsion through the intermediate use of heat; this results in low efficiencies and large amounts of gaseous pollutants. The electrical energy in the coil of the EMP system is translated to potential and kinetic energy through direct energy conversion. This results in the system having the same high efficiencies as other electrical machines while excluding the ejection of any substance into the environment.\n\nThe current thrust-to mass ratios of these systems are relatively low. Nevertheless, since they do not require propulsive mass, the vehicle mass is constant. Also, the thrust can be continuous with relatively low electric consumption. The biggest limitation would be mainly the electrical conductance of materials to produce the necessary values of the current in the propulsion system.\n\nEMP and its applications for seagoing ships and submarines have been investigated since at least 1958 when Warren Rice filed a patent explaining the technology . The technology described by Rice considered charging the hull of the vessel itself. The design was later refined by allowing the water to flow through thrusters as described in a later patent by James Meng . The arrangement consists of a water channel open at both ends extending longitudinally through or attached to the ship, a means for producing magnetic field throughout the water channel, electrodes at each side of the channel and source of power to send direct current through the channel at right angles to magnetic flux in accordance with Lorentz force.\n\nCable-free elevators using EMP, capable of moving both vertically and horizontally, have been developed by German engineering firm Thyssen Krupp for use in high rise, high density buildings.\n\n"}
{"id": "16775799", "url": "https://en.wikipedia.org/wiki?curid=16775799", "title": "Emtp", "text": "Emtp\n\nEMTP is an acronym for Electromagnetic Transients Program. It is a software tool used by power systems engineers to analyse electromagnetic transients and associated insulation issues.\n\nEMTP originated in the habilitation (postdoctoral) thesis of Dr. Hermann Dommel in Germany in the mid 1960s, and has been improved by the cooperation of many power engineering professionals—an effort led by Dr. Dommel (currently with the University of British Columbia, in Vancouver, B.C., Canada), and Dr. Scott Meyer (from the Bonneville Power Administration in Portland, Oregon, U.S.A.).\n\nThere are two basic streams of EMTP programs: the stream known simply as \"EMTP\" originates from the program development at BPA; other versions have been written from scratch. The EMTP-ATP and MT-EMTP programs, for example, are based on the original BPA and DCG-EMTP versions. The alternate stream of EMTP-type programs may use new numerical methods and modeling approaches, and provide significantly improved capabilities and numerical performances. Examples of this alternate stream include RTDS Technologies, PSCAD-EMTDC, EMTP-RV, MT-EMTP, EMTP-ATP, eMEGAsim and HYPERsim from Opal-RT Technologies.\n\n\n\n"}
{"id": "19390984", "url": "https://en.wikipedia.org/wiki?curid=19390984", "title": "Gallium(II) selenide", "text": "Gallium(II) selenide\n\nGallium(II) selenide (GaSe) is a chemical compound. It has a hexagonal layer structure, similar to that of GaS. It is a photoconductor, a second harmonic generation crystal in nonlinear optics, and has been used as a far-infrared conversion material at 14-31 THz and above.\n\nIt is said to have potential for optical applications but the exploitation of this potential has been limited by the ability to readily grow single crystals Gallium selenide crystals show great promise as a nonlinear optical material and photoconductor. Non-linear optical materials are used in the frequency conversion of laser light. Frequency conversion involves the shifting of the wavelength of a monochromatic source of light, usually laser light, to a higher or lower wavelength of light that cannot be produced from a conventional laser source.\n\nSeveral methods of frequency conversion using non-linear optical materials exist. Second harmonic generation leads to doubling of the frequency of infrared carbon dioxide lasers. In optical parametric generation, the wavelength of light is doubled. Near-infrared solid-state lasers are usually used in optical parametric generations.\n\nOne original problem with using gallium selenide in optics is that it is easily broken along cleavage lines and thus it can be hard to cut for practical application. It has been found, however, that doping the crystals with indium greatly enhances their structural strength and makes their application much more practical. There remain, however, difficulties with crystal growth that must be overcome before gallium selenide crystals may become more widely used in optics.\n\nSingle layers of gallium selenide are dynamically stable two-dimensional semiconductors, in which the valence band has an inverted Mexican-hat shape, leading to a Lifshitz transition as the hole-doping is increased.\n\nSynthesis of GaSe nanoparticles is carried out by the reaction of GaMe with trioctylphosphine selenium (TOPSe) in a high temperature solution of trioctylphosphine (TOP) and trioctylphosphine oxide (TOPO).\n\nA solution of 15 g TOPO and 5 mL TOP is heated to 150 °C overnight under nitrogen, removing any water that may be present in the original TOP solution. This initial TOP solution is vacuum distilled at 0.75 torr, taking the fraction from 204 °C to 235 °C. A TOPSe solution (12.5 mL TOP with 1.579 g TOPSe) is then added and the TOPO/TOP/TOPSe reaction mixture is heated to 278 °C. GaMe (0.8 mL) dissolved in 7.5 mL distilled TOP is then injected. After injection, the temperature drops to 254 °C before stabilizing in the range of 266-268 °C after 10 minutes. GaSe nanoparticles then begin to form, and may be detected by a shoulder in the optical absorption spectrum in the 400-450 nm range. After this shoulder is observed, the reaction mixture is left to cool to room temperature to prevent further reaction. After synthesis and cooling, the reaction vessel is opened and extraction of the GaSe nanoparticle solution is accomplished by addition of methanol. The distribution of nanoparticles between the polar (methanol) and non-polar (TOP) phases depends on experimental conditions. If the mixture is very dry, nanoparticles partition into the methanol phase. If the nanoparticles are exposed to air or water, however, the particles become uncharged and become partitioned into the non-polar TOP phase.\n\n"}
{"id": "25700641", "url": "https://en.wikipedia.org/wiki?curid=25700641", "title": "Green Run", "text": "Green Run\n\nThe \"Green Run\" was a secret U.S. Government release of radioactive fission products on December 2–3, 1949, at the Hanford Site plutonium production facility, located in Eastern Washington. Radioisotopes released at that time were supposed to be detected by U.S. Air Force reconnaissance. Freedom of Information Act (FOIA) requests to the U.S. Government have revealed some of the details of the experiment. Sources cite of iodine-131 released, and an even greater amount of xenon-133. The radiation was distributed over populated areas, and caused the cessation of intentional radioactive releases at Hanford until 1962 when more experiments commenced.\n\nThere are some indications contained in the documents released by the FOIA requests that many other tests were conducted in the 1940s prior to the Green Run, although the Green Run was a particularly large test. Evidence suggests that filters to remove the iodine were disabled during the Green Run.\n\nThe project gets its name from the processing of uranium at Hanford, WA in an open loop/water cooled nuclear reactor for the sole purpose of irradiating the Uranium-238 producing the fissile Plutonium-239. Due to other unwanted highly radioactive decay products being formed, normal batch processing would take place 83 to 101 days after reactor extraction to allow the radioactive isotopes to decay before extracting the fissile Plutonium-239 in a safe manner for the 30,000 nuclear weapons amassed and now MOX fuel during the cold war by the United States. For the Green Run test, a batch was fresh from the reactor with only a scheduled 16-day decay period and then was vented into the atmosphere prematurely. The unfiltered exhaust from the production facility was therefore much more radioactive than during a normal batch.\n\nLeland Fox says that his father was in the military and was bivouacked on the banks of the Wenatchee River during the Green Run:\n\nHealth Physicist Carl C. Gamertsfelder, Ph.D. described his recollections as to the reasons for the Green Run by attributing it to the intentions of the Air Force to be able to track Soviet releases.\n\n"}
{"id": "17493392", "url": "https://en.wikipedia.org/wiki?curid=17493392", "title": "Helge Eide", "text": "Helge Eide\n\nHelge Eide (born 1 April 1954) is a Norwegian businessman.\n\nHe graduated from Rogaland University College (now named University of Stavanger) with a bachelor's degree in petroleum engineering.\n\nHe became Managing Director in DNO ASA in 2000, having first joined that company in 1996. He previously held management positions in Smedvig Group, Norsk Hydro, Read Petroleum Energy and Elf.\n"}
{"id": "3220132", "url": "https://en.wikipedia.org/wiki?curid=3220132", "title": "Hibernation (computing)", "text": "Hibernation (computing)\n\nHibernation (or suspend to disk) in computing is powering down a computer while retaining its state. Upon hibernation, the computer saves the contents of its random access memory (RAM) to a hard disk or other non-volatile storage. Upon resumption, the computer is exactly as it was before entering hibernation. \n\nAfter hibernating, the hardware is powered down like a regular shutdown. Hibernation is a means of avoiding the burden of saving unsaved data before shutting down and restoring all running programs after powering back on. Hibernation is used in laptops, which have limited battery power available. It can be set to happen automatically on a low battery alarm. Most desktops also support hibernation, mainly as a general energy saving measure.\n\nMany systems also support a low-power sleep mode in which the processing functions of the machine are powered down, using a little power to preserve the contents of RAM and support waking up. Instantaneous resumption is one of the advantages of sleep mode over hibernation. A hibernated system must start up and read data back to RAM, which typically takes time. A system in sleep mode only needs to power up the CPU and display, which is almost instantaneous. On the other hand, a system in sleep mode still consumes power to keep the data in the RAM. Detaching power from a system in sleep mode results in data loss, while cutting the power of a system in hibernation has no risk; the hibernated system can resume when and if the power is restored. Both shut down and hibernated systems may consume standby power unless they are unplugged.\n\nSleep mode and hibernation can be combined: The contents of RAM are copied to the non-volatile storage and the computer enters sleep mode. This approach combines the benefits of sleep mode and hibernation: The machine can resume instantaneously, and its state, including open and unsaved files, survives a power outage. Hybrid sleep consumes as much power as sleep mode while hibernation powers down the computer.\n\nEarly implementations of hibernation used the BIOS, but modern operating systems usually handle hibernation. Hibernation is defined as sleeping mode S4 in the ACPI specification.\n\nOn Windows computers, hibernation is available only if all hardware and device drivers are ACPI and plug-and-play–compliant. Hibernation can be invoked from the Start menu or the command line.\n\nWindows 95 supports hibernation through hardware manufacturer-supplied drivers and only if compatible hardware and BIOS are present. Since Windows 95 supports only Advanced Power Management (APM), hibernation is called Suspend-to-Disk. Windows 98 and later support ACPI. However, hibernation often caused problems since most hardware was not fully ACPI 1.0 compliant or did not have WDM drivers. There were also issues with the FAT32 file system.\n\nWindows 2000 is the first Windows to support hibernation at the operating system level (OS-controlled ACPI S4 sleep state) without special drivers from the hardware manufacturer. A hidden system file named \"hiberfil.sys\" in the root of the boot partition is used to store the contents of RAM when the computer hibernates. In Windows 2000, this file is as big as the total RAM installed. \n\nWindows Me, the last release in the Windows 9x family, also supports OS controlled hibernation and requires disk space equal to that of the computer's RAM.\n\nWindows XP further improved support for hibernation. Hibernation and resumption are much faster as memory pages are compressed using an improved algorithm; compression is overlapped with disk writes, unused memory pages are freed and DMA transfers are used during I/O. hiberfil.sys contains further information including processor state. This file was documented by a security researcher Matthieu Suiche during Black Hat Briefings 2008 who also provided a computer forensics framework to manage and convert this file into a readable memory dump. The compression algorithm was later documented by Microsoft as well.\n\nAlthough Windows XP added support for more than 4 gigabytes of memory (through Windows XP 64-bit Edition and Windows XP Professional x64 Edition), this operating system, as well as Windows Server 2003, Windows Vista and Windows Server 2008 do not support hibernation when this amount of memory is installed because of performance issues associated with saving such a large pool of data from RAM to disk.\n\nWindows Vista introduced a hybrid sleep feature, which saves the contents of memory to hard disk but instead of powering down, enters sleep mode. If the power is lost, the computer can resume as if hibernated.\n\nWindows 7 introduced compression to the hibernation file and set the default size to 75% of the total physical memory. Microsoft also recommends to increase the size using the codice_1 tool in some rare workloads where the memory footprint exceeds that amount. It can be set from anywhere between 50% to 100%, although decreasing it is not recommended.\n\nWindows 8's resume-from-hibernation algorithm is multi-core optimized. Windows 8 also introduces a \"Hybrid Boot\" feature. When users select the \"Shut Down\" option, it hibernates the computer, but closes all programs and logs out the user session before hibernating. According to Microsoft, a regular hibernation includes more data in memory pages which takes longer to be written to disk. In comparison, when the user session is closed, the hibernation data is much smaller and therefore takes less time to write to disk and resume. Windows 8 also saves the kernel image. Users have the option of performing a traditional shutdown by holding down the \"Shift\" key while clicking \"Shut Down\".\n\nHibernation is often under-used in business environments as it is difficult to enable it on a large network of computers without resorting to third-party PC power management software. This omission by Microsoft has been criticized as having led to a huge waste in energy.\n\nThird-party power management programs offer features beyond those present in Windows. Most products offer Active Directory integration and per-user or per-machine settings with more advanced power plans, scheduled power plans, anti-insomnia features and enterprise power usage reporting. Notable vendors include 1E NightWatchman, Data Synergy PowerMAN (Software), Faronics Power Save and Verdiem SURVEYOR.\n\nIt is possible to disable hibernation and delete codice_2.\n\nOn Macs, a feature known as Safe Sleep saves the contents of volatile memory to the system hard disk each time the Mac enters Sleep mode. The Mac can instantaneously wake from sleep mode if power to the RAM has not been lost. However, if the power supply was interrupted, such as when removing batteries without an AC power connection, the Mac would wake from Safe Sleep instead, restoring memory contents from the hard drive. Because Safe Sleep's hibernation process occurs during regular Sleep, the Apple menu does not have a \"hibernate\" option.\n\nSafe Sleep capability was added in Mac models starting with the October 2005 PowerBook G4 (Double-Layer SD). Safe Sleep requires Mac OS X v10.4 or higher.\n\nShortly after Apple started supporting Safe Sleep, Mac enthusiasts released a hack to enable this feature for much older Mac computers running Mac OS X v10.4. The classic Mac OS once also supported hibernation, but this feature was dropped by Apple.\n\nIn the Linux kernel, hibernation is implemented by swsusp which is built into the 2.6 series. An alternative implementation is TuxOnIce which is available as patches for the kernel version 3.4. TuxOnIce provides advantages such as support for symmetric multiprocessing and preemption. Another alternative implementation is uswsusp. All three refer to it as \"suspend-to-disk\".\n\n"}
{"id": "18713809", "url": "https://en.wikipedia.org/wiki?curid=18713809", "title": "Iernut Power Station", "text": "Iernut Power Station\n\nThe Iernut Power Station is a large thermal power plant located in Iernut, Mureş County having 6 generation groups, 4 of 100 MW and 2 groups of 200 MW having a total electricity generation capacity of 800 MW.\n\nIn 2007, a contract was signed with Austrian company Verbund for the installation of a seventh electricity generation group of 400 MW at a total cost of US$375 million that will increase the installed capacity of the power plant to 1,200 MW.\n"}
{"id": "52325106", "url": "https://en.wikipedia.org/wiki?curid=52325106", "title": "Johnson's parabolic formula", "text": "Johnson's parabolic formula\n\nThe Johnson formula is an empirically based formula relating the slenderness ratio to the stress illustrating the critical load required to buckle a column. The formula is based on empirical results by J. B. Johnson from around 1900 as an alternative to Euler's critical load formula under low slenderness ratio conditions.\n\nBuckling refers to a mode of failure in which the structure loses stability. It is caused by a lack of structural stiffness. Placing a load on a long slender bar will cause a buckling failure before the specimen can fail by compression.\n\nOne way to calculate buckling is to utilize Euler's formula, which produces a critical stress vs. slenderness curve such as the one illustrated to the right.\n\nHowever, depending on the geometry of the structure under stress, this equation is not always applicable, and the Johnson parabola should be used.\n\nEuler's formula is displayed as such:\nformula_1\nwhere\n\nEuler's equation is useful in situations such as an ideal pinned-pinned column, or in cases in which the effective length can be used to adjust the existing formula (ie. Fixed-Free).\n\nHowever, certain geometries are not accurately represented by the Euler formula. One of the variables in the above equation that reflects the geometry of the specimen is the slenderness ratio, which is the column's length divided by the radius of gyration.\n\nThe slenderness ratio of the member can be found with formula_9 while the critical slenderness ratio is formula_10\n\nIn practical terms, the slenderness ratio is an indicator of the specimen's resistance to bending and buckling, due to its length and cross section. If the slenderness ratio is less than the critical slenderness ratio, the column is considered to be a short column. In these cases, the Johnson parabola is more applicable than the Euler formula.\n\nJohnson's formula rounds out the function given by Euler's formula. It creates a new failure border by fitting a parabola to the graph of failure for Euler buckling.\n\nThere is a transition point on the graph of the Euler curve, located at the critical slenderness ratio. At slenderness values lower than this point (occurring in specimens with a relatively short length compared to their cross section), the graph will follow the Johnson parabola; in contrast, larger slenderness values will align more closely with the Euler equation.\n\nOne common material in aerospace applications is Al 2024. Certain material properties of Al 2024 have been determined experimentally, such as the tensile yield strength (324 MPa) and the modulus of elasticity (73.1 GPa). The Euler formula could be used to plot a failure curve, but it would not be accurate below a certain formula_12 value, the critical slenderness ratio.\n\nTherefore, the Euler equation is applicable for values of formula_12 greater than 66.7.\n\nJohnson's parabola takes care of the smaller formula_12 values.\n"}
{"id": "2286731", "url": "https://en.wikipedia.org/wiki?curid=2286731", "title": "Kaonium", "text": "Kaonium\n\nKaonium is an exotic atom consisting of a bound state of a positively charged and a negatively charged kaon. Kaonium has not been observed experimentally and is expected to have a short lifetime on the order of 10 seconds.\n"}
{"id": "5440947", "url": "https://en.wikipedia.org/wiki?curid=5440947", "title": "Khatam", "text": "Khatam\n\nKhātam () is an ancient Persian technique of inlaying. It is a version of marquetry where art forms are made by decorating the surface of wooden articles with delicate pieces of wood, bone and metal precisely-cut intricate geometric patterns. Khatam-kari () or khatam-bandi () refers to the art of crafting a khatam. Common materials used in the construction of inlaid articles are gold, silver, brass, aluminum and twisted wire.\n\nDesigning of inlaid articles is a highly elaborate process. There are sometimes more than 400 pieces per square inch in a work of average quality. In each cubic centimeter of inlaid work, up to approximately 250 pieces of metal, bone, ivory and different kinds of wood are laid side by side, glued together in stages, smoothed, oiled and polished. Inlaid articles in the Safavid era took on a special significance as artists created their precious artworks. Woods used include betel, walnut, cypress and pine. These works include doors and windows, mirror frames, Quran boxes, inlaid boxes, pen and penholders, lanterns and shrines.\n\nThe ornamentation of the doors of holy places predominantly consists of inlaid motifs. Samples of these can be observed in the cities of Mashhad, Qom, Shiraz and Rey. In the Safavid era, the art of marquetry flourished in the southern cities of Iran, especially in Isfahan, Shiraz and Kerman. The inlaid-ornamented rooms at the Saadabad Palace and the Marble Palace in Tehran are among masterpieces of this art.\n\nKhatam is practiced in Isfahan, Shiraz and Tehran. The art of inlaid and sudorific woodwork is undertaken in the workshops of the Cultural Heritage Organization of Iran, as well as in private workshops.\n\nMaster Mohammad Bagher Hakim-Elahi (محمد باقرحكيم الهي) was a master of this art, and learned the techniques from Master Sanee Khatam in Shiraz. Later in life, in early 1950's, he moved to Tehran, where he lived until the end of his life in March 2012. He continued making Khatam master pieces, ranging from small frames, and jewelry boxes, to large items such as coffee tables, bed frames, dinner tables, and large chandeliers, some of which are currently in Museums in Iran, but most are in private collection all around the world, including southern California. He also taught the art to his younger brother Asadolah Hakim-Elahi (ﺍﺴﺪﷲ ﺤﻛﻴﻢﺍﻠﻬﻰ). Asadolah died from lymphoma in the late 1970s when he was in his mid 40s.\n"}
{"id": "27301856", "url": "https://en.wikipedia.org/wiki?curid=27301856", "title": "Lacceroic acid", "text": "Lacceroic acid\n\nLacceroic acid (or dotriacontanoic acid) is a saturated fatty acid.\n\nLacceroic acid can be derived by saponification of lacceryl lacceroate or by oxidation of laccerol and purification of the product. It can also be isolated from stick lac wax, from which the name is derived.\n\nEthyl lacceroate can be obtained as a crystalline solid (rhombic plates, mp 76 °C) by the action of HCl gas on lacceroic acid in boiling absolute alcohol.\n\n\n"}
{"id": "28064435", "url": "https://en.wikipedia.org/wiki?curid=28064435", "title": "Lists of hydroelectric power stations", "text": "Lists of hydroelectric power stations\n\nThe following are lists of hydroelectric power stations based on the four methods of hydroelectric generation:\n\n"}
{"id": "2687105", "url": "https://en.wikipedia.org/wiki?curid=2687105", "title": "Lithium fluoride", "text": "Lithium fluoride\n\nLithium fluoride is an inorganic compound with the chemical formula LiF. It is a colorless solid, that transitions to white with decreasing crystal size. Although odorless, lithium fluoride has a bitter-saline taste. Its structure is analogous to that of sodium chloride, but it is much less soluble in water. It is mainly used as a component of molten salts. Formation of LiF from the elements releases one of the highest energy per mass of reactants, second only to that of BeO.\n\nLiF is prepared from lithium hydroxide or lithium carbonate with hydrogen fluoride.\n\nFluorine is produced by the electrolysis of molten potassium bifluoride. This electrolysis proceeds more efficiently when the electrolyte contains a few percent of LiF, possibly because it facilitates formation of Li-C-F interface on the carbon electrodes. A useful molten salt, FLiNaK, consists of a mixture of LiF, together with sodium fluoride and potassium fluoride. The primary coolant for the Molten-Salt Reactor Experiment was FLiBe; LiF-BeF (66-33 mol%).\n\nBecause of the large band gap for LiF, its crystals are transparent to short wavelength ultraviolet radiation, more so than any other material. LiF is therefore used in specialized UV optics, (See also magnesium fluoride). Lithium fluoride is used also as a diffracting crystal in X-ray spectrometry.\n\nIt is also used as a means to record ionizing radiation exposure from gamma rays, beta particles, and neutrons (indirectly, using the (n,alpha) nuclear reaction) in thermoluminescent dosimeters. LiF nanopowder enriched to 96% has been used as the neutron reactive backfill material for microstructured semiconductor neutron detectors (MSND) .\n\nLithium fluoride (highly enriched in the common isotope lithium-7) forms the basic constituent of the preferred fluoride salt mixture used in liquid-fluoride nuclear reactors. Typically lithium fluoride is mixed with beryllium fluoride to form a base solvent (FLiBe), into which fluorides of uranium and thorium are introduced. Lithium fluoride is exceptionally chemically stable and LiF/BeF mixtures (FLiBe) have low melting points (360 C - 459 C) and the best neutronic properties of fluoride salt combinations appropriate for reactor use. MSRE used two different mixtures in the two cooling circuits.\n\nLithium fluoride is widely used in PLED and OLED as a coupling layer to enhance electron injection. The thickness of LiF layer is usually around 1 nm. The dielectric constant (or relative permittivity) of LiF is 9.0\n\nNaturally occurring lithium fluoride is known as the mineral griceite. It is extremely rare.\n"}
{"id": "14997569", "url": "https://en.wikipedia.org/wiki?curid=14997569", "title": "Location of Earth", "text": "Location of Earth\n\nKnowledge of the location of Earth has been shaped by 400 years of telescopic observations, and has expanded radically in the last century. Initially, Earth was believed to be the center of the Universe, \nwhich consisted only of those planets visible with the naked eye and an outlying sphere of fixed stars. After the acceptance of the heliocentric model in the 17th century, observations by William Herschel and others showed that the Sun lay within a vast, disc-shaped galaxy of stars. By the 20th century, observations of spiral nebulae revealed that our galaxy was one of billions in an expanding universe, grouped into clusters and superclusters. By the end of the 20th century, the overall structure of the visible universe was becoming clearer, with superclusters forming into a vast web of filaments and voids. Superclusters, filaments and voids are the largest coherent structures in the Universe that we can observe. At still larger scales (over 1000 megaparsecs) the Universe becomes homogeneous meaning that all its parts have on average the same density, composition and structure.\n\nSince there is believed to be no \"center\" or \"edge\" of the Universe, there is no particular reference point with which to plot the overall location of the Earth in the universe. Because the observable universe is defined as that region of the Universe visible to terrestrial observers, Earth is, by definition, the center of Earth's observable universe. Reference can be made to the Earth's position with respect to specific structures, which exist at various scales. It is still undetermined whether the Universe is infinite. There have been numerous hypotheses that our universe may be only one such example within a higher multiverse; however, no direct evidence of any sort of multiverse has ever been observed, and some have argued that the hypothesis is not falsifiable.\n\n"}
{"id": "15712540", "url": "https://en.wikipedia.org/wiki?curid=15712540", "title": "Ludovic Hubler", "text": "Ludovic Hubler\n\nLudovic Hubler is a French traveller, most famous for his 5-year tour of the world completed entirely by hitchhiking (car, boat, etc.). He wrote the travelbook Le Monde en stop, rewarded by the 2010 Pierre Loti award.\n\nBorn on September 11, 1977, Ludovic Hubler is the son of Monique and Jacques Hubler and the brother of the consultant Eric Hubler and the photograph Marc Hubler. Passionate about football and geography, he grew up in Wasselonne and Obernai in the Alsace region in eastern France. In June 2002, he graduated from EM Strasbourg Business School with a Master of Science in Management.\n\nAt the end of his Master's program, believing that discovering the realities of the world was a valuable pre-requisite to entering the work world, Ludovic Hubler decided to start a tour of the world using hitchhiking as his only means of transportation.\n\nThis adventure, which he baptized his, \"life Phd\", lasted 5 years during 2003-2008. From \"sail-boat hitchhiking\" to cross the Atlantic and the Pacific, to \"ice breaker hitchhiking\" to reach Antarctica; from crossing the Sahara desert or visiting countries as notorious as Colombia and Afghanistan, Ludovic used his thumb in all kinds of improbable and difficult to imagine situations.\n\nHis encounters were as numerous as they were diverse. Among the most striking is his meeting with the 14th Dalai Lama, who received him in his hometown Dharamshala (India) in 2007 but so were some of the thousands of students in all corners of the world with whom he shared his story.\n\nTotalling the 5 years, the 170,000 kilometers travelled, the 59 countries visited, the hundreds of lectures given and recalling gratitude for the help of over 1,300 drivers gives an idea of the richness of Ludovic's trip - an adventure that was shared on a daily basis with pediatric cancer patients at the Hospital of Strasbourg, back home in Strasbourg, France.\n\nLudovic Hubler now lives in Menton and works as Head of Programmes and Field Operations for Peace and Sport, \"L'organisation pour la paix par le sport\" placed under the High Patronage of HSH Prince Albert II of Monaco.\n\nHe continues to regularly give lectures sharing his journey and what he learned on the road. Ludovic Hubler is now married to Panamanian native Marisol Richards Espinosa, whom he met during the trip in September 2005, as he hitchhiked through Panama.\n\n\n\n\n\n"}
{"id": "26383841", "url": "https://en.wikipedia.org/wiki?curid=26383841", "title": "March 1960 nor'easter", "text": "March 1960 nor'easter\n\nThe March 1960 nor'easter was a severe winter storm that impacted the Mid-Atlantic and New England regions of the United States. The storm ranked as Category 4, or \"crippling\", on the Northeast Snowfall Impact Scale. Northeasterly flow, combined with the storm's slower forward motion, enhanced snowfall across the region. The cyclone began moving away from the United States on March 5. It took place during a stormy period in the affected region, contributing to record snowfall.\n\nThe storm's impacts were wide-reaching; snow accumulated from the southeastern United States through northern New England. Totals exceeding were reported from West Virginia to Maine, while snowfall of over fell in parts of eastern Massachusetts, Rhode Island, northern Connecticut, southern New Hampshire, northern New Jersey and southeastern New York. Nantucket, Massachusetts reported of snow, the most on record. Blizzard conditions organized in eastern Massachusetts, accompanied by intense winds. The storm caused at least 80 fatalities and stranded thousands of residents. Schools were forced to close, and transportation was severely disrupted. Stalled vehicles on roadways hampered snow removal efforts. New York City received the most severe winter storm since 1948. Many commuters in Manhattan became marooned. Major airports closed during the storm, resulting in the cancellation of hundreds of flights.\n\n"}
{"id": "40545481", "url": "https://en.wikipedia.org/wiki?curid=40545481", "title": "Ministry of Energy, Industry and Mineral Resources", "text": "Ministry of Energy, Industry and Mineral Resources\n\nThe Ministry of Energy, Industry and Mineral Resources is one of the governmental bodies of Saudi Arabia and part of the cabinet. The ministry has the function of developing and implementing policies concerning petroleum and related products.\n\nThe ministry was established in December 1960. Prior to the formation of the ministry policies regarding oil production and planning were overseen by the directorate general of petroleum and mineral affairs which was attached to the ministry of finance. Then the directorate was converted into the ministry. The ministry was named the Ministry of Petroleum and Mineral Resources until May 2016 when it was renamed as the Ministry of Energy, Industry and Mineral Resources.\n\nThe ministry is based in Riyadh.\n\nSince 1960 the ministry was headed by the following five ministers:\n\nThe ministry is primarily responsible for the policies concerning oil, gas and natural minerals in the country which is the world's largest holder of crude oil reserves. It closely monitors the activities of the Saudi Aramco together with the Supreme Council for Petroleum and Minerals. However, the ministry has much more responsibility in this regard than the council.\n\nThe other agency with which the ministry works is Petromin, the general petroleum and mineral organization. Through Saudi Arabian Basic Industries Company (SABIC), established in 1976, the ministry oversaw the operation of petrochemicals and other heavy industry projects.\n"}
{"id": "9386081", "url": "https://en.wikipedia.org/wiki?curid=9386081", "title": "Mucking Marshes Landfill", "text": "Mucking Marshes Landfill\n\nMucking Marshes landfill was a major landfill site servicing London, close to the ham of Mucking. Covering hundreds of acres of former gravel quary, it was one of the largest landfills in Western Europe and had been filled for decades with municipal and commercial waste floated thirty miles down the River Thames in barges to Mucking Wharf. The barges, each carrying dozens of distinctive yellow containers, were a familiar, though rarely commented-upon, sight along the Thames through Central London. Once the barges had travelled downstream from Walbrook Wharf, mechanical cranes at Mucking Wharf unloaded the containers onto trucks. The trucks made their way up the artificial mound created by decades of garbage compaction that still towers over the surrounding flat landscape. Flocks of seagulls and other scavenging estuarine birds were a familiar sight as the trucks disgorged their contents. \n\nThe former landfill site itself, although it dominates the village of Mucking, is guarded and surrounded by a perimeter fence more than four miles (6 km) long. Cory Environmental, the operators of the site, gated off Mucking Wharf Road so that views of the Thames meeting the North Sea can now be accessed from Mucking only via a circuitous footpath through the neighbouring village of East Tilbury.\n\nChanges in London governance, including the creation of the Greater London Authority under Ken Livingstone, led to indications of a reassessment of London's waste strategy based more on recycling and less on landfill sites like Mucking Marshes. However, in 2007, Mucking Marshes Landfill was granted an extension to receive London's waste until 2010.\n\nIn 2012, the site was reclaimed for community and environmental use in a project involving The Cory Environmental Trust, DP World port, and the Essex Wildlife Trust working together to create the Thurrock Thameside Nature Park. This is a wildlife site open to the public, expanding to over the next few years, to help establish and protect wildlife and bird populations and environments. The site also houses the recently completed Cory Environmental Trust Visitor Centre, a drum-shaped, timber-clad building designed by van Heyningen and Haward Architects. The reserve and the new building were opened on 11 May 2013 by Sir David Attenborough, who described the building as 'revolutionary'.\n\n"}
{"id": "24923211", "url": "https://en.wikipedia.org/wiki?curid=24923211", "title": "Neslihan Gökdemir", "text": "Neslihan Gökdemir\n\nNeslihan Gökdemir (born 1970, Istanbul) founded the Energy Forum of Turkey in 2001 and has since been the Chairman. She has been developing and delivering projects to establish an infrastructure for the energy industry and build a sustainable future.\n\nTogether with the co-operation of the academia this project ensures a continuous production and flow of information to the government, in particular, and authorities in the public and private sector so as to enhance the Turkish energy industry’s strategic position in the changing juncture across the world and in Turkey.\n\nUpon graduation, Neslihan Gökdemir worked as an economic journalist and columnist. She also worked as a consultant for the construction and energy industries as well as for matters concerning Russia and the CIS countries. She has attended many international conferences in these areas since 1992. She has worked as a journalist for economy, international relations and energy.\n\nNeslihan Gökdemir graduated from the Department of Economics at Marmara University. During her undergraduate studies she attended Exeter University as an exchange student.\n\nNeslihan Gökdemir is the Founder and Chairman of the Energy Forum of Turkey (2001), the Founder of the Energy market Monitoring and Rating Agency (2005) and, the Founder and Chairman of the Association of Energy Journalists (2007).\n\nNeslihan Gökdemir is married. She speaks English and Russian.\n\n"}
{"id": "39152193", "url": "https://en.wikipedia.org/wiki?curid=39152193", "title": "Non-neutral plasmas", "text": "Non-neutral plasmas\n\nA non-neutral plasma is a plasma for which the total charge is sufficiently different from zero, so that the electric field created by the un-neutralized charge plays an important or even dominant role in the plasma dynamics. The simplest non-neutral plasmas are plasmas consisting of a single charge species. Examples of single species non-neutral plasmas that have been created in laboratory experiments are plasmas consisting entirely of electrons, pure ion plasmas, positron plasmas, and antiproton plasmas.\n\nNon-neutral plasmas are used for research into basic plasma phenomena such as cross-magnetic field transport, nonlinear vortex interactions, and plasma waves and instabilities. They have also been used to create cold neutral antimatter, by carefully mixing and recombining cryogenic pure positron and pure antiproton plasmas. Positron plasmas are also used in atomic physics experiments that study the interaction of antimatter with neutral atoms and molecules. Cryogenic pure ion plasmas have been used in studies of and quantum entanglement. More prosaically, pure electron plasmas are used to produce the microwaves in microwave ovens, via the magnetron instability.\n\nNeutral plasmas in contact with a solid surface (that is, most laboratory plasmas) are typically non-neutral in their edge regions. Due to unequal loss rates to the surface for electrons and ions, an electric field (the \"ambipolar field\" ) builds up, acting to hold back the more mobile species until the loss rates are the same. The electrostatic potential (as measured in electron-volts) required to produce this electric field depends on many variables but is often on the order of the electron temperature.\n\nNon-neutral plasmas for which all species have the same sign of charge have exceptional confinement properties compared to neutral plasmas. They can be confined in a thermal equilibrium state using only static electric and magnetic fields, in a Penning trap configuration (see Fig. 1). Confinement times of up to several hours have been achieved. Using the \"rotating wall\" method, the plasma confinement time can be increased arbitrarily.\n\nSuch non-neutral plasmas can also access novel states of matter. For instance, they can be cooled to cryogenic temperatures without recombination (since there is no oppositely charged species with which to recombine). If the temperature is sufficiently low (typically on the order of 10 mK), the plasma can become a . The body-centered-cubic structure of these plasma crystals has been observed by Bragg scattering in experiments on laser-cooled pure beryllium plasmas.\n\nNon-neutral plasmas with a single sign of charge can be confined for long periods of time using only static electric and magnetic fields. One such configuration is called a Penning trap, after the inventor F. M. Penning. The cylindrical version of the trap is also sometimes referred to as a Penning-Malmberg trap, after Prof. John Malmberg. The trap consists of several cylindrically symmetric electrodes and a uniform magnetic field applied along the axis of the trap (Fig 1). Plasmas are confined in the axial direction by biasing the end electrodes so as to create an axial potential well that will trap charges of a given sign (the sign is assumed to be positive in the figure). In the radial direction, confinement is provided by the Lorentz force due to rotation of the plasma about the trap axis. Plasma rotation causes an inward directed Lorentz force that just balances the outward directed forces caused by the unneutralized plasma as well as the centrifugal force. Mathematically, radial force balance implies a balance between electric, magnetic and centrifugal forces:\n\nwhere particles are assumed to have mass \"m\" and charge \"q\", \"r\" is radial distance from the trap axis and \"E\" is the radial component of the electric field. This quadratic equation can be solved for the rotational velocity formula_1, leading to two solutions, a slow-rotation and a fast-rotation solution. The rate of rotation formula_2 for these two solutions can be written as\n\nwhere formula_4 is the cyclotron frequency. Depending on the radial electric field, the solutions for the rotation rate fall in the range formula_5. The slow and fast rotation modes meet when the electric field is such that formula_6. This is called the Brillouin limit; it is an equation for the maximum possible radial electric field that allows plasma confinement.\n\nThis radial electric field can be related to the plasma density \"n\" through the Poisson equation,\n\nand this equation can be used to obtain a relation between the density and the plasma rotation rate. If we assume that the rotation rate is uniform in radius (i.e. the plasma rotates as a rigid body), then Eq. (1) implies that the radial electric field is proportional to radius \"r\". Solving for \"E\" from this equation in terms of formula_8 and substituting the result into Poisson's equation yields\n\nThis equation implies that the maximum possible density occurs at the Brillouin limit, and has the value\n\nwhere formula_10 is the speed of light. Thus, the rest energy density of the plasma, n·m·c, is less than or equal to the magnetic energy density formula_11 of the magnetic field. This is a fairly stringent requirement on the density. For a magnetic field of 10 tesla, the Brillouin density for electrons is only n = .\n\nThe density predicted by Eq.(2), scaled by the Brillouin density, is shown as a function of rotation rate in Fig. (2). Two rotation rates yield the same density, corresponding to the slow and fast rotation solutions.\n\nIn experiments on single species plasmas, plasma rotation rates in the tens of kHz range are not uncommon, even in the slow rotation mode. This rapid rotation is necessary to provide the confining radial Lorentz force for the plasma. However, if there is neutral gas in the trap, collisions between the plasma and the gas cause the plasma rotation to slow, leading to radial expansion of the plasma until it comes in contact with the surrounding electrodes and is lost. This loss process can be alleviated by operating the trap in an ultra high vacuum. However, even under such conditions the plasma rotation can still be slowed through the interaction of the plasma with \"errors\" in the external confinement fields. If these fields are not perfectly cylindrically symmetric, the asymmetries can torque on the plasma, reducing the rotation rate. Such field errors are unavoidable in any actual experiment, and limit the plasma confinement time.\n\nIt is possible to overcome this plasma loss mechanism by applying a rotating field error to the plasma. If the error rotates faster than the plasma, it acts to spin up the plasma (similar to how the spinning blade of a blender causes the food to spin), counteracting the effect of field errors that are stationary in the frame of the laboratory. This rotating field error is referred to as a \"rotating wall\", after the theory idea that one could reverse the effect of a trap asymmetry by simply rotating the entire trap at the plasma rotation frequency. Since this is impractical, one instead rotates the trap electric field rather than the entire trap, by applying suitably phased voltages to a set of electrodes surrounding the plasma.\n\nWhen a non-neutral plasma is cooled to cryogenic temperatures, it does not recombine to a neutral gas as would a neutral plasma, because there are no oppositely charged particles with which to recombine. As a result, the system can access novel strongly coupled non-neutral states of matter, including plasma crystals consisting solely of a single charge species. These strongly coupled non-neutral plasmas are parametrized by the coupling parameter Γ, defined as\n\nwhere formula_13 is the temperature and formula_14 is the Wigner-Seitz radius (or mean inter-particle spacing), given in terms of the density formula_15 by the expression formula_16. The coupling parameter can be thought of as the ratio of the mean interaction energy between nearest-neighbor pairs, formula_17, and the mean kinetic energy of order formula_18. When this ratio is small, interactions are weak and the plasma is nearly an ideal gas of charges moving in the mean-field produced by the other charges. However, when formula_19 interactions between particles are important and the plasma behaves more like a liquid, or even a crystal if formula_20 is sufficiently large. In fact, computer simulations and theory have predicted that for an infinite homogeneous plasma the system exhibits a gradual onset of short-range order consistent with a liquid-like state for formula_21, and there is predicted to be a first-order phase transition to a body-centered-cubic crystal for formula_22.\n\nExperiments have observed this crystalline state in a pure beryllium ion plasma that was laser-cooled to the millikelvin temperature range. The mean inter-particle spacing in this pure ion crystal was on the order of 10-20 µm, much larger than in neutral crystalline matter. This spacing corresponds to a density on the order of 10-10 cm, somewhat less than the Brillouin limit for beryllium in the 4.5 tesla magnetic field of the experiment. Cryogenic temperatures were then required in order to obtain a formula_20 value in the strongly coupled regime. The experiments measured the crystal structure by the Bragg-scattering technique, wherein a collimated laser beam was scattered off of the crystal, displaying Bragg peaks at the expected scattering angles for a bcc lattice (See Fig. 3).\n\nWhen small numbers of ions are laser-cooled, they form crystalline \"Coulomb clusters\". The symmetry of the cluster depends on the form of the external confinement fields. An interactive 3D view of some of the clusters can be found here.\n"}
{"id": "12267400", "url": "https://en.wikipedia.org/wiki?curid=12267400", "title": "Nuclear Safety Research Reactor", "text": "Nuclear Safety Research Reactor\n\nThe Nuclear Safety Research Reactor (NSRR) is a TRIGA design nuclear research reactor operated by the Japan Atomic Energy Agency.\n\n\nAs of 2006, the reactor has performed 3033 pulses and 1280 Nuclear fuel experiments. In 1989 the reactor underwent a power uprate.\n\n"}
{"id": "16862071", "url": "https://en.wikipedia.org/wiki?curid=16862071", "title": "Pipe flow", "text": "Pipe flow\n\nPipe flow, a branch of hydraulics and fluid mechanics, is a type of liquid flow within a closed conduit (conduit in the sense of a means of containment). The other type of flow within a conduit is open channel flow. \nThese two types of flow are similar in many ways, but differ in one important aspect. Pipe flow does not have a free surface which is found in open-channel flow. Pipe flow, being confined within closed conduit, does not exert direct atmospheric pressure, but does exert hydraulic pressure on the conduit.\n\nNot all flow within a closed conduit is considered pipe flow. Storm sewers are closed conduits but usually maintain a free surface and therefore are considered open-channel flow. The exception to this is when a storm sewer operates at full capacity, and then can become pipe flow.\n\nEnergy in pipe flow is expressed as head and is defined by the Bernoulli equation. In order to conceptualize head along the course of flow within a pipe, diagrams often contain a hydraulic grade line. Pipe flow is subject to frictional losses as defined by the Darcy-Weisbach formula.\n\nThe behavior of pipe flow is governed mainly by the effects of viscosity and gravity relative to the inertial forces of the flow. Depending on the effect of viscosity relative to inertia, as represented by the Reynolds number, the flow can be either laminar or turbulent. At a Reynolds number below the critical value of approximately 2040 pipe flow will ultimately be laminar, whereas above the critical value turbulent flow can persist. In addition, the transition between laminar flow and turbulence can be sensitive to disturbance levels and imperfections.\n\nFlow through pipes can roughly be divided into two:\n\nMathematical equations and concepts\nFields of study\nTypes of fluid flow\nFluid properties\nFluid phenomena\n\n"}
{"id": "3295074", "url": "https://en.wikipedia.org/wiki?curid=3295074", "title": "Platinum silicide", "text": "Platinum silicide\n\nPlatinum silicide, also known as platinum monosilicide, is the inorganic compound with the formula PtSi and forms an orthorhombic crystalline structure when synthesized.\n\nThe crystal structure of PtSi is orthorhombic, with each silicon atom having six neighboring platinum atoms. The distances between the silicon and the platinum neighbors are as follows: one at a distance of 2.41 angstroms, two at a distance of 2.43 angstroms, one at a distance of 2.52 angstroms, and the final two at a distance of 2.64 angstroms. Each platinum atom has six silicon neighbors at the same distances, as well as two platinum neighbors, at a distance of 2.87 and 2.90 angstroms. All of the distances over 2.50 angstroms are considered too far to really be involved in bonding interactions of the compound. As a result, it has been shown that two sets of covalent bonds compose the bonds forming the compound. One set is the three center Pt-Si-Pt bond, and the other set the two center Pt-Si bonds. Each silicon atom in the compound has one three center bond and two two center bonds. The thinnest film of PtSi would consist of two alternating planes of atoms, a single sheet of orthorhombic structures. Thicker layers are formed by stacking pairs of the alternating sheets. The mechanism of bonding between PtSi is more similar to that of pure silicon than pure platinum or Pt2Si, though experimentation has revealed metallic bonding character in PtSi that pure silicon lacks.\n\nPtSi can be synthesized in several ways. The standard method involves depositing a thin film of pure platinum onto silicon wafers and heating in a conventional furnace at 450-600 °C for a half an hour in inert ambients. The process cannot be carried out in an oxygenated environment, as this results in the formation of an oxide layer on the silicon, preventing PtSi from forming.\nA secondary technique for synthesis requires a sputtered platinum film deposited on a silicon substrate. Due to the ease with which PtSi can become contaminated by oxygen, several variations of the methods have been reported. Rapid thermal processing has been shown to increase the purity of PtSi layers formed. Lower temperatures (200-450 °C) were also found to be successful \n, higher temperatures produce thicker PtSi layers, though temperatures in excess of 950 °C formed PtSi with increased resistivity due to clusters of large PtSi grains.\n\nDespite the synthesis method employed, PtSi forms in the same way. When pure platinum is first heated with silicon, Pt2Si is formed. Once all the available Pt and Si are used and the only available surfaces are Pt2Si, the silicide will begin the slower reaction of converting into PtSi. The activation energy for the Pt2Si reaction is around 1.38 eV, while it is 1.67 eV for PtSi. \nOxygen is extremely detrimental to the reaction, as it will bind preferably to Pt, limiting the sites available for Pt-Si bonding and preventing the silicide formation. A partial pressure of O as low at 10 has been found to be sufficient to slow the formation of the silicide. To avoid this issue inert ambients are used, as well as small annealing chambers to minimize amount of potential contamination. The cleanliness of the metal film is also extremely important, and unclean conditions result in poor PtSi synthesis. \nHowever, in certain cases an oxide layer can be beneficial. When PtSi is used as a Schottky barrier, an oxide layer has been shown to be protective and prevent wear of the PtSi.\n\nPtSi is a semiconductor and a Schottky barrier with high stability and good sensitivity, and can be used in infrared detection, thermal imaging, or ohmic and Schottyky contacts. Platinum silicide was most widely studied and used in the 1980s and 90’s, but has become less commonly used, due to its low quantum efficiency. PtSi is now most commonly used in infrared detectors, due to the large size of wavelengths it can be used to detect. It has also been used in detectors for infrared astronomy. It can operate with good stability up to 0.05 °C. Platinum silicide offers high uniformity of arrays imaged. The low cost and stability makes it suited for preventative maintenance and scientific IR imaging.\n\n\n"}
{"id": "2207861", "url": "https://en.wikipedia.org/wiki?curid=2207861", "title": "Power density", "text": "Power density\n\nPower density (or \"volume power density\" or \"volume specific power\") is the amount of power (time rate of energy transfer) per unit volume.\n\nIn energy transformers including batteries, fuel cells, motors, etc., and also power supply units or similar, power density refers to a volume. It is then also called volume power density, which is expressed as W/m.\n\nVolume power density is sometimes an important consideration where space is constrained.\n\nIn reciprocating internal combustion engines, power density—power per swept volume or brake horsepower per cubic centimeter —is an important metric. This is based on the \"internal\" capacity of the engine, not its external size.\n\n"}
{"id": "23511734", "url": "https://en.wikipedia.org/wiki?curid=23511734", "title": "Rotary converter plant", "text": "Rotary converter plant\n\nA rotary converter plant is a facility at which rotary converters convert one form of electricity to another form of electricity. The installed combinations of motors and generators at a plant determine the possible type(s) of conversion. Such facilities also allow the setting of voltages and frequencies, if appropriate equipment is installed. Rotary converter plants were commonplace in railway electrification before the invention of mercury arc rectifiers in the 1920s.\n\nAt each facility, power from an AC power grid was converted to DC to feed into an overhead line or a third rail of a railway. Rotary converter plants were also used for coupling power grids of different frequencies and operation modes. The former Neuhof Substation was an example of the latter. Former machinery transmitters like the Alexanderson alternator were, strictly speaking, rotary converter plants.\n\nIn spite of modern power semiconductor technology, rotary converters are still common for feeding railway systems with AC of a different frequency from that of the main electricity grid. In Europe, this would typically be for 15 kV AC railway electrification.\n\n"}
{"id": "5392262", "url": "https://en.wikipedia.org/wiki?curid=5392262", "title": "Shabaka Stone", "text": "Shabaka Stone\n\nThe Shabaka Stone, sometimes Shabaqo, is a relic incised with an ancient Egyptian religious text, which dates from the Twenty-fifth Dynasty of Egypt. In later years, the stone was likely used as a millstone, which damaged the hieroglyphs. This damage is accompanied by other intentional defacements, leaving the hieroglyphic inscription in poor condition.\n\nOriginally erected as a lasting monument at the Great Temple of Ptah in Memphis in the late eighth century BCE, the stone was at some point removed (for unknown reasons) to Alexandria. From there, it was transported by a navy vessel from Alexandria to England. It was brought back as ballast along with a capital of an Egyptian column, fragments of a Greco-Roman black basalt capital, two fragments of quartzite lintel of Senwosret III, and a black granite kneeling statue of Ramesses II. In 1805, the stone was donated to the British Museum by George Spencer, 2nd Earl Spencer (1758-1834), who was First Lord of the Admiralty and since 1794 the trustee of the museum. In 1901, the stone was deciphered, translated, and interpreted for the first time by the American egyptologist, James Henry Breasted. The monument has remained at the museum to the present day.\n\nSince this stone was meant to be a preservation of an older text, the question regarding the dating of the original work has been sought after. Attempts to attribute a definite date for the original text have been inconclusive. Breasted, Adolf Erman, Kurt Sethe, and Hermann Junker all dated the stone to the Old Kingdom. The stone is archaic, both linguistically (its language is similar to that used in the Pyramid Texts of the Old Kingdom) and politically (it alludes to the importance of Memphis as the first royal city). As such, Henri Frankfort, John Wilson, Miriam Lichtheim, and Erik Iverson have also assessed the stone to be from the Old Kingdom. However, Friedrich Junge and most other scholars since then have argued that the monument was produced in the Twenty-fifth Dynasty. Today, scholars feel it is clear that it cannot predate the Nineteenth Dynasty.\n\nThe stela is around wide, with the left side height estimated at and the right side about . The written surface is in width and on average, in height. The rectangular hole in the center is , with eleven radiating lines ranging in length from . The area of the surface which has been completely worn-out measures across.\n\nIn 1901, James Henry Breasted identified the stone as a rectangular slab of black granite. While other scholars postulated that the monument was a slab or basalt or a conglomerate stone, a recent analysis by a scientist of the British Museum revealed the stone to be green breccia originating from Wadi Hammamat.\n\nThe text includes two main divisions with a short introduction and an ending summary. The first division relates the unification of Upper and Lower Egypt. Ptah works through Horus to accomplish this unification. The other is a creation story, the Memphite Theology, that establishes Ptah as the creator of all things, including gods.\n\nThe text stresses that it is in Memphis that the unification of Egypt took place. The inscription also states that this town was the burial-place of Osiris, after he drifted ashore.\n\nThe first line of the stone presents the fivefold royal titulary of the king: \"The living Horus: Who prospers the Two Lands; the Two Ladies: Who prospers the Two Lands; the King of Upper and Lower Egypt: Neferkare; the Son of Re: [Shabaka], beloved of Ptah-South-of-His-Wall, who lives like Re forever.” The first three names emphasize the king's manifestation as a living god (especially of the falcon-headed Horus, patron god to the Egyptian kings), while the latter two names (the king's throne name and birth name) refer to Egypt's division and unification.\n\nThe second line, a dedicatory introduction, claims that the stone is a copy of the surviving contents of a worm-ridden, decaying papyrus found by the pharaoh Shabaka as he was inspecting the Great Temple of Ptah.\n\nLines 3 to 47 describe the unification of Upper and Lower Egypt under the god Horus at Memphis. The text first declares the political and theological supremacy of the Memphite god Ptah, the king of both Upper and Lower Egypt, and the creator of the Ennead. The inscription then describes how Horus, as a manifestation of Ptah, initially rules Lower Egypt while his rival Set rules Upper Egypt. However, Horus receives Upper Egypt from Geb, becoming the sole ruler of the land.\n\nLines 48 to 64 recount the creation myth known as the Memphite Theology. Ptah, the patron god of craftsmen, metalworkers, artisans, and architects was viewed as a creator-god, a divine craftsman of the universe who was responsible for all existence. Creation was first a spiritual and intellectual activity, facilitated by the divine heart (thought) and tongue (speech/word) of Ptah. Then, creation became a physical activity carried out by Atum, who, created by Ptah's teeth and lips, produced the Ennead from his seed and hands.\n\nLines 61 through 64 summarize the text as a whole.\n\nAccording to Ragnhild Bjerre Finnestad, there are three theories on the possible purpose of the Shabaka text:\nAs a temple text written down and set up in the temple of Ptah, it is likely that the Shabaka Stone served a religious, cultic-theological purpose, placing its subject matter within a cultic frame of reference.\n\nProjecting from the rectangular hole in the center of the stone are radial rough stripes, which destroyed the inscription within a radius of 78 cm, measured from the middle of the stone. According to the secondary literature on the monument, this damage occurred because the stone was re-used as a millstone. The oldest reference speculating the stone's use as a millstone is found in the display of the British Museum of 1821. However, the stone could instead have been the foundation of something round, possibly a column or a pillar.\n\nSome parts of the stone were intentionally cut out during the Dynastic Period. This included the name of Seth (line 7), a god which was condemned during this time. Additionally, Psamtik II or Psamtik III erased the proper name and throne name of Shabaka from the stone. Psamtik III then engraved his name onto the stone, but his name was in turn erased by the Persians during their conquest.\n\n"}
{"id": "40058156", "url": "https://en.wikipedia.org/wiki?curid=40058156", "title": "Soft-switching three-level inverter", "text": "Soft-switching three-level inverter\n\nA soft-switching three-level inverter (S3L inverter) is a high-efficiency power electronic inverter intended, in particular, for use with three-phase drives, as a grid-tie inverter for photovoltaic installations or wind turbines and in power supplies. The topology was developed in 2009 at HTWG Konstanz (Constance University of Applied Sciences).\n\nInverters are used for converting DC voltage into AC voltage. Their construction typically makes use of power transistors and diodes. These are operated as electronic switches. In conventional designs using \"hard\" switching, this gives rise to switching losses which, especially for high values of the switching frequency, cause a reduction in their energy conversion efficiency. To improve their efficiency, high-power inverters (from about 10 kW) frequently make use of a technique referred to as a three-level design (three-level inverter).\n\nForming the basis of the S3L inverter is a hard-switching three-level inverter of this kind with a T-type topology. This base design is supplemented by a snubber circuit consisting of a few passive components. It prevents the occurrence of simultaneously high values of voltage and current, and hence high power dissipation values, during the switching process. All switching processes therefore take place in a \"soft\" manner. In this way switching losses are largely avoided. Furthermore, because the snubber circuit functions, in principle, without losses, the conversion efficiency of the inverter remains high even for high values of the switching frequency.\n"}
{"id": "3539049", "url": "https://en.wikipedia.org/wiki?curid=3539049", "title": "Spring box", "text": "Spring box\n\nA spring box is a structure engineered to allow groundwater to be obtained from a natural spring. The spring box functions to protect the spring water from contamination, normally by surface runoff or contact with human and animals, and provides a point of collection and a place for sedimentation. In many instances it also acts as the principal water storage for the household water supply. The area surrounding the spring box should be fenced off in order to reduce the risk of contamination from animal faeces. An overflow pipe should be installed into the spring box, and it should also have a well fitting lid. \n\nIf the flow rate is relatively high, a spring box may not be necessary. Instead, a pipe can be driven horizontally into the spring. These systems are sometimes known as horizontal wells, although the pipe should not be completely horizontal; it should slope downwards away from the spring in order to reduce the risk of contamination. \n\n"}
{"id": "19219297", "url": "https://en.wikipedia.org/wiki?curid=19219297", "title": "Stationary fuel-cell applications", "text": "Stationary fuel-cell applications\n\nStationary fuel-cell applications (or stationary fuel-cell power systems) are applications for fuel cells that are either connected to the electric grid (distributed generation) to provide supplemental power and as emergency power system for critical areas, or installed as a grid-independent generator for on-site service.\n\nIn 2012 more than fuel-cell systems were shipped all over the world — in residential homes, hospitals, nursing homes, hotels, office buildings, schools, utility power plants. \n\nMicro combined heat and power, \"mCHP\" or \"micro cogeneration\" is a so-called distributed energy resource (DER). The installation is usually less than 5 kW in a house or small business. Instead of burning fuel to merely heat space or water, some of the energy is converted to electricity in addition to heat. This electricity can be used within the home or business or, if permitted by the grid management, sold back into the electric power grid. \n\nDelta-ee consultants stated in 2013 that with 64% of global sales the fuel cell micro-combined heat and power passed the conventional systems in sales in 2012. In 2012, over units were sold in Japan as part of the Ene Farm project. With a Lifetime of around 60,000 hours. For PEM fuel cell units, which shut down at night, this equates to an estimated lifetime of between ten and fifteen years. For a price of $22,600 before installation. For 2013 a state subsidy for units is in place.\n\nEmergency power systems are a type of fuel cell system, which may include lighting, generators and other apparatus, to provide backup resources in a crisis or when regular systems fail. They find uses in a wide variety of settings from residential homes to hospitals, scientific laboratories, data centers, telecommunication equipment and modern naval ships.\n\nAn uninterruptible power supply (UPS) provides emergency power and, depending on the topology, provide line regulation as well to connected equipment by supplying power from a separate source when utility power is not available. It differs from an auxiliary power supply or standby generator, which does not provide instant protection from a momentary power interruption.\n\n\"Stationary fuel cell applications\" is a classification in FC hydrogen codes and standards and fuel cell codes and standards. The other main standards are Portable fuel cell applications and Fuel cell vehicle.\n\n\n"}
{"id": "36070324", "url": "https://en.wikipedia.org/wiki?curid=36070324", "title": "Stephan Ouaknine", "text": "Stephan Ouaknine\n\nStephan Ouaknine is a Canadian business magnate, best known for his activities in the telecommunications industry and in renewable energy.\nA native of Montreal, Ouaknine has been active at international events including the Clinton Global Initiative, the Durban World Climate Summit and the Rio+20 World Green Summit, and has been named one of Canada's most notable entrepreneurs by Profit magazine.\n\nOuaknine, the son of Moroccan and Egyptian immigrants to Canada, attended McGill University and moved to the Israeli start-up industry, first joining a dot-com multimedia company then known as Geo-Interactive. As vice-president of business development, Ouaknine led the firm’s 1996 IPO on the London Stock Exchange under the name Emblaze.\n\nIn 1998, Ouaknine founded Airslide Systems, an advanced telecommunications equipment company, from his home in Tel Aviv. He raised over $36 million in venture capital for this firm from investors including George Soros, Sequoia Capital, Intel and SingTel. The firm's assets were ultimately acquired by Dialogic Corporation.\n\nAfter exiting Airslide, Ouaknine returned to Montreal to found Blueslice Networks in 2002. Blueslice produced evolved Subscriber Data Management (eSDM) solutions for mobile operators. The firm was acquired in 2010 by Morrisville-based Tekelec.\n\nSubsequently, to Blueslice's acquisition, Ouaknine moved to the renewable energy industry. With partners including Jigar Shah, founder of SunEdison, Eric Ouaknine and Vincent Martel, Ouaknine founded Inerjys, a renewable energy and clean technology growth equity fund. \nAt Inerjys, Ouaknine’s activities have focused on a hybrid strategy of growth equity investment and infrastructure project finance, with the objective of accelerating the commercialization of innovative cleantech firms. Ouaknine has promoted this investment thesis at major international events including the Durban World Climate Summit, the Clinton Global Initiative, and the World Green Summit, a business conference affiliated with Rio+20.\nIn 2017, Oaknine was a guide in the walk of the living for teenagers.\n\n"}
{"id": "26129552", "url": "https://en.wikipedia.org/wiki?curid=26129552", "title": "Strasskirchen Solar Park", "text": "Strasskirchen Solar Park\n\nThe Strasskirchen Solar Park is a large photovoltaic power station in Bavaria, with an installed capacity of 54 MW. It was developed by a joint venture of MEMC and Q-Cells in 2009. At this time this Solar Park was the second largest PV Power Plant. Q-Cells also provided the solar modules for the facility which is located in Straßkirchen, Bavaria, Germany.\n"}
{"id": "292444", "url": "https://en.wikipedia.org/wiki?curid=292444", "title": "Sulfide", "text": "Sulfide\n\nSulfide (systematically named sulfanediide, and sulfide(2−)) (British English sulphide) is an inorganic anion of sulfur with the chemical formula S or a compound containing one or more S ions. It contributes no color to sulfide salts. As it is classified as a strong base, even dilute solutions of salts such as sodium sulfide (NaS) are corrosive and can attack the skin. Sulfide is the simplest sulfur anion.\n\nThe systematic names \"sulfanediide\" and \"sulfide(2−)\", valid IUPAC names, are determined according to the substitutive and additive nomenclatures, respectively. However, the name sulfide is also used in compositional IUPAC nomenclature which does not take the nature of bonding involved. Examples of such naming are selenium disulfide and titanium sulfide, which contains no sulfide ions whatsoever.\n\n\"Sulfide\" is also used non-systematically, to describe compounds which release hydrogen sulfide upon acidification, or a compound that otherwise incorporates sulfur in some form, such as dimethyl sulfide. \"Hydrogen sulfide\" is itself an example of a non-systematic name of this nature. However, it is also a trivial name, and the preferred IUPAC name for sulfane.\n\nIt has been confirmed that the sulfide ion, S, does not exist even in hyper-concentrated aqueous alkaline solutions of NaS. Thus, the dissociation reaction\ndoes not occur in aqueous solution at any concentration of sulphide. The sulphide ion, S, was previously reported to be undetectable at concentrations up to 5 M NaOH. However, the sulfide ion may be produced when a solid is formed. For example cadmium sulphide precipitates in group 2 of qualitative analysis. \n\nUpon treatment with a standard acid, sulfide converts to hydrogen sulfide (HS) and a metal salt. Oxidation of sulfide gives sulfur or sulfate. Metal sulfides react with nonmetals including iodine, bromine, and chlorine forming sulfur and metal salts.\nSulfur can also be prepared from a sulfide and an appropriate oxidizer:\n\nAqueous solutions of transition metals cations react with sulfide sources (HS, NaHS, NaS) to precipitate solid sulfides. Such inorganic sulfides typically have very low solubility in water, and many are related to minerals with the same composition (see below). One famous example is the bright yellow species CdS or \"cadmium yellow\". The black tarnish formed on sterling silver is AgS. Such species are sometimes referred to as salts. In fact, the bonding in transition metal sulfides is highly covalent, which gives rise to their semiconductor properties, which in turn is related to the deep colors. Several have practical applications as pigments, in solar cells, and as catalysts. The fungus \"Aspergillus niger\" plays a role in the solubilization of heavy metal sulfides.\n\nMany important metal ores are sulfides. Significant examples include: argentite (silver sulfide), cinnabar (mercury), galena (lead sulfide), molybdenite (molybdenum sulfide), pentlandite (nickel sulfide), realgar (arsenic sulfide), and stibnite (antimony), sphalerite (zinc sulfide), and pyrite (iron disulfide), and chalcopyrite (iron-copper sulfide).\n\nDissolved free sulfides (HS, HS and S) are very aggressive species for the corrosion of many metals such as steel, stainless steel, and copper. Sulfides present in aqueous solution are responsible for stress corrosion cracking (SCC) of steel, and is also known as sulfide stress cracking. Corrosion is a major concern in many industrial installations processing sulfides: sulfide ore mills, deep oil wells, pipelines transporting soured oil, Kraft paper factories.\n\nMicrobially-induced corrosion (MIC) or biogenic sulfide corrosion are also caused by sulfate reducing bacteria producing sulfide that is emitted in the air and oxidized in sulfuric acid by sulfur oxidizing bacteria. Biogenic sulfuric acid reacts with sewerage materials and most generally causes mass loss, cracking of the sewer pipes and ultimately, structural collapse. This kind of deterioration is a major process affecting sewer systems worldwide and leading to very high rehabilitation costs.\n\nOxidation of sulfide can also form thiosulfate () an intermediate species responsible for severe problems of pitting corrosion of steel and stainless steel while the medium is also acidified by the production of sulfuric acid when oxidation is more advanced.\n\nIn organic chemistry, \"sulfide\" usually refers to the linkage C–S–C, although the term thioether is less ambiguous. For example, the thioether dimethyl sulfide is CH–S–CH. Polyphenylene sulfide (see below) has the empirical formula CHS. Occasionally, the term sulfide refers to molecules containing the –SH functional group. For example, methyl sulfide can mean CH–SH. The preferred descriptor for such SH-containing compounds is thiol or mercaptan, i.e. methanethiol, or methyl mercaptan.\n\nConfusion arises from the different meanings of the term \"disulfide\". Molybdenum disulfide (MoS) consists of separated sulfide centers, in association with molybdenum in the formal +4 oxidation state (that is, Mo and two S). Iron disulfide (pyrite, FeS) on the other hand consists of , or S–S dianion, in association with divalent iron in the formal +2 oxidation state (ferrous ion: Fe). Dimethyldisulfide has the chemical binding CH–S–S–CH, whereas carbon disulfide has no S–S bond, being S=C=S (linear molecule analog to CO). Most often in sulfur chemistry and in biochemistry, the disulfide term is commonly ascribed to the sulfur analogue of the peroxide –O–O– bond. The disulfide bond (–S–S–) plays a major role in the conformation of proteins and in the catalytic activity of enzymes.\n\nSulfide compounds can be prepared in several different ways:\n\n\nMany metal sulfides are so insoluble in water that they are probably not very toxic. Some metal sulfides, when exposed to a strong mineral acid, including gastric acids, will release toxic hydrogen sulfide.\n\nOrganic sulfides are highly flammable. When a sulfide burns it produces sulfur dioxide (SO) gas.\n\nHydrogen sulfide, some of its salts, and almost all organic sulfides have a strong and putrid stench; rotting biomass releases these.\n\n"}
{"id": "48210455", "url": "https://en.wikipedia.org/wiki?curid=48210455", "title": "The Center for Art in Wood", "text": "The Center for Art in Wood\n\nThe Center for Art in Wood is an American educational wood art institution located in Philadelphia, Pennsylvania. It was officially established as a nonprofit in 1986 by brothers Albert and Alan LeCoff, following a series of international symposium from 1976-1986 presented by the LeCoff's with woodturner Palmer Sharpless. The organization operated as the Wood Turning Center until 2011, when the nonprofit moved to its current location in Old City District of Philadelphia and changed its name to The Center for Art in Wood.\n\nToday, the Center presents changing exhibitions of contemporary art work in the medium of wood, a permanent collection of over 1,000 pieces and a host of educational programs and workshops. The Center also houses a research library, artists files, and a museum store. A series of publications documents the Center's work and highlights premier artists working in the field of wood turning and art in wood.\n\n"}
{"id": "2937772", "url": "https://en.wikipedia.org/wiki?curid=2937772", "title": "Thermophotovoltaic", "text": "Thermophotovoltaic\n\nThermophotovoltaic (TPV) energy conversion is a direct conversion process from heat to electricity via photons. A basic thermophotovoltaic system consists of a thermal emitter and a photovoltaic diode cell.\n\nThe temperature of the thermal emitter varies between different systems from about 900 °C to about 1300 °C, although in principle TPV devices can extract energy from any emitter with temperature elevated above that of the photovoltaic device (forming an optical heat engine). The emitter can be a piece of solid material or a specially engineered structure.Thermal emission is the spontaneous emission of photons due to thermal motion of charges in the material. For these TPV temperatures, this radiation is mostly at near infrared and infrared frequencies. The photovoltaic diodes absorbs some of these radiated photons and converts them into electricity.\n\nThermophotovoltaic systems have few to no moving parts and are therefore quiet and require little maintenance. These properties make thermophotovoltaic systems suitable for remote-site and portable electricity-generating applications. Their efficiency-cost properties, however, are often poor compared to other electricity-generating technologies. Current research in the area aims at increasing system efficiencies while keeping the system cost low.\n\nTPV systems usually attempt to match the optical properties of thermal emission (wavelength, polarization, direction) with the most efficient absorption characteristics of the photovoltaic cell, since unconverted thermal emission is a major source of inefficiency. Most groups focus on gallium antimonide (GaSb) cells. Germanium (Ge) is also suitable. Much research and development concerns methods for controlling the emitter's properties.\n\nTPV cells have been proposed as auxiliary power conversion devices for capture of otherwise lost heat in other power generation systems, such as steam turbine systems or solar cells.\n\nA prototype TPV hybrid car was built, the \"Viking 29\" (TPV) powered automobile, designed and built by the Vehicle Research Institute (VRI) at Western Washington University.\n\nTPV research is an active area. Among others, the University of Houston TPV Radioisotope Power Conversion Technology development effort is attempting to combine a thermophotovoltaic cell with thermocouples to provide a 3 to 4-fold improvement in system efficiency over current radioisotope thermoelectric generators.\n\nHenry Kolm had constructed an elementary TPV system at MIT in 1956. However, Pierre Aigrain is widely cited as the inventor based on the content of lectures he gave at MIT between 1960–1961 which, unlike Kolm's system, led to research and development.\n\nThermophotovoltaics (TPVs) are a class of power generating system that convert thermal energy into electrical energy. They consist of, at a minimum, an emitter and a photovoltaic power converter. Most TPV systems include additional components such as concentrators, filters and reflectors.\n\nThe basic principle is similar to that of traditional photovoltaics (PV) where a p-n junction is used to absorb optical energy, generate and separate electron/hole pairs, and in doing so convert that energy into electricity. The difference is that the optical energy is not directly generated by the Sun, but instead by a material at high temperature (termed the emitter), that causes it to emit light. In this way thermal energy is converted to electrical energy.\n\nThe emitter can be heated by sunlight or other techniques. In this sense, TPVs provide a great deal of versatility in potential fuels. In the case of solar TPVs, large concentrators are needed to provide reasonable temperatures for efficient operation.\n\nImprovements can take advantage of filters or selective emitters to create emissions in a wavelength range that is optimized for a specific photovoltaic (PV) converter. In this way TPVs can overcome a fundamental challenge for traditional PVs, making efficient use of the entire solar spectrum. For black body emitters, photons with energy less than the bandgap of the converter cannot be absorbed and are either reflected and lost or pass through the cell. Photons with energy above the bandgap can be absorbed, but the excess energy, formula_1, is again lost, generating undesirable heating in the cell. In the case of TPVs, similar issues can exist, but the use of either selective emitters (emissivity over a specific wavelength range), or optical filters that only pass a narrow range of wavelengths and reflect all others, can be used to generate emission spectra that can be optimally converted by the PV device.\n\nTo maximize efficiency, all photons should be converted. A process often termed photon recycling can be used to approach this. Reflectors are placed behind the converter and anywhere else in the system that photons might not be efficiently directed to the collector. These photons are directed back to the concentrator where they can be converted, or back to the emitter, where they can be reabsorbed to generate heat and additional photons. An optimal TPV system would use photon recycling and selective emission to convert all photons into electricity.\n\nThe upper limit for efficiency in TPVs (and all systems that convert heat energy to work) is the Carnot efficiency, that of an ideal heat engine. This efficiency is given by:\n\nwhere T is the temperature of the PV converter. For the best reasonable values in a practical system, T~300K and T~1800, giving a maximum efficiency of ~83%. This limit sets the upper limit for the system efficiency. At 83% efficiency, all heat energy is converted to radiation by the emitter which is then converted by the PV into electrical energy without losses, such as thermalization or Joule heating. Maximum efficiency presumes no entropy change, which is only possible if the emitter and cell are at the same temperature. More accurate models are quite complicated.\n\nDeviations from perfect absorption and perfect black body behavior lead to light losses. For selective emitters, any light emitted at wavelengths not matched to the bandgap energy of the photovoltaic may not be efficiently converted (for reasons discussed above) and leads to reduced efficiency. In particular, emissions associated with phonon resonances are difficult to avoid for wavelengths in the deep infrared, which cannot be practically converted. Ideal emitters produce no infrared.\n\nFor black body emitters or imperfect selective emitters, filters reflect non-ideal wavelengths back to the emitter. These filters are imperfect. Any light that is absorbed or scattered and not redirected to the emitter or the converter is lost, generally as heat. Conversely, practical filters often reflect a small percentage of light in desired wavelength ranges. Both are inefficiencies.\n\nEven for systems where only light of optimal wavelengths is passed to the converter, inefficiencies associated with non-radiative recombination and ohmic losses exist. Since these losses can depend on the light intensity incident on the cell, real systems must consider the intensity produced by a given set of conditions (emitter material, filter, operating temperature).\n\nIn an ideal system, the emitter would be surrounded by converters so no light is lost. However, realistically, geometries must accommodate the input energy (fuel injection or input light) used to heat the emitter. Additionally, costs prohibit the placement of converters everywhere. When the emitter reemits light, anything that does not travel to the converters is lost. Mirrors can be used to redirect some of this light back to the emitter; however, the mirrors may have their own losses.\n\nFor black body emitters where photon recirculation is achieved via filters, Planck's law states that a black body emits light with a spectrum given by:\n\nformula_3\n\nwhere I' is the flux of light of a specific wavelength, λ, given in units of 1/m/s. h is Planck's constant, k is Boltzmann's constant, c is the speed of light, and T is the emitter temperature. Thus, the light flux with wavelengths in a specific range can be found by integrating over the range. The peak wavelength is determined by the temperature, T based on Wien's displacement law:\n\nwhere b is Wien's displacement constant. For most materials, the maximum temperature an emitter can stably operate at is about 1800 °C. This corresponds to an intensity that peaks at λ~1600 nm or an energy of ~0.75 eV. For more reasonable operating temperatures of 1200 °C, this drops to ~0.5 eV. These energies dictate the range of bandgaps that are needed for practical TPV converters (though the peak spectral power is slightly higher). Traditional PV materials such as Si (1.1 eV) and GaAs (1.4 eV) are substantially less practical for TPV systems, as the intensity of the black body spectrum is extremely low at these energies for emitters at realistic temperatures.\n\nEfficiency, temperature resistance and cost are the three major factors for choosing a TPV radiator. Efficiency is determined by energy absorbed relative to total incoming radiation. High temperature operation is a crucial factor because efficiency increases with operating temperature. As emitter temperature increases, black body radiation shifts to shorter wavelengths, allowing for more efficient absorption by photovoltaic cells. Cost is another major commercialization issue.\n\nPolycrystalline silicon carbide (SiC) is the most commonly used emitter for burner TPVs. SiC is thermally stable to ~1700 °C. However, SiC radiates much of its energy in the long wavelength regime, far lower in energy than even the narrowest bandgap photovoltaic. This radiation is not converted into electrical energy. However, non-absorbing selective filters in front of the PV, or mirrors deposited on the back side of the PV can be used to reflect the long wavelengths back to the emitter, thereby recycling the unconverted energy. In addition, polycrystalline SiC is cheap to manufacture.\n\nRefractory metals can be used as selective emitters for burner TPVs. Tungsten is the most common choice. It has higher emissivity in the visible and near-IR range of 0.45 to 0.47 and a low emissivity of 0.1 to 0.2 in the IR region. The emitter is usually in the shape of a cylinder with a sealed bottom, which can be considered a cavity. The emitter is attached to the back of a thermal absorber such as SiC and maintains the same temperature. Emission occurs in the visible and near IR range, which can be readily converted by the PV to electrical energy.\n\nRare-earth oxides such as ytterbium oxide (YbO) and erbium oxide (ErO) are the most commonly used selective emitters for TPVs. These oxides emit a narrow band of wavelengths in the near-infrared region, allowing the tailoring of the emission spectra to better fit the absorbance characteristics of a particular PV cell. The peak of the emission spectrum occurs at 1.29 eV for YbO and 0.827 eV for ErO. As a result, YbO can be used a selective emitter for Si PV cells and ErO, for GaSb or InGaAs. However, the slight mismatch between the emission peaks and band gap of the absorber results in a significant loss of efficiency. Selective emission only becomes significant at 1100 °C and increases with temperature, per Planck's Law. At operating temperatures below 1700 °C, selective emission of rare-earth oxides is fairly low, resulting in a further decrease in efficiency. Currently, 13% efficiency has been achieved with YbO and silicon PV cells. In general selective emitters have had limited success. More often filters are used with black body emitters to pass wavelengths matched to the bandgap of the PV and reflect mismatched wavelengths back to the emitter.\n\nPhotonic crystals are a class of periodic materials that allow the precise control of electromagnetic wave properties. These materials give rise to the photonic bandgap (PBG). In the spectral range of the PBG, electromagnetic waves cannot propagate. The engineering of these materials allows some ability to tailor their emission and absorption properties, allowing for more effective design of selective emitters. Selective emitters with peaks at higher energy than the black body peak (for practical TPV temperatures) allow for wider bandgap converters. These converters are traditionally cheaper to manufacture and less temperature sensitive. Researchers at Sandia Labs demonstrated a high-efficiency (34% of light emitted from PBG selective emitter can be converted to electricity) TPV emitter using tungsten photonic crystals. However, manufacturing of these devices is difficult and not commercially feasible.\n\nEarly work in TPVs focused on the use of Si PVs. Silicon's commercial availability, extremely low cost, scalability and ease of manufacture makes this material an appealing candidate. However, the relatively wide bandgap of Si (1.1eV) is not ideal for use with a black body emitter at lower operating temperatures. Calculations using Planck's law, which describes the black body spectrum as a function of temperature, indicates that Si PVs would only be feasible at temperatures much higher than 2000 K. No emitter has been demonstrated that can operate at these temperatures. These engineering difficulties led to the pursuit of lower-bandgap semiconductor PVs.\n\nUsing selective radiators with Si PVs is still a possibility. Selective radiators would eliminate high and low energy photons, reducing heat generated. Ideally, selective radiators would emit no radiation beyond the band edge of the PV converter, increasing conversion efficiency significantly. No efficient TPVs have been realized using Si PVs.\n\nEarly investigations into low bandgap semiconductors focused on germanium (Ge). Ge has a bandgap of 0.66 eV, allowing for conversion of a much higher fraction of incoming radiation. However, poor performance was observed due the extremely high effective electron mass of Ge. Compared to III-V semiconductors, Ge's high electron effective mass leads to a high density of states in the conduction band and therefore a high intrinsic carrier concentration. As a result, Ge diodes have fast decaying \"dark\" current and therefore, a low open-circuit voltage. In addition, surface passivation of germanium has proven extremely difficult.\n\nThe gallium antimonide (GaSb) PV cell, invented in 1989, is the basis of most PV cells in modern TPV systems. GaSb is a III-V semiconductor with the zinc blende crystal structure. The GaSb cell is a key development owing to its narrow bandgap of 0.72 eV. This allows GaSb to respond to light at longer wavelengths than silicon solar cell, enabling higher power densities in conjunction with manmade emission sources. A solar cell with 35% efficiency was demonstrated using a bilayer PV with GaAs and GaSb, setting the solar cell efficiency record.\n\nManufacturing a GaSb PV cell is quite simple. Czochralski Te-doped n-type GaSb wafers are commercially available. Vapor-based Zn diffusion is carried out at elevated temperatures ~450 °C to allow for p-type doping. Front and back electrical contacts are patterned using traditional photolithography techniques and an anti-reflective coating is deposited. Current efficiencies are estimated at ~20% using a 1000 °C black body spectrum. The radiative limit for efficiency of the GaSb cell in this setup is 52%, so vast improvements can still be made.\n\nIndium gallium arsenide antimonide (InGaAsSb) is a compound III-V semiconductor. (InGaAsSb) The addition of GaAs allows for a narrower bandgap (0.5 to 0.6 eV), and therefore better absorption of long wavelengths. Specifically, the bandgap was engineered to 0.55 eV. With this bandgap, the compound achieved a photon-weighted internal quantum efficiency of 79% with a fill factor of 65% for a black body at 1100 °C. This was for a device grown on a GaSb substrate by organometallic vapour phase epitaxy (OMVPE). Devices have been grown by molecular beam epitaxy (MBE) and liquid phase epitaxy (LPE). The internal quantum efficiencies (IQE) of these devices are approaching 90%, while devices grown by the other two techniques exceed 95%. The largest problem with InGaAsSb cells is phase separation. Compositional inconsistencies throughout the device degrade its performance. When phase separation can be avoided, the IQE and fill factor of InGaAsSb approach theoretical limits in wavelength ranges near the bandgap energy. However, the V/E ratio is far from the ideal. Current methods to manufacture InGaAsSb PVs are expensive and not commercially viable.\n\nIndium gallium arsenide (InGaAs) is a compound III-V semiconductor. It can be applied in two ways for use in TPVs. When lattice-matched to an InP substrate, InGaAs has a bandgap of 0.74 eV, no better than GaSb. Devices of this configuration have been produced with a fill factor of 69% and an efficiency of 15%. However, to absorb higher wavelength photons, the bandgap may be engineered by changing the ratio of In to Ga. The range of bandgaps for this system is from about 0.4 to 1.4 eV. However, these different structures cause strain with the InP substrate. This can be controlled with graded layers of InGaAs with different compositions. This was done to develop of device with a quantum efficiency of 68% and a fill factor of 68%, grown by MBE. This device had a bandgap of 0.55 eV, achieved in the compound InGaAs. n has the advantage of being a well-developed material. InGaAs can be made to lattice match perfectly with Ge resulting in low defect densities. Ge as a substrate is a significant advantage over more expensive or harder-to-produce substrates.\n\nThe InPAsSb quaternary alloy has been grown by both OMVPE and LPE. When lattice-matched to InAs, it has a bandgap in the range 0.3–0.55 eV. The benefits of a TPV system with such a low band gap have not been studied in depth. Therefore, cells incorporating InPAsSb have not been optimized and do not yet have competitive performance. The longest spectral response from an InPAsSb cell studied was 4.3 μm with a maximum response at 3 μm. While this is a promising material, it has yet to be developed. For this and other low-bandgap materials, high IQE for long wavelengths is hard to achieve due to an increase in Auger recombination.\n\nTPVs promise efficient and economically viable power systems for both military and commercial applications. Compared to traditional nonrenewable energy sources, burner TPVs have little NO emissions and are virtually silent. Solar TPVs are a source of emission-free renewable energy. TPVs can be more efficient than PV systems owing to recycling of unabsorbed photons. However, TPVs are more complex and losses at each energy conversion step can lower efficiency. Further developments must be made to the absorber/emitter and PV cell. When TPVs are used with a burner source, they provide on-demand energy. As a result, energy storage is not needed. In addition, owing to the PV's proximity to the radiative source, TPVs can generate current densities 300 times that of conventional PVs.\n\nBattlefield dymamics require portable power. Conventional diesel generators are too heavy for use in the field. Scalability allows TPVs to be smaller and lighter than conventional generators. Also, TPVs have few emissions and are silent. Multifuel operation is another potential benefit.\n\nEarly investigations into TPVs in the 1970s failed due to PV limitations. However, with the realization of the GaSb photocell, a renewed effort in the 1990s improved results. In early 2001, JX Crystals delivered a TPV based battery charger to the Army that produced an output of 230 W with propane. This prototype utilized an SiC emitter operating at 1250 °C and GaSb photocells and was approximately 0.5 m tall. The power source had an efficiency of 2.5%, calculated by the ratio of the power generated to the thermal energy of the fuel burned. This is too low for practical battlefield use. To increase efficiency, narrow-band emitters must be realized and the temperature of the burner must be raised. Further thermal management steps, such as water cooling or coolant boiling, must be implemented. Although many successful proof-of-concept prototypes were demonstrated, no portable TPV power sources have reached troop testing or battlefield implementation.\n\nFor space travel power generation systems must provide consistent and reliable power without large amounts of fuel. As a result, solar and radioisotope fuels (extremely high power density and long lifetime) are ideal sources of energy. TPVs have been proposed for each. In the case of solar energy, orbital spacecraft may be better locations for the large and potentially cumbersome concentrators required for practical TPVs. However, because of weight considerations and inefficiencies associated with the somewhat more complicated design of TPVs, conventional PVs will almost surely be more effective for these applications.\n\nProbably more interesting is the prospect of using TPVs for conversion of radioisotope energy. The output of isotopes is thermal energy. In the past thermoelectricity (direct thermal to electrical conversion with no moving parts) has been used because of TPV efficiency is less than the ~10% of thermoelectric converters. Stirling engines have also been considered, but face reliability concerns, which are unacceptable for space missions, despite improved conversion efficiencies (>20%). However, with the recent advances in small-bandgap PVs, TPVs are becoming more promising candidates. A TPV radioisotope converter with 20% efficiency was demonstrated that uses a tungsten emitter heated to 1350 K, with tandem filters and a 0.6 eV bandgap InGaAs PV converter (cooled to room temperature). About 30% of the lost energy was due to the optical cavity and filters. The remainder was due to the efficiency of the PV converter.\n\nLow-temperature operation of the converter is critical to the efficiency of TPV. Heating PV converters increases their dark current, thereby reducing efficiency. The converter is heated by the radiation from the emitter. In terrestrial systems it is reasonable to dissipate this heat without using additional energy with a heat sink. However, space is an isolated system, where heat sinks are impractical. Therefore, it is critical to develop innovative solutions to efficiently remove that heat, or optimized TPV cells that can operate efficiently with higher temperature converters. Both represent substantial challenges. Despite this, TPVs offer substantial promise for use in future space applications.\n\nMany homes are located in remote regions not connected to the power grid. Where available, power line extensions can be impractical. TPVs can provide a continuous supply of power in off-grid homes. Traditional PVs on the other hand, would not provide sufficient power during the winter months and nighttime, while TPVs can utilize alternative fuels to augment solar-only production.\n\nThe greatest advantage for TPV generators is cogeneration of heat and power. In cold climates, it can function as both a heater or stove and a power generator. JX Crystals developed a prototype TPV heating stove and generator. It burns natural gas and uses a SiC source emitter operating at 1250 °C and GaSb photocell to output 25,000 BTU/hr simultaneously generating 100 W. However, costs must be significantly reduced to render it commercially viable.\n\nWhen a furnace is used as a heater and a generator, it is called combined heat and power (CHP). Many TPV CHP scenarios have been theorized, but a generator using boiling coolant was found most cost efficient. The proposed CHP would utilize a SiC IR emitter operating at 1425 °C and GaSb photocells cooled by boiling coolant. The TPV CHP would output 85,000 BTU/hr and generate 1.5 kW. The estimated efficiency would be 12.3% and the investment would be 0.08 €/kWh provided that the lifetime of the CHP furnace is 20 years. The estimated cost of other non-TPV CHPs are 0.12 €/kWh for gas engine CHP and 0.16 €/kWh for fuel cell CHP. This proposed furnace has not been commercialized because the market was not thought to be large enough.\n\nTPVs have been proposed for use in recreational vehicles. With the advent of hybrid and other electrically powered vehicles, power generators with electrical outputs have become more interesting. In particular the versatility of TPVs for fuel choice and the ability to use multiple fuel sources makes them interesting as a wider variety of fuels are being with better sustainability are emerging. The silent operation of TPVs allow the generation of electricity when and where the use of noisy conventional generators is not allowed (i.e. during \"quiet hours\" in national park campgrounds), and do not disturb others. However, the emitter temperatures required for practical efficiencies make TPVs on this scale unlikely.\n\n"}
{"id": "610527", "url": "https://en.wikipedia.org/wiki?curid=610527", "title": "Whiteout (weather)", "text": "Whiteout (weather)\n\nWhiteout is a weather condition in which visibility and contrast are severely reduced by snow or sand. The horizon disappears from view while the sky and landscape appear featureless, leaving no points of visual reference by which to navigate. Whiteout has been defined as: \"A condition of diffuse light when no shadows are cast, due to a continuous white cloud layer appearing to merge with the white snow surface. No surface irregularities of the snow are visible, but a dark object may be clearly seen. There is no visible horizon.\"\n\nA whiteout may be due simply to extremely heavy snowfall rates as seen in lake effect conditions, or to other factors such as diffuse lighting from overcast clouds, mist or fog, or a background of snow. A person traveling in a true whiteout is at significant risk of becoming completely disoriented and losing their way, even in familiar surroundings. Motorists typically have to stop their cars where they are, as the road is impossible to see. Normal snowfalls and blizzards, where snow is falling at /h), or where the relief visibility is not clear yet having a clear field of view for over , are often incorrectly called whiteouts.\n\nThere are three different forms of a whiteout:\n\nA whiteout should not be confused with flat-light. Whilst there are similarities, both the causes and effects are different.\n\nA whiteout is a reduction and scattering of sunlight.\n\nFlat-light is a diffusion of sunlight.\n\nWhiteout conditions pose threats to mountain climbers, skiers, aviation, and mobile ground traffic. Motorists, especially those on large high-speed routes, are also at risk. There have been many major multiple-vehicle collisions associated with whiteout conditions. One motorist may come to a complete stop when he or she cannot see the road, while the motorist behind is still moving.\n\nLocal, short-duration whiteout conditions can be created artificially in the vicinity of airports and helipads due to aircraft operations. Snow on the ground can be stirred up by helicopter rotor down-wash or airplane jet blast, presenting hazards to both aircraft and bystanders on the ground.\n\n"}
{"id": "3892517", "url": "https://en.wikipedia.org/wiki?curid=3892517", "title": "Zbus", "text": "Zbus\n\nZ Matrix or \"bus impedance matrix\" is an important tool in power system analysis. Though, it is not frequently used in\n\nElectric power transmission needs optimization. Only Computer simulation allows the complex handling required. The \"Zbus\" matrix is a big tool in that box.\n\nZ Matrix can be formed by either inverting the Ybus matrix or by using Z bus building algorithm. The latter method is harder to implement but more practical and faster (\"in terms of computer run time and number of floating-point operations per second\") for a relatively large system.\n\nFormulation:\n\nformula_1\n\nBecause the Zbus is the inverse of the Ybus, it is symmetrical like the Ybus. The diagonal elements of the Zbus are referred to as driving-point impedances of the buses and the off-diagonal elements are called transfer impedances. \n\nOne reason the Ybus is so much more popular in calculation is the matrix becomes sparse for large systems; that is, many elements go to zero as the admittance between two far away buses is very small. In the Zbus, however, the impedance between two far away buses becomes very large, so there are no zero elements, making computation much harder.\n\nThe operations to modify an existing Zbus are straightforward, and outlined in Table 1.\n\nTo create a Zbus matrix from scratch, we\nstart by listing the equation for one branch:\n\nformula_2\n\nThen we add additional branches according\nto Table 1 until each bus is expressed in the matrix:\n\nformula_3\n"}
