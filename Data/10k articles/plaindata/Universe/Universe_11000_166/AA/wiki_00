{"id": "19245397", "url": "https://en.wikipedia.org/wiki?curid=19245397", "title": "11-Eicosenoic acid", "text": "11-Eicosenoic acid\n\n11-Eicosenoic acid, also called gondoic acid, is a monounsaturated omega-9 fatty acid found in a variety of plant oils and nuts; in particular jojoba oil. It is one of a number of eicosenoic acids.\n"}
{"id": "1440776", "url": "https://en.wikipedia.org/wiki?curid=1440776", "title": "Adiabatic invariant", "text": "Adiabatic invariant\n\nA property of a physical system, such as the entropy of a gas, that stays approximately constant when changes occur slowly is called an adiabatic invariant. By this it is meant that if a system is varied between two end points, as the time for the variation between the end points is increased to infinity, the variation of an adiabatic invariant between the two end points goes to zero.\n\nIn thermodynamics, an adiabatic process is a change that occurs without heat flow; it may be slow or fast. A reversible adiabatic process is an adiabatic process that occurs slowly compared to the time to reach equilibrium. In a reversible adiabatic process, the system is in equilibrium at all stages and the entropy is constant. In the 1st half of the 20th century the scientists that worked in quantum physics used the term \"adiabatic\" for reversible adiabatic processes and later for any gradually changing conditions which allow the system to adapt its configuration. The quantum mechanical definition is closer to the thermodynamical concept of a quasistatic process, and has no direct relation with adiabatic processes in thermodynamics.\n\nIn mechanics, an adiabatic change is a slow deformation of the Hamiltonian, where the fractional rate of change of the energy is much slower than the orbital frequency. The area enclosed by the different motions in phase space are the \"adiabatic invariants\".\n\nIn quantum mechanics, an adiabatic change is one that occurs at a rate much slower than the difference in frequency between energy eigenstates. In this case, the energy states of the system do not make transitions, so that the quantum number is an adiabatic invariant.\n\nThe old quantum theory was formulated by equating the quantum number of a system with its classical adiabatic invariant. This determined the form of the Bohr–Sommerfeld quantization rule: the quantum number is the area in phase space of the classical orbit.\n\nIn thermodynamics, adiabatic changes are those that do not increase the entropy. They occur slowly in comparison to the other characteristic timescales of the system of interest, and allow heat flow only between objects at the same temperature. For isolated systems, an adiabatic change allows no heat to flow in or out.\n\nIf a container with an ideal gas is expanded instantaneously, the temperature of the gas doesn't change at all, because none of the molecules slow down. The molecules keep their kinetic energy, but now the gas occupies a bigger volume. If the container expands slowly, however, so that the ideal gas pressure law holds at any time, gas molecules lose energy at the rate that they do work on the expanding wall. The amount of work they do is the pressure times the area of the wall times the outward displacement, which is the pressure times the change in the volume of the gas:\n\nIf no heat enters the gas, the energy in the gas molecules is decreasing by the same amount. By definition, a gas is ideal when its temperature is only a function of the internal energy per particle, not the volume. So\n\nWhere formula_3 is the specific heat at constant volume. When the change in energy is entirely due to work done on the wall, the change in temperature is given by:\n\nThis gives a differential relationship between the changes in temperature and volume, which can be integrated to find the invariant. The constant formula_5 is just a unit conversion factor, which can be set equal to one:\n\nSo \n\nis an adiabatic invariant, which is related to the entropy \n\nSo entropy is an adiabatic invariant. The \"N\" log(\"N\") term makes the entropy additive, so the entropy of two volumes of gas is the sum of the entropies of each one.\n\nIn a molecular interpretation, \"S\" is the logarithm of the phase space volume of all gas states with energy \"E\"(\"T\") and volume \"V\".\n\nFor a monatomic ideal gas, this can easily be seen by writing down the energy,\n\nThe different internal motions of the gas with total energy \"E\" define a sphere, the surface of a 3\"N\"-dimensional ball with radius formula_10. The volume of the sphere is\n\nwhere formula_12 is the Gamma function.\n\nSince each gas molecule can be anywhere within the volume \"V\", the volume in phase space occupied by the gas states with energy \"E\" is \n\nSince the \"N\" gas molecules are indistinguishable, the phase space volume is divided by formula_14, the number of permutations of \"N\" molecules.\n\nUsing Stirling's approximation for the gamma function, and ignoring factors that disappear in the logarithm after taking \"N\" large, \n\nSince the specific heat of a monatomic gas is 3/2, this is the same as the thermodynamic formula for the entropy.\n\nFor a box of radiation, ignoring quantum mechanics, the energy of a classical field in thermal equilibrium is infinite, since equipartition demands that each field mode has an equal energy on average and there are infinitely many modes. This is physically ridiculous, since it means that all energy leaks into high frequency electromagnetic waves over time.\n\nStill, without quantum mechanics, there are some things that can be said about the equilibrium distribution from thermodynamics alone, because there is still a notion of adiabatic invariance that relates boxes of different size.\n\nWhen a box is slowly expanded, the frequency of the light recoiling from the\nwall can be computed from the Doppler shift. If the wall is not moving,\nthe light recoils at the same frequency. If the wall is moving slowly, the recoil frequency is only equal in the frame where the wall is stationary. In the frame where the wall is moving away from the light, the light coming in is bluer than the light coming out by twice the Doppler shift factor \"v\"/\"c\".\n\nOn the other hand, the energy in the light is also decreased when the wall is moving away, because the light is doing work on the wall by radiation pressure. Because the light is reflected, the pressure is equal to twice the momentum carried by light, which is \"E\"/\"c\". The rate at which the pressure does work on the wall is found by multiplying by the velocity:\n\nThis means that the change in frequency of the light is equal to the work done on the wall by the radiation pressure. The light that is reflected is changed both in frequency and in energy by the same amount:\n\nSince moving the wall slowly should keep a thermal distribution fixed, the probability that the light has energy \"E\" at frequency \"f\" must only be a function of \"E\"/\"f\".\n\nThis function cannot be determined from thermodynamic reasoning alone, and Wien guessed at the form that was valid at high frequency. He supposed that the average energy in high frequency modes was suppressed by a Boltzmann-like factor. This is not the expected classical energy in the mode, which is formula_20 by equipartition, but a new and unjustified assumption that fit the high-frequency data.\n\nWhen the expectation value is added over all modes in a cavity, this is Wien's distribution, and it describes the thermodynamic distribution of energy in a classical gas of photons. Wien's Law implicitly assumes that light is statistically composed of packets that change energy and frequency in the same way. The entropy of a Wien gas scales as the volume to the power \"N\", where \"N\" is the number of packets. This led Einstein to suggest that light is composed of localizable particles with energy proportional to the frequency. Then the entropy of the Wien gas can be given a statistical interpretation as the number of possible positions that the photons can be in.\n\nSuppose that a Hamiltonian is slowly time varying, for example, a one-dimensional harmonic oscillator with a changing frequency.\n\nThe action \"J\" of a classical orbit is the area\nenclosed by the orbit in phase space.\n\nSince \"J\" is an integral over a full period, it is only a function of the energy. When\nthe Hamiltonian is constant in time and \"J\" is constant in time, the canonically conjugate variable formula_24 increases in time at a steady rate.\n\nSo the constant formula_26 can be used to change time derivatives along the orbit to partial derivatives with respect to formula_24 at constant \"J\". Differentiating the integral for \"J\" with respect to \"J\" gives an identity that fixes formula_28\n\nThe integrand is the Poisson bracket of \"x\" and \"p\". The Poisson bracket of two canonically conjugate quantities like \"x\" and \"p\" is equal to 1 in any canonical coordinate system. So\n\nformula_30\n\nand formula_26 is the inverse period. The variable formula_24 increases by an equal amount in each period for all values of \"J\" – it is an angle-variable.\n\n\nThe Hamiltonian is a function of \"J\" only, and in the simple case of the harmonic oscillator.\n\nWhen \"H\" has no time dependence, \"J\" is constant. When \"H\" is slowly time varying, the\nrate of change of \"J\" can be computed by re-expressing the integral for \"J\"\n\nThe time derivative of this quantity is\n\nReplacing time derivatives with theta derivatives, using formula_36 and setting formula_37 without loss of generality (formula_38 being a global multiplicative constant in the resulting time derivative of the action), yields \n\nSo as long as the coordinates \"J\", formula_24 do not change appreciably over one period, this expression can be integrated by parts to give zero. This means\nthat for slow variations, there is no lowest order change in the area enclosed by\nthe orbit. This is the adiabatic invariance theorem – the action variables are adiabatic invariants.\n\nFor a harmonic oscillator, the area in phase space of an orbit at energy \"E\" is the area\nof the ellipse of constant energy,\n\nThe \"x\"-radius of this ellipse is formula_42, while the \"p\"-radius of the ellipse is formula_10. Multiplying, the area is formula_44. So if a pendulum is slowly drawn in, so that the frequency changes, the energy changes by a proportional amount.\n\nAfter Planck identified that Wien's law can be extended to all frequencies, even very low ones, by interpolating with the classical equipartition law for radiation, physicists wanted to understand the quantum behavior of other systems.\n\nThe Planck radiation law quantized the motion of the field oscillators in units of energy proportional to the frequency:\n\nThe quantum can only depend on the energy/frequency by adiabatic invariance, and since the energy must be additive when putting boxes end to end, the levels must be equally spaced.\n\nEinstein, followed by Debye, extended the domain of quantum mechanics by considering the sound modes in a solid as quantized oscillators. This model explained why the specific heat of solids approached zero at low temperatures,\ninstead of staying fixed at formula_46 as predicted by classical equipartition.\n\nAt the Solvay conference, the question of quantizing other motions was raised, and Lorentz pointed out a problem, known as Rayleigh–Lorentz pendulum. If you consider a quantum pendulum whose string is shortened very slowly, the quantum number of the pendulum cannot change because at no point is there a high enough frequency to cause a transition between the states. But the frequency of the pendulum changes when the string is shorter, so the quantum states change energy.\n\nEinstein responded that for slow pulling, the frequency and energy of the pendulum both change but the ratio stays fixed. This is analogous to Wien's observation that under slow motion of the wall the energy to frequency ratio of reflected waves is constant. The conclusion was that the quantities to quantize must be adiabatic invariants.\n\nThis line of argument was extended by Sommerfeld into a general theory: the quantum number of an arbitrary mechanical system is given by the adiabatic action variable. Since the action variable in the harmonic oscillator is an integer, the general condition is:\n\nThis condition was the foundation of the old quantum theory, which was able to predict the qualitative behavior of atomic systems. The theory is inexact for small quantum numbers, since it mixes classical and quantum concepts. But it was a useful half-way step to the new quantum theory.\n\nIn plasma physics there are three adiabatic invariants of charged particle motion.\n\nThe magnetic moment of a gyrating particle,\n\nis a constant of the motion to all orders in an expansion in formula_49, where formula_38 is the rate of any changes experienced by the particle, e.g., due to collisions or due to temporal or spatial variations in the magnetic field. Consequently, the magnetic moment remains nearly constant even for changes at rates approaching the gyrofrequency. When μ is constant, the perpendicular particle energy is proportional to \"B\", so the particles can be heated by increasing \"B\", but this is a 'one shot' deal because the field cannot be increased indefinitely. It finds applications in magnetic mirrors and magnetic bottles.\n\nThere are some important situations in which the magnetic moment is \"not\" invariant:\n\nThe longitudinal invariant of a particle trapped in a magnetic mirror,\n\nwhere the integral is between the two turning points, is also an adiabatic invariant. This guarantees, for example, that a particle in the magnetosphere moving around the Earth always returns to the same line of force. The adiabatic condition is violated in transit-time magnetic pumping, where the length of a magnetic mirror is oscillated at the bounce frequency, resulting in net heating.\n\nThe total magnetic flux Φ enclosed by a drift surface is the third adiabatic invariant, associated with the periodic motion of mirror-trapped particles drifting around the axis of the system. Because this drift motion is relatively slow, Φ is often not conserved in practical applications.\n\n\n"}
{"id": "1231733", "url": "https://en.wikipedia.org/wiki?curid=1231733", "title": "Airglow", "text": "Airglow\n\nAirglow (also called nightglow) is a faint emission of light by a planetary atmosphere. In the case of Earth's atmosphere, this optical phenomenon causes the night sky to never be completely dark, even after the effects of starlight and diffused sunlight from the far side are removed.\n\nThe airglow phenomenon was first identified in 1868 by Swedish physicist Anders Ångström. Since then, it has been studied in the laboratory, and various chemical reactions have been observed to emit electromagnetic energy as part of the process. Scientists have identified some of those processes that would be present in Earth's atmosphere, and astronomers have verified that such emissions are present.\n\nAirglow is caused by various processes in the upper atmosphere of Earth, such as the recombination of atoms which were photoionized by the Sun during the day, luminescence caused by cosmic rays striking the upper atmosphere, and chemiluminescence caused mainly by oxygen and nitrogen reacting with hydroxyl ions at heights of a few hundred kilometres. It is not noticeable during the daytime due to the glare and scattering of sunlight.\n\nEven at the best ground-based observatories, airglow limits the photosensitivity of optical telescopes. Partly for this reason, space telescopes like Hubble can observe much fainter objects than current ground-based telescopes at visible wavelengths.\n\nAirglow at night may be bright enough for a ground observer to notice and appears generally bluish. Although airglow emission is fairly uniform across the atmosphere, it appears brightest at about 10° above the observer's horizon, since the lower one looks, the greater the depth of atmosphere one is looking through. Very low down, however, atmospheric extinction reduces the apparent brightness of the airglow.\n\nOne airglow mechanism is when an atom of nitrogen combines with an atom of oxygen to form a molecule of nitric oxide (NO). In the process, a photon is emitted. This photon may have any of several different wavelengths characteristic of nitric oxide molecules. The free atoms are available for this process, because molecules of nitrogen (N) and oxygen (O) are dissociated by solar energy in the upper reaches of the atmosphere and may encounter each other to form NO. Other species that can create air glow in the atmosphere are hydroxyl (OH), atomic oxygen (O), sodium (Na), and lithium (Li).\n\nThe sky brightness is typically measured in units of apparent magnitude per square arcsecond of sky.\n\nIn order to calculate the relative intensity of airglow, we need to convert apparent magnitudes into fluxes of photons; this clearly depends on the spectrum of the source, but we will ignore that initially. At visible wavelengths, we need the parameter S(V), the power per square centimetre of aperture and per micrometre of wavelength produced by a zeroth-magnitude star, to convert apparent magnitudes into fluxes — . If we take the example of a \"V\"=28 star observed through a normal \"V\" band filter ( bandpass, frequency ), the number of photons we receive per square centimeter of telescope aperture per second from the source is \"N\":\n\n(where \"h\" is Planck's constant; \"hν\" is the energy of a single photon of frequency \"ν\").\n\nAt \"V\" band, the emission from airglow is per square arc-second at a high-altitude observatory on a moonless night; in excellent seeing conditions, the image of a star will be about 0.7 arc-second across with an area of 0.4 square arc-second, and so the emission from airglow over the area of the image corresponds to about . This gives the number of photons from airglow, \"N\":\n\nThe signal-to-noise for an ideal ground-based observation with a telescope of area \"A\" (ignoring losses and detector noise), arising from Poisson statistics, is only:\n\nIf we assume a 10 m diameter ideal ground-based telescope and an unresolved star: every second, over a patch the size of the seeing-enlarged image of the star, 35 photons arrive from the star and 3500 from air-glow. So, over an hour, roughly arrive from the air-glow, and approximately arrive from the source; so the S/N ratio is about:\n\nWe can compare this with \"real\" answers from exposure time calculators. For an 8 m unit Very Large Telescope telescope, according to the FORS exposure time calculator you need 40 hours of observing time to reach \"V\" = 28, while the 2.4 m Hubble only takes 4 hours according to the ACS exposure time calculator. A hypothetical 8 m Hubble telescope would take about 30 minutes.\n\nIt should be clear from this calculation that reducing the view field size can make fainter objects more detectable against the airglow; unfortunately, adaptive optics techniques that reduce the diameter of the view field of an Earth-based telescope by an order of magnitude only as yet work in the infrared, where the sky is much brighter. A space telescope isn't restricted by the view field, since they are not affected by airglow.\n\nScientific experiments have been conducted to induce airglow by directing high-power radio emissions at the Earth's ionosphere. These radiowaves interact with the ionosphere to induce faint but visible optical light at specific wavelengths under certain conditions.\n\nSwissCube-1 is a Swiss satellite operated by Ecole Polytechnique Fédérale de Lausanne. The spacecraft is a single unit CubeSat, which was designed to conduct research into airglow within the Earth's atmosphere and to develop technology for future spacecraft. Though SwissCube-1 is rather small (10 x 10 x 10 cm) and weighs less than 1 kg, it carries a small telescope for obtaining images of the airglow. The first SwissCube-1 image came down on February 18, 2011 and was quite black with some thermal noise on it. The first airglow image came down on March 3, 2011. This image has been converted to the human optical range (green) from its near-infrared measurement. This image provides a measurement of the intensity of the airglow phenomenon in the near-infrared. The range measured is from 500 to 61400 photons, with a resolution of 500 photons.\n\nThe Venus Express spacecraft contains an infrared sensor which has detected near-IR emissions from the upper atmosphere of Venus. The emissions come from nitric oxide (NO) and from molecular oxygen.\nScientists had previously determined in laboratory testing that during NO production, ultraviolet emissions and near-IR emissions were produced. The UV radiation has been detected in the atmosphere, but until this mission, the atmosphere-produced near-IR emissions were only theoretical.\n\n\n"}
{"id": "2825598", "url": "https://en.wikipedia.org/wiki?curid=2825598", "title": "Amorphous ice", "text": "Amorphous ice\n\nAmorphous ice (non-crystalline (\"vitreous\") ice) is an amorphous solid form of water. Common ice is a crystalline material where the molecules are regularly arranged in a hexagonal lattice whereas amorphous ice is distinguished by a lack of long-range order in its molecular arrangement. Amorphous ice is produced either by rapid cooling of liquid water (so the molecules do not have enough time to form a crystal lattice) or by compressing ordinary ice at low temperatures. \n\nAlthough almost all water ice on Earth is the familiar crystalline ice I, amorphous ice dominates in the depths of interstellar medium, making this likely the most common structure for HO in the universe at large.\n\nJust as there are many different crystalline forms of ice (currently 17+ known), there are also different forms of amorphous ice, distinguished principally by their densities.\n\nThe production of amorphous ice hinges on the fast rate of cooling. Liquid water must be cooled to its glass transition temperature (about 136 K or −137 °C) in milliseconds to prevent the spontaneous nucleation of crystals. This is analogous to the production of ice cream from heterogeneous ingredients, which must also be frozen quickly to prevent the growth of crystals in the mixture.\n\nPressure is another important factor in the formation of amorphous ice, and changes in pressure may cause one form to convert into another.\n\nChemicals known as cryoprotectants can be added to water, to lower its freezing point (like an antifreeze) and increase viscosity, which inhibits formation of crystals. Vitrification without addition of cryoprotectants can be achieved by very rapid cooling. These techniques are used in biology for cryopreservation of cells and tissues.\n\nLow-density amorphous ice, also called LDA, vapor-deposited amorphous water ice, amorphous solid water (ASW) or hyperquenched glassy water (HGW), is usually formed in the laboratory by a slow accumulation of water vapor molecules (physical vapor deposition) onto a very smooth metal crystal surface under 120 K. In outer space it is expected to be formed in a similar manner on a variety of cold substrates, such as dust particles.\n\nMelting past its glass transition temperature (T) between 120 and 140 K, LDA is more viscous than normal water. Recent studies have shown the viscous liquid stays in this alternative form of liquid water up to somewhere between 140 and 210 K, a temperature range that is also inhabited by ice I. LDA has a density of 0.94 g/cm, less dense than the densest water (1.00 g/cm at 277 K), but denser than ordinary ice (ice I).\n\nHyperquenched glassy water (HGW) is formed by spraying a fine mist of water droplets into a liquid such as propane around 80 K or by hyperquenching fine micrometer-sized droplets on a sample-holder kept at liquid nitrogen temperature, 77 K, in a vacuum. Cooling rates above 10 K/s are required to prevent crystallization of the droplets. At liquid nitrogen temperature, 77 K, HGW is kinetically stable and can be stored for many years.\n\nHigh-density amorphous ice (HDA) can be formed by compressing ice I at temperatures below ~140 K. At 77 K, HDA forms from ordinary natural ice at around 1.6 GPa and from LDA at around 0.5 GPa (approximately 5,000 atm). At this temperature, it can be recovered back to ambient pressure and kept indefinitely. At these conditions (ambient pressure and 77 K), HDA has a density of 1.17 g/cm.\n\nPeter Jenniskens and David F. Blake demonstrated in 1994 that a form of high-density amorphous ice is also created during vapor deposition of water on low-temperature (< 30 K) surfaces such as interstellar grains. The water molecules do not fully align to create the open cage structure of low-density amorphous ice. Many water molecules end up at interstitial positions. When warmed above 30 K, the structure re-aligns and transforms into the low-density form.\n\nVery-high-density amorphous ice (VHDA) was discovered in 1996 by Mishima who observed that HDA became denser if warmed to 160 K at pressures between 1 and 2 GPa and has a density of 1.26 g/cm at ambient pressure and temperature of 77 K. More recently it was suggested that this denser amorphous ice was a third amorphous form of water, distinct from HDA, and was named VHDA.\n\nIn general, amorphous ice can form below ~130 K. At this temperature, water molecules are unable to form the crystalline structure commonly found on Earth. Amorphous ice may also form in the coldest region of the Earth's atmosphere, the summer polar mesosphere, where noctilucent clouds exist. These low temperatures are readily achieved in astrophysical environments such as molecular clouds, circumstellar disks, and the surfaces of objects in the outer solar system. In the laboratory, amorphous ice transforms into crystalline ice if it is heated above 130 K, although the exact temperature of this conversion is dependent on the environment and ice growth conditions. The reaction is irreversible and exothermic, releasing 1.26–1.6 kJ/mol.\n\nAn additional factor in determining the structure of water ice is deposition rate. Even if it is cold enough to form amorphous ice, crystalline ice will form if the flux of water vapor onto the substrate is less than a temperature-dependent critical flux. This effect is important to consider in astrophysical environments where the water flux can be low. Conversely, amorphous ice can be formed at temperatures higher than expected if the water flux is high, such as flash-freezing events associated with cryovolcanism.\n\nAt temperatures less than 77 K, irradiation from ultraviolet photons as well as high-energy electrons and ions can damage the structure of crystalline ice, transforming it into amorphous ice. Amorphous ice does not appear to be significantly affected by radiation at temperatures less than 110 K, though some experiments suggest that radiation might lower the temperature at which amorphous ice begins to crystallize.\n\nAmorphous ice can be separated from crystalline ice based on its near-infrared and infrared spectrum. At near-IR wavelengths, the characteristics of the 1.65, 3.1, and 4.53 µm water absorption lines are dependent on the ice temperature and crystal order. The peak strength of the 1.65 µm band as well as the structure of the 3.1 µm band are particularly useful in identifying the crystallinity of water ice.\n\nAt longer IR wavelengths, amorphous and crystalline ice have characteristically different absorption bands at 44 and 62 µm in that the crystalline ice has significant absorption at 62 µm while amorphous ice does not. In addition, these bands can be used as a temperature indicator at very low temperatures where other indicators (such as the 3.1 and 12 µm bands) fail. This is useful studying ice in the interstellar medium and circumstellar disks. However, observing these features is difficult because the atmosphere is opaque at these wavelengths, requiring the use of space-based infrared observatories.\n\nMolecular clouds have extremely low temperatures (~10 K), falling well within the amorphous ice regime. The presence of amorphous ice in molecular clouds has been observationally confirmed. When molecular clouds collapse to form stars, the temperature of the resulting circumstellar disk isn't expected to rise above 120 K, indicating that the majority of the ice should remain in an amorphous state. However, if the temperature rises high enough to sublimate the ice, then it can re-condense into a crystalline form since the water flux rate is so low. This is expected to be the case in the circumstellar disk of IRAS 09371+1212, where signatures of crystallized ice were observed despite a low temperature of 30–70 K.\n\nFor the primordial solar nebula, there is much uncertainty as to the crystallinity of water ice during the circumstellar disk and planet formation phases. If the original amorphous ice survived the molecular cloud collapse, then it should have been preserved at heliocentric distances beyond Saturn's orbit (~12 AU).\n\nEvidence of amorphous ice in comets is found in the high levels of activity observed in long-period, Centaur, and Jupiter Family comets at heliocentric distances beyond ~6 AU. These objects are too cold for the sublimation of water ice, which drives comet activity closer to the sun, to have much of an effect. Thermodynamic models show that the surface temperatures of those comets are near the amorphous/crystalline ice transition temperature of ~130 K, supporting this as a likely source of the activity. The runaway crystallization of amorphous ice can produce the energy needed to power outbursts such as those observed for Centaur Comet 29P/Schwassmann–Wachmann 1.\n\nWith radiation equilibrium temperatures of 40–50 K, the objects in the Kuiper Belt are expected to have amorphous water ice. While water ice has been observed on several objects, the extreme faintness of these objects makes it difficult to determine the structure of the ices. The signatures of crystalline water ice was observed on 50000 Quaoar, perhaps due to resurfacing events such as impacts or cryovolcanism.\n\nThe Near-Infrared Mapping Spectrometer (NIMS) on NASA's Galileo spacecraft spectroscopically mapped the surface ice of the Jovian satellites Europa, Ganymede, and Callisto. The temperatures of these moons range from 90–160 K, warm enough that amorphous ice is expected to crystallize on relatively short timescales. However, it was found that Europa has primarily amorphous ice, Ganymede has both amorphous and crystalline ice, and Callisto is primarily crystalline. This is thought to be the result of competing forces: the thermal crystallization of amorphous ice versus the conversion of crystalline to amorphous ice by the flux of charged particles from Jupiter. Closer to Jupiter than the other three moons, Europa receives the highest level of radiation and thus through irradiation has the most amorphous ice. Callisto is the furthest from Jupiter, receiving the lowest radiation flux and therefore maintaining its crystalline ice. Ganymede, which lies between the two, exhibits amorphous ice at high latitudes and crystalline ice at the lower latitudes. This is thought to be the result of the moon's intrinsic magnetic field, which would funnel the charged particles to higher latitudes and protect the lower latitudes from irradiation.\n\nThe surface ice of Saturn's moon Enceladus was mapped by the Visual and Infrared Mapping Spectrometer (VIMS) on the NASA/ESA/ASI Cassini space probe. The probe found both crystalline and amorphous ice, with a higher degree of crystallinity at the \"tiger stripe\" cracks on the surface and more amorphous ice between these regions. The crystalline ice near the tiger stripes could be explained by higher temperatures caused by geological activity that is the suspected cause of the cracks. The amorphous ice might be explained by flash freezing from cryovolcanism, rapid condensation of molecules from water geysers, or irradiation of high-energy particles from Saturn.\n\nAmorphous ice is used in some scientific experiments, especially in cryo-electron microscopy of biomolecules. The individual molecules can be preserved for imaging in a state close to what they are in liquid water.\n\n"}
{"id": "25210319", "url": "https://en.wikipedia.org/wiki?curid=25210319", "title": "Arieh Ben-Naim", "text": "Arieh Ben-Naim\n\nArieh Ben-Naim (Hebrew: אריה בן-נאים; Jerusalem, 11 July 1934) is a professor of physical chemistry who retired in 2003 from the Hebrew University of Jerusalem. He has made major contributions over 40 years to the theory of the structure of water, aqueous solutions and hydrophobic-hydrophilic interactions. He is mainly concerned with theoretical and experimental aspects of the general theory of liquids and solutions. In recent years, he has advocated the use of information theory to better understand and advance statistical mechanics and thermodynamics.\n\nBooks written by Arieh Ben-Naim:\n\n\nIn 2017, Ben-Naim posted three articles in arXiv. In those articles, three radical ideas were introduced into a field which is considered as classical. The ideas followed from the new definition of entropy based on Shannon's measure of information. The three ideas are, briefly, as follows.\n\nFirst, there is no relationship between either entropy or the second law of thermodynamics and the so called arrow of time. This false association between the Second Law and time was first suggested by Arthur Eddington. Also the Boltzmann's H-Theorem is not about the time dependence of the entropy, but the time dependence of the Shannon's measure of information. In this respect Boltzmann erred in interpreting his (-)H function as entropy.\n\nSecond, the application of the concept of entropy to the entire universe is unwarranted. This association has its origin in Clausius' statement that the entropy of the World always increases. Clausius, who is credited for the formulation of the second law, did not and could not understand the molecular interpretation of entropy. Unfortunately, the application of the concept of entropy to the entire universe features in many textbooks and in popular science books. This erroneous application is discussed in great detail in Ben-Naim's recent books: \"The Briefest History of Time\", \"Entropy, the Truth the whole Truth and nothing but the Truth\", and in \"Information, Entropy, Life and the Universe\".\n\nThird, the application of entropy and the Second Law to living organisms is totally unwarranted. The most famous statement about\nentropy and life was made by Erwin Schrödinger, in his book \"What is Life?\". In this book, Schrödinger not only discusses entropy and life and associates entropy with disorder, he also \"invents\" the concept of \"negative entropy,\" which was later renamed negentropy by Léon Brillouin. This erroneous application is further discussed in Ben-Naim's books.\n\nBen-Naim is a modern antagonist of the term entropy. He advocates abandoning the word entropy altogether, and replacing it with \"missing information\". He also indicates that the Kelvin temperature scale artificially introduces the units of thermodynamic entropy. Because this temperature scale was introduced before the atomic, microscopic nature of matter was widely accepted, the Boltzmann constant was necessary. \"S\" = \"k\"log(\"W\") could be expressed simply as \"S\" = log(\"W\"), if the energy units for temperature \"k\"\"T\" were used.\n\nAn example of the insight that information theory can bring to statistical mechanics is the rederivation of the Sackur-Tetrode equation. It results from stacking the missing information due to four terms: positional uncertainty, momenta uncertainty, quantum mechanical uncertainty principle and the indistinguishability of the particles.\n\nBooks written by Arieh Ben-Naim on entropy and statistical mechanics include:\n\n\n"}
{"id": "39023070", "url": "https://en.wikipedia.org/wiki?curid=39023070", "title": "Augstsprieguma tīkls", "text": "Augstsprieguma tīkls\n\nAugstsprieguma Tīkls AS (AST) is an independent electricity transmission system operator in Latvia. It operates power systems with voltages of 110 kV and above, which is leased from Latvenergo.\n\nOn 15 October 1939 the Ķegums power plant and the first substation in Riga start operation. This day is considered as the beginning of AST.\n\nAST was a part of Latvenergo. On 2 January 2012, it was unbundled from Latvenergo and the Ministry of Finance of Latvia became the sole shareholder of AST.\n\nIn December 2017, AST bought from Uniper Ruhrgas International and Itera Latvija 18.31% and 16.05% stakes in Conexus Baltic Grid, a natural gas transmission system operator in Latvia, for €57.394 million.\n\nIn 2017, the company had a turnover of €158.86 million and a profit of €309,244. In August 2018, the share capital of the company was increased up to €63,139,313 by converting the loan of €57.394 million for purchasing share in Conexus Baltic Grid into the share capital.\n\nAST plans to invest €445 million in the Latvian power transmission system in 2018-2027. In 2018, it invests €89.05 million, of which €62.1 million will be spent on implementation of the third stage in construction of the Kurzeme Ring power transmission line and €4.25 million in construction of the third power interconnection between Estonia and Latvia.\n"}
{"id": "2587563", "url": "https://en.wikipedia.org/wiki?curid=2587563", "title": "Bateau", "text": "Bateau\n\nA bateau or batteau is a shallow-draft, flat-bottomed boat which was used extensively across North America, especially in the colonial period and in the fur trade. It was traditionally pointed at both ends but came in a wide variety of sizes. The name derives from the French word, \"bateau\", which is simply the word for boat and the plural, bateaux, follows the French, an unusual construction for an English plural. In the southern United States, the term is still used to refer to flat-bottomed boats, including those elsewhere called jon boats.\n\nBateaux were flat-bottomed and double-ended. They were built with heavy stems at bow and stern and a series of frames amidships, likely from natural oak crooks when available, and planked with sawn boards, likely pine although builders would have used whatever material was available. These boats would have varied from place to place, from builder to builder and also evolved over time, however in general, they were long and wide. The bottoms were planked and flat, without a keel, but possibly with a larger \"keel-plank\" in the center and sometimes reinforced with cross cleats. The sides were planked, tapering to sharp at either end.\n\nThe French explorers of North America used batteaux as well as the native canoes and cartols. The boats' shallow draft worked well in rivers while its flat bottom profile allowed heavy loading of cargoes and provided stability. The smallest batteau required only one crewman, while larger ones required up to five and reach up to in length. The largest batteaux could carry two to ten tons of cargo. Batteaux could mount a small sail although the flat bottom was not optimal for sailing. In military records, it is seen that the boats were propelled primarily by oars with one oar being used at the stern as a rudder. Of Louisiana in 1763 it was described: \"Beyond the mouth of the Missouri river the bateau of no prying New Orleans trader had ever penetrated.\" The same author wrote of the Roanoke Valley, Virginia: \"One may make a pleasant voyage on the New River from this point to Eggleston's Springs, twenty-five miles further down the current, taking one of the many bateaux which ply constantly on the stream, and simply drifting on the lazy wave until the destination is reached.\" In the same book, the spelling is given as \"batteaux\":\n\nAlong the Greenbrier and New Rivers adventurous boatmen ply in \"batteaux\", carrying merchandise or travelers who wish to explore the wonders of the New River cañon. …Our artists, who made the tour of the New River cañon in a batteau, found it an exciting experience. At the junction of the Greenbrier and New Rivers they engaged one of the boats used in running the rapids. This boat was sixty feet long by six wide, and was managed by three negroes,—the \"steersman\", who guided the boat with a long and powerful oar; the headsman, who stood on the bow to direct the steersman by waving his arms; and an extra hand, who assisted with an oar in the eddies and smooth parts of the river.\n\nMany types of batteaux were deployed by the Colonial French and British militaries, with the largest capable of mounting small cannon or swivel guns. In the wilderness with many rivers but few bridges, batteaux were sometimes constructed, used, then purposely sunk to prevent the enemy from discovering them and using them to raid behind the passing army. Alternately, utilizing the stability of their flat bottoms, batteau could be strung together to form pontoon bridges, which are, therefore, sometimes known as \"batteau bridges\". Some British military batteau of the French and Indian War could haul twenty men or 12 barrels of supplies with a smaller crew. In the Revolutionary War, an extant plan of the British Admiralty calls for batteau of in length, with a beam and a depth of .\"\n\nSpecific designs were developed to suit local conditions. Batteau were used as freight boats on canals in the northern U.S. until replaced by the larger canal boats in the early 1800s. James River batteau were large craft designed for hauling tobacco on Virginia's large rivers, while Mohawk River batteau were smaller and of very shallow draft (and sometimes with awnings). Most of the inland navigations in the southern United States, penetrating the Piedmont by way of the river valleys, were for bateau.\n\nBatteaux were a very important part of the American culture. The town of Ronceverte, West Virginia, commemorates the logging and batteau industry with an annual outdoor theater, \"Riders of the Flood\", where the spring rains sent harvested timbers down the Greenbrier River for the sawmills.\n\nAn ark is used in the play, a scaled-down model of the original crafts that accompanied the batteaux downriver for the spring floods.\n\nWest Virginia author W. E. Blackhurst used \"bateau\" in his books of Pocahontas County and the Greenbrier River. These boats figure in the logging-era book \"Riders of the Flood\", on which the play of the same name is based. This batteau was primarily for logging, meant to maneuver quickly and withstand dangerous river conditions and is built differently from the New River batteau at the confluence of the Greenbrier River.\n\nProper spelling remains a problem with researchers. Dr. William E. Trout III, a member of the Virginia Canals and Navigations Society who has written about the batteaux, explained the issue thus:\n\nWe use the spelling \"batteau\" because we consider that to be the correct spelling for our kind of boat—the James River Batteau, invented by the Rucker brothers in 1771 and later patented. This is the way it was spelled during the batteau era, even in the Virginia state laws. Evidently after batteaux were forgotten and the word was not used anymore, this spelling was forgotten and reverted to the French spelling for that general type of boat (and for boats generally). We have a lot of trouble with editors who don't want to know that the correct spelling for our type of boat was \"batteau,\" just because current dictionaries don't have it. Some regional dictionaries do say that \"batteau\" was the usual American spelling during the 19th century. The plural is \"batteaux.\"\n\nSince 1985 replicas of James River Batteaux have descended the James every year during the week-long James River Batteau Festival, and have explored other rivers in and around Virginia.\n\n"}
{"id": "2903082", "url": "https://en.wikipedia.org/wiki?curid=2903082", "title": "Betty Leslie-Melville", "text": "Betty Leslie-Melville\n\nBetty Leslie Melville (née McDonnell; March 7, 1927 – September 23, 2005) was an American born author and conservationist.\n\nBorn in Baltimore, Maryland on March 7, 1927, the daughter of a chiropractor, Leslie Melville attended Johns Hopkins University. \nLeslie Melville married Jock Leslie Melville in 1964., \n\nShe was instrumental in creating sanctuaries to preserve the subspecies of the Rothschild giraffe in Kenya. Often called the \"Giraffe Lady,\" she spent much of her life living and working in Kenya protecting and caring for the Rothschild giraffe population there, primarily through a breeding programme established at her residence, Giraffe Manor. During her time working there, the Rothschild giraffe population grew from about one hundred twenty to over four hundred.\n\nAlong with her husband Jock Leslie Melville and their adopted giraffe Daisy, they were the subject of the film \"The Last Giraffe\" (1979) with Susan Anspach playing Betty.\n\nAs part of their fund-raising efforts, Betty and Jock Leslie Melville collaborated on a series of books about animals, most of them characterized by Betty's rather entertaining style. \"Raising Daisy Rothschild\" (1977) \"the story of two delightful young people and how they raised and grew to love a young giraffe... or two\" became a best-seller. Other animal stories and fiction publications include: \"Elephant Have Right of Way\" (1973), \"There's a Rhino in the Rose Bed\", \"Mother\" (1973), \"That Nairobi Affair\" (1975), \"Bagomoyo, A Falling Star, The Giraffe Lady, Daisy Rothschild: The Giraffe Who Lives With Me\" (children's book), and \"Walter Warthog\" (1989), a children's story about the tame warthog they named after their friend Walter Cronkite, the CBS news anchorman. The books helped to raise more funds for the Giraffe Centre they set up at Langata, Kenya in 1983. She died on September 23, 2005 in Baltimore. \n\n"}
{"id": "1204012", "url": "https://en.wikipedia.org/wiki?curid=1204012", "title": "Bhangmeter", "text": "Bhangmeter\n\nA bhangmeter is a non-imaging radiometer installed on reconnaissance and navigation satellites to detect atmospheric nuclear detonations and determine the yield of the nuclear weapon. They are also installed on some armored fighting vehicles, in particular NBC reconnaissance vehicles, in order to help detect, localise and analyse tactical nuclear detonations. They are often used alongside pressure and sound sensors in this role in addition to standard radiation sensors. Some nuclear bunkers and military facilities may also be equipped with such sensors alongside seismic event detectors.\n\nThe bhangmeter was invented, and the first proof-of-concept device was built, in 1948 to measure the nuclear test detonations of Operation Sandstone. Prototype and production instruments were later built by EG&G, and the name \"bhangmeter\" was coined in 1950. Bhangmeters became standard instruments used to observe US nuclear tests. A bhangmeter was developed to observe the detonations of Operation Buster-Jangle (1951) and Operation Tumbler-Snapper (1952). These tests lay the groundwork for a large deployment of nationwide North American bhangmeters with the Bomb Alarm System (1961-1967).\n\nUS president John F. Kennedy and the First Secretary of the Communist Party of the Soviet Union Nikita Khrushchev signed the Partial Test Ban Treaty on August 5, 1963, under the condition that each party could use its own technical means to monitor the ban on nuclear testing in the atmosphere or in outer space.\n\nBhangmeters were first installed, in 1961, aboard a modified US KC-135A aircraft monitoring the pre-announced Soviet test of Tsar Bomba.\n\nThe Vela satellites were the first space-based observation devices jointly developed by the U.S. Air Force and the Atomic Energy Commission. The first generation of Vela satellites were not equipped with bhangmeters but with X-ray sensors to detect the intense single pulse of X-rays produced by a nuclear explosion. The first satellites which incorporated bhangmeters were the Advanced Vela satellites.\n\nSince 1980, bhangmeters are part of US GPS navigation satellites.\n\nThe silicon photodiode sensors are designed to detect the distinctive bright double pulse of visible light that is emitted from atmospheric nuclear weapons explosions. This signature consists of a short and intense flash lasting around 1 millisecond, followed by a second much more prolonged and less intense emission of light taking a fraction of a second to several seconds to build up. This signature, with a double intensity maximum, is characteristic of atmospheric nuclear explosions and is the result of the Earth's atmosphere becoming opaque to visible light and transparent again as the explosion's shock wave travels through it.\n\nThe effect occurs because the surface of the early fireball is quickly overtaken by the expanding \"case shock\", the atmospheric shock wave composed of the ionised plasma of what was once the casing and other matter of the device. Although it emits a considerable amount of light itself, it is opaque and prevents the far brighter fireball from shining through. The net result recorded is a decrease of the light visible from outer space as the shock wave expands, producing the first peak recorded by the bhangmeter.\n\nAs it expands, the shock wave cools off and becomes less opaque to the visible light produced by the inner fireball. The bhangmeter starts eventually to record an increase in visible light intensity. The expansion of the fireball leads to an increase of its surface area and consequently an increase of the amount of visible light radiated off to space. The fireball continuing to cool down, the amount of light eventually starts to decrease, causing the second peak observed by the bhangmeter. The time between the first and second peaks can be used to determine its nuclear yield.\n\nThe effect is unambiguous for explosions below about altitude, but above this height a more ambiguous single pulse is produced.\n\nThe name of the detector is a pun, which was bestowed upon it by Fred Reines, one of the scientists working on the project. The name is derived from the Hindi word \"bhang\", a locally grown variety of cannabis which is smoked or drunk to induce intoxicating effects, the joke being that one would have to be on drugs to believe the bhangmeter detectors would work properly. This is in contrast to a \"bangmeter\" one might associate with detection of nuclear explosions.\n\n\n"}
{"id": "42648373", "url": "https://en.wikipedia.org/wiki?curid=42648373", "title": "Breadalbane Hydro-Electric Scheme", "text": "Breadalbane Hydro-Electric Scheme\n\nThe Breadalbane Hydro-Electric Scheme is a hydroelectric scheme in the Breadalbane area of Perthshire, Scotland. It comprises seven power stations which generate power from the dams around Loch Lyon, Loch Earn and Loch Tay.\n\nConstruction began in 1951. The Lawers Dam section began generating in 1956. The dam measures long and in height. Water descends a vertical distance of , the highest drop of any scheme in Scotland.\n\nFurther facilities were added at Killin and Stronuich. The construction was completed in 1961. \n"}
{"id": "2230854", "url": "https://en.wikipedia.org/wiki?curid=2230854", "title": "Characteristic energy length scale", "text": "Characteristic energy length scale\n\nThe characteristic energy length scale formula_1 describes the size of the region from which energy flows to a rapidly moving crack. If material properties change within the characteristic energy length scale, local wave speeds can dominate crack dynamics. This can lead to supersonic fracture.\n"}
{"id": "947375", "url": "https://en.wikipedia.org/wiki?curid=947375", "title": "Chevrolet Suburban", "text": "Chevrolet Suburban\n\nThe Chevrolet Suburban is a full-size/extended-length SUV from Chevrolet. It is the longest continuous use automobile nameplate in production, starting in 1935 for the 1935 U.S. model year, and has traditionally been one of General Motors' most profitable vehicles. The 1935 first generation Carryall Suburban was one of the first production all-metal bodied station wagons.\n\nIn addition to the Chevrolet brand, the Suburban was produced under the GMC marque until its version was rebranded Yukon XL, and also briefly as a Holden. For most of its recent history, the Suburban has been a station wagon-bodied version of the Chevrolet pickup truck, including the Chevrolet C/K and Silverado series of truck-based vehicles. Cadillac offers a version called the Escalade ESV.\n\nThe Suburban is sold in the United States (including the insular territories), Canada, Central America, Chile, Myanmar, Laos, Angola, the Philippines, and the Middle East (except Israel) while the Yukon XL is sold only in North America (United States and Canada) and the Middle East territories (except Israel).\n\nSeveral automotive companies in the United States used the \"Suburban\" designation to indicate a windowed, station wagon type body on a commercial frame including DeSoto, Dodge, Plymouth, Studebaker, Nash, Chevrolet, and GMC. The (Westchester) Suburban name was, in fact, a trademark of U.S. Body and Forging Co. of Tell City, Indiana, which built wooden station wagon bodies for all of these automobile and light truck chassis and more.\n\nChevrolet began production of its all-steel \"carryall-suburban\" in 1935. GMC brought out its version in 1937. These vehicles were also known as the \"Suburban Carryall\" until GM shortened the name to simply \"Suburban\". GMC's equivalent to the Chevrolet model was originally named \"Suburban\" as well, until being rebranded as \"Yukon XL\" for the 2000 model year.\n\nWith the end of production of the Dodge Town Wagon in 1966 and the Plymouth Fury Suburban station wagon in 1978, only General Motors continued to manufacture a vehicle branded as a \"Suburban\", and GM was awarded an exclusive trademark on the name in 1988. The Chevrolet Suburban is one of the largest SUVs on the market today. It has outlasted competitive vehicles such as the International Harvester Travelall, Jeep Wagoneer, and the Ford Excursion. The latest competitor is the extended Ford Expedition EL, which replaced the Excursion.\n\nThe Suburban of today is a full-size SUV with three rows of seating, a full pickup truck frame, and V8 engine. It is one of the few station wagons available with all bench rows. The Suburban is the same height and width as the Chevrolet Tahoe, although the Suburban is longer. The extra length provides a full-sized cargo area behind the 9 passenger seating area. From 1973 to 2013 it had been available in half-ton and 3/4-ton versions, the latter discontinued after the 2013 model year, but was revived in 2015 as a fleet-exclusive vehicle for the 2016 model year.\n\nIn recent years, the Suburban has been used as a police truck, fire chief's vehicle, or EMS vehicle. Suburbans are also used as limousines. Gothic black Suburban vehicles are commonly used by federal intelligence services, such as Secret Service for example. Secret Service operates fully armored versions of the Suburban for the President of the United States when he attends less formal engagements.\n\nIn the late 1990s, GM also introduced a RHD version of the Suburban, badged as a Holden, for the Australian market. Sales were low and GM withdrew the model in 2000 from Holden's lineup.\n\nThere have been twelve generations of Chevrolet Suburbans since its 1935 debut, the most recent (starting with the 2015 model year) entering showrooms in February 2014. In 2015, Chevrolet celebrated the Suburban's 80th anniversary with the Arlington Assembly plant unveiling the 10th million vehicle built at the facility since its 1954 opening, a black 2015 Suburban LTZ. This marked the second time in the Suburban's history that it has achieved this honor, as the tenth generation Suburban from the 2011 model year was also the ninth million vehicle built there.\n\nA 2014 iSeeCars.com study named the Chevrolet Suburban the longest lasting SUV. The Suburban was also ranked the third longest lasting vehicle overall, only bested by the Ford Super Duty and Chevrolet Silverado HD pickup trucks. Its platform mates, the Chevy Tahoe, GMC Yukon, and Yukon XL all ranked within the top-11 on the list. \n\nIn 2015, the Suburban commemorated its 80th anniversary at General Motors Arlington Assembly Plant where the 10 millionth Suburban was produced. A video was posted on Chevrolet's YouTube channel about its eighty-year legacy.\nIn a February 26, 2018 article celebrating the vehicle's 83rd year, \"Car and Driver\" notes that the Suburban's longevity is due to being one of GM's best selling brands, its appeal to customers across the board regardless of race, gender, class, or political affiliation, and a unique loyalty to the SUV. In an interview from Chevrolet's truck/SUV marketing executive Sandor Piszar, who recalls an event celebrating the truck division's 100th anniversary when they asked about what they named their vehicles, “It’s a funny question, but it really is an intriguing point,” Piszar says. “People name what they love. And they love their Suburbans.”\n\nPrior to this first generation Suburban, in 1933 Chevrolet had offered a station wagon body, built on the 1/2 ton truck frame. This model was specifically built for National Guard and Civilian Conservation Corps units. Much of the body was constructed from wood, and could seat up to eight occupants.\n\nThe actual first generation model was offered by Chevrolet as a \"Carryall Suburban\" – a tough, no-nonsense load carrier featuring a station wagon body on the chassis of a small truck. Focused on functionality, the concept was literally to \"carry all\": the whole family and their gear were to find sufficient space in one truck. It shared the front sheetmetal and frames of the 1/2 ton pickup models of the same year, but featured all-metal wagon bodies differing very little in shape from contemporary \"woodie\" wagons. Seating for up to eight occupants was available, with three in front row, two in the middle row, and three in the rear row. Either the side-hinged rear panel doors or a rear tailgate/lift window could be selected for cargo area access.\n\nSuburbans were built in model years 1941, 1942, and 1946. It was also produced during the war as a military transport vehicle. Seating for up to eight occupants was available. Models with rear panel doors were designated \"3106,\" while those with tailgates were designated \"3116.\" The Chevrolet versions were equipped a 216-cubic-inch 6-cylinder engine. The GMC version was equipped with a 228-cubic-inch 6-cylinder engine. It shared much of its mechanicals with the AK Series trucks.\n\nThis model generation was based on the Chevrolet Advance Design series of pickups.\n\nBeginning in 1953, the Hydra-Matic 4-speed automatic transmission was available in GMC models and in the 1954 model year Chevrolet Suburbans. Models with rear panel doors were designated \"3106,\" while those with tailgates were designated \"3116.\" In 1952, the Suburban came with either a tailgate or panel doors. The front bench seat was split, with two seats on the driver's side and a single seat on the passenger side, which slid forward for access to the rear two rows of seats. The second row was a \"2/3\" seat, requiring occupants to move past the front passenger seat, as well as the second-row seats to access the third row.\n\nThis was the last series to feature \"Canopy express\" models.\n\nThe design of the 1947 Suburban would inspire the design of the Chevrolet HHR over half a century later.\n\nUpdated engineering and styling on Chevrolet trucks was not introduced until March 25, 1955, in the middle of the model year that GM called the Chevrolet Task Force/GMC Blue Chip series. All Chevrolet and GMC truck models received new styling that included a flatter hood, front fenders flush with the body, and a trapezoid grill. The trucks' V-shaped speedometer was shared with passenger car models.\n\nEngines included I-6 and the small block V8s.\nChevrolet used its 265 V8 engine, later evolving it to a 283-cubic-inch version. GMC based their V8 on a Pontiac design. Standard Suburban model numbers continued from the previous series, but the introduction of four-wheel-drive models in 1957 added the numbers \"3156\" for 4WD Suburbans with panel doors, and \"3166\" for 4WD Suburbans with tailgates.\n\nThe \"Suburban\" name was also used on GM's fancy 2-door GMC 100 series pickup trucks from 1955–1959, called the Suburban Pickup, which was similar to the Chevrolet Cameo Carrier, but it was dropped at the same time as Chevy's Cameo in March 1958 when GM released the new all-steel \"Fleetside\" bed option replacing the Cameo / Suburban Pickup fiberglass bedsides. The Suburban name was never used again on a 1/2 ton pickup after the discontinuance of the Suburban Pickup. Although not documented due to a fire which destroyed the records, the production of Suburban Pickups is understood to be 300 or fewer each model year it was offered from 1955-1958.\n\nThe styling of the 1960 – 1961 model year took cues from the late 1950s Chevrolet vehicles and had large oval ports above the grille. Front independent suspension was new for 1960. The cab featured a \"wrap around\" windshield. Tailgate and panel door rear openings were available.\n\nFrom 1962 onwards, the hood styling was more conservative, with hoods that eliminated the large ports. In 1964, the front glass area was updated to a flatter windshield and larger door glass. of cargo could be carried in the back.\n\nThis model series introduced a factory-equipped 4WD (\"K\") option for the first time. The 2WD (\"C\") models introduced a torsion bar-based independent front suspension and trailing arm and coil spring rear, but by 1963, returned to a more conventional coil-spring approach.\n\nEngine options included I-6 and small-block V8s. A GMC V6 engine was also available on GMC models. This 305 was actually from GMCs medium-duty truck line. It featured high torque but was also notable for poor fuel economy. Transmissions were a 3-speed and 4-speed manual, the automatic Powerglide and the dual-range Hydramatic in the GMC models.\n\nA 15-passenger conversion was done by Stageway of Fort Smith, Ark. These modified Suburbans had three doors on the right, a wheelbase, were long, and weighed .\n\nOne ton (C-30), panel truck models were no longer available after 1966.\n\nThe 6th generation Suburbans featured a single driver-side door and two passenger-side doors, and were available in both 2WD and 4WD models.\n\nEngines offered over the six model years included the 250 and 292 CI inline 6, 283, 307, 327, and 350 CI small block V-8, and 396/402 CI big block V-8. For the first time, a three-quarter ton version was available. It also became a larger size vehicle as well with the introduction of the K5 Blazer that debuted in 1968 for the 1969 model year.\n\nThis series would also be the last to offer C-10 & C-20 panel truck models for commercial purposes, with 1970 as the last year.\n\n1971 models featured disc brakes on the front wheels, and 1972 was the last year for coil-spring rear suspension on 2WD models. 1972 also introduced a smaller housing for the rear seat air conditioning (a unit that ran the full length of the roof had been available since 1967). The Comfort-Tilt steering wheel became optional in 1971.\n\nThis generation of Suburban coincided with the rapid growth of the recreational vehicle market. While only about 6,200 Suburbans were produced in 1967, by 1972 that number had grown to some 27,000.\n\nIn 1964, Chevrolet in Brazil introduced a 5-door version of the Suburban called C-1416 (known as Veraneio from 1969 onwards, which is Portuguese for \"summertime\"). It was based on the contemporary Brazilian Chevrolet C-14. Like the C-14, the C-1416/Veraneio used the instrument cluster from the US C/K series although the exterior sheet metal layout is exclusive to Brazil. It was initially powered with a Chevrolet 4.2 L inline six based on the pre-1962 \"Stovebolt\" engines. Later it used the 250-cid 4.1 L engine from Chevrolet's Brazilian mid-size sedan – the Opala. The original version of the Veraneio was kept in production, with another grille and interior, until 1988 (model year 1989), but it was eventually replaced with an updated version based on the Série 20 family. The second generation of Veraneio was produced from 1989 to 1995.\n\nIn 1997 GM introduced in Brazil the North American pickups for the local market, replacing the \"C Series\". The Brazilian version of the Suburban was also converted to the current generation at the time and lasted until 2001, was called the Grand Blazer, a successor to the Veraneio. The 4.1 L inline six engine with was offered on both models with option for a MWM 4.2 L turbodiesel unit with .\n\nIn 2015, \"Autoweek\" ranked the Veraneio fourth among the Chevrolet station wagons that America never got. It also cited the vehicle's design as \"baroque\" and summed it up as \"It's a 1960s Brazilian crossover.\" Autoweek notes that The Veraneio can be imported to the United States, depending on the condition of the vehicle.\nWith the third generation Rounded-Line C/R & K/V models, the Suburban became a four-door vehicle. The Rounded-Line 1970s body style remained largely unchanged for 19 model years – making this series the longest Suburban generation in production. 2WD (C/R) and 4WD models (K/V) were both available in 1/2 and 3/4 ton (\"10\" and \"20\") chassis.\n\nSuburbans for model year 1973 now had two doors on each side (the previous generation had just one door on the driver's side), front-and-rear air conditioning, a baggage rack, a heater under the third seat and step-plates for easier access. A new \"Eaton Automatic Differential Lock\" was introduced as an option extra for the rear differential.\n\nFor the 1973 model year the base engine was the 250 CID inline six (100 net HP), with a 307 or 350-cubic-inch small block V8 (115 or 155 net), or the new-for-1973 454 CID big block V8 (240 net HP) optional. The 307 V8 was dropped for 1974, supplanted by the 305 and 400 small block V8's in 1976. The 400 was dropped after 1980, leaving the 350 as the only available engine in K-series Suburbans as the 454 was not yet offered in 4x4's. In 1982 a new Detroit Diesel V8 engine was available, producing 130 horsepower and 240 lb.-ft. of torque. The Diesel later became the engine of choice for Suburbans exported to Europe from USA. Except for the discontinuation of the 305 V8 in '88 the engine line-up continued mostly unchanged, with the 350, 454, and 6.2 diesel, until the Suburban was redesigned onto the GMT400 chassis in 1992.\n\nThree-speed Turbo Hydra-Matic 350 and 400 automatic transmissions were available, and the four-speed Turbo Hydra-Matic 700R4 was introduced in 1981 and was available with the small block and 6.2-liter diesel. Towing packages, with lower axle ratio and heavy-duty cooling additions, were optional.\n\nTrim options included base (Scottsdale) level and upgraded Silverado versions. An optional 3rd row bench seat allowed for nine-passenger configurations. A rear heating system was optional.\n\nFor model year 1981, automatic locking front hubs were added for shift-on-the-move four-wheel drive models, and the NP208 transfer case replaced the NP205 in most models.\n\nFor 1984, asbestos was removed from rear brakes. For 1985, a new grille was used. A total of 64,670 Suburbans were made in 1985.\n\nFor model year 1987, the method of fuel delivery for the engines was switched from carburetors to electronic fuel injection. (However, for the 454 cubic inch or 7.4 liter displacement engine, carburetors were still made available along with electronic fuel injection for the 1987-1989 model years). The system that GM chose was called \"throttle body injection\", or TBI. Suburban gained rear-wheel anti-lock braking system (ABS) for the 1990 model year. A heavy-duty four-speed automatic transmission, the 4L80-E was added for 1991. Also for 1989, the grille was redesigned to accommodate a quad side-by-side headlight setup and larger marker lights/turn signals than the previous dual filament single headlight setup, which was styled concurrently with the GMT400 light duty trucks. Economy models (usually for fleet usage) were available with only two headlights (again, based on the base model C/K series).\n\nGM temporarily changed the usual \"C/K\" designation to \"R\" and \"V\" for the 1987 through 1991 model years. This was done to avoid confusion with the GMT400-based Chevrolet C/K pickup trucks, which were introduced in 1988, during the overlap period.\n\nThe GMT400-based Suburbans were introduced in December 1991 for the 1992 model year. The similar pickup truck models had switched to the newer platforms in the 1988 model year. Both 2WD and 4WD models, designated \"C\" and \"K\", were offered, as well as half ton and three-quarter ton (\"1500\" and \"2500\") models.\n\nThe base engine for all variants was the small-block 5.7 liter (350 cu in) V8. The big-block 7.4 L (454 cu in) V8 was optional for the 2500 series. The optional 6.5 L Turbo diesel was available on all models – though rare on the 1500 series. The 6.5 L Turbo diesel used in the Tahoe was detuned to torque due to the limitation of the axle capacity. 1500 Suburbans with the 6.5 L Turbo diesel used the 14 bolt axle from the 2500 series. Ground clearance was 6.9 inches, the approach angle was 18 degrees for the K-1500 (28 degrees for the K-2500) and the break over angle was 18 degrees.\n\nTransmissions included the \"4L60\" four-speed automatic in the 1500 series, and the heavier duty \"4L80\" four-speed automatic in the 2500 series and the 1500 series fitted with the 6.5 L Turbo diesel. The manual transmission option from the previous generation was dropped.\n\nThe GMT400 series introduced independent front suspension. The 2WD models used coil springs and 4WD models used torsion bars in the front suspension. All models used a live axle and leaf springs in the rear.\n\n0–60 mph time for a 1995 Suburban was 9.3 seconds. Top speed of a 1995 Suburban is governed on the engine for economy. A maximum of can be obtained. City fuel economy was and highway was . The turning circle was . In 1996, fuel economy had improved to highway.\n\nTrim options included a base-level version, the LS, and the LT. Interior seating arrangements allowed for either bucket or bench seating in the first row, and optional third row bench. The vehicle could be configured from two- to nine-passenger seating.\n\nBeginning in 1994, GM began making numerous annual changes to the Suburban, including:\n\nIn Australia and New Zealand, Holden imported the right-hand drive Chevrolet Suburban built by GM in Silao, Mexico between February 1998 and January 2001. The Suburban was first previewed in October 1997 at the Sydney Motor Show. In total, 746 were sold (460 petrol and 286 diesel). After 2001, subsequent models reverted to the original Chevrolet brand, which had also been used before 1998. Over the model's lifetime there were three trim levels: a base model, the LS and the LT. Not to be confused with the trim variants is the model code, designated \"K8\".\n\nThe Holden's interior differed from that of the American version, whereby the dashboard of the Chevrolet Blazer was used instead. However, it had to be stretched to fit in the larger Suburban. A bench seat came standard on the entry-level variant as well as the LS, but the more expensive LT received bucket seats. With the omission of the center seat, the LT has a maximum seating capacity of eight, compared to nine.\n\nCreature comforts standard in all models included a LCD compass in the rear-view mirror, a tilt adjustable steering wheel, a driver's airbag, ABS brakes, and dual-zone air conditioning. The second tier LS brought alloy wheels, power windows and mirrors among some features. To further up the ante, the LT gained electric front seats, leather trim, and a horizontally slated, two-part tailgate. This came as opposed to the \"barn doors\" found on the other specifications.\n\nThe Suburban was offered with the choice of either a 5.7-liter \"Vortec\" V8, producing and of torque, or a 6.5-liter turbodiesel V8 outputting and . The former choice was designated the \"1500\" name, while the turbodiesel saw the \"2500\" identify. The 5.7-liter petrol engine is LPG-compatible, and such systems can be retrofitted if desired. Regardless of the engine specified, the truck was equipped with a four-speed automatic transmission. However, what differed was the type of transmission. Petrol motors were fitted with the \"GM 4L60-E\" transmission, with the \"GM 4L80-E\" reserved for the diesel. A dashboard switch allows the vehicle to power all four wheels simultaneously, or the rear wheels only, and allows the low range gearing to be engaged.\n\nThe vehicle's fuel efficiency has been rated at for the diesel specification, with that figure rising to for the petrol model. With the hefty fuel consumption comes a 159-liter fuel tank.\n\nTo combat the extra payload and towing capacity of the diesel, an improved braking package, as well as super heavy duty axles and suspension were fitted. Holden recommends a maximum towing limit for the turbo diesels, with a reduced figure of for the petrol models.\n\nThe Holden Suburban's run actually wasn't the first or only time that Holden had sold the GMT400 platform in Oceania. Beginning in 1996, they imported GMC C/Ks for ambulance conversions. Unlike the Suburbans, these vehicles were not available to the general public, nor did they bear Holden badging. They also were not built as right-hand-drives from the factory like the Suburban was; the same company that handled the ambulance conversion (Jacab Ambulance in Tamworth) also switched the steering to the other side.\n\nThe GMT800-based Suburbans were introduced in late December 1999 (Texas-only) and January 2000 (nationally) for the 2000 model year. They were sold in two series: 1/2-ton 1500 and 3/4-ton 2500. Suburbans came in Base, LS and LT trims. Optional was pushbutton 4WD with low-range transfer case. A tow hitch with trailer wiring plug was optional.\n\nFor 2000, Chevrolet's long-serving 5.7 L and 7.4 L V8 engines were retired along with the 6.5 L diesel. New engines were Vortec 5300 for the 1500 series and Vortec 6000 for the 2500 series.\n\nNew features included:\n- Digital Components \n- new wheels \n- new interior\n-new modern dashboard\n\nFor 2001, the 6.0 L V8 in 2500-series Suburbans gained from a number of changes including aluminum cylinder heads. The new Vortec 8100 V8 was added as an option for the 2500 as well. OnStar became standard on LT models and LS models with the new Z71 package. Quadrasteer four-wheel steering was added as an option on 1500 series trucks.\n\n2002 saw several optional features made standard equipment on the LS model, including front and rear air conditioning, alloy wheels, power windows, power front seats, side steps, fog lamps, and heated outside rear view mirrors. Base models were discontinued, leaving LS and LT.\n\nThe Vortec 5300 L59 variant in the 1500 series added flex-fuel capability. The 6.0 engine was not available in the 1500 series.\n\nFor 2003, all GM full-size trucks received an upgraded interior, with better-quality materials and other enhancements. New radios offered Radio Data System compatibility, XM satellite radio, Bose sound and improved ergonomics. Adjustable pedals were added as an option, and the instrument cluster-mounted Driver Information Center was improved and monitored up to 34 vehicle functions. A Panasonic DVD system was added as an option.\nGM's Stabilitrak system was added, and Quadrasteer became available on 2500 series Suburbans. Towing capacity for Quadrasteer-equipped vehicles was reduced by 300 lbs (the weight of the system).\n\nFor 2004, 1500-series Suburbans received the Hydroboost braking system that was previously introduced in the 2500 series.\n\nThe Mexican-market Suburban received a front end update this year, matching that of the Silverado.\n\nThe 2005 model year saw the long-standard side-hinged panel doors discontinued in favor of the formerly optional liftgate. All engines switched to an all-electric cooling system to reduce power loss and fuel consumption.\n\nThe Z71 package, long exclusive to 4WD models, became available on 2WD Suburbans. OnStar also became standard across the board.\n\nFinally, Stabilitrak became standard on all models shortly after the start of the model year.\n\nthe 2005 Suburban 1500 won the J.D. Power and Associates award for highest initial quality among large SUVs, beating out its rivals the Ford Expedition and Toyota Sequoia.\n\nFor 2006, the GMT800 Suburban's last year, a special LTZ trim package became available, featuring wheels, all-wheel drive, and the LQ4 6.0 L engine of the 2500 series of trucks and SUVs.\n\nThe catalytic converters were relocated closer to the engine. The XM radio antenna and the OnStar antenna were combined into a single unit.\n\nThe 2007 model year Suburban and Yukon XL were unveiled at the 2006 Los Angeles Auto Show in January. Production of the redesigned GMT900 Suburban and Yukon XL began at Janesville Assembly and Silao Assembly in January 2006 (Suburban) April 2006 (Yukon XL), with the vehicles arriving at dealerships in April.\n\nThe new models were redesigned with more modern, less boxy styling, already seen on the previously released 2007 Tahoes and Yukons. The exterior features a more aerodynamic shape, made partly by a steeply raked windshield angle. The new design is more aerodynamic.\n\nThe interior has a redesigned dashboard and improved seats. It still retains its 9-passenger seating availability, which is available on LS and SLE models only. LT2, and LT3 models have leather seating and available 6-, 7-, and 8-passenger seating. There is a Z71 package available on LT2 and LT3 models which includes two-tone leather seats. All Mexican-built Suburbans including the 9-seat models offer the special two-tone leather seating used by the Z71. The Suburban LTZ comes standard with a DVD player, GPS navigation enhanced radio that is touch screen.\n\nFor the 2010 model year, in which U.S. News & World Report ranked it as the number one affordable large SUV, the Suburban added a premium interior package that includes tri-zone climate control and handy features like Bluetooth and rear audio controls. In addition, radios that are standard in all 2010 trims get a USB port, allowing for music to be played from auxiliary devices though the radio, as well as charging other small electronics. Side blind zone alert becomes an option on LT and LTZ. The 6.0 liter engine in the 2010 models will also be flex fuel capable. Minor front end changes including a slightly raised front bumper and side torso air bags were also made standard for 2010.\n\nIn February 2010, Chevrolet unveiled a 75th anniversary edition of the Suburban, which will have the LTZ trim with white diamond tricoat exterior paint and cashmere interior, along with standard 20-inch chrome-clad wheels, revised roof rails, integrated navigation radio, XM Satellite Radio, Bluetooth phone connectivity, rearview camera, rear park assist, remote starting, adjustable pedals, and leather upholstery with heated/cooled front seats. Chevrolet says that the anniversary edition will be limited to 2,570 units because of the amount of white diamond paint GM can procure.\n\nThe 5.3 L and 6.0 L engines carried over, and a new 6.2 L Vortec V8 was added for the Yukon XL Denali. The 8.1 L engine was dropped.\n\nFor the 2011 model year, the Suburban will add three new exterior colors to the lineup: Mocha Steel Metallic, Green Steel Metallic, and Ice Blue Metallic. The trims will also get an updated modification, with the rear audio system, Bluetooth, floor console/storage area, wood grain interior, luggage rack rails, body-color exterior door handle/mirror caps and premium-cloth front bucket seats now standard on the 1LS trim, and chrome recovery hooks, two-speed transfer case, and 20-inch chrome wheels standard on its 1LS 4WD models. In addition, the trailering package will feature the trailer brake controller as a standard on all trims.\n\nFor the 2012 model year, trailer sway control and Hill Start Assist become standard on all trims, while the LTZ trim added a heated steering wheel and Side Blind Zone Alert as a standard. Also, the LT1/2 options for the Suburban and SLE1/2 and SLT1/2 options on the Yukon XL were discontinued, leaving the Suburban with only a LS, LT and LTZ trim and the Yukon XL with a SLE and SLT trim. In 2012, GMC celebrated its 100th anniversary by releasing a special edition of its Yukon XL, offering a Heritage Edition trim package. This would also be the final year that three colors, Graystone Metallic, Gold Mist Metallic and Blue Topaz Metallic, would be offered, along with the all season blackwall P265/65R18 tires.\n\nFor the 2013 model year, two new colors were offered: Champagne Silver Metallic and Blue Ray Metallic (extra charge). Also new is Powertrain Grade Braking, normal mode. The 2013 arrived for Chevrolet dealers in June 2012.\n\nFor the 2014 model year, power-adjustable pedals, remote vehicle starter system and rear parking assist along with rear vision camera and inside mirror with camera display will become standard on the Suburban LS trims. In addition, Concord Metallic (which was supposed to be available for the 2013 MY) will be added to the Suburban color offerings for the 2014 models. For the Yukon XL, a convenience package will now become standard on its SLE models, along with a new color, Deep Indigo Metallic. In February 2014, The Suburban came in second behind the Tahoe among the top-ranked large affordable SUVs by \"U.S News & World Report.\" This would be followed by being acknowledged as an award recipient in the large SUV category by JD Power and Associates in July 2014.\n\nThe three-quarter ton model's towing capacity is , being one of the best of any 4x4 SUV and Unmatched by any other SUV. The three-quarter ton model also has a GCVW of .\n\nThe 2500 Suburban was originally sourced from Silao, Mexico from 2007 to 2008 but was moved to the Arlington, Texas assembly plant for the 2009 model year, where production of all GM fullsize SUVs was consolidated after the closing of the Janesville plant.\n\nGM discontinued the 2500 3/4 Ton versions of both Suburban and Yukon XL models after the 2013 model year.\n\nThe tenth generation Suburban's design provided the basis for the Declasse Granger and its law enforcement counterparts, a line of fictitious SUVs that are featured in the Grand Theft Auto video game series as of Grand Theft Auto V.\n\nThe eleventh generation Chevrolet Suburban, GMC Yukon XL, and Yukon Denali XL were introduced to the public on September 12, 2013, and GM unveiled the vehicles in different locations (The Suburban in New York, Yukon XL in Los Angeles) on that date. Both vehicles are based on the GMT K2XX platform and will carry unique serial designated numbers, identified by platform (K2), brand (YC for Chevrolet, YG for GMC), drivetrain (C for 2WD; K for 4WD), tonnage (15 for half-ton, 25 for 3/4-ton, 35 for 1-ton), wheelbase (7 for short, 9 for long), and 06 for SUV, which means that a K2YC-K-15-9-06 would be identified as a Chevrolet Suburban 1500 4WD. The Suburban and Yukon XL went on sale in February 2014 as a 2015 model, with the vehicles built exclusively in Arlington, Texas.\n\nThe newly redesigned Suburban and Yukon XL were showcased to the public for the first time on September 27, 2013, at the State Fair of Texas. This move comes on the heels of the 80th anniversary of the first production of the Suburban in 1934.\n\nThe designs and concepts were created by GM's exterior design manager Chip Thole (prior to his transfer to GM's Buick design studio in 2013), who told \"Truck Trend\" \"I start with what intuition tells me about the market and get the team going on that. You look at trends around the industry – fashion, culture, what people are buying, what they say they want now – and project that into the future. The fun part is putting those ideas to paper and going from there.\" He then added \"We wanted to take what was good about today's vehicles, bring that forward and make them new and different with that spark of freshness that people recognize, without making them gimmicky or overdone.\" Thole also challenged his design team to help bring ideas to the SUVs, which lead to the split headlamps and a more graphic feel for the Suburban design, while a more industrial but sculptured look was added to the Yukon XL to give it a unique identity of its own.\nProduction on the Suburban and Tahoe began in December 2013 with the first completed SUVs being used for testing purposes. GM then officially started shipping the vehicles to dealerships on February 5, 2014. It is estimated that it takes 8–10 weeks to assemble the SUVs, save for the upgrades on the level trims and destination scheduling.\n\nChevrolet introduced the designs, choices and build-your-own features for the 2015 models on its website in January 2014 with a set price starting at $47,000 (2WD) to $50,000 (4WD) for the LS trim, $52,000 (2WD) to $55,000 (4WD) for the LT trim, and $61,000 (2WD) to $65,000 (4WD) for the LTZ trim. The pricing on GMC's Yukon XL's are set at around $49,000 (2WD) to $52,000 (4WD) for the SLE, $57,000 (2WD) to $60,000 (4WD) for the SLT, and $65,000 (2WD) to $68,000 (4WD) for the Denali trims.\n\nThe front fascias of the Chevy Suburban and GMC Yukon XL are distinct, but from the base of the A-pillars back, they share most of the same styling cues. This now includes inlaid doors that tuck into the door sills, instead of over them, improving aerodynamics, fuel economy, and lessens interior noise. The hoods and liftgate panels are made of aluminum in an effort to reduce vehicle weight, and the wiper blades that was located on the liftgate door is moved to the rear spoiler located on the top of the rear liftgate window. Also noticeable is the SUV's length, which expands from (the Yukon XL's length is shorter at ) and its width from , while the height decreases from , thus allowing the vehicle to become slightly leaner, a little bit wider, more streamlined, and roomier.\n\nA more-efficient, direct-injected EcoTec3 V8 powertrain (5.3 for the Suburban, 6.2 for Yukon XL/Yukon Denali XL) coupled with improved aerodynamics, helped the SUVs offer greater estimated highway fuel economy and improving its MPG and CAFE estimates to 16MPG (City)/23MPG (Highway)/18 (Combined) for 2WD, and 15MPG (City)/22MPG (Highway)/18 (Combined) for 4WD. The increased MPGs also vaulted the Suburban/Yukon XL into the top spot among large SUVs with the most efficient MPG rating numbers for this segment. However, when \"Motor Trend\" (which placed the 2015 Suburban on the front cover of its June 2014 issue) did a road test review on the SUVs, it estimated the 4WD MPG on the Suburban LTZ to be slightly better at around 15.2 City and 22.3 Highway, while the 4WD Yukon Denali XL, whose MPG is rated at 14 City/20 Highway, was estimated lower at 12.4 City and 19.2 Highway.\n\nLike the 2007–14 version, both the Suburban and Yukon XL do not share a single piece of sheetmetal or lighting element with the brands' full-size pick-up trucks (GMC Sierra and Chevy Silverado), and the front grilles of both vehicles are slightly altered to give it their own identity. The front headlights features projector-beam headlamps that flanks the Chevrolet-signature dual-port grille – chrome on all models, sweeping into the front fenders, while Tahoe and Suburban LTZ and Yukon and Yukon XL Denali trims feature projector beam high-intensity discharge headlamps and light-emitting diode daytime running lamps. The Yukon and Yukon XL also feature projector-beam halogen headlamps on all SLE and SLT trims. The improved safety features included a 360-degree radar detection for crash avoidance and occupant protection and a high-tech anti-theft system that now includes vertical and interior sensors, in-glass and window breaking, a triggering alarm and a shutdown device that prevents the vehicle from moving. The latter is expected to address the issues regarding the constant thefts of the vehicles, especially with the previous generation's removable seats and items left in the cargo space, which has become a target for carjackers who see the third row seats as valuable on the black market. According to General Motors' head of Global Vehicle Security Bill Biondo, \"We have engineered a layered approach to vehicle security,\" adding that \"With new standard features and the available theft protection package, we are making the vehicles less attractive target to thieves and more secure for our customers.\"\n\nAlso new are the addition of fold-flat second and third-row seats (replacing the aforementioned removable third seats), which is now a standard feature but can be equipped with an optional power-folding feature for the upgraded trims, and an additional two inches of leg room for second-row passengers. HD radio became a standard feature on all trims. Multiple USB ports and power outlets are now spread throughout their interiors, including one 110-volt, three-prong outlet on both Suburban and Yukon XL, with the Suburban adding an available eight-inch color touch screen radio with next-generation MyLink connectivity along with an available rear-seat entertainment system with dual screens and Blu-ray DVD player, while the Yukon XL adds a standard eight-inch-diagonal color touch screen radio with enhanced IntelliLink and available navigation. A 4G LTE WiFi access system, along with Siri Eyes Free and text messaging alerts, was included into all vehicles that feature the OnStar device around the second quarter of 2014.\n\nThe Yukon XL interior has more additional features that includes seats stuffed with dual-firmness foam, a standard Bose sound system and SD card slots, and laminated glass for the windshield and front windows, decreasing interior noise. The Denali Yukon XLs comes equipped with active noise-cancellation technology, with GM's third-generation magnetic ride control suspension as a standard feature, which is only featured on the Suburban LTZ models, whose upgraded features also includes a real-time damping system that delivers more precise body motion control by \"reading\" the road every millisecond, and changing damping in just five milliseconds.\n\nThe Suburban will have eight color palettes to choose from for the 2015 model year: Champagne Silver Metallic, Silver Ice Metallic, White Diamond Tricoat, Sable Metallic, Crystal Red Tricoat, Summit White, Tungsten Metallic, and Black. The GMC Yukon XL palettes will be available in nine colors: Onyx Black, Summit White, Quicksilver Metallic, Champagne Silver Metallic, Iridium Metallic, Bronze Alloy Metallic, Crystal Red Tintcoat, White Diamond Tricoat, and Midnight Amethyst Metallic. The latter three will be the most expensive color trim options.\n\nFor Chevrolet Suburban\n\nThe Suburban received a new color palette, Brownstone Metallic, and added a hands-free power liftgate feature that is standard on LTZ, but included on LT with the optional Luxury Package. The added 4G LTE WiFi and Siri features became standard on both LT and LTZ trims, while the MyLink with Navigation feature was upgraded from optional to standard on the LTZ trim. The E85 capability feature is removed from retail orders.\n\nThe Yukon XL, in addition to receiving the aforementioned features, sees the 6.2-liter EcoTec3 V8 engine being updated with the new 8L90E eight-speed automatic transmission for the interim model year, allowing it to improve fuel economy. The Yukon XL Denali however saw its MSRP bumped up by $1,300 in part due to the loaded features.\n\nAll GM Full-sized SUVs received a body colored painted Shark Fin antenna as part of the 2015 mid-year refresh.\n\nFor the 2016 model year (which commenced sales in July 2015), the Chevrolet Suburban received more upgraded changes and new features that included power-adjustable pedals, forward collision alert, IntelliBeam headlamps, lane keep assist and a safety alert seat as part of the newly introduced enhanced driver alert package as an available option on the LS trim. The inside floor console with storage area-SD card reader was removed, while a new AM/FM audio integrated system with Sirius XM, HD Radio, and CD/MP3 capabilities was introduced as a standard feature on all trims; the 8-inch MyLink feature was expanded to the LS trim and became standard (replacing the 4-inch display), although the navigation feature remains as an option on LT and standard on LTZ. A new liftgate shield was added to the Theft Protection Package, along with the new lane keep assist which replaced the lane departure warning. The capless fuel fill tanks became standard on all trims. Siren Red Tintcoat and Iridescent Pearl Tricoat became the new color trims, replacing Crystal Red Tintcoat and White Diamond Tricoat. The instrument cluster was re-configured with a new multi-color enhancement and a heads-up display was introduced as a standard only on the LTZ trim. The 2016 models also saw a price hike as well.\n\nThe 2016 GMC Yukon XL also sees similar changes, with the new enhanced driver alert package as an available option on the SLE trim, liftgate, power, hands-free now packaged on SLT trims, a free-flow feature that replaced the Premium package, a lane keep assist added to all trims, and two new premium colors (Crimson Red Tintcoat and White Frost Tricoat) replacing Crystal Red Tintcoat and White Diamond Tricoat respectively.\n\nChevrolet added Apple CarPlay and Android Auto Capability features to the Suburban starting with the 2016 models. However, only one of their phone brands at any one time can be used, while the Android Auto option will only be available on LT and LTZ trims featuring 8-inch screens.\n\nGM expanded newer 4G LTE features (like detecting battery failure and monitoring insurance discounts based on driver performance) to 2016 model year vehicles, including the Suburban, which GM cited as being the vehicle that is the most used among data subscribers.\n\nThe 2017 model year Chevrolet Suburban received upgraded changes after it went on sale in August 2016. The level trims LS and LT are retained but the LTZ is renamed Premier, the latter serving as the equivalent to the Yukon XL Denali. The LS trims also saw the badged \"LS\" lettering removed. The new features includes two new colors (Blue Velvet Metallic and Pepperdust Metallic), two new 22-inch wheel options (a 7-spoke Silver wheels with Chrome inserts for all trims; Ultra Bright machined aluminum wheels with Bright Silver finish for Premier trim only), black roof rack cross rails (as part of the Texas Edition Package and All-Season Package), front active aero shutters (all trims), and heated and vented seats (Premier trim only). The MyLink was updated to incorporate Teen Driver, App Shop, Rear Seat Reminder customization, and Low Speed Forward Automatic Braking (as part of Enhanced Driver Alert Package on the LS trim, but standard on LT and Premier). The Rear Seat Entertainment System was overhauled to include a new video voiceover feature for the visually and hearing impaired, a HDMI/MHL connector, digital headphones, Digital Living Network Alliance (DLNA) technology incorporated into the Wi-Fi system, and a 2nd USB port with capability of charging up to a 2.1-amp at the back of the console.\n\nThe 2017 model year Yukon XL also received similar changes, but with a few exceptions. Two new colors, \"Dark Blue Sapphire Metallic\" and \"Mineral Metallic\" were introduced, the latter exclusive to the Denali, which ialso added a new 22-inch ultra bright aluminum wheels with midnight silver premium paint and a head-up display to its features. The interior backlights changed from red to blue. The heated and vented driver and front passenger seats are now standard on the SLT and Denali trims.\n\nThe 2018 model year Suburban had a few upgrades and deletions. The LED daytime running lights became standard on all trims, along with a new color, havana brown and satin steel metallic. The cocoa/mahogany interior that was combined with the pepperdust metallic exterior was dropped along with the wireless/inductive phone charging that was part of the Luxury and Texas Edition LT level trims. The fleet/commercial level trim, which had fewer features, is upgraded to include MyLink, HD Radio, multi-color driver information center, LED daytime running lights, and an optional driver alert package.\n\nThe 2018 Yukon Denali XL received a new grille with a layered appearance like the ones on its redesigned 2018 Acadia and Terrain brands, featuring high-intensity-discharge headlights and LED daytime running lights. The refreshed design provides better airflow to the radiator, and when less cooling air is needed, shutters behind the grille close to improve aerodynamics and increase efficiency. The interior featured new ash wood trim that GMC says gives the cabin a richer appearance. A new 10-speed automatic transmission was mated to its 420-horsepower, 6.2-liter V-8 engine, replacing the 8-speed transmission.\n\nThe 2019 Suburban will have both Havana Metallic and Tungsten Metallic deleted in favor of a new Shadow Gray Metallic exterior color, while the top of the line Premier will now have the name displayed on the tailgate. The LS trim continues to make HD Radio a standard feature, but can be deleted if customers opt for the OnStar feature.\n\nThe 2019 model year GMC Yukon XL adds three new exterior colors, Dark Sky Metallic, Pepperdust Metallic, and Smokey Quartz Metallic, while deleting two others, Mineral Metallic and Iridium Metallic. GMC also introduces two new package features, Graphite Edition and Graphite Performance Edition, which will be available in the SLT level trim only\n\nThere has been industry speculation that the 2020 model will refine the aerodynamics by introducing front air curtains to improve efficiency and reduce drag.\n\nAs of the 2018 model year, Chevrolet began offering “Signature Edition” package/trims of the Suburban. Each of the specialty versions are available at the LT and Premier level trims.\n\nOn September 26, 2014, Chevrolet debuted the updated Z71 Suburban at the State Fair of Texas, along with the debut of the Texas Edition Suburban, the latter due to Texas having the largest units of Suburbans sold in the United States (As of August 2014, sales of the Chevrolet SUVs in Texas were up 37 percent) and to celebrate the 60th anniversary of GM's Arlington Assembly plant; production of the Z71 Suburban began in October 2014.\n\nAs with the previous Z71 Suburbans, this version continued to be offered in a 4WD LT trim only, featuring a front skid plate, off-road tires mounted on 18-inch wheels, a unique grille, running boards and \"Z71\" identification inside and out. Fog lamps, front tow hooks and front parking assist are also included. The 2016 Z71 package was modified again, as portions of the Z71 items added on the Texas Edition package as an optional feature by request from customers was discontinued, making it a stand-alone package.\n\nThe Texas Edition Suburban, which became part of the Texas Edition lineup along with the Tahoe and Silverado, was available in both LT and LTZ trims for the 2015 model year, featuring a maximum trailering package, twenty-inch polished aluminum wheels (on LT models), twenty-two-inch premium painted aluminum wheels (on LTZ models), and an exclusive \"Texas Edition\" badge. For the 2016 model year, Chevrolet discontinued the LTZ package but modified for the LT package without the requested Z71 features.\n\nThe Suburban LT Signature Edition is an optional package trim, similar to the luxury package but with less expensive features, available in the LT level trim only.\n\nA Midnight Edition Suburban, an all-black optional package available in 4WD LT and Z71 versions, was introduced for the 2017 model year. This version became part of the Signature Edition Suburban feature for the 2018 model year.\n\nOn August 13, 2018, Chevrolet introduced a more upgraded version of the top of the line Premier level trim of the Suburban, packaged as the Premier Plus, featuring a 6.2L engine, the classic gold bowtie emblems, chrome nameplate badging, and new polished 22-inch wheels. The interior is unique to these models, boasting with heated/ventilated Black-and-Mahogany leather front seats, Jet Black trim surround, a head-up display, and an eight-inch cluster. The exterior features standard cross rails, chrome power steps, and chrome exhaust tips.\n\nOn April 6, 2017, Chevrolet announced that it will add a new package to the 2018 model year Suburban with the introduction of the street-themed RST (Rally Sport Truck) Special Edition Suburban. Originally at the time of the announcement, it was supposed to be available as an option for the LT and Premier trims as a Performance Package that included a 420-hp, 6.2L V-8 engine, a Magnetic Ride Control with performance calibration, and an all-new Hydra-Matic 10L80 10-speed automatic transmission. The press release also detailed additional features; The chrome elements are absent as body-color grilles surround the door and handles, along with an added gloss-black grille and mirror caps, black roof rails, window trim, badging and Chevy bowties, an exclusive 22-inch wheels wrapped in Bridgestone P285/45R 22 tires, a Borla performance exhaust system, massive front red Brembo six-piston, fixed aluminum calipers with brake pads clamping on larger-than-stock Duralife rotors, coupled with an 84 percent increase in brake pad area and a 42 percent increase in rotor area to increase system thermal capacity. \n\nHowever, after the press release Chevrolet confirmed that the RST Suburban would only be available as an appearance package, as the 6.2L engine would not be used for the 2018 model year. But on May 4, 2018, Chevrolet expanded the RST package to the Suburban as an option for the 2019 model year that will now include the 10-speed automatic transmission and 6.2L engine, reversing a decision made by GM that it would be exclusive to GMC and Cadillac's Suburban siblings Yukon XL and Escalade ESV. The packaged trim went on sale in July 2018.\n\nA 2500HD Suburban became available in 2015 as a 2016 model. However, this version, identified as a Class 3 vehicle (around 10,000 to 14,000 pound GVWR), is only available for sale to rental companies and commercial fleet entities as a 4WD vehicle and will use the same design as the eleventh generation version, but uses a different engine altogether. The 2500HD Suburban is only offered in LS and LT trims, reserving the LTZ trim for a more luxurious offering (like limousine or taxi-related services). There is no 2WD or upgraded packages. In addition, there are no plans to offer a 3/4-ton GMC Yukon XL as GM is making the design exclusive to Chevrolet for the time being.\n\nA one-ton \"Suburban 3500HD,\" which would be a GM-unique SUV, was introduced for the 2016 model year. This is government-exclusive and available in 4WD LS and LT trims only, weighing around 11,000 pound GVWR, 17-inch machined aluminum wheels (8 lug), a high capacity air cleaner, 220 amp alternator, external engine oil cooler and auxiliary transmission cooler. Although the vehicle has a high GVWR, it is configured to provide a minimal towing capacity and is primarily intended for conversion to an armored vehicle. This version does not include HD radio, nor is there a GMC Yukon XL equivalent, making this version exclusive to Chevrolet as well.\n\nThere is no diesel variant of the full-size SUVs for the 2015 model year. GM offered another off-road variant or trim for the Suburban and Yukon XL starting with the eleventh generation. With the relaunch of the Z71 package in the Suburban, GM filed papers to trade mark the Trail Boss name for use on future Chevrolet truck and SUV models.\n\nSafety and fuel economy were the reasons for GMC to no longer offer an AWD in the Yukon Denali XL, and there are no volume controls or devices in the rear seating area to operate the DVD system.\n\nOn November 4, 2013, Chevrolet unveiled a concept design for the Suburban called the Half-Pipe, whose features include roof racks and crossbars to mount skis, at the SEMA Show. Afterwards, Chevrolet made its parts available for order at Chevrolet Accessories. \n\nAt the 2017 SEMA Show, a 4x4 off-road Suburban concept inspired by Country singer (and Chevrolet spokesman) Luke Bryan based around his 2016 song “Huntin', Fishin' and Lovin' Every Day” was introduced, decked out in custom Hunter Bronze exterior color with Dark Carbon accents and camo graphics, a roof-mounted light rack and a custom lower fascia, a liftgate re-engineered to swing outward rather than upward and incorporates a custom spare tire mount, 22-inch wheels with 35-inch-tall tires, roof-mounted equipment carrier with a fishing rod holder, an interior design consisting of Black and Two-Tone Olive with Argon Orange, unique seating featuring Argon piping and Platinum camo-pattern perforated designs, and the “Huntin’, Fishin’ and Lovin’ Every Day” badges (designated by symbols in lieu of the words) displayed on both the exterior and interior. The rear windows are removed. The concept was inspired by Bryan himself and collaborated with Chevrolet on this project as he is an owner of a Suburban: “Chevy has been part of our family and a part of our work life on the farm for as long as I can remember,” says Bryan. “If you were a Bryan, you drove a Chevy — and I’m a longtime Suburban owner. This partnership is a natural fit for me and this unique Suburban represents everything I and my family want for our outdoor adventures.”\nOn February 12, 2014, Callaway Cars announced the addition of an upgraded supercharged package to the Suburban for an additional $17,000.\n\nOn September 25, 2015, Corsa Performance added a cat-back exhaust system package for customers looking to upgrade their K2XX Suburban or Yukon XL, which would give the 5.3L versions a 669% flow increase and the 6.2L versions a 342% flow increase.\n\nOn June 11, 2018, Freedom Mobility announced that it partnered with GM to have their trucks and SUVs equipped with features that will make it accessible to disabled and visible/hearing impaired drivers or passengers. The Suburban is among the selected vehicles that will be available with this option.\n\nThe news of the redesigned Suburban and Yukon XL have so far been met with positive reviews from automotive critics who praised GM's devotion and commitment to keeping the Suburban and its sister SUVs in production due to their popularity and sales.\n\nA test drive that was done on February 25, 2014 received positive reviews for its speed, handling, technology, and performance from The Fast Lane, and an overall 9 rating (out of 10) from Canada's Autonet.ca. \"Truck Trend\" quoted that they never really thought of the Suburban and Yukon XL as \"one-percent\" vehicles (referring to their previous generations and its upper-class and family-oriented customers), they're now solidly \"ten-percenters\" with the updated versions.\n\nHowever at the same time it has seen mixed comments from message boards like Autoblog and Chevrolet's Facebook page, mainly about the front grille, its boxy design, GM's decision to not include additional or optional features to the Suburban, like a Duramax engine, and the conspicuous absence of the \"2500\" (or 3/4 ton) version in the current production lineup or making it available for sale to the public. In a review from Automobile Magazine, writer Greg Migliore notes that despite the reception from both sides and the possibility that other automakers might exit the large SUV segment, that with this 12th generation, \"GM is clearly hoping the Suburban makes it to the century mark.\" \"The Detroit News\" gave the 2016 Suburban LTZ a 4 star (excellent) rating, this after comparing the SUV with the Fiat 500L in an experimental test inspired by Pope Francis's 2015 visit to North America, in which the Pontiff used the 500L, which is the equivalent of the President of the United States' armored Suburban The 2016 model also got a workout of sorts at the Florida International Rally & Motorsports Park to test its abilities on the racetrack, where it performed successfully.\n\nThe 2015 Suburban was ranked third among the top affordable SUVs and fifth among affordable SUVs with 3-row seats by \"U.S. News & World Report\", and was among the finalists in \"Motor Trend's\" SUV of the year for 2015. It also received a 2015 MotorWeek Drivers' Choice Award for best large SUV, while \"Consumer Reports\" ranked the Suburban as the best SUV with a third row seat, topping its competitors in this category.\n\nOn May 31, 2016, the Chevrolet Suburban took third place behind the Tahoe and GMC Yukon in the 2016 J.D. Power Vehicle Dependability Study among full-size SUVs, based on responses from owners of the vehicles.\n\nIn February 2018, Good Housekeeping named the 2018 Suburban the “Best New Car of Year 2018” in the Large SUV category. In its review of the vehicle from GH and its reason for the decision to award the SUV: “Whether you're bringing the whole soccer team home or moving kids into college, it's a larger-than-life workhorse that can handle your crew and all their stuff.”\n\n\"Consumer Reports\" added the 2018 Suburban to their recommended list due to high owner satisfaction scores. \n\nThe interest in the redesigned eleventh generation Suburban has also translated into surging sales. In April 2014 it saw a 109.8 percent spike with most dealers reporting that the vehicles are being sold within 10 days after it arrives on the dealership lots with customers opting for the fully loaded LTZ model, making it one of Chevrolet's fastest selling brands in 2014. The eleventh generation Suburban is also a hot seller in the Middle East, where in August 2014 posted a 37% increase in sales, with most of the purchases coming from Saudi Arabia (65%), the United Arab Emirates (15%) and Qatar (108%). By the end of September 2014, GM sold more than 4101 units of the Suburban (up 50.1%), while the Yukon XL posted 2165 units sold (up 64%), with GM boasting that 80% of the vehicles sold were its large SUVs.\n\nIn Canada, sales of the Suburban reached 966 units (up 43%) in 2014, although the Yukon XL is the best seller in that country with 1,760 vehicles sold (up 50.3%) that same year. Overall, they account for 70 percent of GM Canada's SUV sales for 2014.\n\nOn August 17, 2015, GM confirmed plans to increase production on its large SUVs, especially on the Suburban/Yukon XL, due to lower gas prices and a higher demand for the vehicles. The move also resulted in its Arlington Assembly adding more hours and increasing its production from 48,000 SUVs to 60,000 based on the expanding hours and added Saturday overtime shifts.\n\nThe 2017 MY Suburban saw its biggest sales increase in January 2017, when it posted a 72.3% gain (5,634 units), the most ever since January 2008 when it had the 2008 MY Suburban.\n\nWhen production of the CUCV II ended in 2000, GM redesigned it to coincide with civilian truck offerings. The CUCV nomenclature was changed to Light Service Support Vehicle in 2001. In 2005, LSSV production switched to AM General, a unit of MacAndrews and Forbes Holdings. The LSSV is a GM-built Chevrolet Silverado 1500, Chevrolet Silverado 2500 HD, Chevrolet Tahoe, or Chevrolet Suburban that is powered by a Duramax 6.6-liter turbo diesel engine. As GM has periodically redesigned its civilian trucks and SUVs from 2001 to the present, LSSVs have also been updated cosmetically.\n\nThe militarization of the standard GM trucks/SUVs to become LSSVs includes exterior changes such as CARC paint (Forest Green, Desert Sand, or 3-color Camouflage), blackout lights, military bumpers, a brush guard, a NATO slave receptacle/NATO trailer receptacle, a pintle hook, tow shackles and a 24/12 volt electrical system. The dashboard has additional controls and dataplates. The truck also can be equipped with weapon supports in the cab, cargo tie down hooks, folding troop seats, pioneer tools, winches, and other military accessories. In the Canadian Army these vehicles are nicknamed \"Milverado\".\n\nThe Enhanced Mobility Package (EMP) option adds an uprated suspension, 4-wheel anti-lock brakes, a locking rear differential, beadlock tires, a tire pressure monitoring system and other upgrades. About 2,000 LSSV units have been sold to U.S. and international military and law enforcement organizations.\n\nFor the 2009 model year, the Suburban received the U.S. National Highway Traffic Safety Administration's (NHTSA) best rating of 5 stars in the frontal driver/passenger and side driver/passenger categories.\n\nNHTSA Chevrolet Suburban crash test results (For 2009 models):\n\nFor the 2015 model year, the Suburban received an NHTSA rating of 4 stars overall, with the side driver/passenger categories receiving 5 stars. (Because of more stringent tests, 2011 and newer model ratings are not comparable to 1990–2010 ratings.) The NHTSA gave the 2016 models 4 stars overall in its review, similar to its review of its 2015 models.\n\nNHTSA Chevrolet Suburban crash test results (For 2015 models):\n\nIt gave the Suburban an Acceptable rating along with the Tahoe and Yukon. The Escalade also got an Acceptable rating.\n\nThe Suburban, Tahoe, GMC Yukon and Escalade a Good rating and a Best Pick. However, all four including the Yukon XL got a Good rating for head protection in side and seats and head restraints.\n\nThe Suburban, Tahoe, Yukon and Escalade earned a Good rating in the front offset. However, 2007–09 models without side airbags got a Poor rating. Models with them got a Marginal rating. 2010–14 models got a Top Safety pick.\n\nAll of the K2XX SUVs received a top safety pick, except for the Chevrolet / GMC 1500 series pickups, which have not been tested. The pickups have been tested against the moderate overlap test, for which they received a good rating.\n\nOn March 28, 2014, GM announced a recall on the 2015 Suburban and Yukon XL in order to fix a \"transmission oil cooler line that is not securely seated in its fitting,\" causing the vehicle to stop and rupture the oil cooling line, resulting in the engine to malfunction and catch fire immediately. The move comes on the heels on an incident that happened on March 23, 2014, when a 2015 GMC Yukon caught fire in Anaheim, California during a test drive. Despite being an isolated incident, the 2015 Suburban and Yukon XL are not tied to GM's announced recall of its vehicles (from previous generation models and discontinued brands that were produced prior to, during, and after GM's restructuring in 2010) that was made on March 17, 2014.\n\nOn June 6, 2014, GM issued another recall on the 2015 Suburban and Yukon XL because their radio control modules may not work, and thus prevent certain audible safety warnings. This would be followed on June 27 with another recall, in which the transfer case \"may electronically switch to neutral without input from the driver,\" adding that if this occurs while the vehicle is moving, power will not be sent to the wheels, meaning that if the vehicle is parked, it may roll away unexpectedly if the parking brake has not been set.\n\nOn December 5, 2014, GM announced that it is replacing ignition key units on the 2015 Suburban and Yukon XL after customers made complaints that the shift lever strikes the head of the key if the tilt-adjustable steering column is in the fully up position. The lever only can be moved out of \"park\" into a gear when the engine is running and the driver's foot is on the brake. The push-button ignition features are not affected.\n\nOn January 4, 2015, GM issued a recall on tenth-generation Suburbans and Yukon XLs from the 2011 and 2012 model years for a potential ignition lock actuator issue, citing that they are not the right size and can cause the ignition to get stuck in the \"Start\" position, and then either due to a jarring event or a \"cool interior temperature\" the ignition could switch back to the \"Accessory\" position, resulting in a loss of power assistance and prevent the airbags from deploying.\n\nOn December 30, 2015, the NHTSA revealed that it had received complaints from 2015 Suburban/Yukon XL owners about buffeting and vibration problems causing drivers and passengers to endure an annoying vibration inside the cabin so severe that it leads to dizziness and headaches. GM is aware of this and has moved to correct the situation and try to pinpoint the source, but assures the vehicles are safe to drive.\n\nOn May 3, 2016, GM placed a recall on both 2016 model Suburbans and Yukon XLs over inadequate welds on their upper front control arms, which could result in an accident or injury.\n\nOn February 4, 2017, GM issued a recall on 2016 and 2017 Chevrolet Suburban HD models over an improperly fixed right-hand rear-view outside mirror which GM says will replaced for free. The recall affected 211 vehicles.\n\nBeginning in 1999, GM re-branded the GMC Suburban as the Yukon XL.\n\n"}
{"id": "23375296", "url": "https://en.wikipedia.org/wiki?curid=23375296", "title": "Deflagration to detonation transition", "text": "Deflagration to detonation transition\n\nDeflagration to detonation transition (DDT) refers to a phenomenon in ignitable mixtures of a flammable gas and air (or oxygen) when a sudden transition takes place from a deflagration type of combustion to a detonation type of explosion.\n\nA deflagration is characterized by a subsonic flame propagation velocity, typically far below , and relatively modest overpressures, say below . The main mechanism of combustion propagation is of a flame front that moves forward through the gas mixture - in technical terms the reaction zone (chemical combustion) progresses through the medium by processes of diffusion of heat and mass. In its most benign form, a deflagration may simply be a flash fire.\n\nIn contrast, a detonation is characterized by supersonic flame propagation velocities, perhaps up to , and substantial overpressures, up to . The main mechanism of detonation propagation is of a powerful pressure wave that compresses the unburnt gas ahead of the wave to a temperature above the autoignition temperature. In technical terms, the reaction zone (chemical combustion) is a self-driven shock wave where the reaction zone and the shock are coincident, and the chemical reaction is initiated by the compressive heating caused by the shock wave. The process is similar to ignition in a Diesel engine, but much more sudden and violent.\n\nUnder certain conditions, mainly in terms of geometrical conditions (such as partial confinement and many obstacles in the flame path that cause turbulent flame eddy currents), a subsonic flame may accelerate to supersonic speed, transitioning from deflagration to detonation. The exact mechanism is not fully understood,\nand while existing theories are able to explain and model both deflagrations and detonations, there is no theory at present which can predict the transition phenomenon.\n\nA deflagration to detonation transition has been a feature of several major industrial accidents:\n\nThe phenomenon is exploited in pulse detonation engines, because a detonation produces a more efficient combustion of the reactants than a deflagration does, i.e. giving a higher yields. Such engines typically employ a Shchelkin spiral in the combustion chamber to facilitate the deflagration to detonation transition.\n\nThe mechanism has also found military use in thermobaric weapons.\n\nAn analogous deflagration to detonation transition (DDT) has also been proposed for thermonuclear reactions responsible for supernovae initiation. This process has been called a \"carbon detonation\".\n\n"}
{"id": "1881464", "url": "https://en.wikipedia.org/wiki?curid=1881464", "title": "Dental composite", "text": "Dental composite\n\nDental composite resins (better referred to as \"resin-based composites\" or simply \"filled resins\") are types of synthetic resins which are used in dentistry as restorative material or adhesives. Dental composite resins have certain properties that will benefit patients according to the patient's cavity. It has a micro-mechanic property that makes composite more effective for filling small cavities where amalgam fillings are not as effective and could therefore fall out (due to the macro-mechanic property of amalgam). Synthetic resins evolved as restorative materials since they were insoluble, of good tooth-like appearance, insensitive to dehydration, easy to manipulate and reasonably inexpensive. Composite resins are most commonly composed of Bis-GMA and other dimethacrylate monomers (TEGMA, UDMA, HDDMA), a filler material such as silica and in most current applications, a photoinitiator. Dimethylglyoxime is also commonly added to achieve certain physical properties such as flow-ability. Further tailoring of physical properties is achieved by formulating unique concentrations of each constituent.\n\nMany studies have compared the longevity of resin-based composite restorations to the longevity of silver-mercury amalgam restorations. Depending on the skill of the dentist, patient characteristics and the type and location of damage, composite restorations can have similar longevity to amalgam restorations. (See Longevity and clinical performance.) In comparison to amalgam, the appearance of resin-based composite restorations is far superior.\n\nTraditionally resin-based composites set by a chemical setting reaction through polymerization between two pastes. One paste containing an activator (not a tertiary amine, as these cause discolouration) and the other containing an initiator (benzoyl peroxide). To overcome the disadvantages of this method, such as a short working time, light-curing resin composites were introduced in the 1970s. The first light-curing units used ultra-violet light to set the material, however this method had a limited curing depth and was a high risk to patients and clinicians. Therefore, UV light-curing units were later replaced by visible light-curing systems which used Camphorquinone as a light source and overcame the issues produced by the UV light-curing units.\n\nThe Traditional Period\n\nIn the late 1960s, composite resins were introduced as an alternative to silicates and unfulfilled resins, which were frequently used by clinicians at the time. Composite resins displayed superior qualities, in that they had better mechanical properties than silicates and unfulfilled resins. Composite resins were also seen to be beneficial in that the resin would be presented in paste form and, with convenient pressure or bulk insertion technique, would facilitate clinical handling. The faults with composite resins at this time were that they had poor appearance, poor marginal adaptation, difficulties with polishing, difficulty with adhesion to the tooth surface, and occasionally, loss of anatomical form.\n\nThe Microfilled Period\n\nIn 1978, various microfilled systems were introduced into the European market. These composite resins were appealing, in that they were capable of having an extremely smooth surface when finished. These microfilled composite resins also showed a better clinical colour stability and higher resistance to wear than conventional composites, which favoured their tooth tissue-like appearance as well as clinical effectiveness. However, further research showed a progressive weakness in the material over time, leading to micro-cracks and step-like material loss around the composite margin. In 1981, microfilled composites were improved remarkably with regard to marginal retention and adaptation. It was decided, after further research, that this type of composite could be used for most restorations provided the acid etch technique was used and a bonding agent was applied.\n\nThe Hybrid Period\n\nHybrid composites were introduced in the 1980s and are more commonly known as resin-modified glass ionomer cements (RMGICs). The material consists of a powder containing a radio-opaque fluoroaluminosilicate glass and a photoactive liquid contained in a dark bottle or capsule. The material was introduced, as resin composites on their own were not suitable for Class II cavities. RMGICs can be used instead. This mixture or resin and glass ionomer allows the material to be set by light activation (resin), allowing a longer working time. It also has the benefit of the glass ionomer component releasing fluoride and has superior adhesive properties. RMGICs are now recommended over traditional GICs for basing cavities. There is a great difference between the early and new hybrid composites.\n\nInitially, resin-based composite restorations in dentistry were very prone to leakage and breakage due to weak compressive strength. In the 1990s and 2000s, such composites were greatly improved and have a compression strength sufficient for use in posterior teeth.\n\nToday's composite resins have low polymerization shrinkage and low coefficients of thermal shrinkage, which allows them to be placed in bulk while maintaining good adaptation to cavity walls. The placement of composite requires meticulous attention to procedure or it may fail prematurely. The tooth must be kept perfectly dry during placement or the resin will likely fail to adhere to the tooth. Composites are placed while still in a soft, dough-like state, but when exposed to light of a certain blue wavelength (typically 470 nm), they polymerize and harden into the solid filling (for more information, see Light activated resin). It is challenging to harden all of the composite, since the light often does not penetrate more than 2–3 mm into the composite. If too thick an amount of composite is placed in the tooth, the composite will remain partially soft, and this soft unpolymerized composite could ultimately lead to leaching of free monomers with potential toxicity and/or leakage of the bonded joint leading to recurring dental pathology. The dentist should place composite in a deep filling in numerous increments, curing each 2–3 mm section fully before adding the next. In addition, the clinician must be careful to adjust the bite of the composite filling, which can be tricky to do. If the filling is too high, even by a subtle amount, that could lead to chewing sensitivity on the tooth. A properly placed composite is comfortable, of good appearance, strong and durable, and could last 10 years or more. (By most North American insurance companies 2 years minimum)\n\nThe most desirable finish surface for a composite resin can be provided by aluminum oxide disks. Classically, Class III composite preparations were required to have retention points placed entirely in dentin. A syringe was used for placing composite resin because the possibility of trapping air in a restoration was minimized. Modern techniques vary, but conventional wisdom states that because there have been great increases in bonding strength due to the use of dentin primers in the late 1990s, physical retention is not needed except for the most extreme of cases. Primers allow the dentin's collagen fibers to be \"sandwiched\" into the resin, resulting in a superior physical and chemical bond of the filling to the tooth. Indeed, composite usage was highly controversial in the dental field until primer technology was standardized in the mid to late 1990s. The enamel margin of a composite resin preparation should be beveled in order to improve the appearance and expose the ends of the enamel rods for acid attack. The correct technique of enamel etching prior to placement of a composite resin restoration includes etching with 30%-50% phosphoric acid and rinsing thoroughly with water and drying with air only. In preparing a cavity for restoration with composite resin combined with an acid etch technique, all enamel cavosurface angles should be obtuse angles. Contraindications for composite include varnish and zinc oxide-eugenol. Composite resins for Class II restorations were not indicated because of excessive occlusal wear in the 1980s and early 1990s. Modern bonding techniques and the increasing unpopularity of amalgam filling material have made composites more attractive for Class II restorations. Opinions vary, but composite is regarded as having adequate longevity and wear characteristics to be used for permanent Class II restorations. Whether composite materials last as long or has the leakage and sensitivity properties when compared to Class II amalgam restorations was described as a matter of debate in 2008.<ref name=\"DOI10.4103/0972-0707.45247\"></ref>\n\nAs with other composite materials, a dental composite typically consists of a resin-based oligomer matrix, such as a bisphenol A-glycidyl methacrylate (BISGMA), urethane dimethacrylate (UDMA) or semi-crystalline polyceram (PEX), and an inorganic filler such as silicon dioxide (silica). Without a filler the resin wears easily, exhibits high shrinkage and is exothermic. Compositions vary widely, with proprietary mixes of resins forming the matrix, as well as engineered filler glasses and glass ceramics. The filler gives the composite greater strength, wear resistance, decreased polymerisation shrinkage, improved translucency, fluorescence and colour, and a reduced exothermic reaction on polymerisation. It also however causes the resin composite to become more brittle with an increased elastic modulus. Glass fillers are found in multiple different compositions allowing an improvement on the optical and mechanical properties of the material. Ceramic fillers include zirconia-silica and zirconium oxide.\n\nMatrices such as BisHPPP and BBP, contained in the universal adehsive BiSGMA, have been demonstrated to increase the cariogenicity of bacteria leading to the occurrence of secondary caries at the composite-dentin interface. BisHPPP and BBP cause an increase of glycosyltransferase in S. mutans bacteria, which results in increased production of sticky glucans that allow S.mutans' adherence to the tooth. This results in a cariogenic biofilms at the interface of composite and tooth.The cariogenic activity of bacteria increases with concentration of the matrix materials. BisHPPP has furthermore been shown to regulate bacterial genes, making bacteria more cariogenic, thus compromising the longevity of composite restorations. Researchers are highlighting the need for new composite materials to be developed which eliminate the cariogenic products currently contained in composite resin and universal adhesives.\n\nA coupling agent such as silane is used to enhance the bond between these two components. An initiator package (such as: camphorquinone (CQ), phenylpropanedione (PPD) or lucirin (TPO)) begins the polymerization reaction of the resins when blue light is applied. Various additives can control the rate of reaction.\n\nResin filler can be made of glasses or ceramics. Glass fillers are usually made of crystalline silica, silicone dioxide, lithium/barium-aluminium glass, and borosilicate glass containing zinc/strontium/lithium. Ceramic fillers are made of zirconia-silica, or zirconium oxide.\n\nFillers can be further subdivided based on their particle size and shapes such as:\n\nMacrofilled fillers have a particle size ranging from 5 - 10 µm. They have good mechanical strength but poor wear resistance. Final restoration is difficult to polish adequately leaving rough surfaces, and therefore this type of resin is plaque retentive.\n\nMicrofilled fillers are made of colloidal silica with a particle size of 0.4 µm. Resin with this type of filler is easier to polish compared to macrofilled. However, its mechanical properties are compromised as filler load is lower than in conventional (only 40-45% by weight). Therefore, it is contraindicated for load-bearing situations, and has poor wear resistance.\n\nHybrid filler contains particles of various sizes with filler load of 75-85% by weight. It was designed to get the benefits of both macrofilled and microfilled fillers. Resins with hybrid filler have reduced thermal expansion and higher mechanical strength. However, it has higher polymerisation shrinkage due to a larger volume of diluent monomer which controls viscosity of resin.\n\nNanofilled composite has a filler particle size of 20-70 nm. Nanoparticles form nanocluster units and act as a single unit. They have high mechanical strength similar to hybrid material, high wear resistance, and are easily polished. However, nanofilled resins are difficult to adapt to the cavity margins due to high volume of filler.\n\nBulk filler is composed of non-agglomerated silica and zirconia particles. It has nanohybrid particles and filler load of 77% by weight. Designed to decrease clinical steps with possibility of light curing through 4-5mm incremental depth, and reduce stress within remaining tooth tissue. Unfortunately, it is not as strong in compression and has decreased wear resistance compared to conventional material. \n\nAdvantages of composites:\n\n\n\nDirect dental composites are placed by the dentist in a clinical setting. Polymerization is accomplished typically with a hand held curing light that emits specific wavelengths keyed to the initiator and catalyst packages involved. When using a curing light, the light should be held as close to the resin surface as possible, a shield should be placed between the light tip and the operator's eyes. Curing time should be increased for darker resin shades. Light cured resins provide denser restoration than self-cured resins because no mixing is required that might introduce air bubble porosity.\n\nDirect dental composites can be used for:\n\nTypes of setting mechanisms:\nChemically cured resin composite is a two-paste system (base and catalyst) which starts to set when the base and the catalyst are mixed together.\n\nLight cured resin composites contains a photo-initiator (e.g. camphorquinone) and an accelerator. The activator present in light activated composite is diethyl-amino-ethyl-methacrylate (amine) or diketone.They interact when exposed to light at wavelength of 400-500 nm, i.e, blue region of the visible light spectrum. The composite sets when it is exposed to light energy at a set wavelength of light. Light cured resin composites are also sensitive to ambient light, and therefore, polymerisation can begin before use of the curing light.\n\nDual cured resin composite contains both photo-initiators and chemical accelerators, allowing the material to set even where there is insufficient light exposure for light curing.\n\nChemical polymerisation inhibitors (e.g. monomethyl ether of hydroquinone) are added to the resin composite to prevent polymerisation of the material during storage, increasing its shelf life.\n\nThis classification divides resin composite into three broad categories based on their handling characteristics:\n\nManufacturers manipulate the handling characteristics by altering the constituents of the material. Generally, the stiffer materials (packable) exhibit a higher filler content whilst fluid materials (flowable) exhibit lower filler loading.\n\nUniversal:\nThis is the traditional presentation of resin composites and performs well in many situations. However, their use is limited in specialised practice where more complex aesthetic treatments are undertaken. Indications include: the restoration of class I, II and III and IV where aesthetics is not paramount, and the repair of non-carious tooth surface loss (NCTSL) lesions. Contraindications include: restoration of ultraconservative cavities, in areas where aesthetics is critical, and where insufficient enamel is available for etching.\n\nFlowable:\nFlowable composites represent a relatively newer subset of resin-based composite material, dating back to the mid-1990s. Compared to universal composite, flowables have a reduced filler content (37–53%) thereby exhibiting ease of handling, lower viscosity, compressive strength, wear resistance and greater polymerisation shrinkage. Due to the poorer mechanical properties, flowable composites should be used with caution in high stress-bearing areas. However, due to its favourable wetting properties, it can adapt intimately to enamel and dentine surfaces. Indications include: restoration of small class I cavities, preventive resin restorations (PRR), fissure sealants, cavity liners, repair of deficient amalgam margins, and class V (abfraction) lesions caused by NCTSL. Contraindications include: in high stress-bearing areas, restoration of large multi-surface cavities, and if effective moisture control is unattainable.\n\nPackable:\nPackable composites were developed to be used in posterior situations. Unlike flowable composite, they exhibit a higher viscosity thereby necessitating greater force upon application to 'pack' the material into the prepared cavity. Their handling characteristics is more similar to dental amalgam, in that greater force is required to condense the material into the cavity. Therefore, they can be thought of as 'tooth-coloured amalgam'. The increased viscosity is achieved by a higher filler content (>60% by volume) – thereby making the material stiffer and more resistant to fracture, two properties that are ideal for materials to be used in the posterior region of the mouth. The disadvantage of the associated increased filler content is the potential risk of introducing voids along the cavity walls and between each layer of material. In order to seal any marginal deficiencies, the use of a single layer of flowable composite at the base of a cavity has been advocated when undertaking Class II posterior composite restorations when using packable composite.\n\nIndirect composite is cured outside the mouth, in a processing unit that is capable of delivering higher intensities and levels of energy than handheld lights can. Indirect composites can have higher filler levels, are cured for longer times and curing shrinkage can be handled in a better way.\nAs a result, they are less prone to shrinkage stress and marginal gaps and have higher levels and depths of cure than direct composites. For example, an entire crown can be cured in a single process cycle in an extra-oral curing unit, compared to a millimeter layer of a filling.\n\nAs a result, full crowns and even bridges (replacing multiple teeth) can be fabricated with these systems.\n\nIndirect dental composites can be used for:\n\nA stronger, tougher and more durable product is expected in principle. But in the case of inlays, not all clinical long-term-studies detect this advantage in clinical practice (see below).\n\nClinical survival of composite restorations placed in posterior teeth are in the range of amalgam restorations, with some studies seeing a slightly lower \nor slightly higher survival time compared to amalgam restorations.\nImprovements in composite technology and application technique make composites a very good alternative to amalgam, while use in large restorations and in cusp capping situations is still debated.\n\nAccording to a 2012 review article by Demarco \"et al.\" covering 34 relevant clinical studies, \"90% of the studies indicated that annual failure rates between 1% and 3% can be achieved with Class I and II posterior [rear tooth] composite restorations depending on the definition of failure, and on several factors such as tooth type and location, operator [dentist], and socioeconomic, demographic, and behavioral elements.\" This compares to a 3% mean annual failure rate reported in a 2004 review article by Manhart \"et al.\" for amalgam restorations in posterior stress-bearing cavities.\n\nThe Demarco review found that the main reasons cited for failure of posterior composite restorations are secondary caries (i.e. cavities which develop subsequent to the restoration), fracture, and patient behavior, notably bruxism (grinding/clenching.) Causes of failure for amalgam restorations reported in the Manhart \"et al.\"review also include secondary caries, fracture (of the amalgam and/or the tooth), as well as cervical overhang and marginal ditching. The Demarco \"et al.\" review of composite restoration studies noted that patient factors affect longevity of restorations: Compared to patients with generally good dental health, patients with poorer dental health (possibly due to poor dental hygiene, diet, genetics, frequency of dental checkups, etc.) experience higher rates of failure of composite restorations due to subsequent decay. Socioeconomic factors also play a role: \"People who had always lived in the poorest stratus [\"sic\"][stratum?] of the population had more restoration failures than those who lived in the richest layer.\"\n\nThe definition of failure applied in clinical studies may affect the reported statistics. Demarco \"et al\" note: \"Failed restorations or restorations presenting small defects are routinely treated by replacement by most clinicians. Because of this, for many years, the replacement of defective restorations has been reported as the most common treatment in general dental practice...\" Demarco \"et al\" observe that when both repaired and replaced restorations were classified as failures in one study, the Annual Failure Rate was 1.9%. However, when repaired restorations were reclassified as successes instead of failures, the AFR decreased to 0.7%. Reclassifying repairable minor defects as successes rather than failures is justifiable: \"When a restoration is replaced, a significant amount of sound tooth structure is removed and the preparation [i.e. hole] is enlarged\". Applying the narrower definition of failure would improve the reported longevity of composite restorations: Composite restorations can often be easily repaired or extended without drilling out and replacing the entire filling. Resin composites will adhere to the tooth and to undamaged prior composite material. In contrast, amalgam fillings are held in place by the shape of the void being filled rather than by adhesion. This means that it is often necessary to drill out and replace an entire amalgam restoration rather than add to the remaining amalgam.\n\nIt might be expected that the costlier indirect technique leads to a higher clinical performance, however this is not seen in all studies. A study conducted over the course of 11 years reports similar failure rates of direct composite fillings and indirect composite inlays. Another study concludes that although there is a lower failure rate of composite inlays it would be insignificant and anyway too small to justify the additional effort of the indirect technique.\nAlso in the case of ceramic inlays a significantly higher survival rate compared to composite direct fillings can not be detected.\n\nIn general, a clear superiority of tooth coloured inlays over composite direct fillings could not be established by current review literature (as of 2013).\n\n"}
{"id": "2000000", "url": "https://en.wikipedia.org/wiki?curid=2000000", "title": "Dimethylsulfoniopropionate", "text": "Dimethylsulfoniopropionate\n\nDimethylsulfoniopropionate (DMSP), is an organosulfur compound with the formula (CH)SCHCHCOO. This zwitterionic metabolite can be found in marine phytoplankton, seaweeds, and some species of terrestrial and aquatic vascular plants. It functions as an osmolyte as well as several other physiological and environmental roles have also been identified. DMSP was first identified in the marine red alga \"Polysiphonia fastigiata\" by Frederick Challenger and Margaret Simpson (later Dr. Whitaker).\n\nIn higher plants, DMSP is biosynthesized from \"S\"-methylmethionine. Two intermediates in this conversion are dimethylsulfoniumpropylamine and dimethylsulfoniumpropionaldehyde. In algae, however, the biosynthesis starts with the removal of the amino group from methionine, rather than from \"S\"-methylmethionine.\n\nDMSP is broken down by marine microbes to form two major volatile sulfur products, each with distinct effects on the environment. One of its breakdown products is methanethiol (CHSH), which is assimilated by bacteria into protein sulfur. Another volatile breakdown product is dimethyl sulfide (CHSCH; DMS). There is evidence that DMS in seawater can be produced by cleavage of dissolved (extracellular) DMSP by the enzyme DMSP-lyase, although many non-marine species of bacteria convert methanethiol to DMS.\n\nDMS is also taken up by marine bacteria, but not as rapidly as methanethiol. Although DMS usually consists of less than 25% of the volatile breakdown products of DMSP, the high reactivity of methanethiol makes the steady-state DMS concentrations in seawater approximately 10 times those of methanethiol (~3 nM vs. ~0.3 nM). Curiously, there have never been any published correlations between the concentrations of DMS and methanethiol. This is probably due to the non-linear abiotic and microbial uptake of methanethiol in seawater, and the comparatively low reactivity of DMS. However, a significant portion of DMS in seawater is oxidized to dimethyl sulfoxide (DMSO).\n\nRelevant to global climate, DMS is thought to play a role in the Earth's heat budget by decreasing the amount of solar radiation that reaches the Earth's surface. This occurs through degradation of DMS in the atmosphere into hygroscopic compounds that condense water vapor leading to the formation of clouds.\n\nDMSP has also been implicated in influencing the taste and odour characteristics of various products. For example, although DMSP is odourless and tasteless, it is accumulated at high levels in some marine herbivores or filter feeders. Increased growth rates, vigour and stress resistance among animals cultivated on such diets have been reported. DMS, is responsible for repellent, 'off' tastes and odours that develop in some seafood products because of the action of bacterial DMSP-lyase, which cogenerates acrylate.\n\n\n"}
{"id": "530768", "url": "https://en.wikipedia.org/wiki?curid=530768", "title": "Earth leakage circuit breaker", "text": "Earth leakage circuit breaker\n\nAn Earth-leakage circuit breaker (ELCB) is a safety device used in electrical installations with high Earth impedance to prevent shock. It detects small stray voltages on the metal enclosures of electrical equipment, and interrupts the circuit if a dangerous voltage is detected. Once widely used, more recent installations instead use residual current circuit breakers which instead detect leakage current directly.\n\nThe main purpose of Earth leakage protectors is to prevent injury to humans and animals due to electric shock.\n\nThis is a category of devices, which are used to protect instruments, circuits and operators, while Earth leakage. Early ELCBs are voltage sensing devices, which are now replaced by current sensing devices (RCD/RCCB). Usually voltage sensing devices are called ELCB and current sensing devices are called RCCB.\n\nVoltage sensing ELCBs were first introduced about sixty years ago. Current sensing ELCBs were first introduced about forty years ago. For many years, the voltage operated ELCB and the differential current operated ELCB were both referred to as ELCBs because it was a simpler name to remember. But the use of a common name for two different devices gave rise to considerable confusion in the electrical industry. \n\nIf the wrong type was used on an installation, the level of protection given could be substantially less than that intended.\n\nTo eliminate this confusion, IEC decided to apply the term residual current device (RCD) to differential-current-operated ELCBs. Residual current refers to any current over and above the load current...\n\nAn ELCB is a specialised type of latching relay that has a building's incoming mains power connected through its switching contacts so that the ELCB disconnects the power when earth leakage is detected. \n\nThe ELCB detects fault currents from live to the Earth (ground) wire within the installation it protects. If sufficient voltage appears across the ELCB's sense coil, it will switch off the power, and remain off until manually reset. A voltage-sensing ELCB does not sense fault currents from live to any other Earthed body.\n\nThere are two types of Earth-leakage circuit breaker:\n\nVoltage ELCBs have been in widespread use since then, and many are still in operation but are no longer installed in new construction. A voltage-operated ELCB detects a rise in potential between the protected interconnected metalwork (equipment frames, conduits, enclosures) and a distant isolated Earth reference electrode. They operate at a detected potential of around 50 volts to open a main breaker and isolate the supply from the protected premises.\n\nA voltage-operated ELCB has a second terminal for connecting to the remote reference Earth connection.\n\nThe Earth circuit is modified when an ELCB is used; the connection to the Earth rod is passed through the ELCB by connecting to its two Earth terminals. One terminal goes to the installation Earth CPC (circuit protective conductor, aka Earth wire), and the other to the Earth rod (or sometimes other type of Earth connection).\n\nCompared with a current-sensing system, voltage sensing systems have several disadvantages which include:\n\n\nRCD/RCCB is the commonly used ELCB type. An RCCB consists of a 3-winding transformer, which has 2 primary windings and 1 secondary winding. Neutral and line wires act as the two primary windings. A wire wound coil is the secondary winding. The current through the secondary winding is zero at the balanced condition. In balance condition, the flux due to the current through the phase wire will be neutralized by the current through the neutral wire, since the current, which flows from the phase will be returned to the neutral. When a fault occurs, a small current will flow to the ground also. This makes an unbalance between line and neutral current and that creates an unbalanced magnetic field. This induces a current through the secondary winding, which is connected to the sensing circuit. This will sense the leakage and send signal to tripping system.\n\nVoltage sensing ELCBs have one advantage over current sensing RCDs: they are less sensitive to fault conditions, and therefore have fewer nuisance trips. (This does not mean they always do, as practical performance depends on installation details and the discrimination enhancing filtering in the ELCB.) Therefore, by electrically separating cable armour from the cable circuit protective conductor, an ELCB can be arranged to protect against cable damage only, and not trip on faults in downline installations.\n\nVoltage sensing ELCBs have some disadvantages:\n\nIt is not unusual for ELCB protected installation to have a second unintentional connection to Earth somewhere, one that does not pass through the ELCB sense coil. This can occur via metal pipework in contact with the ground, metal structural framework, outdoor home appliances in contact with soil, and so on.\n\nWhen this occurs, fault current may pass to Earth without being sensed by the ELCB. Despite this, perhaps counterintuitively, the operation of the ELCB is not compromised. The purpose of the ELCB is to prevent Earthed metalwork rising to a dangerous voltage during fault conditions, and the ELCB continues to do this just the same, the ELCB will still cut the power at the same CPC voltage level. (The difference is that higher fault current is then needed to reach this voltage.)\n\nWhile voltage and current on the earth line is usually fault current from a live wire, this is not always the case, thus there are situations in which an ELCB can nuisance trip.\n\nWhen an installation has two connections to Earth, a nearby high current lightning strike will cause a voltage gradient in the soil, presenting the ELCB sense coil with enough voltage to cause it to trip.\n\nIf the installation's Earth rod is placed close to the Earth rod of a neighbouring building, a high Earth leakage current in the other building can raise the local ground potential and cause a voltage difference across the two Earths, again tripping the ELCB. Close Earth rods are unsuitable for ELCB use for this reason, but in real life such installations are sometimes encountered.\n\nBoth RCDs and ELCBs are prone to nuisance trips from normal harmless Earth leakage to some degree. On one hand ELCBs are on average older, and hence tend to have less well developed filtering against nuisance trips, and on the other hand ELCBs are inherently immune to some of the causes of false trips RCDs suffer, and are generally less sensitive than RCDs. In practice RCD nuisance trips are much more common.\n\nAnother cause of nuisance tripping is due to accumulated or burden currents caused by items with lowered insulation resistance. This may occur due to older equipment, or equipment with heating elements, or even wiring in buildings in the tropics where prolonged damp and rain conditions can cause the insulation resistance to lower due to moisture tracking. If there is a 30 mA protective device in use and there is a 10 mA burden from various sources then the unit will trip at 20 mA. The individual items may each be electrically safe but a large number of small burden currents accumulates and reduces the tripping level. This was more a problem in past installations where multiple circuits were protected by a single ELCB.\n\nHeating elements of the tubular form are filled with a very fine powder that can absorb moisture if the element has not be used for some time. In the tropics, this may occur, for example if a clothes drier has not been used for a year or a large water boiler used for coffee, etc. has been in storage. In such cases, if the unit is allowed to power up without RCD protection then it will normally dry out and successfully pass inspection. This type of problem can be seen even with brand new equipment.\n\nSome ELCBs do not respond to rectified fault current. This issue is the same in principle with ELCBs and RCDs, but ELCBs are on average much older and specifications have improved considerably over the years, so an old ELCB is more likely to have some fault current waveform that it will not respond to.\n\nWith any mechanical device, failures occur, and ELCBs should ideally be tested periodically to ensure they still work.\n\nIf either of the Earth wires become disconnected from the ELCB, it will no longer trip and the installation will often no longer be properly Earthed.\n\n"}
{"id": "618120", "url": "https://en.wikipedia.org/wiki?curid=618120", "title": "Electric power conversion", "text": "Electric power conversion\n\nIn electrical engineering, power engineering, and the electric power industry, power conversion is converting electric energy from one form to another such as converting between AC and DC; or changing the voltage or frequency; or some combination of these. A power converter is an electrical or electro-mechanical device for converting electrical energy. This could be as simple as a transformer to change the voltage of AC power, but also includes far more complex systems. The term can also refer to a class of electrical machinery that is used to convert one frequency of alternating current into another frequency.\n\nPower conversion systems often incorporate redundancy and voltage regulation.\n\nOne way of classifying power conversion systems is according to whether the input and output are alternating current (AC) or direct current (DC).\n\nThe following devices can convert DC to DC:\n\nThe following devices can convert DC to AC:\n\nThe following devices can convert AC to DC:\n\nThe following devices can convert AC to AC:\n\nThere are also devices and methods to convert between power systems designed for single and three-phase operation.\n\nThe standard power voltage and frequency varies from country to country and sometimes within a country. In North America and northern South America it is usually 120 volt, 60 hertz (Hz), but in Europe, Asia, Africa and many other parts of the world, it is usually 230 volt, 50 Hz. Aircraft often use 400 Hz power internally, so 50 Hz or 60 Hz to 400 Hz frequency conversion is needed for use in the ground power unit used to power the airplane while it is on the ground. Conversely, internal 400 Hz internal power may be converted to 50 Hz or 60 Hz for convenience power outlets available to passengers during flight.\n\nCertain specialized circuits can also be considered power converters, such as the flyback transformer subsystem powering a CRT, generating high voltage at approximately 15 kHz.\n\nConsumer electronics usually include an AC adapter (a type of power supply) to convert mains-voltage AC current to low-voltage DC suitable for consumption by microchips. Consumer voltage converters (also known as \"travel converters\") are used when travelling between countries that use ~120 V versus ~240 V AC mains power. (There are also consumer \"adapters\" which merely form an electrical connection between two differently shaped AC power plugs and sockets, but these change neither voltage nor frequency.)\n\nTransformers are used in power converters to incorporate:\n\nThe secondary circuit is floating, when you touch the secondary circuit, you merely drag its potential to your body potential or the earth potential. There will be no current flowing through your body. That's why you can use your cellphone safely when it is being charged, even if your cellphone has a metal shell and it is connected to the secondary circuit.\n\nOperating at high frequency and supplying low power, power converters have much smaller transformers as compared with those of fundamental frequency, high power applications.\nUsually, in power systems, transformers transmit power simultaneously, no charge!\nThe current in the primary winding of a transformer plays two roles:\n\nFlyback converter's transformer works differently, like an inductor.\nIn each cycle, flyback converter's transformer first gets charged then releases its energy to the load. Accordingly, flyback converter's transformer air gap has two functions. It not only determines inductance, but also stores energy. For flyback converter, the transformer gap can have the function of energy transmission through cycles of charging and discharging.\n\nThe core's relative permeability formula_2 can be > 1,000, even > 10,000. While the air gap features much lower permeability, accordingly has higher energy density.\n\n\n\n"}
{"id": "3682117", "url": "https://en.wikipedia.org/wiki?curid=3682117", "title": "Embalming chemicals", "text": "Embalming chemicals\n\nEmbalming chemicals are a variety of preservatives, sanitising and disinfectant agents, and additives used in modern embalming to temporarily prevent decomposition and restore a natural appearance for viewing a body after death. A mixture of these chemicals is known as embalming fluid and is used to preserve bodies of deceased persons for both funeral purposes and in medical research in anatomical laboratories. The period for which a body is embalmed is dependent on time, expertise of the embalmer and factors regarding duration of stay and purpose. \n\nTypically, embalming fluid contains a mixture of formaldehyde, glutaraldehyde, methanol, and other solvents. The formaldehyde content generally ranges from 5 to 37 percent and the methanol content may range from 9 to 56 percent.\n\nIn the United States alone, about 20 million liters (roughly 5.3 million gallons) of embalming fluid are used every year.\n\nEmbalming fluid acts to \"fix\" (denature) cellular proteins, meaning that they cannot act as a nutrient source for bacteria; embalming fluid also kills the bacteria themselves. Formaldehyde or glutaraldehyde fixes tissue or cells by irreversibly connecting a primary amine group in a protein molecule with a nearby nitrogen in a protein or DNA molecule through a -CH- linkage called a Schiff base. The end result also creates the simulation, via color changes, of the appearance of blood flowing under the skin.\n\nModern embalming is not done with a single fixative. Instead, various chemicals are used to create a mixture, called an arterial solution, which is uniquely generated for the needs of each case. For example, a body needing to be repatriated overseas needs a higher index (percentage of diluted preservative chemical) than one simply for viewing (known in the United States and Canada as a funeral visitation) at a funeral home before cremation or burial.\n\nEmbalming fluid is injected into the arterial system of the deceased. Many other bodily fluids may also be displaced and removed from the body using the arterial system and in the case of cavity treatment aspirated from the body and replaced with a specialty fluid known as cavity fluid.\n\nIt is important to distinguish between an arterial chemical (or fluid), which is generally taken to be the product in its original composition, and an arterial solution, which is a diluted mixture of chemicals and made to order for each body. Non-preservative chemicals in an arterial solution are generally called \"accessory chemicals\" or co/pre-injectants, depending on their time of utilization.\n\nPotential ingredients in an arterial solution include:\n\n\nPrior to the advent of the modern range of embalming chemicals a variety of alternative additives have been used by embalmers, including epsom salts for edema cases and milk in cases of jaundice, but these are of limited effectiveness and can be chalked up as \"embalmer tricks\", as the validity of their use has never been demonstrated by professional embalmers or mortuary science programs. \n\nDuring the American Civil War, the Union Army, wanting to transport slain soldiers from the battlefields back home for burial, consulted with Dr. Thomas Holmes, who developed a technique that involved draining a corpse's blood and embalming it with a fluid made with arsenic for preservation.\n\nEmbalming chemicals are generally produced by specialist manufacturers, The oldest embalming fluid company is the Champion Company, being founded in 1878, followed by the Dodge Company in 1893, with other companies include Egyptian, now U.S. Chemical, as well as Kelco Supply Company (formerly L H Kellogg), Pierce Chemical Company,(Now Owned by The Wilbert Company), Bondol Chemical Company, and Hydrol Chemical Company. There are many smaller and regional producers such as Lear Barber in Sheffield, Genelyn, Frigid Fluid Co., and Trinity Fluids, LLC to name but a few. Some funeral homes produce their own embalming fluids, although this practice has declined in recent decades as commercially available products have become of better quality and more readily available.\n\nFollowing the EU Biocides Legislation some pressure was brought to reduce the use of formaldehyde. IARC Classes Formaldehyde as a . There are alternatives to formaldehyde and phenol-based fluids, but these are technically not preservatives but rather sanitising agents and are not widely accepted.\n\nWhen an embalmed body is buried and decays, the embalming fluid can seep into the ground and affect the surrounding soil and water ecosystems. Since embalming fluid largely consists of formaldehyde, it is the chemical that has the most dramatic effect on its surroundings. The fluid that is injected into the blood vessels of the cadaver is up to 5% formaldehyde while the fluid injected into the body cavity is up to 37%.\n\nFormaldehyde works to stiffen the tissue of the cadaver, allowing the mortician to pose the body. This is the characteristic that also makes formaldehyde hazardous when encountered in the environment. The carbon atom in formaldehyde, CHO, carries a slight positive charge due to the high electronegativity of the oxygen double bonded with the carbon. The electropositive carbon will react with a negatively charged molecule and other electron-rich species. As a result, the carbon in the formaldehyde molecule bonds with electron-rich nitrogen groups called amines found in plant and animal tissue. This leads to formaldehyde cross-linking, bonding proteins with other proteins and DNA, rendering them dysfunctional or no longer useful.\n\nFormaldehyde is featured on the U.S. Environmental Protection Agency's list of the top 10 most hazardous chemicals for damaging the environment. It is carcinogenic in humans and animals because the cross-linking can cause DNA to keep cells from halting the replication process. This unwarranted replication of cells can lead to cancer. Unicellular organisms found in the soil and groundwater are also quite sensitive to cross-linking, experiencing damage at a concentration of 0.3 mg to 22 mg per liter. Formaldehyde also affects aquatic invertebrates, with crustaceans being the most sensitive type. The range of concentration damaging them is 0.4 mg to 20 mg per liter. Studies also show that formaldehyde has been known to injure some marine plant life and kill the root systems of some small plants.\n\nFormaldehyde released from the cremation of embalmed cadavers enters the atmosphere and remains suspended for up to 250 hours. It is readily soluble in water so it will bond with moisture in the atmosphere and rain down onto plants, animals, and water supplies below. As a result, formaldehyde content in precipitation can range from 110 μg to 1380 μg per liter.\n\nThe growing awareness of the negative effects of embalming fluid on the environment has caused some people to consider green burials where there are either no harsh chemicals used in the embalming process. As a result embalming chemical manufacturers now offer less effective \"Green\" Chemicals, as an option. For example the Champion Company began producing their Enigma Line in the early 2000's and these fluids are approved by the Green Burial Council or there is no embalming process at all.\n\n\n"}
{"id": "15751821", "url": "https://en.wikipedia.org/wiki?curid=15751821", "title": "Five Ways to Save the World", "text": "Five Ways to Save the World\n\nFive Ways to Save the World is a British documentary film on environmental issues related to climate change, released in 2006. The film was made by Karen O'Connor, for the big screen and was shot in the English language to reach an international audience. It includes interviews with five environmental scientists and experts including Paul Crutzen, James Roger Angel, John Latham, Ian Jones, and Klaus Lackner.\n\nThe \"five ways\" proposed are geoengineering techniques:\n\nSince the first three methods do not remove carbon dioxide from the atmosphere, they would only reduce global warming but not ocean acidification. Since the last two methods would remove carbon dioxide, they could in theory reduce both global warming and ocean acidification.\n\n"}
{"id": "17265021", "url": "https://en.wikipedia.org/wiki?curid=17265021", "title": "Fizzle (nuclear explosion)", "text": "Fizzle (nuclear explosion)\n\nA fizzle occurs when the detonation of a device for creating a nuclear explosion (such as a nuclear weapon) grossly fails to meet its expected yield. The cause(s) for the failure can be linked to improper design, poor construction, or lack of expertise. All countries that have had a nuclear weapons testing program have experienced some fizzles. A fizzle can spread radioactive material throughout the surrounding area, involve a partial fission reaction of the fissile material, or both. For practical purposes, a fizzle can still have considerable explosive yield when compared to conventional weapons.\n\nIn multistage fission-fusion weapons, full yield of the fission primary that fails to initiate fusion ignition in the fusion secondary is also considered a \"fizzle\", as the weapon failed to reach its design yield despite the fission primary working correctly. Such fizzles can have very high yields, as in the case of Castle Koon, where the secondary stage of a device with a 1 megaton design fizzled, but its primary still generated a yield of 110 kilotons.\n\nIf a deuterium-tritium mixture is placed at the center of the device to be compressed and heated by the fission explosion, a fission yield of 250 tons is sufficient to cause D-T fusion releasing high-energy fusion neutrons which will then fission much of the remaining fission fuel. This is known as a boosted fission weapon.\nIf a fission device designed for boosting is tested without the boost gas, a yield in the sub-kiloton range may indicate a successful test that the device's implosion and primary fission stages are working as designed, though this does not test the boosting process itself.\n\n\n\nOne month after the September 11, 2001 attacks, a CIA informant known as \"Dragonfire\" reported that al-Qaeda had smuggled a low-yield nuclear weapon into New York City. Although the report was found to be false, concerns were expressed that a \"fizzle bomb\" capable of yielding a fraction of the known 10 kiloton weapons could cause \"horrific\" consequences. A detonation in New York City would mean thousands of civilian casualties.\n\n\n"}
{"id": "15110665", "url": "https://en.wikipedia.org/wiki?curid=15110665", "title": "Glassine", "text": "Glassine\n\nGlassine is a smooth and glossy paper that is air, water and grease resistant. It is usually available in densities between 50–90 g/m. It is translucent unless dyes are added to color it or make it opaque. It is manufactured by supercalendering: after pressing and drying, the paper web is passed through a stack of alternating steel and fiber-covered rolls called a supercalender at the end of the paper machine such that the paper fibres flatten facing in the same direction.\n\nGlassine is most commonly used as a base for further silicone coating for manufacture of release liner. \nGlassine is also employed as an interleaving paper in bookbinding, especially to protect fine illustrations from contact with facing pages; the paper can be manufactured with a neutral pH, and can prevent damage from spilling, exposure, or rubbing. Glassine adhesive tape has been used in book repair. In chemistry, glassine is used as an inexpensive weighing paper. It is used in foodservice as a barrier between strips of products (for example: meat, baked goods). Glassine is resistant to grease and facilitates separation of individual foodstuffs.\n\nGlassine is also recommended for protecting the surface of stored acrylic paintings. Philatelists use glassine envelopes to store stamps, and stamp hinges are made of glassine. Amateur insect collectors use glassine envelopes to store specimens temporarily in the field before they are mounted in a collection. Entomologists collecting for research may likewise use such envelopes to store whole specimens in the field. Glassine envelopes are used to carry pharmacy reformulated drugs and illicit drugs such as cocaine and heroin.\nIn the mid-20th century, potato chips were packaged in glassine bags. Glassine is also used to pack firecrackers, as it is moisture resistant. It is used for its transparent qualities to fold origami tessellations. Glassine is an outer covering on paperboard tubes, particularly those used in model rocketry, for water protection.\n"}
{"id": "7802776", "url": "https://en.wikipedia.org/wiki?curid=7802776", "title": "Ground support equipment", "text": "Ground support equipment\n\nGround Support Equipment (GSE) is the support equipment found at an airport, usually on the apron, the servicing area by the terminal. This equipment is used to service the aircraft between flights. As the name suggests, ground support equipment is there to support the operations of aircraft whilst on the ground. The role of this equipment generally involves ground power operations, aircraft mobility, and cargo/passenger loading operations.\n\nMany airlines subcontract ground handling to an airport or a handling agent, or even to another airline. Ground handling addresses the many service requirements of a passenger aircraft between the time it arrives at a terminal gate and the time it departs for its next flight. Speed, efficiency, and accuracy are important in ground handling services in order to minimize the turnaround time (the time during which the aircraft remains parked at the gate).\n\nSmall airlines sometimes subcontract maintenance to a much larger and reputable carrier, as it is a short-term cheaper alternative to setting up an independent maintenance base. Some airlines may enter into a \"Maintenance and Ground Support Agreement\" (MAGSA) with each other, which is used by airlines to assess costs for maintenance and support to aircraft.\n\nMost ground services are not directly related to the actual flying of the aircraft, and instead involve other service tasks. Cabin services ensure passenger comfort and safety. They include such tasks as cleaning the passenger cabin and replenishment of on-board consumables or washable items such as soap, pillows, tissues, blankets, and magazines. Security checks are also made to make sure no threats have been left on the aircraft.\n\nAirport GSE comprises a diverse range of vehicles and equipment necessary to service aircraft during passenger and cargo loading and unloading, maintenance, and other ground-based operations. The wide range of activities associated with aircraft ground operations lead to an equally wide-ranging fleet of GSE. For example, activities undertaken during a typical aircraft gate period include: cargo loading and unloading, passenger loading and unloading, potable water storage, lavatory waste tank drainage, aircraft refueling, engine and fuselage examination and maintenance, and food and beverage catering. Airlines employ specially designed GSE to support all these operations. Moreover, electrical power and conditioned air are generally required throughout gate operational periods for both passenger and crew comfort and safety, and many times these services are also provided by GSE.\n\nDollies for loose baggage are used for the transportation of loose baggages, oversized bags, mail bags, loose cargo carton boxes, etc. between the aircraft and the terminal or sorting facility. Dollies for loose baggage are fitted with a brake system which blocks the wheels from moving when the connecting rod is not attached to a tug. Most dollies for loose baggage are completely enclosed except for the sides which use plastic curtains to protect items from weather. In the US, these dollies are called Baggage Cart, but in Europe Baggage Cart means passenger baggage trolleys.\n\nDollies for unit load device (ULD) and cargo pallets are standard sized flatbed trolley or platform, with many wheels, roller bars or ball bearings protruding above the top surface for easy loading and unloading of ULD and cargo pallets respectively. Since ULD/pallet rest on ball bearings, these dollies are equipped with hinge/locks to secure the position of the ULD/pallet on them during tugging transportation. The aviation industry adopted ULD/pallets to be lightweight containers and supporting platforms respectively, intended to be loaded into aircraft and fly along with their loads, they need to be minimum in weight and thus do not have wheels or strong base structure. Also, the ULD/pallets have stringent dimensional standard following the aircraft cargo bay dimension. Therefore, these dollies are custom designed to complement the ULD/pallet's dimension, hinge/fixture position, weak overall physical strength and transportation need. Advanced dollies for ULD and pallets, such as those used on an airport apron, may have the following specialized facilities:\n\n\nDolly fleet management is an issue specific to the airport ground support industry. Dollies are not inexpensive consumable equipment like a hand trolley. Dollies are numerous (thousands) on a large airport apron. An airport usually has more than one dolly fleet operator, using dollies not greatly different in appearance, and each operator is using many types of dollies simultaneously. The apron is a large area that using direct eyesight to find an item is not easy. A dolly in operation needs frequent detachment and re-attachment from the tug and other dollies. It is not access controlled (it does not need a car key be used, like an automobile). It is not always supervised by the same driver (any tractor can come to pick up any dolly and tug them away, sometimes erroneously). As a result of all above factors, dollies do get lost/misplaced on an apron, or at least dollies fleet management is an ongoing burden for ground support equipment operator. Major airports are starting to attach battery power active RFID tags to dollies to facilitate their fleet management. The active RFID tags can be detected at up to 100m away in open space from the fixed RFID reader antenna, which can be mounted at the aircraft loading bridges. The RFID tag report the dolly's facility number as well as the \"battery weak\" and \"strong collision\" status, making management of the RFID tags (and thus the associated dolly) easier.\n\nChocks are used to prevent an aircraft from moving while parked at the gate or in a hangar. Chocks are placed in the front ('fore') and back ('aft') of the wheels of landing gear. They are made out of hard wood or hard rubber. Corporate safety guidelines in the US almost always specify that chocks must be used in a pair on the same wheel and they must be placed in physical contact with the wheel. Therefore, \"chocks\" are typically found in pairs connected by a segment of rope or cable. The word \"chock\" is also used as a verb, defined as the act of placing chocks in front and back of the wheel.\n\nThey are used to support a parked aircraft to prevent their tail from drooping or even falling to the ground. When the passengers in the front get off an aircraft, the aircraft becomes tail heavy and the tail will droop. Using the jack is optional but not all aircraft need it. When needed, they are tugged to the tail and set up by manpower. Once set up, no supervision to the jack is needed until the aircraft is ready to leave.\nAircraft Services Stair helps the maintenance technician to reach the bottom of aircraft.\n\nAircraft refuelers can be either a self-contained fuel truck, or a hydrant truck or cart. Fuel trucks are self-contained, typically containing up to 10,000 US gallons of fuel and have their own pumps, filters, hoses, and other equipment. A hydrant cart or truck hooks into a central pipeline network and provides fuel to the aircraft. There is a significant advantage with hydrant systems when compared to fuel trucks, as fuel trucks must be periodically replenished.\n\nThe tugs and tractors at an airport have several purposes and represent the essential part of ground support services. They are used to move all equipment that can not move itself. This includes bag carts, mobile air conditioning units, air starters, and lavatory carts.\n\nA ground power unit (GPU) is a vehicle capable of supplying power to aircraft parked on the ground. Ground power units may also be built into the jetway, making it even easier to supply electrical power to aircraft. Many aircraft require 28 V of direct current and 115 V 400 Hz of alternating current. The electric energy is carried from a generator to a connection on the aircraft via 3 phase 4-wire insulated cable capable of handling 261 amps (90 kVA). These connectors are standard for all aircraft, as defined in ISO 6858.\n\nA so-called \"solid state unit\" converts power from AC to DC along with current separation for aircraft power requirements. Solid state units can be supplied stationary, bridge-mounted or as a mobile unit.\n\nBuses at airports are used to move people from the terminal to either an aircraft or another terminal. The specific term for airport buses that drive on the apron only is apron bus. Apron buses may have a low profile like the or aircraft buses because people disembark directly to the apron. Some airports use buses that are raised to the level of a passenger terminal and can only be accessed from a door on the 2nd level of the terminal. These odd-looking buses are usually referred to as \"people movers\" or \"mobile lounges\". Airport buses are usually normal city buses or specialized terminal buses. Specialized airport buses have very low floor and wide doors on both sides of the bus for most efficient passenger movement and flexibility in depot parking. The biggest producers of airport buses are in China (Weihai, Shenyang, Beijing, Jinhua), Portugal and Slovenia.\n\nContainer loaders, also known as cargo loaders or \"K loaders\", are used for the loading and unloading of containers and pallets into and out of aircraft. The loader has two platforms which raise and descend independently. The containers or palettes on the loader are moved with the help of built-in rollers or wheels. There are different container and pallet loaders.\nFor military transport planes special container and pallet loaders are used. Some military applications use airborne loaders, which are transportable within the transport plane itself. Container and pallet loaders are mainly produced in France, Germany, Latvia, Spain, Canada, Brazil, Japan, China, and the United States.\n\nTransporters are cargo platforms constructed so that, beside loading and unloading containers, they can also can transport the cargo. These transporters are not typically used in the United States.\n\nAn Air Start Unit is a device used to start the aircraft's engines when the aircraft's APU is not operational. There are three primary types of these devices that exist currently: a stored air cart, a gas turbine based unit, and a diesel engine driven screw compressor unit. All three devices create a source of high pressure air to start the aircraft engines. Typically one or two hoses are connected to these units, with the largest aircraft engines requiring three.\nPotable water trucks are special vehicles that provide reliability and consistency in the delivery of quality water to an aircraft. The water is filtered and protected from the elements while being stored on the vehicle. A pump in the vehicle assists in moving the water from the truck to the aircraft.\n\nLavatory service vehicles empty and refill lavatories onboard aircraft. Waste is stored in tanks on the aircraft until these vehicles can empty them and remove the waste. After the tank is emptied, it is refilled with a mixture of water and a disinfecting concentrate, commonly called 'blue juice'. Instead of a self-powered vehicle, some airports have lavatory carts, which are smaller and must be pulled by tug.\n\nCatering includes the unloading of unused food and drink from the aircraft, and the loading of fresh food and drinks for passengers and crew. The meals are typically delivered in standardized carts. Meals are prepared mostly on the ground in order to minimize the amount of preparation (apart from chilling or reheating) required in the air.\n\nThe catering vehicle consists of a rear body, lifting system, platform and an electro-hydraulic control mechanism. The vehicle can be lifted up, down and the platform can be moved to place in front of the aircraft.\n\nIn-flight food is prepared in the flight kitchen which is a completely HACCP certified facility where food is made in sterile and controlled environments. The packed food is then placed in trollies and wheeled into the Catering truck at the flight kitchen, which can be located within a 5 km radius of the airport.\n\nThereon the vehicle drives to the airport and is parked in front of the plane. The stabilizers are deployed and the van body is lifted. The platform can be fine controlled to move left-right as well as in-out so that it is aligned with the door correctly.\n\nThe body is made of insulated panels and is capable of maintaining temperatures of 0 degrees by means of refrigeration unit.\n\nA special higher type of catering truck has been designed for the Airbus A380 because of its unique height.\n\nBelt loaders are vehicles with conveyor belts for unloading and loading of baggage and cargo onto aircraft. A belt loader is positioned at the door sill of an aircraft hold (baggage compartment) during operation. Belt loaders are used for narrowbody aircraft, and the bulk hold of wide body aircraft. Stowing baggage without containers is known as \"bulk loading\".\n\nPassenger boarding stairs, sometimes referred to as \"boarding ramps\", \"stair car\" or \"aircraft steps\", provide a mobile means to traverse between the aircraft doors and the ground. Because larger aircraft have door sills 5 to 20 feet high, stairs facilitate safe boarding and deplaning. Smaller units are generally moved by being towed or pushed, while larger units are self-powered. Most models have adjustable height to accommodate various aircraft. Optional features may include canopies, heating, supplementary lighting, and a red carpet for VIP passengers. A jet bridge may be used to board larger aircraft, but ground-based stairs are used when this is unavailable or impractical.\n\nPushback tugs are mostly used to push an aircraft away from the gate when it is ready to leave. These tugs are very powerful and because of the large engines, are sometimes referred to as an engine with wheels. Pushback tugs can also be used to pull aircraft in various situations, such as to a hangar. Different size tugs are required for different size aircraft. Some tugs use a tow-bar as a connection between the tug and the aircraft, while other tugs lift the nose gear off the ground to make it easier to tow or push. Recently there has been a push for towbarless tractors as larger airplanes are designed.\n\nThe procedure of de/anti-icing, protection from fluids freezing up on aircraft, is done from special vehicles. These vehicles have booms, like a cherry picker, to allow easy access to the entire aircraft. A hose sprays a special mixture that melts current ice on the aircraft and also prevents some ice from building up while waiting on the ground.\nAircraft rescue and firefighting (ARFF) is a special category of firefighting that involves the response, hazard mitigation, evacuation and possible rescue of passengers and crew of an aircraft involved in (typically) an airport ground emergency.\n\n"}
{"id": "27289342", "url": "https://en.wikipedia.org/wiki?curid=27289342", "title": "Hexatriacontanoic acid", "text": "Hexatriacontanoic acid\n\nHexatriacontanoic acid (or hexatriacontylic acid) is a 36-carbon long carboxylic acid. It is also a saturated fatty acid. \n\n\n"}
{"id": "4845364", "url": "https://en.wikipedia.org/wiki?curid=4845364", "title": "ISSOW", "text": "ISSOW\n\nISSOW stands for Integrated Safe System of Work. An Integrated Safe System of Work is used in hazardous industry to request, review, approve and document tasks to be carried out by frontline workers. It integrates Permit-To-Work, Risk Assessment and Isolation Management under a single electronic system, providing safety improvements to the user. \n\nThought to have originated in the North Sea upstream oil industry it was actually used several years before within the Power Industry, ISSoW software now forms a significant segment of the Enterprise Software market.\n\nISSOW has now become synonymous with electronic permit to work. The North Sea began adopting several ISSOW suppliers in 2000s, when it was first rolled out to BP's North Everest Platform. From there a number of the major operators in the North Sea oil industry started to adopt this electronic safety system.\n\nAs of 2012, around 80% of the North Sea use an ISSOW product in some form or another, as it is widely recognised as a key frontline safety system. Between 2001 and present, ISSOW began to be utilised by some of the major operators including, but not limited to, Shell, Exxon Mobil, BG Group, and Chevron. ISSOW systems are used in Oil&Gas industry around the world.\n\n"}
{"id": "23114936", "url": "https://en.wikipedia.org/wiki?curid=23114936", "title": "Induction plasma", "text": "Induction plasma\n\nThe 1960s were the incipient period of thermal plasma technology, spurred by the needs of aerospace programs. Among the various methods of thermal plasma generation, induction plasma (or inductively coupled plasma) takes up an important role.\n\nEarly attempts to maintain inductively coupled plasma on a stream of gas date back to Babat in 1947 and Reed in 1961. Effort was concentrated on the fundamental studies of energy coupling mechanism and the characteristics of the flow, temperature and concentration fields in plasma discharge. In 1980s, there was increasing interest in high-performance materials and other scientific issues, and in induction plasma for industrial-scale applications such as waste treatment. Numerous research and development were devoted to bridge the gap between the laboratory gadget and the industry integration. After decades' effort, induction plasma technology has gained a firm foothold in modern advanced industry.\n\nInduction heating is a mature technology with centuries of history. A conductive metallic piece, inside a coil of high frequency, will be \"induced\", and heated to the red-hot state. There is no difference in cardinal principle for either induction heating or \"inductively coupled plasma\", only that the medium to induce, in the latter case, is replaced by the flowing gas, and the temperature obtained is extremely high, as it arrives the \"fourth state of the matter\"—plasma.\n\nAn inductively coupled plasma (ICP) torch is essentially a copper coil of several turns, through which cooling water is running in order to dissipate the heat produced in operation. The coil wraps a confinement tube, inside which the induction plasma is generated. One end of the confinement tube is open; the plasma is actually maintained on a continuum gas flow. During induction plasma operation, the generator supplies an alternating current (ac) of radio frequency (r.f.) to the torch coil; this ac induces an alternating magnetic field inside the coil, after Ampère's law (for a solenoid coil):\n\nformula_1\n\nwhere, formula_2 is the flux of magnetic field, formula_3 is permeability constant formula_4, formula_5 is the coil current, formula_6 is the number of coil turns per unit length, and formula_7 is the mean radius of the coil turns.\n\nAccording to Faraday's Law, a variation in magnetic field flux will induce a voltage, or electromagnetic force:\n\nformula_8\n\nwhere, formula_6 is the number of coil turns, and the item in parenthesis is the rate at which the flux is changing. The plasma is conductive (assuming a plasma already exists in the torch). This electromagnetic force, E, will in turn drive a current of density j in closed loops. The situation is much similar to heating a metal rod in the induction coil: energy transferred to the plasma is dissipated via Joule heating, jR, from Ohm's law, where R is the resistance of plasma.\n\nSince the plasma has a relatively high electrical conductivity, it is difficult for the alternating magnetic field to penetrate it, especially at very high frequencies. This phenomenon is usually described as the \"skin effect\". The intuitive scenario is that the induced currents surrounding each magnetic line counteract each other, so that a net induced current is concentrated only near the periphery of plasma. It means the hottest part of plasma is off-axis. Therefore, the induction plasma is something like an \"annular shell\". Observing on the axis of plasma, it looks like a bright \"bagel\".\n\nIn practice, the ignition of plasma under low pressure conditions (<300 torr) is almost spontaneous, once the r.f. power imposed on the coil achieves a certain threshold value (depending on the torch configuration, gas flow rate etc.). The state of plasma gas (usually argon) will swiftly transit from glow-discharge to arc-break and create a stable induction plasma. For the case of atmospheric ambient pressure conditions, ignition is often accomplished with the aid of a Tesla coil, which produces high-frequency, high-voltage electric sparks that induce local arc-break inside the torch and stimulate a cascade of ionization of plasma gas, ultimately resulting in a stable plasma.\n\nInduction plasma torch is the core of the induction plasma technology. Despite the existence of hundreds of different designs, an induction plasma torch consists of essentially three components:\n\nQ is the carrier gas that is usually introduced into the plasma torch through an injector at the center of the torch head. As the name indicates it, the function of Q is to convey the precursor (powders or liquid) into plasma. Argon is the usual carrier gas, however, many other reactive gases (i.e., oxygen, NH, CH, etc.) are often involved in the carrier gas, depending on the processing requirement.\n\nQ is the plasma forming gas, commonly called as the \"Central Gas\". In today's induction plasma torch design, it is almost unexceptional that the central gas is introduced into the torch chamber by tangentially swirling. The swirling gas stream is maintained by an internal tube that hoops the swirl till to the level of the first turn of induction coil. All these engineering concepts are aiming to create the proper flow pattern necessary to insure the stability of the gas discharge in the center of the coil region.\n\nQ is commonly referred to as \"Sheath Gas\" that is introduced outside the internal tube mentioned above. The flow pattern of Q can be either vortex or straight. The function of sheath gas is twofold. It helps to stabilize the plasma discharge; most importantly, it protects the confinement tube, as a cooling medium.\n\n\nIn practice, the selection of plasma gases in an induction plasma processing is first determined by the processing chemistry, i.e., if the processing requiring a reductive or oxidative, or other environment. Then suitable second gas may be selected and added to argon, so as to get a better heat transfer between plasma and the materials to treat. Ar–He, Ar–H, Ar–N, Ar–O, Air, etc. mixture are very commonly used induction plasmas. Since the energy dissipation in the discharge takes places essentially in the outer annular shell of plasma, the second gas is usually introduced along with the sheath gas line, rather than the central gas line.\n\nFollowing the evolution of the induction plasma technology in laboratory, the major advantages of the induction plasma have been distinguished:\n\n\nThese features of induction plasma technology, has found niche applications in industrial scale operation in the last decade. The successful industrial application of induction plasma process depends largely on many fundamental engineering supports. For example, the industrial plasma torch design, which allows high power level (50 to 600 kW) and long duration (three shifts of 8 hours/day) of plasma processing. Another example is the powder feeders that convey large quantity of solid precursor (1 to 30 kg/h) with reliable and precise delivery performance.\n\nNowadays, we have been in a position to be able to numerate many examples of the industrial applications of induction plasma technology, such as, powder spheroidisation, nanosized powders synthesis, induction plasma spraying, waste treatments, etc., However, the most impressive success of induction plasma technology is doubtless in the fields of spheroidisation and nano-materials synthesis.\n\nThe requirement of powders spheroidisation (as well as densification) comes from very different industrial fields, from powder metallurgy to the electronic packaging. Generally speaking, the pressing need for an industrial process to turn to spherical powders is to seek at least one of the following benefits which result from the spheroidisation process:\n\nSpheroidisation is a process of in-flight melting. The powder precursor of angular shape is introduced into induction plasma, and melted immediately in the high temperatures of plasma. The melted powder particles are assuming the spherical shape under the action of surface tension of liquid state. These droplets will be drastically cooled down when fly out of the plasma plume, because of the big temperature gradient exciting in the plasma. The condensed spheres are thus collected as the spheroidisation products.\n\nA great variety of ceramics, metals and metal alloys have been successfully spheroidized/densified using induction plasma spheroidisation. Following are some typical materials spheroidized on commercial scale.\n\n\nIt is the increased demand for nanopowders that promotes the extensive research and development of various techniques for nanometric powders. The challenges for an industrial application technology are productivity, quality controllability, and affordability. Induction plasma technology implements in-flight evaporation of precursor, even those raw materials of the highest boiling point; operating under various atmospheres, permitting synthesis of a great variety of nanopowders, and thus become much more reliable and efficient technology for synthesis of nanopowders in both laboratory and industrial scales. Induction plasma used for nanopowder synthesis has many advantages over the alternative techniques, such as high purity, high flexibility, easy to scale up, easy to operate and process control.\n\nIn the nano-synthesis process, material is first heated up to evaporation in induction plasma, and the vapours are subsequently subjected to a very rapid quenching in the quench/reaction zone, The quench gas can be inert gases such as Ar and N or reactive gases such as CH and NH, depending on the type of nanopowders to be synthesized. The nanometric powders produced are usually collected by porous filters, which are installed away from the plasma reactor section. Because of the high reactivity of metal powders, special attention should be given to powder pacification prior to the removal of the collected powder from the filtration section of the process.\n\nThe induction plasma system has been successfully used in the synthesis nanopowders. The typical size range of the nano-particles produced is from 20 to 100 nm, depending on the quench conditions employed. The productivity varies from few hundreds g/h to 3~4 kg/h, according to the different materials' physical properties. A typical induction plasma nano-synthsize system for industrial application is shown below. The photos of some nano-product from the same equipment are included.\n\nInduction plasma technology achieves mainly the aforementioned high-added-value processes. Besides the \"spheroidisation\" and \"nanomaterial synthesis\", the high risk waste treatment, refractory materials deposit, noble material synthesis etc. may be the next industrial fields for induction plasma technology.\n\n"}
{"id": "2210437", "url": "https://en.wikipedia.org/wiki?curid=2210437", "title": "Inertron", "text": "Inertron\n\nInertron is a fictional metallic chemical substance found in the DC Comics Legion of Super-Heroes universe. It is the hardest, densest substance in the DC universe, and is often used by the Legion of Super-Heroes in the 30th century. The first mention of inertron was in \"Adventure Comics\" #336.\n\nInertron is frequently described as indestructible and impenetrable; however, this only applies to regular humans, lesser superhumans and 30th century known technology, as, on some occasions, Superboy and Mon-El could break an inertron container by using all their strength. In \"Adventure Comics\" #370, though, a triple steel layer inertron-sealed prison was described as too tough for Superboy or Mon-El to break through. Matter Eater Lad has been shown to be able to eat it, however. Karate Kid has broken shackles made of inertron one time. In \"Superboy and the Legion\" #245, Element Lad pointed out that inertron was an artificial element, which meant it was still an element, and he thus was able to change it to helium.\n\nThe earliest known mention of inertron was in the August 1928 pulp magazine \"Amazing Stories\", in the first Buck Rogers story, entitled \"Armageddon 2419 A.D.\", by Philip Francis Nowlan. Nowlan's inertron was similar to that used in the DC universe, in that it was difficult to obtain and virtually indestructible. Nowlan's inertron was also a perfect gravity shield so that any material above it would be weightless, a characteristic similar to H. G. Wells' cavorite.\n\nAccording to \"Armageddon 2419 A.D.\":\n"}
{"id": "940189", "url": "https://en.wikipedia.org/wiki?curid=940189", "title": "International Nuclear Event Scale", "text": "International Nuclear Event Scale\n\nThe International Nuclear and Radiological Event Scale (INES) was introduced in 1990 by the International Atomic Energy Agency (IAEA) in order to enable prompt communication of safety-significant information in case of nuclear accidents.\n\nThe scale is intended to be logarithmic, similar to the moment magnitude scale that is used to describe the comparative magnitude of earthquakes. Each increasing level represents an accident approximately ten times more severe than the previous level. Compared to earthquakes, where the event intensity can be quantitatively evaluated, the level of severity of a man-made disaster, such as a nuclear accident, is more subject to interpretation. Because of the difficulty of interpreting, the INES level of an incident is assigned well after the incident occurs. Therefore, the scale has a very limited ability to assist in disaster-aid deployment.\n\nAs INES ratings are not assigned by a central body, high-profile nuclear incidents are sometimes assigned INES ratings by the operator, by the formal body of the country, but also by scientific institutes, international authorities or other experts which may lead to confusion as to the actual severity.\n\nA number of criteria and indicators are defined to assure coherent reporting of nuclear events by different official authorities. There are seven nonzero levels on the INES scale: three \"incident\"-levels and four \"accident\"-levels. There is also a level 0.\n\nThe level on the scale is determined by the highest of three scores: off-site effects, on-site effects, and defence in depth degradation.\n\nThere are also events of no safety relevance, characterized as \"out of scale\".\n\nDeficiencies in the existing INES have emerged through comparisons between the 1986 Chernobyl disaster, which had severe and widespread consequences to humans and the environment, and the 2011 Fukushima Daiichi nuclear accident, which caused no fatalities and comparatively small (10%) release of radiological material into the environment. The Fukushima Daiichi nuclear accident was originally rated as INES 5, but then upgraded to INES 7 (the highest level) when the events of units 1, 2 and 3 were combined into a single event and the combined release of radiological material was the determining factor for the INES rating. In light of that, nuclear experts say that the INES emergency scale is very likely to be revisited.\n\nOne study found that the INES scale of the IAEA is highly inconsistent, and the scores provided by the IAEA incomplete, with many events not having an INES rating. Further, the actual accident damage values do not reflect the INES scores. A quantifiable, continuous scale might be preferable to the INES, in the same way that the antiquated Mercalli scale for earthquake magnitudes was superseded by the continuous physically-based Richter scale.\n\nThe following arguments have been proposed: firstly, the scale is essentially a discrete qualitative ranking, not defined beyond event level 7. Secondly, it was designed as a public relations tool, not an objective scientific scale. Thirdly, its most serious shortcoming is that it conflates magnitude and intensity. An alternative nuclear accident magnitude scale (NAMS) was proposed by British nuclear safety expert David Smythe to address these issues.\n\nThe Nuclear Accident Magnitude Scale (NAMS) is an alternative to INES, proposed by David Smythe in 2011 as a response to the Fukushima Daiichi nuclear disaster. There were some concerns that INES was used in a confusing manner, and NAMS was intended to address the perceived INES shortcomings.\n\nAs Smythe pointed out, the INES scale ends at 7; a more severe accident than Fukushima in 2011 or Chernobyl in 1986 cannot be measured by that scale. In addition, it is not continuous, not allowing a fine-grained comparison of nuclear incidents and accidents. But then, the most pressing item identified by Smythe is that INES conflates magnitude with intensity; a distinction long made by seismologists to describe earthquakes. In that area, magnitude describes the physical energy released by an earthquake, while the intensity focuses on the effects of the earthquake. In analogy, a nuclear incident with a high magnitude (e.g. a core meltdown) may not result in an intense radioactive contamination, as the incident at the Swiss research reactor in Lucens shows – but yet it resides in INES category 5, together with the Windscale fire of 1957, which has caused significant contamination outside of the facility.\n\nThe definition of the NAMS scale is:\n\nwith R being the radioactivity being released in terabecquerels, calculated as the equivalent dose of iodine-131. Furthermore, only the atmospheric release affecting the area \"outside\" the nuclear facility is considered for calculating the NAMS, giving a NAMS score of 0 to all incidents which do not affect the outside. The factor of 20 assures that both the INES and the NAMS scales reside in a similar range, aiding a comparison between accidents. An atmospheric release of any radioactivity will only occur in the INES categories 4 to 7, while NAMS does not have such a limitation.\n\nThe NAMS scale does still not take into account the radioactive contamination of liquids such as an ocean, sea, river or groundwater pollution in proximity to any nuclear power plant. \nAn estimation of its magnitude seems to be related to the problematic definition of a radiological equivalence between different type of involved isotopes and the variety of paths by which activity might eventually be ingested, e.g. eating fish or through the food chain.\n\n"}
{"id": "30811847", "url": "https://en.wikipedia.org/wiki?curid=30811847", "title": "Jirau Dam", "text": "Jirau Dam\n\nThe Jirau Dam is a rock-fill dam with an asphalt-concrete core, currently under construction on the Madeira River in the state of Rondônia, Brazil. The dam's hydroelectric power stations will have 50 turbines each 75 MW resulting total installed capacity of 3,750 MW. The power plant's first unit was commissioned in September 2013, the 16th in November 2014, 24th in February 2015, the 41st in December 2015, and the last in December 2016. Most of the power is designed to be exported to south-eastern Brazil via the Rio Madeira HVDC system.\n\nThe dam is part of a planned four power plant Madeira river hydroelectric complex, which will consist of two dams in Brazil (3,580 MW Santo Antonio Dam at the city of Porto Velho and Jirau), a third on the border of Brazil and Bolivia, and a fourth station inside Bolivia. Two of these, Santo Antonio and Jirau, are currently under construction, while the smaller upstream dams are still in the planning stages. In part due to the 2001–2002 power shortage in Brazil, construction of both dams was accelerated in 2009. The total estimated cost of the two facilities currently under construction is $15.6 billion ($8 billion for Jirau), including about $10 billion for the civil engineering and power plants, and $5 billion for ship locks, transmission lines, and environmental re-mediation. The Madeira river hydroelectric complex is part of the Initiative for the Integration of the Regional Infrastructure of South America, an effort by South American governments to integrate the continent's infrastructure with new investments in transportation, energy, and communication. Construction on the project was temporary halted in March 2011, February 2012 and April 2013 due to worker riots or strikes.\n\nThe Brazilian Development Bank approved an additional US$1.6 billion for the project in September 2012. The extra funding will add six more 75 MW bulb turbine-generators to the power station (a total of 50) and pay for transmission lines.\n\nThe Jirau Dam is a combination embankment dam with concrete sections for the power stations and spillway. The length of the entire dam is while the embankment section is . The embankment dam is arched, tall and has an asphalt-core. Its structural volume is of which is asphalt. The dam's spillway consists of 21 gates and has a maximum discharge of . The run-of-the-river dam's power station contain 50 x 75 MW bulb turbines for a total installed capacity of 3,750 MW. The reservoir created by the dam has a surface area of of which is the original riverbed. Bulb turbines are a variation of the Kaplan turbine, with the main differences being that bulb turbines are installed horizontally and are generally considered to be slightly more efficient. The power plant is constructed by the French utility GDF Suez SA and Brazilian company Camargo Correa SA.\n\nBrazilian law requires water impoundments to undergo a very thorough approval process to ensure that each project meets environmental, social, political, and safety criteria. However, critics of the Jirau and Santo Antonio dam claim that many legal criteria were rubber-stamped before all questions from impacted groups had been addressed. The dam's social impacts received the majority of substantive criticism (see below). However, environmental groups noted that the fast track approval for the Madeira dams sets a dangerous precedent. Brazilian law allows for expedited licensing for eco-friendly projects described by the Worldwatch institute as \"kindler, gentler dams with smaller reservoirs, designed to lessen social and environmental impacts.\" The Worldwatch Institute insists that no project should \"fast-track the licensing of new dams in Amazonia and allow projects to circumvent Brazil's tough environmental laws\".\n\nThe most frequent objection is that the dam builders failed to adequately consult with indigenous peoples, as required by law. The Brazilian government indigenous protection foundation FUNAI predicts that there may be un-contacted indigenous populations in the region that will be affected by the Madeira complex. Most of the affected populations are nearest to the Jirau dam. The threat to uncontacted Indians has motivated both internal and external criticism of GDF Suez, the contractor responsible for building the Jirau dam. A coalition of non governmental organizations called for dam construction to be halted, and questions were raised during annual meeting of GDF Suez.\n\nMoreover, federal prosecutors are suing ESBR (\"Energia Sustentável do Brasil\"), the company responsible for the dam, the Brazilian Institute of Environment (IBAMA) and the Brazilian Development Bank (BNDES) for the non-accomplishment of some of the conditions previewed by the environmental license and for the indemnification for losses on the traditional fishers' revenues. \n\nBecause both the Jirau and Santo Antonio dams are run-of-the-river projects, neither dam impounds a large reservoir. Both dams also feature significant environmental re-mediation efforts. As a consequence, there has not been strong environmental opposition to the implementation of the Madeira river complex. However, critics point out that if the fish ladders fail, \"several valuable migratory fish species could suffer near-extinction as a result of the Madeira dams.\"\n\nConstruction on the dam was halted on 18 March 2011 as workers rioted; setting fire to buses and destroying part of the worker housing. Wages and the treatment by security officials was attributed to the rioting. Additional security personal had to be sent to the site and construction was halted. Workers went on strike at Jirau and Santo Antonio in April 2013 after a salary increase proposal was rejected.\n\nBolivia has been a landlocked country since it lost its coastline to Chile in the war of the pacific in 1884. Many Bolivians feel a deep and lasting bitterness due to this loss, and the Bolivian military continues to build and maintain an open ocean navy in Lake Titicaca, awaiting an eventual recovery of access to the sea. The Madeira river complex presents an opportunity for Bolivia because all of the hydroelectric dams would feature ship locks capable of raising and lowering oceangoing vessels. If the project is completed, \"more than 4,000 km of waterways upstream from the dams in Brazil, Bolivia, and Peru would become navigable.\"\nHence, if the project is completed, both Bolivian commercial vessels and the Bolivian navy would have access to the open ocean, and lucrative sea lanes, for the first time in 120 years.\n\nThe body of the Brazilian environmental activist Nilce de Souza Magalhães, also known as \"Nicinha,\" was found on 21 June 2016 in the hydro-power dam's lake of Jirau. Nicinha, leadership of MAB in Rondônia was missing since 7 January 2016. Her body was found only 400 meters away from where she used to live. Her body was found by the workers of the dam, her hands and feet were tied by a rope and tied to a rock.\n\nShe was known in the region for the struggle in defense of the affected populations, denouncing human rights violations committed by the consortium responsible for the Jirau power plant, called Energia Sustentável do Brasil (ESBR). Nicinha was daughter of rubber extractors who came from the Brazilian state of Acre to the city of Abuna (near Porto Velho) in Rondonia, where she lived almost fifty years and was evicted along with other fishers due to the construction of the dam. The encampment where they had lived had no access to clean water or electricity.\n\n\"Nicinha\" made several complaints over the years, attending public hearings and events, including, pointed out the serious impacts of predatory fishing activity on the Madeira River. The complaints generated two civil investigations being conducted by the Federal Prosecutor's Office and the State Prosecutor's Office on the non implementation of the Program of Support to Fishing Activity and another of criminal character, because of data manipulation in monitoring reports.\n\nHis killer, Edione Pessoa da Silva, who was is prison after confessing to murder Nicinha, escaped from the State Penitentiary \"Edvan Mariano Rosendo\", located in Porto Velho (RO) on April 2016.\n\n"}
{"id": "38077605", "url": "https://en.wikipedia.org/wiki?curid=38077605", "title": "List of European tornadoes in 2013", "text": "List of European tornadoes in 2013\n\nThis is a list of all tornadoes that were confirmed throughout Europe by the European Severe Storms Laboratory and local meteorological agencies during 2013. Unlike the United States, the original Fujita Scale and the TORRO scale are used to rank tornadoes across the continent.\n"}
{"id": "56577590", "url": "https://en.wikipedia.org/wiki?curid=56577590", "title": "Liñán's diffusion flame theory", "text": "Liñán's diffusion flame theory\n\nLiñán diffusion flame theory is a theory developed by Amable Liñán in 1974 to explain the diffusion flame structure using activation energy asymptotics and Damköhler number asymptotics. Liñán used counterflowing jets of fuel and oxidizer to study the diffusion flame structure, analyzing for the entire range of Damköhler number. His theory predicted four different type of flame structure as follows,\n\n\nThe theory is well explained in the simplest possible model. Thus, assuming a one-step irreversible Arrhenius law for the combustion chemistry with constant density and transport properties and with unity Lewis number reactants, the governing equation for the non-dimensional temperature field formula_1 in the stagnation point flow reduces to\n\nwhere formula_3 is the mixture fraction, formula_4 is the Damköhler number, formula_5 is the activation temperature and the fuel mass fraction and oxidizer mass fraction are scaled with their respective feed stream values, given by\n\nwith boundary conditions formula_7. Here, formula_8 is the unburnt temperature profile (frozen solution) and formula_9 is the stoichiometric parameter (mass of oxidizer stream required to burn unit mass of fuel stream). The four regime are analyzed by trying to solve above equations using activation energy asymptotics and Damköhler number asymptotics. The solution to above problem is multi-valued. Treating mixture fraction formula_3 as independent variable reduces the equation to\n\nwith boundary conditions formula_12 and formula_13. \n\nThe reduced Damköhler number is defined as follows\n\nwhere formula_15 and formula_16. The theory predicted an expression for the reduced Damköhler number at which the flame will extinguish, given by\n\nwhere formula_18.\n\n"}
{"id": "1005990", "url": "https://en.wikipedia.org/wiki?curid=1005990", "title": "Mr. Zog's Sex Wax", "text": "Mr. Zog's Sex Wax\n\nMr. Zog's Sex Wax is a Carpinteria, California, brand of surfwax manufactured for use on surfboards. This wax is rubbed on the top surface or \"deck\" of a surfboard to allow traction and grip for the surfer. \n\nMr. Zog's Sex Wax was first produced by Frederick Charles Herzog, III (\"Mr. Zog\") and chemist Nate Skinner in 1972. Hank Pitcher designed their original logo.\n\nDue to the product name, promotional materials such as bumper stickers and t-shirts became extremely popular, even among those who had never ridden a surfboard. Their slogans, such as \"The best for your stick\", included innuendos of non-surfing uses. The materials confirmed their counterculture status by being banned from schools and amusement parks.\n\nDifferent wax formulations are sold under the names: \"Quick Humps\", \"Really Tacky\", and \"Navel Wax\".\n\nA snowboard wax is sold by Mr. Zog's, which is applied to the bottom of a snowboard to reduce friction between the snowboard and the snow. One variety is melted and applied to the bottom of the snowboard, another variety is rubbed on as a cold wax.\n\nMr. Zog's Sex Wax is also sold to ice hockey players. The wax is applied by rubbing it on over the tape on the blade of the stick. This does two things: it flattens the tape to allow a smoother shot, and it also makes the blade of the stick stickier, which helps hockey players control the puck for stick-handling, passing, and shooting purposes. Also the wax acts as a surfactant to aid moisture dispersion from the blade during ice hockey sessions. In addition, some goaltenders rub the wax on the shaft of the stick to help them hold the stick when it is struck by hard shots, some of which exceed 100 MPH.\n\n\n"}
{"id": "415513", "url": "https://en.wikipedia.org/wiki?curid=415513", "title": "Net force", "text": "Net force\n\nNet force is the vector sum of forces acting on a particle or body. The net force is a single force that replaces the effect of the original forces on the particle's motion. It gives the particle the same acceleration as all those actual forces together as described by the Newton's second law of motion.\n\nIn physics, it is possible to determine the torque associated with the point of application of a net force so that it maintains the movement of jets of the object under the original system of forces. Its associated torque, the net force, becomes the \"resultant force\" and has the same effect on the rotational motion of the object as all actual forces taken together. It is possible for a system of forces to define a torque-free resultant force. In this case, the net force, when applied at the proper line of action, has the same effect on the body as all of the forces at their points of application. It is not always possible to find a torque-free resultant force.\n\nForce is a vector quantity, which means that it has a magnitude and a direction, and it is usually denoted using boldface such as F or by using an arrow over the symbol, such as formula_1. \n\nGraphically, a force is represented as a line segment from its point of application \"A\" to a point \"B\", which defines its direction and magnitude. The length of the segment \"AB\" represents the magnitude of the force.\n\nVector calculus was developed in the late 1800s and early 1900s. The parallelogram rule used for the addition of forces, however, dates from antiquity and is noted explicitly by Galileo and Newton. \n\nThe diagram shows the addition of the forces formula_2 and formula_3. The sum formula_1 of the two forces is drawn as the diagonal of a parallelogram defined by the two forces. \nForces applied to an extended body can have different points of application. Forces are bound vectors and can be added only if they are applied at the same point. The net force obtained from all the forces acting on a body do not preserve its motion unless applied at the same point, and with the appropriate torque associated with the new point of application determined. The net force on a body applied at a single point with the appropriate torque is known as the resultant force and torque.\n\nA force is known as a bound vector—which means it has a direction and magnitude and a point of application. A convenient way to define a force is by a line segment from a point \"A\" to a point \"B\". If we denote the coordinates of these points as A=(A, A, A) and B=(B, B, B), then the force vector applied at \"A\" is given by\nThe length of the vector B-A defines the magnitude of F and is given by\n\nThe sum of two forces F and F applied at \"A\" can be computed from the sum of the segments that define them. Let F=B-A and F=D-A, then the sum of these two vectors is\nwhich can be written as\nwhere E is the midpoint of the segment BD that joins the points \"B\" and \"D\". \n\nThus, the sum of the forces F and F is twice the segment joining \"A\" to the midpoint \"E\" of the segment joining the endpoints \"B\" and \"D\" of the two forces. The doubling of this length is easily achieved by defining a segments BC and DC parallel to AD and AB, respectively, to complete the parallelogram \"ABCD\". The diagonal AC of this parallelogram is the sum of the two force vectors. This is known as the parallelogram rule for the addition of forces.\n\nWhen a force acts on a particle, it is applied to a single point (the particle volume is negligible): this is a point force and the particle is its application point. But an external force on an extended body (object) can be applied to a number of its constituent particles, i.e. can be \"spread\" over some volume or surface of the body. However, determining its rotational effect on the body requires that we specify its point of application (actually, the line of application, as explained below). The problem is usually resolved in the following ways:\n\n\nIn any case, the analysis of the rigid body motion begins with the point force model. And when a force acting on a body is shown graphically, the oriented line segment representing the force is usually drawn so as to \"begin\" (or \"end\") at the application point.\n\nIn the example shown in the diagram opposite, a single force formula_9 acts at the application point H on a free rigid body. The body has the mass formula_10 and its center of mass is the point C. In the constant mass approximation, the force causes changes in the body motion described by the following expressions:\n\nIn the second expression, formula_13 is the torque or moment of force, whereas formula_14 is the moment of inertia of the body. A torque caused by a force formula_9 is a vector quantity defined with respect to some reference point:\n\nThe vector formula_18 is the position vector of the force application point, and in this example it is drawn from the center of mass as the reference point of(see diagram). The straight line segment formula_19 is the lever arm of the force formula_9 with respect to the center of mass. As the illustration suggests, the torque does not change (the same lever arm) if the application point is moved along the line of the application of the force (dotted black line). More formally, this follows from the properties of the vector product, and shows that rotational effect of the force depends only on the position of its line of application, and not on the particular choice of the point of application along that line.\n\nThe torque vector is perpendicular to the plane defined by the force and the vector formula_18, and in this example it is directed towards the observer; the angular acceleration vector has the same direction. The right hand rule relates this direction to the clockwise or counter-clockwise rotation in the plane of the drawing.\n\nThe moment of inertia formula_14 is calculated with respect to the axis through the center of mass that is parallel with the torque. If the body shown in the illustration is a homogeneous disc, this moment of inertia is formula_23 . If the disc has the mass 0,5 kg and the radius 0,8 m, the moment of inertia is 0,16 kgm. If the amount of force is 2 N, and the lever arm 0,6 m, the amount of torque is 1,2 Nm. At the instant shown, the force gives to the disc the angular acceleration α = /I = 7,5 rad/s, and to its center of mass it gives the linear acceleration a = F/m = 4 m/s.\n\nResultant force and torque replaces the effects of a system of forces acting on the movement of a rigid body. An interesting special case is a torque-free resultant, which can be found as follows:\nwhere formula_25 is the net force, formula_26 locates its application point, and individual forces are formula_27 with application points formula_28. It may be that there is no point of application that yields a torque-free resultant.\nThe diagram opposite illustrates simple graphical methods for finding the line of application of the resultant force of simple planar systems:\n\nIn general, a system of forces acting on a rigid body can always be replaced by one force plus one pure (see previous section) torque. The force is the net force, but to calculate the additional torque, the net force must be assigned the line of action. The line of action can be selected arbitrarily, but the additional pure torque depends on this choice. In a special case, it is possible to find such line of action that this additional torque is zero.\n\nThe resultant force and torque can be determined for any configuration of forces. However, an interesting special case is a torque-free resultant. This is useful, both conceptually and practically, because the body moves without rotating as if it was a particle.\nSome authors do not distinguish the resultant force from the net force and use the terms as synonyms.\n\n"}
{"id": "10906395", "url": "https://en.wikipedia.org/wiki?curid=10906395", "title": "Neutron supermirror", "text": "Neutron supermirror\n\nA neutron supermirror is a highly polished surface used in connection with neutron beams.\n\nSupermirrors are produced by depositing and polishing large numbers of layers of a reflecting substance, such as silicon, nickel, titanium or nickel/titanium composite, on a substrate. In this way, the critical angle of total reflection becomes formula_1, where formula_2 is the number of layers and formula_3 the angle of total reflection for a single layer.\n"}
{"id": "31710889", "url": "https://en.wikipedia.org/wiki?curid=31710889", "title": "Northern Plains Resource Council", "text": "Northern Plains Resource Council\n\nThe Northern Plains Resource Council is an environmental lobbying organization based in Montana. It was founded in 1972 to lobby for stricter environmental controls on coal mining and coal-fired power plants.\n\n\n"}
{"id": "57787119", "url": "https://en.wikipedia.org/wiki?curid=57787119", "title": "Nuro", "text": "Nuro\n\nNuro is an American robotics company based in Mountain View, California and founded by Jiajun Zhu and Dave Ferguson.\n\nZhu was one of the founding engineers of Google's self driving car project, Waymo, serving as the principle software engineer. Ferguson helped lead Carnegie Mellon University's robotics team to victory in the 2007 DARPA Urban Challenge before also joining Google's self driving car project in 2011 as its principal computer-vision and machine-learning engineer. \n\nZhu and Ferguson founded Nuro in September 2016. It has raised a total of $92M from Greylock Partners and Gaorong Capital.\n\nNuro officially launched in January 2018, revealing its first product, the R1. The R1 is an electric self-driving local commerce delivery vehicle. It weighs , and is just over tall and about half the width of a sedan. This vehicle is designed to carry only cargo, with space for 12 grocery bags in the first model.\n\nIn June 2018, Nuro announced its first partnership with Kroger. The partnership will test the fully autonomous delivery of groceries. The pilot launched on August 16, 2018 in Scottsdale, Arizona at a Fry's Food and Drug store. Initially this pilot will use self-driving Toyota Prius cars, and later this year will also include the R1 vehicle. \n\n"}
{"id": "979306", "url": "https://en.wikipedia.org/wiki?curid=979306", "title": "Orbital station-keeping", "text": "Orbital station-keeping\n\nIn astrodynamics, the orbital maneuvers made by thruster burns that are needed to keep a spacecraft in a particular assigned orbit are called orbital station-keeping.\n\nFor many Earth satellites the effects of the non-Keplerian forces, i.e. the deviations of the gravitational force of the Earth from that of a homogeneous sphere, gravitational forces from Sun/Moon, solar radiation pressure and air drag, must be counteracted.\n\nThe deviation of Earth's gravity field from that of a homogeneous sphere and gravitational forces from Sun/Moon will in general perturb the orbital plane. For a sun-synchronous orbit the precession of the orbital plane caused by the oblateness of the Earth is a desirable feature that is part of the mission design but the inclination change caused by the gravitational forces of Sun/Moon is undesirable. For geostationary spacecraft the inclination change caused by the gravitational forces of the Sun & Moon must be counteracted by a rather large expense of fuel, as the inclination should be kept sufficiently small for the spacecraft to be tracked by a non-steerable antenna.\n\nFor spacecraft in low orbits the effects of atmospheric drag must often be compensated for. For some missions this is needed simply to avoid re-entry; for other missions, typically missions for which the orbit should be accurately synchronized with Earth rotation, this is necessary to avoid the orbital period shortening.\n\nSolar radiation pressure will in general perturb the eccentricity (i.e. the eccentricity vector), see Orbital perturbation analysis (spacecraft). For some missions this must be actively counter-acted with manoeuvres. For geostationary spacecraft the eccentricity must be kept sufficiently small for a spacecraft to be tracked with a non-steerable antenna. Also for Earth observation spacecraft for which a very repetitive orbit with a fixed ground track is desirable, the eccentricity vector should be kept as fixed as possible. A large part of this compensation can be done by using a frozen orbit design, but for the fine control manoeuvres with thrusters are needed.\n\nFor spacecraft in a halo orbit around a Lagrangian point station-keeping is even more fundamental, as such an orbit is unstable; without an active control with thruster burns the smallest deviation in position/velocity would result in the spacecraft leaving the orbit completely.\n\nFor a spacecraft in a very low orbit the atmospheric drag is sufficiently strong to cause a re-entry before the intended end of mission if orbit raising manoeuvres are not executed from time to time.\n\nAn example of this is the International Space Station (ISS), which has an operational altitude above Earth's surface of between 330 and 410 km. Due to atmospheric drag the space station is constantly losing orbital energy. In order to compensate for this loss, which would eventually lead to a re-entry of the station, it has from time to time been re-boosted to a higher orbit. The chosen orbital altitude is a trade-off between the average thrust needed to counter-act the air drag and the delta-v needed to send payloads and people to the station. \nThe upper limitation of orbit altitude is due to the constraints imposed by the Soyuz spacecraft. On 25 April 2008, the Automated Transfer Vehicle \"Jules Verne\" raised the orbit of the ISS for the first time, thereby proving its ability to replace (and outperform) the Soyuz at this task.\n\nGOCE which orbited at 255 km (later reduced to 235 km) used ion thrusters to provide up to 20 mN of thrust to compensate for the drag on its frontal area of about 1 m.\n\nFor Earth observation spacecraft typically operated in an altitude above the Earth surface of about 700 – 800 km the air-drag is very faint and a re-entry due to air-drag is not a concern. But if the orbital period should be synchronous with the Earth's rotation to maintain a fixed ground track, the faint air-drag at this high altitude must also be counter-acted by orbit raising manoeuvres in the form of thruster burns tangential to the orbit. These manoeuvres will be very small, typically in the order of a few mm/s of delta-v. If a frozen orbit design is used these very small orbit raising manoeuvres are sufficient to also control the eccentricity vector.\n\nTo maintain a fixed ground track it is also necessary to make out-of-plane manoeuvres to compensate for the inclination change caused by Sun/Moon gravitation. These are executed as thruster burns orthogonal to the orbital plane. For Sun-synchronous spacecraft having a constant geometry relative to the Sun, the inclination change due to the solar gravitation is particularly large; a delta-v in the order of 1–2 m/s per year can be needed to keep the inclination constant.\n\nFor geostationary spacecraft, thruster burns orthogonal to the orbital plane must be executed to compensate for the effect of the lunar/solar gravitation that perturbs the orbit pole with typically 0.85 degrees per year. The delta-v needed to compensate for this perturbation keeping the inclination to the equatorial plane amounts to in the order 45 m/s per year. This part of the GEO station-keeping is called North-South control.\n\nThe East-West control is the control of the orbital period and the eccentricity vector performed by making thruster burns tangential to the orbit. These burns are then designed to keep the orbital period perfectly synchronous with the Earth rotation and to keep the eccentricity sufficiently small. Perturbation of the orbital period results from the imperfect rotational symmetry of the Earth relative the North/South axis, sometimes called the ellipticity of the Earth equator. The eccentricity (i.e. the eccentricity vector) is perturbed by the solar radiation pressure. The fuel needed for this East-West control is much less than what is needed for the North-South control.\n\nTo extend the life-time of ageing geostationary spacecraft with little fuel left one sometimes discontinues the North-South control only continuing with the East-West control. As seen from an observer on the rotating Earth the spacecraft will then move North-South with a period of 24 hours. When this North-South movement gets too large a steerable antenna is needed to track the spacecraft. An example of this is Artemis.\n\nTo save weight, it is crucial for GEO satellites to have the most fuel-efficient propulsion system. Some modern satellites are therefore employing a high specific impulse system like plasma or ion thrusters.\n\nOrbits of spacecraft are also possible around Lagrange points—also referred to as libration points—gravity wells that exist at five points in relation to two larger solar system bodies. For example, there are five of these points in the Sun-Earth system, five in the Earth-Moon system, and so on. Small spacecraft may orbit around these gravity wells with a minimum of propellant required for station-keeping purposes. Two orbits that have been used for such purposes include halo and Lissajous orbits.\n\nOrbits around libration points are dynamically unstable, meaning small departures from equilibrium grow exponentially over time. As a result, spacecraft in libration point orbits must use propulsion systems to perform orbital station-keeping.\n\nOne important libration point is Earth-Sun , and three heliophysics missions have been orbiting L1 since approximately 2000. Station-keeping propellant use can be quite low, facilitating missions that can potentially last decades should other spacecraft systems remain operational. The three spacecraft—Advanced Composition Explorer (ACE), Solar Heliospheric Observatory (SOHO), and the Global Geoscience WIND satellite—each have annual station-keeping propellant requirements of approximately 1 m/s or less.\nEarth-Sun —approximately 1.5 million kilometers from Earth in the anti-sun direction—is another important Lagrangian point, and the ESA Herschel space observatory operated there in a Lissajous orbit during 2009–2013, at which time it ran out of coolant for the space telescope. Small station-keeping orbital maneuvers were executed approximately monthly to maintain the spacecraft in the station-keeping orbit.\n\nThe James Webb Space Telescope needs to use propellant to maintain its halo orbit around the Earth-Sun L2, which provides an upper limit to its designed lifetime: it is being designed to carry enough for ten years.\n\n\n"}
{"id": "2457822", "url": "https://en.wikipedia.org/wiki?curid=2457822", "title": "Pressure swing adsorption", "text": "Pressure swing adsorption\n\nPressure swing adsorption (PSA) is a technology used to separate some gas species from a mixture of gases under pressure according to the species' molecular characteristics and affinity for an adsorbent material. It operates at near-ambient temperatures and differs significantly from cryogenic distillation techniques of gas separation. Specific adsorbent materials (e.g., zeolites, activated carbon, molecular sieves, etc.) are used as a trap, preferentially adsorbing the target gas species at high pressure. The process then swings to low pressure to desorb the adsorbed material.\n\nPressure swing adsorption processes utilize the fact that under high pressure, gases tend to be attracted to solid surfaces, or \"adsorbed\". The higher the pressure, the more gas is adsorbed. When the pressure is reduced, the gas is released, or desorbed. PSA processes can be used to separate gases in a mixture because different gases tend to be attracted to different solid surfaces more or less strongly. If a gas mixture such as air is passed under pressure through a vessel containing an adsorbent bed of zeolite that attracts nitrogen more strongly than oxygen, part or all of the nitrogen will stay in the bed, and the gas exiting the vessel will be richer in oxygen than the mixture entering. When the bed reaches the end of its capacity to adsorb nitrogen, it can be regenerated by reducing the pressure, thus releasing the adsorbed nitrogen. It is then ready for another cycle of producing oxygen-enriched air.\n\nThis is the process used in medical oxygen concentrators used by emphysema patients and others requiring oxygen-enriched air for breathing.\n\nUsing two adsorbent vessels allows near-continuous production of the target gas. It also permits so-called pressure equalisation, where the gas leaving the vessel being depressurised is used to partially pressurise the second vessel. This results in significant energy savings, and is common industrial practice.\n\nAside from their ability to discriminate between different gases, adsorbents for PSA systems are usually very porous materials chosen because of their large specific surface areas. Typical adsorbents are activated carbon, silica gel, alumina, resin and zeolite. Though the gas adsorbed on these surfaces may consist of a layer only one or at most a few molecules thick, surface areas of several hundred square meters per gram enable the adsorption of a significant portion of the adsorbent's weight in gas. In addition to their selectivity for different gases, zeolites and some types of activated carbon called carbon molecular sieves may utilize their molecular sieve characteristics to exclude some gas molecules from their structure based on the size of the molecules, thereby restricting the ability of the larger molecules to be adsorbed.\n\nAside from its use to supply medical oxygen, or as a substitute for bulk cryogenic or compressed-cylinder storage, which is the primary oxygen source for any hospital, PSA has numerous other uses. One of the primary applications of PSA is in the removal of carbon dioxide (CO) as the final step in the large-scale commercial synthesis of hydrogen (H) for use in oil refineries and in the production of ammonia (NH). Refineries often use PSA technology in the removal of hydrogen sulfide (HS) from hydrogen feed and recycle streams of hydrotreating and hydrocracking units. Another application of PSA is the separation of carbon dioxide from biogas to increase the methane (CH) ratio. Through PSA the biogas can be upgraded to a quality similar to natural gas. This includes a process in landfill gas utilization to upgrade landfill gas to utility-grade high purity methane gas to be sold as natural gas.\n\nPSA is also used in:-\n\n\nResearch is currently underway for PSA to capture CO in large quantities from coal-fired power plants prior to geosequestration, in order to reduce greenhouse gas production from these plants.\n\nPSA has also been discussed as a future alternative to the non-regenerable sorbent technology used in space suit Primary Life Support Systems, in order to save weight and extend the operating time of the suit.\n\n(DS-PSA, sometimes referred to as Dual Step PSA).\nWith this variation of PSA developed for use in Laboratory Nitrogen Generators generation of nitrogen gas is divided into two steps: in the first step, the compressed air is forced to pass through a carbon molecular sieve to produce nitrogen at a purity of approximately 98%; in the second step this nitrogen is forced to pass into a second carbon molecular sieve and the nitrogen gas reaches a final purity up to 99.999%. The purge gas from the second step is recycled and partially used as feed gas in the first step.\n\nIn addition, the purge process is supported by active evacuation for better performance in the next cycle. The goals of both of these changes is to improve efficiency over a conventional PSA process.\n\nThe DS-PSA is also applied to up levels oxygen concentration in this case a zeolite aluminum silica based adsorb Nitrogen in the first stage focusing Oxygen 95%, and in the second stage the molecular sieve carbon-based adsorbs the residual nitrogen in a reverse cycle, concentrating to 99% oxygen.\n\nRapid pressure swing adsorption or RPSA is frequently used in portable oxygen concentrators. It allows a significant reduction in the size of the adsorbent bed when high purity is not essential and feed gas can be discarded. It works by quickly cycling the pressure while alternately venting opposite ends of the column at the same rate. This means that unadsorbed gases progress along the column much faster and are vented at the distal end, while adsorbed gases do not get the chance to progress and are vented at the proximal end.\n\n"}
{"id": "27301515", "url": "https://en.wikipedia.org/wiki?curid=27301515", "title": "Psyllic acid", "text": "Psyllic acid\n\nPsyllic acid (also pysslostearic acid, tritriacontanoic acid or ceromelissic acid) is a saturated fatty acid.\n\nThe alkali salts of psyllic acid are precipitated when alcoholic solutions of the acid and an alkali hydroxide are mixed. The silver and barium salts can be obtained by adding aqueous alcoholic solutions of silver nitrate and barium chloride to alcoholic solutions of the acid. The following salts have been analyzed: CHONa, CHOBa, and CHOAg. Psylla wax is hydrolyzed by alcoholic potassium hydroxide as well as by hydrobromic acid.\n\nPsyllic acid is present in Chinese wolfberries.\n\n\n"}
{"id": "48493517", "url": "https://en.wikipedia.org/wiki?curid=48493517", "title": "Ranaram Bishnoi", "text": "Ranaram Bishnoi\n\nRanaram Bishnoi is a 70-year-old environmentalist of Ekalkhori village near Jodhpur, Rajsthan. He initiated planting trees in the desert and dry area near his village. He planted over 27,000 trees in 25 bigha land with a large earthen pitcher single-handedly. He is known as ahhu \"tree-man\" by the people of the area. He walks 3 km to reach the dune from his house. Defying his age he climbs it, goes down the other side to get the water from his friend’s tubewell and comes back. He planted an indigenous varieties of trees such as Neem, Rohida, Fig, Khejri, Kankeri, Babool and Bougainvillea.\n\nHe believes that non-human species have equal or rather more rights to live on earth than humans. He said that \"the plants and the animals were on the planet much before we landed here. They have more rights on the planet than us and if we cannot give that to them at least we can ensure we don’t destroy them in our greed.”\n\nHis co-worker is Khamu Ram Bishnoi, also known as Turban man. Planting these trees he stopped the desert from expanding over his village potentially ruining hundreds of crops.\n"}
{"id": "28940250", "url": "https://en.wikipedia.org/wiki?curid=28940250", "title": "Shweeb", "text": "Shweeb\n\nShweeb is a proposed personal rapid transit network based on human-powered monorail cars. The project prototype was originally designed and implemented in Rotorua, New Zealand as a leisure attraction. In September, 2010, a proposal for development of an expanded network was chosen to receive funding from Google as part of project 10. As of August 2017, there are no active proposals to utilise the system for public transportation although a \"sport resort\" in the United States is considering it. The original track in New Zealand is open to the public.\n\nThe proposed Shweeb transit network relies on recumbent bicycle technology to power pods suspended from monorails. According to Shweeb Monorail Technology, the intent of their proposal is to \"create a solution which provided the user with the same flexibility and comfort offered by the car but without the consequential costs - both direct financial and indirect health and environmental costs.\" The proposal envisions networks of monorail track providing point to point and commuter transit for urban areas.\n\nThe track is built in folded galvanised steel. Its external height is 220mm x width 200 mm. Support piles are also in galvanised steel.\n\nPods are covered with transparent plastic sheets, with ventilation holes. Front and rear long dampers are provided to limit the impact acceleration in case of pods collision and to ease the association of pods to build 'pod trains' which could significantly improve overall aerodynamic efficiency.\n\nTo help climb ramps, an electrically powered chain installed on a track section could push the pods for a limited distance, in a way similar to the Trampe bicycle lift. This could also help entering stations built at a higher elevation than the track. The purpose of this elevation is to help a pod gaining momentum while descending from station track to the main line track.\n\nA similar system developed by SkyRide Technologies has been used as an amusement ride aboard cruise ships operated by Carnival Cruise Line.\n\n\n"}
{"id": "17035657", "url": "https://en.wikipedia.org/wiki?curid=17035657", "title": "Single-stream recycling", "text": "Single-stream recycling\n\nSingle-stream (also known as “fully commingled” or \"single-sort\") recycling refers to a system in which all paper fibers, plastics, metals, and other containers are mixed in a collection truck, instead of being sorted by the depositor into separate commodities (newspaper, paperboard, corrugated fiberboard, plastic, glass, etc.) and handled separately throughout the collection process. In single-stream, both the collection and processing systems are designed to handle this fully commingled mixture of recyclables, with materials being separated for reuse at a materials recovery facility (MRF). The single-stream option replaces the dual-stream option, which is where people separate certain recyclable materials and place them in separate containers for collection.\n\nSingle-stream recycling programs were first developed in several California communities in the 1990s. Subsequently, many large and small municipalities across the United States began single-stream programs. As of 2012, there are 248 MRFs operating in the U.S. As of 2013, 100 million Americans were served by single-stream programs.\n\nProponents of single-stream note several advantages:\n\nPotential disadvantages of single-stream recycling may include:\n\nA single-stream system is a complex network of machinery that uses a combination of newer and older technologies to sort materials for recycling, including PET, HDPE, aluminum, tin cans, cardboard and paper.\n\nList of equipment used in a single-stream system:\n\n\n"}
{"id": "4680415", "url": "https://en.wikipedia.org/wiki?curid=4680415", "title": "Steam-assisted gravity drainage", "text": "Steam-assisted gravity drainage\n\nSteam-assisted gravity drainage (SAGD; \"Sag-D\") is an enhanced oil recovery technology for producing heavy crude oil and bitumen. It is an advanced form of steam stimulation in which a pair of horizontal wells is drilled into the oil reservoir, one a few metres above the other. High pressure steam is continuously injected into the upper wellbore to heat the oil and reduce its viscosity, causing the heated oil to drain into the lower wellbore, where it is pumped out. Dr. Roger Butler, engineer at Imperial Oil from 1955 to 1982, invented the steam assisted gravity drainage (SAGD) process in the 1970s. Butler \"developed the concept of using horizontal pairs of wells and injected steam to develop certain deposits of bitumen considered too deep for mining\". In 1983 Butler became director of technical programs for the Alberta Oil Sands Technology and Research Authority (AOSTRA), a crown corporation created by Alberta Premier Lougheed to promote new technologies for oil sands and heavy crude oil production. AOSTRA quickly supported SAGD as a promising innovation in oil sands extraction technology.\n\nSteam Assisted Gravity Drainage (SAGD) and Cyclic Steam Stimulation (CSS) Steam_injection_(oil_industry) are two commercially applied primal thermal recovery processes used in the oil sands in Geological formation sub-units, such as Grand Rapids Formation, Clearwater Formation, McMurray Formation, General Petroleum Sand, Lloydminster Sand, of the Mannville Group, a Stratigraphic range in the Western Canadian Sedimentary Basin.\n\nCanada is now the single largest supplier of imported oil to the United States, supplying over 35% of US imports, much more than Saudi Arabia or Venezuela, and more than all the OPEC countries combined. Most of the new production comes from Alberta's vast oil sands deposits. There are two primary methods of oil sands recovery. The strip-mining technique is more familiar to the general public, but can only be used for shallow bitumen deposits. However, the more recent steam-assisted gravity drainage technique (SAGD) is better suited to the much larger deep deposits that surround the shallow ones. Much of the expected future growth of production in the Canadian oil sands is predicted to be from SAGD.\n\nSteam Assisted Gravity Drainage emissions are equivalent to what is emitted by the steam flood projects which have long been used to produce heavy oil in California's Kern River Oil Field and elsewhere around the world.\n\nThe SAGD process of heavy oil or bitumen production is an enhancement on the steam injection techniques originally developed to produce heavy oil from the Kern River Oil Field of California. The key to all steam flooding processes is to deliver heat to the producing formation to reduce the viscosity of the heavy oil and enable it to move toward the producing well. The cyclic steam stimulation (CSS) process developed for the California heavy oil fields was able to produce oil from some portions of the Alberta oil sands, such as the Cold Lake oil sands, but did not work as well to produce bitumen from heavier and deeper deposits in the Athabasca oil sands and Peace River oil sands, where the majority of Alberta's oil sands reserves lie. To produce these much larger reserves, the SAGD process was developed, primarily by Dr. Roger Butler of Imperial Oil with the assistance of the Alberta Oil Sands Technology and Research Authority and industry partners. The SAGD process is estimated by the National Energy Board to be economic when oil prices are at least US$30 to $35 per barrel.\n\nIn the SAGD process, two parallel horizontal oil wells are drilled in the formation, one about 4 to 6 metres above the other. The upper well injects steam, and the lower one collects the heated crude oil or bitumen that flows down due to gravity, plus recovered water from the condensation of the injected steam. The basis of the SAGD process is that thermal communication is established with the reservoir so that the injected steam forms a \"steam chamber\". The heat from the steam reduces the viscosity of the heavy crude oil or bitumen which allows it to flow down into the lower wellbore. The steam and associated gas rise because of their low density compared to the heavy crude oil below, ensuring that steam is not produced at the lower production well, tend to rise in the steam chamber, filling the void space left by the oil. Associated gas forms, to a certain extent, an insulating heat blanket above (and around) the steam. Oil and water flow is by a countercurrent, gravity driven drainage into the lower well bore. The condensed water and crude oil or bitumen is recovered to the surface by pumps such as progressive cavity pumps that work well for moving high-viscosity fluids with suspended solids.\n\nSub-cool is the difference between the saturation temperature (boiling point) of water at the producer pressure and the actual temperature at the same place where the pressure is measured. The higher the liquid level above the producer the lower the temperature and higher is the sub-cool. However real life reservoirs are invariably heterogeneous therefore it becomes extremely difficult to achieve a uniform sub-cool along the entire horizontal length of a well. As a consequence many operators, when faced with uneven stunted steam chamber development, allow a small quantity of steam to enter into the producer to keep the bitumen in the entire wellbore hot hence keeping its viscosity low with the added benefit of transferring heat to colder parts of the reservoir along the wellbore. Another variation sometimes called Partial SAGD is used when operators deliberately circulate steam in the producer following a long shut-in period or as a startup procedure. Though a high value of sub-cool is desirable from a thermal efficiency standpoint as it generally includes reduction of steam injection rates but it also results in slightly reduced production due to a corresponding higher viscosity and lower mobility of bitumen caused by lower temperature. Another drawback of very high sub-cool is the possibility of steam pressure eventually not being enough to sustain steam chamber development above the injector, sometimes resulting in collapsed steam chambers where condensed steam floods the injector and precludes further development of the chamber.\n\nContinuous operation of the injection and production wells at approximately reservoir pressure eliminates the instability problems that plague all high-pressure and cyclic steam processes and SAGD produces a smooth, even production that can be as high as 70% to 80% of oil in place in suitable reservoirs. The process is relatively insensitive to shale streaks and other vertical barriers to steam and fluid flow because, as the rock is heated, differential thermal expansion allows steam and fluids to gravity flow through to the production well. This allows recovery rates of 60% to 70% of oil in place, even in formations with many thin shale barriers. Thermally, SAGD is generally twice as efficient as the older cyclic steam stimulation (CSS) process, and it results in far fewer wells being damaged by the high pressures associated with CSS. Combined with the higher oil recovery rates achieved, this means that SAGD is much more economic than cyclic steam processes where the reservoir is reasonably thick.\n\nThe gravity drainage idea was originally conceived by Dr. Roger Butler, an engineer for Imperial Oil in the 1970s In 1975 Imperial Oil transferred Butler from Sarnia, Ontario to Calgary, Alberta to head their heavy oil research effort. He tested the concept with Imperial Oil in 1980, in a pilot at Cold Lake which featured one of the first horizontal wells in the industry, with vertical injectors.\n\nIn 1974, former Premier of Alberta Peter Lougheed created the Alberta Oil Sands Technology and Research Authority (AOSTRA) as an Alberta crown corporation to promote the development and use of new technology for oil sands and heavy crude oil production, and enhanced recovery of conventional crude oil. Its first facility was owned and operated by ten industrial participants and received ample government support (Deutsch and McLennan 2005) including from the Alberta Heritage Savings Trust Fund. One of the main targets of AOSTRA finding of suitable technologies for that part of the Athabasca oil sands that could not be recovered using conventional surface mining technologies.\n\nIn 1984, AOSTRA initiated the Underground Test Facility in the Athabasca oil sands, located between the MacKay Rivers and the Devon River west of the Syncrude plant as an in-situ SAGD bitumen recovery facility. It was here that their first test of twin (horizontal) SAGD wells took place, proving the feasibility of the concept, briefly achieving positive cash flow in 1992 at a production rate of about 2000 bbl/day from 3 well pairs.\n\nThe Foster Creek plant in Alberta Canada, built in 1996 and operated by Cenovus Energy, was the first commercial Steam-assisted gravity drainage (SAGD) project and by 2010 Foster Creek \"became the largest commercial SAGD project in Alberta to reach royalty payout status. \"\n\nThe original UTF SAGD wells were drilled horizontally from a tunnel in the limestone underburden, accessed with vertical mine shafts. The concept coincided with development of directional drilling techniques that allowed companies to drill horizontal wells accurately, cheaply and efficiently, to the point that it became hard to justify drilling a conventional vertical well any more. With the low cost of drilling horizontal well pairs, and the very high recovery rates of the SAGD process (up to 60% of the oil in place), SAGD is economically attractive to oil companies.\n\nAt Foster Creek Cenovus has employed its patented 'wedge well' technology to recover residual resources bypassed by regular SAGD operations, this improves the total recovery rate of the operation. The 'wedge well' technology works by accessing the residual bitumen that is bypassed in regular SAGD operations by drilling an infill well between two established operating SAGD well pairs once the SAGD steam chambers have matured to the point where they have merged and are in fluid communication and then what is left to recover in that reservoir area between the operating SAGD well pairs is a 'wedge' of residual, bypassed oil. Wedge well technology has been shown to improve overall recovery rates by 5%-10% at a reduced capital cost as less steam is required once the steam chambers mature to the point where they are in fluid communication and typically at this stage in the recovery process, also commonly known as the 'blow down' phase, the injected steam is replaced with a non-condensable gas such as methane, further reducing production costs.\n\nThis technology is now being exploited due to increased oil prices. While traditional drilling methods were prevalent up until the 1990s, high crude prices of the 21st Century are encouraging more unconventional methods (such as SAGD) to extract crude oil. The Canadian oil sands have many SAGD projects in progress, since this region is home of one of the largest deposits of bitumen in the world (Canada and Venezuela have the world's largest deposits).\n\nThe SAGD process allowed the Alberta Energy Resources Conservation Board (ERCB) to increase its proven oil reserves to 179 billion barrels, which raised Canada's oil reserves to the third highest in the world after Venezuela and Saudi Arabia and approximately quadrupled North American oil reserves. As of 2011, the oil sands reserves stand at around 169 billion barrels.\n\nSAGD, a thermal recovery process, consumes large quantities of water and natural gas.\n\"Petroleum from the Canadian oil sands extracted via surface mining techniques can consume 20 times more water than conventional oil drilling.\" However, by 2011 there was inadequate data on the amount of water used in the increasingly important steam-assisted gravity drainage technique (SAGD) method. Evaporators can treat the SAGD produced water to produce high quality freshwater for reuse in SAGD operations. However, evaporators produce a high volume blowdown waste which requires further management.\n\nAs in all thermal recovery processes, cost of steam generation is a major part of the cost of oil production. Historically, natural gas has been used as a fuel for Canadian oil sands projects, due to the presence of large stranded gas reserves in the oil sands area. However, with the building of natural gas pipelines to outside markets in Canada and the United States, the price of gas has become an important consideration. The fact that natural gas production in Canada has peaked and is now declining is also a problem. Other sources of generating heat are under consideration, notably gasification of the heavy fractions of the produced bitumen to produce syngas, using the nearby (and massive) deposits of coal, or even building nuclear reactors to produce the heat.\n\nA source of large amounts of fresh and brackish water and large water re-cycling facilities are required in order to create the steam for the SAGD process. Water is a popular topic for debate in regards to water use and management. As of 2008, American petroleum production (not limited to SAGD) generates over 5 billion gallons of produced water every day. The concern of using large amounts of water has little to do with proportion of water used, rather the quality of the water. Traditionally close to 70 million cubic metres of the water volume that was used in the SAGD process was fresh, surface, water. There has been a significant reduction in fresh water use as of 2010, when approximately 18 million cubic metres were used. Though to offset the drastic reduction in fresh water use, industry has begun to significantly increase the volume of saline groundwater involved. This, as well as other, more general water saving techniques have allowed surface water usage by oil sands operations to decrease by more than threefold since production first began.\nRelying upon gravity drainage, SAGD also requires comparatively thick and homogeneous reservoirs, and so is not suitable for all heavy-oil production areas.\n\nBy 2009 the two commercially applied primal thermal recovery processes, Steam Assisted Gravity Drainage (SAGD) and Cyclic Steam Stimulation (CSS), were used in oil sands production in the Clearwater and Lower Grand Rapids Formations in the Cold Lake Area in Alberta.\n\nCanadian Natural Resources employs cyclic steam or \"huff and puff\" technology to develop bitumen resources. This technology requires one well bore and the production consists of the injection to fracture and heat the formation prior to the production phases. First steam is injected above the formation fracture point for several weeks or months, mobilizing cold bitumen, the well is then shut in for several weeks or months to allow the steam to soak into the formation. Then the flow on the injection well is reversed producing oil through the same injection well bore. The injection and production phases together comprise one cycle. Steam is re-injected to begin a new cycle when oil production rates fall below a critical threshold due to the cooling of the reservoir. Cyclic Steam Stimulation (CSS) also has a number of CSS Follow-up or Enhancement Processes, including Pressure Up and Blow Down (PUBD), Mixed Well Steam Drive and Drainage (MWSDD), Vapor Extraction (Vapex), Liquid Addition to Steam for Enhanced Recovery of Bitumen (LASER) and HPCSS Assisted SAGD and Hybrid Process.\n\n\"Roughly 35 per cent of all \"in situ\" production in the Alberta oil sands uses a technique called High Pressure Cyclic Steam Stimulation (HPCSS), which cycles between two phases: first, steam is injected into an underground oil sands deposit to fracture and heat the formation to soften the bitumen just like CSS does, excepting at even higher pressures; then, the cycle switches to production where the resulting hot mixture of bitumen and steam (called a “bitumen emulsion”) is pumped up to the surface through the same well, again just like CSS, until the resulting pressure drop slows production to an uneconomical stage. The process is then repeated multiple times. \" An Alberta Energy Regulator (AER) news release explained the difference between high pressure cyclic steam stimulation (HPCSS) and steam assisted gravity drainage (SAGD). \"HPCSS has been used in oil recovery in Alberta for more than 30 years. The method involves injecting high-pressure steam, well above the ambient reservoir pressure, into a reservoir over a prolonged period of time. As heat softens the bitumen and water dilutes and separates the bitumen from the sand, the pressure creates fractures, cracks and openings through which the bitumen can flow back into the steam-injector wells. HPCSS differs from steam assisted gravity drainage (SAGD) operations where steam is continuously injected at lower pressures without fracturing the reservoir and uses gravity drainage as the primary recovery mechanism. \"\n\nIn the Clearwater Formation near Cold Lake, Alberta the High Pressure Cyclic Steam Stimulation (HPCSS) is used. There are both horizontal and vertical wells. Injection is at fracture pressure. There is a 60 m to 180 m spacing for horizontal wells. Vertical wells are spaced at 2 to 8 Acre spacing for vertical wells. The development can be as low as 7 m net pay. It is used in areas generally with no to minimal bottom water or top gas. The CSOR is 3.3 to 4.5. The ultimate recovery is predicted at 15 to 35%. SAGD thermal recovery method is also used in Clearwater and Lower Grand Rapids Formations with Horizontal Well Pairs (700 to 1000 m), Operating pressure 3 to 5 MPa, Burnt Lake SAGD was started with higher operating pressure close to dilation pressure, 75 m to 120 m spacing, Development to as low as 10 m net pay, In areas with or without bottom water, CSOR: 2.8 to 4.0 (at 100% quality), Predicted ultimate recovery: 45% to 55%.\n\nCanadian Natural Resources Limited’s (CNRL) Primrose and Wolf Lake in situ oil sands project near Cold Lake, Alberta in the Clearwater Formation, operated by CNRL subsidiary Horizon Oil Sands, use the high pressure cyclic steam stimulation (HPCSS). \n\nAlternative enhanced oil recovery mechanisms include VAPEX (Vapor Assisted Petroleum Extraction), Electro-Thermal Dynamic Stripping Process (ET-DSP), and ISC (for In Situ Combustion). VAPEX, a \"gravity-drainage process that uses vapourized solvents rather than steam to displace or produce heavy oil and reduce its viscosity, was also invented by Butler.\n\nET-DSP is a patented process that uses electricity to heat oil sands deposits to mobilize bitumen allowing production using simple vertical wells. ISC uses oxygen to generate heat that diminishes oil viscosity; alongside carbon dioxide generated by heavy crude oil displace oil toward production wells. One ISC approach is called THAI for Toe to Heel Air Injection.\n\neMSAGP is a MEG Energy patented process wherein MEG, in partnership with Cenovus, developed a modified recovery process dubbed “enhanced Modified Steam and Gas Push” (eMSAGP), a modification of SAGP designed to improve the thermal efficiency of SAGD by utilizing additional producers located midway between adjacent SAGD well pairs, at the elevation of the SAGD producers. These additional producers, commonly referred to as “infill” wells, are an integral part of the eMSAGP recovery system.\n\n\n"}
{"id": "305844", "url": "https://en.wikipedia.org/wiki?curid=305844", "title": "Stere", "text": "Stere\n\nThe stere or stère is a unit of volume in the original metric system equal to one cubic metre. The name was coined from the Greek στερεός \"stereos\", \"solid\", in 1793 France as a metric analogue to the cord. The stère is typically used for measuring large quantities of firewood or other cut wood, while the cubic meter is used for uncut wood. It is not part of the modern metric system (SI).\n\nIn Dutch, there is also a \"kuub\", short for \"kubieke meter\" which differs from a stere. Whereas a \"kuub\" is a solid cubic metre, as it was traditionally used for wood, a stère is a cubic metre pile of woodblocks. A stère is less than a kuub or full cubic metre of wood, because the spaces between the woodblocks are included in a stère, while they do not count towards a kuub. In Finnish, the same unit is known as \"motti\" (from Swedish \"mått\", \"measure\").\n\nNote that the stère as used in contexts outside the timber industry is not subject to the same ambiguity. In particular, stère and kilostère are sometimes used in hydrology, as the kilostere (1000 m) is a slightly smaller metric analog of an acre-foot (≈ 1233 m), similar to the relationship of the tonne and (short) ton.\n"}
{"id": "2646364", "url": "https://en.wikipedia.org/wiki?curid=2646364", "title": "Synthetic alexandrite", "text": "Synthetic alexandrite\n\nSynthetic alexandrite is an artificially-grown variety of chrysoberyl crystal, composed of beryllium aluminum oxide (BeAlO). The name is also often used erroneously to describe synthetically-grown corundum.\n\nMost true synthetic Alexandrite is grown by pulling, known as the Czochralski method. Another method is a \"floating zone\". This method was developed by an Armenian scientist, Khachatur Saakovich Bagdasarov, of the Russian (former Soviet) Institute of Crystallography, in Moscow, in 1964, and was widely used in production of white YAG for spacecraft and submarine illuminators, before finding its way into the jewelry market. Alexandrite crystals grown by floating zone method tend to have less intensity in color than crystals grown by the pulled method.\n\nFlux-grown alexandrite is more difficult to identify because the inclusions of undissolved flux can look like natural inclusions. Alexandrite grown by the flux-melt process will contain particles of flux, resembling liquid feathers with a refractive index and specific gravity that echo that of the natural material. Layers of dust-like particles parallel to the seed plate, and strong banding or growth lines may also be apparent. Some stones contain groups of parallel negative crystals. Flux grown alexandrites are more difficult to spot because the colors are convincing and because they are not clean. These stones are expensive to make and are grown in platinum crucibles. Crystals of platinum may still be evident in the cut stones. Due to the cost of this process, it is no longer used commercially.\n\nCzochralski or pulled alexandrite is easier to identify because it is very clean. Curved striations visible with magnification are a give away. The color change in pulled stones has seen change from blue to red, which resemble alexandrite from Brazil, Madagascar, and India. Seiko synthetic alexandrites have a swirled internal structure characteristic of the floating zone method of synthesis. They have tadpole inclusions (with long tails) and spherical bubbles.\n\nThe Inamori synthetic alexandrite had a cat's eye variety, which showed a distinct color change. The eye was broad and of moderate intensity. Specimens were a dark greyish-green with slightly purple overtones under fluorescent lighting. The eye was slightly greenish-bluish-white and the stones were dull and oily. They appeared to be inclusion-free and under a strong incandescent light in the long direction, asterism could be seen with two rays weaker than the eye. This has not been reported in natural alexandrite. Under magnification, parallel striations could be seen along the length of the cabochon and the striations were undulating rather than straight, again not a feature of natural alexandrite.\n\nThe name \"allexite\" has been used for synthetic alexandrite manufactured by the Diamonair Corporation who maintains that its product is Czochralski-grown.\nThe largest producer of jewelry quality laboratory-grown alexandrite to this day is Tairus. Production capacity is in the range of 100 kg/year.\n\nMost gemstones described as synthetic alexandrite are actually simulated alexandrite - synthetic corundum laced with vanadium to produce the color change. This alexandrite like sapphire material has been around for almost 100 years. The material shows a characteristic purple-mauve color change which, although attractive, differs from alexandrite because there is never any green. The stones will be very clean and may be available in large sizes. Gemological testing will reveal a refractive index of 1.759–1.778 (corundum) instead of 1.741–1.760 (chrysoberyl). Under magnification, gas bubbles and curved stria may be evident. When examined with a spectroscope a strong vanadium absorption line at 475 nm will be apparent.\n\n"}
{"id": "724958", "url": "https://en.wikipedia.org/wiki?curid=724958", "title": "Thorium dioxide", "text": "Thorium dioxide\n\nThorium dioxide (ThO), also called thorium(IV) oxide, is a crystalline solid, often white or yellow in color. Also known as thoria, it is produced mainly as a by-product of lanthanide and uranium production. Thorianite is the name of the mineralogical form of thorium dioxide. It is moderately rare and crystallizes in an isometric system. The melting point of thorium oxide is 3300 °C – the highest of all known oxides. Only a few elements (including tungsten and carbon) and a few compounds (including tantalum carbide) have higher melting points. All thorium compounds are radioactive because there are no stable isotopes of thorium.\n\nThoria exists as two polymorphs. One has a fluorite crystal structure. This is uncommon among binary dioxides (others with fluorite structure include cerium dioxide, uranium dioxide and plutonium dioxide). The band gap of thoria is about 6 eV. A tetragonal form of thoria is also known.\n\nThorium dioxide is more stable than thorium monoxide (ThO). Only with careful control of reaction conditions can oxidation of thorium metal give the monoxide rather than the dioxide. At extremely high temperatures, the dioxide can convert to the monoxide either in by an disproportionation reaction (equilibrium with liquid thorium metal) above or by simple dissociation (evolution of oxygen) above .\n\nThorium dioxide (thoria) can be used in nuclear reactors as ceramic fuel pellets, typically contained in nuclear fuel rods clad with zirconium alloys. Thorium is not fissile (but is \"fertile\", breeding fissile uranium-233 under neutron bombardment); hence, it must be used as a nuclear reactor fuel in conjunction with fissile isotopes of either uranium or plutonium. This can be achieved by blending thorium with uranium or plutonium, or using it in its pure form in conjunction with separate fuel rods containing uranium or plutonium. Thorium dioxide offers advantages over conventional uranium dioxide fuel pellets, because of its higher thermal conductivity (lower operating temperature), considerably higher melting point, and chemical stability (does not oxidize in the presence of water/oxygen, unlike uranium dioxide).\n\nThorium dioxide can be turned into a nuclear fuel by breeding it into uranium-233 (see below and refer to the article on thorium for more information on this). The high thermal stability of thorium dioxide allows applications in flame spraying and high-temperature ceramics.\n\nThorium dioxide is used as a stabilizer in tungsten electrodes in TIG welding, electron tubes, and aircraft engines. As an alloy, thoriated tungsten metal is not easily deformed because the high-fusion material thoria augments the high-temperature mechanical properties, and thorium helps stimulate the emission of electrons (thermions). It is the most popular oxide additive because of its low cost, but is being phased out in favor of non-radioactive elements such as cerium, lanthanum and zirconium.\n\nThoria dispersed nickel finds its applications in various high temperature operations like combustion engines because it is a good creep resistant material. It can also be used for hydrogen trapping.\n\nThorium dioxide has almost no value as a commercial catalyst, but such applications have been well investigated. It is a catalyst in the Ruzicka large ring synthesis. Other applications that have been explored include petroleum cracking, conversion of ammonia to nitric acid and preparation of sulfuric acid.\n\nThorium dioxide was the primary ingredient in Thorotrast, a once-common radiocontrast agent used for cerebral angiography, however, it causes a rare form of cancer (hepatic angiosarcoma) many years after administration. This use was replaced with injectable iodine or ingestable barium sulfate suspension as standard X-ray contrast agents.\n\nAnother major use in the past was in gas mantle of lanterns developed by Carl Auer von Welsbach in 1890, which are composed of 99 percent ThO and 1% cerium(IV) oxide. Even as late as the 1980s it was estimated that about half of all ThO produced (several hundred tonnes per year) was used for this purpose. Some mantles still use thorium, but yttrium oxide (or sometimes zirconium oxide) is used increasingly as a replacement.\n\nThorium dioxide was formerly added to glasses during manufacture to increase their refractive index, producing thoriated glass with up to 40% ThO content. These glasses were used in the construction of high-quality photographic lenses. However, the radioactivity of the thorium caused both a safety and pollution hazard and self-degradation of the glass (turning it yellow or brown over time). Lanthanum oxide has replaced thorium dioxide in almost all modern high-index glasses.\n"}
{"id": "45404713", "url": "https://en.wikipedia.org/wiki?curid=45404713", "title": "UCL Australia", "text": "UCL Australia\n\nUCL Australia is an international campus of the University College London, located on Victoria Square in Adelaide, South Australia. It has three parts: the School of Energy and Resources (SERAus), the International Energy Policy Institute (IEPI) and a branch of UCL's Mullard Space Science Laboratory. UCL Australia describes its university community as \"welcoming, dynamic and influential.\"\n\nIn December 2008, Professor Michael Worton (Academic & International UCL Vice-Provost) said of the establishment of UCL Australia that the university was \"committed to working to solve real-world problems and we relish the opportunity to work not only with the South Australian Government but also with Santos and a range of other Australian and international energy companies through our presence in Adelaide.\" UCL Australia established key corporate partnerships with two major resource and energy companies operating in South Australia: Santos and BHP Billiton. Santos' South Australian interests include onshore and offshore oil and gas developments while BHP Billiton's interest is concentrated on the expansion of the Olympic Dam mine- the world's largest known deposit of uranium. Its campus was established in the Torrens Building on Victoria Square, Adelaide after the Government of South Australia committed AUD$4 million to refurbishing the building. The building also houses an international campus of Carnegie Mellon University. In 2010, UCL Australia completed its first full academic year. Agreements between the partners were negotiated by Adelaide lawyer and public servant, Pamela Martin.\n\nIn January 2015, UCL Australia announced that its campus would close within three years but agreed to support currently enrolled students through their degrees and courses. A $10 million agreement with the Government of South Australia and Santos expires in 2017.\n\nIn 2012, research undertaken at UCL Australia included efforts to address problems in water processing for coal seam gas (coal bed methane), design evaporative cooling systems for buildings using sea water and develop integrated energy systems for sustainable wine production.\n\nAs of 2015, UCL Australia's research is focused on the following areas:\n\nThe UCL School of Energy and Resources was established in partnership with the Government of South Australia and oil and gas company, Santos Limited. It was established in 2009, with its first full academic year commencing in 2010. Its objective was to develop management capability to help the resources and energy sector meet the challenges of energy security, affordability and regulation, sustainability, environmental impact and climate change.\n\nIn 2015, current research projects undertaken at the School of Energy and Resources include:\n\nThe School of Energy & Resources has offered incentives for student enrollment, initially awarding 10 Santos scholarships to students wishing to undertake a Masters of Science in Energy and Resources. The scholarships covered full tuition fees and provided each recipient an additional $25,000 annual stipend. In 2016, scholarships were still being offered, with each scholarship \"worth\" up to $114,500 over two years, comprising full tuition plus a AUD$50,000 tax-free stipend.\n\nThe International Energy Policy Institute (IEPI) is housed on the Adelaide campus of University College London, Australia. In 2011, UCL signed a five-year $10 million partnership with BHP Billiton to establish the International Energy Policy Institute in Adelaide and an Institute for Sustainable Resources in London. The Institute was created to address challenges of complexity and sensitivity in the energy policy field through intensive research. Stefaan Simons was appointed the inaugural BHP Billiton Chair of Energy Policy. His directorship of the Institute commenced on 1 September 2012.\n\nThe Institute was seeded by donations from oil and gas company Santos and the resource multi-national BHP Billiton.\n\nResearch at IEPI is focused on upstream (exploration and production) issues, acknowledging the Asia Pacific region's influence on global Coal, nuclear and gas markets, and its growing uptake in renewable energy. The Institute complements and contrasts the downstream (consumer) focus of the UCL Energy Institute which is based in London.\n\nResearch undertaken at IEPI follows four themes:\nAs of 2015, current projects at the IEPI include: \n\nNotable staff of the IEPI include Emeritus Professor Anthony \"Tony\" Owen, Visiting Professor Timothy \"Tim\" Stone CBE (non-executive director of Horizon Nuclear Power) and Honorary Reader James \"Jim\" Voss (former managing director of Pangea Resources).\n\nUCL Australia has presented a series of lectures, most of which have been accessible to the general public. Subjects and presenters have included:\nUCL Australia's governance structure includes a Management Team, an Academic Board and an Advisory Board.\n\nAs of April 2016, its Academic Board's membership includes representatives from: the Australian School of Petroleum at the University of Adelaide, the School of Engineering at the University of South Australia, the Dean of Brunel University London and the CTO of Aveillant in the UK. Its Advisory Board members include representatives of University College London, BHP Billiton, Santos Ltd, Cheung Kong Infrastructure Holdings, the Department of the Premier and Cabinet (South Australia), South Australia's Economic Development Board (Tanya Monro), the University of South Australia and former Australian politicians Jane Lomax-Smith and Martin Ferguson.\n\nIn 2011, former Federal minister Alexander Downer addressed UCL students to discuss the nuclear industry. Prior to his presentation he told the media of his support for the establishment of a nuclear waste dump in South Australia, and described a possible future scenario in which a nuclear power plant could power a seawater desalination plant in order to provide water for BHP Billiton's Olympic Dam mine.\n\nIn 2012, Stefaan Simons was appointed the inaugural Director of the International Energy Policy Institute, and the BHP Billiton Chair of Energy Policy. Simons has acknowledged that asking \"whether Australia could, and should, develop a nuclear power service industry based on uranium enrichment and fuel rod manufacture for the global market\" is a key theme of the Institute's work. In a 2013 article entitled \"Is it time for nuclear energy for Australia?\" Simons proposed that goals of securing energy supply, maintaining economic growth and mitigating impacts of climate change could all be advanced by including nuclear in a \"low-emission energy mix\" for Australia. On UCL's role in the process he wrote: \"University College London’s International Energy Policy Institute (IEPI), based at its Australia campus in Adelaide, undertakes economic, regulatory and policy research on how Australia could develop a nuclear energy industry and manage its externalities, including decommissioning and waste.\"\n\nIn late 2013, UCL staff and students contributed to conference papers investigating the subject of nuclear submarine development in Australia. Papers entitled \"What would it take for Australia to develop a nuclear-powered submarine capability?\" and \"From subs to Mines: What would it take for Australia to develop a nuclear-powered submarine capability?\" were presented in Brisbane, Australia and at the AIChE Annual Meeting in San Francisco, USA respectively. The subject was further explored in 2014 with the presentation of a conference paper entitled \"Selecting Nuclear-Powered Submarines in Australia: Nuclear Waste Consideration\" at a Waste Management conference (WM2014) in Phoenix, Arizona.\n\nIn 2014, former Federal resources and energy minister Martin Ferguson was appointed as chairman of the UCL Australia board. Ferguson is an advocate for nuclear power in Australia. UCL Australia's Chief Executive David Travers said of Ferguson's appointment: \"UCL doesn’t want to be large in Australia, but we do want to be influential and welcome Martin to the team to help us achieve these goals.”Also in 2014, James \"Jim\" Voss, a senior nuclear engineer and Fellow of the UK Nuclear Institute was appointed Honorary Reader at UCL Australia's International Energy Policy Institute. He had previously served in the Executive Office of the President of the United States under two Presidents and advised senior government officials in other countries. He is also a former Managing Director of Pangea Resources, the proponent of a proposal to establish a nuclear waste dump in Australia in the late 1990s.\n\nResearch conducted at UCL in 2014 included several studies investigating the prospect of expanding nuclear industrial activity in Australia and South Australia. These included work by staff Dr Michel Berthelemy and Dr Tim Stone on \"Nuclear fuel cycle strategies\" and work by UCL students investigating nuclear fuel leasing opportunities. Student research subjects included \"The legal merits of an Australian Nuclear Fuel Leasing scheme\" by Owen Sharpe\", and The World’s first integrated nuclear fuel leasing in South Australia? A proposed business model and its economic appraisal\" by Iwan Setiyono Ko. After graduating, Sharpe was recruited to South Australia's Department of the Premier and Cabinet as a Senior Policy Officer.\n\nIn March 2014, briefings on nuclear fuel leasing were given by UCL staff to Parsons Brinkerhoff, Deloitte and Babcock. In May a further briefing on the subject was given by Martin Ferguson at a confidential event.\n\nOn 4 December 2014, Stefaan Simons and Tim Stone presented a conference paper entitled \"The international management of spent nuclear fuel\" at the Nuclear Industries Association Annual Meeting in London, United Kingdom.\n\nIn April 2015, Visiting Professor Dr Timothy Stone was appointed to the Expert Advisory Committee of the Nuclear Fuel Cycle Royal Commission, an inquiry initiated at the request the Government of South Australia.\n\nUCL Australia established a Nuclear Working Group \"to share scientific knowledge in relation to the main issues identified by the Royal Commission; to assist and facilitate the process leading up to informed community decisions\". Group members include: Magnus Nyden (Head), Christian Ekberg, Paola Lettieri, Jonathan Mirrlees-Black, Michael Pollitt, Tim Stone, Pam Sykes, Geraldine Thomas, Jim Voss and Max Zanin.\n\n\n"}
{"id": "16125425", "url": "https://en.wikipedia.org/wiki?curid=16125425", "title": "Weighted average cost of carbon", "text": "Weighted average cost of carbon\n\nThe Weighted average cost of carbon is used in finance to measure a firm's specific cost of carbon. It expresses how much an organization is expending to either reduce carbon emissions internally (abatement) or offsetting externally (carbon offset). As such, the weighted average cost of carbon is the cost a company incurs to balance its carbon liability (carbon footprint).\n\nIt is a term with growing importance as legislation globally moves to internalize the impact of emission through cost mechanisms.\n\nC = ((V × E) + (V × E)) / L\n\nCorporations have multiple ways to balance their carbon liability. They can reduce their carbon emissions (their \"carbon footprint\") through capital investment, projects and demand reduction. They can purchase emission permits, be allocated quotas (such as European Union Allowances (EUA)) or buy carbon credits. The latter are largely produced by CDM projects (Clean Development Mechanism) and Joint Initiatives. These credits are largely traded in form of Certified Emission Reduction (CER), or Emission Reduction Unit (ERU). Voluntary Emissions Reduction (VER) have a similar function but have not registered / cannot be registered under the rules of the Kyoto Protocol.\n\nIn a carbon constrained economy, the efficiency of corporations to respond to the cost factor carbon is an important indicator of competitiveness. Financial analysts are beginning to compare companies within industries based on their ability to either reduce their carbon footprint internally or offset carbon liabilities externally through comparatively low cost channels.\n\nA good article that discusses the weighted average cost of carbon has been published by KyotoPlanet, view the eBook at http://kyotoplanet.newspaperdirect.com/epaper/viewer.aspx pages 98 –100\n\nWeighted average cost of capital\n"}
{"id": "21481546", "url": "https://en.wikipedia.org/wiki?curid=21481546", "title": "Wheelchair", "text": "Wheelchair\n\nA wheelchair is a chair with wheels, used when walking is difficult or impossible due to illness, injury, or disability. Wheelchairs come in a wide variety of formats to meet the specific needs of their users. They may include specialized seating adaptions, individualized controls, and may be specific to particular activities, as seen with sports wheelchairs and beach wheelchairs. The most widely recognised distinction is between powered wheelchairs (\"powerchairs\"), where propulsion is provided by batteries and electric motors, and manually propelled wheelchairs, where the propulsive force is provided either by the wheelchair user/occupant pushing the wheelchair by hand (\"self-propelled\"), or by an attendant pushing from the rear (\"attendant propelled\").\nThe earliest records of wheeled furniture are an inscription found on a stone slate in China and a child’s bed depicted in a frieze on a Greek vase, both dating between the 6th and 5th century BCE. The first records of wheeled seats being used for transporting disabled people date to three centuries later in China; the Chinese used early wheelbarrows to move people as well as heavy objects. A distinction between the two functions was not made for another several hundred years, until around 525 CE, when images of wheeled chairs made specifically to carry people begin to occur in Chinese art.\n\nAlthough Europeans eventually developed a similar design, this method of transportation did not exist until 1595 when an unknown inventor from Spain built one for King Phillip II. Although it was an elaborate chair having both armrests and leg rests, the design still had shortcomings since it did not feature an efficient propulsion mechanism and thus, requires assistance to propel it. This makes the design more of a modern-day highchair or portable throne for the wealthy rather than a modern-day wheelchair for the disabled.\n\nIn 1655, Stephan Farffler, a 22 year old paraplegic watchmaker, built the world's first self-propelling chair on a three-wheel chassis using a system of cranks and cogwheels. However, the device had an appearance of a hand bike more than a wheelchair since the design included hand cranks mounted at the front wheel.\n\nThe invalid carriage or Bath chair brought the technology into more common use from around 1760.\n\nIn 1887, wheelchairs (\"rolling chairs\") were introduced to Atlantic City so invalid tourists could rent them to enjoy the Boardwalk. Soon, many healthy tourists also rented the decorated \"rolling chairs\" and servants to push them as a show of decadence and treatment they could never experience at home.\n\nIn 1933 Harry C. Jennings, Sr. and his disabled friend Herbert Everest, both mechanical engineers, invented the first lightweight, steel, folding, portable wheelchair. Everest had previously broken his back in a mining accident. Everest and Jennings saw the business potential of the invention and went on to become the first mass-market manufacturers of wheelchairs. Their \"X-brace\" design is still in common use, albeit with updated materials and other improvements. The X-brace idea came to Harry from the men’s folding “camp chairs / stools”, rotated 90 degrees, that Harry and Herbert used in the outdoors and at the mines.\n\nThere are a wide variety of types of wheelchair, differing by propulsion method, mechanisms of control, and technology used. Some wheelchairs are designed for general everyday use, others for single activities, or to address specific access needs. Innovation within the wheelchair industry is relatively common, but many innovations ultimately fall by the wayside, either from over-specialization, or from failing to come to market at an accessible price-point. The iBot is perhaps the best known example of this in recent years.\n\nA self-propelled manual wheelchair incorporates a frame, seat, one or two footplates (footrests) and four wheels: usually two caster wheels at the front and two large wheels at the back. There will generally also be a separate seat cushion. The larger rear wheels usually have push-rims of slightly smaller diameter projecting just beyond the tyre; these allow the user to manoeuvre the chair by pushing on them without requiring them to grasp the tyres. Manual wheelchairs generally have brakes that bear on the tyres of the rear wheels, however these are solely a parking brake and in-motion braking is provided by the user's palms bearing directly on the push-rims. As this causes friction and heat build-up, particularly on long downslopes, many wheelchair users will choose to wear padded wheelchair gloves. Manual wheelchairs often have two push handles at the upper rear of the frame to allow for manual propulsion by a second person, however many active wheelchair users will remove these to prevent unwanted pushing from people who believe they are being helpful.\n\nEveryday manual wheelchairs come in two major varieties, folding or rigid. Folding chairs are generally low-end designs, whose predominant advantage is being able to fold, generally by bringing the two sides together. However this is largely an advantage for part-time users who may need to store the wheelchair more often than use it. Rigid wheelchairs, which are increasingly preferred by full-time and active users, have permanently welded joints and many fewer moving parts. This reduces the energy required to push the chair by eliminating many points where the chair would flex and absorb energy as it moves. Welded rather than folding joints also reduce the overall weight of the chair. Rigid chairs typically feature instant-release rear wheels and backrests that fold down flat, allowing the user to dismantle the chair quickly for storage in a car. A few wheelchairs attempt to combine the features of both designs by providing a fold-to-rigid mechanism in which the joints are mechanically locked when the wheelchair is in use.\n\nMany rigid models are now made with ultralight materials such as aircraft-grade aluminium and titanium, and wheelchairs of composite materials such as carbon-fibre have started to appear. Ultra lightweight rigid wheelchairs are commonly known as 'active user chairs' as they are ideally suited to independent use. Another innovation in rigid chair design is the installation of shock absorbers, such as Frog Legs, which cushion the bumps over which the chair rolls. These shock absorbers may be added to the front wheels, to the rear wheels, or both. Rigid chairs also have the option for their rear wheels to have a camber, or tilt, which angles the tops of the wheels in toward the chair. This allows for more mechanically efficient propulsion by the user and also makes it easier to hold a straight line while moving across a slope. Sport wheelchairs often have large camber angles to improve stability.\n\nRigid-framed chairs are generally made to measure, to suit both the specific size of the user and their needs and preferences around areas such as the \"tippyness\" of the chair - its stability around the rear axle. Experienced users with sufficient upper-body strength can generally balance the chair on its rear wheels, a \"wheelie\", and the \"tippyness\" of the chair controls the ease with which this can be initiated. The wheelie allows an independent wheelchair user to climb and descend curbs and move more easily over small obstacles and irregular ground such as cobbles.\n\nThe rear wheels of self-propelled wheelchairs typically range from 20–24 inches (51–61 cm)in diameter, and commonly resemble bicycle wheels. Wheels are rubber-tired and may be solid, pneumatic or gel-filled. The wheels of folding chairs may be permanently attached, but those for rigid chairs are commonly fitted with quick-release axles activated by depressing a button at the centre of the wheel.\n\nAll major varieties of wheelchair can be highly customized for the user's needs. Such customization may encompass the seat dimensions, height, seat angle, footplates, leg rests, front caster outriggers, adjustable backrests and controls. Various optional accessories are available, such as anti-tip bars or wheels, safety belts, adjustable backrests, tilt and/or recline features, extra support for limbs or head and neck, holders for crutches, walkers or oxygen tanks, drink holders, and mud and wheel-guards as clothing protectors.\n\nLight weight and high cost are related in the manual wheelchair market. At the low-cost end, heavy, folding steel chairs with sling seats and little adaptability dominate. Users may be temporarily disabled, or using such a chair as a loaner, or simply unable to afford better. These chairs are common as \"loaners\" at large facilities such as airports, amusement parks and shopping centers. A slightly higher price band sees the same folding design produced in aluminium. Price typically then jumps from low to mid hundreds of pounds/dollars/euros to a four figure price range, with individually custom manufactured lightweight chairs with more options. The high end of the market contains ultra-light models, extensive seating options and accessories, all-terrain features, and so forth. The most expensive manual chairs may rival the cost of a small car.\n\nAn attendant-propelled wheelchair is generally similar to a self-propelled manual wheelchair, but with small diameter wheels at both front and rear. The chair is maneuvered and controlled by a person standing at the rear and pushing on handles incorporated into the frame. Braking is supplied directly by the attendant who will usually also be provided with a foot- or hand-operated parking brake.\n\nThese chairs are common in institutional settings and as loaner-chairs in large public venues. They are usually constructed from steel as light weight is less of a concern when the user is not required to self-propel.\n\nSpecially designed transfer chairs are now required features at airports in much of the developed world in order to allow access down narrow airliner aisles and facilitate the transfer of wheelchair-using passengers to and from their seats on the aircraft.\n\nAn electric-powered wheelchair, commonly called a \"powerchair\" is a wheelchair which additionally incorporates batteries and electric motors into the frame and that is controlled by either the user or an attendant, most commonly via a small joystick mounted on the armrest, or on the upper rear of the frame. For users who cannot manage a manual joystick, headswitches, chin-operated joysticks, sip-and-puff controllers or other specialist controls may allow independent operation of the wheelchair. Ranges of over 10 miles/15 km are commonly available from standard batteries.\n\nPowerchairs are commonly divided by their access capabilities. An indoor-chair may only reliably be able to cross completely flat surfaces, limiting them to household use. An indoor-outdoor chair is less limited, but may have restricted range or ability to deal with slopes or uneven surfaces. An outdoor chair is more capable, but will still have a very restricted ability to deal with rough terrain. A very few specialist designs offer a true cross-country capability.\n\nPowerchairs have access to the full range of wheelchair options, including ones which are difficult to provide in an unpowered manual chair, but have the disadvantage of significant extra weight. Where an ultra-lightweight manual chair may weigh under 10 kg, the largest outdoor power-chairs may weigh 200 kg or more.\n\nSmaller power chairs often have four wheels, with front or rear wheel drive, but large outdoor designs commonly have six wheels, with small wheels at front and rear and somewhat larger powered wheels in the centre.\n\nA power-assisted wheelchair is a recent development that uses the frame & seating of a typical rigid manual chair while replacing the standard rear wheels with wheels of similar size which incorporate batteries and battery-powered motors in the hubs. A floating rim design senses the pressure applied by the users push & activates the motors proportionately to provide a power assist. This results in the convenience, and small size of a manual chair while providing motorised assistance for rough/uneven terrain & steep slopes that would otherwise be difficult or impossible to navigate, especially by those with limited upper-body function. As the wheels necessarily come at a weight penalty it is often possible to exchange them with standard wheels to match the capabilities of the wheelchair to the current activity.\n\nMobility scooters share some features with powerchairs, but primarily address a different market segment, people with a limited ability to walk, but who might not otherwise consider themselves disabled. Smaller mobility scooters are typically three wheeled, with a base on which is mounted a basic seat at the rear, with a control tiller at the front. Larger scooters are frequently four-wheeled, with a much more substantial seat.\n\nOpinions are often polarized as to whether mobility scooters should be considered wheelchairs or not, and negative stereotyping of scooter users is worse than for manual or powerchair users. Some commercial organisations draw a distinction between powerchairs and scooters when making access provisions due to a lack of clarity in the law as to whether scooters fall under the same equality legislation as wheelchairs.\n\nOne-arm or single arm drive enables a user to self-propel a manual wheelchair using only a single arm. The large wheel on the same side as the arm to be used is fitted with two concentric handrims, one of smaller diameter than the other. On most models the outer, smaller rim, is connected to the wheel on the opposite side by an inner concentric axle. When both handrims are grasped together, the chair may be propelled forward or backward in a straight line. When either handrim is moved independently, only a single wheel is used and the chair will turn left or right in response to the handrim used. Some wheelchairs, designed for use by hemiplegics, provide a similar function by linking both wheels rigidly together and using one of the footplates to control steering via a linkage to the front caster.\n\nReclining or tilt-in-space wheelchairs have seating surfaces which can be tilted to various angles. The original concept was developed by an orthotist, Hugh Barclay, who worked with disabled children and observed that postural deformities such as scoliosis could be supported or partially corrected by allowing the wheelchair user to relax in a tilted position. The feature is also of value to users who are unable to sit upright for extended periods for pain or other reasons.\n\nIn the case of reclining wheelchairs, the seat-back tilts back, and the leg rests can be raised, while the seat base remains in the same position, somewhat similar to a common recliner chair. Some reclining wheelchairs lean back far enough that the user can lie down completely flat. Reclining wheelchairs are preferred in some cases for some medical purposes, such as reducing the risk of pressure sores, providing passive movement of hip and knee joints, and making it easier to perform some nursing procedures, such as intermittent catheterization to empty the bladder and transfers to beds, and also for personal reasons, such as people who like using an attached tray. The use of reclining wheelchairs is particularly common among people with spinal cord injuries such as quadriplegia.\n\nIn the case of tilting wheelchairs, the seat-back, seat base, and leg rests tilt back as one unit, somewhat similar to the way a person might tip a four-legged chair backwards to balance it on the back legs. While fully reclining spreads the person's weight over the entire back side of the body, tilting wheelchairs transfer it from only the buttocks and thighs (in the seated position) to partially on the back and head (in the tilted position). Tilting wheelchairs are preferred for people who use molded or contoured seats, who need to maintain a particular posture, who adversely affected by sheer forces (reclining causes the body to slide slightly every time), or who need to keep a communication device, powered wheelchair controls, or other attached device in the same relative position throughout the day. Tilting wheelchairs are commonly used by people with cerebral palsy, people with some muscle diseases, and people with limited range of motion in the hip or knee joints. Tilting options are more common than reclining options in wheelchairs designed for use by children.\n\nA standing wheelchair is one that supports the user in a nearly standing position. They can be used as both a wheelchair and a standing frame, allowing the user to sit or stand in the wheelchair as they wish. Some versions are entirely manual, others have powered stand on an otherwise manual chair, while others have full power, tilt, recline and variations of powered stand functions available. The benefits of such a device include, but are not limited to: aiding independence and productivity, raising self-esteem and psychological well-being, heightening social status, extending access, relief of pressure, reduction of pressure sores, improved functional reach, improved respiration, reduced occurrence of UTI, improved flexibility, help in maintaining bone mineral density, improved passive range motion, reduction in abnormal muscle tone and spasticity, and skeletal deformities. Other wheelchairs provide some of the same benefits by raising the entire seat to lift the user to standing height.\n\nA range of disabled sports have been developed for disabled athletes, including basketball, rugby, tennis, racing and dancing. The wheelchairs used for each sport have evolved to suit the specific needs of that sport and often no longer resemble their everyday cousins. They are usually non-folding (in order to increase rigidity), with a pronounced negative camber for the wheels (which provides stability and is helpful for making sharp turns), and often are made of composite, lightweight materials. Even seating position may be radically different, with racing wheelchairs generally used in a kneeling position. Sport wheelchairs are rarely suited for everyday use, and are often a 'second' chair specifically for sport use, although some users prefer the sport options for everyday use. Some disabled people, specifically lower-limb amputees, may use a wheelchair for sports, but not for everyday activities.\n\nWhile most wheelchair sports use manual chairs, some power chair sports, such as powerchair football, exist.\n\nE-hockey is hockey played from electrical wheelchairs.\n\nWheelchair stretchers are a variant of wheeled stretchers/gurneys that can accommodate a sitting patient, or be adjusted to lie flat to help in the lateral (or supine) transfer of a patient from a bed to the chair or back. Once transferred, the stretcher can be adjusted to allow the patient to assume a sitting position.\n\nAll-terrain wheelchairs can allow users to access terrain otherwise completely inaccessible to a wheelchair user. Two different formats have been developed. One hybridises wheelchair and mountain bike technology, generally taking the form of a frame within which the user sits and with four mountain bike wheels at the corners. In general there are no push-rims and propulsion/braking is by pushing directly on the tyres.\nA more common variant is the beach wheelchair (Beach-Going Wheelchair) which can allow better mobility on beach sand, including in the water, on uneven terrain, and even on snow. The common adaptation among the different designs is that they have extra-wide balloon wheels or tires, to increase stability and decrease ground pressure on uneven or unsteady terrain. Different models are available, both manual and battery-driven. In some countries in Europe, where accessible tourism is well established, many beaches have wheelchairs of this type available for loan/hire.\n\nA smart wheelchair is any powerchair using a control system to augment or replace user control. Its purpose is to reduce or eliminate the user's task of driving a powerchair. Usually, a smart wheelchair is controlled via a computer, has a suite of sensors and applies techniques in mobile robotics, but this is not necessary. The interface may consist of a conventional wheelchair joystick, a \"sip-and-puff\" device or a touch-sensitive display. This differs from a conventional powerchair, in which the user exerts manual control over speed and direction without intervention by the wheelchair's control system.\n\nSmart wheelchairs are designed for a variety of user types. Some are designed for users with cognitive impairments, such as dementia, these typically apply collision-avoidance techniques to ensure that users do not accidentally select a drive command that results in a collision. Othersfocus on users living with severe motor disabilities, such as cerebral palsy, or with quadriplegia, and the role of the smart wheelchair is to interpret small muscular activations as high-level commands and execute them. Such wheelchairs typically employ techniques from artificial intelligence, such as path-planning.\n\nRecent technological advances are slowly improving wheelchair and powerchair technology.\n\nA variation on the manually-propelled wheelchair is the Leveraged Freedom Chair (LFC), designed by the MIT Mobility Lab. This wheelchair is designed to be low-cost, constructed with local materials, for users in developing countries. Engineering modifications have added hand-controlled levers to the LFC, to enable users to move the chair over uneven ground and minor obstacles, such as bumpy dirt roads, that are common in developing countries. It is under development, and has been tested in Kenya and India so far.\n\nThe addition of geared, all-mechanical wheels for manual wheelchairs is a new development incorporating a hypocycloidal reduction gear into the wheel design. The 2-gear wheels can be added to a manual wheelchair. The geared wheels provide a user with additional assistance by providing leverage through gearing (like a bicycle, not a motor). The two-gear wheels offer two speed ratios- 1:1 (no help, no extra torque) and 2:1, providing 100% more hill climbing force. The low gear incorporates an automatic \"hill hold\" function which holds the wheelchair in place on a hill between pushes, but will allow the user to override the hill hold to roll the wheels backwards if needed. The low gear also provides downhill control when descending.\n\nA recent development related to wheelchairs is the handcycle. They come in a variety of forms, from road and track racing models to off-road types modelled after mountain bikes. While dedicated handcycle designs are manufactured, clip-on versions are available than can convert a manual wheelchair to a handcycle in seconds. The general concept is a clip-on front-fork with hand-pedals, usually attaching to a mounting on the footplate. A somewhat related concept is the Freewheel, a large dolley wheel attaching to the front of a manual wheelchair, again generally to the footplate mounting, which improves wheelchair performance over rough terrain. Unlike a handcycle, a wheelchair with Freewheel continues to be propelled via the rear wheels.\n\nThere have been significant efforts over the past 20 years to develop stationary wheelchair trainer platforms that could enable wheelchair users to exercise as one would on a treadmill or bicycle trainer.\nSome devices have been created that could be used in conjunction with virtual travel and interactive gaming similar to an omnidirectional treadmill.\n\nIn 2011, British inventor Andrew Slorance developed \"Carbon Black\" the first wheelchair to be made almost entirely out of carbon fibre\n\nRecently, EPFL's CNBI project has succeeded in making wheelchairs that can be controlled by brain impulses.\n\nExperiments have also been made with unusual variant wheels, like the omniwheel or the mecanum wheel. These allow for a broader spectrum of movement, but have made no mass-market penetration.\nThe electric wheelchair shown on the right is fitted with Mecanum wheels (sometimes known as Ilon wheels) which give it complete freedom of movement. It can be driven forwards, backwards, sideways, and diagonally, and also turned round on the spot or turned around while moving, all operated from a simple joystick.\n\nFoot propulsion of a manual wheelchair by the occupant is possible for users who have limited hand movement capabilities or simply do not wish to use their hands for propulsion. Foot propulsion also allows patients to exercise their legs to increase blood flow and limit further disability. Users who do this commonly may elect to have a lower seat height and no footplate to better suit the wheelchair to their needs.\n\nWheelbase chairs are powered or manual wheelchairs with specially molded seating systems interfaced with them for users with a more complicated posture. A molded seating system involves taking a cast of a person's best achievable seated position and then either carving the shape from memory foam or forming a plastic mesh around it. This seat is then covered, framed, and attached to a wheelbase.\n\nA bariatric wheelchair is one designed to support larger weights; most standard chairs are designed to support no more than 250 lb (113 kg) on average.\n\nPediatric wheelchairs are another available subset of wheelchairs. These can address needs such as being able to play on the floor with other children, or cater for children in large hip-spica casts due to problems such as hip dysplasia.\n\n\"Hemi wheelchairs\" have lower seats which are designed for easy foot propulsion. The decreased seat height also allows them to be used by children and shorter individuals.\n\nA knee scooter is a related device with some features of a wheelchair and some of walking aids. Unlike wheelchairs they are only suitable for below knee injuries to a single leg. The user rests the injured leg on the scooter, grasps the handlebars, and pushes with the uninjured leg.\n\nAdapting the built environment to make it more accessible to wheelchair users is one of the key campaigns of disability rights movements and local equality legislation such the Americans with Disabilities Act of 1990 (ADA). The Social Model of Disability defines 'disability' as the discrimination experienced by people with impairments as a result of the failure of society to provide the adaptions needed for them to participate in society as equals. This includes both physical adaption of the built environment and adaption of organizational and social structures and attitudes. A core principle of access is universal design - that all people regardless of disability are entitled to equal access to all parts of society like public transportation and buildings. A wheelchair user is less disabled in an environment without stairs.\n\nAccess starts outside of the building, with the provision of reduced height kerb-cuts where wheelchair users may need to cross roads, and the provision of adequate wheelchair parking, which must provide extra space in order to allow wheelchair users to transfer directly from seat to chair. Some tension exists between access provisions for visually impaired pedestrians and wheelchair users and other mobility impaired pedestrians as textured paving, vital for visually impaired people to recognise the edge of features such as light-controlled crossings, is uncomfortable at best, and dangerous at worst, to those with mobility impairments.\n\nFor access to public buildings, it is frequently necessary to adapt older buildings with features such as ramps or elevators in order to allow access by wheelchair users and other people with mobility impairments. Other important adaptations can include powered doors, lowered fixtures such as sinks and water fountains, and accessible toilets with adequate space and grab bars to allow the disabled person to transfer out of their wheelchair onto the fixture. Access needs for people with other disabilities, for instance visual impairments, may also be required, such as by provision of high visibility markings on the edges of steps and braille labelling. Increasingly new construction for public use is required by local equality laws to have these features incorporated at the design stage.\n\nThe same principles of access that apply to public buildings also apply to private homes and may be required as part of local building regulations. Important adaptations include external access, providing sufficient space for a wheelchair user to move around the home, doorways that are wide enough for convenient use, access to upper floors, where they exist, which can be provided either by dedicated wheelchair lifts, or in some cases by using a stairlift to transfer between wheelchairs on different floors, and by providing accessible bathrooms with showers and/or bathtubs that are designed for accessibility. Accessible bathrooms can permit the use of mobile shower chairs or transfer benches to facilitate bathing for people with disabilities. Wet rooms are bathrooms where the shower floor and bathroom floor are one continuous waterproof surface. Such floor designs allow a wheelchair user using a dedicated shower chair, or transferring onto a shower seat, to enter the shower without needing to overcome a barrier or lip.\n\nThe construction of low floor trams and buses is increasingly required by law, whereas the use of inaccessible features such as paternosters in public buildings without any alternative methods of wheelchair access is increasingly deprecated. Modern architecture is increasingly required by law and recognised good practise to incorporate better accessibility at the design stage.\n\nIn many countries, such as the UK, the owners of inaccessible buildings who have not provided permanent access measures are still required by local equality legislation to provide 'reasonable adjustments' to ensure that disabled people are able to access their services and are not excluded. These may range from keeping a portable ramp on hand to allow a wheelchair user to cross an inaccessible threshold, to providing personal service to access goods they are not otherwise able to reach.\n\nPublic transit vehicles are increasingly required to be accessible to people who use wheelchairs.\n\nIn the UK, all single deck buses are required to be accessible to wheelchair users by 2017, all double-deck coaches by 2020. Similar requirements exist for trains, with most trains already incorporating a number of wheelchair-spaces.\n\nThe EU has required airline and airport operators to support the use of airports and airliners by wheelchair users and other 'Persons with Reduced Mobility' since the introduction of EU Directive EC1107/2006.\n\nIn Los Angeles there is a program to remove a small amount of seating on some trains to make more room for bicycles and wheelchairs.\n\nNew York City's entire bus system is wheelchair-accessible, and a multimillion-dollar renovation program is underway to provide elevator access to many of the city's 485 subway stations.\n\nIn Adelaide, Australia, all public transport has provision for at least two wheelchairs per bus, tram or train. In addition all trains have space available for bicycles.\n\nThe Washington, D.C. Metro system features complete accessibility on all its subways and buses.\n\nIn Paris, France, the entire bus network, i.e. 60 lines, has been accessible to wheelchair users since 2010.\n\nIn the United States a wheelchair that has been designed and tested for use as a seat in motor vehicles is often referred to as a \"WC19 Wheelchair\" or a \"transit wheelchair\". ANSI-RESNA WC19 (officially, SECTION 19 ANSI/RESNA WC/VOL. 1 Wheelchairs for use in Motor Vehicles) is a voluntary standard for wheelchairs designed for use when traveling facing forward in a motor vehicle. ISO 7176/19 is an international transit wheelchair standard that specifies similar design and performance requirements as ANSI/RESNA WC19.\n\nSeveral organizations exist that help to give and receive wheelchair equipment. Organizations that accept wheelchair equipment donations typically attempt to identify recipients and match them with the donated equipment they have received. Organizations that accept donations in the form of money for wheelchairs typically have the wheelchairs manufactured and distributed in large numbers, often in developing countries. Organizations focusing on wheelchairs include Direct Relief, the Free Wheelchair Mission, Hope Haven, Personal Energy Transportation, the Wheelchair Foundation and WheelPower.\n\nIn the United Kingdom wheelchairs are supplied and maintained free of charge for disabled people whose need for such a chair is permanent. See Disability in the United Kingdom.\n\nWheelchair seating systems are designed both to support the user in the sitting position and to redistribute pressure from areas of the body that are at risk of pressure ulcers. For someone in the sitting position, the parts of the body that are the most at risk for tissue breakdown include the ischial tuberosities, coccyx, sacrum and greater trochanters. Wheelchair cushions are the prime method of delivering this protection and are nearly universally used. Wheelchair cushions are also used to provide stability, comfort, aid posture and absorb shock. Wheelchair cushions range from simple blocks of foam costing a few pounds or dollars, to specifically engineered multilayer designs with costs running into the hundreds of pounds/dollars/euros.\n\nPrior to 1970, little was known about the effectiveness of wheelchair cushions and there was not a clinical method of evaluating wheelchair seat cushions. Most recently, pressure imaging (or pressure mapping) is used to help determine each individual’s pressure distribution to properly determine and fit a seating system.\n\nWhile almost all wheelchair users will use a wheelchair cushion, some users need more extensive postural support. This can be provided by adaptions to the back of the wheelchair, which can provide increased rigidity, head/neck rests and lateral support and in some cases by adaptions to the seat such as pommels and knee-blocks. Harnesses may also be required.\n\n\n\n"}
{"id": "2616214", "url": "https://en.wikipedia.org/wiki?curid=2616214", "title": "Windsor chair", "text": "Windsor chair\n\nA Windsor chair is a chair built with a solid wooden seat into which the chair-back and legs are round-tenoned, or pushed into drilled holes, in contrast to standard chairs, where the back legs and the uprights of the back are continuous. The seats of Windsor chairs were often carved into a shallow dish or saddle shape for comfort. Traditionally, the legs and uprights were usually turned on a pole lathe. The back and sometimes the arm pieces (if arms are present) are formed from steam bent pieces of wood.\n\nIt is not clear when the first Windsor Chairs were made. It is known that, as early as the 16th century, wheelwrights started coping out chair spindles in the same way they made wheel spokes. The design was probably a development of West Country, Welsh and Irish 'stick-back' chairs, but the evidence on origin is not certain. It is thought that the first Windsor chair made its appearance in the county of Buckinghamshire, where the main centre of production eventually moved to High Wycombe. The first Windsors were of the comb-back variety. By the 18th century steam-bending was being used to produce the characteristic \"bow\" of the Windsor chair. The first chairs made this way were shipped to London from the market town of Windsor, Berkshire in 1724. There is speculation that the chair derives its name from the town of Windsor, which became the centre for the trade between the producers and the London dealers. Thus the name \"Windsor Chair\" is more about the style of chair than where it was made, with many diverse forms of Windsor chair being made worldwide.\n\nTraditionally there were three types of craftsmen involved in the construction of a Windsor chair, there was the chair bodger, an itinerant craftsman who worked in the woods and made just the legs and stretchers, on a pole lathe. Then there was the benchman who worked in a small town or village workshop and would produce the seats, backsplats and other sawn parts.\nThe final craftsman involved was the framer. The framer would take the components produced by the bodger and the benchman and would assemble and finish the chair.\n\nEnglish settlers introduced the Windsor chair to North America, with the earliest known chairs being imported by Patrick Gordon who became lieutenant governor of Pennsylvania in 1726. There is speculation that the first American Windsor chair, based on the traditional British design, was made in Philadelphia in 1730.\n\nThere are about seven distinctive forms. These include:\n\n\n\nIt is common to find American Windsors made in the 18th century that contain three different species of wood. Pine, bass or tulip poplar are common for the seat. Non ring porous hardwoods such as Maple are stiff and make crisp turnings, and were used for the undercarriage. Ring porous species such as Oak, ash, and hickory all rive (split) and steam bend nicely. These woods are also straight grained and flexible and thus work well for slender parts such as the spindles.\n\nThe seat of a Windsor chair is an essential part since it provides the stability to both the upper and lower portions. The thickness of the seat allows the legs to be anchored securely into their respective tapered sockets, providing the undercarriage with strength and stability. A timber that will provide the strength and stability whilst also allowing it to be shaped, in order to achieve the desired look and feel, requires a strong durable timber, with interlocking grain, to provide the right characteristics. English Windsors typically have elm seats because its interlocking grain gives good cross-grain strength that resists splitting where holes are placed close to the edge of a seat. There are no real satisfactory alternatives to elm although other woods have been tried, for example, oak and ash in Britain and various types of pine in the USA. Because of elm's strength compared to pine, tulip poplar or bass, English Windsor chair seats are usually not as thick as American Windsors. The English Windsor chair seats are not saddled (or dished) as deeply as their American counterparts- partly because of elm's relative strength, and partly because elm is comparatively more difficult to sculpt than the softer woods chosen by American chair makers. Woodwrights use tools such as the adze, scorp or inshave to form the hollowed out, form fitting, ergonomic top of the seat. \n\nThe legs are splayed at angles fore-and-aft (rake) as well as side-to-side (splay) to provide actual and visual support of the person sitting. Early chairs made in America usually have stretchers connecting the front and back legs and a cross stretcher connecting the two side stretchers, creating what is known as an \"H\" stretcher assembly. A common misconception about this assembly is that the stretchers hold the legs together in order to keep them from pulling apart. In the traditional Windsor design, the wedged tenon joint which joins each leg to the seat is strong enough in itself to prevent the legs from creeping outward. The stretcher system actually pushes the legs apart to retain the necessary tension which reduces slack.\n\n\"Through-holed and wedged\" is one of the primary means of joining Windsor chair parts. A cylindrical or slightly tapered hole is bored in the first piece, the matching cylindrical or tapered end of the second piece is inserted in the hole as a round tenon, and a wedge is driven into the end of this tenon, flaring it tight in the hole. The excess portion of the wedge is then cut flush with the surface. This supplies a mechanical hold that will prevail when the glue fails. In general, early Windsor chair joints are held together mechanically, making glue a redundant detail in their assembly.\nEarly British Windsors were painted, later versions were stained and polished. American Windsors were usually painted, in the 18th century they were grain painted with a light color, then overpainted with a dark color before being coated with linseed oil for protection of the fragile paint. In the 19th century settlers from the mid-west of America to Ontario, Canada would coat their chairs with the solid primary colours of milk paint, a mix of buttermilk, turpentine and cow's blood.\n\nDuring the early 19th century the United States produced vast numbers of chairs, in factories, and an experienced factory painter could paint a chair in less than five minutes. By mid-century, to save production costs, the chair was painted in solid colours with some simple stencilling being the only design.\n\nWith wear in use, the paint wears off around the edges and displays a characteristic wear pattern that reveals the paint colors underneath. As for any antique, this original finish often survives best in unworn areas such as the bottom of the seat or around turnings. Later repainting, even well-intentioned restoration, will diminish the value of an original finish.\n\n\n\n"}
