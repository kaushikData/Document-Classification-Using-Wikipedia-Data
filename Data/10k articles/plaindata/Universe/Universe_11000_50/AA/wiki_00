{"id": "13916632", "url": "https://en.wikipedia.org/wiki?curid=13916632", "title": "Anne Kajir", "text": "Anne Kajir\n\nAnne Kajir is an attorney from Papua New Guinea. She has uncovered evidence of widespread corruption in the Papua New Guinea government, that allowed illegal logging in tropical forests. Kajir was awarded the Goldman Environmental Prize in 2006.\n"}
{"id": "42590737", "url": "https://en.wikipedia.org/wiki?curid=42590737", "title": "Bangladesh Forest Research Institute", "text": "Bangladesh Forest Research Institute\n\nBangladesh Forest Research Institute (BFRI) is an institute for forestry research in Bangladesh, located at Sholashahar, Chattogram.\n\nBangladesh Forest Research Institute was established in 1955 as East Pakistan Forest Research Laboratory, BFRI works under the auspices of the Ministry of Environment and Forest. Aside from its headquarter in Chattogram, BFRI has 21 Research Stations and Sub-Stations under five Field Divisions covering different forest types spread over eight dendrological regions of the country.\n\n\n"}
{"id": "700190", "url": "https://en.wikipedia.org/wiki?curid=700190", "title": "Briquette", "text": "Briquette\n\nA briquette (or briquet) is a compressed block of coal dust or other combustible biomass material such as charcoal, sawdust, wood chips, peat, or paper used for fuel and kindling to start a fire. The term comes from the French language and is related to \"brick\".\n\nCoal briquettes have long been produced as a means of using up 'small coal', the finely broken coal inevitably produced during the mining process. Otherwise this is difficult to burn as it is both hard to arrange adequate airflow through a fire of these small pieces, also it tended to be drawn up and out of the chimney by the draught, giving visible black smoke.\n\nThe first briquettes were known as culm and were hand-moulded with a little wet clay as a binder. These could be difficult to burn efficiently, as the unburned clay produced a large ash content, blocking airflow through a grate.\n\nWith Victorian developments in engineering, particularly the hydraulic press, it became possible to produce machine-made briquettes with minimal binder content. A tar or pitch binder was used, obtained first from gas making and later from petrochemical sources. These binders burned away completely, making it a low-ash fuel. A proprietary brand of briquettes from the South Wales coalfield was Phurnacite, developed by Powell Duffryn. These were intended to emulate a high-quality anthracite coal, such as that from the Cynheidre measures. This involved blending a mixture of coals from different grades and colliery sources. Phurnacite used the following mix:\n\nEarly briquettes were large and brick-shaped. They could be stacked, or even built into walls. The Antarctic expeditions of both Shackleton and Scott took large quantities of these briquettes with them and used them to build pony stables. As the ponies were eaten, as planned, for food, the stables could be dismantled and used for fuel. Phurnacite briquettes later adopted a squared oval shape. This regular shape packed well as a good firebed, with plentiful airflow. They are also easy to mechanically feed, allowing the development of automatically controlled heating boilers that could run for days without human intervention.\n\nCharcoal briquettes sold for cooking food can include: \n\nSome briquettes are compressed and dried brown coal extruded into hard blocks. This is a common technique for low rank coals. They are typically dried to 12-18% moisture, and are primarily used in household and industry.\n\nHome made charcoal briquettes (called \"\") were found after charcoal production in Japanese history. In the Edo period, polysaccharide extracted from red algae was widely used as a binder. After the imports of steam engines in the Meiji period, coal and clay became major ingredients of Japanese briquettes. These briquettes,\"\" and \"\" , were exported to China and Korea. Today, coal briquettes are avoided for their sulfur oxide emission. Charcoal briquettes are still used for traditional or outdoor cooking. Woody flakes such as sawdust or coffee dust are major ingredients of modern mass-consumed briquettes (e.g. \"\").\n\nThroughout China, cylindrical briquettes, called \"fēng wō méi\" (beehive coal 蜂窩煤 / 蜂窝煤) or \"Mei\" (coal 煤) or \"liàn tàn\" (kneaded coal 練炭 / 练炭), are used in purpose-built cookers.\n\nThe origin of \"Mei\" is \"\" (kneaded coal 練炭) of Japan. \"Rentan\" was invented in Japan in the 19th century, and spread to Manchukuo, Korea and China in the first half of the 20th century. There were many \"Rentan\" factories in Manchukuo and Pyongyang. Although \"Rentan\" went out of use in Japan after the 1970s, it is still popular in China and Vietnam (\"than\" coal).\n\nThe cookers are simple, ceramic vessels with metal exteriors. Two types are made: the single, or triple briquette type, the latter holding the briquettes together side by side. These cookers can accommodate a double stack of cylinders. A small fire of tinder is started, upon which the cylinder(s) is placed. When a cylinder is spent, another cylinder is placed on top using special tongs, with the one below igniting it. The fire can be maintained by swapping spent cylinders for fresh ones, and retaining a still-glowing spent cylinder.\n\nEach cylinder lasts for over an hour. These cookers are used to cook, or simmer, pots of tea, eggs, soups, stews, etc.\nThe cylinders are delivered, usually by cart, to businesses, and are very inexpensive.\n\nIn Ireland, peat briquettes are a common type of solid fuel, largely replacing sods of raw peat as a domestic fuel. These briquettes consist of shredded peat, compressed to form a virtually smokeless, slow-burning, easily stored and transported fuel. Although often used as the sole fuel for a fire, they are also used to quickly and easily light a coal fire.\n\nBiomass briquettes are made from agricultural waste and are a replacement for fossil fuels such as oil or coal, and can be used to heat boilers in manufacturing plants, and also have applications in developing countries. Biomass briquettes are a renewable source of energy and avoid adding fossil carbon to the atmosphere.\n\nA number of companies in India have switched from furnace oil to biomass briquettes to save costs on boiler fuels. The use of biomass briquettes is predominant in the southern parts of India, where coal and furnace oil are being replaced by biomass briquettes. A number of units in Maharashtra (India) are also using biomass briquettes as boiler fuel. Use of biomass briquettes can earn Carbon Credits for reducing emissions in the atmosphere. Lanxess India and a few other large companies are supposedly using biomass briquettes for earning Carbon Credits by switching their boiler fuel. Biomass briquettes also provide more calorific value/kg and save around 30-40 percent of boiler fuel costs.\n\nA popular biomass briquette emerging in developed countries takes a waste produce such as sawdust, compresses it and then extrudes it to make a reconstituted log that can replace firewood. It is a similar process to forming a wood pellet but on a larger scale. There are no binders involved in this process. The natural lignin in the wood binds the particles of wood together to form a solid. Burning a wood briquette is far more efficient than burning firewood. Moisture content of a briquette can be as low as 4%, whereas green firewood may be as high as 65%.\n\nFor example, parameters of fuel briquettes made by extrusion from sawdust in Ukraine:\n\nThe extrusion production technology of briquettes is the process of extrusion screw wastes (straw, sunflower husks, buckwheat, etc.) or finely shredded wood waste (sawdust) under high pressure when heated from . As shown in the table above the quality of such briquets, especially heat content, is much higher comparing with other methods like using piston presses.\n\nSawdust briquettes have developed over time with two distinct types: those with holes through the centre, and those that are solid. Both types are classified as briquettes but are formed using different techniques. A solid briquette is manufactured using a piston press that compresses sandwiched layers of sawdust together. Briquettes with a hole are produced with a screw press. The hole is from the screw thread passing through the centre, but it also increases the surface area of the log and aids efficient combustion.\n\nPaper briquettes are the byproduct of a briquettor, which compresses shredded paper material into a small cylindrical form. Briquettors are often sold as add-on systems to existing disintegrator or rotary knife mill shredding systems. The NSA has a maximum particle size regulation for shredded paper material that is passed through a disintegrator or rotary knife mill, which typically does not exceed 1/8” square. This means that material exiting a disintegrator is the appropriate size for compression into paper briquettes, as opposed to strip-cut shredders which produce long sheets of paper.\n\nAfter being processed through the disintegrator, paper particles are typically passed through an air system to remove dust and unwanted magnetic materials before being sent into the briquettor. The air system may also be responsible for regulating moisture content in the waste particles, as briquetting works optimally within a certain range of moisture. Studies have shown that the optimal moisture percentage for shredded particles is 18% for paper and 22% for wheat straw.\n\nBriquetted paper has many notable benefits, many of which minimize the impact of the paper waste generated by a shredding system. Several manufactures claim up to 90% volume reduction of briquetted paper waste versus traditional shredding. Decreasing the volume of shredded waste allows it to be transported and stored more efficiently, reducing the cost and fuel required in the disposal process.\n\nIn addition to the cost savings associated with reducing the volume of waste, paper briquettes are more useful in paper mills to create recycled paper than uncompressed shredded material. Compressed briquettes can also be used as a fuel for starting fires or as an insulating material.\n\nCharcoal burners should not be used in enclosed environments to heat homes, as Carbon monoxide poisoning can be fatal.\n\n\n"}
{"id": "2080260", "url": "https://en.wikipedia.org/wiki?curid=2080260", "title": "Carrier generation and recombination", "text": "Carrier generation and recombination\n\nIn the solid-state physics of semiconductors, carrier generation and radiative recombination are processes by which mobile charge carriers (electrons and electron holes) are created and eliminated. Carrier generation and recombination processes are fundamental to the operation of many optoelectronic semiconductor devices, such as photodiodes, LEDs and laser diodes. They are also critical to a full analysis of p-n junction devices such as BJTs and p-n junction diodes.\n\nThe electron–hole pair is the fundamental unit of generation and recombination, corresponding to an electron transitioning between the valence band and the conduction band where generation of electron is a transition from the valence band to the conduction band and recombination leads to a reverse transition.\n\nLike other solids, semiconductor materials have an electronic band structure determined by the crystal properties of the material. The actual energy distribution among the electrons is described by the Fermi level and the temperature of the electrons. At absolute zero temperature, all of the electrons have energy below the Fermi level; but at non-zero temperatures the energy levels are filled following a Boltzmann distribution.\n\nIn undoped semiconductors the Fermi level lies in the middle of a \"forbidden band\" or band gap between two \"allowed bands\" called the \"valence band\" and the \"conduction band\". The valence band, immediately below the forbidden band, is normally very nearly completely occupied. The conduction band, above the Fermi level, is normally nearly completely empty. Because the valence band is so nearly full, its electrons are not mobile, and cannot flow as electric current.\n\nHowever, if an electron in the valence band acquires enough energy to reach the conduction band, it can flow freely among the nearly empty conduction band energy states. Furthermore, it will also leave behind an electron hole that can flow as current exactly like a physical charged particle. Carrier \"generation\" describes processes by which electrons gain energy and move from the valence band to the conduction band, producing two mobile carriers; while \"recombination\" describes processes by which a conduction band electron loses energy and re-occupies the energy state of an electron hole in the valence band.\n\nIn a material at thermal equilibrium generation and recombination are balanced, so that the net charge carrier density remains constant. The equilibrium carrier density that results from the balance of these interactions is predicted by thermodynamics. The resulting probability of occupation of energy states in each energy band is given by Fermi–Dirac statistics.\n\nCarrier generation and recombination occur when an electron makes transition from the valence band to conduction band in a semiconductor, as a result of interaction with other electrons, holes, photons, or the vibrating crystal lattice itself. These processes must conserve both quantized energy and momentum, and the vibrating lattice plays a large role in conserving momentum as, in collisions, photons can transfer very little momentum in relation to their energy.\n\nRecombination and generation are always happening in semiconductors, both optically and thermally, and their rates are in balance at equilibrium. The product of the electron and hole densities (formula_1 and formula_2) is a constant formula_3 at equilibrium, maintained by recombination and generation occurring at equal rates. When there is a surplus of carriers (i.e., formula_4), the rate of recombination becomes greater than the rate of generation, driving the system back towards equilibrium. Likewise, when there is a deficit of carriers (i.e., formula_5), the generation rate becomes greater than the recombination rate, again driving the system back towards equilibrium. As the electron moves from one energy band to another, the energy and momentum that it has lost or gained must go to or come from the other particles involved in the process (e.g. photons, electron, or the system of vibrating lattice atoms). The following models are used to describe generation and recombination, depending on which particles are involved in the process.\n\nIn \"Shockley-Read-Hall recombination\", also called \"trap-assisted recombination\", the electron in transition between bands passes through a new energy state (localized state) created within the band gap by an impurity in the lattice; such energy states are called deep-level traps. The localized state can absorb differences in momentum between the carriers, and so this process is the dominant generation and recombination process in silicon and other indirect bandgap materials. It can also dominate in direct bandgap materials under conditions of very low carrier densities (very low level injection). The energy is exchanged in the form of lattice vibration, a phonon exchanging thermal energy with the material. The process is named after William Shockley, William Thornton Read and Robert N. Hall.\n\nDuring \"radiative recombination\", a form of spontaneous emission, a photon is emitted with the wavelength corresponding to the energy released. This effect is the basis of LEDs. Because the photon carries relatively little momentum, radiative recombination is significant only in direct bandgap materials.\n\nWhen photons are present in the material, they can either be absorbed, generating a pair of free carriers, or they can \"stimulate\" a recombination event, resulting in a generated photon with similar properties to the one responsible for the event. Absorption is the active process in photodiodes, solar cells, and other semiconductor photodetectors, while stimulated emission is responsible for laser action in laser diodes.\n\nIn thermal equilibrium the radiative recombination formula_6 and thermal generation rate formula_7 equal each other\nwhere formula_9 is called the radiative capture probability and formula_10 the intrinsic carrier density.\n\nUnder steady-state conditions the radiative recombination rate formula_11 and resulting net recombination rate formula_12 are\nwhere the carrier densities formula_14 are made up of equilibrium formula_15 and excess densities formula_16\nThe radiative lifetime formula_18 is given by\n\nIn \"Auger recombination\" the energy is given to a third carrier, which is excited to a higher energy level without moving to another energy band. \nAfter the interaction, the third carrier normally loses its excess energy to thermal vibrations. \nSince this process is a three-particle interaction, it is normally only significant in non-equilibrium conditions when the carrier density is very high. \nThe Auger effect process is not easily produced, because the third particle would have to begin the process in the unstable high-energy state.\n\nIn thermal equilibrium the Auger recombination formula_20 and thermal generation rate formula_7 equal each other\nwhere formula_23 are the Auger capture probabilities.\n\nThe non-equilibrium Auger recombination rate formula_24 and resulting net recombination rate formula_25 under steady-state conditions are\n\nThe Auger lifetime formula_27 is given by\n\nThe mechanism causing LED efficiency droop was identified in 2007 as Auger recombination, which met with a mixed reaction. In 2013, an experimental study claimed to have identified Auger recombination as the cause of efficiency droop. However, it remains disputed whether the amount of Auger loss found in this study is sufficient to explain the droop. Other frequently quoted evidence against Auger as the main droop causing mechanism is the low-temperature dependence of this mechanism which is opposite to that found for the drop.\n\n\n"}
{"id": "8017787", "url": "https://en.wikipedia.org/wiki?curid=8017787", "title": "Christmas 1994 nor'easter", "text": "Christmas 1994 nor'easter\n\nThe Christmas 1994 nor'easter was an intense cyclone along the East Coast of the United States and Atlantic Canada. It developed from an area of low pressure in the southeast Gulf of Mexico near the Florida Keys, and moved across the state of Florida. As it entered the warm waters of the Gulf Stream in the Atlantic Ocean, it began to rapidly intensify, exhibiting traits of a tropical system, including the formation of an eye. It attained a pressure of 970 millibars on December 23 and 24, and after moving northward, it came ashore near New York City on Christmas Eve. Because of the uncertain nature of the storm, the National Hurricane Center (NHC) did not classify it as a tropical cyclone.\n\nHeavy rain from the developing storm contributed to significant flooding in South Carolina. Much of the rest of the East Coast was affected by high winds, coastal flooding, and beach erosion. New York State and New England bore the brunt of the storm; damage was extensive on Long Island, and in Connecticut, 130,000 households lost electric power during the storm. Widespread damage and power outages also occurred throughout Rhode Island and Massachusetts, where the storm generated waves along the coast. Because of the warm weather pattern that contributed to the storm's development, precipitation was limited to rain. Two people were killed, and damage amounted to at least $21 million.\n\nThe storm originated in an upper-level low pressure system that moved southeastward from the central Great Plains into the Deep South of the United States. After reaching the southeast Gulf of Mexico, the disturbance underwent cyclogenesis, and the resultant system moved through Florida on December 22 in response to an approaching trough. National Hurricane Center forecaster Jack Beven noted that \"as it moved out into the Bahamas, it appeared to take on the characteristics of a tropical storm.\" The uncertain nature of the storm prevented the NHC from issuing advisories on it, and forecasters lacked sufficient data to fully assess the cyclone for potential tropical characteristics. The same trough that pushed the storm across Florida had moved to the north, allowing for high pressure to develop in the upper levels of the atmosphere.\n\nDeemed a \"hybrid storm\", the cyclone rapidly intensified in warm waters of up to from the Gulf Stream combined with a cold air mass over the United States. The system continued to rapidly intensify while moving within the Gulf Stream; it developed central convection, an unusual trait for an extratropical cyclone, and at one point exhibited an eye. Despite these indications of tropical characteristics, \"There was no front associated with it and it had a warm core, but the radius of maximum winds was more than , so under the standard NHC criteria it didn't qualify as a tropical storm.\" On December 23 and 24, the nor'easter intensified to attain a barometric pressure of . An upper-level low pressure system that developed behind the storm began to intensify and grew to be larger in size than the original disturbance. In an interaction known as the Fujiwhara effect, the broad circulation of the secondary low swung the primary nor'easter northwestward towards southern New York and New England. The original low passed along the south shore of Long Island, and made landfall near New York City on December 24. Subsequently, it moved over southeastern New York State. On December 25, the system began to rapidly weaken as it moved towards Nova Scotia, before the pair of low pressure systems moved out to sea in tandem in the early hours of December 26.\n\nIn South Carolina, flooding associated with the cyclone was considered to be the worst since 1943. Over of rainfall was reported, while winds brought down trees and ripped awnings. In addition, the coast suffered the effects of beach erosion. Thousands of electric customers in the state lost power. As a result of the heavy rainfall, several dams became overwhelmed by rising waters. Extensive flooding of roads and highways was reported, many of which were closed as a result. Up to of water flooded some homes in the region. Approximately 300 people in Florence County were forced to evacuate because of the flooding, and at least 200 homes were damaged. Two deaths were reported in the state. One woman was killed when her vehicle hydroplaned and struck a tree, and another person drowned after her car was struck by another vehicle. Total damage in South Carolina amounted to at least $4 million.\nStrong winds occurred along the North Carolina coast. Diamond Shoals reported sustained winds of , and offshore, winds gusted to . On Wrightsville Beach, rough surf eroded an ledge into the beach. On Carolina Beach, dunes were breached and some roads, including portions of North Carolina Highway 12, were closed.\n\nAs the primary storm entered New England, the secondary low produced minor coastal flooding in the Tidewater region of Virginia on December 23. Winds of and tides to above normal were reported. In Sandbridge, Virginia Beach, Virginia, a beachfront home collapsed into the sea. Several roads throughout the region suffered minor flooding. Strong winds resulting from the tight pressure gradient between the nor'easter and an area of high pressure located over the United States brought down a few utility poles, which sparked a brush fire on December 24. The fire, quickly spread by the wind, burned a field. The winds brought down several trees.\n\nDamage was light in Maryland. Some sand dunes and wooden structures were damaged, and above-normal tides occurred. In New Jersey, high winds caused power outages and knocked down trees and power lines. Minor coastal flooding of streets and houses was reported. Otherwise, damage in the state was minor.\n\nThe storm brought heavy rainfall and high winds to New York State and New York City on December 23 and 24. Gusts of downed hundreds of trees and many power lines on Long Island. Several homes, in addition to many cars, sustained damage. Roughly 112,000 Long Island Lighting Company customers experienced power outages at some point during the storm. As the cyclone progressed northward into New York State, high winds occurred in the Hudson Valley region. Throughout Columbia, Ulster and Rensselaer Counties, trees, tree limbs, and power lines were downed by the winds. At Stephentown, a gust of was reported. Ulster County suffered substantial impacts, with large trees being uprooted and striking homes. Across eastern New York State, 25,000 households lost power as a result of the nor'easter. On the North Fork of Long Island, in Southold, a seaside home partially collapsed into the water.\n\nIn Connecticut, the storm was described as being more significant than anticipated. Gale-force wind gusts, reaching , blew across the state from the northeast and later from the east. Trees, tree limbs, and power lines were downed, causing damage to property and vehicles. The high winds caused widespread power outages, affecting up to 130,000 electric customers. As a result, electric companies sought help from as far as Pennsylvania and Maine to restore electricity. Bruno Ranniello, a spokesman for Northeast Utilities, reported that \"We've had outages in virtually every community.\" In New Haven, the nor'easter ripped three barges from their moorings. One of the barges traveled across the Long Island Sound and ran aground near Port Jefferson, New York. A man in Milford was killed indirectly when a tree that was partially downed by the storm fell on him during an attempt to remove it from a relative's yard. Northeast Utilities, which reported the majority of the power outages, estimated storm damage in the state to be about $6–$8 million (1994 USD; $8.8–$11.8 million 2008 USD).\n\nEffects were less severe in New Hampshire and Vermont. In southern New Hampshire, a line of thunderstorms produced torrential rainfall, causing flooding on parts of New Hampshire Route 13. Flash flooding of several tributaries feeding into the Piscataquog River was reported. In Maine, the storm brought high winds and heavy rain. Along the coast of southern Maine and New Hampshire, beach erosion was reported. Additionally, minor flooding was reported across the region, as a result of heavy surface runoff and small ice jams. In Rhode Island, the power outages were the worst since Hurricane Bob of the 1991 Atlantic hurricane season. Throughout the state, approximately 40,000 customers were without electric power. As with Massachusetts, downed trees and property damage were widespread. There were many reports of roof shingles being blown off roofs and of damage to gutters. In Warwick, several small boats were damaged after being knocked into other boats. The highest reported wind gust in the state was at Ashaway, Rhode Island. Statewide damage totaled about $5 million.\n\nMassachusetts, particularly Cape Cod and Nantucket, bore the brunt of the nor'easter. Reportedly, wind gusts approached on Cape Cod and, offshore, waves reached . At Walpole, wind gusts peaked at , while on Nantucket gusts of were reported. The winds left 30,000 electric customers without power during the storm, primarily in the eastern part of the state. Power was out for some as long as 48 hours. Property damage was widespread and many trees, signs, and billboards were blown down. A large tent used by the New England Patriots was ripped and blown off its foundation. The winds also spread a deadly house fire in North Attleboro. Although not directly related to the storm, it caused seven fatalities. Because tides were low, little coastal flooding occurred. Outside the Prudential Tower Center in Boston, the storm toppled a Christmas tree. Rainfall of was recorded throughout the eastern part of the state, contributing to heavy runoff that washed away a section of a highway. Total damage in Massachusetts was estimated at about $5 million.\n\n"}
{"id": "36382684", "url": "https://en.wikipedia.org/wiki?curid=36382684", "title": "Dakshin Haryana Bijli Vitran Nigam", "text": "Dakshin Haryana Bijli Vitran Nigam\n\nDakshin Haryana Bijli Vitran Nigam, also known as DHBVN Limited is an Indian state owned electric utility company. It is owned by Government of Haryana and its headquarters is located in Hisar city of Haryana, India.\n\nIn 1998, Haryana State Electricity Board was divided into two parts, namely, Haryana Power Generation Corporation Limited (HPGCL) and Haryana Vidyut Prasaran Nigam Limited (HVPNL). On 1 July 1999, HVPNL was further divided into two parts, namely, Uttar Haryana Bijli Vitran Nigam (UHBVN) and Dakshin Haryana Bijli Vitran Nigam (DHBVN). DHBVN is responsible for distribution of power in South Haryana.\n\nDHBVN is responsible for distribution of power in the following districts of Haryana:\n\n"}
{"id": "45698778", "url": "https://en.wikipedia.org/wiki?curid=45698778", "title": "Dodecaborate", "text": "Dodecaborate\n\nThe dodecaborate(12) anion, [BH], has the structure of a regular icosahedron of boron atoms, with each boron atom being attached to a hydrogen atom. Its symmetry is classified by the molecular point group I.\n\nThe existence of the dodecaborate(12) anion, [BH], was predicted by H. C. Longuet-Higgins and M. de V. Roberts in 1955. Hawthorne and Pitochelli first made it 5 years later, by the reaction of iododecarborane with triethylamine in benzene solution at 80 °C. It is more conveniently prepared in two steps from sodium borohydride. First the borohydride is converted into a triborate anion using the etherate of boron trifluoride:\nPyrolysis of the triborate gives the twelve boron cluster as the sodium salt. A variety of other synthetic methods have been published.\n\nSalts of the dodecaborate ion are stable in air and do not react with hot aqueous sodium hydroxide or hydrochloric acid. The anion can be electrochemically oxidised to [BH].\n\nSalts of BH undergo hydroxylation with hydrogen peroxide to give salts of [B(OH)]. The hydrogen atoms in the ion [BH] can be replaced by the halogens with various degrees of substitution. The following numbering scheme is used to identify the products. The first boron atom is numbered 1, then the closest ring of five atoms around it is numbered anticlockwise from 2 to 6. The next ring of boron atoms is started from 7 for the atoms closest to number 2 and 3, and counts anticlockwise to 11. The atom opposite the original is numbered 12. A related derivative is [B(CH)]. The icosahedron of boron atoms is aromatic in nature.\n\nUnder kilobar pressure of carbon monoxide [BH] reacts to form the carbonyl derivatives [BHCO] and the 1,12- and 1,7-isomers of BH(CO). The para disubstitution at the 1,12 is unusual. In water the dicarbonyls appear to form carboxylic ions: [BH(CO)COH] and [BH(COH)].\n\nAlthough derivatives of [BH] have not found any practical or commercial applications, the unusual nature of this dianion has attracted repeated investigations.\n\nCompounds based on the ion [BH] have been evaluated for solvent extraction of the radioactive ions Eu and Am.\n\n[BH], [B(OH)] and [B(OMe)] show promise for use in drug delivery. They form \"closomers,\" which have been used to make nontargeted high-performance MRI contrast agents which are persistent in tumor tissue.\n\nSalts of [BH] are potential therapeutic agents in cancer treatment. For applications in boron neutron capture therapy (BNCT), derivatives of closo-dodecaborate increase the specificity of neutron irradiation treatment. Neutron irradiation converts nonradioactive dodecaborate containing B to radioactive B, which upon radioactive decay emits an alpha particle near the tumor.\n"}
{"id": "10389884", "url": "https://en.wikipedia.org/wiki?curid=10389884", "title": "Electranet", "text": "Electranet\n\nElectranet is a proposed smart electric grid which would allow people to sell electricity into the grid without any artificial caps. It was proposed in an op-ed article Al Gore wrote in a \"My Turn\" column for \"Newsweek\" in 2006.\n\nLike the internet, which widely distributed information, the Electranet would allow homeowners and small businesses to operate small generating power facilities and contribute to local and regional energy needs by selling power into the grid at a rate that is determined by free market forces. Like the internet, which led to a surge of productivity, the Electranet's distributed generation model is hoped to lead to innovative power generation through alternative sources and lowering greenhouse gas emissions. Electranet advocates predict that as more people participate in the Electranet, the cost of electricity will continue to go down until it becomes free.\n\nIn a 2006 speech to NYU, Al Gore advocated deploying an Electranet as one way to reduce global warming, as well as reduce the cost of energy, by dropping the barriers to entry for average citizens to actively participate in the business of harnessing and distributing power. Al Gore frequently notes that \"a lot of energy is all around us. Instead of having the sun bake us in the summer, why not gather that energy\", he asks. \"Why not collect from road surfaces, roofs, any painted surfaces and more?\"\n\nAl Gore has also said that \"once there is a convenient way to sell energy into the grid, not only will homeowners be compelled to hop onto the grid and supply energy, but there will also be a brand new market to develop and then sell all sorts of energy gathering devices and technologies, such as solar paint, solar fabrics, thin films for windows, micro windmills, and more.\"\n\nAl Gore mentioned the Electranet idea in his congressional Global Warming Testimony on March 21, 2007.\n\nElectranet advocates predict that just like data on the internet, energy itself, including what is necessary for electric cars, will continue to be more economical as more people get onto the smart grid and supply power, and as more and more efficient technologies are developed for gathering the abundant energy around us. It is reasonable to imagine a day when there is no actual cost for driving, per mile, for energy, because of the abundance of energy coming from the Electranet.\n\n"}
{"id": "32459315", "url": "https://en.wikipedia.org/wiki?curid=32459315", "title": "Elizabeth Cushman Titus Putnam", "text": "Elizabeth Cushman Titus Putnam\n\nElizabeth Cushman Titus Putnam is an American conservationist who in 2010 won the second highest civilian award, the Presidential Citizens Medal, and was the first conservationist to have ever won the honor.\n"}
{"id": "37100183", "url": "https://en.wikipedia.org/wiki?curid=37100183", "title": "Estate of Takil-ana-ilīšu kudurru", "text": "Estate of Takil-ana-ilīšu kudurru\n\nThe estate of Takil-ana-ilīšu kudurru is an ancient Mesopotamian white limestone \"narû\", or entitlement stela, dating from the latter part of the Kassite era which gives a history of the litigation concerning a contested inheritance over three generations or more than forty years. It describes a patrimonial redemption, or \"lineage claim,\" and provides a great deal of information concerning inheritance during the late Bronze Age. It is identified by its colophon, \"asumittu annītu garbarê šalati kanīk dīnim\", “this stela is a copy of three sealed documents with (royal) edicts” and records the legal judgments made in three successive reigns of the kings, Adad-šuma-iddina (ca. 1222–1217 BC), Adad-šuma-uṣur (ca. 1216–1187 BC) and Meli-Šipak (ca. 1186–1172 BC). It is a contemporary text which confirms the sequence of these Kassite monarchs given on the Babylonian king list and provides the best evidence that the first of these was unlikely to have been merely an Assyrian appointee during their recent hegemony over Babylonia by Tukulti-Ninurta I, as his judgments were honored by the later kings.\n\nThe kudurru was excavated from the ruins of the Marduk temple in Babylon by Hormuzd Rassam and his chief foreman Daud Thoma in 1880 and is now in the British Museum assigned museum reference BM 90827. The text is inscribed on six columns on a two foot high stone block topped by a triangular apex carved with fifteen divine symbols.\n\nThe case begins “When the house of Takil-ana-ilīšu lapsed for want of an heir in the time of King Adad-šuma-iddina, King Adad-šuma-iddina gave the house (i.e. the estate) of Takil-ana-ilīšu to Ur-Nintinuga, brother of Takil-ana-ilīšu.”\n\nAs was customary on such monuments, various deities were invoked to curse any party who might dispute the legal decision recorded on the kudurru. These included Anu, Enlil, and Ea (evil eye), Sîn, Šamaš, Adad and Marduk (tearing out the foundation); Ningursu and Bau (joyless fate), Šamaš and Adad (lawlessness); Pap-nigin-gara (destruction of the landmark), Uraš and Ninegal (evil); Kassite deities Šuqamuna and Šumalia (humiliation before the king and princes), Ištar (defeat); all the named gods (destruction of the name).\n\nThe roles and readings of the names of the participants have changed since the first publication of the text and the identifications given here follow Paulus (2007), who argues that the outcome of the case hinges on the performance of the \"River ordeal.\"\n\nThe protagonists:\n\n\nLitigants and witnesses during the reign of Adad-šuma-iddina:\n\nLitigants and witnesses under the reign of Adad-šuma-uṣur:\n\n\nLitigants and witnesses under the reign of Meli-Šipak:\n\n\n"}
{"id": "16215818", "url": "https://en.wikipedia.org/wiki?curid=16215818", "title": "Fault gouge", "text": "Fault gouge\n\nFault gouge is a tectonite (a rock formed by tectonic forces) with a very small grain size. Fault gouge has no cohesion and it is normally an unconsolidated rock type, unless cementation took place at a later stage. A fault gouge forms in the same way as fault breccia, the latter also having larger clasts. In comparison to fault breccia, which is another incohesive fault rock, fault gouge has less visible fragments (less than 30% visible fragments regarding fault gouges, and more than 30% regarding fault breccia).\n\nGouge-filled faults can be weak planes in rock masses. If compressive stresses are enough these can cause compressive yielding or eventually rock fracture. \n\nFault gouge forms by tectonic movement along a localized zone of brittle deformation (a fault zone) in a rock. The grinding and milling that results when the two sides of the fault zone move along each other results in a material that is made of loose fragments. First a fault breccia will form, but if the grinding continues the rock becomes fault gouge.\n\n"}
{"id": "1232236", "url": "https://en.wikipedia.org/wiki?curid=1232236", "title": "Fluoromethane", "text": "Fluoromethane\n\nFluoromethane, also known as methyl fluoride, Freon 41, Halocarbon-41 and HFC-41, is a non-toxic, liquefiable, and flammable gas at standard temperature and pressure. It is made of carbon, hydrogen, and fluorine. The name stems from the fact that it is methane (CH) with a fluorine atom substituted for one of the hydrogen atoms. \n\nThe compound is the lowest mass member of the hydrofluorocarbon (HFC) family, compounds which contain only hydrogen, fluorine, and carbon. These are related to the Chlorofluorocarbon (CFC)s, but since they do not contain chlorine, were thought not to be destructive to the ozone layer. However, they are still very harmful to the ozone layer, and the Kigali Amendment to the Montreal Protocol is an attempt to phase them out.\n\nIt is used in the manufacture of semiconductor and electronic products. In the presence of an RF field fluoromethane will dissociate into fluoride ions that selectively etch silicon compound films (reactive-ion etching). Fluoromethane has an agreeable ether-like odor and is also narcotic in high concentrations. Fluoromethane is highly flammable and burns in air with evolution of highly toxic hydrogen fluoride. The flame is colorless, similar to alcohol.\n\nThe C-F bond energy is 552 kJ/mol and its length is 0.139 nm (typically 0.14 nm). Its molecular geometry is tetrahedral.\n\nIts specific heat capacity (C) is 38.171 J·mol·K at 25 °C. The critical point of fluoromethane is at 44.9 °C (318.1 K) and 6.280 MPa.\n\n\n"}
{"id": "616108", "url": "https://en.wikipedia.org/wiki?curid=616108", "title": "Forests of Sweden", "text": "Forests of Sweden\n\nSweden is covered by 53.1% forest. In southern Sweden, human interventions started to have a significant impact on broadleaved forests around 2000 years ago, where the first evidence of extensive agriculture has been found. Recent studies describe a long-term process of borealization in south-central Sweden starting at the beginning of the Holocene where oak (\"Quercus\" spp.) and alder (\"Alnus\" spp.) seemingly started to decline around 2000 years ago due to a decrease in temperature. At the same time the Norway spruce (\"Picea abies\") started to emigrate from the north, and the European beech (\"Fagus sylvatica\") emigrated from the south of Europe. Though, as a primary result of production forest management at the middle of the twentieth century, \"P. abies\" and Scots pine (\"Pinus sylvestris\") covers together around 75% of southern Sweden actual standing tree volume.\n\nWood from the forest has long been used in the southern part of the country and in early agricultural areas as a source of fuel and as a building material. Wood was essential for Sweden's early mining industry, as it was used to produce charcoal for processing ore. Other important forestry products included wood pitch, tar, and potash, which were produced for export beginning in the Middle Ages.\n\nForestry work expanded to Norrland beginning in the early 19th century, and the resulting cleared areas became the site of small farms and pastures. Extensive logging resulted in the development of a sawmill industry, which produced lumber for export. This expansion continued until 1905.\n\n"}
{"id": "47857506", "url": "https://en.wikipedia.org/wiki?curid=47857506", "title": "Fractalgrid", "text": "Fractalgrid\n\nIn electric power distribution, a fractalgrid is a system-of-systems architecture of distributed energy resources or DERs.\nIn a fractalgrid topology, multiple microgrids are strategically arranged to follow a fractal or recursive pattern. Fractals, or self-similar patterns, can be seen in nature. Clouds, river networks, and lightning bolts are a few examples of natural phenomena that display fractal features. In a fractalgrid, a microgrid may be composed of smaller microgrids or “fractal units”. In such a configuration, the network becomes one of simplified power flows and communications through distributed substations.\n\nThere are two main variations of the fractalgrid concept. Though not mutually exclusive, CleanSpark's FractalGrid architecture is a bottom-up implementation while NRECA's flows from the top-down to reach similar architectural quality attributes.\n\nThis variation is a command-and-control platform that integrates energy storage technology with on-site generation, monitored by a distributed data monitoring and controls system. The fundamental goals of the fractalgrid are to ensure energy security to critical facilities and functions of the local area while reducing overall cost.\n\nFeatures include clean energy storage integration, technology-agnostic implementation, load management, demand response, peak shaving, and real-time energy optimization.\n\nWithin a fractalgrid, microgrids are placed in parent-child relationships in which a child microgrid can be islanded from its parent microgrid. Each fractal microgrid can operate autonomously or federated with others. The federated state allows for sharing of resources but also allows for disconnection in the cases of maintenance and emergencies. In the same way that microgrids are able to island from the utility when needed, fractal microgrids can disconnect from one another in order to maintain power supply to critical loads.\n\nThe fractalgrid was conceived in 2012 by Art Villanueva, CleanSpark's founding CTO and CSO and designed and implemented by Jennifer Worrall. The implementation uses recursion and bounds complexity to O(N).\n\nStarting July 2014, an active fractalgrid has been active in San Diego County, California at military base Camp Pendleton, funded by the California Energy Commission to exhibit the uses of fractalgrid technology. The Camp Pendleton fractalgrid is connected to a 1.1 MW facility consisting of several buildings at the Barracks, a three-story parking garage and three cell towers. The project implements three self-sufficient fractal microgrids and a larger microgrid that interconnects the site such that resources can be shared between microgrids in various configurations. Each fractal microgrid's operational status is dependent on the state of power supply and the need to keep critical loads powered. Using real-time data, the system monitors the energy consumption levels and evaluates the power generation received from distributed energy resources (DERs). This analysis is used to determine how many loads can be powered by local generation.\n\nThe CPFD illustrates islanding within a fractalgrid. Each fractal microgrid is capable of completely separating from its parent microgrid in order to best support critical loads.\n\nThe Agile Fractal Grid is a concept envisioned by Craig Miller, scientist at the National Rural Electric Cooperative Association (NRECA), as well as Maurice Martin, David Pinney, and George Walker. According to their report, \"Achieving a Resilient and Agile Grid\" the ideal principles of Fractal Operation are as follows:\n\nAn integral part of the agile grid is segmentation. The concept calls for a collection of independently operating systems that function together in a coordinated manner, as opposed to the traditional utility grid supplying energy for a large geographical area. Units are segmented in such a way that they are able to act as individual units and localize power supply and control. A large benefit to segmentation is the ability for the individual units to act separately from a central control system, which can create more stability in the overall system and decentralizes the source of energy.\n\nThe basis for the agile grid is segmentability, rather than segmentation. It is crucial for the units to have the ability to operate separately from each other, but only when it is practical to do so. Integration between units must also occur for the system to be efficient.\n\n"}
{"id": "806075", "url": "https://en.wikipedia.org/wiki?curid=806075", "title": "Gamma-Linolenic acid", "text": "Gamma-Linolenic acid\n\nGamma-linolenic acid or GLA (γ-Linolenic acid), (INN and USAN gamolenic acid) is a fatty acid found primarily in vegetable oils. When acting on GLA, Arachidonate 5-lipoxygenase produces no leukotrienes and the conversion by the enzyme of arachidonic acid to leukotrienes is inhibited.\n\nGLA is categorized as an \"n\"−6 (also called ω−6 or omega-6) fatty acid, meaning that the first double bond on the methyl end (designated with \"n\" or ω) is the sixth bond. In physiological literature, GLA is designated as 18:3 (\"n\"−6). GLA is a carboxylic acid with an 18-carbon chain and three \"cis\" double bonds. It is an isomer of α-linolenic acid, which is a polyunsaturated n−3 (omega-3) fatty acid, found in rapeseed canola oil, soybeans, walnuts, flax seed (linseed oil), perilla, chia, and hemp seed.\n\nGLA was first isolated from the seed oil of evening primrose. This herbal plant was grown by Native Americans to treat swelling in the body.\nIn the 17th century, it was introduced to Europe and became a popular folk remedy, earning the name \"king's cure-all.\"\nIn 1919, Heiduschka and Lüft extracted the oil from evening primrose seeds and described an unusual linolenic acid, which they name γ-.\nLater, the exact chemical structure was characterized by Riley.\n\nAlthough there are α- and γ- forms of linolenic acid, there is no β- form. One was once identified, but it turned out to be an artifact of the original analytical process.\n\nGLA is obtained from vegetable oils such as evening primrose (\"Oenothera biennis\") oil (EPO), blackcurrant seed oil, borage seed oil, and hemp seed oil. GLA is also found in varying amounts in edible hemp seeds, oats, barley, and spirulina. Normal safflower (\"Carthamus tinctorius\") oil does not contain GLA, but a genetically modified GLA safflower oil available in commercial quantities since 2011 contains 40% GLA. Borage oil contains 20% GLA, evening primrose oil ranges from 8% to 10% GLA, and black-currant oil contains 15-20%. It also comprises 12.23% of the fats from the fruit of the durian species \"Durio graveolens\".\n\nThe human body produces GLA from linoleic acid (LA). This reaction is catalyzed by Δ-desaturase (D6D), an enzyme that allows the creation of a double bond on the sixth carbon counting from the carboxyl terminus. LA is consumed sufficiently in most diets, from such abundant sources as cooking oils and meats. However, a lack of GLA can occur when there is a reduction of the efficiency of the D6D conversion (for instance, as people grow older or when there are specific dietary deficiencies) or in disease states wherein there is excessive consumption of GLA metabolites.\n\nFrom GLA, the body forms dihomo-γ-linolenic acid (DGLA). This is one of the body's three sources of eicosanoids (along with AA and EPA.) DGLA is the precursor of the prostaglandin PGH, which in turn forms PGE and the thromboxane TXA. Both PGE1 and TXA are anti-inflammatory; thromboxane TXA, unlike its series-2 variant, induces vasodilation, and inhibits platelet consequently, TXA modulates (reduces) the pro-inflammatory properties of the thromboxane TXA. PGE has a role in regulation of immune system function and is used as the medicine alprostadil.\n\nUnlike AA and EPA, DGLA cannot yield leukotrienes. However, it can inhibit the formation of pro-inflammatory leukotrienes from AA.\n\nAlthough GLA is an \"n\"−6 fatty acid, a type of acid that is, in general, pro-inflammatory, it has anti-inflammatory properties. \"(See discussion at Essential fatty acid interactions: The paradox of dietary GLA.)\"\n\nGLA has been promoted as medication for a variety of ailments including breast pain and eczema, in particular by David Horrobin (1939 – 2003), whose marketing of evening primrose oil was described by \"The BMJ\" (British Medical Journal) as ethically dubious – the substance was likely to be remembered as \"a remedy for which there is no disease\". \nIn 2002 the UK's Medicines and Healthcare products Regulatory Agency withdrew marketing authorisations for evening primrose oil as an eczema remedy. However, more recently, topical application of borage seed oil (an oil with a high concentration of GLA) has been shown to reduce the symptoms of atopic dermatitis in a double-blind, placebo-controlled clinical trial.\n"}
{"id": "29686888", "url": "https://en.wikipedia.org/wiki?curid=29686888", "title": "Jesús León Santos", "text": "Jesús León Santos\n\nJesús León Santos is a Mexican environmentalist. He was awarded the Goldman Environmental Prize in 2008, for his efforts on a sustainable development of agriculture in the Mixtec region of the state of Oaxaca.\n"}
{"id": "4989070", "url": "https://en.wikipedia.org/wiki?curid=4989070", "title": "Karachi Nuclear Power Complex", "text": "Karachi Nuclear Power Complex\n\nThe Karachi Nuclear Power Complex or KNPC is located in Paradise Point, Karachi, Sindh, Pakistan. It consists of the Karachi Nuclear Power Plant (KANUPP) and the Pakistan Atomic Energy Commission's Control & Instrumentation Analysis Lab (CIAL KARACHI). Two new nuclear power plants, KANUPP-2 and KANUPP-3, are also under construction at the site. When complete, the complex of civilian nuclear power plants will produce over 2000 MW of electricity. The International Atomic Energy Agency safeguards and inspects the complex. The plant is under construction by the Pakistan Atomic Energy Commission (PAEC) and is financed by the IAEA, the China Guangdong Nuclear Power Group, the China National Nuclear Corporation, and the China Atomic Energy Authority.\n\nThe National Engineering Services Pakistan (NESPAK) has been providing consultancy services to the Karachi Nuclear Power Complex for the replacement of the aging circuit breakers and protective relays of the 132 kV double circuit transmission line that links the complex with the Karachi Electric Supply Corporation network at Baldia grid station. The old bulk oil circuit breakers and electromechanical protective relays have been replaced with the latest SF circuit breakers and modern numerical line protection devices.\n\nStand-alone 132 kV current transformers have been provided for protection and metering functions as substitute for the bushing current transformers in the bulk oil circuit breakers. KNPC and NESPAK engineers worked closely in devising a discriminative protection scheme and its integration into the complex systems of the nuclear power plant. Both circuits of KANUPP-Baldia double circuit lines have been re-energised with the new equipment. Performance indicators have since validated the intended objectives of the project vis-a-vis discriminative functioning of the protection system against disturbances in KESC power system for stable operation of KANUPP.\n\nKANUPP-I is a CANDU reactor supplied by the Canadian Government in 1971. It was inaugurated on November 28, 1972, by then-President Zulfiqar Ali Bhutto. KANUPP-I is a single unit pressurized heavy water reactor with a total gross capacity of 137 MW. It remains in operation as of May 2014, and is currently limited to 85 MW. The International Atomic Energy Agency has monitored the reactor since its construction.\n\nSince independence from United Kingdom in 1947, Pakistan had repeatedly suffered energy crises that have contributed to the country's economic slowdown. In 1960, President Field Marshal Ayub Khan appointed Abdus Salam as his Science Advisor. Soon after, Abdus Salam became the head of Pakistan's IAEA delegation. In that capacity, Abdus Salam advocated the UN General Assembly for the support of nuclear energy in Pakistan. Due to Abdus Salam's influence on President Ayub Khan, Salam had the commercial nuclear plant near Karachi personally approved, in spite of opposition to the project from the military establishment. In 1965, Abdus Salam traveled to United States, where in a ceremony, Canada and Pakistan signed a nuclear energy pact with GE Canada establishing the country's first nuclear plant. Per agreement, the Pakistan Atomic Energy Commission's engineers and scientists led the construction of the project, while GE Canada provided funds and enriched uranium based nuclear fuel. Parvez Butt, a nuclear engineer, was the chief designer of the plant at the GE Canada's designing office. In 1966, construction started, and it was completed in 1971. On November 28,1972 then-President Zulfiqar Ali Bhutto, accompanied by Abdus Salam and the PAEC's newly appointed Chairman Munir Ahmad Khan, inaugurated the first unit of the Karachi Nuclear Plant.\n\nThe technology of the nuclear power plant is similar to India's CIRUS and DHRUVA reactor, with another small reactor producing reactor-grade plutonium. Munir Ahmad Khan, now as chairman, indigenously developed and established the nuclear fuel cycle programme. In 1972, Pakistan, under Bhutto, refused to sign the NPT. Because the reactor-grade plutonium was extremely dangerous to be open in public, the PAEC transferred it to the \"New Laboratories\" (known as The New Labs), and produced the first batch of fresh weapons-grade plutonium. In 1976, Canada stopped the supply of fuel and spare parts for the plant. The highly radioactive material was also left openly in Karachi as Canadian technicians departed from Pakistan. Pakistan media then speculated that in the absence of Canadian officials, the city would suffer a major power blackout. Canadian officials later stated that the reactor would be shut down in six months. However, the PAEC, using indigenous resources, produced sufficient feed for \"KANUPP\". In 1978, the PAEC developed its own nuclear fuel and began loading the feedstock into \"KANUPP-I\". As of December 31, 2013 \"KANUPP-I\" has generated 14.7 billion kWhr of electricity and been fueled by thousands of Pakistani-made fuel bundles without any failure. \n\nThe \"KANUPP\" reactor site is on the Arabian Sea coast, about 11 miles (17.7 km) west of Karachi. The Karachi Nuclear Power Plant (\"KANUPP\") has a heavy water moderated and cooled natural uranium-fueled, horizontal pressure tube reactor. Other distinguishing features are once-through, on-power bidirectional fueling, reactor shutdown by moderator dump, and a reactor building designed for total containment of any pressure resulting from an accident.\n\nThe gross plant rating is 137 MWe and the corresponding net output is 125 MWe. The Nuclear Reactor Building contains the entire reactor system and auxiliaries, and consists of a pre-stressed concrete cylindrical wall, a hemispherical segmental dome of pre-stressed concrete, and a concrete base slab. The Turbine Building houses the turbine-generator and auxiliaries, water processing equipment, electrical distribution equipment, and the control room. The building is a reinforced concrete frame and block structure.\n\nThe reactor consists of a tubed calandria vessel of austenitic stainless steel, which contains the heavy water moderator/reflector and 208 coolant tube assemblies. The moderator system consists of the calandria, coolers, pumps and purification system in the heavy water circuit, and control valves, dump valves and helium blowers in the helium circuit. The fuel is natural uranium in the form of sintered uranium dioxide pellets sheathed in thin zirconium alloy tubes to form solid fuel elements about 19.1 inches (48.53 cm) long by 0.6 inches (1.4 cm) diameter.\n\n\"KANUPP\" came into commercial operation in 1972 and after completing its 30 years of design life was shut down on December 6, 2002. The plant resumed operations in 2006. At present the plant is undergoing several safety upgrades for operation beyond design life. On the request of \"KANUPP\", the Pakistan Nuclear Regulatory Authority allowed the plant to operate at 80 MW for the interim period. The PAEC has denied any media reports of shutting down the nuclear power plant, however, on June 30, 2009, a senior official of PAEC stated that the \"KANUPP\" would be decommissioned in 2012. The \"KANUPP-II\", an indigenous nuclear power plant build by PAEC, will be taking the place of the \"KANUPP-I\". In November 2013, Pakistan and China confirmed that two ACP1000 Nuclear reactors, KANUPP-2 and KANUPP-3, will be built at Karachi.\n\nOn 18 October 2011 the \"KANUPP\" Karachi nuclear power plant imposed a seven-hour emergency after heavy water leaked from a feeder pipe to the reactor. The leakage started around midnight on Tuesday during a routine maintenance shut down. After the leakage was detected a state of emergency was imposed at the plant and the affected area was isolated. The emergency was lifted seven hours later, after the leak was reportedly brought under control.\n\nKANUPP-2 and 3 will cost $9.5 billion and each unit will produce 1,100 MW and total of 2,200 MW. On 26 November 2013, Prime Minister Nawaz Sharif ceremonially broke ground on a new governmental power project at the Karachi Nuclear Power Complex for the construction of two ACP1000 (Hualong One) nuclear reactors. Dr. Ansar Pervaiz, the Chairman of the Pakistan Atomic Energy Commission, said that KANUPP-2 would begin commercial operations by 2019 and KANUPP-3 by 2020. The reactor design has been upgraded to the Hualong One, a development of the ACP1000, with commercial operation now planned for 2021 and 2022.\n\nKANUPP-4 is a planned commercial nuclear power plant, to be located at Karachi.\n\nThe Karachi Nuclear Power Complex also contains a nuclear engineering college. KANUPP's Institute of Nuclear Power Engineering (KINPOE) is controlled by PAEC. KINPOE offers two-year master program in nuclear engineering accredited by Pakistan Institute of Engineering and Applied Sciences (PIEAS) and PDTP.\n\n\n"}
{"id": "20094074", "url": "https://en.wikipedia.org/wiki?curid=20094074", "title": "Laser-heated pedestal growth", "text": "Laser-heated pedestal growth\n\nLaser-heated pedestal growth (LHPG) or laser floating zone (LFZ) is a crystal growth technique. A narrow region of a crystal is melted with a powerful CO or YAG laser. The laser and hence the floating zone, is moved along the crystal. The molten region melts impure solid at its forward edge and leaves a wake of purer material solidified behind it. This technique for growing crystals from the melt (liquid/solid phase transition) is used in materials research.\n\nThe main advantages of this technique are the high pulling rates (60 times greater than the conventional Czochralski technique) and the possibility of growing materials with very high melting points. In addition, LHPG is a crucible-free technique, which allows single crystals to be grown with high purity and low stress.\n\nThe geometric shape of the crystals (the technique can produce small diameters), and the low production cost, make the single-crystal fibers (SCF) produced by LHPG suitable substitutes for bulk crystals in many devices, especially those that use high melting point materials. However, single-crystal fibers must have equal or superior optical and structural qualities compared to bulk crystals to substitute for them in technological devices. This can be achieved by carefully controlling the growth conditions.\n\nUntil 1980, laser-heated crystal growth used only two laser beams focused over the source material. This condition generated a high radial thermal gradient in the molten zone, making the process unstable. Increasing the number of beams to four did not solve the problem, although it improved the growth process. \n\nAn improvement to the laser-heated crystal growth technique was made by Fejer \"et al.\", who incorporated a special optical component known as a \"reflaxicon\", consisting of an inner cone surrounded by a larger coaxial cone section, both with reflecting surfaces. This optical element converts the cylindrical laser beam into a larger diameter hollow cylinder surface. This optical component allows radial distribution of the laser energy over the molten zone, reducing radial thermal gradients. The axial temperature gradient in this technique can go as high as 10000 °C/cm, which is very high when compared to traditional crystal growth techniques (10–100 °C/cm).\n\nA feature of the LHPG technique is its high convection speed in the liquid phase due to Marangoni convection. It is possible to see that it spins very fast. Even when it appears to be standing still, it is in fact spinning fast on its axis.\n"}
{"id": "593364", "url": "https://en.wikipedia.org/wiki?curid=593364", "title": "List of mountain types", "text": "List of mountain types\n\nMountains and hills can be characterized in several ways. Some mountains are volcanoes and can be characterized by the type of lava and eruptive history. Other mountains are shaped by glacial processes and can be characterized by their shape. Finally, many mountains can be characterized by the type of rock that make up their composition.\n\n\n\n\n\n"}
{"id": "20981191", "url": "https://en.wikipedia.org/wiki?curid=20981191", "title": "Lukoml power station", "text": "Lukoml power station\n\nThe Lukoml power station (Lukoml GRES, Lukomlskaya GRES, Lukomlskaya DRES, ) is a natural gas-fired thermal power station located in Novolukoml, Vitsebsk Voblast, Belarus. It is operated by Belenergo.\n\nConstruction of the power station started in 1964. It was built in two stages. At the first stage, four double-boiler single-turbine units, each of 300 MW capacity, were commissioned in December 1969–September 1971. At the second stage, four single-boiler single-turbine units, each of 300 MW capacity, were added in December 1972–August 1974. In 2000s the generation units were upgraded by 10% increasing the installed capacity up to 2,640 MW.\n\nThe three flue gas stacks, which serve also as electricity pylons for the outgoing power lines are tall and were built in 1969-1973.\n\n"}
{"id": "30797372", "url": "https://en.wikipedia.org/wiki?curid=30797372", "title": "Marshall JTM45", "text": "Marshall JTM45\n\nThe Marshall JTM45 is the first guitar amplifier made by Marshall. First produced in 1963, it has been called a \"seminal\" amplifier, and is praised as the most desirable of all the company's amplifiers.\n\nThe JTM45 was first built in 1963, handmade in an all-aluminum chassis, by Ken Bran; Dudley Craven and Ken Underwood. Its first ever use in a live performance was in September 1963 when the first amp was tested at the Ealing Club, not far from the original Marshall shops. Bran & Craven and Underwood were in attendance. Because of its power, Marshall decided early on to build it as a head, with a separate 4×12\" cabinet with Celestion speakers. The amplifier itself was based on the Fender Bassman. It uses 6V6 valves, then the EL34 (though early versions had used US 5881, a version of the 6L6) in the output stage, and ECC83 (12AX7) valves in the pre-amplification stage.. Dudley was responsible for the changes from the Fender to what is now known as the JTM45.\n\nSignificant differences between the Bassman and the JTM include the all-aluminum chassis, a 12AX7 valve as the first in the chain (the Bassman has a 12AY7), the Celestion speakers with a closed cabinet (compared to open-backed Jensen speakers), and a modified negative feedback circuit which affects the harmonics produced by the amplifier. As Ken Bran later said, \"The JTM also had different harmonic content, and this was due to the large amount of feedback that Dudley Craven had given it.\" The amp was also available as a bass (which lacked a \"bright\" capacitor) and a PA version (which lacked a \"mixer\" capacitor).\n\nBy the mid 1960s, the JTM45 had become so popular that it began to supplant the ubiquitous Vox amps, even their AC50, though it was just as powerful.\n\nIn late 1965, Marshall introduced its now standard script lettering, in white, and by early 1966 it began calling the amplifiers \"JTM 50\". Some 100 early models had red lettering; these are especially collectible. Other cosmetic changes included a gradual change to different knobs. The JTM 45 became the basis for many subsequent Marshalls, most notably the Marshall 1962 combo (later referred to as the \"Bluesbreaker\" due to its use by Eric Clapton with John Mayall's Bluesbreakers). It ceased being produced in 1966, but was reissued in 1989, though with a modern printed circuit board and 6L6 output valves. In 2014 Marshall reissued a \"handwired\" 30 W amplifier based on the JTM45, the 2245THW, whose circuitry is identical to the 1962 combo circuit; it is a \"fine high-end piece\" according to \"Vintage Guitar\", listed at $4,800.\n\nThe first JTM45s did not have the standard Marshall number that later amps had; models that derived from the JTM 45 did not receive numbers until 1965.\n\nFor all of its differences when compared with the Bassman, the sound of the JTM45 is still described as \"like a tweed Fender\"; it has more sag and less crunch than the later Marshalls, and is favoured for blues and rock rather than for hard rock and metal. The JTM 45 delivers a smooth Marshall \"crunch\" with a warm bass response due to the EL34/KT66 valves.\n\n\nhttp://www.dudleycraven.com/ 2018.\n\n http://www.dudleycraven.com/ \n"}
{"id": "39023855", "url": "https://en.wikipedia.org/wiki?curid=39023855", "title": "Metal powder", "text": "Metal powder\n\nMetal powder is a powdered metal such as aluminium powder and iron powder.\n\n"}
{"id": "6756884", "url": "https://en.wikipedia.org/wiki?curid=6756884", "title": "Miller/Knox Regional Shoreline", "text": "Miller/Knox Regional Shoreline\n\nMiller/Knox Regional Shoreline is a bayside park in the Brickyard Cove neighborhood of the Point Richmond District in Richmond, California.\n\nThe park is centered on the Miller/Knox lagoon which is depicted on a large 200 foot by 50 foot mural at the Richmond Municipal Natatorium nearby. The park affords panoramic views of the Bay Area especially the Oakland and San Francisco skylines, islands, bridges, and the North Bay mountains. The views are the farthest from the park's high point: Nicholls Knob. The regional shoreline includes Keller Beach on San Pablo Bay in addition to large picnic and barbecue areas, parking and a fishing pier. There is also a former train ferry pier and other assorted ruins. The park is also home to the Golden State Model Railroad Museum.\n\nThe park is named for former state senator George Miller, Jr. and former State Assembly member and Point Richmond resident John T. Knox.\n\nThe park features many trails for cyclists, dog-walkers, and hikers, and a salt water lagoon where ducks, seagulls, and Canada Geese frolic.\n\nThe beach was closed due to an Cosco-Busan oil spill in 2007, but reopened months later.\n\n\n"}
{"id": "1051404", "url": "https://en.wikipedia.org/wiki?curid=1051404", "title": "Monounsaturated fat", "text": "Monounsaturated fat\n\nIn biochemistry and nutrition, monounsaturated fatty acids (abbreviated MUFAs, or more plainly monounsaturated fats) are fatty acids that have one double bond in the fatty acid chain with all of the remainder carbon atoms being single-bonded. By contrast, polyunsaturated fatty acids (PUFAs) have more than one double bond.\n\nFatty acids are long-chained molecules having an alkyl group at one end and a carboxylic acid group at the other end. Fatty acid viscosity (thickness) and melting temperature increases with decreasing number of double bonds; therefore, monounsaturated fatty acids have a higher melting point than polyunsaturated fatty acids (more double bonds) and a lower melting point than saturated fatty acids (no double bonds). Monounsaturated fatty acids are liquids at room temperature and semisolid or solid when refrigerated resulting in a isotopic lattice structure.\n\nCommon monounsaturated fatty acids are palmitoleic acid (16:1 n−7), cis-vaccenic acid (18:1 n−7) and oleic acid (18:1 n−9). Palmitoleic acid has 16 carbon atoms with the first double bond occurring 7 carbon atoms away from the methyl group (and 9 carbons from the carboxyl end). It can be lengthened to the 18-carbon cis-vaccenic acid. Oleic acid has 18 carbon atoms with the first double bond occurring 9 carbon atoms away from the carboxylic acid group. The illustrations below show a molecule of oleic acid in Lewis formula and as a space-filling model.\n\nMonounsaturated fats protect against cardiovascular disease by providing more membrane fluidity than saturated fats, but they are more vulnerable to lipid peroxidation (rancidity). The large scale KANWU study found that increasing monounsaturated fat and decreasing saturated fat intake could improve insulin sensitivity, but only when the overall fat intake of the diet was low. However, some monounsaturated fatty acids (in the same way as saturated fats) may promote insulin resistance, whereas polyunsaturated fatty acids may be protective against insulin resistance.\nStudies have shown that substituting dietary monounsaturated fat for saturated fat is associated with increased daily physical activity and resting energy expenditure. More physical activity was associated with a higher-oleic acid diet than one of a palmitic acid diet. From the study, it is shown that more monounsaturated fats lead to less anger and irritability.\n\nFoods containing monounsaturated fats reduce low-density lipoprotein (LDL) cholesterol, while possibly increasing high-density lipoprotein (HDL) cholesterol.\n\nLevels of oleic acid along with other monounsaturated fatty acids in red blood cell membranes were positively associated with breast cancer risk. The saturation index (SI) of the same membranes was inversely associated with breast cancer risk. Monounsaturated fats and low SI in erythrocyte membranes are predictors of postmenopausal breast cancer. Both of these variables depend on the activity of the enzyme delta-9 desaturase (Δ9-d).\n\nIn children, consumption of monounsaturated oils is associated with healthier serum lipid profiles.\n\nThe Mediterranean diet is one heavily influenced by monounsaturated fats. People in Mediterranean countries consume more total fat than Northern European countries, but most of the fat is in the form of monounsaturated fatty acids from olive oil and omega-3 fatty acids from fish, vegetables, and certain meats like lamb, while consumption of saturated fat is minimal in comparison.\nA 2017 review found evidence that the practice of a Mediterranean diet could lead to a decreased risk of cardiovascular diseases, overall cancer incidence, neurodegenerative diseases, diabetes, and early death. A 2018 review showed that the practice of the Mediterranean diet may improve overall health status, such as reduced risk of non-communicable diseases, reduced total costs of living, and reduced costs for national healthcare.\n\nMonounsaturated fats are found in animal flesh such as red meat, whole milk products, nuts, and high fat fruits such as olives and avocados. Olive oil is about 75% monounsaturated fat. The high oleic variety sunflower oil contains as much as 85% monounsaturated fat. Canola oil and cashews are both about 58% monounsaturated fat. Tallow (beef fat) is about 50% monounsaturated fat. and lard is about 40% monounsaturated fat. Other sources include avocado oil, macadamia nut oil, grapeseed oil, groundnut oil (peanut oil), sesame oil, corn oil, popcorn, whole grain wheat, cereal, oatmeal, almond oil, sunflower oil, hemp oil, and tea-oil Camellia.\n\n\n"}
{"id": "47633437", "url": "https://en.wikipedia.org/wiki?curid=47633437", "title": "Netherland Indies Gas Company", "text": "Netherland Indies Gas Company\n\nThe Netherland Indies Gas Company (Nederlandsch Indische Gasmaatschappij (NIGM) was a business in the Dutch East Indies, Suriname and the Netherlands Antilles. It was founded in 1863 in the Dutch East Indies and expanded to Suriname (1908) and Curaçao (1927).\n\nAs of the World War II era, the company operated 11 gas plants and 33 power plants. The company became Overseas Gas and Electric Company, Dutch: Overzeese Gas- en Electriciteitsmaatschappij (OGEM) as of 1950. Indonesia nationalized the business in 1954. Operations in Suriname were nationalized in the 1970s. Curaçao operations were nationalized in 1977.\n\nThe company diversified into trading and construction with acquisitions. In 1959 the electrical installation company Croon & Co Klaas Fibbe was purchased. In 1963, Wolter & Dros. Klaas Fibbe was acquired. In 1969, Dutch technical wholesaler Technische Unie was taken over. Construction company Eesteren was bought in 1972 and Voormolen in 1975.\n\nIn September 1973 Fibbe Berend Jan Udink made became a board member and helped expand international activities of OGEM. From 1977 to 1979 construction project in Dammam, Saudi Arabia was undertaken. The project includes eight buildings. In July 1977 OGEM bought an equity stake of 44% in Beton und Monierbau (B & M) in Düsseldorf.\n\nMany acquisitions were funded with debt and in the early 1980s OGEM went bankrupt after a loss of 120 million guilders in 1980. Parts of the company were reorganized under the name Engineering Building and Industry (TBI). Bankruptcy proceedings were completed in 1994.\n"}
{"id": "7786484", "url": "https://en.wikipedia.org/wiki?curid=7786484", "title": "Non-contact force", "text": "Non-contact force\n\nA Non Contact Force is a force which acts on an object without coming physically in contact with it. The most familiar example of a non-contact force is gravity, which confers weight. In contrast a contact force is a force applied to a body by another body that \"is\" in contact with it.\n\nAll four known fundamental interactions are non-contact forces:\n\n\n"}
{"id": "16334130", "url": "https://en.wikipedia.org/wiki?curid=16334130", "title": "On-die termination", "text": "On-die termination\n\nOn-die termination (ODT) is the technology where the termination resistor for impedance matching in transmission lines is located inside a semiconductor chip instead of on a printed circuit board (PCB).\n\nIn lower frequency (slow edge rate) applications, interconnection lines can be modelled as \"lumped\" circuits. In this case there is no need to consider the concept of \"termination\". Under the low frequency condition, every point in an interconnect wire can be assumed to have the same voltage as every other point for any instance in time.\n\nHowever, if the propagation delay in a wire, PCB trace, cable, or connector is significant (for example, if the delay is greater than 1/6 of the rise time of the digital signal), the \"lumped\" circuit model is no longer valid and the interconnect has to be analyzed as a transmission line. In a transmission line, the signal interconnect path is modeled as a circuit containing distributed inductance, capacitance and resistance throughout its length.\n\nIn order for a transmission line to minimize distortion of the signal, the impedance of every location on the transmission line should be uniform throughout its length. If there is any place in the line where the impedance is not uniform for some reason (open circuit, impedance discontinuity, different material) the signal gets modified by reflection at the impedance change point which results in distortion, ringing and so forth.\n\nWhen the signal path has impedance discontinuity, in other words an impedance mismatch, then a termination impedance with the equivalent amount of impedance is placed at the point of line discontinuity. This is described as \"termination\". For example, resistors can be placed on computer motherboards to terminate high speed busses. There are several ways of termination depending on how the resistors are connected to the transmission line. Parallel termination and series termination are examples of termination methodologies.\n\nInstead of having the necessary resistive termination located on the motherboard, the termination is located inside the semiconductor chips–technique called On-Die Termination (abbreviated to ODT).\n\nAlthough the termination resistors on the motherboard reduce some reflections on the signal lines, they are unable to prevent reflections resulting from the stub lines that connect to the components on the module card (e.g. DRAM Module). A signal propagating from the controller to the components encounters an impedance discontinuity at the stub leading to the components on the module. The signal that propagates along the stub to the component (e.g. DRAM component) will be reflected back onto the signal line, thereby introducing unwanted noise into the signal. In addition, on-die termination can reduce the number of resistor elements and complex wiring on the motherboard. Accordingly, the system design can be simpler and cost effective.\n\nOn-die termination is implemented with several combinations of resistors on the DRAM silicon along with other circuit trees. DRAM circuit designers can use a combination of transistors which have different values of turn-on resistance. In the case of DDR2, there are three kinds of internal resistors 150ohm, 75ohm and 50ohm. The resistors can be combined to create a proper equivalent impedance value to the outside of the chip, whereby the signal line (transmission line) of the motherboard is being controlled by the on-die termination operation signal. Where an on-die termination value control circuit exists the DRAM controller manages the on-die termination resistance through a programmable configuration register which resides in the DRAM. The internal on-die termination values in DDR3 are 120ohm, 60ohm, 40ohm and so forth.\n\n"}
{"id": "44044088", "url": "https://en.wikipedia.org/wiki?curid=44044088", "title": "Paradox of radiation of charged particles in a gravitational field", "text": "Paradox of radiation of charged particles in a gravitational field\n\nThe paradox of a charge in a gravitational field is an apparent physical paradox in the context of general relativity. A charged particle at rest in a gravitational field, such as on the surface of the Earth, must be supported by a force to prevent it from falling. According to the equivalence principle, it should be indistinguishable from a particle in flat space being accelerated by a force. Maxwell's equations say that an accelerated charge should radiate electromagnetic waves, yet such radiation is not observed for stationary particles in gravitational fields.\n\nOne of the first to study this problem was Max Born in his 1909 paper about the consequences of a charge in uniformly accelerated frame. Earlier concerns and possible solutions were raised by Wolfgang Pauli (1918), Max von Laue (1919), and others, but the most recognized work on the subject is the resolution of Thomas Fulton and Fritz Rohrlich in 1960.\n\nIt is a standard result from Maxwell's equations of classical electrodynamics that an accelerated charge radiates. That is, it produces an electric field that falls off as formula_1 in addition to its rest-frame formula_2 Coulomb field. This radiation electric field has an accompanying magnetic field, and the whole oscillating electromagnetic radiation field propagates independently of the accelerated charge, carrying away momentum and energy. The energy in the radiation is provided by the work that accelerates the charge.\n\nThe theory of general relativity is built on the equivalence principle of gravitation and inertia. This principle states that it is impossible to distinguish through any local measurement whether one is in a gravitational field or being accelerated. An elevator out in deep space, far from any planet, could mimic a gravitational field to its occupants if it could be accelerated continuously \"upward\". Whether the acceleration is from motion or from gravity makes no difference in the laws of physics. One can also understand it in terms of the equivalence of so-called gravitational mass and inertial mass. The mass in Newton's law of universal gravitation (gravitational mass) is the same as the mass in Newton's second law of motion (inertial mass). They cancel out when equated, with the result discovered by Galileo Galilei in 1638, that all bodies fall at the same rate in a gravitational field, independent of their mass. A famous demonstration of this principle was performed on the Moon during the Apollo 15 mission, when a hammer and a feather were dropped at the same time and struck the surface at the same time.\n\nClosely tied in with this equivalence is the fact that gravity vanishes in free fall. For objects falling in an elevator whose cable is cut, all gravitational forces vanish, and things begin to look like the free-floating absence of forces one sees in videos from the International Space Station. It is a linchpin of general relativity that everything must fall together in free fall. Just as with acceleration versus gravity, no experiment should be able to distinguish the effects of free fall in a gravitational field, and being out in deep space far from any forces.\n\nPutting together these two basic facts of general relativity and electrodynamics, we seem to encounter a paradox. For if we dropped a neutral particle and a charged particle together in a gravitational field, the charged particle should begin to radiate as it is accelerated under gravity, thereby losing energy and slowing relative to the neutral particle. Then a free-falling observer could distinguish free fall from the true absence of forces, because a charged particle in a free-falling laboratory would begin to be pulled upward relative to the neutral parts of the laboratory, even though no obvious electric fields were present.\n\nEquivalently, we can think about a charged particle at rest in a laboratory on the surface of the Earth. In order to be at rest, it must be supported by something which exerts an upward force on it to balance the Earth's downward gravitational field of 1 \"g\". This system is equivalent to being in outer space accelerated constantly upward at 1 \"g\", and we know that a charged particle accelerated upward at 1 \"g\" would radiate, why don't we see radiation from charged particles at rest in the laboratory? It would seem that we could distinguish between a gravitational field and acceleration, because an electric charge apparently only radiates when it is being accelerated through motion, but not through gravitation.\n\nThe resolution of this paradox, like the twin paradox and ladder paradox, comes through appropriate care in distinguishing frames of reference. This section follows the analysis of Fritz Rohrlich (1965), who shows that a charged particle and a neutral particle fall equally fast in a gravitational field. Likewise, a charged particle at rest in a gravitational field does not radiate in its rest frame, but it does so in the frame of a free falling observer. The equivalence principle is preserved for charged particles.\n\nThe key is to realize that the laws of electrodynamics, Maxwell's equations, hold only within an inertial frame, that is, in a frame in which all forces act locally, and there is no net acceleration when the net local forces are zero. The frame could be free fall under gravity, or far in space away from any forces. The surface of the Earth is \"not\" an inertial frame, as it is being constantly accelerated. We know that the surface of the Earth is not an inertial frame because an object at rest there may not remain at rest—objects at rest fall to the ground when released. Gravity is a non-local fictitious “force” within the Earth’s surface frame, just like centrifugal “force”. So we cannot naively formulate expectations based on Maxwell's equations in this frame. It is remarkable that we now understand the special-relativistic Maxwell equations do not hold, strictly speaking, on the surface of the Earth, even though they were discovered in electrical and magnetic experiments conducted in laboratories on the surface of the Earth. Nevertheless, in this case, we cannot apply Maxwell's equations to the description of a falling charge relative to a \"supported\", non-inertial observer.\n\nMaxwell's equations can be applied relative to an observer in free fall, because free-fall is an inertial frame. So the starting point of considerations is to work in the free-fall frame in a gravitational field—a \"falling\" observer. In the free-fall frame, Maxwell's equations have their usual, flat-spacetime form for the falling observer. In this frame, the electric and magnetic fields of the charge are simple: the falling electric field is just the Coulomb field of a charge at rest, and the magnetic field is zero. As an aside, note that we are building in the equivalence principle from the start, including the assumption that a charged particle falls equally as fast as a neutral particle.\n\nThe fields measured by an observer supported on the surface of the Earth are different. Given the electric and magnetic fields in the falling frame, we have to transform those fields into the frame of the supported observer. This manipulation is \"not\" a Lorentz transformation, because the two frames have a relative acceleration. Instead, the machinery of general relativity must be used.\n\nIn this case the gravitational field is fictitious because it can be \"transformed away\" by appropriate choice of coordinate system in the falling frame. Unlike the total gravitational field of the Earth, here we are assuming that spacetime is locally flat, so that the curvature tensor vanishes. Equivalently, the lines of gravitational acceleration are everywhere parallel, with no convergences measurable in the laboratory. Then the most general static, flat-space, cylindrical metric and line element can be written:\nwhere formula_4 is the speed of light, formula_5 is proper time, formula_6 are the usual coordinates of space and time, formula_7 is the acceleration of the gravitational field, and formula_8 is an arbitrary function of the coordinate but must approach the observed Newtonian value of formula_9. This formula is the metric for the gravitational field measured by the supported observer.\n\nMeanwhile, the metric in the frame of the falling observer is simply the Minkowski metric:\n\nFrom these two metrics Rohrlich constructs the coordinate transformation between them:\n\nWhen this coordinate transformation is applied to the electric and magnetic fields of the charge in the rest frame, it is found not to be radiating, as expected for a charge in an inertial frame. Rohrlich emphasizes that this charge remains at rest in its free-fall frame, just as a neutral particle would. Furthermore, the radiation rate for this situation is Lorentz-invariant, but it is not invariant under the coordinate transformation above because it is not a Lorentz transformation.\n\nSo a falling charge will not radiate, as expected. What about a supported charge, then? Does it not radiate due to the equivalence principle? To answer this question, start again in the falling frame.\n\nAs observed from the freefalling frame, the supported charge appears to be accelerated uniformly upward. The case of constant acceleration of a charge is treated by Rohrlich. He finds a charge formula_12 uniformly accelerated at rate formula_7 has a radiation rate given by the Lorentz invariant:\n\nThe corresponding electric and magnetic fields of an accelerated charge are also given in Rohrlich. To find the fields of the charge in the supporting frame, the fields of the uniformly accelerated charge are transformed according to the coordinate transformation previously given. When that is done, one finds \"no radiation\" in the supporting frame from a supported charge, because the magnetic field is zero in this frame. Rohrlich does note that the gravitational field slightly distorts the Coulomb field of the supported charge, but too small to be observable. So although the Coulomb law was discovered in a supporting frame, general relativity tells us that the field of such a charge is not precisely formula_2.\n\nThe radiation from the supported charge viewed in the freefalling frame (or vice versa) is something of a curiosity: where does it go? Boulware (1980) finds that the radiation goes into a region of spacetime inaccessible to the co-accelerating, supported observer. In effect, a uniformly accelerated observer has an event horizon, and there are regions of spacetime inaccessible to this observer. C. De Almeida and A. Saa (2006) have a more accessible treatment of the event horizon of the accelerated observer.\n\nRohrlich solution contradicts the bremsstrahlung effect. In any case, this solution could be considered a classic approach to a problem that cannot be handled in classical electrodynamics.\n\n"}
{"id": "1189553", "url": "https://en.wikipedia.org/wiki?curid=1189553", "title": "Path (topology)", "text": "Path (topology)\n\nIn mathematics, a path in a topological space \"X\" is a continuous function \"f\" from the unit interval \"I\" = [0,1] to \"X\"\nThe \"initial point\" of the path is \"f\"(0) and the \"terminal point\" is \"f\"(1). One often speaks of a \"path from \"x\" to \"y\"\" where \"x\" and \"y\" are the initial and terminal points of the path. Note that a path is not just a subset of \"X\" which \"looks like\" a curve, it also includes a parameterization. For example, the maps \"f\"(\"x\") = \"x\" and \"g\"(\"x\") = \"x\" represent two different paths from 0 to 1 on the real line.\n\nA loop in a space \"X\" based at \"x\" ∈ \"X\" is a path from \"x\" to \"x\". A loop may be equally well regarded as a map \"f\" : \"I\" → \"X\" with \"f\"(0) = \"f\"(1) or as a continuous map from the unit circle \"S\" to \"X\"\nThis is because \"S\" may be regarded as a quotient of \"I\" under the identification 0 ∼ 1. The set of all loops in \"X\" forms a space called the loop space of \"X\".\n\nA topological space for which there exists a path connecting any two points is said to be path-connected. Any space may be broken up into a set of path-connected components. The set of path-connected components of a space \"X\" is often denoted π(\"X\");.\n\nOne can also define paths and loops in pointed spaces, which are important in homotopy theory. If \"X\" is a topological space with basepoint \"x\", then a path in \"X\" is one whose initial point is \"x\". Likewise, a loop in \"X\" is one that is based at \"x\".\n\nPaths and loops are central subjects of study in the branch of algebraic topology called homotopy theory. A homotopy of paths makes precise the notion of continuously deforming a path while keeping its endpoints fixed.\n\nSpecifically, a homotopy of paths, or path-homotopy, in \"X\" is a family of paths \"f\" : \"I\" → \"X\" indexed by \"I\" such that\nThe paths \"f\" and \"f\" connected by a homotopy are said to be homotopic (or more precisely path-homotopic, to distinguish between the relation defined on all continuous functions between fixed spaces). One can likewise define a homotopy of loops keeping the base point fixed.\n\nThe relation of being homotopic is an equivalence relation on paths in a topological space. The equivalence class of a path \"f\" under this relation is called the homotopy class of \"f\", often denoted [\"f\"].\n\nOne can compose paths in a topological space in an obvious manner. Suppose \"f\" is a path from \"x\" to \"y\" and \"g\" is a path from \"y\" to \"z\". The path \"fg\" is defined as the path obtained by first traversing \"f\" and then traversing \"g\":\nClearly path composition is only defined when the terminal point of \"f\" coincides with the initial point of \"g\". If one considers all loops based at a point \"x\", then path composition is a binary operation.\n\nPath composition, whenever defined, is not associative due to the difference in parametrization. However it \"is\" associative up to path-homotopy. That is, [(\"fg\")\"h\"] = [\"f\"(\"gh\")]. Path composition defines a group structure on the set of homotopy classes of loops based at a point \"x\" in \"X\". The resultant group is called the fundamental group of \"X\" based at \"x\", usually denoted π(\"X\",\"x\").\n\nIn situations calling for associativity of path composition \"on the nose,\" a path in \"X\" may instead be defined as a continuous map from an interval [0,\"a\"] to X for any real \"a\" ≥ 0. A path \"f\" of this kind has a length |\"f\"| defined as \"a\". Path composition is then defined as before with the following modification:\nWhereas with the previous definition, \"f\", \"g\", and \"fg\" all have length 1 (the length of the domain of the map), this definition makes |\"fg\"| = |\"f\"| + |\"g\"|. What made associativity fail for the previous definition is that although (\"fg\")\"h\" and \"f\"(\"gh\") have the same length, namely 1, the midpoint of (\"fg\")\"h\" occurred between \"g\" and \"h\", whereas the midpoint of \"f\"(\"gh\") occurred between \"f\" and \"g\". With this modified definition (\"fg\")\"h\" and \"f\"(\"gh\") have the same length, namely |\"f\"|+|\"g\"|+|\"h\"|, and the same midpoint, found at (|\"f\"|+|\"g\"|+|\"h\"|)/2 in both (\"fg\")\"h\" and \"f\"(\"gh\"); more generally they have the same parametrization throughout.\n\nThere is a categorical picture of paths which is sometimes useful. Any topological space \"X\" gives rise to a category where the objects are the points of \"X\" and the morphisms are the homotopy classes of paths. Since any morphism in this category is an isomorphism this category is a groupoid, called the fundamental groupoid of \"X\". Loops in this category are the endomorphisms (all of which are actually automorphisms). The automorphism group of a point \"x\" in \"X\" is just the fundamental group based at \"x\". More generally, one can define the fundamental groupoid on any subset \"A\" of \"X\", using homotopy classes of paths joining points of \"A\". This is convenient for the Van Kampen's Theorem.\n\n\n"}
{"id": "13283224", "url": "https://en.wikipedia.org/wiki?curid=13283224", "title": "People Powered Vehicle", "text": "People Powered Vehicle\n\nThe People Powered Vehicle, or PPV, was a two-person pedal-powered car introduced in the United States during the oil crisis of the early 1970s. Manufactured by EVI of Sterling Heights, Michigan, it sold for less than $400. Although it offered luggage space and was marketed as a fun and practical vehicle, it offered limited weather protection and was not fast enough to substitute for a car.\n\nThe PPV may be considered a forerunner of the modern velomobile. This tricycle was manufactured with a three-speed, floor shift, open type transmission with a single-wheel drive. Either the driver or the passenger could pedal independently or as a team. Reverse was accomplished by reaching outside and turning one of the rear wheels by hand. At one time, a rear-hinged, surrey top was available. Most were manufactured with a dark blue bottom and a white hood. Red or yellow bottoms with white tops were also offered. Sometimes bicycle accessories were added, e.g. squeeze bulb horn and a rear view mirror.\n\nThe PPV was designed for two adult riders, and with frame and body the total weight could approach three times that of a conventional single-rider bicycle. However, the PPV was fitted with just one brake, of a type intended to be just one of two brakes on a conventional single-rider bicycle (an Atom drum brake built in to the front wheel). Thus, even at relaxed speeds on level ground, the PPV brakes were dangerously inadequate.\n\nAn upgraded version of this vehicle is currently (2011) being offered by the International Surrey Company Ltd. under the trade name Impello.\n\n"}
{"id": "22773987", "url": "https://en.wikipedia.org/wiki?curid=22773987", "title": "Pink algae", "text": "Pink algae\n\nPink algae is a growth of pink, slimey bacterial matter which can sometimes occur in pools and sometimes in laboratory tubings. The title is a misnomer, because, as noted before, pink algae is actually caused by a bacterium, in the genus methylobacterium. The color of the bacterial growth comes from pigments within its cells. The slime formed around the bacteria provides it with a relatively high level of protection from external threats. Like other species in its genus, pink algae is a methane consuming bacterium. It has an affinity for the matrix of PVC plastics, and will attach itself to both the inside and the outside of PVC materials inside of the pool. Pink algae infestation in a pool often occurs alongside an infestation of white water mold.\n\nPrevention of pink algae is the easiest way to deal with the problem. To ensure that pink algae does not grow in a pool, the owner must manually brush and clean all pool surfaces weekly, and regularly expose all pools surfaces to sunlight (pink algae thrives in a dark environment, particularly in areas with slow moving water). Regular doses of certain chemicals are also recommended in a preventative plan against pink algae.\n\nAll visible pink algae must be manually removed from a pool. A dose of algicide, followed by a shock treatment is also necessary to destroy the bacteria. The pool filter must be run constantly until the water is clear and halogen or peroxide levels are normal, then the filter must be chemically cleaned.\n\n"}
{"id": "158741", "url": "https://en.wikipedia.org/wiki?curid=158741", "title": "Power-to-weight ratio", "text": "Power-to-weight ratio\n\nPower-to-weight ratio (or specific power or power-to-mass ratio) is a calculation commonly applied to engines and mobile power sources to enable the comparison of one unit or design to another. Power-to-weight ratio is a measurement of actual performance of any engine or power source. It is also used as a measurement of performance of a vehicle as a whole, with the engine's power output being divided by the weight (or mass) of the vehicle, to give a metric that is independent of the vehicle's size. Power-to-weight is often quoted by manufacturers at the peak value, but the actual value may vary in use and variations will affect performance.\n\nThe inverse of power-to-weight, weight-to-power ratio (power loading) is a calculation commonly applied to aircraft, cars, and vehicles in general, to enable the comparison of one vehicle's performance to another. Power-to-weight ratio is equal to thrust per unit mass multiplied by the velocity of any vehicle.\n\nThe power-to-weight ratio (Specific Power) formula for an engine (power plant) is the power generated by the engine divided by the mass. (\"Weight\" in this context is a colloquial term for \"mass\". To see this, note that what an engineer means by the \"power to weight ratio\" of an electric motor is not infinite in a zero gravity environment.)\n\nA typical turbocharged V8 diesel engine might have an engine power of and a mass of , giving it a power-to-weight ratio of 0.65 kW/kg (0.40 hp/lb).\n\nExamples of high power-to-weight ratios can often be found in turbines. This is because of their ability to operate at very high speeds. For example, the Space Shuttle's main engines used turbopumps (machines consisting of a pump driven by a turbine engine) to feed the propellants (liquid oxygen and liquid hydrogen) into the engine's combustion chamber. The original liquid hydrogen turbopump is similar in size to an automobile engine (weighing approximately ) and produces 72,000 hp (53.6 MW) for a power-to-weight ratio of 153 kW/kg (93 hp/lb).\n\nIn classical mechanics, instantaneous power is the limiting value of the average work done per unit time as the time interval Δ\"t\" approaches zero.\n\nThe typically used metrical unit of the power-to-weight ratio is formula_2 which equals formula_3. This fact allows one to express the power-to-weight ratio purely by SI base units.\n\nIf the work to be done is rectilinear motion of a body with constant mass formula_4, whose center of mass is to be accelerated along a straight line to a speed formula_5 and angle formula_6 with respect to the centre and radial of a gravitational field by an onboard powerplant, then the associated kinetic energy to be delivered to the body is equal to\n\nwhere:\n\nThe instantaneous mechanical pushing/pulling power delivered to the body from the powerplant is then\n\nwhere:\n\nIn propulsion, power is only delivered if the powerplant is in motion, and is transmitted to cause the body to be in motion. It is typically assumed here that mechanical transmission allows the powerplant to operate at peak output power. This assumption allows engine tuning to trade power band width and engine mass for transmission complexity and mass. Electric motors do not suffer from this tradeoff, instead trading their high torque for traction at low speed. The power advantage or power-to-weight ratio is then\n\nwhere:\n\nThe actual useful power of any traction engine can be calculated using a dynamometer to measure torque and rotational speed, with peak power sustained when the transmission and/or operator keeps the product of torque and rotational speed maximised. For jet engines there is e be usefully calculated there, for rockets there is typically no cruise speed, so it is less meaningful.\n\nPeak power of a traction engine occurs at a rotational speed higher than the speed when torque is maximised and at or below the maximum rated rotational speed – Max RPM. A rapidly falling torque curve would correspond with sharp torque and power curve peaks around their maxima at similar rotational speed, for example a small, lightweight engine with a large turbocharger. A slowly falling or near flat torque curve would correspond with a slowly rising power curve up to a maximum at a rotational speed close to Max RPM, for example a large, heavy multi-cylinder engine suitable for cargo/hauling. A falling torque curve could correspond with a near flat power curve across rotational speeds for smooth handling at different vehicle speeds, such as a traction electric motor.\n\nThermal energy is made up from molecular kinetic energy and latent phase energy. Heat engines are able to convert thermal energy in the form of a temperature gradient between a hot source and a cold sink into other desirable mechanical work. Heat pumps take mechanical work to regenerate thermal energy in a temperature gradient. Care should be made when interpreting propulsive power, especially for jet engines and rockets, deliverable from heat engines to a vehicle.\n\nAn electric motor uses electrical energy to provide mechanical work, usually through the interaction of a magnetic field and current-carrying conductors. By the interaction of mechanical work on an electrical conductor in a magnetic field, electrical energy can be generated.\n\nFluids (liquid and gas) can be used to transmit and/or store energy using pressure and other fluid properties. Hydraulic (liquid) and pneumatic (gas) engines convert fluid pressure into other desirable mechanical or electrical work. Fluid pumps convert mechanical or electrical work into movement or pressure changes of a fluid, or storage in a pressure vessel.\nA variety of effects can be harnessed to produce thermoelectricity, thermionic emission, pyroelectricity and piezoelectricity. Electrical resistance and ferromagnetism of materials can be harnessed to generate thermoacoustic energy from an electric current.\n\nAll electrochemical cell batteries deliver a changing voltage as their chemistry changes from \"charged\" to \"discharged\". A nominal output voltage and a cutoff voltage are typically specified for a battery by its manufacturer. The output voltage falls to the cutoff voltage when the battery becomes \"discharged\". The nominal output voltage is always less than the open-circuit voltage produced when the battery is \"charged\". The temperature of a battery can affect the power it can deliver, where lower temperatures reduce power. Total energy delivered from a single charge cycle is affected by both the battery temperature and the power it delivers. If the temperature lowers or the power demand increases, the total energy delivered at the point of \"discharge\" is also reduced.\n\nBattery discharge profiles are often described in terms of a factor of battery capacity. For example, a battery with a nominal capacity quoted in ampere-hours (Ah) at a C/10 rated discharge current (derived in amperes) may safely provide a higher discharge current – and therefore higher power-to-weight ratio – but only with a lower energy capacity. Power-to-weight ratio for batteries is therefore less meaningful without reference to corresponding energy-to-weight ratio and cell temperature. This relationship is known as Peukert's law.\n\nCapacitors store electric charge onto two electrodes separated by an electric field semi-insulating (dielectric) medium. Electrostatic capacitors feature planar electrodes onto which electric charge accumulates. Electrolytic capacitors use a liquid electrolyte as one of the electrodes and the electric double layer effect upon the surface of the dielectric-electrolyte boundary to increase the amount of charge stored per unit volume. Electric double-layer capacitors extend both electrodes with a nanopourous material such as activated carbon to significantly increase the surface area upon which electric charge can accumulate, reducing the dielectric medium to nanopores and a very thin high permittivity separator.\n\nWhile capacitors tend not to be as temperature sensitive as batteries, they are significantly capacity constrained and without the strength of chemical bonds suffer from self-discharge. Power-to-weight ratio of capacitors is usually higher than batteries because charge transport units within the cell are smaller (electrons rather than ions), however energy-to-weight ratio is conversely usually lower.\nFuel cells and flow cells, although perhaps using similar chemistry to batteries, have the distinction of not containing the energy storage medium or fuel. With a continuous flow of fuel and oxidant, available fuel cells and flow cells continue to convert the energy storage medium into electric energy and waste products. Fuel cells distinctly contain a fixed electrolyte whereas flow cells also require a continuous flow of electrolyte. Flow cells typically have the fuel dissolved in the electrolyte.\n\nPower-to-weight ratios for vehicles are usually calculated using curb weight (for cars) or wet weight (for motorcycles), that is, excluding weight of the driver and any cargo. This could be slightly misleading, especially with regard to motorcycles, where the driver might weigh 1/3 to 1/2 as much as the vehicle itself. In the sport of competitive cycling athlete's performance is increasingly being expressed in VAMs and thus as a power-to-weight ratio in W/kg. This can be measured through the use of a bicycle powermeter or calculated from measuring incline of a road climb and the rider's time to ascend it.\n\nMost vehicles are designed to meet passenger comfort and cargo carrying requirements. Different designs trade off power-to-weight ratio to increase comfort, cargo space, fuel economy, emissions control, energy security and endurance. Reduced drag and lower rolling resistance in a vehicle design can facilitate increased cargo space without increase in the (zero cargo) power-to-weight ratio. This increases the role flexibility of the vehicle. Energy security considerations can trade off power (typically decreased) and weight (typically increased), and therefore power-to-weight ratio, for fuel flexibility or drive-train hybridisation. Some utility and practical vehicle variants such as hot hatches and sports-utility vehicles reconfigure power (typically increased) and weight to provide the perception of sports car like performance or for other psychological benefit.\n\nA locomotive generally must be very heavy in order to develop enough adhesion on the rails to start a train. As the coefficient of friction between steel wheels and rails seldom exceeds 0.25 in most cases, improving a locomotive's power-to-weight ratio is often counterproductive. However, the choice of power transmission system, such as variable-frequency drive versus direct current drive, may support a higher power-to-weight ratio by better managing propulsion power.\n\nIncreased engine performance is a consideration, but also other features associated with luxury vehicles. Longitudinal engines are common. Bodies vary from hot hatches, sedans (saloons), coupés, convertibles and roadsters. Mid-range dual-sport and cruiser motorcycles tend to have similar power-to-weight ratios.\n\nPower-to-weight ratio is an important vehicle characteristic that affects the acceleration of sports vehicles.\n\nAircraft depend on high power-to-weight ratio to generate sufficient thrust to achieve sustained flight, and then to fly fast. \n\nJet aircraft produce thrust directly.\n\nPower to weight ratio is important in cycling, since it determines acceleration and the speed during hill climbs. Since a cyclist's power to weight output decreases with fatigue, it is normally discussed with relation to the length of time that he or she maintains that power. A professional cyclist can produce over 20 W/kg as a 5-second maximum. A 225-pound touring cyclist coasting down an exhilarating 10-degree mountain slope at 35 miles per hour rates 62 pounds per horsepower. The calculation is demonstrative and does not imply terminal speed. The power figure is 3.6 horsepower. In ISO units, this is 27 W/kg. In 60 seconds of such coasting, therefore, 39 kcal are lost, or 45 watt-hours. Climbing is the major impediment to progress on road when cycling.\n\n\n"}
{"id": "21491700", "url": "https://en.wikipedia.org/wiki?curid=21491700", "title": "Public Law 110-343", "text": "Public Law 110-343\n\nPublic Law 110-343 () is a US Act of Congress signed into law by U.S. President George W. Bush, which was designed to mitigate the growing financial crisis of the late-2000s by giving relief to so-called \"Troubled Assets.\"\n\nIts formal title is \"An Act To provide authority for the Federal Government to purchase and insure certain types of troubled assets for the purposes of providing stability to and preventing disruption in the economy and financial system and protecting taxpayers, to amend the Internal Revenue Code of 1986 to provide incentives for energy production and conservation, to extend certain expiring provisions, to provide individual income tax relief, and for other purposes.\"\n\nThe Act created a $700 billion Troubled Asset Relief Program under the Emergency Economic Stabilization Act of 2008 (), and also enacted the \"Energy Improvement and Extension Act of 2008\" (), \"Tax Extenders and Alternative Minimum Tax Relief Act of 2008\" (), which also included the \"Paul Wellstone and Pete Domenici Mental Health Parity and Addiction Equity Act of 2008\", and the \"Heartland Disaster Tax Relief Act of 2008\".\n\nThe first version of the Emergency Economic Stabilization Act of 2008 (structured as an amendment to ) was rejected by the House of Representatives on September 29.\n\nAfter its defeat, Senate leaders decided to amend an existing bill from the House in order to circumvent the revenue origination clause of U.S. Constitution, and chose H.R. 1424 as the vehicle for the legislation. \nH.R. 1424 was sponsored by United States House Representative Patrick J. Kennedy.\n\nOn September 30, 2008, Senate Majority Leader Harry Reid and Minority Leader Mitch McConnell announced the proposed draft had been formalized for the amendment that would transform H.R. 1424 into the Senate version of the Emergency Economic Stabilization Act of 2008.\n\nOn October 1, 2008, the amendment to H.R. 1424 was approved by a vote of 74 to 25, and the entire amended bill was passed by 74 to 25, (with one not voting, the cancer-stricken Senator Ted Kennedy). The bill was returned to the House for consideration.\nOn October 3, 2008, the bill as passed by the Senate was accepted by a vote of 263 to 171 in the House. Every member of the House voted, though the House had a vacant seat of the recently deceased Stephanie Tubbs Jones of Ohio. President George W. Bush signed the bill into law a few hours later.\n\nThe \"Emergency Economic Stabilization Act of 2008\" is part of an effort to bail out firms holding mortgage-backed securities in an attempt to restore liquidity to the credit markets.\n\nThe \"Energy Improvement and Extension Act of 2008\" contains a new tax credit for plug-in hybrid electric vehicles for less than a year after the first 250,000 are sold. The credit is a base $2,500 plus $417 for each kWh of battery pack capacity in excess of 4 kWh to a maximum of $15,000 for any vehicle with a gross vehicle weight rating of more than and up to $7,500 for 12 kWh or more in passenger cars (vehicles up to ).\n\nIt also extends existing tax credits for renewable energy initiatives, including cellulosic ethanol and biodiesel development, and wind, solar, geothermal and hydro-electric power. It establishes electricity as a clean-burning fuel for tax purposes.\n\nSeparately, this section also requires the reporting of cost basis by brokers to the IRS for certain securities acquired after 2011 – see covered security.\n\nThe \"Tax Extenders and Alternative Minimum Tax Relief Act\" includes $100 billion in tax breaks for businesses and the middle class, plus a provision to raise the cap on federal deposit insurance from $100,000 to $250,000.\n\nThe Act keeps the alternative minimum tax from hitting 20 million middle-income Americans. It provides $8 billion in tax relief for those hit by natural disasters in the Midwest, Texas and Louisiana.\n\nAs a whole, the Senate tax package would cost $150.5 billion over 10 years. Roughly $43.5 billion would be offset by several revenue-raising provisions. Hedge fund managers would be forbidden from using offshore corporations to defer paying taxes.\n\nThe bill freezes a tax deduction that oil and gas companies get for certain domestic production activities. The deduction, now 6 percent, is scheduled to rise to 9 percent in 2010.\n\nThe provisions of the tax bill included:\n\nThe \"Paul Wellstone and Pete Domenici Mental Health Parity and Addiction Equity Act of 2008\" (a part of Division C) mandates that if U.S. health insurance companies provide coverage for mental health and substance abuse, the coverage must be equal for conditions such as psychological disorders, alcoholism, and drug addiction.\nThis act continues and expands upon the previous Mental Health Parity Act of 1996. It states that financial requirements such as deductibles and copayments and lifetime or dollar limits to mental health benefits and substance abuse disorder benefits should be no more restrictive than those on medical and surgical benefits. MHPAEA applies to employer-sponsored health insurance plans with more than 50 employees, though parity is also extended to small group and individual plans under the Affordable Care Act.\n\nThe Act increased the statutory limit on the public debt by US$700 billion to US$11.3 trillion. However, the legislation is designed to have a net zero long-term cost, and includes language that mandates the President and Congress to develop a plan to recoup any money that is not recouped within 5 years.\n\nJournalists and critics commented that portions of the bill contained earmarks and pork barrel spending.\n\n"}
{"id": "7693358", "url": "https://en.wikipedia.org/wiki?curid=7693358", "title": "Regulated power supply", "text": "Regulated power supply\n\nA regulated power supply is an embedded circuit; it converts unregulated AC (Alternating Current)into a constant DC. With the help of a rectifier it converts AC supply into DC. Its function is to supply a stable voltage (or less often current), to a circuit or device that must be operated within certain power supply limits. The output from the regulated power supply may be alternating or unidirectional, but is nearly always DC (Direct Current).\n\nThe type of stabilization used may be restricted to ensuring that the output remains within certain limits under various load conditions, or it may also include compensation for variations in its own supply source. The latter is much more common today.\n\n\nMany topologies have been used since the regulated supply was invented. Early technologies included iron-hydrogen resistors, resonant transformers, nonlinear resistors, loading resistors, neon stabiliser tubes, vibrating contact regulators etc.\n\nModern regulated supplies mostly use a transformer, silicon diode bridge recitfier, reservoir capacitor and voltage regulator IC. There are variations on this theme, such as supplies with multiple voltage lines, variable regulators, power control lines, discrete circuits and so on. Switched mode regulator supplies also include an inductor.\n\nAt times regulated supplies can be much more complex. An example supply from a 1980s TV which used bidirectional interaction between the main supply and the line output stage to operate, generating a range of output voltages with varying amounts of stabilisation. Since neither stage could start without the other running, the supply also included a kickstart system to pulse the system into operation. The supply also monitored voltages in the TV power circuitry, shutting down if these voltages went out of spec. For special applications, supplies can become even more complex.\n\n"}
{"id": "264746", "url": "https://en.wikipedia.org/wiki?curid=264746", "title": "Saturated fat", "text": "Saturated fat\n\nA saturated fat is a type of fat in which the fatty acid chains have all or predominantly single bonds. A fat is made of two kinds of smaller molecules: glycerol and fatty acids. Fats are made of long chains of carbon (C) atoms. Some carbon atoms are linked by single bonds (-C-C-) and others are linked by double bonds (-C=C-). Double bonds can react with hydrogen to form single bonds. They are called saturated, because the second bond is broken up and each half of the bond is attached to (saturated with) a hydrogen atom. Most animal fats are saturated. The fats of plants and fish are generally unsaturated. Saturated fats tend to have higher melting points than their corresponding unsaturated fats, leading to the popular understanding that saturated fats tend to be solids at room temperatures, while unsaturated fats tend to be liquid at room temperature with varying degrees of viscosity (meaning both saturated and unsaturated fats are found to be liquid at body temperature).\n\nVarious fats contain different proportions of saturated and unsaturated fat. Examples of foods containing a high proportion of saturated fat include animal fat products such as cream, cheese, butter, other whole milk dairy products and fatty meats which also contain dietary cholesterol. Certain vegetable products have high saturated fat content, such as coconut oil and palm kernel oil. Many prepared foods are high in saturated fat content, such as pizza, dairy desserts, and sausage.\n\nGuidelines released by many medical organizations including the World Health Organization have advocated for reduction in the intake of saturated fat to promote health and reduce the risk from cardiovascular diseases. Many review articles also recommend a diet low in saturated fat and argue it will lower risks of cardiovascular diseases, diabetes, or death. However, a smaller number of other reviews have come to different conclusions.\nWhile nutrition labels regularly combine them, the saturated fatty acids appear in different proportions among food groups. Lauric and myristic acids are most commonly found in \"tropical\" oils (e.g., palm kernel, coconut) and dairy products. The saturated fat in meat, eggs, cacao, and nuts is primarily the triglycerides of palmitic and stearic acids.\n\nSome common examples of fatty acids:\n\nSince the 1950s, it has been demonstrated that consumption of foods containing high amounts of saturated fatty acids (including meat fats, milk fat, butter, lard, coconut oil, palm oil, and palm kernel oil) is potentially less healthy than consuming fats with a lower proportion of saturated fatty acids. Sources of lower saturated fat but higher proportions of unsaturated fatty acids include olive oil, peanut oil, canola oil, avocados, corn, sunflower, soy, and cottonseed oils.\n\nThere are strong, consistent, and graded relationships between saturated fat intake, blood cholesterol levels, and the mass occurrence of cardiovascular disease. The relationships are accepted as causal.\n\nMany health authorities such as the American Dietetic Association,<ref name=\"ADA/DOC\"></ref> the British Dietetic Association, American Heart Association, the World Heart Federation, the British National Health Service, among others, advise that saturated fat is a risk factor for cardiovascular disease. The World Health Organization in May 2015 recommends switching from saturated to unsaturated fats.\n\nA small, limited number of systematic reviews have examined the relationship between saturated fat and cardiovascular disease and have come to different conclusions. However, these rely on observational studies and can therefore not be used to establish cause and effect relationships:\n\nA 2017 systematic review by the American Heart Association of randomized controlled clinical trials showed that reducing intake of dietary saturated fat and replacing it with monounsaturated and polyunsaturated fats could reduce cardiovascular disease by about 30%, similar to the reduction achieved by statin treatment for maintaining blood cholesterol within normal limits.\n\nA different 2017 systematic review of randomized, controlled trials concluded that replacing saturated fats with mostly n-6 polyunsaturated fats is unlikely to reduce coronary heart disease (CHD) events, CHD mortality or total mortality. The 2017 review showed that inadequately controlled trials (e.g., failing to control for other lifestyle factors) that were included in earlier meta-analyses explain the prior results.\n\nA 2015 systematic review also found no association between saturated fat consumption and risk of heart disease, stroke, or diabetes. However, this study only looked at observational studies, and can therefore not be used to determine cause and effect.\n\nA 2014 systematic review looking at observational studies of dietary intake of fatty acids, observational studies of measured fatty acid levels in the blood, and intervention studies of polyunsaturated fat supplementation concluded that the findings ″do not support cardiovascular guidelines that promote high consumption of long-chain omega-3 and omega-6 and polyunsaturated fatty acids and suggest reduced consumption of total saturated fatty acids.″ Researchers acknowledged that despite their results, further research is necessary, especially in people who are initially healthy. Due to potential weaknesses in this review, experts recommend people remain with the current guidelines to reduce saturated fat consumption. The American Heart Association noted that these cohort studies, unlike clinical trials, have more difficulty controlling for the consumption of carbohydrates as a replacement macronutrient for those who consumed less saturated fat.\n\nWhile many studies have found that including polyunsaturated fats in the diet in place of saturated fats produces more beneficial CVD outcomes, the effects of substituting monounsaturated fats or carbohydrates are unclear.\n\nThe consumption of saturated fat is generally considered a risk factor for dyslipidemia, which in turn is a risk factor for some types of cardiovascular disease.\n\nAbnormal blood lipid levels, that is high total cholesterol, high levels of triglycerides, high levels of low-density lipoprotein (LDL, \"bad\" cholesterol) or low levels of high-density lipoprotein (HDL, \"good\" cholesterol) cholesterol are all associated with increased risk of heart disease and stroke.\n\nMeta-analyses have found a significant relationship between saturated fat and serum cholesterol levels. High total cholesterol levels, which may be caused by many factors, are associated with an increased risk of cardiovascular disease. However, other indicators measuring cholesterol such as high total/HDL cholesterol ratio are more predictive than total serum cholesterol. In a study of myocardial infarction in 52 countries, the ApoB/ApoA1 (related to LDL and HDL, respectively) ratio was the strongest predictor of CVD among all risk factors. There are other pathways involving obesity, triglyceride levels, insulin sensitivity, endothelial function, and thrombogenicity, among others, that play a role in CVD, although it seems, in the absence of an adverse blood lipid profile, the other known risk factors have only a weak atherogenic effect. Different saturated fatty acids have differing effects on various lipid levels.\n\nA meta-analysis published in 2003 found a significant positive relationship in both control and cohort studies between saturated fat and breast cancer. However two subsequent reviews have found weak or insignificant associations of saturated fat intake and breast cancer risk, and note the prevalence of confounding factors.\n\nOne review found limited evidence for a positive relationship between consuming animal fat and incidence of colorectal cancer.\n\nA meta-analysis of eight observational studies published in 2001 found a statistically significant positive relationship between saturated fat and ovarian cancer.\nHowever, a 2013 study found that a pooled analysis of 12 cohort studies observed no association between total fat intake and ovarian cancer risk. Further analysis revealed that omega-3 fatty acids were protective against ovarian cancer and that trans fats were a risk factor. This study revealed that histological subtypes should be examined in determining the impact of dietary fat on ovarian cancer, rather than an oversimplified focus on total fat intake.\n\nSome researchers have indicated that serum myristic acid and palmitic acid and dietary myristic and palmitic saturated fatty acids and serum palmitic combined with alpha-tocopherol supplementation are associated with increased risk of prostate cancer in a dose-dependent manner. These associations may, however, reflect differences in intake or metabolism of these fatty acids between the precancer cases and controls, rather than being an actual cause.\n\nMounting evidence indicates that the amount and type of fat in the diet can have important effects on bone health. Most of this evidence is derived from animal studies. The data from one study indicated that bone mineral density is negatively associated with saturated fat intake, and that men may be particularly vulnerable.\n\nRecommendations to reduce or limit dietary intake of saturated fats are made by the World Health Organization, American Heart Association, Health Canada, the US Department of Health and Human Services, the UK Food Standards Agency, the Australian Department of Health and Aging, the Singapore Government Health Promotion Board, the Indian Government Citizens Health Portal, the New Zealand Ministry of Health, the Food and Drugs Board Ghana, the Republic of Guyana Ministry of Health, and Hong Kong's Centre for Food Safety.\n\nIn 2003, the World Health Organization (WHO) and Food and Agriculture Organization (FAO) expert consultation report concluded that \"intake of saturated fatty acids is directly related to cardiovascular risk. The traditional target is to restrict the intake of saturated fatty acids to less than 10% of daily energy intake and less than 7% for high-risk groups. If populations are consuming less than 10%, they should not increase that level of intake. Within these limits, intake of foods rich in myristic and palmitic acids should be replaced by fats with a lower content of these particular fatty acids. In developing countries, however, where energy intake for some population groups may be inadequate, energy expenditure is high and body fat stores are low (BMI <18.5 kg/m). The amount and quality of fat supply has to be considered keeping in mind the need to meet energy requirements. Specific sources of saturated fat, such as coconut and palm oil, provide low-cost energy and may be an important source of energy for the poor.\"\n\nA 2004 statement released by the Centers for Disease Control (CDC) determined that \"Americans need to continue working to reduce saturated fat intake…\" In addition, reviews by the American Heart Association led the Association to recommend reducing saturated fat intake to less than 7% of total calories according to its 2006 recommendations. This concurs with similar conclusions made by the US Department of Health and Human Services, which determined that reduction in saturated fat consumption would positively affect health and reduce the prevalence of heart disease.\n\nThe United Kingdom, National Health Service claims the majority of British people eat too much saturated fat. The British Heart Foundation also advises people to cut down on saturated fat. People are advised to cut down on saturated fat and read labels on food they buy.\n\nA 2004 review stated that \"no lower safe limit of specific saturated fatty acid intakes has been identified\" and recommended that the influence of varying saturated fatty acid intakes against a background of different individual lifestyles and genetic backgrounds should be the focus in future studies.\n\nBlanket recommendations to lower saturated fat were criticized at a 2010 conference debate of the American Dietetic Association for focusing too narrowly on reducing saturated fats rather than emphasizing increased consumption of healthy fats and unrefined carbohydrates. Concern was expressed over the health risks of replacing saturated fats in the diet with refined carbohydrates, which carry a high risk of obesity and heart disease, particularly at the expense of polyunsaturated fats which may have health benefits. None of the panelists recommended heavy consumption of saturated fats, emphasizing instead the importance of overall dietary quality to cardiovascular health.\n\nIn a 2017 comprehensive review of the literature and clinical trials, the American Heart Association published a recommendation that saturated fat intake be reduced or replaced by products containing monounsaturated and polyunsaturated fats, a dietary adjustment that could reduce the risk of cardiovascular diseases by 30%.\n\n It should be noted, as this is the defining factor of saturated fats, that the two-dimensional illustration has implicit hydrogen atoms bonded to each of the carbon atoms in the polycarbon tail of the myristic acid molecule (there are 13 carbon atoms in the tail; 14 carbon atoms in the entire molecule).\n\nCarbon atoms are also implicitly drawn, as they are portrayed as intersections between two straight lines. \"Saturated,\" in general, refers to a maximum number of hydrogen atoms bonded to each carbon of the polycarbon tail as allowed by the Octet Rule. This also means that only single bonds (sigma bonds) will be present between adjacent carbon atoms of the tail.\n\n"}
{"id": "31417204", "url": "https://en.wikipedia.org/wiki?curid=31417204", "title": "Scarlet badis", "text": "Scarlet badis\n\nThe scarlet badis (\"Dario dario\") is a tropical freshwater fish and one of the smallest known percoid fish species. It is a micropredator, feeding on small aquatic crustaceans, worms, insect larvae and other zooplankton. It is sold under a variety of names (\"Badis badis bengalensis\", \"B. bengalensis\") in the aquarium trade.\n\nScarlet badis are among the smallest percoid fish species. Males usually do not exceed , with the females being even smaller around . Apart from the size difference, the sexes are easy to distinguish by the vibrant colors and prominent fins of the male.\n\nThe distribution of this species appears to be restricted to tributary systems draining into the Brahmaputra River in parts of West Bengal and Assam states of India, although it might also range into Bhutan. It typically inhabits shallow, clear water streams with sand or gravel substrates and dense growths of marginal and/or aquatic vegetation.\n\nThe scarlet badis is becoming increasingly popular as an aquarium fish, particularly in nano aquaria. Their territorial nature does however mean that their environment must contain sufficient boundaries to prevent aggressive behaviour, especially between males of the species. The careful aquarist would ensure that the tank is sufficiently filled with items of decoration capable of breaking up lines of sight and establishing territories. natural driftwood or coconut shells used as caves are all ideal.\n\nThis species should only be considered for inclusion in an established and well-cycled aquarium with suitable water parameters.\n\n"}
{"id": "50161693", "url": "https://en.wikipedia.org/wiki?curid=50161693", "title": "Targray", "text": "Targray\n\nTargray Technology International Inc., commonly referred to as Targray, is a Canadian multinational chemical and electronic materials corporation headquartered in Kirkland, Quebec that provides materials and supply chain solutions to the international biofuel, natural gas, lithium-ion battery, energy storage, and solar photovoltaics industries.\n\nTargray was established in 1987 in Beaconsfield, Quebec by metallurgical engineer Thomas A. Richardson to supply manufacturing materials and consumables for the compact disc manufacturing industry. As of 2018, the company operated sales offices and warehouses in several countries including Canada, the United States, the Czech Republic, Germany, China, Hong Kong, Taiwan and India.\n\nTargray has supplied materials used in the fabrication of optical discs and DVDs since it was founded in 1989. The company is a global distributor of mastering, replication and electroforming materials used to manufacture optical discs.\n\nIn 2005, Targray launched its Solar Division, which supplies raw materials, electronics, and supply chain solutions for the international solar photovoltaics industry. As of 2016, the company was one of the largest global suppliers of solar wafers, and polysilicon, a component used in the manufacturing of solar cells and semiconductor wafers. In April 2017, Targray announced that it had begun supplying junction boxes for photovoltaics manufacturing.\n\nTargray's Biofuels Division markets and trades bio-based fuels, primarily biodiesel, to local markets in the United States and Canada. Among the largest biodiesel suppliers in North America, the company is accredited by the National Biodiesel Board as a BQ-9000 Biodiesel Marketer, and is a registered supplier with the United States Environmental Protection Agency (EPA) and the California Air Resources Board (CARB). In 2014, Targray opened an inventory location for its biodiesel supply in Bakersfield, California. It has since added fuel terminal locations in Stockton, Fresno and Los Angeles, California, as well as in Cincinnati, Chicago, Tampa and New Orleans.\n\nIn February 2018, the California Air Resources Board published an executive order certifying Targray's Biodiesel additive, CATANOX, as an emissions equivalent additive under Appendix 1 of the ADF regulation, which governs the introduction and use of innovative alternative diesel fuels in California. The ADF regulation includes specific provisions designed to control potential increases in oxides of nitrogen (NOx) emissions that could otherwise be caused by the use of biodiesel under certain circumstances.\n\nTargray's Battery Division supplies advanced materials and electronic components used in the manufacturing of lithium-ion batteries. Notable products distributed by the Battery Division include anode materials, battery-grade lithium hydroxide, cathode materials, coated electrodes, separators, supercapacitors, current collectors, electrolyte, and packaging materials. The company also supplies litium-ion battery producers with manufacturing equipment for commercial, R&D and pilot-line installations.\n\nTargray's NGL Division is a North American marketer and distributor of natural gas liquids, with a specific focus on Natural gasoline. In 2018, the company opened a natural gas liquids trading desk in Norwalk, Connecticut \n\nTargray was legally incorporated in 1989 under the name Targray Technology International Inc. The company's first recorded sales were clean-room gloves sold to Americ Disc in Drummondville, Quebec. By 1995, Targray's selection of optical media products had grown to include a complete line of sputtering targets for all optical disc formats. Beginning in 1998, Targray underwent a series of expansions to accommodate its growing optical media business and workforce, and by 2004 the company had opened 5 international sales offices, located in Dordrecht, Laguna Niguel, Taipei, Panenské Břežany, and New Dehli.\n\nIn 2005, Targray adopted a diversification strategy focused on high-growth industries, beginning with solar photovoltaics. The strategy led to an increase in sales for the company, and in 2006 it was ranked the ninth fastest-growing company in the province of Quebec for the previous five–year period by Canadian French-language magazine L'actualité. In 2008, Targray's solar products had surpassed its optical media line in total sales, and the company was included in Profit Magazine's list of fastest growing companies in Canada. In 2009, Targray's annual revenues exceeded 250 million USD per year.\n\nIn 2010 and 2012, Targray established two new business divisions, created to supply lithium-ion battery materials and bio-based fuels, respectively. Shortly thereafter, the company began marketing biodiesel blends from its terminal location in California. In October 2016, professional services firm Ernst & Young named Targray President Andrew Richardson the winner of the 2016 Quebec Ernst & Young Entrepreneur of the Year Award.\n\nTargray supports and collaborates with several nonprofit organizations operating in the Energy Storage, Photovoltaics and Biofuel industries. The company is an institutional member of the Electrochemical Society, a full voting member of the National Biodiesel Board, and a corporate member of the Solar Energy Industries Association.\n"}
{"id": "57362024", "url": "https://en.wikipedia.org/wiki?curid=57362024", "title": "Tengger Desert Solar Park", "text": "Tengger Desert Solar Park\n\nTengger Desert Solar Park is the world's largest photovoltaic plant. It is located in Zhongwei, Ningxia, China. It currently covers a larger area (43 km) and has a larger peak power output (1,547 MW) than any other.\n\n"}
{"id": "377078", "url": "https://en.wikipedia.org/wiki?curid=377078", "title": "The Ape and the Sushi Master", "text": "The Ape and the Sushi Master\n\nThe Ape and the Sushi Master is a popular science book by Frans de Waal. It is an overview of animal behavior and psychology, with emphasis on primates.\n\nIt pays special emphasis on the anthropomorphological traits of primates of several different species. It also includes a short history of anthropomorphology and some of the field's pioneers.\n"}
{"id": "14664777", "url": "https://en.wikipedia.org/wiki?curid=14664777", "title": "Tracer use in the oil industry", "text": "Tracer use in the oil industry\n\nTracers are used in the oil industry in order to qualitatively or quantitatively gauge how fluid flows through the reservoir, as well as being a useful tool for estimating residual oil saturation. Tracers can be used in either interwell tests or single well tests. In interwell tests, the tracer is injected at one well along with the carrier fluid (water in a waterflood or gas in a gasflood) and detected at a producing well after some period of time, which can be anything from days to years. In single well tests, tracer is injected into the formation from a well and then produced out the same well. The delay between a tracer that does not react with the formation (a conservative tracer) and one that does (a partitioning tracer) will give an indication of residual oil saturation, a piece of information that is difficult to acquire by other means.\n\nTracers can be radioactive or chemical, gas or liquid and have been used extensively in the oil industry and hydrology for decades.\n\n<br>\n"}
{"id": "2377378", "url": "https://en.wikipedia.org/wiki?curid=2377378", "title": "Vestas", "text": "Vestas\n\nVestas Wind Systems A/S is a Danish manufacturer, seller, installer, and servicer of wind turbines founded in 1945. The company operates manufacturing plants in Denmark, Germany, India, Italy, Romania, the United Kingdom, Spain, Sweden, Norway, Australia, China, and the United States, and employs more than 21,000 people globally.\n\nAs of 2013, it is the largest wind turbine company in the world. \n\nVestas traces its roots to 1898 when Hand Smith Hansen bought a blacksmith shop in , West Jutland which operated as a family business. After the second world war Vestas was founded in 1945 by his son Peder Hansen as \"Vestjysk Stålteknik A/S\" (West-Jutlandish steel technology). The company initially manufactured household appliances, moving its focus to agricultural equipment in 1950, intercoolers in 1956, and hydraulic cranes in 1968. It entered the wind turbine industry in 1979, and produced wind turbines exclusively from 1989. In 1997, the company placed in production the NTK 1500/60. The product was designed by Timothy Jacob Jensen and received the German IF Award and the Red Dot Award.\nThe company's North American headquarters was relocated in 2002 from Palm Springs, California to Portland, Oregon.\nIn 2003, the company merged with the Danish wind turbine manufacturer NEG Micon to create the largest wind turbine manufacturer in the world, under the banner of Vestas Wind Systems. After a sales slump and an operational loss in 2005, Vestas recovered in 2006 with a 28% market share and increased production although market share slid to between 12.5. and 14%\n\nVestas began a whistleblower program in 2007, among the first in Denmark.\n\nOn 1 December 2008 Vestas announced plans to expand its North American headquarters in Portland through construction of a new building, but this plan was mothballed in 2009 due to the economic recession, and in August 2010 the company announced a revised plan, scaled back in size, to expand its Portland headquarters by renovating an existing-but-vacant building. At that time, Vestas employed about 400 in Portland and committed to add at least 100 more employees there within five years; the new building will have space for up to 600 workers. The company moved its Portland offices to the new headquarters building, a renovated historic building, in May 2012.\n\nIn February 2009, the company announced the production of two new turbine types, the 3-megawatt V112 and 1.8-megawatt V100. The new models were to be available in 2010.\n\nIn July 2009, Vestas announced its manufacturing operations on the Isle of Wight in England would close due to a lack of UK demand, affecting 525 jobs there and 100 in Southampton. Approximately 25 workers at the wind turbine factory on the island occupied the administration offices in protest on 20 July 2009, demanding nationalisation to save their jobs.\n\nIn August 2009 Vestas hired more than 5,000 extra workers for its new factories in China, the United States, and Spain. The company said it was \"expanding heavily in China and the US because these markets were growing the fastest, in contrast to the sluggish pace of wind farm development in the UK\". As part of this gradual shift in production away from Europe and towards China and the US, in October 2010, the company announced it was closing five factories in Denmark and Sweden, with the loss of 3,000 jobs.\n\nIn November 2010, Vestas shut down the 70-person staff advisory department 'Vestas Excellence', responsible for securing competitiveness, handling suppliers, quality assurance and globalization.\n\nIn January 2012, the company suggested firing 1,600 out of its 3,000 U.S. workers if the U.S. did not renew the 2.2 cents-per-kilowatt-hour Production Tax Credit, which was extended in 2013. On 13 August 2012, an estimated 90 workers were laid off from the Pueblo facility. Six long colored lines, leading to an exit, had been placed on the floor. Those laid off were given one of six different colored papers, and then instructed to follow the colored line that matched the colored paper they had been given. In 2013, the tower factory in Pueblo began ramping up to full utilization as orders rebounded from the 2012 slump. Other facilities in Colorado include a further 750 persons employed at a blade manufacturing facility in Windsor, Colorado. Vestas has nacelle and blade manufacturing facilities in Brighton, Colorado and also operates a tower facility in Pueblo, Colorado. Vestas said it decided to build its North American production facilities in Colorado because of the state’s central location, extensive transportation infrastructure and rail system, existing manufacturing base, and skilled workforce.\n\nIn May 2013, Marika Fredriksson became the company's new Executive Vice President and Chief Financial Officer after her predecessor Dag Andresen resigned for personal reasons. Her strategy is to lead Vestas back to higher earnings after the important losses faced by the company: from €166 million losses in 2011 and increasing to €963 million in 2012.\nIn September 2013, Vestas made a joint venture for offshore wind turbines with Mitsubishi Heavy Industries, including the 7-9 MW Vestas V164, the most powerful turbine on Earth.\n\nIn October 2013 Vestas sold its four casting and two machining factories to VTC Partners GmbH.\n\nIn May 2014, Vestas announced it would be adding hundreds of jobs to its Colorado Windsor and Brighton facilities and following a rough 2012 it called 2013 one of Vestas’s \"best years ever\". Vestas also added employees in Pueblo and expected the tower facility to eventually top 500. Vestas stated that it expected to have 2800 employees in Colorado by the end of 2014. , Vestas has a US nacelle production capacity of 2.6 GW.\n\nIn March 2015, Vestas announced it would be upping jobs by 400 at its blade manufacturing facility in Windsor and stated \"We had a very successful 2014\". In 2015 almost half of all Vestas turbines were going to the American market (nearly 3 GW for US out of 7.5 GW worldwide). Vestas intends to build a blade factory in India in 2016.\n\nIn 2014 and 2015, 26 dishonest employees were detected with the company's whistleblower program (the first in Denmark), and disciplined.\n\nIn February 2016, Vestas got its largest order of 1,000 MW (278 x 3.6 MW) for the Fosen project near Trondheim in Norway. It costs DKK 11 billion, and should deliver 3.4 TWh per year.\n\nIn 1Q 2016, the average wind turbine price was 0.83 million Euro per MW, compared to 0.91 a year before.\n\nIn 2016, Vestas was voted number 7 on the Clean200 list.\n\nVestas spent €92 million ($128 million), or 1.4% of revenue, on research and development in 2009. It has filed 787 wind turbine patents (227 in 2010) according to United Kingdom Intellectual Property Office (UK-IPO), while General Electric has 666 and Siemens Wind Power has 242.\n\nIn October 2009, Vestas and QinetiQ claimed a successful test of a stealth wind turbine blade mitigating radar reflection problems for aviation.\n\nIn December 2010 Vestas were developing the V164 7 MW offshore turbine, with a 164 m rotor diameter. Prototypes of it will be manufactured at Lindø (the former Maersk shipyard) due to size, crane and port access requirements. Series production of nacelles for the 32 turbines (256 MW) extension of the 90 MW Burbo Bank Offshore Wind Farm will occur at Lindø, while blades are made at Vestas' Isle of Wight facilities in England. DONG Energy will test a prototype in the sea off Frederikshavn in 2013, at a cost of DKK 240 million. A V164 was installed for testing in Østerild Wind Turbine Test Field in 2014.\n\nIn June 2011, the Vestas supercomputer Firestorm was number 53 on the TOP500-list of the world's most powerful computers calculating worldwide weather in a 3x3 km grid, and it delivers daily weather reports to the newspaper Ekstra Bladet and similar purposes. In 2012, Vestas donated the older 1344-core supercomputer from 2008 to Aalborg University.\n\nIn October 2011, Vestas participated in the deployment of a floating wind turbine offshore of Portugal. Vestas supplied a v80 2.0 MW offshore turbine to Windplus, S.A. (a joint-venture company including Energias de Portugal, Repsol, Principle Power, A. Silva Matos, Inovcapital and Portugal Ventures). The system, known as the WindFloat, consists of a semi-submersible type floating foundation, a conventional catenary mooring, and the wind turbine. The successful deployment represents the first offshore multi-megawatt wind turbine to be installed without the use of any heavy-lift or specialized offshore construction equipment.\n\nIn 2012, Vestas scaled back and closed some of its R&D offices in Houston, Marlborough, Louisville, China, Singapore and Denmark.\n\nIn August 2013, Vestas started operating its 20 MW test bench for nacelles in Aarhus.\n\nOn 5 September 2013, Dr. Chris Spruce, Vestas Senior Product Engineer, served as member of the Scientific Advisory Board (SAB) for the kite-energy-systems project ERC HIGHWIND, a project at KU Leuven dedicated to the research and development of tethered airfoils dedicated to generating energy by airborne wind energy (AWE).\n\nIn April 2016, Vestas installed a 900 kW quadrotor test wind turbine at Risø, made from 4 recycled 225 kW V29 turbines. Three months of testing have so far confirmed theoretical models. Vestas has no immediate plans of commercializing the prototype.\n\nAs of 2012 Vestas has installed over 48,000 wind turbines for a capacity of 55 GW in over 70 countries on five continents. As of 31 2012 the company has built production facilities in more than 12 countries, among them China, Spain and the United States. In China, Vestas employs 2,600 people.\n\n\n"}
{"id": "13671479", "url": "https://en.wikipedia.org/wiki?curid=13671479", "title": "Viaspan", "text": "Viaspan\n\nViaspan was the trademark under which the University of Wisconsin cold storage solution (also known as University of Wisconsin solution or UW solution) was sold. Currently, UW solution is sold under the Belzer UW trademark and others. UW solution was the first solution designed for use in organ transplantation, and became the first intracellular-like preservation medium. Developed in the late 1980s by Folkert Belzer and James Southard for pancreas preservation, the solution soon displaced EuroCollins solution as the preferred medium for cold storage of livers and kidneys, as well as pancreas. The solution has also been used for hearts and other organs. University of Wisconsin cold storage solution remains what is often called the \"gold standard\" for organ preservation, despite the development of other solutions that are in some respects superior.\n\nThe guiding principles for the development of UW Solution were:\n\n\n\n"}
{"id": "675364", "url": "https://en.wikipedia.org/wiki?curid=675364", "title": "Voltage regulation", "text": "Voltage regulation\n\nIn electrical engineering, particularly power engineering, voltage regulation is a measure of change in the voltage magnitude between the sending and receiving end of a component, such as a transmission or distribution line. Voltage regulation describes the ability of a system to provide near constant voltage over a wide range of load conditions. The term may refer to a passive property that results in more or less voltage drop under various load conditions, or to the active intervention with devices for the specific purpose of adjusting voltage.\n\nIn electrical power systems it is a dimensionless quantity defined at the receiving end of a transmission line as:\nwhere\n\"V\" is voltage at no load and \"V\" is voltage at full load. The percent voltage regulation of an ideal transmission line, as defined by a transmission line with zero resistance and reactance, would equal zero due to \"V\" equaling \"V\" as a result of there being no voltage drop along the line. This is why a smaller value of \"Voltage Regulation\" is usually beneficial, indicating that the line is closer to ideal.\n\nThe Voltage Regulation formula could be visualized with the following: \"Consider power being delivered to a load such that the voltage at the load is the load's rated voltage \"V\", if then the load disappears, the voltage at the point of the load will rise to \"V\".\"\n\nVoltage regulation in transmission lines occurs due to the impedance of the line between its sending and receiving ends. Transmission lines intrinsically have some amount of resistance, inductance, and capacitance that all change the voltage continuously along the line. Both the magnitude and phase angle of voltage change along a real transmission line. The effects of line impedance can be modeled with simplified circuits such as the short line approximation (least accurate), the medium line approximation (more accurate), and the long line approximation (most accurate).\n\nThe short line approximation ignores capacitance of the transmission line and models the resistance and reactance of the transmission line as a simple series resistor and inductor. This combination has impedance R + jL or R + jX. There is a single line current I = I = I in the short line approximation, different from the medium and long line. The medium length line approximation takes into account the shunt admittance, usually pure capacitance, by distributing half the admittance at the sending and receiving end of the line. This configuration is often referred to as a nominal - π. The long line approximation takes these lumped impedance and admittance values and distributes them uniformly along the length of the line. The long line approximation therefore requires the solving of differential equations and results in the highest degree of accuracy.\n\nIn the voltage regulation formula, V is the voltage measured at the receiving end terminals when the receiving end is an open circuit. The entire short line model is an open circuit in this condition, and no current flows in an open circuit, so I = 0 A and the voltage drop across the line given by Ohm’s law V = IZ is 0 V. The sending and receiving end voltages are thus the same. This value is what the voltage at the receiving end would be if the transmission line had no impedance. The voltage would not be changed at all by the line, which is an ideal scenario in power transmission.\n\nV is the voltage across the load at the receiving end when the load is connected and current flows in the transmission line. Now V = IZ is nonzero, so the voltages and the sending and receiving ends of the transmission line are not equal. The current I can be found by solving Ohm’s law using a combined line and load impedance: formula_2. Then the V is given by formula_3.\n\nThe effects of this modulation on voltage magnitude and phase angle is illustrated using phasor diagrams that map V, V, and the resistive and inductive components of V. Three power factor scenarios are shown, where (a) the line serves an inductive load so the current lags receiving end voltage, (b) the line serves a completely real load so the current and receiving end voltage are in phase, and (c) the line serves a capacitive load so the current leads receiving end voltage. In all cases the line resistance R causes a voltage drop that is in phase with current, and the reactance of the line X causes a voltage drop that leads current by 90 degrees. These successive voltage drops are summed to the receiving end voltage, tracing backward from V to V in the short line approximation circuit. The vector sum of V and the voltage drops equals V, and it is apparent in the diagrams that V does not equal V in magnitude or phase angle.\n\nThe diagrams show that the phase angle of current in the line affects voltage regulation significantly. Lagging current in (a) makes the required magnitude of sending end voltage quite large relative to the receiving end. The phase angle difference between sending and receiving end is minimized, however. Leading current in (c) actually allows the sending end voltage magnitude be smaller than the receiving end magnitude, so the voltage counterintuitively increases along the line. In-phase current in (b) does little to affect the magnitude of voltage between sending and receiving ends, but the phase angle shifts considerably.\n\nReal transmission lines typically serve inductive loads, which are the motors that exist everywhere in modern electronics and machines. Transferring a large amount of reactive power Q to inductive loads makes the line current lag voltage, and the voltage regulation is characterized by decrease in voltage magnitude. In transferring a large amount of real power P to real loads, current is mostly in phase with voltage. The voltage regulation in this scenario is characterized by a decrease in phase angle rather than magnitude.\n\nSometimes, the term voltage regulation is used to describe processes by which the quantity \"VR\" is reduced, especially concerning special circuits and devices for this purpose (see below).\n\nThe quality of a system's voltage regulation is described by three main parameters:\nElectric utilities aim to provide service to customers at a specific voltage level, for example, 220 V or 240 V. However, due to Kirchhoff's Laws, the voltage magnitude and thus the service voltage to customers will in fact vary along the length of a conductor such as a distribution feeder (see Electric power distribution). Depending on law and local practice, actual service voltage within a tolerance band such as ±5% or ±10% may be considered acceptable. In order to maintain voltage within tolerance under changing load conditions, various types of devices are traditionally employed:\n\n• a load tap changer (LTC) at the substation transformer, which changes the turns ratio in response to load current and thereby adjusts the voltage supplied at the sending end of the feeder;\n\n• voltage regulators, which are essentially transformers with tap changers to adjust the voltage along the feeder, so as to compensate for the voltage drop over distance; and\n\n• capacitors, which reduce the voltage drop along the feeder by reducing current flow to loads consuming reactive power.\n\nA new generation of devices for voltage regulation based on solid-state technology are in the early commercialization stages.\n\nDistribution regulation involves a \"regulation point\": the point at which the equipment tries to maintain constant voltage. Customers further than this point observe an expected effect: higher voltage at light load, and lower voltage at high load. Customers closer than this point experience the opposite effect: higher voltage at high load, and lower voltage at light load.\n\nDistributed generation, in particular photovoltaics connected at the distribution level, presents a number of significant challenges for voltage regulation.\n\nConventional voltage regulation equipment works under the assumption that line voltage changes predictably with distance along the feeder. Specifically, feeder voltage drops with increasing distance from the substation due to line impedance and the rate of voltage drop decreases farther away from the substation. However, this assumption may not hold when DG is present. For example, a long feeder with a high concentration of DG at the end will experience significant current injection at points where the voltage is normally lowest. If the load is sufficiently low, current will flow in the reverse direction (i.e. towards the substation), resulting in a voltage profile that increases with distance from the substation. This inverted voltage profile may confuse conventional controls. In one such scenario, load tap changers expecting voltage to decrease with distance from the substation may choose an operating point that in fact causes voltage down the line to exceed operating limits.\n\nThe voltage regulation issues caused by DG at the distribution level are complicated by lack of utility monitoring equipment along distribution feeders. The relative scarcity of information on distribution voltages and loads makes it difficult for utilities to make adjustments necessary to keep voltage levels within operating limits.\n\nAlthough DG poses a number of significant challenges for distribution level voltage regulation, if combined with intelligent power electronics DG can actually serve to enhance voltage regulation efforts. One such example is PV connected to the grid through inverters with volt-VAR control. In a study conducted jointly by the National Renewable Energy Laboratory (NREL) and Electric Power Research Institute (EPRI), when volt-VAR control was added to a distribution feeder with 20% PV penetration, the diurnal voltage swings on the feeder were significantly reduced.\n\nOne case of voltage regulation is in a transformer. The unideal components of the transformer cause a change in voltage when current flows. Under no load, when no current flows through the secondary coils, \"V\" is given by the ideal model, where \"V = V*N/N\". Looking at the equivalent circuit and neglecting the shunt components, as is a reasonable approximation, one can refer all resistance and reactance to the secondary side and clearly see that the secondary voltage at no load will indeed be given by the ideal model. In contrast, when the transformer delivers full load, a voltage drop occurs over the winding resistance, causing the terminal voltage across the load to be lower than anticipated. By the definition above, this leads to a nonzero voltage regulation which must be considered in use of the transformer.\n\n"}
{"id": "25254534", "url": "https://en.wikipedia.org/wiki?curid=25254534", "title": "Water gel (plain)", "text": "Water gel (plain)\n\nWater crystal gel or water beads is any gel which contains a large amount of water. Water gel is usually composed of a water-absorbing polymer such as a polyacrylamide (frequently Poly(methyl acrylate) or Sodium polyacrylate). Sometimes referred to as superabsorbent polymer (SAP) or, in dry form, as slush powder.\n\nWater gels are used for:\n\n\n"}
{"id": "58256", "url": "https://en.wikipedia.org/wiki?curid=58256", "title": "Wax", "text": "Wax\n\nWaxes are a diverse class of organic compounds that are lipophilic, malleable solids near ambient temperatures. They include higher alkanes and lipids, typically with melting points above about 40 °C (104 °F), melting to give low viscosity liquids. Waxes are insoluble in water but soluble in organic, nonpolar solvents. Natural waxes of different types are produced by plants and animals and occur in petroleum.\n\nWaxes are organic compounds that characteristically consist of long alkyl chains. They may also include various functional groups such as fatty acids, primary and secondary long chain alcohols, unsaturated bonds, aromatics, amides, ketones, and aldehydes. They frequently contain fatty acid esters as well. Synthetic waxes are often long-chain hydrocarbons (alkanes or paraffins) that lack functional groups.\n\nWaxes are synthesized by many plants and animals. Those of animal origin typically consist of wax esters derived from a variety of carboxylic acids and fatty alcohols. In waxes of plant origin, characteristic mixtures of unesterified hydrocarbons may predominate over esters. The composition depends not only on species, but also on geographic location of the organism.\n\nThe most commonly known animal wax is beeswax, but other insects secrete (release) waxes. A major component of the beeswax used in constructing honeycombs is the ester myricyl palmitate which is an ester of triacontanol and palmitic acid. Its melting point is 62-65 °C. Spermaceti occurs in large amounts in the head oil of the sperm whale. One of its main constituents is cetyl palmitate, another ester of a fatty acid and a fatty alcohol. Lanolin is a wax obtained from wool, consisting of esters of sterols.\n\nPlants secrete waxes into and on the surface of their cuticles as a way to control evaporation, wettability and hydration. The epicuticular waxes of plants are mixtures of substituted long-chain aliphatic hydrocarbons, containing alkanes, alkyl esters, fatty acids, primary and secondary alcohols, diols, ketones, aldehydes.\nFrom the commercial perspective, the most important plant wax is carnauba wax, a hard wax obtained from the Brazilian palm \"Copernicia prunifera\". Containing the ester myricyl cerotate, it has many applications, such as confectionery and other food coatings, car and furniture polish, floss coating, and surfboard wax. Other more specialized vegetable waxes include candelilla wax and ouricury wax.\n\nPlant and animal based waxes or oils can undergo selective chemical modifications to produce waxes with more desirable properties than are available in the unmodified starting material. This approach has relied on green chemistry approaches including olefin metathesis and enzymatic reactions and can be used to produce waxes from inexpensive starting materials like vegetable oils.\n\nAlthough many natural waxes contain esters, paraffin waxes are hydrocarbons, mixtures of alkanes usually in a homologous series of chain lengths. These materials represent a significant fraction of petroleum. They are refined by vacuum distillation. Paraffin waxes are mixtures of saturated n- and iso- alkanes, naphthenes, and alkyl- and naphthene-substituted aromatic compounds. A typical alkane paraffin wax chemical composition comprises hydrocarbons with the general formula CH, such as Hentriacontane, CH. The degree of branching has an important influence on the properties. Microcrystalline wax is a lesser produced petroleum based wax that contains higher percentage of isoparaffinic (branched) hydrocarbons and naphthenic hydrocarbons.\n\nMillions of tons of paraffin waxes are produced annually. They are used in foods (such as chewing gum and cheese wrapping), in candles and cosmetics, as non-stick and waterproofing coatings and in polishes.\n\nMontan wax is a fossilized wax extracted from coal and lignite. It is very hard, reflecting the high concentration of saturated fatty acids and alcohols. Although dark brown and odorous, they can be purified and bleached to give commercially useful products.\n\nAs of 1995, about 200 million kilograms/y were consumed.\n\nPolyethylene waxes are manufactured by one of three methods: 1- direct polymerization of ethylene (may include co -monomers also); 2- thermal degradation of high molecular weight polyethylene resin; 3- recovery of low molecular weight fractions from high molecular weight resin production.\n\nEach production technique generates products with slightly different properties. Key properties of low molecular weight polyethylene waxes are viscosity, density and melt point.\n\nPolyethylene waxes produced by means of degradation or recovery from polyethylene resin streams contain very low molecular weight materials that must be removed to prevent volatilization and potential fire hazards during use. Polyethylene waxes manufactured by this method are usually stripped of low molecular weight fractions to yield a flash point > 500°F(> 260°C). Many polyethylene resin plants produce a low molecular weight stream often referred to as Low Polymer Wax (LPW). LPW is unrefined and contains volatile oligomers, corrosive catalyst and may contain other foreign material and water. Refining of LPW to produce a polyethylene wax involves removal of oligomers and hazardous catalyst. Proper refining of LPW to produce polyethylene wax is especially important when being used in applications requiring FDA or other regulatory certification.\n\nWaxes are mainly consumed industrially as components of complex formulations, often for coatings. The main use of polyethylene and polypropylene waxes is in the formulation of colourants for plastics. Waxes confer matting effects and wear resistance to paints. Polyethylene waxes are incorporated into inks in the form of dispersions to decrease friction. They are employed as release agents, find use as slip agents in furniture, and confer corrosion resistance.\n\nWaxes such as paraffin wax or beeswax, and hard fats such as tallow are used to make candles, used for lighting and decoration.\n\nWaxes are used as finishes and coatings for wood products. Beeswax is frequently used as a lubricant on drawer slides where wood to wood contact occurs.\n\nSealing wax was used to close important documents in the Middle Ages. Wax tablets were used as writing surfaces. There were different types of wax in the Middle Ages, namely four kinds of wax (Ragusan, Montenegro, Byzantine, and Bulgarian), \"ordinary\" waxes from Spain, Poland, and Riga, unrefined waxes and colored waxes (red, white, and green). Waxes are used to make wax paper, impregnating and coating paper and card to waterproof it or make it resistant to staining, or to modify its surface properties. Waxes are also used in shoe polishes, wood polishes, and automotive polishes, as mold release agents in mold making, as a coating for many cheeses, and to waterproof leather and fabric. Wax has been used since antiquity as a temporary, removable model in lost-wax casting of gold, silver and other materials.\n\nWax with colorful pigments added has been used as a medium in encaustic painting, and is used today in the manufacture of crayons, china markers and colored pencils. Carbon paper, used for making duplicate typewritten documents was coated with carbon black suspended in wax, typically montan wax, but has largely been superseded by photocopiers and computer printers. In another context, lipstick and mascara are blends of various fats and waxes colored with pigments, and both beeswax and lanolin are used in other cosmetics. Ski wax is used in skiing and snowboarding. Also, the sports of surfing and skateboarding often use wax to enhance the performance.\n\nSome waxes are considered food-safe and are used to coat wooden cutting boards and other items that come into contact with food. Beeswax or coloured synthetic wax is used to decorate Easter eggs in Romania, Ukraine, Poland, Lithuania and the Czech Republic. Paraffin wax is used in making chocolate covered sweets.\n\nWax is also used in wax bullets, which are used as simulation aids.\n\n\n\n\n\n\n"}
{"id": "18860811", "url": "https://en.wikipedia.org/wiki?curid=18860811", "title": "Wizard Power", "text": "Wizard Power\n\nWizard Power Pty Ltd was an Australian company focused on solar technology research, development and commercialisation. The company was headquartered in Canberra, Australia. The company was wound up in September 2013, owing 8 million dollars to creditors and employees.\n\nWizard Power's technology portfolio use to include:\n\n\nThese technologies enable the delivery of utility-scale, zero emission thermal and electrical energy for dispatchable power applications. The Australian National University developed the first versions of these technologies over a period of more than 30 years. The current Wizard Power Big Dish ‘commercial’ design, represented by the prototype located at the ANU, is based on novel space-frame and mirror panel systems that are optimised for the cost-effective deployment of large (tens to hundreds of megawatt capacity) solar fields.\n\nWizard Power claimed they provided the research, development, technology support and systems integration services necessary to facilitate the development of solutions and projects using these technologies.\n\nIn addition to these technologies, Wizard Power was supposedly developing high-temperature solar gasification solutions for the conversion of coal, biomass and other carbonaceous materials to liquid fuels, plastics and fertilisers.\n\nWizard Power is tried to establish a demonstration site in Whyalla, South Australia which will showcase the company's energy storage technologies. The Whyalla Solar Storage plant will be a pre-commercial demonstration of energy storage, with full integration, demonstration of start up and shut down procedures, ability to handle intermittent solar input, and deliver energy on demand. This project has been supported by the Commonwealth Government of Australia's Advance Electricity Storage Program.\n\nThe Whyalla SolarOasis will use 300 Wizard Power Big Dish solar thermal concentrators to deliver a 40MWe solar thermal power plant. It was to be built 4 km north of Whyalla adjacent to the solar storage research facility. The plant is designed as a peaking power plant operating in the day and early evening to deliver electricity when it is most in demand.\n\nA $230 million project was to use Wizard Power Technology to generate 66GWh of solar electricity each year. for a variety of commonwealth and corporate mismanagement reasons the project did not proceed past the proposal stage.\n\nThe Whyalla SolarOasis has been supported by the Australian Government's Renewable Energy Demonstration Program with a A$60 million grant and will be developed by the SolarOasis consortium.\n\nThe Commonwealth funding was cancelled in June 2013.\n\n"}
{"id": "113266", "url": "https://en.wikipedia.org/wiki?curid=113266", "title": "Woodworking joints", "text": "Woodworking joints\n\nJoinery is a part of woodworking that involves joining together pieces of timber or lumber, to produce more complex items. Some wood joints employ fasteners, bindings, or adhesives, while others use only wood elements. The characteristics of wooden joints - strength, flexibility, toughness, appearance, etc. - derive from the properties of the materials involved and the purpose of the joint. Therefore, different joinery techniques are used to meet differing requirements. For example, the joinery used to construct a house can be different from that used to make puzzle toys, although some concepts overlap.\n\n\nMany wood joinery techniques either depend upon or compensate for the fact that wood is anisotropic: its material properties are different along different dimensions.\n\nThis must be taken into account when joining wood parts together, otherwise the joint is destined to fail. Gluing boards with the grain running perpendicular to each other is often the reason for split boards, or broken joints. Furniture from the 18th century, while made by master craftsmen, did not take this into account. The result is this masterful work suffers from broken bracket feet, which was often attached with a glue block which ran perpendicular to the base pieces. The glue blocks were fastened with both glue and nails, resulting in unequal expansion and contraction between the pieces. This was also the cause of splitting of wide boards, which were commonly used during that period.\n\nIn modern woodworking it is even more critical, as heating and air conditioning cause major changes in the moisture content of the wood. All woodworking joints must take these changes into account, and allow for the resulting movement.\n\nWood is stronger when stressed along the grain (longitudinally) than it is when stressed across the grain (radially and tangentially). Wood is a natural composite material; parallel strands of cellulose fibers are held together by a lignin binder. These long chains of fibers make the wood exceptionally strong by resisting stress and spreading the load over the length of the board. Furthermore, cellulose is tougher than lignin, a fact demonstrated by the relative ease with which wood can be split along the grain compared to across it.\n\nDifferent species of wood have different strength levels, and the exact strength may vary from sample to sample.\n\nTimber expands and contracts in response to humidity, usually much less so longitudinally than in the radial and tangential directions. As tracheophytes, trees have lignified tissues which transport resources such as water, minerals and photosynthetic products up and down the plant. While lumber from a harvested tree is no longer alive, these tissues still absorb and expel water causing swelling and shrinkage of the wood in kind with change in humidity. When the dimensional stability of the wood is paramount, quarter-sawn or rift-sawn lumber is preferred because its grain pattern is consistent and thus reacts less to humidity.\n\n\nMany traditional wood joinery techniques use the distinctive material properties of wood, often without resorting to mechanical fasteners or adhesives. While every culture in which pieces of wood are joined together to make furniture or structures has a joinery tradition, wood joinery techniques have been especially well documented and is celebrated in the Indian, Chinese, European, and Japanese traditions. Because of the physical existence of Indian and Egyptian examples, we know that furniture from the first several dynasties show the use of complex joints, like the Dovetail, over 5,000 years ago. This tradition continued to other later Western styles. The 18th century writer Diderot included over 90 detailed illustrations of wood joints in his comprehensive encyclopedia. While Western techniques focused on concealment of joinery, the Eastern societies, though later, did not attempt to \"hide\" their joints. The Japanese and Chinese traditions in particular required the use of hundreds of types of joints. The reason was that nails and glues used did not stand up well to the vastly fluctuating temperatures and humid weather conditions in most of Central and South-East Asia. As well, the highly resinous woods used in traditional Chinese furniture do not glue well, even if they are cleaned with solvents and attached using modern glues.\n\nMethods that are not considered traditional joinery have come about in modern times, largely to attempt to simplify the job of the woodworker for various reasons. These include biscuit joints and pocket hole joinery.\n\n\n\n"}
