{"id": "30764139", "url": "https://en.wikipedia.org/wiki?curid=30764139", "title": "Affine plane", "text": "Affine plane\n\nIn geometry, an affine plane is a two-dimensional affine space. Typical examples of affine planes are\nAll the affine planes defined over a field are isomorphic. More precisely, the choice of an affine coordinate system (or, in the real case, a Cartesian coordinate system) for an affine plane \"P\" over a field \"F\" induces an isomorphism of affine planes between \"P\" and \"F\".\n\nIn the more general situation, where the affine planes are not defined over a field, they will in general not be isomorphic. Two affine planes arising from the same non-Desarguesian projective plane by the removal of different lines may not be isomorphic.\n\nThere are two ways to formally define affine planes, which are equivalent for affine planes over a field. The first one consists in defining an affine plane as a set on which a vector space of dimension two acts simply transitively. Intuitively, this means that an affine plane is a vector space of dimension two in which one has \"forgotten\" where the origin is. In incidence geometry, an affine plane is defined as an abstract system of points and lines satisfying a system of axioms.\n\nIn the applications of mathematics, there are often situations where an affine plane without the Euclidean metric is used instead of the Euclidean plane. For example, in a graph, which can be drawn on paper, and in which the position of a particle is plotted against time, the Euclidean metric is not adequate for its interpretation, since the distances between its points or the measures of the angles between its lines have, in general, no physical importance (in the affine plane the axes can use different units, which are not comparable, and the measures also vary with different units and scales).\n\n"}
{"id": "44762938", "url": "https://en.wikipedia.org/wiki?curid=44762938", "title": "Akhfenir Wind Farm", "text": "Akhfenir Wind Farm\n\nAkhfenir Wind Farm is located in 15 km from Akhfenir and 100 km from Tarfaya in Morocco and a has a total installed capacity of 200 MW. It is owned by Nareva, a subsidiary of SNI the holding company Mohammed VI. The first 100MW wind turbines were supplied by Alstom and the contract for the extension of the capacity to 200MW was awarded to General Electric.\n\n"}
{"id": "1032295", "url": "https://en.wikipedia.org/wiki?curid=1032295", "title": "At the Abyss", "text": "At the Abyss\n\nAt the Abyss: An Insider's History of the Cold War () is an autobiographical book about Thomas C. Reed's experience at Lawrence Livermore National Laboratory through his time as an advisor to President Ronald Reagan. It reveals new details about the 1962 Cuban Missile Crisis, the Central Intelligence Agency, the Farewell Dossier, and other facets of the Cold War.\n\nIn the book, Reed stated the United States added a Trojan horse to gas pipeline control software that the Soviet Union obtained from a company in Canada. According to Reed, when the components were deployed on a Trans-Siberian gas pipeline, the Trojan horse led to a huge explosion. He wrote: \"The pipeline software that was to run the pumps, turbines and valves was programmed to go haywire, to reset pump speeds and valve settings to produce pressures far beyond those acceptable to the pipeline joints and welds. The result was the most monumental non-nuclear explosion and fire ever seen from space.\"\n\nA report in the \"Moscow Times\" quoted KGB veteran Vasily Pchelintsev as saying that there was a natural gas pipeline explosion in 1982, but it was near Tobolsk on a pipeline connecting the Urengoy gas field to the city of Chelyabinsk, and it was caused by poor construction rather than sabotage; according to Pchelintsev's account, no one was killed in the explosion and the damage was repaired within one day. Reed's account has also not been corroborated by intelligence agencies in the United States.\n\nAnother point of criticism of the sabotage allegations is that, according to Prof. V.D. Zakhmatov, an explosion safety expert who has overseen the safety measures on many of the Soviet oil and gas pipelines built in the Eighties, at the described timeframe Soviet Union simply didn't practice digital control of its pipeline system. Most of the control was manual, and whatever automation was used utilized the analog control systems, most of which worked through pneumatics.\n\nAccording to a review in \"Publishers Weekly\": \"The writing is sometimes discursive if seldom dull, and some areas have already been adequately covered by others. But the book deserves quite high marks for how much it pulls together, as well as offering a viewpoint on the Cold War not nearly sufficiently well-represented in the public literature: that neither the U.S. nor Soviet sciences were dominated by stereotypical, bomb-happy maniacs.\"\n"}
{"id": "3501548", "url": "https://en.wikipedia.org/wiki?curid=3501548", "title": "Bayberry wax", "text": "Bayberry wax\n\nBayberry wax is an aromatic green vegetable wax. It is removed from the surface of the fruit of the bayberry (wax-myrtle) shrub (ex. \"Myrica cerifera\") by boiling the fruits in water and skimming the wax from the surface of the water. It is made up primarily of esters of lauric, myristic, and palmitic acid.\n\nBayberry wax is used primarily in the manufacture of scented candles and other products where its distinctive resinous fragrance is desirable.\n\n"}
{"id": "37507962", "url": "https://en.wikipedia.org/wiki?curid=37507962", "title": "BioSphere Plastic", "text": "BioSphere Plastic\n\nBioSphere Plastic is a manufacturer of biodegradable additives. The company states that their product enhances the biodegradation of synthetic polymers by addition of their technology. It has a capacity at or around 5,800 metric tons a year and has filed for patent protection over their additive worldwide.\n\nBioSphere Plastic released their test reports showing biodegradability of Polyethylene and Polypropylene for public review in July 2012.\n\nBioSphere Plastic scientists have studied microbial biodegradation of polymers and have used this information to increase biodegradation of plastic made with their plastic additives.\n\nBioSphere Plastic offices are located in Portland, OR, USA, Santiago, Chile, and Bangkok, Thailand.\n\n\n"}
{"id": "7521019", "url": "https://en.wikipedia.org/wiki?curid=7521019", "title": "Biocomposite", "text": "Biocomposite\n\nBiocomposite ( from Greek 'alive') is a composite material formed by a matrix (resin) and a reinforcement of natural fibers. These kind of materials often mimic the structure of the living materials involved in the process keeping the strengthening properties of the matrix that was used, but always providing biocompatibility.\nThe matrix phase is formed by polymers derived from renewable and nonrenewable resources. The matrix is important to protect the fibers from environmental degradation and mechanical damage, to hold the fibers together and to transfer the loads on it. In addition, biofibers are the principal components of biocomposites, which are derived from biological origins, for example fibers from crops (cotton, flax or hemp), recycled wood, waste paper, crop processing byproducts or regenerated cellulose fiber (viscose/rayon). \nThe interest in biocomposites is rapidly growing in terms of industrial applications (automobiles, railway coach, aerospace, military applications, construction, and packaging) and fundamental research, due to its great benefits (renewable, cheap, recyclable, and biodegradable). Biocomposites can be used alone, or as a complement to standard materials, such as carbon fiber. Advocates of biocomposites state that use of these materials improve health and safety in their production, are lighter in weight, have a visual appeal similar to that of wood, and are environmentally superior.\n\nThe differential for this class of composites is that they are biodegradable and pollute the environment less which is a concern for many scientists and engineers to minimize the environmental impact of the production of a composite. They are a renewable source, cheap, and in certain cases completely recyclable. One advantage of natural fibers is their low density, which results in a higher tensile strength and stiffness than glass fibers, besides of its lower manufacturing costs. As such, biocomposites could be a viable ecological alternative to carbon, glass, and man-made fiber composites. Natural fibers have a hollow structure, which gives insulation against noise and heat. It is a class of materials that can be easily processed, and thus, they are suited to a wide range of applications, such as packaging, building (roof structure, bridge, window, door, green kitchen), automobiles, aerospace, military applications, electronics, consumer products and medical industry (prosthetic, bone plate, orthodontic archwire, total hip replacement, and composite screws and pins).\n\nBiocomposites are divided into non-wood fibers and wood fibers, all of which present cellulose and lignin. The non-wood fibers (natural fibers) are more attractive for the industry due to the physical and mechanical properties which they present. Also, these fibers are relatively long fibers, and present high cellulose content, which delivers a high tensile strength, and degree of cellulose crystallinity, whereas natural fibers have some disadvantages because they have hydroxyl groups (OH) in the fiber that can attract water molecules, and thus, the fiber might swell. This results in voids at the interface of the composite, which will affect the mechanical properties and loss in dimensional stability. The wood fibers have this name because almost than 60% of its mass is wood elements. It presents softwood fibers (long and flexible) and hardwood fibers (shorter and stiffer), and has low degree of cellulose crystallinity.\n\nThe natural fibers are divided into straw fibers, bast, leaf, seed or fruit, and grass fibers. The fibers most widely used in the industry are flax, jute, hemp, kenaf, sisal and coir. The straw fibers could be found in many parts of the world, and it is an example of a low-cost reinforcement for biocomposites. The wood fibers could be recycled or non-recycled. Thus, many polymers as polyethylene (PE), polypropylene (PP), and polyvinyl chloride (PVC) are being used in wood composites industries.\n\nFlax linen composites work well for applications seeking a lighter weight alternative to other materials, most notably, applications in automotive interior components and sports equipment. For automotive interiors, Composites Evolution has performed prototype testing for the Land Rover Defender and the Jaguar XF, with the Defender's flax composite 60% lighter than the production counterpart at the same stiffness, and the XF's flax composite part 35% lighter than the production component at the same stiffness\n\nIn sports equipment, Ergon Bikes produced a concept saddle that won first place among 439 entries in the Accessories category at the Eurobike 2012, a major bicycling industry trade show. VE Paddles has produce a boat paddle blade. Flaxland Canoes has developed a canoe that has a covering of flax linen. Magine Snowboards has developed a snowboard that incorporates flax linen. Samsara Surfboards has produced a flax linen surfboard. Idris Ski's Lynx won an ISPO Award in 2013 for the Lynx ski\n\nFlax linen composites also work for applications for which the look, feel, or sound of wood is desired, but without susceptibility to warping. Applications include furniture and musical instruments. In furniture, a team at Sheffield Hallam University designed a cabinet with entirely sustainable materials, including flax linen. In musical instruments, Blackbird Guitars has produced a ukulele made with flax linen that has won a number of design awards in the composites industry, as well as a guitar\n\nGreen composites are classified as a biocomposite combined by natural fibers with biodegradable resins. They are called green composites mainly because of their degradable and sustainable properties, which can be easily disposed without harming the environment. Because of their durability, green composites are mainly used to increase the life cycle of products with short life.\n\nAnother class of biocomposite is called 'hybrid biocomposite', which is based on different types of fibers into a single matrix. The fibers can be synthetic or natural, and can be randomly combined to generate the hybrid composites. Its functionality depends directly on the balance between the good and bad properties of each individual material used. Besides, with the use of a composite that has two more types of fibers in the hybrid composite, one fiber can stand on the other one when it is blocked. \nThe properties of this biocomposite depends directly on the fibers counting their content, length, arrangement, and also the bonding to the matrix. In particular, the strength of the hybrid composite depends on the failure strain of the individual fibers.\n\nThe production of biocomposites uses techniques that are used to manufacture plastics or composites materials. These techniques include:\n\n\n"}
{"id": "21309233", "url": "https://en.wikipedia.org/wiki?curid=21309233", "title": "Biogas upgrader", "text": "Biogas upgrader\n\nA biogas upgrader is a facility that is used to concentrate the methane in biogas to natural gas standards. The system removes carbon dioxide, hydrogen sulphide, water and contaminants from the biogas. One technique for doing this uses amine gas treating. This purified biogas is also called biomethane. It can be used interchangeably with natural gas.\n\nRaw biogas produced from digestion is roughly 60% methane and 29% CO with trace elements of HS; it is not high quality enough to be used as fuel gas for machinery. The corrosive nature of HS alone is enough to destroy the internals of a plant.\n\nThe solution is the use of biogas upgrading or purification processes whereby contaminants in the raw biogas stream are absorbed or scrubbed, leaving more methane per unit volume of gas. There are four main methods of upgrading: water washing, pressure swing adsorption, selexol adsorbtion, and amine gas treating.\n\nThe most prevalent method is water washing whereby high pressure gas flows into a column in which the carbon dioxide and other trace elements are scrubbed by cascading water running counter-flow to the gas. This arrangement can deliver 98% methane with manufacturers guaranteeing maximum 2% methane loss in the system. It takes roughly between 3% and 6% of the total energy output in gas to run a biogas upgrading system\n\nA typical PSA system for biogas will have four stages, one each for water vapor, carbon dioxide, nitrogen and oxygen. Gas to be upgraded enters each vessel, is compressed to a high pressure whereby the gas to be removed is adsorbed on to the surface of the adsorbent, and is then decompressed allowing the methane to leave. The adsorbent is then regenerated. For oxygen, molecular sieve is used, for nitrogen a zeolite, for carbon dioxide and water a zeolite or activated carbon.\n\nIn the Selexol process (now licensed by UOP LLC), the Selexol solvent dissolves (absorbs) the acid gases from the feed gas at relatively high pressure, usually 300 to 2000 psia (2.07 to 13.8 MPa). The rich solvent containing the acid gases is then let down in pressure and/or steam stripped to release and recover the acid gases. The Selexol process can operate selectively to recover hydrogen sulfide and carbon dioxide as separate streams, so that the hydrogen sulfide can be sent to either a Claus unit for conversion to elemental sulfur or to a WSA Process unit for conversion to sulfuric acid while, at the same time, the carbon dioxide can be sequestered or used for enhanced oil recovery.\n\nSelexol is a physical solvent, unlike amine based acid gas removal solvents that rely on a chemical reaction with the acid gases. Since no chemical reactions are involved, Selexol usually requires less energy than the amine based processes. However, at feed gas pressures below about 300 psia(2.07 MPa), the Selexol solvent capacity (in amount of acid gas absorbed per volume of solvent) is reduced and the amine based processes will usually be superior.\n\nHS or both HS and CO can be removed with this technology.\n\nThe chemistry involved in the amine treating of such gases varies somewhat with the particular amine being used. For one of the more common amines, monoethanolamine (MEA) denoted as RNH, the chemistry may be expressed as:\n\nA typical amine gas treating process includes an absorber unit and a regenerator unit . In the absorber, the downflowing amine solution absorbs HS and CO from the upflowing sour gas to produce a gas stream free of hydrogen sulfide and carbon dioxide as a product and an amine solution rich in the absorbed acid gases. The resultant \"rich\" amine is then routed into the regenerator (a stripper with a reboiler) to produce regenerated or \"lean\" amine that is recycled for reuse in the absorber. The stripped overhead gas from the regenerator is concentrated HS and CO.\n\nA distinction can be drawn between the basic treatment of raw biogas, which is necessary for example for use in a biogas CHP plant, and the more elaborate treatment needed to obtain natural gas quality (biomethane).\nThe above table shows the composition of raw biogas after primary treatment and biomethane. The fractions of crude biogas can vary greatly, depending on substrate, plant design, and other factors. The nature of the biomethane is adapted to the corresponding qualities of natural gas.\n\nBiogas is used mostly directly in a biogas cogeneration plant. This requires desulfurization and drying in order to avoid corrosion in the CHP. To be able to feed biogas into the natural gas network or for fuel use, a more comprehensive treatment is necessary. In addition to drying and desulfurization the carbon dioxide must be removed and chemical conditioning to obtain properties meeting the specifications for natural gas. This biomethane can be injected into the natural gas network and converted to electricity and heat through CHP at a place where the heat can be used, such as a swimming pool, which has a year-round high heat demand.\n\nUse of the natural gas 'grid' also permits retail customers to purchase a certain proportion of biomethane gas in their gas supply contracts\n\n"}
{"id": "34234822", "url": "https://en.wikipedia.org/wiki?curid=34234822", "title": "Bofors HPM Blackout", "text": "Bofors HPM Blackout\n\nBofors HPM Blackout is a high-powered microwave weapon system, built by BAE Systems, which is stated to be able to destroy at distance a wide variety of commercial off-the-shelf (COTS) electronic equipment. It is stated to be non-lethal to humans. The total weight of the weapon system is less than 500 kg.\n"}
{"id": "29055353", "url": "https://en.wikipedia.org/wiki?curid=29055353", "title": "Borders of the oceans", "text": "Borders of the oceans\n\nThe borders of the oceans are the limits of the Earth's oceanic waters. The definition and number of oceans can vary depending on the adopted criteria.\n\nThough generally described as several separate oceans, the world's oceanic waters constitute one global, interconnected body of salt water sometimes referred to as the World Ocean or global ocean. This concept of a continuous body of water with relatively free interchange among its parts is of fundamental importance to oceanography.\n\nThe major oceanic divisions are defined in part by the continents, various archipelagos, and other criteria. The principal divisions (in descending order of area) are the: Pacific Ocean, Atlantic Ocean, Indian Ocean, Southern (Antarctic) Ocean, and Arctic Ocean. Smaller regions of the oceans are called seas, gulfs, bays, straits, and other names.\n\nGeologically, an ocean is an area of oceanic crust covered by water. Oceanic crust is the thin layer of solidified volcanic basalt that covers the Earth's mantle. Continental crust is thicker but less dense. From this perspective, the Earth has three oceans: the World Ocean, the Caspian Sea, and the Black Sea. The latter two were formed by the collision of Cimmeria with Laurasia. The Mediterranean Sea is at times a discrete ocean, because tectonic plate movement has repeatedly broken its connection to the World Ocean through the Strait of Gibraltar. The Black Sea is connected to the Mediterranean through the Bosporus, but the Bosporus is a natural canal cut through continental rock some 7,000 years ago, rather than a piece of oceanic sea floor like the Strait of Gibraltar.\n\nDespite their names, some smaller landlocked \"seas\" are \"not\" connected with the World Ocean, such as the Caspian Sea (which is nevertheless, geologically, itself a full-fledged ocean—see above) and numerous salt lakes such as the Aral Sea.\n\nA complete hierarchy showing which seas belong to which oceans, according to the International Hydrographic Organization (IHO) and for the whole planet, is available at the European Marine Gazetteer website. See also the list of seas article for the seas included in each ocean area. Also note there are many varying definitions of the world's seas and no single authority.\n\nThe Arctic Ocean covers much of the Arctic and washes upon northern North America and Eurasia and is sometimes considered a sea or estuary of the Atlantic.\n\nThe International Hydrographic Organization (IHO) defines the limits of the Arctic Ocean (excluding the seas it contains) as follows:\n\nNote that these definitions exclude any marginal waterbodies that are separately defined by the IHO (such as the Kara Sea and East Siberian Sea), though these are usually considered to be part of the Arctic Ocean.\n\nThe CIA defines the limits of the Arctic Ocean differently, as depicted in the map comparing its definition to the IHO's definition.\n\nThe Atlantic Ocean separates the Americas from Europe and Africa. It may be further subdivided by the equator into northern and southern portions.\n\nThe 3rd edition, currently in force, of the International Hydrographic Organization's (IHO) \"Limits of Oceans and Seas\" defines the limits of the North Atlantic Ocean (excluding the seas it contains) as follows:\n\nThe 3rd edition (currently in force) of the International Hydrographic Organization's (IHO) \"Limits of Oceans and Seas\" defines the limits of the South Atlantic Ocean (excluding the seas it contains) as follows:\n\nNote that these definitions exclude any marginal waterbodies that are separately defined by the IHO (such as the Bay of Biscay and Gulf of Guinea), though these are usually considered to be part of the Atlantic Ocean.\n\nIn its 2002 draft, the IHO redefined the Atlantic Ocean, moving its southern limit to 60°S, with the waters south of that line identified as the Southern Ocean. This new definition has not yet been ratified (and, in addition, a reservation was lodged in 2003 by Australia.) While the name \"Southern Ocean\" is frequently used, some geographic authorities such as the 10th edition of the World Atlas from the U.S. National Geographic Society generally show the Atlantic, Indian, and Pacific Oceans continuing to Antarctica. If and when adopted, the 2002 definition would be published in the 4th edition of \"Limits of Oceans and Seas\", re-instituting the 2nd edition's \"Southern Ocean\", omitted from the 3rd edition.\n\nThe Indian Ocean washes upon southern Asia and separates Africa and Australia.\n\nThe 3rd edition, currently in force, of the International Hydrographic Organization's (IHO) \"Limits of Oceans and Seas\" defines the limits of the Indian Ocean (excluding the seas it contains) as follows:\n\nNote that this definition excludes any marginal waterbodies that are separately defined by the IHO (such as the Bay of Bengal and Arabian Sea), though these are usually considered to be part of the Indian Ocean.\n\nIn its 2002 draft, the IHO redefined the Indian Ocean, moving its southern limit to 60°S, with the waters south of that line identified as the Southern Ocean. This new definition has not yet been ratified (and, in addition, a reservation was lodged in 2003 by Australia.) While the name \"Southern Ocean\" is frequently used, some geographic authorities such as the 10th edition of the World Atlas from the U.S. National Geographic Society generally show the Atlantic, Indian, and Pacific Oceans continuing to Antarctica. If and when adopted, the 2002 definition would be published in the 4th edition of \"Limits of Oceans and Seas\", re-instituting the 2nd edition's \"Southern Ocean\", omitted from the 3rd edition.\n\nThe boundary of the Indian Ocean is a constitutional issue for Australia. The Imperial South Australia Colonisation Act, 1834, which established and defined the Colony of South Australia defined South Australia’s southern limit as being the “Southern Ocean.” This definition was carried through to Australian constitutional law upon the Federation of Australia in 1901.\n\nThe Pacific is the ocean that separates Asia and Australia from the Americas. It may be further subdivided by the equator into northern and southern portions.\n\nThe 3rd edition, currently in force, of the International Hydrographic Organization's (IHO) \"Limits of Oceans and Seas\" defines the limits of the North Pacific Ocean (excluding the seas it contains) as follows:\n\nThe 3rd edition, currently in force, of the International Hydrographic Organization's (IHO) \"Limits of Oceans and Seas\" defines the limits of the South Pacific Ocean (excluding the seas it contains) as follows:\n\nNote that these definitions exclude any marginal waterbodies that are separately defined by the IHO (such as the Gulf of Alaska and Coral Sea), though these are usually considered to be part of the Pacific Ocean.\n\nIn its 2002 draft, the IHO redefined the Pacific Ocean, moving its southern limit to 60°S, with the waters south of that line identified as the Southern Ocean. This new definition has not yet been ratified (and, in addition, a reservation was lodged in 2003 by Australia.) While the name \"Southern Ocean\" is frequently used, some geographic authorities such as the 10th edition of the World Atlas from the U.S. National Geographic Society generally show the Atlantic, Indian, and Pacific Oceans continuing to Antarctica. If and when adopted, the 2002 definition would be published in the 4th edition of \"Limits of Oceans and Seas\", re-instituting the 2nd edition's \"Southern Ocean\", omitted from the 3rd edition.\n\nThe Southern Ocean contains the waters that surround Antarctica and sometimes is considered an extension of Pacific, Atlantic and Indian Oceans.\n\nIn 1928, the first edition of the International Hydrographic Organization's (IHO) \"Limits of Oceans and Seas\" publication included the Southern Ocean around Antarctica. The Southern Ocean was delineated by land-based limits - the continent of Antarctica to the south, and the continents of South America, Africa, and Australia plus Broughton Island, New Zealand in the north. The detailed land-limits used were Cape Horn in South America, Cape Agulhas in Africa, the southern coast of Australia from Cape Leeuwin, Western Australia, to South East Cape, Tasmania, via the western edge of the water body of Bass Strait, and then Broughton Island before returning to Cape Horn.\n\nThe northern limits of the Southern Ocean were moved southwards in the IHO's 1937 second edition of the \"Limits of Oceans and Seas\". The Southern Ocean then extended from Antarctica northwards to latitude 40° south between Cape Agulhas in Africa (long. 20° east) and Cape Leeuwin in Western Australia (long. 115° east), and extended to latitude 55° south between Auckland Island of New Zealand (long. 165° or 166° east) and Cape Horn in South America (long. 67° west).\n\nThe Southern Ocean did not appear in the 1953 third edition because \"...\"the northern limits ... are difficult to lay down owing to their seasonal change ... Hydrographic Offices who issue separate publications dealing with this area are therefore left to decide their own northern limits. (Great Britain uses the Latitude of 55° South)\"\". Instead, in the IHO 1953 publication, the Atlantic, Indian and Pacific Oceans were extended southward, the Indian and Pacific Oceans (which had not previously touched pre 1953, as per the first and second editions) now abutted at the meridian of South East Cape, and the southern limits of the Great Australian Bight and the Tasman Sea were moved northwards.\n\nThe IHO readdressed the question of the Southern Ocean in a survey in 2000. Of its 68 member nations, 28 responded, and all responding members except Argentina agreed to redefine the ocean, reflecting the importance placed by oceanographers on ocean currents. The proposal for the name \"Southern Ocean\" won 18 votes, beating the alternative \"Antarctic Ocean\". Half of the votes supported a definition of the ocean's northern limit at 60°S (with no land interruptions at this latitude), with the other 14 votes cast for other definitions, mostly 50°S, but a few for as far north as 35°S.\n\nThe 4th edition of \"Limits of Oceans and Seas\" has yet to be published due to 'areas of concern' by several countries relating to various naming issues around the world. The IHB circulated a new draft of the 4th edition of the publication in August 2002, however there were still various changes, 60 seas were added or renamed from the 3rd edition, and even the name of the publication was changed. A reservation had also been lodged by Australia regarding the Southern Ocean limits. Effectively, the 3rd edition (which did not delineate the Southern Ocean leaving delineation to local hydrographic offices) has yet to be superseded and IHO documents declare that it remains \"currently in force.\"\n\nDespite this, the 4th edition definition has \"de facto\" usage by many organisations, scientists and nations - even at times by IHO committees. Some nations' hydrographic offices have defined their own boundaries; the United Kingdom used the 55°S parallel for example.\n\nOther sources, such as the National Geographic Society, show the Atlantic, Pacific and Indian Oceans as extending to Antarctica, although articles on the National Geographic web site have begun to reference the Southern Ocean.\n\nIn Australia, cartographic authorities defined the Southern Ocean as including the entire body of water between Antarctica and the south coasts of Australia and New Zealand. This delineation is basically the same as the original (first) edition of the IHO publication and effectively the same as the second edition. In the second edition, the Great Australian Bight was defined as the only geographical entity between the Australian coast and the Southern Ocean. Coastal maps of Tasmania and South Australia label the sea areas as \"Southern Ocean\", while Cape Leeuwin in Western Australia is described as the point where the Indian and Southern Oceans meet.\n\n"}
{"id": "515683", "url": "https://en.wikipedia.org/wiki?curid=515683", "title": "Carbanion", "text": "Carbanion\n\nA carbanion is an anion in which carbon is trivalent (forms three bonds) and bears a formal negative charge in at least one significant mesomeric contributor (resonance form). Absent π delocalization, carbanions assume a trigonal pyramidal, bent, or linear geometry when the carbanionic carbon is bound to three (e.g., methyl anion), two (e.g., phenyl anion), or one (e.g., acetylide anion) substituents, respectively. Formally, a carbanion is the conjugate base of a carbon acid: \n\nwhere B stands for the base. Carbanions have a concentration of electron density at the negatively charged carbon, which, in most cases, reacts efficiently with a variety of electrophiles of varying strengths, including carbonyl groups, halogenating reagents (e.g., \"N\"-bromosuccinimide and diiodine), and proton donors. A carbanion is one of several reactive intermediates in organic chemistry. In organic synthesis, organolithium reagents and Grignard reagents are commonly regarded as carbanions. This is a convenient approximation, although these species are almost always multinuclear clusters containing polar covalent bonds rather than true carbanions.\n\nCarbanions are typically nucleophilic and basic. The basicity and nucleophilicity of carbanions are determined by the substituents on carbon. These include\n\nGeometry also affects the orbital hybridization of the charge-bearing carbanion. The greater the s-character of the charge-bearing atom, the more stable the anion.\n\nOrganometallic reagents like butyllithium (hexameric cluster, [BuLi]) or methylmagnesium bromide (ether complex, MeMgBr(OEt)) are often referred to as \"carbanions,\" at least in a retrosynthetic sense. However, they are really clusters or complexes containing a polar covalent bond, though with electron density heavily polarized toward the carbon atom. In fact, true carbanions without stabilizing substituents are not available in the condensed phase, and these species must be studied in the gas phase.\n\nFor some time, it was not known whether simple alkyl anions could exist as free species; many theoretical studies predicted that even methanide anion CH should be an unbound species (i.e., the electron affinity of CH• was predicted to be negative). Such a species would decompose immediately by spontaneous ejection of an electron and would therefore be too fleeting to observe directly by mass spectrometry. However, in 1978, methyl anion was unambiguously synthesized by subjecting ketene to electric discharge, and the electron affinity (EA) of CH• was determined by photoelectron spectroscopy to be +1.8 kcal mol. The structure of CH was found to be pyramidal with a 1.3 kcal mol inversion barrier, while CH• was determined to be planar. Most simple primary, secondary and tertiary sp carbanions (e.g., CHCH, (CH)CH, and (CH)C) were subsequently determined to be unbound species (EA of CHCH•, (CH)CH•, (CH)C• = −6, –7.4, –3.6 kcal mol, respectively) indicating that α substitution is destabilizing. However, relatively modest stabilizing effects can render them bound. For example, cyclopropyl and cubyl anions are bound due to increased s character of the lone pair orbital, while neopentyl and phenethyl anion are also bound, as a result of negative hyperconjugation of the lone pair with the β-substituent (n → σ*). The same holds true for anions with benzylic and allylic stabilization. Gas-phase carbanions that are sp and sp hybridized are much more strongly stabilized and are often prepared directly by gas-phase deprotonation.\n\nIn the condensed phase only carbanions that are sufficiently stabilized by delocalization have been isolated as truly ionic species. In 1984, Olmstead and Power presented the lithium crown ether salt of the triphenylmethanide carbanion from triphenylmethane, n-butyllithium and 12-crown-4 (which forms a stable complex with lithium cations) at low temperatures:\n\nAdding \"n\"-butyllithium to triphenylmethane (p\"K\" in DMSO of CHPh = 30.6) in THF at low temperatures followed by 12-crown-4 results in a red solution and the salt complex [Li(12-crown-4)][CPh] precipitates at −20 °C. The central C–C bond lengths are 145 pm with the phenyl ring propellered at an average angle of 31.2°. This propeller shape is less pronounced with a tetramethylammonium counterion. A crystal structure for the analogous diphenylmethanide anion ([Li(12-crown-4)][CHPh]), prepared form diphenylmethane (p\"K\" in DMSO of CHPh = 32.3), was also obtained. However, the attempted isolation of a complex of the benzyl anion [CHPh] from toluene (p\"K\" in DMSO of CHPh ≈ 43) was unsuccessful, due to rapid reaction of the formed anion with the THF solvent.\n\nEarly in 1904 and 1917, Schlenk prepared two red-colored salts, formulated as [NMe][CPh] and [NMe][CHPh], respectively, by metathesis of the corresponding organosodium reagent with tetramethylammonium chloride. Since tetramethylammonium cations cannot form a chemical bond to the carbanionic center, these species are believed to contain free carbanions. While the structure of the former was verified by X-ray crystallography almost a century later, the instability of the latter has so far precluded structural verification. The reaction of the putative \"[NMe][CHPh]\" with water was reported to liberate toluene and tetramethylammonium hydroxide and provides indirect evidence for the claimed formulation.\n\nOne tool for the detection of carbanions in solution is proton NMR. A spectrum of cyclopentadiene in DMSO shows four vinylic protons at 6.5 ppm and two methylene bridge protons at 3 ppm whereas the cyclopentadienyl anion has a single resonance at 5.50 ppm. The use of Li and Li NMR has provided structural and reactivity data for a variety of organolithium species.\n\nAny compound containing hydrogen can, in principle, undergo deprotonation to form its conjugate base. A compound is a carbon acid if deprotonation results in loss of a proton from a carbon atom. Compared to compounds typically considered to be acids (e.g., mineral acids like nitric acid, or carboxylic acids like acetic acid), carbon acids are typically many orders of magnitude weaker, although exceptions exist (see below). For example, benzene is not an acid in the classical Arrhenius sense, since its aqueous solutions are neutral. Nevertheless, it is very weak Brønsted acid with an estimated p\"K\" of 49 which may undergo deprotonation in the presence of a superbase like the Lochmann–Schlosser base (\"n\"-BuLi:KO\"t\"-Bu). As conjugate acid-base pairs, the factors that determine the relative stability of carbanions also determine the ordering of the p\"K\" values of the corresponding carbon acids. Furthermore, p\"K\" values allow the prediction of whether a proton transfer process will be thermodynamically favorable: In order for the deprotonation of an acidic species HA with base B to be thermodynamically favorable (\"K\" > 1), the relationship p\"K\"(BH) > p\"K\"(AH) must hold.\n\nThese values below are p\"K\" values determined in DMSO, which has a broader useful range (~0 to ~35) than values determined in water (~0 to ~15) and better reflect the basicity of the carbanions in typical organic solvents. Values below less than 0 or greater than 35 are indirectly estimated; hence, the numerical accuracy of these values is limited. Aqueous p\"K\" values are also commonly encountered in the literature, particularly in the context of biochemistry and enzymology. Moreover, aqueous values are often given in introductory organic chemistry textbooks for pedagogical reasons, although the issue of solvent dependence is often glossed over. In general, p\"K\" values in water and organic solvent diverge significantly when the anion is capable of hydrogen bonding. For instance, in the case of water, the values differ dramatically: p\"K\" in water of water = 15.7, while p\"K\" in DMSO of water = 31.4, reflecting the differing ability of water and DMSO to stabilize hydroxide anion. On the other hand, for cyclopentadiene, the numerical values are comparable: p\"K\"(Cp-H) = 15, while p\"K\"(Cp-H) = 18. \n\nAs indicated by the examples above, acidity increases (p\"K\" decreases) when the negative charge is delocalized. This effect occurs when the substituents on the carbanion are unsaturated and/or electronegative. Although carbon acids are generally thought of as acids that are much weaker than \"classical\" Brønsted acids like acetic acid or phenol, the cumulative (additive) effect of several electron accepting substituents can lead to acids that are as strong or stronger than the inorganic mineral acids. For example, trinitromethane (HC(NO)), tricyanomethane (HC(CN)), pentacyanocyclopentadiene (HC(CN)), and fulminic acid (HCNO) are all strong acids with aqueous p\"K\" values that indicate complete or nearly complete proton transfer to water. Triflidic acid, with three strongly electron withdrawing triflyl groups, has an estimated p\"K\" well below –10. On the other end of the scale, hydrocarbons bearing only alkyl groups are thought to have p\"K\" values in the range of 55 to 65. The range of acid dissociation constants for carbon acids thus spans over 70 orders of magnitude.\n\nThe acidity of the α-hydrogen in carbonyl compounds crucially these compounds to participate in synthetically important C–C bond-forming reactions including the aldol reaction and Michael addition.\n\nWith the molecular geometry for a carbanion described as a trigonal pyramid the question is whether or not carbanions can display chirality, because if the activation barrier for inversion of this geometry is too low any attempt at introducing chirality will end in racemization, similar to the nitrogen inversion. However, solid evidence exists that carbanions can indeed be chiral for example in research carried out with certain organolithium compounds.\n\nThe first ever evidence for the existence of chiral organolithium compounds was obtained in 1950. Reaction of chiral 2-iodooctane with sec-butyllithium in petroleum ether at −70 °C followed by reaction with dry ice yielded mostly racemic 2-methylbutyric acid but also an amount of optically active 2-methyloctanoic acid which could only have formed from likewise optical active 2-methylheptyllithium with the carbon atom linked to lithium the carbanion:\n\nOn heating the reaction to 0 °C the optical activity is lost. More evidence followed in the 1960s. A reaction of the cis isomer of 2-methylcyclopropyl bromide with sec-butyllithium again followed by carboxylation with dry ice yielded cis-2-methylcyclopropylcarboxylic acid. The formation of the trans isomer would have indicated that the intermediate carbanion was unstable.\n\nIn the same manner the reaction of (+)-(\"S\")-\"l\"-bromo-\"l\"-methyl-2,2-diphenylcyclopropane with \"n\"-butyllithium followed by quench with methanol resulted in product with retention of configuration:\n\nOf recent date are chiral methyllithium compounds:\n\nThe phosphate 1 contains a chiral group with a hydrogen and a deuterium substituent. The stannyl group is replaced by lithium to intermediate 2 which undergoes a phosphate-phosphorane rearrangement to phosphorane 3 which on reaction with acetic acid gives alcohol 4. Once again in the range of −78 °C to 0 °C the chirality is preserved in this reaction sequence.\n\nA carbanionic structure first made an appearance in the reaction mechanism for the benzoin condensation as correctly proposed by Clarke and Arthur Lapworth in 1907. In 1904 Wilhelm Schlenk prepared PhCN in a quest for pentavalent nitrogen (from tetramethylammonium chloride and \nPhCNa) and in 1914 he demonstrated how triarylmethyl radicals could be reduced to carbanions by alkali metals The phrase carbanion was introduced by Wallis and Adams in 1933 as the negatively charged counterpart of the carbonium ion\n\n\n"}
{"id": "2351950", "url": "https://en.wikipedia.org/wiki?curid=2351950", "title": "Catty", "text": "Catty\n\nThe catty, kati or jin (commonly in China) , symbol 斤, is a traditional Chinese unit of mass used across East and Southeast Asia, notably for weighing food and other groceries in some wet markets, street markets, and shops. Related units include the picul, equal to 100 catties, and the tael (also spelled \"tahil\", in Malay/Indonesian), which is of a catty. A stone is a former unit used in Hong Kong equal to 120 catties and a \"gwan\" (鈞) is 30 catties. Catty or \"kati\" is still used in South East Asia as a unit of measurement in some contexts especially by the significant Overseas Chinese populations of Brunei, Indonesia, Malaysia, and Singapore.\n\nThe catty is traditionally equivalent to around 1⅓ pound avoirdupois, formalised as 604.78982 grams in Hong Kong, 604.79 grams in Malaysia and 604.8 grams in Singapore. In some countries, the weight has been rounded to 600 grams (Taiwan, Japan, Korea and Thailand). In mainland China, the catty (more commonly translated as jin within China) has been rounded to 500 grams and is referred to as the market catty (市斤 \"shìjīn\") in order to distinguish it from the \"metric catty\" (公斤 \"gōngjīn\"), or kilogram, and it is subdivided into 10 taels rather than the usual 16.\n\n"}
{"id": "58883", "url": "https://en.wikipedia.org/wiki?curid=58883", "title": "Cedrus", "text": "Cedrus\n\nCedrus (common English name cedar) is a genus of coniferous trees in the plant family Pinaceae (subfamily Abietoideae). They are native to the mountains of the western Himalayas and the Mediterranean region, occurring at altitudes of 1,500–3,200 m in the Himalayas and 1,000–2,200 m in the Mediterranean.\n\n\"Cedrus\" trees can grow up to 30–40 m (occasionally 60 m) tall with spicy-resinous scented wood, thick ridged or square-cracked bark, and broad, level branches. The shoots are dimorphic, with long shoots, which form the framework of the branches, and short shoots, which carry most of the leaves. The leaves are evergreen and needle-like, 8–60 mm long, arranged in an open spiral phyllotaxis on long shoots, and in dense spiral clusters of 15–45 together on short shoots; they vary from bright grass-green to dark green to strongly glaucous pale blue-green, depending on the thickness of the white wax layer which protects the leaves from desiccation. The seed cones are barrel-shaped, 6–12 cm long and 3–8 cm broad, green maturing grey-brown, and, as in \"Abies\", disintegrate at maturity to release the winged seeds. The seeds are 10–15 mm long, with a 20–30 mm wing; as in \"Abies\", the seeds have two or three resin blisters, containing an unpleasant-tasting resin, thought to be a defence against squirrel predation. Cone maturation takes one year, with pollination in autumn and the seeds maturing the same time a year later. The pollen cones are slender ovoid, 3–8 cm long, produced in late summer, and shedding pollen in autumn.\n\nCedars share a very similar cone structure with the firs (\"Abies\") and were traditionally thought to be most closely related to them, but molecular evidence supports a basal position in the family.\n\nThe five taxa of \"Cedrus\" are assigned according to taxonomic opinion to between one and four different species:\n\n\nCedars are adapted to mountainous climates; in the Mediterranean, they receive winter precipitation, mainly as snow, and summer drought, while in the western Himalaya, they receive primarily summer monsoon rainfall.\n\nCedars are used as food plants by the larvae of some Lepidoptera species including pine processionary and turnip moth (recorded on deodar cedar).\n\nCedars are very popular ornamental trees, widely used in horticulture in temperate climates where winter temperatures do not fall below about −25 °C. The Turkish cedar is slightly hardier, to −30 °C or just below. Extensive mortality of planted specimens can occur in severe winters where temperatures do drop lower. Areas with successful long-term cultivation include the entire Mediterranean region, western Europe north to the British Isles, southern Australia and New Zealand, and southern and western North America.\n\nCedar wood and cedar oil are known to be a natural repellent to moths, hence cedar is a popular lining for modern-day cedar chests and closets in which woolens are stored. This specific use of cedar is mentioned in \"The Iliad\" (Book 24), referring to the cedar-roofed or lined storage chamber where Priam goes to fetch treasures to be used as ransom. However, the species typically used for cedar chests and closets in North America is \"Juniperus virginiana\", which is different from the true cedars (note also common confusion with \"Thuja\" spp. below). Cedar is also commonly used to make shoe trees as it can absorb moisture and deodorise.\n\nMany species of cedar trees are suitable for training as bonsai. They work well with many styles, including formal and informal upright, slanting, and cascading.\n\nIn North America, species of the genus \"Thuja\", such as western red cedar, are commonly — though mistakenly — confused with genuine cedar, as is \"J. virginiana\", typically known as red cedar or eastern red cedar. While some naturalized species of cedar (\"Cedrus\", the true cedars) can be found in the Americas, no species is native.\nBoth the Latin word \"cedrus\" and the generic name \"cedrus\" are derived from Greek κέδρος \"kédros\". Ancient Greek and Latin used the same word, \"kédros\" and \"cedrus\", respectively, for different species of plants now classified in the genera \"Cedrus\" and \"Juniperus\" (juniper). Species of both genera are native to the area where Greek language and culture originated, though as the word \"kédros\" does not seem to be derived from any of the languages of the Middle East, it has been suggested the word may originally have applied to Greek species of juniper and was later adopted for species now classified in the genus \"Cedrus\" because of the similarity of their aromatic woods. The name was similarly applied to citron and the word citrus is derived from the same root. However, as a loan word in English, cedar had become fixed to its biblical sense of \"Cedrus\" by the time of its first recorded usage in AD 1000.\n\nThe name \"cedar\" has more recently (since about 1700) been applied to many other trees (such as western red cedar; in some cases the botanical name alludes to this usage, such as the genus \"Calocedrus\" (meaning \"beautiful cedar\"), also known as incense cedar). Such usage is regarded by some authorities as a misapplication of the name to be discouraged.\n\n"}
{"id": "26438129", "url": "https://en.wikipedia.org/wiki?curid=26438129", "title": "Centres of endemism", "text": "Centres of endemism\n\nA Centre of Endemism is an area in which the ranges of restricted-range species overlap, or a localised area which has a high occurrence of endemics. Centres of endemism may overlap with biodiversity hotspots which are biogeographic regions characterized both by high levels of plant endemism \"and by serious levels of habitat loss\". The exact delineation of centres of endemism is difficult and some overlap with one another. Centres of endemism are high conservation priority areas.\n\nTanzania\n\nA local centre of endemism is focussed on an area of lowland forests around the plateaux inland of Lindi in SE Tanzania, with between 40 and 91 species of vascular plants which are not found elseware.\n\nSouthern Africa\n\nThere are at least 19 centres of plant endemism, including the following:\n\n"}
{"id": "47708357", "url": "https://en.wikipedia.org/wiki?curid=47708357", "title": "Crown flash", "text": "Crown flash\n\nMentioned in Nature in 1971 and in a letter to Nature slightly earlier in the same year, this phenomenon is regarded as rare and not well documented. Starting in 2009 several YouTube videos have since emerged that appear to document this phenomenon.\n\n\n"}
{"id": "765457", "url": "https://en.wikipedia.org/wiki?curid=765457", "title": "Diethyl ether", "text": "Diethyl ether\n\nDiethyl ether, or simply ether, is an organic compound in the ether class with the formula , sometimes abbreviated as (see Pseudoelement symbols). It is a colorless, highly volatile flammable liquid. It is commonly used as a solvent in laboratories and as a starting fluid for some engines. It was formerly used as a general anesthetic, until non-flammable drugs were developed, such as halothane. It has been used as a recreational drug to cause intoxication.\n\nMost diethyl ether is produced as a byproduct of the vapor-phase hydration of ethylene to make ethanol. This process uses solid-supported phosphoric acid catalysts and can be adjusted to make more ether if the need arises. Vapor-phase dehydration of ethanol over some alumina catalysts can give diethyl ether yields of up to 95%.\n\nDiethyl ether can be prepared both in laboratories and on an industrial scale by the acid ether synthesis. Ethanol is mixed with a strong acid, typically sulfuric acid, HSO. The acid dissociates in the aqueous environment producing hydronium ions, HO. A hydrogen ion protonates the electronegative oxygen atom of the ethanol, giving the ethanol molecule a positive charge:\n\nA nucleophilic oxygen atom of unprotonated ethanol displaces a water molecule from the protonated (electrophilic) ethanol molecule, producing water, a hydrogen ion and diethyl ether.\n\nThis reaction must be carried out at temperatures lower than 150 °C in order to ensure that an elimination product (ethylene) is not a product of the reaction. At higher temperatures, ethanol will dehydrate to form ethylene. The reaction to make diethyl ether is reversible, so eventually an equilibrium between reactants and products is achieved. Getting a good yield of ether requires that ether be distilled out of the reaction mixture before it reverts to ethanol, taking advantage of Le Chatelier's principle.\n\nAnother reaction that can be used for the preparation of ethers is the Williamson ether synthesis, in which an alkoxide (produced by dissolving an alkali metal in the alcohol to be used) performs a nucleophilic substitution upon an alkyl halide.\n\nIt is particularly important as a solvent in the production of cellulose plastics such as cellulose acetate.\n\nDiethyl ether has a high cetane number of 85–96 and is used as a starting fluid, in combination with petroleum distillates for gasoline and Diesel engines because of its high volatility and low flash point. Ether starting fluid is sold and used in countries with cold climates, as it can help with cold starting an engine at sub-zero temperatures. For the same reason it is also used as a component of the fuel mixture for carbureted compression ignition model engines. In this way diethyl ether is very similar to one of its precursors, ethanol.\n\nDiethyl ether is a common laboratory aprotic solvent. It has limited solubility in water (6.05 g/100 ml at 25 °C) and dissolves 1.5 g/100 g (1.0 g/100 ml) water at 25 °C. This, coupled with its high volatility, makes it ideal for use as the non-polar solvent in liquid-liquid extraction. When used with an aqueous solution, the diethyl ether layer is on top as it has a lower density than the water. It is also a common solvent for the Grignard reaction in addition to other reactions involving organometallic reagents. Due to its application in the manufacturing of illicit substances, it is listed in the Table II precursor under the United Nations Convention Against Illicit Traffic in Narcotic Drugs and Psychotropic Substances as well as substances such as acetone, toluene and sulfuric acid.\n\nWilliam T. G. Morton participated in a public demonstration of ether anesthesia on October 16, 1846 at the Ether Dome in Boston, Massachusetts. However, Crawford Williamson Long, M.D., is now known to have demonstrated its use privately as a general anesthetic in surgery to officials in Georgia, as early as March 30, 1842, and Long publicly demonstrated ether's use as a surgical anesthetic on six occasions before the Boston demonstration. British doctors were aware of the anesthetic properties of ether as early as 1840 where it was widely prescribed in conjunction with opium. \nDiethyl ether largely supplanted the use of chloroform as a general anesthetic due to ether's more favorable therapeutic index, that is, a greater difference between an effective dose and a potentially toxic dose. \n\nDiethyl ether depresses the myocardium and increases tracheobronchial secretions. Diethyl ether could also be mixed with other anesthetic agents such as chloroform to make C.E. mixture, or chloroform and alcohol to make A.C.E. mixture. In the 21st century, ether is rarely used. The use of flammable ether was displaced by nonflammable fluorinated hydrocarbon anesthetics. Halothane was the first such anesthetic developed and other currently used inhaled anesthetics, such as isoflurane, desflurane, and sevoflurane, are halogenated ethers. Diethyl ether was found to have undesirable side effects, such as post-anesthetic nausea and vomiting. Modern anesthetic agents reduce these side effects.\n\nPrior to 2005 it was on the World Health Organization's List of Essential Medicines for use as an anesthetic.\n\nEther was once used in pharmaceutical formulations. A mixture of alcohol and ether, one part of diethyl ether and three parts of ethanol, was known as \"Spirit of ether\", Hoffman's Anodyne or Hoffman's Drops. In the United States this concoction was removed from the Pharmacopeia at some point prior to June 1917, as a study published by William Procter, Jr. in the \"American Journal of Pharmacy\" as early as 1852 showed that there were differences in formulation to be found between commercial manufacturers, between international pharmacopoeia, and from Hoffman’s original recipe.\n\nThe anesthetic and intoxicating effects of ether have made it a recreational drug. Diethyl ether in anesthetic dosage is an inhalant which has a long history of recreational use. One disadvantage is the high flammability, especially in conjunction with oxygen. One advantage is a well-defined margin between therapeutic and toxic doses, which means one would lose consciousness before dangerous levels of dissolved ether in blood would be reached. With a strong, dense smell, ether causes irritation to respiratory mucosa and is uncomfortable to breathe, and in overdose triggering salivation, vomiting, coughing or spasms. In concentrations of 3–5% in air, an anesthetic effect can slowly be achieved in 15–20 minutes of breathing approximately 15–20 ml of ether, depending on body weight and physical condition. Ether causes a very long excitation stage prior to blacking out.\n\nIn the 19th century and early 20th century ether drinking was popular among Polish peasants. It is a traditional and still relatively popular recreational drug among Lemkos. It is usually consumed in a small quantity (\"kropka\", or \"dot\") poured over milk, water with sugar or orange juice in a shot glass. As a drug, it has been known to cause temporary dependence, the only symptom of which is the will to consume more, sometimes referred to as etheromania.\n\nA cytochrome P450 enzyme is proposed to metabolize diethyl ether.\n\nDiethyl ether inhibits alcohol dehydrogenase, and thus slows the metabolism of ethanol. It also inhibits metabolism of other drugs requiring oxidative metabolism.\nFor example, diazepam requires hepatic oxidization whereas its oxidized metabolite oxazepam does not.\n\nDiethyl ether is extremely flammable and may form explosive vapour/air mixtures.\n\nSince ether is heavier than air it can collect low to the ground and the vapour may travel considerable distances to ignition sources, which does not need to be an open flame, but may be a hot plate, steam pipe, heater etc. Vapour may be ignited by the static electricity which can build up when ether is being poured from one vessel into another. The autoignition temperature of diethyl ether is . A common practice in chemical labs is to use steam (thus limiting the temperature to ) when ether must be heated or distilled. The diffusion of diethyl ether in air is (298 K, 101.325 kPa).\n\nEther is sensitive to light and air, tending to form explosive peroxides. Ether peroxides have a higher boiling point than ether and are contact explosives when dry. Commercial diethyl ether is typically supplied with trace amounts of the antioxidant butylated hydroxytoluene (BHT), which reduces the formation of peroxides. Storage over sodium hydroxide precipitates the intermediate ether hydroperoxides. Water and peroxides can be removed by either distillation from sodium and benzophenone, or by passing through a column of activated alumina.\n\nThe compound may have been synthesised by either Jābir ibn Hayyān in the 8th century or Ramon Llull in 1275. It was synthesized in 1540 by Valerius Cordus, who called it \"sweet oil of vitriol\" (\"oleum dulce vitrioli\")—the name reflects the fact that it is obtained by distilling a mixture of ethanol and sulfuric acid (then known as oil of vitriol)—and noted some of its medicinal properties. At about the same time, Paracelsus discovered ether's analgesic properties in chickens. The name \"ether\" was given to the substance in 1729 by August Sigmund Frobenius.\n\n"}
{"id": "15814300", "url": "https://en.wikipedia.org/wiki?curid=15814300", "title": "Dispatcher training simulator", "text": "Dispatcher training simulator\n\nA dispatcher training simulator (DTS), also known as an operator training simulator (OTS), is a computer-based training system for operators (known as dispatchers) of electrical power grids. It performs this role by simulating the behaviour of the electrical network forming the power system under various operating conditions, and its response to actions by the dispatchers. Student dispatchers may therefore develop their skills from exposure not only to routine operations but also to adverse operational situations without compromising the security of supply on a real transmission system.\n\nEarly simulations modelled the transmission system with banks of analog computers linked by scaled-down representations of the interconnecting lines. The operator would simulate the operation of circuit breakers by physically operating their miniature replicas. As transmission systems grew in size and complexity, they could no longer be adequately represented in this manner, and computerised simulations came to the fore.\n\nA modern DTS combines or simulates the following elements:\n\n\nA DTS is frequently purchased by a customer (such as a transmission system operator) at the same time and from the same manufacturer as an energy management system, and is usually designed to mimic it as closely as possible. Operational scenarios are created on the DTS to represent the operator's transmission system under a variety of conditions. These may represent normal operating conditions, or be specially designed to test the student's responses to adverse circumstances, such as frequent line trips during severe weather. The DTS is administered by a team of instructors, who select scenarios and simulate operational events, monitoring the trainee's actions in response.\n\nScenarios may also represent circumstances that the system operator hopes never occur, such as complete system shut-down, and allow it to develop strategies for restoration of service (known as black start).\n\nDeficiencies in operator training were identified as a contributory cause of the 2003 North American blackout, a factor similarly connected to earlier power failures. The joint US-Canadian task force investigating the incident recommended mandatory periods of simulation time for operators, and validation of the models against actual system characteristics.\n\nTo enable the training simulator to respond as realistically as possible to the student's actions, the power flow study at its heart must run on a frequent time basis, such as every few seconds. The simulation may model electrical networks consisting of many thousands of nodes and containing several hundred generating units.\n"}
{"id": "32668736", "url": "https://en.wikipedia.org/wiki?curid=32668736", "title": "District heating substation", "text": "District heating substation\n\nA district heating substation is a component in a district heating system that connects the main network to a building's own heating system.\n\nThe station normally has one, or more of the following parts:\n\nIn addition, a district heating substation may also include:\n"}
{"id": "11274", "url": "https://en.wikipedia.org/wiki?curid=11274", "title": "Elementary particle", "text": "Elementary particle\n\nIn particle physics, an elementary particle or fundamental particle is a with no substructure, thus not composed of other particles. Particles currently thought to be elementary include the fundamental fermions (quarks, leptons, antiquarks, and antileptons), which generally are \"matter particles\" and \"antimatter particles\", as well as the fundamental bosons (gauge bosons and the Higgs boson), which generally are \"force particles\" that mediate interactions among fermions. A particle containing two or more elementary particles is a \"composite particle\".\n\nEveryday matter is composed of atoms, once presumed to be matter's elementary particles—\"atom\" meaning \"unable to cut\" in Greek—although the atom's existence remained controversial until about 1910, as some leading physicists regarded molecules as mathematical illusions, and matter as ultimately composed of energy. Soon, subatomic constituents of the atom were identified. As the 1930s opened, the electron and the proton had been observed, along with the photon, the particle of electromagnetic radiation. At that time, the recent advent of quantum mechanics was radically altering the conception of particles, as a single particle could seemingly span a field as would a wave, a paradox still eluding satisfactory explanation.\n\nVia quantum theory, protons and neutrons were found to contain quarks—up quarks and down quarks—now considered elementary particles. And within a molecule, the electron's three degrees of freedom (charge, spin, orbital) can separate via the wavefunction into three quasiparticles (holon, spinon, orbiton). Yet a free electron—which is not orbiting an atomic nucleus and lacks orbital motion—appears unsplittable and remains regarded as an elementary particle.\n\nAround 1980, an elementary particle's status as indeed elementary—an \"ultimate constituent\" of substance—was mostly discarded for a more practical outlook, embodied in particle physics' Standard Model, what's known as science's most experimentally successful theory. Many elaborations upon and theories beyond the Standard Model, including the popular supersymmetry, double the number of elementary particles by hypothesizing that each known particle associates with a \"shadow\" partner far more massive, although all such superpartners remain undiscovered. Meanwhile, an elementary boson mediating gravitation—the graviton—remains hypothetical.\n\nAll elementary particles are—depending on their \"spin\"—either bosons or fermions. These are differentiated via the spin–statistics theorem of quantum statistics. Particles of \"half-integer\" spin exhibit Fermi–Dirac statistics and are fermions. Particles of \"integer\" spin, in other words full-integer, exhibit Bose–Einstein statistics and are bosons.\n\nIn the Standard Model, elementary particles are represented for predictive utility as point particles. Though extremely successful, the Standard Model is limited to the microcosm by its omission of gravitation and has some parameters arbitrarily added but unexplained.\n\nAccording to the current models of big bang nucleosynthesis, the primordial composition of visible matter of the universe should be about 75% hydrogen and 25% helium-4 (in mass). Neutrons are made up of one up and two down quarks, while protons are made of two up and one down quark. Since the other common elementary particles (such as electrons, neutrinos, or weak bosons) are so light or so rare when compared to atomic nuclei, we can neglect their mass contribution to the observable universe's total mass. Therefore, one can conclude that most of the visible mass of the universe consists of protons and neutrons, which, like all baryons, in turn consist of up quarks and down quarks.\n\nSome estimates imply that there are roughly baryons (almost entirely protons and neutrons) in the observable universe.\n\nThe number of protons in the observable universe is called the Eddington number.\n\nIn terms of number of particles, some estimates imply that nearly all the matter, excluding dark matter, occurs in neutrinos, and that roughly elementary particles of matter exist in the visible universe, mostly neutrinos. Other estimates imply that roughly elementary particles exist in the visible universe (not including dark matter), mostly photons and other massless force carriers.\n\nThe Standard Model of particle physics contains 12 flavors of elementary fermions, plus their corresponding antiparticles, as well as elementary bosons that mediate the forces and the Higgs boson, which was reported on July 4, 2012, as having been likely detected by the two main experiments at the Large Hadron Collider (ATLAS and CMS). However, the Standard Model is widely considered to be a provisional theory rather than a truly fundamental one, since it is not known if it is compatible with Einstein's general relativity. There may be hypothetical elementary particles not described by the Standard Model, such as the graviton, the particle that would carry the gravitational force, and sparticles, supersymmetric partners of the ordinary particles.\n\nThe 12 fundamental fermions are divided into 3 generations of 4 particles each. Half of the fermions are leptons, three of which have an electric charge of −1, called the electron (), the muon (), and the tau (), the other three leptons are neutrinos (, , ), which are the only elementary fermions with no electric or color charge. The remaining six particles are quarks (discussed below).\n\nThe following table lists current measured masses and mass estimates for all the fermions, using the same scale of measure: millions of electron-volts (MeV). For example, the most accurately known quark mass is of the top quark () at 172.7 GeV/c² or 172 700 MeV/c², estimated using the On-shell scheme.\n\nEstimates of the values of quark masses depend on the version of quantum chromodynamics used to describe quark interactions. Quarks are always confined in an envelope of gluons which confer vastly greater mass to the mesons and baryons where quarks occur, so values for quark masses cannot be measured directly. Since their masses are so small compared to the effective mass of the surrounding gluons, slight differences in the calculation make large differences in the masses.\n\nThere are also 12 fundamental fermionic antiparticles that correspond to these 12 particles. For example, the antielectron (positron) \"\" is the electron's antiparticle and has an electric charge of +1.\n\nIsolated quarks and antiquarks have never been detected, a fact explained by confinement. Every quark carries one of three color charges of the strong interaction; antiquarks similarly carry anticolor. Color-charged particles interact via gluon exchange in the same way that charged particles interact via photon exchange. However, gluons are themselves color-charged, resulting in an amplification of the strong force as color-charged particles are separated. Unlike the electromagnetic force, which diminishes as charged particles separate, color-charged particles feel increasing force.\n\nHowever, color-charged particles may combine to form color neutral composite particles called hadrons. A quark may pair up with an antiquark: the quark has a color and the antiquark has the corresponding anticolor. The color and anticolor cancel out, forming a color neutral meson. Alternatively, three quarks can exist together, one quark being \"red\", another \"blue\", another \"green\". These three colored quarks together form a color-neutral baryon. Symmetrically, three antiquarks with the colors \"antired\", \"antiblue\" and \"antigreen\" can form a color-neutral antibaryon.\n\nQuarks also carry fractional electric charges, but, since they are confined within hadrons whose charges are all integral, fractional charges have never been isolated. Note that quarks have electric charges of either + or −, whereas antiquarks have corresponding electric charges of either − or +.\n\nEvidence for the existence of quarks comes from deep inelastic scattering: firing electrons at nuclei to determine the distribution of charge within nucleons (which are baryons). If the charge is uniform, the electric field around the proton should be uniform and the electron should scatter elastically. Low-energy electrons do scatter in this way, but, above a particular energy, the protons deflect some electrons through large angles. The recoiling electron has much less energy and a jet of particles is emitted. This inelastic scattering suggests that the charge in the proton is not uniform but split among smaller charged particles: quarks.\n\nIn the Standard Model, vector (spin-1) bosons (gluons, photons, and the W and Z bosons) mediate forces, whereas the Higgs boson (spin-0) is responsible for the intrinsic mass of particles. Bosons differ from fermions in the fact that multiple bosons can occupy the same quantum state (Pauli exclusion principle). Also, bosons can be either elementary, like photons, or a combination, like mesons. The spin of bosons are integers instead of half integers.\n\nGluons mediate the strong interaction, which join quarks and thereby form hadrons, which are either baryons (three quarks) or mesons (one quark and one antiquark). Protons and neutrons are baryons, joined by gluons to form the atomic nucleus. Like quarks, gluons exhibit color and anticolor—unrelated to the concept of visual color—sometimes in combinations, altogether eight variations of gluons.\n\nThere are three weak gauge bosons: \"W\", \"W\", and \"Z\"; these mediate the weak interaction. The W bosons are known for their mediation in nuclear decay. The \"W\" converts a neutron into a proton then decay into an electron and electron antineutrino pair. The \"Z\" does not convert charge but rather changes momentum and is the only mechanism for elastically scattering neutrinos. The weak gauge bosons were discovered due to momentum change in electrons from neutrino-Z exchange. The massless photon mediates the electromagnetic interaction. These four gauge bosons form the electroweak interaction among elementary particles.\n\nAlthough the weak and electromagnetic forces appear quite different to us at everyday energies, the two forces are theorized to unify as a single electroweak force at high energies. This prediction was clearly confirmed by measurements of cross-sections for high-energy electron-proton scattering at the HERA collider at DESY. The differences at low energies is a consequence of the high masses of the \"W\" and \"Z\" bosons, which in turn are a consequence of the Higgs mechanism. Through the process of spontaneous symmetry breaking, the Higgs selects a special direction in electroweak space that causes three electroweak particles to become very heavy (the weak bosons) and one to remain massless (the photon). On 4 July 2012, after many years of experimentally searching for evidence of its existence, the Higgs boson was announced to have been observed at CERN's Large Hadron Collider. Peter Higgs who first posited the existence of the Higgs boson was present at the announcement. The Higgs boson is believed to have a mass of approximately 125 GeV. The statistical significance of this discovery was reported as 5-sigma, which implies a certainty of roughly 99.99994%. In particle physics, this is the level of significance required to officially label experimental observations as a discovery. Research into the properties of the newly discovered particle continues.\n\nThe graviton is a hypothetical elementary spin-2 particle proposed to mediate gravitation. While it remains undiscovered due to the difficulty inherent in its detection, it is sometimes included in tables of elementary particles. The conventional graviton is massless, although there exist models containing massive Kaluza–Klein gravitons.\n\nAlthough experimental evidence overwhelmingly confirms the predictions derived from the Standard Model, some of its parameters were added arbitrarily, not determined by a particular explanation, which remain mysteries, for instance the hierarchy problem. Theories beyond the Standard Model attempt to resolve these shortcomings.\n\nOne extension of the Standard Model attempts to combine the electroweak interaction with the strong interaction into a single 'grand unified theory' (GUT). Such a force would be spontaneously broken into the three forces by a Higgs-like mechanism. The most dramatic prediction of grand unification is the existence of X and Y bosons, which cause proton decay. However, the non-observation of proton decay at the Super-Kamiokande neutrino observatory rules out the simplest GUTs, including SU(5) and SO(10).\n\nSupersymmetry extends the Standard Model by adding another class of symmetries to the Lagrangian. These symmetries exchange fermionic particles with bosonic ones. Such a symmetry predicts the existence of supersymmetric particles, abbreviated as \"sparticles\", which include the sleptons, squarks, neutralinos, and charginos. Each particle in the Standard Model would have a superpartner whose spin differs by from the ordinary particle. Due to the breaking of supersymmetry, the sparticles are much heavier than their ordinary counterparts; they are so heavy that existing particle colliders would not be powerful enough to produce them. However, some physicists believe that sparticles will be detected by the Large Hadron Collider at CERN.\n\nString theory is a model of physics where all \"particles\" that make up matter are composed of strings (measuring at the Planck length) that exist in an 11-dimensional (according to M-theory, the leading version) or 12-dimensional (according to F-theory) universe. These strings vibrate at different frequencies that determine mass, electric charge, color charge, and spin. A string can be open (a line) or closed in a loop (a one-dimensional sphere, like a circle). As a string moves through space it sweeps out something called a \"world sheet\". String theory predicts 1- to 10-branes (a 1-brane being a string and a 10-brane being a 10-dimensional object) that prevent tears in the \"fabric\" of space using the uncertainty principle (e.g., the electron orbiting a hydrogen atom has the probability, albeit small, that it could be anywhere else in the universe at any given moment).\n\nString theory proposes that our universe is merely a 4-brane, inside which exist the 3 space dimensions and the 1 time dimension that we observe. The remaining 7 theoretical dimensions either are very tiny and curled up (and too small to be macroscopically accessible) or simply do not/cannot exist in our universe (because they exist in a grander scheme called the \"multiverse\" outside our known universe).\n\nSome predictions of the string theory include existence of extremely massive counterparts of ordinary particles due to vibrational excitations of the fundamental string and existence of a massless spin-2 particle behaving like the graviton.\n\nTechnicolor theories try to modify the Standard Model in a minimal way by introducing a new QCD-like interaction. This means one adds a new theory of so-called Techniquarks, interacting via so called Technigluons. The main idea is that the Higgs-Boson is not an elementary particle but a bound state of these objects.\n\nAccording to preon theory there are one or more orders of particles more fundamental than those (or most of those) found in the Standard Model. The most fundamental of these are normally called preons, which is derived from \"pre-quarks\". In essence, preon theory tries to do for the Standard Model what the Standard Model did for the particle zoo that came before it. Most models assume that almost everything in the Standard Model can be explained in terms of three to half a dozen more fundamental particles and the rules that govern their interactions. Interest in preons has waned since the simplest models were experimentally ruled out in the 1980s.\n\nAccelerons are the hypothetical subatomic particles that integrally link the newfound mass of the neutrino to the dark energy conjectured to be accelerating the expansion of the universe.\n\nIn theory, neutrinos are influenced by a new force resulting from their interactions with accelerons. Dark energy results as the universe tries to pull neutrinos apart.\n\n\n\nThe most important address about the current experimental and theoretical knowledge about elementary particle physics is the Particle Data Group, where different international institutions collect all experimental data and give short reviews over the contemporary theoretical understanding.\n\nother pages are:\n\n"}
{"id": "14760584", "url": "https://en.wikipedia.org/wiki?curid=14760584", "title": "Fluoroposs", "text": "Fluoroposs\n\nFluoroPOSS (Fluorinated Polyhedral Oligomeric Silsesquioxanes) is a synthetic microfiber with very low surface energy, which makes it oil-repellent. Mixed with an ordinary polymer, it forms a material which can be applied to other materials such as metal, glass, plastic, plant fibers or leaves. The application process is called electrospinning.\n\nFluoroPOSS has been developed at the Massachusetts Institute of Technology (MIT).\n\n\n"}
{"id": "20585347", "url": "https://en.wikipedia.org/wiki?curid=20585347", "title": "Fábio Rosa", "text": "Fábio Rosa\n\nFábio Luiz de Oliveira Rosa (Porto Alegre, 1960) is a Brazilian social entrepreneur whose initiatives have focused on rural electrification and the use of renewable energy sources.\n\nRosa attended the Federal University of Rio Grande do Sul in Porto Alegre, where he graduated with a degree in agronomic engineering. In 1982, after leaving school, Rosa was invited by a classmate to visit the Palmares do Sul area of the southern Brazilian state, Rio Grande do Sul. Unbeknownst to Rosa at the time, his colleague's father, Ney Azevedo, was the current mayor of Palmares and in speaking with Rosa during this visit, Azevedo was so impressed that he offered him the position of secretary of agriculture.\n\nSeeking ways to improve life in the region, Rosa interviewed residents who noted the ongoing economic pressure driving rural flight from Palmares. Rosa saw that high water costs were a significant impediment for local rice farmers and that lowering these costs could improve productivity and wealth. To address the high water costs, Rosa suggested expanding rural electrification to allow more farmers to drop their own wells for irrigation rather than paying exorbitant fees for water rights held by others.\n\nAt the time, 70 percent of rural residents in Palmares were without electricity. Rosa favored expanding access to electricity through an inexpensive mono-phase electrification system developed by Professor Ennio Amaral of the local Federal Technical School of Pelotas. The use of mono-phase technology reduced the costs for distributing electricity to each of these rural households from $7,000 to $400.\n\nUsing this system, Rosa instituted a successful pilot project that took place between 1984 and 1988. In 1988, a new mayor of Palmares was elected, who ended the program.\n\nIn 1989, Rosa was named a fellow by and using the organization's stipend, he continued his efforts to expand rural electrification. In 1990, along with colleague Ricardo de Souza Mello, Rosa began work on Pro Luz (Project Light) to expand the Palmares model throughout Brazil. The program proved successful and the Brazilian government was receptive, even offering a dedicated line of credit for rural electrification programs. However, in 1992, Brazilian economic troubles led the government to reduce social spending on these types of projects. The repeated difficulties Rosa faced working through government led him to focus future efforts through private organizations.\n\nIn 1992, Rosa established Sistemas de Tecnologia Agroelectro (STA) to popularize solar energy. He recognized that the critical barrier to the adoption of solar energy was its high cost. Thus, he worked to make it attractive by pairing it with electric fencing, which could help address Brazil's problems with overgrazing. Within several years STA had installed 700 solar powered fencing systems and Rosa \"had gained national recognition as a leader in the delivery of low-cost solar energy\".\n\nRosa left STA in 2001 to expand his work with a non-profit he established called the Instituto Para O Desenvolvimento De Energias Alternativas E Da Auto Sustentabilidade (IDEAAS). IDEAAS was designed to take the basic STA framework and apply it as a nonprofit model in lower income areas.\n\nThrough market research and consultation with various experts, Rosa designed two enterprises using solar technology that could work through STA and IDEAAS, respectively: The Sun Shines for All and The Quiron Project. The Sun Shines for All is a for-profit solar energy venture administered through STA that leases comprehensive electrification packages including solar kits and outlets to those without access to the grid. The Quiron Project is a non-profit run through IDEEAS that is designed to improve the well-being of the rural poor in an environmentally responsible manner through solar electrification, improved agricultural management, and conservation.\n\n\n"}
{"id": "644391", "url": "https://en.wikipedia.org/wiki?curid=644391", "title": "Gluino", "text": "Gluino\n\nIn supersymmetry, a gluino (symbol ) is the hypothetical supersymmetric partner of a gluon. Should they exist, gluinos are expected by supersymmetry theorists to be pair produced in particle accelerators such as the Large Hadron Collider.\n\nIn supersymmetric theories, gluinos are Majorana fermions and interact via the strong force as a color octet. Gluinos have a lepton number 0, baryon number 0, and spin 1/2.\n\nIn models of supersymmetry that conserve R-parity, gluinos decay via the strong interaction to a squark and a quark, provided that an appropriate mass relation is satisfied.\nThe squark subsequently decays to another quark and the lightest supersymmetric particle, LSP (which leaves the detector unseen). This means that a typical signal for a gluino at a hadron collider would be four jets plus missing energy.\n\nHowever, if gluinos are lighter than squarks, 3-body decay of a gluino to a neutralino and a quark antiquark pair is kinematically accessible through an off-shell squark.\n"}
{"id": "1191076", "url": "https://en.wikipedia.org/wiki?curid=1191076", "title": "Graviscalar", "text": "Graviscalar\n\nIn theoretical physics, the hypothetical particle called the graviscalar or radion emerges as an excitation of general relativity's metric tensor, i.e. gravitational field, but is indistinguishable from a scalar in four dimensions, as shown in Kaluza–Klein theory. The scalar field formula_1 comes from a component of the metric tensor formula_2 where the figure 5 labels an additional fifth dimension. The only variations in the scalar field represent variations in the size of the extra dimension. Also, in models with multiple extra dimensions, there exist several such particles. Moreover, in theories with extended supersymmetry, a graviscalar is usually a superpartner of the graviton that behaves as a particle with spin 0. This concept closely relates to the gauged Higgs models.\n\n\n"}
{"id": "14275961", "url": "https://en.wikipedia.org/wiki?curid=14275961", "title": "Great ellipse", "text": "Great ellipse\n\nA great ellipse is an ellipse passing through two points on a spheroid and having the same center as that of the spheroid. Equivalently, it is an ellipse on the surface of a spheroid and centered on the origin, or the curve formed by intersecting the spheroid by a plane through its center.\nFor points that are separated by less than about a quarter of the circumference of the earth, about formula_1, the length of the great ellipse connecting the points is close (within one part in 500,000) to the geodesic distance.\n\nThe great ellipse therefore is sometimes proposed as a suitable route for marine navigation, although for no extra computational effort the more accurate normal section may be computed.\n\nAssume that the spheroid, an ellipsoid of revolution, has an equatorial radius formula_2 and polar semi-axis formula_3. Define the flattening formula_4, the eccentricity formula_5, and the second eccentricity formula_6. Consider two points: formula_7 at (geographic) latitude formula_8 and longitude formula_9 and formula_10 at latitude formula_11 and longitude formula_12. The connecting great ellipse (from formula_7 to formula_10) has length formula_15 and has azimuths formula_16 and formula_17 at the two endpoints.\n\nThere are various ways to map an ellipsoid into a sphere of radius formula_2 in such a way as to map the great ellipse into a great circle, allowing the methods of great-circle navigation to be used:\n\nThe last method gives an easy way to generate a succession of way-points on the great ellipse connecting two known points formula_7 and formula_10. Solve for the great circle between formula_27 and formula_28 and find the way-points on the great circle. These map into way-points on the corresponding great ellipse.\n\nIf distances and headings are needed, it is simplest to use the first of the mappings. In detail, the mapping is as follows (this description is taken from ):\n\n\nThe \"indirect\" (or \"inverse problem\") is the determination of formula_15, formula_16, and formula_17, given the positions of formula_7 and formula_10.\n\nThe \"direct problem\", is the determination of the position of formula_10 and formula_17, given formula_7, formula_16, and formula_15.\n\nBoth the inverse and direct geodetic problems for the great ellipse may be solved by using the method in Earth section paths, and setting formula_59 = the position vector of A. \n\nThe inverse problem is solved by computing formula_60 and formula_61 and solving for the great-circle between formula_62 and formula_63.\nThe spherical azimuths are relabeled as formula_34 (from formula_33). Thus formula_42, formula_67, and formula_68 and the spherical azimuths at the equator and at formula_7 and formula_10. The azimuths of the endpoints of great ellipse, formula_16 and formula_17, are computed from formula_67 and formula_68.\nThe semi-axes of the great ellipse can be found using the value of formula_42.\n\nAlso determined as part of the solution of the great circle problem are the arc lengths, formula_76 and formula_77, measured from the equator crossing to formula_7 and formula_10. The distance formula_15 is found by computing the length of a portion of perimeter of the ellipse using the formula giving the meridian arc in terms the parametric latitude. In applying this formula, use the semi-axes for the great ellipse (instead of for the meridian) and substitute formula_76 and formula_77 for formula_20.\n\nThe solution of the \"direct problem\", can be similarly be found (this requires, in addition, the inverse meridian distance formula). This also enables way-points (e.g., a series of equally spaced intermediate points) to be found in the solution of the inverse problem.\n\n\n"}
{"id": "56399711", "url": "https://en.wikipedia.org/wiki?curid=56399711", "title": "Großwelzheim Nuclear Power Plant", "text": "Großwelzheim Nuclear Power Plant\n\nGroßwelzheim Nuclear Power Plant, (), was an experimental nuclear power plant consisting of one 25 MW reactor in Großwelzheim, a district of Karlstein am Main.\n\nThe prototype boiling water reactor, designed to produce superheated steam was under construction from 1965 to 1969, and was first connected to the power grid on October 14th, 1969. The reactor could not be run at full capacity due to structural defects in the fuel elements. Because of this, the reactor was switched off about a year and a half later on April 20, 1970.\n\nAfter the decommissioning in 1983, the plant was used for reactor safety tests until dismantling was completed in 1998. \n\nThe site also included the Kahl Nuclear Power Plant, the first nuclear reactor to be built in Germany, as well as the first to be shut down. This makes the site the first in Germany where two reactors have been dismantled. \n\nWhen the community of Karlstein am Main was created in 1975, an atomic symbol was included in the municipal coat of arms, due to the two reactors present in the newly formed town. \n\nGroßwelzheim Nuclear Power Plant had one Reactor:\n"}
{"id": "42379111", "url": "https://en.wikipedia.org/wiki?curid=42379111", "title": "Gulpur Hydropower Project", "text": "Gulpur Hydropower Project\n\nGulpur Hydropower Plant (GHPP) is an under construction, run-of-the-river hydroelectric generation project located on Poonch River, a major tributary of Jhelum River near Gulpur in Kotli District of Azad Kashmir, Pakistan. The site is about 167 km from Federal Capital Islamabad and from Punjab's Provincial Capital Lahore and is approachable directly from Islamabad and Lahore by a two-lane, all weather paved, partly mountainous road. The location of the Project is about 28 km upstream of Mangla Dam Reservoir.\nGulpur Hydropower Project is a run-of-the-river project designed for the generation of consisting of two Kaplan-type turbine units with average energy output of 436 Gwh. Gulpur Hydropower Project is a part of least-cost energy generation plan, being executed by WAPDA to harness the indigenous hydropower resources of the country. WAPDA awarded the contract to MIRA Power Limited, a subsidiary of Korean company KOSEP, an independent power producer (IPP) being developed in private sector on BOOT basis (Build, Own, Operate and Transfer) under Government of Pakistan Policy for Power Generation Projects 2002 as adopted in Azad Jammu & Kashmir. The project has the capability of generating an average annual energy of 465 GWh.\n\nGulpur Hydropower Project is the third independent hydropower project in Pakistan. Korean company has also been awarded other contracts to build hydropower plants in the country on BOOT basis i.e. under construction 150 MW Patrind Hydropower Plant and 100MW Kotli Hydropower Project.\n\nThe sponsor of the project is Korean South East Power Company along with Daelim Industrial Co., Ltd. and Lotte Engineering and Construction Co., Ltd.. The Construction of the Project has been awarded to a joint venture between Daelim and Lotte.\n\nConstruction of Gulpur Hydropower Plant commenced in September 2014 and the project is expected to be completed in early 2019. The total cost of the project is about PKR 34 billion. Gulpur Hydropower Project is part of the least-cost energy generation plan of the Government of Pakistan. It is being executed on priority basis to harness indigenous hydropower resources to improve the ratio of hydel electricity in the national grid and help provide relief to the consumers.\n\nOn its completion, Gulpur Hydropower project will generate about 465 Gwh of inexpensive electricity annually to earn revenue of about Rs 1 billion. Being an environment friendly hydropower project, it will help reduce dependence on expensive thermal power, thereby saving foreign exchange amounting to $36 million (equivalent to Rs 3 billion) to the country. According to estimation, Gulpur hydropower project will add about Rs 10 billion per annum to the national economy through socio-economic uplift in the country caused by the project. The Asian Development Bank (ADB) is providing the loan to Mira Power, run by three Korean companies, which will build and operate a 102MW hydropower plant.\n\nBasin and Reservoir<br>\n◦ Catchment Area 3,625㎢ <br>\n◦ NHWL EL.540.0m <br>\n◦ L.W.L EL.538.0m <br>\n◦ 100yr frequency flood 13,340㎥/s <br>\n◦ PMF 21,640㎥/s <br>\n◦ Reservoir Capacity(NHWL) 21.893 million㎥\n\nMain Dam<br>\n◦ Type CGD <br>\n◦ Design Flood 21,640㎥/s(PMF)<br> \n◦ Height 35.0m <br>\n◦ Crest EL.545.0m <br>\n◦ Length 238.0m\n\nSpillway<br>\n◦ Radial Gate Size W10.0m×7Gates (EL.518.0m) <br>\n◦ Under Sluice Gate Size W10.0m×4Gates (EL.515.0m) <br>\n◦ Energy Dissipation Submerged Bucket\n\nRiver Diversion<br>\n◦ Type Partial cofferdam(1st) + Sheet File(2nd)<br> \n◦ Design Flood 2,517㎥/s (25yr frequency)\n\nIntake Weir<br>\n◦ Gate Nos. 2 EA <br>\n◦ Gate Size 10.0 × 12.0m\n\nHeadrace Tunnel<br>\n◦ Size D=7.75m <br>\n◦ Type Modified horseshoe(Concrete lined) <br>\n◦ Length 3,063.5m\n\nSurge Tank<br>\n◦ Type Simple Type <br>\n◦ Size D=29.0m(Circular) <br>\n◦ Height 54.0m <br>\n◦ Max Upsurging Level EL.556.523m <br>\n◦ Max Downsurging Level EL.525.986m\n\nVertical Pressure Tunnel<br>\n◦ Type Circular (Concrete Lined)<br> \n◦ Length 44.6m\n\nHorizontal Pressure Tunnel<br>\n◦ Type Circular (Concrete Lined) <br>\n◦ Length 71.7m\n\nSteel Penstock<br>\n◦ Type Circular (Steel Lined) <br>\n◦ Length 11.2m × 1EA 28.9m × 3EA\n\nPowerhouse<br>\n◦ Size W74.0m × L31.5m <br>\n◦ Turbine Type Francis <br>\n◦ Installed Capacity 100MW <br>\n◦ Annual Generation 465 GWh <br>\n◦ Plant Discharge 194㎥/s\n\nTailrace Channel<br>\n◦ Type Open Channel<br> \n◦ Length 50.0m <br>\n◦ Width 45.0m\n\n"}
{"id": "22799475", "url": "https://en.wikipedia.org/wiki?curid=22799475", "title": "Hamilton Ecological District", "text": "Hamilton Ecological District\n\nHamilton Ecological District is part of the Waikato Ecological Region in New Zealand's North Island. It occupies the Hamilton basin and surrounding foothills, and has been heavily modified with less than two percent of its indigenous vegetation remaining. This location has been studied significantly including the process of restoration ecology. \n\nC. Michael Hogan has classified the undisturbed portions of the woodland area as a beech and podocarp forest with associate understory ferns being \"Blechnum filiforme\", \"Asplenium flaccidum\", \"Doodia media\", \"Hymenophyllum demissum\", \"Microsorum pustulatum\" and \"Microsorum scandens\", and some prominent associate shrubs being \"Olearia ranii\" and \"Alseuosmia quercifolia\".\n\n"}
{"id": "26614014", "url": "https://en.wikipedia.org/wiki?curid=26614014", "title": "IBC SOLAR", "text": "IBC SOLAR\n\nIBC Solar is a Germany based photovoltaics specialist, offering solutions for sunlight-generated power. The company, established in 1982, offers tailored solutions, project management, consultation and planning of the photovoltaic installation.\n\nFounder and CEO of the company, Udo Möhrstedt, was named Entrepreneur of the Year 2009 in the trade category by Ernst & Young.\n\nIBC Solar has seven regional companies in Germany, the Netherlands, Turkey, South Africa, India, Japan and Singapore. In 2017, the company generated a consolidated global turnover of 344 million Euro. \n\nTheir distribution network for solar products covers over 1.000 distributors and wholesalers, across over 30 different countries.\n\nIBC Solar markets modules and components of renowned manufacturers and also distributes its own product line through domestic retail partners. While a big part of its business is generated through a network of local installation partners, the company also specializes in the planning and implementation of photovoltaic systems for large commercial clients, as well as subsequent monitoring of the systems.\n\nFor example, the company has installed a PV power station with a peak output of 13.2 Megawatt in Spain near Alicante, which has been producing electricity since 2008. This project was conducted together with the electricity provider Enercoop. For Greenpeace Energy, a German green energy utility provider, IBC Solar planned and implemented one of the largest PV roof systems in the world on the Stuttgart trade fair center (Messe Stuttgart).\n\nIBC Solar AG has long term contracts with solar cell manufactures as well as solar panel suppliers. On a test field near the company headquarters, the company tests the long term efficiency and reliability of the modules and inverters in its portfolio. Based on the findings from its long term testing, IBC SOLAR has defined criteria for modules and inverters with manufactures and markets these product under their own brand.\n\nIBC Solar’s international business has been supported by the German energy agency’s solar roof program. By providing German companies with the opportunity to show their expertise in beacon projects in international markets, dena supports the market entry of German companies. Companies interested in international projects have to apply for one of the projects the energy agency award each year. As part of the solar roof program, IBC SOLAR has installed photovoltaic systems on the roofs of the German school in Rome Lisbon., as well as the Goethe-Institut / Max Mueller Bhavan in Bangalore In 2008 IBC solar has entered into partnership with Indian company Refex Energy to explore opportunities in India.\n\n"}
{"id": "13726331", "url": "https://en.wikipedia.org/wiki?curid=13726331", "title": "Inertia wheel pendulum", "text": "Inertia wheel pendulum\n\nAn inertia wheel pendulum is a pendulum with an inertia wheel attached. It can be used as a pedagogical problem in control theory. This type of pendulum is often confused with the gyroscopic effect, which has completely different physical nature.\n\n"}
{"id": "29557324", "url": "https://en.wikipedia.org/wiki?curid=29557324", "title": "John von Neumann Environmental Research Institute of the Pacific", "text": "John von Neumann Environmental Research Institute of the Pacific\n\nThe John von Neumann Environmental Research Institute of the Pacific is a non profit environmental and anthropological research institute of executive branch of the government of Colombia ascribed to the Ministry of Environment and Sustainable Development and charged with conducting research and investigations on the Pacific littoral and the biodiversity of the Chocó biogeographic hotspot.\n"}
{"id": "25809437", "url": "https://en.wikipedia.org/wiki?curid=25809437", "title": "Kenneth B. Storey", "text": "Kenneth B. Storey\n\nKenneth B. Storey, Ph.D. (born October 23, 1949) is a Canadian scientist whose work draws from a variety of fields including biochemistry and molecular biology. He is a Professor of Biology, Biochemistry and Chemistry at Carleton University in Ottawa, Canada. Storey has a world-wide reputation for his research on biochemical adaptation - the molecular mechanisms that allow animals to adapt to and endure severe environmental stresses such as deep cold, oxygen deprivation, and desiccation.\n\nKenneth Storey studied biochemistry at the University of Calgary (B.Sc. '71) and zoology at the University of British Columbia (Ph.D. '74). Storey is a Professor of Biochemistry, cross-appointed in the Departments of Biology, Chemistry and Neuroscience and holds the Canada Research Chair in Molecular Physiology at Carleton University in Ottawa, Canada.\n\nStorey is an elected fellow of the Royal Society of Canada, of the Society for Cryobiology and of the American Association for the Advancement of Science. He has won fellowships and awards for research excellence including the Fry medal from the Canadian Society of Zoologists (2011), the Flavelle medal from the Royal Society of Canada (2010), Ottawa Life Sciences Council Basic Research Award (1998), a Killam Senior Research Fellowship (1993–1995), the Ayerst Award from the Canadian Society for Molecular Biosciences (1989), an E.W.R. Steacie Memorial Fellowship from the Natural Sciences and Engineering Research Council of Canada (1984–1986), and four Carleton University Research Achievement Awards. Storey is the author of over 844 research articles, the editor of seven books, has given over 500 talks at conferences and institutes worldwide, and organized numerous international symposia.\n\nStorey is one of the most cited researchers in the world. Storey's research includes studies of enzyme properties, gene expression, protein phosphorylation, and cellular signal transduction mechanisms to seek out the basic principles of how organisms endure and flourish under extreme conditions. He is particularly known within the field of cryobiology for his studies of animals that can survive freezing, especially the frozen \"frog-sicles\" (\"Rana sylvatica\") that have made his work popular with multiple TV shows and magazines. Storey's studies of the adaptations that allow frogs, insects, and other animals to survive freezing have made major advances in the understanding of how cells, tissues and organs can endure freezing. Storey was also responsible for the discovery that some turtle species are freeze tolerant: newly hatched painted turtles that spend their first winter on land (\"Chrysemys picta marginata & C. p. bellii\"). These turtles are unique as they are the only reptiles, and highest vertebrate life form, known to tolerate prolonged natural freezing of extracellular body fluids during winter hibernation. These advances may aid the development of organ cryopreservation technology. A second area of his research is metabolic rate depression - understanding the mechanisms by which some animals can sharply reduce their metabolism and enter a state of hypometabolism or torpor that allows them to survive over the long term under difficult environmental stresses. His studies have identified molecular mechanisms that underlie metabolic arrest across phylogeny and that support phenomena including mammalian hibernation, estivation, and anoxia and ischemia tolerance. Control mechanisms include transcription factor changes that alter gene expression, and reversible phosphorylation of key metabolic enzymes by protein kinases and protein phosphatases. These studies across multiple species also hold key applications for medical science, particularly for preservation technologies that aim to extend the survival time of excised organs in cold or frozen storage. Additional applications include insights into hyperglycemia in metabolic syndrome and diabetes, and anoxic and ischemic damage caused by heart attack and stroke. Furthermore, Storey's lab has created several web based programs freely available for data management, data plotting, and micro RNA analysis.\n\nMain Page\n\nPersonal Profile\n\nFull list on Google Scholar\n\nFull list on PubMed\n\n\n\n\nList of Honours\n\nList of Research Activities\n"}
{"id": "995169", "url": "https://en.wikipedia.org/wiki?curid=995169", "title": "Landau theory", "text": "Landau theory\n\nLandau theory in physics is a theory that Lev Landau introduced in an attempt to formulate a general theory of continuous (i.e., second-order) phase transitions.\n\nLandau was motivated to suggest that the free energy of any system should obey two conditions:\n\n\nGiven these two conditions, one can write down (in the vicinity of the critical temperature, \"T\") a phenomenological expression for the free energy as a Taylor expansion in the order parameter.\n\nFor example, the Ising model free energy in the vicinity of the phase transition may be written as the following, where the variable formula_1 is the coarse-grained field of spins, known as the order parameter or the total magnetization.\n\nWe can truncate the free energy to the 4th power in formula_3 without losing the physics of the phase transition, but in general, there are higher order terms present.\nFor the system to be thermodynamically stable, the coefficient of the highest even power of the order parameter must be positive. In this case, we find that formula_4, such that the free energy is bounded.\nThe phase transition occurs at some critical temperature formula_5. Noticing that the minimum in the free energy changes from formula_6 to formula_7 when the parameter formula_8 changes sign, we can write the parameter formula_8 as a function of temperature as such:\n\nwhere formula_11 is some temperature independent constant.\nThe constant formula_12 can be safely taken to be zero because it is simply a constant shift in the free energy, which has no effect on the physics of the phase transition.\n\nThe Ising model theory of Landau first raised the order parameter to prominence. Note that the Ising model exhibits the following discrete symmetry: If every spin in the model is flipped, such that formula_13, where formula_14 is the value of the formula_15 spin, the Hamiltonian (and consequently the free energy) remains unchanged for zero external field formula_16. This symmetry is reflected in the even powers of formula_3 in formula_18.\n\nLandau theory has been extraordinarily useful. While the exact values of the parameters formula_19 and formula_20 were unknown, critical exponents could still be calculated with ease, and only depend on the original assumptions of symmetry and analyticity. For the Ising model case, the equilibrium magnetization formula_1 assumes the following value below the critical temperature formula_22:\n\nAt the time, it was known experimentally that the liquid–gas coexistence curve and the ferromagnet magnetization curve both exhibited a scaling relation of the form formula_24, where formula_25 was mysteriously the same for both systems. This is the phenomenon of universality. It was also known that simple liquid–gas models are exactly mappable to simple magnetic models, which implied that the two systems possess the same symmetries. It then followed from Landau theory why these two apparently disparate systems should have the same critical exponents, despite having different microscopic parameters. It is now known that the phenomenon of universality arises for other reasons (see Renormalization group). In fact, Landau theory predicts the incorrect critical exponents for the Ising and liquid–gas systems.\n\nThe great virtue of Landau theory is that it makes specific predictions for what kind of non-analytic behavior one should see when the underlying free energy is analytic. Then, all the non-analyticity at the critical point, the critical exponents, are because the \"equilibrium value\" of the order parameter changes non-analytically, as a square root, whenever the free energy loses its unique minimum.\n\nThe extension of Landau theory to include fluctuations in the order parameter shows that Landau theory is only strictly valid near the critical points of ordinary systems with spatial dimensions higher than 4. This is the upper critical dimension, and it can be much higher than four in more finely tuned phase transition. In Mukhamel's analysis of the isotropic Lifschitz point, the critical dimension is 8. This is because Landau theory is a mean field theory, and does not include long-range correlations.\n\nThis theory does not explain non-analyticity at the critical point, but when applied to superfluid and superconductor phase transition, Landau's theory provided inspiration for another theory, the Ginzburg–Landau theory of superconductivity.\n\nConsider the Ising model free energy above. Assume that the order parameter formula_26 and external magnetic field, formula_27, may have spatial variations. Now, the free energy of the system can be assumed to take the following modified form:\n\nwhere formula_29 is the total \"spatial\" dimensionality. So,\n\nAssume that, for a \"localized\" external magnetic perturbation formula_31, the order parameter takes the form formula_32. Then,\n\nThat is, the fluctuation formula_34 in the order parameter corresponds to the order-order correlation. Hence, neglecting this fluctuation (like in the earlier mean-field approach) corresponds to neglecting the order-order correlation, which diverges near the critical point.\n\nOne can also solve for formula_35, from which the scaling exponent, formula_36, for correlation length formula_37 can deduced. From these, the Ginzburg criterion for the upper critical dimension for the validity of the Ising mean-field Landau theory (the one without long-range correlation) can be calculated as:\n\nIn our current Ising model, mean-field Landau theory gives formula_39 and so, it (the Ising mean-field Landau theory) is valid only for spatial dimensionality greater than or equal to 4 (at the marginal values of formula_40, there are small corrections to the exponents). This modified version of mean-field Landau theory is sometimes also referred to as the Landau-Ginzburg theory of Ising phase transitions. As a clarification, there is also a Landau-Ginzburg theory specific to superconductivity phase transition, which also includes fluctuations.\n\n\n"}
{"id": "28094490", "url": "https://en.wikipedia.org/wiki?curid=28094490", "title": "List of retired Philippine typhoon names", "text": "List of retired Philippine typhoon names\n\nSince 1963, there have been three agencies who have named tropical cyclones within the north western Pacific Ocean which has often resulted in a cyclone having two names. From 1945 to 2000 the US Navy's Joint Typhoon Warning Center assigned names to tropical cyclones before the Japan Meteorological Agency (JMA), took over the naming of tropical cyclones in 2000. Both agencies assigned names to tropical cyclones when they intensified into a tropical storm. Since 1963 the Philippine Atmospheric, Geophysical and Astronomical Services Administration (PAGASA) has assigned local names to a tropical cyclone should it move into or form as a tropical depression in their area of responsibility located between 135°E and 115°E and between 5°N-25°N even if the cyclone has had an international name assigned to it. All three agencies that have assigned names to tropical cyclones within the Western Pacific have retired the names of significant tropical cyclones, with PAGASA retiring names if a cyclone has caused at least in damage and or have caused at least 300 deaths within the Philippines. Since 1963 the naming lists have been revised in 1979, 1985, 2001 and 2005 for various reasons including to help minimize confusion in the historical records and to remove the names that might have negative associations with real persons. Within this list all information with regards to intensity is taken from while the system was in the Philippine area of responsibility and is thus taken from PAGASA's archives, rather than the JTWC or JMA's archives.\n\nGathering of 10-minute sustained wind data had started in the 1978 Pacific typhoon season.\n\nAt the start of the decade the name Juan was retired after the 2010 season, after it had become a super typhoon and caused around in damages as it made landfall on Luzon. During 2011 the names Bebeng, Juaning, Mina, Pedring and Sendong were retired after each caused over in damages. In addition to causing over in damages, Tropical Storm Sendong and Typhoon Pablo caused over a thousand deaths. The name Katring was retired at the start of 2014 after a typhoon named Katring had affected the archipelago in 1994. The Tropical Storm Kulap named \"Nonoy\" removed from PAGASA it used in 2015 Pacific typhoon season, renamed as \"Nona\".\n\n\n"}
{"id": "33411732", "url": "https://en.wikipedia.org/wiki?curid=33411732", "title": "MAN NLxx3F", "text": "MAN NLxx3F\n\nThe MAN NLxx3F is a series of low-floor single-decker city bus chassis between 10.5 to 12.0 metres length, offered by MAN since 1998, with internal code \"A22\". They are based on the integral low-floor MAN NLxx3 Lion's City (A21). Available versions since launch include NL223F, NL233F, NL243F, NL263F, NL273F, NL283F, NL313F and NL323F. LPG and CNG versions are also available, with NL313F CNG being the most powerful.\n\nWrightbus initially developed a bodywork for the MAN NL273F, called the Wright Meridian. It was the first MAN chassis to be bodied by Wrightbus. A Meridian demonstrator was launched at the Coach & Bus Live 2007, which entered service with Whitelaws. Regal Busways, Newbury Buses and Diamond Bus North West each operate an handful of Wright Meridian-bodied NL273Fs across the United Kingdom. It can only have 1 door (front entrance).\n\nEMT Madrid of Madrid, Spain received a total of 84 CNG-powered NL313F CNGs in 2008. They were bodied by Castrosua and Burillo.\n\nIn 2011, five 10.4m NL283Fs and one Hispano Area-bodied NL283F began operating in Santiago de Compostela and Buñol respectively.\n\nDan Bus Company and Egged operate a fleet of NL323Fs in Jerusalem, Tel Aviv and Haifa.\n\nSMRT Buses of Singapore received a 12.0m MCV Evolution-bodied NL323F demonstrator in 2010 for evaluation, registered as SMB138Y. Following a successful trial, SMRT Buses placed a follow-up order of 200 buses through ST Kinetics, the largest order of NL323Fs at that time. These production batch buses were bodied under licence by Gemilang Coachworks with the bodywork design based on the MAN Lion's City Hybrid, and entered service on 2 October 2011. In 2013, SMRT Buses placed an additional order for 202 buses with similar specifications as the first batch of buses, with minor differences in the seating configuration for later buses. A further order of 332 buses was made in 2014, with largely similar specifications as the earlier buses. \n\nAs part of the Bus Contracting Model (BCM), some of the buses were transferred to Tower Transit Singapore and SBS Transit under the Bulim bus package and Seletar Bus Package respectively. These buses were repainted into the lush green livery. The last 26 buses to be registered from the order of 332 buses were also painted into the lush green livery prior to entering service.\n\nIn October 2016, a MAN NL323F concept bus with 3 doors was exhibited at the Singapore International Transport Congress and Exhibition (SITCE). Marketed as the MAN Lion's City SD 3Dr, it features a modified bodywork design based on the MAN Lion's City Hybrid, built to a low-entry configuration under licence by Gemilang Coachworks. The bus was later painted into the lush green livery and began a 6-month trial service with SMRT Buses from 19 June 2017, starting on bus service 190.\n\nOn 27 May 2018, a new service, \"Express 851e\", was launched. New MAN NL323Fs, with an emission standard of Euro VI entered service. Registered from SG1749E onwards , they had LTA specifications, such as Passenger Information Display LCD screens, charging ports between pairs of seats and new seat colours. These are known as the MAN A22 Euro VI, which were delivered to SBS Transit, SMRT Buses and Go-Ahead Singapore. The buses came with updated MAN Lion's City SD bodywork by Gemilang Coachworks and were painted in the lush green livery under the Bus Contracting Model.\n\nIn March 2017, MAN exhibited a 12.0m bodied MAN NL313F CNG concept bus with 3 doors at the 11th Seoul Motor Show. It was marketed as the MAN Lion's City CNG, despite featuring Castrosua bodywork, which has a different design with the integral MAN Lion's City's body.\n\nGimpo Trans Co. received the first delivery of NL313F CNG and began service on Gimpo City bus route 1002 in November 2017.\n\n \n"}
{"id": "52855933", "url": "https://en.wikipedia.org/wiki?curid=52855933", "title": "Mangalore Anantha Pai", "text": "Mangalore Anantha Pai\n\nMangalore Anantha Pai (born 1931) is an Indian electrical engineer, academic and a Professor Emeritus at the University of Illinois at Urbana–Champaign. A former professor of electrical engineering at the Indian Institute of Technology, Kanpur, he is known for his contributions in the fields of power stability, power grids, large scale power system analysis, system security and optimal control of nuclear reactors and he has published 8 books and several articles. Pai is the first India born scientist to be awarded a PhD in Electrical Engineering from the University of California, Berkeley.\n\nPai is an IEEE Life Fellow and is an elected fellow of the Indian National Science Academy, Indian Academy of Sciences, and Indian National Academy of Engineers and an elected and life fellow of the Institute of Electrical and Electronics Engineers The Council of Scientific and Industrial Research, the apex agency of the Government of India for scientific research, awarded him the Shanti Swarup Bhatnagar Prize for Science and Technology, one of the highest Indian science awards for his contributions to Engineering Sciences in 1974.\n\nM. A. Pai. born on 5 October 1931 in the south Indian state of Karnataka, did his graduate studies in electrical engineering at the University of Madras and after completing the course in 1953, started his career as an electrical engineer at the Electric Supply Department of Brihanmumbai Electric Supply and Transport, then known as Bombay Electric Supply and Transport (BEST) Undertaking. After a service of 4 years, he moved to the US in 1957 where he completed his master's degree in electrical engineering (MS) at University of California, Berkeley in 1958 and continued at the institution to secure a PhD in 1961. Subsequently, he joined the university as an assistant professor at University of California, Berkeley (1961–62) before taking up Professorship at University of California, Los Angeles (UCLA) in 1962. At UCLA, he had the opportunity to work with Eliahu I. Jury, a Rufus Oldenburger laureate on discrete time systems, and in 1963 he returned to India to join the Electrical Engineering department of the Indian Institute of Technology, Kanpur as an assistant professor. He served the institute till 1981 during which period he held the positions of an associate professor from 1966 to 1969 and a professor thereafter till he went back to the US as a visiting professor at University of Illinois at Urbana–Champaign in 1981. He also had a stint as a consultant to the Uttar Pradesh Power Corporation Limited from 1971 to 1979. After two years of service as a visiting faculty at Illinois, he became a regular professor in 1983 and served out his academic career to superannuate in 2003. In between, he also served as a visiting professor at the Memorial University of Newfoundland and Iowa State University and visited India on a CSIR-UNDP program at Central Power Research Institute. Post-retirement, Illinois University made him an emeritus professor. He also serves as a consultant to Pacific Northwest National Laboratory, advising them on \"Model Reduction and Dynamic Security of Power System\".\n\nPai's early researches were based on power system stability, security and model reduction and his work on \"Model Reduction of Large-Scale Power Systems\" has been reported to be of significance to field of power systems. He is known to have introduced Lyapunov's method in researches on power system stability and energy functions. Introduction of computer applications in power systems is another of his contributions and a power system software he has developed is in use with several power distribution networks. Later, he worked on Smart Grid, Microgrid and Renewable Energy and how they could be integrated into the main electrical grid of a power system; earlier, at IIT Kanpur, he encouraged research on \"Control and Power\". He has also contributed in the field of industrial consultancy and has served as a member of the editorial boards of Power and Energy series of Springer.\n\nIn 1963, Pai was selected to join IIT Kanpur as one of the earliest member of the Electrical Engineering faculty. Under the guidance of economist John Kenneth Galbraith, IIT Kanpur became the first institute in India to offer Computer science education. The earliest computer courses were started at IIT Kanpur in August 1963 on an IBM 1620 system, which was flown from Princeton to Kanpur. The initiative for computer education came from the Electrical engineering department. An intense outreach program in Computer Education was established during his time at IIT Kanpur. It was during his years at IIT Kanpur, Pai published his first book, \"Computer Techniques in Power System Analysis\". His second book, \"Power System Stability: Analysis by the Direct Method of Lyapunov\", was also published before he moved to US in 1981. This was followed by five more authored books, \"Com Tech In Power Sys Ana\", \"Power System Dynamics and Stability\", \"Power Circuits and Electromechanics\", \"Energy Function Analysis for Power System Stability\" and \"Small Signal Analysis of Integrated Power Systems\", the last one published in 2016. In between, he co-edited an Oxford University Press festschrift on Homi J. Bhabha titled \"Homi Bhabha and the Computer Revolution\", which was featured in the Indian edition of MIT Technology Review in May/June 2011. He has also contributed chapters to books published by others and has authored over 125 peer-reviewed articles; the online article repository of Indian Academy of Sciences has listed 107 of them. He is reported to have helped organize two symposia of International Federation of Automatic Control in India and has delivered keynote or invited speeches at several science seminars in India and abroad including the Seminar on \"Voltage Instability in Power Network\" organized by the Delhi chapter of the Institute of Electrical and Electronics Engineers in 1991. He has also mentored a number of doctoral scholars in their studies.\n\n\n\n\nThe Council of Scientific and Industrial Research awarded Pai the Shanti Swarup Bhatnagar Prize, one of the highest Indian science awards in 1974. The Indian Academy of Sciences elected him as a fellow in 1979 and he became an elected fellow of the Indian National Science Academy in 1980. The Institute of Electrical and Electronics Engineers elected him as a fellow in 1986; IEEE honored him again in 1996 with life fellowship. He is also an elected fellow of the Indian National Academy of Engineers. The Electrical and Computer Engineering (ECE) department of the University of Illinois organized a day-long felicitation event, \"PaiFest\", in his honor on 15 October 2015 at the department. The event included a symposium where a number of his students and colleagues such as William H. Sanders, Peter W. Sauer, Ian Hiskens, Bernie Lesieutre, Marija D. Ilic, Kash Khorasani and Vijay Vittal presented papers and Pai himself made a presentation titled, \"From Lyapunov to Krylov – My Research Journey\". In 2014, IIT Kanpur honored him with the Institute Fellow award.\n\n\n"}
{"id": "36614013", "url": "https://en.wikipedia.org/wiki?curid=36614013", "title": "Motor control center", "text": "Motor control center\n\nA motor control center (MCC) is an assembly to control some or all electric motors in a central location. It consists of multiple enclosed sections having a common power bus and with each section containing a combination starter, which in turn consists of motor starter, fuses or circuit breaker, and power disconnect. A motor control center can also include push buttons, indicator lights, variable-frequency drives, programmable logic controllers, and metering equipment. It may be combined with the electrical service entrance for the building.\n\nMCC's are typically found in large commercial or industrial buildings where there are many electric motors that need to be controlled from a central location, such as a mechanical room or electrical room.\n\nMotor control centers are usually used for low voltage three-phase alternating current motors from 208 V to 600 V. Medium-voltage motor control centers are made for large motors running at 2300 V to around 15000 V, using vacuum contactors for switching and with separate compartments for power switching and control.\n\nMotor control centers have been used since 1950 by the automobile manufacturing industry which used large numbers of electric motors. Today they are used in many industrial and commercial applications. Where very dusty or corrosive processes are used, the motor control center may be installed in a separate air-conditioned room, but often an MCC will be on the factory floor adjacent to the machinery controlled.\n\nA motor control center consists of one or more vertical metal cabinet sections with power bus and provision for plug-in mounting of individual motor controllers. Very large controllers may be bolted in place but smaller controllers can be unplugged from the cabinet for testing or maintenance. Each motor controller contains a contactor or a solid-state motor controller, overload relays to protect the motor, fuses or a circuit breaker to provide short-circuit protection, and a disconnecting switch to isolate the motor circuit. Three-phase power enters each controller through separable connectors. The motor is wired to terminals in the controller. Motor control centers provide wire ways for field control and power cables.\n\nEach motor controller in an MCC can be specified with a range of options such as separate control transformers, pilot lamps, control switches, extra control terminal blocks, various types of thermal or solid-state overload protection relays, or various classes of power fuses or types of circuit breakers. A motor control center can either be supplied ready for the customer to connect all field wiring, or can be an engineered assembly with internal control and interlocking wiring to a central control terminal panel board or programmable controller.\n\nMotor control centers usually sit on floors, which are often required to have a fire-resistance rating. Firestops may be required for cables that penetrate fire-rated floors and walls.\n\n"}
{"id": "56982506", "url": "https://en.wikipedia.org/wiki?curid=56982506", "title": "Ocean Grazer", "text": "Ocean Grazer\n\nThe Ocean Grazer is a conceptual energy collection platform, projected to house several renewable energy generation modules, including wave energy, solar energy and wind energy. The development of the Ocean Grazer platform has been carried out by the University of Groningen in the Netherlands.\n\nThe concept of the platform is currently on its version 3.0 centering on the modular design as opposed to the massiveness of the platform, as in the previous concepts. The majority of the harvested energy for all concepts is to be delivered by a wave energy converter that uses the motion of ocean surface waves to generate electrical energy\n\nThe operating principle of the Ocean Grazer wave energy converter is to store potential energy by creating a hydraulic head, due to the differences in pressure between two reservoirs. All three concepts rely on this principle to function. The hydraulic head is created by circulating internal fluid from the lower to the upper reservoir via a novel hydro-mechanical power take off system, composed of distributed and coupled floaters. Each floater is linked to a separate multi-piston pumping system, consisting of differently sized pistons that regulate the amount of pumped fluid and that can be adapted to the surface wave conditions. Check valve systems are needed to minimize the back flow when the pistons return to their resting position. Lastly, similar to a hydroelectric plant, once enough fluid has been stored in the upper reservoir it can be circulated through a turbine system to generate electrical energy.\n\nThere are advantages in using a device like the Ocean Grazer wave energy converter, such as:\n\nThere are also disadvantages in using a device like the Ocean Grazer wave energy converter, namely:\n\n"}
{"id": "10841699", "url": "https://en.wikipedia.org/wiki?curid=10841699", "title": "Organization for Tropical Studies", "text": "Organization for Tropical Studies\n\nThe Organization for Tropical Studies is a network of ecological research stations in Costa Rica and South Africa in 1963. OTS is run by a non-profit consortium of 63 universities, based in the United States, Australia, South Africa and Latin America . The North American Office is located at Duke University, in Durham, North Carolina. OTS offers a variety of courses in Spanish and English for high school, university, and graduate students. Most of the coursework and research conducted at OTS stations focuses on tropical ecology, and the three research stations are located in distinct ecozones.\n\nAlong with Cocha Cashu Biological Station and the Manu Learning Centre in Peru, and the Smithsonian Tropical Research Institute on Barro Colorado Island in Panama, the OTS research stations in general (and La Selva in particular) provide some of the most important and productive sites of original research on neotropical ecology.\n\nOTS research stations in Costa Rica:\nOTS research station in South Africa:\n\n"}
{"id": "34778599", "url": "https://en.wikipedia.org/wiki?curid=34778599", "title": "RNA CoSSMos", "text": "RNA CoSSMos\n\nThe RNA Characterization of Secondary Structure Motifs database (RNA CoSSMos) is a repository of three-dimensional nucleic acid PDB structures containing secondary structure motifs ( loops, hairpin loops ...).\n\n\n"}
{"id": "2031319", "url": "https://en.wikipedia.org/wiki?curid=2031319", "title": "Save the Pine Bush", "text": "Save the Pine Bush\n\nSave the Pine Bush is a not-for-profit community group which came into being on February 6, 1978. Its mission is to stop development of the Albany Pine Bush in the Capital District of the U.S. state of New York. Today the group continues to be an active environmental community.\n\nMainly through litigation, Save the Pine Bush has prevented or delayed the construction of several developments in the Pine Bush.\n\n"}
{"id": "37830", "url": "https://en.wikipedia.org/wiki?curid=37830", "title": "Solid-propellant rocket", "text": "Solid-propellant rocket\n\nA solid-propellant rocket or solid rocket is a rocket with a rocket engine that uses solid propellants (fuel/oxidizer). The earliest rockets were solid-fuel rockets powered by gunpowder; they were used in warfare by the Chinese, Indians, Mongols and Persians, as early as the 13th century.\n\nAll rockets used some form of solid or powdered propellant up until the 20th century, when liquid-propellant rockets offered more efficient and controllable alternatives. Solid rockets are still used today in military armaments world-wide, model rockets and on larger applications for their simplicity and reliability.\n\nSince solid-fuel rockets can remain in storage for long periods, and then reliably launch on short notice, they have been frequently used in military applications such as missiles. The lower performance of solid propellants (as compared to liquids) does not favor their use as primary propulsion in modern medium-to-large launch vehicles customarily used to orbit commercial satellites and launch major space probes. Solids are, however, frequently used as strap-on boosters to increase payload capacity or as spin-stabilized add-on upper stages when higher-than-normal velocities are required. Solid rockets are used as light launch vehicles for low Earth orbit (LEO) payloads under 2 tons or escape payloads up to .\n\nA simple solid rocket motor consists of a casing, nozzle, grain (propellant charge), and igniter.\n\nThe solid grain mass burns in a predictable fashion to produce exhaust gases. The nozzle dimensions are calculated to maintain a design chamber pressure, while producing thrust from the exhaust gases.\n\nOnce ignited, a simple solid rocket motor cannot be shut off, because it contains all the ingredients necessary for combustion within the chamber in which they are burned. More advanced solid rocket motors can not only be throttled but also be extinguished and then re-ignited by controlling the nozzle geometry or through the use of vent ports. Also, pulsed rocket motors that burn in segments and that can be ignited upon command are available.\n\nModern designs may also include a steerable nozzle for guidance, avionics, recovery hardware (parachutes), self-destruct mechanisms, APUs, controllable tactical motors, controllable divert and attitude control motors, and thermal management materials.\nThe medieval Song dynasty Chinese invented a very primitive form of solid-propellant rocket. Illustrations and descriptions in the 14th century Chinese military treatise \"Huolongjing\" by the Ming dynasty military writer and philosopher Jiao Yu confirm that the Chinese in 1232 used proto solid propellant rockets then known as \"fire arrows\" to drive back the Mongols during the Siege of Kaifeng. Each arrow took a primitive form of a simple, solid-propellant rocket tube that was filled with gunpowder. One open end allowed the gas to escape and was attached to a long stick that acted as a guidance system for flight direction control.\n\nModern castable composite solid rocket motors were invented by the American aerospace engineer Jack Parsons at Caltech in 1942 when he replaced double base propellant with roofing asphalt and potassium perchlorate. This made possible slow-burning rocket motors of adequate size and with sufficient shelf-life for jet-assisted take off applications. Charles Bartley, employed at JPL (Caltech), substituted curable synthetic rubber for the gooey asphalt, creating a flexible but geometrically stable load-bearing propellant grain that bonded securely to the motor casing. This made possible much larger solid rocket motors. Atlantic Research Corporation significantly boosted composite propellant I in 1954 by increasing the amount of powdered aluminium in the propellant to as much as 20%.\n\nThe largest solid rocket motors ever built were Aerojet's three monolithic solid motors cast in Florida. Motors 260 SL-1 and SL-2 were in diameter, long, weighed and had a maximum thrust of . Burn duration was two minutes. The nozzle throat was large enough to walk through standing up. The motor was capable of serving as a 1-to-1 replacement for the 8-engine Saturn I liquid-propellant first stage but was never used as such. Motor 260 SL-3 was of similar length and weight but had a maximum thrust of thrust and a shorter duration.\n\nDesign begins with the total impulse required, which determines the fuel/oxidizer mass. Grain geometry and chemistry are then chosen to satisfy the required motor characteristics.\n\nThe following are chosen or solved simultaneously. The results are exact dimensions for grain, nozzle, and case geometries:\n\nThe grain may or may not be bonded to the casing. Case-bonded motors are more difficult to design, since the deformation of the case and the grain under flight must be compatible.\n\nCommon modes of failure in solid rocket motors include fracture of the grain, failure of case bonding, and air pockets in the grain. All of these produce an instantaneous increase in burn surface area and a corresponding increase in exhaust gas production rate and pressure, which may rupture the casing.\n\nAnother failure mode is casing seal failure. Seals are required in casings that have to be opened to load the grain. Once a seal fails, hot gas will erode the escape path and result in failure. This was the cause of the Space Shuttle \"Challenger\" disaster.\n\nSolid rocket fuel deflagrates from the surface of exposed propellant in the combustion chamber. In this fashion, the geometry of the propellant inside the rocket motor plays an important role in the overall motor performance. As the surface of the propellant burns, the shape evolves (a subject of study in internal ballistics), most often changing the propellant surface area exposed to the combustion gases. Since the propellant volume is equal to the cross sectional area formula_1 times the fuel length, the volumetric propellant consumption rate is the cross section area times the linear burn rate formula_2, and the instantaneous mass flow rate of combustion gases generated is equal to the volumetric rate times the fuel density formula_3:\n\nSeveral geometric configurations are often used depending on the application and desired thrust curve:\n\n\nThe casing may be constructed from a range of materials. Cardboard is used for small black powder model motors, whereas aluminium is used for larger composite-fuel hobby motors. Steel was used for the space shuttle boosters. Filament-wound graphite epoxy casings are used for high-performance motors.\n\nThe casing must be designed to withstand the pressure and resulting stresses of the rocket motor, possibly at elevated temperature. For design, the casing is considered a pressure vessel.\n\nTo protect the casing from corrosive hot gases, a sacrificial thermal liner on the inside of the casing is often implemented, which ablates to prolong the life of the motor casing.\n\nA convergent-divergent design accelerates the exhaust gas out of the nozzle to produce thrust. The nozzle must be constructed from a material that can withstand the heat of the combustion gas flow. Often, heat-resistant carbon-based materials are used, such as amorphous graphite or carbon-carbon.\n\nSome designs include directional control of the exhaust. This can be accomplished by gimballing the nozzle, as in the Space Shuttle SRBs, by the use of jet vanes in the exhaust as in the V-2 rocket, or by liquid injection thrust vectoring (LITV).\n\nAn early Minuteman first stage used a single motor with four gimballed nozzles to provide pitch, yaw, and roll control.\n\nLITV consists of injecting a liquid into the exhaust stream after the nozzle throat. The liquid then vaporizes, and in most cases chemically reacts, adding mass flow to one side of the exhaust stream and thus providing a control moment. For example, the Titan IIIC solid boosters injected nitrogen tetroxide for LITV; the tanks can be seen on the sides of the rocket between the main center stage and the boosters.\n\nA typical, well-designed ammonium perchlorate composite propellant (APCP) first-stage motor may have a vacuum specific impulse (Isp) as high as 285.6 seconds (Titan IVB SRMU).\nThis compares to 339.3 s for kerosene/liquid oxygen (RD-180) and 452.3 s for hydrogen/oxygen (Block II SSME) bipropellant engines. Upper stage specific impulses are somewhat greater: as much as 303.8 s for APCP (Orbus 6E), 359 s for kerosene/oxygen (RD-0124) and 465.5 s for hydrogen/oxygen (RL10B-2). Propellant fractions are usually somewhat higher for (non-segmented) solid propellant first stages than for upper stages. The Castor 120 first stage has a propellant mass fraction of 92.23% while the Castor 30 upper stage developed for Orbital Science's Taurus II COTS (International Space Station resupply) launch vehicle has a 91.3% propellant fraction with 2.9% graphite epoxy motor casing, 2.4% nozzle, igniter and thrust vector actuator, and 3.4% non-motor hardware including such things as payload mount, interstage adapter, cable raceway, instrumentation, etc. Castor 120 and Castor 30 are in diameter, respectively, and serve as stages on the Athena IC and IIC commercial launch vehicles. A four-stage Athena II using Castor 120s as both first and second stages became the first commercially developed launch vehicle to launch a lunar probe (\"Lunar Prospector\") in 1998.\n\nSolid rockets can provide high thrust for relatively low cost. For this reason, solids have been used as initial stages in rockets (for example the Space Shuttle), while reserving high specific impulse engines, especially less massive hydrogen-fueled engines, for higher stages. In addition, solid rockets have a long history as the final boost stage for satellites due to their simplicity, reliability, compactness and reasonably high mass fraction. A spin-stabilized solid rocket motor is sometimes added when extra velocity is required, such as for a mission to a comet or the outer solar system, because a spinner does not require a guidance system (on the newly added stage). Thiokol's extensive family of mostly titanium-cased \"Star\" space motors has been widely used, especially on Delta launch vehicles and as spin-stabilized upper stages to launch satellites from the cargo bay of the Space Shuttle. \"Star\" motors have propellant fractions as high as 94.6% but add-on structures and equipment reduce the operating mass fraction by 2% or more.\n\nHigher performing solid rocket propellants are used in large strategic missiles (as opposed to commercial launch vehicles). HMX, CHN(NO), a nitramine with greater energy than ammonium perchlorate, was used in the propellant of the Peacekeeper ICBM and is the main ingredient in NEPE-75 propellant used in the Trident II D-5 Fleet Ballistic Missile. It is because of explosive hazard that the higher energy military solid propellants containing HMX are not used in commercial launch vehicles except when the LV is an adapted ballistic missile already containing HMX propellant (Minotaur IV and V based on the retired Peacekeeper ICBMs). The Naval Air Weapons Station at China Lake, California, developed a new compound, CHN(NO), called simply CL-20 (China Lake compound 20). Compared to HMX, CL-20 has 14% more energy per mass, 20% more energy per volume, and a higher oxygen-to-fuel ratio. One of the motivations for development of these very high energy density military solid propellants is to achieve mid-course exo-atmospheric ABM capability from missiles small enough to fit in existing ship-based below-deck vertical launch tubes and air-mobile truck-mounted launch tubes. CL-20 propellant compliant with Congress' 2004 insensitive munitions (IM) law has been demonstrated and may, as its cost comes down, be suitable for use in commercial launch vehicles, with a very significant increase in performance compared with the currently favored APCP solid propellants. With a specific impulse of 309 s already demonstrated by Peacekeeper's second stage using HMX propellant, the higher energy of CL-20 propellant can be expected to increase specific impulse to around 320 s in similar ICBM or launch vehicle upper stage applications, without the explosive hazard of HMX.\n\nAn attractive attribute for military use is the ability for solid rocket propellant to remain loaded in the rocket for long durations and then be reliably launched at a moment's notice.\n\nBlack powder (gunpowder) is composed of charcoal (fuel), potassium nitrate (oxidizer), and sulfur (fuel and catalyst). It is one of the oldest pyrotechnic compositions with application to rocketry. In modern times, black powder finds use in low-power model rockets (such as Estes and Quest rockets), as it is cheap and fairly easy to produce. The fuel grain is typically a mixture of pressed fine powder (into a solid, hard slug), with a burn rate that is highly dependent upon exact composition and operating conditions. The performance or specific impulse of black powder is low, around 80 seconds. The grain is sensitive to fracture and, therefore, catastrophic failure. Black powder does not typically find use in motors above .\n\nComposed of powdered zinc metal and powdered sulfur (oxidizer), ZS or \"micrograin\" is another pressed propellant that does not find any practical application outside specialized amateur rocketry circles due to its poor performance (as most ZS burns outside the combustion chamber) and fast linear burn rates on the order of 2 m/s. ZS is most often employed as a novelty propellant as the rocket accelerates extremely quickly leaving a spectacular large orange fireball behind it.\n\nIn general, rocket candy propellants are an oxidizer (typically potassium nitrate) and a sugar fuel (typically dextrose, sorbitol, or sucrose) that are cast into shape by gently melting the propellant constituents together and pouring or packing the amorphous colloid into a mold. Candy propellants generate a low-medium specific impulse of roughly 130 s and, thus, are used primarily by amateur and experimental rocketeers.\n\nDB propellants are composed of two monopropellant fuel components where one typically acts as a high-energy (yet unstable) monopropellant and the other acts as a lower-energy stabilizing (and gelling) monopropellant. In typical circumstances, nitroglycerin is dissolved in a nitrocellulose gel and solidified with additives. DB propellants are implemented in applications where minimal smoke is required yet medium-high performance (I of roughly 235 s) is required. The addition of metal fuels (such as aluminium) can increase the performance (around 250 s), though metal oxide nucleation in the exhaust can turn the smoke opaque.\n\nA powdered oxidizer and powdered metal fuel are intimately mixed and immobilized with a rubbery binder (that also acts as a fuel). Composite propellants are often either ammonium nitrate-based (ANCP) or ammonium perchlorate-based (APCP). Ammonium nitrate composite propellant often uses magnesium and/or aluminium as fuel and delivers medium performance (I of about 210 s) whereas ammonium perchlorate composite propellant often uses aluminium fuel and delivers high performance (vacuum I up to 296 s with a single piece nozzle or 304 s with a high area ratio telescoping nozzle). Aluminium is used as fuel because it has a reasonable specific energy density, a high volumetric energy density, and is difficult to ignite accidentally. Composite propellants are cast, and retain their shape after the rubber binder, such as Hydroxyl-terminated polybutadiene (HTPB), cross-links (solidifies) with the aid of a curative additive. Because of its high performance, moderate ease of manufacturing, and moderate cost, APCP finds widespread use in space rockets, military rockets, hobby and amateur rockets, whereas cheaper and less efficient ANCP finds use in amateur rocketry and gas generators. Ammonium dinitramide, NHN(NO), is being considered as a 1-to-1 chlorine-free substitute for ammonium perchlorate in composite propellants. Unlike ammonium nitrate, ADN can be substituted for AP without a loss in motor performance.\n\nPolyurethane-bound aluminium-APCP solid fuel was used in the submarine launched Polaris missiles. APCP used in the space shuttle Solid Rocket Boosters consisted of ammonium perchlorate (oxidizer, 69.6% by weight), aluminium (fuel, 16%), iron oxide (a catalyst, 0.4%), polybutadiene acrylonitrile (PBAN) polymer (a non-urethane rubber binder that held the mixture together and acted as secondary fuel, 12.04%), and an epoxy curing agent (1.96%). It developed a specific impulse of 242 seconds (2.37 km/s) at sea level or 268 seconds (2.63 km/s) in a vacuum. The 2005-2009 Constellation Program was to use a similar PBAN-bound APCP.\n\nIn 2009, a group succeeded in creating a propellant of water and nanoaluminium (ALICE).\n\nTypical HEC propellants start with a standard composite propellant mixture (such as APCP) and add a high-energy explosive to the mix. This extra component usually is in the form of small crystals of RDX or HMX, both of which have higher energy than ammonium perchlorate. Despite a modest increase in specific impulse, implementation is limited due to the increased hazards of the high-explosive additives.\n\nComposite modified double base propellants start with a nitrocellulose/nitroglycerin double base propellant as a binder and add solids (typically ammonium perchlorate (AP) and powdered aluminium) normally used in composite propellants. The ammonium perchlorate makes up the oxygen deficit introduced by using nitrocellulose, improving the overall specific impulse. The aluminium improves specific impulse as well as combustion stability. High performing propellants such as NEPE-75 used to fuel the Trident II D-5, SLBM replace most of the AP with polyethylene glycol-bound HMX, further increasing specific impulse. The mixing of composite and double base propellant ingredients has become so common as to blur the functional definition of double base propellants.\n\nOne of the most active areas of solid propellant research is the development of high-energy, minimum-signature propellant using CHN(NO) CL-20 nitroamine (China Lake compound #20), which has 14% higher energy per mass and 20% higher energy density than HMX. The new propellant has been successfully developed and tested in tactical rocket motors. The propellant is non-polluting: acid-free, solid particulates-free, and lead-free. It is also smokeless and has only a faint shock diamond pattern that is visible in the otherwise transparent exhaust. Without the bright flame and dense smoke trail produced by the burning of aluminized propellants, these smokeless propellants all but eliminate the risk of giving away the positions from which the missiles are fired. The new CL-20 propellant is shock-insensitive (hazard class 1.3) as opposed to current HMX smokeless propellants which are highly detonable (hazard class 1.1). CL-20 is considered a major breakthrough in solid rocket propellant technology but has yet to see widespread use because costs remain high.\n\nElectric solid propellants (ESPs) are a family of high performance plastisol solid propellants that can be ignited and throttled by the application of electric current. Unlike conventional rocket motor propellants that are difficult to control and extinguish, ESPs can be ignited reliably at precise intervals and durations. It requires no moving parts and the propellant is insensitive to flames or electrical sparks.\n\nSolid propellant rocket motors can be bought for use in model rocketry; they are normally small cylinders of black powder fuel with an integral nozzle and sometimes a small charge that is set off when the propellant is exhausted after a time delay. This charge can be used to trigger a camera, or deploy a parachute. Without this charge and delay, the motor may ignite a second stage (black powder only).\n\nIn mid- and high-power rocketry, commercially made APCP motors are widely used. They can be designed as either single-use or reloadables. These motors are available in impulse ranges from \"D\" to \"O\", from several manufacturers. They are manufactured in standardized diameters, and varying lengths depending on required impulse. Standard motor diameters are 13, 18, 24, 29, 38, 54, 75, 98, and 150 millimeters. Different propellant formulations are available to produce different thrust profiles, as well as \"special effects\" such as colored flames, smoke trails, or large quantities of sparks (produced by adding titanium sponge to the mix).\n\nAlmost all sounding rockets use solid motors.\n\nDue to reliability, ease of storage and handling, solid rockets are used on missiles and ICBMs.\n\nSolid rockets are suitable for launching small payloads to orbital velocities, especially if three or more stages are used. Many of these are based on repurposed ICBMs.\n\nLarger liquid-fueled orbital rockets often use solid rocket boosters to gain enough initial thrust to launch the fully fueled rocket.\n\n\nSolid fuel is also used for some upper stages, particularly the Star 37 (sometimes referred to as the \"Burner\" upper stage) and the Star 48 (sometimes referred to as the \"Payload Assist Module\", or PAM), both manufactured originally by Thiokol, and today by Orbital ATK. They are used to lift large payloads to intended orbits (such as the Global Positioning System satellites), or smaller payloads to interplanetary—or even interstellar—trajectories. Another solid-fuel upper stage, used by the Space Shuttle and the Titan IV, was the Boeing-manufactured Inertial Upper Stage (IUS).\n\nSome rockets, like the Antares (manufactured by Orbital ATK), have mandatory solid-fuel upper stages. The Antares rocket uses the Orbital ATK-manufactured Castor 30 as an upper stage.\n\n\n\n"}
{"id": "35740129", "url": "https://en.wikipedia.org/wiki?curid=35740129", "title": "Square antiprismatic molecular geometry", "text": "Square antiprismatic molecular geometry\n\nIn chemistry, the square antiprismatic molecular geometry describes the shape of compounds where eight atoms, groups of atoms, or ligands are arranged around a central atom, defining the vertices of a square antiprism. This shape has D symmetry and is one of the three common shapes for octacoordinate transition metal complexes, along with the dodecahedron and the bicapped trigonal prism.\n\nLike with other high coordination numbers, eight-coordinate compounds are often distorted from idealized geometries, as illustrated by the structure of NaTaF. In this case, with the small Na ions, lattice forces are strong. With the diatomic cation NO, the lattice forces are weaker, such as in (NO)XeF, which crystallizes with a more idealized square antiprismatic geometry.\n\n"}
{"id": "16270953", "url": "https://en.wikipedia.org/wiki?curid=16270953", "title": "Stonehenge in its landscape", "text": "Stonehenge in its landscape\n\nStonehenge in its landscape: Twentieth century excavations by Rosamund M. J. Cleal, Karen E. Walker and Rebecca Montague is an archaeological report on Stonehenge published in 1995. It presented the results of a two-year intensive study of all the known records of the various excavations at Stonehenge in the twentieth century, including a rephasing of the development of the monument.\n\nUnlike popular books on the subject, \"Stonehenge in its landscape\" details the complex archaeological stratigraphy of the site. It has been described as \"an essential reference work for the specialist\".\n\nIn 1993, both the setting and the presentation of Stonehenge was described as \"a national disgrace\" by the House of Commons Public Accounts Committee. The criticisms were several: two major roads ran close to the monument, one of which cut the processional Avenue; a large car park lay nearby; the pedestrian access to the monument was via a shabby underpass, and the visitor facilities were very limited, with no visitor centre. In addition, there was no proper plan of the monument.\n\nThe documentation of the site was in a similarly poor state. Professor Richard J. C. Atkinson had published a partial account of the site in 1956, and this had been revised in 1979, but after nearly 90 years of archaeological investigations at the monument there was still no definitive publication presenting the complex stratigraphy and the finds recovered from the site. In 1993 English Heritage commissioned Wessex Archaeology to prepare such a volume. The work involved detailed study of all available site records, including plans, photographs, site notebooks, letters and other documentary sources, as well as analysis of all the finds from the site, and a new suite of radiocarbon dates for the monument. The volume aimed to relate the site to its local landscape.\n\nThe volume is split into four parts. Part 1 deals with the geography and history of Stonehenge, including previous work at the site, and the site in its modern setting. Part 2, entitled \"Stonehenge, the monument in its setting\" tackles the Mesolithic and earlier Neolithic evidence from the site, and then moves on to describe the development of the monument, including the three major phases of development, and the post-Bronze Age use of the site. Part 3 deals with the artefacts and ecofacts (environmental data), and Part 4 is the discussion section.\n\nThe analysis showed that Atkinson's phasing of the monument no longer held, and a new scheme was set out:\n\nThe Avenue was also constructed during Phase 3.\n\nThe book was launched at the Society of Antiquaries in London on 5 October 1995. One reviewer wryly noted:\n\nIf this book were a new car - such is its importance - it would have descended from the clouds amidst lasers, escorted by angelic supermodels. Instead, it was launched at the Society of Antiquaries with tea and biscuits. Such is the world.\n\nReviewers described the monograph as \"a massive achievement\", \"fundamental\" and \"one of the more important British archaeological publications this century\". The original edition had a print run of just 800 copies; the volume was reprinted in 2006.\n\n\n"}
{"id": "58912479", "url": "https://en.wikipedia.org/wiki?curid=58912479", "title": "Suswa Geothermal Power Station", "text": "Suswa Geothermal Power Station\n\nThe Suswa Geothermal Power Station also known as Suswa Geothermal Power Plant is a planned geothermal power plant in Kenya, with installed electric generating capacity of \n\nThe facility would be located in the Suswa Area, in Narok County, approximately , northwest of Nairobi, the country's capital and largest city. This is approximately , by road, southeast of Narok, where the county headquarters are located.\n\nCYRQ Energy, a United States-based energy and technology company, has carried out feasibility studies for a geothermal power station at the project site. The results of those studies support the development of an economically viable power station, with capacity of 330 megawatts.\n\nThe power station, to be developed in phases, will start with capacity of 75 megawatts. subsequent phases will raise output to 330 megawatts. The energy from this power station is expected to be sold to Kenya Power and Lighting Company, under a 25 year power purchase agreement.\n\nAs of October 2018, the station owner/developers have submitted requests to install the power station at the development site. When those requests are approved, construction of the first phase is expected to follow and take an estimated three to four years. Later phases of the development would then follow. The entire development is estimated to cost KSh30 billion (US$3 billion), financed through equity and debt.\n\nThe power station is owned by the company that is developing it; CYRQ Energy, headquartered in Salt Lake City, Utah.\n\n\n"}
{"id": "9885419", "url": "https://en.wikipedia.org/wiki?curid=9885419", "title": "Tip-speed ratio", "text": "Tip-speed ratio\n\nThe tip-speed ratio, X, or TSR for wind turbines is the ratio between the tangential speed of the tip of a blade and the actual speed of the wind, formula_1. The tip-speed ratio is related to efficiency, with the optimum varying with blade design. Higher tip speeds result in higher noise levels and require stronger blades due to large centrifugal forces.\n\nThe tip speed of the blade can be calculated as formula_3 times R, where formula_3 is the rotor rotational speed in radians/second, and R is the rotor radius in meters. Therefore, we can also write:\n\nwhen formula_1 is the wind speed specified in meters/second.\n\nThe power coefficient, formula_7 is a quantity that expresses what fraction of the power in the wind is being extracted by the wind turbine. It is generally assumed to be a function of both tip-speed ratio and pitch angle. Below is a plot of the variation of the power coefficient with variations in the tip-speed ratio when the pitch is held constant:\n\nOriginally, wind turbines were fixed speed. This has the benefit that the rotor speed in the generator is constant, thus the frequency of the AC Voltage is fixed. This allows the wind turbine to be directly connected to a transmission system. However, from the figure above, we can see that the power coefficient is a function of the tip-speed ratio. By extension, the efficiency of the wind turbine is a function of the tip-speed ratio.\n\nIdeally, one would like to have a turbine operating at the maximum value of formula_7 at all wind speeds. This means that as the wind speed changes, the rotor speed must change to such that formula_9. A wind turbine with a variable rotor speed is called a variable speed wind turbine. Whilst this does mean that the wind turbine operates at or close to formula_10 for a range of wind speeds, the frequency of the AC voltage generator will not be constant. This can be seen in the following equation:\n\nformula_11\n\nwhere formula_12 is the rotor angular speed, formula_13 is the frequency of the AC voltage generated in the stator windings, formula_14 is the number of poles in the generator inside the nacelle. That is, direct connection to a transmission system for a variable speed is not permissible. What is required is a power converter which converts the signal generated by the turbine generator into DC and then converts that signal to an AC signal with the grid/transmission system frequency.\n\nAs already mentioned, variable speed wind turbines cannot be directly connected to a transmission system. One of the drawbacks of this is that the inertia of the transmission system is reduced as more variable speed wind turbines are put online. This can result in more significant drops in the transmission system's voltage frequency in the event of the loss of a generating unit. Furthermore, variable speed wind turbines require power electronics, which increases the complexity of the turbine and introduces new sources of failures. It has also been suggested that additional energy capture achieved by comparing a variable speed wind turbine to a fixed speed wind turbine is approximately 2%.\n"}
{"id": "2810349", "url": "https://en.wikipedia.org/wiki?curid=2810349", "title": "Underweight", "text": "Underweight\n\nAn underweight person is a person whose body weight is considered too low to be healthy. Underweight people have a body mass index (BMI) of under 18.5 or a weight 15% to 20% below that normal for their age and height group.\n\nA person may be underweight due to genetics, metabolism, drug use, lack of food (frequently due to poverty), eating disorder, or illness (both physical and mental).\n\nBeing underweight is associated with certain medical conditions, including diabetes, hyperthyroidism, cancer, or tuberculosis. People with gastrointestinal or liver problems may be unable to absorb nutrients adequately. People with certain eating disorders can also be underweight due to lack of nutrients/over exercise.\n\nUnderweight might be secondary to or symptomatic of an underlying disease. Unexplained weight loss may require professional medical diagnosis.\n\nUnderweight can also be a primary causative condition. Severely underweight individuals may have poor physical stamina and a weak immune system, leaving them open to infection. According to Robert E. Black of the Johns Hopkins School of Public Health (JHSPH), \"Underweight status ... and micronutrient deficiencies also cause decreases in immune and non-immune host defenses, and should be classified as underlying causes of death if followed by infectious diseases that are the terminal associated causes.\"\nPeople who are malnutrative underweight raise special concerns, as not only gross caloric intake may be inadequate, but also intake and absorption of other vital nutrients, especially essential amino acids and micro-nutrients such as vitamins and minerals.\n\nIn women, being severely underweight as a result of an eating disorder, or due to excessive strenuous exercise can result in amenorrhea (absence of menstruation), infertility and, if gestational weight gain is too low, possible complications during pregnancy. \n\nMalnourishment can also cause anemia and hair loss. \n\nBeing underweight is an established risk factor for osteoporosis, even for young people. This is seen in individuals suffering from the female athlete triad, when disordered eating or excessive exercise cause amenorrhea, hormone changes during an ovulation leads to loss of bone mineral density. After the occurrence of first spontaneous fractures the damage is often already irreversible.\n\nAlthough being underweight has been reported to increase mortality at rates comparable to that seen in morbidly obese people, the effect is much less drastic when restricted to non-smokers with no history of disease, suggesting that smoking and disease-related weight loss are the leading causes of the observed effect.\n\nUnderweight individuals may be advised to gain weight by increasing calorie intake. This can be done by eating a sufficient volume of sufficiently calorie-dense foods. Body weight may also be increased through the consumption of liquid nutritional supplements. Other nutritional supplements may be recommended for individuals with insufficient vitamin or mineral intake.\n\nAnother way for underweight people to gain weight is by exercising. Muscle hypertrophy increases body mass. Weight lifting exercises are effective in helping to improve muscle tone as well as helping with weight gain. Weight lifting has also been shown to improve bone mineral density, for which underweight people have an increased risk of deficiency.\n\nExercise itself is catabolic, which results in a brief reduction in mass. The gain in weight that can result of it comes from the anabolic overcompensation when the body recovers and overcompensates via muscle hypertrophy. This can happen by an increase in the muscle proteins, or through enhanced storage of glycogen in muscles. Exercise can help stimulate a person's appetite if they are not inclined to eat.\n\nCertain drugs may increase appetite either as their primary effect or as a side effect. Antidepressants, such as mirtazapine or amitriptyline, and antipsychotics, particularly chlorpromazine and haloperidol, as well as tetrahydrocannabinol (found in cannabis), all present an increase in appetite as a side effect. In states where it is approved, medicinal cannabis may be prescribed for severe appetite loss, such as that caused by cancer, AIDS, or severe levels of persistent anxiety. Other drugs or supplements which may increase appetite include antihistamines (such as diphenhydramine, promethazine or cyproheptadine), and B vitamin supplements (such as vitamin B12 and vitamin B-complex).\n\n"}
{"id": "165198", "url": "https://en.wikipedia.org/wiki?curid=165198", "title": "Wind speed", "text": "Wind speed\n\nWind speed, or wind flow velocity, is a fundamental atmospheric quantity caused by air moving from high to low pressure, usually due to changes in temperature. Note that wind direction is usually almost parallel to isobars (and not perpendicular, as one might expect), due to Earth's rotation.\n\nWind speed affects weather forecasting, aviation and maritime operations, construction projects, growth and metabolism rate of many plant species, and countless other implications.\n\nWind speed is now commonly measured with an anemometer, but can also be classified using the older Beaufort scale, which is based on personal observation of specifically defined wind effects.\n\nWind speed is affected by a number of factors and situations, operating on varying scales (from micro to macro scales). These include the pressure gradient, Rossby waves and jet streams, and local weather conditions. There are also links to be found between wind speed and wind direction, notably with the pressure gradient and terrain conditions.\n\nPressure gradient is a term to describe the difference in air pressure between two points in the atmosphere or on the surface of the Earth. It is vital to wind speed, because the greater the difference in pressure, the faster the wind flows (from the high to low pressure) to balance out the variation. The pressure gradient, when combined with the Coriolis effect and friction, also influences wind direction.\n\nRossby waves are strong winds in the upper troposphere. These operate on a global scale and move from West to East (hence being known as Westerlies). The Rossby waves are themselves a different wind speed from what we experience in the lower troposphere.\n\nLocal weather conditions play a key role in influencing wind speed, as the formation of hurricanes, monsoons and cyclones as freak weather conditions can drastically affect the flow velocity of the wind.\n\nThe fastest wind speed not related to tornadoes ever recorded was during the passage of Tropical Cyclone Olivia on 10 April 1996: an automatic weather station on Barrow Island, Australia, registered a maximum wind gust of 408 km/h (220 kn; 253 mph; 113 m/s). The wind gust was evaluated by the WMO Evaluation Panel who found that the anemometer was mechanically sound and the gust was within statistical probability and ratified the measurement in 2010. The anemometer was mounted 10 m above ground level (and thus 64 m above sea level). During the cyclone, several extreme gusts of greater than 300 km/h (160 kn; 83 m/s) were recorded, with a maximum 5-minute mean speed of 176 km/h (95 kn; 110 mph; 49 m/s), the extreme gust factor was in the order of 2.27–2.75 times the mean wind speed. The pattern and scales of the gusts suggest that a mesovortex was embedded in the already strong eyewall of the cyclone.\n\nThe now second highest surface wind speed ever officially recorded is 372 km/h (231 mph; 103 m/s) at the Mount Washington (New Hampshire) Observatory: 6,288 ft (1917 m) above sea level in the US on 12 April 1934, using a heated anemometer. The anemometer, specifically designed for use on Mount Washington was later tested by the US National Weather Bureau and confirmed to be accurate.\n\nWind speeds within certain atmospheric phenomena (such as tornadoes) may greatly exceed these values but have never been accurately measured. Directly measuring these tornadic winds is rarely done as the violent wind would destroy the instruments. A method of estimating speed is to use Doppler on Wheels to sense the wind speeds remotely, and, using this method, the figure of 486 km/h (302 mph; 135 m/s) during the 1999 Bridge Creek–Moore tornado in Oklahoma on 3 May 1999 is often quoted as the highest-recorded surface wind speed, although another figure of has also been quoted for the same tornado. Yet another number used by the Center for Severe Weather Research for that measurement is . However, speeds measured by Doppler radar are not considered official records.\n\nAn anemometer is one of the tools used to measure wind speed. A device consisting of a vertical pillar and three or four concave cups, the anemometer captures the horizontal movement of air particles (wind speed). \n\nAnother tool used to measure wind velocity includes a GPS combined with pitot tube. A fluid flow velocity tool, the Pitot tube is primarily used to determine the air velocity of an aircraft.\n\nWind speed is a common factor in the design of structures and buildings around the world. It is often the governing factor in the required lateral strength of a structure's design.\n\nIn the United States, the wind speed used in design is often referred to as a \"3-second gust\" which is the highest sustained gust over a 3-second period having a probability of being exceeded per year of 1 in 50 (ASCE 7-05). This design wind speed is accepted by most building codes in the United States and often governs the lateral design of buildings and structures.\n\nIn Canada, reference wind pressures are used in design and are based on the \"mean hourly\" wind speed having a probability of being exceeded per year of 1 in 50. The reference wind pressure (q) is calculated in Pascals using the following equation: q=(1/2)pV² where p is the air density in kg/m³ and V is wind speed in m/s.\n\nHistorically, wind speeds have been reported with a variety of averaging times (such as fastest mile, 3-second gust, 1-minute and mean hourly) which designers may have to take into account. To convert wind speeds from one averaging time to another, the Durst Curve was developed which defines the relation between probable maximum wind speed averaged over t seconds, V, and mean wind speed over one hour V.\n\n"}
{"id": "17088684", "url": "https://en.wikipedia.org/wiki?curid=17088684", "title": "Windflow Technology", "text": "Windflow Technology\n\nWindflow Technology is a company based in Christchurch, New Zealand known for its unique wind turbine gearbox design.\n\nIt was listed on the New Zealand Stock Exchange on 2 December 2003. In October 2008, Mighty River Power purchased a 19.95% share of Windflow Technology.\n\nThe operation of a Windflow Technology turbine installed at Gebbies Pass near Christchurch encountered opposition from local residents who raised concerns about noise. The neighbouring valley was very sheltered, and therefore had very little background sound. Because of this, Windflow offered to comply with low sound levels of 30 dBA, 10 dBA lower than the District Council's usual requirement of 40 dBA. However, sound levels were at 35 dBA, so Windflow modified the gearbox, which reduced the turbine's sound. Sound levels were again measured and this time be within the requirement of its resource consent.\nWindflow Technology set up New Zealand Windfarms to own and run the Te Rere Hau Wind Farm near Palmerston North. Initially, this was wholly owned but now has been completely sold off. Windflow Technology supplied the wind turbines for this wind farm, and are also contracted to supply turbines for Mighty River Power's proposed Long Gully Wind Farm.\n\nThe company achieved ISO 9001:2000 certification for the design and production of its Windflow 500 wind turbine in 2008. In September 2010, the Windflow 500 gained Class 1A of the International Electrotechnical Commission (IEC) design standard IEC 61400-1 (edition 3) through Lloyd's Register.\n\nIn September 2011, Windflow Technology signed a contract to install a Windflow 500 turbine on the island of Shetland \n\nIn March 2012, the company signed a licensing agreement with a subsidiary of large US company General Dynamics (GD Satcom)who will manufacture and sell the Windflow designed 500 kW turbines to customers in North and South America, Africa and U.S. territories and military bases worldwide. GD Energy Solutions is promoting the original Class 1 500 kW turbine as the GD33-500 and a newly developed Class 2 500 kW turbine as the GD45-500.\n\nIn February 2013, Windflow Technology installed a 500 kW turbine on the windy island of Orkney, its first exported turbine which is expected to generate over GBP350,000 per year (through the UK Feed-In Tariff which is 17.5 p/kWh for 500 kW turbines).\n\nWindflow UK has wrapped up installation of its 100th 500 kW wind turbine. The twin-bladed machine, which was designed and manufactured in New Zealand by parent company Windflow Technology Ltd, was erected at New Holland Farm on the Orkney mainland in Scotland. \nFor four years, the North Harris Trust has been working with Windflow Technology Ltd, who manufacture a unique two-bladed machine that can deal with the turbulent wind conditions found amongst the North Harris hills. The first of three machines arrived from New Zealand in late 2014 and was lifted into place.\n\n1 May 2015, Windflow Technology Ltd announced the installation and first operation of the prototype 45-500 wind turbine at a site in Mitchell County, Texas USA. The turbine was installed to supply power for the Mitchell County Desalination Plant, which will supplement water supplies to areas in the County not covered by municipal water utilities.\n\n\n"}
{"id": "3698703", "url": "https://en.wikipedia.org/wiki?curid=3698703", "title": "Wolds and Riverbank Countryside Society", "text": "Wolds and Riverbank Countryside Society\n\nWARCS (Wolds and Riverbank Countryside Society) is a society dedicated towards education, promotion, awareness and appreciation of regional natural and local history. Based in the East Riding of Yorkshire, it was formed in May 1996. \n\nThey have three (3) acres named WARCS Nature Reserve, located on Brantingham Road, Elloughton. Some money was granted \"Enventure Northern (UK)\". They sponsor talks, reports, essays and photo contests. Reports topics include superstitions, grasses, moths, fungus and butterflies.\n\n"}
{"id": "41615203", "url": "https://en.wikipedia.org/wiki?curid=41615203", "title": "ZEV 10 LRC", "text": "ZEV 10 LRC\n\nThe ZEV10 LRC is a two-wheeler electric scooter manufactured by Z Electric Vehicle (ZEV). ZEV positions the bike as a long range commuter, and states that it is the world's longest range and fastest production motor scooter.\n\nSpecifications in infobox are as claimed by ZEV.\n\nLife cycle cost analysis by ZEV customers indicate that it costs approximately 60% less to drive a ZEV than to drive a gasoline fueled scooter of equal performance and cost to the ZEV electric if the bike is driven to the point of battery replacement. These customers report full costs not including depreciation of 12 cents/mile. Fuel costs on test bikes show fuel costs run about $.0127/mile or about $127 per 10,000 miles in the Ohio/WV/Pa area of the USA. With the large battery capacity, the need to even charge more than one time a day is generally eliminated.\n"}
