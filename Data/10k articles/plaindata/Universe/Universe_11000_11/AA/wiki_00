{"id": "1078558", "url": "https://en.wikipedia.org/wiki?curid=1078558", "title": "1993 Storm of the Century", "text": "1993 Storm of the Century\n\nThe 1993 Storm of the Century (also known as the 93 Superstorm, The No Name Storm, or the Great Blizzard of 1993) was a large cyclonic storm that formed over the Gulf of Mexico on March 12, 1993. The storm was unique and notable for its intensity, massive size, and wide-reaching effects; at its height, the storm stretched from Canada to Honduras. The cyclone moved through the Gulf of Mexico and then through the eastern United States before moving on to Canada. The storm eventually dissipated in the North Atlantic Ocean on March 15, 1993.\n\nHeavy snow was first reported in highland areas as far south as Alabama and northern Georgia, with Union County, Georgia reporting up to 35 inches of snow in the north Georgia mountains. Birmingham, Alabama, reported a rare of snow. The Florida Panhandle reported up to of snow, with hurricane-force wind gusts and record low barometric pressures. Between Louisiana and Cuba, the hurricane-force winds produced high storm surges across the big bend of Florida which, in combination with scattered tornadoes, killed dozens of people.\n\nRecord cold temperatures were seen across portions of the south and east of the US in the wake of this storm. In the United States, the storm was responsible for the loss of electric power to more than 10 million households. An estimated 40 percent of the country's population experienced the effects of the storm with a total of 208 fatalities.\n\nDuring March 11 and 12, 1993, temperatures over much of the eastern United States began to drop as an arctic high pressure system built over the Midwest and Great Plains. Concurrently, an extratropical area of low pressure formed over Mexico along a stationary front draped west to east. By the afternoon of March 12, a defined airmass boundary was present along the deepening low. An initial burst of convective precipitation off the southern coast of Texas (facilitated by the transport of tropical moisture into the region) enabled initial intensification of the surface feature on March 12. Supported by a strong split-polar jet stream and a shortwave trough, the nascent system rapidly deepened. The system's central pressure fell to by 00:00 UTC on March 13. A powerful low-level jet over eastern Cuba and the Gulf of Mexico enhanced a cold front extending from the low southward to the Isthmus of Tehuantepec. Furthermore, the subtropical jet stream was displaced unusually far south, reaching into the Pacific Ocean near Central America and extending toward Honduras and Jamaica. Intense ageostrophic flow was noted over the southern United States, with winds flowing perpendicular to isobars over Louisiana.\n\nAs the area of low pressure moved through the central Gulf of Mexico, a short wave trough in the northern branch of the jet stream fused with the system in the southern stream, which further strengthened the surface low. A squall line developed along the system's cold front, which moved rapidly across the eastern Gulf of Mexico through Florida and Cuba. The cyclone's center moved into north-west Florida early on the morning of March 13, with a significant storm surge in the northwestern Florida peninsula that drowned several people.\n\nBarometric pressures recorded during the storm were low. Readings of were recorded in Tallahassee, Florida, and even lower readings of were observed in New England. Low pressure records for March were set in areas of twelve states along the Eastern Seaboard, with all-time low pressure records set between Tallahassee and Washington, D.C. Snow began to spread over the eastern United States, and a large squall line moved from the Gulf of Mexico into Florida and Cuba. The storm system tracked up the East Coast during Saturday and into Canada by early Monday morning. In the storm's wake, unseasonably cold temperatures were recorded over the next day or two in the Southeast.\n\nThe 1993 Storm of the Century marked a milestone in the weather forecasting of the United States. By March 8, 1993, several operational numerical weather prediction models and medium-range forecasters at the United States National Weather Service recognized the threat of a significant snowstorm. This marked the first time National Weather Service meteorologists were able to predict accurately a system's severity five days in advance. Official blizzard warnings were issued two days before the storm arrived, as shorter-range models began to confirm the predictions. Forecasters were finally confident enough of the computer-forecast models to support decisions by several northeastern states to declare a State of Emergency even before the snow started to fall.\n\nThe storm complex was large and widespread, affecting at least 26 US states and much of eastern Canada. It brought in cold air along with heavy precipitation and hurricane-force winds which, ultimately, caused a blizzard over the affected area; this also included thundersnow from Georgia to Pennsylvania and widespread whiteout conditions. Snow flurries were seen in the air as far south as Jacksonville, Florida, and some areas of central Florida received a trace of snow. The storm severely impacted both ground and air travel. Airports were closed all along the eastern seaboard, and flights were cancelled or diverted, thus stranding many passengers along the way. Every airport from Halifax, Nova Scotia, to Tampa, was closed for some time because of the storm. Highways were also closed or restricted all across the affected region, even in states generally well prepared for snow emergencies.\n\nSome affected areas in the Appalachian Mountain region saw of snow, and snowdrifts as high as . Mount Leconte, Tennessee recorded 60 inches of snowfall and Mount Mitchell, N.C. recorded 49 inches. The volume of the storm's total snowfall was later computed to be , an amount which would weigh (depending on the variable density of snow) between 5.4 and 27 billion tons.\n\nThe weight of the record snowfalls collapsed several factory roofs in the South; and snowdrifts on the windward sides of buildings caused a few decks with substandard anchors to fall from homes. Though the storm was forecast to strike the snow-prone Appalachian Mountains, hundreds of people were nonetheless rescued from the Appalachians, many caught completely off guard on the Appalachian Trail or in cabins and lodges in remote locales. Snow drifts up to were observed at Mount Mitchell. Snowfall totals of between were widespread across northwestern North Carolina. Boone, North Carolina—in a high-elevation area accustomed to heavy snowfalls—was nonetheless caught off-guard by more than 30 inches of snow and 24 hours of temperatures below . Boone's Appalachian State University closed that week, for the first time in its history. Stranded motorists at Deep Gap broke into Parkway Elementary School to survive, and National Guard helicopters dropped hay in fields to keep livestock from starving in northern N.C. mountain counties. \n\nIn Virginia, the LancerLot sports arena in Vinton collapsed due to the weight of the record snowfall, as did the roofs of a Lowe's store in Christiansburg and the Dedmon Center, at Radford University. Thousands of travelers were stranded along interstate highways in Southwest Virginia.\nElectricity was not restored to many isolated rural areas for up to three weeks, with power outages occurring all over the east. Nearly 60,000 lightning strikes were recorded as the storm swept over the country for a total of 72 hours. As one of the most powerful, complex storms in recent history, this storm was described as the \"Storm of the Century\" by many of the areas affected.\n\nThe United States Coast Guard dealt with \"absolutely incredible, unbelievable\" conditions within the Gulf of Mexico. The 200-ft. freighter \"Fantastico\" sank 70 miles off Ft. Myers, Florida, and seven of her crew died when a Coast Guard helicopter was forced back to base due to low fuel levels after rescuing three of her crew. The 147-ft. freighter \"Miss Beholden\" ran aground on a coral reef ten miles from Key West, Florida. Several other smaller vessels sank in the rough seas. In all, the Coast Guard rescued 235 people from over 100 boats across the Gulf of Mexico during the tempest.\n\nBesides producing record-low barometric pressure across a swath of the Southeast and Mid-Atlantic states, and contributing to one of the nation's biggest snowstorms, the low produced a potent squall line ahead of its cold front. The squall line produced a serial derecho as it moved into Florida and Cuba shortly after midnight on March 13. Straight-line winds gusted above 100 mph (, ) at many locations in Florida as the squall line moved through. A substantial tree fall was seen statewide from this system. The supercells in the derecho produced eleven tornadoes. The first tornado was an F2 that touched down in Chiefland at 04:38 UTC on March 13, damaging several mobile and downing trees and power lines. Three people were killed and seven people sustained injures. Around the same time, an F1 tornado was spawned near Crystal River. After moving eastward into the town, the twister damaged 15 homes, several of them severely. A total of three people were injured. The next tornado was a waterspout that moved ashore over Treasure Island around 05:00 UTC. Rated F0, the tornado deroofed one home, damaged several others, and impacted a few boats.\nAround 05:04 UTC, an F0 tornado was reported in New Port Richey, damaging several homes and injuring 11 people. About 16 minutes later, an F2 tornado formed to the southwest of Ocala. Many trees fell and several storage buildings and a warehouse suffered extensive damage, while one hangar was destroyed and two others received major damage at the Ocala International Airport. At 05:20 UTC, approximately the same time as the Ocala tornado, another twister – rated F1 – touched down near LaCrosse. Several trees and power lines were downed and a few homes were destroyed, one from a propane explosion. One person was killed and four others received injuries. About 10 minutes later, another F2 twister was spawned near Howey-in-the-Hills. It moved through Mount Dora, destroying 13 homes, substantially damaging 80 homes, and inflicting minor damage on 266 homes. One person, a 5-month-old baby, was killed, while two others were injured.\n\nAt 05:30 UTC, a waterspout-turned F0 tornado tossed a sailboat about at the Davis Islands yacht club in Tampa, while five other boats broke loose from their cradles and twelve were smashed into the seawall. About 30 minutes later,\nan F1 tornado formed in Jacksonville, demolishing four dwellings and damaging sixteen others. Also at 06:00 UTC, an F0 tornado spawned near Bartow snapped a few trees and damaged a few doors. The eleventh and final tornado developed in Jacksonville at 06:10 UTC. The twister damaged a few trees near the Jacksonville International Airport. At the airport itself, the tornado damaged several jetways and service vehicles, while a Boeing 737 was pushed about .\n\nA substantial storm surge was also generated along the gulf coast from Apalachee Bay in the Florida Panhandle to north of Tampa Bay. Due to the angle of the coast relative to the approaching squall, Taylor County along the eastern portion of Apalachee Bay and Hernando County north of Tampa were especially hard-hit.\n\nStorm surges in those areas reached up to , higher than many hurricanes. With little advance warning of incoming severe conditions, some coastal residents were awakened in the early morning of March 13 by the waters of the Gulf of Mexico rushing into their homes. More people died from drowning in this storm than during Hurricanes Hugo and Andrew combined. Overall, the storm's surge, winds, and tornadoes damaged or destroyed 18,000 homes. A total of 47 lives were lost in Florida due to this storm.\n\nIn Cuba, wind gusts reached in the Havana area. A survey conducted by a research team from the Institute of Meteorology of Cuba suggests that the maximum winds could have been as high as . It is the most damaging squall line ever recorded in Cuba.\n\nThere was widespread and significant damage in Cuba, with damage estimated as intense as F2. The squall line finally moved out of Cuba near sunrise, leaving 10 deaths and US$1 billion in damage on the island.\n\n"}
{"id": "12527791", "url": "https://en.wikipedia.org/wiki?curid=12527791", "title": "Alkali metal halide", "text": "Alkali metal halide\n\nAlkali metal halides (also known as alkali halides) are the family of inorganic compounds with the chemical formula MX, where M is an alkali metal and X is a halogen. These compounds are the often commercially significant sources of these metals and halides. The best known of these compounds is sodium chloride, table salt. \nMost alkali metal halides crystallize with the face centered cubic lattices. In this structure both the metals and halides feature octahedral coordination geometry, in which each ion has a coordination number of six. Caesium chloride, bromide, and iodide crystallize in a body-centered cubic lattice that accommodates coordination number of eight for the larger metal cation (and the anion also).\n\nThe alkali metal halides exist as colourless crystalline solids, although as finely ground powders appear white. They melt at high temperature, usually several hundred degrees to colorless liquids. Their high melting point reflects their high lattice energies. At still higher temperatures, these liquids evaporate to give gases composed of diatomic molecules.\n\nThese compounds dissolve in polar solvents to give ionic solutions that contain highly solvated anions and cations. Alkali halides dissolve large amounts of the corresponding alkali metal: caesium is completely miscible at all temperatures above the melting point.\n\nThe table below provides links to each of the individual articles for these compounds. The numbers beside the compounds show the electronegativity difference between the elements based on the Pauling scale. The higher the number is, the more ionic the solid is.\n\n"}
{"id": "2706104", "url": "https://en.wikipedia.org/wiki?curid=2706104", "title": "Americathon", "text": "Americathon\n\nAmericathon (also known as Americathon 1998) is a 1979 American comedy film starring John Ritter, Fred Willard, Peter Riegert, Harvey Korman, and Nancy Morgan, with narration by George Carlin, based on a play by Firesign Theatre alumni Phil Proctor and Peter Bergman. The film also includes appearances by Jay Leno, Meat Loaf, Tommy Lasorda, and Chief Dan George, with a musical performance by Elvis Costello.\n\nBeing set 20 years into the future, the film contains many prophetic elements, such as: predicting the demise of the Soviet Union, the prevalence of reality television, and the sale of public assets to the private sector (a trend starting shortly after the film's release). Also, The Beach Boys are shown still together and recording in 1998.\n\nIn the (then-near future) year 1998, the United States has run out of oil, and many Americans are living in their now-stationary cars and using nonpowered means of transportation such as jogging, riding bicycles and rollerskating. Many Americans wear sweatsuits. Paper money has become completely worthless, with all business transactions being conducted in gold; even a coin-operated elevator warns, \"Gold Coins Only\". In search of leadership, Americans elect Chet Roosevelt (Ritter) as President. Roosevelt, a \"cosmically inspired\" former governor of California, proves to have little in common with Teddy Roosevelt or FDR other than his name. Roosevelt, an overly-optimistic man who quotes positive affirmation slogans, stages a number of highly publicized fund raising events, all of which fail. He becomes interested in having a relationship with Vietnamese American pop superstar Mouling Jackson. Real money comes in the form of loans from a cartel of Native Americans, led by billionaire Sam Birdwater (George), in control of Nike (which has been renamed \"National Indian Knitting Enterprise\").\n\nThe federal government, now housed in \"The Western White House\" (a sub-leased condominium in Marina del Rey, California), finds itself facing national bankruptcy and in danger of being foreclosed and repossessed when Birdwater goes public on national television with the fact that he lent America billions of dollars and now wants the money back, the alternative being foreclosure and the country reverting to its original owners, stating, \"Hey, I have to eat, too. Does that make me a bad guy?\".\n\nIn desperation, Roosevelt hires a young television consultant Eric McMerkin (Riegert) to help produce a national raffle. Instead, they decide that the only way enough money can be raised to save America is instead to run a national telethon, and hire vapid TV celebrity Monty Rushmore (Korman) to host it. However, Presidential adviser Vincent Vanderhoff (Willard) is secretly plotting to have the telethon fail so that representatives of the United Hebrab Republic (formed by the merger of Israel and the Arab states) can purchase what is left of the country when Birdwater forecloses.\n\n\nThe soundtrack features \"It's a Beautiful Day\" by The Beach Boys, \"Crawling to the USA\" by Elvis Costello and \"Get a Move On\" by Eddie Money.\n\nDorothy Stratten appears, uncredited and in a brief non-speaking role, in a Playboy bunny style outfit during a scene where Meat Loaf's character donates blood. The Del Rubio triplets can be seen performing \"America the Beautiful\" behind several posing bodybuilders. John Carradine was to have played \"Uncle Sam\" in this film, but his scenes did not make the final cut edit. Director Neal Israel has a cameo as a protesting Rabbi holding a picket sign reading \"The President Is A Yutz\" (Yiddish for \"a stupid, clueless person\").\n\nIn a scene where Eric McMerkin is reading a list of \"Government Approved\" performers, the names of \"Proctor & Bergman\" (the co-authors of the original play) can be seen fifth on the list, credited as \"Comics.\" Peter Bergman and Phil Proctor were members of the satirical comedy performance group Firesign Theatre.\n\nTo promote the movie, in 1979 Ted Coombs roller skated across the United States and back and gained a place in the \"Guinness Book of World Records.\" A photo novel of the film was also released in 1979, and the musical soundtrack was released on both vinyl and audiocassette by Lorimar Records.\n\nThe film was made available on VHS and laserdisc in the 1980s by Lorimar Home Video, both of which are now out of print. The home video rights passed to Warner Bros. in the late 1980s as part of their purchase of Lorimar. Warner Home Video made the film available in January 2011 on DVD in widescreen (1.85:1) format as part of their Warner Archive Manufacture-on-demand collection.\n\nIn 1984, New York City public radio station WNYC sponsored a marathon of American music dubbed \"Americathon '84.\"\n\nReferencing the movie's futuristic premise itself, there were many societal or political forecasts woven into the storyline, and a number of these have become reality, including:\n\nThe film's official coming attractions trailer includes the quote: \"...see \"Americathon\" at your local theater before you see it happening in your own front yard!\"\n\n"}
{"id": "55031144", "url": "https://en.wikipedia.org/wiki?curid=55031144", "title": "Ardcanaght Stones", "text": "Ardcanaght Stones\n\nThe Ardcanaght Stones are a pair of ogham stones (CIIC 246) forming a National Monument located in County Kerry, Ireland.\n\nArdcanaght Stones are located west of Castlemaine, to the north of the River Maine.\n\nThe inscriptions are too fragmentary to give them a precise date. Ogham carvings were made in Ireland between the 4th and 10th centuries. They were rediscovered in the 1940s and moved here in recent years from a cillín.\n\nThe two stones are accompanied by a large standing stone, 1.6 m (5 ft 3 in) tall.\n\nThe stones are:\n"}
{"id": "39490476", "url": "https://en.wikipedia.org/wiki?curid=39490476", "title": "Axial turbine", "text": "Axial turbine\n\nAn axial turbine is a turbine in which the flow of the working fluid is parallel to the shaft, as opposed to radial turbines, where the fluid runs around a shaft, as in a watermill. An axial turbine has similar construction as an axial compressor, but it operates in the reverse, converting flow of the fluid into rotating mechanical energy. \n\nA set of static guide vanes or nozzle vanes accelerates and adds swirl to the fluid and directs it to the next row of turbine blades mounted on a turbine rotor.\n\nThe angles in the absolute system are noted by alpha (α) and the angles in the relative system are noted by beta (β). Axial and tangential components of both absolute and relative velocities are shown in the figure. Static and stagnation values of pressure and enthalpy in the absolute and relative systems are also shown.\n\nIt is often assumed that the axial velocity component remains constant through the stage. From this condition we get,\nc = c cos α = c cos α:= w cos β = c cos α = w cos α\nAlso, for constant axial velocity yields a useful relation:\n\ntan α + tan α = tan β + tan β\n\nA single-stage impulse turbine is shown in Figure\n\nThere is no change in the static pressure through the rotor of an impulse machine. The variation of pressure and velocity of the fluid through the stage is also shown in Figure.\n\nThe absolute velocity of the fluid increases corresponding to the pressure drop through the nozzle blade row in which only transformation of energy occurs. The transfer of energy occurs only across the rotor blade row. Therefore, the absolute fluid velocity decreases through this as shown in the figure.\nIn the absence of any pressure drop through the rotor blades the relative velocities at their entry and exit are the same for fricitionless flow. To obtain this condition the rotor blade angles must be equal. Therefore, the utilization factor is given by\n\nWhen the pressure drop available is large, it cannot all be used in one turbine stage. A single stage utilizing a large pressure drop will have an impractically high peripheral speed of its rotor. This would lead to either a larger diameter or a very high rotational speed. Therefore, machines with large pressure drops employ more than one stage.\n\nOne of the methods to employ multi-stage expansion in impulse turbines is to generate high velocity of the fluid by causing it to expand through a large pressure drop in the nozzle blade row. This high velocity fluid then transfers its energy in a number of stages by employing many rotor blade rows separated by rows of fixed guide blades. A two-stage velocity compounded impulse turbine is shown in Figure\n\nThe decrease in the absolute velocity of the fluid across the two rotor blade rows (R and R) is due to the energy transfer; the slight decrease in the fluid velocity through the fixed guide blades (F) is due to losses. Since the turbine is of the impulse type, the pressure of the fluid remains constant after its expansion in the nozzle blade row. Such stages are referred to as velocity or Curtis stages.\n\nThere are two major problems in velocity-compounded stages:\n\nTo avoid these problems, another method of utilizing a high pressure ratio is employed in which the total pressure drop is divided into a number of impulse stages. These are known as pressure-compounded or Rateau stages. On account of the comparatively lower pressure drop, the nozzle blade rows are subsonic (M < 1). Therefore, such a stage does not suffer from the disabilities of the velocity stages.\n\nFigure shows the variation of pressure and velocity of steam through the two pressure stages of an impulse turbine. The nozzle blades in\neach stage receive flow in the axial direction.\n\nSome designers employ pressure stages up to the last stage. This gives a turbine of shorter length as compared to the reaction type, with a penalty on efficiency.\n\nFigure shows two reaction stages and the variation of pressure and velocity of the gas in them. The gas pressure decreases continuously over both fixed and moving rows of blades. Since the pressure drop in each stage is smaller as compared to the impulse stages, the gas velocities are relatively low. Besides this the flow is accelerating throughout. These factors make the reaction stages aerodynamically more efficient though the tip leakage loss is increased on account of the relatively higher pressure difference across the rotor blades.\n\nMulti-stage reaction turbines employ a large pressure drop by dividing it to smaller values in individual stages. Thus the reaction stages are like the pressure-compounded stages with a new element of “reaction” introduced in them, i.e. of accelerating the flow through rotor blade rows also.\n\nThe blade-to-gas speed ratio parameter (velocity ratio) σ = u/c. Efficiencies of the turbine stages can also be plotted against this ratio. Such plots for some impulse and reaction stages are shown in the figure.\n\nThe performance of steam turbines is often presented in this form. The curves in Figure also show the optimum values of the velocity ratio and the range of off-design for various types of stages. The fifty per cent reaction stage shows a wider range. Another important aspect that is depicted here is that in applications where high gas velocities (due to high pressure ratio) are unavoidable, it is advisable to employ impulse stages to achieve practical and convenient values of the size and speed of the machine. Sometimes it is more convenient to use an isentropic velocity ratio. This is the ratio of the blade velocity and the isentropic gas velocity that would be obtained in its isentropic expansion through the stage pressure ratio.\n\nThe losses occur in an actual turbine due to disc and bearing friction. Figure shows the energy flow diagram for the impulse stage of an axial turbine. Numbers in brackets indicate the order of energy or loss corresponding to 100 units of isentropic work (h – h).\n\nIt is seen that the energy reaching the shaft after accounting for stage cascade losses (nozzle and rotor blade aerodynamic losses) and leaving loss is about 85% of the ideal value; shaft losses are a negligible proportion of this value.\n\n\n"}
{"id": "8735392", "url": "https://en.wikipedia.org/wiki?curid=8735392", "title": "Blowing agent", "text": "Blowing agent\n\nA blowing agent is a substance which is capable of producing a cellular structure via a foaming process in a variety of materials that undergo hardening or phase transition, such as polymers, plastics, and metals. They are typically applied when the blown material is in a liquid stage. The cellular structure in a matrix reduces density, increasing thermal and acoustic insulation, while increasing relative stiffness of the original polymer.\n\nBlowing agents (also known as 'pneumatogens') or related mechanisms to create holes in a matrix producing cellular materials, have been classified as follows: \n\nPhysical blowing agents e.g. CFCs (however, these are ozone depletants, banned by Montreal Protocol of 1987), HCFCs (replaced CFCs, but are still ozone depletants, therefore being phased out), hydrocarbons (e.g. pentane, isopentane, cyclopentane), liquid CO. The bubble/foam-making process is irreversible and endothermic, i.e. it needs heat (e.g. from a melt process or the chemical exotherm due to cross-linking), to volatilize a liquid blowing agent. However, on cooling the blowing agent will condense, i.e. a reversible process.\n\nChemical blowing agents include isocyanate and water for polyurethane, azodicarbonamide for vinyl, hydrazine and other nitrogen-based materials for thermoplastic and elastomeric foams, and sodium bicarbonate for thermoplastic foams. Gaseous products and other byproducts are formed by a chemical reaction of the chemical blowing agent, promoted by the heat of the foam production process or a reacting polymer's exothermic heat. Since the blowing reaction occurs forming low molecular weight compounds acting as the blowing gas, additional exothermic heat is also released. Powdered titanium hydride is used as a foaming agent in the production of metal foams, as it decomposes to form hydrogen gas and titanium at elevated temperatures. Zirconium(II) hydride is used for the same purpose. Once formed the low molecular weight compounds will never revert to the original blowing agent; the reaction is irreversible.\n\nMixed physical/chemical blowing agents are used to produce flexible PU foams with very low densities. Here both the chemical and physical blowing are used in tandem to balance each other out with respect to thermal energy released and absorbed, minimizing temperature rise. Otherwise excessive exothermic heat because of high loading of a physical blowing agent can cause thermal degradation of a developing thermoset or polyurethane material. For instance, to avoid this in polyurethane systems isocyanate and water (which react to form carbon dioxide) are used in combination with liquid carbon dioxide (which boils to give gaseous form) in the production of very low density flexible PU foams for mattresses.\n\nMechanically made foams and froths, involves methods of introducing bubbles into liquid polymerisable matrices (e.g. an unvulcanised elastomer in the form of a liquid latex). Methods include whisking-in air or other gases or low boiling volatile liquids in low viscosity lattices, or the injection of a gas into an extruder barrel or a die, or into injection molding barrels or nozzles and allowing the shear/mix action of the screw to disperse the gas uniformly to form very fine bubbles or a solution of gas in the melt. When the melt is molded or extruded and the part is at atmospheric pressure, the gas comes out of solution expanding the polymer melt immediately before solidification. Frothing (akin to beating egg whites making a meringue), is also used to stabilize foamed liquid reactants, e.g. to prevent slumping occurring on vertical walls before cure – (i.e. avoiding foam collapse and sliding down a vertical face due to gravity).\n\nSoluble fillers, e.g. solid sodium chloride crystals mixed into a liquid urethane system, which is then shaped into a solid polymer part, the sodium chloride is later washed out by immersing the solid molded part in water for some time, to leave small inter-connected holes in relatively high density polymer products, (e.g. Porvair synthetic leather materials for shoe uppers).\n\nHollow spheres and porous particles (e.g. glass shells/spheres, epoxide shells, PVDC shells, fly ash, vermiculite, other reticulated materials) are mixed and dispersed in the liquid reactants, which are then shaped into a solid polymer part containing a network of voids.\n"}
{"id": "144843", "url": "https://en.wikipedia.org/wiki?curid=144843", "title": "Brisance", "text": "Brisance\n\nBrisance is the shattering capability of a high explosive, determined mainly by its detonation pressure. The term can be traced from the French verb \"briser\" (to break or shatter) ultimately derived from the Celtic word \"brissim\" (to break). Brisance is of practical importance for determining the effectiveness of an explosion in fragmenting shells, bomb casings, grenades, structures, and the like. The sand crush test and Trauzl lead block test are commonly used to determine the relative brisance in comparison to TNT (which is considered a standard reference for many purposes).\n\nFragmentation occurs by the action of the transmitted shock wave, the strength of which depends on the detonation pressure of the explosive. Generally, the higher this pressure, the finer the fragments generated. High detonation pressure correlates with high detonation velocity, the speed at which the detonation wave propagates through the explosive, but not necessarily with the explosive's total energy (or work capacity), some of which may be released after passage of the detonation wave. A more brisant explosive, therefore, projects smaller fragments but not necessarily at a higher velocity than a less brisant one.\n\nOne of the most brisant of the conventional explosives is cyclotrimethylene trinitramine (also known as RDX or Hexogen). RDX is the explosive agent in the plastic explosive commonly known as C-4, constituting 91% RDX by mass.\n\n\n"}
{"id": "9270946", "url": "https://en.wikipedia.org/wiki?curid=9270946", "title": "Calcium aluminates", "text": "Calcium aluminates\n\nCalcium aluminates are a range of minerals obtained by heating calcium oxide and aluminium oxide together at high temperatures. They are encountered in the manufacture of refractories and cements.\n\nThe stable phases shown in the phase diagram (formed at atmospheric pressure under an atmosphere of normal humidity) are:\n\n\nIn addition, other phases include:\n\n\n"}
{"id": "1994705", "url": "https://en.wikipedia.org/wiki?curid=1994705", "title": "Ceftazidime", "text": "Ceftazidime\n\nCeftazidime, sold under the brand names Fortaz among others, is an antibiotic useful for the treatment of a number of bacterial infections. Specifically it is used for joint infections, meningitis, pneumonia, sepsis, urinary tract infections, malignant otitis externa, \"Pseudomonas aeruginosa\" infection, and vibrio infection. It is given by injection into a vein or muscle.\nCommon side effects include nausea, allergic reactions, and pain at the site of injection. Other side effects may include \"Clostridium difficile\" diarrhea. It is not recommended in people who have had previous anaphylaxis to a penicillin. Its use is relatively safe during pregnancy and breastfeeding. It is in the third-generation cephalosporin family of medications and works by interfering with the bacteria's cell wall.\nCeftazidime was patented in 1978 and came into commercial use in 1984. It is on the World Health Organization's List of Essential Medicines, the most effective and safe medicines needed in a health system. Ceftazidime is available as a generic medication. The wholesale cost in the developing world is about US$2.84–16.76 per day. In the United States a course of treatment costs $100–200.\n\nCeftazidime is used to treat lower respiratory tract, skin, urinary tract, blood-stream, joint, and abdominal infections, and meningitis. \nThe drug is given intravenously (IV) or intramuscularly (IM) every 8–12 hours (two or three times a day), with dose and frequencing varying by the type of infection, severity, and/or renal function of the patient. Ceftazidime is also commonly prescribed off-label for nebulization in Cystic Fibrosis patients for the suppression of Pseudomonas in the lungs as well as the treatment of pulmonary exacerbations. Those with kidney disease are dosed less frequently.\n\nCeftazidime is the first-line treatment for the tropical infection, melioidosis, an important cause of sepsis in Asia and Australia.\nLabeled indications include the treatment of patients with: \n\nAs a class, cephalosporins have activity against Gram-positive and Gram-negative bacteria. The balance of activity tips toward Gram-positive organisms for earlier generations; later generations of cephalosporins have more Gram-negative coverage. Ceftazidime is one of the few in this class with activity against \"Pseudomonas\". It is not active against methicillin-resistant \"Staphylococcus aureus\".\n\nClinically relevant organisms against which ceftazidime has activity include:\nThe following represents MIC susceptibility data for a few clinically significant pathogens:\n\nCeftazidime is generally well-tolerated. When side effects do occur, they are most commonly local effects from the intravenous line site, allergic reactions, and gastrointestinal symptoms. According to one manufacturer, in clinical trials, allergic reactions including itching, rash, and fever, happened in fewer than 2% of patients. Rare but more serious allergic reactions, such as toxic epidermal necrolysis, Stevens-Johnson syndrome, and erythema multiforme, have been reported with this class of antibiotics, including ceftazidime. Gastrointestinal symptoms, including diarrhea, nausea, vomiting, and abdominal pain, were reported in fewer than 2% of patients.\n\nAnother source reported, in addition, blood tests of patients may show increased eosinophils (8%), increased lactate dehydrogenase (6%), increased gamma-glutamyl transferase (5%), positive direct Coombs test (4%), increased platelets (thrombocythemia) (2%), increased ALT (7%), increased AST (6%), or increased alkaline phosphatase (4%).\n\nCeftazidime is contraindicated in people with a known allergy to ceftazidime or to any other cephalosporin antibiotic.\n\nCeftazidime is mainly eliminated by the kidneys into the urine. As such, drug levels in the blood may build up in persons with kidney injury or kidney disease. This includes those on dialysis. In these cases of renal impairment, the drug is dosed less frequently. No dose adjustment is needed for those with liver disease.\n\nCeftazidime falls under the pregnancy category B. According to the manufacturer, research studies in mice and rats showed no evidence of harm to the fetus, even at up to 40 times the human dose of ceftazidime. Importantly, though, no high-quality research studies of the effects of the drug in pregnant women were conducted.\n\nThird-generation cephalosporins differ from earlier generations in the presence of a C=N-OCH group in their chemical structure (cefuroxime & cefuzonam also bear this functional group but are only listed as class II). This group provides improved stability against certain beta-lactamase enzymes produced by Gram-negative bacteria. These bacterial enzymes rapidly destroy earlier-generation cephalosporins by breaking open the drug's beta-lactam chemical ring, leading to antibiotic resistance. Though initially active against these bacteria, with widespread use of third-generation cephalosporins, some Gram-negative bacteria that produce extended-spectrum beta-lactamases (ESBLs) are even able to inactivate the third-generation cephalosporins. Infections caused by ESBL-producing Gram-negative bacteria are of particular concern in hospitals and other healthcare facilities.\n\nIn addition to the \"syn\"-configuration of the imino side chain, compared to other third-generation cephalosporins, the more complex moiety (containing two methyl and a carboxylic acid group) confers extra stability to β-lactamase enzymes produced by many Gram-negative bacteria. The extra stability to β-lactamases increases the activity of ceftazidime against otherwise resistant Gram-negative organisms including \"Pseudomonas aeruginosa\". The charged pyridinium moiety increases water-solubility. Ceftazidime shares the same variable R-group side chain with aztreonam, a monobactam antibiotic; the two drugs share a similar spectrum of activity, including activity against \"Pseudomonas\".\n\n"}
{"id": "7534", "url": "https://en.wikipedia.org/wiki?curid=7534", "title": "Centripetal force", "text": "Centripetal force\n\nA centripetal force (from Latin \"centrum\", \"center\" and \"petere\", \"to seek\") is a force that makes a body follow a curved path. Its direction is always orthogonal to the motion of the body and towards the fixed point of the instantaneous center of curvature of the path. Isaac Newton described it as \"a force by which bodies are drawn or impelled, or in any way tend, towards a point as to a centre\". In Newtonian mechanics, gravity provides the centripetal force responsible for astronomical orbits.\n\nOne common example involving centripetal force is the case in which a body moves with uniform speed along a circular path. The centripetal force is directed at right angles to the motion and also along the radius towards the centre of the circular path. The mathematical description was derived in 1659 by the Dutch physicist Christiaan Huygens.\n\nThe magnitude of the centripetal force on an object of mass \"m\" moving at tangential speed \"v\" along a path with radius of curvature \"r\" is: \n\nwhere formula_2 is the centripetal acceleration.\nThe direction of the force is toward the center of the circle in which the object is moving, or the osculating circle (the circle that best fits the local path of the object, if the path is not circular).\nThe speed in the formula is squared, so twice the speed needs four times the force. The inverse relationship with the radius of curvature shows that half the radial distance requires twice the force. This force is also sometimes written in terms of the angular velocity \"ω\" of the object about the center of the circle, related to the tangential velocity by the formula\n\nso that\n\nExpressed using the orbital period \"T\" for one revolution of the circle,\n\nthe equation becomes\n\nIn particle accelerators, velocity can be very high (close to the speed of light in vacuum) so the same rest mass now exerts greater inertia (relativistic mass) thereby requiring greater force for the same centripetal acceleration, so the equation becomes:\n\nwhere\n\nis called the Lorentz factor.\n\nMore intuitively:\n\nwhich is the rate of change of relativistic momentum (formula_10)\n\nIn the case of an object that is swinging around on the end of a rope in a horizontal plane, the centripetal force on the object is supplied by the tension of the rope. The rope example is an example involving a 'pull' force. The centripetal force can also be supplied as a 'push' force, such as in the case where the normal reaction of a wall supplies the centripetal force for a wall of death rider.\n\nNewton's idea of a centripetal force corresponds to what is nowadays referred to as a central force. When a satellite is in orbit around a planet, gravity is considered to be a centripetal force even though in the case of eccentric orbits, the gravitational force is directed towards the focus, and not towards the instantaneous center of curvature.\n\nAnother example of centripetal force arises in the helix that is traced out when a charged particle moves in a uniform magnetic field in the absence of other external forces. In this case, the magnetic force is the centripetal force that acts towards the helix axis.\n\nBelow are three examples of increasing complexity, with derivations of the formulas governing velocity and acceleration.\n\nUniform circular motion refers to the case of constant rate of rotation. Here are two approaches to describing this case.\n\nIn two dimensions, the position vector formula_11, which has magnitude (length) formula_12 and directed at an angle formula_13 above the x-axis, can be expressed in Cartesian coordinates using the unit vectors formula_14 and formula_15:\n\nAssume uniform circular motion, which requires three things.\n\nNow find the velocity formula_21 and acceleration formula_22 of the motion by taking derivatives of position with respect to time.\n\nNotice that the term in parenthesis is the original expression of formula_11 in Cartesian coordinates. Consequently,\n\nnegative shows that the acceleration is pointed towards the center of the circle (opposite the radius), hence it is called \"centripetal\" (i.e. \"center-seeking\"). While objects naturally follow a straight path (due to inertia), this centripetal acceleration describes the circular motion path caused by a centripetal force.\n\nThe image at right shows the vector relationships for uniform circular motion. The rotation itself is represented by the angular velocity vector Ω, which is normal to the plane of the orbit (using the right-hand rule) and has magnitude given by:\n\nwith \"θ\" the angular position at time \"t\". In this subsection, d\"θ\"/d\"t\" is assumed constant, independent of time. The distance traveled dℓ of the particle in time d\"t\" along the circular path is\n\nwhich, by properties of the vector cross product, has magnitude \"r\"d\"θ\" and is in the direction tangent to the circular path.\n\nConsequently,\n\nDifferentiating with respect to time,\n\nLagrange's formula states:\n\nApplying Lagrange's formula with the observation that Ω • r(\"t\") = 0 at all times,\n\nIn words, the acceleration is pointing directly opposite to the radial displacement r at all times, and has a magnitude:\n\nwhere vertical bars |...| denote the vector magnitude, which in the case of r(\"t\") is simply the radius \"r\" of the path. This result agrees with the previous section, though the notation is slightly different.\n\nWhen the rate of rotation is made constant in the analysis of nonuniform circular motion, that analysis agrees with this one.\n\nA merit of the vector approach is that it is manifestly independent of any coordinate system.\n\nThe upper panel in the image at right shows a ball in circular motion on a banked curve. The curve is banked at an angle \"θ\" from the horizontal, and the surface of the road is considered to be slippery. The objective is to find what angle the bank must have so the ball does not slide off the road. Intuition tells us that, on a flat curve with no banking at all, the ball will simply slide off the road; while with a very steep banking, the ball will slide to the center unless it travels the curve rapidly.\n\nApart from any acceleration that might occur in the direction of the path, the lower panel of the image above indicates the forces on the ball. There are \"two\" forces; one is the force of gravity vertically downward through the center of mass of the ball \"mg, where \"m\" is the mass of the ball and g is the gravitational acceleration; the second is the upward normal force exerted by the road at a right angle to the road surface \"ma. The centripetal force demanded by the curved motion is also shown above. This centripetal force is not a third force applied to the ball, but rather must be provided by the net force on the ball resulting from vector addition of the normal force and the force of gravity. The resultant or net force on the ball found by vector addition of the normal force exerted by the road and vertical force due to gravity must equal the centripetal force dictated by the need to travel a circular path. The curved motion is maintained so long as this net force provides the centripetal force requisite to the motion.\n\nThe horizontal net force on the ball is the horizontal component of the force from the road, which has magnitude |F| = \"m\"|a|sin\"θ\". The vertical component of the force from the road must counteract the gravitational force: |F| = \"m\"|a|cos\"θ\" = \"m\"|g|, which implies |a|=|g| / cos\"θ\". Substituting into the above formula for |F| yields a horizontal force to be:\n\nOn the other hand, at velocity |v| on a circular path of radius \"r\", kinematics says that the force needed to turn the ball continuously into the turn is the radially inward centripetal force F of magnitude:\n\nConsequently, the ball is in a stable path when the angle of the road is set to satisfy the condition:\n\nor,\n\nAs the angle of bank \"θ\" approaches 90°, the tangent function approaches infinity, allowing larger values for |v|/\"r\". In words, this equation states that for faster speeds (bigger |v|) the road must be banked more steeply (a larger value for \"θ\"), and for sharper turns (smaller \"r\") the road also must be banked more steeply, which accords with intuition. When the angle \"θ\" does not satisfy the above condition, the horizontal component of force exerted by the road does not provide the correct centripetal force, and an additional frictional force tangential to the road surface is called upon to provide the difference. If friction cannot do this (that is, the coefficient of friction is exceeded), the ball slides to a different radius where the balance can be realized.\n\nThese ideas apply to air flight as well. See the FAA pilot's manual.\n\nAs a generalization of the uniform circular motion case, suppose the angular rate of rotation is not constant. The acceleration now has a tangential component, as shown the image at right. This case is used to demonstrate a derivation strategy based on a polar coordinate system.\n\nLet r(\"t\") be a vector that describes the position of a point mass as a function of time. Since we are assuming circular motion, let r(\"t\") = \"R\"·u, where \"R\" is a constant (the radius of the circle) and u is the unit vector pointing from the origin to the point mass. The direction of u is described by \"θ\", the angle between the x-axis and the unit vector, measured counterclockwise from the x-axis. The other unit vector for polar coordinates, u is perpendicular to u and points in the direction of increasing \"θ\". These polar unit vectors can be expressed in terms of Cartesian unit vectors in the \"x\" and \"y\" directions, denoted i and j respectively:\n\nand\n\nOne can differentiate to find velocity:\n\nwhere \"ω\" is the angular velocity d\"θ\"/d\"t\".\n\nThis result for the velocity matches expectations that the velocity should be directed tangentially to the circle, and that the magnitude of the velocity should be \"rω\". Differentiating again, and noting that\n\nwe find that the acceleration, a is:\n\nThus, the radial and tangential components of the acceleration are:\n\nwhere |v| = \"r\" ω is the magnitude of the velocity (the speed).\n\nThese equations express mathematically that, in the case of an object that moves along a circular path with a changing speed, the acceleration of the body may be decomposed into a perpendicular component that changes the direction of motion (the centripetal acceleration), and a parallel, or tangential component, that changes the speed.\n\nThe above results can be derived perhaps more simply in polar coordinates, and at the same time extended to general motion within a plane, as shown next. Polar coordinates in the plane employ a radial unit vector u and an angular unit vector u, as shown above. A particle at position r is described by:\n\nwhere the notation \"ρ\" is used to describe the distance of the path from the origin instead of \"R\" to emphasize that this distance is not fixed, but varies with time. The unit vector u travels with the particle and always points in the same direction as r(\"t\"). Unit vector u also travels with the particle and stays orthogonal to u. Thus, u and u form a local Cartesian coordinate system attached to the particle, and tied to the path traveled by the particle. By moving the unit vectors so their tails coincide, as seen in the circle at the left of the image above, it is seen that u and u form a right-angled pair with tips on the unit circle that trace back and forth on the perimeter of this circle with the same angle \"θ\"(\"t\") as r(\"t\").\n\nWhen the particle moves, its velocity is\n\nTo evaluate the velocity, the derivative of the unit vector u is needed. Because u is a unit vector, its magnitude is fixed, and it can change only in direction, that is, its change du has a component only perpendicular to u. When the trajectory r(\"t\") rotates an amount d\"θ\", u, which points in the same direction as r(\"t\"), also rotates by d\"θ\". See image above. Therefore, the change in u is\n\nor\n\nIn a similar fashion, the rate of change of u is found. As with u, u is a unit vector and can only rotate without changing size. To remain orthogonal to u while the trajectory r(\"t\") rotates an amount d\"θ\", u, which is orthogonal to r(\"t\"), also rotates by d\"θ\". See image above. Therefore, the change du is orthogonal to u and proportional to d\"θ\" (see image above):\n\nThe image above shows the sign to be negative: to maintain orthogonality, if du is positive with d\"θ\", then du must decrease.\n\nSubstituting the derivative of u into the expression for velocity:\n\nTo obtain the acceleration, another time differentiation is done:\n\nSubstituting the derivatives of u and u, the acceleration of the particle is:\n\nAs a particular example, if the particle moves in a circle of constant radius \"R\", then d\"ρ\"/d\"t\" = 0, v = v, and:\n\nwhere formula_60\n\nThese results agree with those above for nonuniform circular motion. See also the article on non-uniform circular motion. If this acceleration is multiplied by the particle mass, the leading term is the centripetal force and the negative of the second term related to angular acceleration is sometimes called the Euler force.\n\nFor trajectories other than circular motion, for example, the more general trajectory envisioned in the image above, the instantaneous center of rotation and radius of curvature of the trajectory are related only indirectly to the coordinate system defined by u and u and to the length |r(\"t\")| = \"ρ\". Consequently, in the general case, it is not straightforward to disentangle the centripetal and Euler terms from the above general acceleration equation.\n\nLocal coordinates mean a set of coordinates that travel with the particle, and have orientation determined by the path of the particle. Unit vectors are formed as shown in the image at right, both tangential and normal to the path. This coordinate system sometimes is referred to as \"intrinsic\" or \"path coordinates\" or \"nt-coordinates\", for \"normal-tangential\", referring to these unit vectors. These coordinates are a very special example of a more general concept of local coordinates from the theory of differential forms.\n\nDistance along the path of the particle is the arc length \"s\", considered to be a known function of time.\n\nA center of curvature is defined at each position \"s\" located a distance \"ρ\" (the radius of curvature) from the curve on a line along the normal u (\"s\"). The required distance \"ρ\"(\"s\") at arc length \"s\" is defined in terms of the rate of rotation of the tangent to the curve, which in turn is determined by the path itself. If the orientation of the tangent relative to some starting position is \"θ\"(\"s\"), then \"ρ\"(\"s\") is defined by the derivative d\"θ\"/d\"s\":\n\nThe radius of curvature usually is taken as positive (that is, as an absolute value), while the \"curvature\" \"κ\" is a signed quantity.\n\nA geometric approach to finding the center of curvature and the radius of curvature uses a limiting process leading to the osculating circle. See image above.\n\nUsing these coordinates, the motion along the path is viewed as a succession of circular paths of ever-changing center, and at each position \"s\" constitutes non-uniform circular motion at that position with radius \"ρ\". The local value of the angular rate of rotation then is given by:\n\nwith the local speed \"v\" given by:\n\nAs for the other examples above, because unit vectors cannot change magnitude, their rate of change is always perpendicular to their direction (see the left-hand insert in the image above):\n\nConsequently, the velocity and acceleration are:\nand using the chain-rule of differentiation:\n\nIn this local coordinate system, the acceleration resembles the expression for nonuniform circular motion with the local radius \"ρ\"(\"s\"), and the centripetal acceleration is identified as the second term.\n\nExtending this approach to three dimensional space curves leads to the Frenet–Serret formulas.\n\nLooking at the image above, one might wonder whether adequate account has been taken of the difference in curvature between \"ρ\"(\"s\") and \"ρ\"(\"s\" + d\"s\") in computing the arc length as d\"s\" = \"ρ\"(\"s\")d\"θ\". Reassurance on this point can be found using a more formal approach outlined below. This approach also makes connection with the article on curvature.\n\nTo introduce the unit vectors of the local coordinate system, one approach is to begin in Cartesian coordinates and describe the local coordinates in terms of these Cartesian coordinates. In terms of arc length \"s\", let the path be described as:\n\nThen an incremental displacement along the path d\"s\" is described by:\n\nwhere primes are introduced to denote derivatives with respect to \"s\". The magnitude of this displacement is d\"s\", showing that:\n\nThis displacement is necessarily a tangent to the curve at \"s\", showing that the unit vector tangent to the curve is:\n\nwhile the outward unit vector normal to the curve is\n\nOrthogonality can be verified by showing that the vector dot product is zero. The unit magnitude of these vectors is a consequence of Eq. 1. Using the tangent vector, the angle \"θ\" of the tangent to the curve is given by:\n\nThe radius of curvature is introduced completely formally (without need for geometric interpretation) as:\n\nThe derivative of \"θ\" can be found from that for sin\"θ\":\n\nNow:\n\nin which the denominator is unity. With this formula for the derivative of the sine, the radius of curvature becomes:\n\nwhere the equivalence of the forms stems from differentiation of Eq. 1:\nWith these results, the acceleration can be found:\nas can be verified by taking the dot product with the unit vectors u(\"s\") and u(\"s\"). This result for acceleration is the same as that for circular motion based on the radius \"ρ\". Using this coordinate system in the inertial frame, it is easy to identify the force normal to the trajectory as the centripetal force and that parallel to the trajectory as the tangential force. From a qualitative standpoint, the path can be approximated by an arc of a circle for a limited time, and for the limited time a particular radius of curvature applies, the centrifugal and Euler forces can be analyzed on the basis of circular motion with that radius.\n\nThis result for acceleration agrees with that found earlier. However, in this approach, the question of the change in radius of curvature with \"s\" is handled completely formally, consistent with a geometric interpretation, but not relying upon it, thereby avoiding any questions the image above might suggest about neglecting the variation in \"ρ\".\nTo illustrate the above formulas, let \"x\", \"y\" be given as:\n\nThen:\n\nwhich can be recognized as a circular path around the origin with radius \"α\". The position \"s\" = 0 corresponds to [\"α\", 0], or 3 o'clock. To use the above formalism, the derivatives are needed:\n\nWith these results, one can verify that:\n\nThe unit vectors can also be found:\n\nwhich serve to show that \"s\" = 0 is located at position [\"ρ\", 0] and \"s\" = \"ρ\"π/2 at [0, \"ρ\"], which agrees with the original expressions for \"x\" and \"y\". In other words, \"s\" is measured counterclockwise around the circle from 3 o'clock. Also, the derivatives of these vectors can be found:\n\nTo obtain velocity and acceleration, a time-dependence for \"s\" is necessary. For counterclockwise motion at variable speed \"v\"(\"t\"):\nwhere \"v\"(\"t\") is the speed and \"t\" is time, and \"s\"(\"t\" = 0) = 0. Then:\nwhere it already is established that α = ρ. This acceleration is the standard result for non-uniform circular motion.\n\n\n\n"}
{"id": "360835", "url": "https://en.wikipedia.org/wiki?curid=360835", "title": "Coercivity", "text": "Coercivity\n\nIn electrical engineering and materials science, the coercivity, also called the magnetic coercivity, coercive field or coercive force, is a measure of the ability of a ferromagnetic material to withstand an external magnetic field without becoming demagnetized. An analogous property, electric coercivity, is the ability of a ferroelectric material to withstand an external electric field without becoming depolarized.\n\nFor ferromagnetic material the coercivity is the intensity of the applied magnetic field required to reduce the magnetization of that material to zero \"after\" the magnetization of the sample has been driven to saturation. Thus coercivity measures the resistance of a ferromagnetic material to becoming demagnetized. Coercivity is usually measured in oersted or ampere/meter units and is denoted \"H\". It can be measured using a B-H analyzer or magnetometer.\n\nFerromagnetic materials with high coercivity are called magnetically \"hard\" materials, and are used to make permanent magnets. Materials with low coercivity are said to be magnetically \"soft\". The latter are used in transformer and inductor cores, recording heads, microwave devices, and magnetic shielding.\n\nTypically the coercivity of a magnetic material is determined by measurement of the magnetic hysteresis loop, also called the \"magnetization curve\", as illustrated in the figure. The apparatus used to acquire the data is typically a vibrating-sample or alternating-gradient magnetometer. The applied field where the data line crosses zero is the coercivity. If an antiferromagnet is present in the sample, the coercivities measured in increasing and decreasing fields may be unequal as a result of the exchange bias effect.\n\nThe coercivity of a material depends on the time scale over which a magnetization curve is measured. The magnetization of a material measured at an applied reversed field which is nominally smaller than the coercivity may, over a long time scale, slowly relax to zero. Relaxation occurs when reversal of magnetization by domain wall motion is thermally activated and is dominated by magnetic viscosity. The increasing value of coercivity at high frequencies is a serious obstacle to the increase of data rates in high-bandwidth magnetic recording, compounded by the fact that increased storage density typically requires a higher coercivity in the media.\n\nAt the coercive field, the vector component of the magnetization of a ferromagnet measured along the applied field direction is zero. There are two primary modes of magnetization reversal: single-domain rotation and domain wall motion. When the magnetization of a material reverses by rotation, the magnetization component along the applied field is zero because the vector points in a direction orthogonal to the applied field. When the magnetization reverses by domain wall motion, the net magnetization is small in every vector direction because the moments of all the individual domains sum to zero. Magnetization curves dominated by rotation and magnetocrystalline anisotropy are found in relatively perfect magnetic materials used in fundamental research. Domain wall motion is a more important reversal mechanism in real engineering materials since defects like grain boundaries and impurities serve as nucleation sites for reversed-magnetization domains. The role of domain walls in determining coercivity is complicated since defects may \"pin\" domain walls in addition to nucleating them. The dynamics of domain walls in ferromagnets is similar to that of grain boundaries and plasticity in metallurgy since both domain walls and grain boundaries are planar defects.\n\nAs with any hysteretic process, the area inside the magnetization curve during one cycle represents the work that is performed on the material by the external field in reversing the magnetization, and is dissipated as heat. Common dissipative processes in magnetic materials include magnetostriction and domain wall motion. The coercivity is a measure of the degree of magnetic hysteresis and therefore characterizes the lossiness of soft magnetic materials for their common applications.\n\nThe \"squareness\" (saturation remanence divided by saturation magnetization) and coercivity are figures of merit for hard magnets although \"energy product\" (saturation magnetization times coercivity) is most commonly quoted. The 1980s saw the development of rare-earth magnets with high energy products but undesirably low Curie temperatures. Since the 1990s new exchange spring hard magnets with high coercivities have been developed.\n\n\n"}
{"id": "317844", "url": "https://en.wikipedia.org/wiki?curid=317844", "title": "Cooling pond", "text": "Cooling pond\n\nA cooling pond is a man-made body of water primarily formed for the purpose storing heated water and/or supplying cooling water to a nearby power plant or industrial facility such as a petroleum refinery, pulp and paper mill, chemical plant, steel mill or smelter.\n\nCooling ponds are used where sufficient land is available, as an alternative to cooling towers or discharging of heated water to a nearby river or coastal bay, a process known as “once-through cooling.” The latter process can cause thermal pollution of the receiving waters. Cooling ponds are also sometimes used with air conditioning systems in large buildings as an alternative to cooling towers.\n\nThe pond receives thermal energy in the water from the plant’s condensers during the process of energy production and the thermal energy is then dissipated mainly through evaporation. Once the water has cooled in the pond, it is reused by the plant. New water is added to the system (“make-up” water) to replace the water lost through evaporation.\n\nA 1970 cooling pond research study done by the Environmental Protection Agency stated cooling ponds as having a lower overall electrical cost than cooling towers while providing the same benefits. They also concluded that a cooling pond will work optimally within 5 degrees Fahrenheit of natural water temperature with an area encompassing approximately 4 acres per megawatt of dissipated thermal energy.\n\nLake Anna is a cooling pond in Virginia, which provides cooling water for the North Anna Nuclear Generating Station. This pond has recreational uses such as fishing, swimming, boating, camping, and picnicking as well as being a cooling pond for the nuclear plant. Chernobyl's cooling pond has abundant wildlife, despite the radiation present in the area. Some accounts of wels catfish \"(Silurus glanis)\" growing up to 350 pounds and having a lifespan of up to 50 years in the area. The Columbia station in Pardeeville, Wisconsin is a coal fired power plant with a capacity of 1000 MW. A duel cooling system is used for heat rejection that consists of a cooling pond and two cooling towers. The pond and towers are connected in a parallel arrangement to help dissipate thermal energy at expedited rates.<ref>"}
{"id": "52218351", "url": "https://en.wikipedia.org/wiki?curid=52218351", "title": "Difluorodisulfanedifluoride", "text": "Difluorodisulfanedifluoride\n\n1,1,1,2-tetrafluorodisulfane, also known as 1,2-difluorodisulfane 1,1-difluoride or just difluorodisulfanedifluoride (FSSF) is an unstable molecular compound of fluorine and sulfur. The molecule has a pair of sulfur atoms, with one fluorine atom on one sulfur, and three fluorine atoms on the other. It has the uncommon property that all the bond lengths are different. The bond strength is not correlated with bond length but is inversely correlated with the force constant (Badger's rule). The molecule can be considered as sulfur tetrafluoride in which a sulfur atom is inserted into a S-F bond.\n\nAtoms are labelled with the sulfur atom connected to three fluorine atoms as S (for hypervalent) and S. The fluorine atoms are labelled F attached to S, and on the hypervalent S atom: F, the closest F atom to F, F the furthest away F atom from F, and F\n\nCarlowitz first determined the structure in 1983.\nF is 90° from F, and 84° from F, and the torsion compared to F is about 95°.\n\nThe dimerization reaction 2SF FSSF is reversible. It also disproportionates: SF + FSSF → FSSF + SF. A side reaction also produces the intermediate FSSSF. hydrogen fluoride catalyses disproportionation to sulfur and sulfur tetrafluoride by forming a reactive intermediate HSF molecule.\nWhen FSSF dissociates, the F atom forms a new bond to the S atom, and the S-S bond breaks.\nAs a gas, at ambient and totally clean conditions, FSSF decomposes with a half life of about 10 hours. Disproportionation to SSF and SF catalysed by metal fluorides can take place in under one second. However it is indefinitely stable at -196 °C.\n\nA symmetrical molecule FSSF is calculated to be 15.1 kcal/mol higher in energy than FSSF.\n\nFSSF is easily hydrolysed with water.\n\nFSSF spontaneously reacts with oxygen gas to make thionyl fluoride, the only sulfur fluoride that does not need any assistance to do this.\nFSSF reacts with copper at high temperatures producing copper fluoride and copper sulfide.\n\nSFSF can be made in the laboratory when low pressure (10mm) SCl vapour is passed over potassium fluoride or mercuric fluoride heated to 150 °C. Byproducts include FSSF, SSF, SF, SFSCl, and FSSCl. SFSCl can be removed from this mixture in a reaction with mercury. Separation of the sulfur fluorides can be achieved by low temperature distillation. SFSF distills just above -50 °C.\n\nSFSF is also made in small amounts by reacting sulfur with silver fluoride, or photolysis of disulfur difluoride and SSF.\nThe molecule is formed by the dimerization of sulfur difluoride.\n\nThe nuclear magnetic resonance spectrum of FSSF shows four bands, each of eight lines at -53.2, -5.7, 26.3 and 204.1 ppm.\n\nFSSF is stable as a solid, as a liquid below -74 °C and dissolved in other sulfur fluoride liquids. This is in contrast to SF which is only stable as a dilute gas.\n\nInfrared vibration bands for FSSF are at 810, 678, 530, 725, and 618(S-S) cm.\n\nThe related compound FSSSF has a similar structure. Thiothionyltetrafluoride, S=SF may exist as a gas. It is less energetically favourable to FSSF by 37 kJ/mol, but has a high energy barrier of 267 kJ/mol. However it may disproportionate rapidly to sulfur and sulfur tetrafluoride. The other known sulfur fluorides are sulfur difluoride, sulfur tetrafluoride, sulfur hexafluoride, disulfur decafluoride, disulfur difluoride and 1,1-difluorodisulfane, difluorotrisulfane, and difluorotetrasulfane. The F atom can be substituted with Cl to yield ClSSF (2-chloro-1,1,1-trifluorodisulfane).\n\n"}
{"id": "17904316", "url": "https://en.wikipedia.org/wiki?curid=17904316", "title": "Eolica Istria Wind Farm", "text": "Eolica Istria Wind Farm\n\nThe Eolica Istria Wind Farm is a proposed wind power project in Istria, Constanţa County, Romania. It will consist of two individual wind farms connected together. It will have 37 individual wind turbines with a nominal output of around 2 MW which will deliver up to 74 MW of power, enough to power over 47,000 homes, with a capital investment required of approximately US$75 million.\n"}
{"id": "33894614", "url": "https://en.wikipedia.org/wiki?curid=33894614", "title": "Erianthemum dregei", "text": "Erianthemum dregei\n\nErianthemum dregei is a species of parasitic plant in the Loranthaceae family, and is commonly known as the hairy mistletoe or wood flower.\n\nThese plants are native to Africa and are parasitic on a large number of tree species in higher rainfall areas from the Eastern Cape of South Africa, through KwaZulu-Natal, Swaziland and Mpumalanga, to East Africa, as far as northern Ethiopia. They are also found in southern Angola.\n\nA branched parasitic shrub with spreading or pendent stems, forming clumps of up to 2m x 1.5m. The leaves are leathery and hairless, usually alternate (sometimes opposite), with conspicuous side veins. The growing points are velvety brown. The flowers are massed in small clusters and are densely hairy, pale yellowish-green and sometimes flushed orange to pink. The fruit is an orange to bright red berry, 10–15 mm in size. \"Erianthemum dregei\" shows great variation across its range.\n\n\"Erianthemum dregei\" is used in African traditional medicine to treat stomach complaints in children and cattle.\n\nThe flowers and fruit attract birds. The leaves are eaten by the larvae of \"Mylothris agathina\".\n"}
{"id": "1827283", "url": "https://en.wikipedia.org/wiki?curid=1827283", "title": "Faith McNulty", "text": "Faith McNulty\n\nFaith McNulty (November 28, 1918 – April 10, 2005) was an American non-fiction author, probably best known for her 1980 literary journalism genre book \"The Burning Bed\". She is also known for her authorship of wildlife pieces and books, including children's books.\n\nFaith Trumbull Corrigan was born in New York City, the daughter of a judge. She attended Barnard College for one year, then attended Rhode Island State College. But she dropped out of college once she got a job as a copy girl at the \"New York Daily News\". She later went to work for \"Life\" magazine. She worked for the U.S. Office of War Information in London during World War II.\n\nMcNulty was a staff writer at \"The New Yorker\" magazine from 1953 to 1994. In 1980, a collection of her \"New Yorker\" work was published as \"The Wildlife Stories of Faith McNulty\". For many years, she edited the annual \"New Yorker\" compilation of the year's best children's books.\n\nShe also frequently wrote children's books on wildlife, including \"How to Dig a Hole to the Other Side of the World\" in 1979 and \"When I Lived With Bats\" in 1998. Her 1966 book \"The Whooping Crane: The Bird that Defies Distinction\" was written for adults.\n\nHer husband, John McNulty, was also a writer for \"The New Yorker\" and with Thomas Wolfe, Truman Capote, Gay Talese and James Baldwin, a major figure in the development of the literary genre of Creative nonfiction, which is also known as literary journalism or literature in fact. As earlier here noted, having herself been years exposed to Harold Ross' New Yorker magazine's rarefied environment, which was then so promoting of this evolving genre, Faith's own major nonfiction work, \"The Burning Bed\", is, itself, a quintessential and quality example of the genre of literary journalism or, as Thomas Wolfe once labeled it, the “New Journalism”. After her husband John died in 1956, Faith remarried, to Richard Martin, a set designer and an inventive designer of set props.\n\n\"The Burning Bed\" told the true story of Francine Hughes, who set fire to the bedroom in which her husband was sleeping. Hughes defended herself by saying that her husband had been abusing her for 13 years. The jury at her trial ruled that she had been temporarily insane, and she was found not guilty.\n\nFaith had fonder memories of life with kinder family, however. \"I can remember my father in his nightshirt, digging for worms for the baby robin in the bathroom. That's the kind of household it was; I had woodchucks in the bathroom, cats, squirrels, chipmunks\", McNulty once said.\n\nTowards the end of her life, she wrote a weekly column for \"The Providence Journal\" on a local animal shelter run by the Animal Welfare League. Her mother had founded the Animal Welfare League in southern Rhode Island. McNulty had long been known for taking in stray animals at her farm.\n\nShe suffered a stroke in 2004. She died at her farm in Wakefield, Rhode Island.\n\nMcNulty's last book was illustrated by Steven Kellogg and published by Scholastic Books in 2005, \"If You Decide to Go to the Moon\"—a picture book written in the second person. Next year (after McNulty's death) it won a major \"year's best\" children's literary award, the Boston Globe–Horn Book Award for Nonfiction.\n\n"}
{"id": "63978", "url": "https://en.wikipedia.org/wiki?curid=63978", "title": "Fresnel lens", "text": "Fresnel lens\n\nA Fresnel lens ( or ) is a type of compact lens originally developed by French physicist Augustin-Jean Fresnel for lighthouses.\n\nThe design allows the construction of lenses of large aperture and short focal length without the mass and volume of material that would be required by a lens of conventional design. A Fresnel lens can be made much thinner than a comparable conventional lens, in some cases taking the form of a flat sheet. A Fresnel lens can capture more oblique light from a light source, thus allowing the light from a lighthouse equipped with one to be visible over greater distances.\n\nThe idea of creating a thinner, lighter lens in the form of a series of annular steps is often attributed to Georges-Louis Leclerc, Comte de Buffon. Whereas Buffon proposed grinding such a lens from a single piece of glass, the Marquis de Condorcet (1743–1794) proposed making it with separate sections mounted in a frame.\nFrench physicist and engineer Augustin-Jean Fresnel is most often given credit for the development of the multi-part lens for use in lighthouses. According to \"Smithsonian\" magazine, the first Fresnel lens was used in 1823 in the Cordouan lighthouse at the mouth of the Gironde estuary; its light could be seen from more than out. Scottish physicist Sir David Brewster is credited with convincing the United Kingdom to adopt these lenses in their lighthouses.\n\nThe Fresnel lens reduces the amount of material required compared to a conventional lens by dividing the lens into a set of concentric annular sections. An ideal Fresnel lens would have an infinite number of sections. In each section, the overall thickness is decreased compared to an equivalent simple lens. This effectively divides the continuous surface of a standard lens into a set of surfaces of the same curvature, with stepwise discontinuities between them.\n\nIn some lenses, the curved surfaces are replaced with flat surfaces, with a different angle in each section. Such a lens can be regarded as an array of prisms arranged in a circular fashion, with steeper prisms on the edges, and a flat or slightly convex center. In the first (and largest) Fresnel lenses, each section was actually a separate prism. 'Single-piece' Fresnel lenses were later produced, being used for automobile headlamps, brake, parking, and turn signal lenses, and so on. In modern times, computer-controlled milling equipment (CNC) might be used to manufacture more complex lenses.\n\nFresnel lens design allows a substantial reduction in thickness (and thus mass and volume of material), at the expense of reducing the imaging quality of the lens, which is why precise imaging applications such as photography usually still use larger conventional lenses.\n\nFresnel lenses are usually made of glass or plastic; their size varies from large (old historical lighthouses, meter size) to medium (book-reading aids, OHP viewgraph projectors) to small (TLR/SLR camera screens, micro-optics). In many cases they are very thin and flat, almost flexible, with thicknesses in the range.\n\nModern Fresnel lenses usually consist of all refractive elements. However many of the lighthouses have both refracting and reflecting elements, as shown in the photographs and diagram. That is, the outer elements are sections of reflectors while the inner elements are sections of refractive lenses. Total internal reflection was often used to avoid the light loss in reflection from a silvered mirror.\n\nFresnel produced six sizes of lighthouse lenses, divided into four \"orders\" based on their size and focal length. In modern use, these are classified as first through sixth order. An intermediate size between third and fourth order was added later, as well as sizes above first order and below sixth.\n\nA first-order lens has a focal length of 920 mm (36 in) and a maximum diameter 2590 mm (8.5 ft) high. The complete assembly is about 3.7 m (12 ft) tall and 1.8 m (6 ft) wide. The smallest (sixth-order) has a focal length of 150 mm (5.9 in) and an optical diameter 433 mm (17 in) high.\n\nThe largest Fresnel lenses are called hyperradiant Fresnel lenses. One such lens was on hand when it was decided to build and outfit the Makapuu Point Light in Hawaii. Rather than order a new lens, the huge optic construction, tall and with over a thousand prisms, was used there.\n\nThere are two main types of Fresnel lens: \"imaging\" and \"non-imaging\". Imaging Fresnel lenses use segments with curved cross-sections and produce sharp images, while non-imaging lenses have segments with flat cross-sections, and do not produce sharp images. As the number of segments increases, the two types of lens become more similar to each other. In the abstract case of an infinite number of segments, the difference between curved and flat segments disappears.\n\n\n\n\n\nFresnel lenses are used as simple hand-held magnifiers. They are also used to correct several visual disorders, including ocular-motility disorders such as strabismus. Fresnel lenses have been used to increase the visual size of CRT displays in pocket televisions, notably the Sinclair TV80. They are also used in traffic lights.\n\nFresnel lenses are used in left-hand-drive European lorries entering the UK and Republic of Ireland (and vice versa, right-hand-drive Irish and British trucks entering mainland Europe) to overcome the blind spots caused by the driver operating the lorry while sitting on the wrong side of the cab relative to the side of the road the car is on. They attach to the passenger-side window.\n\nAnother automobile application of a Fresnel lens is a rear view enhancer, as the wide view angle of a lens attached to the rear window permits examining the scene behind a vehicle, particularly a tall or bluff-tailed one, more effectively than a rear-view mirror alone.\n\nMulti-focal Fresnel lenses are also used as a part of retina identification cameras, where they provide multiple in- and out-of-focus images of a fixation target inside the camera. For virtually all users, at least one of the images will be in focus, thus allowing correct eye alignment.\n\nFresnel lenses have also been used in the field of popular entertainment. The British rock artist Peter Gabriel made use of them in his early solo live performances to magnify the size of his head, in contrast to the rest of his body, for dramatic and comic effect. In the Terry Gilliam film \"Brazil\", plastic Fresnel screens appear ostensibly as magnifiers for the small CRT monitors used throughout the offices of the Ministry of Information. However, they occasionally appear between the actors and the camera, distorting the scale and composition of the scene to humorous effect. The Pixar movie Wall-E features a fresnel lens in the scenes where the protagonist watches the musical Hello, Dolly! magnified on an iPod.\n\nCanon and Nikon have used Fresnel lenses to reduce the size of telephoto lenses. Photographic lenses that include Fresnel elements can be much shorter than corresponding conventional lens design. Nikon calls their technology \"Phase Fresnel\".\n\nThe Polaroid SX-70 camera used a Fresnel reflector as part of its viewing system.\n\nView and large format cameras can utilize a Fresnel lens in conjunction with the ground glass, to increase the perceived brightness of the image projected by a lens onto the ground glass, thus aiding in adjusting focus and composition.\n\nHigh-quality glass Fresnel lenses were used in lighthouses, where they were considered state of the art in the late 19th and through the middle of the 20th centuries; most are now retired from service. Lighthouse Fresnel lens systems typically include extra annular prismatic elements, arrayed in faceted domes above and below the central planar Fresnel, in order to catch all light emitted from the light source. The light path through these elements can include an internal reflection, rather than the simple refraction in the planar Fresnel element. These lenses conferred many practical benefits upon the designers, builders, and users of lighthouses and their illumination. Among other things, smaller lenses could fit into more compact spaces. Greater light transmission over longer distances, and varied patterns, made it possible to triangulate a position.\n\nPerhaps the most widespread use of Fresnel lenses, for a time, occurred in automobile headlamps, where they can shape the roughly parallel beam from the parabolic reflector to meet requirements for dipped and main-beam patterns, often both in the same headlamp unit (such as the European H4 design). For reasons of cost, weight, and impact resistance, newer cars have dispensed with glass Fresnel lenses, using multifaceted reflectors with plain polycarbonate lenses. However, Fresnel lenses continue in wide use in automobile tail, marker, and backup lights.\n\nGlass Fresnel lenses also are used in lighting instruments for theatre and motion pictures (see Fresnel lantern); such instruments are often called simply \"Fresnels\". The entire instrument consists of a metal housing, a reflector, a lamp assembly, and a Fresnel lens. Many Fresnel instruments allow the lamp to be moved relative to the lens' focal point, to increase or decrease the size of the light beam. As a result, they are very flexible, and can often produce a beam as narrow as 7° or as wide as 70°. The Fresnel lens produces a very soft-edged beam, so is often used as a wash light. A holder in front of the lens can hold a colored plastic film (\"gel\") to tint the light or wire screens or frosted plastic to diffuse it. The Fresnel lens is useful in the making of motion pictures not only because of its ability to focus the beam brighter than a typical lens, but also because the light is a relatively consistent intensity across the entire width of the beam of light.\n\nAircraft carriers and naval air stations typically use Fresnel lenses in their optical landing systems. The \"meatball\" light aids the pilot in maintaining proper glide slope for the landing. In the center are amber and red lights composed of Fresnel lenses. Although the lights are always on, the angle of the lens from the pilot's point of view determines the color and position of the visible light. If the lights appear above the green horizontal bar, the pilot is too high. If it is below, the pilot is too low, and if the lights are red, the pilot is very low.\n\nThe Fresnel lens has seen applications for enhancing passenger reading lights on Airbus aircraft: in a dark cabin, the focused beam of light does not dazzle neighboring passengers.\n\nThe use of Fresnel lenses for image projection reduces image quality, so they tend to occur only where quality is not critical or where the bulk of a solid lens would be prohibitive. Cheap Fresnel lenses can be stamped or molded of transparent plastic and are used in overhead projectors and projection televisions.\n\nFresnel lenses of different focal lengths (one collimator, and one collector) are used in commercial and DIY projection. The collimator lens has the lower focal length and is placed closer to the light source, and the collector lens, which focuses the light into the triplet lens, is placed after the projection image (an active matrix LCD panel in LCD projectors). Fresnel lenses are also used as collimators in overhead projectors.\n\nSince plastic Fresnel lenses can be made larger than glass lenses, as well as being much cheaper and lighter, they are used to concentrate sunlight for heating in solar cookers, in solar forges, and in solar collectors used to heat water for domestic use. They can also be used to generate steam or to power a Stirling engine.\n\nFresnel lenses can concentrate sunlight onto solar cells with a ratio of almost 500:1. This allows the active solar-cell surface to be reduced, lowering cost and allowing the use of more efficient cells that would otherwise be too expensive. In the early 21st century, Fresnel reflectors began to be used in concentrating solar power (CSP) plants to concentrate solar energy. One application was to preheat water at the coal-fired Liddell Power Station, in Hunter Valley Australia.\n\nFresnel lenses can be used to sinter sand, allowing 3D printing in glass.\n\nThe English children's fantasy television series \"Shadows\" features an episode \"The Other Window\", in which a scientist places a Fresnel lens on a window of his home, and his children and his mother are able to see visions of their ancestors in previous centuries in it.\n\nIn the horror manga \"Uzumaki\" by Junji Ito, the concentric patterns of the Fresnel lens of a lighthouse are melted by heat and form a spiral.\n\nA subplot in Jimmy Buffett's novel \"A Salty Piece of Land\" follows the characters' efforts to find a Fresnel lens to refurbish a Bahamian lighthouse.\n\nA Fresnel lens is featured in the 2016 film \"To Keep the Light\" wherein a lighthouse keeper's wife fulfills her husband's duties.\n\n\n\n\n"}
{"id": "67401", "url": "https://en.wikipedia.org/wiki?curid=67401", "title": "Grapefruit", "text": "Grapefruit\n\nThe grapefruit (\"Citrus\" × \"paradisi\") is a subtropical citrus tree known for its sour to semi-sweet, somewhat bitter fruit. Grapefruit is a hybrid originating in Barbados as an accidental cross between two introduced species, sweet orange (\"C. sinensis\") and pomelo, or shaddock (\"C. maxima\"), both of which were introduced from Asia in the seventeenth century. When found, it was named the \"forbidden fruit\"; and frequently, it has been misidentified with the pomelo.\n\nThe grapefruit's name alludes to clusters of the fruit on the tree, which often appear similar to that of grapes.\n\nThe evergreen grapefruit trees usually grow to around tall, although they may reach . The leaves are glossy, dark green, long (up to ), and thin. It produces white four-petaled flowers. The fruit is yellow-orange skinned and generally, an oblate spheroid in shape; it ranges in diameter from . The flesh is segmented and acidic, varying in color depending on the cultivars, which include white, pink, and red pulps of varying sweetness (generally, the redder varieties are the sweetest). The 1929 U.S. Ruby Red (of the Redblush variety) has the first grapefruit patent.\n\nThe genetic origin of the grapefruit is a hybrid mix. One ancestor of the grapefruit was the Jamaican sweet orange (\"Citrus sinensis\"), itself an ancient hybrid of Asian origin; the other was the Indonesian pomelo (\"C. maxima\"). One story of the fruit's origin is that a certain \"Captain Shaddock\" brought pomelo seeds to Jamaica and bred the first fruit, however, it probably originated as a naturally occurring hybrid between the two plants some time after they had been introduced there.\nThe hybrid fruit, then called \"the forbidden fruit\", was first documented in 1750 by a Welshman, Rev. Griffith Hughes, who described specimens from Barbados in \"The Natural History of Barbados\". Currently, the grapefruit is said to be one of the \"Seven Wonders of Barbados\".\n\nThe grapefruit was brought to Florida by Count Odet Philippe in 1823 in what is now known as Safety Harbor. Further crosses have produced the tangelo (1905), the Minneola tangelo (1931), and the oroblanco (1984).\n\nThe grapefruit was known as the \"shaddock\" or \"shattuck\" until the nineteenth century. Its current name alludes to clusters of the fruit on the tree, which often appear similar to that of grapes. Botanically, it was not distinguished from the pomelo until the 1830s, when it was given the name \"Citrus paradisi.\" Its true origins were not determined until the 1940s. This led to the official name being altered to \"Citrus × paradisi,\" the \"×\" identifying its hybrid origin.\nAn early pioneer in the American citrus industry was Kimball Atwood, a wealthy entrepreneur who founded the Atwood Grapefruit Company in the late nineteenth century. The Atwood Grove became the largest grapefruit grove in the world, with a yearly output of 80,000 boxes of fruit. It was there that pink grapefruit was first discovered in 1906.\n\nThe 1929 Ruby Red patent was associated with real commercial success, which came after the discovery of a red grapefruit growing on a pink variety. Using radiation to trigger mutations, new varieties were developed to retain the red tones which typically faded to pink. The Rio Red variety is the current (2007) Texas grapefruit with registered trademarks Rio Star and Ruby-Sweet, also sometimes promoted as \"Reddest\" and \"Texas Choice\". The Rio Red is a mutation bred variety that was developed by treatment of bud sticks with thermal neutrons. Its improved attributes of mutant variety are fruit and juice color, deeper red, and wide adaptation.\n\nThe Star Ruby is the darkest of the red varieties. Developed from an irradiated Hudson grapefruit, it has found limited commercial success because it is more difficult to grow than other varieties.\n\nThe varieties of Texas and Florida grapefruit include: Oro Blanco, Ruby Red, Pink, Rio Star, Thompson, White Marsh, Flame, Star Ruby, Duncan, and Pummelo HB.\n\nChina is the top producer of grapefruit and pomelo. It is followed by The United States and Mexico.\n\nGrapefruit comes in many varieties. One way to differentiate between varieties is by the flesh color of fruit they produce. The most popular varieties currently cultivated are red, white, and pink hues, referring to the internal pulp color of the fruit. The family of flavors range from highly acidic and somewhat sour, to sweet and tart. Grapefruit mercaptan, a sulfur-containing terpene, is one of the substances which has a strong influence on the taste and odor of grapefruit, compared with other citrus fruits.\n\nGrapefruit and grapefruit juice have been found to interact with numerous drugs and in many cases, to result in adverse direct and/or side effects (if dosage is not carefully adjusted.)\n\nThis happens in two very different ways. In the first, the effect is from bergamottin, a natural furanocoumarin in both grapefruit flesh and peel that inhibits the CYP3A4 enzyme, (among others from the P450 enzyme family responsible for metabolizing 90% of drugs). The action of the CYP3A4 enzyme itself is to metabolize many medications. If the drug's breakdown for removal is lessened, then the level of the drug in the blood may become too high or stay too long, leading to adverse effects. On the other hand, some drugs must be broken down to become active, and inhibiting CYP3A4 may lead to reduced drug effects.\n\nThe other effect is that grapefruit can block the absorption of drugs in the intestine. If the drug is not absorbed, then not enough of it is in the blood to have a therapeutic effect. Each affected drug has either a specific increase of effect or decrease.\n\nOne whole grapefruit, or a glass of of grapefruit juice may cause drug overdose toxicity. Typically, drugs that are incompatible with grapefruit are so labeled on the container or package insert. People taking drugs should ask their health care provider or pharmacist questions about grapefruit and drug interactions.\n\nGrapefruit is a rich source of vitamin C (>20% of the Daily Value, DV in a 100 gram serving), contains the fiber pectin, and the pink and red hues contain the beneficial antioxidant lycopene. Studies have shown grapefruit helps lower cholesterol, and there is evidence that the seeds have antioxidant properties. Grapefruit forms a core part of the \"grapefruit diet\", the theory being that the fruit's low glycemic index is able to help the body's metabolism burn fat.\n\nAlthough grapefruit seed extract (GSE) is promoted as a plant-based preservative by some natural personal care manufacturers, studies have shown that the apparent antimicrobial activity associated with GSE preparations is merely due to contamination with synthetic preservatives such as parabens.\n\nCitrus fruits show high amounts of putrescine, they contain very little spermidine.\n\nGrapefruit juice contains about half the citric acid of lime or lemon juice (which contain about 47 g/l), and about two-and-a-half times the amount of citric acid found in orange juice.\n\nIn Costa Rica, especially in Atenas, grapefruit are often cooked to remove their sourness, rendering them as sweets; they are also stuffed with \"dulce de leche\", resulting in a dessert called \"toronja rellena\" (stuffed grapefruit). In Haiti, grapefruit is used primarily for its juice (\"jus de Chadèque\"), but also is used to make jam (\"confiture de Chadèque\").\n\nGrapefruit has also been investigated in cancer medicine pharmacodynamics. Its inhibiting effect on the metabolism of some drugs may allow smaller doses to be used, which can help to reduce costs.\n\nGrapefruit is a pomelo backcross, a hybrid of pomelo × sweet orange, with sweet orange itself being a pomelo × mandarin hybrid.\n\nThe grapefruit is a parent to many hybrids:\n\nThe grapefruit's cousins include:\n\n"}
{"id": "43156192", "url": "https://en.wikipedia.org/wiki?curid=43156192", "title": "Helion Energy", "text": "Helion Energy\n\nHelion Energy, Inc. is an American company in Redmond, WA developing a magneto-inertial fusion power technology called . Their approach combines the stability of magnetic containment and once-per-second heating pulsed inertial fusion. They are developing a 50 MW scale system. \n\nHelion Energy was founded in 2013 by Dr. David Kirtley, Dr. John Slough, Chris Pihl, and Dr. George Votroubek. Helion Energy is a spin-off of Redmond company MSNW LLC that now develops space propulsion related technologies. Investors in Helion include YCombinator, Mithril Capital Management, and Capricorn Investment Group. \nThe management team won the 2013 National Cleantech Open Energy Generation competition and awards at the 2014 ARPA-E Future Energy Startup competition and were members of the 2014 YCombinator program.\n\nThe Fusion Engine technology is based on the Inductive Plasmoid Accelerator (IPA) experiments performed at MSNW LLC from 2005 through 2012. This system theoretically operates at 1 Hz, injecting plasma, compressing it to fusion conditions, expanding it and directly recovering the energy to provide electricity. The IPA experiments claimed 300 km/s velocities, deuterium neutron production, and 2 keV deuterium ion temperatures.\n\nHelion intends to use helium-3/deuterium fuel. This fuel allows essentially aneutronic fusion, releasing only 5% of its energy in the form of neutrons. The helium is captured and reused, eliminating supply concerns.\n\nFusion reaction: D + He → He + p + 18.3 MeV\n\nThe IPA experiments used deuterium-deuterium fusion, which produces a 2.4 MeV neutron per reaction. Helion and MSNW published articles describing a deuterium-tritium implementation which is the easiest to achieve but generate 14 MeV neutrons.\n\nThis fusion approach uses the magnetic field of a Field Reversed Configuration (FRC) plasmoid (operated with solid state electronics derived from power switching electronics in wind turbines) to prevent plasma losses. An FRC is a magnetized plasma configuration notable for its closed field lines, high Beta and lack of internal penetrations.\n\nTo inject the plasmoid into the fusion ‘burn’ chamber two plasmoids are accelerated at high velocity with pulsed magnetic fields and merge into a single plasmoid at high pressure. Their experiments achieved plasmas of 1.5 Tesla and 2 keV temperatures. Published records show plans to compress fusion plasmas to 12 Tesla.\n\nEnergy is captured by direct energy conversion that translates high-energy alpha particles directly into a voltage. This eliminates the need for steam turbines and cooling towers (and the associated energy losses).\n\nHelion Energy received $7 million in funding from NASA, the U.S. Department of Energy and the Department of Defense, followed by $1.5 million from the private sector in August 2014, through the seed accelerators Y Combinator and Mithril Capital Management. The company raised an additional $10.6 million in July, 2015. \nHelion Energy’s strategy is to generate revenue based on a royalty model of electricity produced with projected electricity prices of 40-60 $/MWhr (4 to 6 cents per kwh). Penetration of the new capacity market is estimated at 20% of market growth (2.5%) per annum eventually reaching 50% of new power generation worldwide – $52 B/yr. Gradual displacement of existing supplies enables continued growth to 20% of world electrical generation after 20 years with a net return of over $300 billion.\n"}
{"id": "39422710", "url": "https://en.wikipedia.org/wiki?curid=39422710", "title": "Hsingyuan Power Plant", "text": "Hsingyuan Power Plant\n\nThe Hsingyuan Power Plant or Star Buck Power Plant () is a gas-fired power plant in Chang-Bin Industrial Park, Lukang Township, Changhua County, Taiwan.\n\nHsingyuan Power Plant was commissioned in March 2009 and went into full operation in June 2009.\n\n"}
{"id": "248159", "url": "https://en.wikipedia.org/wiki?curid=248159", "title": "Inert gas", "text": "Inert gas\n\nAn inert gas/noble gas is a gas which does not undergo chemical reactions under a set of given conditions. The noble gases often do not react with many substances, and were historically referred to as the inert gases. Inert gases are used generally to avoid unwanted chemical reactions degrading a sample. These undesirable chemical reactions are often oxidation and hydrolysis reactions with the oxygen and moisture in air. The term \"inert gas\" is context-dependent because several of the noble gases can be made to react under certain conditions.\n\nPurified argon and nitrogen gases are most commonly used as inert gases due to their high natural abundance (78.2% N, 1% Ar in air) and low relative cost.\n\nUnlike noble gases, an inert gas is not necessarily elemental and is often a compound gas. Like the noble gases the tendency for non-reactivity is due to the valence, the outermost electron shell, being complete in all the inert gases. This is a tendency, not a rule, as noble gases and other \"inert\" gases can react to form compounds.\n\nThe inert gases are obtained by fractional distillation of air, with the exception of helium which is separated from a few natural gas sources rich in this element, through cryogenic distillation or membrane separation. For specialized applications, purified inert gas may be produced by specialized generators on-site. They are often used aboard chemical tankers and product carriers (smaller vessels). Benchtop specialized generators are also available for laboratories.\n\nBecause of the non-reactive properties of inert gases they are often useful to prevent undesirable chemical reactions from taking place. Food is packed in inert gas to remove oxygen gas. This prevents bacteria from growing. It also prevents chemical oxidation by oxygen in normal air. An example is the rancidification (caused by oxidation) of edible oils. In food packaging, inert gases are used as a passive preservative, in contrast to active preservatives like sodium benzoate (an antimicrobial) or BHT (an antioxidant).\n\nHistorical documents may also be stored under an inert gas to avoid degradation. For example, the original documents of the U.S. Constitution is stored under humidified argon. Helium was previously used, but it was less suitable because it diffuses out of the case more quickly than argon.\n\nInert gases are often used in the chemical industry. In a chemical manufacturing plant, reactions can be conducted under inert gas to minimize fire hazards or unwanted reactions. In such plants and in oil refineries, transfer lines and vessels can be purged with inert gas as a fire and explosion prevention measure. At the bench scale, chemists perform experiments on air-sensitive compounds using air-free techniques developed to handle them under inert gas.\n\nInert gas is produced on board crude oil carriers (above 8,000 tonnes)(from Jan 1, 2016) by using either a flue gas system or by burning kerosene in a dedicated inert gas generator. The inert gas system is used to prevent the atmosphere in cargo tanks or bunkers from coming into the explosive range. IG keeps the oxygen content of the tank atmosphere below 5% (on crude carriers, less for product carriers and gas tankers), thus making any air/hydrocarbon gas mixture in the tank too rich (too high a fuel to oxygen ratio) to ignite. IG is most important during discharging and during the ballast voyage when more hydrocarbon vapour is likely to be present in the tank atmosphere. Inert gas can also be used to purge the tank of the volatile atmosphere in preparation for gas freeing - replacing the atmosphere with breathable air - or vice versa. \n\nThe flue gas system uses the boiler exhaust as its source, so it is important that the fuel/air ratio in the boiler burners is properly regulated to ensure that high quality inert gases is produced. Too much air would result in an oxygen content exceeding 5%, too much fuel oil would result in carryover of dangerous hydrocarbon gas. The flue gas is cleaned and cooled by the scrubber tower. Various safety devices prevent overpressure, return of hydrocarbon gas to the engine room, or supply of IG with too high oxygen content. \n\nGas tankers and product carriers cannot rely on flue gas systems (because they require IG with O content of 1% or less) and so use inert gas generators instead. The inert gas generator consists of a combustion chamber and scrubber unit supplied by fans and a refrigeration unit which cools the gas. A drier in series with the system removes moisture from the gas before it is supplied to the deck. Cargo tanks on gas carriers are not inerted, but the hold space around them is. This arrangement allows the tanks to be kept cool using a small heel of cargo while the vessel is in ballast while retaining the explosion protection provided by the inert gas.\n\nIn gas tungsten arc welding (GTAW), inert gases are used to shield the tungsten from contamination. It also shields the fluid metal (created from the arc) from the reactive gases in air which can cause porosity in the solidified weld puddle. Inert gases are also used in gas metal arc welding (GMAW) for welding non-ferrous metals. Some gases which are not usually considered inert but which behave like inert gases in all the circumstances likely to be encountered in some use can often be used as a substitute for an inert gas. This is useful when an appropriate pseudo-inert gas can be found which is inexpensive and common. For example, carbon dioxide is sometimes used in gas mixtures for GMAW because it is not reactive to the weld pool created by arc welding. But it is reactive to the arc. The more carbon dioxide that is added to the inert gas, such as argon, will increase your penetration. The amount of carbon dioxide is often determined by what kind of transfer you will be using in GMAW. The most common is spray arc transfer, and the most commonly used gas mixture for spray arc transfer is 90% argon and 10% carbon dioxide. (Listed as many different names depending on the gas supplier).\n\nIn underwater diving an inert gas is a component of the breathing mixture which is not metabolically active, and serves to dilute the gas mixture. The inert gas may have effects on the diver, but these are thought to be mostly physical effects, such as tissue damage caused by bubbles in decompression sickness. The most common inert gas used in breathing gas for commercial diving is helium.\n\n"}
{"id": "3192035", "url": "https://en.wikipedia.org/wiki?curid=3192035", "title": "Jojoba ester", "text": "Jojoba ester\n\nJojoba esters are the hydrogenation or transesterification product of Jojoba oil. Jojoba Esters are commonly used in cosmetic formulations as an emollient, due to its remarkable similarity to the natural oils produced by the human skin, and its high oxidative stability. Fully hydrogenated jojoba esters are most often small beads used to exfoliate the skin.\n\nJojoba esters are proper waxes; there is no triglyceride component of jojoba esters.\nChemically, jojoba esters are a complex mixture of long chain (C36-C46) fatty acids and fatty alcohols joined by an ester bond. Jojoba esters are produced by the interesterification of jojoba oil, hydrogenated jojoba oil, or a mixture of the two. Pure jojoba oil and pure hydrogenated jojoba oil are also correctly described as jojoba esters. The CTFA does not regard \"partially\"-hydrogenated jojoba oil as jojoba esters. For this reason, jojoba esters must not contain any trans-unsaturation. Jojoba esters' chemical structure is very similar to that of human sebum and of whale oil.\n\nPhysically, jojoba esters are an odourless, colourless liquid, white cream, or hard white wax, with melting points ranging from 15°C to 70°C. Their texture and crystallinity may be modified by rapid cooling, thus altering their cosmetic properties. Jojoba esters are very resistant to oxidation, more so than castor oil, coconut oil, macadamia oil, even many fractions of mineral oil.\n\nJojoba esters are mainly used as emollients in cosmetics such as lipsticks, shampoos and moisturizing lotions. Jojoba esters may be ethoxylated to form such water-soluble materials as PEG-150 Jojoba, PEG-120 Jojoba or PEG-80 Jojoba. Jojoba esters are excellent botanical substitutes for whale oil and its derivatives, such as cetyl alcohol and spermaceti.\n"}
{"id": "13982472", "url": "https://en.wikipedia.org/wiki?curid=13982472", "title": "KMPR1000", "text": "KMPR1000\n\nKMPR1000 is a negative tone photoresist of high aspect ratio made by MicroChem used in microfabrication and other application like BioMEMS.\n"}
{"id": "884879", "url": "https://en.wikipedia.org/wiki?curid=884879", "title": "Kapton", "text": "Kapton\n\nKapton is a polyimide film developed by DuPont in the late 1960s that remains stable across a wide range of temperatures, from . Kapton is used in, among other things, flexible printed circuits (flexible electronics) and thermal blankets used on spacecraft, satellites, and various space instruments.\n\nThe chemical name for Kapton K and HN is poly (4,4'-oxydiphenylene-pyromellitimide). It is produced from the condensation of pyromellitic dianhydride and 4,4'-oxydiphenylamine. Kapton synthesis is an example of the use of a dianhydride in step polymerization. The intermediate polymer, known as a \"poly(amic acid)\", is soluble because of strong hydrogen bonds to the polar solvents usually employed in the reaction. The ring closure is carried out at high temperatures ().\n\nThe thermal conductivity of Kapton at temperatures from 0.5 to 5 Kelvin is rather high for such low temperatures, κ = 4.638×10 \"T\" W·m·K. \nThis, together with its good dielectric qualities and its availability as thin sheets have made it a favorite material in cryogenics, as it provides electrical insulation at low thermal gradients. Kapton is regularly used as an insulator in ultra-high vacuum environments due to its low outgassing rate.\n\nKapton-insulated electrical wiring has been widely used in civil and military aircraft because it is lighter than other insulators and has good insulating and temperature characteristics. \nHowever, Kapton insulation ages poorly: an FAA study shows degradation in under 100 hours in a hot, humid environment, or in the presence of seawater. It was found to have very poor resistance to mechanical wear, mainly abrasion within cable harnesses due to aircraft movement. Many aircraft models have had to undergo extensive rewiring modifications—sometimes completely replacing all the Kapton-insulated wiring—because of short circuits caused by the faulty insulation. Kapton-wire degradation and chafing due to vibration and heat has been implicated in multiple crashes of both fixed wing and rotary wing aircraft, with loss of life.\n\nThe descent stage of the Apollo Lunar Module, and the bottom of the ascent stage surrounding the ascent engine, were covered in blankets of aluminized Kapton foil to provide thermal insulation. During the return journey from the Moon, Apollo 11 astronaut Neil Armstrong commented that during the launch of the Lunar Module ascent stage, he could see \"Kapton and other parts on the LM staging scattering all around the area for great distances.\" \n\nAccording to a NASA internal report, space shuttle \"wires were coated with an insulator known as Kapton that tended to break down over time, causing short circuits and, potentially, fires.\" The NASA Jet Propulsion Laboratory has considered Kapton as a good plastic support for solar sails because of its long duration in the space environment.\n\nNASA's New Horizons spacecraft used Kapton in an innovative \"Thermos bottle\" insulation design to keep the craft operating between throughout its more than nine-year, 3 billion mile journey to rendezvous with the dwarf planet Pluto on July 14, 2015. The main body is covered in lightweight, gold-colored, multilayered thermal insulation which holds in heat from operating electronics to keep the spacecraft warm. The thermal blanketing–18 layers of Dacron mesh cloth sandwiched between aluminized Mylar and Kapton film–also helped to protect the craft from micrometeorites.\n\nThe sunshield of the James Webb Space Telescope is also made of aluminized Kapton.\n\nThe crew aboard the International Space Station used Kapton tape to temporarily repair a slow leak in a Soyuz spacecraft attached to the Russian segment of the orbital complex in August, 2018.\n\nKapton is also commonly used as a material for windows of all kinds at X-ray sources (synchrotron beam-lines and X-ray tubes) and X-ray detectors. Its high mechanical and thermal stability and high transmittance to X-rays make it the preferred material. It is also relatively insensitive to radiation damage.\n\nDue to its large range of temperature stability, and its electrical isolation ability, Kapton tape is usually used in electronic manufacturing as an insulation and protection layer on electrostatic sensitive and fragile components. As it can sustain the temperature needed for a reflow soldering operation, its protection is available throughout the whole production process, and Kapton is often still present in the final consumer product.\n\nKapton and ABS adhere to each other very well, which has led to widespread use of Kapton as a build surface for 3D printers. Kapton is laid down on a flat surface and the ABS is extruded on to the Kapton surface. The ABS part being printed will not detach from the build platform as it cools and shrinks, a common cause of print failure by warping of the part.\n\nKapton is a registered trademark of E. I. du Pont de Nemours and Company.\n"}
{"id": "11848801", "url": "https://en.wikipedia.org/wiki?curid=11848801", "title": "Keldysh formalism", "text": "Keldysh formalism\n\nIn non-equilibrium physics, the Keldysh formalism is a general framework for describing the quantum mechanical evolution of a system in a non-equilibrium state, e.g. in the presence of time varying fields (electrical field, magnetic field etc.). Keldysh formalism is named after Leonid Keldysh. It is sometimes called Schwinger–Keldysh formalism, referring to Julian Schwinger. However, many physicists, like Leo Kadanoff, Gordon Baym, O. V. Konstantinov and V. I. Perel, made significant contributions to developing the method that is now called Keldysh formalism.\n\nTo study non-equilibrium systems, one is interested in one-point functions or average values of quantum operators, two-point functions and so on. These quantities are calculated using Keldysh formalism. The main mathematical object in the Keldysh formalism is the non-equilibrium Green's function (NEGF), which is related to the two-point function of operators in the system we are looking at.\n\nConsider a general quantum mechanical system. This system has the Hamiltonian formula_1. Let the initial state of the system be formula_2, which can either be a pure state or a mixed state. If we now add a time-dependent perturbation to this Hamiltonian, say formula_3, the full Hamiltonian is formula_4 and hence the system will evolve in time under the full Hamiltonian. In this section, we will see how time evolution actually works in quantum mechanics.\n\nLet us consider a Hermitian operator formula_5. In the Heisenberg Picture of quantum mechanics, this operator is time-dependent and the state is not. The expectation value of the operator formula_6 is given by\n\nformula_7\n\nWhere, due to time evolution of operators in the Heisenberg Picture, formula_8. The time-evolution unitary operator formula_9is the time-ordered exponential of an integral formula_10 (Note that if the Hamiltonian at one time commutes with the Hamiltonian at different times, then this can be simplified to formula_11)\n\nFor perturbative quantum mechanics and quantum field theory, it is often more convenient to use the interaction picture. The interaction picture operator is\n\nformula_12\n\nWhere formula_13. Then defining formula_14 we have\n\nformula_15\n\nSince the time-evolution unitary operators satisfy formula_16, the above expression can be rewritten as\n\nformula_17\n\nor with formula_18 replaced by any time value greater than formula_19.\n\nWe can write the above expression more succinctly by, purely formally, replacing each operator formula_20 with a contour-ordered operator formula_21 , such that formula_22 parametrizes the contour path on the time axis starting at formula_23, proceeding to formula_24, and then returning to formula_23. This path is known as the Keldysh contour. formula_26 has the same operator action as formula_20 (where formula_19 is the time value corresponding to formula_22) but also has the additional information of formula_22 (that is, strictly speaking formula_31 if formula_32, even if for the corresponding times formula_33).\n\nThen we can introduce notation of path ordering on this contour, by defining formula_34, where formula_35 is a permutation such that formula_36, and the plus and minus signs are for bosonic and fermionic operators respectively. Note that this is a generalization of time ordering.\n\nWith this notation, the above time evolution is written as\n\nformula_37\n\nWhere formula_22 corresponds to the time formula_19 on the forward branch of the Keldysh contour, and the integral over formula_40 goes over the entire Keldysh contour. For the rest of this article, as is conventional, we will usually simply use the notation formula_20 for formula_26 where formula_19 is the time corresponding to formula_22, and whether formula_22 is on the forward or reverse branch is inferred from context.\n\nThe non-equilibrium Green's function is defined as formula_46.\n\nOr in the interaction picture formula_47 . We can expand the exponential as a Taylor series to obtain the perturbation series formula_48. This is the same procedure as in equilibrium diagrammatic perturbation theory, but with the important difference that both forward and reverse contour branches are included.\n\nIf, as is often the case, formula_49 is a polynomial or series as a function of the elementary fields formula_50, we can organize this perturbation series into monomial terms and apply all possible Wick pairings to the fields in each monomial, obtaining a summation of Feynman diagrams. However, the edges of the Feynman diagram correspond to different propagators depending on whether the paired operators come from the forward or reverse branches. Namely,\n\nwhere the anti-time ordering formula_55 orders operators in the opposite way as time ordering and the formula_56 sign in formula_57 is for bosonic or fermionic fields. Note that formula_58 is the propagator used in ordinary ground state theory.\n\nThus, Feynman diagrams for correlation functions can be drawn and their values computed the same way as in ground state theory, except with the following modifications to the Feynman rules: Each internal vertex of the diagram is labeled with either formula_59 or formula_60, while external vertices are labelled with formula_61. Then each (unrenormalized) edge directed from a vertex formula_62 (with position formula_63, time formula_64 and sign formula_65) to a vertex formula_66 (with position formula_67, time formula_68 and sign formula_69) corresponds to the propagator formula_70. Then the diagram values for each choice of formula_56 signs (there are formula_72 such choices, where formula_73 is the number of internal vertices) are all added up to find the total value of the diagram.\n\n\n"}
{"id": "24255209", "url": "https://en.wikipedia.org/wiki?curid=24255209", "title": "Khimera", "text": "Khimera\n\nKhimera is a software product from Kintech Lab intended for calculation of the kinetic parameters of microscopic processes, thermodynamic and transport properties of substances and their mixtures in gases, plasmas and also of heterogeneous processes.\nThe development of a kinetic mechanism is a key stage of present-day technologies for the creation of hi-tech devices and processes in a wide range of fields, such as microelectronics, chemical industry, and the design and optimization of combustion engines and power stations.\nKhimera with Chemical WorkBench, another software product from Kintech Lab, allows both the development of complex physical and chemical mechanisms and their validation. Essential feature of Khimera is its user-friendly interface for importing and utilizing the results of quantum-chemical calculations for estimating rate constants of elementary processes and thermodynamic and transport properties.\n\nKhimera incorporates up to date achievements in the development of the wide range of models of elementary physicochemical processes; these models are of particular importance for hi-tech applications in:\n\nThe computation modules of Khimera allow one to calculate the kinetic parameters of elementary processes and thermodynamic and transport properties from the data on the molecular structures and properties obtained from quantum-chemical calculations or from an experiment. The molecular properties and the parameters of molecular interactions can be calculated using quantum-chemical software (Gaussian, GAMESS, Jaguar, ADF) and directly imported into Khimera in an automatic mode. The results of calculations can be presented visually and exported for the further use in kinetic modeling and CFD packages.\n\n\n1. J Comput Chem 23: 1375–1389, 2002\n2. https://web.archive.org/web/20160611153527/http://www.softscout.com/software/Science-and-Laboratory/Laboratory-Information-Management-LIMS/Khimera.html\n"}
{"id": "1335495", "url": "https://en.wikipedia.org/wiki?curid=1335495", "title": "Lambda point", "text": "Lambda point\n\nThe Lambda point is the temperature at which normal fluid helium (helium I) makes the transition to superfluid helium II (approximately 2.17 K at 1 atmosphere). The lowest pressure at which He-I and He-II can coexist is the vapor−He-I−He-II triple point at and , which is the \"saturated vapor pressure\" at that temperature (pure helium gas in thermal equilibrium over the liquid surface, in a hermetic container). The highest pressure at which He-I and He-II can coexist is the bcc−He-I−He-II triple point with a helium solid at , .\n\nThe point's name derives from the graph (pictured) that results from plotting the specific heat capacity as a function of temperature (for a given pressure in the above range, in the example shown, at 1 atmosphere), which resembles the Greek letter lambda. The specific heat capacity tends towards infinity as the temperature approaches the lambda point. The tip of the peak is so sharp that a critical exponent characterizing the divergence of the heat capacity can be measured precisely only in zero gravity, to provide a uniform density over a substantial volume of fluid. Hence the heat capacity was measured within 2 nK below the transition in an experiment included in a Space Shuttle payload in 1992.\n\n\n"}
{"id": "38731334", "url": "https://en.wikipedia.org/wiki?curid=38731334", "title": "Los Barrios Power Plant", "text": "Los Barrios Power Plant\n\nThe Los Barrios Power Plant coal-fired power station is based on the Rankine Cycle. It is located in the municipality of Los Barrios in southern Spain, next to the Gibraltar-San Roque Refinery.\n\nThe plant is kept in reserve for Spain's electricity grid. It provides direct employment to over 200 employees and it has a capacity of 567.5 MW.\n"}
{"id": "15501741", "url": "https://en.wikipedia.org/wiki?curid=15501741", "title": "Mango showers", "text": "Mango showers\n\nMango fruit s, or ‘mango rains’s colloquial term to describe the occurrence of pre-monsoon rainfall. Sometimes these rains are referred to generically as ‘April rains’ or ‘Summer showers’. They are notable across much of South and Southeast Asia, including India, and Cambodia. In southern Asia, these rains greatly influence human activities because the control the rains have on crops that are culturally significant like mangoes and coffee.\n\nThese rains normally occur from March to April, although their arrival is often difficult to predict. Their intensity can range from light showers to heavy and persistent thunderstorms. In India, the mango showers occurs as the result of thunderstorm development over the Bay of Bengal.They are also known as 'Kalbaishakhi' in Bengal, as Bordoisila in Assam and as Cherry Blossom shower or Coffee Shower in Kerala.\n\nTowards the close of the summer season, pre-monsoon showers are common, especially in Kerala, Karnataka and parts of Tamil Nadu in India. They help in the early ripening of mangoes, thus often referred to as 'mango showers'.\n"}
{"id": "23922461", "url": "https://en.wikipedia.org/wiki?curid=23922461", "title": "Materials Science Laboratory", "text": "Materials Science Laboratory\n\nThe Materials Science Laboratory (MSL) of the European Space Agency is a payload on board the International Space Station for materials science experiments in low gravity.\n\nIt is installed in NASA's first Materials Science Research Rack which is placed in the Destiny laboratory on board the ISS. Its purpose is to process material samples in different ways: directional solidification of metals and alloys, crystal growth of semi-conducting materials, thermo-physical properties and diffusion experiments of alloys and glass-forming materials, and investigations on polymers and ceramics at the liquid-solid phase transition.\n\nMSL was built for ESA by EADS Astrium in Friedrichshafen, Germany. It is operated and monitored by the Microgravity User Support Center (MUSC) of the German Aerospace Center (DLR) in Cologne, Germany.\n\nMSL was launched with Space Shuttle Discovery on its STS-128 mission at the end of August 2009. It was transferred from the Multi-Purpose Logistics Module to the Destiny Laboratory shortly after the shuttle docked to the International Space Station some two days after launch. \nAfter that the commissioning activities started to check out first the functionality of the Materials Science Research Rack and MSL inside MSRR. The commissioning included the processing of the first two samples which took place at the beginning of November. After bringing those two samples back to ground for analysis by the scientists the rest of the samples from batch 1 will be processed in early 2010.\n\nThe Materials Science Laboratory (MSL) facility is the contribution of the European Space Agency to NASA's MSRR-1. It occupies one half of an International Standard Payload Rack.\n\nThe MSL consists of a Core Facility, together with associated support sub-systems. The Core Facility consists mainly of a vacuum-tight stainless steel cylinder (Process Chamber) capable of accommodating different individual Furnace Inserts (FIs), within which sample processing is carried out. The processing chamber provides an accurately controlled processing environment and measurement of microgravity levels. It can house several different Furnace Inserts. During the first batch of experiments the Low Gradient Furnace (LGF) is installed. Another furnace, the Solidification and Quenching Furnace (SQF) is already produced and waiting on ground for future operations. The FI can be moved with a dedicated drive mechanism, to process each sample according to requirements from the scientists. Processing normally takes place under vacuum.\n\nThe Core Facility supports FIs with up to eight heating elements, and provides the mechanical, thermal and electrical infrastructure necessary to handle the FIs, the Sample Cartridge Assembly (SCA), together with any associated experiment-dedicated electronics that may be required.\n\nA FI is an arrangement of heating elements, isolating zones and cooling zones contained in a thermal insulation assembly. On the outer envelope of this assembly is a water-cooled metal jacket forming the mechanical interface to the Core Facility.\n\nThe major characteristics of the two produced Furnace Inserts are:\n\nThe LGF is designed mainly for Bridgman crystal growth of semiconductor materials. It consists of two heated cavities separated by an adiabatic zone. This assembly can establish low and precisely controlled gradients between two very stable temperature levels.\n\nThe SQF is designed mainly for metallurgical research, with the option of quenching the solidification interface at the end of processing by quickly displacing the cooling zone. It consists of a heated cavity and a water-cooled cooling zone separated by an adiabatic zone. It can establish medium to steep temperature gradients along the experiment sample. For creating large gradients, a Liquid Metal Ring enhances the thermal coupling between the SCA and the cooling zone.\n\nThe samples to be processed are contained in experiment cartridges, the SCAs, that consist of a leak-tight tube, crucible, sensors for process control, sample probe and cartridge foot (i.e. the mechanical and electrical interface to the process chamber). The MSL safety concept requires that experiment samples containing toxic compounds are contained in SCAs that support the detection of potential leaks. The volume between the experiment sample and the cartridge tube is filled with a pre-defined quantity of krypton, allowing leak detection by mass spectrometry. However the first batch of experiments does not contain any toxic substances.\n\nUp to 12 scientific thermocouples provide the sample's temperature profile and allow differential thermal analysis.\n\nMaterials Science Laboratory - Columnar-to-Equiaxed Transition in Solidification Processing (CETSOL) and Microstructure Formation in Casting of Technical Alloys under Diffusive and Magnetically Controlled Convective Conditions (MICAST) are two investigations which will examine different growth patterns and evolution of microstructures during crystallization of metallic alloys in microgravity.\n\nMICAST studies microstructure formation during casting of technical alloys under diffusive and magnetically controlled convective conditions. The experimental results together with parametric studies using numerical simulations, will be used to optimize industrial casting processes. MICAST identifies and controls experimentally the fluid-flow patterns that affect microstructure evolution during casting processes, and to develop analytical and advanced numerical models. The microgravity environment of the International Space Station is of special importance to this project because only there are all gravity-induced convections eliminated and well-defined conditions for solidification prevail that can be disturbed by artificial fluid flow being under full control of the experimenters. Design solutions that make it possible to improve casting processes and especially aluminium alloys with well-defined properties will be provided. MICAST studies the influence of pure diffusive and convective conditions on aluminium-silicon (AlSi) and aluminium-silicon-iron (AlSiFe) cast alloys on the microstructure evolution during directional solidification with and without rotating magnetic field.\n\nThe major objective of CETSOL is to improve and validate the modelling of Columnar-Equiaxed Transition (CET) and of the grain microstructure in solidification processing. This aims to give industry confidence in the reliability of the numerical tools introduced in their integrated numerical models of casting, and their relationship. To achieve this goal, intensive deepening of the quantitative characterization of the basic physical phenomena that, from the microscopic to the macroscopic scales, govern microstructure\nformation and CET will be pursued.\nCET occurs during columnar growth when new grains grow ahead of the columnar front in the undercooled liquid. Under certain conditions, these grains can stop the columnar growth and then the solidification microstructure becomes equiaxed. Experiments have to take place on the ISS due to the long-duration required to solidify samples with the objective to study the CET. Indeed, the length scale of the grain structure when columnar growth takes place is of the order of the casting scale rather than the microstructure scale. This is due to the fact that, to a first approximation, it is the heat flow that controls the transition rather than the solute flow. Experimental programs are being carried out on aluminium-nickel and aluminium-silicon alloys.\n\n\nScientific research on the ISS\n\n"}
{"id": "88771", "url": "https://en.wikipedia.org/wiki?curid=88771", "title": "Metronome", "text": "Metronome\n\nA metronome, from ancient Greek μέτρον (\"métron\", \"measure\") and νέμω (\"némo\", \"I manage\", \"I lead\"), is a device that produces an audible click or other sound at a regular interval that can be set by the user, typically in beats per minute (BPM). Musicians use the device to practice playing to a regular pulse. Metronomes typically include synchronized visual motion (e.g., swinging pendulum or blinking lights).\n\nA kind of metronome was among the inventions of Andalusian polymath Abbas ibn Firnas (810–887). In 1815, Johann Maelzel patented it as a tool for musicians, under the title \"Instrument/Machine for the Improvement of all Musical Performance, called Metronome\".\n\nMusicians practice with metronomes to improve their timing, especially the ability to stick to a tempo. Metronome practice helps internalize a clear sense of timing and tempo. Composers often use a metronome as a standard tempo reference—and may play or sing their work to the metronome to derive beats per minute if they want to indicate that in a composition.\n\nWhen interpreting emotion and other qualities in music, performers seldom play exactly on every beat. Typically, every beat of a musically expressive performance does not align exactly with each click of a metronome. This has led some musicians to criticize use of a metronome, because \"metronome time\" is different from \"musical time\". Some go as far as to suggest that musicians should not use metronomes at all, and have leveled criticism at metronome markings as well.\n\nThe word metronome first appeared in English 1815 and is Greek in origin: \"metron\" \"measure\" and \"nomos\" \"regulating, law\".\n\nAccording to Lynn Townsend White, Jr., the Andalusian inventor, Abbas Ibn Firnas (810–887), made the earliest attempt at creating a metronome.\n\nGalileo Galilei first studied and discovered concepts involving the pendulum in the late 16th and early 17th centuries. In 1696, Etienne Loulié first successfully used an adjustable pendulum to make the first mechanical metronome—however, his design produced no sound, and did not have an escapement to keep the pendulum in motion. To get the correct pulse with this kind of visual device, the musician watches the pendulum as if watching a conductor's baton.\n\nThe more familiar mechanical musical chronometer was invented by Dietrich Nikolaus Winkel in Amsterdam in 1814. Through questionable practice, Johann Maelzel, incorporating Winkel's ideas, added a scale, called it a metronome and started manufacturing the metronome under his own name in 1816: \"Maelzel's Metronome.\" The original text of Maelzel's patent in England (1815) can be downloaded.\n\nLudwig van Beethoven was maybe the first notable composer to indicate specific metronome markings in his music. This was done in 1817.\n\nMusicians practice playing to metronomes to develop and maintain a sense of timing and tempo. For example, a musician fighting a tendency to speed up might play a phrase repeatedly while slightly slowing the BPM setting each time. Even pieces that do not require a strictly constant tempo (such as with rubato) sometimes provide a BPM marking to indicate the general tempo.\n\nTempo is almost always measured in beats per minute (BPM). A metronome's tempo typically is adjustable from 40 to 208 BPM. Another mark that denotes tempo is M.M. (or MM), or Mälzel's Metronome. The notation M.M. is often followed by a note value and a number that indicates the tempo, as in .\nSpecific uses include:\n\nMetronome makers typically mark the speed adjustment for these common tempos:\n\nA mechanical metronome uses an adjustable weight on the end of an inverted pendulum rod to control tempo. The weight slides up the pendulum rod to decrease tempo, or down to increase tempo. (This mechanism is also called a double-weighted pendulum, because there is a second, fixed weight on the other side of the pendulum pivot, inside the metronome case.) The pendulum swings back and forth in tempo, while a mechanism inside the metronome produces a clicking sound with each oscillation. Mechanical metronomes don't need a battery, but run from a spring-wound clockwork escapement.\n\nMost modern metronomes are electronic and use a quartz crystal to maintain accuracy, comparable to those used in wristwatches. The simplest electronic metronomes have a dial or buttons to control the tempo; some also produce tuning notes, usually around the range of A440 (440 hertz). Sophisticated metronomes can produce two or more distinct sounds. Tones can differ in pitch, volume, and/or timbre to demarcate downbeats from other beats, as well as compound and complex time signatures.\n\nMany electronic musical keyboards have built-in metronome functions.\n\nSoftware metronomes run either as stand-alone applications on computers and smart phones, or in music sequencing and audio multitrack software packages. In recording studio applications, such as film scoring, a software metronome may provide a click track to synchronize musicians.\n\nUsers of iPods and other portable MP3 players can use prerecorded MP3 metronome click tracks, which can use different sounds and samples instead of just the regular metronome beep. Users of smartphones can install a wide range of metronome apps. Either method avoids the need to bring a physical metronome along to lessons or practice sessions.\n\nPerhaps the most famous, and most direct, use of the metronome as an instrument is György Ligeti's 1962 composition, \"Poème Symphonique for 100 metronomes\". Two years earlier, Toshi Ichiyanagi wrote \"Music for Electric Metronomes\". Maurice Ravel used three metronomes at different speeds for the opening of his opera \"L'heure espagnole\" (1911).\n\nThe clicking sounds of mechanical metronomes have sometimes been used to provide a soft rhythm track without using any percussion. Paul McCartney did this on \"Distractions\" (\"Flowers in the Dirt\"). Following the metronome, McCartney performed a rhythm track by hitting various parts of his body. Also, in Ennio Morricone's theme \"Farewell to Cheyenne\" (featured on \"Once Upon a Time in the West\"), the steady clip-clop beat is provided by the deliberately distorted and slowed-down sound of a mechanical metronome.\n\nWilliam Kentridge's \"The Refusal of Time\" (2012) features five metronomes in the video installation.\n\nIn the 20th century the metronome is usually positively viewed by performers, musicologists (who spend considerable time analyzing metronome markings), teachers and conservatories. The common view is reflected in the following quote:\n\nMetronomes are often recommended to students without reservation:\n\nNumerous other quotations in favour of the metronome, can be found in the book \"Metronome Techniques: Potpourri of quotations\".\n\nThe quotations above show the importance of the metronome in the 20th century (\"Most music teachers consider the metronome indispensable, and most professional musicians, in fact, continue to practice with a metronome throughout their careers\").\n\nIn the early 19th century the metronome was \"not\" used for ticking all through a piece, but only to check the tempo and then set it aside. This is in great contrast with many musicians today:\nThe metronome has been largely unquestioned in musical pedagogy or scholarship since the twentieth century.\n\nSome writers draw parallels between a modern society that is \"ordered by the clock\" and what they see as metronomic performance practice of today's musicians.\n\nWhile this section highlights the modern trends of strict mechanical performance as something widespread in the 20th century and now, as early as 1860, some people advocated this type of \"modern\" performance practice.|Franz Petersilea (ca. 1860) While some in the 19th century welcomed the metronome, others were critical.\n\nA metronome only provides a fixed, rigid, relentless pulse. Therefore, metronome markings on sheet music provide a reference, but cannot accurately communicate the pulse, swing, or groove of music. The pulse is often irregular, e.g., in accelerando, rallentando, or in musical expression as in phrasing (rubato, etc.).\n\nSome argue that a metronomic performance stands in conflict with an expressive culturally-aware performance of music, so that a metronome is in this respect a very limited tool. Even such highly rhythmical musical forms as Samba, if performed in correct cultural style, cannot be captured with the beats of a metronome. A style of performance that is unfailingly regular rhythmically might be criticized as being \"metronomic.\"\n\nMany notable composers, including Felix Mendelssohn, Richard Wagner, Giuseppe Verdi and Johannes Brahms, criticised use of the metronome.\n\nNumerous other quotations critical of the metronome can be found at .\n\nMetronome technique is extensive and has been the subject of several books. So this short section just summarizes some of the main ideas and approaches. The \"intuitive\" approach to metronome practise, is to simply play along with a metronome. With metronome technique however, musicians do separate exercises to strengthen and steady their sense of rhythm and tempo, and increase their sensitivity to musical time and precision.\n\nThe basic skill required is the ability to play precisely in the pocket with the metronome in a relaxed fashion. This first step helps the musician to relate to the time of the metronome clearly and precisely at the millisecond level, to help internalize a similarly precise sense of time in yourself. It is not a goal in itself, and the aim is not particularly to be able to play like a metronome.\n\nIt is harder to play in the pocket with the metronome than one might expect, especially with piano or percussion. That's because the metronome click may seem to vanish when you hit the click exactly – or may be heard less distinctly. The further you are away from the click the more easily you hear the metronome. Musicians who attempt to play in the pocket with a metronome without use of the established techniques for doing this may find that it introduces tension and effort into their instrument technique.\n\nTo address these issues, musicians start by learning to play consistently ahead or behind the beat whenever they want to. As a result, they develop a clear sense of \"where the click is\" and so can also play to hit the click as well, in a relaxed way.\n\nThe other thing they do is to listen out to hear how the sound of their playing merges with the metronome to create a new sound when you play precisely in the pocket with the metronome. By listening in this way (and through other exercises) it is possible to play precisely in the pocket with the metronome in a relaxed fashion. At the same time as they work on playing in the pocket, they also work on flexibility and the ability to play in the same precise way anywhere in the beat.\n\nMany exercises are used to help with precision of timing and sensitivity to time, also independence, to make sure you don't become too dependent to the metronome. These exercises include:\n\n\nAnd many other exercises. Much of modern metronome technique is to do with various methods to help resolve timing issues, and to encourage and develop a clear sense of musical time and to help with precision of timing.\n\nThis steadiness and precision you can develop and encourage through metronome technique does no harm to musical expression in timing and rhythm; indeed one of the motivations is to help with nuances of timing and tempo. An analogy with art may help. It's like Giotto's circle, or Apelles' straight line, if you can play a perfectly steady and precise beat, it helps with nuances of timing., It doesn't mean that you can only play perfectly steady beats, just as Giotto or Apelles impressive displays of technique didn't mean that they could only draw circles and straight lines. \n\nModern metronome technique addresses the issues of expressive musical rhythms in many ways. For instance, much of the focus of modern metronome technique is on encouraging and developing a good sense of tempo and timing in your playing, and in your mind. So you may work with the metronome in separate exercises to achieve this. When you have a more precise sense of the passage of time, you can then choose for yourself how to use this in your musical performance. You still play in a musically expressive fashion with continually changing tempo and beat; the only difference is that as a result of your work on precision of timing with use of a metronome, you are more aware of what you are doing..\n\nSpecial metronome exercises are used to help keep this fluid sense of rhythm and timing as you work with the metronome. There are many of them, they include:\n\n\nAt the same time you can work on developing a higher level of awareness of the many natural rhythms in your everyday life and use exercises to help bring those rhythms into your music.\n\nIn this way, with suitable metronome techniques, use of a metronome helps you to improve your sense of time and exact timing without causing any of the expected issues for musicality and expressive timing. The thing to bear in mind all the way through is that you use the metronome to help with exact timing – but that the sense of rhythm and musically expressive timing is something that comes from yourself. Rhythm is natural to human beings and pervades our lives, though you may need help to bring that rhythm into music. As Andrew Lewis says in his book:\n\nAn exact sense of the passage of time doesn't come to humans so naturally (sometimes time may seem to pass quickly and sometimes more slowly) and that's where the metronome can help most. That's how the teachers of metronome technique referenced here think of the tool – as a way to increase your sensitivity to musical time, and develop greater precision of timing and a clearer sense of the passage of musical time – relative to which musicians can then use expressive, natural and fluid rhythms, with as much rubato and tempo variance as they wish for.\n\nIf a musician decides not to use a metronome, other methods are required to deal with timing and tempo glitches, and rushing and dragging without its help. These ideas may also be useful as a complementary approach along with metronome technique.\n\nOne starting point is to notice that we rely on a sense of rhythm to perform ordinary activities such as walking, running, hammering nails or chopping vegetables. Even speech and thought has a rhythm of sorts. So one way to work on rhythms is to work on bringing these into music, becoming a \"rhythm antenna\" in Andrew Lewis's words. Until the nineteenth century in Europe, people used to sing as they worked, in time to the rhythms of their work. Musical rhythms were part of daily life, Cecil Sharp collected some of these songs before they were forgotten. For more about this see Work song and Sea shanties. In many parts of the world music is an important part of daily life even today. There are many accounts of people (especially tribal people) who sing frequently and spontaneously in their daily life, as they work, and as they engage in other activities.\n\nMusicians may also work on strengthening their sense of pulse using inner sources, such as breath, and subdividing breaths. Or work with the imagination, imagining a pulse. They may also work with their heart beat, and rhythms in their chest muscles in the same way.\n\nAnother thing they do is to play music in their mind's ear along with the rhythms of walking or other daily life rhythms. Other techniques include hearing music in ones mind's ear first before playing it. Musicians can deal with timing and tempo glitches by learning to hear a perfect performance in their mind's ear first.\n\nIn some styles of music such as early music notes inégales (according to one minority view interpretation) it can be appropriate to use a different approach that doesn't work so much with a sense of inner pulse and instead works on ideas of gestures and is more closely related to rhythms of speech and poetry. Ideas from this approach can be useful for all styles of music.\n\nThe basic ideas are:\n\n\nThis just touches on some of the ideas; for more details, see \"The Craft of Musical Communication\".\n\nThis is a minority view on interpretation of this style of music, but well worth a mention here because of its different approach to musical time and rhythm, and its relevance to the way rhythms can be practised. The more generally accepted view is that Notes inégales were played with the same amount of swing nearly all the time, like modern Jazz.\n\n\n"}
{"id": "8862061", "url": "https://en.wikipedia.org/wiki?curid=8862061", "title": "Natural-gas processing", "text": "Natural-gas processing\n\nNatural-gas processing is a complex industrial process designed to clean raw natural gas by separating impurities and various non-methane hydrocarbons and fluids to produce what is known as \"pipeline quality\" dry natural gas.\n\nNatural-gas processing begins at the well head. The composition of the raw natural gas extracted from producing wells depends on the type, depth, and location of the underground deposit and the geology of the area. Oil and natural gas are often found together in the same reservoir. The natural gas produced from oil wells is generally classified as \"associated-dissolved\", meaning that the natural gas is associated with or dissolved in crude oil. Natural gas production absent any association with crude oil is classified as “non-associated.” In 2009, 89 percent of U.S. wellhead production of natural gas was non-associated.\n\nNatural-gas processing plants purify raw natural gas by removing common contaminants such as water, carbon dioxide (CO) and hydrogen sulfide (HS). Some of the substances which contaminate natural gas have economic value and are further processed or sold. A fully operational plant delivers pipeline-quality dry natural gas that can be used as fuel by residential, commercial and industrial consumers.\n\nRaw natural gas comes primarily from any one of three types of wells: crude oil wells, gas wells, and condensate wells.\n\nNatural gas that comes from crude oil wells is typically called \"associated gas\". This gas can have existed as a gas cap above the crude oil in the underground formation, or could have been dissolved in the crude oil.\n\nNatural gas from gas wells and from condensate wells, in which there is little or no crude oil, is called \"non-associated gas\". Gas wells typically produce only raw natural gas, while condensate wells produce raw natural gas along with other low molecular weight hydrocarbons. Those that are liquid at ambient conditions (i.e., pentane and heavier) are called \"natural-gas condensate\" (sometimes also called \"natural gasoline\" or simply \"condensate\").\n\nNatural gas is called \"sweet gas\" when relatively free of hydrogen sulfide; gas that does contain hydrogen sulfide is called \"sour gas\". Natural gas, or any other gas mixture, containing significant quantities of hydrogen sulfide, carbon dioxide or similar acidic gases, is called \"acid gas.\"\n\nRaw natural gas can also come from methane deposits in the pores of coal seams, and especially in a more concentrated state of adsorption onto the surface of the coal itself. Such gas is referred to as \"coalbed gas\" or \"coalbed methane\" (\"coal seam gas\" in Australia). Coalbed gas has become an important source of energy in recent decades.\n\nRaw natural gas typically consists primarily of methane (CH), the shortest and lightest hydrocarbon molecule. It also contains varying amounts of:\n\nThe raw natural gas must be purified to meet the quality standards specified by the major pipeline transmission and distribution companies. Those quality standards vary from pipeline to pipeline and are usually a function of a pipeline system's design and the markets that it serves. In general, the standards specify that the natural gas:\n\nThere are a great many ways in which to configure the various unit processes used in the processing of raw natural gas. The block flow diagram below is a generalized, typical configuration for the processing of raw natural gas from non-associated gas wells. It shows how raw natural gas is processed into sales gas pipelined to the end user markets. It also shows how processing of the raw natural gas yields these byproducts:\n\n\nRaw natural gas is commonly collected from a group of adjacent wells and is first processed at that collection point for removal of free liquid water and natural gas condensate. The condensate is usually then transported to an oil refinery and the water is disposed of as wastewater.\n\nThe raw gas is then pipelined to a gas processing plant where the initial purification is usually the removal of acid gases (hydrogen sulfide and carbon dioxide). There are many processes that are available for that purpose as shown in the flow diagram, but amine treating is the process that was historically used. However, due to a range of performance and environmental constraints of the amine process, a newer technology based on the use of polymeric membranes to separate the carbon dioxide and hydrogen sulfide from the natural gas stream has gained increasing acceptance. Membranes are attractive since no reagents are consumed.\n\nThe acid gases, if present, are removed by membrane or amine treating can then be routed into a sulfur recovery unit which converts the hydrogen sulfide in the acid gas into either elemental sulfur or sulfuric acid. Of the processes available for these conversions, the Claus process is by far the most well known for recovering elemental sulfur, whereas the conventional Contact process and the WSA (Wet sulfuric acid process) are the most used technologies for recovering sulfuric acid. \n\nThe residual gas from the Claus process is commonly called \"tail gas\" and that gas is then processed in a tail gas treating unit (TGTU) to recover and recycle residual sulfur-containing compounds back into the Claus unit. Again, as shown in the flow diagram, there are a number of processes available for treating the Claus unit tail gas and for that purpose a WSA process is also very suitable since it can work autothermally on tail gases.\n\nThe next step in the gas processing plant is to remove water vapor from the gas using either the regenerable absorption in liquid triethylene glycol (TEG), commonly referred to as glycol dehydration, deliquescent chloride desiccants, and or a Pressure Swing Adsorption (PSA) unit which is regenerable adsorption using a solid adsorbent. Other newer processes like membranes may also be considered.\n\nMercury is then removed by using adsorption processes (as shown in the flow diagram) such as activated carbon or regenerable molecular sieves.\n\nAlthough not common, nitrogen is sometimes removed and rejected using one of the three processes indicated on the flow diagram:\n\n\nThe next step is to recover the natural gas liquids (NGL) for which most large, modern gas processing plants use another cryogenic low temperature distillation process involving expansion of the gas through a turbo-expander followed by distillation in a demethanizing fractionating column. Some gas processing plants use lean oil absorption process rather than the cryogenic turbo-expander process.\n\nThe recovered NGL stream is sometimes processed through a fractionation train consisting of three distillation towers in series: a deethanizer, a depropanizer and a debutanizer. The overhead product from the deethanizer is ethane and the bottoms are fed to the depropanizer. The overhead product from the depropanizer is propane and the bottoms are fed to the debutanizer. The overhead product from the debutanizer is a mixture of normal and iso-butane, and the bottoms product is a C+ mixture. The recovered streams of propane, butanes and C+ may be \"sweetened\" in a Merox process unit to convert undesirable mercaptans into disulfides and, along with the recovered ethane, are the final NGL by-products from the gas processing plant. Currently, most cryogenic plants do not include fractionation for economic reasons, and the NGL stream is instead transported as a mixed product to standalone fractionation complexes located near refineries or chemical plants that use the components for feedstock. In case laying pipeline is not possible for geographical reason,or the distance between source and consumer exceed 3000 km, natural gas is then transported by ship as LNG (liquefied natural gas) and again converted into its gaseous state in the vicinity of the consumer.\n\nThe residue gas from the NGL recovery section is the final, purified sales gas which is pipelined to the end-user markets. Rules and agreements are made between buyer and seller regarding the quality of the gas. These usually specify the maximum allowable concentration of CO, HS and HO as well as requiring the gas to be commercially free from objectionable odours and materials, and dust or other solid or liquid matter, waxes, gums and gum forming constituents, which might damage or adversely affect operation of the buyers equipment. When an upset occurs on the treatment plant buyers can usually refuse to accept the gas, lower the flow rate or re-negotiate the price. \n\nIf the gas has significant helium content, the helium may be recovered by fractional distillation. Natural gas may contain as much as 7% helium, and is the commercial source of the noble gas. For instance, the Hugoton Gas Field in Kansas and Oklahoma in the United States contains concentrations of helium from 0.3% to 1.9%, which is separated out as a valuable byproduct.\n\nNatural gas consumption patterns, across nations, vary based on access. Countries with large reserves tend to handle the raw-material natural gas more generously, while countries with scarce or lacking resources tend to be more economical. Despite the considerable findings, the predicted availability of the natural-gas reserves has hardly changed.\n\n\n\n\n"}
{"id": "21005166", "url": "https://en.wikipedia.org/wiki?curid=21005166", "title": "New Electricity Trading Arrangements", "text": "New Electricity Trading Arrangements\n\nNew Electricity Trading Arrangements (NETA) is the system of market trading arrangements under which electricity is traded in the United Kingdom's wholesale electricity market as of 27 March 2001. The arrangements provided that parties could trade off their imbalances close to real time.\n\nAs of April 2005, NETA changed its name to the British Electricity Trading Transmission Arrangements, and expanding to become the single Great Britain electricity market of England, Wales and Scotland.\n\n\n"}
{"id": "55271247", "url": "https://en.wikipedia.org/wiki?curid=55271247", "title": "Pacific Manuscripts Bureau", "text": "Pacific Manuscripts Bureau\n\nThe Pacific Manuscripts Bureau is a non-profit organisation sponsored by an international consortium of libraries specialising in Pacific research. The Pacific Manuscripts Bureau was formed in 1968 to copy archives, manuscripts and rare printed material relating to the Pacific Islands. The aim of the Bureau is to help with long-term preservation of the documentary heritage of the Pacific Islands and to make it accessible.\n\nPacific scholar Harry E. Maude, Department of Pacific History at the Australian National University from 1958, was the first to conceptualise the workings of the Bureau when he discussed the need for interlibrary co-operation in the copying of documents of Pacific interest for Pacific researchers with Dr Floyd Cammack, University of Hawaii in December 1962. Dr Cammack proposed an association of Pacific research libraries and contacted potential members, winning support from Gordon Richardson, Mitchell Librarian and Principal Librarian of the NSW Public Library. In 1966 Richardson pursued the idea of a joint copying scheme and with the University of Hawaii requested Maude produce a paper to make the scheme a reality.\n\nMaude’s Paper entitled: The Documentary Basis for Pacific Studies: a Report on Progress and Desiderata, was released in March 1967. It proposed setting up a Pacific Islands Manuscripts Clearing Centre to identify and obtain microfilm copies of unpublished documents on the Pacific Islands.\n\nMaude’s Paper recommended the proposed Pacific Islands Manuscripts Clearing Centre be established at the Australian National University (ANU), the only institution in the world at that time with a Department of Pacific History. The Australian National University supported Maude’s proposal and invited the National Library of Australia, National Library of New Zealand, the Mitchell Library (State Library of New South Wales), and the University of Hawaii to sponsor and establish the Centre effective 1 January 1968.\n\nThe Pacific Islands Manuscripts Clearing Centre was declared fully operational under the name Pacific Manuscripts Bureau on 1 July 1968 with Robert Langdon, then editor of the \"Pacific Islands Monthly\", appointed to manage the Bureau.\n\nThe aim of the Bureau is to help with long-term preservation of the documentary heritage of the Pacific Islands and to make it accessible.\n\nTo this end the Bureau undertakes fieldwork in the Pacific Islands to copy archives, manuscripts and rare printed material, particularly when the material is in danger of loss or destruction. Island based records have been identified as most at risk due to climatic factors and lack of trained staff to care for them. Fieldwork reports can be found on the Bureau’s website.\n\nThe Bureau has an interest in copying archives relating to the Pacific in major collections throughout the world, including member libraries. An earlier co-operative microfilming project, The Australian Joint Copying Project, achieved some success. In its 45 year history it microfilmed a large number of Pacific manuscripts in the United Kingdom, many identified by Mitchell Librarian, Phyllis Mander-Jones during the 1960’s.\n\nThe Bureau is based at the Australian National University in the College of Asia and the Pacific and is sponsored by an international consortium of Pacific research libraries. Each member Library has the complete PMB Collection.\n\nThe Libraries are:\n\n\nThe Pacific Manuscripts Bureau collection is the most extensive collection of non-government primary documentation on the Pacific Islands available to researchers. The Archive includes in excess of 4000 microfilm reels comprising a diverse range of records, copied from following documents:\n\n\nThe Pacific Manuscripts Bureau microfilm collection is divided into three series:\n\nUnpublished records of organisations and people associated with the Pacific Islands. Records include manuscripts, diaries, minutes, correspondence, linguistic materials and research papers.\n\nPublished material including newspapers, newsletters and other serials published by organisations with interests in the Pacific Islands.\n\nRecords of the Catholic Church in islands of the Western Pacific. Includes all six dioceses: Tonga, Samoa and Tokelau, Wallis and Futuna, Port Vila, Noumea, Suva. The records are indexed in the guide: \"The Catholic Church in the Western Pacific: a guide to records on microfilm\". The catalogue has been digitised and can be accessed on the Bureau's website.\n\nThe Pacific Manuscripts Bureau collection also includes audio and photographic collections.\n\nAccess to the collection is via the Pacific Manuscripts Bureau online catalogue. This can be freely searched on the Internet by researchers to find relevant documents. Most of these documents are microfilm copies of originals held in archives and libraries throughout the Pacific. However, the Bureau is in the process of digitising its microfilm collections. Member libraries have access to these digital collections via the PMB catalogue.\n\nThe Bureau publishes finding aids to assist researchers in accessing PMB collections. These include subject guides and country specific guides including Fiji, Vanuatu and the Cook Islands.\n\nThe newsletter, Pambu, has been published since the Bureau’s inception in 1968. Pambu reports on the Bureau’s microfilming projects and fieldtrips as well as advertising books on the Pacific and publishing articles by Pacific scholars.\n"}
{"id": "5199213", "url": "https://en.wikipedia.org/wiki?curid=5199213", "title": "Phasor measurement unit", "text": "Phasor measurement unit\n\nA phasor measurement unit (PMU) is a device used to estimate the magnitude and phase angle of an electrical Phasor quantity like voltage or current in the electricity grid using a common time source for synchronization. Time synchronization is usually provided by GPS and allows synchronized real-time measurements of multiple remote measurement points on the grid. PMUs are capable of capturing samples from a waveform in quick succession and reconstruct the Phasor quantity. The resulting measurement is known as a synchrophasor. These devices can also be used to measure the frequency in the power grid. A typical commercial PMU can report measurements with very high temporal resolution in the order of 30-60 measurements per second. This helps engineers in analyzing dynamic events in the grid which is not possible with traditional SCADA measurements that generate one measurement every 2 or 4 seconds. Therefore, PMUs equip utilities with enhanced monitoring and control capabilities and are considered to be one of the most important measuring devices in the future of power systems. A PMU can be a dedicated device, or the PMU function can be incorporated into a protective relay or other device.\n\nIn 1893, Charles Proteus Steinmetz presented a paper on simplified mathematical description of the waveforms of alternating current electricity. Steinmetz called his representation a phasor. With the invention of phasor measurement units (PMU) in 1988 by Dr. Arun G. Phadke and Dr. James S. Thorp at Virginia Tech, Steinmetz’s technique of phasor calculation evolved into the calculation of real time phasor measurements that are synchronized to an absolute time reference provided by the Global Positioning System. We therefore refer to synchronized phasor measurements as \"synchrophasors\". Early prototypes of the PMU were built at Virginia Tech, and Macrodyne built the first PMU (model 1690) in 1992.\nWith the growth of increasingly more distributed energy resources on the power grid, more observability and controls systems will be needed to monitor power flow. Historically, power has been delivered in a uni-directional fashion through passive components to customers. With an increasingly more complex network of generation and loads, it is imperative that the electrical conditions of transmission and distribution networks are continuously being observed through advanced sensor technology––PMUs and uPMUs.\n\nIn simple terms, the public electrical grid that a power company operates was originally designed to take power from a single source: the operating company's generators and power plants, and feed it into the grid, where the customers consume the power. Now, some customers are operating power generating devices (solar panels, windmills, etc.) and to save costs (or to generate income) are also feeding power back into the grid. As the electric utility is required by law to buy power produced by customers, the utility wants to make sure this does not damage the grid. The grid must now be measured and controlled in order to ensure the power going into the grid from other sources is of the same quality and standards that customer equipment has been expecting from it, or, if this is not done, as Rob Landley put it, \"people's light bulbs start exploding.\" This measurement function is what these devices do.\n\nA PMU can measure 50/60 Hz AC waveforms (voltages and currents) typically at a rate of 48 samples per cycle. The analog AC waveforms are digitized by an analog-to-digital converter for each phase. A phase-locked oscillator along with a Global Positioning System (GPS) reference source provides the needed high-speed synchronized sampling with 1 microsecond accuracy. However, PMUs can take in multiple time sources including non-GPS references as long as they are all calibrated and working synchronously. The resultant time-stamped phasors can be transmitted to a local or remote receiver at rates up to 120 samples per second. Historically, only small numbers of PMUs have been used to monitor transmission lines with acceptable errors of around 1%. These were simply coarser devices installed to prevent catastrophic blackouts. Now, with the invention of micro-synchronous phasor technology, many more of them are desired to be installed on distribution networks where power can be monitored at a very high degree of precision. This high degree of precision creates the ability to drastically improve system visibility and implement smart and preventative control strategies. No longer are PMUs just required at sub-stations, but are required at several places in the network including tap-changing transformers, complex loads, and PV generation buses.\n\nA phasor is a complex number that represents both the magnitude and phase angle of the sine waves found in electricity. Phasor measurements that occur at the same time are called \"synchrophasors\". While it is commonplace for the terms \"PMU\" and \"synchrophasor\" to be used interchangeably they actually represent two separate technical meanings. A synchrophasor is the metered value whereas the PMU is the metering device. In typical applications, phasor measurement units are sampled from widely dispersed locations in the power system network and synchronized from the common time source of a Global Positioning System (GPS) radio clock. Synchrophasor technology provides a tool for system operators and planners to measure the state of the electrical system and manage power quality. \n\nPMUs measure voltages and currents at principal intersecting locations (critical substations) on a power grid and can output accurately time-stamped voltage and current phasors. Because these phasors are truly synchronized, synchronized comparison of two quantities is possible in real time. These comparisons can be used to assess system conditions-such as; frequency changes, MW, MVARs, kVolts, etc. The monitored points are preselected through various studies to make extremely accurate phase angle measurements to indicate shifts in system (grid) stability. The phasor data is collected either on-site or at centralized locations using Phasor Data Concentrator technologies. The data is then transmitted to a regional monitoring system which is maintained by the local Independent System Operator (ISO). These ISO's will monitor phasor data from individual PMU's or from as many as 150 PMU's — this monitoring provides an accurate means of establishing controls for power flow from multiple energy generation sources (nuclear, coal, wind, etc.).\n\nThe technology has the potential to change the economics of power delivery by allowing increased power flow over existing lines. Synchrophasor data could be used to allow power flow up to a line's dynamic limit instead of to its worst-case limit. Synchrophasor technology will usher in a new process for establishing centralized and selective controls for the flow of electrical energy over the grid. These controls will affect both large scale (multiple-states) and individual transmission line sections at intersecting substations. Transmission line congestion (over-loading), protection, and control will therefore be improved on a multiple region scale (US, Canada, Mexico) through interconnecting ISO's. \n\nA phasor network consists of phasor measurement units (PMUs) dispersed throughout the electricity system, Phasor Data Concentrators (PDC) to collect the information and a Supervisory Control And Data Acquisition (SCADA) system at the central control facility. Such a network is used in Wide Area Measurement Systems (WAMS), the first of which began in 2000 by the Bonneville Power Administration. The complete network requires rapid data transfer within the frequency of sampling of the phasor data. GPS time stamping can provide a theoretical accuracy of synchronization better than 1 microsecond. \"Clocks need to be accurate to ± 500 nanoseconds to provide the one microsecond time standard needed by each device performing synchrophasor measurement.\" For 60 Hz systems, PMUs must deliver between 10 and 30 synchronous reports per second depending on the application. The PDC correlates the data, and controls and monitors the PMUs (from a dozen up to 60). At the central control facility, the SCADA system presents system wide data on all generators and substations in the system every 2 to 10 seconds.\n\nPMUs often use phone lines to connect to PDCs, which then send data to the SCADA or Wide Area Measurement System (WAMS) server. Additionally, PMUs can use ubiquitous mobile (cellular) networks for data transfer (GPRS, UMTS), which allows potential savings in infrastructure and deployment costs, at the expense of a larger data reporting latency. However, the introduced data latency makes such systems more suitable for R&D measurement campaigns and near real-time monitoring, and limits their use in real-time protective systems.\n\nPMUs from multiple vendors can yield inaccurate readings. In one test, readings differed by 47 microseconds – or a difference of 1 degree of at 60 Hz- an unacceptable variance. China's solution to the problem was to build all its own PMUs adhering to its own specifications and standards so there would be no multi-vendor source of conflicts, standards, protocols, or performance characteristics.\n\nInstallation of a typical 10 Phasor PMU is a simple process. A phasor will be either a 3 phase voltage or a 3 phase current. Each phasor will, therefore, require 3 separate electrical connections (one for each phase). Typically an electrical engineer designs the installation and interconnection of a PMU at a substation or at a generation plant. Substation personnel will bolt an equipment rack to the floor of the substation following established seismic mounting requirements. Then the PMU along with a modem and other support equipment will be mounted on the equipment rack. They will also install the Global Positioning Satellite (GPS) antenna on the roof of the substation per manufacturer instructions. Substation personnel will also install \"shunts\" in all Current transformer (CT) secondary circuits that are to be measured. The PMU will also require communication circuit connection (Modem if using 4-wire connection or Ethernet for network connection).\n\n\n\nThe IEEE 1344 standard for synchrophasors was completed in 1995, and reaffirmed in 2001. In 2005, it was replaced by IEEE C37.118-2005, which was a complete revision and dealt with issues concerning use of PMUs in electric power systems. The specification describes standards for measurement, the method of quantifying the measurements, testing & certification requirements for verifying accuracy, and data transmission format and protocol for real-time data communication. This standard was not comprehensive- it did not attempt to address all factors that PMUs can detect in power system dynamic activity. A new version of the standard was released in December 2011, which split the IEEE C37.118-2005 standard into two parts: C37.118-1 dealing with the phasor estimation & C37.118-2 the communications protocol. It also introduced two classifications of PMU, M — measurement & P — protection. M class is close in performance requirements to that in the original 2005 standard, primarily for steady state measurement. P class has relaxed some performance requirements and is intended to capture dynamic system behaviour.\n\nOther standards used with PMU interfacing:\n\n\n"}
{"id": "2757018", "url": "https://en.wikipedia.org/wiki?curid=2757018", "title": "Phenomenology (physics)", "text": "Phenomenology (physics)\n\nIn physics, phenomenology is the application of theoretical physics to experimental data by making quantitative predictions based upon known theories. It is in contrast to experimentation in the scientific method, in which the goal of the experiment is to test a scientific hypothesis instead of making predictions. Phenomenology is related to the philosophical notion in that these predictions describe anticipated behaviors for the phenomena in reality. \n\nPhenomenology is commonly applied to the field of particle physics, where it forms a bridge between the mathematical models of theoretical physics (such as quantum field theories and theories of the structure of space-time) and the results of the high-energy particle experiments. It is sometimes used in other fields such as in condensed matter physics and plasma physics, when there are no existing theories for the observed experimental data.\n\nWithin the well-tested and generally accepted Standard Model, phenomenology is the calculating of detailed predictions for experiments, usually at high precision (e.g., including radiative corrections).\n\nExamples include:\n\n\nThe CKM matrix is useful in these predictions:\n\n\nIn physics Beyond the Standard Model, phenomenology addresses the experimental consequences of new models: how their new particles could be searched for, how the model parameters could be measured, and how the model could be distinguished from other, competing models.\n\nPhenomenological analyses, in which one studies the experimental consequences of adding the most general set of beyond-the-Standard-Model effects in a given sector of the Standard Model, usually parameterized in terms of anomalous couplings and higher-dimensional operators. In this case, the term \"phenomenological\" is being used more in its philosophy of science sense.\n\n\n"}
{"id": "37528177", "url": "https://en.wikipedia.org/wiki?curid=37528177", "title": "Plug-in electric vehicles in France", "text": "Plug-in electric vehicles in France\n\nThe adoption of plug-in electric vehicles in the France is actively supported by the French government through a bonus-malus system and other incentives. The government provides subsidies towards the purchase of all-electric vehicles and plug-in hybrids with low emissions. The French government also set up a national purchase incentive scheme for all-electric utility vans.\n\n, the stock of light-duty plug-in electric vehicles registered in France since 2010 totaled almost 150,000 units. The country, , ranked as the second largest plug-in market in Europe after Norway, and the world's fifth. , the French plug-in electric stock consisted of 92,256 all-electric passenger cars, 25,269 all-electric utility vans, and 32,272 plug-in hybrids. The split among type of powertrain is influenced by the rules of the government subsidies, which favors pure electric vehicles over plug-in hybrids.\n\nThe market share of all-electric passenger cars increased from 0.30% of new car registered in 2012, to 0.49% in 2013, and reached 0.59% in 2014. Until 2013, most plug-in cars sold in France were pure electric cars, but from 2015, sales of plug-in hybrid cars rose significantly. After the introduction of super-bonus for the scrappage of old diesel-power cars in 2015, sales of both segments of plug-in cars surged, and for the first time the French plug-in market share passed the 1% mark, ending 2015 with a market share of 1.17% of total new car registrations that year. The plug-in market share climbed to 1.40% of new car registrations in 2016, and achieved a record market share of 1.98% in 2017.\n\n, France ranked as the country with the world's largest market for light-duty electric commercial vehicles. Nearly half of the vans sold in the European Union are sold in the country. The market share of all-electric utility vans reached a market share of 1.22% of new vans registered in 2014, and 1.30% in 2015.\n\nThe Renault Zoe has led all-electric car sales in France since 2013, and is the country's all-time best selling plug-in electric car with 48,582 units registered since 2012 through December 2017. The Renault Kangoo Z.E. utility van ranks second after the Zoe, and it is the all-time top selling electric van with 15,032 units registered since 2010.\n\nSince 2008 France has a bonus-malus system offering a financial incentive, or bonus, for the purchase of cars with low carbon emissions, and a fee, or malus, for the purchase of high-emission vehicles. The bonus applies to private and company vehicles purchased on or after 5 December 2007 and is deducted from the purchase price of the vehicle. The malus penalty applies to all vehicles registered after 1 January 2008, and is added at the time of registration. Since 2009, every family with more than two children receives a deduction from the malus of 20 g of per km per child.\n\nUntil July 31, 2012, a premium up to , under the bonus-malus system, was granted for the purchase of new cars with emissions of 60 g/km or less which benefited all-electric cars and any plug-in hybrid with such low emissions. Vehicles emitting up to 125 g/km or less, such as conventional hybrids and natural gas vehicles, were granted up to . The incentive could not exceed 20% of the sales price including VAT, increased with the cost of the battery if it is rented.\n\nEffective on August 1, 2012, the government increased the bonus for electric cars up to but capped at 30% of the vehicle price including VAT. The price includes any battery leasing charges, and therefore, electric cars which need a battery leasing contract also are eligible for the bonus. For example, an electric car sold for including VAT was eligible for the maximum bonus of . The emission level for the maximum bonus was raised to 20 g/km or less. Cars with emission levels between 20 and 50 g/km were eligible to a bonus of up to , and between 50 and 60 g/km were eligible to a bonus of up to . After this limit, the bonus dropped to .\n\nThe fee schedule for the bonus-malus was modified in 2013. Effective November 1, 2013, the bonus was reduced from to for all-electrics and any other vehicle with emissions of less than 21 g/km. Vehicles emitting between 21 and 60 g/km, such as plug-in hybrids and conventional hybrids, were eligible to a bonus up to , and for emissions between 61 and 90 g/km up to , down from . Effective January 1, 2014, the fee schedule for the malus was increased to a maximum penalty of from for vehicles emitting over 200 g/km. A neutral class applied to vehicles emitting between 91-130 g/km.\n\nFrom April 1, 2015, a super-bonus was introduced, increasing the financial incentive to a cumulative total of , consisting of the regular bonus of for purchasing a pure electric car, plus up to for customers scrapping a diesel-powered car in circulation before 1 January 2001. In the case of plug-in hybrids with emission levels between 21 and 60 g/km, the purchase bonus is plus the scrapping premium of . Also a specific grant was introduced for families which are below the income tax threshold who buy an ordinary new or second hand car below certain emission thresholds or a hybrid or electric car.\n\nEffective January 4, 2016, the purchase bonus limited to 27% of the purchase price for vehicles emitting up to 20 g/km was kept. This bonus corresponds to pure electric vehicles and those equipped with a range extender. Vehicles emitting between 21 and 60 g/km were entitled to a bonus. This bonus corresponds to the majority of plug-in hybrids. Conventional hybrid passenger cars emitting between 61 and 110 g/km with sufficient level of hybridization, with an electric motor with an output power of not be less than 10 kW, are entitle to a bonus. The super-bonus for the purchase or lease of a new all-electric car was maintained. To be eligible for the additional scrappage bonus, the old diesel-powered car have to be owned for at least a year and in circulation before 1 January 2006. The new vehicle must not be sold within 6 months of acquisition or have traveled less than .\n\nThe scrappage bonus for the purchase of pure electric cars was maintained at , while the bonus for plug-in hybrid car emitting between 21 and 60 g/km was set at . Only individuals or professionals are eligible for the scrappage bonus. Commercial vehicles are not eligible. Neither demonstration vehicles are eligible to the superbonus unless the vehicles are sold or leased within one year following the date of first registration. , the scrappage bonus of for trading in old diesel-powered cars has been granted to more than 10,000 purchase transactions.\n\n, the government proposal to be in force from 1 January 2017 provides that the super-bonus for scrapping a diesel vehicle over 10 years-old will be renewed. However, the bonus for the purchase of a pure electric car will drop to from in 2016, but to compensate, the additional scrappage bonus will be increased to from in 2016. Also, the government plans to introduce a purchase price cap to the vehicles eligible for the bonus, and to introduce a new bonus for two-wheeled motor vehicles. For the more polluting vehicles, the government intends to increase the maximum malus fee to from in 2016 for vehicles emitting more than 191 g/km, lowering the limit from 200 g/km in 2016.\n\nThe government intends to maintain the purchase bonus for plug-in hybrids with a emission level between 21 and 60 g/km. However, the proposal does not include anything about the conversion premium for scrapping a 10-year-old diesel car for the purchase of a plug-in hybrid. The purchase bonus for non-rechargeable hybrid vehicles will be eliminated.\n\n\nIn September 2013, several French news outlets reported that according to the Norwegian newspaper Dagens Næringsliv, some car dealers in Norway have been buying electric cars in France and earning the (~ ) government subsidy. These cars are then imported to Norway and after discounting the freight costs, they are sold at a discount. Dagens Næringsliv cited the case of one dealer near Oslo with 70% of its electric car sales corresponding to vehicles imported from France, and with at least 40 Leafs imported, totaling ( ~ ) in benefits at a cost of the French taxpayers. These dealers took advantage of a loophole in the French law, which only requires to have an address in the country when buying a new car.\n\nThe stock of light-duty plug-in electric vehicles registered in France passed the 100,000 unit milestone in October 2016, making the country the second largest plug-in market in Europe after Norway. It ranked also as the world's fifth largest plug-in market after the U.S., China, Japan and Norway. , a total of 149,797 light-duty plug-in electric vehicles have been registered in France since 2010. The plug-in electric stock consisted of 92,256 all-electric passenger cars, 25,269 all-electric utility vans, and 32,272 plug-in hybrids. Until 2013, most plug-in cars registered in France were pure electric cars, but from 2015, sales of plug-in hybrid cars rose significantly. After the introduction of super-bonus for the scrappage of old diesel-power cars in April 2015, sales of both segments of plug-in cars surged. The plug-in passenger car segment achieved a record market share of 1.98% of new car registrations in 2017.\n\n, France is the country with the world's largest market for light-duty electric commercial vehicles or utility vans. Nearly half of the electric vans sold in the European Union are sold in France as a result of a national purchase incentive scheme, which French companies have embraced. The market share of all-electric utility vans reached a market share of 1.22% of new vans registered in 2014, and 1.30% in 2015. \n\nElectric car registrations increased from 184 units in 2010 to 2,630 in 2011. Sales in 2012 increased 115% from 2011 to 5,663 electric cars, making France the world's fourth largest all-electric country market, with an 11% market share of global all-electric car sales in 2012.\n\nAll-electric car sales in the French market for 2011 were led by the Citroën C-Zero with 645 units followed by the Peugeot iOn with 639 vehicles, and the Bolloré Bluecar with 399 units. During 2012, all-electric car registrations in France were led by the Bluecar with 1,543 units, the C-Zero with 1,409, and the iOn with 1,335, together representing 76% of all electric car sales that year. The Renault Kangoo Z.E. was the top selling utility electric vehicle with 2,869 units registered in 2012, capturing 82% of the segment sales. The Renault Twizy electric quadricycle, launched in March 2012, sold 2,232 units during 2012, surpassing the Bolloré Bluecar, the top selling highway-capable electric car, and ranked as the second best selling plug-in electric vehicle after the Kangoo Z.E.\n\nRegistrations reached 8,779 electric cars in 2013, up 55.0% from 2012, and the all-electric market share of total new car sales went up to 0.49% from 0.3% in 2012. In addition, 5,175 electric utility vans were registered in 2013, up 42% from 2012, and representing a market share of 1.4% of all new light commercial vehicles sold in 2013. Sales of electric passenger cars and utility vans totaled 13,954 units in 2013, capturing a combined market share of 0.65% of these two segments new car sales. When accounting together sales of pure electric cars and light utility vehicles, France was the leading European all-electric market in 2012 and 2013.\n\nA total of 666 plug-in hybrids were registered during 2012. The segment sales were led by the Toyota Prius PHV, with 413 registrations, followed by the Opel Ampera with 190. During 2013 a total of 800 plug-in hybrids were sold, up 20% from 2012, with the Prius PHEV continuing as the segment leader with 393 units, followed by the Volvo V60 PHEV with 241 units and the Porsche Panamera S E-Hybrid with 90 units. When plug-in hybrids sales in 2013 are accounted for, a total of 14,762 plug-in electric vehicles were registered in France in 2013, making the country to rank second in the plug-in European market after the Netherlands, which sold 28,673 plug-in electric vehicles in 2013.\n\nDuring 2013, registrations of pure electric cars were led by the Renault Zoe with 5,511 units representing 62.8% of total electric car sales, followed by the Nissan Leaf with 1,438 units. Registrations of all-electric light utility vehicles were led by the Renault Kangoo Z.E. with 4,174 units, representing 80.7% of the segment sales. During 2013 several electric cars from major manufacturers were launched in France. Tesla Model S deliveries to retail customers began in September 2013, the BMW i3 was launched in October, and the Volkswagen e-Up! in November.\n\nA total of 15,045 all-electric cars and vans were registered in 2014, up 7.8% from 2013. With 10,560 cars registered in 2014, up 20.3% from the previous year, sales of all-electric vehicles passed the 10,000 unit milestone for the first time. This figure rises to 10,968 units if the BMW i3 with range extender is accounted for. All-electric utility vans continued to be a significant share of the all-electric segment, with 4,485 units registered in 2014, but down 13.3% from 2013. All-electric cars captured a 0.59% market share of the 1.7 million new car registered in France in 2014, while pure electric utility vans reached a 1.22% market share of their segment. Combined both segments represented a market share of 0.70% of new registrations in the country in 2014.\n\nLight-duty all-electric vehicle sales achieved its best monthly volume on record ever in December 2014, with 2,227 units registered, twice the volume registered the same month in 2013. The slow down in sales that took place in the French EV market during the first half of 2014, allowed Norway, with 18,649 new all-electric vehicles registered, to end 2014 as the top selling European market in the all-electric segment, with France ranking second.\n\nBetween 2012 and 2014, cumulative plug-in hybrid registrations reached 2,985 units, rising cumulative French registrations of plug-in electric vehicles since 2005 to 46,590 units, just ahead of the Netherlands (45,020), and making France the European country where there were more plug-in electric vehicles on the road .\n\nThe Zoe continued leading plug-in electric vehicle registrations in 2014, with 5,970 units registered, followed by the Kangoo Z.E. van with 2,657 registrations, and the Nissan Leaf ranked next with 1,600 units. Plug-in hybrid car registrations totaled 1,527 units in 2014, almost doubling registrations from a year earlier. Plug-in hybrid sales were driven by the Mitsubishi Outlander P-HEV, with 820 units registered in 2014, representing 54% of the segment registrations in France that year.\n\nA total of 22,695 light-duty all-electric vehicles were sold in 2015. Sales during this period consisted of 17,779 all-electric cars, up 62.1% from 2014, and 4,916 all-electric utility vehicles, up 9.6% from 2014. All-electric cars captured a 0.9% market share of new car sales in 2015, and electric utility vans a 1.30%. Combining sales of the two segments, the market share of battery electric vehicles rises to 1.2%.\n\nSales of plug-in hybrids surged in 2015, with 5,006 plug-in hybrids registered in France, up 228% from 2014. The market share of the plug-in hybrid segment reached a market share of 0.26% of the 1.94 million new car registered in 2015. Light-duty plug-in registrations totaled 27,701 units in 2015. Plug-in passenger cars achieved a market share of 1.17% of total new car registrations in 2015.\n\nAll-electric car registrations in 2015 continued to be led by the Renault Zoe, with 10,406 units, followed by the Nissan Leaf with 2,220 and the Bolloré Bluecar with 1,166 units. The all-electric utility van segment was led by the Kangoo Z.E. with 2,836 units sold, up 6.7% from 2014. The plug-in hybrid segment was led by the Volkswagen Golf GTE with 1,687 units, followed by the Audi A3 e-tron with 1,123, and the Mitsubishi Ourlander P-HEV with 907.\n\nA total of 33,774 light-duty electric vehicles were registered in France in 2016, making the country the third largest in Europe in 2016 after Norway and the UK. France was the top selling European market in the light-duty all-electric segment with 27,307 units registered, up 23% from 2015. Total registrations in 2016 consisted of 21,751 all-electric cars, 5,556 electric utility vans and 6,467 plug-in hybrid cars. The plug-in car segment achieved a market share of 1.40% of new car registrations in the country in 2016. The Renault Zoe, with 11,404 units registered in 2016, ranked as the top selling pure electric car for the fourth year on a row, followed by the Nissan Leaf with 3,887 units, and the BMW i3 with 1,347 (both variants). The top selling plug-in hybrids were the Volkswagen Golf GTE with about 1,060 units, followed by the Volvo XC90 with 742 units, and the Audi A3 e-tron with 659. The Renault Kangoo ZE again ranked as the top selling utility van with 2,389 units registered.\n\nA total of 41,724 light-duty plug-in electric vehicles were registered in France in 2017 consisting of 24,910 all-electric cars, 6,011 electric utility vans and 10,803 plug-in hybrid cars. The plug-in car segment achieved a market share of 1.98% of new car registrations in the country in 2017, with pure electric cars representing 1.47% and plug-in hybrids 0.51% of total registrations. The all-electric Renault Zoe for the fifth year running, continued as the top selling plug-in electric car with 15,245 units registered in 2017. The Renault Kangoo Z.E. ranked one more time as the top selling electric van with 2,546 units registered in 2017. The Mercedes Benz GLC, launched in the second semester of 2016, topped plug-in hybrid registrations with 2,112 units in 2017. \n\n, the Renault Zoe is the all-time best-selling plug-in electric vehicle in the French market with 48,582 units registered since 2012. Ranking second was the Kangoo Z.E. utility van with 15,032 units registered through September 2016. , the Nissan Leaf ranked third with 8,979 units, followed by the Bolloré Bluecar with 5,689 units. Most units of the Bluecar are in operation for the Autolib' car sharing service in Paris, and similar schemes in Lyon and Bordeaux.\n\nThe following table presents registrations of light-duty highway-capable all-electric vehicles by type (all electric cars and vans, and plug-in hybrids) with detailed all-electric car registrations by model between 2010 and December 2015.\n\n"}
{"id": "1424291", "url": "https://en.wikipedia.org/wiki?curid=1424291", "title": "Radioisotope heater unit", "text": "Radioisotope heater unit\n\nRadioisotope heater units (RHU) are small devices that provide heat through radioactive decay. They are similar to tiny radioisotope thermoelectric generators (RTG) and normally provide about one watt of heat each, derived from the decay of a few grams of plutonium-238—although other radioactive isotopes could be used. The heat produced by these RHUs is given off continuously for several decades and, theoretically, for up to a century or more.\n\nIn spacecraft, RHUs are necessary to heat critical components and subsystems. RHUs also reduce spacecraft complexity by making heater subsystems unnecessary. By having as few heating subsystems as possible, the overall complexity of the spacecraft can be reduced.\n\nWhile both RHUs and RTGs use the decay heat of a radioactive isotope (usually Pu-238), RHUs are generally much smaller as a result of omitting the thermocouples and heat sinks/radiators required to generate electricity from heat. Both RHUs and RTGs feature rugged, heat-resistant casings to safely contain the radioisotope in the event of a launch vehicle failure or an accidental collision with the earth from deep space. The total mass of a single RHU (including shielding) is about 40 grams. Similar schemes, such as thermo-ionic generators, have also been used.\n\n Most lunar and Martian surface probes use RHUs for heat, including many probes that use solar panels rather than RTGs to generate electricity. Examples include the seismometer deployed on the Moon by Apollo 11 in 1969, which contained 1.2 ounces (34 grams) of plutonium-238; Mars Pathfinder; and the Mars Exploration Rovers \"Spirit\" and \"Opportunity\". RHUs are especially useful on the Moon because of its lengthy and cold two-week night.\n\nVirtually every deep space mission beyond Mars uses both RHUs and RTGs. Solar insolation decreases with the square of the distance from the Sun, so additional heat is needed to keep spacecraft components at nominal operating temperature. Some of this heat is produced electrically because it is easier to control, but electrical heaters are far less efficient than a RHU because RTGs convert only a few percent of their heat to electricity and reject the rest to space.\n\nThe Cassini–Huygens spacecraft sent to Saturn contained eighty-two of these units (in addition to three main RTGs for power generation). The associated Huygens probe contained thirty-five.\n\nThe United States Department of Energy has developed the General Purpose Heat Source (GPHS) primarily for space use. These GPHSes can be used individually or in groups of up to eighteen for component heating and sources for RTGs. Each GPHS contains four iridium-clad Pu-238 fuel pellets, standing 5 cm tall, 10 cm square and weighs 1.44 kg.\n\n\n"}
{"id": "14283262", "url": "https://en.wikipedia.org/wiki?curid=14283262", "title": "Richland Electric Cooperative", "text": "Richland Electric Cooperative\n\nRichland Electric Cooperative is an electric distribution cooperative located in Richland Center, Wisconsin. Richland Electric Cooperative serves approximately 3500 members with electric service throughout Richland County, Wisconsin and parts of Sauk, Vernon, and Crawford counties as well.\n\nThe cooperative was formed on January 8, 1936 as the Richland Cooperative Electric Association as the result of the Rural Electrification Administration order issued by President Franklin D. Roosevelt in 1935. It was the first electricity cooperative in the state. The co-op first provided electricity was on May 7, 1937. The farmhouse had been wired for electricity when it was built in the 1917, but had no power grid with which to connect.\n"}
{"id": "21686373", "url": "https://en.wikipedia.org/wiki?curid=21686373", "title": "Ryburn Reservoir", "text": "Ryburn Reservoir\n\nRyburn Reservoir is a supply reservoir operated by Yorkshire Water close to Ripponden in the Yorkshire Pennines, England. It lies in the valley of the River Ryburn and is the lower of two reservoirs built in the valley to supply Wakefield with water and was completed in 1933. The upper reservoir is Baitings Reservoir.\n\nRyburn reservoir is the earlier of the two reservoirs. It lies just south of the A58 road and its concrete dam is situated in a deep part of the valley. Being surrounded by woods, it is a popular area for walkers just outside the settlements of Ripponden and Rishworth.\n"}
{"id": "25425327", "url": "https://en.wikipedia.org/wiki?curid=25425327", "title": "SHARON Wastewater Treatment", "text": "SHARON Wastewater Treatment\n\nSHARON (Single reactor system for High activity Ammonium Removal Over Nitrite) is a sewage treatment process.\n\nA partial nitrification process of sewage treatment used for the removal of ammonia and organic nitrogen components from wastewater flow streams. The process results in stable nitrite formation, rather than complete oxidation to nitrate. Nitrate formation by nitrite oxidising bacteria (NOB) (such as \"Nitrobacter\") is prevented by adjusting temperature, pH, and retention time to select for nitr\"i\"fying ammonia oxidising bacteria (AOB) (such as \"Nitrosomonas\").\n\nDenitrification of waste streams utilizing SHARON reactors can proceed with an annoxic reduction, such as anammox.\n\n"}
{"id": "20553413", "url": "https://en.wikipedia.org/wiki?curid=20553413", "title": "Silk waste", "text": "Silk waste\n\nSilk waste includes all kinds of raw silk which may be unwindable, and therefore unsuited to the throwing process. Before the introduction of machinery applicable to the spinning of silk waste, the refuse from cocoon reeling, and also from silk winding, which is now used in producing spun silk fabrics, was nearly all destroyed as being useless, with the exception of that which could be hand-combed and spun by means of the distaff and spinning wheel, a method which is still practised by some of the peasantry in India and other countries in Asia.\n\nThe supply of waste silk is drawn from the following sources:\n\n\nA silk \"throwster\" receives the silk in skein form, the thread of which consists of a number of silk fibres wound together to make a certain diameter or size, the separate fibre having actually been spun by the worm. The silk-waste spinner receives the silk in quite a different form: merely the raw material, packed in bales of various sizes and weights, the contents being a much-tangled mass of all lengths of fibre mixed with much foreign matter, such as ends of straws, twigs, leaves, worms and chrysalis. It is the spinner's business to straighten out these fibres, with the aid of machinery, and then to so join them that they become a thread, which is known as spun silk.\n\nAll silk produced by the worm is composed of two substances: fibroin, the true thread, and sericin, which is a hard, gummy coating of the fibroin. Before the silk can be manipulated by machinery to any advantage, the gum coating must be removed, really dissolved and washed away. Where the method used in achieving this operation is through fermentation, the product is called schappe. The former, schapping, is the French, Italian and Swiss method, from which the silk when finished is neither so bright nor so good in colour as the discharged silk; but it is very clean and level, and for some purposes essential, as, for instance, in velvet manufacture.\n\n\n"}
{"id": "27884677", "url": "https://en.wikipedia.org/wiki?curid=27884677", "title": "Sunrayce 99", "text": "Sunrayce 99\n\nSunrayce 99 was an intercollegiate solar car race sponsored by General Motors, EDS, and the US Department of Energy. The race began on June 20, 1999 in Washington, D.C. and finished on June 29, 1999 at Epcot Center in Orlando, Florida. The event was won by the University of Missouri at Rolla (now Missouri S&T). Sunrayce 99 was the fifth and final race in the Sunrayce series and was followed by the first American Solar Challenge in 2001. The overall success of the event was limited by difficult weather conditions. Teams battled clouds and rain throughout the race, and no team avoided having to trailer their car for at least part of the race route.\n\nTeams were allowed deviate from the traditional lead–acid battery technology for the first time in Sunrayce 99 when nickel–metal hydride batteries were approved for use. The new battery technology offered equivalent capacity to lead acid batteries but weighed several hundred pounds less. Nickel–metal hydride batteries were very expensive at the time, so only the most well funded teams could afford them. Unfortunately the rainy weather and slow speeds neutralized the advantage of the new battery technology.\n\n"}
{"id": "241029", "url": "https://en.wikipedia.org/wiki?curid=241029", "title": "Top quark", "text": "Top quark\n\nThe top quark, also known as the t quark (symbol: t) or truth quark, is the most massive of all observed elementary particles. Like all quarks, the top quark is a fermion with spin , and experiences all four fundamental interactions: gravitation, electromagnetism, weak interactions, and strong interactions. It has an electric charge of + \"e\". It has a mass of 172.44 ± 0.13 (stat) ± 0.47 (syst), which is about the same mass as an atom of tungsten. The antiparticle of the top quark is the top antiquark (symbol: , sometimes called \"antitop quark\" or simply \"antitop\"), which differs from it only in that some of its properties have equal magnitude but opposite sign.\n\nThe top quark interacts primarily by the strong interaction, but can only decay through the weak force. It decays to a W boson and either a bottom quark (most frequently), a strange quark, or, on the rarest of occasions, a down quark. The Standard Model predicts its mean lifetime to be roughly . This is about a twentieth of the timescale for strong interactions, and therefore it does not form hadrons, giving physicists a unique opportunity to study a \"bare\" quark (all other quarks hadronize, meaning that they combine with other quarks to form hadrons, and can only be observed as such). Because it is so massive, the properties of the top quark allow predictions to be made of the mass of the Higgs boson under certain extensions of the Standard Model (see Mass and coupling to the Higgs boson below). As such, it is extensively studied as a means to discriminate between competing theories.\n\nIts existence (and that of the bottom quark) was postulated in 1973 by Makoto Kobayashi and Toshihide Maskawa to explain the observed CP violations in kaon decay, and was discovered in 1995 by the CDF and DØ experiments at Fermilab. Kobayashi and Maskawa won the 2008 Nobel Prize in Physics for the prediction of the top and bottom quark, which together form the third generation of quarks.\n\nIn 1973, Makoto Kobayashi and Toshihide Maskawa predicted the existence of a third generation of quarks to explain observed CP violations in kaon decay. The names top and bottom were introduced by Haim Harari in 1975,\nto match the names of the first generation of quarks (up and down) reflecting the fact that the two were the 'up' and 'down' component of a weak isospin doublet. The top quark was sometimes called \"truth quark\" in the past, but over time \"top quark\" became the predominant use.\n\nThe proposal of Kobayashi and Maskawa heavily relied on the GIM mechanism put forward by Sheldon Lee Glashow, John Iliopoulos and Luciano Maiani, which predicted the existence of the then still unobserved charm quark. When in November 1974 teams at Brookhaven National Laboratory (BNL) and the Stanford Linear Accelerator Center (SLAC) simultaneously announced the discovery of the J/ψ meson, it was soon after identified as a bound state of the missing charm quark with its antiquark. This discovery allowed the GIM mechanism to become part of the Standard Model. With the acceptance of the GIM mechanism, Kobayashi and Maskawa's prediction also gained in credibility. Their case was further strengthened by the discovery of the tau by Martin Lewis Perl's team at SLAC between 1974 and 1978. This announced a third generation of leptons, breaking the new symmetry between leptons and quarks introduced by the GIM mechanism. Restoration of the symmetry implied the existence of a fifth and sixth quark.\n\nIt was in fact not long until a fifth quark, the bottom, was discovered by the E288 experiment team, led by Leon Lederman at Fermilab in 1977. This strongly suggested that there must also be a sixth quark, the top, to complete the pair. It was known that this quark would be heavier than the bottom, requiring more energy to create in particle collisions, but the general expectation was that the sixth quark would soon be found. However, it took another 18 years before the existence of the top was confirmed.\n\nEarly searches for the top quark at SLAC and DESY (in Hamburg) came up empty-handed. When, in the early eighties, the Super Proton Synchrotron (SPS) at CERN discovered the W boson and the Z boson, it was again felt that the discovery of the top was imminent. As the SPS gained competition from the Tevatron at Fermilab there was still no sign of the missing particle, and it was announced by the group at CERN that the top mass must be at least . After a race between CERN and Fermilab to discover the top, the accelerator at CERN reached its limits without creating a single top, pushing the lower bound on its mass up to .\n\nThe Tevatron was (until the start of LHC operation at CERN in 2009) the only hadron collider powerful enough to produce top quarks. In order to be able to confirm a future discovery, a second detector, the DØ detector, was added to the complex (in addition to the Collider Detector at Fermilab (CDF) already present). In October 1992, the two groups found their first hint of the top, with a single creation event that appeared to contain the top. In the following years, more evidence was collected and on April 22, 1994, the CDF group submitted their paper presenting tentative evidence for the existence of a top quark with a mass of about . In the meantime, DØ had found no more evidence than the suggestive event in 1992. A year later, on March 2, 1995, after having gathered more evidence and a reanalysis of the DØ data (who had been searching for a much lighter top), the two groups jointly reported the discovery of the top at a mass of .\n\nIn the years leading up to the top quark discovery, it was realized that certain precision measurements of the electroweak vector boson masses and couplings are very sensitive to the value of the top quark mass. These effects become much larger for higher values of the top mass and therefore could indirectly see the top quark even if it could not be directly detected in any experiment at the time. The largest effect from the top quark mass was on the T parameter and by 1994 the precision of these indirect measurements had led to a prediction of the top quark mass to be between and . It is the development of techniques that ultimately allowed such precision calculations that led to Gerardus 't Hooft and Martinus Veltman winning the Nobel Prize in physics in 1999.\n\n\nBecause top quarks are very massive, large amounts of energy are needed to create one. The only way to achieve such high energies is through high energy collisions. These occur naturally in the Earth's upper atmosphere as cosmic rays collide with particles in the air, or can be created in a particle accelerator. In 2011, after the Tevatron ceased operations, the Large Hadron Collider at CERN became the only accelerator that generates a beam of sufficient energy to produce top quarks, with a center-of-mass energy of 7 TeV. There are multiple processes that can lead to the production of top quarks, but they can be conceptually divided in two categories.\n\nThe most common is production of a top–antitop pair via strong interactions. In a collision, a highly energetic gluon is created, which subsequently decays into a top and antitop. This process was responsible for the majority of the top events at Tevatron and was the process observed when the top was first discovered in 1995. It is also possible to produce pairs of top–antitop through the decay of an intermediate photon or Z-boson. However, these processes are predicted to be much rarer and have a virtually identical experimental signature in a hadron collider like Tevatron.\n\nA distinctly different process is the production of single top quarks via weak interaction. This can happen in several ways (called channels): either an intermediate W-boson decays into a top and antibottom quark (\"s-channel\") or a bottom quark (probably created in a pair through the decay of a gluon) transforms to a top quark by exchanging a W-boson with an up or down quark (\"t-channel\"). A single top quark can also be produced in association with a W boson, requiring an initial state bottom quark (\"tW-channel\"). The first evidence for these processes was published by the DØ collaboration in December 2006, and in March 2009 the CDF and DØ collaborations released twin papers with the definitive observation of these processes. The main significance of measuring these production processes is that their frequency is directly proportional to the  component of the CKM matrix.\n\nBecause of its enormous mass, the top quark is extremely short-lived with a predicted lifetime of only . As a result, top quarks do not have time before they decay to form hadrons as other quarks do, which provides physicists with the unique opportunity to study the behavior of a \"bare\" quark. The only known way the top quark can decay is through the weak interaction producing a W-boson and a down-type quark (down, strange, or bottom).\n\nIn particular, it is possible to directly determine the branching ratio Γ(Wb) / Γ(W\"q\" (\"q\" = b,s,d)). The best current determination of this ratio is . Since this ratio is equal to according to the Standard Model, this gives another way of determining the CKM element , or in combination with the determination of from single top production provides tests for the assumption that the CKM matrix is unitary.\n\nThe Standard Model also allows more exotic decays, but only at one loop level, meaning that they are extremely suppressed. In particular, it is possible for a top quark to decay into another up-type quark (an up or a charm) by emitting a photon or a Z-boson. Searches for these exotic decay modes have provided no evidence for their existence in accordance with expectations from the Standard Model. The branching ratios for these decays have been determined to be less than 5.9 in 1,000 for photonic decay and less than 2.1 in 1,000 for Z-boson decay at 95% confidence.\n\nThe Standard Model describes fermion masses through the Higgs mechanism. The Higgs boson has a Yukawa coupling to the left- and right-handed top quarks. After electroweak symmetry breaking (when the Higgs acquires a vacuum expectation value), the left- and right-handed components mix, becoming a mass term.\n\nThe top quark Yukawa coupling has a value of\n\nwhere 246 GeV is the value of the Higgs vacuum expectation value.\n\nIn the Standard Model, all of the quark and lepton Yukawa couplings are small compared to the top quark Yukawa coupling. Understanding this hierarchy in the fermion masses is an open problem in theoretical physics. Yukawa couplings are not constants and their values change depending on the energy scale (distance scale) at which they are measured. The dynamics of Yukawa couplings are determined by the renormalization group equation.\n\nOne of the prevailing views in particle physics is that the size of the top quark Yukawa coupling is determined by the renormalization group, leading to the \"quasi-infrared fixed point.\"\n\nThe Yukawa couplings of the up, down, charm, strange and bottom quarks, are hypothesized to have small values at the extremely high energy scale of grand unification, 10 GeV. They increase in value at lower energy scales, at which the quark masses are generated by the Higgs. The slight growth is due to corrections from the QCD coupling. The corrections from the Yukawa couplings are negligible for the lower mass quarks.\n\nIf, however, a quark Yukawa coupling has a large value at very high energies, its Yukawa corrections will evolve and cancel against the QCD corrections. This is known as a (quasi-) infrared fixed point. No matter what the initial starting value of the coupling is, if it is sufficiently large it will reach this fixed point value. The corresponding quark mass is then predicted.\n\nThe top quark Yukawa coupling lies very near the infrared fixed point of the Standard Model. The renormalization group equation is:\n\nwhere is the color gauge coupling, is the weak isospin gauge coupling, and is the weak hypercharge gauge coupling. This equation describes how the Yukawa coupling changes with energy scale . Solutions to this equation for large initial values cause the right-hand side of the equation to quickly approach zero, locking to the QCD coupling . The value of the fixed point is fairly precisely determined in the Standard Model, leading to a top quark mass of 230 GeV. However, if there is more than one Higgs doublet, the mass value will be reduced by Higgs mixing angle effects in an unpredicted way.\n\nIn the minimal supersymmetric extension of the Standard Model (MSSM), there are two Higgs doublets and the renormalization group equation for the top quark Yukawa coupling is slightly modified:\n\nwhere \"y\" is the bottom quark Yukawa coupling. This leads to a fixed point where the top mass is smaller, 170–200 GeV. The uncertainty in this prediction arises because the bottom quark Yukawa coupling can be amplified in the MSSM. Some theorists believe this is supporting evidence for the MSSM.\n\nThe quasi-infrared fixed point has subsequently formed the basis of top quark condensation theories of electroweak symmetry breaking in which the Higgs boson is composite at \"extremely\" short distance scales, composed of a pair of top and antitop quarks.\n\n\n\n"}
{"id": "5637517", "url": "https://en.wikipedia.org/wiki?curid=5637517", "title": "Toxicant", "text": "Toxicant\n\nA toxicant (pronounced TOK-sih-kunt) is any toxic substance. Toxicants can be poisonous. The term covers substances that may be man-made, biologically produced, or naturally occurring. There are different types of toxicants and they can be found in the air, soil, water, or food.\n\nToxicants can be found in the air, soil, water, or food. Humans can be exposed to environmental toxicants. Fish can contain environmental toxicants. Cigarette smoke contains toxicants. E-cigarette aerosol also contains toxicants. Most heavy metals are toxicants. Diesel exhaust contains toxicants. Pesticides, benzene, and asbestos-like fibers such as carbon nanotubes are toxicants. Possible developmental toxicants include phthalates, phenols, sunscreens, pesticides, halogenated flame retardants, perfluoroalkyl coatings, nanoparticles, e-cigarettes, and dietary polyphenols.\n\nIn contrast, a toxin is a poison produced naturally by an organism (e.g. plant, animal, insect). The 2011 book \"A Textbook of Modern Toxicology\" states, \"A toxin is a toxicant that is produced by a living organism and is not used as a synonym for toxicant—all toxins are toxicants, but not all toxicants are toxins. Toxins, whether produced by animals, plants, insects, or microbes are generally metabolic products that have evolved as defense mechanisms for the purpose of repelling or killing predators or pathogens.\"\n\nBiocides are oxidized or non-oxidized toxicants. Chlorine is the most commonly manufactured oxidized toxicant. Chlorine is ubiquitously added to drinking water to disinfect it. Non-oxidized toxicants include isothiazolinones and quaternary ammonium compounds.\n\nAn intoxicant is a substance that intoxicates such as an alcoholic drink. An intoxicant is a substance that impairs the mind and causes a person to be in a state varying from exhilaration to lethargy.\n"}
{"id": "2505643", "url": "https://en.wikipedia.org/wiki?curid=2505643", "title": "Triiodide", "text": "Triiodide\n\nIn chemistry, triiodide usually refers to the triiodide ion, . This anion, one of the polyhalogen ions, is composed of three iodine atoms. It is formed by combining aqueous solutions of iodide salts and iodine. Some salts of the anion have been isolated, including thallium(I) triiodide (Tl[I]) and ammonium triiodide ([NH][I]). Triiodide is observed to be a red colour in solution\n\nOther chemical compounds with \"triiodide\" in their name may contain three iodide centers that are not bonded to each other as the triiodide ion, but exist instead as separate iodine atoms or iodide ions. Examples include nitrogen triiodide (NI) and phosphorus triiodide (PI), where individual iodines are covalently bonded to a central atom. As some cations have the theoretical possibility to form compounds with both triiodide and iodide ions, such as ammonium, compounds containing iodide anions in a 3:1 stoichiometric ratio should only be referred to as triiodides in cases where the triiodide anion is present. It may also be helpful to indicate the oxidation number of a metal cation, where appropriate. For example, the covalent molecule gallium triiodide (GaI) is better referred to as gallium(III) iodide to emphasise that it is iodide anions that are present, and not triiodide.\n\nThe ion is linear and symmetrical. According to VSEPR theory, the central iodine atom has three equatorial lone pairs, and the terminal iodines are bonded axially in a linear fashion, due to the three lone pairs bonding to the central iodine-atom. In the molecular orbital model, a common explanation for the hypervalent bonding on the central iodine involves a three-center four-electron bond. The I−I bond is longer than in diatomic iodine, I.\n\nIn ionic compounds, the bond lengths and angles of triiodide vary depending on the nature of the cation. The triiodide anion is easily polarised and in many salts, one I−I bond becomes shorter than the other. Only in combination with large cations, e.g. a quaternary ammonium such as [N(CH)], may the triiodide remain roughly symmetrical. The dimensions of the triiodide [I−I−I] bonds in a few sample compounds are shown below:\nThe triiodide ion is the simplest polyiodide; several higher polyiodides exist. In solution, it appears yellow in low concentrations, and brown at higher concentrations. The triiodide ion is responsible for the well-known blue-black color which arises when iodine solutions interact with starch. Iodide does not react with starch; nor do solutions of iodine in nonpolar solvents. \n\nLugol's iodine contains potassium iodide and a stoichiometric amount of elemental iodine, so that significant amounts of triiodide ion exist in this solution.\n\nTincture of iodine, although nominally a solution of elemental iodine in ethanol, also contains significant amounts of triiodide, due to its content of both iodide and water.\n\nThe following exergonic equilibrium gives rise to the triiodide ion:\nIn this reaction, iodide is viewed as a Lewis base, and the iodine is a Lewis acid. The process is analogous to the reaction of S with sodium sulfide, except that the higher polyiodides have branched structures.\n\n"}
{"id": "27385764", "url": "https://en.wikipedia.org/wiki?curid=27385764", "title": "Vandal (tanker)", "text": "Vandal (tanker)\n\nVandal was a river tanker designed by Karl Hagelin and Johny Johnson for Branobel. Russian \"Vandal\" and French \"Petite-Pierre\", launched in 1903, were the world's first diesel-powered ships (sources disagree over which of the two, \"Vandal\" or \"Petite-Pierre\", was \"the\" first). \"Vandal\" was the first equipped with fully functional diesel-electric transmission.\n\nIn the 1890s oil industry searched for an economical oil-burning engine, and the solution was found by German engineer Rudolph Diesel. Diesel marketed his technology to oil barons around the world; in February 1898 he granted exclusive licenses to build his engines in Sweden and Russia to Emanuel Nobel of the Nobel family. The Russian licence cost Nobel 800,000 marks in cash and stock of the newly founded Russian Diesel Company. The Saint Petersburg engine plant was a quick success; it started with diesel-powered industrial pumps for oil pipelines and soon grabbed the mass market for flour mill engines. It produced more diesel engines than any other concern in the world.\n\nIn 1902 Karl Hagelin, \"a veteran of the Volga and sometime visionary\", suggesting mating diesel engines to river barges. He envisioned direct shipment of oil through a 1,800-mile route from the lower Volga to Saint Petersburg and Finland. The canals of the Volga–Baltic Waterway dictated use of relatively small barges, making use of steam engines uneconomical. Diesel engine seemed a natural choice. Hagelin believed that reversing the engine and regulating its speed could be done with an electrical transmission, and contracted Swedish ASEA to test the electrical drive system. Hagelin then recruited naval architect Johny Johnson of Gothenburg to design the ship. Johnson placed the diesel engine and electric generator in the middle, and the electric motors in the stern, driving the propellers directly. The holds were separated by longitudinal (rather than transverse) bulkheads running the length of the ship, a feature that became common on ocean-going tankers.\n\nThe ship's powerplant (3×120 horsepowers) was built by Sweden by Swedish Diesel (Aktiebolaget Diesels Motorer) and ASEA. Each engine had three cylinders with a bore of 290 mm and stroke of 430 mm. They ran at a constant 240 rpm, and the electrical transmission, controlled by a tram-like lever, varied propeller speed from 30 to 300 rpm. The hull was built at Sormovo shipyard in Nizhny Novgorod and towed to Saint Petersburg for the final assembly. Its size (244.5 × 31¾ × 8 feet) was taylored to the canals of the North rather than the Volga. Named \"Vandal\", it commenced commercial operation in the spring of 1903. \"Vandal\" was accidentally damaged on its maiden voyage, repaired and served on the Volga route for ten years.\n\nThe larger \"Sarmat\", with four 180 h.p. engines, was launched next summer. Unlike \"Vandal\", \"Sarmat\"'s engines could be coupled to the propellers directly, bypassing the electrical drive and saving up to 15% of engine power that would be otherwise lost in the electric transmission. \"Sarmat\" operated until 1923; the hulk was moored in Nizhny Novgorod until the 1970s.\n\nThe new ships attracted public and professional interest and brought in new orders. Plant payroll expanded to more than a thousand men, but growth brought in management problems. Rolf Nobel, Ludwig Nobel Jr. and Hagelin split with Emanuel over the future of diesel-powered shipping. Hagelin's proposal to convert existing steam-powered fleet to diesel engines was rejected by Emanuel. Hagelin quit, and accepted the post of Swedish consul general in Saint Petersburg. In 1907 Hagelin and Johnson designed a 4,500-ton tanker, and again Emanuel Nobel rejected the proposal. The inventors sold their blueprints to Merkulyev Brothers of Kolomna who built the world's first true seagoing diesel-powered tanker, \"Mysl\", in 1908. This, at last, compelled Emanuel to grant Hagelin sweeping rights to modernize the company fleet that reached 315 vessels in 1915.\n\n"}
{"id": "247768", "url": "https://en.wikipedia.org/wiki?curid=247768", "title": "Ventifact", "text": "Ventifact\n\nA ventifact (also wind-faceted stone, windkanter) is a rock that has been abraded, pitted, etched, grooved, or polished by wind-driven sand or ice crystals. These geomorphic features are most typically found in arid environments where there is little vegetation to interfere with aeolian particle transport, where there are frequently strong winds, and where there is a steady but not overwhelming supply of sand.\n\nVentifacts can be abraded to eye-catching natural sculptures such as the main features of the White Desert near Farafra oasis in Egypt. In moderately tall, isolated rock outcrops, mushroom shaped pillars of rock may form as the outcrop is eroded by saltating sand grains. This occurs because, even in strong winds, sand grains can't be continuously held in the air. Instead, the particles bounce along the ground, rarely reaching higher than a few feet above the earth. Over time, the bouncing sand grains can erode the lower portions of a ventifact, while leaving a larger less eroded cap. The results can be fantastic stone mushrooms.\n\nIndividual stones, such as those forming desert pavement, are often found with grooved, etched, or polished surfaces where these same wind-driven processes have slowly worn away the rock.\n\nStones with a small number of wind facets have names reflecting this number, e.g., dreikanter is a pyramidal-shaped stone with 3 wind facets (and one buried, rounded side).\n\nWhen ancient ventifacts are preserved without being moved or disturbed, they may serve as a paleo-wind indicators. The wind direction at the time the ventifact formed will be parallel to grooves or striations cut in the rock. \n\nVentifacts have also been discovered on Mars, where such sharp immobile rocks have caused significant damage to the wheels of the \"Curiosity\" rover. An example of a Martian ventifact was named Jake Matijevic. By analyzing its shape, it was possible to reconstruct the main wind direction which sculpted the rock.\n\n"}
{"id": "19368992", "url": "https://en.wikipedia.org/wiki?curid=19368992", "title": "Victor Gustave Robin", "text": "Victor Gustave Robin\n\nVictor Gustave Robin (; 17 May 1855 – 1897) was a French mathematical analyst and applied mathematician\nwho lectured in mathematical physics at the Sorbonne in Paris and also worked in the area of thermodynamics. He is known especially for the Robin boundary condition. The French Academy of Sciences awarded him the \"Prix Francœur\" for 1893 and again for 1897 and the \"Prix Poncelet\" for 1895.\n"}
{"id": "1622030", "url": "https://en.wikipedia.org/wiki?curid=1622030", "title": "Walker circulation", "text": "Walker circulation\n\nThe Walker circulation, also known as the Walker cell, is a conceptual model of the air flow in the tropics in the lower atmosphere (troposphere). According to this model, parcels of air follow a closed circulation in the zonal and vertical directions. This circulation, which is roughly consistent with observations, is caused by differences in heat distribution between ocean and land. It was discovered by Gilbert Walker. In addition to motions in the zonal and vertical direction the tropical atmosphere also has considerable motion in the meridional direction as part of, for example, the Hadley Circulation.\n\nThe term \"Walker circulation\" was coined in 1969 by the Norwegian-American meteorologist Jacob Bjerknes.\n\nGilbert Walker was an established applied mathematician at the University of Cambridge when he became director-general of observatories in India in 1904. While there, he studied the characteristics of the Indian Ocean monsoon, the failure of whose rains had brought severe famine to the country in 1899. Analyzing vast amounts of weather data from India and the rest of the world, over the next fifteen years he published the first descriptions of the great seesaw oscillation of atmospheric pressure between the Indian and Pacific Ocean, and its correlation to temperature and rainfall patterns across much of the Earth's tropical regions, including India. He also worked with the Indian Meteorological Department especially in linking the monsoon with Southern Oscillation phenomenon. He was made a Companion of the Order of the Star of India in 1911.\n\nWalker determined that the time scale of a year (used by many studying the atmosphere) was unsuitable because geospatial relationships could be entirely different depending on the season. Thus, Walker broke his temporal analysis into December–February, March–May, June–August, and September–November.\n\nWalker then selected a number of \"centers of action\", which included areas such as the Indian Peninsula. The centers were in the hearts of regions with either permanent or seasonal high and low pressures. He also added points for regions where rainfall, wind or temperature was an important control.\n\nHe examined the relationships of the summer and winter values of pressure and rainfall, first focusing on summer and winter values, and later extending his work to the spring and autumn.\n\nHe concludes that variations in temperature are generally governed by variations in pressure and rainfall. It had previously been suggested that sunspots could be the cause of the temperature variations, but Walker argued against this conclusion by showing monthly correlations of sunspots with temperature, winds, cloud cover, and rain that were inconsistent.\n\nWalker made it a point to publish all of his correlation findings, both of relationships found to be important as well as relationships that were found to be unimportant. He did this for the purpose of dissuading researchers from focusing on correlations that did not exist.\n\nThe Walker Circulations of the tropical Indian, Pacific, and Atlantic basins result in westerly surface winds in Northern Summer in the first basin and easterly winds in the second and third basins. As a result, the temperature structure of the three oceans display dramatic asymmetries. The equatorial Pacific and Atlantic both have cool surface temperatures in Northern Summer in the east, while cooler surface temperatures prevail only in the western Indian Ocean. These changes in surface temperature reflect changes in the depth of the thermocline.\n\nChanges in the Walker Circulation with time occur in conjunction with changes in surface temperature. Some of these changes are forced externally, such as the seasonal shift of the Sun into the Northern Hemisphere in summer. Other changes appear to be the result of coupled ocean-atmosphere feedback in which, for example, easterly winds cause the sea surface temperature to fall in the east, enhancing the zonal heat contrast and hence intensifying easterly winds across the basin. These enhanced easterlies induce more equatorial upwelling and raise the thermocline in the east, amplifying the initial cooling by the southerlies. This coupled ocean-atmosphere feedback was originally proposed by Bjerknes. From an oceanographic point of view, the equatorial cold tongue is caused by easterly winds. Were the earth climate symmetric about the equator, cross-equatorial wind would vanish, and the cold tongue would be much weaker and have a very different zonal structure than is observed today.\nThe Walker cell is indirectly related to upwelling off the coasts of Peru and Ecuador. This brings nutrient-rich cold water to the surface, increasing fishing stocks.\n\nThe Walker circulation is caused by the pressure gradient force that results from a high pressure system over the eastern Pacific Ocean, and a low pressure system over Indonesia. When the Walker circulation weakens or reverses, an El Niño results, causing the ocean surface to be warmer than average, as upwelling of cold water occurs less or not at all. An especially strong Walker circulation causes a La Niña, resulting in cooler ocean temperatures due to increased upwelling.\n\nA scientific study published in May 2006 in the journal \"Nature\" indicates that the Walker circulation has been slowing since the mid-19th Century. The authors argue that global warming is a likely causative factor in the weakening of the wind pattern. However, a new study from The Twentieth Century Reanalysis Project shows that the Walker circulation has not been slowing (or increasing) from 1871–2008.\n\n\n"}
