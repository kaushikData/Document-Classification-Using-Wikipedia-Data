{"id": "8892305", "url": "https://en.wikipedia.org/wiki?curid=8892305", "title": "2002 Eastern Mediterranean event", "text": "2002 Eastern Mediterranean event\n\nThe 2002 Eastern Mediterranean Event was a high-energy upper atmosphere explosion over the Mediterranean Sea, around 34°N 21°E (between Libya and Crete) on June 6, 2002. This explosion, similar in power to a small atomic bomb, has been related to a small asteroid undetected while approaching Earth. The object disintegrated in an air burst impact and no meteorite fragments were recovered. The air burst occurred over the sea.\n\nThe event occurred during the 2001–2002 India–Pakistan standoff, and there were concerns by General Simon Worden of the U.S. Air Force that if the upper atmosphere explosion had occurred closer to Pakistan or India, it could have sparked a nuclear war between the two countries.\n\n"}
{"id": "31464039", "url": "https://en.wikipedia.org/wiki?curid=31464039", "title": "Albrecht effect", "text": "Albrecht effect\n\nThe Albrecht effect describes how cloud condensation nuclei (CCN), possibly from anthropogenic pollution, may increase cloud lifetime and hence increase the amount of solar radiation reflected from clouds. Because it does not directly interact with incoming or outgoing radiation, it has an indirect effect on climate.\n\nAerosol particles act as CCNs creating more droplets of a smaller size. These take more time to coalesce to raindrop size (>100μm), reducing precipitation efficiency and hence increasing the lifetime of the cloud. The increased scattering of incoming radiation leads to a cooling of -0.3 to -1.4 Wm. This effect is not as well understood as the Twomey effect.\n\nThere are many other effects, indirect and semi-direct aerosol effects resulting in a large uncertainty in the radiative forcing due to aerosols.\n\n\n"}
{"id": "44281286", "url": "https://en.wikipedia.org/wiki?curid=44281286", "title": "Amber Grid", "text": "Amber Grid\n\nAB Amber Grid is the main natural gas transmission operator in Lithuania. It was established in 2013 by spin-off of gas transmission operations from the gas company Lietuvos Dujos.\n\nAmber Grid owns all main pipelines in Lithuania, including natural gas commission stations and natural gas transmission stations.\n\n"}
{"id": "26281722", "url": "https://en.wikipedia.org/wiki?curid=26281722", "title": "Around the world sailing record", "text": "Around the world sailing record\n\nThe first around the world sailing record for circumnavigation of the world was Juan Sebastián Elcano and the remaining members of Ferdinand Magellan's crew who completed their journey in 1522. The first solo record was set by Joshua Slocum in the \"Spray\" (1898).\n\nMost races or solo attempts start from Europe. Due to the configuration of the continents, sailing around the world consists of sailing on the Southern Ocean around the Antarctica continent, passing south of Cape Horn, Cape of Good Hope and Cape Leeuwin. Since 1918 the Panama Canal is an option but the locks must be entered and exited using engine power. Large stretches of the canal can be crossed under sail power.\n\nSailing around the world can be done by two directions: eastward or westward. The dominant winds and currents (outside tropical areas) make the voyage eastwards on the Southern hemisphere faster, most skippers and yachts who race prefer this route. Today, the multihulls perform much better than monohulls and hold the best times. Leisure yacht skippers who prefer tropical seas more often go westward, using the trade winds (and the Panama canal).\n\nThe most famous races around the world are:\n\nFormer races including:\n\nThe Jules Verne Trophy is awarded to the skipper who breaks the outright record, starting from an imaginary line between the Créac'h lighthouse on Ouessant (Ushant) Island, France, and the Lizard Lighthouse, UK.\n\nThe records are homologated by the World Sailing Speed Record Council (WSSRC).\n\nAccording to the WSSRC, for around the world sailing records, there is a rule saying that the length must be at least 21,600 nmi calculated along the shortest possible track from the starting port and back that does not cross land and does not go below 63°S. The great-circle distance formulas are to be used, assuming that the great circle length is 21,600 nmi. It is allowed to have one single waypoint to lengthen the calculated track. The equator must be crossed. In reality, this means that the boat should pass a waypoint at or not far from the antipode of the starting port of the journey (the exact position depends on how short the shortest possible track is). For example, the Vendée Globe starts at 46°N 2°W, has a waypoint at 57°S 180°E, and barely makes the distance requirement. The participants don't have to go to the antipode at 46°S 178°E since the rounding of Africa gives extra distance.\n\nThis route is the more demanding one, as it faces the dominant winds and currents. There are fewer attempts and records.\n\nAs of February 2010, no record has been homologated.\n\nIn May 2006, Dee Caffari became the first woman to sail around the world alone non-stop and single-handed westward on the Monohull Aviva, in 178 days.\n\nThe rules for intermediate records are set by the WSSRC.\n\nFrom the Atlantic Ocean: Equator => Cape Agulhas (South Africa) => Around Antarctica => Cape Horn => Equator\n\nfrom Cape Agulhas, South Africa (longitude 20°E) to Tasmania south point, (longitude : 146°49'E)\n\nTasmania south point, (longitude : 146°49'E) to Cape Horn (longitude 67°16'W)\n\nFrom Cape Horn (longitude 67°16'W) to Cape Agulhas, South Africa (longitude 20°E)\n\nFrom the cape Horn, cutting the longitude 67°16'W, up to the Equator\n\n"}
{"id": "1955450", "url": "https://en.wikipedia.org/wiki?curid=1955450", "title": "Babaco", "text": "Babaco\n\nThe babaco (\"Vasconcellea × heilbornii\"; syn. \"Carica pentagona\"), is a hybrid cultivar in the genus \"Vasconcellea\" from Ecuador. It is a hybrid between \"Vasconcellea cundinamarcensis\" (syn. \"Carica pubescens\", \"Mountain Papaya\".), and \"Vasconcellea stipulata\" (syn. \"Carica stipulata\", \"Toronche\".).\n\nIt can grow at high altitudes (over 2,000 m), and is the most cold-tolerant plant in the genus \"Vasconcellea\". The babaco is classified as a herbaceous shrub like \"Carica papaya\" (pawpaw or papaya) but unlike papaya it produces only female flowers. The babaco plant can produce from 30–60 fruits annually, and has an average life span of about eight years. The small plant is better suited as a container specimen than its cousin the papaya, which needs constant moisture and high temperatures to survive.\n\nThe babaco is suitable for greenhouse cultivation, where it produces high-quality fruit with soft edible skin. The plant is easy to cultivate under greenhouse conditions (Kempler et al., 1993). Propagation of the plant is achieved by rooting axillary shoots. Stem sections root poorly. The opening of axillary buds and subsequent shoot growth is stimulated by removing the apical meristem. The shoots are cut at 15–25 cm long and only the apical leaf is kept. The basal area of the stem is mildly injured and dipped in 0.4% IBA rooting powder and planted into suitable rooting media such as peat:perlite:sand and placed under intermittent mist. The cutting can be ready for transplanting within 3 weeks.\n\nIn contrast to the tropical papaya, babaco require a cool subtropical climate. Recommended greenhouse minimum temperature of 10 °C at night, 12 °C during the day, and 18 °C for fruit to ripen quickly and uniformly. Excessively low temperatures cause fruit to mature with rough, pitted skin.\n\nThe flowering habit is indeterminate. Under greenhouse conditions, plant growth during winter month (October- March at 49°N) is slow, and flowers senesce and fail to set fruit (Kempler et al., 1993). In Ecuador, the trees begin cropping 10 months after planting and continue bearing for 6 months, producing about 40 fruit/tree with each weighing about 1-1.5 kg, in the greenhouse we produced about 32 kg fruit per square meter in 16 months when planted 0.8 plants per sq. meter, and 25 kg sq. meter after 12 months space at 3 plants sq. meter. Because of the heavy fruit load a support system for the plants was required. Small fruit size can be achieved by growing plants at high densities. Fruit begins to mature with gradual color change from green to yellow and, if harvest it delayed, the fruit stalk will abscise and fruit will drop and bruise. The delicate-skinned fruit is harvested with pruners with a short stalk and handled carefully. In northern climate, ripening begins in November (March planting), and continues until June. During this period new flowers are set (April–September) and the second crop starts to ripen in November, the plant is cut above the last-formed fruit as flowers senesce in October. After the end of the second season, cropping plants can be cut and rejuvenated from the base or replaced with new plants.\n\nIt is a small, unbranched or sparsely branched tree reaching 5–8 m tall. The fruit differs from the related papaya (\"C. papaya\") in being narrower, typically less than 10 cm in diameter. The babaco fruit is seedless and the smooth skin can be eaten, and is said to have tastes of strawberry, papaya, kiwi and pineapple. The fruit is pentagonal in shape, therefore giving it the scientific name of \"Carica pentagona\". The fruit is not especially acidic, but contains papain, a proteolytic enzyme, which may cause mild irritation or \"burns\".\n\nLike the papaya, the babaco is grown for its edible fruit and for its fruit juice. Cultivation away from its native range has been successful as far south as New Zealand, and as far north as California, some regions of England, Guernsey, Channel Islands, and somewhat also in Italy (mostly Sicily and Calabria).\n\n"}
{"id": "35675607", "url": "https://en.wikipedia.org/wiki?curid=35675607", "title": "Blau Monuments", "text": "Blau Monuments\n\nThe Blau Monuments are a pair of inscribed stone objects from Mesopotamia now in the British Museum. They are commonly thought to be a form of ancient kudurru.\n\nThe Monuments were purchased by A. Blau in 1886 near the city of Uruk (modern-day Iraq). They were carved of bright blue stone commonly regarded as a dark shale or kind of schist. Thought to be forgeries for some time, excavations at Uruk redeemed them through revealing stylistic parallels in a basalt stele and the famous Warka Vase. Some assyriologists accepted the Monuments’ authenticity as early as 1901, although this was not a universal belief. Even though widely accepted today, some authors nonetheless maintain scrutiny on the Monuments’ status within Mesopotamian history.\n\nThe Blau Monuments are dated within the Uruk III/Jemdet Nasr to Early Dynastic I period. Some authors date the pair as early as 3100 BC on the basis of the proto-cuneiform script, while others date them to the ED I period around 2700 BC because of other stylistically similar land sale records with a definite date in the ED I period. Both dates are equally represented in scholarly works; the majority of works citing the earliest date usually reference Gelb in their turn.\n\nThe iconography on the Monuments is a matter of some dispute, upon which even recent scholars cannot wholly agree. Throughout their history, it has been consistently believed that the Monuments represented some form of transaction, usually in the form of a gift from a temple to its craftsmen. More recently, the imagery has been taken to represent a ceremonial feast given by a new land-owner after a land sale.\n\nThe obelisk shows male forms in two separate registers. The lower figure is a nude male who kneels with a pestle and mortar, which could suggest equally craftsmen or food preparation. He has parallels on the Warka Vase, where likewise nude males carry baskets of food for religious purposes. In the upper register, a standing man with a beard, bordered net-skirt, and headband holds a four-legged animal in his hands (usually identified as either a sheep or a goat). This figure is a very common way throughout Mesopotamian art of depicting a male with power.\n\nThe plaque’s obverse shows a similarly hierarchical scene. The leftmost figure is another bearded, skirted man. He holds a long object with both hands. This object has been interpreted as a pestle or a land-sale cone; when compared to the workers elsewhere on the Monuments who are working with pestles, the former identification seems unlikely. Most texts identify the right figure as a woman on the basis of Gelb’s identification, but if so, then ‘she’ lacks the characteristic dress present on other women from the same period, such as on the Ushumgal Stele. The reverse shows four men. Two are nude workers, kneeling with pestles and mortars. Between them stands a beardless, bald man wearing a net skirt. He holds his hands up like the smaller figure on the obverse. The remaining figure sits to the right, and appears nearly identical to the workers except slightly larger. This evidently depicts the process of food preparation for the feast.\n\nLike the Ushumgal Stele and other works from a similar time period, the text on the Blau Monuments is not fully comprehensible. Certain signs are readily identified, while others have no known identification. The obelisk’s inscription clearly refers to ‘5 bur’ of land, as well as a temple household and the profession “engar”. This title refers to a high official in an agricultural field, which would suggest the presence of an individual able to undertake such a transaction. The plaque’s text lists commodities of many sorts, as well as several other names and additional undeciphered information. The assumption, then, is that these items were exchanged for the land on the obelisk.\n\nAs a result of the yet-undeciphered text and certain oddities in the iconography, any attempt at definitively declaring a purpose for the Monuments is naturally tenuous. The Monuments are two of only five “ancient kudurrus” listed by Gelb (a classification currently in dispute). Consequently, they resist classification, and such a task is certainly not aided by the lack of definite information on their use.\n"}
{"id": "2615571", "url": "https://en.wikipedia.org/wiki?curid=2615571", "title": "Business action on climate change", "text": "Business action on climate change\n\nBusiness action on climate change includes a range of activities relating to global warming, and to influencing political decisions on global-warming-related regulation, such as the Kyoto Protocol. Major multinationals have played and to some extent continue to play a significant role in the politics of global warming, especially in the United States, through lobbying of government and funding of global warming skeptics. Business also plays a key role in the mitigation of global warming, through decisions to invest in researching and implementing new energy technologies and energy efficiency measures. (See also individual and political action on climate change.)\n\nIn 1989 in the US, the petroleum and automotive industries and the National Association of Manufacturers created the Global Climate Coalition (GCC) to oppose mandatory actions to address global warming. In 1997, when the US Senate overwhelmingly passed a resolution against ratifying the Kyoto Protocol, the industry funded a $13 million industry advertising blitz in the run-up to the vote.\n\nIn 1998 \"The New York Times\" published an American Petroleum Institute (API) memo outlining a strategy aiming to make \"recognition of uncertainty ... part of the 'conventional wisdom.'\" The memo has been compared to a late 1960s memo by tobacco company Brown and Williamson, which observed: \"Doubt is our product since it is the best means of competing with the 'body of fact' that exists in the mind of the general public. It is also the means of establishing a controversy.\" Those involved in the memo included Jeffrey Salmon, then executive director of the George C. Marshall Institute, Steven Milloy, a prominent skeptic commentator, and the Competitive Enterprise Institute's Myron Ebell. In June 2005 a former API lawyer, Philip Cooney, resigned his White House post after accusations of politically motivated tampering with scientific reports.\n\nIn 2002 the GCC considered its work in the US against regulation on global warming to have been so successful that it \"deactivated\" itself, although the loss of some leading members may also have been a factor.\n\nAt the same time, since 1989 many previously skeptical petroleum and automobile industry corporations have changed their position as the political and scientific consensus has grown, with the creation of the Kyoto Protocol and the publication of the International Panel on Climate Change's Second and Third Assessment Reports. These corporations include major petroleum companies like Royal Dutch Shell, Texaco, and BP, as well as automobile manufacturers like Ford, General Motors, and DaimlerChrysler. Some of these have joined with the Center for Climate and Energy Solutions (formerly the Pew Center on Global Climate Change), a non-profit organization aiming to support efforts to address global climate change.\n\nSince 2000, the Carbon Disclosure Project has been working with major corporations and investors to disclose the emissions of the largest companies. By 2007, the CDP published the emissions data for 2400 of the largest corporations in the world, and represented major institutional investors with $41 trillion combined assets under management. The pressure from these investors had had some success in working with companies to reduce emissions.\n\nThe World Business Council for Sustainable Development, a CEO-led association of some 200 multinational companies, has called on governments to agree on a global targets, and suggests that it is necessary to cut emissions by 60-80 percent from current levels by 2050.\n\nIn 2017, after the election of Donald Trump, backing was shown in the business community for the Paris Agreement, which became effective November 4, 2016.\n\nA central organization in climate skepticism was the Global Climate Coalition (1989–2002), a group of mainly United States businesses opposing immediate action to reduce greenhouse gas emissions. The coalition funded skeptical scientists to be public spokespeople, provided industry a voice on climate change, and fought the Kyoto Protocol. \"The New York Times\" reported that \"even as the coalition worked to sway opinion [towards skepticism], its own scientific and technical experts were advising that the science backing the role of greenhouse gases in global warming could not be refuted.\"\n\nIn the year 2000, the rate of corporate members leaving accelerated when they became the target of a national divestiture campaign run by John Passacantando and Phil Radford with the organization Ozone Action. According to \"The New York Times\", when Ford Motor Company was the first company to leave the coalition, it was “the latest sign of divisions within heavy industry over how to respond to global warming.” After that, between December 1999 and early March 2000, the GCC was deserted by Daimler-Chrysler, Texaco, the Southern Company and General Motors.\n\nThe organization closed in 2002, or in their own words, 'deactivated'.\n\nThe U.S. Climate Action Partnership (USCAP) was formed in January 2007 with the primary goal of influencing the US government's regulation of greenhouse gas emissions. Original members included General Electric, Alcoa, Natural Resources Defense Council, etc., but they were joined in April, 2007 by ConocoPhilips and AIG.\n\nExxonMobil has been a leading figure in the business world's position on climate change, providing substantial funding to a range of global-warming-skeptical organizations. \"Mother Jones\" counted some 40 ExxonMobil-funded organization that \"either have sought to undermine mainstream scientific findings on global climate change or have maintained affiliations with a small group of \"skeptic\" scientists who continue to do so.\" Between 2000 and 2003 these organizations received more than $8m in funding.\n\nIt has also had a key influence in the Bush administration's energy policy, including on the Kyoto Protocol, supported by both $55m spent on lobbying since 1999, and direct contacts between the company and leading politicians. It was a leading member of the Global Climate Coalition. It encouraged (and may have been instrumental in) the replacement in 2002 of the head of the IPCC, Robert Watson. It has also invested $100m into the Global Climate and Energy Project, with Stanford University, and other programs at institutions such as the Massachusetts Institute of Technology, Carnegie Mellon University and the International Energy Agency Greenhouse Gas Research and Development Program.\n\nSome of Exxon's activities on climate change produced strong criticism from environmental groups, including reactions such as a leaflet produced by the Stop Esso campaign, saying 'Don't buy E$$o', and featuring a tiger hand setting fire to the Earth. The company's carbon dioxide emissions are more than 50% higher than those of British rival BP, despite the US firm's oil and gas production being only slightly larger.\n\nAccording to a 2004 study commissioned by Friends of the Earth, ExxonMobil and its predecessors caused 4.7 to 5.3 percent of the world's man-made carbon dioxide emissions between 1882 and 2002. The group suggested that such studies could form the basis for eventual legal action.\n\nBP left the Global Climate Coalition in 1997 and said that global warming was a problem that had to be dealt with, although it subsequently joined others in lobbying the Australian government not to sign the Kyoto Protocol unless the US did. In March 2002 BP's chief executive, Lord Browne, declared in a speech that global warming was real and that urgent action was needed, saying that \"Companies composed of highly skilled and trained people can't live in denial of mounting evidence gathered by hundreds of the most reputable scientists in the world.\". In 2005 BP was considering testing carbon sequestration in one of its North Sea oil fields, by pumping carbon dioxide into them (and thereby also increasing yields).\nThroughout 2006 BP, led by their CEO Lord John Browne, has continued to take a leadership stance on climate change. It has cut its own operational emissions of CO by 10%. It is investing $8 billion in renewable energy over the next 10 years. And most recently it has launched a 'target zero' campaign in the UK to encourage its customers to offset their vehicle emissions when they fill up at the petrol station.\n\nBP's American division is a member of the U.S. Climate Action Partnership (USCAP) (see above).\n\nFrom 2005 to 2008, Koch Industries donated $5.7 million on political campaigns and $37 million on direct lobbying to support fossil fuel industries. Between 1997 and 2008, Koch Industries donated a total of nearly $48 million to climate opposition groups. According to Greenpeace, Koch Industries is the major source of funds of what Greenpeace calls \"climate denial\". Koch Industries and its subsidiaries spent more than $20 million on lobbying in 2008 and $12.3 million in 2009, according to the Center for Responsive Politics, a nonpartisan research group.\n\nAmerican Electric Power, the world's largest private producer of carbon dioxide, said in 2005 that targets for carbon reduction \"represent a common-sense approach that can begin the process of lowering emissions along a gradual, cost-effective path.\" The company complained that \"uncertainties over the cost of carbon\" made it very difficult to make decisions about capital investment.\n\nDuPont has cut its greenhouse gas emissions by 65% since 1990, saving hundreds of millions of dollars in the process. \"Give us a date, tell us how much we need to cut, give us the flexibility to meet the goals, and we'll get it done\", Xcel Energy CEO Wayne Brunetti told \"Business Week\" in 2004.\n\nDuke Energy, FPL Group, and PG&E Corporation are members of the U.S. Climate Action Partnership (USCAP) (see above).\n\nA large proportion of carbon dioxide emissions occur because of transportation. Several companies have formed or invested in electric substitutes for standard automobiles. The Tesla Roadster (2008) is an all-electric sports car, and Tesla also produces the Tesla Model S sedan. Vectrix produces and sells an electric scooter rated for 100 km/h (60 mph).\n\nThere has also been greatly increased interest in personal rapid transit, which applies system engineering principles to reduce energy use, eliminate traffic jams, and produce an acceptable substitute to replace cars, all at the same time. Most systems fully meet Kyoto Treaty carbon emission goals now, 60 years ahead of schedule. Korean steel maker POSCO and its partner Vectus Ltd. have produced a working safety case, including test track and vehicles, that remains fully functional in Swedish winters. Vectus and Suncheon S. Korea signed a memorandum of understanding to install a system.\nAdvanced Transportation Systems' ULTra passed safety certification by the UK Rail Inspectorate in 2003, and won a demonstration project at Heathrow Airport due to be in service in early 2010. ATS Ltd. estimates its ULTra PRT will consume 839 BTU per passenger mile (0.55 MJ per passenger km). By comparison, automobiles consume 3,496 BTU, and personal trucks consume 4,329 BTU per passenger mile.\n2getthere Inc. sells automated electric freight handling and transit vehicles designed to share existing rights of way with normal traffic.\nThe company recently won the personal rapid transit competition for Masdar.\n\nClimate change threatens to reduce the insurance industry's profits and to slow its growth into developing countries. This has led some to argue that the insurance industry is the natural business to lead a response to climate change.\n\nIn 2004 Swiss Re, the world's second largest reinsurance company, warned that the economic costs of climate-related disasters threatened to reach $150 billion a year within ten years.\n\nIn 2006 Lloyd's of London, published a report highlighting the latest science and implications for the insurance industry.\n\nSwiss Re, has said that if the shore communities of four Gulf Coast states choose not to implement adaptation strategies, they could see annual climate-change related damages jump 65 percent a year to $23 billion by 2030. \"Society needs to reduce its vulnerability to climate risks, and as long as they remain manageable, they remain insurable, which is our interest as well,\" said Mark D. Way, head of Swiss Re's sustainable development for the Americas.\n\nAIG is a member of the U.S. Climate Action Partnership (USCAP) (see above).\n\nIn the UK, some newspapers (\"Daily Mail\", \"Daily Telegraph\") are significantly skeptical, while most others (with varying enthusiasm, \"The Independent\" giving it most prominence) support action on global warming. Overall, British newspapers have given the issue three times more coverage than US newspapers. In 2006 (\"British Sky Broadcasting\" (Sky) became the world's first media company to go 'climate neutral' by purchasing enough carbon offsets. The CEO of the company James Murdoch (son of Rupert Murdoch and heir apparent for the News International empire) is a strong advocate of action on climate change and is thought to be influential on the issue within the wider group of companies, \"The Sun\" announced it was \"going green\" and now covers the global warming issue extensively. In June 2006, to much industry interest, Rupert Murdoch invited Al Gore to make his climate change presentation at the annual News Corp (including the Fox Network) gathering at the Pebble Beach golf resort, (USA). In August 2007, Rupert Murdoch announced plans for News Corp. to be carbon neutral by 2010.\n\nBusinesses take action on climate change for several reasons. Action improves corporate image and better aligns corporate actions with the environmental interests of owners, employees, suppliers, and customers. Action also occurs to reduce costs, increase return on investments, and to reduce dependency on uncontrollable costs.\n\nFor many companies, looking at more efficient energy use can pay off in the medium to long term; unfortunately, shareholders need to be satisfied in the short term, so regulatory intervention is often required, to encourage prudent conservation measures. However, as carbon intensity starts to show up on balance books through organizations such as the Carbon Disclosure Project, voluntary action is starting to take place.\n\nRecently there has been a spate of companies acting to improve their energy efficiency. Possibly the most prominent of these companies is Wal-Mart. Wal-Mart, the largest retailer in the US, has announced specific environmental goals to reduce energy use in its stores and pressure its 60,000 suppliers in its worldwide supply chain to follow its lead. On energy efficiency, Wal-Mart wants to increase the fuel efficiency of its truck fleet by 25% over the next three years and double it within ten years, moving from 6.5 mpg. This seems an attainable goal, and by 2020, it is expected to save the company $494 million a year. The company also wants to build a store that is at least 25% more energy efficient within four years.\n\nIn August 2002, the largest gathering of ministers in the history of the world met at the World Summit on Sustainable Development in Johannesburg. The global environmental community discussed the role of renewables and energy efficiency in lowering carbon emissions, mitigating poverty reduction (energy access) and improving energy security. One result from WSSD was the formation of to carry forward the international dialogue on sustainable energy and its role in the energy mix.\n\nPartnerships formed include the Renewable Energy and Energy Efficiency Partnership, the Global Village Energy Partnership, the Johannesburg Renewable Energy Coalition (JREC), and the Global Network on Energy for Sustainable Development.\n\nRenewable energies and renewable energy technologies have many advantages over their fossil fuel counterparts. These advantages include the absence of local pollution such as particulates, sulphur oxides (SOX's) and nitrous oxides (NOX's). For the business community, the economic advantages are also becoming clearer. Numerous studies have shown that the working environment has a significant effect on workforce morale. Renewable energy solutions are a part of this, wind turbines in particular being seen by many as a potent symbol of a new modernity, where environmental considerations are taken seriously. A workforce seeing a forward-looking and responsible company is more likely to feel good about working for such a company. A happier workforce is a more productive workforce.\n\nMore directly, the high petroleum (oil) and gas prices of 2005 have only added to the attraction of renewable energy sources. Although most renewable energies are more expensive at current fuel prices, the difference is narrowing, and uncertainty in oil and gas markets is a factor worth considering for highly energy-intensive businesses.\n\nAnother factor affecting the uptake of renewable energies in Europe is the EU Energy Trading Scheme (ETS or EUTS). Many large businesses are fined for increases in emissions, but can sell any \"excess\" reductions they make.\n\nCompanies with high-profile renewable energy portfolios include an aluminium smelter (Alcan), a cement company (Lafarge), and a microchip manufacturer (Intel). Many examples of corporate leadership in this area can be found on the website of The Climate Group, an independent organization set up for promoting such action by business and government.\n\nThe principle of carbon offset is fairly simple: a business decides that it doesn't want to contribute further to global warming, and it has already made efforts to reduce its carbon (dioxide) emissions, so it decides to pay someone else to further reduce its net emissions by planting trees or by taking up low-carbon technologies. Every unit of carbon that is absorbed by trees—or not emitted due to funding of renewable energy deployment—offsets the emissions from fossil fuel use. In many cases, funding of renewable energy, energy efficiency, or tree planting—particularly in developing nations—can be a relatively cheap way of making an event, project, or business \"carbon neutral\". Many carbon offset providers—some as inexpensive as $0.10 per ton of carbon dioxide—are referenced in the Carbon Offset article of this encyclopedia.\n\nMany businesses are now looking to carbon offset all their work. An example of a business going carbon neutral is FIFA: their 2006 World Cup Final will be carbon neutral. FIFA estimate they are offsetting one hundred thousand tons of carbon dioxide created by the event, largely as a result of people travelling there. Other carbon neutral companies include the bank HSBC, the consumer staples manufacturer Annie's Homegrown, world leading society publisher Blackwell Publishing, and the publishing house New Society Publishers. The Guardian newspaper also offsets its carbon emissions resulting from international air travel.\n\n\n"}
{"id": "34423174", "url": "https://en.wikipedia.org/wiki?curid=34423174", "title": "Forecasting (heating)", "text": "Forecasting (heating)\n\nForecasting is a method of controlling building heating by calculating demand for heating energy that should be supplied to the building in each time unit. By combining physics of structures with meteorology, properties of the building, weather conditions including outdoor temperature, wind power and direction, as well as solar radiation can be taken into account. In the case of conventional heating control, only current outdoor temperature is considered.\n\nThe starting point for developing the method of forecasting was the ENLOSS mathematical energy balance model developed by Prof. Roger Taesler\n\nUntil 2010 inclusive, forecasting method has been introduced in nearly seven million square metres of floorage of residential buildings and commercial premises. Forecasting method is offered and developed by many companies and organizations. Estimated data indicate 10 - 15 kWh/m2 reduction of average annual heat energy consumption. Since forecasting method contains information about future demand and is not in conflict with other methods of increasing energy efficiency, it is always a good foreground solution.\n\nAs far as practical use of forecasting method is concerned, usually remote control forecasting receivers are used to send and receive data by means of GPRS or GSM network. Then, the forecasting receivers manage the operation of control panels installed in buildings which adjust distribution of heat energy in the heating system of a given property.\n\nRecently, special remote control weather loggers have started to be used in combination with forecasting receivers. The weather loggers measure air temperature and humidity with high accuracy and the measurements are sent in real time to forecasting receivers to which they are connected. Such a turning point in technology indicates even higher precision of forecasting method.\n\n"}
{"id": "18406207", "url": "https://en.wikipedia.org/wiki?curid=18406207", "title": "Futi", "text": "Futi\n\nFuti is the first Portuguese electric vehicle made by António Febra's company FUTI. It runs exclusively on electric power and can be charged at home.\n\nAccording to António Febra himself in an interview for TVI television channel, it was scheduled to be commercialized by June 2009. It was to cost between 13000 and 16000 Euros depending on battery option. SIC also covered this story. Both channels featured footage of António Febra driving Futi on public roads.\n\nFuti is also fully recyclable.\n\n\nFuti flyer\n"}
{"id": "697387", "url": "https://en.wikipedia.org/wiki?curid=697387", "title": "Granular convection", "text": "Granular convection\n\nGranular convection, or granular segregation, is a phenomenon where granular material subjected to shaking or vibration will exhibit circulation patterns similar to types of fluid convection. It is sometimes described as the Brazil nut effect when the largest particles end up on the surface of a granular material containing a mixture of variously sized objects; this derives from the example of a typical container of mixed nuts, where the largest will be Brazil nuts. The phenomenon is also known as the muesli effect since it is seen in packets of breakfast cereal containing particles of different sizes but similar density, such as muesli mix.\n\nUnder experimental conditions, granular convection of variously sized particles has been observed forming convection cells similar to fluid motion. The convection of granular flows is becoming a well-understood phenomenon.\n\nIt may be counterintuitive to find that the largest and (presumably) heaviest particles rise to the top, but several explanations are possible:\n\nThe phenomenon is related to Parrondo's paradox in as much as the Brazil nuts move to the top of the mixed nuts against the gravitational gradient when subjected to random shaking.\n\nGranular convection has been probed by the use of MRI where convection rolls similar to those in fluids (Bénard cells) can be visualized.\n\nThe effect is of serious interest for some manufacturing operations; once a homogeneous mixture of granular materials has been produced, it is usually undesirable for the different particle types to segregate. Several factors determine the severity of the Brazil nut effect, including the sizes and densities of the particles, the pressure of any gas between the particles, and the shape of the container. A rectangular box (such as a box of breakfast cereal) or cylinder (such as a can of nuts) works well to favour the effect, while a cone-shaped container results in what is known as the reverse Brazil nut effect.\n\nIn astronomy, it is also seen in some low density, or rubble pile asteroids, for example the asteroid 25143 Itokawa.\n\nIn geology, the effect is common in formerly glaciated areas such as New England and areas in regions of permafrost where the landscape is shaped into hummocks by frost heave — new stones appear in the fields every year from deeper underground. Horace Greeley noted \"Picking stones is a never-ending labor on one of those New England farms. Pick as closely as you may, the next plowing turns up a fresh eruption of boulders and pebbles, from the size of a hickory nut to that of a tea-kettle.\" A hint to the cause appears in his further description that \"this work is mainly to be done in March or April, when the earth is saturated with ice-cold water\". Underground water freezes, lifting all particles above it. As the water starts to melt, smaller particles can settle into the opening spaces while larger particles are still raised. By the time ice no longer supports the larger rocks, they are at least partially supported by the smaller particles that slipped below them. Repeated freeze-thaw cycles in a single year speeds up the process.\n\nThis phenomenon is one of the causes of inverse grading which can be observed in many situations including soil liquefaction during earthquakes or mudslides. Granular convection is also exemplified by debris flow, which is a fast moving, liquefied landslide of unconsolidated, saturated debris that looks like flowing concrete. These flows can carry material ranging in size from clay to boulders, including woody debris such as logs and tree stumps. Flows can be triggered by intense rainfall, glacial melt, or a combination of the two.\n\n\n"}
{"id": "584346", "url": "https://en.wikipedia.org/wiki?curid=584346", "title": "Great Western main line", "text": "Great Western main line\n\nThe Great Western main line is a main line railway in England, that runs westwards from London Paddington to . Opened in 1841, it was the original route of the pre-1948 Great Western Railway which was merged into the Western Region of British Railways and is now a part of the national rail system managed by Network Rail.\n\nThe line is currently being electrified. It was electrified from Paddington to Heathrow Airport in the late 1990s. Work to electrify the remainder of the route started in 2011 with an initial aim to complete the work all the way to Bristol by 2016. The programme however has been deferred with no end completion forecast because costs have tripled. The four sections deferred are: Didcot Parkway to Oxford, Bristol Parkway to Bristol Temple Meads, Royal Wootton Bassett Junction to Bristol Temple Meads and the Thames Valley branches to Henley and Windsor.\n\nThe line was built by the Great Western Railway and engineered by Isambard Kingdom Brunel as a dual track line using a wider broad gauge and was opened in stages between 1838 and 1841. The final section, between Chippenham and Bath, was opened on completion of the Box Tunnel in June 1841.\n\nThe alignment was so level and straight it was nicknamed \"Brunel's billiard table\". It was supplemented with a third rail for dual gauge operation, allowing standard gauge trains to also operate on the route, in stages between 1854 and 1875. Dual gauge was introduced as follows: London to Reading (October 1861), Reading to (December 1856), Didcot to (February 1872), Swindon to Thingley Junction, (June 1874), Thingley Junction to (March 1875), Bathampton to Bristol (June 1874), Bristol station area (May 1854). The broad gauge remained in use until 1892. Evidence of the original broad gauge can still be seen at many places where bridges are a wider than usual, or where tracks are ten feet apart instead of the usual six.\n\nThe original dual tracks were widened to four in places, mainly in the east half, between 1877 and 1899: Paddington to (October 1877), Southall to (November 1878), West Drayton to (June 1879), Slough to east side of Maidenhead Bridge (September 1884), Maidenhead Bridge to (June 1893), Reading station (1899), Reading to (July 1893), Pangbourne to Cholsey and Moulsford (?), Cholsey and Moulsford to Didcot (December 1892); also short sections between Didcot and Swindon, and at Bristol.\n\nFollowing the Slough rail accident in 1900 when five passengers were killed, improved vacuum braking systems were used on locomotives and passenger rolling stock and Automatic Train Control (ATC) was introduced in 1908.\n\nFurther widenings of the line took place between 1903 and 1910 and more widening work took place between 1931 and 1932.\n\nAt the outbreak of World War I in 1914, the Great Western Railway was taken into government control, as were most major railways in Britain and were reorganised after the war into the \"big four\" companies, of which the Great Western Railway was one. The railways returned to direct government control during World War II before being nationalised to form British Railways (BR) in 1948.\n\nThe line speed was upgraded in the 1970s to support the introduction of the InterCity 125 (HST).\n\nIn 1977 the Parliamentary Select Committee on Nationalised Industries recommended considering electrification of more of Britain's rail network, and by 1979 BR presented a range of options that included electrifying the line from Paddington to Swansea by 2000. Under the 1979–90 Conservative governments that succeeded the 1976–79 Labour government, the proposal was not implemented.\n\nIn August 2008 it was announced that a number of speed limits on the relief lines between Reading and London had been raised, so that 86% of the line could be used at .\n\nThe route of the GWML includes dozens of listed buildings and structures, including tunnel portals, bridges and viaducts, stations, and associated hotels. Part of the route passes through and contributes to the Georgian Architecture of the City of Bath World Heritage Site; the path through Sydney Gardens has been described as a \"piece of deliberate railway theatre by Brunel without parallel\". Grade I listed structures on the line include London Paddington, Wharncliffe Viaduct, the 1839 Tudor gothic River Avon Bridge in Bristol, and Bristol Temple Meads station.\n\nThe communities served by the Great Western main line include: West London (including Acton, Ealing, Hanwell, Southall, Hayes, Harlington and West Drayton); Iver; Langley; Slough; Burnham; Taplow; Maidenhead; Twyford; Reading; Tilehurst; Pangbourne; Goring-on-Thames; Streatley; Cholsey; Didcot; Swindon; Chippenham; Bath; Keynsham; and Bristol.\n\nFrom London to Didcot, the line follows the Thames Valley, crossing the River Thames three times, including on the famous Maidenhead Railway Bridge. After Swindon, trains pass the Swindon Steam Railway Museum. From Wootton Bassett there are two different routes to Bristol, firstly via Box Tunnel and secondly via .\n\nIt is also possible to run via the Wessex Main Line, but this involves a reversal at Bradford Junction, so is only really suitable for multiple unit trains or via Reading to Bath via Newbury. Trains on the Great Western main line are sometimes diverted from Reading along the Reading to Taunton line, as far as , from where they can use the Wessex Main Line to reach either Chippenham, or Bath Spa. Beyond Bristol, some trains continue on the Bristol to Taunton Line to or beyond.\n\nThe following routes are managed by Network Rail as part of the Great Western main line (Route 13): Didcot to and Worcester via the Cherwell Valley Line and Cotswold Line, Swindon to via the Golden Valley Line, Swindon to and via the South Wales Main Line, Cross Country Routes south of Birmingham and also all connecting branch lines.\n\nMain line and local services are provided by Great Western Railway (GWR). The stations served by trains between London Paddington and Bristol Temple Meads are: , , , , , and . Some trains between London and Bristol do not call at Didcot Parkway and very few stop at Slough.\n\nFast trains from Paddington to London Heathrow Airport are operated by Heathrow Airport Holdings as the Heathrow Express. Local services on this route were previously jointly operated by GWR and BAA under the Heathrow Connect name.\n\nCrossCountry operate trains between Reading and Oxford, using the Great Western main line as far as Didcot and South Western Railway operate a limited number of trains between Bath and Bristol.\n\nGreat Western Railway also operate a train between London Paddington – Cardiff Central every 30 minutes, with hourly extensions to Swansea. At Swansea/Cardiff there is a connecting Transport for Wales boat train to/from Fishguard Harbour for the Stena Line ferry to Rosslare Europort in Ireland. An integrated timetable is offered between London Paddington and Rosslare Europort with through ticketing available. Daytime and nocturnal journeys are offered in both directions daily (including Sundays). Additionally, 2–3 Great Western Railway trains continue to Pembroke Dock on weekends during the Summer season to connect with ferry services to Ireland.\n\nBetween London and Didcot there are four tracks, two for each direction. The main lines are mostly used by the faster trains and are on the south side of the route. The relief lines on the north side are used for slower services and those that call at all stations, as only London Paddington, Slough, Maidenhead, Twyford, Reading and Didcot Parkway stations have platforms on the main lines (although a few others have main line platforms that can be used in an emergency). Between Didcot and Royal Wootton Bassett, a series of passing loops allow fast trains to overtake slower ones. This section is signalled for bi-directional running on each line but this facility is usually only used during engineering working or when there is significant disruption to traffic in one direction.\n\nThe summit of the line is at Swindon, and falls away in each direction: Swindon is above Paddington, and above Bristol Temple Meads. The maximum gradient between Paddington and Didcot is 1 in 1320 (0.75 ‰ or 0.075 %); between Didcot and Swindon it is 1 in 660 (1.5 ‰ or 0.15 %) but west of Swindon, gradients as steep as 1 in 100 (10 ‰ or 1 %) are found in places, such as Box Tunnel and to the east of .\n\nThe line is electrified between Paddington and Didcot Parkway using 25 kV AC overhead supply lines.\n\nThe line speed is . The relief lines from Paddington to Didcot are limited to as far as Reading, and then to Didcot. Lower restrictions apply at various locations. The line is one of two Network Rail-owned lines equipped with the Automatic Train Protection (ATP) system, the other being the Chiltern Main Line.\n\nMajor civil engineering structures on the Great Western main line include the following.\nLine-side train monitoring equipment includes hot axle box detectors (HABD) and wheel impact load detectors (WILD) ‘Wheelchex’, these are located as follows.\n\nSince 2011, the Great Western has been undergoing a £5 billion modernisation by Network Rail.\n\nReading railway station saw a major redevelopment with new platforms, a new entrance, footbridge and lifts; the work was completed a year ahead of schedule in July 2014.\n\nThe Crossrail project covered electrification of the line from Airport Junction to Maidenhead and, following a number of announcements and delays, the government announced in March 2011 that it would electrify the line between London and Cardiff together with the section linking Bristol Parkway and Bristol Temple Meads. In July 2012, the government announced that the final portion of the Great Western, from Cardiff to Swansea, would be electrified.\n\nFollowing delays to the original plan, and a major escalation of costs, the Conservative government announced in July 2017 that, for the time being, the electrification would only be completed as far as Thingley Junction, west of Chippenham on the Swindon to Bristol Temple Meads section of the route. At the same time the Cardiff to Swansea section, that from Bristol Parkway to Bristol Temple Meads, and Didcot to Oxford were also postponed. The government argued that bi-mode trains will fill in the gaps pending completion of electrification, although the Class 800 trains are much slower in diesel mode than under electric power. The electrification as far as Didcot Parkway was completed in December 2017.\n\nIn addition to allowing Crossrail services with the new EMUs, the electrification allowed the introduction of EMUs by GWR. It was originally planned to bring second-hand s from Great Northern after the arrival of their new trains but it was later decided to order new Class 387s for GWR instead. Eight were delivered during 2016, with more on order to bring the total to 45. Some of the and DMUs currently used by GWR for Thames Valley services will be displaced to services on the lines around Cardiff and Bristol.\n\nThe line will also be used by the new Hitachi Super Express high speed trains – the s and s – which will gradually replace the InterCity 125 and sets currently used for the long-distance services.\n\nNetwork Rail plans to install European Rail Traffic Management System (ERTMS) in-cab signalling on the Great Western line; this is a pre-requisite for the Super Express trains to run at 140 mph (225 km/h). Some or all of the resignalling work will be undertaken during the electrification work.\n\nFurther capacity improvements are also scheduled at Swindon, adding to recent changes and the new Platform 4.\n\nCrossrail services are planned to terminate at Reading. Some of the current suburban services into London Paddington are planned to be transferred to the new Crossrail service, which will in turn free up some surface-level capacity at London Paddington.\n\nOther more distant aspirations include resignalling and capacity improvements at Reading; the provision of four continuous tracks between Didcot and Swindon (including a grade-separated junction at Milton, where westbound relief line switches from the north side of the line to the south); and resignalling between Bath and Bristol to enable trains to run closer together.\n\nAccess to Heathrow Airport from the west remains an aspiration and the 2009 Heathrow Airtrack scheme, abandoned in 2011, proposed a route south of the Great Western main line to link the airport with Reading. Plans for electrification of the line will make it easier to access Heathrow from Reading, since lack of electrification between Reading station and Airport Junction (near West Drayton station) was a limiting factor. Plans under consideration in 2014 included new tunnels between Heathrow and Langley.\n\nNetwork Rail intends to replace the ATP system with ETCS – Level 2 from 2017 to 2035 along with the introduction of the new IEP trains.\n\nSignalling Solutions is to resignal the from Paddington to , including the Airport branch, as part of the Crossrail project.\n\nThere are calls for the reintroduction of Corsham station due to recent growth of the town. The original station was closed to passengers in 1965.\n\nA local group is campaigning for the reopening of Saltford station between Bath and Bristol, to coincide with electrification.\n\nThere have also been calls to reopen the former Wantage Road station. Oxfordshire County Council include a proposal for a new station to serve for Wantage and Grove in their 2015-2031 local transport plan.\n\n\nThe reference for the route map diagram is:-\n\n \n"}
{"id": "2886158", "url": "https://en.wikipedia.org/wiki?curid=2886158", "title": "Hand warmer", "text": "Hand warmer\n\nHand warmers are small (mostly disposable) packets which are held in the hand and produce heat on demand to warm cold hands. They are commonly used in outdoor activities. Other types of warmers are available to provide soothing heat for muscular or joint aches.\n\nDepending on the type and the source of heat, hand warmers can last from 30 minutes (recrystallisation) up to 24 hours (platinum catalyst).\nThe hand warmer was created by Japanese inventor Niichi Matoba. Matoba received a patent for applying the principle of an oxidation reaction that produces heat by means of platinum catalysis. He then devoted his time to researching how to make the product suitable for practical use. In 1923, he manufactured a prototype of his device naming it HAKUKIN-kairo (HAKKIN warmer). A version of these original portable hand warmers is still produced in Japan.\n\nAir-activated hand warmers contain cellulose, iron, water, activated carbon, vermiculite (water reservoir) and salt and produce heat from the exothermic oxidation of iron when exposed to air. They typically emit heat for 1 to 10 hours, although the heat given off rapidly diminishes after 1–2 hours. The oxygen molecules in the air react with iron, forming rust. Salt is often added to catalyze the process.\n\nThis type of hand warmers can be recharged by immersing the hand-warmer in very hot water until the contents are uniform and then allowing it to cool. The release of heat is triggered by flexing a small metal disk in the hand warmer, which generates nucleation centers that initiate crystallisation. Heat is required to dissolve the salt in its own water of crystallisation and it is this heat that is released when crystallisation is initiated.\n\nThe latent heat of fusion is about 264–289 kJ/kg.\nThis process can be scaled up to provide a domestic heating store developed by Sunamp Scotland \n\nLighter fuel hand-warmers use lighter fluid (petroleum naptha). These can be re-used by simply refuelling. Typical models can generate heat for either half a day or a whole day, depending on conditions.\n\nBattery operated hand warmers use electrically resistive heating devices to convert electrical energy in the battery. Typically hand warmers can heat for up to six hours, with heat outputs from 40-48C. Rechargeable electronic hand warmers can be charged from a mains power supply or from a 5V USB power supply, with up to 500 recharge cycles possible.\n\nCharcoal hand-warmers provide heat by burning charcoal in a special case. These can last up to 6 hours and become comfortably hot. Charcoal cases for these usually have felt on the outside and have items in it that do not produce heat, but spread the heat such as metal. A charcoal hand warmer can start heating when both ends of charcoal are struck and then extinguished to create a hot charcoal. The smouldering stick is then placed inside the case. The charcoal sticks are available from most outdoor activity shops and are fairly inexpensive.\n\n"}
{"id": "38958421", "url": "https://en.wikipedia.org/wiki?curid=38958421", "title": "Hostivické Ponds", "text": "Hostivické Ponds\n\nThe Hostivické Ponds are a natural monument located at the western edge of Prague, about 4 km from Zličín in the municipalities of Hostivice, Břve and Litovice. This small-scale protected area was established on 14 October 1996 by the District Office of Western Prague with the aim of the conservation of natural communities of the Hostivické pond system.\n\nIn the adjacent wetlands and forests rare and endangered species of plants and animals can be found. During the reign of Rudolf II the fishponds in the area served as a source of water for Prague Castle.\n\nIn 1972 rare fungus \"Haasiella venustissima\", previously found in only two localities in Czechoslovakia, was found in the area. Also found in the reeds are tufted sedge (\"Carex cespitosa\"), bistort (\"Persicaria bistorta\") and great burnet (\"Sanguisorba officinalis\"). In the past western marsh orchids and black poplars, could be found, but they have not been seen in recent years.\n\nLitovická valley attracted to the settlement as early as the Neolithic (New Stone Age). The oldest settlement in Hostivice ponds is supported by the present village on the hill and at the top Břve Krahulov. The mere mention of Hostivice dates from the 14th century. All three villages united under one domain Gothard Zdarsky Zdar from around 1640.\n\nFrom a historical perspective, it was important that the pond system served as the source region of water for water supply, supplying the Prague Castle during the reign of Rudolf II.\n\nOn the shore of the pond Břevského was then built camp, which was subsequently abandoned. Currently, the buildings in the camp removed. This area is often used for illegal waste disposal. Natural Monument Hostivické ponds was established in 1996 and since that time the development of its old Czech Union for Nature Conservation, in particular its basic organization in Hostivice.\n\n"}
{"id": "9242224", "url": "https://en.wikipedia.org/wiki?curid=9242224", "title": "Hovensa", "text": "Hovensa\n\nHovensa (known also as St. Croix Refinery) was a petroleum refinery located on the island of St. Croix in the United States Virgin Islands. The refinery was a joint venture between Hess Corporation and Petroleos de Venezuela. For most of its operating life as HOVENSA it supplied heating oil and gasoline to the U.S. Gulf Coast and the eastern seaboard with the crude mainly sourced from Venezuela. Previously it had sourced its crude feedstock from a number of other countries including Libya. At a capacity of about as of 2010 it was in the top 10 largest refineries in the world.\n\nHess Oil Virgin Islands Corporation started refinery construction in January 1966 having purchased the property from Annie de Chabert and, in October of the same year, the refinery started operating. In 1974, the capacity of refinery was expanded up to its peak at . Hovensa LLC, which took over the refinery operatorship, was established in 1998.\n\nIn January 2011, Hovensa paid a $5.3 million penalty for Clean Air Act violations. On January 18, 2012 the company announced that it would close the refinery by mid-February 2012. The company sought to close the refinery and have the property serve as only a storage terminal.\n\nIn the summer of 2018, a Boston-based private equity firm announced plans to acquire the facility and resume refinery operations by 2020.\n\n\n"}
{"id": "31018803", "url": "https://en.wikipedia.org/wiki?curid=31018803", "title": "Humistor", "text": "Humistor\n\nA humistor is a type of variable resistor whose resistance varies based on humidity.\n\nA humistor has a ceramic composition comprising at least one component having a spinel type cubic symmetry selected from the group consisting of MgCrO4, FeCrO, NiCrO, CoCrO, MnCrO, CuCrO, MgTiO, ZnTiO, MgSnO and ZnSnO, and, if desired, at least one component selected from the group consisting of TiO, ZrO, HfO and SnO. A humidity sensor has a sensing portion which usually comprises a humidity-sensitive resistor composed of an organic polymer, such as a polyamide resin, polyvinyl chloride or polyethylene, or a metal oxide.\n\nA capacitive humidity sensor detects humidity based on a change of capacitance between two detection electrodes provided on a semiconductor substrate. The capacitance type humidity sensor detects humidity by measuring the change in the electrostatic capacity of an element corresponding to the ambient humidity. A resistive humidity sensor detects relative humidity by measuring the change in the resistance of an element corresponding to the ambient humidity. Most of the resistance type humidity sensors include an electrolytic, polymeric, or metallic oxide sensor element. An impedance humidity sensor changes its electrical impedance as the humidity of the surrounding environment changes, and the measured impedance is converted into humidity readings\n\nHumidity sensors can be used not only to measure the humidity in an atmosphere but also to automatically control humidifiers, dehumidifiers, and air conditioners for humidity adjustment.\n"}
{"id": "4167130", "url": "https://en.wikipedia.org/wiki?curid=4167130", "title": "Isthmus-34 Light", "text": "Isthmus-34 Light\n\nIsthmus-34 Light is a sour crude oil produced in Mexico mainly in the Campeche zone, in the Gulf of Mexico along with the extraction centers in Chiapas, Tabasco, and Veracruz. The name derives from the nearby Isthmus of Tehuantepec and the oil is a component of the OPEC Reference Basket (despite Mexico's not being a part of OPEC). It has the following characteristics:\n\n"}
{"id": "3907887", "url": "https://en.wikipedia.org/wiki?curid=3907887", "title": "Johannesburg Renewable Energy Coalition", "text": "Johannesburg Renewable Energy Coalition\n\nThe Johannesburg Renewable Energy Coalition, also known as JREC, is the group of countries supporting the Declaration on The Way Forward on Renewable Energy (also known as the JREC Declaration), made at the World Summit on Sustainable Development in Johannesburg, South Africa, in September 2002. The JREC is co-chaired by the European Commission and the Government of Morocco.\n\n\n"}
{"id": "39182285", "url": "https://en.wikipedia.org/wiki?curid=39182285", "title": "Küre Mountains National Park", "text": "Küre Mountains National Park\n\nKüre Mountains National Park ()), established on July 7, 2000, is a national park in northern Turkey. The national park stretches over the mountain range of Küre Mountains and is located in the districts Pınarbaşı, Cide, Şenpazar in Kastamonu Province and Ulus in Bartın Province.\n\nIt covers an area of at an average elevation of .\n"}
{"id": "44168110", "url": "https://en.wikipedia.org/wiki?curid=44168110", "title": "Leven Beach Conservation Park", "text": "Leven Beach Conservation Park\n\nLeven Beach Conservation Park is a protected area in the Australian state of South Australia located on the north coast of the lower part of Yorke Peninsula within the boundaries of the gazetted localities of Point Souttar and The Pines about west north-west of Point Turton.\n\nThe conservation park was proclaimed in 1988 for the purpose of conserving ‘sheoak woodland and potentially provides habitat for a nationally endangered species of butterfly, the Yellowish Sedge-skipper Butterfly’.\n\nThe conservation park is classified as an IUCN Category III protected area.\n\n"}
{"id": "42351967", "url": "https://en.wikipedia.org/wiki?curid=42351967", "title": "List of droughts", "text": "List of droughts\n\nThis is a list of significant droughts, organized by large geographical area and then year.\n\n\n\n\n\n\n\nPart of the 2010-12 UK Drought. 2011 UK September-October Heatwave\n\nPart of the 2010-12 UK Drought. 2012 UK March Heatwave\n\n"}
{"id": "1460740", "url": "https://en.wikipedia.org/wiki?curid=1460740", "title": "List of virus species", "text": "List of virus species\n\nThis is a list of all virus species. Excluded are other ranks of virus, viroids and prions. Also excluded common names and obsolete names for viruses.\n\n\n\n"}
{"id": "49954874", "url": "https://en.wikipedia.org/wiki?curid=49954874", "title": "Little Box Challenge", "text": "Little Box Challenge\n\nThe Little Box Challenge was an engineering competition run by Google and the IEEE's Power Electronics Society. The original challenge was posted on July 22, 2014 with modifications on December 16, 2014 and March 23, 2015. Testing was in October 2015 at the National Renewable Energy Laboratory. From the 18 finalists, CE+T Power's team called \"Red Electrical Devils\" won the $1 million prize, which was awarded to them in March 2016.\n\nThe challenge was to build a power inverter that was about 10 times smaller than the state-of-the-art at the time. It had to have an efficiency greater than 95 percent and handle loads of 2 kVA. It also had to fit in a metal enclosure of no more than 40 cubic inches (the eponymous \"little box\") and withstand 100 hours of testing.\n\nThe goals of the competition were lower cost solar photovoltaic power, more efficient uninterruptible power supplies, affordable microgrids, and the ability to use an electric vehicle's battery as backup power during a power outage. Google also hoped a smaller inverter could make its data centers run more efficiently.\n\n\n"}
{"id": "54173227", "url": "https://en.wikipedia.org/wiki?curid=54173227", "title": "MARMOK-A-5", "text": "MARMOK-A-5\n\nMARMOK-A-5 is an offshore electrical power generator that uses wave energy to create electricity. This device is a spar buoy installed in the maritime testing site Bimep, in the Bay of Biscay. It is the first grid connected maritime generator in Spain, and one of the first in the word.\n\nDeveloped by the Basque company Oceantec Energias Marinas inside the European project OPERA, it is delivering electrical energy to the grid since December 2016. The buoy is located in the ocean, 4Km from the coastline and is connected to the sea with a submarine electrical cable. With a nominal power of 30kW, the principal aim of the MARMOK-A-5 device is obtaining results in the way of designing a new generation cost effective high power marine energy generator.\n\nThe operation principle of MARMOK-A-5 is a point absorber OWC (Oscillating Water Column). The device is 5m in diameter and a length of 42m, 6m above the water. It has a weight of more than 80 tons. The bouy is floating in a 90m depth and is tied to the sea bed with a mooring system based on anchors. This wave energy converter has demonstrated its robustness surviving difficult environmental conditions with waves as big as 12m.\n"}
{"id": "7951430", "url": "https://en.wikipedia.org/wiki?curid=7951430", "title": "Malay units of measurement", "text": "Malay units of measurement\n\nUnits of measurement used in Malaysia and neighbouring countries include the \"kati\", a unit of mass, and the \"gantang\", a unit of volume.\n\nIn measuring amount by mass, the common unit is \"kati\", which is about 1 lb (604.79 g). A higher unit is \"pikul\" or \"picul\", which is 100 \"kati\" or .\n\nIn measuring amount by volume, the common unit is \"gantang\" (gallon), which is equivalent to or 8 pints. \nTo make it clear,\nGantang equals a Gallon. A Gallon has sub measurements of 4 Quarts, or 8 Pints, or 16 Cups. A Gantang has sub measurements of 4 Churpak, or 8 Leng, or 16 Chentong. \nChurpak equals a Quart.\nLeng equals a Pint.\nChentong equals a Cup.<ref>\n\n\"Chentong\" is also normally used only in certain areas, and so eight \"leng\" makes up a \"gantang\".\n\nWhen used to measure unhusked rice, a \"gantang\" weights about .\n\n"}
{"id": "50491314", "url": "https://en.wikipedia.org/wiki?curid=50491314", "title": "Martand Singh", "text": "Martand Singh\n\nMartand Singh (1923–95) was an Indian wildlife conservationist, parliamentarian and the last Maharaja of the princely state of Rewa. Born in 1923 to Gulab Singh at Fort of Govindgarh, then the Maharajah of Rewa, he did his college studies at Daly College, Indore and continued at Mayo College, Ajmer from where he graduated in 1941. After the death of his father in 1946, he became the Maharajah of Rewa and retained the title, but not the power, until the government abolished royalty in 1970.\n\nFascinated by the rare breed of white tiger which was native to Rewa, he worked to protect the species and making the region poacher-free. He also reared a white tiger which he found as a cub. After the abolition of royalty, Singh represented Rewa in the 5th Lok Sabha (1971), 7th Lok Sabha (1980) and the 8th Lok Sabha (1984). The Government of India awarded him the third-highest civilian honour of the Padma Bhushan, in 1986, for his contributions to society.\n\nSingh was married to Princess Pravina of Kutch and the couple had one son. He died on 20 November 1995, at the age of 72. He was again in the news in 2013 when his son filed a lawsuit regarding the allegedly illegal sale of \"Rewa Kothi\", their Mumbai bungalow with a reported value of 2 billion, using a fake power of attorney. Martand Singh's property is controlled by His Highness Maharaja Martand Singh Charitable Trust.\n"}
{"id": "14758735", "url": "https://en.wikipedia.org/wiki?curid=14758735", "title": "Martin Weitzman", "text": "Martin Weitzman\n\nMartin Lawrence \"Marty\" Weitzman (born April 1, 1942) is an economist and a Professor of Economics at Harvard University. He is among the most influential economists in the world according to IDEAS/RePEc. His current research is focused on environmental economics, specifically climate change and the economics of catastrophes.\n\nWeitzman received a B.A. in Mathematics and Physics from Swarthmore College in 1963. He went on to receive an M.S. in Statistics and Operations Research from Stanford University in 1964, and then attended Massachusetts Institute of Technology where he received a Ph.D. in Economics in 1967.\n\nWeitzman's research has covered a wide range of topics including Environmental and Natural Resource Economics, Green Accounting, Economics of Biodiversity, Economics of Environmental Regulation, Economics of Climate Change, Discounting, Comparative Economic Systems, Economics of Profit Sharing, Economic Planning, and Microfoundations of Macro Theory.\n\nMuch of Weitzman's research is focused on climate change. Traditional cost-benefit analysis of climate change looks at the costs of reducing global warming (the cost of reducing greenhouse gas emissions) versus the benefits (potentially stopping or slowing climate change). However, in most analyses, the damages that would stem from dramatic climate change are not taken into consideration. Weitzman has added dramatic climate change to the cost-benefit analysis to show that immediate measures must be taken in regards to climate change regulation.\n\nWeitzman's past research was focused on fixed versus profit sharing wages and their effect on unemployment. He proposed that when firms use profit sharing wages, meaning employees receive higher wages when a company is doing well, firms have lower rates of unemployment and do better during recessions.\n\nAnother topic of research that Weitzman is well known for is his study of price versus quantity controls. Weitzman proposed a theory that when faced with uncertainty the relative slopes of the marginal benefits versus the marginal costs must be examined in order to determine which type of control will be most effective. For example, in the case of pollution, the relative slopes of marginal costs and marginal damages must be examined (the marginal benefits are the avoidance of the marginal damages). His research showed that if the slope of marginal costs is steeper, price controls are more effective and if the relative slope of marginal damages is steeper, then quantity controls are more effective.\n\nWeitzman has written three books: \"The Share Economy: Conquering Stagflation\", \"Income, Wealth, and the Maximum Principle\", and, most recently, \"Climate Shock\", jointly with Gernot Wagner. In \"The Share Economy: Conquering Stagflation\", Weitzman proposes that a main cause of stagflation is paying workers a fixed wage, regardless of how the company is performing. He introduces an alternate labor payment system as a way of combating stagflation.\n\"Income, Wealth, and the Maximum Principle\" is a book geared towards advanced economic students particularly those who want to be able to formulate and solve complex allocation problems and who are interested in the relationship between income accounting and wealth or welfare. \"Climate Shock\" details how what we know about global warming is bad and what we don't know is potentially much worse.\n\nWeitzman began his teaching career in 1967 as an assistant professor of economics at Yale University. Three years later Weitzman was promoted to be an associate professor, and he remained in this position until 1972; at this time he joined the faculty at Massachusetts Institute of Technology in 1972 still as an associate professor. In 1974, Weitzman became a professor at MIT, where he taught until 1989. From 1986 to 1989, Weitzman was recognized as a Mitsui professor at MIT. In 1989, Weitzman became an Ernest E. Monrad Professor of Economics at Harvard University and has remained in this position for the last 18 years. He currently teaches two graduate courses:\nEc 2680 Environmental and Natural Resource Economics & Ec 2690 Environmental Economics and Policy Seminar.\n\nWeitzman serves as a consultant to The World Bank, Stanford Research Institute, International Monetary Fund, Agency for International Development, Arthur D. Little Co., Canadian Parliamentary Committee on Employment, Icelandic Committee on Natural Resources, National Academy Panel on Integrated Environmental and Economic Accounting.\n\nHe also serves as associate editor of the following publications: \"Journal of Comparative Economics\", \"Economic Letters\", \"Journal of Japanese and International Economies\", \"Journal of Environmental Economics and Management\".\n\n\nWeitzman has published over 90 papers, many of which have appeared in economics journals. Some of his recent papers are listed below.\n\n"}
{"id": "1131393", "url": "https://en.wikipedia.org/wiki?curid=1131393", "title": "Mawsynram", "text": "Mawsynram\n\nMawsynram () is a village in the East Khasi Hills district of Meghalaya state in north-eastern India, 65 kilometres from Shillong. Mawsynram receives one of the highest rainfalls in India. It is reportedly the wettest place on Earth, with an average annual rainfall of , but that claim is disputed by Lloró, Colombia, which reported an average yearly rainfall of between 1952 and 1989 and López de Micay, also in Colombia, which reported per year between 1960 and 2012. According to the \"Guinness Book of World Records\", Mawsynram received of rainfall in 1985.\n\nMawsynram is located at 25° 18′ N, 91° 35′ E, at an altitude of about 1,400 metres (4,600 ft), 15 km west of Cherrapunji, in the Khasi Hills in the state of Meghalaya (India) . The name of the village contains \"Maw\", a Khasi word meaning \"stone\".\n\nUnder the Köppen climate classification, Mawsynram features a subtropical highland climate with an extraordinarily showery and long monsoonal season and a short dry season. Based on the data of a recent few decades, it appears to be the wettest place in the world, or the place with the highest average annual rainfall. Mawsynram receives over 10,000 millimeters of rain in an average year, and the vast majority of rain it falls during the monsoon months. A comparison of rainfalls for Cherrapunji and Mawsynram for some years is given in Table 1.\n\nPrimarily due to the high altitude, it seldom gets truly hot in Mawsynram. Average monthly temperatures range from around 11°C in January to just above 20°C in August. The village also experiences a brief but noticeably drier season from December until February, when monthly precipitation on average does not exceed . The little precipitation during the village’s \"low sun\" season is something that is shared by many areas with this type of climate.\nTable 1: Comparison of rainfalls for Cherrapunji and Mawsynram between 1970 and 2010.\n\nThree reasons can be cited for high rainfall at Mawsynram:\n\nLocated in Mawsynram, is a cave named Mawjymbuin, known for its stalagmites. Inside this cave is a pair of notable speleothems - breast-shaped stalactite over a massive stalagmite. The area is known for its many caves, commercialised and non-commercialised, which contain remnants of a past that tells us of the history of the plateau that was once present.\n\n"}
{"id": "867221", "url": "https://en.wikipedia.org/wiki?curid=867221", "title": "Mercury Mountaineer", "text": "Mercury Mountaineer\n\nThe Mercury Mountaineer is a mid-size luxury sport utility vehicle (SUV) that was sold by Mercury from 1996 until 2010. Sharing many of its features with the Ford Explorer, the vehicles were virtually identical in terms of hardware. Externally, they were styled somewhat differently, and the Mountaineer was positioned with a more upscale interior, with the Mountaineer's MSRP coming in at $1,000–$6,000 more than the Explorer. It was last redesigned for the 2006 model year with a new frame, looking very similar to its previous model.\n\nSome controversy resulted after the media highlighted a number of rollovers involving Explorers and Mountaineers fitted with Firestone tires. The Mountaineer has been praised for its excellent handling and stability. The Mountaineer was never sold in Canada. As part of the discontinuation of the Mercury brand, production of the Mountaineer ended in late 2010.\n\nIn 1991, General Motors introduced the Oldsmobile Bravada sport-utility vehicle, derived from the four-door Chevrolet S-10 Blazer. Though far lower in price, the Bravada was marketed as a competitor to the Range Rover (and the later Land Rover Discovery) and Toyota Land Cruiser. While sharing its body with the Blazer, Oldsmobile differentiated the Bravada with the use of model-specific trim and a dedicated all-wheel drive powertrain (in place of part-time four-wheel drive). For 1993, Jeep briefly revived the long-running Grand Wagoneer nameplate as part of the Jeep Grand Cherokee model line, using woodgrain exterior trim and a leather interior; limited sales led to its cancellation after a single model year.\n\nAs a response to the Bravada and the Grand Wagoneer, Ford introduced the Ford Explorer Limited in 1993. In contrast to the outdoors-themed Explorer Eddie Bauer, the Limited was geared towards on-road driving; it was fitted with all-wheel drive in place of traditional four-wheel drive. The Limited was also distinguished by monochromatic body trim, body-color bumpers, and chrome wheels.\n\nAs part of the redesign of the Explorer for 1995, the Limited remained part of the Explorer lineup, with the segment gaining additional competitors through the use of badge engineering. For 1996, the Acura SLX (Isuzu Trooper), Infiniti QX4 (Nissan Pathfinder), and the larger Lexus LX450 (Toyota Land Cruiser) were introduced together.\n\nAs these brands, along with Oldsmobile, competed more directly with luxury brands than the Ford model line, Ford Motor Company sought to develop SUVs for its Lincoln-Mercury division. To minimize model overlap, Mercury was chosen to sell a version of the mid-size Ford Explorer, while Lincoln would sell a version of the then-upcoming full-size Ford Expedition.\n\nThe Mercury Mountaineer was began production in late 1996 as a 1997 model. As with the Ford Explorer Limited, the Mercury Mountaineer was offered only in a four-door body configuration. After its first year, sales of the Mountaineer fell short of Lincoln-Mercury sales projections. Following several revisions in 1997 and 1998, the Mountaineer would go on to become the third-best selling vehicle in the Mercury division, behind only the Sable and the Grand Marquis.\n\nAt its launch, the Mercury Mountaineer was closest in appearance to the Ford Explorer XLT, though trimmed between the Explorer Eddie Bauer and Explorer Limited. To differentiate it from its Ford counterpart, the Mountaineer was styled with a distinct dark-gray lower body color scheme; while visually similar to the Explorer, the Mountaineer adopted the chrome waterfall grille styling of the Grand Marquis. While the taillights were model-specific, the rear hatch and bumper were shared with the European-export version of the Explorer.\n\nAs part of a 1998 model revision, the Mountaineer was given a model-specific grille and headlights, larger wheels, and a new rear hatch design.\n\nThe 1997-2001 Mercury Mountaineer shares the chassis of the four-door Ford Explorer, following its 1995 redesign. Though heavily based upon the first-generation Ford Ranger, the sport-utility vehicles are wider and are based on a separate wheelbase. As with the Ford Explorer, the Mountaineer is fitted with fully independent wishbone front suspension.\n\nFor 1997, the Mercury Mountaineer was fitted with a 215hp 5.0L V8 with a 4-speed automatic transmission. In contrast to the Ford Explorer, the Mountaineer was available with rear-wheel drive or all-wheel drive; part-time four-wheel drive was not available. For 1998, a 205hp 4.0L V6 was added as an option.\n\nThe first-generation Mercury Mountaineer was introduced in a single trim level, offering many optional features of the Ford Explorer Eddie Bauer and Limited as standard. For 1998, Mercury introduced a V6-engined version of the Mountaineer to expand its price range. Though including a different powertrain, Mercury chose to offer largely the same features on both versions.\n\nFor the 2002 model year, the second-generation Mercury Mountaineer was introduced as a counterpart to the third-generation Ford Explorer. Previewed by a concept vehicle at the 2000 Los Angeles Auto Show, the 2002 Mercury Mountaineer marked the introduction of a new design language for the Mercury line, with style elements later appearing on the Mercury Monterey, Mercury Mariner, Mercury Montego, and Mercury Milan; along with a silver waterfall grille and silver-trimmed taillamps.\n\nAs part of the redesign, the Mountaineer was given further differentiation from the Explorer; while sharing the same roofline and doors, much of the lower sheetmetal was different, with the Mountaineer having different front fenders and hood, front and rear bumpers, liftgate, and taillamps.\n\nSharing a chassis with the third-generation Ford Explorer, the Mountaineer was fitted with four-wheel independent suspension. Sharing the 4.0L V6 of its predecessor, the second-generation Mountaineer also was fitted with the 4.6L V8 engine option of the Explorer, with four-wheel drive and all-wheel drive optional powertrain layouts; a 5-speed automatic transmission was standard.\n\nIn line with other Mercury models, the Mountaineer expanded from a single trim level to a base Convenience trim and a deluxe Premier trim; slotted above the Explorer Limited, the Mountaineer Premier offered features including a rear TV/DVD player, rear ceiling air vents, chrome exhaust tip and roof rack, and body-color bumpers.\n\nFor the 2006 model year, Ford redesigned its mid-size SUVs. While its U251 platform was all-new, the third-generation Mountaineer followed on with the success of its predecessor by retaining nearly its entire exterior, unlike the Explorer. For the 2006 Mountaineer, the exterior redesign featured all-white taillamps, turn signal repeaters on the front fenders, larger wheels, satin silver trim on the sideview mirrors and bumpers; the Mercury logos on the grille and tailgate were enlarged.\n\nSince the discontinuation of the Lincoln Aviator left the Mountaineer as the top nameplate of the Ford mid-size SUV model lineup, much of the attention of the redesign was focused in the interior trim and features to better differentiate it from the Explorer Eddie Bauer and Limited. Carried over from the Lincoln Aviator was the option of a DVD-based navigation system with voice control; this system would be unavailable on the Explorer until 2008. As an option, power retracting running boards (as seen on the Lincoln Navigator) were a new feature.\n\nAs with all Mountaineers since 2001, a 210 hp 4.0L SOHC V6 was the standard engine. As with the Explorer, the 292 hp 4.6L Modular 24-valve V8 was an option. V6 Mountaineers used a 5-speed 5R55W automatic transmission. For V8 models, Ford developed an all-new 6-speed transmission based on a ZF design; the 6R automatic was fitted to all V8 Explorers and Mountaineers.\n\nDuring its production, this generation saw relatively few functional changes. In 2008, side curtain airbags became standard. On the outside, the \"MOUNTAINEER\" lettering was deleted from the front doors. Due to reliability issues, the retracting running boards were discontinued. For 2009, versions of the Mountaineer configured for towing were upgraded as trailer sway control was made standard. To potentially save fuel for drivers, the navigation system was given upgrades, including traffic flow monitoring and live updates on gasoline prices from nearby service stations. For 2010, Ford's MyKey was added as a standard feature on all trim levels; it is a programmable security system designed for vehicles owned by multiple drivers.\n\nFollowing the June 2010 announcement by Ford Motor Company to shelve the Mercury brand, 2010 would be the end of Mountaineer production; the final vehicle was produced on October 1, 2010. Unlike the Milan, Mariner, and Grand Marquis, the Mountaineer was not produced for a short 2011 model year. The third-best selling vehicle of the division in 2000, the Mountaineer was the worst-selling Mercury ten years later. \n\nIn May 2000, the National Highway Traffic Safety Administration (NHTSA) contacted Ford and Firestone about the high incidence of tire failure on first generation Mercury Mountaineers, first and second generation Ford Explorers, and Mazda Navajo 3-doors fitted with Firestone tires. Ford investigated and found that several models of 15-inch (381 mm) Firestone tires (\"ATX\", \"ATX II\", and \"Wilderness AT\") had very high failure rates, especially those made at Firestone's Decatur, Illinois plant. To this day the Ford Motor Company refuses to equip any vehicle they sell with Firestones. Individuals can still opt to purchase tires aftermarket though.\n\n\n"}
{"id": "20018", "url": "https://en.wikipedia.org/wiki?curid=20018", "title": "Metric space", "text": "Metric space\n\nIn mathematics, a metric space is a set for which distances between all members of the set are defined. Those distances, taken together, are called a metric on the set. A metric on a space induces topological properties like open and closed sets, which lead to the study of more abstract topological spaces.\n\nThe most familiar metric space is 3-dimensional Euclidean space. In fact, a \"metric\" is the generalization of the Euclidean metric arising from the four long-known properties of the Euclidean distance. The Euclidean metric defines the distance between two points as the length of the straight line segment connecting them. Other metric spaces occur for example in elliptic geometry and hyperbolic geometry, where distance on a sphere measured by angle is a metric, and the hyperboloid model of hyperbolic geometry is used by special relativity as a metric space of velocities.\n\nIn 1906 Maurice Fréchet introduced metric spaces in his work \"Sur quelques points du calcul fonctionnel\". However the name is due to Felix Hausdorff.\n\nA metric space is an ordered pair formula_1 where formula_2 is a set and formula_3 is a metric on formula_2, i.e., a function\n\nsuch that for any formula_6, the following holds:\n\nThe first condition follows from the other three. Since for any formula_7:\n\nThe function formula_3 is also called \"distance function\" or simply \"distance\". Often, formula_3 is omitted and one just writes formula_2 for a metric space if it is clear from the context what metric is used.\n\nIgnoring mathematical details, for any system of roads and terrains the distance between two locations can be defined as the length of the shortest route connecting those locations. To be a metric there shouldn't be any one-way roads. The triangle inequality expresses the fact that detours aren't shortcuts. If the distance between two points is zero, the two points are indistinguishable from one-another. Many of the examples below can be seen as concrete versions of this general idea.\n\n\nEvery metric space is a topological space in a natural manner, and therefore all definitions and theorems about general topological spaces also apply to all metric spaces.\n\nAbout any point formula_14 in a metric space formula_2 we define the open ball of radius formula_77 (where formula_78 is a real number) about formula_14 as the set\nThese open balls form the base for a topology on \"M\", making it a topological space.\n\nExplicitly, a subset formula_81 of formula_2 is called open if for every formula_14 in formula_81 there exists an formula_77 such that formula_86 is contained in formula_81. The complement of an open set is called closed. A neighborhood of the point formula_14 is any subset of formula_2 that contains an open ball about formula_14 as a subset.\n\nA topological space which can arise in this way from a metric space is called a metrizable space; see the article on metrization theorems for further details.\n\nA sequence (formula_91) in a metric space formula_2 is said to converge to the limit formula_93 iff for every formula_94, there exists a natural number \"N\" such that formula_95 for all formula_96. Equivalently, one can use the general definition of convergence available in all topological spaces.\n\nA subset formula_68 of the metric space formula_2 is closed iff every sequence in formula_68 that converges to a limit in formula_2 has its limit in formula_68.\n\nA metric space formula_2 is said to be complete if every Cauchy sequence converges in formula_2. That is to say: if formula_104 as both formula_73 and formula_72 independently go to infinity, then there is some formula_107 with formula_108.\n\nEvery Euclidean space is complete, as is every closed subset of a complete space. The rational numbers, using the absolute value metric formula_109, are not complete.\n\nEvery metric space has a unique (up to isometry) completion, which is a complete space that contains the given space as a dense subset. For example, the real numbers are the completion of the rationals.\n\nIf formula_30 is a complete subset of the metric space formula_2, then formula_30 is closed in formula_2. Indeed, a space is complete iff it is closed in any containing metric space.\n\nEvery complete metric space is a Baire space.\n\nA metric space \"M\" is called bounded if there exists some number \"r\", such that \"d\"(\"x\",\"y\") ≤ \"r\" for all \"x\" and \"y\" in \"M\". The smallest possible such \"r\" is called the diameter of \"M\". The space \"M\" is called precompact or totally bounded if for every \"r\" > 0 there exist finitely many open balls of radius \"r\" whose union covers \"M\". Since the set of the centres of these balls is finite, it has finite diameter, from which it follows (using the triangle inequality) that every totally bounded space is bounded. The converse does not hold, since any infinite set can be given the discrete metric (one of the examples above) under which it is bounded and yet not totally bounded.\n\nNote that in the context of intervals in the space of real numbers and occasionally regions in a Euclidean space formula_114 a bounded set is referred to as \"a finite interval\" or \"finite region\". However boundedness should not in general be confused with \"finite\", which refers to the number of elements, not to how far the set extends; finiteness implies boundedness, but not conversely. Also note that an unbounded subset of formula_114 may have a finite volume.\n\nA metric space \"M\" is compact if every sequence in \"M\" has a subsequence that converges to a point in \"M\". This is known as sequential compactness and, in metric spaces (but not in general topological spaces), is equivalent to the topological notions of countable compactness and compactness defined via open covers.\n\nExamples of compact metric spaces include the closed interval [0,1] with the absolute value metric, all metric spaces with finitely many points, and the Cantor set. Every closed subset of a compact space is itself compact.\n\nA metric space is compact iff it is complete and totally bounded. This is known as the Heine–Borel theorem. Note that compactness depends only on the topology, while boundedness depends on the metric.\n\nLebesgue's number lemma states that for every open cover of a compact metric space \"M\", there exists a \"Lebesgue number\" δ such that every subset of \"M\" of diameter < δ is contained in some member of the cover.\n\nEvery compact metric space is second countable, and is a continuous image of the Cantor set. (The latter result is due to Pavel Alexandrov and Urysohn.)\n\nA metric space is said to be locally compact if every point has a compact neighborhood. Euclidean spaces are locally compact, but infinite-dimensional Banach spaces are not.\n\nA space is proper if every closed ball {\"y\" : \"d\"(\"x\",\"y\") ≤ \"r\"} is compact. Proper spaces are locally compact, but the converse is not true in general.\n\nA metric space formula_2 is connected if the only subsets that are both open and closed are the empty set and formula_2 itself.\n\nA metric space formula_2 is path connected if for any two points formula_7 there exists a continuous map formula_120 with formula_121 and formula_122.\nEvery path connected space is connected, but the converse is not true in general.\n\nThere are also local versions of these definitions: locally connected spaces and locally path connected spaces.\n\nSimply connected spaces are those that, in a certain sense, do not have \"holes\".\n\nA metric space is separable space if it has a countable dense subset. Typical examples are the real numbers or any Euclidean space. For metric spaces (but not for general topological spaces) separability is equivalent to second-countability and also to the Lindelöf property.\n\nIf formula_30 is a nonempty metric space and formula_124 then formula_125 is called a \"pointed metric space\", and formula_126 is called a \"distinguished point\". Note that a pointed metric space is just a nonempty metric space with attention drawn to its distinguished point, and that any nonempty metric space can be viewed as a pointed metric space. The distinguished point is sometimes denoted formula_23 due to its similar behavior to zero in certain contexts.\n\nSuppose (\"M\",\"d\") and (\"M\",\"d\") are two metric spaces.\n\nThe map \"f\":\"M\"→\"M\" is continuous\nif it has one (and therefore all) of the following equivalent properties:\n\nMoreover, \"f\" is continuous if and only if it is continuous on every compact subset of \"M\".\n\nThe image of every compact set under a continuous function is compact, and the image of every connected set under a continuous function is connected.\n\nThe map \"ƒ\" : \"M\" → \"M\" is uniformly continuous if for every \"ε\" > 0 there exists \"δ\" > 0 such that\n\nEvery uniformly continuous map \"ƒ\" : \"M\" → \"M\" is continuous. The converse is true if \"M\" is compact (Heine–Cantor theorem).\n\nUniformly continuous maps turn Cauchy sequences in \"M\" into Cauchy sequences in \"M\". For continuous maps this is generally wrong; for example, a continuous map\nfrom the open interval (0,1) \"onto\" the real line turns some Cauchy sequences into unbounded sequences.\n\nGiven a real number \"K\" > 0, the map \"ƒ\" : \"M\" → \"M\" is \"K\"-Lipschitz continuous if\n\nEvery Lipschitz-continuous map is uniformly continuous, but the converse is not true in general.\n\nIf \"K\" < 1, then \"ƒ\" is called a contraction. Suppose \"M\" = \"M\" and \"M\" is complete. If \"ƒ\" is a contraction, then \"ƒ\" admits a unique fixed point (Banach fixed point theorem). If \"M\" is compact, the condition can be weakened a bit: \"ƒ\" admits a unique fixed point if\n\nThe map \"f\":\"M\"→\"M\" is an isometry if\nIsometries are always injective; the image of a compact or complete set under an isometry is compact or complete, respectively. However, if the isometry is not surjective, then the image of a closed (or open) set need not be closed (or open).\n\nThe map \"f\" : \"M\" → \"M\" is a quasi-isometry if there exist constants \"A\" ≥ 1 and \"B\" ≥ 0 such that\n\nand a constant \"C\" ≥ 0 such that every point in \"M\" has a distance at most \"C\" from some point in the image \"f\"(\"M\").\n\nNote that a quasi-isometry is not required to be continuous. Quasi-isometries compare the \"large-scale structure\" of metric spaces; they find use in geometric group theory in relation to the word metric.\n\nGiven two metric spaces (\"M\", \"d\") and (\"M\", \"d\"):\n\n\nMetric spaces are paracompact Hausdorff spaces and hence normal (indeed they are perfectly normal). An important consequence is that every metric space admits partitions of unity and that every continuous real-valued function defined on a closed subset of a metric space can be extended to a continuous map on the whole space (Tietze extension theorem). It is also true that every real-valued Lipschitz-continuous map defined on a subset of a metric space can be extended to a Lipschitz-continuous map on the whole space.\n\nMetric spaces are first countable since one can use balls with rational radius as a neighborhood base.\n\nThe metric topology on a metric space \"M\" is the coarsest topology on \"M\" relative to which the metric \"d\" is a continuous map from the product of \"M\" with itself to the non-negative real numbers.\n\nA simple way to construct a function separating a point from a closed set (as required for a completely regular space) is to consider the distance between the point and the set. If (\"M\",\"d\") is a metric space, \"S\" is a subset of \"M\" and \"x\" is a point of \"M\", we define the distance from \"x\" to \"S\" as\n\nThen \"d\"(\"x\", \"S\") = 0 if and only if \"x\" belongs to the closure of \"S\". Furthermore, we have the following generalization of the triangle inequality:\nwhich in particular shows that the map formula_137 is continuous.\n\nGiven two subsets \"S\" and \"T\" of \"M\", we define their Hausdorff distance to be\n\nIn general, the Hausdorff distance \"d\"(\"S\",\"T\") can be infinite. Two sets are close to each other in the Hausdorff distance if every element of either set is close to some element of the other set.\n\nThe Hausdorff distance \"d\" turns the set \"K\"(\"M\") of all non-empty compact subsets of \"M\" into a metric space. One can show that \"K\"(\"M\") is complete if \"M\" is complete.\n\nOne can then define the Gromov–Hausdorff distance between any two metric spaces by considering the minimal Hausdorff distance of isometrically embedded versions of the two spaces. Using this distance, the class of all (isometry classes of) compact metric spaces becomes a metric space in its own right.\n\nIf formula_140 are metric spaces, and \"N\" is the Euclidean norm on \"R\", then formula_141 is a metric space, where the product metric is defined by\n\nand the induced topology agrees with the product topology. By the equivalence of norms in finite dimensions, an equivalent metric is obtained if \"N\" is the taxicab norm, a p-norm, the max norm, or any other norm which is non-decreasing as the coordinates of a positive \"n\"-tuple increase (yielding the triangle inequality).\n\nSimilarly, a countable product of metric spaces can be obtained using the following metric\n\nAn uncountable product of metric spaces need not be metrizable. For example, formula_144 is not first-countable and thus isn't metrizable.\n\nIn the case of a single space formula_1, the distance map formula_146 (from the definition) is uniformly continuous with respect to any of the above product metrics formula_147, and in particular is continuous with respect to the product topology of formula_148.\n\nIf \"M\" is a metric space with metric \"d\", and \"~\" is an equivalence relation on \"M\", then we can endow the quotient set \"M/~\" with the following (pseudo)metric. Given two equivalence classes [\"x\"] and [\"y\"], we define\n\nwhere the infimum is taken over all finite sequences formula_150 and formula_151 with formula_152, formula_153, formula_154. In general this will only define a pseudometric, i.e. formula_155 does not necessarily imply that formula_156. However, for nice equivalence relations (e.g., those given by gluing together polyhedra along faces), it is a metric.\n\nThe quotient metric \"d\" is characterized by the following universal property. If formula_157 is a metric map between metric spaces (that is, formula_158 for all \"x\", \"y\") satisfying \"f\"(\"x\")=\"f\"(\"y\") whenever formula_159 then the induced function formula_160, given by formula_161, is a metric map formula_162\n\nA topological space is sequential if and only if it is a quotient of a metric space.\n\n\nThe ordered set formula_164 can be seen as a category by requesting exactly one morphism formula_165 if formula_166 and none otherwise. By using formula_167 as the tensor product and formula_23 as the identity, it becomes a monoidal category formula_169.\nEvery metric space formula_1 can now be viewed as a category formula_171 enriched over formula_169:\n\nSee the paper by F.W. Lawvere listed below.\n\n\nThis is reprinted (with author commentary) at Reprints in Theory and Applications of Categories\nAlso (with an author commentary) in Enriched categories in the logic of geometry and analysis. Repr. Theory Appl. Categ. No. 1 (2002), 1–37.\n\n"}
{"id": "3779015", "url": "https://en.wikipedia.org/wiki?curid=3779015", "title": "Mount Livadiyskaya", "text": "Mount Livadiyskaya\n\nMount Livadiyskaya ( or ), unofficially known as Mount Pedan () or Mount Pidan () is one of the highest peaks in Shkotovsky District of Primorsky Krai, Russia.\n\nLivadiyskaya is a part of the Livadiysky Range of the Sikhote-Alin.\n\nThe history of the mountain is often mystified, due to the presence of megaliths. These attract adepts of different neopagan and other faiths.\n\nThe mountain is a popular hiking spot, which has seen a surge in the number of ascensions in recent years. This has resulted in worsening contamination.\n"}
{"id": "53663583", "url": "https://en.wikipedia.org/wiki?curid=53663583", "title": "Multilayer medium", "text": "Multilayer medium\n\nIn the physical sciences, a multilayer or stratified medium is a stack of different thin films. Typically, a multilayer is man made for a specific purpose. Since layers are thin with respect to some relevant length scale, interface effects are much more important than in bulk materials, giving rise to novel physical properties.\n\nThe term \"multilayer\" is \"not\" an extension of \"monolayer\" and \"bilayer\", which describe a \"single\" layer that is one or two molecules thick. A multilayer medium rather consists of several thin films.\n\nAn optical coating, as used for instance in a dielectric mirror, is made of several layers that have different refractive indexes.\n\nGiant magnetoresistance is a macroscopic quantum effect observed in alternating ferromagnetic and non-magnetic conductive layers.\n"}
{"id": "51025099", "url": "https://en.wikipedia.org/wiki?curid=51025099", "title": "Norther Offshore Wind Farm", "text": "Norther Offshore Wind Farm\n\nNorther is an offshore wind farm to be built in the Belgian North Sea, within the Exclusive Economic Zone of Belgium, approximately 23 kilometres from the Belgian port of Ostend. The concession was granted at the end of 2009, which is when the actual development of the project began.\n\nThe wind farm will comprise 44 Vestas V164-8.4 MW turbines on monopile foundations for a nameplate capacity of 369.6 MW with a projected annual production of 1,394 GWh, corresponding to a capacity factor of 43.1% and the average consumption of almost 400,000 households. The wind farm will span an area of 38 km with water depths up to 33 m.\n\nThe Norther offshore wind farm is being developed through a 50-50 joint venture between Elicio NV, a Belgian renewable energy producer operating internationally, and the Dutch utility company Eneco, who is a major producer and supplier of renewable electricity, natural gas and heat in the Netherlands and Belgium.\n\nOffshore wind is expected to be a key element of Belgium's future energy mix and is projected to represent around 10% of total generated electricity by 2025. Norther Wind is expected to be commissioned in 2019.\n\n\n"}
{"id": "14659295", "url": "https://en.wikipedia.org/wiki?curid=14659295", "title": "Oil megaprojects (2007)", "text": "Oil megaprojects (2007)\n\nThis page summarizes projects that brought more than of new liquid fuel capacity to market with the first production of fuel beginning in 2007. This is part of the Wikipedia summary of Oil Megaprojects.\n\nTerminology\n"}
{"id": "922636", "url": "https://en.wikipedia.org/wiki?curid=922636", "title": "PET bottle recycling", "text": "PET bottle recycling\n\nBottles made of polyethylene terephthalate (PET, sometimes PETE) can be used to make lower grade products, such as carpets. To make a food grade plastic, the bottles need to be hydrolysed down to monomers, which are purified and then re-polymerised to make new PET. \nIn many countries, PET plastics are coded with the resin identification code number \"1\" inside the universal recycling symbol, usually located on the bottom of the container.\n\nPET is used as a raw material for making packaging materials such as bottles and containers for packaging a wide range of food products and other consumer goods. Examples include soft drinks, alcoholic beverages, detergents, cosmetics, pharmaceutical products and edible oils. PET is one of the most common consumer plastics used. Polyethylene terephthalate can also be used as the main material in making water-resistant paper.\n\nThe empty PET packaging is discarded by the consumer, after use and becomes PET waste. In the recycling industry, this is referred to as \"post-consumer PET.\" Many local governments and waste collection agencies have started to collect post-consumer PET separately from other household waste. Besides that there is container deposit legislation in some countries which also applies to PET bottles.\n\nIt is debatable whether exporting circulating resources that damages the domestic recycling industry is acceptable or not. In Japan, overseas market pressure led to a significant cost reduction in the domestic market. The cost of the plastics other than PET bottles remained high.\n\nWhen the PET bottles are returned to an authorized redemption center, or to the original seller in some jurisdictions, the deposit is partly or fully refunded to the redeemer. In both cases the collected post-consumer PET is taken to recycling centres known as materials recovery facilities (MRF) where it is sorted and separated from other materials such as metal, objects made out of other rigid plastics such as PVC, HDPE, polypropylene, flexible plastics such as those used for bags (generally low density polyethylene), drink cartons, glass, and anything else which is not made out of PET.\n\nPost-consumer PET is often sorted into different colour fractions: transparent or uncoloured PET, blue and green coloured PET, and the remainder into a mixed colours fraction. The emergence of new colours (such as amber for plastic beer bottles) further complicates the sorting process for the recycling industry.\n\nThe sorted post-consumer PET waste is crushed, pressed into bales and offered for sale to recycling companies. Colourless/light blue post-consumer PET attracts higher sales prices than the darker blue and green fractions. The mixed color fraction is the least valuable.\n\nThe further treatment process includes crushing, washing, separating and drying.\nRecycling companies further treat the post-consumer PET by shredding the material into small fragments. These fragments still contain residues of the original content, shredded paper labels and plastic caps. These are removed by plastic granulation, resulting in pure PET fragments, or \"PET flakes\". PET flakes are used as the raw material for a range of products that would otherwise be made of polyester. Examples include polyester fibres (a base material for the production of clothing, pillows, carpets, etc.), polyester sheets, strapping, or back into PET bottles.\n\nMelt filtration is typically used to remove contaminants from polymer melts during the extrusion process. There is a mechanical separation of the contaminants within a machine called a ‘screen changer’. A typical system will consist of a steel housing with the filtration medium contained in moveable pistons or slide plates that enable the processor to remove the screens from the extruder flow without stopping production. The contaminants are usually collected on woven wire screens which are supported on a stainless steel plate called a ‘breaker plate’—a strong circular piece of steel drilled with large holes to allow the flow of the polymer melt. For the recycling of polyester it is typical to integrate a screen changer into the extrusion line. This can be in a pelletizing, sheet extrusion or strapping tape extrusion line.\n\nPET polymer is very sensitive to hydrolytic degradation, resulting in severe reduction in its molecular weight, thereby adversely affecting its subsequent melt processability. Therefore, it is essential to dry the PET flakes or granules to a very low moisture level prior to melt extrusion.\n\nPET must be dried to <100 parts per million (ppm) moisture and maintained at this moisture level to minimize hydrolysis during melt processing.\n\nDehumidifying Drying - These types of dryers circulate hot and de-humidified dry air onto the resin, suck the air back, dry it and then pump again in a closed loop operation. This process reduces moisture level in the PET down to 50ppm or lower. The efficiency of moisture removal depends on the air dew point. If the air dew point is not good, then some moisture remains in the chips and cause IV loss during processing.\nInfrared Drying polyester pellets and flakes - A new type of dryer has been introduced in recent years, using Infrared drying (IRD). Due to the high rate of energy transfer with IR heating in combination with the specific wavelength used, the energy costs involved with these systems can be greatly reduced, along with the size. Polyester can be dried and amorphous flake crystallized and dried within only about 15 minutes down to a moisture level of approx. 300ppm in one step, and down to <50 ppm using a buffer hopper to complete the drying in typically under 1 hour\n\nWorldwide, approximately 7.5 million tons of PET were collected in 2011. This gave 5.9 million tons of flake. In 2009 3.4 million tons were used to produce fibre, 500,000 tons to produce bottles, 500,000 tons to produce APET sheet for thermoforming, 200,000 tons to produce strapping tape and 100,000 tons for miscellaneous applications.\n\nPetcore, the European trade association that fosters the collection and recycling of PET, reported that in Europe alone, 1.6 million tonnes of PET bottles were collected in 2011 - more than 51% of all bottles. After exported bales were taken into account, 1.12 million tons of PET flake were produced. 440,000 tons were used to produce fibres, 283,000 tons to produce more bottles, 278,000 tons to produce APET sheets, 102,000 tons for strapping tape and 18,000 tons for miscellaneous applications. (Source: PCI for Petcore and EuPR)\n\nIn 2008 the amount of post-consumer PET bottles collected for recycling and sold in the United States was approx. 1.45 billion pounds.\n\nIn 2012, 81% of the PET bottles sold in Switzerland were recycled.\n\nIncreasing energy prices may increase the volume of recycling PET bottles. In Europe, the EU Waste Framework Directive mandates that by 2020 there should be 50% recycling or reuse of plastics from household streams.\n\nIn the United States the recycling rate for PET packaging was 31.2% in 2013, according to a report from The National Association for PET Container Resources (NAPCOR) and The Association of Postconsumer Plastic Recyclers (APR). A total of 1,798 million pounds was collected and 475 million pounds of recycled PET used out of a total of 5,764 million pounds of PET bottles.\n\nPET bottles recycle-rate globally\nPET bottles are also recycled as-is (re-used) for various purposes, including for use in school projects, and for use in solar water disinfection in developing nations, in which empty PET bottles are filled with water and left in the sun to allow disinfection by ultraviolet radiation. PET is useful for this purpose because many other materials (including window glass) that are transparent to visible light are opaque to ultraviolet radiation.\n\nA novel use is as a building material in third-world countries. According to online sources, the bottles, in a labor-intensive process, are filled with sand, then stacked and either mudded or cemented together to form a wall. Some of the bottles can be filled instead with air or water, to admit light into the structure.\n\n"}
{"id": "67029", "url": "https://en.wikipedia.org/wiki?curid=67029", "title": "Passive solar building design", "text": "Passive solar building design\n\nIn passive solar building design, windows, walls, and floors are made to collect, store, reflect, and distribute solar energy in the form of heat in the winter and reject solar heat in the summer. This is called passive solar design because, unlike active solar heating systems, it does not involve the use of mechanical and electrical devices.\n\nThe key to design a passive solar building is to best take advantage of the local climate performing an accurate site analysis. Elements to be considered include window placement and size, and glazing type, thermal insulation, thermal mass, and shading. Passive solar design techniques can be applied most easily to new buildings, but existing buildings can be adapted or \"retrofitted\".\n\n\"Passive solar\" technologies use sunlight without active mechanical systems (as contrasted to active solar). Such technologies convert sunlight into usable heat (in water, air, and thermal mass), cause air-movement for ventilating, or future use, with little use of other energy sources. A common example is a solarium on the equator-side of a building. Passive cooling is the use of the same design principles to reduce summer cooling requirements.\n\nSome passive systems use a small amount of conventional energy to control dampers, shutters, night insulation, and other devices that enhance solar energy collection, storage, and use, and reduce undesirable heat transfer.\n\nPassive solar technologies include direct and indirect solar gain for space heating, solar water heating systems based on the thermosiphon, use of thermal mass and phase-change materials for slowing indoor air temperature swings, solar cookers, the solar chimney for enhancing natural ventilation, and earth sheltering.\n\nMore widely, passive solar technologies include the solar furnace, but this typically requires some external energy for aligning their concentrating mirrors or receivers, and historically have not proven to be practical or cost effective for widespread use. 'Low-grade' energy needs, such as space and water heating, have proven over time to be better applications for passive use of solar energy.\n\nThe scientific basis for passive solar building design has been developed from a combination of climatology, thermodynamics (particularly heat transfer: conduction (heat), convection, and electromagnetic radiation), fluid mechanics/natural convection (passive movement of air and water without the use of electricity, fans or pumps), and human thermal comfort based on heat index, psychrometrics and enthalpy control for buildings to be inhabited by humans or animals, sunrooms, solariums, and greenhouses for raising plants.\n\nSpecific attention is divided into: the site, location and solar orientation of the building, local sun path, the prevailing level of insolation (latitude/sunshine/clouds/precipitation), design and construction quality/materials, placement/size/type of windows and walls, and incorporation of solar-energy-storing thermal mass with heat capacity.\n\nWhile these considerations may be directed toward any building, achieving an ideal optimized cost/performance solution requires careful, holistic, system integration engineering of these scientific principles. Modern refinements through computer modeling (such as the comprehensive U.S. Department of Energy \"Energy Plus\" building energy simulation software), and application of decades of lessons learned (since the 1970s energy crisis) can achieve significant energy savings and reduction of environmental damage, without sacrificing functionality or aesthetics. In fact, passive-solar design features such as a greenhouse/sunroom/solarium can greatly enhance the livability, daylight, views, and value of a home, at a low cost per unit of space.\n\nMuch has been learned about passive solar building design since the 1970s energy crisis. Many unscientific, intuition-based expensive construction experiments have attempted and failed to achieve zero energy – the total elimination of heating-and-cooling energy bills.\n\nPassive solar building construction may not be difficult or expensive (using off-the-shelf existing materials and technology), but the scientific passive solar building design is a non-trivial engineering effort that requires significant study of previous counter-intuitive lessons learned, and time to enter, evaluate, and iteratively refine the simulation input and output.\n\nOne of the most useful post-construction evaluation tools has been the use of thermography using digital thermal imaging cameras for a formal quantitative scientific energy audit. Thermal imaging can be used to document areas of poor thermal performance such as the negative thermal impact of roof-angled glass or a skylight on a cold winter night or hot summer day.\n\nThe scientific lessons learned over the last three decades have been captured in sophisticated comprehensive building energy simulation computer software systems (like U.S. DOE Energy Plus).\n\nScientific passive solar building design with quantitative cost benefit product optimization is not easy for a novice. The level of complexity has resulted in ongoing bad-architecture, and many intuition-based, unscientific construction experiments that disappoint their designers and waste a significant portion of their construction budget on inappropriate ideas.\n\nThe economic motivation for scientific design and engineering is significant. If it had been applied comprehensively to new building construction beginning in 1980 (based on 1970s lessons learned), America could be saving over $250,000,000 per year on expensive energy and related pollution today.\n\nSince 1979, Passive Solar Building Design has been a critical element of achieving zero energy by educational institution experiments, and governments around the world, including the U.S. Department of Energy, and the energy research scientists that they have supported for decades. The cost effective proof of concept was established decades ago, but cultural assimilation into architecture, construction trades, and building-owner decision making has been very slow and difficult to change.\n\nThe new terms \"Architectural Science\" and \"Architectural Technology\" are being added to some schools of Architecture, with a future goal of teaching the above scientific and energy-engineering principles.\n\nThe ability to achieve these goals simultaneously is fundamentally dependent on the seasonal variations in the sun's path throughout the day.\n\nThis occurs as a result of the inclination of the Earth's axis of rotation in relation to its orbit. The sun path is unique for any given latitude.\n\nIn Northern Hemisphere non-tropical latitudes farther than 23.5 degrees from the equator:\n\n\nThe converse is observed in the Southern Hemisphere, but the sun rises to the east and sets toward the west regardless of which hemisphere you are in.\n\nIn equatorial regions at less than 23.5 degrees, the position of the sun at solar noon will oscillate from north to south and back again during the year.\n\nIn regions closer than 23.5 degrees from either north-or-south pole, during summer the sun will trace a complete circle in the sky without setting whilst it will never appear above the horizon six months later, during the height of winter.\n\nThe 47-degree difference in the altitude of the sun at solar noon between winter and summer forms the basis of passive solar design. This information is combined with local climatic data (degree day) heating and cooling requirements to determine at what time of the year solar gain will be beneficial for thermal comfort, and when it should be blocked with shading. By strategic placement of items such as glazing and shading devices, the percent of solar gain entering a building can be controlled throughout the year.\n\nOne passive solar sun path design problem is that although the sun is in the same relative position six weeks before, and six weeks after, the solstice, due to \"thermal lag\" from the thermal mass of the Earth, the temperature and solar gain requirements are quite different before and after the summer or winter solstice. Movable shutters, shades, shade screens, or window quilts can accommodate day-to-day and hour-to-hour solar gain and insulation requirements.\n\nCareful arrangement of rooms completes the passive solar design. A common recommendation for residential dwellings is to place living areas facing solar noon and sleeping quarters on the opposite side. A heliodon is a traditional movable light device used by architects and designers to help model sun path effects. In modern times, 3D computer graphics can visually simulate this data, and calculate performance predictions.\n\nPersonal thermal comfort is a function of personal health factors (medical, psychological, sociological and situational), ambient air temperature, mean radiant temperature, air movement (wind chill, turbulence) and relative humidity (affecting human evaporative cooling). Heat transfer in buildings occurs through convection, conduction, and thermal radiation through roof, walls, floor and windows.\n\nConvective heat transfer can be beneficial or detrimental. Uncontrolled air infiltration from poor weatherization / weatherstripping / draft-proofing can contribute up to 40% of heat loss during winter; however, strategic placement of operable windows or vents can enhance convection, cross-ventilation, and summer cooling when the outside air is of a comfortable temperature and relative humidity. Filtered energy recovery ventilation systems may be useful to eliminate undesirable humidity, dust, pollen, and microorganisms in unfiltered ventilation air.\n\nNatural convection causing rising warm air and falling cooler air can result in an uneven stratification of heat. This may cause uncomfortable variations in temperature in the upper and lower conditioned space, serve as a method of venting hot air, or be designed in as a natural-convection air-flow loop for passive solar heat distribution and temperature equalization. Natural human cooling by perspiration and evaporation may be facilitated through natural or forced convective air movement by fans, but ceiling fans can disturb the stratified insulating air layers at the top of a room, and accelerate heat transfer from a hot attic, or through nearby windows. In addition, high relative humidity inhibits evaporative cooling by humans.\n\nThe main source of heat transfer is radiant energy, and the primary source is the sun. Solar radiation occurs predominantly through the roof and windows (but also through walls). Thermal radiation moves from a warmer surface to a cooler one. Roofs receive the majority of the solar radiation delivered to a house. A cool roof, or green roof in addition to a radiant barrier can help prevent your attic from becoming hotter than the peak summer outdoor air temperature (see albedo, absorptivity, emissivity, and reflectivity).\n\nWindows are a ready and predictable site for thermal radiation.\nEnergy from radiation can move into a window in the day time, and out of the same window at night. Radiation uses photons to transmit electromagnetic waves through a vacuum, or translucent medium. Solar heat gain can be significant even on cold clear days. Solar heat gain through windows can be reduced by insulated glazing, shading, and orientation. Windows are particularly difficult to insulate compared to roof and walls. Convective heat transfer through and around window coverings also degrade its insulation properties. When shading windows, external shading is more effective at reducing heat gain than internal window coverings.\n\nWestern and eastern sun can provide warmth and lighting, but are vulnerable to overheating in summer if not shaded. In contrast, the low midday sun readily admits light and warmth during the winter, but can be easily shaded with appropriate length overhangs or angled louvres during summer and leaf bearing summer shade trees which shed their leaves in the fall. The amount of radiant heat received is related to the location latitude, altitude, cloud cover, and seasonal / hourly angle of incidence (see Sun path and Lambert's cosine law).\n\nAnother passive solar design principle is that thermal energy can be stored in certain building materials and released again when heat gain eases to stabilize diurnal (day/night) temperature variations. The complex interaction of thermodynamic principles can be counterintuitive for first-time designers. Precise computer modeling can help avoid costly construction experiments.\n\n\n\nThe precise amount of equator-facing glass and thermal mass should be based on careful consideration of latitude, altitude, climatic conditions, and heating/cooling degree day requirements.\n\nFactors that can degrade thermal performance:\n\n\nTechnically, PSH is highly efficient. Direct-gain systems can utilize (i.e. convert into \"useful\" heat) 65–70% of the energy of solar radiation that strikes the aperture or collector.\n\nPassive solar fraction (PSF) is the percentage of the required heat load met by PSH and hence represents potential reduction in heating costs. RETScreen International has reported a PSF of 20–50%. Within the field of sustainability, energy conservation even of the order of 15% is considered substantial.\n\nOther sources report the following PSFs:\n\n\nIn favorable climates such as the southwest United States, highly optimized systems can exceed 75% PSF.\n\nFor more information see Solar Air Heat\n\nThere are three primary passive solar energy configurations:\n\nIn a direct-gain passive solar system, the indoor space acts as a solar collector, heat absorber, and distribution system. South-facing glass in the northern hemisphere(north-facing in the southern hemisphere) admits solar energy into the building interior where it directly heats (radiant energy absorption) or indirectly heats (through convection) thermal mass in the building such as concrete or masonry floors and walls. The floors and walls acting as thermal mass are incorporated as functional parts of the building and temper the intensity of heating during the day. At night, the heated thermal mass radiates heat into the indoor space.\n\nIn cold climates, a sun-tempered building is the most basic type of direct gain passive solar configuration that simply involves increasing (slightly) the south-facing glazing area, without adding additional thermal mass. It is a type of direct-gain system in which the building envelope is well insulated, is elongated in an east–west direction, and has a large fraction (~80% or more) of the windows on the south side. It has little added thermal mass beyond what is already in the building (i.e., just framing, wall board, and so forth). In a sun-tempered building, the south-facing window area should be limited to about 5 to 7% of the total floor area, less in a sunny climate, to prevent overheating. Additional south-facing glazing can be included only if more thermal mass is added. Energy savings are modest with this system, and sun tempering is very low cost.\n\nIn genuine direct gain passive solar systems, sufficient thermal mass is required to prevent large temperature fluctuations in indoor air; more thermal mass is required than in a sun tempered building. Overheating of the building interior can result with insufficient or poorly designed thermal mass. About one-half to two-thirds of the interior surface area of the floors, walls and ceilings must be constructed of thermal storage materials. Thermal storage materials can be concrete, adobe, brick, and water. Thermal mass in floors and walls should be kept as bare as is functionally and aesthetically possible; thermal mass needs to be exposed to direct sunlight. Wall-to-wall carpeting, large throw rugs, expansive furniture, and large wall hangings should be avoided.\n\nTypically, for about every 1 ft of south-facing glass, about 5 to 10 ft of thermal mass is required for thermal mass (1 m per 5 to 10 m). When accounting for minimal-to-average wall and floor coverings and furniture, this typically equates to about 5 to 10 ft per ft (5 to 10 m per m) of south-facing glass, depending upon whether the sunlight strikes the surface directly. The simplest rule of thumb is that thermal mass area should have an area of 5 to 10 times the surface area of the direct-gain collector (glass) area.\n\nSolid thermal mass (e.g., concrete, masonry, stone, etc.) should be relatively thin, no more than about 4 in (100 mm) thick. Thermal masses with large exposed areas and those in direct sunlight for at least part of the day (2 hour minimum) perform best. Medium-to-dark, colors with high absorptivity, should be used on surfaces of thermal mass elements that will be in direct sunlight. Thermal mass that is not in contact with sunlight can be any color. Lightweight elements (e.g., drywall walls and ceilings) can be any color. Covering the glazing with tight-fitting, moveable insulation panels during dark, cloudy periods and nighttime hours will greatly enhance performance of a direct-gain system. Water contained within plastic or metal containment and placed in direct sunlight heats more rapidly and more evenly than solid mass due to natural convection heat transfer. The convection process also prevents surface temperatures from becoming too extreme as they sometimes do when dark colored solid mass surfaces receive direct sunlight.\n\nDepending on climate and with adequate thermal mass, south-facing glass area in a direct gain system should be limited to about 10 to 20% of the floor area (e.g., 10 to 20 ft of glass for a 100 ft floor area). This should be based on the net glass or glazing area. Note that most windows have a net glass/glazing area that is 75 to 85% of the overall window unit area. Above this level, problems with overheating, glare and fading of fabrics are likely.\n\nIn an indirect-gain passive solar system, the thermal mass (concrete, masonry, or water) is located directly behind the south-facing glass and in front of the heated indoor space and so there is no direct heating The position of the mass prevents sunlight from entering the indoor space and can also obstruct the view through the glass. There are two types of indirect gain systems: thermal storage wall systems and roof pond systems.\n\nThermal Storage (Trombe) Walls\n\nIn a thermal storage wall system, often called a Trombe wall, a massive wall is located directly behind south-facing glass, which absorbs solar energy and releases it selectively towards the building interior at night. The wall can be constructed of cast-in-place concrete, brick, adobe, stone, or solid (or filled) concrete masonry units. Sunlight enters through the glass and is immediately absorbed at the surface of the mass wall and either stored or conducted through the material mass to the inside space. The thermal mass cannot absorb solar energy as fast as it enters the space between the mass and the window area. Temperatures of the air in this space can easily exceed 120 °F (49 °C). This hot air can be introduced into interior spaces behind the wall by incorporating heat-distributing vents at the top of the wall. This wall system was first envisioned and patented in 1881 by its inventor, Edward Morse. Felix Trombe, for whom this system is sometimes named, was a French engineer who built several homes using this design in the French Pyrenees in the 1960s.\n\nA thermal storage wall typically consists of a 4 to 16 in (100 to 400 mm) thick masonry wall coated with a dark, heat-absorbing finish (or a selective surface) and covered with a single or double layer of high transmissivity glass. The glass is typically placed from ¾ in to 2 in from the wall to create a small airspace. In some designs, the mass is located 1 to 2 ft (0.6 m) away from the glass, but the space is still not usable. The surface of the thermal mass absorbs the solar radiation that strikes it and stores it for nighttime use. Unlike a direct gain system, the thermal storage wall system provides passive solar heating without excessive window area and glare in interior spaces. However, the ability to take advantage of views and daylighting are eliminated. The performance of Trombe walls is diminished if the wall interior is not open to the interior spaces. Furniture, bookshelves and wall cabinets installed on the interior surface of the wall will reduce its performance.\n\nA classical Trombe wall, also generically called a vented thermal storage wall, has operable vents near the ceiling and floor levels of the mass wall that allow indoor air to flow through them by natural convection. As solar radiation heats the air trapped between the glass and wall and it begins to rise. Air is drawn into the lower vent, then into the space between the glass and wall to get heated by solar radiation, increasing its temperature and causing it to rise, and then exit through the top (ceiling) vent back into the indoor space. This allows the wall to directly introduce heated air into the space; usually at a temperature of about 90 °F (32 °C).\n\nIf vents are left open at night (or on cloudy days), a reversal of convective airflow will occur, wasting heat by dissipating it outdoors. Vents must be closed at night so radiant heat from the interior surface of the storage wall heats the indoor space. Generally, vents are also closed during summer months when heat gain is not needed. During the summer, an exterior exhaust vent installed at the top of the wall can be opened to vent to the outside. Such venting makes the system act as a solar chimney driving air through the building during the day.\n\nVented thermal storage walls vented to the interior have proven somewhat ineffective, mostly because they deliver too much heat during the day in mild weather and during summer months; they simply overheat and create comfort issues. Most solar experts recommended that thermal storage walls should not be vented to the interior.\n\nThere are many variations of the Trombe wall system. An unvented thermal storage wall (technically not a Trombe wall) captures solar energy on the exterior surface, heats up, and conducts heat to the interior surface, where it radiates from the interior wall surface to the indoor space later in the day. A water wall uses a type of thermal mass that consists of tanks or tubes of water used as thermal mass.\n\nA typical unvented thermal storage wall consists of a south facing masonry or concrete wall with a dark, heat-absorbing material on the exterior surface and faced with a single or double layer of glass. High transmission glass maximizes solar gains to the mass wall. The glass is placed from ¾ to 6 in. (20 to 150 mm) from the wall to create a small airspace. Glass framing is typically metal (e.g., aluminum) because vinyl will soften and wood will become super dried at the 180 °F (82 °C) temperature that can exist behind the glass in the wall. Heat from sunlight passing through the glass is absorbed by the dark surface, stored in the wall, and conducted slowly inward through the masonry. As an architectural detail, patterned glass can limit the exterior visibility of the wall without sacrificing solar transmissivity.\n\nA water wall uses containers of water for thermal mass instead of a solid mass wall. Water walls are typically slightly more efficient than solid mass walls because they absorb heat more efficiently due to the development of convective currents in the liquid water as it is heated. These currents cause rapid mixing and quicker transfer of heat into the building than can be provided by the solid mass walls.\n\nTemperature variations between the exterior and interior wall surfaces drive heat through the mass wall. Inside the building, however, daytime heat gain is delayed, only becoming available at the interior surface of the thermal mass during the evening when it is needed because the sun has set. The time lag is the time difference between when sunlight first strikes the wall and when the heat enters the building interior. Time lag is contingent upon the type of material used in the wall and the wall thickness; a greater thickness yields a greater time lag. The time lag characteristic of thermal mass, combined with dampening of temperature fluctuations, allows the use of varying daytime solar energy as a more uniform night-time heat source. Windows can be placed in the wall for natural lighting or aesthetic reasons, but this tends to lower the efficiency somewhat.\n\nThe thickness of a thermal storage wall should be approximately 10 to 14 in (250 to 350 mm) for brick, 12 to 18 in (300 to 450 mm) for concrete, 8 to 12 in (200 to 300 mm) for earth/adobe, and at least 6 in (150 mm) for water. These thicknesses delay movement of heat such that indoor surface temperatures peak during late evening hours. Heat will take about 8 to 10 hours to reach the interior of the building (heat travels through a concrete wall at rate of about one inch per hour). A good thermal connection between the inside wall finishes (e.g., drywall) and the thermal mass wall is necessary to maximize heat transfer to the interior space.\n\nAlthough the position of a thermal storage wall minimizes daytime overheating of the indoor space, a well-insulated building should be limited to approximately 0.2 to 0.3 ft of thermal mass wall surface per ft of floor area being heated (0.2 to 0.3 m per m of floor area), depending upon climate. A water wall should have about 0.15 to 0.2 ft of water wall surface per ft (0.15 to 0.2 m per m) of floor area.\n\nThermal mass walls are best-suited to sunny winter climates that have high diurnal (day-night) temperature swings (e.g., southwest, mountain-west). They do not perform as well in cloudy or extremely cold climates or in climates where there is not a large diurnal temperature swing. Nighttime thermal losses through the thermal mass of the wall can still be significant in cloudy and cold climates; the wall loses stored heat in less than a day, and then leak heat, which dramatically raises backup heating requirements. Covering the glazing with tight-fitting, moveable insulation panels during lengthy cloudy periods and nighttime hours will enhance performance of a thermal storage system.\n\nThe main drawback of thermal storage walls is their heat loss to the outside. Double glass (glass or any of the plastics) is necessary for reducing heat loss in most climates. In mild climates, single glass is acceptable. A selective surface (high-absorbing/low-emitting surface) applied to the exterior surface of the thermal storage wall improves performance by reducing the amount of infrared energy radiated back through the glass; typically, it achieves a similar improvement in performance without the need for daily installation and removal of insulating panels. A selective surface consists of a sheet of metal foil glued to the outside surface of the wall. It absorbs almost all the radiation in the visible portion of the solar spectrum and emits very little in the infrared range. High absorbency turns the light into heat at the wall's surface, and low emittance prevents the heat from radiating back towards the glass.\n\nRoof Pond System\n\nA roof pond\" \"passive solar system, sometimes called a solar roof, uses water stored on the roof to temper hot and cold internal temperatures, usually in desert environments. It typically is constructed of containers holding 6 to 12 in (150 to 300 mm) of water on a flat roof. Water is stored in large plastic bags or fiberglass containers to maximize radiant emissions and minimize evaporation. It can be left unglazed or can be covered by glazing. Solar radiation heats the water, which acts as a thermal storage medium. At night or during cloudy weather, the containers can be covered with insulating panels. The indoor space below the roof pond is heated by thermal energy emitted by the roof pond storage above. These systems require good drainage systems, movable insulation, and an enhanced structural system to support a 35 to 70 lb/ft (1.7 to 3.3 kN/m) dead load.\n\nWith the angles of incidence of sunlight during the day, roof ponds are only effective for heating at lower and mid-latitudes, in hot to temperate climates. Roof pond systems perform better for cooling in hot, low humidity climates. Not many solar roofs have been built, and there is limited information on the design, cost, performance, and construction details of thermal storage roofs.\n\nIn an \"isolated gain passive solar system,\" the components (e.g., collector and thermal storage) are isolated from the indoor area of the building.\n\nAn attached sunspace, also sometimes called a solar room or solarium, is a type of isolated gain solar system with a glazed interior space or room that is part of or attached to a building but which can be completely closed off from the main occupied areas. It functions like an attached greenhouse that makes use of a combination of direct-gain and indirect-gain system characteristics. A sunspace may be called and appear like a greenhouse, but a greenhouse is designed to grow plants whereas a sunspace is designed to provide heat and aesthetics to a building. Sunspaces are very popular passive design elements because they expand the living areas of a building and offer a room to grow plants and other vegetation. In moderate and cold climates, however, supplemental space heating is required to keep plants from freezing during extremely cold weather.\n\nAn attached sunspace’s south-facing glass collects solar energy as in a direct-gain system. The simplest sunspace design is to install vertical windows with no overhead glazing. Sunspaces may experience high heat gain and high heat loss through their abundance of glazing. Although horizontal and sloped glazing collects more heat in the winter, it is minimized to prevent overheating during summer months. Although overhead glazing can be aesthetically pleasing, an insulated roof provides better thermal performance. Skylights can be used to provide some daylighting potential. Vertical glazing can maximize gain in winter, when the angle of the sun is low, and yield less heat gain during the summer. Vertical glass is less expensive, easier to install and insulate, and not as prone to leaking, fogging, breaking, and other glass failures. A combination of vertical glazing and some sloped glazing is acceptable if summer shading is provided. A well-designed overhang may be all that is necessary to shade the glazing in the summer.\n\nThe temperature variations caused by the heat losses and gains can be moderated by thermal mass and low-emissivity windows. Thermal mass can include a masonry floor, a masonry wall bordering the house, or water containers. Distribution of heat to the building can be accomplished through ceiling and floor level vents, windows, doors, or fans. In a common design, thermal mass wall situated on the back of the sunspace adjacent to the living space will function like an indirect-gain thermal mass wall. Solar energy entering the sunspace is retained in the thermal mass. Solar heat is conveyed into the building by conduction through the shared mass wall in the rear of the sunspace and by vents (like an unvented thermal storage wall) or through openings in the wall that permit airflow from the sunspace to the indoor space by convection (like a vented thermal storage wall).\n\nIn cold climates, double glazing should be used to reduce conductive losses through the glass to the outside. Night-time heat loss, although significant during winter months, is not as essential in the sunspace as with direct gain systems since the sunspace can be closed off from the rest of the building. In temperate and cold climates, thermally isolating the sunspace from the building at night is important. Large glass panels, French doors, or sliding glass doors between the building and attached sunspace will maintain an open feeling without the heat loss associated with an open space.\n\nA sunspace with a masonry thermal wall will need approximately 0.3 ft of thermal mass wall surface per ft of floor area being heated (0.3 m per m of floor area), depending on climate. Wall thicknesses should be similar to a thermal storage wall. If a water wall is used between the sunspace and living space, about 0.20 ft of thermal mass wall surface per ft of floor area being heated (0.2 m per m of floor area) is appropriate. In most climates, a ventilation system is required in summer months to prevent overheating. Generally, vast overhead (horizontal) and east- and west-facing glass areas should not be used in a sunspace without special precautions for summer overheating such as using heat-reflecting glass and providing summer-shading systems areas.\n\nThe internal surfaces of the thermal mass should be dark in color. Movable insulation (e.g., window coverings, shades, shutters) can be used help trap the warm air in the sunspace both after the sun has set and during cloudy weather. When closed during extremely hot days, window coverings can help keep the sunspace from overheating.\n\nTo maximize comfort and efficiency, the non-glass sunspace walls, ceiling and foundation should be well insulated. The perimeter of the foundation wall or slab should be insulated to the frost line or around the slab perimeter. In a temperate or cold climate, the east and west walls of the sunspace should be insulated (no glass).\n\nMeasures should be taken to reduce heat loss at night e.g. window coverings or movable window insulation.\n\nThe sun doesn't shine all the time. Heat storage, or thermal mass, keeps the building warm when the sun can't heat it.\n\nIn diurnal solar houses, the storage is designed for one or a few days. The usual method is a custom-constructed thermal mass. This includes a Trombe wall, a ventilated concrete floor, a cistern, water wall or roof pond. It is also feasible to use the thermal mass of the earth itself, either as-is or by incorporation into the structure by banking or using rammed earth as a structural medium.\n\nIn subarctic areas, or areas that have long terms without solar gain (e.g. weeks of freezing fog), purpose-built thermal mass is very expensive. Don Stephens pioneered an experimental technique to use the ground as thermal mass large enough for annualized heat storage. His designs run an isolated thermosiphon 3 m under a house, and insulate the ground with a 6 m waterproof skirt.\n\nThermal insulation or superinsulation (type, placement and amount) reduces unwanted leakage of heat. Some passive buildings are actually constructed of insulation.\n\nThe effectiveness of direct solar gain systems is significantly enhanced by insulative (e.g. double glazing), spectrally selective glazing (low-e), or movable window insulation (window quilts, bifold interior insulation shutters, shades, etc.).\n\nGenerally, Equator-facing windows should not employ glazing coatings that inhibit solar gain.\n\nThere is extensive use of super-insulated windows in the German Passive House standard. Selection of different spectrally selective window coating depends on the ratio of heating versus cooling degree days for the design location.\n\nThe requirement for vertical equator-facing glass is different from the other three sides of a building. Reflective window coatings and multiple panes of glass can reduce useful solar gain. However, direct-gain systems are more dependent on double or triple glazing to reduce heat loss. Indirect-gain and isolated-gain configurations may still be able to function effectively with only single-pane glazing. Nevertheless, the optimal cost-effective solution is both location and system dependent.\n\nSkylights admit harsh direct overhead sunlight and glare either horizontally (a flat roof) or pitched at the same angle as the roof slope. In some cases, horizontal skylights are used with reflectors to increase the intensity of solar radiation (and harsh glare), depending on the roof angle of incidence. When the winter sun is low on the horizon, most solar radiation reflects off of roof angled glass ( the angle of incidence is nearly parallel to roof-angled glass morning and afternoon ). When the summer sun is high, it is nearly perpendicular to roof-angled glass, which maximizes solar gain at the wrong time of year, and acts like a solar furnace. Skylights should be covered and well-insulated to reduce natural convection ( warm air rising ) heat loss on cold winter nights, and intense solar heat gain during hot spring/summer/fall days.\n\nThe equator-facing side of a building is south in the northern hemisphere, and north in the southern hemisphere. Skylights on roofs that face away from the equator provide mostly indirect illumination, except for summer days when the sun may rise on the non-equator side of the building (at some latitudes). Skylights on east-facing roofs provide maximum direct light and solar heat gain in the summer morning. West-facing skylights provide afternoon sunlight and heat gain during the hottest part of the day.\n\nSome skylights have expensive glazing that partially reduces summer solar heat gain, while still allowing some visible light transmission. However, if visible light can pass through it, so can some radiant heat gain (they are both electromagnetic radiation waves).\n\nYou can partially reduce some of the unwanted roof-angled-glazing summer solar heat gain by installing a skylight in the shade of deciduous (leaf-shedding) trees, or by adding a movable insulated opaque window covering on the inside or outside of the skylight. This would eliminate the daylight benefit in the summer. If tree limbs hang over a roof, they will increase problems with leaves in rain gutters, possibly cause roof-damaging ice dams, shorten roof life, and provide an easier path for pests to enter your attic. Leaves and twigs on skylights are unappealing, difficult to clean, and can increase the glazing breakage risk in wind storms.\n\n\"Sawtooth roof glazing\" with vertical-glass-only can bring some of the passive solar building design benefits into the core of a commercial or industrial building, without the need for any roof-angled glass or skylights.\n\nSkylights provide daylight. The only view they provide is essentially straight up in most applications. Well-insulated light tubes can bring daylight into northern rooms, without using a skylight. A passive-solar greenhouse provides abundant daylight for the equator-side of the building.\n\nInfrared thermography color thermal imaging cameras ( used in formal energy audits ) can quickly document the negative thermal impact of roof-angled glass or a skylight on a cold winter night or hot summer day.\n\nThe U.S. Department of Energy states: \"vertical glazing is the overall best option for sunspaces.\" Roof-angled glass and sidewall glass are not recommended for passive solar sunspaces.\n\nThe U.S. DOE explains drawbacks to roof-angled glazing: Glass and plastic have little structural strength. When installed vertically, glass (or plastic) bears its own weight because only a small area (the top edge of the glazing) is subject to gravity. As the glass tilts off the vertical axis, however, an increased area (now the sloped cross-section) of the glazing has to bear the force of gravity. Glass is also brittle; it does not flex much before breaking. To counteract this, you usually must increase the thickness of the glazing or increase the number of structural supports to hold the glazing. Both increase overall cost, and the latter will reduce the amount of solar gain into the sunspace.\n\nAnother common problem with sloped glazing is its increased exposure to the weather. It is difficult to maintain a good seal on roof-angled glass in intense sunlight. Hail, sleet, snow, and wind may cause material failure. For occupant safety, regulatory agencies usually require sloped glass to be made of safety glass, laminated, or a combination thereof, which reduce solar gain potential. Most of the roof-angled glass on the Crowne Plaza Hotel Orlando Airport sunspace was destroyed in a single windstorm. Roof-angled glass increases construction cost, and can increase insurance premiums. Vertical glass is less susceptible to weather damage than roof-angled glass.\n\nIt is difficult to control solar heat gain in a sunspace with sloped glazing during the summer and even during the middle of a mild and sunny winter day. Skylights are the antithesis of zero energy building Passive Solar Cooling in climates with an air conditioning requirement.\n\nThe amount of solar gain transmitted through glass is also affected by the angle of the incident solar radiation. Sunlight striking a single sheet of glass within 45 degrees of perpendicular is mostly transmitted (less than 10% is reflected), whereas for sunlight striking at 70 degrees from perpendicular over 20% of light is reflected, and above 70 degrees this percentage reflected rises sharply.\n\nAll of these factors can be modeled more precisely with a photographic light meter and a heliodon or optical bench, which can quantify the ratio of reflectivity to transmissivity, based on angle of incidence.\n\nAlternatively, passive solar computer software can determine the impact of sun path, and cooling-and-heating degree days on energy performance.\n\nA design with too much equator-facing glass can result in excessive winter, spring, or fall day heating, uncomfortably bright living spaces at certain times of the year, and excessive heat transfer on winter nights and summer days.\n\nAlthough the sun is at the same altitude 6-weeks before and after the solstice, the heating and cooling requirements before and after the solstice are significantly different. Heat storage on the Earth's surface causes \"thermal lag.\" Variable cloud cover influences solar gain potential. This means that latitude-specific fixed window overhangs, while important, are not a complete seasonal solar gain control solution.\n\nControl mechanisms (such as manual-or-motorized interior insulated drapes, shutters, exterior roll-down shade screens, or retractable awnings) can compensate for differences caused by thermal lag or cloud cover, and help control daily / hourly solar gain requirement variations.\nHome automation systems that monitor temperature, sunlight, time of day, and room occupancy can precisely control motorized window-shading-and-insulation devices.\n\nMaterials and colors can be chosen to reflect or absorb solar thermal energy. Using information on a Color for electromagnetic radiation to determine its thermal radiation properties of reflection or absorption can assist the choices.<br>See Lawrence Berkeley National Laboratory and Oak Ridge National Laboratory: \"Cool Colors\"\n\nEnergy-efficient landscaping materials for careful passive solar choices include hardscape building material and \"softscape\" plants. The use of landscape design principles for selection of trees, hedges, and trellis-pergola features with vines; all can be used to create summer shading. For winter solar gain it is desirable to use deciduous plants that drop their leaves in the autumn gives year round passive solar benefits. Non-deciduous evergreen shrubs and trees can be windbreaks, at variable heights and distances, to create protection and shelter from winter wind chill. Xeriscaping with 'mature size appropriate' native species of-and drought tolerant plants, drip irrigation, mulching, and organic gardening practices reduce or eliminate the need for energy-and-water-intensive irrigation, gas powered garden equipment, and reduces the landfill waste footprint. Solar powered landscape lighting and fountain pumps, and covered swimming pools and plunge pools with solar water heaters can reduce the impact of such amenities.\n\nPassive solar lighting techniques enhance taking advantage of natural illumination for interiors, and so reduce reliance on artificial lighting systems.\n\nThis can be achieved by careful building design, orientation, and placement of window sections to collect light. Other creative solutions involve the use of reflecting surfaces to admit daylight into the interior of a building. Window sections should be adequately sized, and to avoid over-illumination can be shielded with a Brise soleil, awnings, well placed trees, glass coatings, and other passive and active devices.\n\nAnother major issue for many window systems is that they can be potentially vulnerable sites of excessive thermal gain or heat loss. Whilst high mounted clerestory window and traditional skylights can introduce daylight in poorly oriented sections of a building, unwanted heat transfer may be hard to control. Thus, energy that is saved by reducing artificial lighting is often more than offset by the energy required for operating HVAC systems to maintain thermal comfort.\n\nVarious methods can be employed to address this including but not limited to window coverings, insulated glazing and novel materials such as aerogel semi-transparent insulation, optical fiber embedded in walls or roof, or hybrid solar lighting at Oak Ridge National Laboratory.\n\nReflecting elements, from active and passive daylighting collectors, such as light shelves, lighter wall and floor colors, mirrored wall sections, interior walls with upper glass panels, and clear or translucent glassed hinged doors and sliding glass doors take the captured light and passively reflect it further inside. The light can be from passive windows or skylights and solar light tubes or from active daylighting sources. In traditional Japanese architecture the Shōji sliding panel doors, with translucent Washi screens, are an original precedent. International style, Modernist and Mid-century modern architecture were earlier innovators of this passive penetration and reflection in industrial, commercial, and residential applications.\n\nThere are many ways to use solar thermal energy to heat water for domestic use. Different active-and-passive solar hot water technologies have different location-specific economic cost benefit analysis implications.\n\nFundamental passive solar hot water heating involves no pumps or anything electrical. It is very cost effective in climates that do not have lengthy sub-freezing, or very-cloudy, weather conditions. Other active solar water heating technologies, etc. may be more appropriate for some locations.\n\nIt is possible to have active solar hot water which is also capable of being \"off grid\" and qualifies as sustainable. This is done by the use of a photovoltaic cell which uses energy from the sun to power the pumps.\n\nThere is growing momentum in Europe for the approach espoused by the Passive House (\"Passivhaus\" in German) Institute in Germany. Rather than relying solely on traditional passive solar design techniques, this approach seeks to make use of all passive sources of heat, minimises energy usage, and emphasises the need for high levels of insulation reinforced by meticulous attention to detail in order to address thermal bridging and cold air infiltration. Most of the buildings built to the Passive House standard also incorporate an active heat recovery ventilation unit with or without a small (typically 1 kW) incorporated heating component.\n\nThe energy design of Passive House buildings is developed using a spreadsheet-based modeling tool called the Passive House Planning Package (PHPP) which is updated periodically. The current version is PHPP2007, where 2007 is the year of issue. A building may be certified as a \"Passive House\" when it can be shown that it meets certain criteria, the most important being that the annual specific heat demand for the house should not exceed 15kWh/ma.\n\nTraditionally a heliodon was used to simulate the altitude and azimuth of the sun shining on a model building at any time of any day of the year. In modern times, computer programs can model this phenomenon and integrate local climate data (including site impacts such as overshadowing and physical obstructions) to predict the solar gain potential for a particular building design over the course of a year. GPS-based smartphone applications can now do this inexpensively on a hand held device. These design tools provide the passive solar designer the ability to evaluate local conditions, design elements and orientation prior to construction. Energy performance optimization normally requires an iterative-refinement design-and-evaluate process. There is no such thing as a \"one-size-fits-all\" universal passive solar building design that would work well in all locations.\n\nMany detached suburban houses can achieve reductions in heating expense without obvious changes to their appearance, comfort or usability. This is done using good siting and window positioning, small amounts of thermal mass, with good-but-conventional insulation, weatherization, and an occasional supplementary heat source, such as a central radiator connected to a (solar) water heater. Sunrays may fall on a wall during the daytime and raise the temperature of its thermal mass. This will then radiate heat into the building in the evening. External shading, or a radiant barrier plus air gap, may be used to reduce undesirable summer solar gain.\n\nAn extension of the \"passive solar\" approach to seasonal solar capture and storage of heat and cooling. These designs attempt to capture warm-season solar heat, and convey it to a seasonal thermal store for use months later during the cold season (\"annualised passive solar.\") Increased storage is achieved by employing large amounts of thermal mass or earth coupling. Anecdotal reports suggest they can be effective but no formal study has been conducted to demonstrate their superiority. The approach also can move cooling into the warm season. Examples:\n\n\nA \"purely passive\" solar-heated house would have no mechanical furnace unit, relying instead on energy captured from sunshine, only supplemented by \"incidental\" heat energy given off by lights, computers, and other task-specific appliances (such as those for cooking, entertainment, etc.), showering, people and pets. The use of natural convection air currents (rather than mechanical devices such as fans) to circulate air is related, though not strictly solar design. Passive solar building design sometimes uses limited electrical and mechanical controls to operate dampers, insulating shutters, shades, awnings, or reflectors. Some systems enlist small fans or solar-heated chimneys to improve convective air-flow. A reasonable way to analyse these systems is by measuring their coefficient of performance. A heat pump might use 1 J for every 4 J it delivers giving a COP of 4. A system that only uses a 30 W fan to more-evenly distribute 10 kW of solar heat through an entire house would have a COP of 300.\n\nPassive solar building design is often a foundational element of a cost-effective zero energy building. Although a ZEB uses multiple passive solar building design concepts, a ZEB is usually not purely passive, having active mechanical renewable energy generation systems such as: wind turbine, photovoltaics, micro hydro, geothermal, and other emerging alternative energy sources.\n\nThere has been recent interest in the utilization of the large amounts of surface area on skyscrapers to improve their overall energy efficiency. Because skyscrapers are increasingly ubiquitous in urban environments, yet require large amounts of energy to operate, there is potential for large amounts of energy savings employing passive solar design techniques. One study, which analyzed the proposed 22 Bishopsgate tower in London, found that a 35% energy decrease in demand can theoretically be achieved through indirect solar gains, by rotating the building to achieve optimum ventilation and daylight penetration, usage of high thermal mass flooring material to decrease temperature fluctuation inside the building, and using double or triple glazed low emissivity window glass for direct solar gain. Indirect solar gain techniques included moderating wall heat flow by variations of wall thickness (from 20 to 30 cm), using window glazing on the outdoor space to prevent heat loss, dedicating 15–20% of floor area for thermal storage, and implementing a Trombe wall to absorb heat entering the space. Overhangs are used to block direct sunlight in the summer, and allow it in the winter, and heat reflecting blinds are inserted between the thermal wall and the glazing to limit heat build-up in the summer months.\n\nAnother study analyzed double-green skin facade (DGSF) on the outside of high rise buildings in Hong Kong. Such a green facade, or vegetation covering the outer walls, can combat the usage of air conditioning greatly - as much as 80%, as discovered by the researchers.\n\nIn more temperate climates, strategies such as glazing, adjustment of window-to-wall ratio, sun shading and roof strategies can offer considerable energy savings, in the 30% to 60% range.\n\n\n"}
{"id": "23535", "url": "https://en.wikipedia.org/wiki?curid=23535", "title": "Photon", "text": "Photon\n\nThe photon is a type of elementary particle, the quantum of the electromagnetic field including electromagnetic radiation such as light, and the force carrier for the electromagnetic force (even when static via virtual particles). The photon has zero rest mass and always moves at the speed of light within a vacuum.\n\nLike all elementary particles, photons are currently best explained by quantum mechanics and exhibit wave–particle duality, exhibiting properties of both waves and particles. For example, a single photon may be refracted by a lens and exhibit wave interference with itself, and it can behave as a particle with definite and finite measurable position or momentum, though not both at the same time. The photon's wave and quantum qualities are two observable aspects of a single phenomenon – they cannot be described by any mechanical model; a representation of this dual property of light that assumes certain points on the wavefront to be the seat of the energy is not possible. The quanta in a light wave are not spatially localized. \n\nThe modern concept of the photon was developed gradually by Albert Einstein in the early 20th century to explain experimental observations that did not fit the classical wave model of light. The benefit of the photon model is that it accounts for the frequency dependence of light's energy, and explains the ability of matter and electromagnetic radiation to be in thermal equilibrium. The photon model accounts for anomalous observations, including the properties of black-body radiation, that others (notably Max Planck) had tried to explain using \"semiclassical models\". In that model, light is described by Maxwell's equations, but material objects emit and absorb light in \"quantized\" amounts (i.e., they change energy only by certain particular discrete amounts). Although these semiclassical models contributed to the development of quantum mechanics, many further experiments beginning with the phenomenon of Compton scattering of single photons by electrons, validated Einstein's hypothesis that \"light itself\" is quantized. In 1926 the optical physicist Frithiof Wolfers and the chemist Gilbert N. Lewis coined the name \"photon\" for these particles. After Arthur H. Compton won the Nobel Prize in 1927 for his scattering studies, most scientists accepted that light quanta have an independent existence, and the term \"photon\" was accepted.\n\nIn the Standard Model of particle physics, photons and other elementary particles are described as a necessary consequence of physical laws having a certain symmetry at every point in spacetime. The intrinsic properties of particles, such as charge, mass, and spin, are determined by this gauge symmetry. The photon concept has led to momentous advances in experimental and theoretical physics, including lasers, Bose–Einstein condensation, quantum field theory, and the probabilistic interpretation of quantum mechanics. It has been applied to photochemistry, high-resolution microscopy, and measurements of molecular distances. Recently, photons have been studied as elements of quantum computers, and for applications in optical imaging and optical communication such as quantum cryptography.\n\nThe word \"quanta\" (singular \"quantum,\" Latin for \"how much\") was used before 1900 to mean particles or amounts of different quantities, including electricity. In 1900, the German physicist Max Planck was studying black-body radiation: he suggested that the experimental observations would be explained if the energy carried by electromagnetic waves could only be released in \"packets\" of energy. In his 1901 article in \"Annalen der Physik\" he called these packets \"energy elements\". In 1905, Albert Einstein published a paper in which he proposed that many light-related phenomena—including black-body radiation and the photoelectric effect—would be better explained by modelling electromagnetic waves as consisting of spatially localized, discrete wave-packets. He called such a wave-packet \"the light quantum\" (German: \"das Lichtquant\"). The name \"photon\" derives from the Greek word for light, \"\" (transliterated \"phôs\"). Arthur Compton used \"photon\" in 1928, referring to Gilbert N. Lewis. The same name was used earlier, by the American physicist and psychologist Leonard T. Troland, who coined the word in 1916, in 1921 by the Irish physicist John Joly, in 1924 by the French physiologist René Wurmser (1890–1993) and in 1926 by the French physicist Frithiof Wolfers (1891–1971). The name was suggested initially as a unit related to the illumination of the eye and the resulting sensation of light and was used later in a physiological context. Although Wolfers's and Lewis's theories were contradicted by many experiments and never accepted, the new name was adopted very soon by most physicists after Compton used it.\n\nIn physics, a photon is usually denoted by the symbol \"γ\" (the Greek letter gamma). This symbol for the photon probably derives from gamma rays, which were discovered in 1900 by Paul Villard, named by Ernest Rutherford in 1903, and shown to be a form of electromagnetic radiation in 1914 by Rutherford and Edward Andrade. In chemistry and optical engineering, photons are usually symbolized by \"hν\", which is the photon energy, where \"h\" is Planck constant and the Greek letter \"ν\" (nu) is the photon's frequency. Much less commonly, the photon can be symbolized by \"hf\", where its frequency is denoted by \"f\".\n\nA photon is massless, has no electric charge, and is a stable particle. A photon has two possible polarization states. In the momentum representation of the photon, which is preferred in quantum field theory, a photon is described by its wave vector, which determines its wavelength \"λ\" and its direction of propagation. A photon's wave vector may not be zero and can be represented either as a spatial 3-vector or as a (relativistic) four-vector; in the latter case it belongs to the light cone (pictured). Different signs of the four-vector denote different circular polarizations, but in the 3-vector representation one should account for the polarization state separately; it actually is a spin quantum number. In both cases the space of possible wave vectors is three-dimensional.\n\nThe photon is the gauge boson for electromagnetism, and therefore all other quantum numbers of the photon (such as lepton number, baryon number, and flavour quantum numbers) are zero. Also, the photon does not obey the Pauli exclusion principle.\n\nPhotons are emitted in many natural processes. For example, when a charge is accelerated it emits synchrotron radiation. During a molecular, atomic or nuclear transition to a lower energy level, photons of various energy will be emitted, ranging from radio waves to gamma rays. Photons can also be emitted when a particle and its corresponding antiparticle are annihilated (for example, electron–positron annihilation).\n\nIn empty space, the photon moves at \"c\" (the speed of light) and its energy and momentum are related by , where \"p\" is the magnitude of the momentum vector p. This derives from the following relativistic relation, with :\n\nThe energy and momentum of a photon depend only on its frequency (\"ν\") or inversely, its wavelength (\"λ\"):\n\nwhere k is the wave vector (where the wave number ), is the angular frequency, and is the reduced Planck constant.\n\nSince p points in the direction of the photon's propagation, the magnitude of the momentum is\n\nThe photon also carries a quantity called spin angular momentum that does not depend on its frequency. The magnitude of its spin is \"ħ\" and the component measured along its direction of motion, its helicity, must be ±\"ħ\". These two possible helicities, called right-handed and left-handed, correspond to the two possible circular polarization states of the photon.\n\nTo illustrate the significance of these formulae, the annihilation of a particle with its antiparticle in free space must result in the creation of at least \"two\" photons for the following reason. In the center of momentum frame, the colliding antiparticles have no net momentum, whereas a single photon always has momentum (since, as we have seen, it is determined by the photon's frequency or wavelength, which cannot be zero). Hence, conservation of momentum (or equivalently, translational invariance) requires that at least two photons are created, with zero net momentum. (However, it is possible if the system interacts with another particle or field for the annihilation to produce one photon, as when a positron annihilates with a bound atomic electron, it is possible for only one photon to be emitted, as the nuclear Coulomb field breaks translational symmetry.) The energy of the two photons, or, equivalently, their frequency, may be determined from conservation of four-momentum. Seen another way, the photon can be considered as its own antiparticle. The reverse process, pair production, is the dominant mechanism by which high-energy photons such as gamma rays lose energy while passing through matter. That process is the reverse of \"annihilation to one photon\" allowed in the electric field of an atomic nucleus.\n\nThe classical formulae for the energy and momentum of electromagnetic radiation can be re-expressed in terms of photon events. For example, the pressure of electromagnetic radiation on an object derives from the transfer of photon momentum per unit time and unit area to that object, since pressure is force per unit area and force is the change in momentum per unit time.\n\nEach photon carries two distinct and independent forms of angular momentum of light. The spin angular momentum of light of a particular photon is always either +\"ħ\" or −\"ħ\".\nThe light orbital angular momentum of a particular photon can be any integer \"N\", including zero.\n\nCurrent commonly accepted physical theories imply or assume the photon to be strictly massless. If the photon is not a strictly massless particle, it would not move at the exact speed of light, \"c\", in vacuum. Its speed would be lower and depend on its frequency. Relativity would be unaffected by this; the so-called speed of light, \"c\", would then not be the actual speed at which light moves, but a constant of nature which is the upper bound on speed that any object could theoretically attain in spacetime. Thus, it would still be the speed of spacetime ripples (gravitational waves and gravitons), but it would not be the speed of photons.\n\nIf a photon did have non-zero mass, there would be other effects as well. Coulomb's law would be modified and the electromagnetic field would have an extra physical degree of freedom. These effects yield more sensitive experimental probes of the photon mass than the frequency dependence of the speed of light. If Coulomb's law is not exactly valid, then that would allow the presence of an electric field to exist within a hollow conductor when it is subjected to an external electric field. This thus allows one to test Coulomb's law to very high precision. A null result of such an experiment has set a limit of .\n\nSharper upper limits on the speed of light have been obtained in experiments designed to detect effects caused by the galactic vector potential. Although the galactic vector potential is very large because the galactic magnetic field exists on very great length scales, only the magnetic field would be observable if the photon is massless. In the case that the photon has mass, the mass term \"m'A'A\" would affect the galactic plasma. The fact that no such effects are seen implies an upper bound on the photon mass of . The galactic vector potential can also be probed directly by measuring the torque exerted on a magnetized ring. Such methods were used to obtain the sharper upper limit of (the equivalent of ) given by the Particle Data Group.\n\nThese sharp limits from the non-observation of the effects caused by the galactic vector potential have been shown to be model-dependent. If the photon mass is generated via the Higgs mechanism then the upper limit of from the test of Coulomb's law is valid.\n\nPhotons inside superconductors do develop a nonzero effective rest mass; as a result, electromagnetic forces become short-range inside superconductors.\n\nIn most theories up to the eighteenth century, light was pictured as being made up of particles. Since particle models cannot easily account for the refraction, diffraction and birefringence of light, wave theories of light were proposed by René Descartes (1637), Robert Hooke (1665), and Christiaan Huygens (1678); however, particle models remained dominant, chiefly due to the influence of Isaac Newton. In the early nineteenth century, Thomas Young and August Fresnel clearly demonstrated the interference and diffraction of light and by 1850 wave models were generally accepted. In 1865, James Clerk Maxwell's prediction that light was an electromagnetic wave—which was confirmed experimentally in 1888 by Heinrich Hertz's detection of radio waves—seemed to be the final blow to particle models of light.\nThe Maxwell wave theory, however, does not account for \"all\" properties of light. The Maxwell theory predicts that the energy of a light wave depends only on its intensity, not on its frequency; nevertheless, several independent types of experiments show that the energy imparted by light to atoms depends only on the light's frequency, not on its intensity. For example, some chemical reactions are provoked only by light of frequency higher than a certain threshold; light of frequency lower than the threshold, no matter how intense, does not initiate the reaction. Similarly, electrons can be ejected from a metal plate by shining light of sufficiently high frequency on it (the photoelectric effect); the energy of the ejected electron is related only to the light's frequency, not to its intensity.\n\nAt the same time, investigations of blackbody radiation carried out over four decades (1860–1900) by various researchers culminated in Max Planck's hypothesis that the energy of \"any\" system that absorbs or emits electromagnetic radiation of frequency \"ν\" is an integer multiple of an energy quantum . As shown by Albert Einstein, some form of energy quantization \"must\" be assumed to account for the thermal equilibrium observed between matter and electromagnetic radiation; for this explanation of the photoelectric effect, Einstein received the 1921 Nobel Prize in physics.\n\nSince the Maxwell theory of light allows for all possible energies of electromagnetic radiation, most physicists assumed initially that the energy quantization resulted from some unknown constraint on the matter that absorbs or emits the radiation. In 1905, Einstein was the first to propose that energy quantization was a property of electromagnetic radiation itself. Although he accepted the validity of Maxwell's theory, Einstein pointed out that many anomalous experiments could be explained if the \"energy\" of a Maxwellian light wave were localized into point-like quanta that move independently of one another, even if the wave itself is spread continuously over space. In 1909 and 1916, Einstein showed that, if Planck's law of black-body radiation is accepted, the energy quanta must also carry momentum , making them full-fledged particles. This photon momentum was observed experimentally by Arthur Compton, for which he received the Nobel Prize in 1927. The pivotal question was then: how to unify Maxwell's wave theory of light with its experimentally observed particle nature? The answer to this question occupied Albert Einstein for the rest of his life, and was solved in quantum electrodynamics and its successor, the Standard Model (see ' and ', below).\n\nUnlike Planck, Einstein entertained the possibility that there might be actual physical quanta of light—what we now call photons. He noticed that a light quantum with energy proportional to its frequency would explain a number of troubling puzzles and paradoxes, including an unpublished law by Stokes, the ultraviolet catastrophe, and the photoelectric effect. Stokes's law said simply that the frequency of fluorescent light cannot be greater than the frequency of the light (usually ultraviolet) inducing it. Einstein eliminated the ultraviolet catastrophe by imagining a gas of photons behaving like a gas of electrons that he had previously considered. He was advised by a colleague to be careful how he wrote up this paper, in order to not challenge Planck, a powerful figure in physics, too directly, and indeed the warning was justified, as Planck never forgave him for writing it.\n\nEinstein's 1905 predictions were verified experimentally in several ways in the first two decades of the 20th century, as recounted in Robert Millikan's Nobel lecture. However, before Compton's experiment showed that photons carried momentum proportional to their wave number (1922), most physicists were reluctant to believe that electromagnetic radiation itself might be particulate. (See, for example, the Nobel lectures of Wien, Planck and Millikan.) Instead, there was a widespread belief that energy quantization resulted from some unknown constraint on the matter that absorbed or emitted radiation. Attitudes changed over time. In part, the change can be traced to experiments such as Compton scattering, where it was much more difficult not to ascribe quantization to light itself to explain the observed results.\n\nEven after Compton's experiment, Niels Bohr, Hendrik Kramers and John Slater made one last attempt to preserve the Maxwellian continuous electromagnetic field model of light, the so-called BKS model. To account for the data then available, two drastic hypotheses had to be made:\n\n\nHowever, refined Compton experiments showed that energy–momentum is conserved extraordinarily well in elementary processes; and also that the jolting of the electron and the generation of a new photon in Compton scattering obey causality to within 10 ps. Accordingly, Bohr and his co-workers gave their model \"as honorable a funeral as possible\". Nevertheless, the failures of the BKS model inspired Werner Heisenberg in his development of matrix mechanics.\n\nA few physicists persisted in developing semiclassical models in which electromagnetic radiation is not quantized, but matter appears to obey the laws of quantum mechanics. Although the evidence from chemical and physical experiments for the existence of photons was overwhelming by the 1970s, this evidence could not be considered as \"absolutely\" definitive; since it relied on the interaction of light with matter, and a sufficiently complete theory of matter could in principle account for the evidence. Nevertheless, \"all\" semiclassical theories were refuted definitively in the 1970s and 1980s by photon-correlation experiments. Hence, Einstein's hypothesis that quantization is a property of light itself is considered to be proven.\n\nPhotons, like all quantum objects, exhibit wave-like and particle-like properties. Their dual wave–particle nature can be difficult to visualize. The photon displays clearly wave-like phenomena such as diffraction and interference on the length scale of its wavelength. For example, a single photon passing through a double-slit experiment exhibits interference phenomena but only if no measure was made at the slit. A single photon passing through a double-slit experiment lands on the screen with a probability distribution given by its interference pattern determined by Maxwell's equations. However, experiments confirm that the photon is \"not\" a short pulse of electromagnetic radiation; it does not spread out as it propagates, nor does it divide when it encounters a beam splitter. Rather, the photon seems to be a point-like particle since it is absorbed or emitted \"as a whole\" by arbitrarily small systems, systems much smaller than its wavelength, such as an atomic nucleus (≈10 m across) or even the point-like electron. Nevertheless, the photon is \"not\" a point-like particle whose trajectory is shaped probabilistically by the electromagnetic field, as conceived by Einstein and others; that hypothesis was also refuted by the photon-correlation experiments cited above. According to our present understanding, the electromagnetic field itself is produced by photons, which in turn result from a local gauge symmetry and the laws of quantum field theory (see ' and ' below).\n\nA key element of quantum mechanics is Heisenberg's uncertainty principle, which forbids the simultaneous measurement of the position and momentum of a particle along the same direction. Remarkably, the uncertainty principle for charged, material particles \"requires\" the quantization of light into photons, and even the frequency dependence of the photon's energy and momentum.\n\nAn elegant illustration of the uncertainty principle is Heisenberg's thought experiment for locating an electron with an ideal microscope. The position of the electron can be determined to within the resolving power of the microscope, which is given by a formula from classical optics\n\nwhere θ is the aperture angle of the microscope and λ is the wavelength of the light used to observe the electron. Thus, the position uncertainty formula_6 can be made arbitrarily small by reducing the wavelength λ. Even if the momentum of the electron is initially known, the light impinging on the electron will give it a momentum \"kick\" formula_7 of some unknown amount, rendering the momentum of the electron uncertain. If light were \"not\" quantized into photons, the uncertainty formula_7 could be made arbitrarily small by reducing the light's intensity. In that case, since the wavelength and intensity of light can be varied independently, one could simultaneously determine the position and momentum to arbitrarily high accuracy, violating the uncertainty principle. By contrast, Einstein's formula for photon momentum preserves the uncertainty principle; since the photon is scattered anywhere within the aperture, the uncertainty of momentum transferred equals\n\ngiving the product formula_10, which is Heisenberg's uncertainty principle. Thus, the entire world is quantized; both matter and fields must obey a consistent set of quantum laws, if either one is to be quantized.\n\nThe analogous uncertainty principle for photons forbids the simultaneous measurement of the number formula_11 of photons (see Fock state and the Second quantization section below) in an electromagnetic wave and the phase formula_12 of that wave\n\nSee coherent state and squeezed coherent state for more details.\n\nBoth photons and electrons create analogous interference patterns when passed through a double-slit experiment. For photons, this corresponds to the interference of a Maxwell light wave whereas, for material particles (electron), this corresponds to the interference of the Schrödinger wave equation. Although this similarity might suggest that Maxwell's equations describing the photon's electromagnetic wave are simply Schrödinger's equation for photons, most physicists do not agree. For one thing, they are mathematically different; most obviously, Schrödinger's one equation for the electron solves for a complex field, whereas Maxwell's four equations solve for real fields. More generally, the normal concept of a Schrödinger probability wave function cannot be applied to photons. As photons are massless, they cannot be localized without being destroyed; technically, photons cannot have a position eigenstate formula_14, and, thus, the normal Heisenberg uncertainty principle formula_15 does not pertain to photons. A few substitute wave functions have been suggested for the photon, but they have not come into general use. Instead, physicists generally accept the second-quantized theory of photons described below, quantum electrodynamics, in which photons are quantized excitations of electromagnetic modes.\n\nAnother interpretation, that avoids duality, is the De Broglie–Bohm theory: known also as the \"pilot-wave model\". In that theory, the photon is both, wave and particle. \"This idea seems to me so natural and simple, to resolve the wave-particle dilemma in such a clear and ordinary way, that it is a great mystery to me that it was so generally ignored\", J.S.Bell.\n\nIn 1924, Satyendra Nath Bose derived Planck's law of black-body radiation without using any electromagnetism, but rather by using a modification of coarse-grained counting of phase space. Einstein showed that this modification is equivalent to assuming that photons are rigorously identical and that it implied a \"mysterious non-local interaction\", now understood as the requirement for a symmetric quantum mechanical state. This work led to the concept of coherent states and the development of the laser. In the same papers, Einstein extended Bose's formalism to material particles (bosons) and predicted that they would condense into their lowest quantum state at low enough temperatures; this Bose–Einstein condensation was observed experimentally in 1995. It was later used by Lene Hau to slow, and then completely stop, light in 1999 and 2001.\n\nThe modern view on this is that photons are, by virtue of their integer spin, bosons (as opposed to fermions with half-integer spin). By the spin-statistics theorem, all bosons obey Bose–Einstein statistics (whereas all fermions obey Fermi–Dirac statistics).\n\nIn 1916, Albert Einstein showed that Planck's radiation law could be derived from a semi-classical, statistical treatment of photons and atoms, which implies a link between the rates at which atoms emit and absorb photons. The condition follows from the assumption that functions of the emission and absorption of radiation by the atoms are independent of each other, and that thermal equilibrium is made by way of the radiation's interaction with the atoms. Consider a cavity in thermal equilibrium with all parts of itself and filled with electromagnetic radiation and that the atoms can emit and absorb that radiation. Thermal equilibrium requires that the energy density formula_16 of photons with frequency formula_17 (which is proportional to their number density) is, on average, constant in time; hence, the rate at which photons of any particular frequency are \"emitted\" must equal the rate at which they \"absorb\" them.\n\nEinstein began by postulating simple proportionality relations for the different reaction rates involved. In his model, the rate formula_18 for a system to \"absorb\" a photon of frequency formula_17 and transition from a lower energy formula_20 to a higher energy formula_21 is proportional to the number formula_22 of atoms with energy formula_20 and to the energy density formula_16 of ambient photons of that frequency,\n\nwhere formula_26 is the rate constant for absorption. For the reverse process, there are two possibilities: spontaneous emission of a photon, or the emission of a photon initiated by the interaction of the atom with a passing photon and the return of the atom to the lower-energy state. Following Einstein's approach, the corresponding rate formula_27 for the emission of photons of frequency formula_17 and transition from a higher energy formula_21 to a lower energy formula_20 is\n\nwhere formula_32 is the rate constant for emitting a photon spontaneously, and formula_33 is the rate constant for emissions in response to ambient photons (induced or stimulated emission). In thermodynamic equilibrium, the number of atoms in state i and those in state j must, on average, be constant; hence, the rates formula_18 and formula_27 must be equal. Also, by arguments analogous to the derivation of Boltzmann statistics, the ratio of formula_36 and formula_22 is formula_38 where formula_39 are the degeneracy of the state i and that of j, respectively, formula_40 their energies, k the Boltzmann constant and T the system's temperature. From this, it is readily derived that\nformula_41 and\nThe A and Bs are collectively known as the \"Einstein coefficients\".\n\nEinstein could not fully justify his rate equations, but claimed that it should be possible to calculate the coefficients formula_32, formula_26 and formula_33 once physicists had obtained \"mechanics and electrodynamics modified to accommodate the quantum hypothesis\". In fact, in 1926, Paul Dirac derived the formula_33 rate constants by using a semiclassical approach, and, in 1927, succeeded in deriving \"all\" the rate constants from first principles within the framework of quantum theory. Dirac's work was the foundation of quantum electrodynamics, i.e., the quantization of the electromagnetic field itself. Dirac's approach is also called \"second quantization\" or quantum field theory; earlier quantum mechanical treatments only treat material particles as quantum mechanical, not the electromagnetic field.\n\nEinstein was troubled by the fact that his theory seemed incomplete, since it did not determine the \"direction\" of a spontaneously emitted photon. A probabilistic nature of light-particle motion was first considered by Newton in his treatment of birefringence and, more generally, of the splitting of light beams at interfaces into a transmitted beam and a reflected beam. Newton hypothesized that hidden variables in the light particle determined which of the two paths a single photon would take. Similarly, Einstein hoped for a more complete theory that would leave nothing to chance, beginning his separation from quantum mechanics. Ironically, Max Born's probabilistic interpretation of the wave function was inspired by Einstein's later work searching for a more complete theory.\n\nIn 1910, Peter Debye derived Planck's law of black-body radiation from a relatively simple assumption. He correctly decomposed the electromagnetic field in a cavity into its Fourier modes, and assumed that the energy in any mode was an integer multiple of formula_47, where formula_17 is the frequency of the electromagnetic mode. Planck's law of black-body radiation follows immediately as a geometric sum. However, Debye's approach failed to give the correct formula for the energy fluctuations of blackbody radiation, which were derived by Einstein in 1909.\n\nIn 1925, Born, Heisenberg and Jordan reinterpreted Debye's concept in a key way. As may be shown classically, the Fourier modes of the electromagnetic field—a complete set of electromagnetic plane waves indexed by their wave vector k and polarization state—are equivalent to a set of uncoupled simple harmonic oscillators. Treated quantum mechanically, the energy levels of such oscillators are known to be formula_49, where formula_17 is the oscillator frequency. The key new step was to identify an electromagnetic mode with energy formula_49 as a state with formula_11 photons, each of energy formula_47. This approach gives the correct energy fluctuation formula.\nDirac took this one step further. He treated the interaction between a charge and an electromagnetic field as a small perturbation that induces transitions in the photon states, changing the numbers of photons in the modes, while conserving energy and momentum overall. Dirac was able to derive Einstein's formula_32 and formula_33 coefficients from first principles, and showed that the Bose–Einstein statistics of photons is a natural consequence of quantizing the electromagnetic field correctly (Bose's reasoning went in the opposite direction; he derived Planck's law of black-body radiation by \"assuming\" B–E statistics). In Dirac's time, it was not yet known that all bosons, including photons, must obey Bose–Einstein statistics.\n\nDirac's second-order perturbation theory can involve virtual photons, transient intermediate states of the electromagnetic field; the static electric and magnetic interactions are mediated by such virtual photons. In such quantum field theories, the probability amplitude of observable events is calculated by summing over \"all\" possible intermediate steps, even ones that are unphysical; hence, virtual photons are not constrained to satisfy formula_56, and may have extra polarization states; depending on the gauge used, virtual photons may have three or four polarization states, instead of the two states of real photons. Although these transient virtual photons can never be observed, they contribute measurably to the probabilities of observable events. Indeed, such second-order and higher-order perturbation calculations can give apparently infinite contributions to the sum. Such unphysical results are corrected for using the technique of renormalization.\n\nOther virtual particles may contribute to the summation as well; for example, two photons may interact indirectly through virtual electron–positron pairs. In fact, such photon–photon scattering (see two-photon physics), as well as electron–photon scattering, is meant to be one of the modes of operations of the planned particle accelerator, the International Linear Collider.\n\nIn modern physics notation, the quantum state of the electromagnetic field is written as a Fock state, a tensor product of the states for each electromagnetic mode\n\nwhere formula_58 represents the state in which formula_59 photons are in the mode formula_60. In this notation, the creation of a new photon in mode formula_60 (e.g., emitted from an atomic transition) is written as formula_62. This notation merely expresses the concept of Born, Heisenberg and Jordan described above, and does not add any physics.\n\nMeasurements of the interaction between energetic photons and hadrons show that the interaction is much more intense than expected by the interaction of merely photons with the hadron's electric charge. Furthermore, the interaction of energetic photons with protons is similar to the interaction of photons with neutrons in spite of the fact that the electric charge structures of protons and neutrons are substantially different. A theory called Vector Meson Dominance (VMD) was developed to explain this effect. According to VMD, the photon is a superposition of the pure electromagnetic photon which interacts only with electric charges and vector mesons. However, if experimentally probed at very short distances, the intrinsic structure of the photon is recognized as a flux of quark and gluon components, quasi-free according to asymptotic freedom in QCD and described by the photon structure function. A comprehensive comparison of data with theoretical predictions was presented in a review in 2000.\n\nThe electromagnetic field can be understood as a gauge field, i.e., as a field that results from requiring that a gauge symmetry holds independently at every position in spacetime. For the electromagnetic field, this gauge symmetry is the Abelian U(1) symmetry of complex numbers of absolute value 1, which reflects the ability to vary the phase of a complex field without affecting observables or real valued functions made from it, such as the energy or the Lagrangian.\n\nThe quanta of an Abelian gauge field must be massless, uncharged bosons, as long as the symmetry is not broken; hence, the photon is predicted to be massless, and to have zero electric charge and integer spin. The particular form of the electromagnetic interaction specifies that the photon must have spin ±1; thus, its helicity must be formula_63. These two spin components correspond to the classical concepts of right-handed and left-handed circularly polarized light. However, the transient virtual photons of quantum electrodynamics may also adopt unphysical polarization states.\n\nIn the prevailing Standard Model of physics, the photon is one of four gauge bosons in the electroweak interaction; the other three are denoted W, W and Z and are responsible for the weak interaction. Unlike the photon, these gauge bosons have mass, owing to a mechanism that breaks their SU(2) gauge symmetry. The unification of the photon with W and Z gauge bosons in the electroweak interaction was accomplished by Sheldon Glashow, Abdus Salam and Steven Weinberg, for which they were awarded the 1979 Nobel Prize in physics. Physicists continue to hypothesize grand unified theories that connect these four gauge bosons with the eight gluon gauge bosons of quantum chromodynamics; however, key predictions of these theories, such as proton decay, have not been observed experimentally.\n\nThe energy of a system that emits a photon is \"decreased\" by the energy formula_64 of the photon as measured in the rest frame of the emitting system, which may result in a reduction in mass in the amount formula_65. Similarly, the mass of a system that absorbs a photon is \"increased\" by a corresponding amount. As an application, the energy balance of nuclear reactions involving photons is commonly written in terms of the masses of the nuclei involved, and terms of the form formula_65 for the gamma photons (and for other relevant energies, such as the recoil energy of nuclei).\n\nThis concept is applied in key predictions of quantum electrodynamics (QED, see above). In that theory, the mass of electrons (or, more generally, leptons) is modified by including the mass contributions of virtual photons, in a technique known as renormalization. Such \"radiative corrections\" contribute to a number of predictions of QED, such as the magnetic dipole moment of leptons, the Lamb shift, and the hyperfine structure of bound lepton pairs, such as muonium and positronium.\n\nSince photons contribute to the stress–energy tensor, they exert a gravitational attraction on other objects, according to the theory of general relativity. Conversely, photons are themselves affected by gravity; their normally straight trajectories may be bent by warped spacetime, as in gravitational lensing, and their frequencies may be lowered by moving to a higher gravitational potential, as in the Pound–Rebka experiment. However, these effects are not specific to photons; exactly the same effects would be predicted for classical electromagnetic waves.\n\nLight that travels through transparent matter does so at a lower speed than \"c\", the speed of light in a vacuum. For example, photons engage in so many collisions on the way from the core of the sun that radiant energy can take about a million years to reach the surface; however, once in open space, a photon takes only 8.3 minutes to reach Earth. The factor by which the speed is decreased is called the refractive index of the material. In a classical wave picture, the slowing can be explained by the light inducing electric polarization in the matter, the polarized matter radiating new light, and that new light interfering with the original light wave to form a delayed wave. In a particle picture, the slowing can instead be described as a blending of the photon with quantum excitations of the matter to produce quasi-particles known as polariton (other quasi-particles are phonons and excitons); this polariton has a nonzero effective mass, which means that it cannot travel at \"c\". Light of different frequencies may travel through matter at different speeds; this is called dispersion (not to be confused with scattering). In some cases, it can result in extremely slow speeds of light in matter. The effects of photon interactions with other quasi-particles may be observed directly in Raman scattering and Brillouin scattering.\n\nPhotons can also be absorbed by nuclei, atoms or molecules, provoking transitions between their energy levels. A classic example is the molecular transition of retinal (CHO), which is responsible for vision, as discovered in 1958 by Nobel laureate biochemist George Wald and co-workers. The absorption provokes a cis-trans isomerization that, in combination with other such transitions, is transduced into nerve impulses. The absorption of photons can even break chemical bonds, as in the photodissociation of chlorine; this is the subject of photochemistry.\n\nPhotons have many applications in technology. These examples are chosen to illustrate applications of photons \"per se\", rather than general optical devices such as lenses, etc. that could operate under a classical theory of light. The laser is an extremely important application and is discussed above under stimulated emission.\n\nIndividual photons can be detected by several methods. The classic photomultiplier tube exploits the photoelectric effect: a photon of sufficient energy strikes a metal plate and knocks free an electron, initiating an ever-amplifying avalanche of electrons. Semiconductor charge-coupled device chips use a similar effect: an incident photon generates a charge on a microscopic capacitor that can be detected. Other detectors such as Geiger counters use the ability of photons to ionize gas molecules contained in the device, causing a detectable change of conductivity of the gas.\n\nPlanck's energy formula formula_67 is often used by engineers and chemists in design, both to compute the change in energy resulting from a photon absorption and to determine the frequency of the light emitted from a given photon emission. For example, the emission spectrum of a gas-discharge lamp can be altered by filling it with (mixtures of) gases with different electronic energy level configurations.\n\nUnder some conditions, an energy transition can be excited by \"two\" photons that individually would be insufficient. This allows for higher resolution microscopy, because the sample absorbs energy only in the spectrum where two beams of different colors overlap significantly, which can be made much smaller than the excitation volume of a single beam (see two-photon excitation microscopy). Moreover, these photons cause less damage to the sample, since they are of lower energy.\n\nIn some cases, two energy transitions can be coupled so that, as one system absorbs a photon, another nearby system \"steals\" its energy and re-emits a photon of a different frequency. This is the basis of fluorescence resonance energy transfer, a technique that is used in molecular biology to study the interaction of suitable proteins.\n\nSeveral different kinds of hardware random number generators involve the detection of single photons. In one example, for each bit in the random sequence that is to be produced, a photon is sent to a beam-splitter. In such a situation, there are two possible outcomes of equal probability. The actual outcome is used to determine whether the next bit in the sequence is \"0\" or \"1\".\n\nMuch research has been devoted to applications of photons in the field of quantum optics. Photons seem well-suited to be elements of an extremely fast quantum computer, and the quantum entanglement of photons is a focus of research. Nonlinear optical processes are another active research area, with topics such as two-photon absorption, self-phase modulation, modulational instability and optical parametric oscillators. However, such processes generally do not require the assumption of photons \"per se\"; they may often be modeled by treating atoms as nonlinear oscillators. The nonlinear process of spontaneous parametric down conversion is often used to produce single-photon states. Finally, photons are essential in some aspects of optical communication, especially for quantum cryptography.\n\nTwo-photon physics studies interactions between photons, which are rare. In 2018, MIT researchers announced the discovery of bound photon triplets, which may involve polaritons.\n\nBy date of publication:\nEducation with single photons:\n\n"}
{"id": "5351223", "url": "https://en.wikipedia.org/wiki?curid=5351223", "title": "Pinch (plasma physics)", "text": "Pinch (plasma physics)\n\nA pinch is the compression of an electrically conducting filament by magnetic forces. The conductor is usually a plasma, but could also be a solid or liquid metal. Pinches were the first type of device used for controlled nuclear fusion.\n\nThe phenomenon may also be referred to as a Bennett pinch (after Willard Harrison Bennett), electromagnetic pinch, magnetic pinch, pinch effect or plasma pinch.\n\nPinches occur naturally in electrical discharges such as lightning bolts, the aurora, current sheets, and solar flares.\n\nPinches exist in laboratories and in nature. Pinches differ in their geometry and operating forces. These include:\n\nPinches may become unstable. They radiate energy as light across the whole electromagnetic spectrum including radio waves, x-rays, gamma rays, synchrotron radiation, and visible light. They also produce neutrons, as a product of fusion.\n\nPinches are used to generate X-rays and the intense magnetic fields generated are used in electromagnetic forming of metals. They also have applications in particle beams including particle beam weapons, astrophysics studies and it has been proposed to use them in space propulsion. A number of large pinch machines have been built to study fusion power; here are several:\n\n\nMany high-voltage electronics enthusiasts make their own crude electromagnetic forming devices. They use pulsed power techniques to produce a theta pinch capable of crushing an aluminium soft drink can using the Lorentz forces created when large currents are induced in the can by the strong magnetic field of the primary coil.\n\nAn electromagnetic aluminium can crusher consists of four main components: a high voltage DC power supply, which provides a source of electrical energy, a large \"energy discharge\" capacitor to accumulate the electrical energy, a high voltage switch or spark gap, and a robust coil (capable of surviving high magnetic pressure) through which the stored electrical energy can be quickly discharged in order to generate a correspondingly strong pinching magnetic field (see diagram below).\n\nIn practice, such a device is somewhat more sophisticated than the schematic diagram suggests, including electrical components that control the current in order to maximize the resulting pinch, and to ensure that the device works safely. For more details, see the notes.\n\nThe first creation of a Z-pinch in the laboratory may have occurred in 1790 in Holland when Martinus van Marum created an explosion by discharging 100 Leyden jars into a wire. The phenomenon was not understood until 1905, when Pollock and Barraclough investigated a compressed and distorted length of copper tube from a lightning rod after it had been struck by lightning. Their analysis showed that the forces due to the interaction of the large current flow with its own magnetic field could have caused the compression and distortion. A similar, and apparently independent, theoretical analysis of the pinch effect in liquid metals was published by Northrupp in 1907. The next major development was the publication in 1934 of an analysis of the radial pressure balance in a static Z-pinch by Bennett (see the following section for details).\n\nThereafter, the experimental and theoretical progress on pinches was driven by fusion power research. In their article on the \"Wire-array Z-pinch: a powerful x-ray source for ICF\", M G Haines \"et al.\", wrote on the \"Early history of Z-pinches\".\n\nIn 1958, the worlds' first controlled thermonuclear fusion experiment was accomplished using a theta-pinch machine named Scylla I at the Los Alamos National Laboratory. A cylinder full of deuterium was converted into a plasma and compressed to 15 million degrees Celsius under a theta-pinch effect. Lastly, at Imperial College in 1960, led by R Latham, the Plateau-Rayleigh instability was shown, and its growth rate measured in a dynamic Z-pinch.\n\nIn plasma physics three pinch geometries are commonly studied: the θ-pinch, the Z-pinch, and the Screw Pinch. These are cylindrically shaped. The cylinder is symmetric in the axial (\"z\") direction and the azimuthal (θ) directions. The one-dimensional pinches are named for the direction the current travels.\n\nThe θ-pinch has a magnetic field directed in the z direction and a large diamagnetic current directed in the θ direction. Using Ampère's law (discarding the displacement term)\n\nSince \"B\" is only a function of \"r\" we can simplify this to\n\nSo \"J\" points in the θ direction.\n\nThus, the equilibrium condition (∇\"p\" = j × Β) for the θ-pinch reads:\n\nθ-pinches tend to be resistant to plasma instabilities; This is due in part to Alfvén's Theorem (or, frozen in flux theorem).\n\nThe Z-pinch has a magnetic field in the θ direction and a current \"J\" flowing in the \"z\" direction. Again, by electrostatic Ampère's Law\n\nThus, the equilibrium condition, ∇\"p\" = j × Β, for the Z-pinch reads:\n\nSince particles in a plasma basically follow magnetic field lines, Z-pinches lead them around in circles. Therefore, they tend to have excellent confinement properties.\n\nThe screw pinch is an effort to combine the stability aspects of the θ-pinch and the confinement aspects of the Z-pinch. Referring once again to Ampère's Law\n\nBut this time, the \"B\" field has a θ component \"and\" a \"z\" component\n\nSo this time \"J\" has a component in the \"z\" direction and a component in the θ direction.\n\nFinally, the equilibrium condition (∇\"p\" = j × Β) for the screw pinch reads:\n\nThe \" screw pinch \" might be produced in laser plasma by colliding optical vortices of ultrashort duration. For this purpose optical vortices ought to be phase-conjugated.\n\nThe magnetic field distribution is given here again via Ampère's law:\n\nA common problem with one-dimensional pinches is the end losses. Most of the motion of particles is along the magnetic field. With the θ-pinch and the screw-pinch, this leads particles out of the end of the machine very quickly, leading to a loss of mass and energy. On top of this problem, the Z-pinch has major stability problems. Though particles can be reflected to some extent with magnetic mirrors, even these allow many particles to pass. A common method of beating these end losses, is to bend the cylinder around into a torus. Unfortunately this breaks θ symmetry, as paths on the inner portion (inboard side) of the torus are shorter than similar paths on the outer portion (outboard side). Thus, a new theory is needed. This gives rise to the famous Grad–Shafranov equation. Numerical solutions to the Grad–Shafranov equation have also yielded some equilibria, most notably that of the reversed field pinch.\n\nAs of 2015, there is not a coherent analytical theory for three-dimensional equilibria. The general approach to finding three-dimensional equilibria is to solve the vacuum ideal MHD equations. Numerical solutions have yielded designs for stellarators. Some machines take advantage of simplification techniques such as helical symmetry (for example University of Wisconsin's Helically Symmetric eXperiment). However, for an arbitrary three-dimensional configuration an equilibrium relation, similar to that of the 1-D configurations exists:\n\nWhere κ is the curvature vector defined as:\n\nwith \"b\" the unit vector tangent to \"B\".\n\nConsider a cylindrical column of fully ionized quasineutral plasma, with an axial electric field, producing an axial current density, j, and associated azimuthal magnetic field, B. As the current flows through its own magnetic field, a pinch is generated with an inward radial force density of j x B. In a steady state with forces balancing:\n\nwhere ∇\"p\" is the magnetic pressure gradient, \"p\" and p is the electron and ion pressures. Then using Maxwell's equation ∇ × B = μ j and the ideal gas law \"p = N k T\", we derive:\nwhere \"N\" is the number of electrons per unit length along the axis, \"T\" and \"T\" are the electron and ion temperatures, \"I\" is the total beam current, and \"k\" is the Boltzmann constant.\n\nThe \"Generalized Bennett Relation\" considers a current-carrying magnetic-field-aligned cylindrical plasma pinch undergoing rotation at angular frequency ω. Along the axis of the plasma cylinder flows a current density j, resulting in an azimuthal magnetίc field Β. Originally derived by Witalis, the Generalized Bennett Relation results in:\n\nThe positive terms in the equation are expansional forces while the negative terms represent beam compressional forces.\n\nThe Carlqvist Relation, published by Per Carlqvist in 1988, is a specialization of the Generalized Bennett Relation (above), for the case that the kinetic pressure is much smaller at the border of the pinch than in the inner parts. It takes the form\n\nand is applicable to many space plasmas.\n\nThe Carlqvist Relation can be illustrated (see right), showing the total current (\"I\") versus the number of particles per unit length (\"N\") in a Bennett pinch. The chart illustrates four physically distinct regions. The plasma temperature is quite cold (\"T\" = \"T\" = \"T\" = 20 K), containing mainly hydrogen with a mean particle mass 3×10 kg. The thermokinetic energy \"W\" » \"πa\" \"p\"(a). The curves, ΔW show different amounts of excess magnetic energy per unit length due to the axial magnetic field B. The plasma is assumed to be non-rotational, and the kinetic pressure at the edges is much smaller than inside.\n\nChart regions: (a) In the top-left region, the pinching force dominates. (b) Towards the bottom, outward kinetic pressures balance inwards magnetic pressure, and the total pressure is constant. (c) To the right of the vertical line Δ\"W\" = 0, the magnetic pressures balances the gravitational pressure, and the pinching force is negligible. (d) To the left of the sloping curve Δ\"W\" = 0, the gravitational force is negligible. Note that the chart shows a special case of the Carlqvist relation, and if it is replaced by the more general Bennett relation, then the designated regions of the chart are not valid.\n\nCarlqvist further notes that by using the relations above, and a derivative, it is possible to describe the Bennett pinch, the Jeans criterion (for gravitational instability, in one and two dimensions), force-free magnetic fields, gravitationally balanced magnetic pressures, and continuous transitions between these states.\n\nA fictionalized pinch-generating device was used in \"Ocean's Eleven\", where it was used to disrupt Las Vegas's power grid just long enough for the characters to begin their heist.\n\n\n"}
{"id": "12269011", "url": "https://en.wikipedia.org/wiki?curid=12269011", "title": "Polyphenyl ether", "text": "Polyphenyl ether\n\nPhenyl ether polymers are a class of polymers that contain a phenoxy and/or a thiophenoxy group as the repeating group in ether linkages. Commercial phenyl ether polymers belong to two chemical classes: polyphenyl ethers (PPEs) and polyphenylene oxides (PPOs). The phenoxy groups in the former class of polymers do not contain any substituents whereas those in the latter class contain 2 to 4 alkyl groups on the phenyl ring. The structure of an oxygen-containing PPE is provided in Figure 1 and that of a 2, 6-xylenol derived PPO is shown in Figure 2. Either class can have the oxygen atoms attached at various positions around the rings.\n\nThe proper name for a phenyl ether polymer is poly(phenyl ether) or polyphenyl polyether, but the name polyphenyl ether is widely accepted. Polyphenyl ethers (PPEs) are obtained by repeated application of the Ullmann Ether Synthesis: reaction of an alkali-metal phenate with a halogenated benzene catalyzed by copper.\n\nPPEs of up to 6 phenyl rings, both oxy and thio ethers, are commercially available. See Table 1. They are characterized by indicating the substitution pattern of each ring, followed by the number of phenyl rings and the number of ether linkages. Thus, the structure in Figure 1 with \"n\" equal to 1 is identified as pmp5P4E, indicating para, meta, para substitution of the three middle rings, a total of 5 rings, and 4 ether linkages. Meta substitution of the aryl rings in these materials is most common and often desired. Longer chain analogues with up to 10 benzene rings are also known.\n\nThe simplest member of the phenyl ether family is diphenyl ether (DPE), also called diphenyl oxide, the structure of which is provided in Figure 4. Low molecular weight polyphenyl ethers and thioethers are used in a variety of applications, and include high-vacuum devices, optics, electronics, and in high-temperature and radiation-resistant fluids and greases. Figure 5 shows the structure of the sulfur analogue of 3-R polyphenyl ether shown in Figure 3.\n\nTypical physical properties of polyphenyl ethers are provided in Table 2. Physical properties of a particular PPE depend upon the number of aromatic rings, their substitution pattern, and whether it is an ether or a thioether. In the case of products of mixed structures, properties are hard to predict from only the structural features; hence, they must be determined via measurement. \nThe important attributes of PPEs include their thermal and oxidative stability and stability in the presence of ionizing radiation. PPEs have the disadvantage of having somewhat high pour points. For example, PPEs that contain two and three benzene rings are actually solids at room temperatures. The melting points of the ordinarily solid PPEs are lowered if they contain more m-phenylene rings, alkyl groups, or are mixtures of isomers. PPEs that contain only o- and p-substituted rings have the highest melting points. \n\nPPEs have excellent high temperature properties and good oxidation stability. With respect to volatilities, p-derivatives have the lowest volatilities, and the o-derivatives have the highest volatilities. The opposite is true for flash points and fire points. Spontaneous ignition temperatures of polyphenyl ethers lie between , alkyl substitution reduces this value by ~. PPEs are compatible with most metals and elastomers that are commonly used in high-temperature applications. They typically swell common seal materials.\n\nOxidation stability of un-substituted PPEs is quite good, partly because they lack easily oxidizable carbon-hydrogen bonds. Thermal decomposition temperature, as measured by the isoteniscope procedure, is between .\n\nIonizing radiation affects all organic compounds, causing a change in their properties because radiation disrupts covalent bonds that are most prevalent in organic compounds. One result of ionization is that the organic molecules disproportionate to form smaller hydrocarbon molecules as well as larger hydrocarbons molecules. This is reflected by increased evaporation loss, lowering of the flash and fire points, and increased viscosity. Other chemical reactions caused by radiation include oxidation and isomerization. The former leads to increased acidity, corrosivity, and coke formation; the latter causes a change in viscosity and volatility.\n\nPPEs have extremely high radiation resistance. Of all classes of synthetic lubricants (with the possible exception of perfluoropolyethers) the polyphenyl ethers are the most radiation resistant. Excellent radiation stability of PPEs can be ascribed to the limited number of ionizable carbon-carbon and carbon-hydrogen bonds. In one study, the performance of PPE under the influence of 1x10 ergs/gram of radiation at was compared with synthetic ester, synthetic hydrocarbon, and silicone fluids. PPE showed a viscosity increase of only 35%, while all other fluids showed a viscosity increase of 1700% and gelled. Further tests have shown PPEs to be resistant to gamma and associated neutron radiation dosages of 1x10 erg/g at temperatures up to .\n\nPPEs have high surface tension; hence these fluids have a lower tendency to wet metal surfaces. The surface tension of the commercially available 5R4E is 49.9 dynes/cm, one of the highest in pure organic liquids. This property is useful in applications where migration of the lubricant into the surrounding environment must be avoided.\n\nWhile originally PPEs were developed for use in extreme environments that were experienced in aerospace applications, they are now used in other applications requiring low volatility and excellent thermo-oxidative and ionizing radiation stability. Such applications include use as diffusion pump fluids; high vacuum fluids; and in formulating jet engine/turbine lubricants, high-temperature hydraulic lubricants and greases, and heat transfer fluids. In addition, because of excellent optical properties these fluids have found use in optical devices.\n\nVacuum pumps are devices that remove gases from an enclosed space to greatly reduce pressure. Oil diffusion pumps in combination with a fore pump are amongst the most popular. Diffusion pumps use a high boiling liquid of low vapor pressure to create a high-speed jet that strikes the gaseous molecules in the system to be evacuated and direct them into space that is being evacuated by the fore pump. A good diffusion fluid must therefore reflect low vapor pressure, high flash point, high thermal and oxidative stability and chemical resistance. If the diffusion pump is operating in the proximity of ionizing radiation source, good radiation stability is also desired.\n\nData presented in Table 3 demonstrates polyphenyl ether to be superior to other fluids that are commonly used in diffusion pumps. PPEs help achieve the highest vacuum of 4 x 10 torr at 25 °C. Such high vacuums are necessary in equipment such as electron microscopes, mass spectrometers and that used for various surface physics studies. Vacuum pumps are also used in the production of electric lamps, vacuum tubes, and cathode ray tubes (CRTs), semiconductor processing, and vacuum engineering.\n\n5R4E PPE has a surface tension of 49.9 dynes/cm, which is amongst the highest in pure organic liquids. Because of this, this PPE and the other PPEs do not effectively wet metal surfaces. This property is useful when migration of a lubricant from one part of the equipment to another part must be avoided, such as in certain electronic devices. A thin film of polyphenyl ether on a surface is not a thin contiguous film as one would envision, but rather comprises tiny droplets. This PPE property tends to keep the film stationary, or at least to cause it to remain in the area where the lubrication is needed, rather than migrating away by spreading and forming a new surface. As a result, contamination of other components and equipment, which do not require a lubricant, is avoided. The high surface tension of PPEs, therefore, makes them useful in lubricating electronic contacts. \nPolyphenyl ether lubricants have a 30-year history of commercial service for connectors with precious and base metal contacts in telecom, automotive, aerospace, instrumentation and general-purpose applications. In addition to maintaining the current flow and providing long-term lubrication, PPEs offer protection to connectors against aggressive acidic and oxidative environments. By providing a protective surface film, polyphenyl ethers not only protect connectors against corrosion but also against vibration-related wear and abrasion that leads to fretting wear. The devices that benefit from the specialized properties of PPEs include cell phones, printers, and a variety of other electronic appliances. The protection lasts for decades or for the life of the equipment.\n\nPolyphenyl ethers (PPEs) possess good optical clarity, a high refractive index, and other beneficial optical properties. Because of these, PPEs have the ability to meet the rigorous performance demands of signal processing in advanced photonics systems. Optical clarity of PPEs resembles that of the other optical polymers, that is, they have refractive indices of between 1.5 and 1.7 and provide good propagation of light between approximately 400 nm and 1700 nm. Close refractive index (RI) matching between materials is important for proper propagation of light through them. Because of the ease of RI matching, PPEs are used in many optical devices as optical fluids. Extreme resistance to ionizing radiation gives PPEs an added advantage in the manufacture of solar cells and solid-state UV/blue emitters and telecommunication equipment made from high-index glasses and semiconductors.\n\nPPEs, being of excellent thermo-oxidative stability and radiation resistance, have found extensive use in high temperature applications that also require radiation resistance. In addition, PPEs demonstrate better wear control and load-carrying ability than mineral oils, especially when used in bearings.\n\nAs noted earlier, PPEs were developed for use in jet engines that involved high speed-related frictional temperatures of as high as . While the use of PPEs in lubricating jet engines has somewhat subsided due to their higher cost, they are still used in some aerospace applications. PPEs are also used as base fluids for radiation-resistant greases used in nuclear power plant mechanisms. PPEs and their derivatives have also found use as vapor phase lubricants in gas turbines and custom bearings, and wherever extreme environmental conditions exist. Vapor phase lubrication is achieved by heating the liquid lubricant above its boiling point. The resultant vapors are then transported to the hot bearing surface. If the temperatures of the bearing surface are kept below the lubricant’s boiling point, the vapors re-condense to provide liquid lubrication.\n\nPolyphenyl ether technology can also provide superior fire safety and fatigue life, depending on the specific bearing design. In this application, PPEs have the advantage of providing lubrication both as a liquid at low temperatures and as a vapor at temperatures above . Due to the low volatility and excellent high-temperature thermo-oxidative stability, PPEs have also found use as a lubricant for chains used in and around kilns, metal fabrication plants, and glass molding and manufacturing equipment. In these high-temperature applications, PPEs do not form any sludge and hard deposits. The low soft-carbon residue that is left behind is removed easily by wiping. PPEs' low volatility, low flammability, and good thermodynamic properties make them ideally suited for use as heat transfer fluids and in heat sink applications as well.\n\nThese polymers are made through oxidative coupling of substituted phenol in the presence of oxygen and copper and amine containing catalysts, such as cuprous bromide and pyridine. See Figure 2 for the PPO structure. PPO polymers can be classified as plastic resins. They and their composites with polystyrene, glass, and nylon are used as high-strength, moisture-resistant engineering plastics in a number of industries, including computer, telecommunication, and automotive parts. PPOs are marketed by SABIC Innovative Plastics under the trademarked name of Noryl.\n"}
{"id": "17037703", "url": "https://en.wikipedia.org/wiki?curid=17037703", "title": "San Juanico disaster", "text": "San Juanico disaster\n\nThe San Juanico disaster was an industrial disaster caused by a massive series of explosions at a liquid petroleum gas (LPG) tank farm in San Juanico, Mexico (outside of Mexico City, Mexico) on 19 November 1984. The explosions consumed 11,000 m of LPG, representing one third of Mexico City's entire liquid petroleum gas supply. The explosions destroyed the facility and devastated the local town of San Juan Ixhuatepec, with 500–600 people killed, and 5000–7000 others suffering severe burns. The San Juanico disaster was one of the deadliest industrial disasters in world history.\n\nThe incident took place at a storage and distribution facility (a \"terminal\") for liquified petroleum gas (LPG) belonging to the multi-state enterprise, Petroleos Mexicanos (PEMEX). The facility consisted of 54 LPG storage tanks; 6 large spherical tanks (four holding 1,600 m and two holding 2,400 m) and 48 smaller horizontal bullet shaped tanks of various sizes. All together the tanks contained 11,000 m of a propane/butane mixture at the time of the accident.\n\nThe disaster was initiated by a gas leak on the site, likely caused by a pipe rupture during transfer operations, which caused a plume of LPG to concentrate at ground level for 10 minutes. The plume eventually grew large enough to drift on the wind towards the west end of the site, where the facility's waste-gas flare pit was located.\n\nAt 5:40 a.m., the cloud reached the flare and ignited, resulting in a vapor cloud explosion that severely damaged the tank farm and resulted in a massive conflagration fed by the LPG leaking from newly damaged tanks. Just four minutes later, at 5:44 a.m., the first tank underwent a BLEVE (Boiling Liquid/Expanding Vapor Explosion). Over the next hour, 12 separate BLEVE explosions were recorded. The fire and smaller explosions continued until 10 a.m. the next morning. It is believed that the escalation was caused by an ineffective gas detection system.\n\nThe town of San Juan Ixhuatepec surrounded the facility and consisted of 40,000 residents, with an additional 61,000 more living in the hills. The explosions demolished houses and propelled twisted metal fragments (some measuring 30 tons) over distances ranging from a few meters to up to 1200 m. Much of the town was destroyed by the explosions and ensuing fire, with the current statistics indicating 500 to 600 deaths, and 5,000–7,000 severe injuries. Radiant heat generated by the inferno incinerated most corpses to ashes, with only 2% of the recovered remains left in recognizable condition.\n\n\n"}
{"id": "22162602", "url": "https://en.wikipedia.org/wiki?curid=22162602", "title": "Shock metamorphism", "text": "Shock metamorphism\n\nShock metamorphism or impact metamorphism describes the effects of shock-wave related deformation and heating during impact events. The formation of similar features during explosive volcanism is generally discounted due to the lack of metamorphic effects unequivocally associated with explosions and the difficulty in reaching sufficient pressures during such an event.\n\nPlanar fractures are parallel sets of multiple planar cracks or cleavages in quartz grains; they develop at the lowest\npressures characteristic of shock waves (~5–8 GPa) and a common feature of quartz grains found associated with impact structures. Although the occurrence of planar fractures is relatively common in other deformed rocks, the development of intense, widespread, and closely spaced planar fractures is considered diagnostic of shock metamorphism.\n\nPlanar deformation features, or PDFs, are optically recognizable microscopic features in grains of silicate minerals (usually quartz or feldspar), consisting of very narrow planes of glassy material arranged in parallel sets that have distinct orientations with respect to the grain's crystal structure. PDFs are only produced by extreme shock compressions on the scale of meteor impacts. They are not found in volcanic environments.\n\nThis form of twinning in quartz is relatively common but the occurrence of close-spaced Brazil twins parallel to the basal plane, (0001), has only been reported from impact structures. Experimental formation of basal-orientated Brazil twins in quartz requires high stresses (about 8 GPa) and high strain rates, and it seems probable that such features in natural quartz can also be regarded as unique impact indicators.\n\nThe very high pressures associated with impacts can lead to the formation of high-pressure polymorphs of various minerals. Quartz may occur as either of its two high-pressure forms, coesite and stishovite. Coesite occasionally occurs associated with eclogites formed during very high pressure regional metamorphism but was first discovered in a meteorite crater in 1960. Stishovite, however, is only known from impact structures.\n\nTwo of the high-pressure polymorphs of titanium dioxide, one with a baddeleyite-like form and the other with a α-PbO structure, have been found associated with the Nördlinger Ries impact structure.\n\nDiamond, the high-pressure allotrope of carbon, has been found associated with many impact structures, and both fullerenes and carbynes have been reported.\n\nShatter cones have a distinctively conical shape that radiates from the top of the cones repeating cone-on-cone, at various scales in the same sample. They are only known to form in rocks beneath meteorite impact craters or underground nuclear explosions. They are evidence that the rock has been subjected to a shock with pressures in the range of 2-30 GPa.\n\nThere exist macroscopic white lamellae inside quartz and other minerals in the Bohemian Massif and even at another places in whole of the world like wavefronts generated by a meteorite impact according to the Rajlich's Hypothesis. The hypothetical wavefronts are composed of many microcavities. Their origin is seen in a physical phenomenon of ultrasonic cavitation, which is well known from the technical practice.\n\nThe effects described above have been found singly, or more often in combination, associated with every impact structure that has been identified on Earth. The search for such effects therefore forms the basis for identifying possible candidate impact structures, particularly to distinguish them from volcanic features.\n\n\n"}
{"id": "315676", "url": "https://en.wikipedia.org/wiki?curid=315676", "title": "Snakeboard", "text": "Snakeboard\n\nA Snakeboard, also known as pivotboard or streetboard, is a board that was invented in South Africa in 1989 by James Fisher, Simon King and Oliver Macleod Smith. The concept was to fuse the original skateboard with elements of snowboarding and surfing to create a fun riding experience. The first prototype was constructed using two square wooden boards, an old roller skate chopped in half, and a piece of plumbing pipe to join them together. Many variants were tried before manufacturing began. The first boards to be mass-produced were made from a strong plastic nylon known as Zytel ST801.\n\nThe rider of a snakeboard stands with one foot on each footplate (the feet are usually fixed to the board using bindings) and, by moving his/her feet in and out in conjunction with the shoulders and hips, is able to propel the board in any direction using only body weight. This transfer of energy is called nonholonomic locomotion. The board moves in a motion similar to that of a snake—hence the sport's original name of snakeboarding. The rider of a snakeboard can ride the board on almost any terrain (depending on the setup and model of board) and even propel himself uphill and perform extreme stunts similar to those done by skateboarders and snowboarders. Snakeboards allow riders to gain momentum without the need to push themselves with their feet like skateboarders.\n\nQuoted directly from the Snakeboard patent Abstract and Claims: \"This invention relates to a skateboard comprising two footboards each of the footboards including a foot platform and wheel-set which carries two wheels in axial alignment fixed to the underside of the platform, a spacer element for holding the footboards in a spaced relationship and a pivot arrangement having a vertical pivot axis connecting each footboard to the spacer element to enable both footboards to pivot relatively to the spacer element, each wheel set including a wheel body, wheel axles which are fixed to and project from the opposite sides of the body with the wheels being journaled for rotation on the axles, a first pivot pin being attached to the wheel body with its axis in a vertical direction, a support member on the wheel body, a second pivot pin pivotally connecting the foot platform to the support member with its pivot axis normal to the wheel axis, and resilient suspension means between the support member and the underside of the foot platform for holding the platform horizontal, said pivot arrangement being pivotally engaged with the first pivot pin.\"\n\nSkatex International (Pty) Ltd is the name of the first company that manufactured boards of this kind and was the inventors' original company. Skatex International licensed Snakeboard USA to sell and distribute the boards in the United States; the sport rapidly became known as \"snakeboarding\". James Fisher, Simon King and Oliver Macleod Smith jointly owned the patents and trademarks for the sport. In the late 1990s, the inventors licensed PMS (UK), a toy retailer, to manufacturer the \"Sydewynder\" under license. Over 50,000 Sydewynders were sold in the UK and Europe. The inventors then listed their company Snakeboard International AIM market, a division of the London Stock Exchange. Later there was a reverse acquisition of Snakeboard International by a company called MV Sports. MV Sports eventually stopped manufacturing the Snakeboard. However, the sport survives today with more advanced boards and is becoming more widely known as streetboarding.\n\nA snakeboard is self-propelled, and there is no need to touch a foot on the ground. Moreover, it is considerably easier to generate and maintain momentum than on a skateboard. To perform a trick, one's feet must be strapped to the board, which makes it harder to \"bail out\" of a trick or dismount. It should also be noted that while the footstraps limit the amount a rider can manipulate the board, they also facilitate certain techniques. Much like a snowboarder, the rider can perform spins and somersaults higher and farther than any skateboarder. At the same time, this means a faster learning curve. While a skateboarder will spend weeks learning the most basic maneuvers, such as the Ollie, or jump, a streetboarder (snakeboarder) simply has to strap himself to the board and jump as he would without the board attached.\n"}
{"id": "794439", "url": "https://en.wikipedia.org/wiki?curid=794439", "title": "Sodium sulfate", "text": "Sodium sulfate\n\nSodium sulfate (also known as sodium sulphate or sulfate of soda) is the inorganic compound with formula NaSO as well as several related hydrates. All forms are white solids that are highly soluble in water. With an annual production of 6 million tonnes, the decahydrate is a major commodity chemical product. It is mainly used for the manufacture of detergents and in the kraft process of paper pulping.\n\n\nThe decahydrate of sodium sulfate is known as Glauber's salt after the Dutch/German chemist and apothecary Johann Rudolf Glauber (1604–1670), who discovered it in 1625 in Austrian spring water. He named it \"sal mirabilis\" (miraculous salt), because of its medicinal properties: the crystals were used as a general purpose laxative, until more sophisticated alternatives came about in the 1900s.\n\nIn the 18th century, Glauber's salt began to be used as a raw material for the industrial production of soda ash (sodium carbonate), by reaction with potash (potassium carbonate). Demand for soda ash increased and the supply of sodium sulfate had to increase in line. Therefore, in the nineteenth century, the large scale Leblanc process, producing synthetic sodium sulfate as a key intermediate, became the principal method of soda ash production.\n\nSodium sulfate is very stable, being unreactive toward most oxidizing or reducing agents at normal temperatures. At high temperatures, it can be converted to sodium sulfide by carbothermal reduction:\n\nSodium sulfate is a neutral salt: its aqueous solutions exhibit a pH of 7. The neutrality of such solutions reflects the fact that sulfate is derived, formally, from the strong acid sulfuric acid. Furthermore, the Na ion, with only a single positive charge, only weakly polarizes its water ligands provided there are metal ions in solution. Sodium sulfate reacts with sulfuric acid to give the acid salt sodium bisulfate:\nThe equilibrium constant for this process depends on concentration and temperature.\n\nSodium sulfate has unusual solubility characteristics in water. Its solubility in water rises more than tenfold between 0 °C to 32.384 °C, where it reaches a maximum of 49.7 g/100 mL. At this point the solubility curve changes slope, and the solubility becomes almost independent of temperature. This temperature at 32.384 °C, corresponding to the release of crystal water and melting of the hydrated salt, serves as an accurate temperature reference for thermometer calibration.\n\nSodium sulfate is a typical electrostatically bonded ionic sulfate, containing Na ions and SO ions. The existence of sulfate in solution is indicated by the easy formation of insoluble sulfates when these solutions are treated with Ba or Pb salts:\nSodium sulfate displays a moderate tendency to form double salts. The only alums formed with common trivalent metals are NaAl(SO) (unstable above 39 °C) and NaCr(SO), in contrast to potassium sulfate and ammonium sulfate which form many stable alums. Double salts with some other alkali metal sulfates are known, including NaSO·3KSO which occurs naturally as the mineral glaserite. Formation of glaserite by reaction of sodium sulfate with potassium chloride has been used as the basis of a method for producing potassium sulfate, a fertiliser. Other double salts include 3NaSO·CaSO, 3NaSO·MgSO (vanthoffite) and NaF·NaSO.\n\nCrystals consist of [Na(OH)] ions with octahedral molecular geometry. These octahedral share edges such that eight of the 10 water molecules are bound to sodium and two others are interstitial, being hydrogen bonded to sulfate. These cations are linked to the sulfate anions via hydrogen bonds. The Na-O distances are 240 pm. Crystalline sodium sulfate decahydrate is also unusual among hydrated salts in having a measureable residual entropy (entropy at absolute zero) of 6.32 J·K·mol. This is ascribed to its ability to distribute water much more rapidly compared to most hydrates.\n\nThe world production of sodium sulfate, almost exclusively in the form of the decahydrate amounts to approximately 5.5 to 6 million tonnes annually (Mt/a). In 1985, production was 4.5 Mt/a, half from natural sources, and half from chemical production. After 2000, at a stable level until 2006, natural production had increased to 4 Mt/a, and chemical production decreased to 1.5 to 2 Mt/a, with a total of 5.5 to 6 Mt/a. For all applications, naturally produced and chemically produced sodium sulfate are practically interchangeable.\n\nTwo thirds of the world's production of the decahydrate (Glauber's salt) is from the natural mineral form mirabilite, for example as found in lake beds in southern Saskatchewan. In 1990, Mexico and Spain were the world's main producers of natural sodium sulfate (each around 500,000 tonnes), with Russia, United States and Canada around 350,000 tonnes each. Natural resources are estimated at over 1 billion tonnes.\n\nMajor producers of 200,000 to 1,500,000 tonnes/year in 2006 included Searles Valley Minerals (California, US), Airborne Industrial Minerals (Saskatchewan, Canada), Química del Rey (Coahuila, Mexico), Minera de Santa Marta and Criaderos Minerales Y Derivados, also known as Grupo Crimidesa (Burgos, Spain), Minera de Santa Marta (Toledo, Spain), Sulquisa (Madrid, Spain), Chengdu Sanlian Tianquan Chemical (Tianquan County, Sichuan, China), Hongze Yinzhu Chemical Group (Hongze District, Jiangsu, China), (Shanxi, China), Sichuan Province Chuanmei Mirabilite (, Dongpo District, Meishan, Sichuan, China), and Kuchuksulphat JSC (Altai Krai, Siberia, Russia).\n\nAnhydrous sodium sulfate occurs in arid environments as the mineral thenardite. It slowly turns to mirabilite in damp air. Sodium sulfate is also found as glauberite, a calcium sodium sulfate mineral. Both minerals are less common than mirabilite.\n\nAbout one third of the world's sodium sulfate is produced as by-product of other processes in chemical industry. Most of this production is chemically inherent to the primary process, and only marginally economical. By effort of the industry, therefore, sodium sulfate production as by-product is declining.\n\nThe most important chemical sodium sulfate production is during hydrochloric acid production, either from sodium chloride (salt) and sulfuric acid, in the Mannheim process, or from sulfur dioxide in the Hargreaves process. The resulting sodium sulfate from these processes is known as salt cake.\n\nThe second major production of sodium sulfate are the processes where surplus sodium hydroxide is neutralised by sulfuric acid, as applied on a large scale in the production of rayon. This method is also a regularly applied and convenient laboratory preparation.\n\nIn the laboratory it can also be synthesized from the reaction between sodium bicarbonate and magnesium sulfate.\n\nFormerly, sodium sulfate was also a by-product of the manufacture of sodium dichromate, where sulfuric acid is added to sodium chromate solution forming sodium dichromate, or subsequently chromic acid. Alternatively, sodium sulfate is or was formed in the production of lithium carbonate, chelating agents, resorcinol, ascorbic acid, silica pigments, nitric acid, and phenol.\n\nBulk sodium sulfate is usually purified via the decahydrate form, since the anhydrous form tends to attract iron compounds and organic compounds. The anhydrous form is easily produced from the hydrated form by gentle warming.\n\nMajor sodium sulfate by-product producers of 50–80 Mt/a in 2006 include Elementis Chromium (chromium industry, Castle Hayne, NC, US), Lenzing AG (200 Mt/a, rayon industry, Lenzing, Austria), Addiseo (formerly Rhodia, methionine industry, Les Roches-Roussillon, France), Elementis (chromium industry, Stockton-on-Tees, UK), Shikoku Chemicals (Tokushima, Japan) and Visko-R (rayon industry, Russia).\n\nWith US pricing at $30 per tonne in 1970,6 up to $90 per tonne for salt cake quality and $130 for better grades, sodium sulfate is a very cheap material. The largest use is as filler in powdered home laundry detergents, consuming approx. 50% of world production. This use is waning as domestic consumers are increasingly switching to compact or liquid detergents that do not include sodium sulfate.\n\nAnother formerly major use for sodium sulfate, notably in the US and Canada, is in the Kraft process for the manufacture of wood pulp. Organics present in the \"black liquor\" from this process are burnt to produce heat, needed to drive the reduction of sodium sulfate to sodium sulfide. However, due to advances in the thermal efficiency of the Kraft recovery process in the early 1960s, more efficient sulfur recovery was achieved and the need for sodium sulfate makeup was drastically reduced\n. Hence, the use of sodium sulfate in the US and Canadian pulp industry declined from 1.4 Mt/a in 1970 to only approx. 150,000 tonnes in 2006.\n\nThe glass industry provides another significant application for sodium sulfate, as second largest application in Europe. Sodium sulfate is used as a fining agent, to help remove small air bubbles from molten glass. It fluxes the glass, and prevents scum formation of the glass melt during refining. The glass industry in Europe has been consuming from 1970 to 2006 a stable 110,000 tonnes annually.\n\nSodium sulfate is important in the manufacture of textiles, particularly in Japan, where it is the largest application. Sodium sulfate helps in \"levelling\", reducing negative charges on fibres so that dyes can penetrate evenly. Unlike the alternative sodium chloride, it does not corrode the stainless steel vessels used in dyeing. This application in Japan and US consumed in 2006 approximately 100,000 tonnes.\n\nThe high heat storage capacity in the phase change from solid to liquid, and the advantageous phase change temperature of 32 °C (90 °F) makes this material especially appropriate for storing low grade solar heat for later release in space heating applications. In some applications the material is incorporated into thermal tiles that are placed in an attic space while in other applications the salt is incorporated into cells surrounded by solar–heated water. The phase change allows a substantial reduction in the mass of the material required for effective heat storage (the heat of fusion of sodium sulfate decahydrate is 82 kJ/mol or 252 kJ/kg), with the further advantage of a consistency of temperature as long as sufficient material in the appropriate phase is available.\n\nFor cooling applications, a mixture with common sodium chloride salt (NaCl) lowers the melting point to 18 °C (64 °F). The heat of fusion of NaCl·NaSO·10HO, is actually \"increased\" slightly to 286 kJ/kg.\n\nIn the laboratory, anhydrous sodium sulfate is widely used as an inert drying agent, for removing traces of water from organic solutions. It is more efficient, but slower-acting, than the similar agent magnesium sulfate. It is only effective below about 30 °C, but it can be used with a variety of materials since it is chemically fairly inert. Sodium sulfate is added to the solution until the crystals no longer clump together; the two video clips (see above) demonstrate how the crystals clump when still wet, but some crystals flow freely once a sample is dry.\n\nGlauber's salt, the decahydrate, was historically used as a laxative. It is effective for the removal of certain drugs such as paracetamol (acetaminophen) from the body, for example, after an overdose.\n\nIn 1953, sodium sulfate was proposed for heat storage in passive solar heating systems. This takes advantage of its unusual solubility properties, and the high heat of crystallisation (78.2 kJ/mol).\n\nOther uses for sodium sulfate include de-frosting windows, starch manufacture, as an additive in carpet fresheners, and as an additive to cattle feed.\n\nAt least one company, Thermaltake, makes a laptop computer chill mat (iXoft Notebook Cooler) using sodium sulfate decahydrate inside a quilted plastic pad. The material slowly turns to liquid and recirculates, equalizing laptop temperature and acting as an insulation.\n\nAlthough sodium sulfate is generally regarded as non-toxic, it should be handled with care. The dust can cause temporary asthma or eye irritation; this risk can be prevented by using eye protection and a paper mask. Transport is not limited, and no Risk Phrase or Safety Phrase applies.\n\n"}
{"id": "52046282", "url": "https://en.wikipedia.org/wiki?curid=52046282", "title": "Stable salt reactor", "text": "Stable salt reactor\n\nThe stable salt reactor (SSR) is a nuclear reactor design proposed by Moltex Energy Ltd based in the United Kingdom.\n\nMoltex Energy proposes the SSR as an improved version of the molten salt reactor with improved safety characteristics and economics. \nStable salt reactors do not need expensive containment structures and components to keep them in a stable condition. \nIn the Chernobyl accident the two most troublesome by-products were caesium-137 and iodine-131 in gaseous form, which are harmful to land and people. This hazard is inherent with any water-cooled reactor, but in a molten salt reactor these elements do not exist in the form of a gas — they are bound in nonvolatile salts, which cannot escape the plant in most accident scenarios.\n\nMoltex Energy have used computational fluid dynamics to prove the feasibility of a static fuel concept. Solid fuel in fuel rods is replaced by molten salt fuel, in assemblies that are very similar to current light water reactor technology. The result is a simple, low cost reactor that uses components from today’s nuclear fleet but has all the safety advantages of a molten salt fuel.\n\nThe basic unit of the reactor core is the fuel assembly. Each assembly contains nearly 400 fuel tubes of 10 mm diameter with a 1 mm helical wire wrap filled to a height of 1.6 metres with fuel salt. The tubes have diving bell gas vents at the top to allow fission gasses to escape.\n\nAn unusual design feature of the reactor is that its core is rectangular in shape. This is neutronically inefficient compared to a cylindrical core but allows for simpler movement of fuel assemblies, and extension of the core as required simply by adding additional modules.\n\nThe assemblies move laterally through the core, with fresh assemblies entering at the sides in opposite directions, similar to the refuelling of CANDU reactors. They are raised only slightly to move them into an adjacent slot, remaining in the coolant at all times.\n\nThe reactor core is composed of modules, each with a thermal output of 375 MW, containing 10 rows of 10 fuel assemblies, upper and lower support grids, heat exchangers, pumps, control assemblies and instrumentation. \nTwo or more of these modules are assembled side by side in a rectangular reactor tank.\nA 1200 MWe reactor is possible in a tank that can fit on the back of a truck, making the technology significantly more compact than today’s reactors.\n\nThe modules (without fuel assemblies) are planned to be delivered to the construction site pre-assembled and pre-tested as single road-transportable components. \nThey are installed into the stainless steel tank when the civil works phase is complete during commissioning.\n\nThe upper part of the reactor consists of an argon containment dome, incorporating two crane-type systems, a low-load device designed to move fuel assemblies within the reactor core and a high-load device designed to raise and lower fuel assemblies into the coolant, and to replace entire modules should that be necessary. \nAll reactor maintenance is planned to be carried out remotely.\n\nThe fuel in the SSR is composed of two-thirds sodium chloride (table salt) and one-third plutonium and mixed lanthanide/actinide trichlorides. \nFuel for the initial six reactors is expected to come from stocks of pure plutonium dioxide from PUREX reprocessed conventional spent nuclear fuel, mixed with pure depleted uranium trichloride. \nFurther fuel can come from reprocessed nuclear waste from today’s fleet of reactors.\n\nTrichlorides are more thermodynamically stable than the corresponding fluoride salts and can therefore be maintained in a strongly reducing state by contact with sacrificial nuclear grade zirconium metal added as a coating on, or an insert within, the fuel tube.\nAs a result, the fuel tube can be made from standard nuclear certified steel without risk of corrosion. \nSince the reactor operates in the fast spectrum, the tubes will be exposed to very high neutron flux and suffer high damage (dpa) levels, estimated at 100–200 dpa over the tube life. \nHighly neutron damage tolerant steels such as PE16 will therefore be used for the tubes. \nAssessment is also being carried out on other steels with fast-neutron data such as HT9, NF616 and 15-15Ti.\n\nThe average power density in the fuel salt is 150 kW/l which allows a large temperature margin below the boiling point of the salt.\nPower peaking to double this level for substantial periods would not exceed the safe operating conditions for the fuel tube.\n\nThe coolant salt in the reactor tank is a sodium zirconium fluoride mixture. The zirconium is not nuclear-grade and still contains ~2% hafnium. This has minimal effect on core reactivity but makes the coolant salt low-cost and a highly effective neutron shield. One metre of coolant reduces neutron flux by four orders of magnitude. All components in the SSR are protected by this coolant shield.\n\nThe coolant also contains 1 mol% zirconium metal (which dissolves forming 2 mol% ZrF). This reduces its redox potential to a level making it virtually non-corrosive to standard steels. The reactor tank, support structures and heat exchangers can therefore be constructed from standard 316L stainless steel.\n\nThe coolant salt is circulated through the reactor core by four pumps attached to the heat exchangers in each module. Flow rates are modest, approximately 1 m/s with resulting low requirement for pump power. There is redundancy to continue operation in the event of a pump failure.\n\nThe stable salt reactor was designed with intrinsic safety characteristics being the first line of defence. There is no operator or active system required to maintain the reactor in a safe and stable state. The following are primary intrinsic safety features behind the SSR:\n\nThe SSR is self-controlling and no mechanical control is required. There is zero excess reactivity at any time; i.e. the rate at which fission generates heat matches the rate at which heat is being removed. This is made possible by the combination of a high negative temperature coefficient of reactivity and the ability to continually extract heat from the fuel tubes. As heat is taken out of the system the temperature drops, causing the reactivity to go up. When the reactor heats up the reactivity goes down, making it stable at all times.\n\nUse of molten salt fuel with the appropriate chemistry eliminates the hazardous volatile iodine and caesium, making multi-layered containment unnecessary in preventing airborne radioactive plumes in severe accident scenarios.\nThe noble gasses xenon and krypton would leave the reactor core in normal operation, but be trapped until their radioactive isotopes decay, so there would be very little that could be released in an accident.\n\nHigh pressures within a reactor provide a driving force for dispersion of radioactive materials from a water-cooled reactor. Molten salt fuels and coolants have boiling points far above the SSR's operating temperature, so its core runs at atmospheric pressure. Physical separation of the steam generating system from the radioactive core by means of a secondary coolant loop eliminates that driving force from the reactor. High pressures within fuel tubes are avoided by venting off fission gases into the surrounding coolant salt.\n\nZirconium in pressurized water reactors (PWRs) and sodium in fast reactors both create the potential for severe explosion and fire risks. \nThere are no chemically-reactive materials used in the SSR.\n\nImmediately after a nuclear reactor shuts down, almost 7% of its previous operating power continues to be generated, from the decay of short-halflife fission products. In conventional reactors, removing this decay heat passively is challenging because of their low temperatures. The SSR operates at much higher temperatures so this heat can be rapidly transferred away from the core. In the event of a reactor shutdown and failure of all active heat-removal systems in the SSR, decay heat from the core dissipates into air cooling ducts around the perimeter of the tank that operate continually. The main heat transfer mechanism is radiative. Heat transfer goes up substantially with temperature so is negligible at operating conditions but is sufficient for decay heat removal at higher accident temperatures. The reactor components are not damaged during this process and the plant can be restarted afterwards.\n\nMost countries that use nuclear power choose to store spent nuclear fuel deep underground until its radioactivity has reduced to levels similar to natural uranium. Acting as a wasteburner, the SSR offers a different way to manage this waste.\n\nOperating in the fast spectrum, the SSR is effective at transmuting long-lived actinides into more stable isotopes. Today’s reactors that are fuelled by reprocessed spent fuel need very-high-purity plutonium to form a stable pellet. The SSR can have any level of lanthanide and actinide contamination in its fuel as long as it can still go critical. This low level of purity greatly simplifies the reprocessing method for existing waste.\n\nThe method used is based on pyroprocessing and is well understood. A 2016 report by the Canadian National Laboratories on reprocessing of CANDU fuel estimates that pyroprocessing would be about half the cost of more conventional reprocessing. Pyroprocessing for the SSR uses only one third of the steps of conventional pyroprocessing, which will make it even cheaper. It is potentially competitive with the cost of manufacturing fresh fuel from mined uranium.\n\nThe waste stream from the SSR will be in the form of solid salt in tubes. This can be vitrified and stored underground for over 100,000 years as is planned today, or it can be reprocessed. In that case, fission products would be separated out and safely stored at ground level for the few hundred years needed for them to decay to levels similar to uranium ore. The troublesome long-lived actinides and the remaining fuel would go back into the reactor where they to be burnt and transmuted into more-stable isotopes.\n\nStable salt reactor technology is highly flexible and can be adapted to several different reactor designs. The use of molten salt fuel in standard fuel assemblies allows Stable Salt versions of many of the large variety of nuclear reactors considered for development worldwide. The focus today however is to allow rapid development and roll out of low-cost reactors.\n\nMoltex Energy is focussed on deployment of the fast spectrum SSR-Wasteburner discussed above. This decision is primarily driven by the lower technical challenges and lower predicted cost of this reactor.\n\nIn the longer term the fundamental breakthrough of molten fuel salt in tubes opens up other options. These have been developed to a conceptual level to confirm their feasibility. They include:\n\nWith this range of reactor options and the large global reserves of uranium and thorium available, the Stable Salt Reactor can fuel the planet for several thousands of years.\n\nThe overnight capital cost of the stable salt reactor was estimated at $1,950/kW by an independent UK nuclear engineering firm. \nFor comparison, the capital cost of a modern pulverised coal power station in the United States is $3,250/kW and the cost of large-scale nuclear is $5,500/kW. \nFurther reductions to this overnight cost are expected for modular factory-based construction.\n\nThis low capital cost results in a levelised cost of electricity (LCOE) of $44.64/MWh with substantial potential for further reductions, because of the greater simplicity and intrinsic safety of the SSR.\n\nThe International Energy Agency predicts that nuclear will maintain a constant small role in global energy supply with a market opportunity of 219 GWe up to 2040. \nWith the improved economics of the SSR, Moltex Energy predicts that it has the potential to access a market of over 1,300 GWe by 2040.\n\nA patent was granted in 2014. \nA design has been developed and a safety case is well under way, to begin formal discussions with nuclear regulatory authorities.\n\n"}
{"id": "28676576", "url": "https://en.wikipedia.org/wiki?curid=28676576", "title": "Svenska Spindlar", "text": "Svenska Spindlar\n\nThe book or (Swedish and Latin, respectively, for \"Swedish spiders\") was one of the major works of the Swedish arachnologist and entomologist Carl Alexander Clerck and appeared in Stockholm in the year 1757. It was the first comprehensive book on the spiders of Sweden and one of the first regional monographs of a group of animals worldwide. The full title of the work was \" – \", (\"Swedish spiders into their main genera separated, and as sixty and a few particular species described and with illuminated figures illustrated\") and included 162 pages of text (eight pages were unpaginated) and 6 colour plates. It was published in Swedish, with a Latin translation printed in a slightly smaller font below the Swedish text.\n\nClerck described in detail 67 species of Swedish spiders, and for the first time in a zoological work consistently applied binomial nomenclature as proposed by Carl Linnaeus and used for the first time for botanical names in his 1753 work \"Species Plantarum\", and which he presented in 1758 in the 10th edition of his work \"Systema Naturae\" for more than 4,000 animal species.\n\n\"Svenska Spindlar\" is the first zoological work to make systematic use of binomial nomenclature, and the only pre-Linnaean source to be recognised as a taxonomic authority for such names.\n\nClerck explained in the last (9th of the 2nd part) chapter of his work that in contrast to previous authors he used the term \"spider\" in the strict sense, for animals possessing eight eyes and separated prosoma and opisthosoma, and that his concept of this group of animals did not include Opiliones (because they had two eyes and a broadly joined prosoma and opisthosoma) and other groups of arachnids.\n\nFor all spiders Clerck used a single generic name (\"Araneus\"), to which was added a specific name which consisted of only one word. Each species was presented in the Swedish text with their Latin scientific names, followed by detailed information containing the exact dates when he had found the animals, and a detailed description of eyes, legs and body. The differences between the sexes were also described. Each species was illustrated in impressively accurate drawings printed on coloured copper plates which were bound at the end of the volume.\n\nBecause of the exceptionally thorough treatment of the spider species, the scientific names proposed by Clerck (which were adopted by Carl Linnaeus in his \"Systema Naturae\" in 1758 with only minor modifications) had traditionally been recognized by arachnologists as binomial and available. In 1959 the ICZN Commission decided that Clerck's work should be available for zoological nomenclature, but the International Code of Zoological Nomenclature did not mention Clerck's work. Only after 1999 was this officially recognized in the Code. This means that in case of doubt the spelling of a spider name as from Clerck's 1757 work has priority over that proposed by Linnaeus in 1758 (an example is \"Araneus\" instead of \"Aranea\"), and that Clerck's spiders were the first animals in modern zoology to have obtained an available scientific name in the Linnean system.\n\nIn the late 1800s, Clerck's 1757 work was commonly accepted as the first application of binomial nomenclature to spiders. In 1959 the ICZN Commission ruled that the date 1758 should be used for Clerck's names, this date 1758 was repeated to apply to Clerck's names in the 4th edition of the International Code of Zoological Nomenclature in 1999.\n\nIn a complete binomial name with author and year, the year corresponds to the year of publication of the original source. Since 2000, the ICZN Code includes an exception of this very basic rule. From the beginning on the new provision in the Code has been misunderstood by many researchers who believed that by setting the date for Clerck's work to 1758 (overriding its true date 1757) and the date for \"Systema Naturae\" to 1 January 1758, the priority was changed. In 2007, a case was even brought before the Commission because the researchers were no longer sure whether the generic name should be \"Araneus\" Clerck or \"Aranea\" Linnaeus. In their judgement the year 1758 for Clerck's \"Svenska Spindlar\" could be interpreted in a way that the Linnean work from 1 January 1758 should have priority. In 2009 the Commission saw itself forced to repeat once more, although this was already explicit in the Code's Article 3.1, that the name \"Araneus\" established by Clerck shall have priority and be used for the genus.\n\n\"Svenska Spindlar\" lists the following 67 species of spider; their current identities follow Platnick (2000–2010).\nChapter 2 (Araneidae, Tetragnathidae)\n\nChapter 3 (Theridiidae, Nesticidae, Linyphiidae)\n\nChapter 4 (Agelenidae, Clubionidae)\n\nChapter 5 (Lycosidae, Pisauridae)\n\nChapter 6 (Salticidae)\n\nChapter 7 (Thomisidae, Philodromidae, Sparassidae)\n\nChapter 8 (Cybaeidae)\n"}
{"id": "4092180", "url": "https://en.wikipedia.org/wiki?curid=4092180", "title": "T. F. Bourdillon", "text": "T. F. Bourdillon\n\nThomas Fulton Bourdillon (1849 – 19 December 1930) was Conservator of Forests in the princely state of Travancore.\n\nHe came to Travancore (at present Southern Kerala) as a planter in 1871 and was appointed by the Travancore Durbar in 1886 as a special forest officer to explore the forests and to report on their resources. In 1891 he was appointed as Conservator of Forests, a position he retained till his retirement in June 1908. He was a keen botanist and an all-round forest officer. During his period, he brought the Department to a high state of efficiency.\n\nIn 1908, he authored the first book on the trees of the region \"The Forest Trees of Travancore\". In 1901 he was admitted a Fellow of the Linnean Society. He also wrote about bird life to Allan Octavian Hume and contributed many articles on forestry to the \"Indian Forester\". He worked in close association with other naturalists of his time including R. H. Beddome and Harold S. Ferguson.\n\nToday there exists in Arienkavu on the northern side of Shendurney valley a place called Bourdillon's Plot which was the location of the first plot where teak was planted using stumps in 1891. This technique was developed to grow teak sustainably for the needs of the navy. This attempt to grow teak for the needs of the Royal Navy was started by Mr. H. V. Conolly, the then Collector of Malabar.\n\nThe tree species Aglaia bourdillonii, bird subspecies great eared-nightjar \"Eurostopodus macrotis bourdilloni\" and the blackbird \"Turdus merula bourdilloni\" are named after him.\n\n"}
{"id": "12034210", "url": "https://en.wikipedia.org/wiki?curid=12034210", "title": "Tokaimura nuclear accident", "text": "Tokaimura nuclear accident\n\nThere have been two Tokaimura nuclear accidents at the nuclear facility at Tōkai, Ibaraki: on 11 March 1997, an explosion occurred in a Dōnen plant, and \non 30 September 1999, a serious criticality accident happened in a JCO plant.\n\nThe first Tokaimura nuclear accident was the accident which occurred on 11 March 1997, in a nuclear reprocessing plant of the Dōnen (Power Reactor and Nuclear Fuel Development Corporation). Another name is the .\n\nOn the night of Tuesday 11 March 1997, a small explosion occurred in a nuclear reprocessing plant of the Dōnen. Windows were smashed and smoke escaped to the atmosphere. On Thursday, workers repaired thirty broken windows and three doors with duct tape. They had been damaged during the blast. At least 37 workers were exposed to elevated levels of radiation during the incident.\n\nA week after the event, meteorological officials detected unusually high levels of caesium south-west of the plant.\n\nThe second and more serious Tokaimura nuclear accident ( \"Tōkai-mura JCO-rinkai-jiko\") indicates the nuclear disaster which occurred on 30 September 1999, resulting in two deaths. It was the worst civilian nuclear radiation accident in Japan prior to the Fukushima Daiichi nuclear disaster of 2011.\n\nThe criticality accident occurred in a uranium reprocessing facility operated by JCO (formerly Japan Nuclear Fuel Conversion Co.), a subsidiary of Sumitomo Metal Mining Co. in the village of Tōkai, Naka District, Ibaraki Prefecture.\n\nThe accident occurred as three workers, Hisashi Ouchi, Masato Shinohara, and Yutaka Yokokawa, were preparing a small batch of fuel for the Jōyō experimental fast breeder reactor, using uranium enriched to 18.8% with the fissile radionuclide (radioisotope) U235 (with the remainder being the fissionable-only U238). It was JCO's first batch of fuel for that reactor in three years, and no proper qualification and training requirements appear to have been established to prepare those workers for the job. At around 10:35, a precipitation tank reached critical mass when its fill level, containing about of uranium, reached about .\n\nCriticality was reached upon the technicians adding a seventh bucket of an aqueous uranyl nitrate solution to the tank. The nuclear fission chain reaction became self-sustaining and began to emit intense gamma and neutron radiation. At the time of the criticality event, Ouchi had his body draped over the tank while Shinohara stood on a platform to pour the solution into it; Yokokawa was sitting at a desk four meters away. All three technicians observed a blue flash (possibly Cherenkov radiation) and gamma-radiation alarms sounded. \n\nTechnicians Ouchi and Shinohara immediately experienced pain, nausea, difficulty breathing, and other symptoms. Ouchi then began to vomit in the decontamination room a few minutes later and lost consciousness shortly after. Fission products such as yttrium94 and barium140 began contaminating the building.\n\nBeing a wet process with an intended liquid result, the water sustained the chain reaction by serving as a neutron moderator, whereby neutrons emitted from fissioned nuclei are slowed so they are more readily absorbed by neighboring nuclei, inducing them to fission in turn. The criticality continued intermittently for about 20 hours. As the solution boiled vigorously, steam bubbles attenuated the liquid water's action as a neutron moderator (see \"Void coefficient\" ) and the solution lost criticality. However, the reaction resumed as the solution cooled and the voids disappeared. \n\nThe following morning, workers permanently stopped the reaction by draining water from a cooling jacket surrounding the precipitation tank since that water was serving as a neutron reflector. A boric acid solution (boron being a good neutron absorber) was then added to the tank to ensure that the contents remained subcritical. These operations exposed 27 workers to radioactivity.\n\nThe cause of the accident was the workers adding a uranyl nitrate solution which contained about 16 kg of uranium into the precipitation tank. This greatly exceeded the tank's uranium limit of 2.4 kg and caused an instantaneous and uncontrolled nuclear fission. Under correct procedures, the uranyl nitrate would have been stored inside a buffer tank and then pumped from there into the precipitation tank at intervals of the correct volume level not exceeding 2.4 kg. \n\nIn this case, the workers bypassed using any buffer tanks entirely and instead poured the uranyl nitrate directly into the precipitation tank with a stainless steel bucket rather than using a pump. The buffer tank would have actually held this solution safely, as it had a tall and narrow geometry and was designed to prevent criticality. The precipitation tank however had not been designed to hold this type of solution and was not configured to prevent criticality.\n\nFive hours after the start of the criticality, evacuation commenced of some 161 people from 39 households within a 350-meter radius from the conversion building. Residents were allowed home two days later with sandbags and other shielding to protect from residual gamma radiation. Twelve hours after the start of the incident residents within 10 km were asked to stay indoors as a precautionary measure, and this restriction was lifted the following afternoon.\n\nDozens of emergency workers and nearby residents were hospitalized and hundreds of thousands of others were forced to remain indoors for 24 hours; 39 of the workers were exposed to the radiation. At least 667 workers, emergency responders, and nearby residents were exposed to excess radiation as a result of the accident.\n\nBy measuring the concentration of sodium-24, created by a neutron activation whereby sodium-23 nuclei were rendered radioactive by absorbing neutrons from the accident, it was possible to deduce the dose received by the technicians. According to the STA, Hisashi Ouchi was exposed to 17 sieverts (Sv) of radiation, Masato Shinohara received 10 Sv, and Yutaka Yokokawa 3 Sv. By comparison, a dose of .05 sieverts is the maximum allowable annual dose for Japanese nuclear workers. A dose of 8 Sv (800 rem) is normally fatal and more than 10 Sv almost invariably so. Normal background radiation amounts to an annual exposure of about 3 mSv (millisieverts). There were 56 plant workers whose exposures ranged up to 23 mSv and a further 21 workers received elevated doses when draining the precipitation tank. Seven workers immediately outside the plant received doses estimated at 6-15 mSv (combined neutron and gamma effects).\n\nThe two technicians who received the higher doses, Ouchi and Shinohara, died several months later. Ouchi suffered serious burns to most of his body, experienced severe damage to his internal organs, and had a near-zero white blood cell count. Shinohara received numerous skin grafts, which were successful, but he ultimately succumbed to infection due to the damage his immune system sustained in the incident. Ouchi died on December 21, 1999, while Shinohara died on April 27, 2000. \n\nThe cause of the accident was said to be \"human error and serious breaches of safety principles\", according to the International Atomic Energy Agency.\n\nIn September 2000 JCO agreed to pay $121 million in compensation to settle 6,875 claims from people exposed to radiation and affected agricultural and service businesses.\n\nIn April 2001 six employees, including the plant administrator and accident survivor Yutaka Yokokawa, plead guilty to a charge of negligence resulting in death. The JCO President also plead guilty on behalf of the company. The court heard that a 1995 safety committee had approved the use of buckets in the procedure, and a widely distributed but unauthorised 1996 manual recommended the use of buckets in making the solution. A Science and Technology Agency report indicated JCO management had since 1993 permitted the use of a stainless steel bucket as a shortcut in the process, even though it was contrary to written procedures.\n\n\n"}
{"id": "1999139", "url": "https://en.wikipedia.org/wiki?curid=1999139", "title": "Turbomachinery", "text": "Turbomachinery\n\nTurbomachinery, in mechanical engineering, describes machines that transfer energy between a rotor and a fluid, including both turbines and compressors. While a turbine transfers energy from a fluid to a rotor, a compressor transfers energy from a rotor to a fluid.\n\nThese two types of machines are governed by the same basic relationships including Newton's second Law of Motion and Euler's pump and turbine equation for compressible fluids. Centrifugal pumps are also turbomachines that transfer energy from a rotor to a fluid, usually a liquid, while turbines and compressors usually work with a gas.\n\nThe first use of turbomachines were technically water wheels between the 3rd and 1st century BCE, credited to the people in the Mediterranean region (see more at water wheel). The first real modern turbomachines did not appear until the late 1880s. It was not until the industrial revolution, however, steam power started to be utilized with reciprocating engines and turbines, which opened up the potential of steam power. The first impulse type turbine was created by Carl Gustaf de Laval in 1883. This was closely followed by the first practical reaction type turbine in 1884, built by Charles Parsons. Parsons’ first design was a multi-stage axial-flow unit, which George Westinghouse acquired and began manufacturing in 1895, while General Electric acquired de Laval’s designs in 1897. Since then, development has skyrocketed from Parsons’ early design, producing 0.746 kW, to modern nuclear steam turbines producing upwards of 1500 MW. Today, steam turbines account for roughly 90% of electrical power generated in the United States. The first patents for gas turbines were filed in 1791 by John Barber. Then the first functioning industrial gas turbines were used in the late 1890s to power street lights (Meher-Homji, 2000).\n\nIn general, the two kinds of turbomachines encountered in practice are open and closed turbomachines. Open machines such as propellers, windmills, and unshrouded fans act on an infinite extent of fluid, whereas closed machines operate on a finite quantity of fluid as it passes through a housing or casing.\n\nTurbomachines are also categorized according to the type of flow. When the flow is parallel to the axis of rotation, they are called axial flow machines, and when flow is perpendicular to the axis of rotation, they are referred to as radial (or centrifugal) flow machines. There is also a third category, called mixed flow machines, where both radial and axial flow velocity components are present.\n\nTurbomachines may be further classified into two additional categories: those that absorb energy to increase the fluid pressure, i.e. pumps, fans, and compressors, and those that produce energy such as turbines by expanding flow to lower pressures. Of particular interest are applications which contain pumps, fans, compressors and turbines. These components are essential in almost all mechanical equipment systems, such as power and refrigeration cycles.\n\nAny device that extracts energy from or imparts energy to a continuously moving stream of fluid can be called a turbomachine. Elaborating, a turbomachine is a power or head generating machine which employs the dynamic action of a rotating element, the rotor; the action of the rotor changes the energy level of the continuously flowing fluid through the machine. Turbines, compressors and fans are all members of this family of machines.\n\nIn contrast to positive displacement machines (particularly of the reciprocating type which are low speed machines based on the mechanical and volumetric efficiency considerations), the majority of turbomachines run at comparatively higher speeds without any mechanical problems and volumetric efficiency close to one hundred percent.\n\nTurbomachines can be categorized on the basis of the direction of energy conversion:\n\n\nTurbomachines can be categorized on the basis of the nature of flow path through the passage of the rotor:\n\"Axial flow turbomachines\" - When the path of the through-flow is wholly or mainly parallel to the axis of rotation, the device is termed an axial flow turbomachine. The radial component of the fluid velocity is negligible. Since there is no change in the direction of the fluid, several axial stages can be used to increase power output.\n\nA Kaplan turbine is an example of an axial flow turbine.\n\nIn the figure:\n\n\"Radial flow turbomachines\" - When the path of the throughflow is wholly or mainly in a plane perpendicular to the rotation axis, the device is termed a radial flow turbomachine. Therefore, the change of radius between the entry and the exit is finite. A Radial turbomachine can be inward or outward flow type depending on the purpose that needs to be served. Outward flow type increases the energy level of the fluid and vice versa. Due to continuous change in direction, several radial stages are generally not used.\n\nA centrifugal pump is an example of a radial flow turbomachine.\n\n\"Mixed flow turbomachines\" – When axial and radial flow are both present and neither is negligible, the device is termed a mixed flow turbomachine. It combines flow and force components of both radial and axial types.\n\nA Francis turbine is an example of a mixed-flow turbine.\n\nTurbomachines can finally be classified on the relative magnitude of the pressure changes that take place across a stage:\n\"Impulse Turbomachines\" operate by accelerating and changing the flow direction of fluid through a stationary nozzle (the stator blade) onto the rotor blade. The nozzle serves to change the incoming pressure into velocity, the enthalpy of the fluid decreases as the velocity increases. Pressure and enthalpy drop over the rotor blades is minimal. Velocity will decrease over the rotor.\n\nNewton's second law describes the transfer of energy. Impulse turbomachines do not require a pressure casement around the rotor since the fluid jet is created by the nozzle prior to reaching the blading on the rotor.\n\nA Pelton wheel is an impulse design.\n\"Reaction Turbomachines\" operate by reacting to the flow of fluid through aerofoil shaped rotor and stator blades. The velocity of the fluid through the sets of blades increases slightly (as with a nozzle) as it passes from rotor to stator and vice versa. The velocity of the fluid then decreases again once it has passed between the gap. Pressure and enthalpy consistently decrease through the sets of blades.\n\nNewton's third law describes the transfer of energy for reaction turbines. A pressure casement is needed to contain the working fluid. For compressible working fluids, multiple turbine stages are usually used to harness the expanding gas efficiently.\n\nMost turbomachines use a combination of impulse and reaction in their design, often with impulse and reaction parts on the same blade.\n\nThe following dimensionless ratios are often used for the characterisation of fluid machines. They allow a comparison of flow machines with different dimensions and boundary conditions.\n\nHydro electric- Hydro-electric turbomachinery uses potential energy stored in water to flow over an open impeller to turn a generator which creates electricity\n\nSteam turbines- Steam turbines used in power generation come in many different variations. The overall principle is high pressure steam is forced over blades attached to a shaft, which turns a generator. As the steam travels through the turbine, it passes through smaller blades causing the shaft to spin faster, creating more electricity.\n\nGas turbines- Gas turbines work much like steam turbines. Air is forced in through a series of blades that turn a shaft. Then fuel is mixed with the air and causes a combustion reaction, increasing the power. This then causes the shaft to spin faster, creating more electricity.\n\nWindmills- Also known as a wind turbine, windmills are increasing in popularity for their ability to efficiently use the wind to generate electricity. Although they come in many shapes and sizes, the most common one is the large three-blade. The blades work on the same principle as an airplane wing. As wind passes over the blades, it creates an area of low and high pressure, causing the blade to move, spinning a shaft and creating electricity. It is most like a steam turbine, but work with an infinite supply of wind.\n\nSteam turbine- Steam turbines in marine applications are very similar to those in power generation. The few differences between them are size and power output. Steam turbines on ships are much smaller because they don’t need to power a whole town. They aren’t very common because of their high initial cost, high specific fuel consumption, and expensive machinery that goes with it.\n\nGas turbines- Gas turbines in marine applications are becoming more popular due to their smaller size, increased efficiency, and ability to burn cleaner fuels. They run just like gas turbines for power generation, but are also much smaller and do require more machinery for propulsion. They are most popular in naval ships as they can be at a dead stop to full power in minutes (Kayadelen, 2013), and are much smaller for a given amount of power.\nFlow of air through a turbocharger and engine\n\nWater jet- Essentially a waterjet drive is like an aircraft turbojet with the difference that the operating fluid is water instead of air. Water jets are best suited to fast vessels and are thus used often by the military. Water jet propulsion has many advantages over other forms of marine propulsion, such as stern drives, outboard motors, shafted propellers and surface drives.\n\nTurbochargers- Turbochargers are one of the most popular turbomachines. They are used mainly for adding power to engines by adding more air. It combines both forms of turbomachines. Exhaust gases from the engine spin a bladed wheel, much like a turbine. That wheel then spins another bladed wheel, sucking and compressing outside air into the engine.\n\nSuperchargers- Superchargers are used for engine-power enhancement as well, but only work off the principle of compression. They use the mechanical power from the engine to spin a screw or vein, some way to suck in and compress the air into the engine.\n\nPumps- Pumps are another very popular turbomachine. Although there are very many different types of pumps, they all do the same thing. Pumps are used to move fluids around using some sort of mechanical power, from electric motors to full size diesel engines. Pumps have thousands of uses, and are the true basis to turbomachinery (Škorpík, 2017).\n\nAir compressors- Air compressors are another very popular turbomachine. They work on the principle of compression by sucking in and compressing air into a holding tank. Air compressors are one of the most basic turbomachines.\n\nFans- Fans are the most general type of turbomachines. They work opposite of wind turbines. Mechanical power spins the blades, forcing air through them and forcing out. Basic desk-top fans to large turbofan airplane engines work this way.\n\nGas turbines- Aerospace gas turbines, more commonly known as jet engines, are the most common gas turbines. They are the most like power generation turbines because the electricity used on the airplane is from the turbines, while also providing the propulsion. These turbines are the smallest out of the industrial turbines, and are most often the most advanced.\n\nMany types of dynamic continuous flow turbomachinery exist. Below is a partial list of these types. What is notable about these turbomachines is that the same fundamentals apply to all. Certainly there are significant differences between these machines and between the types of analysis that are typically applied to specific cases. This does not negate the fact that they are unified by the same underlying physics of fluid dynamics, gas dynamics, aerodynamics, hydrodynamics, and thermodynamics.\n\n\n"}
{"id": "40625088", "url": "https://en.wikipedia.org/wiki?curid=40625088", "title": "Voltage sag", "text": "Voltage sag\n\nA voltage sag (U.S. English) or voltage dip (British English) is a short duration reduction in rms voltage which can be caused by a short circuit, overload or starting of electric motors. \nA voltage sag happens when the rms voltage decreases between 10 and 90 percent of nominal voltage for one-half cycle to one minute. Some references defines the duration of a sag for a period of 0.5 cycle to a few seconds, and longer duration of low voltage would be called a \"sustained sag\".\n\nThe term \"sag\" should not be confused with brownout which is the reduction of voltage for minutes or hours.\n\nThe term \"transient\" as used in power quality is an umbrella term and can refer to sags, but also to swells, dropouts, etc.\n\nVoltage swell is the opposite of voltage sag. Voltage swell, which is a momentary increase in voltage, happens when a heavy load turns off in a power system.\n\n\nThere are several factors which cause a voltage sag to happen:\n"}
{"id": "1917522", "url": "https://en.wikipedia.org/wiki?curid=1917522", "title": "Yttrium orthovanadate", "text": "Yttrium orthovanadate\n\nYttrium orthovanadate (YVO) is a transparent crystal. Undoped YVO is also used to make efficient high-power polarizing prisms similar to Glan–Taylor prisms.\n\nThere are two principal applications for doped Yttrium orthovanadate:\n\n\n\n"}
