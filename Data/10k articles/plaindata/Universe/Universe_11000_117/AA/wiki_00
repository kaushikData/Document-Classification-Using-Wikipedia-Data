{"id": "1386588", "url": "https://en.wikipedia.org/wiki?curid=1386588", "title": "21st Century Medicine", "text": "21st Century Medicine\n\n21st Century Medicine (21CM) is a California cryobiological research company which has as its primary focus the development of perfusates and protocols for viable long-term cryopreservation of human organs, tissues and cells at cryogenic temperatures (temperatures below −100 °C) through the use of vitrification. 21CM was founded in 1993.\n\nDr. Gregory M. Fahy, who pioneered the use of vitrification in reproductive cryopreservation, serves on the company’s Board of Directors and prioritizes, develops and directs the company’s research activities. He also manages all extramural collaborative research projects with universities, industry and research institutions to create specific products and services.\n\nThe company holds a number of patents, most notably for cryoprotectant mixtures that greatly reduce ice formation while minimizing cryoprotectant toxicity, as well as for synthetic ice-blockers that inexpensively simulate the antifreeze protein found in arctic organisms. Their website lists peer-reviewed journal publications based on research conducted in their laboratories. In 2004 21CM received a $900,000 grant from the U.S. National Institutes of Health (NIH) to study a preservation solution developed by the University of Rochester in New York for extending simple cold storage time of human hearts removed for transplant.\n\nAt the July 2005 annual conference of the Society for Cryobiology, 21st Century Medicine announced the vitrification of a rabbit kidney to -135 °C with their proprietary vitrification mixture. The kidney was successfully transplanted upon rewarming to a rabbit, the rabbit being euthanized on the 48th day for histological follow-up.\n\n\n"}
{"id": "48495014", "url": "https://en.wikipedia.org/wiki?curid=48495014", "title": "Aggreko Power Station", "text": "Aggreko Power Station\n\nThe Aggreko Power Station was a temporary 20-megawatt fuel oil-fired power station which was commissioned in October 2003 after Aggreko, the world's largest temporary power generation company, won an open bid offered by the Ceylon Electricity Board to overcome the Sri Lanka energy crisis faced in the 1990s to early 2000s. It was later decommissioned on 31 December 2012. Prior to decommissioning, Aggreko sold electricity to the Ceylon Electricity Board at a rate of .\n\n"}
{"id": "2710640", "url": "https://en.wikipedia.org/wiki?curid=2710640", "title": "Anticyclonic tornado", "text": "Anticyclonic tornado\n\nAn anticyclonic tornado is a tornado which rotates in a clockwise direction in the Northern Hemisphere and a counterclockwise direction in the Southern Hemisphere. The term is a naming convention denoting the anomaly from normal rotation which is cyclonic in upwards of 98 percent of tornadoes. Many anticyclonic tornadoes are smaller and weaker than cyclonic tornadoes, forming from a different process.\nMost strong tornadoes form in the inflow and updraft area bordering the updraft-downdraft interface (which is also near the mesoscale \"triple point\") zone of supercell thunderstorms. The thunderstorm itself is rotating, with a rotating updraft known as a mesocyclone, and then a smaller area of rotation at lower altitude the tornadocyclone (or low-level mesocyclone) which produces or enables the smaller rotation that is a tornado. All of these may be quasi-vertically aligned continuing from the ground to the mid-upper levels of the storm. All of these cyclones and scaling all the way up to large extratropical (low-pressure systems) and tropical cyclones rotate cyclonically. Rotation in these synoptic scale systems stems partly from the Coriolis effect, but thunderstorms and tornadoes are too small to be significantly affected. The common property here is an area of lower pressure, thus surrounding air flows into the area of less dense air forming cyclonic rotation. The rotation of the thunderstorm itself is induced mostly by vertical wind shear.\n\nVarious processes can produce an anticyclonic tornado. Most often they are satellite tornadoes of larger tornadoes which are directly associated with the tornadocyclone and mesocyclone. Occasionally anticyclonic tornadoes occur as an anticyclonic companion (mesoanticyclone) to a mesocyclone within a single storm. Anticyclonic tornadoes can occur as the primary tornado with a mesocyclone and under a rotating wall cloud. Also, anticyclonic supercells (with mesoanticyclone), which usually are storms that split and move to the left of the parent storm motion, though very rarely spawning tornadoes, spawn anticyclonic tornadoes. There is an increased incidence of anticyclonic tornadoes associated with tropical cyclones, and mesovortices within bow echoes may spawn anticyclonic tornadoes.\n\nThe first anticyclonic tornado associated with a mesoanticyclone was spotted on WSR-88D weather radar in Sunnyvale, California May 4, 1998. The tornado was an F-2 on the Fujita Scale.\n\n\n"}
{"id": "1201381", "url": "https://en.wikipedia.org/wiki?curid=1201381", "title": "Apothecaries' system", "text": "Apothecaries' system\n\nThe apothecaries' system or apothecaries' weights and measures is a historical system of mass and volume units that were used by physicians and apothecaries for medical recipes, and also sometimes by scientists. The English version of the system is closely related to the English troy system of weights, the pound and grain being exactly the same in both. It divides a pound into 12 ounces, an ounce into 8 drachms, and a drachm into 3 scruples or 60 grains. This exact form of the system was used in the United Kingdom; in some of its former colonies it survived well into the 20th century. The apothecaries' system of measures is a similar system of volume units based on the fluid ounce. For a long time, medical recipes were written in Latin, often using special symbols to denote weights and measures.\n\nThe use of different measure and weight systems depending on the purpose was an almost universal phenomenon in Europe between the decline of the Roman Empire and metrication. This was connected with international commerce, especially with the need to use the standards of the target market and to compensate for a common weighing practice that caused a difference between actual and nominal weight. In the 19th century, most European countries or cities still had at least a \"commercial\" or \"civil\" system (such as the English avoirdupois system) for general trading, and a second system (such as the troy system) for precious metals such as gold and silver. The system for precious metals was usually divided in a different way from the commercial system, often using special units such as the carat. More significantly, it was often based on different weight standards.\n\nThe apothecaries' system often used the same ounces as the precious metals system, although even then the number of ounces in a pound could be different. The apothecaries' pound was divided into its own special units, which were inherited (via influential treatises of Greek physicians such as Dioscorides and Galen, 1st and 2nd century) from the general-purpose weight system of the Romans. Where the apothecaries' weights and the normal commercial weights were different, it was not always clear which of the two systems was used in trade between merchants and apothecaries, or by which system apothecaries weighed medicine when they actually sold it. In old merchants' handbooks the former system is sometimes referred to as the pharmaceutical system, and distinguished from the apothecaries' system.\n\nThe traditional English apothecaries' system of weights is as shown in the table, the pound, ounce and grain being identical to the troy pound, ounce and grain.\nIn the United Kingdom, a reform in 1826 made the troy pound the primary weight unit (a role in which it was superseded half a century later by the Avoirdupois pound), but this had no effect on apothecaries' weights. However, the Medicinals Act of 1858 completely abolished the apothecaries' system in favour of the standard Avoirdupois system. The confusing variety of definitions and conversions for pounds and ounces is covered elsewhere in a . In the United States, the apothecaries' system remained official until it was abolished in 1971 in favour of the metric system.\n\nFrom the pound down to the scruple, the English apothecaries' system was a subset of the Roman weight system except that the troy pound and its subdivisions were slightly heavier than the Roman pound and its subdivisions. Similar systems were used all over Europe, but with considerable local variation described below under Variants.\n\nThe English-speaking countries also used a system of units of fluid measure, or in modern terminology volume units, based on the apothecaries' system. A volume of liquid that was approximately that of an apothecaries' ounce of water was called a fluid ounce, and was divided into fluid drachms and sometimes also fluid scruples. The analogue of the grain was called a minim.\n\nThe imperial and US systems differ in the size of the basic unit (the gallon or the pint, one gallon being equal to eight pints), and in the number of fluid ounces per pint. Apothecaries' systems for volumes were internationally much less common than those for weights. Before introduction of the imperial units in the UK, all apothecaries' measures were based on the wine gallon, which survived in the US under the name \"liquid gallon\" or \"wet gallon\".\n\nThe wine gallon was abolished in Britain in 1826, and this system was replaced by a new one based on the newly introduced imperial gallon. Since the imperial gallon is 20% more than the liquid gallon, the same is true for the imperial pint in relation to the liquid pint. This explains why the number of fluid ounces per gallon had to be adjusted in the new system so that the fluid ounce was not changed too much by the reform. Even so, the modern UK fluid ounce is 4% \"less\" than the US fluid ounce, and the same is true for the smaller units. For some years both systems were used concurrently in the UK.\n\nThere were also commonly used, but unofficial divisions of the Apothecaries' system, consisting of:\n\nIn the US, the similar measures in use were once\nThe cited book states, \"In almost all cases the modern teacups, tablespoons, dessertspoons, and teaspoons, after careful test by the author, were found to average 25 per cent. greater capacity than the theoretical quantities given above, and thus the use of accurately graduated medicine glasses, which may be had now at a trifling cost, should be insisted upon.\"\n\nApothecaries' measures eventually fell out of use in the UK and were officially abolished in 1971. In the US, they are still occasionally used, for example with prescribed medicine being sold in four ounce (℥ iv) bottles.\n\nUntil around 1900, medical recipes and most European pharmacopoeias were written in Latin. Here is a typical example from the middle of the 19th century.\n\nThe use of Latin ensured that the recipes could be read by an international audience. There was a technical reason why \"3 ʒ\" was written \"ʒiij,\" and \" ʒ\" as \"ʒss\": Writing \"iii\" as \"iij\" would prevent tampering with or misinterpretation of a number after it is written. The letters \"ss\" are an abbreviation for the Latin \"semis\" meaning \"half\", which were sometimes written with a ß. In Apothecaries' Latin, numbers were generally written in Roman numerals, immediately following the symbol. Since only the units of the apothecaries' system were used in this way, this made it clear that the civil weight system was not meant.\n\nThe basic form of the apothecaries' system is essentially a subset of the Roman weight system. An apothecaries' pound normally consisted of 12 ounces. (In France this was changed to 16 ounces, and in Spain the customary unit was the \"marco\", a mark of 8 ounces.) In the south of Europe and in France, the scruple was generally divided into 24 grains, so that one ounce consisted of 576 grains. Nevertheless, the subdivision of an ounce was somewhat more uniform than that of a pound, and a common feature of all variants is that 12 ounces are roughly 100 drachms (96–128 drachms) and a grain is roughly the weight of a physical grain.\nIt is most convenient to compare the various local weight standards by the metric weights of their ounces. The actual mass of an ounce varied by ±17% (5 g) around the typical value of 30 g. The table only shows approximate values for the most important standards; even the same nominal standard could vary slightly between one city and its neighbour. The range from 25 g to 31 g is filled with numerous variants, especially the Italian range up to 28 g. But there is a relatively large gap between the troy ounces of 31 g and the Habsburg ounce of 35 g. The latter is the product of an 18th-century weight reform.\n\nEven in Turkey a system of weights similar to the European apothecaries' system was used for the same purpose. For medical purposes the tcheky (approx. 320 g) was divided in 100 drachms, and the drachm in (16 killos or) 64 grains. This is close to the classical Greek weight system, where a \"mina\" (corresponding roughly to a Roman \"libra\") was also divided into 100 drachms.\n\nWith the beginning of metrication, some countries standardized their apothecaries' pound to an easily remembered multiple of the French gramme. E.g. in the Netherlands the Dutch troy pound of 369.1 g was standardized in 1820 to 375.000 g, to match a similar reform in France. The British troy pound retained its value of 373.202 g until in 2000 it was legally defined in metric terms, as 373.2417216 g. (At this time its use was already illegal for all purposes except trading precious metals.)\n\nIn the Romance speaking part of Europe the scruple was divided in 24 grains, in the rest of Europe in 20 grains. Notable exceptions were Venice and Sicily, where the scruple was also divided in 20 grains.\n\nThe Sicilian apothecaries' ounce was divided in 10 drachms. Since the scruple was divided in only 20 grains, like in the northern countries, an ounce consisted of 600 grains. This was not too different from the situation in most of the other mediterranean countries, where an ounce consisted of 576 grains.\n\nIn France, at some stage the apothecaries' pound of 12 ounces was replaced by the larger civil pound of 16 ounces. The subdivisions of the apothecaries' ounce were the same as in the other Romance countries, however, and were different from the subdivisions of the otherwise identical civil ounce.\n\nThe basic apothecaries' system consists of the units pound, ounce and scruple from the classical Roman weight system, together with the originally Greek drachm and a new subdivision of the scruple into either 20 (\"barley\") or 24 (\"wheat\") grains (). In some countries other units of the original system remained in use, for example in Spain the \"obolo\" and \"siliqua\". In some cases the apothecaries' and civil weight systems had the same ounces (\"an ounce is an ounce\"), but the civil pound consisted of 16 ounces. \"Siliqua\" is Latin for the seed of the carob tree.\n\nMany attempts were made to reconstruct the exact mass of the Roman pound. One method for doing this consists in weighing old coins; another uses the fact that Roman weight units were derived from Roman units of length similarly to the way the kilogramme was originally derived from the metre, i.e. by weighing a known volume of water. Nowadays the Roman pound is often given as 327.45 g, but one should keep in mind that (apart from the other uncertainties that come with such a reconstruction) the Roman weight standard is unlikely to have remained constant to such a precision over the centuries, and that the provinces often had somewhat inexact copies of the standard. The weight and subdivision of the pound in the Holy Roman Empire was reformed by Charlemagne, but in the Byzantine Empire it remained essentially the same. Since Byzantine coins circulated up to Scandinavia, the old Roman standard continued to be influential through the Middle Ages.\n\nThe history of mediaeval medicine started roughly around the year 1000 with the school of medicine in Salerno, which combined elements of Latin, Greek, Arabic and Jewish medicine. Galen and Dioscorides (who had used the Graeco-Roman weight system) were among the most important authorities, but also Arabic physicians, whose works were systematically translated into Latin.\n\nAccording to \"De ponderibus et mensuris\", a famous 13th century text that exists in numerous variations and is often ascribed to Dino di Garbo, the system of weights used in Salerno was different from the systems used in Padua and Bologna. As can be seen from the table, it was also different from the Roman weight system used by Galenus and Dioscorides and from all modern apothecaries' systems: The ounce was divided into 9 drachms, rather than 8 drachms.\n\nCenturies later, the region around Salerno was the only exception to the rule that (except for skipping units that had regionally fallen out of use) the apothecaries' ounce was subdivided down to the scruple in exactly the same way as in the Roman system: It divided the ounce into 10 drachms.\n\nWhile there will naturally have been some changes throughout the centuries, this section only tries to give a general overview over the situation that was recorded in detail in numerous 19th century merchants' handbooks.\n\nOn the Iberian Peninsula, apothecaries' weights in the 19th century were relatively uniform, with 24 grains per scruple (576 grains per ounce), the standard in Romance countries. The weight of an apothecaries' pound was 345.1 g in Spain and 344.2 g in Portugal. As in Italy, some of the additional subdivisions of the Roman system, such as the \"obolo\", were still in use there. It was standard to use the \"marco\", defined as 8 ounces, instead of the pound.\n\nIn 18th century France, there was a national weight standard, the \"marc de Paris\" of 8 ounces. The civil pound of 16 ounces was equivalent to 2 marks, and it was also used as the apothecaries' pound. With 30.6 g, the ounces were considerably heavier than other apothecaries' ounces in Romance countries, but otherwise the French system was not remarkable. Its history and connections to the English and Flemish standards are discussed below under Weight standards named after Troyes.\n\nDue in part to the political conditions in what would become a united Kingdom of Italy only in 1861, the variation of apothecaries' systems and standard weights in this region was enormous. (For background information, see History of Italy during foreign domination and the unification.) The \"libbra\" (pound) generally consisted of the standard twelve ounces, however.\n\nThe civil weight systems were generally very similar to the apothecaries' system, and since the \"libbra\" (or the \"libbra sottile\", where different systems were in use for light and heavy goods) generally had a suitable weight for an apothecaries' pound it was often used for this purpose. Extreme cases were Rome and Genoa, where the same system was used for everything, including medicine. On the other hand, there were relatively large differences even between two cities in the same state. E.g. Bologna (in the Papal States) had an apothecaries' pound that was less than the local civil pound, and 4% lighter than the pound used in Rome.\n\nThe weight of an apothecaries' pound ranged generally between 300 g and 320 g, slightly less than that of a pound in the Roman Empire. An important exception to this rule is that the Kingdom of Lombardy–Venetia was under rule of the Habsburg monarchy 1814–1859 and therefore had the extremely large Habsburg apothecaries' pound of 420 g. (See below under Habsburg standard.) E.g. in the large city of Milan the apothecaries' system based on a pound of 326.8 g was officially replaced by the metric system as early as 1803, because Milan was part of the Napoleonic Italian Republic. Since the successor of this little state, the Napoleonic Kingdom of Italy, fell to Habsburg in 1814 (at a time when even in France the \"système usuel\" had been introduced because the metric system was not accepted by the population), an apothecaries' system was officially introduced again, but now based on the Habsburg apothecaries' pound, which weighed almost 30% more.\nThe apothecaries' pound in Venice had exactly the same subdivisions as those in the non-Romance countries, but its total weight of 301 g was at the bottom of the range. During the Habsburg reign of 1814–1859 an exception was made for Venice; as a result the extreme weights of 301 g and 420 g coexisted within one state and in immediate proximity. The Venice standard was also used elsewhere, for example in Udine. In Dubrovnik (called \"Ragusa\" until 1909) its use was partially continued for a long time in spite of the official Habsburg weight reform.\n\nThe measure and weight systems for the large mainland part of the Kingdom of the Two Sicilies were unified in 1840. The area consisted of the southern half of the Italian Peninsula and included Naples and Salerno. The subdivision of apothecaries' weight in the unified system was essentially the same as that for gold, silver, coins and silk. It was the most excentric variant in that the ounce was divided in 10 drachms, rather than the usual 8. The scruple, like in Venice but unlike in the rest of the Romance region, was divided into 20 grains. The existence of a unit called \"aureo\", the equivalent of 1 \"dramme\", is interesting because 6 \"aurei\" were 9 \"dramme\". In the original Salerno weight system an ounce was divided into 9 drachms, and so an \"aureo\" would have been ⅙ of an ounce.\n\nAs early as 1147 in Troyes in Champagne (in the Middle Ages an important trading town) a unit of weight called \"marc de Troyes\" was used.\n\nThe national French standard until 1799 was based on a famous artefact called the \"\", which probably dates back to the second half of the 15th century. It is an elaborate set of nesting weight pieces, with a total metric weight of 12.238 \"kg\". The set is now shown in the Musée des Arts et Métiers in Paris. The total nominal value of the set is 50 \"marcs de Troyes\" or \"marcs de Paris\", a mark being 8 ounces. The ounce \"poids de marc\" had therefore a metric equivalent of 30.59 g. The \"poids de marc\" was used as a national French standard for trading, for gold, silver and jewels, and for weighing medicine. It was also used in international communications between scientists. In the time before the French Revolution, the civil pound also played the role of the apothecaries' pound in the French apothecaries' system, which otherwise remained a standard system of the Romance (24 grains per scruple) type.\nIn Bruges, Amsterdam, Antwerpen and other Flemish cities, a \"troy\" unit (\"trooisch pond\") was also in use as a standard for valuable materials and medicine. As in France, the way in which the Flemish troy ounce was subdivided depended on what was weighed. Unlike the French, the Flemish apothecaries divided the scruple in 20 grains. The Flemish troy pound became the standard for the gold and apothecaries' system in the United Kingdom of the Netherlands; it was also used in this way in Lübeck. (The London troy pound was referred to as the 'trooisch pond', after metrification.)\n\nThe Dutch troy mark consisted of 8 Flemish troy ounces, with each ounce of 20 engels, and each engel divided into 32 assen. The Amsterdam Pound of two marks, used in commerce, weighed 10,280 assen, while the Amsterdam Troy pound weighed 10,240 assen, i.e. exactly two troy marks.\n\nIn 1414, six years before the Treaty of Troyes, a statute of Henry V of England gave directions to the goldsmiths in terms of the troy pound. (In 1304 it had apparently not yet been introduced, since it did not appear in the statute of weights and measures.) There is evidence from the 15th century that the troy pound was used for weighing metals and spices. After the abolishment of the Tower pound in 1527 by Henry VIII of England, the troy pound was the official basis for English coin weights. The British apothecaries' system was based on the troy pound until metrication, and it survived in the United States and Australia well into the 20th century.\n\nSince the modern (English, American and Imperial) troy ounces are roughly 1.5% heavier than the late Paris ounce, the exact historical relations between the original \"marc de Troyes\", the French \"poids de marc\", the Flemish trooisch pond and the English troy pound are unclear. It is known, however, that the numerical relation between the English and French troy ounces was \"exactly\" 64:63 in the 14th century.\n\nIn the Middle Ages the Imperial Free City of Nuremberg, an important trading place in the south of Germany, produced large amounts of nesting weight pieces to various European standards. In the 1540s, the first pharmacopoeia in the modern sense was also printed there. In 1555, a weight standard for the apothecaries' pound of 12 ounces was set in Nuremberg. Under the name \"Nuremberg pharmaceutical weight\" () it would become the standard for most of the north-east of Europe. However, some cities kept local copies of the standard.\n\nAs of 1800 all German states and cities except Lübeck (which had the Dutch troy standard) followed the Nuremberg standard. It was also the standard for Denmark, Norway, the Russian Empire and most cantons of Switzerland. Poland and Sweden had their own variants of the standard, which differed from each other by 0.6%.\n\nIn 1811, Bavaria legally defined the apothecaries' pound as 360.00 g (an ounce of 30.00 g). In 1815, Nuremberg lost its status as a free city and became part of Bavaria. From now on the Nuremberg apothecaries' pound was no longer the official apothecaries' pound in Nuremberg; but the difference was only 0.6%. In 1836 the Greek apothecaries' pound was officially defined by this standard, four years after Otto, the son of the king of Bavaria, became the first king of Greece. But only few German states followed the example of Bavaria, and with a long delay. The apothecaries' pound of 360 g was also adopted in Lübeck, where it was official as of 1861.\n\nAustria and the states of the Habsburg monarchy officially had a different standard since 1761, and Prussia, followed by its neighbours Anhalt, Lippe and Mecklenburg, would diverge in the opposite direction with a reform in 1816. But in both cases apothecaries continued to use the Nuremberg standard unofficially for a long time after it became illegal.\n\nIn Russia the apothecaries' system survived well into the 20th century. The Soviet Union officially abolished it only in January 1927.\n\nEmpress Maria Theresia of Austria reformed the measures and weights of the Habsburg monarchy in 1761. The weight of an apothecaries' pound of 12 ounces was increased to a value that was later (after the kilogramme was defined) found to be 420.009 g; this was called the \"libra medicinalis major\". It was defined as / of the unusually heavy Habsburg civil pound (defined as / of the civil pound of Cologne) and corresponded to a record ounce weight of 35 g.\n\nBefore the reform, in the north of the empire the Nuremberg standard had been in effect, and in Italy the local standards had been even lighter. It is not surprising that an increase by 17% and more met with some inertia. The 1770 edition of the pharmacopoeia \"Dispensatorium Austriaco-Viennense\" still used the Nuremberg standard \"libra medicinalis minor\", indicating that even in the Austrian capital Vienna it took some time for the reform to become effective. In 1774, the \"Pharmacopoea Austriaco-provincialis\" used the new standard, and in 1783 all old apothecaries' weight pieces that were still in use were directed to be destroyed.\n\nVenice was not part of these reforms and kept its standard of approximately 25 g per ounce.\n\nWhen Austria started producing scales and weight pieces to the new standard with an excellent quality/price ratio, these were occasionally used by German apothecaries as well.\n\nAt the time of the Industrial Revolution, the fact that each state had its own system of weights and measures became increasingly problematic. Serious work on a \"scientific\" system was started in France under Louis XVI, and completed in 1799 (after the French Revolution) with its implementation. The French population, however, was initially unhappy with the new system. In 1812, Napoleon Bonaparte reintroduced some of the old measures and weights, but in a modified form that was defined with respect to the metric system. This \"système usuel\" was finally abolished in 1837 and became illegal in 1840.\n\nDue to the large expansion of the First French Empire under Napoleon I, French metrication also affected what would be (parts of) France's neighbour countries after the Congress of Vienna.\n\nThe Netherlands were partially metricated when they were French, in the years 1810–1813. With full metrication, effective January 1821, the Netherlands reformed the trooisch pond. The apothecaries' new pound was 375.00 g. Apart from rounding issues concerning the subdivisions, this corresponded exactly to the French \"système usuel\". (The reform was not followed in the north German city of Lübeck, which continued to use the trooisch pond.) In Belgium, apothecaries' weight was metricized effective 1856.\n\nBetween 1803 and 1815 all German regions west of the River Rhine were French, organised in the \"départements\" Roer, Sarre, Rhin-et-Moselle, and Mont-Tonnerre. As a result of the Congress of Vienna these became part of various German states. A large part of the Palatinate fell to Bavaria, but having the metric system it was excepted from the Bavarian reform of weights and measures.\n\nIn Prussia, a reform in 1816 defined the Prussian civil pound in terms of the Prussian foot and distilled water. It also redefined the apothecaries' pound as 12 ounces, i.e. /, of the civil pound: 350.78 g. This reform was not popular with apothecaries, because it broke the uniformity of the apothecaries' pound in Germany at a time when a German national state was beginning to form. It seems that many apothecaries did not follow this reduction by 2%.\n\nAnother reform in 1856 increased the civil pound from 467.711 g to 500.000 g (the German civil pound defined by the Zollverein), as a first step towards metrication. As a consequence the official apothecaries' pound was now 375.000 g, i.e. it was increased by 7%, and it was now very close to the troy standards. §4 of the law that introduced this reform said: \"Further, a pharmaceutical weight deviating from the civil weight does not take place.\" But this paragraph was suspended until further notice.\n\nThe abolishment of the apothecaries' system meant that doctors' prescriptions had to take place in terms of the current civil weight: grammes and kilograms. This was considered unfeasible by many, and the state received numerous protests and asked for expertises. Nevertheless, by 1868 §4 of the earlier reform was finally put into force.\n\nBritain was initially involved in the development of the metric system, and the US was among the 17 initial signatories of the Metre Convention in 1875. Yet in spite of enthusiastic support for the new system by intellectuals such as Charles Dickens, these two countries were particularly slow to implement it.\n\nTo unify all weight systems used by apothecaries, the Irish pharmacopœia of 1850 introduced a new variant of the apothecaries' system which subdivided a new apothecaries' pound of 12 avoirdupois ounces instead of the troy pound. To allow effective use of the new system, new weight pieces were produced. Since an avoirdupois ounce corresponds to 28.35 g, the proposed system was very similar to that in use in Portugal and Spain, and in some locations in Italy. But it would have doubled the value of the avoirdupois drachm (an existing unit, but by then only used for weighing silk). Therefore, it conflicted with other non-standard variations that were based on that nearly obsolete unit.\n\nThe Irish proposal was not widely adopted, but British legislation, in the form of the Medicinals Act 1858, was more radical: It prescribed the use of the avoirdupois system for the United Kingdom (then including Ireland), with none of the traditional subdivisions. This innovation was first used in the united British pharmacopœia of 1864. In practice the old apothecaries' system based on the troy pound was still widely used, however, until it was abolished by the Weights and Measures Act of 1976. Since then it can only be used to measure precious metals and stones. (The troy pound was already declared illegal for most other uses by the Weights and Measures Act of 1878.)\n\nIn the US, the metric system replaced the apothecaries' system in the United States Pharmacopoeia of 1971.\n\n\n"}
{"id": "10035641", "url": "https://en.wikipedia.org/wiki?curid=10035641", "title": "Battery electric bus", "text": "Battery electric bus\n\nA battery electric bus is an electric bus that is driven by an electric motor and obtains energy from on-board batteries. Many trolleybuses use batteries as an auxiliary or emergency power source.\n\nBattery buses are equipped for charging with fixed pantographs at bus stops and in the Depot. Supercapacitors can be charged rapidly, reducing the time needed to prepare to resume operation.\n\nBattery electric buses offer zero-emission, quiet operation and better acceleration compared to traditional buses. They also eliminate infrastructure needed for a constant grid connection and allow routes to be modified without infrastructure changes compared to a Trolleybus. They typically recover braking energy to increase efficiency by a regenerative brake. With energy consumption of about 1.2 kWh/km, the cost of ownership is lower than diesel buses.\n\nAs of 2016 battery buses have less range, higher weight, higher procurement costs. The reduced infrastructure for overhead lines is partially offset by the costs of the infrastructure to recharge the batteries. Battery buses are used almost exclusively in urban areas rather than for long-haul transport. Urban transit features relatively short intervals between charging opportunities. Sufficient recharging can take place within 4 to 5 minutes (250 to 450 kW) usually by induction or catenary.\n\nThe first battery buses were mostly small, mini- or midi- buses. The improvement of battery technology from around 2010 led to the emergence of the battery bus, including heavier units such as twelve-meter standard buses and articulated wagons.\n\n\n"}
{"id": "31879870", "url": "https://en.wikipedia.org/wiki?curid=31879870", "title": "Bernoulli grip", "text": "Bernoulli grip\n\nA Bernoulli grip uses airflow to adhere to an object without physical contact. Such grippers rely on the Bernoulli airflow principle. A high velocity airstream has a low static pressure. With careful design the pressure in the high velocity airstream can be lower than atmospheric pressure. This can cause a net force on the object in the direction normal to the side with lower local pressure. A Bernoulli gripper takes advantage of this by maintaining a positive pressure at the gripper face compared to the ambient pressure, while maintaining an air gap between the gripper and the object being held.\n\nCommercially available Bernoulli grips are commonly used to handle rigid sheet like material such as silicon wafers in circuit board manufacturing, or photovoltaic cell components. Since the grip is contactless, this form of gripping lends itself to handling sterile material to prevent chemical and/or biological contamination. Research has been done into using Bernoulli grippers to transport sample sheet foodstuffs in a food processing context, although this work found difficulties as the flexible foods would vibrate against the gripper, deforming and alternately blocking the gripper or and being blown away from the airway.\nThe Bernoulli grip is also being investigated as a non-contact adhesion mechanism for wall climbing robots.\n"}
{"id": "2414460", "url": "https://en.wikipedia.org/wiki?curid=2414460", "title": "Busemann's Biplane", "text": "Busemann's Biplane\n\nBusemann's Biplane is a conceptual airframe design invented by Adolf Busemann which avoids the formation of N-type shock waves and thus does not create a sonic boom.\n\nIt consists of two triangular cross-section plates a certain distance apart, with the flat sides parallel to the fluid flow. The spacing between the plates is sufficiently large that the flow does not choke and supersonic flow is maintained between them.\n\nUsually with supersonic flow a positive pressure shock wave is generated at the front and a negative pressure shock wave at the rear. In Busemann's biplane, the high pressure shock wave is created internally and reflects symmetrically between the two plates. This cancels/fills the expansion fan forming at the rear, leaving no external shock waves to propagate to infinity. The flat upper and lower surfaces generate no shock waves because the flow is parallel.\n\nThe internal alignment of the shock waves means that Busemann's biplane produces minimum wave drag. However, the flat external surfaces and internal symmetry also mean that the airfoil does not produce any lift at the design point zero angle of attack. Hence, no implementation has yet flown, although the concept has been successfully tested in wind tunnels and for ammunition.\n\n\n"}
{"id": "9977922", "url": "https://en.wikipedia.org/wiki?curid=9977922", "title": "CEB–NEPA Power Interconnection", "text": "CEB–NEPA Power Interconnection\n\nCEB–NEPA Power Interconnection is a long 330 kV powerline between Sakété (Benin) and Ikeja (Nigeria). It connects Nigeria's power grid (NEPA) with Benin's grid (CEB).\n\nThe project started in December 2001 and was officially opened on 13 February 2007. The purpose of the powerline is to supply electricity from Nigeria to Benin and Togo.\n\nThe interconnection consists of of a 330 kV single circuit line from Ikeja West substation in Nigeria to Nigeria – Benin border and of a 330 kV single circuit line from Nigeria – Benin border to the Sakete 330/161 kV substation.\n\n"}
{"id": "26288711", "url": "https://en.wikipedia.org/wiki?curid=26288711", "title": "Coulomb's law", "text": "Coulomb's law\n\nCoulomb's law, or Coulomb's inverse-square law, is a law of physics for quantifying the amount of force with which stationary electrically charged particles repel or attract each other. In its scalar form, the law is:\n\nwhere \"k\" is Coulomb's constant (\"k\" ≈ ), \"q\" and \"q\" are the signed magnitudes of the charges, and the scalar \"r\" is the distance between the charges. The force of the interaction between the charges is attractive if the charges have opposite signs (i.e., \"F\" is negative) and repulsive if like-signed (i.e., \"F\" is positive).\n\nThe law was first published in 1785 by French physicist Charles-Augustin de Coulomb and was essential to the development of the theory of electromagnetism. Being an inverse-square law, it is analogous to Isaac Newton's inverse-square law of universal gravitation. Coulomb's law can be used to derive Gauss's law, and vice versa. The law has been tested extensively, and all observations have upheld the laws of Newton.\n\nAncient cultures around the Mediterranean knew that certain objects, such as rods of amber, could be rubbed with cat's fur to attract light objects like feathers. Thales of Miletus made a series of observations on static electricity around 600 BC, from which he believed that friction rendered amber magnetic, in contrast to minerals such as magnetite, which needed no rubbing. Thales was incorrect in believing the attraction was due to a magnetic effect, but later science would prove a link between magnetism and electricity. Electricity would remain little more than an intellectual curiosity for millennia until 1600, when the English scientist William Gilbert made a careful study of electricity and magnetism, distinguishing the lodestone effect from static electricity produced by rubbing amber. He coined the New Latin word \"electricus\" (\"of amber\" or \"like amber\", from [\"elektron\"], the Greek word for \"amber\") to refer to the property of attracting small objects after being rubbed. This association gave rise to the English words \"electric\" and \"electricity\", which made their first appearance in print in Thomas Browne's \"Pseudodoxia Epidemica\" of 1646.\n\nEarly investigators of the 18th century who suspected that the electrical force diminished with distance as the force of gravity did (i.e., as the inverse square of the distance) included Daniel Bernoulli and Alessandro Volta, both of whom measured the force between plates of a capacitor, and Franz Aepinus who supposed the inverse-square law in 1758.\n\nBased on experiments with electrically charged spheres, Joseph Priestley of England was among the first to propose that electrical force followed an inverse-square law, similar to Newton's law of universal gravitation. However, he did not generalize or elaborate on this. In 1767, he conjectured that the force between charges varied as the inverse square of the distance.\n\nIn 1769, Scottish physicist John Robison announced that, according to his measurements, the force of repulsion between two spheres with charges of the same sign varied as .\n\nIn the early 1770s, the dependence of the force between charged bodies upon both distance and charge had already been discovered, but not published, by Henry Cavendish of England.\n\nFinally, in 1785, the French physicist Charles-Augustin de Coulomb published his first three reports of electricity and magnetism where he stated his law. This publication was essential to the development of the theory of electromagnetism. He used a torsion balance to study the repulsion and attraction forces of charged particles, and determined that the magnitude of the electric force between two point charges is directly proportional to the product of the charges and inversely proportional to the square of the distance between them.\n\nThe torsion balance consists of a bar suspended from its middle by a thin fiber. The fiber acts as a very weak torsion spring. In Coulomb's experiment, the torsion balance was an insulating rod with a metal-coated ball attached to one end, suspended by a silk thread. The ball was charged with a known charge of static electricity, and a second charged ball of the same polarity was brought near it. The two charged balls repelled one another, twisting the fiber through a certain angle, which could be read from a scale on the instrument. By knowing how much force it took to twist the fiber through a given angle, Coulomb was able to calculate the force between the balls and derive his inverse-square proportionality law.\n\nCoulomb's law states that:\n\nCoulomb's law can also be stated as a simple mathematical expression. The scalar and vector forms of the mathematical equation are\n\nIn this way, the verification is limited to measuring the distance between the charges and check that the division approximates the theoretical value.\n\nCoulomb's law holds even within atoms, correctly describing the force between the positively charged atomic nucleus and each of the negatively charged electrons. This simple law also correctly accounts for the forces that bind atoms together to form molecules and for the forces that bind atoms and molecules together to form solids and liquids. Generally, as the distance between ions increases, the force of attraction, and binding energy, approach zero and ionic bonding is less favorable. As the magnitude of opposing charges increases, energy increases and ionic bonding is more favorable.\n\n\n\n"}
{"id": "34030887", "url": "https://en.wikipedia.org/wiki?curid=34030887", "title": "Dos Bocas oil fire", "text": "Dos Bocas oil fire\n\nThe \"Dos Bocas\" oil fire was a 1908 blowout and oil fire that took place in Veracruz, Mexico. The fire started after a blowout occurred, causing crude oil to make contact with the flames that powered the oil derrick. The fire continued from 4 July 1908 to 30 August 1908, when the oil deposit was burnt out. For the nearly two months that the fire burned, nearly of oil flowed into the surrounding landscape each day. Ultimately, the \"Dos Bocas\" blowout (named for the two gaping black holes it left it the ground - \"two mouths\") ended up being one of the largest oil spills in the history of the oil industry. The two large craters formed in the ground around the oil well were caused by the pressure of the oilfield. It was impossible to stop the flow of oil because the well casing had been blown off during the gush.The heat exacerbated the difficulties workers faced when trying to stop the flow of oil. Temperatures were high which caused those working to put out the fire and contain the spill to be unable to go closer than a few hundred feet. The oil coming out was a column and the fire burned at over . away from the fire, town residents said that they were able to read a newspaper at night by the light of the flames. The cost of extinguishing the fire and capping the well was $3,000,000.\n\n\n"}
{"id": "18914636", "url": "https://en.wikipedia.org/wiki?curid=18914636", "title": "ECOnetic", "text": "ECOnetic\n\nECOnetic is a tradename for certain car models produced by Ford of Europe, currently consisting of one model in each of the Fiesta, the Focus and the Mondeo range, with an emphasis on higher fuel efficiency and low-CO emissions.\n\nAs opposed to developing hybrid vehicles like Prius, Ford of Europe took a different design philosophy: to create vehicles that are as fuel efficient as possible, without compromising driving experience. The resultant ECOnetic range focuses on improved design and refinement of existing technology, focusing on three main streams of improvement:\n\nFirst announced in late-2007, Ford proposed ECOnetic models in the Fiesta, Focus and Mondeo ranges. The first cars were available to order from dealers in early 2008, priced at £250 above the \"Style\" models. Delivered to United Kingdom from April 2008, the models was in the lowest bracket of Vehicle Excise Duty and exempt form the London Congestion Charge.\n\nThe second generation Fiesta ECOnetic was launched at the 2008 British International Motor Show. To encourage drivers to be efficient, a green upwards arrow-shaped indicator light within the dashboard's rev counter signals to the driver when to change up to a higher gear for optimum fuel efficiency and thus economy, later also introduced to the Focus model.\n\nThe 2012 Focus ECOnetic model includes: Smart Regenerative Charging to reduce alternator resistance; Auto-Start-Stop; electrically operated front grille shutter, to improve high speed aerodynamics.\n\nIt is not presently planned to introduce ECOnetic models to the North American market, because, as \"Business Week\" noted, the company \"doesn't believe it could charge enough to make money on an imported ECOnetic\" and doesn't think it would sell enough of the model (350,000/year) to justify the $350 million in upgrades required at their Mexico plant to manufacture it.\n\n"}
{"id": "27579471", "url": "https://en.wikipedia.org/wiki?curid=27579471", "title": "Electrical network frequency analysis", "text": "Electrical network frequency analysis\n\nElectrical network frequency (ENF) analysis is a forensic science technique for validating audio recordings by comparing frequency changes in background mains hum in the recording with long-term high-precision historical records of mains frequency changes from a database. In effect the mains hum signal is treated as if it were a time-dependent digital watermark that can help identify when the recording was created, and help detect any edits in the recording. Historical records of mains frequency changes are kept on record, e.g., by police in the German federal state of Bavaria since 2010.\n\nThe technology has been hailed as \"the most significant development in audio forensics since Watergate.\" However, according to a paper by Huijbregtse and Geradts, the ENF technique, although powerful, has significant limitations caused by ambiguity based on fixed frequency offsets during recording, and self-similarity within the mains frequency database, particularly for recordings shorter than 10 minutes.\n\nMore recently, researchers demonstrated that indoor lights such as fluorescent lights and incandescent bulbs vary their light intensity in accordance with the voltage supplied, which in turn depends on the voltage supply frequency. As a result, the light intensity can carry the frequency fluctuation information to the visual sensor recordings in a similar way as the electromagnetic waves from the power transmission lines carry the ENF information to audio sensing mechanisms. Based on this result, researchers demonstrated that visual track from still video taken in indoor lighting environments also contain ENF traces that can be extracted by estimating the frequency at which ENF will appear in a video as low sampling frequency of video (25–30 Hz) cause significant aliasing. It was also demonstrated in the same research that the ENF signatures from the visual stream and the ENF signature from the audio stream in a given video should match. As a result, the matching between the two signals can be used to determine if the audio and visual track were recorded together or superimposed later.\n\nThe distinctive electrical hums have been used to provide forensic verification of audio recordings, a process fully automated in the United Kingdom.\n"}
{"id": "32875014", "url": "https://en.wikipedia.org/wiki?curid=32875014", "title": "Energy in Estonia", "text": "Energy in Estonia\n\nEnergy in Estonia describes energy and electricity production, consumption and import in Estonia. \n\nElectricity production in Estonia is largely dependent on fossil fuels.\nIn 2007, more than 90% of power was generated from oil shale.\nThe Estonian energy company Eesti Energia owns the largest oil shale-fuelled power plants in the world, Narva Power Plants.\n\nEstonia has for the population of 1.3 million the network of 165 \"fast chargers\" for the electric cars since February 2013.\n"}
{"id": "26202513", "url": "https://en.wikipedia.org/wiki?curid=26202513", "title": "Energy park", "text": "Energy park\n\nAn energy park is a separate area used and planned for the purpose of clean energy development, like wind and solar generation facilities.\n\nEnergy parks create many other economic development benefits too. In Ohio, energy parks are creating thousands of green jobs. In Minnesota, community wind parks are also popular. In England, wind parks are commonly known as wind farms. A more \"lightweight\" version of an energy park is a wind park or solar park. These have one type of clean energy generation, rather than two or more technologies, as in an energy park. \n\nSome energy parks feature additional features beyond clean energy generation. Additional benefits include: green job creation, Smart grid connections, as well as new recreational, technology innovation and agricultural opportunities. The Stamford Energy Park in Vermont is one example of an integrated energy park.\n\n"}
{"id": "1659305", "url": "https://en.wikipedia.org/wiki?curid=1659305", "title": "Framer", "text": "Framer\n\nA framer is someone who frames (shapes or gives shape to or someone who constructs). \nIn building construction a framer is a carpenter who assembles the major structural elements of a wood-framed building called the framing. Framers build walls out of studs, sills, and headers; build floors from joists and beams; and frame roofs using ridge poles and rafters. \"Timber framers\" are framers who work in the traditional style of timber framing with wooden joinery.\nIn the traditional chair making industry, it was the bodger who produced the turned parts of a chair and the benchman who produced the splats, side rails and other sawn parts. However it was the framer who assembled and finished the chair with the parts supplied by the bodger and benchman.\n"}
{"id": "25069073", "url": "https://en.wikipedia.org/wiki?curid=25069073", "title": "Hammer paint", "text": "Hammer paint\n\nHammer paint (or hammered paint) is a special lacquer with a surface that looks like hammered metal when dried. It is also known as hammertone.\n\nThe slightly iridescent areas are caused by the different orientation of very small shiny particles which are suspended in the lacquer. These particles are often made of the mineral mica. Mica is chemically inactive and very resistant. Alternatively, aluminium or bronze powder may be used. To make the \"hammered\" effect more pronounced, a small percentage of silicone oil may be added directly before application.\n\nHammer paint is often used to beautify technical apparatus and not so commonly as a protective coating. The optical advantage of hammer paint is that surfaces look acceptable even if the underlying surface is not flat and smooth. To get a regular paint to look smooth the surface would have to be prepared first, for example by spackling, sanding, grinding or polishing. With hammer paint, this step can be omitted. Some hammer paints (e.g. by Hammerite) are formulated to be usable directly on rusted steel without surface preparation other than brushing to remove the loose rust.\n\nBeyond that, the resistance of the mica improves the durability of the paint job. The mica reduces aging by protecting the underlying binders from UV radiation. The mica also makes hammer paint relatively hard and scratch resistant.\n"}
{"id": "30355365", "url": "https://en.wikipedia.org/wiki?curid=30355365", "title": "Hongjiadu Dam", "text": "Hongjiadu Dam\n\nThe Hongjiadu Dam is a concrete face rock-fill embankment dam on the Liuchong River in Qianxi County, Guizhou Province, China. The dam is tall and was built for the purposes of hydroelectric power generation and water supply. The dam supports a 600 MW and withholds a reservoir.\n\n"}
{"id": "58571360", "url": "https://en.wikipedia.org/wiki?curid=58571360", "title": "Hurricane response", "text": "Hurricane response\n\nHurricane response is the disaster response after a hurricane. This response encompasses assessment and repairs to buildings and infrastructure, removal of debris, and providing public health services. Hurricane responders may be exposed to many hazards such as chemical and biological contaminants, and injuries from work activities.\n\nActivities performed by hurricane responders include assessment, restoration, and demolition of buildings; removal of debris and waste; repairs to land-based and maritime infrastructure; and providing public health services including search and rescue operations. Maritime response activities include checking for submerged obstructions and updating nautical charts for affected port areas; aerial survey missions to assess damages to affected areas to provide information for emergency responders; identification and survey of vessels or containers that may be leaking hazardous materials; and assessment of environmental impacts of contaminants in coastal and estuarine waters, including the health risk of eating fish and shellfish.\n\nHurricane response requires coordination between federal, tribal, state, local, and private entities. An Incident Command System is used in the United States to coordinate activities between entities involved in disaster response. According to the National Voluntary Organizations Active in Disaster, potential response volunteers should affiliate with established organizations and should not self-deploy, so that proper training and support can be provided to mitigate the danger and stress of response work.\n\nHurricane responders may be exposed to chemical and biological contaminants. These include sewage and stored industrial or household chemicals, human remains, and mold growth encouraged by flooding. Proper personal protective equipment, possibly including respirators, can help mitigate these hazards. In addition, fire ants are often disturbed during hurricanes and can float along in floodwaters, leading to the hazards of bites and stings. Asbestos and lead may be present in older buildings, and radon may be a concern in some areas. \n\nCommon injuries arise from falls from heights, such as from a ladder; trips, slips, and falls from level surfaces; and use of chainsaws. There is a danger of electrocution from flooded areas, including from backfeed from portable generators. In addition, the use of generators, heaters, or stoves can lead to carbon monoxide poisoning if used indoors or close to an open window or air conditioner. Motor vehicle safety is important as most responders ride in motor vehicles, or may be required to direct traffic. Violence and looting is another hazard. Injuries may also result from entering buildings with compromised structural integrity, noise, unexpected start-up or release of stored energy by machines or equipment, impact to the eyes or face, manual lifting, animal bites, poisonous plants, and sunburn.\n\nLong and irregular shifts may lead to sleep deprivation and fatigue, increasing the risk of injuries. Additionally, heat stress is a concern as workers are often exposed to hot and humid temperatures, wear protective clothing and equipment, and have physically difficult tasks. Heat stress may increase the risk of other injuries due to sweaty palms, fogged-up safety glasses, mental confusion, and dizziness.\n\nPre-exposure and post-exposure medical monitoring is recommended to establish fitness for and identify adverse effects from response work. Workers may experience mental stress associated with a traumatic incident.\n"}
{"id": "58333549", "url": "https://en.wikipedia.org/wiki?curid=58333549", "title": "Karuka", "text": "Karuka\n\nPandanus julianettii, also called karuka, karuka nut, or \"Pandanus\" nut, is a species of tree in the Pandanaceae family and an important regional food crop. The nuts are more nutritious than coconuts, and are so popular that villagers will move their entire households closer to trees for the harvest season.\n\nThe specific epithet \"julianettii\" honors naturalist Amedeo Giulianetti, who found the original type specimens.\n\nIn New Guinea it goes by different names among each of the Papuan peoples. In the Ankave language it is . It is in the Baruya language. The Huli language word is , and it is also in the Duna language. In Kewa language it is \"aga\", but it is unclear which dialect(s). In the Kewa pandanus language it is \"rumala agaa\". The Kalam language term, in both standard and pandanus languages, is . The plant is called in the Wiru language. In the Pole language it's called \"maisene\". It goes by \"ank\" in Angal language, and in the Wola dialect. The Imbongu language word is .\n\nThe plant also has many names on the other half of the island. In Indonesian it is called (lit.) and (), but the latter can also refer to \"P. brosimos\" and \"P. iwen\". The Dani people call it \"tuke\". The Lani people call it , but this might be a separate species in the complex.\n\nThe species was originally described by Ugolino Martelli from only a few drupes in the collections of the Royal Botanic Gardens, Kew He was hesitant to describe it as a new species from only that, but the characteristics were so salient he published his description.\n\nThe tree is dioecious (individual plants either have male flowers of female ones), with male trees uncommon compared to females. It reaches in height, with a grey trunk of in diameter and supported by buttress roots. The trunk has white mottling and is generally smooth with occasional warts or small knobs as well as rings of leaf scars. Inside the trunk is pithy and lacking cambium. The top of the tree sometimes branches, producing three or four crowns of leaves. Each crown will produce a single cluster of nuts, typically once every other season. Production is affected by the seasonality of local rainfall.\n\nLeaves spiral up the trunk in opposite pairs. The large leathery leaves are long and wide. The apex of the leaf is attenuate and doubly-pleated, with prickles pointing up at the tip and along the margins and midrib. The leaves are dark green on top and dull cyan underneath.\n\nThe inflorescence on male trees is a densely-branched spadix with a dozen long spikes, each containing many staminate phalanges. In each phalange is a column 3 mm long topped by up to 9 subsessile anthers. The male flowers are white, and the whole male flowering organ may be up to long.\n\nThe pollen has a psilate exine (unornamented outer wall) 0.8 μm thick. The ornamentation is granular between echinae (short spines). The ulcerate aperture is 3 μm in diameter. Pollen grains measure an average of 30 × 14.5 μm in size.\n\nOn female trees, the inflorescence is a single ellipsoid or ovoid syncarp, or fruiting head, with off-white bracts. Female flowers can produce fruit without pollination, and are typically the only trees cultivated. The tree stops making leaves when new fruit is growing. The syncarp has up to a thousand densely-packed single-celled carpels that later turn into drupes.\n\nThe clavate, pentagonal drupes measure up to 12 cm long and have a sharpened base, but typically are 9×1.5 cm, and are a pale blue-green color. Each cluster contains about 1000 nuts. The endocarp is bony and thin, 5½ cm long, with rounded edges about 1½ cm wide. The seed-bearing locule is around 4 cm long. The core of the mature head (mesocarp) has an appearance like honeycomb and is spongy and pink. The top of the mesocarp is fibrous, from 3 cm long and up. Though Martelli did not have a complete syncarp, he knew the cluster of fruit must be large, estimating at least 30 cm in diameter. He was correct, as the fruiting cluster is typically 15 to 30 cm in diameter. A mature head and stalk weigh up to 16 kg, but average 6 kg. \n\nIt most closely resembles \"P. utilissimus\", which is found the Philippines. People also harvest and eat nuts of \"P. antaresensis\", \"P. brosimos\", \"P. dubius\", \"P. iwen\", and \"P. limbatus\", and \"P. odoratissima\"\n\nThere are up to 45 cultivated varieties of karuka, many with different kernel shapes. There are likely many more, as some cultivars are known only to a small number of people in a single settlement. 'Tabuna' and 'Henga' are some of the most important cultivars. 'Tabuna' is popular because it is high-yielding, tastes good, and has no taboos on who/what can eat it and how/if it is cooked. At least two varieties are edible raw.\n\nNamed varieties include:\n\nIt is possible a cultivar is listed more than once under different names, as Papua New Guinea has a very high linguistic diversity.\n\nBenjamin Clemens Stone posits that \"P. julianettii\" and \"P. brosimos\" are a single species with many varieties, but does not support this point. However, Simon G. Haberle notes that the pollen of the two trees are indistinguishable by light microscopy. \"P. iwen\" may also be part of the species complex.\n\nGiulianetti's type specimens were collected from Vanapa, British New Guinea (now southern Papua New Guinea). The tree can be found cultivated or wild on New Guinea, both in PNG and Papua province. Wild trees are found on the Huon Peninsula and in the highlands of New Guinea's central cordillera. In Papua New Guinea, the tree is most commonly grown in Southern Highlands, Western Highlands, Eastern Highlands, Enga, and Chimbu Provinces, and it is found in all provinces on the mainland except East Sepik. It grows in montane forests between 1,300 and 3,300 m in elevation in areas that get 2-5 m mean annual precipitation. It grows in both dry and wet soils, but prefers good soil fertility. Trees will grow in clumped groups of 5 to 10 individuals per hectare.\n\nKaruka produces fruit around February, with an occasional secondary season in July. Typically each branch will only flower every other year. The natural pollination syndrome is unknown, but the flowers can be pollinated by humans. Seed dispersal is by humans, birds, and other animals. A fallen syncarp will disintegrate completely in about 3 days in the forest.\n\nFungal pests of karuka include leaf spot, diffuse leaf spot, black leaf mould (\"Lembosia pandani\"), sooty mold (\"Meliola juttingii\"), and fungus on seeds (\"Macrophoma pandani\"). The leaf moulds do not do much damage. The sooty mould seems to grow on insect frass. The black leaf mold only affects some varieties.\n\nThe bacteria \"Pectobacterium carotovorum\" subsp. \"carotovorum\" can also cause bacterial soft rot and necrosis on the leaves, but causes more severe damage to the related species \"Pandanus conoideus\".\n\nLonghorn grasshoppers (Tettigoniidae) are serious insect pests. \"Segestes gracilis\" and \"Segestidea montana\" eat the leaves and can sometimes kill trees. Growers will stuff leaves and grass in between the leaves of the crown to keep insects out. An unknown species of black grub will burrow into the cluster and eat the spongy core, causing the nuts to turn black and the whole bunch to fall off the tree. Woodboring beetles sometimes attack the prop root of the tree.\n\nPossums also eat the nuts, as do rodents such as squirrel-toothed rats (\"Anisomys imitator\"), eastern white-eared giant rats (\"Hyomys goliath\"), Rothschild's woolly rats (\"Mallomys rothschildi\"), and giant naked-tailed rats (\"Uromys anak\"). Growers will put platforms or other obstacles on the trunks of trees to keep the pests out.\n\nHarvested nuts are often beset by rats and cockroaches. Hanging nuts in the smoky areas above fires can prevent this, but after a while the taste of the nuts is affected.\n\nOn New Guinea karuka is cultivated crop, and has been used as a major food source since nearly 31,000 years ago in the Pleistocene. In PNG nearly 2 million people (almost half the rural population) live in regions where karuka is commonly eaten. There is high demand for it in the New Guinea Highlands: Entire households (including pigs, who are sometimes fed the fruits) will move from the valleys to higher elevations at harvest time, often for several weeks. Each household will average 12 to 176 trees.\n\nTrade in karuka is small-scale and not commercial. Local marketplaces typically will have 12 to 50 fruits for sale. With some coordination between state agencies and private sector, karuka could have export market access. The crop has a medium potential for large-scale sustainable commercialization in the region, but care must be taken in the sensitive local environments to expanded agriculture. Diets of tree owners could also be negatively influenced by rapid commercialization.\n\nThe endosperm, a white kernel, is eaten raw, roasted, smoked, or mumued. Nuts that aren't immediately eaten are typically sun-dried for storage. The karuka kernels have a sweet, coconut taste, or savory and like walnuts. Smoked or cooked karuka is either stored in the rafters or sold at local marketplaces. The uncooked clusters can also be stored for months buried in waterlogged earth, which possibly ferments it. It is a regional staple food and one of the few plants in the area with a high protein content. The spongy core of the multiple fruit cluster can also be cooked and eaten after the nuts are removed.\n\nThe high fat content means the nuts can be processed into an edible yellow oil. Karuka contains 52.39% oleic acid, 44.90% palmitic acid, and 0.19% stearic acid. The oil is a good source of Vitamin E (α-tocopherol 5.03 mg/100 g). The color of the oil is from the carotenoids, which are at a concentration of 2.75 µg/g. The antioxidant activity for the oil is fairly low, and it is higher in saturated than unsaturated fats.\n\nSome subjective reports indicate that children are healthier after karuka season, but there may also be increased incidence of tropical ulcers and pig-bel (\"Clostridium perfringens\"). But the connections, if valid, are unclear.\n\nTrunks and buttress roots are used for building. The sheets of bark are used for house walls. The leaves are used for bush shelters and raincapes. The leaves were the preferred building material for housing in Papua New Guinea before colonial contact. The durable white spathe leaves on male inflorescences are used by the Wola people to wrap pearl shells.\n\nKaruka can be cultivated by cutting a mature branch and replanting it (vegetative propagation). Suckers can also be replanted. Nurseries also plant seeds directly. New nuts will grow when a tree is at least five or six years old, and can keep producing for up to fifty years. The tree can tolerate temperatues down to 3°C for extended periods and 0°C for short periods. The USDA hardiness is 10-12, and is hardy to zone 10 in the UK system.\n\nIn Upper Karint near Pingirip, karukas are planted as boundary lines between garden plots.\n\nRitual pandanus languages are known from multiple areas of Papua New Guinea. In the Mount Giluwe area three separate language groups, Kewa, Melpa, and Mendi, use pandanus language, as does Imbongu, Huli, and Kalam. The grammar and vocabulary of pandanus language is based on the mother tongue, but a restricted and consolidated form, especially for names of living organisms. The new vocabulary focuses on words involved with trips to harvest karuka nuts, and changes as words become known outside an area. The language is spoken to control the magical properties of the higher elevations where the karuka grows, and to placate dangerous spirits like \"Kita-Menda\" (also called ), the ritual keeper of the wild dogs. All ages and genders are expected to know the ritual language before entering the taboo areas, but outsiders who do not know the language may be allowed to speak Tok Pisin instead. The taboo areas are typically marked with signs, usually \"Cordyline\" or other leaves tied to sticks. Pandanus language is not spoken outside the designated areas for fear of mountain spirits hearing it and coming down to investigate.\n\nAs Tok Pisin has become more widely spoken in the area, pandanus languages have been spoken less. Newer generations also seem to be less afraid of the deep forest, and do not see much need for the protective talk. The Kewa and Imbongu pandanus languages are already thought to be dying out.\n\nIn PNG's Central Province Premier Rugby League the team for Goilala District is called the Karukas.\n"}
{"id": "3487947", "url": "https://en.wikipedia.org/wiki?curid=3487947", "title": "Lethal yellowing", "text": "Lethal yellowing\n\nLethal yellowing (LY) is a phytoplasma disease that attacks many species of palms, including some commercially important species such as the coconut and date palm. In the Caribbean it is spread by the planthopper \"Haplaxius crudus\" (former name \"Myndus crudus\") which is native to Florida, parts of the Caribbean and Central America. The only effective cure is prevention, i.e. planting resistant varieties of coconut palm and preventing a park or 'golf course like' environments which attracts the planthopper. Some cultivars, such as the Jamaica Tall coconut cultivar, nearly died out by lethal yellowing. Heavy turf grasses and similar green ground cover will attract the planthopper to lay its eggs and the nymphs develop at the roots of these grasses. The planthoppers' eggs and nymphs may pose a great threat to coconut growing countries' economies, into which grass seeds for golf courses and lawns are imported from the Americas.\n\nIt is not clearly understood how the disease was spread to East Africa as the planthopper \"Haplaxius crudus\" is not native in East Africa.\nThe only explanation is that it was imported with grass seed from Florida that was used to create golf courses and lawns in beach resorts. There is a direct connection between green lawns and the spread of lethal yellowing in Florida. Even so-called 'resistant cultivars' such as the Malayan Dwarf or the Maypan hybrid between that dwarf and the Panama Tall were never claimed to have a 100% immunity. The nymphs of the planthoppers develop on roots of grasses, hence the areas of grass in the vicinity of palm trees is connected with the spread of this phytoplasma disease. The problem arose as a direct result of using coconut and date palms for ornamental and landscaping purposes in lawns, golf courses and gardens together with these grasses. When these two important food palms were grown in traditional ways (without grasses) in plantations and along the shores, the palm groves were not noticeably affected by lethal yellowing. There is no evidence that disease can be spread when instruments used to cut an infected palm are then used to cut or trim a healthy one. Seed transmission has never been demonstrated, although the phytoplasma can be found in coconut seednuts, but phytosanitary quarantine procedures that prevent movement of coconut seed, seedlings and mature palms out of an LY epidemic area should be applied to grasses and other plants that may be carrying infected vectors.\n\nBeside coconut palm (\"Cocus nucifera\"), more than 30 palm species have also been reported as susceptible to lethal phytoplasmas around the globe.\n\n\n"}
{"id": "24631911", "url": "https://en.wikipedia.org/wiki?curid=24631911", "title": "List of physical properties of glass", "text": "List of physical properties of glass\n\nThis is a list of some physical properties of common glasses. Unless otherwise stated, the technical glass compositions and many experimentally determined properties are taken from one large study. Unless stated otherwise, the properties of fused silica (quartz glass) and germania glass are derived from the SciGlass glass database by forming the arithmetic mean of all the experimental values from different authors (in general more than 10 independent sources for quartz glass and T of germanium oxide glass).\n\nThe list is not exhaustive.\n"}
{"id": "58485331", "url": "https://en.wikipedia.org/wiki?curid=58485331", "title": "List of pipeline accidents in the United States in 2001", "text": "List of pipeline accidents in the United States in 2001\n\nThe following is a list of pipeline accidents in the United States in 2001. It is one of several lists of U.S. pipeline accidents. See also list of natural gas and oil production accidents in the United States.\n\nThis is not a complete list of all pipeline accidents. For natural gas alone, the Pipeline and Hazardous Materials Safety Administration (PHMSA), a United States Department of Transportation agency, has collected data on more than 3,200 accidents deemed serious or significant since 1987.\n\nA \"significant incident\" results in any of the following consequences:\n\nPHMSA and the National Transportation Safety Board (NTSB) post incident data and results of investigations into accidents involving pipelines that carry a variety of products, including natural gas, oil, diesel fuel, gasoline, kerosene, jet fuel, carbon dioxide, and other substances. Occasionally pipelines are repurposed to carry different products.\n\n"}
{"id": "20656228", "url": "https://en.wikipedia.org/wiki?curid=20656228", "title": "Maize", "text": "Maize\n\nMaize ( ; \"Zea mays\" subsp. \"mays\", from after ), also known as corn, is a cereal grain first domesticated by indigenous peoples in southern Mexico about 10,000 years ago. The leafy stalk of the plant produces pollen inflorescences and separate ovuliferous inflorescences called ears that yield kernels or seeds, which are fruits.\n\nMaize has become a staple food in many parts of the world, with the total production of maize surpassing that of wheat or rice. However, little of this maize is consumed directly by humans: most is used for corn ethanol, animal feed and other , such as corn starch and corn syrup. The six major types of maize are dent corn, flint corn, pod corn, popcorn, flour corn, and sweet corn.\n\nMost historians believe maize was domesticated in the Tehuacán Valley of Mexico. Recent research in the early 21st century has modified this view somewhat; scholars now indicate the adjacent Balsas River Valley of south-central Mexico as the center of domestication.\n\nAn influential 2002 study by Matsuoka \"et al\". has demonstrated that, rather than the multiple independent domestications model, all maize arose from a single domestication in southern Mexico about 9,000 years ago. The study also demonstrated that the oldest surviving maize types are those of the Mexican highlands. Later, maize spread from this region over the Americas along two major paths. This is consistent with a model based on the archaeological record suggesting that maize diversified in the highlands of Mexico before spreading to the lowlands.\n\nArchaeologist Dolores Piperno has said:\n\nSince then, even earlier dates have been published.\n\nAccording to a genetic study by Embrapa, corn cultivation was introduced in South America from Mexico, in two great waves: the first, more than 6000 years ago, spread through the Andes. Evidence of cultivation in Peru has been found dating to about 6700 years ago. The second wave, about 2000 years ago, through the lowlands of South America.\n\nBefore domestication, maize plants grew only small, long corn cobs, and only one per plant. In Spielvogel's view, many centuries of artificial selection (rather than the current view that maize was exploited by interplanting with \"teosinte\") by the indigenous people of the Americas resulted in the development of maize plants capable of growing several cobs per plant, which were usually several centimetres/inches long each. The Olmec and Maya cultivated maize in numerous varieties throughout Mesoamerica; they cooked, ground and processed it through nixtamalization. It was believed that beginning about 2500 BC, the crop spread through much of the Americas. Research of the 21st century has established even earlier dates. The region developed a trade network based on surplus and varieties of maize crops.\n\nMaize is the most widely grown grain crop throughout the Americas, with 361 million metric tons grown in the United States in 2014 (Production table). Approximately 40% of the crop—130 million tons—is used for corn ethanol. Genetically modified maize made up 85% of the maize planted in the United States in 2009.\n\nSugar-rich varieties called sweet corn are usually grown for human consumption as kernels, while field corn varieties are used for animal feed, various corn-based human food uses (including grinding into cornmeal or masa, pressing into corn oil, and fermentation and distillation into alcoholic beverages like bourbon whiskey), and as chemical feedstocks.\n\nAfter the arrival of Europeans in 1492, Spanish settlers consumed maize and explorers and traders carried it back to Europe and introduced it to other countries. Spanish settlers far preferred wheat bread to maize, cassava, or potatoes. Maize flour could not be substituted for wheat for communion bread, since in Christian belief only wheat could undergo transubstantiation and be transformed into the body of Christ. Some Spaniards worried that by eating indigenous foods, which they did not consider nutritious, they would weaken and risk turning into Indians. \"In the view of Europeans, it was the food they ate, even more than the environment in which they lived, that gave Amerindians and Spaniards both their distinctive physical characteristics and their characteristic personalities.\" Despite these worries, Spaniards did consume maize. Archeological evidence from Florida sites indicate they cultivated it as well.\n\nMaize spread to the rest of the world because of its ability to grow in diverse climates. It was cultivated in Spain just a few decades after Columbus's voyages and then spread to Italy, West Africa and elsewhere.\n\nThe word \"maize\" derives from the Spanish form of the indigenous Taíno word for the plant, \"mahiz\". It is known by other names around the world.\n\nThe word \"corn\" outside North America, Australia, and New Zealand refers to any cereal crop, its meaning understood to vary geographically to refer to the local staple. In the United States, Canada, Australia, and New Zealand, \"corn\" primarily means maize; this usage started as a shortening of \"Indian corn\". \"Indian corn\" primarily means maize (the staple grain of indigenous Americans), but can refer more specifically to multicolored \"flint corn\" used for decoration.\n\nIn places outside North America, Australia, and New Zealand, \"corn\" often refers to maize in culinary contexts. The narrower meaning is usually indicated by some additional word, as in \"sweet corn\", \"sweetcorn\", \"corn on the cob\", \"baby corn\", the puffed confection known as \"popcorn\" and the breakfast cereal known as \"corn flakes\".\n\nIn Southern Africa, maize is commonly called \"mielie\" (Afrikaans) or \"mealie\" (English), words derived from the Portuguese word for maize, \"milho\".\n\n\"Maize\" is preferred in formal, scientific, and international usage because it refers specifically to this one grain, unlike \"corn\", which has a complex variety of meanings that vary by context and geographic region. \"Maize\" is used by agricultural bodies and research institutes such as the FAO and CSIRO. National agricultural and industry associations often include the word \"maize\" in their name even in English-speaking countries where the local, informal word is something other than \"maize\"; for example, the Maize Association of Australia, the Indian Maize Development Association, the Kenya Maize Consortium and Maize Breeders Network, the National Maize Association of Nigeria, the Zimbabwe Seed Maize Association. However, in commodities trading, \"corn\" consistently refers to maize and not other grains.\n\nThe maize plant is often in height, though some natural strains can grow . The stem is commonly composed of 20 internodes of length. A leaf, which grows from each node, is generally in width and in length.\n\nEars develop above a few of the leaves in the midsection of the plant, between the stem and leaf sheath, elongating by around per day, to a length of with being the maximum alleged in the subspecies. They are female inflorescences, tightly enveloped by several layers of ear leaves commonly called husks. Certain varieties of maize have been bred to produce many additional developed ears. These are the source of the \"baby corn\" used as a vegetable in Asian cuisine.\n\nThe apex of the stem ends in the tassel, an inflorescence of male flowers. When the tassel is mature and conditions are suitably warm and dry, anthers on the tassel dehisce and release pollen. Maize pollen is anemophilous (dispersed by wind), and because of its large settling velocity, most pollen falls within a few meters of the tassel.\n\nElongated stigmas, called silks, emerge from the whorl of husk leaves at the end of the ear. They are often pale yellow and in length, like tufts of hair in appearance. At the end of each is a carpel, which may develop into a \"kernel\" if fertilized by a pollen grain. The pericarp of the fruit is fused with the seed coat referred to as \"caryopsis\", typical of the grasses, and the entire kernel is often referred to as the \"seed\". The cob is close to a multiple fruit in structure, except that the individual fruits (the kernels) never fuse into a single mass. The grains are about the size of peas, and adhere in regular rows around a white, pithy substance, which forms the ear. The maximum size of kernels is reputedly . An ear commonly holds 600 kernels. They are of various colors: blackish, bluish-gray, purple, green, red, white and yellow. When ground into flour, maize yields more flour with much less bran than wheat does. It lacks the protein gluten of wheat and, therefore, makes baked goods with poor rising capability. A genetic variant that accumulates more sugar and less starch in the ear is consumed as a vegetable and is called sweet corn. Young ears can be consumed raw, with the cob and silk, but as the plant matures (usually during the summer months), the cob becomes tougher and the silk dries to inedibility. By the end of the growing season, the kernels dry out and become difficult to chew without cooking them tender first in boiling water.\n\nPlanting density affects multiple aspects of maize. Modern farming techniques in developed countries usually rely on dense planting, which produces one ear per stalk. Stands of silage maize are yet denser, and achieve a lower percentage of ears and more plant matter.\n\nMaize is a facultative short-day plant and flowers in a certain number of growing degree days <nowiki»</nowiki> in the environment to which it is adapted. The magnitude of the influence that long nights have on the number of days that must pass before maize flowers is genetically prescribed and regulated by the phytochrome system. Photoperiodicity can be eccentric in tropical cultivars such that the long days characteristic of higher latitudes allow the plants to grow so tall that they do not have enough time to produce seed before being killed by frost. These attributes, however, may prove useful in using tropical maize for biofuels.\n\nImmature maize shoots accumulate a powerful antibiotic substance, 2,4-dihydroxy-7-methoxy-1,4-benzoxazin-3-one (DIMBOA). DIMBOA is a member of a group of hydroxamic acids (also known as benzoxazinoids) that serve as a natural defense against a wide range of pests, including insects, pathogenic fungi and bacteria. DIMBOA is also found in related grasses, particularly wheat. A maize mutant (bx) lacking DIMBOA is highly susceptible to attack by aphids and fungi. DIMBOA is also responsible for the relative resistance of immature maize to the European corn borer (family Crambidae). As maize matures, DIMBOA levels and resistance to the corn borer decline.\n\nBecause of its shallow roots, maize is susceptible to droughts, intolerant of nutrient-deficient soils, and prone to be uprooted by severe winds.\n\nWhile yellow maizes derive their color from lutein and zeaxanthin, in red-colored maizes, the kernel coloration is due to anthocyanins and phlobaphenes. These latter substances are synthesized in the flavonoids synthetic pathway from polymerisation of flavan-4-ols by the expression of maize pericarp color1 (p1) gene which encodes an R2R3 myb-like transcriptional activator of the A1 gene encoding for the dihydroflavonol 4-reductase (reducing dihydroflavonols into flavan-4-ols) while another gene (Suppressor of Pericarp Pigmentation 1 or SPP1) acts as a suppressor. The p1 gene encodes an Myb-homologous transcriptional activator of genes required for biosynthesis of red phlobaphene pigments, while the P1-wr allele specifies colorless kernel pericarp and red cobs, and unstable factor for orange1 (Ufo1) modifies P1-wr expression to confer pigmentation in kernel pericarp, as well as vegetative tissues, which normally do not accumulate significant amounts of phlobaphene pigments. The maize P gene encodes a Myb homolog that recognizes the sequence CCT/AACC, in sharp contrast with the C/TAACGG bound by vertebrate Myb proteins.\n\nMaize flowers may sometimes exhibit mutations that lead to the formation of female flowers in the tassel. These mutations, \"ts4\" and \"Ts6\", prohibit the development of the stamen while simultaneously promoting pistil development. This may cause inflorescences containing both male and female flowers, or hermaphrodite flowers.\n\nMaize is an annual grass in the family Gramineae, which includes such plants as wheat, rye, barley, rice, sorghum, and sugarcane. There are two major species of the Zea genus: \"Zea mays\" (maize) and Zea diploperennis, which is a perennial type of teosinte. The annual teosinte variety called \"Zea mays mexicana\" is the closest botanical relative to maize. It still grows in the wild as an annual in Mexico and Guatemala.\n\nMany forms of maize are used for food, sometimes classified as various subspecies related to the amount of starch each has:\n\nThis system has been replaced (though not entirely displaced) over the last 60 years by multivariable classifications based on ever more data. Agronomic data were supplemented by botanical traits for a robust initial classification, then genetic, cytological, protein and DNA evidence was added. Now, the categories are forms (little used), races, racial complexes, and recently branches.\n\nMaize is a diploid with 20 chromosomes (n=10). The combined length of the chromosomes is 1500 cM. Some of the maize chromosomes have what are known as \"chromosomal knobs\": highly repetitive heterochromatic domains that stain darkly. Individual knobs are polymorphic among strains of both maize and teosinte.\n\nBarbara McClintock used these knob markers to validate her transposon theory of \"jumping genes\", for which she won the 1983 Nobel Prize in Physiology or Medicine. Maize is still an important model organism for genetics and developmental biology today.\n\nThe Maize Genetics Cooperation Stock Center, funded by the USDA Agricultural Research Service and located in the Department of Crop Sciences at the University of Illinois at Urbana-Champaign, is a stock center of maize mutants. The total collection has nearly 80,000 samples. The bulk of the collection consists of several hundred named genes, plus additional gene combinations and other heritable variants. There are about 1000 chromosomal aberrations (e.g., translocations and inversions) and stocks with abnormal chromosome numbers (e.g., tetraploids). Genetic data describing the maize mutant stocks as well as myriad other data about maize genetics can be accessed at MaizeGDB, the Maize Genetics and Genomics Database.\n\nIn 2005, the US National Science Foundation (NSF), Department of Agriculture (USDA) and the Department of Energy (DOE) formed a consortium to sequence the B73 maize genome. The resulting DNA sequence data was deposited immediately into GenBank, a public repository for genome-sequence data. Sequences and genome annotations have also been made available throughout the project's lifetime at the project's official site.\n\nPrimary sequencing of the maize genome was completed in 2008. On November 20, 2009, the consortium published results of its sequencing effort in \"Science\". The genome, 85% of which is composed of transposons, was found to contain 32,540 genes (By comparison, the human genome contains about 2.9 billion bases and 26,000 genes). Much of the maize genome has been duplicated and reshuffled by helitrons—group of rolling circle transposons.\n\nMaize reproduces sexually each year. This randomly selects half the genes from a given plant to propagate to the next generation, meaning that desirable traits found in the crop (like high yield or good nutrition) can be lost in subsequent generations unless certain techniques are used.\n\nMaize breeding in prehistory resulted in large plants producing large ears. Modern breeding began with individuals who selected highly productive varieties in their fields and then sold seed to other farmers. James L. Reid was one of the earliest and most successful developing Reid's Yellow Dent in the 1860s. These early efforts were based on mass selection. Later breeding efforts included ear to row selection (C. G. Hopkins c. 1896), hybrids made from selected inbred lines (G. H. Shull, 1909), and the highly successful double cross hybrids using four inbred lines (D. F. Jones c. 1918, 1922). University supported breeding programs were especially important in developing and introducing modern hybrids (Ref Jugenheimer Hybrid Maize Breeding and Seed Production pub. 1958). By the 1930s, companies such as Pioneer devoted to production of hybrid maize had begun to influence long term development. Internationally important seed banks such as the International Maize and Wheat Improvement Center (CIMMYT) and the US bank at the Maize Genetics Cooperation Stock Center University of Illinois at Urbana-Champaign maintain germplasm important for future crop development.\n\nSince the 1940s the best strains of maize have been first-generation hybrids made from inbred strains that have been optimized for specific traits, such as yield, nutrition, drought, pest and disease tolerance. Both conventional cross-breeding and genetic modification have succeeded in increasing output and reducing the need for cropland, pesticides, water and fertilizer. There is conflicting evidence to support the hypothesis that maize yield potential has increased over the past few decades. This suggests that changes in yield potential are associated with leaf angle, lodging resistance, tolerance of high plant density, disease/pest tolerance, and other agronomic traits rather than increase of yield potential per individual plant.\n\nCIMMYT operates a conventional breeding program to provide optimized strains. The program began in the 1980s. Hybrid seeds are distributed in Africa by the Drought Tolerant Maize for Africa project.\n\nGenetically modified (GM) maize was one of the 26 GM crops grown commercially in 2016. Grown since 1997 in the United States and Canada, 92% of the US maize crop was genetically modified in 2016 and 33% of the worldwide maize crop was GM in 2016. As of 2011, Herbicide-tolerant maize varieties were grown in Argentina, Australia, Brazil, Canada, China, Colombia, El Salvador, the European Union, Honduras, Japan, Korea, Malaysia, Mexico, New Zealand, Philippines, the Russian Federation, Singapore, South Africa, Taiwan, Thailand, and USA, and insect-resistant corn was grown in Argentina, Australia, Brazil, Canada, Chile, China, Colombia, Czech Republic, Egypt, the EU, Honduras, Japan, Korea, Malaysia, Mexico, Netherlands, New Zealand, Philippines, Romania, Russian Federation, South Africa, Switzerland, Taiwan, USA, and Uruguay.\n\nIn September 2000, up to $50 million worth of food products were recalled due to the presence of Starlink genetically modified corn, which had been approved only for animal consumption and had not been approved for human consumption, and was subsequently withdrawn from the market.\n\nMaize is the domesticated variant of teosinte. The two plants have dissimilar appearance, maize having a single tall stalk with multiple leaves and teosinte being a short, bushy plant. The difference between the two is largely controlled by differences in just two genes.\n\nSeveral theories had been proposed about the specific origin of maize in Mesoamerica:\nIn the late 1930s, Paul Mangelsdorf suggested that domesticated maize was the result of a hybridization event between an unknown wild maize and a species of \"Tripsacum\", a related genus. This theory about the origin of maize has been refuted by modern genetic testing, which refutes Mangelsdorf's model and the fourth listed above.\n\nThe teosinte origin theory was proposed by the Russian botanist Nikolai Ivanovich Vavilov in 1931 and the later American Nobel Prize-winner George Beadle in 1932. It is supported experimentally and by recent studies of the plants' genomes. Teosinte and maize are able to cross-breed and produce fertile offspring. A number of questions remain concerning the species, among them:\n\nThe domestication of maize is of particular interest to researchers—archaeologists, geneticists, ethnobotanists, geographers, etc. The process is thought by some to have started 7,500 to 12,000 years ago. Research from the 1950s to 1970s originally focused on the hypothesis that maize domestication occurred in the highlands between the states of Oaxaca and Jalisco, because the oldest archaeological remains of maize known at the time were found there.\n\nGenetic studies, published in 2004 by John Doebley, identified \"Zea mays\" ssp. \"parviglumis\", native to the Balsas River valley in Mexico's southwestern highlands, and also known as Balsas teosinte, as being the crop wild relative teosinte genetically most similar to modern maize. This was confirmed by further studies, which refined this hypothesis somewhat. Archaeobotanical studies, published in 2009, point to the middle part of the Balsas River valley as the likely location of early domestication; this river is not very long, so these locations are not very distant. Stone milling tools with maize residue have been found in an 8,700 year old layer of deposits in a cave not far from Iguala, Guerrero.\n\nDoebley was part of the team that first published, in 2002, that maize had been domesticated only once, about 9,000 years ago, and then spread throughout the Americas.\n\nA primitive corn was being grown in southern Mexico, Central America, and northern South America 7,000 years ago. Archaeological remains of early maize ears, found at Guila Naquitz Cave in the Oaxaca Valley, date back roughly 6,250 years; the oldest ears from caves near Tehuacan, Puebla, 5,450 B.P. (Before Present)\n\nMaize pollen dated to 7,300 B.P. from San Andres, Tabasco, on the Caribbean coast has also been recovered.\n\nAs maize was introduced to new cultures, new uses were developed and new varieties selected to better serve in those preparations. Maize was the staple food, or a major staple – along with squash, Andean region potato, quinoa, beans, and amaranth – of most pre-Columbian North American, Mesoamerican, South American, and Caribbean cultures. The Mesoamerican civilization, in particular, was deeply interrelated with maize. Its traditions and rituals involved all aspects of maize cultivation – from the planting to the food preparation. Maize formed the Mesoamerican people's identity.\n\nIt is unknown what precipitated its domestication, because the edible portion of the wild variety is too small, and hard to obtain, to be eaten directly, as each kernel is enclosed in a very hard bivalve shell.\n\nIn 1939, George Beadle demonstrated that the kernels of teosinte are readily \"popped\" for human consumption, like modern popcorn. Some have argued it would have taken too many generations of selective breeding to produce large, compressed ears for efficient cultivation. However, studies of the hybrids readily made by intercrossing teosinte and modern maize suggest this objection is not well founded.\n\nAround 4,500 B.P., maize began to spread to the north; it was first cultivated in what is now the United States at several sites in New Mexico and Arizona, about 4,100 B.P.\n\nDuring the first millennium AD, maize cultivation spread more widely in the areas north. In particular, the large-scale adoption of maize agriculture and consumption in eastern North America took place about A.D. 900. Native Americans cleared large forest and grassland areas for the new crop.\n\nIn 2005, research by the USDA Forest Service suggested that the rise in maize cultivation 500 to 1,000 years ago in what is now the southeastern United States corresponded with a decline of freshwater mussels, which are very sensitive to environmental changes.\n\nBecause it is cold-intolerant, in the temperate zones maize must be planted in the spring. Its root system is generally shallow, so the plant is dependent on soil moisture. As a plant that uses C4 carbon fixation, maize is a considerably more water-efficient crop than plants that use C3 carbon fixation such as alfalfa and soybeans. Maize is most sensitive to drought at the time of silk emergence, when the flowers are ready for pollination. In the United States, a good harvest was traditionally predicted if the maize was \"knee-high by the Fourth of July\", although modern hybrids generally exceed this growth rate. Maize used for silage is harvested while the plant is green and the fruit immature. Sweet corn is harvested in the \"milk stage\", after pollination but before starch has formed, between late summer and early to mid-autumn. Field maize is left in the field until very late in the autumn to thoroughly dry the grain, and may, in fact, sometimes not be harvested until winter or even early spring. The importance of sufficient soil moisture is shown in many parts of Africa, where periodic drought regularly causes maize crop failure and consequent famine. Although it is grown mainly in wet, hot climates, it has been said to thrive in cold, hot, dry or wet conditions, meaning that it is an extremely versatile crop.\nMaize was planted by the Native Americans in hills, in a complex system known to some as the Three Sisters. Maize provided support for beans, and the beans provided nitrogen derived from nitrogen-fixing rhizobia bacteria which live on the roots of beans and other legumes; and squashes provided ground cover to stop weeds and inhibit evaporation by providing shade over the soil. This method was replaced by single species hill planting where each hill apart was planted with three or four seeds, a method still used by home gardeners. A later technique was \"checked maize\", where hills were placed apart in each direction, allowing cultivators to run through the field in two directions. In more arid lands, this was altered and seeds were planted in the bottom of deep furrows to collect water. Modern technique plants maize in rows which allows for cultivation while the plant is young, although the hill technique is still used in the maize fields of some Native American reservations. When maize is planted in rows, it also allows for planting of other crops between these rows to make more efficient use of land space.\n\nIn most regions today, maize grown in residential gardens is still often planted manually with a hoe, whereas maize grown commercially is no longer planted manually but rather is planted with a planter. In North America, fields are often planted in a two-crop rotation with a nitrogen-fixing crop, often alfalfa in cooler climates and soybeans in regions with longer summers. Sometimes a third crop, winter wheat, is added to the rotation.\n\nMany of the maize varieties grown in the United States and Canada are hybrids. Often the varieties have been genetically modified to tolerate glyphosate or to provide protection against natural pests. Glyphosate is an herbicide which kills all plants except those with genetic tolerance. This genetic tolerance is very rarely found in nature.\n\nIn the midwestern United States, low-till or no-till farming techniques are usually used. In low-till, fields are covered once, maybe twice, with a tillage implement either ahead of crop planting or after the previous harvest. The fields are planted and fertilized. Weeds are controlled through the use of herbicides, and no cultivation tillage is done during the growing season. This technique reduces moisture evaporation from the soil, and thus provides more moisture for the crop.\nThe technologies mentioned in the previous paragraph enable low-till and no-till farming. Weeds compete with the crop for moisture and nutrients, making them undesirable.\n\nBefore the 20th century, all maize harvesting was by manual labour, by grazing, or by some combination of those. Whether the ears were hand-picked and the stover was grazed, or the whole plant was cut, gathered, and shocked, people and livestock did all the work. Between the 1890s and the 1970s, the technology of maize harvesting expanded greatly. Today, all such technologies, from entirely manual harvesting to entirely mechanized, are still in use to some degree, as appropriate to each farm's needs, although the thoroughly mechanized versions predominate, as they offer the lowest unit costs when scaled to large farm operations. For small farms, their unit cost can be too high, as their higher fixed cost cannot be amortized over as many units.\n\nBefore World War II, most maize in North America was harvested by hand. This involved a large numbers of workers and associated social events (husking or shucking bees). From the 1890s onward, some machinery became available to partially mechanize the processes, such as one- and two-row mechanical pickers (picking the ear, leaving the stover) and corn binders, which are reaper-binders designed specifically for maize (for example, ). The latter produce sheaves that can be shocked. By hand or mechanical picker, the entire ear is harvested, which then requires a separate operation of a maize sheller to remove the kernels from the ear. Whole ears of maize were often stored in corn cribs, and these whole ears are a sufficient form for some livestock feeding use. Today corn cribs with whole ears, and corn binders, are less common because most modern farms harvest the grain from the field with a combine and store it in bins. The combine with a corn head (with points and snap rolls instead of a reel) does not cut the stalk; it simply pulls the stalk down. The stalk continues downward and is crumpled into a mangled pile on the ground, where it usually is left to become organic matter for the soil. The ear of maize is too large to pass between slots in a plate as the snap rolls pull the stalk away, leaving only the ear and husk to enter the machinery. The combine separates out the husk and the cob, keeping only the kernels.\n\nWhen maize is a silage crop, the entire plant is usually chopped at once with a forage harvester (chopper) and ensiled in silos or polymer wrappers. Ensiling of sheaves cut by a corn binder was formerly common in some regions but has become uncommon.\n\nFor storing grain in bins, the moisture of the grain must be sufficiently low to avoid spoiling. If the moisture content of the harvested grain is too high, grain dryers are used to reduce the moisture content by blowing heated air through the grain. This can require large amounts of energy in the form of combustible gases (propane or natural gas) and electricity to power the blowers.\n\nMaize is widely cultivated throughout the world, and a greater weight of maize is produced each year than any other grain. In 2014, total world production was 1.04 billion tonnes, led by the United States with 35% of the total (table). China produced 21% of the global total.\n\nIn 2016, maize (corn) production was forecast to be over 15 billion bushels, an increase of 11% over 2014 American production. Based on conditions as of August 2016, the expected yield would be the highest ever for the United States. The area of harvested maize was forecast to be 87 million acres, an increase of 7% over 2015. Maize is especially popular in Midwestern states such as Indiana and Illinois; in the latter, it was named the state's official grain in 2017.\n\n\nThe susceptibility of maize to the European corn borer and corn rootworms, and the resulting large crop losses which are estimated at a billion dollars worldwide for each pest, led to the development of transgenics expressing the \"Bacillus thuringiensis\" toxin. \"Bt maize\" is widely grown in the United States and has been approved for release in Europe.\n\n\nMaize and cornmeal (ground dried maize) constitute a staple food in many regions of the world.\n\nMaize is central to Mexican food. Virtually every dish in Mexican cuisine uses maize. In the form of grain or cornmeal, maize is the main ingredient of tortillas, tamales, pozole, atole and all the dishes based on them, like tacos, quesadillas, chilaquiles, enchiladas, tostadas and many more. In Mexico even a fungus of maize, known as huitlacoche is considered a delicacy.\n\nIntroduced into Africa by the Portuguese in the 16th century, maize has become Africa's most important staple food crop. Maize meal is made into a thick porridge in many cultures: from the polenta of Italy, the \"angu\" of Brazil, the \"mămăligă\" of Romania, to cornmeal mush in the US (and hominy grits in the South) or the food called mealie pap in South Africa and \"sadza\", \"nshima\" and \"ugali\" in other parts of Africa. Maize meal is also used as a replacement for wheat flour, to make cornbread and other baked products. Masa (cornmeal treated with limewater) is the main ingredient for tortillas, atole and many other dishes of Central American food.\n\nPopcorn consists of kernels of certain varieties that explode when heated, forming fluffy pieces that are eaten as a snack. Roasted dried maize ears with semihardened kernels, coated with a seasoning mixture of fried chopped spring onions with salt added to the oil, is a popular snack food in Vietnam. \"Cancha\", which are roasted maize chulpe kernels, are a very popular snack food in Peru, and also appears in traditional Peruvian \"ceviche\". An unleavened bread called \"makki di roti\" is a popular bread eaten in the Punjab region of India and Pakistan.\n\n\"Chicha\" and \"chicha morada\" (purple chicha) are drinks typically made from particular types of maize. The first one is fermented and alcoholic, the second is a soft drink commonly drunk in Peru.\n\nCorn flakes are a common breakfast cereal in North America and the United Kingdom, and found in many other countries all over the world.\n\nMaize can also be prepared as hominy, in which the kernels are soaked with lye in a process called nixtamalization; or grits, which are coarsely ground hominy. These are commonly eaten in the Southeastern United States, foods handed down from Native Americans, who called the dish sagamite.\n\nThe Brazilian dessert \"canjica\" is made by boiling maize kernels in sweetened milk.\nMaize can also be harvested and consumed in the unripe state, when the kernels are fully grown but still soft. Unripe maize must usually be cooked to become palatable; this may be done by simply boiling or roasting the whole ears and eating the kernels right off the cob. Sweet corn, a genetic variety that is high in sugars and low in starch, is usually consumed in the unripe state. Such corn on the cob is a common dish in the United States, Canada, United Kingdom, Cyprus, some parts of South America, and the Balkans, but virtually unheard of in some European countries. Corn on the cob was hawked on the streets of early 19th-century New York City by poor, barefoot \"Hot Corn Girls\", who were thus the precursors of hot dog carts, churro wagons, and fruit stands seen on the streets of big cities today. The cooked, unripe kernels may also be shaved off the cob and served as a vegetable in side dishes, salads, garnishes, etc. Alternatively, the raw unripe kernels may also be grated off the cobs and processed into a variety of cooked dishes, such as maize purée, tamales, \"pamonhas\", \"curau\", cakes, ice creams, etc.\n\nMaize is a major source of starch. Cornstarch (maize flour) is a major ingredient in home cooking and in many industrialized food products. Maize is also a major source of cooking oil (corn oil) and of maize gluten. Maize starch can be hydrolyzed and enzymatically treated to produce syrups, particularly high fructose corn syrup, a sweetener; and also fermented and distilled to produce grain alcohol. Grain alcohol from maize is traditionally the source of Bourbon whiskey. Maize is sometimes used as the starch source for beer.\n\nWithin the United States, the usage of maize for human consumption constitutes about 1/40th of the amount grown in the country. In the United States and Canada, maize is mostly grown to feed livestock, as forage, silage (made by fermentation of chopped green cornstalks), or grain. Maize meal is also a significant ingredient of some commercial animal food products, such as dog food.\n\nRaw, yellow, sweet maize kernels are composed of 76% water, 19% carbohydrates, 3% protein, and 1% fat (table). In a 100-gram serving, maize kernels provide 86 calories and are a good source (10-19% of the Daily Value) of the B vitamins, thiamin, niacin (but see Pellagra warning below), pantothenic acid (B5) and folate (right table for raw, uncooked kernels, USDA Nutrient Database). In moderate amounts, they also supply dietary fiber and the essential minerals, magnesium and phosphorus whereas other nutrients are in low amounts (table).\n\nMaize has suboptimal amounts of the essential amino acids tryptophan and lysine, which accounts for its lower status as a protein source. The indigenous Americans overcame this deficiency with the inclusion of beans in their diet.\n\nMaize is a major source of both grain feed and fodder for livestock. It is fed to the livestock in various ways. When it is used as a grain crop, the dried kernels are used as feed. They are often kept on the cob for storage in a corn crib, or they may be shelled off for storage in a grain bin. The farm that consumes the feed may produce it, purchase it on the market, or some of both. When the grain is used for feed, the rest of the plant (the corn stover) can be used later as fodder, bedding (litter), or soil amendment. When the whole maize plant (grain plus stalks and leaves) is used for fodder, it is usually chopped all at once and ensilaged, as digestibility and palatability are higher in the ensilaged form than in the dried form. Maize silage is one of the most valuable forages for ruminants. Before the advent of widespread ensilaging, it was traditional to gather the corn into shocks after harvesting, where it dried further. With or without a subsequent move to the cover of a barn, it was then stored for weeks to several months until fed to the livestock. Today ensilaging can occur not only in siloes but also in silage wrappers. However, in the tropics maize can be harvested year-round and fed as green forage to the animals.\n\nStarch from maize can also be made into plastics, fabrics, adhesives, and many other chemical products.\n\nThe corn steep liquor, a plentiful watery byproduct of maize wet milling process, is widely used in the biochemical industry and research as a culture medium to grow many kinds of microorganisms.\n\nChrysanthemin is found in purple corn and is used as a food coloring.\n\n\"Feed maize\" is being used increasingly for heating; specialized corn stoves (similar to wood stoves) are available and use either feed maize or wood pellets to generate heat. Maize cobs are also used as a biomass fuel source. Maize is relatively cheap and home-heating furnaces have been developed which use maize kernels as a fuel. They feature a large hopper that feeds the uniformly sized maize kernels (or wood pellets or cherry pits) into the fire.\n\nMaize is increasingly used as a feedstock for the production of ethanol fuel. When considering where to construct an ethanol plant, one of the site selection criteria is to ensure there is locally available feedstock. Ethanol is mixed with gasoline to decrease the amount of pollutants emitted when used to fuel motor vehicles. High fuel prices in mid-2007 led to higher demand for ethanol, which in turn led to higher prices paid to farmers for maize. This led to the 2007 harvest being one of the most profitable maize crops in modern history for farmers. Because of the relationship between fuel and maize, prices paid for the crop now tend to track the price of oil. \n\nThe price of food is affected to a certain degree by the use of maize for biofuel production. The cost of transportation, production, and marketing are a large portion (80%) of the price of food in the United States. Higher energy costs affect these costs, especially transportation. The increase in food prices the consumer has been seeing is mainly due to the higher energy cost. The effect of biofuel production on other food crop prices is indirect. Use of maize for biofuel production increases the demand, and therefore price of maize. This, in turn, results in farm acreage being diverted from other food crops to maize production. This reduces the supply of the other food crops and increases their prices.\nMaize is widely used in Germany as a feedstock for biogas plants. Here the maize is harvested, shredded then placed in silage clamps from which it is fed into the biogas plants. This process makes use of the whole plant rather than simply using the kernels as in the production of fuel ethanol.\n\nA biomass gasification power plant in Strem near Güssing, Burgenland, Austria, began in 2005. Research is being done to make diesel out of the biogas by the Fischer Tropsch method.\n\nIncreasingly, ethanol is being used at low concentrations (10% or less) as an additive in gasoline (gasohol) for motor fuels to increase the octane rating, lower pollutants, and reduce petroleum use (what is nowadays also known as \"biofuels\" and has been generating an intense debate regarding the human beings' necessity of new sources of energy, on the one hand, and the need to maintain, in regions such as Latin America, the food habits and culture which has been the essence of civilizations such as the one originated in Mesoamerica; the entry, January 2008, of maize among the commercial agreements of NAFTA has increased this debate, considering the bad labor conditions of workers in the fields, and mainly the fact that NAFTA \"opened the doors to the import of maize from the United States, where the farmers who grow it receive multimillion dollar subsidies and other government supports. (...) According to OXFAM UK, after NAFTA went into effect, the price of maize in Mexico fell 70% between 1994 and 2001. The number of farm jobs dropped as well: from 8.1 million in 1993 to 6.8 million in 2002. Many of those who found themselves without work were small-scale maize growers.\"). However, introduction in the northern latitudes of the US of tropical maize for biofuels, and not for human or animal consumption, may potentially alleviate this.\n\nAs a result of the US federal government announcing its production target of of biofuels by 2017, ethanol production will grow to by 2010, up from 4.5 billion in 2006, boosting ethanol's share of maize demand in the US from 22.6 percent to 36.1 percent.\n\nMaize is bought and sold by investors and price speculators as a tradable commodity using corn futures contracts. These \"futures\" are traded on the Chicago Board of Trade (CBOT) under ticker symbol C. They are delivered every year in March, May, July, September, and December.\n\nSome forms of the plant are occasionally grown for ornamental use in the garden. For this purpose, variegated and colored leaf forms as well as those with colorful ears are used.\n\nCorncobs can be hollowed out and treated to make inexpensive smoking pipes, first manufactured in the United States in 1869.\nAn unusual use for maize is to create a \"corn maze\" (or \"maize maze\") as a tourist attraction. The idea of a maize maze was introduced by the American Maze Company who created a maze in Pennsylvania in 1993. Traditional mazes are most commonly grown using yew hedges, but these take several years to mature. The rapid growth of a field of maize allows a maze to be laid out using GPS at the start of a growing season and for the maize to grow tall enough to obstruct a visitor's line of sight by the start of the summer. In Canada and the US, these are popular in many farming communities.\n\nMaize kernels can be used in place of sand in a sandboxlike enclosure for children's play.\n\nStigmas from female maize flowers, popularly called corn silk, are sold as herbal supplements.\n\nMaize is used as a fish bait, called \"dough balls\". It is particularly popular in Europe for coarse fishing.\n\nAdditionally, feed corn is sometimes used by hunters to bait animals such as deer or wild hogs.\n\nThe breakdown of usage of the 12.1-billion-bushel (307-million-tonne) 2008 US maize crop was as follows, according to the World Agricultural Supply and Demand Estimates Report by the USDA.\n\nIn the US since 2009/2010, maize feedstock use for ethanol production has somewhat exceeded direct use for livestock feed; maize use for fuel ethanol was 5,130 million bushels (130 million tonnes) in the 2013/2014 marketing year.\n\nA fraction of the maize feedstock dry matter used for ethanol production is usefully recovered as DDGS (dried distillers grains with solubles). In the 2010/2011 marketing year, about 29.1 million tonnes of DDGS were fed to US livestock and poultry. Because starch utilization in fermentation for ethanol production leaves other grain constituents more concentrated in the residue, the feed value per kg of DDGS, with regard to ruminant-metabolizable energy and protein, exceeds that of the grain. Feed value for monogastric animals, such as swine and poultry, is somewhat lower than for ruminants.\n\nThe following table shows the nutrient content of maize and major staple foods in a raw harvested form. Raw forms are not edible and cannot be digested. These must be sprouted, or prepared and cooked for human consumption. In sprouted or cooked form, the relative nutritional and anti-nutritional contents of each of these staples are different from that of raw form of these staples reported in the table below.\n\nWhen maize was first introduced into farming systems other than those used by traditional native-American peoples, it was generally welcomed with enthusiasm for its productivity. However, a widespread problem of malnutrition soon arose wherever maize was introduced as a staple food. This was a mystery, since these types of malnutrition were not normally seen among the indigenous Americans, for whom maize was the principal staple food.\n\nIt was eventually discovered that the indigenous Americans had learned to soak maize in alkali-water (the process now known as nixtamalization) —made with ashes and lime (calcium oxide) since at least 1200–1500 BC by Mesoamericans and North Americans—which liberates the B-vitamin niacin, the lack of which was the underlying cause of the condition known as pellagra.\n\nMaize was introduced into the diet of nonindigenous Americans without the necessary cultural knowledge acquired over thousands of years in the Americas. In the late 19th century, pellagra reached epidemic proportions in parts of the southern US, as medical researchers debated two theories for its origin: the deficiency theory (which was eventually shown to be true) said that pellagra was due to a deficiency of some nutrient, and the germ theory said that pellagra was caused by a germ transmitted by stable flies. A third theory, promoted by the eugenicist Charles Davenport, held that people only contracted pellagra if they were susceptible to it due to certain \"constitutional, inheritable\" traits of the affected individual.\n\nOnce alkali processing and dietary variety were understood and applied, pellagra disappeared in the developed world. The development of high lysine maize and the promotion of a more balanced diet have also contributed to its demise. Pellagra still exists today in food-poor areas and refugee camps where people survive on donated maize.\n\nMaize contains lipid transfer protein, an indigestible protein that survives cooking. This protein has been linked to a rare and understudied allergy to maize in humans. The allergic reaction can cause skin rash, swelling or itching of mucous membranes, diarrhea, vomiting, asthma and, in severe cases, anaphylaxis. It is unclear how common this allergy is in the general population.\n\nMaize has been an essential crop in the Andes since the pre-Columbian era. The Moche culture from Northern Peru made ceramics from earth, water, and fire. This pottery was a sacred substance, formed in significant shapes and used to represent important themes. Maize was represented anthropomorphically as well as naturally.\n\nIn the United States, maize ears along with tobacco leaves are carved into the capitals of columns in the United States Capitol building. Maize itself is sometimes used for temporary architectural detailing when the intent is to celebrate the fall season, local agricultural productivity and culture. Bundles of dried maize stalks are often displayed often along with pumpkins, gourds and straw in autumnal displays outside homes and businesses. A well-known example of architectural use is the Corn Palace in Mitchell, South Dakota, which uses cobs and ears of colored maize to implement a mural design that is recycled annually. Another well known example is the Field of Corn in Dublin, Ohio, where hundreds of concrete ears of corn lay in a grassy field.\n\nA maize stalk with two ripe ears is depicted on the reverse of the Croatian 1 lipa coin, minted since 1993.\n\n\n\n"}
{"id": "56240252", "url": "https://en.wikipedia.org/wiki?curid=56240252", "title": "Masaka–Mutukula–Mwanza High Voltage Power Line", "text": "Masaka–Mutukula–Mwanza High Voltage Power Line\n\nThe Masaka–Mutukula–Mwanza High Voltage Power Line is a proposed high voltage electricity power line, connecting the high voltage substation at Masaka, in Masaka District, in the Central Region of Uganda, to another high voltage substation at Mwanza, in Mwanza Region, in the Republic of Tanzania.\n\nThe power line starts at the Uganda Electricity Transmission Company Limited (UETCL) 220kV substation in Masaka. The line travels in a general southwesterly direction to Mutukula, at the international border with Tanzania. From Mutukula, the power line continues in a general southeasterly direction to Bukoba, on the western shores of Lake Victoria. From Bukoba, the line loops around the lake, and continues eastwards to end at Mwanza. The Ugandan portion of the power line measures . The distance travelled by the power line in Tanzania is approximately .\n\nThis power line is intended to enable Uganda and Tanzania to share electric energy. As part of the energy-sharing protocols of the East African Community, and Nile Basin Initiative, Uganda plans to increase energy sales to Tanzania, when Karuma Hydroelectric Power Station comes online in 2019. The construction of this high voltage power line has been planned as far back as 2011.\n\nAs of January 2018, the feasibility study, environmental impact assessment and population resettlement plans were complete. Plans to construct the power line were underway, under the auspices of the East African Community.\n\n\n"}
{"id": "40897003", "url": "https://en.wikipedia.org/wiki?curid=40897003", "title": "Ministry of Energy (Turkmenistan)", "text": "Ministry of Energy (Turkmenistan)\n\nThe Ministry of Energy of Turkmenistan () is a Cabinet Ministry in the Turkmenistan. Since 2015, the portfolio has been headed by Minister Döwran Rejepow.\n\nAccording to the Decree of the President of Turkmenistan Gurbanguly Berdimuhamedov on July 7, 2012 \"On creation of the Ministry of Energy of Turkmenistan\" was the successor of the abolished department of the Ministry of Energy and Industry of Turkmenistan. By focusing on the power sector, the ministry will be able to deliver it more effective management and coordination. Before the agency faces the challenges of large-scale modernization of existing power generation capacity, the creation of new ones, to increase electricity export.\n\n"}
{"id": "57002927", "url": "https://en.wikipedia.org/wiki?curid=57002927", "title": "Mud horse", "text": "Mud horse\n\nA mud horse is a traditional type of hand-built wooden sledge used for fishing in Bridgewater Bay. \n\nAs of 2010 the only remaining mud-horse fisherman was fifth generation fisherman Adrian Sellick. His father, Brendan, was still selling the catch from Mudhorse Cottage in Stolford.\n"}
{"id": "40901634", "url": "https://en.wikipedia.org/wiki?curid=40901634", "title": "Multivariate ENSO index", "text": "Multivariate ENSO index\n\nThe multivariate ENSO index, abbreviated as MEI, is a method used to characterize the intensity of an El Niño Southern Oscillation (ENSO) event. Given that ENSO arises from a complex interaction of a variety of climate systems, MEI is regarded as the most comprehensive index for monitoring ENSO since it combines analysis of multiple meteorological and oceanographic components.\n\nMEI is determined as the first principal component of six different parameters: sea level pressure, zonal and meridional components of the surface wind, sea surface temperature, surface air temperature and cloudiness using data from the International Comprehensive Ocean-Atmosphere Data Set (ICOADS ). MEI is calculated twelve times per year for each “sliding bi-monthly season”, characterized as January–February, February–March, March–April, and so on. Large positive MEI values indicate the occurrence of El Niño conditions, while large negative MEI values indicate the occurrence of La Niña conditions.\n\nWhile the National Oceanic and Atmospheric Administration (NOAA) has recorded MEI values from 1950 to the present, various researchers have cited the need for data before 1950 in order to better characterize typical ENSO behavior versus unusual occurrences that may be a result of climate change. According to some sources, it is inadvisable to attempt to calculate MEI before 1950 given that environmental measurements were unreliable during the World Wars and there was a revolution in virtually all meteorological measurement methods on ships during the 1940s. However, a module known as “Extended MEI” or “MEI.txt” has been created that estimates MEI values from as far back as 1871. This was accomplished by using reconstructed data on sea level pressure and sea surface temperature, the two components thought to be most influential to determining MEI. Plots comparing MEI and MEI.ext values have shown that data from both methods are highly correlated, supporting the accuracy and effectiveness of MEI.ext.\n\nThe Southern Oscillation Index (SOI) is calculated based on the sea level pressure difference between Tahiti and Darwin, Australia. Despite being used frequently in ENSO studies, it is not considered as reliable as MEI given that it only takes into account one environmental variable.\n\nSimilar to SOI, Niño 3.4 SST uses one parameter – sea surface temperature – to characterize ENSO. The Niño 3.4 SST region consists of temperature measurements from between 5° N – 5° S and 120° – 170° W.\n\nThe Coupled ENSO Index (CEI) uses a combination of both the SOI and Niño 3.4 SST to account for both an atmospheric and oceanic component.\n\nDeveloped by Braganza, et al., 2009, this index uses coral, tree ring and ice core data to characterize ENSO events from 1525 – 1982. The proxy ENSO index covers a wide area across the Pacific, and includes data from the western and central Pacific, New Zealand, and subtropical North America. Although the proxy ENSO index covers a large time scale of over four centuries, it shows high correlation (> 40%) with the SOI, Niño 3.4 SST and CEI, indicating its accuracy and utility.\n"}
{"id": "217524", "url": "https://en.wikipedia.org/wiki?curid=217524", "title": "Neoprene", "text": "Neoprene\n\nNeoprene (also polychloroprene or pc-rubber) is a family of synthetic rubbers that are produced by polymerization of chloroprene. Neoprene exhibits good chemical stability and maintains flexibility over a wide temperature range. Neoprene is sold either as solid rubber or in latex form and is used in a wide variety of applications, such as laptop sleeves, orthopaedic braces (wrist, knee, etc.), electrical insulation, liquid and sheet applied elastomeric membranes or flashings, and automotive fan belts.\n\nNeoprene is produced by free-radical polymerization of chloroprene. In commercial production, this polymer is prepared by free radical emulsion polymerization. Polymerization is initiated using potassium persulfate. Bifunctional nucleophiles, metal oxides (e.g. zinc oxide), and thioureas are used to crosslink individual polymer strands. \n\nNeoprene was invented by DuPont scientists on April 17, 1930 after Dr Elmer K. Bolton of DuPont attended a lecture by Fr Julius Arthur Nieuwland, a professor of chemistry at the University of Notre Dame. Nieuwland's research was focused on acetylene chemistry and during the course of his work he produced divinyl acetylene, a jelly that firms into an elastic compound similar to rubber when passed over sulfur dichloride. After DuPont purchased the patent rights from the university, Wallace Carothers of DuPont took over commercial development of Nieuwland's discovery in collaboration with Nieuwland himself. Arnold Collins at DuPont focused on monovinyl acetylene and allowed it to react with hydrogen chloride gas, manufacturing chloroprene.\n\nDuPont first marketed the compound in 1931 under the trade name DuPrene, but its commercial possibilities were limited by the original manufacturing process, which left the product with a foul odor. A new process was developed, which eliminated the odor-causing byproducts and halved production costs, and the company began selling the material to manufacturers of finished end-products. To prevent shoddy manufacturers from harming the product's reputation, the trademark DuPrene was restricted to apply only to the material sold by DuPont. Since the company itself did not manufacture any DuPrene-containing end products, the trademark was dropped in 1937 and replaced with a generic name, neoprene, in an attempt \"to signify that the material is an ingredient, not a finished consumer product\". DuPont then worked extensively to generate demand for its product, implementing a marketing strategy that included publishing its own technical journal, which extensively publicized neoprene's uses as well as advertising other companies' neoprene-based products. By 1939, sales of neoprene were generating profits over $300,000 for the company ().\n\nNeoprene resists degradation more than natural or synthetic rubber. This relative inertness makes it well suited for demanding applications such as gaskets, hoses, and corrosion-resistant coatings. It can be used as a base for adhesives, noise isolation in power transformer installations, and as padding in external metal cases to protect the contents while allowing a snug fit. It resists burning better than exclusively hydrocarbon based rubbers, resulting in its appearance in weather stripping for fire doors and in combat related attire such as gloves and face masks. Because of its tolerance of extreme conditions, neoprene is used to line landfills. Neoprene's burn point is around 260 °C (500 °F). \n\nIn its native state, neoprene is a very pliable rubber-like material with insulating properties similar to rubber or other solid plastics.\n\nNeoprene foam is used in many applications and is produced in either closed-cell or open-cell form. The closed-cell form is waterproof, less compressible and more expensive. The open-cell form can be breathable. It is manufactured by foaming the plastic with nitrogen gas, for the insulation properties of the tiny enclosed and separated gas bubbles (nitrogen is used for chemical convenience, not because it is superior to air as an insulator).\n\nNeoprene is used as a load bearing base, usually between two prefabricated reinforced concrete elements or steel plates as well to evenly guide force from one element to another.\n\nNeoprene is a popular material in making protective clothing for aquatic activities. Foamed neoprene is commonly used to make fly fishing waders and wetsuits, as it provides excellent insulation against cold. The foam is quite buoyant, and divers compensate for this by wearing weights. Thick wet suits made at the extreme end of their cold water protection are usually made of 7 mm thick neoprene. Since foam neoprene contains gas pockets, the material compresses under water pressure, getting thinner at greater depths; a 7 mm neoprene wet suit offers much less exposure protection under 100 feet of water than at the surface. A recent advance in neoprene for wet suits is the \"super-flex\" variety, which mixes spandex into the neoprene for greater flexibility.\n\nNeoprene waders are usually about 5 mm thick, and in the medium price range as compared to cheaper materials such as nylon and more expensive waterproof fabrics made with breathable membranes.\n\nCompetitive swimming wetsuits are made of the most expanded foam; they have to be very flexible to allow the swimmer unrestricted movement. The downside is that they are quite fragile.\n\nRecently, neoprene has become a favorite material for lifestyle and other home accessories including laptop sleeves, tablet holders, remote controls, mouse pads, and cycling chamois. In this market, it sometimes competes with LRPu (low-resilience polyurethane), which is a sturdier (more impact-resistant) but less-used material.\n\nThe Rhodes piano used hammer tips made of neoprene in its electric pianos, after changing from felt hammers around 1970.\n\nNeoprene is also used for speaker cones and drum practice pads.\n\nHydroponic and aerated gardening systems make use of small neoprene inserts to hold plants in place while propagating cuttings or using net cups. Inserts are relatively small, ranging in size from . Neoprene is a good choice for supporting plants because of its flexibility and softness, allowing plants to be held securely in place without the chance of causing damage to the stem. Neoprene root covers also help block out light from entering the rooting chamber of hydroponic systems, allowing for better root growth and to help deter the growth of algae.\n\nIn automotive industry, neoprene is often used as a material for car seats or car seat covers. Due to being wear-resistant and its protective characteristics, many car makers offer neoprene seats as an option for additional seat protection. Such seat covers are also included into vehicle packages for active drivers. Neoprene is based on a rubber layer for seat protection against moisture.\n\nNeoprene is used for Halloween masks and masks used for face protection, for insulating CPU sockets, to make waterproof automotive seat covers, in liquid and sheet-applied elastomeric roof membranes or flashings, and in a neoprene-spandex mixture for manufacture of wheelchair positioning harnesses. Because of its chemical resistance and overall durability, neoprene is sometimes used in the manufacture of dishwashing gloves, especially as an alternative to latex. In fashion, neoprene has been used by designers such as Gareth Pugh, Balenciaga, Rick Owens, Lanvin and Vera Wang. This trend, promoted by street style bloggers such as Jim Joquico of Fashion Chameleon, gained traction and trickled down to mainstream fashion around 2014.\n\nSome people are allergic to neoprene while others can get dermatitis from thiourea residues left from its production. The most common accelerator in the vulcanization of polychloroprene is ethylene thiourea (ETU), which has been classified as reprotoxic. The European rubber industry project called SafeRubber focused on alternatives to the use of ETU.\n\n\n"}
{"id": "1878412", "url": "https://en.wikipedia.org/wiki?curid=1878412", "title": "Nernst effect", "text": "Nernst effect\n\nIn physics and chemistry, the Nernst effect (also termed first Nernst–Ettingshausen effect, after Walther Nernst and Albert von Ettingshausen) is a thermoelectric (or thermomagnetic) phenomenon observed when a sample allowing electrical conduction is subjected to a magnetic field and a temperature gradient normal (perpendicular) to each other. An electric field will be induced normal to both.\n\nThis effect is quantified by the Nernst coefficient |\"N\"|, which is defined to be\n\nwhere formula_2 is the y-component of the electric field that results from the magnetic field's z-component formula_3 and the temperature gradient formula_4.\n\nThe reverse process is known as the Ettingshausen effect and also as the second Nernst–Ettingshausen effect.\n\nMobile energy carriers (for example conduction-band electrons in a semiconductor) will move along temperature gradients due to statistics and the relationship\nbetween temperature and kinetic energy. If there is a magnetic field transversal to the temperature gradient and the carriers are electrically charged, they experience a force perpendicular to their direction of motion (also the direction of the temperature gradient) and to the magnetic field. Thus, a perpendicular electric field is induced.\n\nSemiconductors exhibit the Nernst effect. This has been studied in the 1950s by Krylova, Mochan and many others. In metals however, it is almost non-existent. It appears in the vortex phase\nof type-II superconductors due to vortex motion. This has been studied by Huebener et al. High-temperature superconductors exhibit the Nernst effect both in the superconducting and in the pseudogap phase, as was first found by Xu et al. Heavy-Fermion superconductors can show a strong Nernst signal which is likely not due to the vortices, as was found by Bel et al.\n\n\n"}
{"id": "21283072", "url": "https://en.wikipedia.org/wiki?curid=21283072", "title": "Olivia Irvine Dodge", "text": "Olivia Irvine Dodge\n\nOlivia Irvine Dodge (October 7, 1918 – January 24, 2009) was a philanthropist who, along with her sister Clotilde, donated the house that is now the Minnesota Governor's Residence.\n\nDodge was a well-known environmentalist, founding the Dodge Nature Center in West St. Paul and Mendota Heights, Minnesota in 1967 and the Irvine Nature Center in Baltimore, Maryland in 1975. The centers teaches local schoolchildren about nature and the environment.\n\nDodge also had a renowned collection of President Franklin D. Roosevelt related material (one of the largest in the country), which she donated to the University of Minnesota in 1975.\n\n"}
{"id": "18892879", "url": "https://en.wikipedia.org/wiki?curid=18892879", "title": "Olmsted Locks and Dam", "text": "Olmsted Locks and Dam\n\nThe Olmsted Locks and Dam is a locks and concrete dam on the Ohio River at river mile 964.4. The project is intended to reduce tow and barge delays by replacing the existing older, and frequently congested, locks and dams Number 52 and Number 53. The locks are located about 17 miles upstream from the confluence of the Ohio and Mississippi rivers at Olmsted, Illinois.\n\nThe project is operational while the removal of Locks and Dams 52 and 53 should be completed around 2020. The project is both the largest and the most expensive inland waterway project ever undertaken in the United States.\n\nThe US Congress, through the Water Resources and Development Act of 1988 first approved a $775 million budget for the project in 1988 (October 1987 Price Levels). The lock chambers, completed in 2002, are wide and long.\n\nAccording to the US Army Corps of Engineers, the new dam and locks will reduce passage time to under one hour with the new system. Due to queuing at Lock and Dam Number 52 and Lock and Dam Number 53, it can take cargo traffic 15 to 20 hours each to transit the locks the Olmsted complex is intended to replace.\n\nWhen initiated the complex was projected to cost $775 million. As of February 2018, the estimated cost of the project is over $3 billion.\n\nWhile the project was initially scheduled for completion in 1998, by 2016 it was projected to become operational between 2018 and by 2020, Locks and Dams 52 and 53 would be decommissioned. \n\nThe United States Army Corps of Engineers (USACE), the Federal agency responsible for maintaining navigation on the USA's rivers, estimates the delay in completing the project results in a yearly loss of about $640 million to $800 million in lost benefits to the nation. While calculating these benefits is complex because of the amount of variables considered, it essentially takes into account the reduced costs industry (or businesses using the river for commerce) would experience if the Olmsted Project was operational versus the current means of transit through the aging and often unreliable Locks and Dams 52 and 53. These benefits further calculate the reduced costs in moving cargo through the river versus the next available cheapest alternative, usually by rail or by truck.\n\nThe Locks and Dams 52 and 53 Replacement Project, better known as the Olmsted Locks and Dam Project makes use of the innovative in-the-wet construction. When a dam is constructed on a small river, engineers usually create a cofferdam (or a enclosure) within a river and drain the water out of it to facilitate construction. However, building this entire project through the use of cofferdams would have been incredibly impeding to river traffic. As the hub of the Inland waterways, the largest transit point in the nation's river system where approximately 90 million tons of goods pass through each year, blocking large parts of the river would have caused major delays to river traffic even more so than evidenced at Locks and Dams 52 and 53.\n\nEngineers instead chose to construct the dam portion of the project using the in-the-wet technique, where concrete portions of the dam itself, were built offsite at a concrete casting yard, transported into the river for placement, and placed on the bottom of the river, all with minimal disruption to the river.\nThis has been perhaps the longest and largest civil works project in the history of the Corps of Engineers. Multiple delays, especially in funding have created a 30 year endeavor that has been inflated from a $700+ million price tag to over $3 billion in early 2018.\n\nThe biggest contributors to these increases have been:\n\n\n"}
{"id": "13090984", "url": "https://en.wikipedia.org/wiki?curid=13090984", "title": "Pneumatic chemistry", "text": "Pneumatic chemistry\n\nPneumatic chemistry is a term most-closely identified with an area of scientific research of the seventeenth, eighteenth, and early nineteenth centuries. Important goals of this work were an understanding of the physical properties of gases and how they relate to chemical reactions and, ultimately, the composition of matter.\n\nIn the eighteenth century, as the field of chemistry was evolving from alchemy, a field of the natural philosophy was created around the idea of air as a reagent. Before this, air was primarily considered a static substance that would not react and simply existed. However, as Lavoisier and several other pneumatic chemists would insist, the air was indeed dynamic, and would not only be influenced by combusted material, but would also influence the properties of different substances.\n\nThe initial concern of pneumatic chemistry was combustion reactions, beginning with Stephen Hales. These reactions would give off different \"airs\" as chemists would call them, and these different airs contained more simple substances. Until Lavoisier, these airs were considered separate entities with different properties; Lavoisier was responsible largely for changing the idea of air as being constituted by these different airs that his contemporaries and earlier chemists had discovered.\n\nThis study of gases was brought about by Hales with the invention of the pneumatic trough, an instrument capable of collecting the gas given off by reactions with reproducible results. The term \"gas\" was coined by J. B. van Helmont, in the early seventeenth century. This term was derived from the Greek word chaos as a result of his inability to collect properly the substances given off by reactions, as he was the first natural philosopher to make an attempt at carefully studying the third type of matter. However, it was not until Lavoisier performed his research in the eighteenth century that the word was used universally by scientists as a replacement for \"airs\".\n\nVan Helmont (1579 – 1644) is sometimes considered the founder of pneumatic chemistry, as he was the first natural philosopher to take an interest in air as a reagent. Alessandro Volta began investigating pneumatic chemistry in 1776 and argued that there were different types of inflammable air based on experiments on marsh gases. Pneumatic chemists credited with discovering chemical elements include Joseph Priestley, Henry Cavendish, Joseph Black, Daniel Rutherford, and Carl Scheele. Other individuals who investigated gases during this period include Robert Boyle, Stephen Hales, William Brownrigg, Antoine Lavoisier, Joseph Louis Gay-Lussac, and John Dalton.\n\nThe pneumatic trough was integral thereon in work with gases (or, as contemporary chemists called them, airs). Work done by Joseph Black, Joseph Priestley, Herman Boerhaave, and Henry Cavendish revolved largely around the use of the instrument, allowing them to collect airs given off by different chemical reactions and combustion analyses. Their work led to the discovery of many types of airs, such as dephlogisticated air (discovered by Joseph Priestley).\n\nMoreover, the chemistry of airs was not limited to combustion analyses. During the eighteenth century, many chymists used the discovery of airs as a new path for exploring old problems, with one example being the field of medicinal chemistry. One particular Englishman, James Watt, began to take the idea of airs and use them in what was referred to as \"pneumatic therapy\", or the use of airs to make laboratories more workable with fresh airs and also aid patients with different illnesses, with varying degrees of success. Most human experimentation done was performed on the chymists themselves, as they believed that self-experimentation was a necessary part or progressing the field.\n\nJames Watt's research in pneumatic chemistry involved the use of inflammable (H) and dephlogisticated (O) airs to create water. In 1783, James Watt showed that water was composed of inflammable and dephlogisticated airs, and that the masses of gases before combustion were exactly equal to the mass of water after combustion. Until this point, water was viewed as a fundamental element rather than a compound. James Watt also sought to explore the use of different airs in medical treatments as \"pneumatic therapy\" by collaborating with Dr. Thomas Beddoes to treat his daughter Jessie Watt using fixed air.\n\nJoseph Black was a chemist that took interest in the pneumatic field after studying under William Cullen. He was first interested in the topic of magnesia alba, or magnesium carbonate, and limestone, or calcium carbonate, and wrote a dissertation called \"De Humore acido a cibis orto, et magnesia alba\" on the properties of both. His experiments on magnesium carbonate led him to discover that fixed air, or carbon dioxide, was being given off during reactions with various chemicals, including breathing. Despite him never using the pneumatic trough or other instrumentation invented to collect and analyze the airs, his inferences led to more research into fixed air instead of common air, with the trough actually being used.\n\nAfter moving on to Glasgow to teach, Black turned his interests to the topic of heat. Through his experiments with ice and water, he made several discoveries about the latent heat of fusion and the latent heat of freezing water, as well as working extensively with specific heats of a number of liquids.\n\nJoseph Priestley, in \"Observations on different kinds of air,\" was one of the first people to describe air as being composed of different states of matter, and not as one element. Priestley elaborated on the notions of fixed air (CO), mephitic air and inflammable air to include \"inflammable nitrous air,\" \"vitriolic acid air,\" \"alkaline air\" and \"dephlogisticated air\". Priestley also described the process of respiration in terms of phlogiston theory. Priestley also established a process for treating scurvy and other ailments using fixed air in his \"Directions for impregnating water with fixed air.\" Priestley's work on pneumatic chemistry had an influence on his natural world views. His belief in an \"aerial economy\" stemmed from his belief in \"dephlogisticated air\" being the purest type of air and that phlogiston and combustion were at the heart of nature. \nJoseph Priestley chiefly researched with the pneumatic trough, but he was responsible for collecting several new water-soluble airs. This was achieved primarily by his substitution of mercury for water, and implementing a shelf under the head for increased stability, capitalizing on the idea Cavendish proposed and popularizing the mercury pneumatic trough.\n\nWhile not credited for direct research into the field of pneumatic chemistry, Boerhaave (teacher, researcher, and scholar) did publish the \"Elementa Chimiae\" in 1727. This treatise included support for Hales' work and also elaborated upon the idea of airs. Despite not publishing his own research, this section on airs in the \"Elementa Chimie\" was cited by many other contemporaries and contained much of the current knowledge of the properties of airs. Boerhaave is also credited with adding to the world of chemical thermometry through his work with Daniel Fahrenheit, also discussed in \"Elementa Chimiae.\"\n\nHenry Cavendish, despite not being the first to replace water in the trough with mercury, he was among the first to observe that fixed air was insoluble over mercury and therefore could be collected more efficiently using the adapted instrument. He also characterized fixed air (CO) and inflammable air (H). Inflammable air was one of the first gases isolated and discovered using the pneumatic trough. However, he did not exploit his own idea to its limit, and therefore did not use the mercury pneumatic trough to its full extent. Cavendish is credited with nearly correctly analyzing the content of gases in the atmosphere. Cavendish also showed that inflammable air and atmospheric air could be combined and heated to produce water in 1784.\n\nIn the eighteenth century, with the rise of combustion analysis in chemistry, Stephen Hales invented the pneumatic trough in order to collect gases from the samples of matter he used; while uninterested in the properties of the gases he collected, he wanted to explore how much gas was given off from the materials he burned or let ferment. Hales was successful in preventing the air from losing its \"elasticity,\" i.e. preventing it from experiencing a loss in volume, by bubbling the gas through water, and therefore dissolving the soluble gases.\n\nAfter the invention of the pneumatic trough, Stephen Hales continued his research into the different airs, and performed many Newtonian analyses of the various properties of them. He published his book \"Vegetable Staticks\" in 1727, which had a profound impact on the field of pneumatic chemistry, as many researchers cited this in their academic papers. In \"Vegetable Staticks\", Hales not only introduced his trough, but also the results he obtained from collected the air, such as the elasticity and composition of airs along with their ability to mix with others.\n\nStephen Hales, called the creator of pneumatic chemistry, created the pneumatic trough in 1727. This instrument was widely used by many chemists to explore the properties of different airs, such as what was called inflammable air (what is modernly called hydrogen). Lavoisier used this in addition to his gasometer to collect gases and analyze them, aiding him in creating his list of simple substances.\n\nThe pneumatic trough, while integral throughout the eighteenth century, was modified several times to collect gases more efficiently or just to collect more gas. For example, Cavendish noted that the amount of fixed air that was given off by a reaction was not entirely present above the water; this meant that fixed water was absorbing some of this air, and could not be used quantitatively to collect that particular air. So, he replaced the water in the trough with mercury instead, in which most airs were not soluble. By doing so, he could not only collect all airs given off by a reaction, but he could also determine the solubility of airs in water, beginning a new area of research for pneumatic chemists. While this was the major adaptation of the trough in the eighteenth century, several minor changes were made before and after this substitution of mercury for water, such as adding a shelf to rest the head on while gas collection occurred. This shelf would also allow for less conventional heads to be used, such as Brownrigg's animal bladder.\n\nDuring his chemical revolution, Lavoisier created a new instrument for precisely measuring out gases. He called this instrument the \"gazomèter\" He had two different versions; the one he used in demonstrations to the Academie and to the public, which was a large expensive version meant to make people believe that it had a large precision, and the smaller, more lab practical version with a similar precision. This more practical version was cheaper to construct, allowing more chemists to use Lavoisier's instrument.\n\n"}
{"id": "76084", "url": "https://en.wikipedia.org/wiki?curid=76084", "title": "Potentiometer", "text": "Potentiometer\n\nA potentiometer is a three-terminal resistor with a sliding or rotating contact that forms an adjustable voltage divider. If only two terminals are used, one end and the wiper, it acts as a variable resistor or rheostat.\n\nThe measuring instrument called a potentiometer is essentially a voltage divider used for measuring electric potential (voltage); the component is an implementation of the same principle, hence its name.\n\nPotentiometers are commonly used to control electrical devices such as volume controls on audio equipment. Potentiometers operated by a mechanism can be used as position transducers, for example, in a joystick. Potentiometers are rarely used to directly control significant power (more than a watt), since the power dissipated in the potentiometer would be comparable to the power in the controlled load.\n\nThere are a number of terms in the electronics industry used to describe certain types of potentiometers:\n\n\nPotentiometers consist of a resistive element, a sliding contact (wiper) that moves along the element, making good electrical contact with one part of it, electrical terminals at each end of the element, a mechanism that moves the wiper from one end to the other, and a housing containing the element and wiper.\n\nSee drawing. Many inexpensive potentiometers are constructed with a resistive element \"(B)\" formed into an arc of a circle usually a little less than a full turn and a wiper \"(C)\" sliding on this element when rotated, making electrical contact. The resistive element can be flat or angled. Each end of the resistive element is connected to a terminal \"(E, G)\" on the case. The wiper is connected to a third terminal \"(F)\", usually between the other two. On panel potentiometers, the wiper is usually the center terminal of three. For single-turn potentiometers, this wiper typically travels just under one revolution around the contact. The only point of ingress for contamination is the narrow space between the shaft and the housing it rotates in.\n\nAnother type is the linear slider potentiometer, which has a wiper which slides along a linear element instead of rotating. Contamination can potentially enter anywhere along the slot the slider moves in, making effective sealing more difficult and compromising long-term reliability. An advantage of the slider potentiometer is that the slider position gives a visual indication of its setting. While the setting of a rotary potentiometer can be seen by the position of a marking on the knob, an array of sliders can give a visual impression of, for example, the effect of a multi-band equalizer (hence the term \"graphic equalizer\").\n\nThe resistive element of inexpensive potentiometers is often made of graphite. Other materials used include resistance wire, carbon particles in plastic, and a ceramic/metal mixture called cermet.\nConductive track potentiometers use conductive polymer resistor pastes that contain hard-wearing resins and polymers, solvents, and lubricant, in addition to the carbon that provides the conductive properties.\n\nMultiturn potentiometers are also operated by rotating a shaft, but by several turns rather than less than a full turn. Some multiturn potentiometers have a linear resistive element with a sliding contact moved by a lead screw; others have a helical resistive element and a wiper that turns through 10, 20, or more complete revolutions, moving along the helix as it rotates. Multiturn potentiometers, both user-accessible and preset, allow finer adjustments; rotation through the same angle changes the setting by typically a tenth as much as for a simple rotary potentiometer.\n\nA string potentiometer is a multi-turn potentiometer operated by an attached reel of wire turning against a spring, enabling it to convert linear position to a variable resistance.\n\nUser-accessible rotary potentiometers can be fitted with a switch which operates usually at the anti-clockwise extreme of rotation. Before digital electronics became the norm such a component was used to allow radio and television receivers and other equipment to be switched on at minimum volume with an audible click, then the volume increased, by turning a knob. Multiple resistance elements can be ganged together with their sliding contacts on the same shaft, for example, in stereo audio amplifiers for volume control. In other applications, such as domestic light dimmers, the normal usage pattern is best satisfied if the potentiometer remains set at its current position, so the switch is operated by a push action, alternately on and off, by axial presses of the knob.\n\nOthers are enclosed within the equipment and are intended to be adjusted to calibrate equipment during manufacture or repair, and not otherwise touched. They are usually physically much smaller than user-accessible potentiometers, and may need to be operated by a screwdriver rather than having a knob. They are usually called \"preset potentiometers\" or \"trim[ming] pots\". Some presets are accessible by a small screwdriver poked through a hole in the case to allow servicing without dismantling.\n\nThe relationship between slider position and resistance, known as the \"taper\" or \"law\", is controlled by the manufacturer. In principle any relationship is possible, but for most purposes linear or logarithmic (aka \"audio taper\") potentiometers are sufficient.\n\nA letter code may be used to identify which taper is used, but the letter code definitions are not standardized. Potentiometers made in Asia and the USA are usually marked with an \"A\" for logarithmic taper or a \"B\" for linear taper; \"C\" for the rarely seen reverse logarithmic taper. Others, particularly those from Europe, may be marked with an \"A\" for linear taper, a \"C\" or \"B\" for logarithmic taper, or an \"F\" for reverse logarithmic taper. The code used also varies between different manufacturers. When a percentage is referenced with a non-linear taper, it relates to the resistance value at the midpoint of the shaft rotation. A 10% log taper would therefore measure 10% of the total resistance at the midpoint of the rotation; i.e. 10% log taper on a 10 kOhm potentiometer would yield 1 kOhm at the midpoint. The higher the percentage, the steeper the log curve.\n\nA \"linear taper potentiometer\" (\"linear\" describes the electrical characteristic of the device, not the geometry of the resistive element) has a resistive element of constant cross-section, resulting in a device where the resistance between the contact (wiper) and one end terminal is proportional to the distance between them. Linear taper potentiometers are used when the division ratio of the potentiometer must be proportional to the angle of shaft rotation (or slider position), for example, controls used for adjusting the centering of the display on an analog cathode-ray oscilloscope. Precision potentiometers have an accurate relationship between resistance and slider position.\n\nA \"logarithmic taper potentiometer\" is a potentiometer that has a bias built into the resistive element. Basically this means the center position of the potentiometer is not one half of the total value of the potentiometer. The resistive element is designed to follow a logarithmic taper, aka a mathematical exponent or \"squared\" profile. \nA logarithmic taper potentiometer is constructed with a resistive element that either \"tapers\" in from one end to the other, or is made from a material whose resistivity varies from one end to the other. This results in a device where output voltage is a logarithmic function of the slider position.\n\nMost (cheaper) \"log\" potentiometers are not accurately logarithmic, but use two regions of different resistance (but constant resistivity) to approximate a logarithmic law. The two resistive tracks overlap at approximately 50% of the potentiometer rotation; this gives a stepwise logarithmic taper. A logarithmic potentiometer can also be simulated (not very accurately) with a linear one and an external resistor. True logarithmic potentiometers are significantly more expensive.\n\nLogarithmic taper potentiometers are often used in connection with audio amplifiers, as human perception of audio volume is logarithmic.\n\nThe most common way to vary the resistance in a circuit is to use a rheostat. The word \"rheostat\" was coined about 1845 by Sir Charles Wheatstone, from the Greek \"rheos\" meaning \"stream\", and - -\"states\" (from \"histanai\", \" to set, to cause to stand\") meaning \"setter, regulating device\", which is a two-terminal variable resistor. The term \"rheostat\" is becoming obsolete, with the general term \"potentiometer\" replacing it. For low-power applications (less than about 1 watt) a three-terminal potentiometer is often used, with one terminal unconnected or connected to the wiper.\n\nWhere the rheostat must be rated for higher power (more than about 1 watt), it may be built with a resistance wire wound around a semicircular insulator, with the wiper sliding from one turn of the wire to the next. Sometimes a rheostat is made from resistance wire wound on a heat-resisting cylinder, with the slider made from a number of metal fingers that grip lightly onto a small portion of the turns of resistance wire. The \"fingers\" can be moved along the coil of resistance wire by a sliding knob thus changing the \"tapping\" point. Wire-wound rheostats made with ratings up to several thousand watts are used in applications such as DC motor drives, electric welding controls, or in the controls for generators. The rating of the rheostat is given with the full resistance value and the allowable power dissipation is proportional to the fraction of the total device resistance in circuit.\n\nA digital potentiometer (often called digipot) is an electronic component that mimics the functions of analog potentiometers. Through digital input signals, the resistance between two terminals can be adjusted, just as in an analog potentiometer. There are two main functional types: volatile, which lose their set position if power is removed, and are usually designed to initialise at the minimum position, and non-volatile, which retain their set position using a storage mechanism similar to flash memory or EEPROM.\n\nUsage of a digipot is far more complex than that of a simple mechanical potentiometer, and there are many limitations to observe; nevertheless they are widely used, often for factory adjustment and calibration of equipment, especially where the limitations of mechanical potentiometers are problematic. A digipot is generally immune to the effects of moderate long-term mechanical vibration or environmental contamination, to the same extent as other semiconductor devices, and can be secured electronically against unauthorised tampering by protecting the access to its programming inputs by various means.\n\nIn equipment which has a microprocessor, FPGA or other functional logic which can store settings and reload them to the \"potentiometer\" every time the equipment is powered up, a multiplying DAC can be used in place of a digipot, and this can offer higher setting resolution, less drift with temperature, and more operational flexibility.\n\nA membrane potentiometer uses a conductive membrane that is deformed by a sliding element to contact a resistor voltage divider. Linearity can range from 0.50% to 5% depending on the material, design and manufacturing process. The repeat accuracy is typically between 0.1 mm and 1.0 mm with a theoretically infinite resolution. The service life of these types of potentiometers is typically 1 million to 20 million cycles depending on the materials used during manufacturing and the actuation method; contact and contactless (magnetic) methods are available (to sense position). Many different material variations are available such as PET, FR4, and Kapton. Membrane potentiometer manufacturers offer linear, rotary, and application-specific variations. The linear versions can range from 9 mm to 1000 mm in length and the rotary versions range from 0° to multiple full turns, with each having a height of 0.5 mm. Membrane potentiometers can be used for position sensing.\n\nFor touch-screen devices using resistive technology, a two-dimensional membrane potentiometer provides x and y coordinates. The top layer is thin glass spaced close to a neighboring inner layer. The underside of the top layer has a transparent conductive coating; the surface of the layer beneath it has a transparent resistive coating. A finger or stylus deforms the glass to contact the underlying layer. Edges of the resistive layer have conductive contacts.\nLocating the contact point is done by applying a voltage to opposite edges, leaving the other two edges temporarily unconnected. The voltage of the top layer provides one coordinate. Disconnecting those two edges, and applying voltage to the other two, formerly unconnected, provides the other coordinate. Alternating rapidly between pairs of edges provides frequent position updates. An analog-to digital converter provides output data.\n\nAdvantages of such sensors are that only five connections to the sensor are needed, and the associated electronics is comparatively simple. Another is that any material that depresses the top layer over a small area works well. A disadvantage is that sufficient force must be applied to make contact. Another is that the sensor requires occasional calibration to match touch location to the underlying display. (Capacitive sensors require no calibration or contact force, only proximity of a finger or other conductive object. However, they are significantly more complex.)\n\nPotentiometers are rarely used to directly control significant amounts of power (more than a watt or so). Instead they are used to adjust the level of analog signals (for example volume controls on audio equipment), and as control inputs for electronic circuits. For example, a light dimmer uses a potentiometer to control the switching of a TRIAC and so indirectly to control the brightness of lamps.\n\nPreset potentiometers are widely used throughout electronics wherever adjustments must be made during manufacturing or servicing.\n\nUser-actuated potentiometers are widely used as user controls, and may control a very wide variety of equipment functions. The widespread use of potentiometers in consumer electronics declined in the 1990s, with rotary encoders, up/down push-buttons, and other digital controls now more common. However they remain in many applications, such as volume controls and as position sensors.\n\nLow-power potentiometers, both linear and rotary, are used to control audio equipment, changing loudness, frequency attenuation, and other characteristics of audio signals.\n\nThe 'log pot' is used as the volume control in audio power amplifiers, where it is also called an \"audio taper pot\", because the amplitude response of the human ear is approximately logarithmic. It ensures that on a volume control marked 0 to 10, for example, a setting of 5 sounds subjectively half as loud as a setting of 10. There is also an \"anti-log pot\" or \"reverse audio taper\" which is simply the reverse of a logarithmic potentiometer. It is almost always used in a ganged configuration with a logarithmic potentiometer, for instance, in an audio balance control.\n\nPotentiometers used in combination with filter networks act as tone controls or equalizers.\n\nPotentiometers were formerly used to control picture brightness, contrast, and color response. A potentiometer was often used to adjust \"vertical hold\", which affected the synchronization between the receiver's internal sweep circuit (sometimes a multivibrator) and the received picture signal, along with other things such as audio-video carrier offset, tuning frequency (for push-button sets) and so on.\n\nPotentiometers can be used as position feedback devices in order to create \"closed loop\" control, such as in a servomechanism. This method of motion control used in the DC Motor is the simplest method of measuring the angle, speed and displacement.\n\nPotentiometers are also very widely used as a part of displacement transducers because of the simplicity of construction and because they can give a large output signal.\n\nIn analog computers, high precision potentiometers are used to scale intermediate results by desired constant factors, or to set initial conditions for a calculation. A motor-driven potentiometer may be used as a function generator, using a non-linear resistance card to supply approximations to trigonometric functions. For example, the shaft rotation might represent an angle, and the voltage division ratio can be made proportional to the cosine of the angle.\n\nThe potentiometer can be used as a voltage divider to obtain a manually adjustable output voltage at the slider (wiper) from a fixed input voltage applied across the two ends of the potentiometer. This is their most common use.\n\nThe voltage across can be calculated by:\n\nIf is large compared to the other resistances (like the input to an operational amplifier), the output voltage can be approximated by the simpler equation:\n\nAs an example, assume\n\nSince the load resistance is large compared to the other resistances, the output voltage will be approximately:\n\nDue to the load resistance, however, it will actually be slightly lower: .\n\nOne of the advantages of the potential divider compared to a variable resistor in series with the source is that, while variable resistors have a maximum resistance where some current will always flow, dividers are able to vary the output voltage from maximum () to ground (zero volts) as the wiper moves from one end of the potentiometer to the other. There is, however, always a small amount of contact resistance.\n\nIn addition, the load resistance is often not known and therefore simply placing a variable resistor in series with the load could have a negligible effect or an excessive effect, depending on the load.\n\n\n\n"}
{"id": "11945733", "url": "https://en.wikipedia.org/wiki?curid=11945733", "title": "Radial turbine", "text": "Radial turbine\n\nA radial turbine is a turbine in which the flow of the working fluid is radial to the shaft. The difference between axial and radial turbines consists in the way the fluid flows through the components (compressor and turbine). Whereas for an axial turbine the rotor is 'impacted' by the fluid flow, for a radial turbine, the flow is smoothly orientated perpendicular to the rotation axis, and it drives the turbine in the same way water drives a watermill. The result is less mechanical stress (and less thermal stress, in case of hot working fluids) which enables a radial turbine to be simpler, more robust, and more efficient (in a similar power range) when compared to axial turbines. When it comes to high power ranges (above 5 MW) the radial turbine is no longer competitive (due to heavy and expensive rotor) and the efficiency becomes similar to that of the axial turbines.\n\nCompared to an axial flow turbine, a radial turbine can employ a relatively higher pressure ratio (≈4) per stage with lower flow rates. Thus these machines fall in the lower specific speed and power ranges. For high temperature applications rotor blade cooling in radial stages is not as easy as in axial turbine stages. Variable angle nozzle blades can give higher stage efficiencies in a radial turbine stage even at off-design point operation. In the family of hydro-turbines, Francis turbine is a very well-known IFR turbine which generates much larger power with a relatively large impeller.\n\nThe radial and tangential components of the absolute velocity c are c and c, respectively. The relative velocity of the flow and the peripheral speed of the rotor are w and u respectively. The air angle at the rotor blade entry is given by\n\nThe stagnation state of the gas at the nozzle entry is represented by point 01. The gas expands adiabatically in the nozzles from a pressure p to p with an increase in its velocity from c to c. Since this is an energy transformation process, the stagnation enthalpy remains constant but the stagnation pressure decreases (p > p) due to losses.\nThe energy transfer accompanied by an energy transformation process occurs in the rotor.\n\nA reference velocity (c) known as the isentropic velocity, spouting velocity or stage terminal velocity is defined as that velocity which will be obtained during an isentropic expansion of the gas between the entry and exit pressures of the stage.\n\nThe total-to-static efficiency is based on this value of work.\n\nThe relative pressure or enthalpy drop in the nozzle and rotor blades are determined by the degree of reaction of the stage. This is defined by\n\nR=formula_5\n\nThe two quantities within the parentheses in the numerator may have the same or opposite signs. This, besides other factors, would also govern the value of reaction. The stage reaction decreases as C increases because this results in a large proportion of the stage enthalpy drop to occur in the nozzle ring.\n\nThe stage work is less than the isentropic stage enthalpy drop on account of aerodynamic losses in the stage. The actual output at the turbine shaft is equal to the stage work minus the losses due to rotor disc and bearing friction.\n\n\nThe blade-to-gas speed ratio can be expressed in terms of the isentropic stage terminal velocity c.\n\n    for β=90\nσ=0.707\n\nIn outward flow radial turbine stages, the flow of the gas or steam occurs from smaller to larger diameters. The stage consists of a pair of fixed and moving blades. The increasing area of cross-section at larger diameters accommodates the expanding gas.\n\nThis configuration did not become popular with the steam and gas turbines. The only one which is employed more commonly is the Ljungstrom double rotation type turbine. It consists of rings of cantilever blades projecting from two discs rotating in opposite directions. The relative peripheral velocity of blades in two adjacent rows, with respect to each other, is high. This gives a higher value of enthalpy drop per stage.\n\nIn the early 1900s, Nikola Tesla developed and patented his bladeless Tesla turbine. One of the difficulties with bladed turbines is the complex and highly precise requirements for balancing and manufacturing the bladed rotor which has to be very well balanced. The blades are subject to corrosion and cavitation. Tesla attacked this problem by substituting a series of closely spaced disks for the blades of the rotor. The working fluid flows between the disks and transfers its energy to the rotor by means of the boundary layer effect or adhesion and viscosity rather than by impulse or reaction. Tesla stated his turbine could realize incredibly high efficiencies by steam. There has been no documented evidence of Tesla turbines achieving the efficiencies Tesla claimed. They have been found to have low overall efficiencies in the roll of a turbine or pump. In recent decades there has been further research into bladeless turbine and development of patented designs that work with corrosive/abrasive and hard to pump material such as ethylene glycol, fly ash, blood, rocks, and even live fish.\n\n"}
{"id": "23843989", "url": "https://en.wikipedia.org/wiki?curid=23843989", "title": "Recycling cooperative", "text": "Recycling cooperative\n\nA recycling cooperative is an industrial cooperative, co-owned and maintained by either workers or consumers, which specializes in the recycling of various materials. Such cooperatives are either non-profit or not-for-profit; a major theoretical benefit of mass co-ownership is that raw recycled materials can become increasingly and equally distributed among the membership population at a low cost, be it for reusage at home or for reusage in the manufacturing of newer goods or versions of goods to be sold to customers at cheaper prices than would be possible with freshly obtained raw materials. A subset is the business recycling cooperative, which, according to the Northeast Recycling Council, is a group of business in a particular region, which separate recyclable waste, usually produced by their own functions, for prearranged collection by a shared hauler.\n"}
{"id": "24317585", "url": "https://en.wikipedia.org/wiki?curid=24317585", "title": "Ricardo Navarro", "text": "Ricardo Navarro\n\nRicardo Navarro is an engineer from El Salvador. He was founder and president of the environmental organization CESTA (Salvadoran Center for Appropriate Technology). He received the Goldman Environmental Prize in 1995, for his contributions to sustainable development.\n"}
{"id": "1596937", "url": "https://en.wikipedia.org/wiki?curid=1596937", "title": "Samuel Wallis", "text": "Samuel Wallis\n\nSamuel Wallis (23 April 1728 – 21 January 1795 in London) was a British naval officer and explorer of the Pacific Ocean.\n\nWallis was born at Fenteroon Farm, near Camelford, Cornwall. He served under John Byron, and in 1766 was promoted to captain and was given the command of as part of an expedition led by Philip Carteret in the with an assignment to circumnavigate the globe. The two ships were parted by a storm shortly after sailing through the Strait of Magellan, Wallis continuing to Tahiti, which he named \"King George the Third's Island\" in honour of the King (June 1767). Wallis himself was ill and remained in his cabin: lieutenant Tobias Furneaux was the first to set foot, hoisting a pennant and turning a turf, taking possession in the name of His Majesty. \n\n\"Dolphin\" stayed in Matavai Bay in Tahiti for over a month. Wallis went on to name or rename five more islands in the Society Islands and six atolls in the Tuamotu Islands, as well as confirming the locations of Rongerik and Rongelap in the Marshall Islands. He renamed the Polynesian island of Uvea as Wallis after himself, before reaching Tinian in the Mariana Islands. He continued to Batavia, where many of the crew died from dysentery, then via the Cape of Good Hope to England, arriving in May 1768. \n\nHe was able to pass on useful information to James Cook who was due to depart shortly for the Pacific, and some of the crew from the \"Dolphin\" sailed with Cook.\n\nIn 1780 Wallis was appointed Commissioner of the Admiralty.\n\n\n\n"}
{"id": "32181178", "url": "https://en.wikipedia.org/wiki?curid=32181178", "title": "Santa Cruz do Sul University Private Natural Heritage Reserve", "text": "Santa Cruz do Sul University Private Natural Heritage Reserve\n\nThe Private Reserve of Natural Heritage (RPPN) of University of Santa Cruz do Sul (Unisc) is a protected area created in 2009, through Ordinance nº 16, of March 18, having an area of 221,39 hectares, being nowadays one of the largest protected area of this category (RPPN) in Rio Grande do Sul state, Brazil. This preservation area is within the Atlantic Forest Biome and the predominant vegetation is the seasonal deciduous forest.\n\nThe Private Reserve of Natural Heritage (RPPN) of University of Santa Cruz do Sul is located in municipality of Sinimbu in Rio Grande do Sul state, Brazil, distant about 48 km north of municipality of Santa Cruz do Sul. Unfortunately it is the only RPPN present in the highlands scarp region in this state, although the creation of Preservation Areas in this region in order to protect the last primary forest fragments of this region had been suggested years ago.\n\nCreated in 2009 and having an area of 221.39 hectares, the Reserve is within an ecotone between the seasonal deciduous forest and the Araucaria moist forest. Altitude vary from 150 m eastwards, at the Pardinho river's riverbed and 650 m at hilltops. The reserve is within the Atlantic Forest Biome, having a seasonal deciduous forest predominantly secondary once the former residents used a good part of the local's natural resources by removing vegetation and livestock practice. Less impacted areas were left on hillsides due to its topographic conditions.\n\nDespite being created only in 2009, research activities began in 2006 resulting quali-quantitative publications about the local and regional fauna and flora:\n\n- Mammalian fauna of medium and large size.\n\n- Avifauna\n\n- Trees \n\nUntil 2010, 149 native species and 11 exotic species of trees were found in the area and surroundings, where, among the exotic species, five are considered invasive species and need urgent control measures. The arboreal vegetation is characterized by presenting species from the seasonal deciduous forest and from the Araucaria moist forest. The reserve is home for 12 endangered species according to the lists of Threatened Species of Rio Grande do Sul state, Brazilian Endangered Species and IUCN Red List: \"Araucaria angustifolia\" (Bertol.) Kuntze, \"Gochnatia polymorpha\" (Less.) Cabrera, \"Maytenus aquifolia\" Mart., \"Dicksonia sellowiana\" Hook, \"Albizia edwallii\" (Hoehne) Barneby & J.Grimes, \"Myrocarpus frondosus\" Allemão, \"Cedrela fissilis\" Vell., \"Myrcianthes pungens\" (O.Berg) D.Legrand, \"Podocarpus lambertii\" Klotzsch ex Endl., \"Rudgea parquioides\"(Cham.) Müll.Arg., \"Picramnia parvifolia\" Engl. and \"Picrasma crenata\" (Vell.) Engl.\nIn a survey located in a one-hectare plot in a slope in the area, 1,063 individuals belonging to 69 species were found.\n\nUntil the year 2009, 16 species of wild mammals were registered in the area, which eight are endangered: \"Alouatta guariba clamitans\", \"Cebus nigritus\", \"Eira barbara\", \"Nasua nasua\", \"Leopardus wiedii\" \"Chironectes minimus\", \"Lontra longicaudis\" and \"Cuniculus paca\". Domestic animals were also found in the area and control suggestions were proposed in order to ensure the survival and a sobrevivência and viability of local wildlife populations\n\nWere found 169 birds species, being most part forest species and 44 are endemic of Atlantic Forest. Five species are in extinction risk for the state of Rio Grande do Sul, Brazil: \"Odontophorus capueira\", \"Patagioenas cayennensis\", \"Triclaria malachitacea\", \"Grallaria varia\" and \"Amazona pretrei\" (the latter is globally threatened), among other rare or \"near threatened\" species, as well as species typically found in seasonal forests and in higher parts of the highlands showing that the area is an ecotone between the seasonal deciduous forest and the Araucaria moist forest.\n\nActivities of ecotourism and environmental education are also realized in the area, where three different tracks are available between the forest, having different difficulty levels.\nIn the years 2007, 2008 and 2009 the RPPN received special visits of students and docents from Biology and Geoecology courses from Germany universities of Tübingen and Rottenburg.\n\n"}
{"id": "23780979", "url": "https://en.wikipedia.org/wiki?curid=23780979", "title": "Sequoyah Fuels Corporation", "text": "Sequoyah Fuels Corporation\n\nSequoyah Fuels Corporation owned and operated a uranium processing plant near Gore, Oklahoma. The company was created in 1983 as a subsidiary of Kerr-McGee. In 1988 it was sold to General Atomics.\n\nThe plant is located near Gore, Oklahoma, close to the Illinois River and Interstate 40. The plant started construction in 1968 and began operation in 1970. It converted yellowcake uranium into uranium hexafluoride. In 1987 it started converting depleted uranium hexafluoride into depleted uranium tetrafluoride. The plant ceased operation in 1993.\n\nThe plant was operated under Kerr-McGee Nuclear Corporation. In 1983 KMNC split into Quivira Mining Corporation and Sequoyah Fuels Corporation. The latter was given control of the plant. In 1988 Sequoyah Fuels Corporation was sold to General Atomics.\n\nIn 2008 a company named International Isotopes said it would buy equipment and intellectual property from the Sequoyah Fuels Corp plant. The equipment would be used in a new location. It would be used for converting depleted uranium hexafluoride to depleted uranium tetrafluoride.\n\nOn January 4, 1986, Sequoyah Fuels Corporation experienced a rupture in an overfilled UF cylinder that contained an estimated 29 500 pounds of gaseous UF. The incident led to the death of a 26-year-old worker, James Harrison, of African American and Cherokee heritage, and the hospitalization of 37 of the 42 onsite workers. Health care providers examined up to 100 people, many from the community, for health effects, and 21 were hospitalized for short periods.\n\nIn 1988, Kerr McGee sold the facility to General Atomics. The \"American Journal of Public Health\" describes the plant as having \"never fully recovered\" from the accident.\n\nAnother accident involving the release of UF occurred in 1992. The plant ceased production operations in 1993 and was decommissioned.\n\n\n"}
{"id": "41241518", "url": "https://en.wikipedia.org/wiki?curid=41241518", "title": "Small stationary reformer", "text": "Small stationary reformer\n\nA small stationary reformer is an on-site device used for the production of hydrogen from hydrocarbons on a small scale.\n\nA membrane reactor is a device where oxygen separation, steam reforming and POX is combined in a single step. In 1997 Argonne National Laboratory and Amoco published a paper \"Ceramic membrane reactor for converting methane to syngas\" which resulted in different small scale systems that combined an ATR based oxygen membrane with a water-gas shift reactor and a hydrogen membrane.\n\nPartial oxidation (POX) is a type of chemical reaction. It occurs when a substoichiometric fuel-air mixture is partially combusted in a reformer, creating a hydrogen-rich syngas which can then be put to further use, for example in a fuel cell. A distinction is made between \"thermal partial oxidation\" (TPOX) and \"catalytic partial oxidation\" (CPOX).\n\n\nThe capital cost of methane reformer plants is prohibitive for small to medium size applications because the technology does not scale down well. Conventional steam reforming plants operate at pressures between 200 and 600 psi with outlet temperatures in the range of 815 to 925 °C. However, analyses have shown that even though it is more costly to construct, a well-designed SMR can produce hydrogen more cost-effectively than an ATR. To lower the costs both pressure and used temperature are lowered which allows for the use of cheaper materials.\n\n"}
{"id": "1437746", "url": "https://en.wikipedia.org/wiki?curid=1437746", "title": "South-pointing chariot", "text": "South-pointing chariot\n\nThe south-pointing chariot (or carriage) was an ancient Chinese two-wheeled vehicle that carried a movable pointer to indicate the south, no matter how the chariot turned. Usually, the pointer took the form of a doll or figure with an outstretched arm. The chariot was supposedly used as a compass for navigation and may also have had other purposes.\n\nThe ancient Chinese invented a mobile-like armored cart used in from the 5th century BC called the Dongwu Che (). It was used for the purpose of protecting warriors on the battlefield. The Chinese war wagon was designed as a kind of mobile protective cart with a shed-like roof. It would serve to be rolled up to city fortifications to provide protection for sappers digging underneath to weaken a wall's foundation. The early Chinese war wagon became the basis of technologies for the making of ancient Chinese south-pointing chariots.\n\nThere are legends of earlier south-pointing chariots, but the first reliably documented one was created by the Chinese mechanical engineer Ma Jun (c. 200–265 CE) of Cao Wei during the Three Kingdoms. No ancient chariots still exist, but many extant ancient Chinese texts mention them, saying they were used intermittently until about 1300 CE. Some include information about their inner components and workings.\n\nThere were probably several types of south-pointing chariot which worked differently. In most or all of them, the rotating road wheels mechanically operated a geared mechanism to keep the pointer aimed correctly. The mechanism had no magnets and did not automatically detect which direction was south. The pointer was aimed southward by hand at the start of a journey. Subsequently, whenever the chariot turned, the mechanism rotated the pointer relative to the body of the chariot to counteract the turn and keep the pointer aiming in a constant direction, to the south. Thus the mechanism did a kind of directional dead reckoning, which is inherently prone to cumulative errors and uncertainties. Some chariots' mechanisms may have had differential gears. If so, it was probably the first use of differentials anywhere in the world.\n\nThe south-pointing chariot, a mechanical-geared, wheeled vehicle used to discern the southern cardinal direction (without magnetics), was given a brief description by Ma's contemporary Fu Xuan. The contemporary 3rd century CE source of the \"Weilüe\", written by the East Han Dynasty politician Yuan Huan also described the south-pointing chariot of belonging to the Chinese mechanical engineer and politician Ma Jun. The Jin Dynasty (265–420 CE) era text of the \"Shu Zheng Ji\" (Records of Military Expeditions), written by Guo Yuansheng, recorded that south-pointing chariots were often stored in the northern gatehouse of the Government Workshops (Shang Fang) of the capital city. However, the later written \"Song Shu\" (\"Book of Song\") (6th century CE) recorded the south-pointing chariot's design and use in further detail, as well as creating the background legend of the device's (supposed) use long before Ma's time, in the Western Zhou Dynasty (1050–771 BCE). The book also provided a description of the south-pointing chariot's re-invention and use in times after Ma Jun and the Three Kingdoms. The 6th century CE text, translated by the British scientist and historian Joseph Needham, reads as follows (the south-pointing chariot is referred to as the south-pointing carriage):\n\nThe last sentence of the passage is of great interest for navigation at sea, since the magnetic compass used for seafaring navigation was not used until the time of Shen Kuo (1031–1095). Although the \"Song Shu\" text describes earlier precedents of the south-pointing chariot before the time of Ma Jun, this is not entirely credible, as there are no pre-Han or Han Dynasty era texts that describe the device. In fact, the first known source to describe stories of its legendary use during the Zhou period was the \"Gu Jin Zhu\" book of Cui Bao (c. 300 CE), written soon after the Three Kingdoms era. Cui Bao also wrote that the intricate details of construction for the device were once written in the \"Shang Fang Gu Shi\" (\"Traditions of the Imperial Workshops\"), but the book was lost by his time.\n\nThe invention of the south-pointing chariot also made its way to Japan by the 7th century. The \"Nihon Shoki\" (The Chronicles of Japan) of 720 CE described the earlier Chinese Buddhist monks Zhi Yu and Zhi You constructing several south-pointing Chariots for Emperor Tenji of Japan in 658 CE. This was followed up by several more chariot devices built in 666 CE as well.\n\nThe south-pointing chariot was also combined with the earlier Han Dynasty era invention of the odometer (also Greco-Roman), a mechanical device used to measure distance traveled, and found in all modern automobiles. It was mentioned in the Song Dynasty (960–1279 CE) historical text of the \"Song Shi\" (compiled in 1345) that the engineers Yan Su (in 1027 CE) and Wu Deren (in 1107 CE) both created south-pointing chariots, which it details as follows. (In Needham's translation, inches and feet (ft) are used as units of distance. 1 inch is 25.4 millimetres. 1 ft is 12 inches or 304.8 mm.)\n\nAfter this initial description of Yan Su's device, the text continues to describe the work of Wu Deren, who crafted a wheeled device that would combine the odometer and south-pointing chariot:\n\nThere is a widely believed hypothesis that most, if not all, south-pointing chariots worked by means of differential gears. A differential is an assembly of gears, nowadays used in almost all automobiles except some electric and hybrid-electric ones, which has three shafts linking it to the external world. They are conveniently labelled A, B, and C. The gears cause the rotation speed of Shaft A to be proportional to the \"sum\" of the rotation speeds of Shafts B and C. There are no other limitations on the rotation speeds of the shafts.\n\nIn an automobile, Shaft A is connected to the engine (through the transmission), and Shafts B and C are connected to two road wheels, one on each side of the vehicle. When the vehicle turns, the wheel going around the outside of the turning curve has to roll further and rotate faster than the wheel on the inside. The differential permits this to happen while both wheels are being driven by the engine. If the sum of the speeds of the wheels is constant, the speed of the engine does not change.\n\nIn a south-pointing chariot, according to the hypothesis, Shaft B was connected to one road wheel and Shaft C was connected \"through a direction-reversing gear\" to the other road wheel. This made Shaft A rotate at a speed that was proportional to the \"difference\" between the rotation speeds of the two wheels. The pointing doll was connected (possibly through intermediate gears) to Shaft A. When the chariot moved in a straight line, the two wheels turned at equal speeds, and the doll did not rotate. When the chariot turned, the wheels rotated at different speeds (for the same reason as in an automobile), so the differential caused the doll to rotate, compensating for the turning of the chariot.\n\nThe hypothesis that there were south-pointing chariots with differential gears originated in the 20th century. People who were familiar with modern (e.g. automotive) uses of differentials interpreted some of the ancient Chinese descriptions in ways that agreed with their own ideas. Essentially, they re-invented the south-pointing chariot, as it had previously been re-invented several times in antiquity. Working chariots that use differentials have been constructed in recent decades. Whether any such chariots existed previously is not known with certainty.\n\nAlthough the Antikythera mechanism is believed to have used differential gears, the first true differential gear definitely known to have been used was by Joseph Williamson in 1720. He used a differential for correcting the equation of time for a clock that displayed both mean and solar time.\n\nIf the south-pointing chariot were built perfectly accurately, using a differential gear, and if it travelled on an Earth that was perfectly smooth, it would have interesting properties. It would be a mechanical compass that transports a direction, given by the pointer, along the path it travels. Mathematically the device performs parallel transport along the path it travels.\n\nThe chariot can be used to detect straight lines or geodesics. A path on a surface the chariot travels along is a geodesic if and only if the pointer does not rotate with respect to the base of the chariot.\n\nBecause of the curvature of the Earth's surface (due to it being curved around as a globe), the chariot would generally not continue to point due south as it moves. For example, if the chariot moves along a geodesic (as approximated by any great circle) the pointer should instead stay at a fixed angle to the path. Also, if two chariots travel by different routes between the same starting and finishing points, their pointers, which were aimed in the same direction at the start, usually do not point in the same direction at the finish. Likewise, if a chariot goes around a closed loop, starting and finishing at the same point on the Earth's surface, its pointer generally does not aim in the same direction at the finish as it did at the start. The difference is the holonomy of the path, and is proportional to the enclosed area. If the journeys are short compared with the radius of the Earth, these discrepancies are small and may have no practical importance. Nevertheless, they show that this type of chariot, based on differential gears, would be an imperfect compass even if constructed exactly and used in ideal conditions.\n\nReal machines are never built perfectly accurately. Simple geometry shows that if the chariot's mechanism is based on a differential gear and if, for example, the width of the track of the chariot (the separation between its wheels) is three metres, and if the wheels are intended to be identical but actually differ in diameter by one part in a thousand, then if the chariot travels one kilometre in a straight line, the \"south-pointing\" figure will rotate nearly twenty degrees. If it initially points exactly to the south, at the end of the one-kilometre trip it will point almost to the south-southeast or south-southwest, depending on which wheel is the larger. If the chariot travels nine kilometres, the figure will end up pointing almost due north. Obviously, this would make it useless as a south-pointing compass. To be a useful navigational tool, the figure would have to rotate no more than a couple of degrees over a journey of a hundred kilometres, but this would require the chariot's wheels to be equal in diameter to within one part in a million. Even if the process of manufacturing the wheels were capable of this precision (which would not be possible with ancient Chinese methods), it is doubtful that the equality of the wheels could be maintained for long as they are subjected to the wear and tear of travelling across open country. Irregularity of the ground would add further errors to the device's functioning.\n\nConsiderable scepticism is therefore warranted as to whether this type of south-pointing chariot, using a differential gear for the whole time, was used in practice to navigate over long distances. Conceivably, the south-pointing doll was fixed to the body of the chariot while it was travelling in straight lines, and coupled to the differential only when the chariot was turning. The charioteer could have operated a control to do this just before and after making each turn, or maybe shouted commands to someone inside the chariot who connected and disconnected the doll and the differential. This could have been done without stopping the chariot. If turns were brief and rare, this would have greatly reduced the pointing errors, since they would have accumulated only during the short periods when the doll and differential were connected. However, it raises the problem of how the chariot could have been kept travelling in straight lines with sufficient accuracy without using the pointing doll.\n\nIf the real purposes of the chariot and the accounts of it were amusement and impressing visiting foreigners, rather than actual long-distance navigation, then its inaccuracy might not have been important. Considering that a large mechanical wagon or chariot would be obligated to travel on roads, the destination in question would typically not be in an unknown direction. The fact that the sources cited above mention that the chariot was placed at the front of processions, its high level of mechanical complexity and fragility, and that it was 'reinvented' several times contribute to the conclusion that it was not used for navigation, as a truly practical and useful navigational tool would not be forgotten or left unused.\n\nAlthough the hypothesis that the south-pointing chariot used differential gears has gained wide acceptance, it should be recognized that functional south-pointing chariots without differential gears are possible. The ancient descriptions are often unclear, but they suggest that the Chinese implemented several different designs, at least some of which did not include differentials.\n\nSome of the ancient descriptions suggest that some south-pointing chariots could move in only three ways: straight ahead, or turning left or right with a fixed radius of curvature. A third wheel might have been used to fix the turning radius. If the chariot was turning, the pointing doll was connected by gears to one or other of the two main road wheels (e.g. whichever was on the outside of the curve around which the chariot was moving) so the doll rotated at a fixed speed, relative to the rate of the chariot's movement, to compensate for the predetermined rate of turn. The doll turned in opposite directions depending on which road wheel was connected to it, so its rotation compensated for the chariot turning left or right. This design would have been simpler than using a differential gear.\n\nThe chariots of Yan Su and Wu Deren appear to have used this type of mechanism. (See descriptions quoted from the \"Song Shi\", above.) Apart from the presence of components in Wu Deren's vehicle to make it function as an odometer, there were only minor differences between them. In each chariot, the two main road wheels were attached to vertical gear wheels. A large horizontal gear wheel was linked (possibly via intermediate gearing) to the pointing doll, and was positioned so a diameter almost spanned the space between the uppermost points of the vertical gear wheels. When the chariot was moving straight ahead, there was no connection between these gears, but when the chariot turned, a small gear wheel was lowered into contact with the horizontal gear and one of the vertical gears, thus linking the doll to one of the road wheels. Two small gear wheels were available, one to connect the horizontal gear to each of the vertical ones. Of course, they were not used simultaneously. The small gear wheels were raised and lowered by an arrangement of weights, pulleys and cords which were attached to the pole to which the horses that pulled the chariot were harnessed. When the horses moved to one side or the other, in order to turn the chariot, the pole moved and the cords lowered the appropriate small gear wheel into its working position. When the horses returned to walking straight ahead, the small gear wheel was raised out of contact with the main horizontal and vertical gears. Thus the system functioned automatically. The mirror-symmetry of the vertical gears being linked by the small gears to the horizontal gear at diametrically opposite points caused the horizontal gear to rotate in opposite directions depending on which road wheel was linked to it, thus rotating the pointing doll in opposite directions when the chariot turned left and right.\n\nThe description does not mention a third road wheel to fix the turning radius, but it is possible that such a wheel was present. No gears would have been attached to it, so perhaps the author of the description did not mention it because he did not realize that it was an important part of the mechanism. Putting such a wheel on the chariot and making it function properly would not have been difficult. It might have been attached to the pole to which the horses were harnessed. Stops would have been provided to limit the motions of the pole to left and right.\n\nIf a third road wheel was included, this type of south-pointing chariot could have worked quite accurately as a compass when used for short journeys under good conditions, but if used for long journeys it would have been subject to cumulative errors, like chariots using the differential mechanism.\n\nIf in fact there was no third road wheel, the chariot might have functioned as a compass if turns were always made so that one of the two wheels was stationary and only the other rotated, with the pointing doll connected to it by gears. The charioteer could have kept the stationary wheel from turning by controlling the horses appropriately. (A brake would have helped, but there is no mention of one in the description.) The radius of the curve around which the rotating wheel moved would have equalled the track-width of the chariot, and the gears turning the doll would have been chosen accordingly. This design would have worked as a compass for short journeys, but would have suffered from cumulative errors if used for long ones. Also, the chariot would have been slow and awkward to turn. This might not have mattered if turns were rarely executed.\n\nThe \"Song Shi\" description of the gears in Yan Su's chariot, and the numbers of teeth on them, suggests that it worked this way, without a third road wheel. The gear ratios would have been correct if the pointing doll was attached directly to the large horizontal gear wheel, and the track-width of the chariot equalled the diameter of the road wheels. Wu Deren's chariot also appears to have been designed to work this way. The width of the chariot, and therefore presumably the track-width, was greater than the diameter of the wheels. The gear ratios were appropriate for these dimensions.\n\nThe charioteer would have had to use great skill to ensure that the radius of each turn of the chariot was correct to make one of the wheels exactly stop rotating. Unless he did this correctly, the pointing doll would not have kept aiming to the south. He would have been able to adjust the direction in which it aimed by making turns that were more or less sharp. This would sometimes have given him opportunities to use the chariot dishonestly. If it was being demonstrated to spectators, for example, and was being driven around in front of them, making many turns, the charioteer, who would have known which way was south, would have been able to make the chariot \"appear to\" work extremely accurately as a compass for long periods. The spectators could have been shown the machinery, and would have seen that the charioteer could not manipulate the doll. They would presumably have been impressed by the apparent accuracy of the mechanism. It is possible that this type of chariot was sometimes constructed with the prime purpose of fraudulently impressing spectators. Possibly, people who built these chariots deceived their own employers with them, which could have gained them fame and fortune provided nobody tried using the chariots for real navigation.\n\nOther mechanical designs for the south-pointing chariot are also possible, including ones that employ a device that is used today, the gyrocompass. However, there is no indication that the ancient Chinese knew of these.\n\nSome south-pointing chariots may not have been purely mechanical devices. Someone riding inside the chariot may have used some non-mechanical method of determining the compass directions, and turned the doll on top of the chariot accordingly. There are several methods that could have been used, for example:\n\n\nUnlike mechanisms that rely on the rotation of road wheels, most of these methods can be used at sea. This may account for the mention (see \"Earliest sources\" above) that a marine version of the south-pointing chariot existed.\n\nThese methods can work accurately over long distances, unlike the mechanical designs for the chariot.\n\nSome non-mechanical method of finding the south must have been used when a mechanical south-pointing chariot was initialized, aiming its pointer to the south at the start of a journey. Any of the methods mentioned above in \"Non-mechanical possibilities\" could have been used.\n\nIf any south-pointing chariot was really used for long-distance navigation, it must have relied (after initialization) on a non-mechanical direction-finding method. It might have been operated non-mechanically by someone riding in it, as outlined above. Alternatively, if it had a mechanical mechanism, it must have been frequently re-initialized non-mechanically to eliminate accumulated errors and uncertainties.\n\nThe only chariots that might not have needed non-mechanical methods of finding the south would have been those that were never used for long-distance navigation. If some chariots were used only for amusement or fraud, they could have worked purely mechanically. Even initialization could have been avoided by simply declaring some arbitrary direction to be \"south\".\n\nWhile none of the historic south-pointing chariots remain, full sized replicas can be found.\n\nThe History Museum in Beijing, China, holds a replica based on the mechanism of Yen Su (1027).\nThe National Palace Museum in Taipei, Taiwan, holds a replica based on the Lanchester mechanism of 1932.\n\nReferred to as the \"southern pointing man\", two replicas can also be seen (and physically experimented with) at the Ontario Science Centre in Toronto, Canada.\n\n\n\n"}
{"id": "8826501", "url": "https://en.wikipedia.org/wiki?curid=8826501", "title": "Southwest Windpower", "text": "Southwest Windpower\n\nSouthwest Windpower (SWWP) is a wind turbine manufacturer established in 1987 based in Flagstaff, Arizona, United States. The company specializes in small, reliable battery charging wind generators that complement photovoltaics (solar energy or PV) in supplying energy to rural areas.\n\nSouthwest Windpower closed for business in the first quarter of 2013.\n\nThe remaining Skystream assets were acquired by Xzeres wind in July 2013. Xzeres CEO Frank Greco had been the CEO of Southwest Windpower.\n\nIn 2000, SWWP became the first wind turbine manufacturer to receive Underwriters Laboratories (UL) and International Electrotechnical Commission (IEC) certification, for its AIR series wind turbines. Southwest Windpower’s latest turbine, the Skystream 3.7, has a rotor diameter of about and can generate up to half an average U.S. home's energy in optimal winds. It is the first residential-scale wind generator designed specifically for the grid-connected home or small business. \n\nSkystream is the first device to be spun out of the U.S. Department of Energy's Wind Energy Program. That program includes the National Renewable Energy Laboratory Wind Technology Center in Boulder CO. Andrew Kruse estimates that one Skystream will produce about 100 MWh of energy over its 20-year design life. At a typical total installed cost of $15,000, that gives an average energy cost of 9 cents per kilowatthour.\n\nIn 2011 the company received a Statement of Compliance from Germanischer Lloyd which confirms that Skystream 3.7 meets the requirements of the IEC Standard 61400-2:2006, Edition 2, \"Design requirements for Small wind turbines,\" which addresses safety philosophy, quality assurance and engineering integrity of the turbine.\n\nThe Small Wind Certification Council (SWCC) issued a consumer label and certification in 2011 that validated manufacturer claims for Skystream 3.7 performance, reliability and safety were accurate.\nA Skystream 3.7 has been tested as an airborne wind turbine in an Altaeros helium-filled balloon.\n\n"}
{"id": "18525884", "url": "https://en.wikipedia.org/wiki?curid=18525884", "title": "Stave (wood)", "text": "Stave (wood)\n\nA stave is a narrow length of wood with a slightly bevelled edge to form the sides of barrels, tanks and pipelines, originally handmade by coopers. They have been used in the construction of large holding tanks and penstocks at hydro power developments.\nThey are also used in the construction of certain musical instruments with rounded bodies or backs.\n\n"}
{"id": "449744", "url": "https://en.wikipedia.org/wiki?curid=449744", "title": "Storm surge", "text": "Storm surge\n\nA storm surge, storm flood or storm tide is a coastal flood or tsunami-like phenomenon of rising water commonly associated with low pressure weather systems (such as tropical cyclones and strong extratropical cyclones), the severity of which is affected by the shallowness and orientation of the water body relative to storm path, as well as the timing of tides. Most casualties during tropical cyclones occur as the result of storm surges. It is a measure of the rise of water beyond what would be expected by the normal movement related to tides.\n\nThe two main meteorological factors contributing to a storm surge are a long fetch of winds spiraling inward toward the storm, and a low-pressure-induced dome of water drawn up under and trailing the storm's center.\n\nThe deadliest storm surge on record was the 1970 Bhola cyclone, which killed up to 500,000 people in the area of the Bay of Bengal. The low-lying coast of the Bay of Bengal is particularly vulnerable to surges caused by tropical cyclones. The deadliest storm surge in the twenty-first century was caused by the Cyclone Nargis, which killed more than 138,000 people in Myanmar in May 2008. The next deadliest in this century was caused by the Typhoon Haiyan (Yolanda), which killed more than 6,000 people in the central Philippines in 2013 and resulted in economic losses estimated at $14 billion (USD).\n\nThe Galveston Hurricane of 1900, a Category 4 hurricane that struck Galveston, Texas, drove a devastating surge ashore; between 6,000 and 12,000 lives were lost, making it the deadliest natural disaster ever to strike the United States.\n\nThe highest storm tide noted in historical accounts was produced by the 1899 Cyclone Mahina, estimated at almost 44 ft (13 metres) at Bathurst Bay, Australia, but research published in 2000 concluded that the majority of this likely was wave run-up because of the steep coastal topography. In the United States, one of the greatest recorded storm surges was generated by Hurricane Katrina in 2005, which produced a maximum storm surge of more than 25 ft (8 metres) in southern Mississippi, with a storm surge height of 27.8 ft (8.5 m) in Pass Christian. Another record storm surge occurred in this same area from Hurricane Camille in 1969, with a storm tide of 24.6 ft (7.5 m), also at Pass Christian. A storm surge of 14 ft (4.2 m) occurred in New York City during Hurricane Sandy in October 2012.\n\nAt least five processes can be involved in altering tide levels during storms: \n\nThe pressure effects of a tropical cyclone will cause the water level in the open ocean to rise in regions of low atmospheric pressure and fall in regions of high atmospheric pressure. The rising water level will counteract the low atmospheric pressure such that the total pressure at some plane beneath the water surface remains constant. This effect is estimated at a increase in sea level for every millibar (hPa) drop in atmospheric pressure.\n\nStrong surface winds cause surface currents at a 45° angle to the wind direction, by an effect known as the Ekman Spiral. Wind stresses cause a phenomenon referred to as \"wind set-up\", which is the tendency for water levels to increase at the downwind shore and to decrease at the upwind shore. Intuitively, this is caused by the storm blowing the water toward one side of the basin in the direction of its winds. Because the Ekman Spiral effects spread vertically through the water, the effect is proportional to depth. The pressure effect and the wind set-up on an open coast will be driven into bays in the same way as the astronomical tide.\n\nThe Earth's rotation causes the Coriolis effect, which bends currents to the right in the Northern Hemisphere and to the left in the Southern Hemisphere. When this bend brings the currents into more perpendicular contact with the shore, it can amplify the surge, and when it bends the current away from the shore it has the effect of lessening the surge.\n\nThe effect of waves, while directly powered by the wind, is distinct from a storm's wind-powered currents. Powerful wind whips up large, strong waves in the direction of its movement. Although these surface waves are responsible for very little water transport in open water, they may be responsible for significant transport near the shore. When waves are breaking on a line more or less parallel to the beach, they carry considerable water shoreward. As they break, the water particles moving toward the shore have considerable momentum and may run up a sloping beach to an elevation above the mean water line, which may exceed twice the wave height before breaking.\n\nThe rainfall effect is experienced predominantly in estuaries. Hurricanes may dump as much as of rainfall in 24 hours over large areas and higher rainfall densities in localized areas. As a result, surface runoff can quickly flood Streams and rivers. This can increase the water level near the head of tidal estuaries as storm-driven waters surging in from the ocean meet rainfall flowing downstream into the estuary.\n\nIn addition to the above processes, surge and wave heights on shore are also affected by the flow of water over the underlying topography, i.e. the configuration and bathymetry of the ocean bottom and affected coastal area. A narrow shelf, for example, or one that has a steep drop from the shoreline and subsequently produces deep water in proximity to the shoreline, tends to produce a lower surge but a higher and more powerful wave. This situation is well exemplified by the southeast coast of Florida. The edge of the Floridian Plateau, where the water depths reach , lies just offshore of Palm Beach; just offshore, the depth increases to over . The depth contour followed southward from Palm Beach County lies more than to the east of the Florida Keys.\n\nConversely, coastlines along North America such as those along the Gulf of Mexico coast from Texas to Florida, and Asia such as the Bay of Bengal, have long, gently sloping shelves and shallow water depths. On the Gulf side of Florida, the edge of the Floridian Plateau lies more than offshore of Marco Island in Collier County. Florida Bay, lying between the Florida Keys and the mainland, is also very shallow; depths typically vary between and . These areas are subject to higher storm surges with smaller waves. This difference is because in deeper water, a surge can be dispersed down and away from the hurricane. However, upon entering a shallow, gently sloping shelf, the surge cannot be disperse, but is driven ashore by the wind stresses of the hurricane. Topography of the land surface is another important element in storm surge extent. Areas where the land lies less than a few meters above sea level are at particular risk from storm surge inundation.\n\nFor a given topography and bathymetry the surge height is not solely affected by peak wind speed; the size of the storm also affects the peak surge. With any storm, the area of piled up water can flow out of the storm perimeter, and this escape mechanism is reduced in proportion to the surge force (for the same peak wind speed) when the storm covers more area (storm perimeter length per area is inversely proportional to a circular storm's diameter).\nSimilar to tropical cyclones, extra-tropical storms cause an offshore rise of water. However, unlike most tropical cyclone storm surges, extra-tropical storms can cause higher water levels across a large area for longer periods of time, depending on the system.\n\nIn North America, extra-tropical storm surges may occur on the Pacific and Alaska coasts, and north of 31°N on the Atlantic Coast. Coasts with sea ice may experience an \"ice tsunami\" causing significant damage inland. Extra-tropical storm surges may be possible further south for the Gulf coast mostly during the wintertime, when extra-tropical cyclones affect the coast, such as in the 1993 Storm of the Century.\n\nNovember 9–13, 2009, marked a significant extratropical storm surge event on the United States east coast when the remnants of Hurricane Ida developed into a Nor'easter off the southeast U.S. coast. During the event, winds from the east were present along the northern periphery of the low pressure center for a number of days, forcing water into locations such as Chesapeake Bay. Water levels rose significantly and remained as high as above normal in numerous locations throughout the Chesapeake for a number of days as water was continually built-up inside the estuary from the onshore winds and freshwater rains flowing into the bay. In many locations, water levels were shy of records by only .\n\nSurge can be measured directly at coastal tidal stations as the difference between the forecast tide and the observed rise of water. Another method of measuring surge is by the deployment of pressure transducers along the coastline just ahead of an approaching tropical cyclone. This was first tested for Hurricane Rita in 2005. These types of sensors can be placed in locations that will be submerged and can accurately measure the height of water above them.\n\nAfter surge from a cyclone has receded, teams of surveyors map high-water marks (HWM) on land, in a rigorous and detailed process that includes photographs and written descriptions of the marks. HWMs denote the location and elevation of flood waters from a storm event. When HWMs are analyzed, if the various components of the water height can be broken out so that the portion attributable to surge can be identified, then that mark can be classified as storm surge. Otherwise, it is classified as storm tide. HWMs on land are referenced to a vertical datum (a reference coordinate system). During evaluation, HWMs are divided into four categories based on the confidence in the mark; only HWMs evaluated as \"excellent\" are used by National Hurricane Center in post-storm analysis of the surge.\n\nTwo different measures are used for storm tide and storm surge measurements. Storm tide is measured using a geodetic vertical datum (NGVD 29 or NAVD 88). Since storm surge is defined as the rise of water beyond what would be expected by the normal movement caused by tides, storm surge is measured using tidal predictions, with the assumption that the tide prediction is well-known and only slowly varying in the region subject to the surge. Since tides are a localized phenomenon, storm surge can only be measured in relationship to a nearby tidal station. Tidal bench mark information at a station provides a translation from the geodetic vertical datum to mean sea level (MSL) at that location, then subtracting the tidal prediction yields a surge height above the normal water height.\n\nThe National Hurricane Center forecasts storm surge using the SLOSH model, which is an abbreviation for Sea, Lake and Overland Surges from Hurricanes. The model is accurate to within 20 percent. SLOSH inputs include the central pressure of a tropical cyclone, storm size, the cyclone's forward motion, its track, and maximum sustained winds. Local topography, bay and river orientation, depth of the sea bottom, astronomical tides, as well as other physical features, are taken into account in a predefined grid referred to as a SLOSH basin. Overlapping SLOSH basins are defined for the southern and eastern coastline of the continental U.S. Some storm simulations use more than one SLOSH basin; for instance, Hurricane Katrina SLOSH model runs used both the Lake Ponchartrain / New Orleans basin, and the Mississippi Sound basin, for the northern Gulf of Mexico landfall. The final output from the model run will display the maximum envelope of water, or MEOW, that occurred at each location.\n\nTo allow for track or forecast uncertainties, usually several model runs with varying input parameters are generated to create a map of MOMs, or Maximum of Maximums. For hurricane evacuation studies, a family of storms with representative tracks for the region, and varying intensity, eye diameter, and speed, are modeled to produce worst-case water heights for any tropical cyclone occurrence. The results of these studies are typically generated from several thousand SLOSH runs. These studies have been completed by the United States Army Corps of Engineers, under contract to the Federal Emergency Management Agency, for several states and are available on their Hurricane Evacuation Studies (HES) website. They include coastal county maps, shaded to identify the minimum category of hurricane that will result in flooding, in each area of the county.\n\nAlthough meteorological surveys alert about hurricanes or severe storms, in the areas where the risk of coastal flooding is particularly high, there are specific storm surge warnings. These have been implemented, for instance, in the Netherlands, Spain, the United States, and the United Kingdom.\n\nA prophylactic method introduced after the North Sea Flood of 1953 is the construction of dams and storm-surge barriers (flood barriers). They are open and allow free passage, but close when the land is under threat of a storm surge. Major storm surge barriers are the Oosterscheldekering and Maeslantkering in the Netherlands, which are part of the Delta Works project; the Thames Barrier protecting London; and the Saint Petersburg Dam in Russia.\n\nAnother modern development (in use in the Netherlands) is the creation of housing communities at the edges of wetlands with floating structures, restrained in position by vertical pylons. Such wetlands can then be used to accommodate runoff and surges without causing damage to the structures while also protecting conventional structures at somewhat higher low-lying elevations, provided that dikes prevent major surge intrusion.\n\nFor mainland areas, storm surge is more of a threat when the storm strikes land from seaward, rather than approaching from landward.\n\nWater can also be sucked away from shore prior to a storm surge. This was the case on the western Florida coast in 2017, just before Hurricane Irma made landfall, uncovering land usually underwater. This phenomenon is known as a reverse storm surge, or a negative storm surge.\n\n\n"}
{"id": "28349922", "url": "https://en.wikipedia.org/wiki?curid=28349922", "title": "Tight gas", "text": "Tight gas\n\nTight gas is natural gas produced from reservoir rocks with such low permeability that massive hydraulic fracturing is necessary to produce the well at economic rates. Tight gas reservoirs are generally defined as having less than 0.1 millidarcy (mD) matrix permeability and less than ten percent matrix porosity. Although shales have low permeability and low effective porosity, shale gas is usually considered separate from tight gas, which is contained most commonly in sandstone, but sometimes in limestone. Tight gas is considered an unconventional source of natural gas.\n\nRock with permeabilities as little as one nanodarcy, reservoir stimulation may be economically productive with optimized spacing and completion of staged fractures to maximize yield with respect to cost.\n\nSome examples of tight gas reservoirs are:\n\n\n"}
{"id": "30008912", "url": "https://en.wikipedia.org/wiki?curid=30008912", "title": "Tridecylic acid", "text": "Tridecylic acid\n\nTridecylic acid, or tridecanoic acid, is a 13-carbon saturated fatty acid with the chemical formula CH(CH)COOH. It is commonly found in dairy products.\n\n"}
{"id": "17823510", "url": "https://en.wikipedia.org/wiki?curid=17823510", "title": "True vertical depth", "text": "True vertical depth\n\nTrue vertical depth is the measurement of a straight line perpendicularly downwards from a horizontal plane.\n\nIn the petroleum industry, true vertical depth, abbreviated as TVD, is the measurement from the surface to the bottom of the borehole (or anywhere along its length) in a straight perpendicular line represented by line (a) in the image.\n\nLine (b) is the actual borehole and its length would be considered the measured depth in oilfield terminology. The TVD is \"always\" equal to or less than (≤) the \"measured depth\". If you imagine line (b) were a piece of string and pull it straight down, you would see that it would be longer than line (a). This example oilwell would be considered a directional well because it deviates from a straight vertical line.\n\nDepth in a well\n"}
{"id": "4228371", "url": "https://en.wikipedia.org/wiki?curid=4228371", "title": "Vertical wind tunnel", "text": "Vertical wind tunnel\n\nA vertical wind tunnel (VWT) is a wind tunnel which moves air up in a vertical column. Unlike standard wind tunnels which have test sections that are oriented horizontally, as experienced in level flight, a vertical orientation enables gravity to be countered by drag instead of lift, as experienced in an aircraft spin or by a skydiver at terminal velocity.\n\nAlthough vertical wind tunnels have been built for aerodynamic research, the most high-profile are those used as recreational wind tunnels, frequently advertised as indoor skydiving or bodyflight, which have also become a popular training tool for skydivers.\n\nA recreational wind tunnel enables human beings to experience the sensation of flight without planes or parachutes, through the force of wind being generated vertically. Air moves upwards at approximately 195 km/h (120 mph or 55 m/s), the terminal velocity of a falling human body belly-downwards. A vertical wind tunnel is frequently called 'indoor skydiving' due to the popularity of vertical wind tunnels among skydivers, who report that the sensation is extremely similar to skydiving. The human body 'floats' in midair in a vertical wind tunnel, replicating the physics of 'body flight' or 'bodyflight' experienced during freefall.\n\nThe first human to fly in a vertical wind tunnel was Jack Tiffany in 1964 at Wright-Patterson Air Force Base located in Greene and Montgomery County, Ohio.\n\nIn 1982 Jean St-Germain, an inventor from Drummondville, Quebec, sold a vertical wind tunnel concept to both Les Thompson and Marvin Kratter, both of whom went on to build their own wind tunnels. Soon after, St Germain sold the franchising rights to Kratter for $1.5 million. Originally known as the \"Aérodium\", it was patented as the \"Levitationarium\" by Jean St. Germain in the USA in 1984 and 1994 under Patent Nos. 4,457,509 and 5,318,481, respectively.\nSt. Germain then helped build two wind tunnels in America. The first vertical wind tunnel, built solely for a commercial use, opened in the summer of 1982 in Las Vegas, Nevada. Later that same year, a second wind tunnel opened in Pigeon Forge, Tennessee. Both facilities opened and operated under the name of Flyaway Indoor Skydiving. In 2005 the 15-year Flyaway Manager Keith Fields purchased the Las Vegas facility and later renamed it \"Vegas Indoor Skydiving\".\n\nThe first reference, in print, to a Vertical Wind Tunnel specifically for parachuting was published in CANPARA (the Canadian Sport Parachuting Magazine) in 1979.\n\nA milestone in vertical wind tunnel history was 'Wind Machine' at the closing ceremonies of the 2006 Torino Winter Olympics. This was a custom-built unit by Aerodium (Latvia/Canada) for the closing ceremony. Many people had never seen a vertical wind tunnel before, and were fascinated by the flying humans with no wires.\n\nA vertical wind tunnel performance in Moscow's Red Square was shown in 2009 during the presentation of logotype of Sochi 2014 Winter Olympics. In 2010, a vertical wind tunnel was shown at the Latvian exhibition of Expo 2010 in Shanghai, China.\n\nOutdoor vertical wind tunnels can either be portable or stationary. Portable vertical wind tunnels are often used in movies and demonstrations, and are often rented for large events such as conventions and state fairs. Portable units offer a dramatic effect for the flying person and the spectators, because there are no walls around the flight area. These vertical wind tunnels allow people to fly with a full or partial outdoor/sky view. Outdoor vertical wind tunnels may also have walls or netting around the wind column, to keep beginner tunnel flyers from falling out of the tunnel.\n\nStationary indoor vertical wind tunnels include recirculating and non-recirculating types. Non-recirculating vertical wind tunnels usually suck air through inlets near the bottom of the building, through the bodyflight area, and exhaust through the top of the building. Recirculating wind tunnels form an aerodynamic loop with turning vanes, similar to a scientific wind tunnel, but using a vertical loop with a bodyflight chamber within a vertical part of the loop. Recirculating wind tunnels are usually built in climates that are too cold for non-recirculating wind tunnels. The airflow of an indoor vertical wind tunnel is usually smoother and more controlled than that of an outdoor unit. Indoor tunnels are more temperature-controllable, so they are operated year-round even in cold climates.\n\nVarious propellers and fan types can be used as the mechanism to move air through a vertical wind tunnel. Motors can either be diesel-powered or electric-powered, and typically provide a vertical column of air between 6 and 16 feet wide. A control unit allows for air speed adjustment by a controller in constant view of the flyers. Wind speed can be adjusted at many vertical wind tunnels, usually between 130 and 300 km/h (80 and 185 mph, or 35 and 80 m/s), to accommodate the abilities of an individual and to compensate for variable body drag during advanced acrobatics.\n\nIndoor skydiving also appeals to the mass market audience that are afraid of heights, since in a vertical wind tunnel, one only floats a few meters above trampoline-type netting. Indoor vertical wind tunnels contain the person within a chamber through the use of walls. While wind tunnel flying is considered a low impact activity, it does exert some strain on the flier's back, neck, and shoulders. Therefore, people with shoulder dislocations or back/neck problems should check with a doctor first. While actual skydiving out of an aircraft is subject to age limitations which vary from country to country, and even from state to state in the US, bodyflying has no set lower or upper limits. Children can fly provided that they are happy and are not being pressed to participate, and they have parental/guardian signed consent.\n\nA number of competitions based on indoor skydiving have emerged, such as the FAI World Cup of Indoor Skydiving and the Windoor Wind Games.\n\n\n"}
{"id": "1739001", "url": "https://en.wikipedia.org/wiki?curid=1739001", "title": "Wetting", "text": "Wetting\n\nWetting is the ability of a liquid to maintain contact with a solid surface, resulting from intermolecular interactions when the two are brought together. The degree of wetting (wettability) is determined by a force balance between adhesive and cohesive forces. Wetting deals with the three phases of materials: gas, liquid, and solid. It is now a center of attention in nanotechnology and nanoscience studies due to the advent of many nanomaterials in the past two decades (e.g. graphene, carbon nanotube, boron nitride nanomesh).\n\nWetting is important in the bonding or adherence of two materials. Wetting and the surface forces that control wetting are also responsible for other related effects, including capillary effects.\n\nThere are two types of wetting: non-reactive wetting and active wetting.\n\nAdhesive forces between a liquid and solid cause a liquid drop to spread across the surface. Cohesive forces within the liquid cause the drop to ball up and avoid contact with the surface.\n\nThe contact angle (θ), as seen in Figure 1, is the angle at which the liquid–vapor interface meets the solid-liquid interface. The contact angle is determined by the balance between adhesive and cohesive forces. As the tendency of a drop to spread out over a flat, solid surface increases, the contact angle decreases. Thus, the contact angle provides an inverse measure of wettability.\n\nA contact angle less than 90° (low contact angle) usually indicates that wetting of the surface is very favorable, and the fluid will spread over a large area of the surface. Contact angles greater than 90° (high contact angle) generally means that wetting of the surface is unfavorable, so the fluid will minimize contact with the surface and form a compact liquid droplet.\nFor water, a wettable surface may also be termed hydrophilic and a nonwettable surface hydrophobic. Superhydrophobic surfaces have contact angles greater than 150°, showing almost no contact between the liquid drop and the surface. This is sometimes referred to as the \"Lotus effect\". The table describes varying contact angles and their corresponding solid/liquid and liquid/liquid interactions. For nonwater liquids, the term lyophilic is used for low contact angle conditions and lyophobic is used when higher contact angles result. Similarly, the terms omniphobic and omniphilic apply to both polar and apolar liquids.\n\nLiquids can interact with two main types of solid surfaces. Traditionally, solid surfaces have been divided into high-energy solids and low-energy types. The relative energy of a solid has to do with the bulk nature of the solid itself. Solids such as metals, glasses, and ceramics are known as 'hard solids' because the chemical bonds that hold them together (e.g., covalent, ionic, or metallic) are very strong. Thus, it takes a large input of energy to break these solids (alternatively large amount of energy is required to cut the bulk and make two separate surfaces so high surface energy), so they are termed “high energy”. Most molecular liquids achieve complete wetting with high-energy surfaces.\n\nThe other type of solids is weak molecular crystals (e.g., fluorocarbons, hydrocarbons, etc.) where the molecules are held together essentially by physical forces (e.g., van der Waals and hydrogen bonds). Since these solids are held together by weak forces, a very low input of energy is required to break them, thus they are termed “low energy”. Depending on the type of liquid chosen, low-energy surfaces can permit either complete or partial wetting.\n\nDynamic surfaces have been reported that undergo changes in surface energy upon the application of an appropriate stimuli. For example, a surface presenting photon-driven molecular motors was shown to undergo changes in water contact angle when switched between bistable conformations of differing surface energies.\n\nLow-energy surfaces primarily interact with liquids through dispersion (van der Waals) forces. William Zisman produced several key findings:\n\nZisman observed that cos θ increases linearly as the surface tension (γ) of the liquid decreased. Thus, he was able to establish a linear function between cos θ and the surface tension (γ) for various organic liquids.\n\nA surface is more wettable when γ and θ is low. Zisman termed the intercept of these lines when cos θ = 1, as the critical surface tension (γ) of that surface. This critical surface tension is an important parameter because it is a characteristic of only the solid.\n\nKnowing the critical surface tension of a solid, it is possible to predict the wettability of the surface. \nThe wettability of a surface is determined by the outermost chemical groups of the solid.\nDifferences in wettability between surfaces that are similar in structure are due to differences in the packing of the atoms. For instance, if a surface has branched chains, it will have poorer packing than a surface with straight chains. \nLower critical surface tension means a less wettable material surface.\n\nAn ideal surface is flat, rigid, perfectly smooth, and chemically homogeneous, and has zero contact angle hysteresis. Zero hysteresis implies the advancing and receding contact angles are equal. In other words, only one thermodynamically stable contact angle exists. When a drop of liquid is placed on such a surface, the characteristic contact angle is formed as depicted in Fig. 1. Furthermore, on an ideal surface, the drop will return to its original shape if it is disturbed. The following derivations apply only to ideal solid surfaces; they are only valid for the state in which the interfaces are not moving and the phase boundary line exists in equilibrium.\n\nFigure 3 shows the line of contact where three phases meet. In equilibrium, the net force per unit length acting along the boundary line between the three phases must be zero. The components of net force in the direction along each of the interfaces are given by:\n\nwhere α, β, and θ are the angles shown and γ is the surface energy between the two indicated phases. These relations can also be expressed by an analog to a triangle known as Neumann’s triangle, shown in Figure 4. Neumann’s triangle is consistent with the geometrical restriction that formula_2, and applying the law of sines and law of cosines to it produce relations that describe how the interfacial angles depend on the ratios of surface energies.\n\nBecause these three surface energies form the sides of a triangle, they are constrained by the triangle inequalities, γ < γ + γ meaning that not one of the surface tensions can exceed the sum of the other two. If three fluids with surface energies that do not follow these inequalities are brought into contact, no equilibrium configuration consistent with Figure 3 will exist.\n\nIf the β phase is replaced by a flat rigid surface, as shown in Figure 5, then β = π, and the second net force equation simplifies to the Young equation,\n\nwhich relates the surface tensions between the three phases: solid, liquid and gas. Subsequently, this predicts the contact angle of a liquid droplet on a solid surface from knowledge of the three surface energies involved. This equation also applies if the \"gas\" phase is another liquid, immiscible with the droplet of the first \"liquid\" phase.\n\nThe Young equation assumes a perfectly flat and rigid surface often referred to as an ideal surface. In many cases, surfaces are far from this ideal situation, and two are considered here: the case of rough surfaces and the case of smooth surfaces that are still real (finitely rigid). Even in a perfectly smooth surface, a drop will assume a wide spectrum of contact angles ranging from the so-called advancing contact angle, formula_4, to the so-called receding contact angle, formula_5. The equilibrium contact angle (formula_6) can be calculated from formula_4 and formula_5 as was shown by Tadmor as,\n\nwhere\n\nThe Young–Dupré equation (Thomas Young 1805; Anthanase Dupré and Paul Dupré 1869) dictates that neither γ nor γ can be larger than the sum of the other two surface energies. The consequence of this restriction is the prediction of complete wetting when γ > γ + γ and zero wetting when γ > γ + γ. The lack of a solution to the Young–Dupré equation is an indicator that there is no equilibrium configuration with a contact angle between 0 and 180° for those situations.\n\nA useful parameter for gauging wetting is the \"spreading parameter S\",\n\nWhen \"S\" > 0, the liquid wets the surface completely (complete wetting). When \"S\" < 0, partial wetting occurs.\n\nCombining the spreading parameter definition with the Young relation yields the Young–Dupré equation:\n\nwhich only has physical solutions for θ when S < 0.\n\nUnlike ideal surfaces, real surfaces do not have perfect smoothness, rigidity, or chemical homogeneity. Such deviations from ideality result in phenomenon called contact-angle hysteresis, which is defined as the difference between the advancing (θ) and receding (θ) contact angles \n\nWhen the contact angle is between the advancing and receding cases, the contact line is considered to be pinned and hysteretic behaviour can be observed, namely contact angle hysteresis. When these values are exceeded, the displacement of the contact line, such as the one in Figure 3, will take place by either expansion or retraction of the droplet . Figure 6 depicts the advancing and receding contact angles. The advancing contact angle is the maximum stable angle, whereas the receding contact angle is the minimum stable angle. Contact-angle hysteresis occurs because many different thermodynamically stable contact angles are found on a nonideal solid. These varying thermodynamically stable contact angles are known as metastable states.\n\nSuch motion of a phase boundary, involving advancing and receding contact angles, is known as dynamic wetting. The difference between dynamic and static wetting angles is proportional to the Capillary number, formula_14, When a contact line advances, covering more of the surface with liquid, the contact angle is increased and generally is related to the velocity of the contact line. If the velocity of a contact line is increased without bound, the contact angle increases, and as it approaches 180°, the gas phase will become entrained in a thin layer between the liquid and solid. This is a kinetic nonequilibrium effect which results from the contact line moving at such a high speed that complete wetting cannot occur.\n\nA well-known departure from ideal conditions is when the surface of interest has a rough texture. The rough texture of a surface can fall into one of two categories: homogeneous or heterogeneous. A homogeneous wetting regime is where the liquid fills in the roughness grooves of a surface. A heterogeneous wetting regime, though, is where the surface is a composite of two types of patches. An important example of such a composite surface is one composed of patches of both air and solid. Such surfaces have varied effects on the contact angles of wetting liquids. Cassie–Baxter and Wenzel are the two main models that attempt to describe the wetting of textured surfaces. However, these equations only apply when the drop size is sufficiently large compared with the surface roughness scale. When the droplet size is comparable to that of the underlying pillars, the effect of line tension should be considered. .\n\nThe Wenzel model (Robert N. Wenzel 1936) describes the homogeneous wetting regime, as seen in Figure 7, and is defined by the following equation for the contact angle on a rough surface: \n\nwhere formula_16 is the apparent contact angle which corresponds to the stable equilibrium state (i.e. minimum free energy state for the system). The roughness ratio, r, is a measure of how surface roughness affects a homogeneous surface. The roughness ratio is defined as the ratio of true area of the solid surface to the apparent area.\n\nθ is the Young contact angle as defined for an ideal surface. Although Wenzel's equation demonstrates the contact angle of a rough surface is different from the intrinsic contact angle, it does not describe contact angle hysteresis.\n\nWhen dealing with a heterogeneous surface, the Wenzel model is not sufficient. A more complex model is needed to measure how the apparent contact angle changes when various materials are involved. This heterogeneous surface, like that seen in Figure 8, is explained using the Cassie–Baxter equation (Cassie's law):\n\nHere the \"r\" is the roughness ratio of the wet surface area and \"f\" is the fraction of solid surface area wet by the liquid. It is important to realize that when \"f\" = 1 and \"r = r\", the Cassie–Baxter equations becomes the Wenzel equation. On the other hand, when there are many different fractions of surface roughness, each fraction of the total surface area is denoted by formula_18.\n\nA summation of all \"f\" equals 1 or the total surface. Cassie–Baxter can also be recast in the following equation:\n\nHere γ is the Cassie–Baxter surface tension between liquid and vapor, the γ is the solid vapor surface tension of every component and γ is the solid liquid surface tension of every component. A case that is worth mentioning is when the liquid drop is placed on the substrate and creates small air pockets underneath it. This case for a two-component system is denoted by:\n\nHere the key difference to notice is that there is no surface tension between the solid and the vapor for the second surface tension component. This is because of the assumption that the surface of air that is exposed is under the droplet and is the only other substrate in the system. Subsequently, the equation is then expressed as (1 – \"f\"). Therefore, the Cassie equation can be easily derived from the Cassie–Baxter equation. Experimental results regarding the surface properties of Wenzel versus Cassie–Baxter systems showed the effect of pinning for a Young angle of 180 to 90°, a region classified under the Cassie–Baxter model. This liquid/air composite system is largely hydrophobic. After that point, a sharp transition to the Wenzel regime was found where the drop wets the surface, but no further than edges of the drop.\n\nWith the advent of high resolution imaging, researchers have started to obtain experimental data which have led them to question the assumptions of the Cassie–Baxter equation when calculating the apparent contact angle. These groups believe the apparent contact angle is largely dependent on the triple line. The triple line, which is in contact with the heterogeneous surface, cannot rest on the heterogeneous surface like the rest of the drop. In theory, it should follow the surface imperfection. This bending in triple line is unfavorable and is not seen in real-world situations. A theory that preserves the Cassie–Baxter equation while at the same time explaining the presence of minimized energy state of the triple line hinges on the idea of a precursor film. This film of submicrometer thickness advances ahead of the motion of the droplet and is found around the triple line. Furthermore, this precursor film allows the triple line to bend and take different conformations that were originally considered unfavorable. This precursor fluid has been observed using environmental scanning electron microscopy (ESEM) in surfaces with pores formed in the bulk. With the introduction of the precursor film concept, the triple line can follow energetically feasible conformations and thereby correctly explaining the Cassie–Baxter model.\n\nThe intrinsic hydrophobicity of a surface can be enhanced by being textured with different length scales of roughness. The red rose takes advantage of this by using a hierarchy of micro- and nanostructures on each petal to provide sufficient roughness for superhydrophobicity. More specifically, each rose petal has a collection of micropapillae on the surface and each papilla, in turn, has many nanofolds. The term “petal effect” describes the fact that a water droplet on the surface of a rose petal is spherical in shape, but cannot roll off even if the petal is turned upside down. The water drops maintain their spherical shape due to the superhydrophobicity of the petal (contact angle of about 152.4°), but do not roll off because the petal surface has a high adhesive force with water.\n\nWhen comparing the \"petal effect\" to the \"lotus effect\", it is important to note some striking differences. The surface structure of the lotus leaf and the rose petal, as seen in Figure 9, can be used to explain the two different effects. The lotus petal has a randomly rough surface and low contact angle hysteresis, which means the water droplet is not able to wet the microstructure spaces between the spikes. This allows air to remain inside the texture, causing a heterogeneous surface composed of both air and solid. As a result, the adhesive force between the water and the solid surface is extremely low, allowing the water to roll off easily (i.e. \"self-cleaning\" phenomenon).\n\nThe rose petal's micro- and nanostructures are larger in scale than those of the lotus leaf, which allows the liquid film to impregnate the texture. However, as seen in Figure 9, the liquid can enter the larger-scale grooves, but it cannot enter into the smaller grooves. This is known as the Cassie impregnating wetting regime. Since the liquid can wet the larger-scale grooves, the adhesive force between the water and solid is very high. This explains why the water droplet will not fall off even if the petal is tilted at an angle or turned upside down. This effect will fail if the droplet has a volume larger than 10 µl because the balance between weight and surface tension is surpassed.\n\nIn the Cassie–Baxter model, the drop sits on top of the textured surface with trapped air underneath. During the wetting transition from the Cassie state to the Wenzel state, the air pockets are no longer thermodynamically stable and liquid begins to nucleate from the middle of the drop, creating a \"mushroom state\" as seen in Figure 10. The penetration condition is given by:\n\nwhere\n\nThe penetration front propagates to minimize the surface energy until it reaches the edges of the drop, thus arriving at the Wenzel state. Since the solid can be considered an absorptive material due to its surface roughness, this phenomenon of spreading and imbibition is called hemiwicking. The contact angles at which spreading/imbibition occurs are between 0 and π/2.\n\nThe Wenzel model is valid between θ and π/2. If the contact angle is less than Θ, the penetration front spreads beyond the drop and a liquid film forms over the surface. Figure 11 depicts the transition from the Wenzel state to the surface film state. The film smoothes the surface roughness and the Wenzel model no longer applies. In this state, the equilibrium condition and Young's relation yields:\n\nBy fine-tuning the surface roughness, it is possible to achieve a transition between both superhydrophobic and superhydrophilic regions. Generally, the rougher the surface, the more hydrophobic it is.\n\nIf a drop is placed on a smooth, horizontal surface, it is generally not in the equilibrium state. Hence, it spreads until an equilibrium contact radius is reached (partial wetting). While taking into account capillary, gravitational, and viscous contributions, the drop radius as a function of time can be expressed as\n\nFor the complete wetting situation, the drop radius at any time during the spreading process is given by\n\nwhere\n\nMany technological processes require control of liquid spreading over solid surfaces. When a drop is placed on a surface, it can completely wet, partially wet, or not wet the surface. By reducing the surface tension with surfactants, a nonwetting material can be made to become partially or completely wetting. The excess free energy (σ) of a drop on a solid surface is:\n\nBased on this equation, the excess free energy is minimized when γ decreases, γ decreases, or γ increases. Surfactants are absorbed onto the liquid–vapor, solid–liquid, and solid–vapor interfaces, which modify the wetting behavior of hydrophobic materials to reduce the free energy. When surfactants are absorbed onto a hydrophobic surface, the polar head groups face into the solution with the tail pointing outward. In more hydrophobic surfaces, surfactants may form a bilayer on the solid, causing it to become more hydrophilic. The dynamic drop radius can be characterized as the drop begins to spread. Thus, the contact angle changes based on the following equation:\n\nAs the surfactants are absorbed, the solid–vapor surface tension increases and the edges of the drop become hydrophilic. As a result, the drop spreads.\n\nFerrocene is a redox-active organometallic compound which can be incorporated into various monomers and used to make polymers which can be tethered onto a surface. Vinyl<nowiki></nowiki>ferrocene (ferroceneylethene) can be prepared by a Wittig reaction and then polymerised to form polyvinylferrocene (PVFc), an analogue of polystyrene. Another polymer which can be formed is poly( ferrocenecarboxylate), PFcMA. Both PVFc and PFcMA have been tethered onto silica wafers and the wettability measured when the polymer chains are uncharged and when the ferrocene moieties are oxidised to produce positively charged groups, as illustrated at right. The contact angle with water on the PFcMA-coated wafers was 70° smaller following oxidation, while in the case of PVFc the decrease was 30°, and the switching of wettability has been shown to be reversible. In the PFcMA case, the effect of longer chains with more ferrocene groups (and also greater molar mass) has been investigated, and it was found that longer chains produce significantly larger contact angle reductions.\n\n"}
