{"id": "4994693", "url": "https://en.wikipedia.org/wiki?curid=4994693", "title": "1979 Mississauga train derailment", "text": "1979 Mississauga train derailment\n\nThe Mississauga train derailment of 1979, also known as the Mississauga Miracle occurred on Saturday, November 10, 1979, in Mississauga, Ontario, Canada, when a 106-car Canadian Pacific freight train carrying chemicals and explosives including styrene, toluene, propane, caustic soda, and chlorine from Windsor, Ontario derailed near the intersection of Mavis Road and Dundas Street in Mississauga, Ontario. As a result of the derailment, more than 200,000 people were evacuated in what was the largest peacetime evacuation in North America until the New Orleans evacuation of 2005. There were no deaths resulting from the incident. This was the last major explosion in the Greater Toronto Area until the Sunrise Propane blast in 2008.\n\nOn the 33rd car, heat began to build up in an improperly-lubricated journal bearing on one of the wheels, one of the few still in use at that time as most had long since been replaced with roller bearings, resulting in the condition known among train workers as a \"hot box\". Residents living beside the tracks reported smoke and sparks coming from the car, and those who were close to Mississauga thought the train was afire. The friction eventually burned through the axle and bearing, and as the train was passing the Burnhamthorpe Road level crossing, a wheelset (one axle and pair of wheels) fell off completely.\n\nAt 11:53 p.m., at the Mavis Road crossing, the damaged bogie (undercarriage) left the track, causing the remaining parts of the train to derail. The impact caused several tank cars filled with propane to burst into flames.\n\nThe derailment also ruptured several other tankers, spilling styrene, toluene, propane, caustic soda, and chlorine onto the tracks and into the air. A huge explosion resulted, sending a fireball into the sky which could be seen from away. As the flames were erupting, the train's brakeman, Larry Krupa, 27, at the suggestion of the engineer (also his father-in-law), managed to close an air brake angle spigot at the west end of the undamaged 32nd car, allowing the engineer to release the air brakes between the locomotives and the derailed cars and move the front part of the train eastward along the tracks, away from danger. This prevented those cars from becoming involved in the fire, important as many of them also contained dangerous goods. Mr. Krupa was later recommended for the Order of Canada for his bravery, which a later writer has described as \"bordering on lunacy.\"\n\nAfter more explosions, firefighters concentrated on cooling cars, allowing the fire to burn itself out, but a ruptured chlorine tank became a cause for concern. With the possibility of a deadly cloud of chlorine gas spreading through suburban Mississauga, more than 200,000 people were evacuated. A number of residents (mostly the extreme west and north of Mississauga) allowed evacuees to stay with them until the crisis abated. Some of these people were later moved again as their hosts were also evacuated. The evacuation was managed by various officials including the mayor of Mississauga, Hazel McCallion, the Peel Regional Police and other governmental authorities. McCallion sprained her ankle early during the crisis, but continued to hobble to press conferences.\n\nWithin a few days Mississauga was practically deserted, until the contamination had been cleared, the danger neutralized and residents were allowed to return to their homes. The city was finally reopened on the evening of November 16. The chlorine tank was emptied on November 19.\n\nIt was the largest peacetime evacuation in North American history until the evacuation of New Orleans due to Hurricane Katrina in 2005, and remains the second-largest as of 2016.\n\nDue to the speed and efficiency with which it was conducted, many cities later studied and modelled their own emergency plans after Mississauga's.\n\nAs a result of the accident, rail regulators in both the U.S. and Canada required that any line used to carry hazardous materials into or through a populated area have hotbox detectors.\nLarry Krupa was inducted into the North America Railway Hall of Fame for his contribution to the railway industry. He was recognized in the \"National\" division of the \"Railway Workers & Builders\" category.\nHazel McCallion, in her first term as mayor at the time of the accident, was continuously re-elected ever since until her retirement in 2014 at age 93.\n\nThe song Trainwreck 1979 by Canadian band Death From Above 1979 is about the derailment:\n\n\"It ran off the track, 11-79\"\n\"While the immigrants slept, there wasn't much time\"\n\"The mayor came calling and got 'em outta bed\"\n\"They packed up their families and headed upwind\"\n\"A poison cloud, a flaming sky, 200,000 people and no one died\"\n\"And all before the pocket dial, yeah!\"\n\n\n"}
{"id": "39417871", "url": "https://en.wikipedia.org/wiki?curid=39417871", "title": "Ahuroa Gas Storage Facility", "text": "Ahuroa Gas Storage Facility\n\nThe Ahuroa Gas Storage Facility is an underground natural gas storage facility situated at Ahuroa in the Taranaki region of New Zealand, owned by Gas Services New Zealand. It was officially opened in 2011. The development cost of the facility was $177m. The stored gas is used to supply the Stratford Power Station when needed during peak electricity demand. Gas can be stored by injection at up to 32 terajoules per day and withdrawn at up to 45 terajoules per day.\n\nThe Tariki / Ahuroa field was discovered in 1986. Construction of wellsite facilities began in 1995 and production commenced in 1996. The facility was in turn owned by Fletcher Challenge, Shell and Swift Energy.\n\nIn 2008, when the field was largely depleted, it was acquired by Origin Energy as part of the Tariki / Ahuroa / Waihapa / Ngaere assets. The gas storage facility was constructed by Contact Energy in 2009 and 2010.\n\nIn 2017, Contact Energy sold the gas storage facility to Gas Services New Zealand.\n\n"}
{"id": "18851062", "url": "https://en.wikipedia.org/wiki?curid=18851062", "title": "Asplenium trichomanes", "text": "Asplenium trichomanes\n\nAsplenium trichomanes (commonly known as maidenhair spleenwort) is a small fern in the spleenwort genus \"Asplenium\". It is a widespread and common species, occurring almost worldwide in a variety of rocky habitats. It is a variable fern with several subspecies.\n\nThe specific epithet \"trichomanes\" refers to a Greek word for “fern”.\n\nIt grows in tufts from a short rhizome. The fronds are long and narrow, gradually tapering towards the tip. They are simply divided into small, yellow-green to dark-green pinnae. The stipe and rachis of the frond are dark all along their length. The fronds can reach 40 cm in length but are more commonly 8–20 cm. They bear long, narrow sori which contain the spores.\n\nIt is widespread in temperate and subarctic areas and also occurs in mountainous regions in the tropics. Its range includes most of Europe and much of Asia south to Turkey, Iran and the Himalayas with a population in Yemen. It occurs in northern, southern and parts of eastern Africa and also in eastern Indonesia, south-east Australia, Tasmania, New Zealand and Hawaii. It is found in North America and Central America and Cuba, and the northern and western regions of South America such as Chile.\n\nIt grows in rocky habitats such as cliffs, scree slopes, walls and mine waste, the type of rock used as a substrate depending on the subspecies. It grows from sea-level up to 3000 metres in North America while in the British Isles it reaches 870 metres.\n\n\"Asplenium trichomanes\" is valued in cultivation for its hardiness (down to ), its evergreen foliage and its ability to colonise crevices in stone walls. It prefers a fully or partially shaded aspect. It has gained the Royal Horticultural Society’s Award of Garden Merit.\n\n\"Asplenium trichomanes\" has diploid, tetraploid and hexaploid cytotypes, which it has been argued should be recognised as distinct species. A triploid cytotype (a sterile hybrid between the diploid and tetraploid cytotype) is also known. Within these cytotypes several subspecies are recognised, including\n\n\n\n"}
{"id": "51646790", "url": "https://en.wikipedia.org/wiki?curid=51646790", "title": "B. R. Murty", "text": "B. R. Murty\n\nBhyravabhotla Radhakrishna Murty (1928–2003) was an Indian botanist, known for his contributions the fields of Conservation genetics and Radiation genetics. He was a professor of Biochemistry Division at Indian Agricultural Research Institute, Pusa and was an elected fellow of Indian Academy of Sciences and the Indian National Science Academy. The Council of Scientific and Industrial Research, the apex agency of the Government of India for scientific research, awarded him the Shanti Swarup Bhatnagar Prize for Science and Technology, one of the highest Indian science awards, in 1973, for his contributions to biological sciences.\n\nRadhakrishna Murty, born on 4 April 1928, secured his PhD from Cornell University in 1960, working under the supervision of renowned Botanist, Royse Peak Murphy. Returning to India, he joined the Indian Agricultural Research Institute, Delhi as a biometrical geneticist in 1961 and in 1966 he was promoted as the co-ordinator of the All India Project for Crop Improvement, a post he held till 1974. After serving as a Nuffield University Professional Fellow at Cambridge University during 1973–74, he joined International Atomic Energy Agency in 1974 where he worked till 1986, holding the position of the project director of the Nuclear Research Laboratory from 1974 to 1984. He also served as a visiting professor at three US universities, Cornell University, Colorado State University and Ohio State University, as an honorary professor at University of Zulia, Venezuela, as an INSA Senior Scientist from 1988 to 1991 and as a professor of biochemistry at the Pusa campus of the Indian Agricultural Research Institute.\nMurty died on 16 May 2003, at the age of 75. The Indian National Science Academy has included a biographical sketch on him in their publication, \"Biographical memoirs of fellows of the Indian National Science Academy\", published in 2005.\n\nMurty was the pioneer of biometrical genetics at the Indian Agricultural Research Institute when he started his research on the subject in 1961 during his stint at their Delhi campus. There, he worked on the genetic divergence of crops for disease resistance, stress tolerance and quality, which was reported to have assisted in the development of different varieties of crops. His researches have been documented by way of several articles and three books, \"Genetic Studies of Some Hybrid Derivatives of Nicotiana Rustica and Nicotiana Tabacum\", \"Bajra Production: Problems and Prospects\" and \"Breeding Procedures in Pearl-millet: (Pennisetum Typhoides S. & H.)\". He sat in the International Committee on International Biological Programme of the United Nations from 1969 to 1975, International Committee on Plant Breeding Perspectives from 1974 1979 and the project evaluation Committee of the United Nations Development Program in 1987 and served as the director, International Training Program of Food and Agriculture Organization/IAEA and as a consultant at International Maize and Wheat Improvement Center (CIMMYT) in 1992 and International Crops Research Institute for the Semi-Arid Tropics (ICRISAT) in 1975, 1979 and 1982.\nHe also served as a member of the editorial boards of journals such as \"Theoretical and Applied Breeding\" and \"Genetics and Breeding\".\n\nMurty, who was the president of the Indian Society of Genetics and Plant Breeding in 1979, was elected as a fellow by the Indian National Science Academy in 1974 and the Indian Academy of Sciences in 1975. He was also a fellow of the Royal Statistical Society and a member of the International Biometric Society, Genetics Society and American Statistical Association. He was awarded the Shanti Swarup Bhatnagar Prize, one of the highest Indian science awards, by the Council of Scientific and Industrial Research in 1973.\n\n"}
{"id": "2120633", "url": "https://en.wikipedia.org/wiki?curid=2120633", "title": "Baytown Refinery", "text": "Baytown Refinery\n\nExxonMobil's Baytown Refinery is the second largest oil refinery in the United States located in Baytown, Texas. It has capacity of . The site first opened in 1919 and was originally operated by the Humble Oil Company. Today, it is the largest employer in the city. The plant takes up of land next to the Houston Ship Channel.\n\n"}
{"id": "33653162", "url": "https://en.wikipedia.org/wiki?curid=33653162", "title": "Booster pump", "text": "Booster pump\n\nA booster pump is a machine which will increase the pressure of a fluid. They may be used with liquids or gases, but the construction details will vary depending on the fluid.\nA gas booster is similar to a gas compressor, but generally a simpler mechanism which often has only a single stage of compression, and is used to increase pressure of a gas already above ambient pressure. Two-stage boosters are also made.\nBoosters may be used for increasing gas pressure, transferring high pressure gas, charging gas cylinders and scavenging.\n\nOn new construction and retrofit projects, water pressure booster pumps are used to provide adequate water pressure to upper floors of high rise buildings. The need for a water pressure booster pump can also arise after the installation of a backflow prevention device (BFP), which is currently mandated in many municipalities to protect the public water supplies from contaminants within a building entering the public water supply. The use of BFPs began after The Clean Water Act was passed. These devices can cause a loss of 12 PSI, and can cause flushometers on upper floors not to work properly.\nAfter pipes have been in service for an extended period, scale can build up on the inside surfaces which will cause a pressure drop when the water flows.\n\nBooster pumps are usually piston or plunger type compressors. A single-acting, single-stage booster is the simplest configuration, and comprises a cylinder, designed to withstand the operating pressures, with a piston which is driven back and forth inside the cylinder. The cylinder head is fitted with supply and discharge ports, to which the supply and discharge hoses or pipes are connected, with a non-return valve on each, constraining flow in one direction from supply to discharge. When the booster is inactive, and the piston is stationary, gas will flow from the inlet hose, through the inlet valve into the space between the cylinder head and the piston. If the pressure in the outlet hose is lower, it will then flow out and to whatever the outlet hose is connected to. This flow will stop when the pressure is equalized, taking valve opening pressures into account.\n\nOnce the flow has stopped, the booster is started, and as the piston withdraws along the cylinder, increasing the volume between the cylinder head and the piston crown, the pressure in the cylinder will drop, and gas will flow in from the inlet port. On the return cycle, the piston moves toward the cylinder head, decreasing the volume of the space and compressing the gas until the pressure is sufficient to overcome the pressure in the outlet line and the opening pressure of the outlet valve. At that point, the gas will flow out of the cylinder via the outlet valve and port. \n\nThere will always be some compressed gas remaining in the cylinder and cylinder head spaces at the top of the stroke. The gas in this \"dead space\" will expand during the next induction stroke, and only after it has dropped below the supply gas pressure, more supply gas will flow into the cylinder. The ratio of the volume of the cylinder space with the piston fully withdrawn, to the dead space, is the \"compression ratio\" of the booster, also termed \"boost ratio\" in this context. Efficiency of the booster is related to the compression ratio, and gas will only be transferred while the pressure ratio between supply and discharge gas is less than the boost ratio, and delivery rate will drop as the inlet to delivery pressure ratio increases.\n\nDelivery rate starts at very close to swept volume when there is no pressure difference, and drops steadily until there is no effective transfer when the pressure ratio reaches the maximum boost ratio.\n\nCompression of gas will cause a rise in temperature. The heat is mostly carried out by the compressed gas, but the booster components will also be heated by contact with the hot gas. Some boosters are cooled by water jackets or external fins to increase convectional cooling by the ambient air, but smaller models may have no special cooling facilities at all. Cooling arrangements will improve efficiency, but will cost more to manufacture.\n\nBoosters to be used with oxygen must be made from oxygen-compatible materials, and use oxygen-compatible lubricants to avoid fire.\n\nBoosters may be driven by an electric motor, hydraulics, low or high pressure air or manually by a lever system. Those powered by compressed air are usually linear actuated systems, where a pneumatic cylinder directly drives the compression piston, often in a common housing, separated by a seal. A high pressure pneumatic drive arrangement may use the same pressure as the output pressure to drive the piston, and a low pressure drive will use a larger diameter piston to multiply the applied force.\n\nHigh pressure gas boosters are manufactured by Haskel, Draeger and others. Rugged and unsophisticated models (KN-3 and KN-4) were manufactured for the Soviet Armed Forces and surplus examples are now used by technical divers as they are relatively inexpensive and are supplied with a comprehensive spares and tool kit. \n"}
{"id": "14620877", "url": "https://en.wikipedia.org/wiki?curid=14620877", "title": "Carlin stone", "text": "Carlin stone\n\nCarlin Stone or Carlin Stane is the name given to a number of prehistoric standing stones and natural stone or landscape features in Scotland. The significance of the name is unclear, other than its association with old hags, witches, and the legends of the Cailleach.\n\nA 'Carle' in Scots is a commoner, a husband or in a derogatory sense, a churl or male of low birth. The name 'Carline', 'Cairlin', Carlin, 'Cyarlin', 'Kerlin' or 'Kerl' was also used in lowland Scots as a derogatory term for an old woman meaning an 'old hag'. It is from Old Norse \"Kerling\" or a corruption or equivalent in Scots of the Gaelic word “Cailleach”, meaning a witch or the 'old Hag', the Goddess of Winter.\n\nCarlin is used as a surname and has several variations e.g., Carlen, Carlon, Carolan, O'Carlin, O'Carlen, O'Carlon, O'Carolan, Carling, Carlton, etc. It is stated as being of Irish Gaelic origin and is found somewhat less frequently in Scotland.\n\nThis is a rocky islet in the South of Orkney.\n\nNear Sandlaw Farm in the parish of Alvah is the Carlin Cist, thought to have been part of a Cromlech at one time.\n\nThis stone was part of a recumbent stone circle, around in diameter. It has several alternative names, such as the Caerlin stone; Cairn Riv; Cairn Rib; or Cairn-Rieve. Its map reference in the parish of Inverkeithny is NJ 6744 4659. Three stones remain in line, the Carlin Stone between two others quite small in comparison. In addition, there are two set stones projecting inward from the Carlin Stone.\nThis boulder is rugged, unshapely, and most unusual in height. Other stones were broken up and removed within relatively recent times; the mounds of stones being carted away for making dikes or drystone walls. In or near the circle were found a small perforated axe-hammer, portions of 3 bronze armlets, flint chips and a jet button.\n\nThis natural stone outcrop is known as Carlin Maggie and has the look of something imported from Easter Island, but it is natural. It is said to be a witch turned to stone by the Devil after she got on his nerves (\"carline\" is an old Scots word for 'witch'). The Devil threw a lightning bolt which had the effect of petrifying her. It is a rock pillar estimated to be high, on the Western slope of Bishop Hill, overlooking Loch Leven. The OS grid reference is NO 18403 04413.\n\nA \"Carlin Stone\" is marked on the OS 6 inch series of maps from 1843-82 at this location approximately 5 km NNW of Fintry.\n\nOn top of the Common Crags overlooking the village of Dunlop and the Glazert Water is a large procumbent boulder known on the OS map as the ‘Carlin’s Stone or Stane’. It is also known locally as the Hag's Stone.\n\nIt is not listed by the RCAHMS and is not as well known locally as the nearby megalith known as the Thurgartstone.\n\nThree farms named 'Carlingcrags' on the Ordnance Survey maps are to be found above Darvel in East Ayrshire.\n\nA Carlin Stone is situated on Whitelee Moor near Craigends Farm, below Cameron's Moss near Waterside in East Ayrshire. A nearby watercourse is known as the Carlin Burn, joining the Hareshawmuir water just below the site of the Carlin stone. The stone has been much visited in the past; indicated by the remains of a footbridge running to it across the Hareshawmuir Water.\nCarlin knowe is a low hill with a prehistoric cairn on its summit near Knockshinnoch farm.\n\nThe OS Maps locate a Carlin Stone or Carlin Crags/Craigs near Bonnyton Golf Club on the outskirts of Eaglesham. Cup marked stones are present at the site. At least two fairly horizontal flat rock faces have cups on them, rings being entirely absent. Two sets of crags are present at the site but only the upper has the petroglyphs.\n\nThis is a mountain in the south-west of Carsphairn parish.\n\nThis town was known as Carlinwark until 1792. The title came from nearby Carlinwark loch in the north of the parish of Kelton.\n\nA Carlin Stone is to ben found at 'The Derry', near to the head of Elrig Loch near Wigtown. It is thought to have been part of a Stone circle and is situated at the OS Map Reference NX326497.\n\nThe Carlin's Tooth is the name of a natural rock outcrop in the borders between Knocks Knowe and Carter Fell.\n\nNear Kirkhill outside Stewarton are several farms having the name 'Kilbride' in their title. Bride - an anglicization of Brìghde, Brìd or Saint Brigid - was originally the Celtic Goddess linked with the festival of Imbolc, the eve of the first of February. She was the goddess of Spring and was associated with healing and sacred wells, therefore the antithesis of the Carlin or Cailleach.\n\nPapers in the Scottish National Archive state that the lands of Kilbride Cunninghame near Stewarton were also called the 'Lands of Carlin.'\n\n"}
{"id": "49015956", "url": "https://en.wikipedia.org/wiki?curid=49015956", "title": "Copper(I) thiocyanate", "text": "Copper(I) thiocyanate\n\nCopper(I) thiocyanate (or cuprous thiocyanate) is a coordination polymer with formula CuSCN. It is an air-stable, white solid used as a precursor for the preparation of other thiocyanate salts.\n\nTwo polymorphs have been characterized. The one pictured above features copper(I) in a characteristic tetrahedral coordination geometry. The sulfur end of the SCN- ligand is triply bridging.\n\nCopper(I) thiocyanate forms from the spontaneous decomposition of dry black copper(II) thiocyanate, releasing thiocyanogen, especially when heated. It is also formed from copper(II) thiocyanate under water, releasing (among others) thiocyanic acid and the highly poisonous hydrogen cyanide.<br>\nIt is conveniently prepared from solutions of copper(II) in water, such as copper(II) sulphate. To a copper(II) solution sulphurous acid is added and then a soluble thiocyanate is added (preferably slowly, while stirring). Copper(I) thiocyanate is precipitated as a white powder. Alternatively, a thiosulfate solution may be used as a reducing agent.\n\nCopper(I) thiocyanate forms one double salt with the group 1 elements, CsCu (SCN). The double salt only forms from concentrated solutions of CsSCN, into which CuSCN dissolves. From less concentrated solutions, solid CuSCN separates reflecting its low solubility.\nWhen brought together with potassium, sodium or barium thiocyanate, and brought to crystallisation by concentrating the solution, mixed salts will crystallise out. These are not considered true double salts. As with CsCu (SNC), copper(I) thiocyanate separates out when these mixed salts are redissolved or their solutions diluted.\n\nCopper(I) thiocyanate is a hole conductor, a semiconductor with a wide band gap (3.6 eV, therefore transparent to visible and near infrared light). It is used in photovoltaics in some third-generation cells as a hole transfer layer. It acts as a P-type semiconductor and as a solid-state electrolyte. It is often used in dye-sensitized solar cells. Its hole conductivity is however relatively poor (0.01 S.m. This can be improved by various treatments, e.g. exposure to gaseous chlorine or doping with (SCN).\n\nCuSCN with NiO act synergically as a smoke suppressant additive in PVC.\n\nCuSCN precipitated on carbon support can be used for conversion of aryl halides to aryl thiocyanates.\n\nCopper thiocyanate is used in some anti-fouling paints. Advantages compared to cuprous oxide include that the compound is white and a more efficient biocide.\n"}
{"id": "4377184", "url": "https://en.wikipedia.org/wiki?curid=4377184", "title": "Current limiting", "text": "Current limiting\n\nCurrent limiting is the practice in electrical or electronic circuits of imposing an upper limit on the current that may be delivered to a load with the purpose of protecting the circuit generating or transmitting the current from harmful effects due to a short-circuit or similar problem in the load.\n\nAn inrush current limiter is a device or group of devices used to limit inrush current. Negative temperature coefficient (NTC) thermistors and resistors are two of the simplest options, with cool-down time and power dissipation being their main drawbacks, respectively. More complex solutions can be used when design constraints make simpler options infeasible.\n\nSome electronic circuits employ active current limiting, since a fuse may not protect solid-state devices. \n\nOne style of current limiting circuit is shown in the image. The schematic is representative of a simple protection mechanism used in regulated DC supplies and class-AB power amplifiers.\n\nQ1 is the pass or output transistor. R is the load current sensing device. Q2 is the protection transistor which turns on as soon as the voltage across R becomes about 0.65 V. This voltage is determined by the value of R and the load current through it (I). When Q2 turns on, it removes base current from Q1 thereby reducing the collector current of Q1. which is very nearly the load current. Thus, R fixes the maximum current to a value given by 0.65/R. For example, if R = 0.33 Ω, the current is limited to about 2 A even if R becomes a short (and V becomes zero). \n\nFurther, this power dissipation will remain as long as the overload exists, which means that the devices must be capable of withstanding it for a substantial period. This power dissipation will be substantially less than if no current limiting circuit had been provided. In this technique, beyond the current limit the output voltage will decrease to a value depending on the current limit and load resistance.\n\nAn issue with the previous circuit is that Q1 will not be saturated unless its base is biased about 0.5 volts above V.\n\nThe circuits at right and left operate more efficiently from a single (V) supply. In both circuits, R1 allows Q1 to turn on and pass voltage and current to the load. When the current through R_sense exceeds the design limit, Q2 begins to turn on, which in turn begins to turn off Q1, thus limiting the load current. The optional component R2 protects Q2 in the event of a short-circuited load. When V is at least a few volts, a MOSFET can be used for Q1 for lower dropout voltage. Due to its simplicity, this circuit is sometimes used as a current source for high-power LEDs. \nMany electronics designers put a small resistor on IC output pins.\nThis slows the edge rate which improves electromagnetic compatibility. Some devices have this \"slew rate limiting\" output resistor built in; some devices have programmable slew rate limiting. This provides overall slew rate control.\n\n\n"}
{"id": "4469544", "url": "https://en.wikipedia.org/wiki?curid=4469544", "title": "Density gradient", "text": "Density gradient\n\nDensity gradient is a spatial variation in density over an area. The term is used in the natural sciences to describe varying density of matter, but can apply to any quantity whose density can be measured.\nIn the study of supersonic flight, Schlieren photography observes the density gradient of air as it interacts with aircraft.\nAlso in the field of Computational Fluid Dynamics, Density gradient is used to observe the acoustic waves, shock waves or expansion waves in the flow field.\n\nA steep density gradient in a body of water can have the effect of trapping energy and preventing convection, such a gradient is employed in solar ponds. In the case of salt water, sharp gradients can lead to stratification of different concentrations of salinity. This is called a Halocline. \nIn the life sciences, a special technique called density gradient separation is used for isolating and purifying cells, viruses and subcellular particles. Variations of this include Isopycnic centrifugation, Differential centrifugation, and Sucrose gradient centrifugation. A blood donation technique called Pheresis involves density gradient separation.\n\nThe understanding of what is at the centre of the earth, the earth core, requires the framework of density gradients in which elements and compounds then interact. Fast breeder nuclear reactor at the core of the earth is one theory by reason of density gradient and supported and espoused by J. Marvin Herndon (7 & 8).\n\nIn the study of population, the density gradient can refer to the change in density in an urban area from the center to the periphery.\n"}
{"id": "863572", "url": "https://en.wikipedia.org/wiki?curid=863572", "title": "Dodge Stratus", "text": "Dodge Stratus\n\nThe Dodge Stratus is a mid-size car that was introduced by Dodge in February 1995, and was based on the 4-door sedan Chrysler JA platform. The Stratus, Plymouth Breeze, and Chrysler Cirrus were all on \"Car and Driver\" magazine's Ten Best list for 1996 and 1997. It received critical acclaim at launch, but ratings fell over time. An updated version of the Stratus was introduced for 2001, with the Cirrus being renamed as the Chrysler Sebring, and a coupé model was also added to the range. However, production ended at the Sterling Heights Assembly Plant in early 2006 which had built 1,308,123 Stratus and Sebrings since 2000. The Dodge Avenger replaced the Stratus nameplate in early 2007 for the 2008 model year.\n\nAfter the discontinuation of the Stratus sedan in 2006, the assembly line and tooling were sold to the Russian concern, GAZ, which manufactured 9,000 examples of a very slightly modified Stratus from 2008 through 2010 called the Volga Siber.\n\nThe Dodge Stratus was the middle entry of the Chrysler JA platform (with the Cirrus being the higher-end model and the Breeze being the lower-end model). The three cars differed only in the front fascia, rear bumper, taillights, and wheels. The interiors also had little variation between the three models; being almost identical, save for the name on the steering wheel, and a few available options. The Stratus directly replaced the high-volume Spirit (United States only)\n\nIn Mexico, since 1996 to 2000 the Stratus R/T came in a turbocharged version (also the Cirrus). The Stratus R/T's turbocharged EDZ 2.4L engine went through some improvements in 1996 when power was increased to 168 hp's @2,200. All 2.4L turbo engines was only for the Mexican market.\nThe car competed in the Swedish Touring Car Championship under the name Chrysler Stratus.\n\nIn 2000, the Stratus became the last of the surviving Cloud Cars, with the Cirrus renamed as the Sebring, and the Breeze discontinued (along with the Plymouth brand). This generation of the Dodge Stratus was not sold in Canada, although 1999 was the last year for Dodge Stratus sales in Canada. 2002 models dropped the \"DODGE\" badges from the doors.\n\nThe Stratus and Sebring sedans for the second generation used a revised version of the Chrysler JA platform named JR. The coupe models with the same names were entirely different cars; they were actually based on the Mitsubishi Eclipse.\n\nDuring this time, sales declined as its ratings from consumer and auto magazines fell below average among mid-size cars, while the sedan market had shifted and pushed the larger Intrepid and later Charger to record sales. 2004 brought styling revisions, which did not reverse this trend. The Stratus was discontinued in May 2006 (the Sebring name was continued).\n\nIn Mexico, the Stratus R/T came in a turbocharged version. The Stratus R/T's turbocharged 2.4 L engine went through some improvements in 2001, when power was increased to . This improved engine would later be used in the U.S. in the Dodge SRT-4 and PT Cruiser GT. Stratus R/T engines built from March 2004 and later generated at 5200 rpm and of torque at 4200 rpm. Stratus R/T models with the turbocharged engine could be recognized by a rear badge saying \"Turbo\".\n\n\nFor 2001, Dodge introduced the Stratus coupe, replacing the discontinued Avenger. This model along with the Chrysler Sebring coupe was built at the former Diamond Star Motors plant by Mitsubishi, using the ST-22 platform. Like its Chrysler counterpart, the coupe models shared very little other than the name and a few exterior styling cues with sedan and convertible models. The Stratus coupe was restyled for the 2003 model year. The coupe was discontinued after 2005, one year before the sedan. The next midsize Dodge, the Avenger, did not include a coupe version.\n\n\nThe first generation Dodge Stratus received a \"Poor\" rating in the IIHS frontal crash test. It was actually a Chrysler Cirrus that was tested, but the results also apply to the Stratus, and also the Plymouth Breeze. The second generation Stratus and its twin, the Chrysler Sebring, received an overall \"Acceptable\" rating in the IIHS frontal test due to a possible injury to the right leg. On the side test, the Stratus receives a \"Poor\" rating without optional side airbags due to a serious neck injury, a weak side structure, possible rib fractures, and high forces on the shoulder and pelvis. Its seats and head restraints earn an overall \"Acceptable\" rating from the IIHS. \n\nThe license and production facilities for the second generation Dodge Stratus and Chrysler Sebring sedans were sold in April 2006 to Russian billionaire Oleg Deripaska, owner the GAZ company in Nizhny Novgorod, which builds the Volga automobile. The models were built by GAZ in Russia from late 2007 through 2010 as the Volga Siber at a price of approximately US$151 million (€ 124 million). The production facilities are planned to build up to 65,000 cars of both models yearly. Four-cylinder engines were to be purchased from Chrysler and made in Mexico.\n"}
{"id": "11700383", "url": "https://en.wikipedia.org/wiki?curid=11700383", "title": "Enerhodar Dnipro Powerline Crossing", "text": "Enerhodar Dnipro Powerline Crossing\n\nEnerhodar Dnipro Powerline Crossing is the designation of two powerline crossings of the Kakhovka Reservoir on the Dnipro river near Enerhodar. \n\nThe first powerline, which was built in 1977 and runs from Zaporizhzhia Thermal Power Plant, consists of five 100 metres and two 90 metres high pylons, which were built on caissons in the water. The caissons with the pylons were prefabricated and then erected in the reservoir.\n\nIn 1984 a 750 kV powerline with a single circuit for transport of energy produced in the Zaporizhzhia Nuclear Power Plant was built. It consists of three 126 metres and two 100 metres high pylons, which stand also on caissons in Kahovskoe reservoir of the Dnipro river.\n\n\n \n"}
{"id": "11972837", "url": "https://en.wikipedia.org/wiki?curid=11972837", "title": "Ferroelasticity", "text": "Ferroelasticity\n\nFerroelasticity is a phenomenon in which a material may exhibit a spontaneous strain. In ferroics, ferroelasticity is the mechanical equivalent of ferroelectricity and ferromagnetism. When stress is applied to a ferroelastic material, a phase change will occur in the material from one phase to an equally stable phase, either of different crystal structure (e.g. cubic to tetragonal), or of different orientation (a 'twin' phase). This stress-induced phase change results in a spontaneous strain in the material.\n\nThe shape memory effect and superelasticity are manifestations of ferroelasticity. Nitinol (nickel titanium), a common ferroelastic alloy, can display either superelasticity or the shape - memory effect at room temperature, depending on the nickel / titanium ratio.\n\n\n"}
{"id": "19422025", "url": "https://en.wikipedia.org/wiki?curid=19422025", "title": "Fiber volume ratio", "text": "Fiber volume ratio\n\nFiber volume ratio is an important mathematical element in composite engineering. Fiber volume ratio, or fiber volume fraction, is the percentage of fiber volume in the entire volume of a fiber-reinforced composite material. When manufacturing polymer composites, fibers are impregnated with resin. The amount of resin to fiber ratio is calculated by the geometric organization of the fibers, which affects the amount of resin that can enter the composite. The impregnation around the fibers is highly dependent on the orientation of the fibers and the architecture of the fibers. The geometric analysis of the composite can be seen in the cross-section of the composite. Voids are often formed in a composite structure throughout the manufacturing process and must be calculated into the total fiber volume fraction of the composite. The fraction of fiber reinforcement is very important in determining the overall mechanical properties of a composite. A higher fiber volume fraction typically results in better mechanical properties of the composite.\n\nCalculating the volume of fiber ratio in a composite is relatively simple. The volume fiber fraction can be calculated using a combination of weights, densities, elastic moduli, stresses in respective directions, poison's ratios, and volumes of the matrix (resin system), fibers, and voids.\n\nformula_1 \n\nwhere:\n\nand\n\nThis procedure involves the digestion of the polymer matrix using an acid which does not attack\nthe fibers. Following digestion, the remaining fibers are washed, dried, and weighed. Knowing the initial weight of the composite specimen as well as the densities of the fiber and resin, the volume fraction of both the fiber and matrix in the original laminate may be determined. This method is generally used for composites composed of carbon fiber reinforcement.\n\nOptical microscopy-based techniques involve potting sectioned samples of the laminate, polished using standard metallographic techniques, and obtaining digital cross-sectional photomicrographs using an optical microscope and magnifications between 100 and 2500. Digital images may be recorded at a number of locations along the length and through-the-thickness of the laminate. Computer programs aid in the analysis of fiber ratio in the photomicrograph of the polished composite specimen. This method is preferred as a non-destructive approach to determining fiber volume fraction.\n\nThis method involves heating up the composite to a temperature at which resin will melt and fibers remain stable, burning off resin and weighing fibers, the volume fraction can be calculated from the initial weight of composite and fiber’s weight. This method is typically used with glass fibers.\n\nThe amount of fiber in a fiber reinforced composite directly corresponds with the mechanical properties of the composite. Theoretically the maximum fiber ratio of round fibers that can be achieved in a composite is 90.8% if the fibers are in a unidirectional hexagonal close packed configuration. Realistically the highest fiber volume ratio is around 70% due to manufacturing parameters and is usually in the range of 50% to 65%. Adding too little fiber reinforcement in the composite will actually deteriorate the properties of the material. Too much fiber volume may also decrease the strength of the composite due to the lack of space for the matrix to fully surround and bond with the fibers. Therefore, there is an optimal space between fibers that will fully exploit the uniform load transfer between fibers. Given the fiber volume fraction, the theoretical elastic properties of a composite can be determined. The elastic modulus of a composite in the fiber direction of a unidirectional composite can be calculated using the following equation:\nformula_5\n\nWhere:\n\nand\n\nFibers are commonly arranged in a square or hexagonal lattice as well as in a layer-wise fiber array.\nAssuming that each fiber has a circular cross-sectional with the same diameter, the fiber volume fraction of these two kinds of packing are respectively:\n\nHexagonal \n\nformula_9\n\nSquare\n\nformula_10\n\nwhere:\n\nand \n\nThe maximum fiber volume fraction will occur when the fibers are touching, i.e. r=R. For a hexagonal array formula_13 = 0.907, and for square packing formula_13 = 0.785.\n\nHowever, these are ideal situations only used for theoretical analysis. In practical cases there can be variation in fiber diameter and irregular packing. In practice, it’s hard to achieve a volume fraction greater than 0.7 and this must be regarded as the realistic limit for commercial materials.\n\nIn the production process, using different fiber architecture methods can obtain different volume fractions. 2D aligned unidirectional fabrics with pre-preg (usually carbon) fibers are considered to have the highest volume fraction among common fiber architectures. Filament winding is also usually associated with high fiber volume fractions – with careful control of fiber tension and resin content, values of around 70% are possible.\n\nPorosity or void fraction is a measure of the void (i.e., \"empty\") spaces in a material, and is a fraction of the volume of voids over the total volume, between 0 and 1, or as a percentage between 0 and 100%. There are many ways to determine if a composite part contains voids, such as industrial CT scanning or ultrasound. If the volume fraction of the fibers and matrix is known, the volume voids can also be found using the following equation:\n\nformula_15 \n\nwhere:\n\nand\n\nAnother equation used to calculate void volume fraction is:\n\nformula_21\n\nwhere:\n\nand\n\nThere are many methods of evaluating the void content of materials (including composites). The first is to exam a polished section, identifying the voids in the section, either manually or using computer-aid analysis and determining the area fraction which corresponds to the volume fraction of the composite.\n\nAnother method requires accurate measurement of the density of the sample and comparing it to the theoretical density in the equation described in the previous section. The density is determined by weighing the sample in air and then in a liquid of known density. Application of Archimedes’ principle leads to the following expression for the measured density of the sample in terms of measured weight, where subscripts “a” and “L” refer to air and liquid,respectively:\n\nformula_25 \n\nWhere:\n\nand\n\nThe liquid used in this method should have a high density and chemical stability and low vapour pressure and surface tension. The most popular liquid currently in use is perfluoro- 1 - methyl decalin.\n\n"}
{"id": "2796239", "url": "https://en.wikipedia.org/wiki?curid=2796239", "title": "Full depth recycling", "text": "Full depth recycling\n\nFull depth recycling or full depth reclamation (FDR), is a process that rebuilds worn out asphalt pavements by recycling the existing roadway.\n\nOld asphalt and base materials are pulverized using a specialized machine called a reclaimer. On top of the pulverized material, water is added to reach the optimal moisture content for compaction and then a variety of materials, such as dry cement, lime, fly ash, or asphalt emulsion are incorporated for stabilization. A reclaimer is used again to mix all the materials. After shaping and grading, the new base is compacted to produce a strong, durable base for either an asphalt or concrete surface.\n\nSince this method recycles the materials \"in situ\", there is no need to haul in aggregate or haul out old material for disposal. The vehicle movements are reduced and there is no need for detours since it can be done under traffic, making this process more convenient for local residents.\n\nFDR with cement saves money while preserving natural resources by using existing materials and conserving virgin aggregates. The road performance is improved through better stabilization, building a stronger, low-maintenance road that will last for many years.\n\nWith proper engineering and testing protocols the FDR process provides a design life-cycle of 30-years. FDR is a manufacturing process and not an installation. Other pavement materials, such as concrete, asphalt, or aggregate base go through a rigorous quality control program that meets a qualified standard prior to site delivery and contractor installation. The FDR process requires the same level of understanding and product controls during lab testing and field verification to meet long-term performance goals.\n\n\n"}
{"id": "19984549", "url": "https://en.wikipedia.org/wiki?curid=19984549", "title": "Gellénháza Power Plant", "text": "Gellénháza Power Plant\n\nThe Gellénháza Power Plant will be one of Hungary's largest biomass power plants having an installed electric capacity of 142 MW.\n"}
{"id": "23149866", "url": "https://en.wikipedia.org/wiki?curid=23149866", "title": "Gold universe", "text": "Gold universe\n\nA Gold universe is a cosmological model of the universe. In these models, the universe starts with a Big Bang and expands for some time, with increasing entropy and a thermodynamic arrow of time pointing in the direction of the expansion. After the universe reaches a low-density state, it recontracts, but entropy now decreases, pointing the thermodynamic arrow of time in the opposite direction, until the universe ends in a low-entropy, high-density Big Crunch. \nThere are two models of the universe which support the possibility of a reversed direction of time. The first begins with a state of low entropy at the Big Bang which continually increases until the Big Crunch. The second, a Gold Universe, posits that entropy will increase only until a moment of contraction, then gradually decrease. This latter model suggests the universe will become more orderly after the moment of contraction. The Gold model has been linked to the possibility of retrocausal change, questions concerning the preservation of information in a time-reversed universe (states of decreasing entropy), and causation in general. The Gold Universe is named after the cosmologist Thomas Gold, who proposed the model in the 1960s.\n"}
{"id": "1567358", "url": "https://en.wikipedia.org/wiki?curid=1567358", "title": "Gustnado", "text": "Gustnado\n\nA gustnado is a short-lived, shallow surface-based vortex which forms within the downburst emanating from a thunderstorm. The name is a portmanteau of \"gust front tornado\", as gustnadoes form due to non-tornadic straight-line wind features in the downdraft (outflow), specifically within the gust front of strong thunderstorms. Gustnadoes tend to be noticed when the vortices loft sufficient debris or form condensation cloud to be visible although it is the wind that makes the gustnado, similarly to tornadoes. As these eddies very rarely connect from the surface to the cloud base, they are very rarely considered as tornadoes. The gustnado has little in common with tornadoes structurally or dynamically in regard to vertical development, intensity, longevity, or formative process --as classic tornadoes are associated with mesocyclones within the inflow (updraft) of the storm, not the outflow.\nThe average gustnado lasts a few seconds to a few minutes, although there can be several generations and simultaneous swarms. Most have the winds of an EF-0 or EF-1 tornado (up to 110 mph or 177 km/h), and are commonly mistaken for tornadoes. However, unlike tornadoes, the rotating column of air in a gustnado \"usually\" does not extend all the way to the base of the thundercloud. Gustnadoes actually have more in common with whirlwinds. They are not considered true tornadoes (unless they connect the surface to the ambient cloud base) by most meteorologists and are not included in tornado statistics in most areas. Sometimes referred to as spin-up tornadoes, that term more correctly describes the rare tornadic gustnado that connects the surface to the ambient clouded base, or more commonly to the relatively brief but true tornadoes that are associated with a mesovortex. \n\nThe most common setting for a gustnado is along the gust front of a severe thunderstorm (more than winds), along which horizontal shear of the wind may be large. A particularly common location is along the rear-flank gust front of supercell storms. Gustnadoes probably form owing to shear instability associated with the strong horizontal shear; a relative maximum in vertical vorticity must exist in order for shear instability to be present. The bigger question is probably what the dynamical origin(s) of the vertical vorticity is (are), such as the tilting of horizontal vorticity into the vertical or vertical vorticity in the ambient environment that preexists the storm. Along the rear-flank gust front of supercell storms, vertical vorticity very likely has its origins in the upward tilting of vorticity that can occur within descending air in the presence of baroclinity. \n\nWhile injuries or deaths are rare from gustnadoes, strong ones can cause damage and they are hazardous to drivers. There is some speculation that a gustnado might have been responsible for the collapse of a stage at the Indiana State Fair on August 13, 2011 which killed 7 people and injured 58.\n\n\n\n"}
{"id": "1131151", "url": "https://en.wikipedia.org/wiki?curid=1131151", "title": "Heliosphere", "text": "Heliosphere\n\nThe heliosphere is the bubble-like region of space dominated by the Sun, which extends far beyond the orbit of Pluto. Plasma \"blown\" out from the Sun, known as the solar wind, creates and maintains this bubble against the outside pressure of the interstellar medium, the hydrogen and helium gas that permeates the Milky Way Galaxy. The solar wind flows outward from the Sun until encountering the termination shock, where motion slows abruptly. The Voyager spacecraft have explored the outer reaches of the heliosphere, passing through the shock and entering the heliosheath, a transitional region which is in turn bounded by the outermost edge of the heliosphere, called the heliopause. The shape of the heliosphere is controlled by the interstellar medium through which it is traveling, as well as the Sun and is not perfectly spherical. The limited data available and unexplored nature of these structures have resulted in many theories. The word \"heliosphere\" is said to have been coined by Alexander J. Dessler, who is credited with first use of the word in the scientific literature.\n\nOn September 12, 2013, NASA announced that Voyager 1 left the heliopause on August 25, 2012, when it measured a sudden increase in plasma density of about forty times. Because the heliopause marks one boundary between the Sun's solar wind and the rest of the galaxy, a spacecraft such as Voyager 1 which has departed the heliosphere, can be said to have reached interstellar space.\n\nThe idea of Heliosphere is an area of the influence of the Sun, and a major component is magnetic field lines and the solar wind; three major aspects to its edge are the termination shock, the heliosheath, and the heliopause. Five spacecraft have returned much of the data about its furthest reaches including Pioneer 10 (1972-1997; data to 67 AU), Pioneer 11 (1973-1995;44 AU), Voyager 1 and 2 (launched 1977, ongoing), and New Horizons (Launched 2006). A type of particle called an ENA has also been observed to have been produced from its edges. Near the Sun, there is a host of spacecraft that observe the Sun and interplanetary space, and one of the latest to explore nearer to the Sun than ever before is the Parker Solar Probe launched in 2018. Solar observations, such by Solar eclipses that allow observation of the Corona and various types of space observatories also provide data on the Sun and its influences. The study of other Stars can also allow insights indirectly.\n\nExcept for regions near obstacles such as planets or comets, the heliosphere is dominated by material emanating from the Sun, although cosmic rays, fast-moving neutral atoms, and cosmic dust can penetrate the heliosphere from the outside. Originating at the extremely hot surface of the corona, solar wind particles reach escape velocity, streaming outwards at 300 to 800 km/s (671 thousand to 1.79 million mph or 1 to 2.9 million km/h). As it begins to interact with the interstellar medium, its velocity slows to a stop. The point where the solar wind becomes slower than the speed of sound is called the termination shock; the solar wind continues to slow as it passes through the heliosheath leading to a boundary called the heliopause, where the interstellar medium and solar wind pressures balance. The termination shock was traversed by Voyager 1 in 2004, and Voyager 2 in 2007.\n\nIt was thought that beyond the heliopause there was a bow shock, but data from Interstellar Boundary Explorer suggested the velocity of the Sun through the interstellar medium is too low for it to form. It may be a more gentle \"bow wave\". Voyager data led to a new theory that the heliosheath has \"magnetic bubbles\" and a stagnation zone.\n\nThe 'stagnation region' within the heliosheath, starting around , was detected by Voyager 1 in 2010. There the solar wind velocity drops to zero, the magnetic field intensity doubles and high-energy electrons from the galaxy increase 100-fold. Starting in May 2012 at , Voyager 1 detected a sudden increase in cosmic rays, an apparent signature of approach to the heliopause. In December 2012 NASA announced that in late August 2012 Voyager 1, at about from the Sun, entered a new region they called the \"magnetic highway\", an area still under the influence of the Sun but with some dramatic differences. In the summer of 2013, NASA announced that Voyager 1 had reached interstellar space as of August 25, 2012.\n\nCassini and IBEX data challenged the \"heliotail\" theory in 2009. In July 2013, IBEX results revealed a 4-lobed tail on the Solar System's heliosphere.\n\nPioneer 10 was launched in March 1972, and within 10 hours passed by the Moon; over the next 35 years or so the mission would be the first out laying out many firsts of discoveries about the nature of heliosphere as well as Jupiter's impact on it. Pioneer 10 was the first spacecraft to detect sodium and aluminum ions in the Solar Wind, as well as helium in the inner solar system. In November of 1972, Pioneer 10 encountered Jupiter's enormous (compared to Earth) magnetosphere, and would pass in and out of it and heliosphere 17 times charting its interaction with the Solar Wind. Pioneer 10 returned scientific data until March 1997, including data on the Solar wind out to about 67 AU at that time. It was also contacted in 2003, when it was a distance of 7.6 billion miles from Earth (82 AU), but no instrument data about the Solar Wind was returned then. \n\nVoyager 1, surpassed the radial distance from Sun of Pioneer 10 at 69.4 AU on 17 February 1998, because it was traveling faster gaining about 1.02 AU per year. Pioneer 11, launched a year after Pioneer 10, took similar data as Pioneer out to 44.7 AU in 1995 when that mission was concluded. Pioneer 11 had a similar instrument suite as 10, but also had a Flux-Gate Magnetometer. Pioneer and Voyager spacecraft were on different trajectories, and thus recorded data on the heliosphere in different overall directions away from the Sun. Pioneer and Voyager data helped corroborate the detection of a hydrogen wall. \n\nVoyager 1 and 2 were launched in 1977 and operated continuously to at least the late 2010s, and encountered various aspects of the Heliosphere past Pluto and Neptune. In 2012 Voyager 1 is thought to have pass through the Heliopause, and in 2018 there is some indications Voyager 2 may be experiencing the same threshold.\n\nThe solar wind consists of particles (ionized atoms from the solar corona) and fields (in particular, magnetic fields). Because the Sun rotates once approximately every 25 days, the magnetic field transported by the solar wind gets wrapped into a spiral. Variations in the Sun's magnetic field are carried outward by the solar wind and can produce magnetic storms in the Earth's own magnetosphere.\n\nThe heliospheric current sheet is a ripple in the heliosphere created by the rotating magnetic field of the Sun. Extending throughout the heliosphere, it is considered the largest structure in the Solar System and is said to resemble a \"ballerina's skirt\".\n\nThe outer structure of the heliosphere is determined by the interactions between the solar wind and the winds of interstellar space. The solar wind streams away from the Sun in all directions at speeds of several hundred km/s in the Earth's vicinity. At some distance from the Sun, well beyond the orbit of Neptune, this supersonic wind must slow down to meet the gases in the interstellar medium. This takes place in several stages:\n\nThe termination shock is the point in the heliosphere where the solar wind slows down to subsonic speed (relative to the Sun) because of interactions with the local interstellar medium. This causes compression, heating, and a change in the magnetic field. In the Solar System the termination shock is believed to be 75 to 90 astronomical units from the Sun. In 2004, Voyager 1 crossed the Sun's termination shock followed by Voyager 2 in 2007.\n\nThe shock arises because solar wind particles are emitted from the Sun at about 400 km/s, while the speed of sound (in the interstellar medium) is about 100 km/s. (The exact speed depends on the density, which fluctuates considerably.) The interstellar medium, although very low in density, nonetheless has a constant pressure associated with it; the pressure from the solar wind decreases with the square of the distance from the Sun. As one moves far enough away from the Sun, the pressure of the solar wind drops to where it can no longer maintain supersonic flow against the pressure of the interstellar medium, at which point the solar wind slows to below its speed of sound, causing a shock wave. Further from the Sun, the termination shock is followed by the heliopause, where the two pressures become equal and solar wind particles are stopped by the interstellar medium.\n\nOther termination shocks can be seen in terrestrial systems; perhaps the easiest may be seen by simply running a water tap into a sink creating a hydraulic jump. Upon hitting the floor of the sink, the flowing water spreads out at a speed that is higher than the local wave speed, forming a disk of shallow, rapidly diverging flow (analogous to the tenuous, supersonic solar wind). Around the periphery of the disk, a shock front or wall of water forms; outside the shock front, the water moves slower than the local wave speed (analogous to the subsonic interstellar medium).\n\nEvidence presented at a meeting of the American Geophysical Union in May 2005 by Ed Stone suggests that the \"Voyager 1\" spacecraft passed the termination shock in December 2004, when it was about 94 AU from the Sun, by virtue of the change in magnetic readings taken from the craft. In contrast, \"Voyager 2\" began detecting returning particles when it was only 76 AU from the Sun, in May 2006. This implies that the heliosphere may be irregularly shaped, bulging outwards in the Sun's northern hemisphere and pushed inward in the south.\n\nThe heliosheath is the region of the heliosphere beyond the termination shock. Here the wind is slowed, compressed and made turbulent by its interaction with the interstellar medium. Its distance from the Sun is approximately 80 to 100 astronomical units (AU) at its closest point.\n\nA proposed model hypothesizes that the heliosheath is shaped like the coma of a comet, and trails several times that distance in the direction opposite to the Sun's path through space. At its windward side, its thickness is estimated to be between 10 and 100 AU. However, observations in 2009 showed that model may be incorrect.\n\nThe \"Voyager 1\" and \"Voyager 2\" spacecraft have studied the heliosheath. In late 2010, \"Voyager 1\" reached a region of the heliosheath where the solar wind's velocity had dropped to zero. In 2011, astronomers announced that the \"Voyagers\" had determined that the heliosheath is not smooth, but is filled with 100 million-mile-wide bubbles created by the impact of the solar wind and the interstellar medium. \"Voyager 1\" and \"2\" began detecting evidence for the bubbles in 2007 and 2008, respectively. The probably sausage-shaped bubbles are formed by magnetic reconnection between oppositely oriented sectors of the solar magnetic field as the solar wind slows down. They probably represent self-contained structures that have detached from the interplanetary magnetic field.\n\nThe heliopause is the theoretical boundary where the Sun's solar wind is stopped by the interstellar medium; where the solar wind's strength is no longer great enough to push back the stellar winds of the surrounding stars. This is the boundary where the interstellar medium and solar wind pressures balance. The crossing of the heliopause should be signaled by a sharp drop in the temperature of charged particles, a change in the direction of the magnetic field, and an increase in the number of galactic cosmic rays. In May 2012, \"Voyager 1\" detected a rapid increase in such cosmic rays (a 9% increase in a month, following a more gradual increase of 25% from Jan. 2009 to Jan. 2012), suggesting it was approaching the heliopause. In September of 2013, NASA announced that Voyager 1 had crossed the heliopause as of August 25, 2012. This was at a distance of 121 AU (18 billion km) from the Sun. Contrary to predictions, data from Voyager 1 indicates the magnetic field of the galaxy is aligned with the solar magnetic field.\n\nThe heliotail is the tail of the heliosphere, and thus the solar system's tail. It can be compared to the tail of a comet (however, a comet's tail does not stretch behind it as it moves; it is always pointing away from the Sun). The tail is a region where the Sun's solar wind slows down and ultimately escapes the heliosphere, slowly evaporating because of charge exchange.\nThe shape of the heliotail (newly found by NASA's Interstellar Boundary Explorer - IBEX), is that of a four-leaf clover. The particles in the tail do not shine, therefore it cannot be seen with conventional optical instruments. IBEX made the first observations of the heliotail by measuring the energy of \"energetic neutral atoms\", neutral particles created by collisions in the solar system's boundary zone.\n\nThe tail has been shown to contain fast and slow particles; the slow particles are on the side and the fast particles are encompassed in the center. The shape of the tail can be linked to the sun sending out fast solar winds near its poles and slow solar wind near its equator more recently. The clover-shaped tail moves further away from the sun, which makes the charged particles begin to morph into a new orientation.\n\nThe heliopause is the final known boundary between the heliosphere and the interstellar space that is filled with material, especially plasma, not from our own star, the Sun, but from other stars. Even so, just outside the heliosphere (i.e. the \"solar bubble\") there is a transitional region, as detected by Voyager 1. Just as some interstellar pressure was detected as early as 2004, some of the Sun's material seeps into the interstellar medium. The heliosphere is thought to reside in the Local Interstellar Cloud inside the Local Bubble, which is a region in the Orion Arm of the Milky Way Galaxy.\n\nOutside the heliosphere there is a forty-fold increase in plasma density. There is also a radical reduction in the detection of certain types of particles from the Sun, and a large increase in Galactic cosmic rays.\n\nThe flow of the interstellar medium (ISM) into the heliosphere has been measured by at least 11 different spacecraft as of 2013. By 2013, it was suspected that the direction of the flow had changed over time. The flow, coming from Earth's perspective from the constellation Scorpius, has probably changed direction by several degrees since the 1970s.\n\nAccording to one hypothesis, there exists a region of hot hydrogen known as the hydrogen wall between the bow shock and the heliopause. The wall is composed of interstellar material interacting with the edge of the heliosphere. One paper released in 2013 studied the concept of a bow wave and hydrogen wall.\n\nAnother hypothesis suggests that the heliopause could be smaller on the side of the Solar System facing the Sun's orbital motion through the galaxy. It may also vary depending on the current velocity of the solar wind and the local density of the interstellar medium. It is known to lie far outside the orbit of Neptune. The current mission of the \"Voyager 1\" and \"2\" spacecraft is to find and study the termination shock, heliosheath, and heliopause. Meanwhile, the Interstellar Boundary Explorer (IBEX) mission is attempting to image the heliopause from Earth orbit within two years of its 2008 launch. Initial results (October 2009) from IBEX suggest that previous assumptions are insufficiently cognisant of the true complexities of the heliopause.\n\nIn August 2018, long-term studies about the hydrogen wall by the \"New Horizons\" spacecraft confirmed results first detected in 1992 by the two \"Voyager\" spacecrafts. The hydrogen is being detected by extra ultraviolet light, which is possible may come from another source, but the detection by \"New Horizons\" corroborates the earlier detections by Voyager at a much higher level of sensitivity.\n\nIt was long hypothesized that the Sun produces a \"shock wave\" in its travels within the interstellar medium. It would occur if the interstellar medium is moving supersonically \"toward\" the Sun, since its solar wind moves \"away\" from the Sun supersonically. When the interstellar wind hits the heliosphere it slows and creates a region of turbulence. A bow shock was thought to possibly occur at about 230 AU, but in 2012 it was determined it probably does not exist. This conclusion resulted from new measurements: The velocity of the LISM (Local Interstellar Medium) relative to the Sun's was previously measured to be 26.3 km/s by Ulysses, whereas IBEX measured it at 23.2 km/s.\n\nThis phenomenon has been observed outside the Solar System, around stars other than the Sun, by NASA's now retired orbital GALEX telescope. The red giant star Mira in the constellation Cetus has been shown to have both a debris tail of ejecta from the star and a distinct shock in the direction of its movement through space (at over 130 kilometers per second).\n\nThe precise distance to, and shape of the heliopause is still uncertain. Interplanetary/interstellar spacecraft such as \"Pioneer 10\", \"Pioneer 11\", New Horizons and \"Voyager 2\" are traveling outward through the Solar System and will eventually pass through the heliopause. Contact to Pioneer 10 and 11 has been lost.\n\nRather than a comet-like shape, the heliosphere appears to be bubble-shaped according to data from Cassini's Ion and Neutral Camera (MIMI / INCA). Rather than being dominated by the collisions between the solar wind and the interstellar medium, the INCA (ENA) maps suggest that the interaction is controlled more by particle pressure and magnetic field energy density.\n\nInitial data from Interstellar Boundary Explorer (IBEX), launched in October 2008, revealed a previously unpredicted \"very narrow ribbon that is two to three times brighter than anything else in the sky.\" Initial interpretations suggest that \"the interstellar environment has far more influence on structuring the heliosphere than anyone previously believed\"\n\"No one knows what is creating the ENA (energetic neutral atoms) ribbon, ...\"\n\n\"The IBEX results are truly remarkable! What we are seeing in these maps does not match with any of the previous theoretical models of this region. It will be exciting for scientists to review these (ENA) maps and revise the way we understand our heliosphere and how it interacts with the galaxy.\" In October 2010, significant changes were detected in the ribbon after 6 months, based on the second set of IBEX observations. IBEX data did not support the existence of a bow shock, but there might be a 'bow wave' according to one study.\n\nOf particular interest is the Earth's interaction with the heliosphere, but its extent and interaction with other bodies in the solar system have also been studied.\nSome examples of missions that have or continue to collect data related to the heliosphere include \"(see also List of heliophysics missions)\":\n\nDuring a total eclipse the high-temperature corona can be more readily observed from Earth solar observatories. During the Apollo program the Solar wind was measured on the Moon via the Solar Wind Composition Experiment. Some examples of Earth surface based Solar observatories include the McMath–Pierce solar telescope or the newer GREGOR Solar Telescope, and the refurbished Big Bear Solar Observatory. (see also [[List of solar telescopes])\n\n\nThese depictions include features that may not reflect most recent models.\n\n\n\n[[Category:Articles containing video clips]]\n[[Category:Local Interstellar Cloud]]\n[[Category:Plasma physics]]\n[[Category:Space plasmas]]\n[[Category:Sun|Sphere]]\n[[Category:Trans-Neptunian region]]\n[[Category:Voyager program]]"}
{"id": "12563713", "url": "https://en.wikipedia.org/wiki?curid=12563713", "title": "High production volume chemicals", "text": "High production volume chemicals\n\nHigh production volume chemicals, also referred to as HPV chemicals, are produced or imported into the United States in quantities of 1 million pounds or 500 tons per year. In OECD countries, HPV chemicals are defined as being produced at levels greater than 1,000 metric tons per producer/importer per year in at least one member country/region. A list of HPV chemicals serves as an overall priority list, from which chemicals are selected to gather data for a screening information dataset (SIDS), for testing and for initial hazard assessment.\n\nIn 1987, member countries of the Organisation for Economic Co-operation and Development decided to investigate existing chemicals. In 1991, they agreed to begin by focusing on High production volume (HPV) chemicals, where production volume was used as a surrogate for data on occupational, consumer, and environmental exposure. Each country agreed to \"sponsor\" the assessment of a proportion of the HPV chemicals. Countries also agreed on a minimum set of required information, the screening information dataset (SIDS). Six tests are: acute toxicity, chronic toxicity, developmental toxicity/reproductive toxicity, mutagenicity, ecotoxicity and environmental fate. Using SIDS and detailed exposure data OECD's High Production Volume Chemicals Programme conducted initial risk assessments to screen and to identify any need for further work.\n\nDuring the late 1990s, OECD member countries began to assess chemical categories and to use quantitative structure–activity relationship (QSAR) results to create OECD guidance documents, as well as a computerized QSAR toolbox. In 1998, the global chemical industry, organized in the International Council of Chemical Associations (ICCA) initiative, offered to join OECD efforts. The ICCA promised to sponsor by 2013 about 1,000 substances from the OECD's HPV chemicals list \"to establish as priorities for investigation\", based on \"presumed wide dispersive use, production in two or more global regions or similarity to another chemical, which met either of these criteria\". OECD in turn agreed to refocus and to \"increase transparency, efficiency and productivity and allow longer-term planning for governments and industry\". The OECD refocus was on initial hazard assessments of HPV chemicals only, and no longer extensive exposure information gathering and evaluation. Detailed exposure assessments within national (or regional) programmes and priority setting activities were postponed as post-SIDS work.\n\nOn October 9, 1998, EPA Administrator Carol Browner sent letters to the CEOs of more than 900 chemical companies that manufacture HPV chemicals, asking them to participate in EPA's voluntary testing initiative, the so-called \"HPV Challenge Program\". The Environmental Defense Fund, the American Petroleum Institute, and American Chemistry Council joined in the effort.\n\nThe OECD list of HPV chemicals keeps changing. A 2004 list of 143 pages contained 4,842 entries. A 2007 list was published in 2009.\n\nThe EPA has published an online list of HPV chemicals since 2010. The list is not numerated and without footnotes.\n\nThe \"Strategic Approach to International Chemicals Management\" (SAICM) is a policy for achieving safe production and use of chemicals worldwide by 2020, developed with stakeholders from more than 140 countries, signed by 100 governments, adopted by the UNEP Governing Council in February 2006. \nThe Registration, Evaluation, Authorisation and Restriction of Chemicals (REACH) proposal and the European Chemicals Agency will help the EU to fulfill objectives of SAICM.\nThe Stockholm Convention on Persistent Organic Pollutants has aimed to control production, use, trade, disposal and release of twelve Persistent organic pollutants (POPs); the European Community has proposed five additional chemicals. The Convention bans deliberate production and use of POPs, bans the development of new POPs, and aims at minimizing releases of unintentionally produced POPs. The Convention has so far been ratified by the European Community, 18 member states and the two accession countries.\n\nThe 1976 Toxic Substances Control Act requires the EPA to \"compile, keep current, and publish a list of each chemical substance that is manufactured or processed in the United States\". In 1998, the EPA reported the most heavily used HPV chemicals in commerce were largely untested: 43% of 2,800 HPV chemicals had no basic toxicity data or screening level data at all, 50% had incomplete screening data, and only 7% of the HPV chemicals had a complete set of screening level toxicity data. However, screening level data, even if they indicated a problem, were not sufficient to restrict the use of a compound. \nIn 1986, 2003, 2005, and in 2011 EPA issued regulations to amend and update the TSCA inventory.\n\nAs of April 2010, about 84,000 chemicals were on the TSCA inventory, per a GAO report. TSCA Section 4 gives EPA the authority to demand chemical testing.\n\nIn 1982, U.S. manufacturers, processors, and importers of 75 chemicals that the International Agency for Research on Cancer had found to cause cancers in animals, but the carcinogenicity of which in humans was uncertain, were surveyed. Only for 13 of the 75 chemicals had epidemiologic studies on human health been completed or were in progress. Eighteen of the 75 were HPV chemicals and only for eight HPV chemicals had epidemiologic studies been completed or were in progress. The largest number of chemicals (19) were drugs, and none of them had been epidemiologically studied. Seven chemicals that had been studied were used as pesticides.\n\nIn 1997 the Environmental Defense Fund reported in “Toxic Ignorance” results of its analysis of the availability of basic health test data on HPV chemicals that only 29% of the HPV chemicals in the US met minimum data requirements.\nIn 1998 the EPA published a report CHEMICAL HAZARD DATA AVAILABILITY STUDY showing \"55% of TRI chemicals have had full SIDS testing, while only 7% of other chemicals have full test data\". They wrote \n\"...of the 830 companies making HPV chemicals in the US, 148 companies have NO SIDS data available on their chemicals; an additional 459 companies sell products for which, on average, half or less of SIDS tests are available. Only 21 companies (or 3% of the 830 companies) have all SIDS tests available for their chemicals. The basic set of test data costs about $200,000 per chemical.\"\nIn 1999, the European Union (EU) published a study about how many EU-HPV chemicals were publicly available in a comprehensive chemical data base called IUCLID: Only 14% of the EU-HPV chemicals had data at the level of the base-set, 65% had less than base-set, and 21% had no data available. The authors concluded, \"more data [were] publicly available than most previous studies\" had shown.\n\nIn 2004, one of the partners in EPA's HPV Challenge Program assessed 532 up to then unsponsored chemicals, whether they were \"orphaned\" or not, and found:\nSince 2009, the EPA required companies to perform toxicity testing on merely 34 chemicals. In 2011, the EPA announced, but as of 2013 had yet to finalize, plans to require testing for 23 additional chemicals, so altogether 57 chemicals. The EPA has prioritized 83 chemicals for risk assessment, and initiated seven assessments in 2012, with plans to start 18 additional assessments in 2013 and 2014.\nIn 2007, EPA began Toxcast which uses \"automated chemical screening technologies (called \"high-throughput screening assays\") to expose living cells or isolated proteins to chemicals\"., \n\nIn 2009, EPA reported that it developed a system called ACToR (Aggregated Computational Toxicology Resource) to expose living cells or isolated proteins to chemicals. It pooled chemical research, data and screening tools from multiple federal agencies including the National Toxicology Program/ National Institute of Environmental Health Science, National Center for Advancing Translational Sciences and the Food and Drug Administration. \n\n"}
{"id": "45004618", "url": "https://en.wikipedia.org/wiki?curid=45004618", "title": "Hydrodenitrogenation", "text": "Hydrodenitrogenation\n\nHydrodenitrogenation (HDN) is an industrial process for the removal of nitrogen from petroleum. Organonitrogen compounds, even though they occur at low levels, are undesirable because they cause poisoning of downstream catalysts. Furthermore, upon combustion, organonitrogen compounds generate NOx, a pollutant. HDN is effected as general hydroprocessing, which traditionally focuses on hydrodesulfurization (HDS) because sulfur compounds are even more problematic. To some extent, hydrodeoxygenation (HDO) is also effected.\n\nTypical organonitrogen compounds in petroleum include quinolines and porphyrins and their derivatives. The total nitrogen content is typically less than 1% and the targeted levels are in the ppm range. As described in organic geochemistry, organonitrogen compounds are derivatives or degradation products of the compounds in the living matter that comprised the precursor to fossil fuels. In HDN, the organonitrogen compounds are treated at high temperatures with hydrogen in the presence of a catalyst, the net transformation being:\n\nThe catalysts generally consist of cobalt and nickel as well as molybdenum disulfide or less often tungsten disulfide supported on alumina]]. The precise composition of the catalyst, i.e. Co/Ni and Mo/W ratios, are tuned for particular feedstocks. A wide variety of catalyst compositions have been considered, including metal phosphides.\n"}
{"id": "39501095", "url": "https://en.wikipedia.org/wiki?curid=39501095", "title": "Indian Salt Service", "text": "Indian Salt Service\n\nIndian Salt Service is a Central Civil Service of the Government of India. Under the administrative control of the Ministry of Commerce and Industry, it is one of the smallest Central services under the Government of India.\n\nThe organized and uniform collection of tax revenue on salt in British India began under the British Raj. Both before and after that, various native rulers of the Indian Princely states (outside British India proper) collected such revenue in accordance with their own revenue and administrative requirements and resources. In 1856, the government appointed the young William Chichele Plowden, Secretary of the Board of Revenue of the North West Provinces, to report on the establishment of a uniform system of revenue realisation from salt within the British Provinces, and he recommended the extension of the excise system, the reduction of duty, and the introduction of a system of licensing as the measures to achieve this goal. \n\nIn 1876, separate departments under a Salt Commissioner were set up, and these operated at the level of each British Province and Presidency. It was with the passing of the Government of India Act, 1935, that within British India (which then included much of present-day Pakistan) salt came under the exclusive control of the central government, with the Government of India taking over the task of collecting salt revenue and transferring it from the provincial salt agencies to the Central Excise and Revenue Department. In 1944, the Government of India passed the Central Excises and Salt Act which unified and amended all laws dealing with duties on excise and salt. In the 1940s and 1950s, after Independence, the Princely States were integrated into the new Union of India, so that for the first time this service was extended to all of them.\n\nThe Salt Department was originally a part of the Central Board of Revenue under the Ministry of Finance, but since a reorganisation of the ministries of India in 1957 it has come under the authority of the Ministry of Commerce and Industry.\n\nAccording to the Union List of subjects under the Seventh Schedule of the Indian Constitution, the \"manufacture, supply and distribution of salt by Union agencies; regulation and control of manufacture, supply and distribution of salt by other agencies\", is the responsibility of the Government of India. The posts of Salt Controller, Deputy Salt Controller and Assistant Salt Controller were re-categorized as Salt Commissioner, Deputy Salt Commissioner and Assistant Salt Commissioner in 1952 and the Indian Salt Services were created in 1954 for the realisation of the entry under the Union List. The Salt Service has both Group A and Group B wings.\n\nThe Salt Service is one of the smallest services under the Government of India with a sanctioned strength of only 11 posts. As a central civil service, recruitment to the Indian Salt Service is conducted by the Union Public Service Commission.\n\nThe Indian Salt Service is part of India's Salt Organization which is headquartered in Jaipur. The service is headed by the Salt Commissioner below whom are five Deputy Salt Commissioners and nine Assistant Salt Commissioners who man the agency with the help of other supporting staff. The Deputy Salt Commissioners head regional offices and the Assistant Salt Commissioners are in charge of divisional offices of the organisation. The Service has four regional offices at Chennai, Mumbai, Ahmedabad and Kolkata and field offices in the salt producing states.\n\nThe Salt Service is tasked with several functions including monitoring and quality updation of salt, setting production targets, providing technical guidance to salt manufacturers and leasing and managing department lands for the same, collection of cess, fees and rents and the implementation of various schemes aimed at combating iodine deficiency and programs for promoting the growth of the salt industry in India.\n"}
{"id": "32937468", "url": "https://en.wikipedia.org/wiki?curid=32937468", "title": "John Reid (conservationist)", "text": "John Reid (conservationist)\n\nJohn Reid is the founder and president of the Conservation Strategy Fund (CSF), a nonprofit international environmental organization in located in Sebastopol, CA. John has worked in conservation since 1991, promoting the use of economics to address conservation challenges. Before founding CSF, he held positions with Resources for the Future, Conservation International and Pacific Forest Trust.\n\nTo address the challenges inherent in conservation, John developed an innovative training program at CSF. The program teaches conservationists how to use practical, policy-relevant analyses on a number of themes in the Amazon Rainforest, Central America and the Brazilian Atlantic Forest. These themes include energy and transportation infrastructure, logging, ranching, protected areas and agriculture, among others. He designed CSF as an independent technical organization with the aim of spreading economics skills among conservation professionals. John’s work has appeared in \"Scientific American\", \"Conservation Biology\", \"Environment\", the \"Journal of Political Economy\", \"Megadiversidade\" and \"Ambio\". He speaks Portuguese and Spanish, and holds a Masters in Public Policy from Harvard University.\n\n"}
{"id": "11910131", "url": "https://en.wikipedia.org/wiki?curid=11910131", "title": "KEVA Planks", "text": "KEVA Planks\n\nKEVA Planks are cuboid wooden block toys for children and adults. Each block is sized approximately 1/4 inch (6.35 mm) x 3/4 inch (19.05 mm) x 4 1/2 inches (114.3 mm). The blocks are available for sale in maple, that is produced in the United States, and less expensive imported pine versions.\n\nKEVA Planks started out as a simple construction set that is unusual because they only use one piece and no connectors in glue, in contrast to other building sets that often have specific instructions and require sorting. It has developed into a teaching tool used in classrooms and enjoyed in homes.\n\nA number of museums have KEVA exhibits for hands on experience with design and construction including:\n\nThe tallest tower built with KEVA planks was 51 feet, 8 inches constructed at the National Building Museum in 2006.\n\nKEVA planks is a company located in Virginia now owned by Mindware division of Oriental Trading subsidiary of Berkshire Hathaway.\n\nKEVA Planks are used in schools, libraries, museums, and maker spaces. They are a teaching tool that can be used as a manipulative to teach subjects including math, science, geography, history, and humanities. They were featured at Destination Imagination Global Finals in Knoxville, TN in 2011. \n\nBeginning in 2015, KEVA Planks traveled with Share Fair Nation STEMosphere events and was one of the most popular sessions in the professional development workshops. STEMosphere highlights innovative and creative teaching tools.\n\nKEVA Planks were named number 3 in Worlds of Learning's Top Ten Makerspace Favorites of 2016.\n\nThey have been used as \"de-stressors\" at libraries at Duke University and the University of Virginia.\n\n"}
{"id": "4514759", "url": "https://en.wikipedia.org/wiki?curid=4514759", "title": "Karrick process", "text": "Karrick process\n\nThe Karrick process is a low-temperature carbonization (LTC) and pyrolysis process of carbonaceous materials. Although primarily meant for coal carbonization, it also could be used for processing of oil shale, lignite or any carbonaceous materials. These are heated at to in the absence of air to distill out synthetic fuels–unconventional oil and syngas. It could be used for a coal liquefaction as also for a semi-coke production. The process was the work of oil shale technologist Lewis Cass Karrick at the United States Bureau of Mines in the 1920s.\n\nThe Karrick process was invented by Lewis Cass Karrick in the 1920s. Although Karrick did not invent coal LTC as such, he perfected the existing technologies resulting the Karrick process. The retort used for the Karrick process based on the Nevada–Texas–Utah Retort, used for the shale oil extraction.\n\nIn 1935, a Karrick LTC pilot plant was constructed in the coal research laboratory at the University of Utah. Commercial-size processing plants were operated during the 1930s in Colorado, Utah and Ohio. During World War II, similar processing plant was operated by the United States Navy. In Australia, during World War II the Karrick process plants were used for shale oil extraction in New South Wales. In 1950s–1970s, the technology was used by the Rexco Company in its Snibston plant at Coalville in Leicestershire, England.\n\nThe Karrick process is a low-temperature carbonization process, which uses a hermetic retort. For commercial scale production, a retort about in diameter and high would be used. The process of carbonization would last about 3 hours.\n\nSuperheated steam is injected continuously into the top of a retort filled by coal. At first, in contact with cool coal, the steam condenses to water acting as a cleaning agent. While temperature of coal rises, the destructive distillation starts. Coal is heated at to in the absence of air. The carbonization temperature is lower compared with to for producing metallurgic coke. The lower temperature optimizes the production of coal tars richer in lighter hydrocarbons than normal coal tar, and therefore it is suitable for processing into fuels. Resulting water, oil and coal tar, and syngas moves out from retort through outlet valves at the bottom of the retort. The residue (char or semi-coke) remains in the retort. While the produced liquids are mostly a by-product, the semi-coke is the main product, a solid and smokeless fuel.\n\nThe Karrick LTC process generates no carbon dioxide, but it does produce a significant amount of carbon monoxide.\n\nIn the Karrick process, 1 short ton of coal yields up to 1 barrel of oils and coal tars (12% by weight), and produces of rich coal gas and of solid smokeless char or semi-coke (for one metric ton, 0.175 m³ of oils and coal tars, 95 m³ of gas, and 750 kg of semi-coke). Yields by volume of approximately 25% gasoline, 10% kerosene and 20% good quality fuel oil are obtainable from coal. Gasoline obtained from coal by the Karrick process combined with cracking and refining is equal in quality to tetraethyl lead gasolines. More power is developed in internal combustion engines and an increase in fuel economy of approximately 20% is obtainable under identical operating conditions.\n\nSemi-coke can be used for utility boilers and coking coal in steel smelters, yields more heat than raw coal and can be converted to water gas. Water gas can be converted to oil by the Fischer-Tropsch process. Coal gas from Karrick LTC yields greater energy content than natural gas. Phenolic wastes are used by the chemical industry as feedstock for plastics, etc. Electrical power can be cogenerated at nominal equipment cost.\n\nOils, including petroleum, have long been extracted from coal. Production plants were merely shut down in the 1880s because crude oil became cheaper than coal liquefaction. The capability itself, however, has never disappeared. Eight years of pilot plant tests by Karrick attest that states, cities or even smaller towns, could make their own gas and generate their own electricity.\n\nA 30-ton plant and oil refinery will show a profit over and above all operating and capital costs and the products will sell at attractive prices for equivalent products. The private sector should require no subsidies, but not in competition with those who skim off the oil from coal and sell the residual smokeless fuel to power plants.\n\nThe cheapest liquid fuel from coal will come when processed by LTC for both liquid fuels and electric power. As a tertiary product of the coal distilling process, electrical energy can be generated at a minimum equipment cost. A Karrick LTC plant with 1 kiloton of daily coal capacity produces sufficient steam to generate 100,000 kilowatt hours of electrical power at no extra cost excepting capital investment for electrical equipment and loss of steam temperature passing through turbines. The process steam cost could be low since this steam could be derived from off-peak boiler capacity or from turbines in central electric stations. Fuel for steam and superheating would subsequently be reduced in cost.\n\nCompared to the Bergius process, the Karrick process is cheaper, requires less water and destroys less the thermal value (one-half that of the Bergius process). The smokeless semi-coke fuel, when burned in an open grate or in boilers, delivers 20% to 25% more heat than raw coal. The coal gas should deliver more heat than natural gas per heat unit contained due to the greater quantity of combined carbon and lower dilution of the combustion gases with water vapor.\n\n\n"}
{"id": "37256834", "url": "https://en.wikipedia.org/wiki?curid=37256834", "title": "Kryptolebias", "text": "Kryptolebias\n\nKryptolebias is a genus of fish in the family Aplocheilidae mostly native to warm parts of South America but with one species (\"K. marmoratus\") found north through the Caribbean region to the Gulf Coast states of the United States. They are small fish, up to in total length. They are non-annual killifish.\n\nThere are currently 8 recognized species in this genus:\n\n"}
{"id": "22477882", "url": "https://en.wikipedia.org/wiki?curid=22477882", "title": "Kyshtym disaster", "text": "Kyshtym disaster\n\nThe Kyshtym Disaster was a radioactive contamination accident that occurred on 29 September 1957 at Mayak, a plutonium production site in Russia for nuclear weapons and nuclear fuel reprocessing plant of the Soviet Union. It measured as a Level 6 disaster on the International Nuclear Event Scale (INES), making it the third-most serious nuclear accident ever recorded, behind the Fukushima Daiichi nuclear disaster and the Chernobyl disaster (both Level 7 on the INES). The event occurred in Ozyorsk, Chelyabinsk Oblast, a closed city built around the Mayak plant, and spread hot particles over more than , where at least 270,000 people lived. Since Ozyorsk/Mayak (named Chelyabinsk-40, then Chelyabinsk-65, until 1994) was not marked on maps, the disaster was named after Kyshtym, the nearest known town.\n\nAfter World War II, the Soviet Union lagged behind the US in development of nuclear weapons, so it started a rapid research and development program to produce a sufficient amount of weapons-grade uranium and plutonium. The Mayak plant was built in haste between 1945 and 1948. Gaps in physicists' knowledge about nuclear physics at the time made it difficult to judge the safety of many decisions. Environmental concerns were not taken seriously during the early development stage. Initially Mayak was dumping high-level radioactive waste into a nearby river, which flowed to the river Ob, flowing further down to the Arctic Ocean. All six reactors were on Lake Kyzyltash and used an open-cycle cooling system, discharging contaminated water directly back into the lake. When Lake Kyzyltash quickly became contaminated, Lake Karachay was used for open-air storage, keeping the contamination a slight distance from the reactors but soon making Lake Karachay the \"most-polluted spot on Earth\".\n\nA storage facility for liquid nuclear waste was added around 1953. It consisted of steel tanks mounted in a concrete base, underground. Because of the high level of radioactivity, the waste was heating itself through decay heat (though a chain reaction was not possible). For that reason, a cooler was built around each bank, containing 20 tanks. Facilities for monitoring operation of the coolers and the content of the tanks were inadequate.\n\nIn 1957 the cooling system in one of the tanks containing about 70–80 tons of liquid radioactive waste failed and was not repaired. The temperature in it started to rise, resulting in evaporation and a chemical explosion of the dried waste, consisting mainly of ammonium nitrate and acetates (see ammonium nitrate/fuel oil bomb). The explosion, on 29 September 1957, estimated to have a force of about 70–100 tons of TNT, threw the 160-ton concrete lid into the air. There were no immediate casualties as a result of the explosion, but it released an estimated 20 MCi (800 PBq) of radioactivity. Most of this contamination settled out near the site of the accident and contributed to the pollution of the Techa River, but a plume containing 2 MCi (80 PBq) of radionuclides spread out over hundreds of kilometers. Previously contaminated areas within the affected area include the Techa river, which had previously received 2.75 MCi (100 PBq) of deliberately dumped waste, and Lake Karachay, which had received 120 MCi (4,000 PBq).\n\nIn the next 10 to 11 hours, the radioactive cloud moved towards the north-east, reaching from the accident. The fallout of the cloud resulted in a long-term contamination of an area of more than , depending on what contamination level is considered significant, primarily with caesium-137 and strontium-90. This area is usually referred to as the East-Ural Radioactive Trace (EURT).\n\nAt least 22 villages were exposed to radiation from the disaster, with a total population of around 10,000 people evacuated. Some were evacuated after a week, but it took almost 2 years for evacuations to occur at other sites.\n\nBecause of the secrecy surrounding Mayak, the populations of affected areas were not initially informed of the accident. A week later (on 6 October 1957), an operation for evacuating 10,000 people from the affected area started, still without giving an explanation of the reasons for evacuation.\n\nVague reports of a \"catastrophic accident\" causing \"radioactive fallout over the Soviet and many neighboring states\" began appearing in the western press between 13 and 14 April 1958, and the first details emerged in the Viennese paper \"Die Presse\" on 17 March 1959. But it was only in 1976 (18 years after) that Zhores Medvedev made the nature and extent of the disaster known to the world. In the absence of verifiable information, exaggerated accounts of the disaster were given. People \"grew hysterical with fear with the incidence of unknown 'mysterious' diseases breaking out. Victims were seen with skin 'sloughing off' their faces, hands and other exposed parts of their bodies.\" Medvedev's description of the disaster in the \"New Scientist\" was initially derided by western nuclear industry sources, but the core of his story was soon confirmed by Professor Leo Tumerman, former head of the Biophysics Laboratory at the Engelhardt Institute of Molecular Biology in Moscow.\n\nThe true number of fatalities remains uncertain because radiation-induced cancer is clinically indistinguishable from any other cancer, and its incidence rate can only be measured through epidemiological studies. One book claims that \"in 1992, a study conducted by the Institute of Biophysics at the former Soviet Health Ministry in Chelyabinsk found that 8,015 people had died within the preceding 32 years as a result of the accident.\" By contrast, only 6,000 death certificates have been found for residents of the Techa riverside between 1950 and 1982 from all causes of death, though perhaps the Soviet study considered a larger geographic area affected by the airborne plume. The most commonly quoted estimate is 200 deaths due to cancer, but the origin of this number is not clear. More-recent epidemiological studies suggest that around 49 to 55 cancer deaths among riverside residents can be associated to radiation exposure. This would include the effects of all radioactive releases into the river, 98% of which happened long before the 1957 accident, but it would not include the effects of the airborne plume that was carried north-east. The area closest to the accident produced 66 diagnosed cases of chronic radiation syndrome, providing the bulk of the data about this condition.\nTo reduce the spread of radioactive contamination after the accident, contaminated soil was excavated and stockpiled in fenced enclosures that were called \"graveyards of the earth\". The Soviet government in 1968 disguised the EURT area by creating the East Ural Nature Reserve, which prohibited any unauthorised access to the affected area.\n\nAccording to Gyorgy, who invoked the Freedom of Information Act to gain access to the relevant Central Intelligence Agency (CIA) files, the CIA had known of the 1957 Mayak accident since 1959, but kept it secret to prevent adverse consequences for the fledgling American nuclear industry. Starting in 1989 the Soviet government gradually declassified documents pertaining to the disaster.\n\nThe level of radiation in Ozyorsk itself, at about 0.1 mSv a year, is harmless, but the area of the EURT is still heavily contaminated with radioactivity.\n\n\n"}
{"id": "5649501", "url": "https://en.wikipedia.org/wiki?curid=5649501", "title": "Lems", "text": "Lems\n\nThe Lems was an English electric car manufactured by the London Electromobile Syndicate in London from 1903 to 1904. The two-seater runabout claimed to run on a single charge and reach a top speed of 12 mph (19 km/h). It was sold for 180 guineas.\n\nIn the United States, there is an example of this car at Larz Anderson Auto Museum in Brookline, Massachusetts.\n\n\nDavid Burgess Wise, \"The New Illustrated Encyclopedia of Automobiles\".\n"}
{"id": "57809847", "url": "https://en.wikipedia.org/wiki?curid=57809847", "title": "List of individuals (disambiguation)", "text": "List of individuals (disambiguation)\n\nList of individuals, usually refer to lists of people. It could also refer to:\n\n"}
{"id": "29472914", "url": "https://en.wikipedia.org/wiki?curid=29472914", "title": "Luz para Todos", "text": "Luz para Todos\n\nLuz para Todos (\"Light for All\") is a program of the Federal Government of Brazil, launched in November 2003, with a goal of bringing electricity to more than 10 million rural people by the year 2008. It was initiated by Dilma Rousseff, then Minister of Mines and Energy of Brazil, operated by the large power utility company Eletrobras, and executed by electricity concessionaires and cooperatives.\n\nThe project promotes renewable energy as the most practical solution in remote areas. To encourage utilizing that kind of energy, the federal government pays up to 85% of the costs for renewable energy projects in those areas.\n\nDuring the program execution, more families without power at home were located, and the program was extended to be completed in 2010.\n"}
{"id": "6327901", "url": "https://en.wikipedia.org/wiki?curid=6327901", "title": "Mixed waste", "text": "Mixed waste\n\nMixed waste can refer to any combination of waste types with different properties. \n\nTypically commercial and municipal wastes are mixtures of plastics, metals, glass, biodegradable waste including paper and textiles along with other nondescript junk.\n\nAs defined by The United States Environmental Protection Agency, \nMixed Waste contains both hazardous waste (as defined by RCRA and its amendments) and radioactive waste (as defined by AEA and its amendments). It is jointly regulated by NRC or NRC's Agreement States and EPA or EPA's RCRA Authorized States. The fundamental and most comprehensive statutory definition is found in the Federal Facilities Compliance Act (FFCA) where Section 1004(41) was added to RCRA: \"The term 'mixed waste' means waste that contains both hazardous waste and source, special nuclear, or byproduct material subject to the Atomic Energy Act of 1954.\"\n\n"}
{"id": "3560122", "url": "https://en.wikipedia.org/wiki?curid=3560122", "title": "Moisture recycling", "text": "Moisture recycling\n\nIn hydrology, moisture recycling or precipitation recycling refer to the process by which a portion of the precipitated water that evapotranspired from a given area contributes to the precipitation over the same area. Moisture recycling is thus a component of the hydrologic cycle. The ratio of the locally derived precipitation (formula_1) to total precipitation (formula_2) is known as the recycling ratio, formula_3: formula_4.\n\nThe recycling ratio is a diagnostic measure of the potential for interactions between land surface hydrology and regional climate. Land use changes, such as deforestation or agricultural intensification, have the potential to change the amount of precipitation that falls in a region. The recycling ratio for the entire world is one, and for a single point is zero. Estimates for the recycling ratio for the Amazon basin range from 24% to 56%, and for the Mississippi basin from 21% to 24%.\n\nThe concept of moisture recycling has been integrated into the concept of the precipitationshed. A precipitationshed is the upwind ocean and land surface that contributes evaporation to a given, downwind location's precipitation. In much the same way that a watershed is defined by a topographically explicit area that provides surface runoff, the precipitationshed is a statistically defined area within which evaporation, traveling via moisture recycling, provides precipitation for a specific point.\n\n\n</ref>\n"}
{"id": "637242", "url": "https://en.wikipedia.org/wiki?curid=637242", "title": "Mokume-gane", "text": "Mokume-gane\n\nMokume-gane has been used to create many artistic objects. Though the technique was first developed for production of decorative sword fittings, the craft is today mostly used in the production of jewelry and hollowware.\n\nFirst developed in 17th-century Japan, mokume-gane was used for swords. As the customary Japanese sword stopped serving as a weapon and became largely a status symbol, a demand arose for elaborate decorative handles and sheaths.\n\nTo meet this demand, Denbei Shoami (1651–1728), a master metalworker from Akita prefecture, invented the mokume gane process. He initially called his product \"guri bori\" for its simplest form's resemblance to \"guri\", a type of carved lacquerwork with alternating layers of red and black. Other historical names for it were \"kasumi-uchi\" (cloud metal), \"itame-gane\" (wood-grain metal), and \"yosefuki\".\n\nThe early components were relatively soft metals and alloys (gold, copper, silver, shakudō, shibuichi, and kuromido) which would form liquid phase diffusion bonds with one another without completely melting. This was useful in the traditional techniques of fusing and soldering the layers together.\n\nOver time, the practice of making mokume gane faded. The katana industry dried up in the late 19th century with Meiji Restoration returning ruling power to the emperor and the dissolution of the Shogunate government and the end of the Samurai class. The public display of swords as a sign of samurai status was outlawed. After this the few metalsmiths who practiced in mokume gane along with most other sword related artisans largely transferred their skills to create other objects.\n\nTiffany & Co.'s silver division under the direction of Edward C. Moore began to experiment with mokume gane techniques around 1877 and at the Paris exposition of 1878 Tiffany's in its grand prize-winning display of Moore's \"Japanesque\" silver wares included a magnificent \"Conglomerate Vase\" with asymmetrical panels of mokume gane. (The Conglomerate Vase which has been widely acclaimed as the most important work of nineteenth-century American silver was sold at Sotheby's on January 20, 1998 for $585,500.) Moore and Tiffany's silver smiths continued to develop its popular mokume techniques in preparation for the Paris exposition of 1889 where it displayed a vast array of Japanesque silver using ever more complex alloys of shakado, sedo and shibuci along with gold and silver to make laminates of up to twenty-four layers. Tiffany's display again won the grand prize for silver wares, and the company continued to produce its Japanesque silver with mokume up into the twentieth century.\n\nBy the mid 20th century, mokume gane was almost entirely unknown. Japan’s movement away from traditional craftwork, paired with the great difficulty of mastering the mokume gane art, had brought mokume gane artisans to the brink of extinction. It reached a point where only scholars and collectors of metalwork were aware of the technique. It was not until the 1970s, when Hiroko Sato Pijanowski who learned the craft from Norio Tamagawa. Hiroko and her husband Eugene Pijanowski brought the craft of mokume gane back to the United States and began teaching it to their students, at this point the artform re-emerged in the public eye. \n\nToday, jewelry, flatware, hollowware, spinning tops and other artistic objects are made using this material.\n\nModern processes are highly controlled and include a compressive force on the billet. This has allowed the technique to include many nontraditional components such as titanium, platinum, iron, bronze, brass, nickel silver, and various colors of karat gold including yellow, white, sage, and rose hues as well as sterling silver. At the Santa Fe Symposium, a major annual gathering of jewelers from around the world, there have been several papers presented on new, more predictable, and more economic, methods of producing mokume gane materials, along with new possibilities for laminating metals such as the use of friction-stir welding.\n\nMetal sheets were stacked and carefully heated; the solid billet of simple stripes could be forged and carved to increase the pattern's complexity. Successful lamination using the traditional process requires a highly skilled smith with a great deal of experience. Bonding in the traditional process is achieved when some or all of the alloys in the stack are heated to the point of becoming partially molten (above solidus) this liquid alloy is what fuses the layers together. Careful heat control and skillful forging are required for this process.\n\nIn attempting to recreate the appearance of traditional mokume gane some artisans tried brazing layers together. The sheets were soldered using silver solder or some other brazing alloy. This technique joined the metals, but is difficult to perfect, particularly on larger sheets. Flux inclusions could be trapped or bubbles could form. Commonly, imperfections need to be cut out, and the metal re-soldered. Ultimately the brazed sheets do not display the ductility and work-ability of diffusion bonded material.\n\nThe modernized process typically uses a controlled atmosphere in a temperature-controlled furnace. Mechanical aids such as a hydraulic press or torque plates (bolted clamps) are also typically used to apply compressive force on the billet during lamination. These provide for the implementation of lower temperature solid-state diffusion between the interleaved layers, thus allowing the inclusion of non-traditional materials.\n\nAfter the fusion of layers, the surface of the billet is cut with chisel to expose lower layers, then flattened. This cutting and flattening process will be repeated over and over again to develop intricate patterns.\n\nTo increase the contrast between the laminate layers many mokume-gane items are colored by the application of a patina (a controlled corrosion layer) to accentuate or even totally change the colors of the metal's surface.\n\nOne example of a traditional Japanese patination for mokume-gane is the use of the niiro process, usually involving rokushō, a complex copper verdigris compound produced specifically for use as a patina. The piece to be patinated is prepared, then immersed in a boiling solution until it reaches the desired color, and each element of a compound piece may be transformed to a different color. Historically, a paste of ground daikon radish was also used to prepare the work for the patina. The paste is applied immediately before the piece is boiled in the rokushō to protect the surface against tarnish and uneven coloring.\n\nIn an accidental but parallel development, Sheffield plate was developed in England. It follows a similar principal of bonded layers, without use of solder, but typically had 2–3 layers, whereas mokume-gane could have many more.\n\n\n"}
{"id": "1501608", "url": "https://en.wikipedia.org/wiki?curid=1501608", "title": "Montmorillonite", "text": "Montmorillonite\n\nMontmorillonite is a very soft phyllosilicate group of minerals that form when they precipitate from water solution as microscopic crystals, known as clay. It is named after Montmorillon in France. Montmorillonite, a member of the smectite group, is a 2:1 clay, meaning that it has two tetrahedral sheets of silica sandwiching a central octahedral sheet of alumina. The particles are plate-shaped with an average diameter around 1 μm and a thickness of 0.96 nm; magnification of about 25,000 times, using an electron microscope, is required to \"see\" individual clay particles. Members of this group include saponite.\n\nMontmorillonite is a subclass of smectite, a 2:1 phyllosilicate mineral characterized as having greater than 50% octahedral charge; its cation exchange capacity is due to isomorphous substitution of Mg for Al in the central alumina plane. The substitution of lower valence cations in such instances leaves the nearby oxygen atoms with a net negative charge that can attract cations. In contrast, beidellite is smectite with greater than 50% tetrahedral charge originating from isomorphous substitution of Al for Si in the silica sheet.\n\nThe individual crystals of montmorillonite clay are not tightly bound hence water can intervene, causing the clay to swell. The water content of montmorillonite is variable and it increases greatly in volume when it absorbs water. Chemically, it is hydrated sodium calcium aluminium magnesium silicate hydroxide (Na,Ca)(Al,Mg)(SiO)(OH)·\"n\"HO. Potassium, iron, and other cations are common substitutes, and the exact ratio of cations varies with source. It often occurs intermixed with chlorite, muscovite, illite, cookeite, and kaolinite.\n\nMontmorillonite can be concentrated and transformed within cave environments. The natural weathering of the cave can leave behind concentrations of aluminosilicates which were contained within the bedrock. Montmorillonite can form slowly in solutions of aluminosilicates. High HCO concentrations and long periods of time can aid in its formation. Montmorillonite can then transform to palygorskite under dry conditions and to halloysite-10Å (endellite) in acidic conditions (pH 5 or lower). Halloysite-10Å can further transform into halloysite-7Å by drying.\n\nMontmorillonite is used in the oil drilling industry as a component of drilling mud, making the mud slurry viscous, which helps in keeping the drill bit cool and removing drilled solids. It is also used as a soil additive to hold soil water in drought-prone soils, used in the construction of earthen dams and levees, and to prevent the leakage of fluids. It is also used as a component of foundry sand and as a desiccant to remove moisture from air and gases.\n\nMontmorillonite clays have been extensively used in catalytic processes. Cracking catalysts have used montmorillonite clays for over 60 years. Other acid-based catalysts use acid-treated montmorillonite clays.\n\nSimilar to many other clays, montmorillonite swells with the addition of water. Montmorillonites expand considerably more than other clays due to water penetrating the interlayer molecular spaces and concomitant adsorption. The amount of expansion is due largely to the type of exchangeable cation contained in the sample. The presence of sodium as the predominant exchangeable cation can result in the clay swelling to several times its original volume. Hence, sodium montmorillonite has come to be used as the major constituent in nonexplosive agents for splitting rock in natural stone quarries in an effort to limit the amount of waste, or for the demolition of concrete structures where the use of explosive charges is unacceptable.\n\nThis swelling property makes montmorillonite-containing bentonite useful also as an annular seal or plug for water wells and as a protective liner for landfills. Other uses include as an anticaking agent in animal feed, in paper making to minimize deposit formation, and as a retention and drainage aid component. Montmorillonite has also been used in cosmetics.\n\nIn a fine powder form, it can also be used as a flocculant in ponds. Tossed on the surface as it drops into the water, making the water \"clouded\", it attracts minute particles in the water and then settles to the bottom, cleaning the water. Koi and goldfish (carp) then actually feed on the \"clump\" which can aid in the digestion of the fish. It is sold in pond supply shops.\n\nSodium montmorillonite is also used as the base of some cat litter products, due to its adsorbent and clumping properties.\n\nMontmorillonite can be calcined to produce arcillite, a porous material. This calcined clay is sold as a soil conditioner for playing fields and other soil products such as for use as bonsai soil as an alternative to akadama. \n\nMontmorillonite is effective as an adsorptive of heavy metals.\n\nFor external use, montmorillonite has been used to treat contact dermatitis.\n\nMontmorillonite clay is added to some dog and cat foods as an anti-caking agent and because it may provide some resistance to environmental toxins, though research on the subject is not yet conclusive.\n\nMontmorillonite was first described in 1847 for an occurrence in Montmorillon in the department of Vienne, France, more than 50 years before the discovery of bentonite in the US. It is found in many locations worldwide and known by other names.\n\nMontmorillonite is also known to cause micelles (lipid spheres) to assemble together into vesicles. These structures resemble cell membranes on many cells. It can also help nucleotides to assemble into RNA which will end up inside the vesicles. This could have generated highly complex RNA polymers that could reproduce the RNA trapped within the vesicles. This process may have played a part in abiogenesis which led to life on Earth. Minerals similar to montmorillonites have also been found on Mars.\n\n\n"}
{"id": "13657910", "url": "https://en.wikipedia.org/wiki?curid=13657910", "title": "Pack goat", "text": "Pack goat\n\nA pack goat is a goat used as a beast of burden, for packing cargo. Generally, large wether (castrated buck) goats are used for packing, though does may also be packed. While does are generally smaller and therefore able to carry somewhat less cargo, they may also provide fresh milk.\n\nGoats are domesticated herd animals. They will usually stay near camp and follow their human masters on the trail, much as dogs will, without having to be leashed or tethered. They are generally used in wilderness camping settings.\n\nA healthy and fit pack goat can carry up to 25 percent of its weight and walk up to about 12 miles per day depending on its load weight, the terrain, and the animal's conditioning. They are generally less expensive to own and operate than other pack animals since they are natural browsing animals and can feed themselves along the way.\n\nThe North American Packgoat Association is a Boise, Idaho-based not-for-profit corporation formed to promote packing with packgoats. In the U.S., goats bred for packing are usually tall and lean; belonging to one of the larger dairy goat breeds such as Alpine, Toggenburg, Saanen, Lamancha, Oberhasli or a crossbreed thereof. However, any type of goat can be trained to pack. Craftspeople and enthusiasts in the U.S design and build specialized goat packing equipment, publish newsletters and raise pack goats for personal use and sale.\n\nGoat packing is popular with both young families, and older individuals who want to be able to go into the wilderness and need help carrying all their gear. Some public lands require permits for the use of goats as pack animals. Concerns have been raised about the disease-spreading potential that domestic goats may pose to wild animals, such as mountain sheep and mountain goats. Accordingly, the use of pack goats is restricted in certain areas.\n\nGoat packing was popularized in the U.S. in the 1980s and 1990s by John Mionczynski of Atlantic City, Wyoming. He developed a herd of large mixed-breed goats and used them as early as the 1970s to pack supplies for scientists working in the mountains and later to carry food and gear for tourists on hiking trips. He designed and built customized pack saddles and saddlebags and, with illustrator Hannah Hinchman, published a book, \"The Pack Goat\", in 1992.\n\n\n\n"}
{"id": "2812977", "url": "https://en.wikipedia.org/wiki?curid=2812977", "title": "Pair of pants (mathematics)", "text": "Pair of pants (mathematics)\n\nIn mathematics, a pair of pants is a surface which is homeomorphic to the three-holed sphere. The name comes from considering one of the removed disks as the waist and the two others as the cuffs of a pair of pants.\n\nPairs of pants are used as building blocks for compact surfaces in various theories. Two important applications are to hyperbolic geometry, where decompositions of closed surfaces into pairs of pants are used to construct the Fenchel-Nielsen coordinates on Teichmüller space, and in Topological quantum field theory where they are the simplest non-trivial cobordisms between 1-dimensional manifolds.\n\nAs said in the lede a pair of pants is any surface which is homeomorphic to a sphere with three holes, which formally are three open disks with pairwise disjoint closures removed from the sphere. Thus a pair of pants is a compact surface of genus zero with three boundary components. \n\nThe Euler characteristic of a pair of pants is equal to -1. Among all surfaces of negative Euler characteristic it has the maximal one; the only other surface with this property is the punctured torus (a torus minus an open disk).\n\nThe importance of the pairs of pants in the study of surfaces stems from the following property: define the complexity of a connected compact surface formula_1 of genus formula_2 with formula_3 boundary components to be formula_4, and for a non-connected surface take the sum over all components. Then the only surfaces with negative Euler characteristic and complexity zero are disjoint unions of pairs of pants. Furthermore, for any surface formula_1 and any simple closed curve formula_6 on formula_1 which is not homotopic to a boundary component, the compact surface obtain by cutting formula_1 along formula_6 has a complexity that is strictly less than formula_1. In this sense pairs of pants are the only \"irreducible\" surfaces among all surfaces of negative Euler characteristic.\n\nBy an immediate recursion this implies that for any surface there is a system of simple closed curves which cut the surface into pairs of pants. This is called a \"pants decomposition\" for the surface, and the curves are called the \"cuffs\" of the decomposition. Of course there is no unicity in the decompositions obtained by this method, but by quantifying the argument one sees that all pants decomposition of a given surface have the same number of curves which is exactly the complexity. For connected surfaces a pants decomposition has exactly formula_11 pants.\n\nA collection of simple closed curves on a surface is a pants decomposition if and only if they are disjoint, no two of them are homotopic and none is homotopic to a boundary component, and the collection is maximal for these properties.\n\nA given surface has infinitely many distinct pants decompositions (we understand two decompositions to be distinct when they are not homotopic). One way to try to understand the relations between all these decompositions is the \"pants complex associated to the surface\". This is a graph with vertex set the pants decompositions of formula_1, and two vertices are joined when they are related by an elementary move, which is one of the two following operations:\n\nThe pants complex is connected (meaning any two pants decompositions are related by a sequence of elementary moves) and has infinite diameter (meaning that there is no upper bounds on the number of moves needed to get from one decomposition to the other). In the particular case when the surface has complexity 1, the pants complex is isomorphic to the Farey graph. \n\nThe action of the mapping class group on the pants complex is of interest for studying this group. For example, Allen Hatcher and William Thurston have used it to give a proof of the fact that it is finitely presented.\n\nThe interesting hyperbolic structures on a pair of pants are easily classified.\n\nBy taking the length of a cuff to be equal to zero one obtains a complete metric on the pair of pants minus the cuff, which is replaced by a cusp. This structure is of finite volume.\n\nThe geometric proof of the classification in the previous paragraph is important to understand the structure of hyperbolic pants. It proceeds as follows: given an hyperbolic pair of pants with totally geodesic boundary the three geodesic arcs joining the cuffs pairwise and which are perpendicular to them at their ends are uniquely determined, and are called the \"seams\" of the pants.\n\nCutting the pants along the seams one gets two right-angled hyperbolic hexagons which have three alternate sides of matching lengths. The following lemma can be proven with elementary hyperbolic geometry.\n\nSo we see that the pair of pants is the double of a right-angled hexagon along alternate sides. Since the isometry class of the hexagon is also uniquely determined by the lengths of the sides that were not glued the classification of pants follows from that of hexagons.\n\nWhen a length of one cuff is zero one replaces the corresponding side in the right-angled hexagon by an ideal vertex.\n\nA point in the Teichmüller space of a surface formula_1 is represented by a pair formula_20 where formula_16 is a complete hyperbolic surface and formula_22 a diffeomorphism.\n\nIf formula_1 has a pants decomposition by curves formula_24 then one can parametrise Teichmüller pairs by the Fenchel-Nielsen coordinates which are defined as follows. The \"cuff lengths\" formula_18 are simply the lengths of the closed geodesics homotopic to the formula_26.\n\nThe \"twist parameters\" formula_27 are harder to define. They correspond to how much one turns when gluing two pairs of pants along formula_24: this defines them modulo formula_29. One can refine the definition (using either analytic continuation or geometric techniques) to obtain twist parameters valued in formula_30 (roughly, the point is that when one makes a full turn one changes the point in Teichmüller space by precomposing formula_31 with a Dehn twist around formula_24).\n\nOne can define a map from the pants complex to Teichmüller space, which takes a pants decomposition to an arbitrarily chosen point in the region where the cuff part of the Fenchel-Nielsen coordinates are bounded by a large enough constant. It is a quasi-isometry when Teichmüller space is endowed with the Weil-Petersson metric, which has proven useful in the study of this metric.\n\nThese structures correspond to Schottky groups on two generators (more precisely, if the quotient of the hyperbolic plane by a Schottky group on two generators is homeomorphic to the interior of a pair of pants then its convex core is an hyperbolic pair of pants as described above, and all are obtained as such).\n\nA cobordism between two n-dimensional closed manifolds is a compact (n+1)-dimensional manifold whose boundary is the disjoint union of the two manifolds. The category of cobordisms of dimension n+1 is the category with objects the closed manifolds of dimension n, and morphisms the cobordisms between them (note that the definition of a cobordism includes the identification of the boundary to the manifolds). Note that one of the manifolds can be empty; in particular a closed manifold of dimension n+1 is viewed as an endomorphism of the empty set. One can also compose two cobordisms when the end of the first is equal to the start of the second. A n-dimensional topological quantum field theory (TQFT) is a monoidal functor from the category of n-cobordisms to the category of complex vector space (where multiplication is given by the tensor product).\n\nIn particular, cobordisms between 1-dimensional manifolds (which are unions of circles) are compact surfaces whose boundary has been separated into two disjoint unions of circles. Two-dimensional TQFTs correspond to Frobenius algebras, where the circle (the only connected closed 1-manifold) maps to the underlying vector space of the algebra, while the pair of pants gives a product or coproduct, depending on how the boundary components are grouped – which is commutative or cocommutative. Further, the map associated with a disk gives a counit (trace) or unit (scalars), depending on grouping of boundary, which completes the correspondence.\n\n"}
{"id": "41969617", "url": "https://en.wikipedia.org/wiki?curid=41969617", "title": "Potential-induced degradation", "text": "Potential-induced degradation\n\nPotential-induced degradation (PID) is a potential-induced performance degradation in crystalline photovoltaic modules, caused by so-called stray currents. This effect may cause power loss of up to 30 percent.\n\nThe cause of the harmful leakage currents, besides the structure of the solar cell, is the voltage of the individual photovoltaic (PV) modules to the ground. In most ungrounded PV systems, the PV modules with a positive or negative voltage to the ground are exposed to PID. PID occurs mostly at negative voltage with respect to the ground potential and is accelerated by high system voltages, high temperatures, and high humidity.\n\nThe term \"potential-induced degradation\" (PID) was first introduced in the English language in a published study by S. Pingel and coworkers in 2010. It was introduced as a degradation mode resulting from voltage potential between the cells in the photovoltaic module and ground. Research in this field was pioneered by the Jet Propulsion Laboratory, focusing primarily on electrochemical degradation in crystalline silicon and amorphous silicon photovoltaic modules. The degradation mechanism known as polarization found in the first generation crystalline silicon high performance modules from SunPower in strings having positive voltage potential with respect to ground was discussed in 2005. Degradation of conventional front junction (n+/p) solar cell modules under voltage potential was also observed. The degradation by polarization was also covered in the trade journal Photon (4/2006, 6/2006, and 4/2007).\n\nIn 2007, PID was reported in a number of solar panels from Evergreen Solar (Photon 1/2008 and 8 /2008). In this case, the degradation mechanism occurring in photovoltaic modules containing the more common front junction (n+/p) crystalline silicon solar cells when the modules were in negative voltage potential with respect to ground. PID was further discussed as a problem in ordinary crystalline modules (Photon 12/2010, lecture by solar energy company Solon SE at PVSEC in Valencia 2010). Statement of the solar module manufacturer Solon SE: \"At 1000 V , a now quite common voltage for larger PV systems, it can be critical for each module technology\". PID of the shunting type (PID-s), which is the most prevalent and most detrimental type of PID for crystalline silicon modules, was discovered to be caused by microscopic crystal defects penetrating the p-n front junction of affected solar cells.\n\nThe PID-s that occurs in modules in negative polarity strings can be completely prevented if an inverter is used with the possibility of grounding (or effectively grounding) the positive or negative pole. This is possible if the inverter is galvanically isolated, e.g. using a transformer, if specially designed transformerless inverter topologies are used, or by altering the electric grid potential to ground. Which pole must be grounded, is clarified with the solar module manufacturer.\n\nIf the PID effect is present in the solar module, the effect can be reversed. Five companies, VIGDU, SMA Solar Technology, iLumen, PADCON and Pidbull have made a device that can prevent and reverse this effect.\n\n__notoc__\n"}
{"id": "1430548", "url": "https://en.wikipedia.org/wiki?curid=1430548", "title": "Preon", "text": "Preon\n\nIn particle physics, preons are point particles, conceived of as subcomponents of quarks and leptons. The word was coined by Jogesh Pati and Abdus Salam in 1974. Interest in preon models peaked in the 1980s but has slowed as the Standard Model of particle physics continues to describe the physics mostly successfully, and no direct experimental evidence for lepton and quark compositeness has been found.\n\nIn the hadronic sector, some effects are considered anomalies within the Standard Model. For example, the proton spin puzzle, the EMC effect, the distributions of electric charges inside the nucleons as found by Hofstadter in 1956, and the \"ad hoc\" CKM matrix elements.\n\nWhen the term \"preon\" was coined, it was primarily to explain the two families of spin-½ fermions: leptons and quarks. More-recent preon models also account for spin-1 bosons, and are still called \"preons\". Each of the preon models postulates a set of fewer fundamental particles than those of the Standard Model, together with the rules governing how those fundamental particles combine and interact. Based on these rules, the preon models try to explain the Standard Model, often predicting small discrepancies with this model and generating new particles and certain phenomena, which do not belong to the Standard Model.\n\nPreon research is motivated by the desire to:\n\nBefore the Standard Model (SM) was developed in the 1970s (the key elements of the Standard Model known as quarks were proposed by Murray Gell-Mann and George Zweig in 1964), physicists observed hundreds of different kinds of particles in particle accelerators. These were organized into relationships on their physical properties in a largely ad-hoc system of hierarchies, not entirely unlike the way taxonomy grouped animals based on their physical features. Not surprisingly, the huge number of particles was referred to as the \"particle zoo\".\n\nThe Standard Model, which is now the prevailing model of particle physics, dramatically simplified this picture by showing that most of the observed particles were mesons, which are combinations of two quarks, or baryons which are combinations of three quarks, plus a handful of other particles. The particles being seen in the ever-more-powerful accelerators were, according to the theory, typically nothing more than combinations of these quarks.\n\nWithin the Standard Model, there are several classes of particles. One of these, the quarks, has six types, of which there are three varieties in each (dubbed \"colors\", red, green, and blue, giving rise to quantum chromodynamics).\n\nAdditionally, there are six different types of what are known as leptons. Of these six leptons, there are three charged particles: the electron, muon, and tau. The neutrinos comprise the other three leptons, and for each neutrino there is a corresponding member from the other set of three leptons.\n\nIn the Standard Model, there are also bosons, including the photons; W, W, and Z bosons; gluons and the Higgs boson; and an open space left for the graviton. Almost all of these particles come in \"left-handed\" and \"right-handed\" versions (see \" chirality\"). The quarks, leptons, and W boson all have antiparticles with opposite electric charge.\n\nThe Standard Model also has a number of problems which have not been entirely solved. In particular, no successful theory of gravitation based on a particle theory has yet been proposed. Although the Model assumes the existence of a graviton, all attempts to produce a consistent theory based on them have failed.\n\nKalman observes that, according to the concept of atomism, fundamental building blocks of nature are indivisible bits of matter that are ungenerated and indestructible. Quarks are not truly indestructible, since some can decay into other quarks. Thus, on fundamental grounds, quarks are not themselves fundamental building blocks but must be composed of other, fundamental quantities—preons. Although the mass of each successive particle follows certain patterns, predictions of the rest mass of most particles cannot be made precisely, except for the masses of almost all baryons which have been recently described very well by the model of de Souza.\n\nThe Standard Model also has problems predicting the large scale structure of the universe. For instance, the SM generally predicts equal amounts of matter and antimatter in the universe. A number of attempts have been made to \"fix\" this through a variety of mechanisms, but to date none have won widespread support. Likewise, basic adaptations of the Model suggest the presence of proton decay, which has not yet been observed.\n\nPreon theory is motivated by a desire to replicate the achievements of the periodic table, which reduced the elements to three building-blocks, and the later Standard Model which tamed the \"particle zoo\", by finding more fundamental answers to the huge number of arbitrary constants present in the Standard Model.\n\nThere are several models forward in an attempt to provide a more fundamental explanation of the results in experimental and theoretical particle physics, using names such as \"parton\" or \"preon\" for their basic particles. The particular preon model discussed below has attracted comparatively little interest to date among the particle physics community, in part because no evidence has been obtained so far in collider experiments to show that the fermions of the Standard Model are composite.\n\nA number of physicists have attempted to develop a theory of \"pre-quarks\" (from which the name \"preon\" derives) in an effort to justify theoretically the many parts of the Standard Model that are known only through experimental data. Other names which have been used for these proposed fundamental particles (or particles intermediate between the most fundamental particles and those observed in the Standard Model) include \"prequarks\", \"subquarks\", \"maons\", \"alphons\", \"quinks\", \"rishons\", \"tweedles\", \"helons\", \"haplons\", \"Y-particles\", and \"primons\". \"Preon\" is the leading name in the physics community.\n\nEfforts to develop a substructure date at least as far back as 1974 with a paper by Pati and Salam in \"Physical Review\". Other attempts include a 1977 paper by Terazawa, Chikashige and Akama, similar, but independent, 1979 papers by Ne'eman, Harari, and Shupe, a 1981 paper by Fritzsch and Mandelbaum, and a 1992 book by D'Souza and Kalman. None of these has gained wide acceptance in the physics world. However, in a recent work de Souza has shown that his model describes well all weak decays of hadrons according to selection rules dictated by a quantum number derived from his compositeness model. In his model leptons are elementary particles and each quark is composed of two \"primons\", and thus, all quarks are described by four \"primons\". Therefore, there is no need for the Standard Model Higgs boson and each quark mass is derived from the interaction between each pair of \"primons\" by means of three Higgs-like bosons.\n\nIn his 1989 Nobel Prize acceptance lecture, Hans Dehmelt described a most fundamental elementary particle, with definable properties, which he called the \"cosmon\", as the likely end result of a long but finite chain of increasingly more elementary particles.\n\nMany preon models either do not account for the Higgs boson or rule it out, and propose that electro-weak symmetry is broken not by a scalar Higgs field but by composite preons. For example, Fredriksson preon theory does not need the Higgs boson, and explains the electro-weak breaking as the rearrangement of preons, rather than a Higgs-mediated field. In fact, the Fredriksson preon model and the de Souza model predict that the Standard Model Higgs boson does not exist.\n\nThe \"rishon model\" (RM) is the earliest effort to develop a preon model to explain the phenomenon appearing in the Standard Model (SM) of particle physics. It was first developed by Haim Harari and Michael A. Shupe (independently of each other), and later expanded by Harari and his then-student Nathan Seiberg.\n\nThe model has two kinds of fundamental particles called rishons (which means \"primary\" in Hebrew). They are T (\"Third\" since it has an electric charge of ⅓ \"e\", or Tohu which means \"unformed\" in Hebrew Genesis) and V (\"Vanishes\", since it is electrically neutral, or Vohu. [Bohu means \"void\" in the Hebrew Tanakh (the Old Testament), though \"bohu\" may be pronounced as \"vohu\" by modern Israelis when the \"b\" is preceded by a vowel and thus lacks dagesh.] All leptons and all flavours of quarks are three-rishon ordered triplets. These groups of three rishons have spin-½.\n\nThe Rishon model illustrates some of the typical efforts in the field. Many of the preon models theorize that the apparent imbalance of matter and antimatter in the universe is in fact illusory, with large quantities of preon-level antimatter confined within more complex structures.\n\nOne preon model started as an internal paper at the Collider Detector at Fermilab (CDF) around 1994. The paper was written after an unexpected and inexplicable excess of jets with energies above 200 GeV were detected in the 1992–1993 running period. However, scattering experiments have shown that quarks and leptons are \"pointlike\" down to distance scales of less than 10 m (or of a proton diameter). The momentum uncertainty of a preon (of whatever mass) confined to a box of this size is about 200 GeV/c, 50,000 times larger than the rest mass of an up-quark and 400,000 times larger than the rest mass of an electron.\n\nHeisenberg's uncertainty principle states that formula_1 and thus anything confined to a box smaller than formula_2 would have a momentum uncertainty proportionally greater. Thus, the preon model proposed particles smaller than the elementary particles they make up, since the momentum uncertainty formula_3 should be greater than the particles themselves.\n\nSo the preon model represents a mass paradox: How could quarks or electrons be made of smaller particles that would have many orders of magnitude greater mass-energies arising from their enormous momenta? This paradox is resolved by postulating a large binding force between preons cancelling their mass-energies.\n\nPreon models propose additional unobserved forces or dynamics to account for the observed properties of elementary particles, which may have implications in conflict with observation. For example, now that the LHC's observation of a Higgs boson is confirmed, the observation contradicts the predictions of many preon models that did not include it.\n\nPreon theories require that quarks and leptons should have a finite size. It is possible that the Large Hadron Collider will observe this when raised to higher energies.\n\n\n\n\n\n"}
{"id": "4098326", "url": "https://en.wikipedia.org/wiki?curid=4098326", "title": "Pulsed DC", "text": "Pulsed DC\n\nPulsed DC (PDC) or pulsating direct current is a periodic current which changes in value but never changes direction. Some authors use the term pulsed DC to describe a signal consisting of one or more rectangular (\"flat-topped\"), rather than sinusoidal, pulses.\n\nPulsed DC is commonly produced from AC (alternating current) by a half-wave rectifier or a full-wave rectifier. Full wave rectified ac is more commonly known as Rectified AC. PDC has some characteristics of both alternating current (AC) and direct current (DC) waveforms. The voltage of a DC wave is roughly constant, whereas the voltage of an AC waveform continually varies between positive and negative values. Like an AC wave, the voltage of a PDC wave continually varies, but like a DC wave, the sign of the voltage is constant.\n\nPulsating direct current is used on PWM controllers.\n\nMost modern electronic items function using a DC voltage, so the PDC waveform must usually be smoothed before use. A reservoir capacitor converts the PDC wave into a DC waveform with some superimposed ripple. When the PDC voltage is initially applied, it charges the capacitor, which acts as a short term storage device to keep the output at an acceptable level while the PDC waveform is at a low voltage. Voltage regulation is often also applied using either linear or switching regulation.\n\nPulsating direct current has an average value equal to a constant (DC) along with a time-dependent pulsating component added to it, while the average value of alternating current is zero in steady state (or a constant if it has a DC offset, value of which will then be equal to that offset). Devices and circuits may respond differently to pulsating DC than they would to non-pulsating DC, such as a battery or regulated power supply and should be evaluated.\n\nPulsed DC may also be generated for purposes other than rectification. It is often used to reduce electric arcs when generating thin carbon films, and for increasing yield in semiconductor fabrication by reducing electrostatic build-up. It is also generated by the voltage regulators in some automobiles, e.g., the classic air-cooled Volkswagen Beetle.\n"}
{"id": "9653746", "url": "https://en.wikipedia.org/wiki?curid=9653746", "title": "Resaw", "text": "Resaw\n\nA resaw is a large band saw optimized for cutting timber along the grain to reduce larger sections into smaller sections or veneers. Resawing veneers requires a wide blade – commonly 2 to 3 inches (52–78 mm) – with a small kerf to minimize waste. Resaw blades of up to 1 inch (26 mm) may be fitted to a standard band saw. Many small and medium-sized sawmills use 1- to -inch band saw blades.\n\nTimber mills use larger resaws to rip large planks into smaller sizes. A typical mill sized resaw blade is eight inches wide and made with 16 gauge steel. Resaw blades can be identified by their straight back, as opposed to headsaws and doublecut blades, which have notched or toothed backs.\n"}
{"id": "17124400", "url": "https://en.wikipedia.org/wiki?curid=17124400", "title": "SkyTran", "text": "SkyTran\n\nskyTran is a Personal Rapid Transit system concept first proposed by inventor Douglas Malewicki in 1990, and under development by Unimodal Inc. A prototype of the skyTran vehicle and a section of track have been constructed. The early magnetic levitation system, Inductrack, now abandoned by skytran, has been tested by General Atomics with a full-scale model. In 2010 UniModal Inc. signed an agreement with NASA to test and develop skyTran. A skyTran system is being built in Israel as a pilot project. It was initially projected to be completed in Tel Aviv by the end of 2015. As of January 2016 work is ongoing with the test demonstration track. SkyTran has proposed additional projects in France, Germany, India, Indonesia, Malaysia, the United Kingdom, and the United States.\n\nTo minimize maintenance and make switching on and off the tracks efficient at high speeds, early versions of the system proposed using the Inductrack passive magnetic levitation system instead of wheels. Passive maglev requires no external power to levitate vehicles. Rather, the magnetic repulsion is produced by the movement of the vehicle over shorted wire coils in the track. The cars would be driven by a Linear motor in the track or vehicle. Therefore, the system will have very few moving parts; primarily just the vehicle itself moving along the track, its parking wheels and door, and fans in heating and air conditioning units; so its promoters refer to the system as \"solid state\".\n\nOn this first version, the passive maglev coils are enclosed and supported by a light shell called a guideway that also captures the vehicles mechanically to prevent derailment. Malewicki proposes a 3D grid design that avoids accident-prone intersections by grade separation, with guideways and their exit and entry ramps crossing above or below each other. Tracks will be supported above the ground by standard metal utility poles. They could also be attached to the sides of buildings.\n\nAfter identifying problems with Inductrack and the cost associated with it, skyTran described an improved design during a Horizon BBC interview with skyTran at NASA Ames in Mountain View, CA.\n\nNew details about the levitation and motor were described in a keynote speech in June 2016, showing levitation stator being plain aluminium plates and motor stator an aluminium tube. The guideway is also significantly enlarged and wider than the vehicle, so the switching can be vertical, going through the guideway. Guideway shape is shown at 16:26 in above referenced video. This new concept can be seen in a short simulation film.\nInstead of the purely passive inductrack system, the new mechanism modify lift by mechanically angling the magnetic pads and need a servo controlled actuation. The lift control also do the switching by moving vertically through the rails.\n\nPatents applications were filed by skytran for this new system: and \n\nMalewicki conceived the basic idea of skyTran in 1990, filing a US patent application that year that was granted as US Patent #5108052 in 1992.\nHe published several technical papers on skyTran in the following years. In 1991, he presented a paper entitled \"People Pods - Miniature Magnetic Levitation Vehicles for Personal Non-Stop Transportation\" to the Society of Automotive Engineers (SAE) Future Transportation Conference in Portland, Oregon. The paper is a thorough description of the concept at that point, although some important features of the current skyTran design are only discussed as options, including magnetic levitation rather than wheels and hanging below the guideway instead of riding above it.\n\nThe paper describes how Malewicki had built and driven a freeway-legal 154-MPG car in 1981, but realized it could never be safe on a street surrounded by far larger and heavier vehicles. Elevated tracks would allow a very light vehicle to be safe. They are also basic to the system's inexpensiveness, because there is no need to acquire a huge right of way and tear down buildings. It presents an aerodynamic analysis (Malewicki is an aerospace engineer) supporting claims of very high energy efficiency (the paper claims for skyTran's current two-passenger tandem design, though the Unimodal site claims only, \"over \"). It also described how a very light vehicle that can squeeze both surfaces of a track simultaneously could reliably achieve a 6-G deceleration, allowing it to brake safely to a stop from in just .\n\nThe 2008 energy shortages stimulated renewed interest in Green vehicle proposals such as skyTran. The \"Maglev skyTran\" topic quoted a number of skyTran and Personal Rapid Transit ideas, such as passengers exiting and boarding at off-line elevated \"portal\" stops while high-speed traffic continues to speed by on its main line.\n\nIn September 2009, the US NASA (National Aeronautics and Space Administration) signed a Space Act joint development agreement with Unimodal. Unimodal has tested prototype vehicles on short guideway sections at NASA's Ames Research Center, in Mountain View, California. NASA control and vehicle dynamics simulation software was made available to Unimodal, which hired NASA subcontractors to program them using US DOT grant funding.\n\nIn June 2014, Unimodal and Israel Aerospace Industries (IAI) contracted to build a 400-500 meter elevated loop test track on IAI's campus in central Israel. If the pilot project is successful, IAI will build a commercial skyTran network in the city of Tel Aviv, Herzliya and Netanya. In April 2015, Money transfer for the project in Herzliya was approved, which significantly promotes the project in Israel.\n\nIn 2018, it was announced that Indian conglomerate Reliance Industries had acquired a 12.7% stake in SkyTran. As part of the deal, Reliance would supply communication equipment and a prototype would be built in India.\n\n\n"}
{"id": "35988061", "url": "https://en.wikipedia.org/wiki?curid=35988061", "title": "Sofia Gatica", "text": "Sofia Gatica\n\nSofía Gatica (1976) is an Argentine environmentalist, whose infant daughter Sofia, died just three days after her birth of kidney failure, which is likely caused by pesticide exposure. She was awarded the Goldman Environmental Prize in 2012, for her fight against use of toxic pesticides used in agriculture in Argentina, in particular agents containing glyphosate and endosulfan. \nIn November 2013 she was threatened with death at gunpoint and beaten by unidentified men.\n"}
{"id": "9789521", "url": "https://en.wikipedia.org/wiki?curid=9789521", "title": "Spin kit", "text": "Spin kit\n\nA spin kit is a kit for a sailplane to make it spin. The kit consists of ballast weights (usually discs) applied to the tail to move the center of gravity rearward. This increases the instability of the glider, enabling it to spin.\n\nA few sailplanes are very difficult, if not impossible, to spin under normal conditions. To make these sailplanes spin easily, for training purposes or demonstrations, a spin kit is available from the manufacturer.\n\n\n"}
{"id": "53620028", "url": "https://en.wikipedia.org/wiki?curid=53620028", "title": "Storm Vivian", "text": "Storm Vivian\n\nVivian was one of a series of severe hurricanes in 1990. It struck large parts of Europe from 25 to 27 February 1990 and cost 64 people their lives. A few days later it was followed by \"Wiebke\". After Hurricane Andrew in 1992 (26.5 B. USD), \"Daria\" in 1990 and \"Lothar\" in 1999 (each costing c. B USD), and the West European Hurricane of 1987 (4.3 B USD) \"Vivian/Wiebke\" was one of the most expensive Atlantic storms in history, costing 4 B USD of insurance payments.\nIn his report \"Winter storms in Europe - History from 1703 to 2012\" (\"Winterstürme in Europa - Historie von 1703 bis 2012\"), Aon Benfield assessed the cost of insurance payouts for storm damage in Germany as 1.5 billion euros.\n\nApart from Germany (15 deaths), other countries affected badly were the United Kingdom, Ireland, France, the Netherlands, Belgium and Switzerland.\n\nIn Hamburg there were several successive storm surges. Thanks to the storms, the Rosenmontag parade in Düsseldorf was postponed until May, whilst in Cologne it went ahead with high safety precautions.\n\nDeath toll: The fatalities are given as 64 persons, the damage is estimated as ca. 1.11 billion €.\n\nParticularly in mountain regions, a large number of trees were damaged. (Even complete spruce and beech stands/forests).\nHundreds of trees were bent or thrown like matches. Extrapolations go of 60 to 70 million cubic meters of extra \"cut\" wood by the storm, which corresponded to about twice the annual harvesting in Germany.\n\n"}
{"id": "54388605", "url": "https://en.wikipedia.org/wiki?curid=54388605", "title": "Telluropyrylium", "text": "Telluropyrylium\n\nTelluropyrylium is an aromatic heterocyclic compound consisting of a six member ring with five carbon atoms, and a positively charged tellurium atom. Derivatives of telluropyrylium are important in research of infrared dyes.\n\nFormerly it was named tellurapyrylium. However this is misleading, as \"tellura\" indicates that tellurium substitutes for carbon atom, but actually tellurium is substituted for the oxygen atom in pyrilium. In the Hantzsch-Widman system it is called tellurinium. This is the name used by Chemical Abstracts. Replacement nomenclature would call this telluroniabenzene.\n\nNumbering in telluropyrylium starts with 1 on the tellurium atom and counts up to 6 counter-clockwise on the carbon atoms. The positions adjacent to the chalcogen, numbered 2 and 6 can also be called α, the next two positions 3 and 5 can be termed \"β\" and the opposite carbon at position 4 can be called \"γ\".\n\nBecause telluropyrylium is a positively charged cation, it takes the solid form as a salt with non-nucleophillic anions like perchlorate, tetrafluoroborate, or hexafluorophosphate.\n\nThe positive charge is not confined to the tellurium atom in telluropyrylium, but distributes on the ring in several resonance structures, so that the α and γ positions have some positive charge. A nucleophillic attack targets these carbon atoms.\n\nThe shape of the telluropyrylium molecule is not a prefect hexagon, as the bondlengths to the tellurium atom at about 2.068 Å compared to about 1.4 Å for the carbon-carbon bonds. The angle at the tellurium atom is laso reduced to about 94°, angles at the α and γ carbon atoms in the ring are about 122° and at the β positions 129°. The whole ring is bent so that it forms a boat shape with an angles of 8.7° on the Te-γ axis. (THis was measured in the crystal structure of tetraphenyl telluropyrylium-pyrylium monomethine fluoroborate.\n\nWhen the ring of telluropyrylium is fused with other aromatic rings larger aromatic structures such as tellurochromenylium, telluroflavylium, and telluroxanthylium result.\n"}
{"id": "402178", "url": "https://en.wikipedia.org/wiki?curid=402178", "title": "Thyratron", "text": "Thyratron\n\nA thyratron is a type of gas-filled tube used as a high-power electrical switch and controlled rectifier. Thyratrons can handle much greater currents than similar hard-vacuum tubes. Electron multiplication occurs when the gas becomes ionized, producing a phenomenon known as Townsend discharge. Gases used include mercury vapor, xenon, neon, and (in special high-voltage applications or applications requiring very short switching times) hydrogen. Unlike a vacuum tube (valve), a thyratron cannot be used to amplify signals linearly.\n\nIn the 1920s, thyratrons were derived from early vacuum tubes such as the UV-200, which contained a small amount of argon gas to increase its sensitivity as a radio signal detector; and the German LRS Relay tube, which also contained argon gas. Gas rectifiers, which predated vacuum tubes, such as the argon-filled General Electric \"Tungar bulb\" and the Cooper-Hewitt mercury-pool rectifier, also provided an influence. Irving Langmuir and G. S. Meikle of GE are usually cited as the first investigators to study controlled rectification in gas tubes, about 1914. The first commercial thyratrons didn't appear until around 1928.\n\nThe term \"thyristor\" was derived from a combination of \"thyratron\" and \"transistor\". Since the 1960s thyristors have replaced thyratrons in most low- and medium-power applications.\n\nThyratrons resemble vacuum tubes both in appearance and construction but differ in behavior and operating principle. In a vacuum tube, conduction is dominated by free electrons because the distance between anode and cathode is small compared to the mean free path of electrons. A thyratron, on the other hand, is intentionally filled with gas so that the distance between anode and cathode is comparable with the mean free path of electrons. This means that conduction in a thyratron is dominated by plasma conductivity. Due to the high conductivity of plasma, a thyratron is capable of switching higher currents than vacuum tubes which are limited by space charge. A vacuum tube has the advantage that conductivity may be modulated at any time whereas a thyratron becomes filled with plasma and continues to conduct as long as a voltage exists between the anode and cathode. A pseudospark switch operates in a similar regime of the Paschen curve as a thyratron and is sometimes called a cold cathode thyratron.\n\nA thyratron consists of a hot cathode, an anode, and one or more control grids between the anode and cathode in an airtight glass or ceramic envelope that is filled with gas. The gas is typically hydrogen or deuterium at a pressure of 300 to 500 mTorr (40 to 70 Pa). Commercial thyratrons also contain a titanium hydride reservoir and a reservoir heater that together maintain gas pressure over long periods regardless of gas loss.\n\nConductivity of a thyratron remains low as long as the control grid is negative relative to the cathode because the grid repels electrons emitted by the cathode. Space charge limited electron current flows from the cathode through the control grid toward the anode if the grid is made positive relative to the cathode. Sufficiently high space charge limited current initiates Townsend discharge between anode and cathode. The resulting plasma provides high conductivity between anode and cathode and is not limited by space charge. Conductivity remains high until the current between anode and cathode drops to a small value for a sufficiently long time that the gas ceases to be ionized. This recovery process takes 25 to 75 μs and limits thyratron repetition rates to a few kHz.\n\nLow-power thyratrons (\"relay tubes\" and \"trigger tubes\") were manufactured for controlling incandescent lamps, electromechanical relays or solenoids, for bidirectional counters, to perform various functions in Dekatron calculators, for voltage threshold detectors in RC timers, etc. \"Glow thyratrons\" were optimized for high gas-discharge light output or even phosphorized and used as self-displaying shift registers in large-format, crawling-text dot-matrix displays.\n\nAnother use of the thyratron was in relaxation oscillators. Since the plate turn-on voltage is much higher than the turn-off voltage, the tube exhibits hysteresis and, with a capacitor across it, it can function as a sawtooth oscillator. The voltage on the grid controls the breakdown voltage and thus the period of oscillation. Thyratron relaxation oscillators were used in power inverters and oscilloscope sweep circuits.\n\nOne miniature thyratron, the triode 6D4, found an additional use as a potent noise source, when operated as a diode (grid tied to cathode) in a transverse magnetic field. Sufficiently filtered for \"flatness\" (\"white noise\") in a band of interest, such noise was used for testing radio receivers, servo systems and occasionally in analog computing as a random value source.\n\nThe miniature RK61/2 thyratron marketed in 1938 was designed specifically to operate like a vacuum triode below its ignition voltage, allowing it to amplify analog signals as a self-quenching superregenerative detector in radio control receivers, and was the major technical development which led to the wartime development of radio-controlled weapons and the parallel development of radio controlled modelling as a hobby.\n\nSome early television sets, particularly British models, used thyratrons for vertical (frame) and horizontal (line) oscillators.\n\nMedium-power thyratrons found applications in machine tool motor controllers, where thyratrons, operating as phase-controlled rectifiers, are utilized in the tool's armature regulator (zero to \"base speed\", \"constant torque\" mode) and in the tool's field regulator (\"base speed\" to about twice \"base speed\", \"constant horsepower\" mode). Examples include Monarch Machine Tool 10EE lathe, which used thyratrons from 1949 until solid-state devices replaced them in 1984.\n\nHigh-power thyratrons are still manufactured, and are capable of operation up to tens of kiloamperes (kA) and tens of kilovolts (kV). Modern applications include pulse drivers for pulsed radar equipment, high-energy gas lasers, radiotherapy devices, particle accelerators and in Tesla coils and similar devices. Thyratrons are also used in high-power UHF television transmitters, to protect inductive output tubes from internal shorts, by grounding the incoming high-voltage supply during the time it takes for a circuit breaker to open and reactive components to drain their stored charges. This is commonly called a \"crowbar circuit\".\n\nThyratrons have been replaced in most low and medium-power applications by corresponding semiconductor devices known as thyristors (sometimes called silicon-controlled rectifiers, or SCRs) and triacs. However, switching service requiring voltages above 20 kV and involving very short risetimes remains within the domain of the thyratron.\n\nVariations of the thyratron idea are the krytron, the sprytron, the ignitron, and the triggered spark gap, all still used today in special applications, such as nuclear weapons (krytron) and AC/DC-AC power transmission (ignitron).\n\nThe \"885\" is a small thyratron tube, using xenon gas. This device was used extensively in the timebase circuits of early oscilloscopes in the 1930s. It was employed in a circuit called a relaxation oscillator. During World War II, small thyratrons similar to the 885 were utilized in pairs to construct bistables, the \"memory\" cells used by early computers and code breaking machines. Thyratrons were also used for phase angle control of alternating current (AC) power sources in battery chargers and light dimmers, but these were usually of a larger current handling capacity than the 885. The 885 is a 2.5 volt, 5-pin based variant of the 884/6Q5.\n\n\n"}
{"id": "36083734", "url": "https://en.wikipedia.org/wiki?curid=36083734", "title": "Unchambered long barrow", "text": "Unchambered long barrow\n\nThe unchambered long barrow earthen long barrow, non-megalithic long barrow or non-megalithic mound ( or \"Hünenbett ohne Kammer\"), is a type of long barrow found across the British Isles, in a belt of land in Brittany, and in northern Europe as far east as the River Vistula (the \"Niedźwiedź\" type graves - NTT). The term \"unchambered\" means that there is no stone chamber within the stone enclosure. In Great Britain they are often known as non-megalithic long barrows or unchambered long cairns. \nSince the 1980s, barrows of the Passy type, part of the Cerny culture, have been discovered in the French département of Essonne in the Paris Basin. These are not, however, megalithic structures.\n\nNeolithic monuments are an expression of the culture and ideology of neolithic communities. Their emergence and function are a hallmark of social development.\n\nIn the region occupied by the peoples of the Funnelbeaker culture (TBK), unchambered long barrows fall into the megalith category because, in many cases, their generally very low mounds, which are located mainly along the lower reaches of the rivers Elbe (Lower Elbe), Oder and Vistula, have an enclosure of megaliths, about one metre high. Due to their small dimensions they were not suitable for constructing chambers, which is why there are no chambers made of large stone blocks. The enclosures (see Nordic megalith architecture) are trapezoidal or rectangular. East of the River Oder they are often trapezoidal or triangular with rounded tips, (Mound 9 at Sarnowo, near Konin, Poland) mostly, however, without transverse walls (megalithic and non-megalithic) dividing them into separate chambers. The site of Kritzow (Ludwigslust-Parchim), has guardian stones higher than a man. Apart from the sites researched by Ewald Schuldt in Mecklenburg-Vorpommern in Gnewitz, Rothenmoor and Stralendorf there are a further 11 in the area and five more examples in the forest of Sachsenwald. One group of three grave sites was first discovered in 1969 in the \"Alt Plestliner Holz\", Vorpommern-Greifswald. One of these enclosures is 80 metres long. Five unchambered barrows were investigated in the 19th century by J. Ritter in the county of Hagenow.\n\nAll these sites are characterized by clearly defined mounds of stone (cobbles), which are covered beneath the mound with packed boulders. In the complex of Stralendorf (Ludwigslust-Parchim county) were six such mounds of cobbles, lying transversely and longitudinally, bounded by a 125-metre-long trapezoidal enclosure. Such mounds are sometimes found outside the enclosures or are found in or adjacent to barrows in which there are chambers, for example, in two of the four barrows of Grundoldendorf. The barrow of Alter Hau in the forest of Sachsenwald has a length of 154 metres and is one of the longest sites in Nordic megalith architecture.\n\nThe Tinnum long barrow (\"Langbett von Tinnum\") on the island of Sylt is a long barrow that has neither a chamber nor a megalithic enclosure, but is constructed of stones about the size of a football. It clearly represents a transitional type.\n\nIf one considers sites without stone enclosures, whose mound had an enclosure of wooden posts in the past, of which there is now little trace, then the category of unchambered long barrows widens further, for example, to include the Tinnum long barrow, Barkjær (in Djursland) or Danica Nørremark (on Jutland). These so-called \"Konens Høj type (Danish) or \"Niedźwiedź\" type graves (Polish) are especially common in the Funnelbeaker culture area east of the River Oder.\n\nThe 200 or so British earthen long barrows were constructed with an enclosure of wooden posts. They are especially common in Wiltshire and Yorkshire. Three sites lie in Scotland and one on the Isle of Man. The barrows were formed over wooden chambers. In East Scotland there is another chamberless and non-megalithic variant: the \"chamberless cairn\", of which there are about 50 cairns without chambers. These only occur in England (12) in Cumbria and Northumberland.\n\nThe earth mounds or tumuli in Brittany are pre-megalithic, such as the \"tertres allongés\" in Landes and Morbihan. They are low, slab-enclosed mounds, 15 to 35 metres wide and 40 to 100 metres long. They are rectangular or oval and contain dry walled internal structures for cremation ashes and grave goods. In the early megalithic period oversized earth mounds emerged, like the tumulus of Carnac, that has ciste-like elements. A newly discovered barrow of this type lies in La Trinité-sur-Mer.\n\nBarrows with enclosures of wooden posts (without stone) are the Middle Neolithic enclosures of the Passy type, some of which are ascribed to the Cerny culture. This type of mound with wooden post or palisade enclosures are also found in the region of the early Funnelbeaker culture: the \"Konens Høj\" and \"Niedźwiedź\" type graves in Central Germany and Poland.\n\nBritish, French and Nordic sites have no cultural connexion with one another at all.\n\n"}
{"id": "48334299", "url": "https://en.wikipedia.org/wiki?curid=48334299", "title": "Worldwide energy supply", "text": "Worldwide energy supply\n\nWorldwide energy supply is the global production and preparation of fuel, generation of electricity, and energy transport. Energy supply is a vast industry, powering the world economy. More than 10% of the world expenditures is used for energy purposes.\n\nOf all produced energy 80% is fossil. Half of that is produced by China, the United States and the Arab states of the Persian Gulf. The Gulf States and Norway export most of their production, largely to the European Union and Japan where not sufficient energy is produced to satisfy their users. Energy production increases slowly, except for solar and wind energy which grows more than 20% per year.\n\nProduced energy, for instance crude oil, is processed to make it suitable for consumption by end users. The supply chain between production and final consumption involves many conversion activities and much trade and transport among countries, causing a loss of one third of energy before it is consumed.\n\nEnergy consumption per person in N-America is very high while in developing countries it is low and more renewable.\n\nWorldwide carbon dioxide emission from fuel combustion was 32 gigaton in 2015. In view of contemporary energy policy of countries the IEA expects that the worldwide energy consumption in 2040 will have increased more than a quarter and that the goal, set in the Paris Agreement about Climate Change, will not nearly be reached.\n\nThis is the worldwide production of energy, extracted or captured directly from natural sources. In energy statistics Primary Energy (PE) refers to the first stage where energy enters the supply chain before any further conversion or transformation process.\n\nEnergy production is usually classified as\n\nPrimary energy assessment follows certain rules to ease measurement and comparison of different kinds of energy. Due to these rules uranium is not counted as PE but as the natural source of nuclear PE. Similarly water and air flow energy that drives hydro and wind turbines, and sunlight that powers solar panels, are not taken as PE but as PE sources.\n\nThe table lists the worldwide PE production and the countries/regions producing most (90%) of that.\nThe amounts are given in million tonnes of oil equivalent per year (1 Mtoe/a = 11.63 TWh/a = 1.327 GW). The data are of 2015.\n\nClick on a column header to arrange countries/regions by that kind of primary energy.\n\nThe top producers of the USA are Texas 20%, Wyoming 11%, Pennsylvania 8%, W Virginia 5% and Oklahoma 4%.\n\nIn the Mid-East the Persian Gulfstates Iran, Iraq, Kuwait, Oman, Qatar, Saudi Arabia and the Arab Emirates produce most. A small part comes from Bahrain, Jordan, Lebanon, Syria and Yemen.\n\nThe top producers in Africa are Nigeria (254), S-Africa (167), Algeria (138) and Angola (100).\n\nIn the EU France (138, mainly nuclear), Germany (120), UK (119), Poland (68, mainly coal) and Netherlands (48, mainly natural gas) produce most.\n\nOf the world renewable supply 1319 is biofuel and waste, mostly in developing countries, 334 is generated with hydro power and 200 with other renewables.\n\nFor more detailed energy production see\n\nFrom 2010 to 2015 worldwide production increased 8%, with big differences among regions. The EU produced 9% less, Africa 5% less, China 12% more, the USA 17% more.\nA small part of the renewables, solar and wind energy, increased fast: a factor 3. in line with the strong growth since 1990. In China not only solar and wind increased, 5 times, but also nuclear production, 130%.\n\nPrimary energy is converted in many ways to energy carriers, also known as secondary energy.\n\nElectricity generators are driven by\n\n\nThe invention of the PV cell in 1954 started electricity generation by solar panels, connected to a power inverter. Around 2000 mass production of panels made this economic.\n\nMuch of primary and converted energy is traded among countries, about 5350 Mtoe/a worldwide, mostly oil and gas.\nThe table lists countries/regions with large difference of export and import.\nA negative value indicates that much energy import is needed for the economy.\nThe quantities are expressed in Mtoe/a and the data are of 2015.\n\nBig transport goes by tanker ship, tank truck, LNG carrier, rail freight transport, pipeline and by electric power transmission.\n\n32% of primary production is used for conversion and transport, and 6% for non-energy products like lubricants, asphalt and petrochemicals. 62% remains for end-users.\n\nTotal Primary Energy Supply (TPES) indicates the sum of production and imports subtracting exports and storage changes. For the whole world TPES nearly equals primary energy PE but for countries TPES and PE differ in quantity and quality. Usually secondary energy is involved, e.g., import of an oil refinery product, so TPES is often not PE. P in TPES has not the same meaning as in PE. It refers to energy needed as \"input\" to produce some or all energy for end-users.\n\nThe table lists the worldwide TPES and the countries/regions using most (81%) of that in 2015.\n\nThis is the worldwide consumption of energy by end-users. This energy consists of fuel (80%) and electricity (20%). The tables list amounts, expressed in million tonnes of oil equivalent per year (1 Mtoe = 11.63 TWh), how much of these is renewable energy, and energy used per person per year in toe/a. Non-energy products are not considered here. The data are of 2015.\n\nFuel:\nThe amounts are based on lower heating value.\n\nElectricity:\n\nThe first table lists worldwide final consumption and the countries/regions which use most (83%). In developing countries fuel consumption per person is low and more renewable. Canada, Venezuela and Brazil generate most electricity with hydropower.\n\nIn Africa 32 of the 48 nations are declared to be in an energy crisis by the World Bank. See Energy in Africa.\n\nThe next table shows countries consuming most (83%) in the European Union, and Norway. The last four countries generate electricity largely renewable.\nFor more details in Europe see Energy in Germany, Energy in France, etc.\n\nSome fuel and electricity is used to construct, maintain and demolish/recycle installations that produce fuel and electricity, such as oil platforms, uranium isotope separators and wind turbines.\nFor these producers to be economic the ratio of energy returned on energy invested (EROEI) or energy return on investment (EROI) should be large enough.\nThere is little consensus in the technical literature about methods and results in calculating these ratios, but it is likely that for fuels (fossil and nuclear), hydro power and wind turbines the ratio is at least 10, for solar panels about 7 and for solar collectors (hot water) only 2.\nIn southern European countries solar EROEI exceeds ten but more to the North it is less as it takes a greater part of the life time to regain the invested energy.\n\nBased on examination of Current Policies the IEA expects \"increasing strains on almost all aspects of energy security\".\nIncluding new announced policies and targets the IEA estimates that to 2040 the global energy demand will have increased more than a quarter due to developing countries led by India. Renewable energy meets nearly 40% of this growth, oil and gas consumption rises by 50% and coal use will not grow. Global energy-related CO emissions increase slightly. The IEA calls this New Policies Scenario \"far out of step with what scientific knowledge says will be required to tackle climate change.\"\n\nIn the Sustainable Development Scenario, overall primary energy demand in 2040 can be kept at today’s level by increasing efficiency. Renewable sources can increase their share to 30% half of which is solar and wind, nuclear increases to 10% by growth in Asia, natural gas increases slightly, oil peaks soon, coal declines immediately. Fossil fuels will meet 60% of total demand, down from 82% now. CO emission can reduce from 33 Mton in 2017 to 18 Mton in 2040 by capture and storage, decreasing methane emissions and eliminating flaring.\nGlobal electricity generation can increase 40% to 36000 TWh (= 3200 Mtoe) of which 64% is renewable. The direct use of renewables (bioenergy, solar and geothermal heat) to provide heat and mobility can grow strongly. But the actions taken by governments will be decisive.\n\n\n"}
{"id": "1702894", "url": "https://en.wikipedia.org/wiki?curid=1702894", "title": "Zolotnik", "text": "Zolotnik\n\nA zolotnik (in Russian: золотни́к; abbr.: zol.) was a small Russian unit of weight, equal to 0.1505 avoirdupois ounces, or 4.2658 grams (about 65.83 grains). Used from the 10th to 20th centuries, its name is derived from the Russian word \"zoloto\", meaning gold. As a unit, the zolotnik was the standard for silver manufacture, much as the troy ounce is currently used for gold and other precious metals.\n\nThis unit was originally based on a coin of the same name. The zolotnik circulated in the Kievan Rus until the 11th century; it was equal in weight to the Byzantine Empire's solidus.\n\nThe Russian pound was known as the funt. There were 96 zolotnik in a pound. A smaller unit, the lot, was equal to three zolotnik. There were 96 dolya in a single zolotnik.\n\nThe zolotnik was also used to measure fineness of precious metals (gold, silver, platinum). In this case, the ratio zolotnik/funt was meant, so one zolotnik meant 1/96. For example, 14-karat (58.33%) gold was named \"56-zolotnik gold\" in Russia. As one karat means 1/24, one zolotnik is 1/4 karat.\n\n\n\n"}
