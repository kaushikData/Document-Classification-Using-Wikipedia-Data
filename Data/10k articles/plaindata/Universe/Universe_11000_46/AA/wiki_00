{"id": "20806064", "url": "https://en.wikipedia.org/wiki?curid=20806064", "title": "3T Cycling", "text": "3T Cycling\n\n3T Cycling is an Italian cycle sport company associated with many champion cyclists. It was founded in 1961 and soon won a reputation for lightweight racing cycle componentry.\n\nThe company used aerospace-grade aluminum alloys in its products, designing handlebars with input from professional riders.\n\n3T switched production to carbon-fiber composite materials and in 2008 returned to pro cycling after several years' absence. For the 2008 season it sponsored the CSC team, which won the Tour de France. 3T sponsored three pro teams for the 2009 professional season.\n\nIn 2013 3T was a sponsor for .\n\nIn June 2017, 3T introduced its first bicycle, the Strada. The bicycle was notable for eliminating one of the front rings, doing away with the need for a front derailleur.\n\n3T is an Italian cycle sport firm, located in Bergamo, near Milano. It was originally known as 3TTT — Tecnologia del Tubo Torino (Turin Tube Technology). Many competitions have been won on 3T components, including the Tour de France, Olympic races, World Championships, and the World Hour record.\n\nThe firm was founded by Mario Dedioniggi in Torino in 1961. Dedioniggi was skilled at manipulating and bending steel tubes to fabricate the lightweight handlebars desired by racing cyclists, and Italian professional riders were among 3T’s first customers.\n\nBy 1970 3T handlebars and stems were in widespread use in the European professional peloton. In the quest for lighter weight, 3T switched production to aluminum alloy in place of steel. It was among the first to use aluminum for these components, where strength is critical to safety.\n\nIn 1975 it produced the Superleggera handlebar weighing 250 grammes — claimed to be the world's lightest drop handlebar. This bar was the first to be made of 7075 aluminium alloy, a material usually used in aerospace applications. The high strength-to-weight ratio of this alloy, also known as Ergal, found uses in other sports: 3T ski poles sold well for several seasons during the 1970s.\n\nThe firm worked closely with professional racers to refine the design of their handlebars. The various 'bends' took their name from the champions of the era — Merckx, Saronni, Moser, and Gimondi. In 1984 Francesco Moser used a newly developed 3T bar to capture the world hour record, breaking through the 50-kilometre barrier for the first time.\n\nThis new 'bullhorn' bar put the rider in a lower, more aerodynamic position for greater speed. Most riders in search of pure speed now use a similar position. In Moser's honor, 3T named the bar '51.151' — the distance Moser covered in kilometres (31.784 miles) at the Mexico City velodrome on January 23, 1984.\n\n3T continued to develop new handlebar designs to assist riders reach their goal of greater speed. The growing power in world cycling, the United States, started to take notice. Scott USA commissioned 3T to create the first handlebars specific to triathlon use, and in a departure from its roots in road racing, 3T partnered with Salsa Cycles to create a range of mountain-bike components. Success was immediate: in 1994, Nicolas Vouilloz won the third of his many World Championship downhill titles in Vail (CO) on a 3T bar.\n\nBicycle components underwent radical change in the 1990s. The traditional quill handlebar stem was rendered obsolete by the introduction of the threadless headset. 3T responded with the industry's first forged stem for this application, the ForgeAhead. Other manufacturers followed 3T's lead and switched to this technique, abandoning welded stems.\n\nBy the late 1990s, carbon-fiber composites were becoming the major driver of change. 3T, best known for its skill in aluminum alloy fabrication, took up the challenge of re-engineering to build components in composite materials. Race successes continued with Gold in the World Championships in 2000 and 2002 and a string of popular wins for Erik Zabel in the Italian home classic, Milan–San Remo, but it was not until 2008 that 3T again saw victory in a major tour.\n\nFor the 2008 season, 3T sponsored Team CSC. The team debuted a new, all-composites aerofoil time-trial bar, the 3T Ventus. The team's World Champion time trialist, Fabian Cancellara, rode this to victory at the Prologue of the Amgen Tour of California. Moving on to the European season at the Giro d'Italia, CSC's youthful team narrowly failed to win the race's opening Team Time Trial.\n\nGreat success followed at the Tour de France. CSC captain Carlos Sastre won the Yellow Jersey of the Overall winner, his team mate Andy Schleck won the White Jersey for Best Young Rider, and CSC won Best Team. Race reports noted Sastre's outstanding performance in limiting his losses in the final weekend's Individual Time trial, where he was widely expected to lose the lead to his nearest rival, Cadel Evans.\n\nRiding for Switzerland at the Beijing Olympics shortly after, Cancellara won Gold in the Men's Time Trial, beating his team mate Gustav Larsson, riding for Sweden, into second place, both riders using the Ventus bar. Their team captain Carlos Sastre animated the Men's Road Race, while Cancellara eventually placed third, one of three riders in the first five finishers riding 3T.\n\nFor the 2009 season, 3T is sponsoring three professional teams: Cervélo TestTeam (captained by Carlos Sastre), Garmin Slipstream, and Milram.\n"}
{"id": "2378513", "url": "https://en.wikipedia.org/wiki?curid=2378513", "title": "APC Smart-UPS", "text": "APC Smart-UPS\n\nThe Smart-UPS is a series of enterprise-level uninterruptible power supplies (UPS) made by American Power Conversion (APC). Most of the units have a SmartSlot (with the exception of SC and SMC series) which accepts an optional interface card providing features ranging from network connectivity to temperature and humidity monitoring. With the exception of RT and SRT series, Smart-UPS units are line-interactive UPS systems, only running their inverters when the grid power is unavailable.\n\nThere are a few different variations in the Smart-UPS lineup.\n\n\nAs a way to provide UPS management, monitoring and automatic shutdown of attached equipment, all Smart-UPS models include at least one serial data interface (RS-232 or USB), while most models also have at least one \"SmartSlot\" expansion port, with the larger (and in many cases older) Smart-UPS models supporting two SmartSlots. Availability of more SmartSlots in a single Smart-UPS unit can be achieved via expansion modules such as the AP9600 and AP9604 models. Installing an add-on card into a SmartSlot will provide the UPS with additional features, capable beyond the default serial data interface.\n\nEarly SmartSlot cards, such as the AP9605, provide SNMP functionality and Telnet access. The AP9606 and later cards add a web interface that can be used to configure and administer the UPS, as well as email-based alerting. The AP9617 and newer cards add 10/100Base-T connectivity, Secure HTTP, Secure Shell (SSH), RADIUS, SNMP Version 3, PCNS and syslog functionalities. The AP9612, AP9618, AP9619 and AP9631 models provide environmental monitoring when used together with APC temperature/humidity probes. The AP9618 model also provides out-of-band management via a modem connection in case the 10/100 Ethernet connection is down.\n\nSmartSlot cards remain powered by the battery even when the UPS is switched off, allowing the UPS to be remotely cold-started even in a power loss situation (providing the network infrastructure is still powered up and functioning). The cards will also continue to work for a short while after the UPS has been switched off either manually or due to a low battery condition.\n\nBeginning with the 2009 release of the SMT models (and later SMX and SURTD models), the old RS-232 data interface (a serial port with DB-9 connector) has been replaced with a 10-pin RJ50 socket and an RJ50-to-DB-9 cable connecting to the protected computer's serial port (together with a standard USB interface). Also, SmartSlot interface has been electrically modified in a backward-incompatible way and appropriately keyed mechanically. As stated by APC, \"the new connection arrangement denotes the new signalling systems.\" These new serial and SmartSlot interfaces use the new \"Microlink\" signalling protocol which, unlike the previous APC protocol \"UPSLINK\", has not been publicly documented. However, using the new AP9620 interface card, users are able to add support for the previous signalling protocol to the newer SMT and SMX models. Addition of this card makes the current models backwards compatible with older software built for the older Smart-UPS series.\n\n, an additional option has been made available in form of new firmware for certain SMT and SURTD models, adding publicly documented Modbus signalling capability to the proprietary Microlink protocol, allowing third-party developers to support UPS signalling and control on the newer Smart-UPS series.\n\nIn late 2009, with the release of the SMT and SMX product lines, SmartSlot was migrated to a new communications platform utilizing the APC Microlink protocol. Both electrical and mechanical (slot keying) properties of the new SmartSlot make older SmartSlot cards incompatible with current Smart-UPS models.<ref name=\"SMT/SMX FAQ\"></ref> For example, current () Smart-UPS model SMT1500 is compatible only with AP9613, AP9620, AP9630 and AP9631 SmartSlot cards.\n\nThe new card/slot design is referred-to as \"Network Management Card 2\" or NMC2. However, most new-style SmartSlot cards (including AP9613, AP9630 and AP9631) are backward compatible with older Smart-UPS models.\n\n\n\n"}
{"id": "14260512", "url": "https://en.wikipedia.org/wiki?curid=14260512", "title": "An Exceptionally Simple Theory of Everything", "text": "An Exceptionally Simple Theory of Everything\n\n\"An Exceptionally Simple Theory of Everything\" is a physics preprint proposing a basis for a unified field theory, often referred to as \"E Theory\", which attempts to describe all known fundamental interactions in physics and to stand as a possible theory of everything. The paper was posted to the physics arXiv by Antony Garrett Lisi on November 6, 2007, and was not submitted to a peer-reviewed scientific journal. The title is a pun on the algebra used, the Lie algebra of the largest \"simple\", \"exceptional\" Lie group, E. The paper's goal is to describe how the combined structure and dynamics of all gravitational and Standard Model particle fields, including fermions, are part of the E Lie algebra.\n\nThe theory is presented as an extension of the grand unified theory program, incorporating gravity and fermions. In the paper, Lisi states that all three generations of fermions do not directly embed in E with correct quantum numbers and spins, but that they must be described via a triality transformation, noting that the theory is incomplete and that a correct description of the relationship between triality and generations, if it exists, awaits a better understanding.\n\nThe theory received accolades from a few physicists amid a flurry of media coverage, but also met with widespread skepticism. \"Scientific American\" reported in March 2008 that the theory was being \"largely but not entirely ignored\" by the mainstream physics community, with a few physicists picking up the work to develop it further. In a follow-up paper, Lee Smolin proposed a spontaneous symmetry breaking mechanism for obtaining the classical action in Lisi's model, and speculated on the path to its quantization. In July 2009, Jacques Distler and Skip Garibaldi published a critical paper in \"Communications in Mathematical Physics\" called \"There is no 'Theory of Everything' inside E\", arguing that Lisi's theory, and a large class of related models, cannot work. They offer a direct proof that it is impossible to embed all three generations of fermions in E, or to obtain even the one-generation Standard Model without the presence of an antigeneration. In response to Distler and Garibaldi's paper, Lisi argued in a new paper, \"An Explicit Embedding of Gravity and the Standard Model in E\", peer reviewed and published in a conference proceedings, that Distler and Garibaldi's assumptions about fermion embeddings are incorrect and that the antigeneration is not by itself a problem sufficient to rule out the one-generation Standard Model. In July 2010, a group of mathematicians and physicists, including David Vogan, Garibaldi, and Lisi, met for a week-long conference in Banff to discuss the mathematics and physics related to the exceptional groups. In December 2010, \"Scientific American\" published a feature article on \"A Geometric Theory of Everything\", authored by Lisi and James Owen Weatherall. In May 2011, Lisi wrote an entry in the blog section of \"Scientific American\" addressing some of the criticism of his theory and how it had progressed, noting that the theory was still incomplete and made only tenuous predictions, with a precise description of the three generations of fermions and their masses remaining as the largest outstanding problem. In June 2015, Lisi posted a paper, \"Lie Group Cosmology”, describing the geometry of E Theory as an extension of Cartan geometry, and providing a description of the three generations of fermions via triality, while not predicting their masses.\n\nThe goal of E Theory is to describe all elementary particles and their interactions, including gravitation, as quantum excitations of a single Lie group geometry—specifically, excitations of the noncompact quaternionic real form of the largest simple exceptional Lie group, E. A Lie group, such as a one-dimensional circle, may be understood as a smooth manifold with a fixed, highly symmetric geometry. Larger Lie groups, as higher-dimensional manifolds, may be imagined as smooth surfaces composed of many circles (and hyperbolas) twisting around one another. At each point in a N-dimensional Lie group there can be N different orthogonal circles, tangent to N different orthogonal directions in the Lie group, spanning the N-dimensional Lie algebra of the Lie group. For a Lie group of rank R, one can choose at most R orthogonal circles that do not twist around each other, and so form a \"maximal torus\" within the Lie group, corresponding to a collection of R mutually-commuting Lie algebra generators, spanning a \"Cartan subalgebra\". Each elementary particle state can be thought of as a different orthogonal direction, having an integral number of twists around each of the R directions of a chosen maximal torus. These R twist numbers (each multiplied by a scaling factor) are the R different kinds of elementary charge that each particle has. Mathematically, these charges are eigenvalues of the Cartan subalgebra generators, and are called roots or weights of a representation.\n\nIn the Standard Model of particle physics, each different kind of elementary particle has four different charges, corresponding to twists along directions of a four-dimensional maximal torus in the twelve-dimensional Standard Model Lie group, SU(3)×SU(2)×U(1). The two strong “color” charges, g and g, correspond to twists along directions in the two-dimensional maximal torus of the eight-dimensional SU(3) Lie group of the strong interaction. The weak isospin, T (or W), and weak hypercharge, Y (or Y), correspond to twists along directions in the two-dimensional maximal torus of the four-dimensional SU(2)×U(1) Lie group of the electroweak interaction, with W and Y combining as electric charge, Q. Whenever an interaction occurs between elementary particles, with two coming together and becoming a third, or one particle becoming two, each type of charge must be conserved. For example, a red up quark, having charges (gformula_1, gformula_2, \"W\"formula_3, \"Y\"formula_4) can interact with a weak boson, \"W\", having charges (\"g\" = 0, \"g\" = 0, \"W\" = −1, \"Y\" = 0), to produce a red down quark, having charges (\"g\" formula_1, \"g\"formula_2, \"W\"formula_7, \"Y\"formula_4). The complete pattern of all Standard Model particle charges in four dimensions may be projected down to two dimensions and plotted in a charge diagram.\n\nIn grand unified theories (GUTs), the 12-dimensional Standard Model Lie group, SU(3)×SU(2)×U(1) (modded by Z), is considered as a subgroup of a higher-dimensional Lie group, such as of 24-dimensional SU(5) in the Georgi–Glashow model or of 45-dimensional Spin(10) in the SO(10) model (Spin(10) being the double cover of SO(10), and having the same Lie algebra). Since there is a different elementary particle for each dimension of the Lie group, in addition to the 12 Standard Model gauge bosons there are 12 X and Y bosons in the SU(5) Model and 18 more X bosons and 3 W' and Z' bosons in Spin(10). In Spin(10) there is a five-dimensional maximal torus, and the Standard Model hypercharge, Y, is a combination of two new Spin(10) charges: “weaker charge”, W', and baryon minus lepton number, B. In the Spin(10) model, one generation of 16 fermions (including left-handed electrons, neutrinos, three colors of up quarks, three colors of down quarks, and their anti-particles) lives neatly in the 16-complex-dimensional spinor representation space of Spin(10). The combination of these 32 real fermions and 45 bosons, along with another U(1) Lie group (corresponding to Peccei–Quinn symmetry), constitute the 78-dimensional real compact exceptional Lie group, E6. (This unusual algebraic structure, reminiscent of supersymmetry, of gauge fields and spinors combined in a simple Lie group, is characteristic of the exceptional groups.)\n\nAs well as being in some representation space of the Standard Model or Grand Unified Theory Lie group, each physical fermion is a spinor under the gravitational noncompact Spin(1,3) Lie group of rotations and boosts. This six-dimensional Lie group has a two-dimensional maximal torus (technically a hyperboloid) and thus two kinds of charge, spin, S, and boost, S. A Dirac fermion (consisting of fermion and anti-fermion) has eight real degrees of freedom corresponding to its real vs. imaginary parts, left or right chirality, and being spin up or down. Using the Lie group equivalence of Spin(1,3) and SL(2,C), and the chirality of Standard Model weak force fermion interactions, each fermion (and each anti-fermion) can be described as a two-complex-dimensional left-chiral Weyl spinor under gravitational SL(2,C). Accounting for the up or down spin for each of the 16 left-chiral fermions of one generation (or 15 fermions if neutrinos are Majorana), each fermion generation corresponds to 64 (or 60) real degrees of freedom.\n\nIn GraviGUT unification, the gravitational Spin(1,3) and Spin(10) GUT Lie groups are combined (modded by Z) as parts of a Spin(11,3) Lie group, acting on each generation of fermions in a real 64-dimensional spinor representation. The remaining parts of Spin(11,3) include the 4-dimensional spacetime frame and a Higgs field transforming as a 10 under Spin(10). The resulting gauge theory of gravity, Higgs, and gauge bosons is an extension of the MacDowell-Mansouri formalism to higher dimensions. Several physicists objected to the apparent violation of the Coleman-Mandula theorem, which states the impossibility of mixing gravity and gauge fields in a unified Lie group over spacetime, given reasonable assumptions. Proponents of GraviGUT unification and E Theory claim that the Coleman-Mandula theorem is not violated because the assumptions are not met.\n\nIn E Theory, it is observed that the GraviGUT algebra of spin(11,3) acting on one generation of fermions in a real positive-chiral 64-spinor, 64, can be part of the 248-dimensional real quaternionic e8 Lie algebra,\n\nThe strongest criticism of E Theory, stated by Distler, Garibaldi, and others, including Lisi in the original paper, is that given an embedding of gravitational spin(1,3) in the spin(12,4) subalgebra of e8, the 128 includes not only the 64 of a generation of fermions, but a 64 “anti-generation” of mirror fermions with non-physical chirality. Since we do not see mirror fermions in nature, Distler and Garibaldi consider this to be a disproof of E Theory. Lisi has voiced two responses to this criticism. The first response is that these mirror fermions might exist and have very large masses. The second response, stated in the original paper and in his latest work, is that there is not a single embedding of gravitational spin(1,3) in e8, but three embeddings related by triality, with respect to which the 64 contains a second generation of physical fermions, and the third generation of fermions is contained within spin(12,4).\n\nThe algebraic breakdown of the 248-dimensional e8 Lie algebra relevant to E8 Theory is\n\nThis decomposition, attributed to Bertram Kostant, relies on the triality isomorphism between eight-dimensional vectors, 8, positive-chiral spinors, 8, and negative-chiral spinors, 8, relating to the division algebra of the octonions. Within this decomposition, the strong force su(3) embeds in spin(8), three triality-related gravitational spin(1,3)’s embed in spin(4,4), the three generations of 60 fermions embed in 8 ⊗ 8 + 8 ⊗ 8 + 8 ⊗ 8, and the gravitational frame, Higgs, and electroweak bosons embed throughout, with 18 colored X bosons remaining as new predicted particles.\n\nIn E Theory’s current state, it is not possible to calculate masses for the existing or predicted particles. Lisi states the theory is young and incomplete, requiring a better understanding of the three fermion generations and their masses, and places a low confidence in its predictions. However, the discovery of new particles that do not fit in Lisi's classification, such as superpartners or new fermions, would fall outside the model and falsify the theory.\n\nThe fundamental geometric idea of E Theory is that our universe and its contents exists as quantum excitations of the largest simple real quaternionic exceptional Lie group, \"E\". This is described via an extension of Cartan geometry employing a superconnection. The relevant Cartan geometry is modeled on Klein geometry, beginning with a homogeneous space, G/H, in which the initial Lie group is \"G\" = \"E\" and the subgroup is \"H\" = SL(2,C)xS(U(3)×U(2))xZ, in which Z = {1,\"T\",\"T\"} is the cyclic group of order three corresponding to a triality automorphism, T, of E.\n\nUsually, in Cartan geometry, the deformation of a Lie group, \"G\", preserving the structure of a subgroup, \"H\", is described by allowing the Lie group’s Maurer–Cartan form, θ, to vary, becoming the Cartan connection,\n\nThe resulting geometry, \"G̃\", is that of a principal bundle, with W the principal H-connection, a 1-form valued in Lie(H) over a base manifold, \"B\", modeled on \"G\"/\"H\", with the frame, Ɛ, a 1-form valued in Lie(G/H). If H is a reductive subgroup of \"G\", the curvature of the Cartan connection is\n\nIn Lisi’s extension of Cartan geometry, the Cartan connection over B is interpreted as a superconnection,\n\nover spacetime, M (a subspace of B), in which, Ψ, a Lie(G/H) valued 1-form over B assumed to be orthogonal to M, is interpreted as a set of three fermionic (Grassmann number) fields over M, valued in Lie(G/H), related by triality, T. According to Lisi, the description of Grassmann number fermions as 1-forms orthogonal to spacetime, valued in a spinor representation, provides a clear geometric understanding of what fermions are. In E Theory, the H-connection is physically the gravitational spin connection, ½ω, plus the Standard Model and X boson gauge fields, H = g + W + B + X, while E is the 1-form frame over M, assumed to equal the gravitational frame 1-form, e, times (in the Clifford algebra sense) the Higgs, E = eΦ. The fermionic part of the superconnection, Ψ, is interpreted as the three generation multiplets of Standard Model fermions. The curvature of the resulting superconnection,\nis\n\nin which R = dω + ½ωω is the gravitational Riemann curvature 2-form, F = d H + H H is the gauge field curvature, T = de + ½ωe + ½eω is the gravitational torsion, DΦ = dΦ + [H,Φ] is the covariant derivative of the Higgs, and DΨ = dΨ + [½ω + H + eΦ,Ψ] is the covariant Dirac derivative of the fermions in curved spacetime.\n\nIn this geometric description, physical four-dimensional spacetime, M, is considered as a sheaf of gauge-related subspaces of G̃. For the case in which the curvature vanishes, F = 0, there is no excitation of the Lie group, G, and the Higgs field has a vacuum expectation value, Φ=Φ, corresponding to a positive cosmological constant, Λ = − 12 Φ, with the vacuum spacetime, as a subspace of G, identified as de Sitter spacetime, satisfying R = −6Λee.\n\nWithin a Lie group, the Maurer–Cartan form, θ, is the natural frame and determines the Haar measure for integration over the group manifold. With the Killing form of the Lie algebra, this also determines a natural metric and Hodge duality operator on the group manifold. For a deforming Lie group, the Maurer–Cartan form is replaced by the superconnection, G, defined over the entire deforming Lie group manifold via gauge transformation. This superconnection, G, determines the Hodge duality operator, formula_9, and the curvature, F, of the deforming Lie group. The action for E Theory is the Yang–Mills action, integrated over the entire deforming Lie group.\n\nSince the structure of the H subgroup and fermionic directions of B are preserved, this action reduces to an integral over spacetime,\n\nin which V is a constant volume factor, and the Hodge star, formula_9, is now the Hodge star for M determined by the gravitational frame, e. As well as the usual Einstein-Hilbert action for gravity, Yang–Mills action, and Higgs action, this action includes a quadratic torsion term, a quadratic curvature term, and a quadratic spinor Lagrangian.\n\nThree previous arXiv preprints by Lisi deal with mathematical physics related to the theory. \"Clifford Geometrodynamics\", in 2002, endeavors to describe fermions geometrically as BRST ghosts. \"Clifford bundle formulation of BF gravity generalized to the standard model\", in 2005, describes the algebra of gravitational and Standard Model fields acting on a generation of fermions, but does not mention E. \"Quantum mechanics from a universal action reservoir\", in 2006, attempts to derive quantum mechanics using information theory.\n\nBefore writing his 2007 paper, Lisi discussed his work on an Foundational Questions Institute (FQXi) forum, at an FQXi conference, and for an FQXi article. Lisi gave his first talk on E Theory at the Loops '07 conference in Morelia, Mexico, soon followed by a talk at the Perimeter Institute. John Baez commented on Lisi's work in \"This Week's Finds in Mathematical Physics (Week 253)\", Lisi's arXiv preprint, \"An Exceptionally Simple Theory of Everything\", appeared on November 6, 2007, and immediately attracted a great deal of attention. Lisi made a further presentation for the International Loop Quantum Gravity Seminar on November 13, 2007, and responded to press inquiries on an FQXi forum. He presented his work at the TED Conference on February 28, 2008.\n\nNumerous news sites from all over the world reported on the new theory in 2007 and 2008, noting Lisi's personal history and the controversy in the physics community. The first mainstream and scientific press coverage began with articles in \"The Daily Telegraph\" and \"New Scientist\", with articles soon following in many other newspapers and magazines.\n\nLisi's paper spawned a variety of reactions and debates across various physics blogs and online discussion groups. The first to comment was Sabine Hossenfelder, summarizing the paper and noting the lack of a dynamical symmetry breaking mechanism. Luboš Motl offered a colorful critique, objecting to the addition of bosons and fermions in Lisi's superconnection, and to the violation of the Coleman-Mandula theorem. In the presentation \"What's new at the arXiv?\" on May 20, 2008, Simeon Warner stated that Lisi's paper is the most downloaded article on the arXiv. Among the physicists early to comment on E Theory, Sabine Hossenfelder, Peter Woit and Lee Smolin were generally supportive, while Luboš Motl and Jacques Distler were critical.\n\nOn his blog, \"Musings\", Jacques Distler offered one of the strongest criticisms of Lisi's approach, claiming to demonstrate that, unlike in the Standard Model, Lisi's model is nonchiral — consisting of a generation and an anti-generation — and to prove that any alternative embedding in E must be similarly nonchiral. These arguments were distilled in a paper written jointly with Skip Garibaldi, \"There is no 'Theory of Everything' inside E\", published in \"Communications in Mathematical Physics\". In this paper, Distler and Garibaldi offer a proof that it is impossible to embed all three generations of fermions in E, or to obtain even the one-generation Standard Model. In a press release from his university, \"Rock climber takes on surfer's theory\", Garibaldi states that his article with Distler is a rebuttal of Lisi's theory. In response, Lisi argues that Distler and Garibaldi made unnecessary assumptions about how the embedding needs to happen. Addressing the one generation case, in June 2010 Lisi posted a new paper on E Theory, \"An Explicit Embedding of Gravity and the Standard Model in E\", peer reviewed and published in a conference proceedings, describing how the algebra of gravity and the Standard Model with one generation of fermions embeds in the E Lie algebra explicitly using matrix representations. When this embedding is done, Lisi agrees that there is an antigeneration of fermions (also known as \"mirror fermions\") remaining in E; but while Distler and Garibaldi state that these mirror fermions make the theory nonchiral, Lisi states that these mirror fermions might have high masses, making the theory chiral, or that they might be related to the other generations.\n\nThe group blog, \"The n-Category Cafe\", provides some of the more technical discussions, with posts by Lisi, Urs Schreiber, Kea, and Jacques Distler.\n\nThirty-eight arXiv preprints have cited Lisi's work. Lee Smolin's \"The Plebanski action extended to a unification of gravity and Yang–Mills theory\", December 6, 2007, proposes a symmetry breaking mechanism to go from an E symmetric action to Lisi's action for the Standard Model and gravity. Roberto Percacci's \"Mixing internal and spacetime transformations: some examples and counterexamples\" addresses a general loophole in the Coleman-Mandula theorem also thought to work in E Theory. Percacci and Fabrizio Nesti's \"Chirality in unified theories of gravity\" confirms the embedding of the algebra of gravitational and Standard Model forces acting on a generation of fermions in spin(3,11) + 64, mentioning that Lisi's \"ambitious attempt to unify all known fields into a single representation of E stumbled into chirality issues\". Mathematician Bertram Kostant discussed Lisi's work in a colloquium presentation at UC Riverside. In a joint paper with Lee Smolin and Simone Speziale, published in \"Journal of Physics A\", Lisi proposes a new action and symmetry breaking mechanism. In \"An Explicit Embedding of Gravity and the Standard Model in E\", Lisi describes E Theory using explicit matrix representations. In \"Lie Group Cosmology\", Lisi describes the extension of Cartan geometry underlying E Theory.\n\nOn August 4, 2008, FQXi awarded Lisi a grant for further development of E Theory.\n\nIn September 2010, \"Scientific American\" reported on a conference inspired by Lisi's work.\n\nIn December 2010 \"Scientific American\" published a feature article on E Theory, \"A Geometric Theory of Everything\", written by Lisi and James Owen Weatherall.\n\nIn December 2011, in his paper, \"String and M-theory: answering the critics\", for a Special Issue of Foundations of Physics: \"Forty Years Of String Theory: Reflecting On the Foundations\", Michael Duff argues against Lisi's theory and the attention it has received in the popular press. Duff states that Lisi's paper was incorrect, citing Distler and Garibaldi's proof, and criticizes the press for giving too much positive attention to an \"outsider\" scientist and theory.\n\n"}
{"id": "15773977", "url": "https://en.wikipedia.org/wiki?curid=15773977", "title": "Beaver Dyke Reservoirs", "text": "Beaver Dyke Reservoirs\n\nBeaver Dyke Reservoirs are two water supply reservoirs near to Harrogate, North Yorkshire, England. The main reservoir was constructed in 1890 and has a surface area of 9 Ha. The water is relatively nutrient rich, but it has been known to suffer from potentially toxic blooms of Blue green algae.\n\nAs of 2013 the main Beaver Dyke Reservoir is being decommissioned. The smaller reservoir (also called John O'Gaunts due to the proximity of John O'Gaunt's Castle) is still filled with water.\n"}
{"id": "1987207", "url": "https://en.wikipedia.org/wiki?curid=1987207", "title": "Black hole electron", "text": "Black hole electron\n\nIn physics, there is a speculative notion that if there were a black hole with the same mass, charge and angular momentum as an electron, it would share some of the properties of the electron. Most notably, Brandon Carter showed in 1968 that the magnetic moment of such an object would match that of an electron. This is interesting because calculations ignoring special relativity and treating the electron as a small rotating sphere of charge give a magnetic moment that is off by roughly a factor of 2, the so-called gyromagnetic ratio.\n\nHowever, Carter's calculations also show that a would-be black hole with these parameters would be 'super-extremal'. Thus, unlike a true black hole, this object would display a naked singularity, meaning a singularity in spacetime not hidden behind an event horizon. It would also give rise to closed timelike curves.\n\nStandard quantum electrodynamics (QED) treats the electron as a point particle, a view completely supported by experiment. There is no evidence that the electron is a black hole (or naked singularity). Furthermore, since the electron is quantum mechanical in nature, any description purely in terms of general relativity is inadequate.\n\nA paper published in 1938 by Albert Einstein, Leopold Infeld and Banesh Hoffmann showed that if elementary particles are treated as singularities in spacetime, it is unnecessary to postulate geodesic motion as part of general relativity. The electron may be treated as such a singularity.\n\nIf one ignores the electron's angular momentum and charge, as well as the effects of quantum mechanics, one can treat the electron as a black hole and attempt to compute its radius. The Schwarzschild radius of a mass is the radius of the event horizon for a non-rotating, uncharged black hole of that mass. It is given by\n\nwhere is Newton's gravitational constant and is the speed of light. For the electron,\n\nso\n\nThus, if we ignore the electric charge and angular momentum of the electron, and naively apply general relativity on this very small length scale without taking quantum theory into account, a black hole of the electron's mass would have this radius.\n\nIn reality, physicists expect quantum gravity effects to become significant even at much larger length scales, comparable to the Planck length\n\nSo, the above purely classical calculation cannot be trusted. Furthermore, even classically, electric charge and angular momentum affect the properties of a black hole. To take them into account, while still ignoring quantum effects, one should use the Kerr–Newman metric. If we do, we find the angular momentum and charge of the electron are too large for a black hole of the electron's mass: a Kerr-Newman object with such a large angular momentum and charge would instead be 'super-extremal', displaying a naked singularity, meaning a singularity not shielded by an event horizon.\n\nTo see that this is so, it suffices to consider the electron's charge and neglect its angular momentum. In the Reissner–Nordström metric, which describes electrically charged but non-rotating black holes, there is a quantity , defined by\n\nwhere is the electron's charge and is the vacuum permittivity. For an electron with = − = , this gives a value\n\nSince this (vastly) exceeds the Schwarzschild radius, the Reissner–Nordström metric has a naked singularity.\n\nIf we include the effects of the electron's rotation using the Kerr-Newman metric, there is still a naked singularity, which is now a ring singularity, and spacetime also has closed timelike curves. The size of this ring singularity is on the order of\n\nwhere as before is the electron's mass and is the speed of light, but = formula_5 is the spin angular momentum of the electron. This gives\n\nwhich is much larger than the length scale associated to the electron's charge. As noted by Carter, this length is on the order of the electron's Compton wavelength. Unlike the Compton wavelength, it is not quantum-mechanical in nature.\n\nMore recently, Alexander Burinskii has pursued the idea of treating the electron as Kerr–Newman naked singularity.\n\n\n\n"}
{"id": "31573758", "url": "https://en.wikipedia.org/wiki?curid=31573758", "title": "Bredig's arc method", "text": "Bredig's arc method\n\nBredig's arc method is a method of preparation of colloidal solution, of metals such as gold, silver or platinum.\n\nThis method consists of both dispersion and condensation. An arc is struck between electrodes, under the surface of water containing some stabilizing agent such as traces of KOH (potassium hydroxide).The intense heat of the arc vaporizes some of the metal which then condenses under cold water. The water is kept cold as an ice bath.The Colloidal particle prepared is stabilised by adding a small amount of KOH to it. \n\nThis method is not suitable when the dispersion medium is an organic liquid as considerable charring occurs.\n\n"}
{"id": "43474691", "url": "https://en.wikipedia.org/wiki?curid=43474691", "title": "Caking", "text": "Caking\n\nCaking is a powder's tendency to form lumps or masses rather than flow smoothly.\n\nFactors involved in the degree of caking include the substance's particle surface chemistry, particle size and particle-size distribution, particle shape, and environmental factors such as temperature and humidity. Caking also depends on any pressure on the powdered substance, and how long the substance has been stored.\n\nPhysically, the caking process involves electrostatic attraction or the formation of weak chemical bonds between the particle surfaces. This gives the bulk substance the strength to crack and collapse abruptly, to form vertical surfaces, and so on.\n\nUsually caking is undesirable, but is useful when pressing powdered substances into pills or briquettes. Granular materials can also be subject to caking, particularly those that are hygroscopic such as salt, sugar, and many chemical fertilizers, because of the effects of moisture on grain surfaces.\n\nCaking properties must be considered when designing and constructing bulk material handling equipment. Powdered substances that need to be stored, and flow smoothly at some time in the future, are often pelletized or made into pills. Anti caking agents are also commonly added to control caking.\n"}
{"id": "10148651", "url": "https://en.wikipedia.org/wiki?curid=10148651", "title": "Coal combustion products", "text": "Coal combustion products\n\nCoal combustion products (CCPs), also called coal combustion wastes (CCWs) or coal combustion residuals (CCRs), are categorized in four groups, each based on physical and chemical forms derived from coal combustion methods and emission controls:\n"}
{"id": "45485083", "url": "https://en.wikipedia.org/wiki?curid=45485083", "title": "Dark photon", "text": "Dark photon\n\nThe dark photon (also hidden, heavy, para-, secluded photon, or phaeton) is a hypothetical hidden sector particle, proposed as a force carrier similar to the photon of electromagnetism but potentially connected to dark matter. In a minimal scenario, this new force can be introduced by extending the gauge group of the Standard Model of Particle Physics with a new abelian U(1) gauge symmetry. The corresponding new spin-1 gauge boson (i.e. the dark photon) can then couple very weakly to electrically charged particles through kinetic mixing with the ordinary photon and could thus be detected. Other types of couplings beyond kinetic mixing are also possible.\n\nObservations of gravitational effects, that cannot be explained by visible matter alone, imply the existence of matter which does not or does only very weakly couple to the known forces of Nature. This dark matter dominates the matter density of the Universe, but its particles (if there are any) have eluded direct and indirect detection so far. Given the rich interaction structure of the well-known Standard Model particles, which make up only the subdominant component of the Universe, it is natural to think about a similarly interactive behaviour of dark sector particles. Dark photons could be part of these interactions among dark matter particles and provide a non-gravitational window (a so-called vector portal) into their existence by kinematically mixing with the Standard Model photon. Further motivation for the search for dark photons comes from several observed anomalies in astrophysics (e.g. in cosmic rays) that could be related to dark matter interacting with a dark photon. Arguably the most interesting application of dark photons arises in the explanation of the discrepancy between the measured and the calculated anomalous magnetic moment of the muon. This discrepancy is usually thought of as a persisting hint for physics beyond the Standard Model and should be accounted for by general new physics models. Beside the effect on electromagnetism via kinetic mixing and possible interactions with dark matter particles, dark photons (if massive) can also play the role of a dark matter candidate themselves. This is theoretically possible through the misalignment mechanism.\n\nAdding a sector containing dark photons to the Lagrangian of the Standard Model can be done in a straightforward and minimal way by introducing a new U(1) gauge field. The specifics of the interaction between this new field, potential new particle content (e.g. a Dirac fermion for dark matter) and the Standard Model particles are virtually only limited by the creativity of the theorist and the constraints that have already been put on certain kinds of couplings. The arguably most popular basic model involves a single new broken U(1) gauge symmetry and kinetic mixing between the corresponding dark photon field formula_1 and the Standard Model hypercharge fields. The operator at play is formula_2, where formula_3 is the field strength tensor of the dark photon field and formula_4denotes the field strength tensor of the Standard Model weak hypercharge fields. This term arises naturally by writing down all terms allowed by the gauge symmetry. After electroweak symmetry breaking and diagonalising the terms containing the field strength tensors (kinetic terms) by redefining the fields, the relevant terms in the Lagrangian are\n\nformula_5\n\nwhere formula_6is the mass of the dark photon (in this case it can be thought of as being generated by the Higgs or Stueckelberg mechansim), formula_7 is the parameter describing the kinetic mixing strength and formula_8denotes the electromagnetic current with its coupling formula_9. The fundamental parameters of this model are thus the mass of the dark photon and the strength of the kinetic mixing. Other models leave the new U(1) gauge symmetry unbroken, resulting in a massless dark photon carrying a long-range interaction. A massless dark photon, however, will experimentally be hard to distinguish from the Standard Model photon. The incorporation of new Dirac fermions as dark matter particles in this theory is uncomplicated and can be achieved by simply adding the Dirac terms to the Lagrangian.\n\n"}
{"id": "45094689", "url": "https://en.wikipedia.org/wiki?curid=45094689", "title": "Dipankar Das Sarma", "text": "Dipankar Das Sarma\n\nDipankar Das Sarma, popularly known as D.D. Sarma, is an Indian scientist and structural chemist, known for his researches in the fields of Solid State Chemistry, Spectroscopy, Condensed Matter Physics, Materials Science, and Nanoscience. He is a former MLS Chair Professor of Physics and Chairman of the Centre for Advanced Materials and the GAST Professor of Uppsala University, Sweden, A recipient of TWAS Physics Prize and the UNESCO Biennial Javed Husain Prize, Sarma was honored by the Council for Scientific and Industrial Research (CSIR), Government of India, in 1994, with the Shanti Swarup Bhatnagar Prize for Science and Technology.\n\nDipankar Das Sarma was born on 15 September 1955 in Kolkata, in West Bengal. He did a five-year integrated masters course in Physics from the Indian Institute of Technology, Kanpur in 1977 and enrolled for research at the Indian Institute of Science, (IISc) Bengaluru from where he secured his PhD in 1982 under the tutelage of renowned solid state chemist, C. N. R. Rao. He worked as a research associate at IISc for one year (1982–83), moved to Forschungszentrum Jülich, (Jülich Research Centre) Germany as a guest scientist in 1984 and returned to IISc as a lecturer in 1986. He stayed at IISc where he became the assistant professor in 1989, associate professor in 1993 and a professor in 1999. He remains a professor and the Chairman of the Solid State and Structural Chemistry Unit at the institution. He also served as a visiting professor at the University of Tokyo (2001–02) and at the Istituto di Struttura della Materia, CNR at their Rome and Trieste centres in 2002.\n\nProf. Sarma holds a number of academic positions in India and abroad. Presently, he is J.N. Tata Chair Professor at the Solid State and Structural Chemistry Unit of the Indian Institute of Science. Besides heading the Solid State and Structural Chemistry Unit, he served as a guest professor at the Uppsala University, Sweden, a Distinguished Scientist at the CSIR-Network Institutes of Solar Energy, an honorary professor at the Jawaharlal Nehru Centre for Advanced Scientific Research, Bengaluru and a distinguished visiting professor at the Indian Association for the Cultivation of Science, Kolkata. He has also been an adjunct professor at the Tata Institute of Fundamental Research, Mumbai till October 2014 for a second term, the initial tenure being a six-year period from 2003 to 2009. He also held the position of the distinguished visiting professor at the Indian Association for the Cultivation of Science, Kolkata for two terms, from 2004 to 2006 and from 2009 to 2014. He has also been the adjunct professor of the Indian Institute of Science Education and Research, Kolkata (2007–09), a senior associate at the S.N. Bose National Centre for Basic Sciences (2003-2006), a member of faculty at the UGC-DAE Consortium for Scientific Research. and an MLS Chair professor of the Centre for Advanced Materials.\n\nProf. Sarma is a member of many professional bodies such as the Council of Raja Ramanna Centre for Advanced Tachnology, Indore, the Steering Committee for the Sophisticated Analytical Instrument Facilities (SAIF) programme of the Department of Science and Technology, Government of India and the Academic Advisory Committee of the Jawaharlal Nehru Centre for Advanced Scientific Research and is the coordinator of the group set up by the Government of India for India's collaboration with the International Centre for Synchrotron-Light for Experimental Science Applications in the Middle East (SESAME). He sits in various committees of the Department of Science and Technology and the Council for Scientific and Industrial Research. He holds the chair of the Research Council of the National Chemical Laboratory, Pune and has held the chairs of the Proposal Review Committee of Elettra Synchrotron Centre, Trieste and the Scientific Advisory Committee of the UGC-DAE Consortium for Scientific Research of which he has also served as a member of the governing body and the governing council.\n\nD. D. Sarma is a former member of the Scientific Advisory Boards of CRANN and Trinity College Dublin, the Scientific Advisory Committees of the Inter University Consortium, Departamento de Asistencia Económica y Financiera, (DAEF) and the Inter University Accelerator Centre, New Delhi, the Academic Program Advisory Committee of the S.N. Bose National Centre for Basic Sciences, Kolkata and the councils of the Indian National Science Academy and the Raja Ramanna Centre for Advanced Technology, Indore. He has also served the scientific council of the Indo-French Centre for the Promotion of Advanced Research (IFCPAR), the general council of the Asia Pacific Centre for Theoretical Physics, Pohang, Korea, and the working group of the Department of Scientific and Industrial Research (DSIR) for the formulation of the 12th Five Year Plan, 2011 as a member.\n\nProf. Sarma is a Senior Editor of ACS Energy Letters and a member of the editorial boards of several peer reviewed journals such as \"J. Electron Spectroscopy and Related Phenomena\", \"Solid State Communication\", \"Indian Journal of Physics\", and \"Surface and Interface Analysis\". He has also been associated with \"Advances in Physical Chemistry\", \"Pramana – Journal of Physics\", Journal of Physical Chemistry, The Open Condensed Matter Physics Journal and Research Letters in Physical Chemistry, as an editorial board member. He is the Series Editor of \"Advances in Condensed Matter Science\", the South Asian regional Editor for \"J. Experimental Nanoscience\" and the Associate Editor of \"Applied Physics\". He is a reviewer for many scientific magazines and the American Physical Society selected him as an \"Outstanding Referee\" in 2009.\n\nDipankar Das Sarma is credited with extensive research on nanomaterials and strongly correlated materials. His contribution is reported in discovering the existence of \"a new phase in solid state materials through high-energy spectroscopies and theory\". His researches have been documented by way of several articles published in various peer reviewed journals. Google Scholar, an online repository of scientific articles has listed 521 of Sarma's articles and has accorded him an h-index of 38 (since 2010) and an i10-index of 128 (since 2010) and his articles have been cited over 13300 times. He holds many patents and Justia Patents has an online record of 16 of them.\n\nSarma is credited with the establishment of the Centre for Advanced Materials, a centre for advanced research on nanomaterials, smart materials, functional polymers, spintronics, strongly correlated electron systems, biomaterials and biology-inspired materials at the Indian Association for the Cultivation of Science. He has attended many national and international seminars and conferences where he has delivered keynote addresses. He has also mentored many research students for their PhD theses.\n\nSarma received the Sir J. C. Ghosh Medal in 1981 and the \"Young Scientist Medal\" from the Indian National Science Academy in 1983. UNESCO awarded him the Biennial Javed Hussain Prize in 1989 and a year later, in 1990, he received the medal of excellence from the Materials Research Society of India. The Government of India awarded him the Shanti Swarup Bhatnagar Prize in 1994 and he received the C. V. Raman Award in 2004. The year 2005 brought him three awards, the \"Hari Om Ashram Trust Award\" by the University Grants Commission, G. D. Birla Award and the Alumnus Award for Excellence in Research from the Indian Institute of Science. He received FICCI Award in 2006, TWAS Prize in Physics in 2007, the National Research Award in 2009 and H. K. Firodia Award in 2013. He was honored with the Knight of \"The Order of the Star of Italy\" in 2014, and an Honorary Doctorate from the Faculty of Science and Technology at Uppsala University in 2015.\n\nSarma, an elected Fellow of the Indian National Science Academy, delivered three INSA lectures in 2006, \"Dr. Jagdish Shankar Memorial Award Lecture\", \"Professor R. P. Mitra Memorial Award Lecture\" and \"A. V. Rama Rao Foundation Lecture\". Two years later, he delivered the INSA Kotcherlakota Rangadhama Rao Memorial Lecture in 2008. Some of the other lectures delivered by Das Sarma are \"CSIR Foundation Day Lecture\", \"Distinguished Public Lecture\", \"8th Atma Ram Memorial Lecture\" and the \"Joy Kissen Memorial Lecture\", \"CNR Rao Prize Lecture\".\n\nHe is an elected Fellow of the American Physical Society, The Academy of Sciences for the Developing World, the National Academy of Sciences, India, and the Indian Academy of Sciences and holds the fellowship of the Asia-Pacific Academy of Materials (APAM). He is also a J. C. Bose National Fellow and Homi Bhabha Fellow.\n\n"}
{"id": "39171284", "url": "https://en.wikipedia.org/wiki?curid=39171284", "title": "Dirac equation in curved spacetime", "text": "Dirac equation in curved spacetime\n\nIn mathematical physics, the Dirac equation in curved spacetime generalizes the original Dirac equation to curved space.\n\nIt can be written by using vierbein fields and the gravitational spin connection. The vierbein defines a local rest frame, allowing the constant Dirac matrices to act at each spacetime point. In this way, Dirac's equation takes the following form in curved spacetime:\n\nHere is the vierbein and is the covariant derivative for fermionic fields, defined as follows\n\nwhere is the commutator of Dirac matrices:\n\nand are the spin connection components.\n\nNote that here Latin indices denote the \"Lorentzian\" vierbein labels while Greek indices denote manifold coordinate indices.\n\n\n"}
{"id": "38318077", "url": "https://en.wikipedia.org/wiki?curid=38318077", "title": "EPS Europhysics Prize", "text": "EPS Europhysics Prize\n\nThe EPS Europhysics Prize is awarded (currently every 2nd year) since 1975 by the Condensed Matter Division of the European Physical Society, in recognition of recent work (completed in the 5 years preceding the attribution of the award) by one or more individuals, for scientific excellence in the area of condensed matter physics. It is one of Europe’s most prestigious prizes in the field of condensed matter physics. Several laureates of the EPS Europhysics Prize also received a Nobel Prize in Physics or Chemistry (Geim, Novoselov, Fert, Grünberg, Kroto, Smalley, Ertl, Bednorz, Müller, Binnig, Rohrer, von Klitzing, Alferov).\n\nSource: European Physical Society\n"}
{"id": "439202", "url": "https://en.wikipedia.org/wiki?curid=439202", "title": "Electron cyclotron resonance", "text": "Electron cyclotron resonance\n\nElectron cyclotron resonance is a phenomenon observed in plasma physics, condensed matter physics, and accelerator physics. An electron in a static and uniform magnetic field will move in a circle due to the Lorentz force. The circular motion may be superimposed with a uniform axial motion, resulting in a helix, or with a uniform motion perpendicular to the field (e.g., in the presence of an electrical or gravitational field) resulting in a cycloid. The angular frequency (ω = 2π\"f\" ) of this \"cyclotron\" motion for a given magnetic field strength \"B\" is given (in SI units) by\n\nwhere formula_2 is the elementary charge and formula_3 is the mass of the electron. For the commonly used microwave frequency 2.45 GHz and the bare electron charge and mass, the resonance condition is met when \"B\" = 875G = 0.0875T.\n\nFor particles of charge \"q\", electron rest mass \"m\" moving at relativistic speeds \"v\", the formula needs to be adjusted according to the special theory of relativity to:\n\nwhere\n\nAn ionized plasma may be efficiently produced or heated by superimposing a static magnetic field and a high-frequency electromagnetic field at the electron cyclotron resonance frequency. In the toroidal magnetic fields used in magnetic fusion energy research, the magnetic field decreases with the major radius, so the location of the power deposition can be controlled within about a centimeter. Furthermore, the heating power can be rapidly modulated and is deposited directly into the electrons. These properties make electron cyclotron heating a very valuable research tool for energy transport studies. In addition to heating, electron cyclotron waves can be used to drive current. The inverse process of electron cyclotron emission can be used as a diagnostic of the radial electron temperature profile.\nSince the early 1980s, following the award-winning pioneering work done by Dr. Richard Geller, Dr. Claude Lyneis, and Dr. H. Postma; respectively from French Atomic Energy Commission, Lawrence Berkeley National Laboratory and the Oak Ridge National Laboratory, the use of electron cyclotron resonance for efficient plasma generation, especially to obtain large numbers of multiply charged ions, has acquired a unique importance in various technological fields. Many diverse activities depend on electron cyclotron resonance technology, including\n\nThe ECR ion source makes use of the electron cyclotron resonance to ionize a plasma. Microwaves are injected into a volume at the frequency corresponding to the electron cyclotron resonance, defined by the magnetic field applied to a region inside the volume. The volume contains a low pressure gas. The alternating electric field of the microwaves is set to be synchronous with the gyration period of the free electrons of the gas, and increases their perpendicular kinetic energy. Subsequently, when the energized free electrons collide with the gas in the volume they can cause ionization if their kinetic energy is larger than the ionization energy of the atoms or molecules. The ions produced correspond to the gas type used, which may be pure, a compound, or vapor of a solid or liquid material.\n\nECR ion sources are able to produce singly charged ions with high intensities (e.g. H and D ions of more than 100 mA (electrical) in DC mode using a 2.45 GHz ECR ion source).\n\nFor multiply charged ions, the ECR ion source has the advantages that it is able to confine the ions for long enough for multiple collisions and multiple ionization to take place, and the low gas pressure in the source avoids recombination. The VENUS ECR ion source at Lawrence Berkeley National Laboratory has produced in intensity of 0.25 mA (electrical) of Bi.\n\nSome important industrial fields would not exist without the use of this fundamental technology, which makes electron cyclotron resonance ion and plasma sources one of the enabling technologies of today's world.\n\nWithin a solid the mass in the cyclotron frequency equation above is replaced with the effective mass tensor formula_6. Cyclotron resonance is therefore a useful technique to measure effective mass and Fermi surface cross-section in solids. In a sufficiently high magnetic field at low temperature in a relatively pure material\n\nwhere formula_8 is the carrier scattering lifetime, formula_9 is Boltzmann's constant and formula_10 is temperature. When these conditions are satisfied, an electron will complete its cyclotron orbit without engaging in a collision, at which point it is said to be in a well-defined Landau level.\n\n\n"}
{"id": "27895488", "url": "https://en.wikipedia.org/wiki?curid=27895488", "title": "Energy Technology Data Exchange", "text": "Energy Technology Data Exchange\n\nThe Energy Technology Data Exchange (ETDE) was formed in 1987 and officially ended 30 June 2014. It was initiated as a multilateral agreement under the International Energy Agency (IEA) agreement network, replacing numerous other bilateral agreements. The multilateral agreement was for the international exchange of energy research and development and information. The exchange resulted in a database which was the world's largest collection of energy research, technology, and development (RTD) information (ETDEWEB - described below). The collection of information was generated from energy RTD literature published in member countries and through other partnering arrangements with organizations such as the International Nuclear Information System (a unit of the International Atomic Energy Agency). This had the effect of creating a broad spectrum of information that was included in the ETDE database. The range of content included fossil fuels, renewable energies (including Hydrogen), End-Use (Buildings, Industry and Transport), fusion, energy policy, conservation, and efficiency, and cross-sectional activities. This fulfilled the need for timely exchange of global information towards the goal of a sustainable energy future. ETDE operated under an IEA Implementing Agreement and was governed by an Executive Committee of delegates from ETDE member countries. Officers of the Executive Committee included a Chair and two Vice-Chairs, elected to three-year terms. Day-to-day operations were managed through an operating agent organization, which reported to the Executive Committee.\n\nETDE's Energy Database was a substantial collection that focused on energy research literature and technology literature. This database contained more than 4.5 million abstracted and indexed records, and was updated twice per month. Temporal coverage was from 1974 to 2014. The principle access point for this database was ETDEWEB (see next section). However, access was also available through commercial online hosts, and some countries offered their own products for access. Member country representatives supplied the best options for their citizens to access this database. Furthermore, the United States fed this database to Dialog which provided online access. Likewise, Germany fed this database to STN International.\n\nBroad subject coverage included information on energy research and development; energy policy and planning; basic sciences (e.g., physics, chemistry and biomedical) and materials research; the environmental impact of energy production and use, including climate change; energy conservation; nuclear (e.g., reactors, isotopes, waste management); coal and fossil fuels; and renewable energy technologies (e.g., solar energy, wind energy, biomass, geothermal, hydro). The scope of topical coverage was worldwide in some areas. The database was used by scientists, researchers, engineers, policymakers, information specialists, librarians, industry leaders, university faculty, and university students, among others.\n\nEnergy Technology Data Exchange employed an internet related database to disseminate the energy research and technology information which was collected and exchanged. The database was named ETDE World Energy Base or ETDEWEB.\n\nETDEWEB was produced and made available by ETDE. It had over 5 million references for literature that encompassed broad topical coverage, and allowed access to more than 500 000 full text documents and reports, which amounted to more than 1 million pages. ETDEWEB had unique access to these reports, which were often not available through other conventional sources. Over a million other references linked to sites containing cited documents. Open access was provided to member countries, countries with developing country status, or by Executive Committee decision.\n\nOnline access was extended through the wordwideenergy.org website.\n\nAfter the previous ETDE consortium ended in 2014, the WWE application allowed ETDEWEB and other content to remain accessible, thanks to remaining funds and former member country support. But this extended access also ended in July 2016. There may be a possibility that ETDEWEB will again become accessible at some unspecified time in the future via the US DOE’s Office of Scientific and Technical Information (OSTI) systems.\n\nETDEWEB covered an extensive base of topics, the main areas included information on energy research and development along with energy policy and planning. Other areas of coverage included basic sciences (e.g., physics, chemistry and biomedical); materials research; the environmental impacts of energy production and use (including climate change); energy conservation; nuclear energy (e.g., reactors, isotopes, and nuclear waste management); coal and fossil fuels; and renewable energy technologies (e.g., solar, wind, biomass, geothermal, hydro).\n\n\n"}
{"id": "4679720", "url": "https://en.wikipedia.org/wiki?curid=4679720", "title": "Explosives trace detector", "text": "Explosives trace detector\n\nExplosives trace detectors (ETD) are explosive detection equipment able to detect explosives of small magnitude. The detection is accomplished by sampling non-visible \"trace\" amounts of particulates. Devices similar to ETDs are also used to detect narcotics. The equipment is used mainly in airports and other vulnerable areas considered susceptible to acts of unlawful interference. Dr. Stephen Lee is credited with inventing the Fido explosives detector while working at the Army Research Laboratory.\n\nDetection limit is defined as the lowest amount of explosive matter a detector can detect reliably. It is expressed in terms of nano-grams (ng), pico-grams (pg) or femto-grams (fg) with fg being better than pg better than ng. It can also be expressed in terms of parts per billion (ppb), parts per trillion (ppt) or parts per quadrillion (ppq).\n\nSensitivity is important because most explosives have a low vapor pressure . The detector with the highest sensitivity is the best in detecting vapors of explosives reliably.\n\nPortable explosive detectors need to be as light weight as possible to allow users to not fatigue when holding them. Also, light weight detectors can easily be placed on top of robots.\n\nPortable explosive detectors need to be as small as possible to allow for sensing of explosives in hard to reach places like under a car or inside a trash bin.\n\nThe start up time for any trace detector is the time required by the detector to reach the optimized temperature for detection of contraband substances.\n\nThe use of colorimetric test kits for explosive detection is one of the oldest, simplest, and most widely used methods for the detection of explosives. Colorimetric detection of explosives involves applying a chemical reagent to an unknown material or sample and observing a color reaction. Common color reactions are known and indicate to the user if there is an explosive material present and in many cases the group of explosive from which the material is derived. The major groups of explosives are nitroaromatic explosives, nitrate ester and nitramine explosives, improvised explosives not containing nitro groups which includes inorganic nitrate based explosives, chlorate based explosives, and peroxide based explosives.\n\nExplosive detection using ion mobility spectrometry (IMS) is based on velocities of ions in a uniform electric field. There are some variant to IMS such as Ion trap mobility spectrometry (ITMS) or Non-linear dependence on ion mobility (NLDM) which are based on IMS principle. The sensitivity of devices using this technology is limited to pg levels. The technology also requires the ionization of sample explosives which is accomplished by a radioactive source such as nickel-63 or americium-241. This technology is found in most commercially available explosive detectors such as the GE VaporTracer, Smith Sabre 4000 and Russian built MO-2M and MO-8. The presence of radioactive materials in these equipments cause regulatory hassles and requires special permissions at customs ports. These detectors cannot be field serviced and may pose radiation hazard to the operator if the casing of the detector cracks due to mishandling. Bi-yearly checks are mandatory on such equipment in most countries by regulating agencies to ensure that there are no radiation leaks. Disposal of these equipments is also controlled owing to the high half-life of the radioactive material used.\n\nElectrospray ionization, mobility analysis (DMA) and tandem mass spectrometry (MS/MS) is used by SEDET (Sociedad Europea de Detección) for the “Air Cargo Explosive Screener (ACES)”, targeted to aviation cargo containers currently under development in Spain.\n\nThis technology is based on decomposition of explosive substance followed by the reduction of the nitro groups. Most military grade explosives are nitro compounds and have an abundance of NO groups on them. Explosive vapors are pulled into an adsorber at a high rate and then pyrolized. The presence of nitro groups in the pyrolized products is then detected. This technology has significantly more false alarms because many other harmless compounds also have an abundance of nitro groups. For example, most fertilizers have nitro groups which are falsely identified as explosives, and the sensitivity of this technology is also fairly low. A popular detector using this technology is Scientrex EVD 3000.\n\nThis technology is based on the luminescence of certain compounds when they attach to explosive particles. This is mostly used in non-electronic equipment such as sprays and test papers. The sensitivity is pretty low in the order of nanograms.\n\nAmplifying fluorescent polymer (AFP) is a promising new technology and is based on synthesized polymers which bind to explosive molecules and give an amplified signal upon detection. When compounds that are not polymers are utilized for such purpose, the quenching of the fluorescence by the traces of explosives is not detectable. When amplifying fluorescent polymer in thin films absorbs a photon of light, excited state polymers (exitons) are able to migrate along the polymer backbone and between the adjacent polymer films. These sensors were originally made in order to detect trinitrotoluene. In AFP, binding of one TNT molecule results in quenching of fluorescence significantly due to the conjugated structure of the polymers. It has been reported that in practice the polymers result in 100-1000 fold increase of amplification of the quenching response.\n\n\"During its excited state lifetime, the exciton propagates by a random walk through a finite volume of the polymer film.\" Once TNT, or any other electron-deficient (i.e., electron accepting) molecule comes in contact with the polymer, a so-called low-energy ‘trap’ forms. \"If the exciton migrates to the site of the bound electron-deficient molecule before transitioning back to the ground state, the exciton will be trapped (a non-radiative process), and no fluorescence will be observed from the excitation event. Since the exciton samples many potential analyte binding sites during its excited state lifetime, the probability that the exciton will sample an occupied ‘receptor’ site and be quenched is greatly increased.\"\n\nThe explosive trace detectors utilizing AFPs, known as Fido, were originally developed under the Defense Advanced Research Projects Agency (DARPA) Dog’s Nose program and is now produced by FLIR Systems. The current generation, provides broad-band trace explosive detection and weighs less than 3 lbs. The sensitivity is in the order of femtogram (1 × 10 grams). This is the only such technology in the field that can achieve such sensitivity.\n\nRecently, mass spectrometry (MS) has emerged as another ETD technology. Adoption of mass spectrometry should lower false alarms rates often associated with ETD due to the higher resolution of the core technology. It also uses a non-radioactive ionization method. Primarily used in desktop ETD systems, mass spectrometry can be miniaturized for handheld ETD but at the cost of compromising much of the performance that defines the technology.\n"}
{"id": "2380869", "url": "https://en.wikipedia.org/wiki?curid=2380869", "title": "Ferrite (magnet)", "text": "Ferrite (magnet)\n\nA ferrite is a ceramic material made by mixing and firing large proportions iron(III) oxide (FeO, rust) blended with small proportions of one or more additional metallic elements, such as barium, manganese, nickel, and zinc. They are both electrically \"non\"-conductive, meaning that they are insulators, and ferrimagnetic, meaning they can easily be magnetized or attracted to a magnet. Ferrites can be divided into two families based on their resistance to being demagnetized (magnetic coercivity).\n\nHard ferrites have high coercivity, so are difficult to demagnetize. They are used to make permanent magnets for refrigerator magnets, loudspeakers, small electric motors, and so on.\n\nSoft ferrites have low coercivity, so they easily change their magnetization, and act as conductors of magnetic fields. They are used in the electronics industry to make efficient magnetic cores called ferrite cores for high-frequency inductors and transformers, and in various microwave components.\n\nFerrite compounds have extremely low cost, being made of mostly rusted iron (iron oxide), and also have excellent corrosion resistance. They are very stable and difficult to demagnetize, and can be made with both high and low coercive forces. Yogoro Kato and Takeshi Takei of the Tokyo Institute of Technology synthesized the first ferrite compounds in 1930.\n\nFerrites are usually ferrimagnetic ceramic compounds derived from iron oxides. Magnetite (FeO) is a famous example. Like most of the other ceramics, ferrites are hard, brittle, and poor conductors of electricity.\n\nMany ferrites adopt the spinel structure with the formula ABO, where A and B represent various metal cations, usually including iron (Fe). Spinel ferrites usually adopt a crystal motif consisting of cubic close-packed (fcc) oxides (O) with A cations occupying one eighth of the tetrahedral holes and B cations occupying half of the octahedral holes, i.e., .\n\nFerrite crystals do not adopt the ordinary spinel structure, but rather the inverse spinel structure: One eighth of the tetrahedral holes are occupied by B cations, one fourth of the octahedral sites are occupied by A cations. and the other one fourth by B cation. It is also possible to have mixed structure spinel ferrites with formula [MFe][MFe]O where δ is the degree of inversion.\n\nThe magnetic material known as \"ZnFe\" has the formula ZnFeO, with Fe occupying the octahedral sites and Zn occupy the tetrahedral sites, it is an example of normal structure spinel ferrite.\n\nSome ferrites adopt hexagonal crystal structure, like barium and strontium ferrites BaFeO (BaO:6FeO) and SrFeO (SrO:6FeO).\n\nIn terms of their magnetic properties, the different ferrites are often classified as \"soft\", \"semi-hard\" or \"hard\", which refers to their low or high magnetic coercivity, as follows.\n\nFerrites that are used in transformer or electromagnetic cores contain nickel, zinc, and/or manganese compounds. They have a low coercivity and are called soft ferrites. The low coercivity means the material's magnetization can easily reverse direction without dissipating much energy (hysteresis losses), while the material's high resistivity prevents eddy currents in the core, another source of energy loss. Because of their comparatively low losses at high frequencies, they are extensively used in the cores of RF transformers and inductors in applications such as switched-mode power supplies and loopstick antennas used in AM radios.\n\nThe most common soft ferrites are:\n\nFor applications below 5 MHz, MnZn ferrites are used; above that, NiZn is the usual choice. The exception is with common mode inductors, where the threshold of choice is at 70 MHz.\n\n\nIn contrast, permanent ferrite magnets are made of hard ferrites, which have a high coercivity and high remanence after magnetization. Iron oxide and barium or strontium carbonate are used in manufacturing of hard ferrite magnets. The high coercivity means the materials are very resistant to becoming demagnetized, an essential characteristic for a permanent magnet. They also have high magnetic permeability. These so-called \"ceramic magnets\" are cheap, and are widely used in household products such as refrigerator magnets. The maximum magnetic field \"B\" is about 0.35 tesla and the magnetic field strength \"H\" is about 30 to 160 kiloampere turns per meter (400 to 2000 oersteds). The density of ferrite magnets is about 5 g/cm.\n\nThe most common hard ferrites are:\n\nFerrites are produced by heating a mixture of the oxides of the constituent metals at high temperatures, as shown in this idealized equation:\nIn some cases, the mixture of finely-powdered precursors is pressed into a mold. For barium and strontium ferrites, these metals typically supplied as their carbonates, BaCO or SrCO. During the heating process, these carbonates undergo calcination:\nAfter this step, the two oxides combine to give the ferrite. The resulting mixture of oxides undergoes sintering.\n\nHaving obtained the ferrite, the cooled product is milled to particles smaller than 2 µm, sufficiently small that each particle consists of a single magnetic domain. Next the powder is pressed into a shape, dried, and re-sintered. The shaping may be performed in an external magnetic field, in order to achieve a preferred orientation of the particles (anisotropy).\n\nSmall and geometrically easy shapes may be produced with dry pressing. However, in such a process small particles may agglomerate and lead to poorer magnetic properties compared to the wet pressing process. Direct calcination and sintering without re-milling is possible as well but leads to poor magnetic properties.\n\nElectromagnets are pre-sintered as well (pre-reaction), milled and pressed. However, the sintering takes place in a specific atmosphere, for instance one with an oxygen shortage. The chemical composition and especially the structure vary strongly between the precursor and the sintered product.\n\nTo allow efficient stacking of product in the furnace during sintering and prevent parts sticking together, many manufacturers separate ware using ceramic powder separator sheets. These sheets are available in various materials such as alumina, zirconia and magnesia. They are also available in fine, medium and coarse particle sizes. By matching the material and particle size to the ware being sintered, surface damage and contamination can be reduced while maximizing furnace loading.\n\nFerrite cores are used in electronic inductors, transformers, and electromagnets where the high electrical resistance of the ferrite leads to very low eddy current losses. They are commonly seen as a lump in a computer cable, called a ferrite bead, which helps to prevent high frequency electrical noise (radio frequency interference) from exiting or entering the equipment.\n\nEarly computer memories stored data in the residual magnetic fields of hard ferrite cores, which were assembled into arrays of \"core memory\". Ferrite powders are used in the coatings of magnetic recording tapes. One such type of material is iron (III) oxide.\n\nFerrite particles are also used as a component of radar-absorbing materials or coatings used in stealth aircraft and in the absorption tiles lining the rooms used for electromagnetic compatibility measurements.\n\nMost common audio magnets, including those used in loudspeakers and electromagnetic instrument pickups, are ferrite magnets. Except for certain \"vintage\" products, ferrite magnets have largely displaced the more expensive Alnico magnets in these applications.\n\nFerrite nanoparticles exhibit superparamagnetic properties.\n\nYogoro Kato and Takeshi Takei of the Tokyo Institute of Technology synthesized the first ferrite compounds in 1930. This led to the founding of TDK Corporation in 1935, to manufacture the material.\n\nBarium hexaferrite (BaFeO) was discovered in 1950 at the Philips Natuurkundig Laboratorium (\"Philips Physics Laboratory\"). The discovery was somewhat accidental—due to a mistake by an assistant who was supposed to be preparing a sample of hexagonal lanthanum ferrite for a team investigating its use as a semiconductor material. On discovering that it was actually a magnetic material, and confirming its structure by X-ray crystallography, they passed it on to the magnetic research group. Barium hexaferrite has both high coercivity (170 kA/m) and low raw material costs. It was developed as a product by Philips Industries (Netherlands) and from 1952 was marketed under the trade name \"Ferroxdure\". The low price and good performance led to a rapid increase in the use permanent magnets.\n\nIn the 1960s Philips developed strontium hexaferrite (SrFeO), with better properties than barium hexaferrite. Barium and strontium hexaferrite dominate the market due to their low costs. However other materials have been found with improved properties. BaFeFeO came in 1980. and BaZnFeO came in 1991.\n\n\n\n"}
{"id": "12586075", "url": "https://en.wikipedia.org/wiki?curid=12586075", "title": "Flamanville Nuclear Power Plant", "text": "Flamanville Nuclear Power Plant\n\nThe Flamanville Nuclear Power Plant is located at Flamanville, Manche, France on the Cotentin Peninsula. The power plant houses two pressurized water reactors (PWRs) that produce 1.3 GW each and came into service in 1986 and 1987, respectively. It produced 18.9 TWh in 2005, which amounted to 4% of the electricity production in France. In 2006 this figure was about 3.3%.\n\nA third reactor at the site, a EPR unit, is currently under construction. \nAs of July 2016, the project was three times over budget and years behind schedule. \nVarious safety problems have been raised, including weakness in the steel used in the reactor.\nIn 2006, before the start of construction of the EPR (unit 3), there were 671 workers regularly working at the two operational reactors.\n\nConstruction on a new reactor, Flamanville 3, began on 4 December 2007. The new unit is an Areva European Pressurized Reactor type and is planned to have a nameplate capacity of 1,650 MWe.\n\nEDF has previously said France's first EPR would cost €3.3 billion and start commercial operations in 2012, after construction lasting 54 months. The latest cost estimate (July 2018) is at €10.9 billion. \n\nOn 3 December 2012 EDF announced that the estimated costs have escalated to €8.5 billion ($11 billion), and the completion of construction is delayed to 2016. The next day the Italian power company Enel announced it was relinquishing its 12.5% stake in the project, and 5 future EPRs, so would be reimbursed its project stake of €613 million plus interest.\n\nIn November 2014 EDF announced that completion of construction was delayed to 2017 due to delays in component delivery by Areva.\n\nIn April 2015 Areva informed the French nuclear regulator, Autorité de sûreté nucléaire (ASN), that anomalies had been detected in the reactor vessel steel, causing \"lower than expected mechanical toughness values\". Further tests are underway. \nSegolene Royal, Minister of Ecology, Sustainable Development and Energy in the \nSecond Valls Government, has asked the producer for further details and possible consequences.\n\nVarious safety problems have been raised, including weakness in the steel used in the reactor together with heterogeneity of the steel alloy forged high integrity components used in the reactor pressure vessel, that have also been shown to be present in Japanese-sourced components that have entered the French nuclear equipment supply chain. The safety of the Flamanville EPR plant has also been questioned due to the danger of flooding of the kind experienced during the 1999 Blayais Nuclear Power Plant flood. In June 2015 multiple faults in cooling system safety valves were discovered by ASN.\n\nThe EPR (Flamanville 3) aimed to be safer than any previous reactor, but as of 2016 the project is three times over budget and years behind schedule. In September 2015 EDF announced that the estimated costs had escalated to €10.5 billion, and the start-up of the reactor was delayed to the fourth quarter of 2018.\n\nThe delays of Unit 3 of Flamanville received additional attention when in December 2016 The Economist reported that the British loan guarantees for Hinkley Point C require Unit 3 to be operational by 2020, that the regulator will rule on the future of Unit 3 mid-2017 and that one possible outcome of this ruling can delay its opening far beyond 2018, thus jeopardizing the British loan guarantees thereby preventing EDF from building the EPRs at Hinkley Point. In February 2017 renewed delays in the construction of the EPR-reactors at Taishan Nuclear Power Plant prompted EDF to state that Flamanville 3 remains on schedule to start operations by the end of 2018, assuming it receives regulator approval. In June 2017 the French regulator issued a provisional ruling that Flamanville 3 is safe to start.\n\nIn January 2018, cold functional tests were completed.\nIn February EDF found that some secondary cooling circuit welds did not meet specifications, causing EDF to carry out further checks and a report. Following this ASN requested EDF to extend the welding checks to other systems. Hot functional tests have been postponed.\n\nIn July 2018, EDF further delayed fuel loading to Q4 2019 and increased the project's cost estimate by a further €400 million ($467.1 million USD). Startup is now scheduled to occur no earlier than Q2 2020 and EDF now estimates project costs at €10.9 billion ($12.75 billion USD), three times the original cost estimates. Hot testing is currently planned to occur by the end of 2018.\n\nOn 9 February 2017 a mechanical problem with a fan in the turbine hall caused an explosion and fire, causing five people to be treated for smoke inhalation. While the non-nuclear accident did not cause any radioactive leak, it did cause the number one reactor to be disconnected from the power grid. EDF initially estimated the reactor would be operational within a week, but later estimated the end of March.\n"}
{"id": "6489678", "url": "https://en.wikipedia.org/wiki?curid=6489678", "title": "Friendship One", "text": "Friendship One\n\nFriendship One was a successful attempt at beating the round-the-world air speed record.\n\nThe flight was conducted from January 29 to January 30, 1988 and was operated by a Boeing 747SP owned by United Airlines. A charitable foundation, the Friendship Foundation, was established and all money went to children's charities. A ticket on the flight cost USD $5,000, and, in total, the flight raised about $500,000. A total of 141 passengers were on board, including Neil Armstrong and Bill Lear's widow, Moya.\n\nThe previous speed record of 45 hours, 26 minutes, and 55 seconds was set by a Gulfstream III business jet. Friendship One, captained by Clay Lacy, followed a 23,125-mile route from Seattle, Washington to refueling stops in Athens and Taipei, and back to Seattle. It completed the trip in 36 hours, 54 minutes, and 15 seconds. Later in 1988, a Gulfstream IV made a similar eastbound circumnavigation in 36 hours, 8 minutes, which stands as the world record as of 2013.\n"}
{"id": "25122739", "url": "https://en.wikipedia.org/wiki?curid=25122739", "title": "Grus (geology)", "text": "Grus (geology)\n\nGrus is an accumulation of angular, coarse-grained fragments (particles of sand and gravel) resulting from the granular disintegration by the processes of chemical and mechanical weathering of crystalline rocks (most notably granitoids) generally in an arid or semiarid region. Grus sand, when cemented into a sandstone, will form an arkose.\n\nWithin a European context most of the saprolite mantles of Late Cenozoic age are made up grus, contrasting with Mesozoic and Early Cenozoic saprolites made up of kaolinitic and ferrallitic material.\n\n"}
{"id": "8530877", "url": "https://en.wikipedia.org/wiki?curid=8530877", "title": "Hasegawa–Mima equation", "text": "Hasegawa–Mima equation\n\nIn plasma physics, the Hasegawa–Mima equation, named after Akira Hasegawa and Kunioki Mima, is an equation that describes a certain regime of plasma, where the time scales are very fast, and the distance scale in the direction of the magnetic field is long. In particular the equation is useful for describing turbulence in some tokamaks. The equation was introduced in Hasegawa and Mima's paper submitted in 1977 to \"Physics of Fluids\", where they compared it to the results of the ATC tokamak.\n\n\n\n\nThe Hasegawa–Mima equation is a second order nonlinear partial differential equation that describes the electric potential. The form of the equation is:\n\nAlthough the quasi neutrality condition holds, the small differences in density between the electrons and the ions cause an electric potential.\nThe Hasegawa–Mima equation is derived from the continuity equation:\n\nThe fluid velocity can be approximated by the E cross B drift:\n\nPrevious models derived their equations from this approximation. The divergence of the E cross B drift is zero, which keeps the fluid incompressible. However, the compressibility of the fluid is very important in describing the evolution of the system. Hasegawa and Mima argued that the assumption was invalid. The Hasegawa–Mima equation introduces a second order term for the fluid velocity known as the polarization drift in order to find the divergence of the fluid velocity. Due to the assumption of large magnetic field, the polarization drift is much smaller than the E cross B drift. Nevertheless, it introduces important physics.\n\nFor a two-dimensional incompressible fluid which is not a plasma, the Navier–Stokes equations say:\n\nafter taking the curl of the momentum balance equation. This equation is almost identical to the Hasegawa–Mima equation except the second and fourth terms are gone, and the electric potential is replaced with the fluid velocity vector potential where:\n\nThe first and third terms to the Hasegawa–Mima equation, which are the same as the Navier Stokes equation, are the terms introduced by adding the polarization drift. In the limit where the wavelength of a perturbation of the electric potential is much smaller than the gyroradius based on the sound speed, the Hasegawa–Mima equations become the same as the two-dimensional incompressible fluid.\n\nOne way to understand an equation more fully is to understand what it is normalized to, which gives you an idea of the scales of interest. The time, position, and electric potential are normalized to t',x', and formula_10\n\nThe time scale for the Hasegawa–Mima equation is the inverse ion gyrofrequency:\n\nFrom the large magnetic field assumption the normalized time is very small. However, it is still large enough to get information out of it.\n\nThe distance scale is the gyroradius based on the sound speed:\n\nIf you transform to k-space, it is clear that when k, the wavenumber, is much larger than one, the terms that make the Hasegawa–Mima equation differ from the equation derived from Navier-Stokes equation in a two dimensional incompressible flow become much smaller than the rest.\n\nFrom the distance and time scales we can determine the scale for velocities. This turns out to be the sound speed. The Hasegawa–Mima equation, shows us the dynamics of fast moving sounds as opposed to the slower dynamics such as flows that are captured in the MHD equations. The motion is even faster than the sound speed given that the time scales are much smaller than the time normalization.\n\nThe potential is normalized to:\n\nSince the electrons fit a Maxwellian and the quasineutrality condition holds, this normalized potential is small, but similar order to the normalized time derivative.\n\nThe entire equation without normalization is:\n\nAlthough the time derivative divided by the cyclotron frequency is much smaller than unity, and the normalized electric potential is much smaller than unity, as long as the gradient is on the order of one, both terms are comparable to the nonlinear term. The unperturbed density gradient can also be just as small as the normalized electric potential and be comparable to the other terms.\n\nOften the Hasegawa–Mima equation is expressed in a different form using Poisson brackets. These Poisson brackets are defined as:\n\nUsing these Poisson brackets, the equation can be reexpressed as:\n\nOften the particle density is assumed to vary uniformly just in one direction, and the equation is written in a sightly different form. The Poisson bracket including the density is replaced with the definition of the Poisson bracket, and a constant replaces the derivative of the density dependent term.\n\nThere are two quantities that are conserved in a two-dimensional incompressible fluid.\nThe kinetic energy:\n\nAnd the enstrophy:\n\nFor the Hasegawa–Mima equation, there are also two conserved quantities, that are related to the above quantities. The generalized energy:\n\nAnd the generalized enstrophy:\n\nIn the limit where the Hasegawa–Mima equation is the same as an incompressible fluid, the generalized energy, and enstrophy become the same as the kinetic energy and enstrophy.\n\n\n\n"}
{"id": "21284666", "url": "https://en.wikipedia.org/wiki?curid=21284666", "title": "Helmholtz-Zentrum Berlin", "text": "Helmholtz-Zentrum Berlin\n\nHelmholtz-Zentrum Berlin für Materialien und Energie (Helmholtz Center for Materials and Energy, HZB) is part of the Helmholtz Association of German Research Centres. The institute studies the structure and dynamics of materials and investigates solar cell technology. Several large scale facilities are available, the most important of which are the 10 MW BER II nuclear research reactor at the Lise Meitner campus in Wannsee and the third-generation BESSY II synchrotron in Adlershof.\n\nThe Helmholtz-Zentrum Berlin was created on 1 January 2009 by the merger of Hahn-Meitner-Institut Berlin (HMI) and Berliner Elektronenspeicherring-Gesellschaft für Synchrotronstrahlung (BESSY), thus bringing BESSY into the Helmholtz Association.\n\nThe Hahn-Meitner-Institut Berlin (HMI), named after Otto Hahn and Lise Meitner, was founded 14 March 1959 in Berlin-Wannsee to operate the BER I research reactor that began operation with 50 kW on 24 July 1958. Research originally focused on radiochemistry. In 1971, the federal government took over a 90% share in the HMI.\n\nThe Berliner Elektronenspeicherring-Gesellschaft für Synchrotronstrahlung (BESSY) was founded in 1979. The first synchrotron BESSY I in Berlin-Wilmersdorf began operations in 1982.\n\n"}
{"id": "39282694", "url": "https://en.wikipedia.org/wiki?curid=39282694", "title": "Hopton Wood stone", "text": "Hopton Wood stone\n\nHopton Wood stone (sometimes Hopton-Wood stone or Hoptonwood stone) is a type of limestone quarried west of Middleton-by-Wirksworth, Derbyshire, England. Described as \"very fine, almost like marble\" and as \"England’s premier \ndecorative stone\", it is particularly suited to carving, making it popular for tombstones (including many thousands for the Commonwealth War Graves Commission), sculpture and building.\n\nBuildings and structures made using Hopton Wood stone include the Houses of Parliament, Westminster Abbey, the Albert Memorial, Lichfield Cathedral, Calke Abbey, Chatsworth House and Oscar Wilde's tomb.\n\nIn 1947 the Hopton-Wood Stone Firms Ltd commissioned a book about Hopton Wood stone, published by Fanfare press.\n\n"}
{"id": "65424", "url": "https://en.wikipedia.org/wiki?curid=65424", "title": "Hydraulics", "text": "Hydraulics\n\nHydraulics (from Greek: Υδραυλική) is a technology and applied science using engineering, chemistry, and other sciences involving the mechanical properties and use of liquids. At a very basic level, hydraulics is the liquid counterpart of pneumatics, which concerns gases. Fluid mechanics provides the theoretical foundation for hydraulics, which focuses on the applied engineering using the properties of fluids. In its fluid power applications, hydraulics is used for the generation, control, and transmission of power by the use of pressurized liquids. Hydraulic topics range through some parts of science and most of engineering modules, and cover concepts such as pipe flow, dam design, fluidics and fluid control circuitry. The principles of hydraulics are in use naturally in the human body within the vascular system and erectile tissue.\nFree surface hydraulics is the branch of hydraulics dealing with free surface flow, such as occurring in rivers, canals, lakes, estuaries and seas. Its sub-field open-channel flow studies the flow in open channels.\n\nThe word \"hydraulics\" originates from the Greek word (\"hydraulikos\") which in turn originates from (\"hydor\", Greek for water) and (\"aulos\", meaning pipe).\n\nEarly uses of water power date back to Mesopotamia and ancient Egypt, where irrigation has been used since the 6th millennium BC and water clocks had been used since the early 2nd millennium BC. Other early examples of water power include the Qanat system in ancient Persia and the Turpan water system in ancient Central Asia.\n\nThe Greeks constructed sophisticated water and hydraulic power systems. An example is the construction by Eupalinos, under a public contract, of a watering channel for Samos, the Tunnel of Eupalinos. An early example of the usage of hydraulic wheel, probably the earliest in Europe, is the Perachora wheel (3rd century BC).\n\nThe construction of the first hydraulic automata by Ctesibius (flourished c. 270 BC) and Hero of Alexandria (c. 10 – 80 AD) is notable. Hero describes a number of working machines using hydraulic power, such as the force pump, which is known from many Roman sites as having been used for raising water and in fire engines.\n\nThe Persians constructed an intricate system of water mills, canals and dams known as the Shushtar Historical Hydraulic System. The project, commenced by Achaemenid king Darius the Great and finished by a group of Roman engineers captured by Sassanian king Shapur I , has been referred to by UNESCO as \"a masterpiece of creative genius.\" They were also the inventors of the Qanat, an underground aqueduct. Several of Iran's large, ancient gardens were irrigated thanks to Qanats. \n\nIn ancient China there was Sunshu Ao (6th century BC), Ximen Bao (5th century BC), Du Shi (circa 31 AD), Zhang Heng (78 – 139 AD), and Ma Jun (200 – 265 AD), while medieval China had Su Song (1020 – 1101 AD) and Shen Kuo (1031–1095). Du Shi employed a waterwheel to power the bellows of a blast furnace producing cast iron. Zhang Heng was the first to employ hydraulics to provide motive power in rotating an armillary sphere for astronomical observation.\n\nIn ancient Sri Lanka, hydraulics were widely used in the ancient kingdoms of Anuradhapura and Polonnaruwa. The discovery of the principle of the valve tower, or valve pit, (Bisokotuwa in Sinhalese) for regulating the escape of water is credited to ingenuity more than 2,000 years ago. By the first century AD, several large-scale irrigation works had been completed. Macro- and micro-hydraulics to provide for domestic horticultural and agricultural needs, surface drainage and erosion control, ornamental and recreational water courses and retaining structures and also cooling systems were in place in Sigiriya, Sri Lanka. The coral on the massive rock at the site includes cisterns for collecting water. Large ancient reservoirs of Sri Lanka are Kalawewa (King Dhatusena), Parakrama Samudra (King Parakrama Bahu), Tisa Wewa (King Dutugamunu), Minneriya (King Mahasen)\n\nIn Ancient Rome, many different hydraulic applications were developed, including public water supplies, innumerable aqueducts, power using watermills and hydraulic mining. They were among the first to make use of the siphon to carry water across valleys, and used hushing on a large scale to prospect for and then extract metal ores. They used lead widely in plumbing systems for domestic and public supply, such as feeding thermae.\n\nHydraulic mining was used in the gold-fields of northern Spain, which was conquered by Augustus in 25 BC. The alluvial gold-mine of Las Medulas was one of the largest of their mines. It was worked by at least 7 long aqueducts, and the water streams were used to erode the soft deposits, and then wash the tailings for the valuable gold content.\n\nIn 1619 Benedetto Castelli (1576 – 1578–1643), a student of Galileo Galilei, published the book \"Della Misura dell'Acque Correnti\" or \"On the Measurement of Running Waters\", one of the foundations of modern hydrodynamics. He served as a chief consultant to the Pope on hydraulic projects, i.e., management of rivers in the Papal States, beginning in 1626.\n\nBlaise Pascal (1623–1662) studied fluid hydrodynamics and hydrostatics, centered on the principles of hydraulic fluids. His inventions include the hydraulic press, which multiplied a smaller force acting on a smaller area into the application of a larger force totaled over a larger area, transmitted through the same pressure (or same change of pressure) at both locations. Pascal's law or principle states that for an incompressible fluid at rest, the difference in pressure is proportional to the difference in height and this difference remains the same whether or not the overall pressure of the fluid is changed by applying an external force. This implies that by increasing the pressure at any point in a confined fluid, there is an equal increase at every other point in the container, i.e., any change in pressure applied at any point of the fluid is transmitted undiminished throughout the fluids.\n\nA French physician, Poiseuille researched the flow of blood through the body and discovered an important law governing the rate of flow with the diameter of the tube in which flow occurred.\n\nSeveral cities developed citywide hydraulic power networks in the 19th century, to operate machinery such as lifts, cranes, capstans and the like. Joseph Bramah was an early innovator and William Armstrong perfected the apparatus for power delivery on an industrial scale. In London, the London Hydraulic Power Company was a major supplier its pipes serving large parts of the West End of London, City and the Docks, but there were schemes restricted to single enterprises such as docks and railway goods yards.\n\nAfter students understand the basic principles of hydraulics, some teachers use a hydraulic analogy to help students learn other things.\nFor example:\n\nThe conservation of mass requirement combined with fluid compressibility yields a fundamental relationship between pressure, fluid flow, and volumetric expansion, as shown below :\n\nAssuming an incompressible fluid or a \"very large\" ratio of compressibility to contained fluid volume, a finite rate of pressure rise requires that any net flow into the contained fluid volume create a volumetric change.\n\n"}
{"id": "36833412", "url": "https://en.wikipedia.org/wiki?curid=36833412", "title": "Incredible Edible", "text": "Incredible Edible\n\nThe Incredible Edible project is an urban gardening project which was started in 2008 by Pamela Warhurst, Mary Clear and a group of like minded people in Todmorden, West Yorkshire, England. The project aims to bring people together through actions around local food, helping to change behaviour towards the environment and to build a kinder and more resilient world. In some cases, it also envisions to have the groups become self-sufficient in food production, hence having all food being produced locally. \n\nSince its conception, the Incredible Edible ethos has been taken up by communities all over the world and there are now 120 Incredible Edible official groups in the UK and more than 700 worldwide. In 2008 to help sustain existing groups and continue to inspire new ones in the UK, the Incredible Edible Network was launched with Pam Warhurst as its chair and Tanya Wall, as its operational lead.\n\nIn the UK, these groups' collective success has begun to directly influence decision-makers both on a national and local level. In response, the network has evolved from a resource for members into a fully fledged movement, simply known as Incredible Edible.\n\n\"At first, we had trouble getting people to help themselves, because we're from a country where people say, 'Get off my land', so we had to tell people it was OK ... nearly 50% said it had had a positive impact on their income.\" -- Mary Clear\nIn 2009, Prince Charles visited the Incredible Edible Todmorden project in support. The group meet regularly at Todmorden Unitarian Church and manage plantings and other food producing projects throughout the town.\n"}
{"id": "2315927", "url": "https://en.wikipedia.org/wiki?curid=2315927", "title": "Index of steam energy articles", "text": "Index of steam energy articles\n\n\n"}
{"id": "28989696", "url": "https://en.wikipedia.org/wiki?curid=28989696", "title": "James Clerk Maxwell", "text": "James Clerk Maxwell\n\nJames Clerk Maxwell (13 June 1831 – 5 November 1879) was a Scottish scientist in the field of mathematical physics. His most notable achievement was to formulate the classical theory of electromagnetic radiation, bringing together for the first time electricity, magnetism, and light as different manifestations of the same phenomenon. Maxwell's equations for electromagnetism have been called the \"second great unification in physics\" after the first one realised by Isaac Newton.\n\nWith the publication of \"A Dynamical Theory of the Electromagnetic Field\" in 1865, Maxwell demonstrated that electric and magnetic fields travel through space as waves moving at the speed of light. Maxwell proposed that light is an undulation in the same medium that is the cause of electric and magnetic phenomena. The unification of light and electrical phenomena led to the prediction of the existence of radio waves.\n\nMaxwell helped develop the Maxwell–Boltzmann distribution, a statistical means of describing aspects of the kinetic theory of gases. He is also known for presenting the first durable colour photograph in 1861 and for his foundational work on analysing the rigidity of rod-and-joint frameworks (trusses) like those in many bridges.\n\nHis discoveries helped usher in the era of modern physics, laying the foundation for such fields as special relativity and quantum mechanics. Many physicists regard Maxwell as the 19th-century scientist having the greatest influence on 20th-century physics. His contributions to the science are considered by many to be of the same magnitude as those of Isaac Newton and Albert Einstein. In the millennium poll – a survey of the 100 most prominent physicists – Maxwell was voted the third greatest physicist of all time, behind only Newton and Einstein. On the centenary of Maxwell's birthday, Einstein described Maxwell's work as the \"most profound and the most fruitful that physics has experienced since the time of Newton\".\n\nJames Clerk Maxwell was born on 13 June 1831 at 14 India Street, Edinburgh, to John Clerk Maxwell of Middlebie, an advocate, and Frances Cay daughter of Robert Hodshon Cay and sister of John Cay. (His birthplace now houses a museum operated by the James Clerk Maxwell Foundation.) His father was a man of comfortable means of the Clerk family of Penicuik, holders of the baronetcy of Clerk of Penicuik. His father's brother was the 6th Baronet. He had been born \"John Clerk\", adding the surname Maxwell to his own after he inherited (as an infant in 1793) the Middlebie country estate near Corsock, Kirkcudbrightshire, from connections to the Maxwell family, themselves members of the peerage. James was a first cousin of both the artist Jemima Blackburn (the daughter of his father's sister) and the civil engineer William Dyce Cay (the son of his mother's brother). Cay and Maxwell were close friends and Cay acted as his best man when Maxwell married.\n\nMaxwell's parents met and married when they were well into their thirties; his mother was nearly 40 when he was born. They had had one earlier child, a daughter named Elizabeth, who died in infancy.\n\nWhen Maxwell was young his family moved to Glenlair House, which his parents had built on the Middlebie estate. All indications suggest that Maxwell had maintained an unquenchable curiosity from an early age. By the age of three, everything that moved, shone, or made a noise drew the question: \"what's the go o' that?\" In a passage added to a letter from his father to his sister-in-law Jane Cay in 1834, his mother described this innate sense of inquisitiveness:\nRecognising the potential of the young boy, Maxwell's mother Frances took responsibility for James's early education, which in the Victorian era was largely the job of the woman of the house. At eight he could recite long passages of Milton and the whole of the 119th psalm (176 verses). Indeed, his knowledge of scripture was already detailed; he could give chapter and verse for almost any quotation from the psalms. His mother was taken ill with abdominal cancer and, after an unsuccessful operation, died in December 1839 when he was eight years old. His education was then overseen by his father and his father's sister-in-law Jane, both of whom played pivotal roles in his life. His formal schooling began unsuccessfully under the guidance of a 16 year old hired tutor. Little is known about the young man hired to instruct Maxwell, except that he treated the younger boy harshly, chiding him for being slow and wayward. The tutor was dismissed in November 1841 and, after considerable thought, Maxwell was sent to the prestigious Edinburgh Academy. He lodged during term times at the house of his aunt Isabella. During this time his passion for drawing was encouraged by his older cousin Jemima.\n\nThe 10 year old Maxwell, having been raised in isolation on his father's countryside estate, did not fit in well at school. The first year had been full, obliging him to join the second year with classmates a year his senior. His mannerisms and Galloway accent struck the other boys as rustic. Having arrived on his first day of school wearing a pair of homemade shoes and a tunic, he earned the unkind nickname of \"Daftie\". He never seemed to resent the epithet, bearing it without complaint for many years. Social isolation at the Academy ended when he met Lewis Campbell and Peter Guthrie Tait, two boys of a similar age who were to become notable scholars later in life. They remained lifelong friends.\n\nMaxwell was fascinated by geometry at an early age, rediscovering the regular polyhedra before he received any formal instruction. Despite winning the school's scripture biography prize in his second year, his academic work remained unnoticed until, at the age of 13, he won the school's mathematical medal and first prize for both English and poetry.\n\nMaxwell's interests ranged far beyond the school syllabus and he did not pay particular attention to examination performance. He wrote his first scientific paper at the age of 14. In it he described a mechanical means of drawing mathematical curves with a piece of twine, and the properties of ellipses, Cartesian ovals, and related curves with more than two foci. His work \"Oval Curves\" was presented to the Royal Society of Edinburgh by James Forbes, a professor of natural philosophy at the University of Edinburgh, because Maxwell was deemed too young to present the work himself. The work was not entirely original, since René Descartes had also examined the properties of such multifocal ellipses in the 17th century, but he had simplified their construction.\n\nMaxwell left the Academy in 1847 at age 16 and began attending classes at the University of Edinburgh. He had the opportunity to attend the University of Cambridge, but decided, after his first term, to complete the full course of his undergraduate studies at Edinburgh. The academic staff of the University included some highly regarded names; his first year tutors included Sir William Hamilton, who lectured him on logic and metaphysics, Philip Kelland on mathematics, and James Forbes on natural philosophy. He did not find his classes at the University demanding, and was therefore able to immerse himself in private study during free time at the University and particularly when back home at Glenlair. There he would experiment with improvised chemical, electric, and magnetic apparatus, however his chief concerns regarded the properties of polarised light. He constructed shaped blocks of gelatine, subjected them to various stresses, and with a pair of polarising prisms given to him by William Nicol, viewed the coloured fringes that had developed within the jelly. Through this practice he discovered photoelasticity, which is a means of determining the stress distribution within physical structures.\n\nAt age 18, Maxwell contributed two papers for the Transactions of the Royal Society of Edinburgh. One of these, \"On the Equilibrium of Elastic Solids\", laid the foundation for an important discovery later in his life, which was the temporary double refraction produced in viscous liquids by shear stress. His other paper was \"Rolling Curves\" and, just as with the paper \"Oval Curves\" that he had written at the Edinburgh Academy, he was again considered too young to stand at the rostrum to present it himself. The paper was delivered to the Royal Society by his tutor Kelland instead.\n\nIn October 1850, already an accomplished mathematician, Maxwell left Scotland for the University of Cambridge. He initially attended Peterhouse, however before the end of his first term transferred to Trinity, where he believed it would be easier to obtain a fellowship. At Trinity he was elected to the elite secret society known as the Cambridge Apostles. Maxwell's intellectual understanding of his Christian faith and of science grew rapidly during his Cambridge years. He joined the \"Apostles\", an exclusive debating society of the intellectual elite, where through his essays he sought to work out this understanding.\n\nThe extent to which Maxwell \"ploughed up\" his Christian beliefs and put them to the intellectual test, can be judged only incompletely from his writings. But there is plenty of evidence, especially from his undergraduate days, that he did deeply examine his faith. Certainly, his knowledge of the Bible was remarkable, so his confidence in the Scriptures was not based on ignorance.\n\nIn the summer of his third year, Maxwell spent some time at the Suffolk home of the Rev C.B. Tayler, the uncle of a classmate, G.W.H. Tayler. The love of God shown by the family impressed Maxwell, particularly after he was nursed back from ill health by the minister and his wife.\n\nOn his return to Cambridge, Maxwell writes to his recent host a chatty and affectionate letter including the following testimony,\n\nIn November 1851, Maxwell studied under William Hopkins, whose success in nurturing mathematical genius had earned him the nickname of \"senior wrangler-maker\".\n\nIn 1854, Maxwell graduated from Trinity with a degree in mathematics. He scored second highest in the final examination, coming behind Edward Routh and earning himself the title of Second Wrangler. He was later declared equal with Routh in the more exacting ordeal of the Smith's Prize examination. Immediately after earning his degree, Maxwell read his paper \"On the Transformation of Surfaces by Bending\" to the Cambridge Philosophical Society. This is one of the few purely mathematical papers he had written, demonstrating Maxwell's growing stature as a mathematician. Maxwell decided to remain at Trinity after graduating and applied for a fellowship, which was a process that he could expect to take a couple of years. Buoyed by his success as a research student, he would be free, apart from some tutoring and examining duties, to pursue scientific interests at his own leisure.\n\nThe nature and perception of colour was one such interest which he had begun at the University of Edinburgh while he was a student of Forbes. With the coloured spinning tops invented by Forbes, Maxwell was able to demonstrate that white light would result from a mixture of red, green, and blue light. His paper \"Experiments on Colour\" laid out the principles of colour combination and was presented to the Royal Society of Edinburgh in March 1855. Maxwell was this time able to deliver it himself.\n\nMaxwell was made a fellow of Trinity on 10 October 1855, sooner than was the norm, and was asked to prepare lectures on hydrostatics and optics and to set examination papers. The following February he was urged by Forbes to apply for the newly vacant Chair of Natural Philosophy at Marischal College, Aberdeen. His father assisted him in the task of preparing the necessary references, but died on 2 April at Glenlair before either knew the result of Maxwell's candidacy. Maxwell accepted the professorship at Aberdeen, leaving Cambridge in November 1856.\n\nThe 25-year-old Maxwell was a good 15 years younger than any other professor at Marischal. He engaged himself with his new responsibilities as head of a department, devising the syllabus and preparing lectures. He committed himself to lecturing 15 hours a week, including a weekly \"pro bono\" lecture to the local working men's college. He lived in Aberdeen with his cousin William Dyce Cay, a Scottish civil engineer, during the six months of the academic year and spent the summers at Glenlair, which he had inherited from his father.\nHe focused his attention on a problem that had eluded scientists for 200 years: the nature of Saturn's rings. It was unknown how they could remain stable without breaking up, drifting away or crashing into Saturn. The problem took on a particular resonance at that time because St John's College, Cambridge had chosen it as the topic for the 1857 Adams Prize. Maxwell devoted two years to studying the problem, proving that a regular solid ring could not be stable, while a fluid ring would be forced by wave action to break up into blobs. Since neither was observed, Maxwell concluded that the rings must be composed of numerous small particles he called \"brick-bats\", each independently orbiting Saturn. Maxwell was awarded the £130 Adams Prize in 1859 for his essay \"On the stability of the motion of Saturn's rings\"; he was the only entrant to have made enough headway to submit an entry. His work was so detailed and convincing that when George Biddell Airy read it he commented \"It is one of the most remarkable applications of mathematics to physics that I have ever seen.\" It was considered the final word on the issue until direct observations by the \"Voyager\" flybys of the 1980s confirmed Maxwell's prediction.\n\nIn 1857 Maxwell befriended the Reverend Daniel Dewar, who was then the Principal of Marischal. Through him Maxwell met Dewar's daughter, Katherine Mary Dewar. They were engaged in February 1858 and married in Aberdeen on 2 June 1858. On the marriage record, Maxwell is listed as Professor of Natural Philosophy in Marischal College, Aberdeen. Seven years Maxwell's senior, comparatively little is known of Katherine, although it is known that she helped in his lab and worked on experiments in viscosity. Maxwell's biographer and friend, Lewis Campbell, adopted an uncharacteristic reticence on the subject of Katherine, though describing their married life as \"one of unexampled devotion\".\n\nIn 1860 Marischal College merged with the neighbouring King's College to form the University of Aberdeen. There was no room for two professors of Natural Philosophy, so Maxwell, despite his scientific reputation, found himself laid off. He was unsuccessful in applying for Forbes's recently vacated chair at Edinburgh, the post instead going to Tait. Maxwell was granted the Chair of Natural Philosophy at King's College, London, instead. After recovering from a near-fatal bout of smallpox in 1860, Maxwell moved to London with his wife.\n\nMaxwell's time at King's was probably the most productive of his career. He was awarded the Royal Society's Rumford Medal in 1860 for his work on colour and was later elected to the Society in 1861. This period of his life would see him display the world's first light-fast colour photograph, further develop his ideas on the viscosity of gases, and propose a system of defining physical quantities—now known as dimensional analysis. Maxwell would often attend lectures at the Royal Institution, where he came into regular contact with Michael Faraday. The relationship between the two men could not be described as being close, because Faraday was 40 years Maxwell's senior and showed signs of senility. They nevertheless maintained a strong respect for each other's talents.\nThis time is especially noteworthy for the advances Maxwell made in the fields of electricity and magnetism. He examined the nature of both electric and magnetic fields in his two-part paper \"On physical lines of force\", which was published in 1861. In it he provided a conceptual model for electromagnetic induction, consisting of tiny spinning cells of magnetic flux. Two more parts were later added to and published in that same paper in early 1862. In the first additional part he discussed the nature of electrostatics and displacement current. In the second additional part, he dealt with the rotation of the plane of the polarisation of light in a magnetic field, a phenomenon that had been discovered by Faraday and is now known as the Faraday effect.\n\nIn 1865 Maxwell resigned the chair at King's College, London, and returned to Glenlair with Katherine. In his paper 'On governors' (1868) he mathematically described the behaviour of governors, devices that control the speed of steam engines, thereby establishing the theoretical basis of control engineering . In his paper \"On reciprocal figures, frames and diagrams of forces\" (1870) he discussed the rigidity of various designs of lattice. He wrote the textbook \"Theory of Heat\" (1871) and the treatise \"Matter and Motion\" (1876). Maxwell was also the first to make explicit use of dimensional analysis, in 1871.\n\nIn 1871 he returned to Cambridge to become the first Cavendish Professor of Physics. Maxwell was put in charge of the development of the Cavendish Laboratory, supervising every step in the progress of the building and of the purchase of the collection of apparatus. One of Maxwell's last great contributions to science was the editing (with copious original notes) of the research of Henry Cavendish, from which it appeared that Cavendish researched, amongst other things, such questions as the density of the Earth and the composition of water.\n\nMaxwell died in Cambridge of abdominal cancer on 5 November 1879 at the age of 48. His mother had died at the same age of the same type of cancer. The minister who regularly visited him in his last weeks was astonished at his lucidity and the immense power and scope of his memory, but comments more particularly,\n\nAs death approached Maxwell told a Cambridge colleague,\n\nMaxwell is buried at Parton Kirk, near Castle Douglas in Galloway close to where he grew up. The extended biography \"The Life of James Clerk Maxwell\", by his former schoolfellow and lifelong friend Professor Lewis Campbell, was published in 1882. His collected works were issued in two volumes by the Cambridge University Press in 1890.\n\nAs a great lover of Scottish poetry, Maxwell memorised poems and wrote his own. The best known is \"Rigid Body Sings\", closely based on \"Comin' Through the Rye\" by Robert Burns, which he apparently used to sing while accompanying himself on a guitar. It has the opening lines\n\nA collection of his poems was published by his friend Lewis Campbell in 1882.\n\nDescriptions of Maxwell remark upon his remarkable intellectual qualities being matched by social awkwardness.\n\nMaxwell was an evangelical Presbyterian and in his later years became an Elder of the Church of Scotland. Maxwell's religious beliefs and related activities have been the focus of a number of papers. Attending both Church of Scotland (his father's denomination) and Episcopalian (his mother's denomination) services as a child, Maxwell later underwent an evangelical conversion in April 1853. One facet of this conversion may have aligned him with an antipositivist position.\n\nMaxwell had studied and commented on electricity and magnetism as early as 1855 when his paper \"On Faraday's lines of force\" was read to the Cambridge Philosophical Society. The paper presented a simplified model of Faraday's work and how electricity and magnetism are related. He reduced all of the current knowledge into a linked set of differential equations with 20 equations in 20 variables. This work was later published as \"On Physical Lines of Force\" in March 1861.\n\nAround 1862, while lecturing at King's College, Maxwell calculated that the speed of propagation of an electromagnetic field is approximately that of the speed of light (see speed of light#electromagnetic constants). He considered this to be more than just a coincidence, commenting, \"We can scarcely avoid the conclusion that light consists in the transverse undulations of the same medium which is the cause of electric and magnetic phenomena.\"\n\nWorking on the problem further, Maxwell showed that the equations predict the existence of waves of oscillating electric and magnetic fields that travel through empty space at a speed that could be predicted from simple electrical experiments; using the data available at the time, Maxwell obtained a velocity of . In his 1864 paper \"A Dynamical Theory of the Electromagnetic Field\", Maxwell wrote, \"The agreement of the results seems to show that light and magnetism are affections of the same substance, and that light is an electromagnetic disturbance propagated through the field according to electromagnetic laws\".\n\nHis famous twenty equations, in their modern form of four partial differential equations, first appeared in fully developed form in his textbook \"A Treatise on Electricity and Magnetism\" in 1873. Most of this work was done by Maxwell at Glenlair during the period between holding his London post and his taking up the Cavendish chair. Maxwell expressed electromagnetism in the algebra of quaternions and made the electromagnetic potential the centrepiece of his theory. In 1881 Oliver Heaviside replaced Maxwell's electromagnetic potential field by 'force fields' as the centrepiece of electromagnetic theory. Heaviside reduced the complexity of Maxwell's theory down to four differential equations, known now collectively as Maxwell's Laws or Maxwell's equations. According to Heaviside, the electromagnetic potential field was arbitrary and needed to be \"murdered\". The use of scalar and vector potentials is now standard in the solution of Maxwell's equations.\n\nA few years later there was a debate between Heaviside and Peter Guthrie Tait about the relative merits of vector analysis and quaternions. The result was the realisation that there was no need for the greater physical insights provided by quaternions if the theory was purely local, and vector analysis became commonplace. Maxwell was proven correct, and his quantitative connection between light and electromagnetism is considered one of the great accomplishments of 19th century mathematical physics.\n\nMaxwell also introduced the concept of the \"electromagnetic field\" in comparison to force lines that Faraday described. By understanding the propagation of electromagnetism as a field emitted by active particles, Maxwell could advance his work on light. At that time, Maxwell believed that the propagation of light required a medium for the waves, dubbed the luminiferous aether. Over time, the existence of such a medium, permeating all space and yet apparently undetectable by mechanical means, proved impossible to reconcile with experiments such as the Michelson–Morley experiment. Moreover, it seemed to require an absolute frame of reference in which the equations were valid, with the distasteful result that the equations changed form for a moving observer. These difficulties inspired Albert Einstein to formulate the theory of special relativity; in the process Einstein dispensed with the requirement of a stationary luminiferous aether.\n\nAlong with most physicists of the time, Maxwell had a strong interest in psychology. Following in the steps of Isaac Newton and Thomas Young, he was particularly interested in the study of colour vision. From 1855 to 1872, Maxwell published at intervals a series of investigations concerning the perception of colour, colour-blindness, and colour theory, and was awarded the Rumford Medal for \"On the Theory of Colour Vision\".\n\nIsaac Newton had demonstrated, using prisms, that white lights, such as sunlight, are composed of a number of monochromatic components which could then be recombined into white light. Newton also showed that an orange paint made of yellow and red could look exactly like a monochromatic orange light, although being composed of two monochromatic yellow and red lights. Hence the paradox that puzzled physicists of the time: two complex lights (composed of more than one monochromatic light) could look alike but be physically different, called \"metameres\". Thomas Young later proposed that this paradox could be explained by colours being perceived through a limited number of channels in the eyes, which he proposed to be threefold, the \"trichromatic colour theory\". Maxwell used the recently developed Linear algebra to prove Young's theory. Any monochromatic light stimulating three receptors should be able to be equally stimulated by a set of three different monochromatic lights (in fact, by any set of three different lights). He demonstrated that to be the case, inventing colour matching experiments and Colourimetry.\n\nMaxwell was also interested in applying his theory of colour perception, namely in colour photography. Stemming directly from his psychological work on colour perception: if a sum of any three lights could reproduce any perceivable colour, then colour photographs could be produced with a set of three coloured filters. In the course of his 1855 paper, Maxwell proposed that, if three black-and-white photographs of a scene were taken through red, green and blue filters and transparent prints of the images were projected onto a screen using three projectors equipped with similar filters, when superimposed on the screen the result would be perceived by the human eye as a complete reproduction of all the colours in the scene.\n\nDuring an 1861 Royal Institution lecture on colour theory, Maxwell presented the world's first demonstration of colour photography by this principle of three-colour analysis and synthesis. Thomas Sutton, inventor of the single-lens reflex camera, took the picture. He photographed a tartan ribbon three times, through red, green, and blue filters, also making a fourth photograph through a yellow filter, which, according to Maxwell's account, was not used in the demonstration. Because Sutton's photographic plates were insensitive to red and barely sensitive to green, the results of this pioneering experiment were far from perfect. It was remarked in the published account of the lecture that \"if the red and green images had been as fully photographed as the blue,\" it \"would have been a truly-coloured image of the riband. By finding photographic materials more sensitive to the less refrangible rays, the representation of the colours of objects might be greatly improved.\" Researchers in 1961 concluded that the seemingly impossible partial success of the red-filtered exposure was due to ultraviolet light, which is strongly reflected by some red dyes, not entirely blocked by the red filter used, and within the range of sensitivity of the wet collodion process Sutton employed.\n\nMaxwell also investigated the kinetic theory of gases. Originating with Daniel Bernoulli, this theory was advanced by the successive labours of John Herapath, John James Waterston, James Joule, and particularly Rudolf Clausius, to such an extent as to put its general accuracy beyond a doubt; but it received enormous development from Maxwell, who in this field appeared as an experimenter (on the laws of gaseous friction) as well as a mathematician.\n\nBetween 1859 and 1866, he developed the theory of the distributions of velocities in particles of a gas, work later generalised by Ludwig Boltzmann. The formula, called the Maxwell–Boltzmann distribution, gives the fraction of gas molecules moving at a specified velocity at any given temperature. In the kinetic theory, temperatures and heat involve only molecular movement. This approach generalised the previously established laws of thermodynamics and explained existing observations and experiments in a better way than had been achieved previously. Maxwell's work on thermodynamics led him to devise the thought experiment that came to be known as Maxwell's demon, where the second law of thermodynamics is violated by an imaginary being capable of sorting particles by energy.\n\nIn 1871 he established Maxwell's thermodynamic relations, which are statements of equality among the second derivatives of the thermodynamic potentials with respect to different thermodynamic variables. In 1874, he constructed a plaster thermodynamic visualisation as a way of exploring phase transitions, based on the American scientist Josiah Willard Gibbs's graphical thermodynamics papers.\n\nMaxwell published a paper \"On governors\" in the \"Proceedings of the Royal Society\", vol. 16 (1867–1868). This paper is considered a central paper of the early days of control theory. Here \"governors\" refers to the governor or the centrifugal governor used to regulate steam engines.\n\nHis name is honoured in several ways:\n\n\n"}
{"id": "3516085", "url": "https://en.wikipedia.org/wiki?curid=3516085", "title": "Kelly hose", "text": "Kelly hose\n\nA Kelly hose (also known as a \"mud hose\" or \"rotary hose\") is a flexible, steel reinforced, high pressure hose that connects the standpipe to the kelly (or more specifically to the goose-neck on the swivel above the kelly) and allows free vertical movement of the kelly while facilitating the flow of drilling fluid through the system and down the drill string. The Kelly hose has a diameter of 3-5 inches (inside diameter).\n"}
{"id": "34127295", "url": "https://en.wikipedia.org/wiki?curid=34127295", "title": "Neelum–Jhelum Hydropower Plant", "text": "Neelum–Jhelum Hydropower Plant\n\nThe Neelum–Jhelum Hydropower Plant is part of a run-of-the-river hydroelectric power scheme in Pakistan, designed to divert water from the Neelum River to a power station on the Jhelum River. The power station is located in (Independent)Azad Kashmir, south of Muzaffarabad, and has an installed capacity of 969 MW. Construction on the project began in 2008 after a Chinese consortium was awarded the construction contract in July 2007. After delay of many years, the first generator was commissioned in April 2018 and the entire project is completed in August 2018 when the fourth and last unit was synchronized with the national grid on 13 August and attained its maximum generation capacity of 969 MW on August 14, 2018. It will generate 5,150 gigawatt per year at the levelised tariff of Rs 13.50 per unit for 30 years.\n\nAfter being approved in 1989, the design was improved, increasing the tunnel length and generation capacity. The project was intended to begin in 2002 and be completed in 2008 but this time-frame experienced significant delays due to problems meeting rising costs. Additionally, the 2005 Kashmir earthquake which devastated the region required a redesign of the project to conform to more stringent seismic standards.\n\nOn 7 July 2007, the Chinese consortium CGGC-CMEC (Gezhouba Group and China National Machinery Import and Export Corporation) were offered a contract to construct the dam and power station. Terms were settled by the end of the year and in January 2008, the letter of commencement was issued. On 8 February, Pakistan's President Pervez Musharraf announced that the project would begin. In October 2011, the diversion tunnel required to reroute the Neelum River around the dam site was completed. \n\nOn 1 November, Pakistan's Prime Minister Syed Yusuf Raza Gilani publicly stated his concern for the project's delay. At its appraisal in 1989, it was to cost $167 million USD (2011) and after another redesign in 2005, that cost rose to $935 million USD (2011). Currently costs have risen to $2.89 billion USD (2011). The project is being constructed under the supervision of the Water and Power Development Authority (WAPDA) and funding is being achieved through the Neelum Jhelum Hydropower Company, taxes, bond offerings, Middle Eastern and Chinese banks. WAPDA has successfully secured loans from a consortium of Chinese banks and from Middle East. Tunnel-boring machines (TBM) were brought to help speed up the excavation of the remaining tunnels. They became operational in February 2013. The project was 66 percent complete as of August 2013 while at the same time the diversion tunnel was 75 percent complete. US$475 million in funding was still not secured by the Economic Affairs Division at that time. In mid-2014 Prime Minister Nawaz Sharif visited the construction site and expressed the hope that at least one generator would be operational by mid-2015. On 24 December 2014 a wall near the diversion tunnel's intake collapsed, killing four workers including a Chinese engineer. On November 05, 2016, the project entered into terminal phase with 100 percent perfect design while achieving 85.5 percent progress and is heading towards completion despite all delays in release of funds, weather conditions, non-availability of power during early stage of construction and delays in land acquisition. \n\nIn Mar 2017 it was reported that the cost of the dam had escalated to PKR 500 Billion. thus the cost of electricity from Neelum Jhelum will be Pakistani Rupees 20 per unit. \n\nAll the civil work including tunnel boring, installation of generators and turbines was completed and water filling of the dam began on October 17, 2017 to put it on the test. \n\nIn October 2017, residents of Muzaffarabad expressed serious concerns that the commissioning of Neelum Jhelum project will drastically reduce the flow of Neelum river thru Muzaffarabad town. \n\nIn Jan 2018 it was reported that the retaining wall of the rock filled dam has got shifted by 18mm from its original position in Nov 2017 when the dam was loaded to design height of 1017 meter. The electricity generation from the dam may be delayed to June-July 2018. \n\nIn early March 2018 it was reported that the filling of water in the head race tunnel has started and the first unit will start electricity generation by end-March, followed by the second, third and fourth units at one month intervals respectively \n\nIn April 2018 the first unit of 242.25 MW was commissioned at a levelised tariff of Rs 13.50 per unit. \n\nThe strategically crucial Neelum-Jhelum Hydropower Project achieved a historic landmark, as the project attained its maximum generation capacity of 969 megawatts (MW) on 14 August 2018. All units of the project are generating power to their maximum capacity.\n\nIn 2007, India began construction on a run-of-the-river power station on the Kishanganga (Neelum) River upstream of the Neelum–Jhelum Dam. The Kishanganga Hydroelectric Plant operates in a similar sense as the Neelum–Jhelum, using a dam to divert the Kishanganga (Neelum) River to a power station before it is discharged into Wular Lake which is fed by the Jhelum River. The Kishanganga Project will divert a portion of the Neelum River from Pakistan which will reduce power generation at the Neelum–Jhelum Hydropower Plant. India states the project will divert 10 percent of the river's flow while other estimates stand as high as 33 percent. Nevertheless, water flow below the Neelum–Jhelum Dam, in Pakistan's Neelum Valley, is expected to be minimal as both projects are diverting water to the Jhelum River. This has the potential to have adverse impacts in the Neelum Valley.\n\nIn 2010, Pakistan appealed to the Hague's Permanent Court of Arbitration (CoA), complaining that the Kishanganga Hydroelectric Plant violates the Indus River Treaty by increasing the catchment of the Jhelum River and depriving Pakistan of its water rights. In June 2011, the CoA visited both the Kishanganga and Neelum–Jhelum Projects. In August 2011, they ordered India to submit more technical data on the project. India had previously reduced the height of the dam from to . After their application was first rejected, the court asked India late September to stop constructing any permanent works that would inhibit restoration of the river. While India could not construct the dam, they continued work on the tunnel and power plant. In February 2013 the Hague ruled that India could divert a minimum of water for the Kishanganga Hydroelectric Plant.\n\nThe Neelum–Jhelum Dam will be a tall and long gravity dam. It will withhold a pondage (reservoir) with a capacity of which is peak storage. The dam diverts up to of the Neelum southeast into a long head-race tunnel, the first of the head-race is two tunnels which later meet into one. The tunnel passes below the Jhelum River and through its bend. At the terminus of the tunnel, the water reaches the surge chamber which contains a tall surge shaft (to prevent water hammer) and a long surge tunnel. From the surge chamber, the water is split into four different penstocks which feed each of the four 242 MW Francis turbine-generators in the underground power house. After being used to generate electricity, the water is discharged southeast back into the Jhelum River at through a long tail-race tunnel. The drop in elevation between the dam and power station afford an average hydraulic head of .\n\nIt is alleged that the procurement of TBM machines resulted in $74 million in kickbacks, according to Transparency International Pakistan.\n\n"}
{"id": "143202", "url": "https://en.wikipedia.org/wiki?curid=143202", "title": "Newsprint", "text": "Newsprint\n\nNewsprint is a low-cost non-archival paper consisting mainly of wood pulp and most commonly used to print newspapers and other publications and advertising material. Invented in 1844 by Charles Fenerty of Nova Scotia, Canada, it usually has an off white cast and distinctive feel. It is designed for use in printing presses that employ a long web of paper (web offset, letterpress and flexographic) rather than individual sheets of paper.\n\nNewsprint is favored by publishers and printers as it is relatively low cost (compared with paper grades used for glossy magazines and sales brochures), strong (to run through modern high-speed web printing presses) and can accept four-color printing at qualities that meet the needs of typical newspapers.\n\nCharles Fenerty began experimenting with wood pulp around 1838, making his discovery in 1844. On October 26, 1844, Fenerty took a sample of his paper to Halifax's top newspaper, the Acadian Recorder, where he had written a letter on his newly invented paper saying:\nThe web of paper is placed on the printer, in the form of a roll of paper, from a paper mill (surplus newsprint can also be cut into individual sheets by a processor for use in a variety of other applications such as wrapping or commercial printing). World demand of newsprint in 2006 totaled about 37.2 million metric tonnes, according to the Montreal-based Pulp & Paper Products Council (PPPC). This was about 1.6% less than in 2000. Between 2000 and 2006, the biggest changes were in Asia—which saw newsprint demand grow by about 20%—and North America, where demand fell by about 25%. Demand in China virtually doubled during the period, to about 3.2 million metric tonnes.\n\nAbout 35% of global newsprint usage in 2006 was in Asia, with approximately 26% being in North America and about 25% in Western Europe. Latin America and Eastern Europe each represented about 5% of world demand in 2006, according to PPPC, with smaller shares going to Oceania and Africa.\n\nAmong the biggest factors depressing demand for newsprint in North America have been the decline in newspaper readership among many sectors of the population—particularly young adults—along with increasing competition for advertising business from the Internet and other media. According to the Newspaper Association of America, a United States newspaper trade group, average U.S. daily circulation in 2006 on a typical weekday was 52.3 million (53.2 million on Sundays), compared with 62.5 million in 1986 (58.9 million on Sundays) and 57.0 million in 1996 (60.8 million on Sundays). According to NAA, daily ad revenues (not adjusted for inflation) reached their all-time peak in 2000, and by 2007 had fallen by 13%. Newsprint demand has also been affected by attempts on the part of newspaper publishers to reduce marginal printing costs through various conservation measures intended to cut newsprint usage.\n\nWhile demand has been trending down in North America in recent years, the rapid economic expansion of such Asian countries as China and India greatly benefited the print newspaper, and thus their newsprint suppliers. According to the World Association of Newspapers, in 2007 Asia was the home to 74 of the world’s 100 highest-circulation dailies. With millions of Chinese and Indians entering the ranks of those with disposable income, newspapers have gained readers along with other news media.\n\nNewsprint is used worldwide in the printing of newspapers, flyers, and other printed material intended for mass distribution. In the U.S., about 80% of all newsprint that is consumed is purchased by daily newspaper publishers, according to PPPC. Dailies use a large majority of total demand in most other regions as well.\n\nTypically in North America, newsprint is purchased by a daily newspaper publisher and is shipped from the mill to the publisher's pressroom or pressrooms, where it is used to print the main body of the newspaper (called the run-of-press, or ROP, sections). The daily newspaper publisher may also be hired by outside companies such as advertisers or publishers of weekly newspapers or other daily newspapers to produce printed products for those companies using its presses. In such cases the press owner might also purchase newsprint from the mill for such contract printing jobs.\n\nFor the roughly 20% of demand which is not purchased by a daily newspaper, common end uses include the printing of weekly newspapers, advertising flyers and other printed products, generally by a commercial printer, a company whose business consists largely of printing products for other companies using its presses. In such a case, the newsprint may be purchased by the printer on behalf of an advertiser or a weekly newspaper publisher, or it may be purchased by the client and then ordered to be shipped to the printer's location.\n\nThe biggest inputs to the newsprint manufacturing process are energy, fiber, and labor. Mill operating margins have been significantly affected in the 2006–2008 time-frame by rising energy costs. Many mills' fiber costs have also been affected during the U.S. housing market slowdown of 2007–8 by the shutdown of many sawmills, particularly in Canada, since the virgin fiber used by mills generally comes from nearby sawmills in the form of wood chips produced as a residual product of the saw milling process.\n\nAnother consideration in the newsprint business is delivery, which is affected by energy cost trends. Newsprint around the world may be delivered by rail or truck; or by barge, container or break-bulk shipment if a water delivery is appropriate. (Aside from delivery cost, another consideration in selecting freight mode may be the potential for avoiding damage to the product.) All things being equal, for domestic shipments in areas like North America or Europe where modern road and rail networks are readily available, trucks can be more economical than rail for short-haul deliveries (a day or less from the mill), while rail may be more economical for longer shipments. The cost-competitiveness of each freight mode for a specific mill’s business may depend on local infrastructure issues, as well as the degree of truck-vs-freight competition in the mill's region. The appropriate freight mode for delivery from a mill to a specific pressroom can also depend on the press room ability to accept enough trucks or rail cars.\n\nA newspaper roll's width is called its web width and is defined by how many front pages it can print. A full roll prints four front pages with four back pages behind it (two front and back on each of the two sections). Modern printing facilities most efficiently print newspapers in multiples of eight pages on a full newsprint roll in two sections of four pages each. The two sections are then cut in half.\n\nFaced with dwindling revenue from competition with broadcast, cable, and internet outlets, U.S. newspapers in the 21st century—particularly broadsheets—have begun to reduce the width of their newsprint rolls, and hence of the newspapers, to a standard size across the business.\n\nThe longtime standard 54-inch web (13½ inch front page) (metric: 137.16 cm web, 34.29 cm front page) has given way to smaller newspaper sizes. New broadsheet standards in the U.S. are 44, 46, and 48-inch webs (11, 11.5, and 12 inch newspaper page widths, respectively) (metric: 111.76 cm, 116.84 cm, 121.92 cm, page widths: 27.94 cm, 29.21 cm, 30.48 cm respectively). Newspapers such as \"USA Today\" have already converted to new, narrower web width standards, which are also easier for readers to handle, especially commuters. Interest in the reduced standard increased when \"The Wall Street Journal\" abandoned its iconic 60-inch web (15 inch page) format (metric: 152.4 cm, 38.1 cm page) in favor of the new 48-inch newspaper industry standard starting on January 2, 2007. \"The New York Times\" has followed suit, abandoning its 54-inch web (13½ inch page) on August 6, 2007.\n\nIn February 2009, \"The Seattle Times\" moved from a 50-inch web to a 46-inch web, producing an 11½ inch page width (metric: was 127 cm, became 116.84 cm, page 29.21 cm).\n\nNewspapers in many other parts of the world, including \"The Times\", \"The Guardian\", and \"The Independent\" in the United Kingdom, are also downsizing their broadsheets.\n\nNewsprint is generally made by a mechanical milling process, without the chemical processes that are often used to remove lignin from the pulp. The lignin causes the paper to become brittle and yellow when exposed to air or sunlight. Traditionally, newsprint was made from fibers extracted from various softwood species of trees (most commonly, spruce, fir, balsam fir or pine). However, an increasing percentage of the world's newsprint is made with recycled fibers.\n\nThere are upper limits on the percentage of the world's newsprint that can be manufactured from recycled fiber. For instance, some of the fiber that enters a recycled pulp mill is lost in pulping, due to inefficiencies inherent in the process. According to the web site of the U.K. chapter of Friends of the Earth, wood fiber can normally only be recycled up to five times due to damage to the fiber. Thus, unless the quantity of newsprint used each year worldwide declines in line with the lost fiber, a certain amount of new (virgin) fiber is required each year globally, even though individual newsprint mills may use 100% recycled fiber.\n\n"}
{"id": "41297973", "url": "https://en.wikipedia.org/wiki?curid=41297973", "title": "Nohra Padilla", "text": "Nohra Padilla\n\nNohra Padilla is a Colombian environmentalist. She grew up in Bogotá. She has assumed a leading position in the Association of Recyclers of Bogotá, and of the National Association of Recyclers in Colombia, which organizes about 12,000 members. She was awarded the Goldman Environmental Prize in 2013 for her contribution to waste management and recycling in Colombia.\n"}
{"id": "21490", "url": "https://en.wikipedia.org/wiki?curid=21490", "title": "Nylon", "text": "Nylon\n\nNylon is a generic designation for a family of synthetic polymers, based on aliphatic or semi-aromatic polyamides. \nNylon is a thermoplastic silky material \nthat can be melt-processed into fibers films or shapes.\n\nNylon was the first commercially successful synthetic thermoplastic polymer. DuPont began its research project in 1930.\nThe first example of nylon (nylon 6,6) was produced using diamines on February 28, 1935, by Wallace Hume Carothers at DuPont's research facility at the DuPont Experimental Station. In response to Carothers' work, Paul Schlack at IG Farben developed nylon 6, a different molecule based on caprolactam, on January 29, 1938.\n\nNylon was first used commercially in a nylon-bristled toothbrush in 1938, followed more famously in women's stockings or \"nylons\" which were shown at the 1939 New York World's Fair and first sold commercially in 1940. During World War II, almost all nylon production was diverted to the military for use in parachutes and parachute cord. Wartime uses of nylon and other plastics greatly increased the market for the new materials.\n\nNylon is made of repeating units linked by amide links similar to the peptide bonds in proteins.\nCommercially, nylon polymer is made by reacting monomers which are either lactams, acid/amines or stoichiometric mixtures of diamines (-NH) and diacids (-COOH). Mixtures of these can be polymerized together to make copolymers. Nylon polymers can be mixed with a wide variety of additives to achieve many different property variations.\nNylon polymers have found significant commercial applications in fabric and fibers (apparel, flooring and rubber reinforcement), in shapes (molded parts for cars, electrical equipment, etc.), and in films (mostly for food packaging).\n\nDuPont, founded by Éleuthère Irénée du Pont, first produced gunpowder and later cellulose-based paints. Following WWI, DuPont produced synthetic ammonia and other chemicals. DuPont began experimenting with the development of cellulose based fibres, eventually producing the synthetic fibre rayon. DuPont's experience with rayon was an important precursor to its development and marketing of nylon.\n\nDuPont's invention of nylon spanned a nine-year period, ranging from the start of the project in 1930 to its exhibition at the World Fair in New York in 1939. The project grew from a new structure at DuPont, suggested by Charles Stine in 1927, in which the chemical department would be composed of several small research teams that would focus on “pioneering research” in chemistry and would “lead to practical applications”. Harvard instructor Wallace Hume Carothers was hired to direct the polymer research group. Initially he was allowed to focus on pure research, building on and testing the theories of German chemist Hermann Staudinger. He was very successful as research he undertook greatly improved the knowledge of polymers and contributed to science.\n\nIn the spring of 1930, Carothers and his team had already synthesized two new polymers. One was neoprene, a synthetic rubber greatly used during the war. The other was a white elastic but strong paste that would later become nylon. After these discoveries Carother’s team was made to shift its research from a more pure research approach investigating general polymerization to a more practically-focused goal of finding “one chemical combination that would lend itself to industrial applications”.\n\nIt wasn’t until the beginning of 1935 that a polymer called \"polymer 6-6\" was finally produced. The first example of nylon (nylon 6,6) was produced by Wallace Carothers on February 28, 1935, at DuPont's research facility at the DuPont Experimental Station. It had all the desired properties of elasticity and strength.\nHowever, it also required a complex manufacturing process that would become the basis of industrial production in the future. DuPont obtained a patent for the polymer in September 1938, and quickly achieved a monopoly of the fibre.\n\nThe production of nylon required interdepartmental collaboration between three departments at DuPont: the Department of Chemical Research, the Ammonia Department, and the Department of Rayon. Some of the key ingredients of nylon had to be produced using high pressure chemistry, the main area of expertise of the Ammonia Department. Nylon was considered a “godsend to the Ammonia Department”, which had been in financial difficulties. The reactants of nylon soon constituted half of the Ammonia department’s sales and helped them come out of the period of the Great Depression by creating jobs and revenue at DuPont.\n\nDuPont's nylon project demonstrated the importance of chemical engineering in industry, helped create jobs, and furthered the advancement of chemical engineering techniques. In fact, it developed a chemical plant that provided 1800 jobs and used the latest technologies of the time, which are still used as a model for chemical plants today. “The success of the nylon project thus had to do with its ability to achieve the rapid mobilization of a large number of DuPont’s chemists and engineers”. The first nylon plant was located at Seaford, Delaware, beginning commercial production on December 15, 1939. On October 26, 1995, the Seaford plant was designated a National Historic Chemical Landmark by the American Chemical Society.\n\nDuPont went through an extensive process to generate names for its new product. \nIn 1940, John W. Eckelberry of DuPont stated that the letters \"nyl\" were arbitrary and the \"on\" was copied from the suffixes of other fibers such as cotton and Rayon. A later publication by DuPont (\"Context\", vol. 7, no. 2, 1978) explained that the name was originally intended to be \"No-Run\" (\"run\" meaning \"unravel\"), but was modified to avoid making such an unjustified claim. Since the products were not really run-proof, the vowels were swapped to produce \"nuron\", which was changed to \"nilon\" \"to make it sound less like a nerve tonic\". For clarity in pronunciation, the \"i\" was changed to \"y.\"\n\nAn important part of nylon’s popularity stems from DuPont’s marketing strategy. The fibre was promoted to increase demand before the product was available to the general market. Nylon’s commercial announcement occurred on October 27, 1938, at the final session of the \"Herald Tribune\"s yearly \"Forum on Current Problems\", on the site of the approaching New York City world's fair. The “first man-made organic textile fiber” which was derived from “coal, water and air” and promised to be “as strong as steel, as fine as the spider’s web” was received enthusiastically by the audience, many of them middle-class women, and made the headlines of most newspapers. Nylon was introduced as part of \"The world of tomorrow\" at the 1939 New York World's Fair and was featured at DuPont's \"Wonder World of Chemistry\" at the Golden Gate International Exposition in San Francisco in 1939. Actual nylon stockings were not shipped to selected stores in the national market until May 15, 1940. However, a limited number were released for sale in Delaware before that. The first public sale of nylon stockings occurred on October 24, 1939, in Wilmington, Delaware. 4,000 pairs of stockings were available, all of which were sold within three hours.\n\nAnother added bonus to the campaign was that it meant reducing silk imports from Japan, an argument that won over many wary customers. Nylon was even mentioned by President Roosevelt’s cabinet, which addressed its “vast and interesting economic possibilities” five days after the material was formally announced.\n\nHowever, the early excitement over nylon also caused problems. It fueled unreasonable expectations that nylon would be better than silk, a miracle fabric as strong as steel that would last forever and never run. Realizing the danger of claims such as “New Hosiery Held Strong as Steel” and “No More Runs”, Du Pont scaled back the terms of the original announcement, especially those stating that nylon would possess the strength of steel.\n\nAlso, DuPont executives marketing nylon as a revolutionary man-made material did not at first realize that some consumers experienced a sense of unease and distrust, even fear, towards synthetic fabrics.\nA particularly damaging news story, drawing on DuPont's 1938 patent for the new polymer, suggested that one method of producing nylon might be to use cadaverine (pentamethylenediamine), a chemical extracted from corpses. Although scientists asserted that cadaverine was also extracted by heating coal, the public often refused to listen. A woman confronted one of the lead scientists at DuPont and refused to accept that the rumour was not true.\n\nDuPont changed its campaign strategy, emphasizing that nylon was made from “coal, air and water”, and started focusing on the personal and aesthetic aspects of nylon, rather than its intrinsic qualities. Nylon was thus domesticated, and attention shifted to the material and consumer aspect of the fiber with slogans like “If it’s nylon, it’s prettier, and oh! How fast it dries!”.\n\nAfter nylon’s nationwide release in 1940, production was increased. 1300 tons of the fabric were produced during 1940. During their first year on the market, 64 million pairs of nylon stockings were sold.\nWhile nylon was marketed as the durable and indestructible material of the people, it was sold at almost twice the price of silk stockings ($4.27 per pound of nylon versus $2.79 per pound of silk). Sales of nylon stockings were strong in part due to changes in women’s fashion. As Lauren Olds explains: “by 1939 [hemlines] had inched back up to the knee, closing the decade just as it started off”. The shorter skirts were accompanied by a demand for stockings that offered fuller coverage without the use of garters to hold them up.\n\nHowever, as of February 11, 1942, nylon production was redirected from being a consumer material to one used by the military. DuPont's production of nylon stockings and other lingerie stopped, and most manufactured nylon was used to make parachutes and tents for World War II.\n\nOnce the war ended, the return of nylon was awaited with great anticipation. Although DuPont projected yearly production of 360 million pairs of stockings, there were delays in converting back to consumer rather than wartime production. In 1946, the demand for nylon stockings could not be satisfied, which led to the Nylon Riots. In one case, an estimated 40,000 people lined up in Pittsburgh to buy 13,000 pairs of nylons. In the meantime, women cut up nylon tents and parachutes left from the war in order to make blouses and wedding dresses. Between the end of the war and 1952, production of stockings and lingerie used 80% of the world’s nylon. DuPont put a lot of focus on catering to the civilian demand, and continually expanded its production.\n\nAs pure nylon hosiery was sold in a wider market, problems became apparent. Nylon stockings were found to be fragile, in the sense that the thread often tended to unravel lengthwise, creating ‘runs’. People also reported that pure nylon textiles could be uncomfortable due to nylon's lack of absorbency. Moisture stayed inside the fabric near the skin under hot or moist conditions instead of being \"wicked\" away. Nylon fabric could also be itchy, and tended to cling and sometimes spark as a result of static electrical charge built up by friction.\nAlso, under some conditions stockings could decompose turning back into nylon's original components of air, coal, and water. Scientists explained this as a result of air pollution, attributing it to London smog in 1952, as well as poor air quality in New York and Los Angeles.\n\nThe solution found to problems with pure nylon fabric was to blend nylon with other existing fibres or polymers such as cotton, polyester, and spandex. This led to the development of a wide array of blended fabrics. The new nylon blends retained the desirable properties of nylon (elasticity, durability, ability to be dyed) and kept clothes prices low and affordable.\nAs of 1950, the New York Quartermaster Procurement Agency (NYQMPA), which developed and tested textiles for the army and navy, had committed to developing a wool-nylon blend. They were not the only ones to introduce blends of both natural and synthetic fibres. \"America's Textile Reporter\" referred to 1951 as the \"Year of the blending of the fibers\". Fabric blends included mixes like \"Bunara\" (wool-rabbit-nylon) and \"Casmet\" (wool-nylon-fur). In Britain in November 1951, the inaugural address of the 198th session of the Royal Society for the Encouragement of Arts, Manufactures and Commerce focused on the blending of textiles.\n\nDuPont's Fabric Development Department cleverly targeted French fashion designers, supplying them with fabric samples. In 1955, designers such as Coco Chanel, Jean Patou, and Christian Dior showed gowns created with DuPont fibers, and fashion photographer Horst P. Horst was hired to document their use of DuPont fabrics. \"American Fabrics\" credited blends with providing \"creative possibilities and new ideas for fashions which had been hitherto undreamed of.\"\n\nIn spite of oil shortages in the 1970s, consumption of nylon textiles continued to grow by 7.5 per cent per annum between the 1960s and 1980s. \nOverall production of synthetic fibres, however, dropped from 63% of the worlds textile production in 1965, to 45% of the world's textile production in early 1970s. The appeal of \"new\" technologies wore off, and nylon fabric \"was going out of style in the 1970s\". Also, consumers became concerned about environmental costs throughout the production cycle: obtaining the raw materials (oil), energy use during production, waste produced during creation of the fibre, and eventual waste disposal of materials that were not biodegradable. \nSynthetic fibres have not dominated the market since the 1950s and 1960s. , nylon continued to represent about 12% (8 million pounds) of the world's production of synthetic fibres.\n\nAlthough pure nylon has many flaws and is now rarely used, its derivatives have greatly influenced and contributed to society. From scientific discoveries relating to the production of plastics and polymerization, to economic impact during the depression and the changing of women's fashion, nylon was a revolutionary product. The Lunar Flag Assembly, the first flag planted on the moon in a symbolic gesture of celebration, was made of nylon. The flag itself cost $5.50, but had to have a specially-designed flagpole with a horizontal bar so that it would appear to \"fly\".\n\"Nylon is one of the great symbols of the American century, on a par no doubt with Coca-Cola in the consumer dreams of 20th century men and women. [...] it is not only a technologically advanced product, it has also captured the public's imagination [...]: nylon as an object of desire.\"\n\nNylons are condensation polymers or copolymers, formed by reacting difunctional monomers containing equal parts of amine and carboxylic acid, so that amides are formed at both ends of each monomer in a process analogous to polypeptide biopolymers. Most nylons are made from the reaction of a dicarboxylic acid with a diamine (e.g. PA66) or a lactam or amino acid with itself (e.g. PA6). In the first case, the \"repeating unit\" consists of one of each monomer, so that they alternate in the chain, similar to the so-called ABAB structure of polyesters and polyurethanes. Since each monomer in this copolymer has the same reactive group on both ends, the direction of the amide bond reverses between each monomer, unlike natural polyamide proteins, which have overall directionality: C terminal → N terminal. In the second case (so called AA), the repeating unit corresponds to the single monomer.\n\nIn common usage, the prefix 'PA' (polyamide) or the name 'Nylon' are used interchangeably and are equivalent in meaning.\n\nThe nomenclature used for nylon polymers was devised during the synthesis of the first simple aliphatic nylons and uses numbers to describe the number of carbons in each monomer unit, including the carbon(s) of the carboxylic acid(s). Subsequent use of cyclic and aromatic monomers required the use of letters or sets of letters. One number after 'PA' or 'Nylon' indicates a homopolymer which is \"monadic\" or based on one amino acid (minus HO) as monomer:\n\nTwo numbers or sets of letters indicate a \"dyadic\" homopolymer formed from two monomers: one diamine and one dicarboxylic acid. The first number indicates the number of carbons in the diamine. The two numbers should be separated by a comma for clarity, but the comma is often omitted.\n\nFor copolymers the comonomers or pairs of comonomers are separated by slashes:\n\nThe term polyphthalamide (abbreviated to PPA) is used when 60% or more moles of the carboxylic acid portion of the repeating unit in the polymer chain is composed of a combination of terephthalic acid (TPA) and isophthalic acid (IPA).\n\nWallace Carothers at DuPont patented nylon 66 using amides. \nIn the case of nylons that involve reaction of a diamine and a dicarboxylic acid, it is difficult to get the proportions exactly correct, and deviations can lead to chain termination at molecular weights less than a desirable 10,000 daltons (u). To overcome this problem, a crystalline, solid \"nylon salt\" can be formed at room temperature, using an exact 1:1 ratio of the acid and the base to neutralize each other. The salt is crystallized to purify it and obtain the desired precise stoichiometry. Heated to 285 °C (545 °F), the salt reacts to form nylon polymer with the production of water.\n\nThe synthetic route using lactams (cyclic amides) was developed by Paul Schlack at IG Farben, leading to nylon 6, or polycaprolactam — formed by a ring-opening polymerization. The peptide bond within the caprolactam is broken with the exposed active groups on each side being incorporated into two new bonds as the monomer becomes part of the polymer backbone.\n\nThe 428 °F (220 °C) melting point of nylon 6 is lower than the 509 °F (265 °C) melting point of nylon 66.\n\nNylon 510, made from pentamethylene diamine and sebacic acid, was studied by Carothers even before nylon 66 and has superior properties, but is more expensive to make. In keeping with this naming convention, \"nylon 6,12\" or \"PA 612\" is a copolymer of a 6C diamine and a 12C diacid. Similarly for PA 510 PA 611; PA 1012, etc. Other nylons include copolymerized dicarboxylic acid/diamine products that are \"not\" based upon the monomers listed above. For example, some fully aromatic nylons (known as \"aramids\") are polymerized with the addition of diacids like terephthalic acid (→ Kevlar, Twaron) or isophthalic acid (→ Nomex), more commonly associated with polyesters. There are copolymers of PA 66/6; copolymers of PA 66/6/12; and others. In general linear polymers are the most useful, but it is possible to introduce branches in nylon by the condensation of dicarboxylic acids with polyamines having three or more amino groups.\n\nThe general reaction is:\nTwo molecules of water are given off and the nylon is formed. Its properties are determined by the R and R' groups in the monomers. In nylon 6,6, R = 4C and R' = 6C alkanes, but one also has to include the two carboxyl carbons in the diacid to get the number it donates to the chain. In Kevlar, both R and R' are benzene rings.\n\nIndustrial synthesis is usually done by heating the acids, amines or lactams to remove water, but in the laboratory, diacid chlorides can be reacted with diamines. For example, a popular demonstration of interfacial polymerization (the \"nylon rope trick\") is the synthesis of nylon 66 from adipoyl chloride and hexamethylene diamine.\n\nNylons can also be synthesized from dinitriles using acid catalysis. For example, this method is applicable for preparation of nylon 1,6 from adiponitrile, formaldehyde and water. Additionally, nylons can be synthesized from diols and dinitriles using this method as well.\n\nNylon monomers are manufactured by a variety of routes, starting in most cases from crude oil but sometimes from biomass. Those in current production are described below.\n\n\n\nVarious diamine components can be used, which are derived from a variety of sources. Most are petrochemicals, but bio-based materials are also being developed.\n\nDue to the large number of diamines, diacids and aminoacids that can be synthesized, many nylon polymers have been made experimentally and characterized to varying degrees. A smaller number have been scaled up and offered commercially, and these are detailed below.\n\nHomopolymer nylons derived from one monomer\n\nExamples of these polymers that are or were commercially available\n\nHomopolymer polyamides derived from pairs of diamines and diacids (or diacid derivatives). Shown in the table below are polymers which are or have been offered commercially either as homopolymers or as a part of a copolymer.\n\nExamples of these polymers that are or were commercially available\n\nIt is easy to make mixtures of the monomers or sets of monomers used to make nylons to obtain copolymers. This lowers crystallinity and can therefore lower the melting point.\n\nSome copolymers that have been or are commercially available are listed below: \n\nMost nylon polymers are miscible with each other allowing a range of blends to be made. The two polymers can react with one another by transamidation to form random copolymers.\n\nAccording to their crystallinity, polyamides can be:\n\nAccording to this classification, PA66, for example, is an aliphatic semi-crystalline homopolyamide.\n\nNylon clothing tends to be less flammable than cotton and rayon, but nylon fibres may melt and stick to skin.\n\nAbove their melting temperatures, \"T\", thermoplastics like nylon are amorphous solids or viscous fluids in which the chains approximate random coils. Below \"T\", amorphous regions alternate with regions which are lamellar crystals. The amorphous regions contribute elasticity and the crystalline regions contribute strength and rigidity. The planar amide (-CO-NH-) groups are very polar, so nylon forms multiple hydrogen bonds among adjacent strands. Because the nylon backbone is so regular and symmetrical, especially if all the amide bonds are in the \"trans\" configuration, nylons often have high crystallinity and make excellent fibers. The amount of crystallinity depends on the details of formation, as well as on the kind of nylon.\n\nNylon 66 can have multiple parallel strands aligned with their neighboring peptide bonds at coordinated separations of exactly 6 and 4 carbons for considerable lengths, so the carbonyl oxygens and amide hydrogens can line up to form interchain hydrogen bonds repeatedly, without interruption (see the figure opposite). Nylon 510 can have coordinated runs of 5 and 8 carbons. Thus parallel (but not antiparallel) strands can participate in extended, unbroken, multi-chain β-pleated sheets, a strong and tough supermolecular structure similar to that found in natural silk fibroin and the β-keratins in feathers. (Proteins have only an amino acid α-carbon separating sequential -CO-NH- groups.) Nylon 6 will form uninterrupted H-bonded sheets with mixed directionalities, but the β-sheet wrinkling is somewhat different. The three-dimensional disposition of each alkane hydrocarbon chain depends on rotations about the 109.47° tetrahedral bonds of singly bonded carbon atoms.\n\nWhen extruded into fibers through pores in an industry spinneret, the individual polymer chains tend to align because of viscous flow. If subjected to cold drawing afterwards, the fibers align further, increasing their crystallinity, and the material acquires additional tensile strength. In practice, nylon fibers are most often drawn using heated rolls at high speeds.\n\nBlock nylon tends to be less crystalline, except near the surfaces due to shearing stresses during formation. Nylon is clear and colorless, or milky, but is easily dyed. Multistranded nylon cord and rope is slippery and tends to unravel. The ends can be melted and fused with a heat source such as a flame or electrode to prevent this.\n\nNylons are hygroscopic, and will absorb or desorb moisture as a function of the ambient humidity. Variations in moisture content have several effects on the polymer. Firstly, the dimensions will change, but more importantly moisture acts as a plasticizer, lowering the glass transition temperature (\"T\"), and consequently the elastic modulus at temperatures below the \"T\"\n\nWhen dry, polyamide is a good electrical insulator. However, polyamide is hygroscopic. The absorption of water will change some of the material's properties such as its electrical resistance. Nylon is less absorbent than wool or cotton.\n\nThe characteristic features of nylon 6,6 include:\n\nOn the other hand, nylon 6 is easy to dye, more readily fades; it has a higher impact resistance, a more rapid moisture absorption, greater elasticity and elastic recovery.\n\nBill Pittendreigh, DuPont, and other individuals and corporations worked diligently during the first few months of World War II to find a way to replace Asian silk and hemp with nylon in parachutes. It was also used to make tires, tents, ropes, ponchos, and other military supplies. It was even used in the production of a high-grade paper for U.S. currency. At the outset of the war, cotton accounted for more than 80% of all fibers used and manufactured, and wool fibers accounted for nearly all of the rest. By August 1945, manufactured fibers had taken a market share of 25%, at the expense of cotton. After the war, because of shortages of both silk and nylon, nylon parachute material was sometimes repurposed to make dresses.\n\nNylon 6 and 66 fibers are used in carpet manufacture.\n\nNylon is one kind of fiber used in tire cord. Herman E. Schroeder pioneered application of nylon in tires.\n\nNylon resins are widely used in the automobile industry especially in the engine compartment.\n\nMolded nylon is used in hair combs and mechanical parts such as machine screws, gears, gaskets, and other low- to medium-stress components previously cast in metal. Engineering-grade nylon is processed by extrusion, casting, and injection molding. Type 6,6 Nylon 101 is the most common commercial grade of nylon, and Nylon 6 is the most common commercial grade of molded nylon. For use in tools such as spudgers, nylon is available in glass-filled variants which increase structural and impact strength and rigidity, and molybdenum disulfide-filled variants which increase lubricity. Its various properties also make it very useful as a material in additive manufacturing; specifically as a filament in consumer and professional grade fused deposition modeling 3D printers. Nylon can be used as the matrix material in composite materials, with reinforcing fibers like glass or carbon fiber; such a composite has a higher density than pure nylon. Such thermoplastic composites (25% to 30% glass fiber) are frequently used in car components next to the engine, such as intake manifolds, where the good heat resistance of such materials makes them feasible competitors to metals.\n\nNylon was used to make the stock of the Remington Nylon 66 rifle. The frame of the modern Glock pistol is made of a nylon composite.\n\nNylon resins are used as a component of food packaging films where an oxygen barrier is needed. Some of the terpolymers based upon nylon are used every day in packaging. Nylon has been used for meat wrappings and sausage sheaths. The high temperature resistance of nylon makes it useful for oven bags.\n\nNylon filaments are primarily used in brushes especially toothbrushes and string trimmers. They are also used as monofilaments in fishing line. Nylon 610 and 612 are the most used polymers for filaments.\n\nIts various properties also make it very useful as a material in additive manufacturing; specifically as a filament in consumer and professional grade fused deposition modeling 3D printers.\n\nNylon resins can be extruded into rods, tubes and sheets.\n\nNylon powders are used to powder coat metals. Nylon 11 and nylon 12 are the most widely used.\n\nIn the mid-1940s, classical guitarist Andrés Segovia mentioned the shortage of good guitar strings in the United States, particularly his favorite Pirastro catgut strings, to a number of foreign diplomats at a party, including General Lindeman of the British Embassy. A month later, the General presented Segovia with some nylon strings which he had obtained via some members of the DuPont family. Segovia found that although the strings produced a clear sound, they had a faint metallic timbre which he hoped could be eliminated.\n\nNylon strings were first tried on stage by Olga Coelho in New York in January, 1944.\n\nIn 1946, Segovia and string maker Albert Augustine were introduced by their mutual friend Vladimir Bobri, editor of Guitar Review. On the basis of Segovia's interest and Augustine's past experiments, they decided to pursue the development of nylon strings. DuPont, skeptical of the idea, agreed to supply the nylon if Augustine would endeavor to develop and produce the actual strings. After three years of development, Augustine demonstrated a nylon first string whose quality impressed guitarists, including Segovia, in addition to DuPont.\n\nWound strings, however, were more problematic. Eventually, however, after experimenting with various types of metal and smoothing and polishing techniques, Augustine was also able to produce high quality nylon wound strings.\n\nAll nylons are susceptible to hydrolysis, especially by strong acids, a reaction essentially the reverse of the synthetic reaction shown above. The molecular weight of nylon products so attacked drops, and cracks form quickly at the affected zones. Lower members of the nylons (such as nylon 6) are affected more than higher members such as nylon 12. This means that nylon parts cannot be used in contact with sulfuric acid for example, such as the electrolyte used in lead–acid batteries.\n\nWhen being molded, nylon must be dried to prevent hydrolysis in the molding machine barrel since water at high temperatures can also degrade the polymer. The reaction is of the type:\n\nBerners-Lee calculates the average greenhouse gas footprint of nylon in manufacturing carpets at 5.43 kg CO equivalent per kg, when produced in Europe. This gives it almost the same carbon footprint as wool, but with greater durability and therefore a lower overall carbon footprint.\n\nData published by PlasticsEurope indicates for nylon 66 a greenhouse gas footprint of 6.4 kg CO equivalent per kg, and an energy consumption of 138 kJ/kg. When considering the environmental impact of nylon, it is important to consider the use phase. In particular when cars are lightweighted, significant savings in fuel consumption and CO emissions are realised.\n\nVarious nylons break down in fire and form hazardous smoke, and toxic fumes or ash, typically containing hydrogen cyanide. Incinerating nylons to recover the high energy used to create them is usually expensive, so most nylons reach the garbage dumps, decaying slowly. Discarded nylon fabric takes 30–40 years to decompose. Nylon is a robust polymer and lends itself well to recycling. Much nylon resin is recycled directly in a closed loop at the injection molding machine, by grinding sprues and runners and mixing them with the virgin granules being consumed by the molding machine.\n\nAs one of the largest engineering polymer families, the global demand of nylon resins and compounds was valued at roughly US$20.5 billion in 2013. The market is expected to reach US$30 billion by 2020 by following an average annual growth of 5.5%.\n\n\n"}
{"id": "11300116", "url": "https://en.wikipedia.org/wiki?curid=11300116", "title": "Off-Site Source Recovery Project", "text": "Off-Site Source Recovery Project\n\nThe \"Off-Site Source Recovery Project\" (OSRP) is a U.S. government project funded by the Department of Energy at Los Alamos National Laboratory. The OSRP mission is to remove excess, unwanted, abandoned, or orphan radioactive sealed sources that pose a potential risk to health, safety, and national security.\n\nThis government project had its beginnings in the Energy Department's early Environmental Management program. Environmental management was intended to clean up environmental contamination and dispose of vast quantities of radioactive and hazardous waste stemming from several decades of nuclear weapons design, research, testing and production. \n\nOne of the issues faced by the early Environmental Management program was substantial amounts of radioactive and hazardous materials at universities and other locations left over from contracts and grants into various aspect of nuclear technology research. The Energy Department began addressing this problem with a low-budget effort called the Offsite Waste Program.\n\nThe initial scope of the OSR Project included all Greater than Class C (GTCC) sealed sources. However, since September 11, 2001, the mission expanded from environmental concerns to public safety and national security. As a result, OSRP direction and oversight moved from the Department of Energy's (DOE) Office of Environmental Management, to the National Nuclear Security Administration in 2003. \n\nThe project mainly addresses sources containing americium and plutonium. The recently expanded mission also includes recovery of beta or gamma emitting sources, like caesium and strontium. \n\nOSRP has been able to recover more than 18,000 sources from nearly 700 sites in 48 States, the DC area, Puerto Rico and a few foreign countries.\n\n"}
{"id": "20073329", "url": "https://en.wikipedia.org/wiki?curid=20073329", "title": "Orșova Wind Farm", "text": "Orșova Wind Farm\n\nThe Orşova Wind Farm is an under construction wind power project in Mehedinţi County, Romania. It will consist of an individual wind farm with 25 individual wind turbines with a nominal output of around 1.4 MW which will deliver up to 35 MW of power, enough to power over 22,925 homes, with a capital investment required of approximately US$75 million.\n"}
{"id": "4639166", "url": "https://en.wikipedia.org/wiki?curid=4639166", "title": "Pipeflow", "text": "Pipeflow\n\nIn hydrology, pipeflow is a type of subterranean water flow where water travels along cracks in the soil or old root systems found in above ground vegetation.\n\nIn such soils which have a high vegetation content water is able to travel along the 'pipes', allowing water to travel faster than throughflow. Here, water can move at speeds between 50 and 500 m/h.\n"}
{"id": "19215181", "url": "https://en.wikipedia.org/wiki?curid=19215181", "title": "Plate detector (radio)", "text": "Plate detector (radio)\n\nIn electronics, a plate detector (anode bend detector, grid bias detector) is a vacuum tube circuit in which an amplifying tube having a control grid is operated in a non-linear region of its grid voltage versus plate current transfer characteristic near plate current cutoff in order to demodulate an amplitude modulated carrier signal. This differs from the grid leak detector, which utilizes non-linearity of the grid voltage versus grid current characteristic for demodulation. It also differs from the diode detector, which is a two terminal device.\n\nPlate detector circuits were commonly used from the 1920s until the start of World War II. In 1927, the advent of screen grid tubes permitted much more radio frequency amplification prior to the detector stage than previously practically possible. The previously used grid leak detector was less suited to the higher radio frequency signal level than the plate detector. Diode detectors also became popular during the later 1920s because, unlike plate detector circuits, they could also provide automatic gain control voltage (A.V.C.) for the radio frequency amplifier stages of the receiver. However, the dual-diode/triode and dual-diode/pentode tubes commonly used for detection/A.V.C. circuits had bulk wholesale costs that were as much as twice the cost of the tubes commonly used as plate detectors. This made plate detector circuits more practical for low-priced radios sold during the depths of the Great Depression.\n\nNegative bias is applied to the grid to bring the plate current almost to cutoff. The grid is connected directly to the secondary of a radio frequency or intermediate frequency transformer. An incoming signal will cause the plate current to increase much more during the positive 180 degrees of the carrier frequency cycle than it decreases during the negative 180 degrees. The plate current variation will include the modulation envelope. The plate current is passed through a plate load impedance chosen to produce the desired amplification in conjunction with the tube characteristics. A capacitor of low impedance at the carrier frequency and high impedance at audio frequencies is provided between the tube plate and cathode, to minimize amplification of the carrier frequency and remove carrier frequency variations from the recovered modulation waveform. The allowable peak 100% modulated input signal voltage is limited to the magnitude of the bias voltage, corresponding to an unmodulated carrier peak voltage of half the bias voltage magnitude.\n\nEither fixed bias or cathode bias may be used for the plate detector. When cathode bias is implemented, a capacitor of low impedance at the carrier frequency and high impedance at audio frequencies bypasses the cathode resistor. Cathode bias reduces the amplification obtainable.\n\nPlate detector circuits usually do not produce A.V.C. voltage for the radio frequency (R.F.) stages of the receiver. In these receivers, volume control is often accomplished by providing variable cathode bias of one or more stages prior to the detector. A potentiometer is used to implement the variable cathode bias. The most common connection of the potentiometer (typically 4 kΩ to 15 kΩ linear taper) is as follows:\n\n\nTo set a limit on the ability of the volume control to reduce the bias on the stages that it controls, the potentiometer is often equipped with a mechanical rotation limit facility that prevents the resistance from being reduced below a specific amount.\n\n\nBecause the volume control in non-A.V.C. receivers adjusts R.F. signal levels rather than A.F. signal levels, the volume control must be manipulated while tuning the radio in order to find weak signals.\n\n\nIn the Infinite-Impedance detector, the load resistance is placed in series with the cathode, rather than the plate, and the demodulated output is taken from the cathode. The circuit is operated in the region where grid current does not occur during any portion of the carrier frequency cycle, thus the name \"Infinite Impedance Detector\". An example schematic diagram of an implementation using a field effect transistor is shown.\n\nAs with the standard plate detector, the device is biased almost completely off. The positive-going 180 degrees of the carrier input signal causes substantial increase of cathode or source current above the amount set by the bias, and the negative going 180 degrees of the carrier cycle causes very little decrease of cathode current below the level set by the bias. C is charged to a dc voltage determined by the carrier amplitude. C can only be discharged via R, and the circuit acts as a peak detector at the carrier frequency. The C R time constant is much shorter than the period of the highest modulating frequency, permitting the voltage across C to follow the modulation envelope. Negative feedback takes place at the recovered modulation frequencies, reducing distortion. The infinite impedance detector can demodulate higher modulation percentages with less distortion than the plate detector.\n\nR values of 50,000 to 150,000 ohms are typical for tubes. The time constant of C with R is chosen to be several times the period of the lowest carrier frequency, with C values of 100 to 500 picofarads being typical. The low pass filter in the V+ power supply line, C4 and the RFC (RF Choke) shown in the diagram, minimizes unwanted RF coupling through the power supply to other circuitry and does not contribute to the function of the detector.\n\n"}
{"id": "16369918", "url": "https://en.wikipedia.org/wiki?curid=16369918", "title": "Plutonium hexafluoride", "text": "Plutonium hexafluoride\n\nPlutonium hexafluoride is the highest fluoride of plutonium, and is of interest for laser enrichment of plutonium, in particular for the production of pure plutonium-239 from irradiated uranium. This pure plutonium is needed to avoid premature ignition of low-mass nuclear weapon designs by neutrons produced by spontaneous fission of plutonium-240.\n\nIt is a red-brown volatile crystalline solid; the heat of sublimation is 12.1 kcal/mol and the heat of vaporization 7.4 kcal/mol. It is relatively hard to handle, being very corrosive and prone to auto-radiolysis.\n\nIt is prepared by fluorination of plutonium tetrafluoride (PuF) by powerful fluorinating agents such as elemental fluorine.\n\nIt can also be obtained by fluorination of plutonium(III) fluoride or plutonium(IV) oxide.\n\nIn 1984, the synthesis of plutonium hexafluoride was achieved at unprecedented low temperatures through the use of dioxygen difluoride. Previous techniques needed temperatures so high that the plutonium hexafluoride produced would decompose rapidly. Hydrogen fluoride is not sufficient; even though it is a powerful fluorinating agent.\n\nUnder laser irradiation at a wavelength of less than 520 nm, it decomposes to plutonium pentafluoride and fluorine; after more irradiation it decomposes further to plutonium tetrafluoride.\n"}
{"id": "469201", "url": "https://en.wikipedia.org/wiki?curid=469201", "title": "Plymax", "text": "Plymax\n\nPlymax is a building material, used in airplanes. It consists of a thin sheet of duralumin bonded to a thicker sheet of plywood. Most notably it was used in the Morane-Saulnier M.S.406, and French aircraft of the 1930s. It was also used on the bodies of some cars such as a 1931 Triumph Super 9. The articulated body of the Trojan Tasker also used this material.\n"}
{"id": "276485", "url": "https://en.wikipedia.org/wiki?curid=276485", "title": "Polariton", "text": "Polariton\n\nIn physics, polaritons are quasiparticles resulting from strong coupling of electromagnetic waves with an electric or magnetic dipole-carrying excitation. They are an expression of the common quantum phenomenon known as level repulsion, also known as the avoided crossing principle. Polaritons describe the crossing of the dispersion of light with any interacting resonance. To this extent polaritons can also be thought as the new normal modes of a given material or structure arising from the strong coupling of the bare modes, which are the photon and the dipolar oscillation. The polariton is a bosonic quasiparticle, and should not be confused with the polaron (a fermionic one), which is an electron plus an attached phonon cloud.\n\nWhenever the polariton picture is valid, the model of photons propagating freely in crystals is insufficient. A major feature of polaritons is a strong dependency of the propagation speed of light through the crystal on the frequency of the photon. For exciton-polaritons, rich experimental results on various aspects have been gained in copper (I) oxide.\n\nOscillations in ionized gases were observed by Tonks and Langmuir in 1929. Polaritons were first considered theoretically by Tolpygo. They were termed light-excitons in Ukrainian and Russian scientific literature. That name was suggested by Pekar, but the term \"polariton,\" proposed by Hopfield, was adopted. Coupled states of electromagnetic waves and phonons in ionic crystals and their dispersion relation, now known as phonon polaritons, were obtained by Tolpygo in 1950 and, independently, by Kun in 1951. Collective interactions were published by Pines and Bohm in 1952, and plasmons were described in silver by Fröhlich and Pelzer in 1955. Ritchie predicted surface plasmons in 1957, then Ritchie and Eldridge published experiments and predictions of emitted photons from irradiated metal foils in 1962. Otto first published on surface plasmon-polaritons in 1968.\nRoom-temperature superfluidity of polaritons was observed in 2016 by Giovanni Lerario et al., at CNR NANOTEC Institute of Nanotechnology, using an organic microcavity supporting stable Frenkel exciton-polaritons at room temperature. In February 2018, scientists reported the discovery of a new three-photon form of light, which may involve polaritons, that could be useful in the development of quantum computers.\n\nA polariton is the result of the mixing of a photon with a polar excitation of a material. The following are types of polaritons:\n\n\n\n"}
{"id": "43941844", "url": "https://en.wikipedia.org/wiki?curid=43941844", "title": "Pump (film)", "text": "Pump (film)\n\nPump is a 2014 documentary film by Josh Tickell and Rebecca Harrell Tickell. The film begins by exploring the history of petroleum-based fuel consumption, the use of the Internal combustion engine and the geopolitics involved with petroleum. It is primarily focuses on the United States but also includes a segment on the automotive industry in China. The film then explores in-depth on the alternative energy options for vehicles that are either readily available for use or can be on a mass scale. This includes ethanol fuel, methanol fuel, Flexible-fuel vehicles in Brazil, flexible-fuel vehicles in the United States, and electric vehicles including Tesla Motors.\n\nFunding for the film came from Patrón tequila founder John Paul DeJoria, Rhino Films executive Stephen Nemeth and the Fuel Freedom Foundation.\n"}
{"id": "45756", "url": "https://en.wikipedia.org/wiki?curid=45756", "title": "Pyrite", "text": "Pyrite\n\nThe mineral pyrite, or iron pyrite, also known as fool's gold, is an iron sulfide with the chemical formula FeS (iron(II) disulfide). Pyrite is considered the most common of the sulfide minerals.\n\nPyrite's metallic luster and pale brass-yellow hue give it a superficial resemblance to gold, hence the well-known nickname of \"fool's gold\". The color has also led to the nicknames \"brass\", \"brazzle\", and \"Brazil\", primarily used to refer to pyrite found in coal.\n\nThe name \"pyrite\" is derived from the Greek πυρίτης (\"pyritēs\"), \"of fire\" or \"in fire\", in turn from πύρ (\"pyr\"), \"fire\". In ancient Roman times, this name was applied to several types of stone that would create sparks when struck against steel; Pliny the Elder described one of them as being brassy, almost certainly a reference to what we now call pyrite.\n\nBy Georgius Agricola's time, c. 1550, the term had become a generic term for all of the sulfide minerals.\nPyrite is usually found associated with other sulfides or oxides in quartz veins, sedimentary rock, and metamorphic rock, as well as in coal beds and as a replacement mineral in fossils, but has also been identified in the sclerites of scaly-foot gastropods. Despite being nicknamed fool's gold, pyrite is sometimes found in association with small quantities of gold. Gold and arsenic occur as a coupled substitution in the pyrite structure. In the Carlin–type gold deposits, arsenian pyrite contains up to 0.37% gold by weight.\n\nPyrite enjoyed brief popularity in the 16th and 17th centuries as a source of ignition in early firearms, most notably the wheellock, where the cock held a lump of pyrite against a circular file to strike the sparks needed to fire the gun.\n\nPyrite has been used since classical times to manufacture \"copperas\" (iron(II) sulfate). Iron pyrite was heaped up and allowed to weather (an example of an early form of heap leaching). The acidic runoff from the heap was then boiled with iron to produce iron sulfate. In the 15th century, new methods of such leaching began to replace the burning of sulfur as a source of sulfuric acid. By the 19th century, it had become the dominant method.\n\nPyrite remains in commercial use for the production of sulfur dioxide, for use in such applications as the paper industry, and in the manufacture of sulfuric acid. Thermal decomposition of pyrite into FeS (iron(II) sulfide) and elemental sulfur starts at ; at around \"p\" is about .\n\nA newer commercial use for pyrite is as the cathode material in Energizer brand non-rechargeable lithium batteries.\n\nPyrite is a semiconductor material with a band gap of 0.95 eV.\n\nDuring the early years of the 20th century, pyrite was used as a mineral detector in radio receivers, and is still used by crystal radio hobbyists. Until the vacuum tube matured, the crystal detector was the most sensitive and dependable detector available – with considerable variation between mineral types and even individual samples within a particular type of mineral. Pyrite detectors occupied a midway point between galena detectors and the more mechanically complicated perikon mineral pairs. Pyrite detectors can be as sensitive as a modern 1N34A germanium diode detector.\n\nPyrite has been proposed as an abundant, inexpensive material in low-cost photovoltaic solar panels. Synthetic iron sulfide was used with copper sulfide to create the photovoltaic material.\n\nPyrite is used to make marcasite jewelry. Marcasite jewelry, made from small faceted pieces of pyrite, often set in silver, was known since ancient times and was popular in the Victorian era. At the time when the term became common in jewelry making, \"marcasite\" referred to all iron sulfides including pyrite, and not to the orthorhombic FeS mineral marcasite which is lighter in color, brittle and chemically unstable, and thus not suitable for jewelry making. Marcasite jewelry does not actually contain the mineral marcasite.\n\nChina represents the main importing country with an import of around 376,000 tonnes, which resulted at 45% of total global imports. China is also the fastest growing in terms of the unroasted iron pyrites imports, with a CAGR of +27.8% from 2007 to 2016. In value terms, China ($47M) constitutes the largest market for imported unroasted iron pyrites worldwide, making up 65% of global imports.\n\nFrom the perspective of classical inorganic chemistry, which assigns formal oxidation states to each atom, pyrite is probably best described as FeS. This formalism recognizes that the sulfur atoms in pyrite occur in pairs with clear S–S bonds. These persulfide units can be viewed as derived from hydrogen disulfide, HS. Thus pyrite would be more descriptively called iron persulfide, not iron disulfide. In contrast, molybdenite, MoS, features isolated sulfide (S) centers and the oxidation state of molybdenum is Mo. The mineral arsenopyrite has the formula FeAsS. Whereas pyrite has S subunits, arsenopyrite has [AsS] units, formally derived from deprotonation of HAsSH. Analysis of classical oxidation states would recommend the description of arsenopyrite as Fe[AsS].\n\nIron-pyrite FeS represents the prototype compound of the crystallographic pyrite structure. The structure is simple cubic and was among the first crystal structures solved by X-ray diffraction. It belongs to the crystallographic space group \"Pa\" and is denoted by the Strukturbericht notation C2. Under thermodynamic standard conditions the lattice constant formula_1 of stoichiometric iron pyrite FeS amounts to . The unit cell is composed of a Fe face-centered cubic sublattice into which the S ions are embedded. The pyrite structure is also used by other compounds \"MX\" of transition metals \"M\" and chalcogens \"X\" = O, S, Se and Te. Also certain dipnictides with \"X\" standing for P, As and Sb etc. are known to adopt the pyrite structure.\n\nIn the first bonding sphere, the Fe atoms are surrounded by six S nearest neighbours, in a distorted octahedral arrangement. The material is a diamagnetic semiconductor and the Fe ions should be considered to be in a \"low spin\" divalent state (as shown by Mössbauer spectroscopy as well as XPS), rather than a tetravalent state as the stoichiometry would suggest.\n\nThe positions of \"X\" ions in the pyrite structure may be derived from the fluorite structure, starting from a hypothetical Fe(S) structure. Whereas F ions in CaF occupy the centre positions of the eight subcubes of the cubic unit cell ( ) etc., the S ions in FeS are shifted from these high symmetry positions along <111> axes to reside on (\"uuu\") and symmetry-equivalent positions. Here, the parameter \"u\" should be regarded as a free atomic parameter that takes different values in different pyrite-structure compounds (iron pyrite FeS: \"u\"(S) = 0.385 ). The shift from fluorite \"u\" = 0.25 to pyrite \"u\" = 0.385 is rather large and creates a S-S distance that is clearly a binding one. This is not surprising as in contrast to F an ion S is not a closed shell species. It is isoelectronic with a chlorine \"atom\", also undergoing pairing to form Cl molecules. Both low spin Fe and the disulfide S moeties are closed shell entities, explaining the diamagnetic and semiconducting properties.\n\nThe S atoms have bonds with three Fe and one other S atom. The site symmetry at Fe and S positions is accounted for by point symmetry groups \"C\" and \"C\", respectively. The missing center of inversion at S lattice sites has important consequences for the crystallographic and physical properties of iron pyrite. These consequences derive from the crystal electric field active at the sulfur lattice site, which causes a polarisation of S ions in the pyrite lattice. The polarisation can be calculated on the basis of higher-order Madelung constants and has to be included in the calculation of the lattice energy by using a generalised Born–Haber cycle. This reflects the fact that the covalent bond in the sulfur pair is inadequately accounted for by a strictly ionic treatment.\n\nArsenopyrite has a related structure with heteroatomic As-S pairs rather than homoatomic ones. Marcasite also possesses homoatomic anion pairs, but the arrangement of the metal and diatomic anions is different from that of pyrite. Despite its name a chalcopyrite does not contain dianion pairs, but single S sulfide anions.\n\nPyrite usually forms cuboid crystals, sometimes forming in close association to form raspberry-shaped masses called framboids. However, under certain circumstances, it can form anastamozing filaments or T-shaped crystals.\nPyrite can also form almost perfect dodecahedral shapes known as pyritohedra and this suggests an explanation for the artificial geometrical models found in Europe as early as the 5th century BC.\n\nCattierite (Co S) and vaesite (Ni S) are similar in their structure and belong also to the pyrite group.\n\nBravoite is a nickel-cobalt bearing variety of pyrite, with > 50% substitution of Ni for Fe within pyrite. Bravoite is not a formally recognised mineral, and is named after Peruvian scientist Jose J. Bravo (1874–1928).\n\nIt is distinguishable from native gold by its hardness, brittleness and crystal form. Natural gold tends to be anhedral (irregularly shaped), whereas pyrite comes as either cubes or multifaceted crystals. Pyrite can often be distinguished by the striations which, in many cases, can be seen on its surface. Chalcopyrite is brighter yellow with a greenish hue when wet and is softer (3.5–4 on Mohs' scale). Arsenopyrite is silver white and does not become more yellow when wet.\n\nIron pyrite is unstable at Earth's surface: iron pyrite exposed to air and water decomposes into iron oxides and sulfate. This process is hastened by the action of \"Acidithiobacillus\" bacteria which oxidize the pyrite to produce ferrous iron and sulfate. These reactions occur more rapidly when the pyrite is in fine crystals and dust, which is the form it takes in most mining operations.\n\nSulfate released from decomposing pyrite combines with water, producing sulfuric acid, leading to acid rock drainage. An example of acid rock drainage caused by pyrite is the 2015 Gold King Mine waste water spill.\n\nPyrite oxidation is sufficiently exothermic that underground coal mines in high-sulfur coal seams have occasionally had serious problems with spontaneous combustion in the mined-out areas of the mine. The solution is to hermetically seal the mined-out areas to exclude oxygen.\n\nIn modern coal mines, limestone dust is sprayed onto the exposed coal surfaces to reduce the hazard of dust explosions. This has the secondary benefit of neutralizing the acid released by pyrite oxidation and therefore slowing the oxidation cycle described above, thus reducing the likelihood of spontaneous combustion. In the long term, however, oxidation continues, and the hydrated sulfates formed may exert crystallization pressure that can expand cracks in the rock and lead eventually to roof fall.\n\nBuilding stone containing pyrite tends to stain brown as the pyrite oxidizes. This problem appears to be significantly worse if any marcasite is present. The presence of pyrite in the aggregate used to make concrete can lead to severe deterioration as the pyrite oxidizes. In early 2009, problems with Chinese drywall imported into the United States after Hurricane Katrina were attributed to oxidation of pyrite, which releases hydrogen sulfide gas. These problems included a foul odor and corrosion of copper wiring. In the United States, in Canada, and more recently in Ireland, where it was used as underfloor infill, pyrite contamination has caused major structural damage. Modern tests for aggregate materials certify such materials as free of pyrite.\n\nPyrite and marcasite commonly occur as replacement pseudomorphs after fossils in black shale and other sedimentary rocks formed under reducing environmental conditions. \nHowever, \"pyrite dollars\" or \"pyrite suns\" which have an appearance similar to sand dollars are pseudofossils and lack the pentagonal symmetry of the animal.\n\n\n"}
{"id": "42956042", "url": "https://en.wikipedia.org/wiki?curid=42956042", "title": "Rossano Ercolini", "text": "Rossano Ercolini\n\nRossano Ercolini is an Italian teacher and grassroots environmentalist from Tuscany. He was awarded the Goldman Environmental Prize in 2013, in particular for his efforts on informing the public on health and environmental risks of incineration, and for his advocating for the zero waste principles.\n"}
{"id": "33315517", "url": "https://en.wikipedia.org/wiki?curid=33315517", "title": "Slieve Rushen Wind Farm", "text": "Slieve Rushen Wind Farm\n\nSlieve Rushen Wind Farm is an 18 turbine wind farm in County Fermanagh, Northern Ireland with a total capacity of 54 MW, enough to power over 30,000 homes. It was commissioned in April 2008.\n\n"}
{"id": "25187903", "url": "https://en.wikipedia.org/wiki?curid=25187903", "title": "Special fine paper", "text": "Special fine paper\n\nSpecial fine paper is a classification of paper used for copying and digital printing.\n\nCopy paper is used for copying and laser printers. The basis weight is 70-90 g/m² (approximately 18-24 lb) and ISO brightness 80-96%. It is made of 90–100% virgin chemical pulp or 100% deinked pulp with total pigment content of 10-15%. The most important quality is smooth run in a copying machine / printer and good dimensional stability. It must not show curling or cockling, nor may it retain dust.\n\nDigital printing paper is also called \"electronic printing paper\". The basis weight is 40-400 g/m². This paper quality may be either coated or uncoated. The demands of the paper may vary substantially depending on printing method: electrical charge, thermal, magnetic or ink-jet. All require good dimensional stability, no curling or cockling, good surface strength and surface smoothness. For ink-jet paper it is also important with sufficient and uniform porosity to counteract spreading of the ink.\n\n"}
{"id": "37408607", "url": "https://en.wikipedia.org/wiki?curid=37408607", "title": "Szyszkowski equation", "text": "Szyszkowski equation\n\nThe Szyskowski Equation has been used by Meissner and Michaels to describe the decrease in surface tension of aqueous solutions of carboxylic acids, alcohols and esters at varying mole fractions. It describes the exponential decrease of the surface tension at low concentrations reasonably but should be used only at concentrations below 1 mole%.\n\nwith:\n\nThe equation can be rearranged to be explicit in \"a\":\nThis allows the direct calculation of that component specific parameter \"a\" from experimental data.\n\nThe equation can also be written as:\nwith:\n\nThe surface tension of pure water is dependent on temperature. At room temperature (298 K), it is equal to 71.97 mN/m \n\nMeissner and Michaels published the following \"a\" constants:\nThe following table and diagram show experimentally determined surface tensions in the mixture of water and propionic acid.\nThis example shows a good agreement between the published value a=2.6*10 and the calculated value a=2.59*10 at the smallest given mole fraction of 0.00861 but at higher concentrations of propionic acid the value of an increases considerably, showing deviations from the predicted value.\n\n"}
{"id": "31151610", "url": "https://en.wikipedia.org/wiki?curid=31151610", "title": "Tilt test (geotechnical engineering)", "text": "Tilt test (geotechnical engineering)\n\nIn geomechanics, a tilt test is a simple test to estimate the shear strength parameters of a discontinuity. Two pieces of rock containing a discontinuity are held in hand or mounted in test equipment with the discontinuity horizontal. The sample is slowly tilted until the top block moves. The angle with the horizontal at onset of movement is called the \"tilt-angle\".\n\nThe size of the specimen is limited to 10–20 cm for hand-held tests, while machine-operated tilt test equipment may handle up to meter-sized samples. In the field, the angle can be determined most easily with an inclinometer as present in most geological or structural compasses.\n\nThe \"tilt-angle\" equals the material friction of the discontinuity wall plus the roughness i-angle (\"tilt-angle\" = \"φ\" + \"i\") if no real cohesion is present (i.e. no cementing or gluing material between the two blocks), no infill material is present, the asperities do not break, and the walls of the discontinuity are completely fitting at the start of the test, while if the walls of the discontinuity are completely non-fitting, the \"tilt-angle\" equals the friction of the material of the discontinuity walls (\"tilt-angle\" = \"φ\"). If cementation or gluing material is present or asperities break, the \"tilt-angle\" represents a combination of the (apparent or real) cohesion and the friction along the discontinuity. If infill material is present, the \"tilt-angle\" is governed partially or completely by the infill, depending on the thickness of the infill and height of asperities.\n"}
{"id": "2218225", "url": "https://en.wikipedia.org/wiki?curid=2218225", "title": "Vessel element", "text": "Vessel element\n\nA vessel element or vessel member (trachea) is one of the cell types found in xylem, the water conducting tissue of plants. Vessel elements (tracheae) are typically found in flowering plants (angiosperms) but absent from most gymnosperms such as conifers. Vessel elements are the main feature distinguishing the \"hardwood\" of angiosperms from the \"softwood\" of conifers.\n\nXylem is the tissue in vascular plants which conducts water (and substances dissolved in it) upwards in a plant. There are two kinds of cell which are involved in the actual transport: tracheids and vessel elements. Vessel elements are the building blocks of vessels, which constitute the major part of the water transporting system in those plants in which they occur. Vessels form an efficient system for transporting water (including necessary minerals) from the root to the leaves and other parts of the plant.\n\nIn secondary xylem – the xylem which is produced as a stem thickens rather than when it first appears – a vessel element originates from the vascular cambium. A long cell, oriented along the axis of the stem, called a \"fusiform initial\", divides along its length forming new vessel elements. The cell wall of a vessel element becomes strongly \"lignified\", i.e. it develops reinforcing material made of lignin. The side walls of a vessel element have pits: more or less circular regions in contact with neighbouring cells. Tracheids also have pits, but only vessel elements have openings at both ends that connect individual vessel elements to form a continuous tubular vessel. These end openings are called perforations or perforation plates. They have a variety of shapes: the most common are the simple perforation (a simple opening) and the scalariform perforation (several elongated openings in a ladder-like design). Other types include the foraminate perforation plate (several round openings) and the reticulate perforation plate (a net-like pattern, with many openings).\n\nAt maturity the protoplast – the living material of the cell – dies and disappears, but the lignified cell walls persist. A vessel element is then a dead cell, but one that still has a function, and is still being protected by surrounding living cells.\n\nThe presence of vessels in xylem has been considered to be one of the key innovations that led to the success of the flowering plants. It was once thought that vessel elements were an evolutionary innovation of flowering plants, but their absence from some basal angiosperms and their presence in some members of the Gnetales suggest that this hypothesis must be re-examined; vessel elements in Gnetales may not be homologous with those of angiosperms, or vessel elements that originated in a precursor to the angiosperms may have been subsequently lost in some basal lineages (e.g., Amborellaceae, Tetracentraceae, Trochodendraceae, and Winteraceae), described by Arthur Cronquist as \"primitively vesselless\". Cronquist considered the vessels of \"Gnetum\" to be convergent with those of angiosperms.\n\nVessel-like cells have also been found in the xylem of \"Equisetum\" (horsetails), \"Selaginella\" (spike-mosses), \"Pteridium aquilinum\" (bracken fern), \"Marsilea\" and \"Regnellidium\" (aquatic ferns), and the enigmatic fossil group Gigantopteridales. In these cases, it is generally agreed that the vessels evolved independently. It is possible that vessels may have appeared more than once among the angiosperms as well.\n\n\n"}
{"id": "45358446", "url": "https://en.wikipedia.org/wiki?curid=45358446", "title": "World Bank", "text": "World Bank\n\nThe World Bank () is an international financial institution that provides loans to countries of the world for capital projects. It comprises two institutions: the International Bank for Reconstruction and Development (IBRD), and the International Development Association (IDA). The World Bank is a component of the World Bank Group.\n\nThe World Bank's most recent stated goal is the reduction of poverty. As of November 2018, the largest recipients of world bank loans were India ($859 million in 2018) and China ($370 million in 2018), through loans from IBRD.\n\nThe World Bank is different from the World Bank Group, an extended family of five international organizations:\n\n\nThe World Bank was created at the 1944 Bretton Woods Conference along with the International Monetary Fund (IMF). The president of the World Bank is, traditionally, an American. The World Bank and the IMF are both based in Washington, D.C., and work closely with each other.\n\nAlthough many countries were represented at the Bretton Woods Conference, the United States and United Kingdom were the most powerful in attendance and dominated the negotiations. The intention behind the founding of the World Bank was to provide temporary loans to low-income countries which were unable to obtain loans commercially. The Bank may also make loans and demand policy reforms from recipients.\n\nBefore 1974, the reconstruction and development loans provided by the World Bank were relatively small. The Bank's staff were aware of the need to instill confidence in the bank. Fiscal conservatism ruled, and loan applications had to meet strict criteria.\n\nThe first country to receive a World Bank loan was France. The Bank's president at the time, John McCloy, chose France over two other applicants, Poland and Chile. The loan was for US$250 million, half the amount requested, and it came with strict conditions. France had to agree to produce a balanced budget and give priority of debt repayment to the World Bank over other governments. World Bank staff closely monitored the use of the funds to ensure that the French government met the conditions. In addition, before the loan was approved, the United States State Department told the French government that its members associated with the Communist Party would first have to be removed. The French government complied and removed the Communist coalition government - the so-called tripartite. Within hours, the loan to France was approved.\n\nWhen the Marshall Plan went into effect in 1947, many European countries began receiving aid from other sources. Faced with this competition, the World Bank shifted its focus to non-European countries. Until 1968, its loans were earmarked for the construction of infrastructure works, such as seaports, highway systems, and power plants, that would generate enough income to enable a borrower country to repay the loan. In 1960, the International Development Association was formed (as opposed to a UN fund named SUNFED), providing soft loans to developing countries.\n\nFrom 1974 to 1980 the bank concentrated on meeting the basic needs of people in the developing world. The size and number of loans to borrowers was greatly increased as loan targets expanded from infrastructure into social services and other sectors.\n\nThese changes can be attributed to Robert McNamara, who was appointed to the presidency in 1968 by Lyndon B. Johnson. McNamara implored bank treasurer Eugene Rotberg to seek out new sources of capital outside of the northern banks that had been the primary sources of funding. Rotberg used the global bond market to increase the capital available to the bank. One consequence of the period of poverty alleviation lending was the rapid rise of third world debt. From 1976 to 1980 developing world debt rose at an average annual rate of 20%.\n\nIn 1980 the World Bank Administrative Tribunal was established to decide on disputes between the World Bank Group and its staff where allegation of non-observance of contracts of employment or terms of appointment had not been honored.\n\nIn 1980 McNamara was succeeded by US President Jimmy Carter's nominee, Alden W. Clausen. Clausen replaced many members of McNamara's staff and crafted a different mission emphasis. His 1982 decision to replace the bank's Chief Economist, Hollis B. Chenery, with Anne Krueger was an example of this new focus. Krueger was known for her criticism of development funding and for describing Third World governments as \"rent-seeking states.\"\n\nDuring the 1980s the bank emphasized lending to service Third-World debt, and structural adjustment policies designed to streamline the economies of developing nations. UNICEF reported in the late 1980s that the structural adjustment programs of the World Bank had been responsible for \"reduced health, nutritional and educational levels for tens of millions of children in Asia, Latin America, and Africa\".\n\nBeginning in 1989, in response to harsh criticism from many groups, the bank began including environmental groups and NGOs in its loans to mitigate the past effects of its development policies that had prompted the criticism. It also formed an implementing agency, in accordance with the Montreal Protocols, to stop ozone-depletion damage to the Earth's atmosphere by phasing out the use of 95% of ozone-depleting chemicals, with a target date of 2015. Since then, in accordance with its so-called \"Six Strategic Themes\", the bank has put various additional policies into effect to preserve the environment while promoting development. For example, in 1991 the bank announced that to protect against deforestation, especially in the Amazon, it would not finance any commercial logging or infrastructure projects that harm the environment.\n\nIn order to promote global public goods, the World Bank tries to control communicable disease such as malaria, delivering vaccines to several parts of the world and joining combat forces. In 2000 the bank announced a \"war on AIDS\" and in 2011 the Bank joined the Stop Tuberculosis Partnership.\n\nTraditionally, based on a tacit understanding between the United States and Europe, the president of the World Bank has always been selected from candidates nominated by the United States. In 2012, for the first time, two non-US citizens were nominated.\n\nOn 23 March 2012, U.S. President Barack Obama announced that the United States would nominate Jim Yong Kim as the next president of the Bank. Jim Yong Kim was elected on 27 April 2012.\n\nVarious developments had brought the Millennium Development Goals targets for 2015 within reach in some cases. For the goals to be realized, six criteria must be met: stronger and more inclusive growth in Africa and fragile states, more effort in health and education, integration of the development and environment agendas, more as well as better aid, movement on trade negotiations, and stronger and more focused support from multilateral institutions like the World Bank.\n\n\nTo make sure that World Bank-financed operations do not compromise these goals but instead add to their realisation, environmental, social and legal safeguards were defined. However, these safeguards have not been implemented entirely yet. At the World Bank's annual meeting in Tokyo 2012 a review of these safeguards has been initiated, which was welcomed by several civil society organisations.\n\nThe President of the Bank is the president of the entire World Bank Group. The president, currently Jim Yong Kim, is responsible for chairing the meetings of the Boards of Directors and for overall management of the Bank. Traditionally, the President of the Bank has always been a US citizen nominated by the United States, the largest shareholder in the bank (the managing director of the International Monetary Fund having always been a European). The nominee is subject to confirmation by the Board of Executive Directors, to serve for a five-year, renewable term. While most World Bank presidents have had banking experience, some have not.\n\nThe vice presidents of the Bank are its principal managers, in charge of regions, sectors, networks and functions. There are two Executive Vice presidents, three Senior Vice presidents, and 24 Vice presidents.\n\nThe Boards of Directors consist of the World Bank Group President and 25 Executive Directors. The President is the presiding officer, and ordinarily has no vote except a deciding vote in case of an equal division. The Executive Directors as individuals cannot exercise any power nor commit or represent the Bank unless specifically authorized by the Boards to do so. With the term beginning 1 November 2010, the number of Executive Directors increased by one, to 25.\n\nThe International Bank for Reconstruction and Development (IBRD) has 189 member countries, while the International Development Association (IDA) has 173 members. Each member state of IBRD should be also a member of the International Monetary Fund (IMF) and only members of IBRD are allowed to join other institutions within the Bank (such as IDA).\n\nIn 2010 voting powers at the World Bank were revised to increase the voice of developing countries, notably China. The countries with most voting power are now the United States (15.85%), Japan (6.84%), China (4.42%), Germany (4.00%), the United Kingdom (3.75%), France (3.75%), India (2.91%), Russia (2.77%), Saudi Arabia (2.77%) and Italy (2.64%). Under the changes, known as 'Voice Reform – Phase 2', countries other than China that saw significant gains included South Korea, Turkey, Mexico, Singapore, Greece, Brazil, India, and Spain. Most developed countries' voting power was reduced, along with a few developing countries such as Nigeria. The voting powers of the United States, Russia and Saudi Arabia were unchanged.\n\nThe changes were brought about with the goal of making voting more universal in regards to standards, rule-based with objective indicators, and transparent among other things. Now, developing countries have an increased voice in the \"Pool Model\", backed especially by Europe. Additionally, voting power is based on economic size in addition to International Development Association contributions.\n\nThe following table shows the subscriptions of the top 20 member countries of the World Bank by voting power in the following World Bank institutions as of December 2014 or March 2015: the International Bank for Reconstruction and Development (IBRD), the International Finance Corporation (IFC), the International Development Association (IDA), and the Multilateral Investment Guarantee Agency (MIGA). Member countries are allocated votes at the time of membership and subsequently for additional subscriptions to capital (one vote for each share of capital stock held by the member).\n\nFor the poorest developing countries in the world, the bank's assistance plans are based on poverty reduction strategies; by combining a cross-section of local groups with an extensive analysis of the country's financial and economic situation the World Bank develops a strategy pertaining uniquely to the country in question. The government then identifies the country's priorities and targets for the reduction of poverty, and the World Bank aligns its aid efforts correspondingly.\n\nForty-five countries pledged US$25.1 billion in \"aid for the world's poorest countries\", aid that goes to the World Bank International Development Association (IDA), which distributes the loans to eighty poorer countries. While wealthier nations sometimes fund their own aid projects, including those for diseases, and although IDA is the recipient of criticism, Robert B. Zoellick, the former president of the World Bank, said when the loans were announced on 15 December 2007, that IDA money \"is the core funding that the poorest developing countries rely on\".\n\nWorld Bank organizes Development Marketplace Awards, a competitive grant program that surfaces and funds innovative, development projects with high potential for development impact that are scalable and/or replicable. The grant beneficiaries are social enterprises with projects that aim to deliver a range of social and public services to the most underserved low-income groups.\n\nThe World Bank has been assigned temporary management responsibility of the Clean Technology Fund (CTF), focused on making renewable energy cost-competitive with coal-fired power as quickly as possible, but this may not continue after UN's Copenhagen climate change conference in December 2009, because of the Bank's continued investment in coal-fired power plants.\n\nTogether with the World Health Organization, the World Bank administers the International Health Partnership (IHP+). IHP+ is a group of partners committed to improving the health of citizens in developing countries. Partners work together to put international principles for aid effectiveness and development cooperation into practice in the health sector. IHP+ mobilizes national governments, development agencies, civil society and others to support a single, country-led national health strategy in a well-coordinated way.\n\nWorld Bank President Jim Yong Kim said in 2012 that: \n\nThe World Bank doubled its aid for climate change adaptation from $2.3bn (£1.47bn) in 2011 to $4.6bn in 2012. The planet is now 0.8 °C warmer than in pre-industrial times. It says that 2 °C warming will be reached in 20 to 30 years.\n\n\nThe World Bank Institute (WBI) creates learning opportunities for countries, World Bank staff and clients, and people committed to poverty reduction and sustainable development. WBI's work program includes training, policy consultations, and the creation and support of knowledge networks related to international economic and social development.\n\nThe World Bank Institute (WBI) is a \"global connector of knowledge, learning and innovation for poverty reduction\". It aims to inspire change agents and prepare them with essential tools that can help achieve development results. \nWBI has four major strategies to approach development problems: innovation for development, knowledge exchange, leadership and coalition building, and structured learning. World Bank Institute(WBI) was formerly known as Economic Development Institute (EDI), established on 11 March 1955 with the support of the Rockefeller and Ford Foundations. The purpose of the institute was to serve as provide an open place where senior officials from developing countries could discuss development policies and programs. Over the years, EDI grew significantly and in 2000, the Institute was renamed as the World Bank Institute. Currently Sanjay Pradhan is the Vice President of the World Bank Institute.\n\nThe Global Development Learning Network (GDLN) is a partnership of over 120 learning centers (GDLN Affiliates) in nearly 80 countries around the world. GDLN Affiliates collaborate in holding events that connect people across countries and regions for learning and dialogue on development issues.\n\nGDLN clients are typically NGOs, government, private sector and development agencies who find that they work better together on subregional, regional or global development issues using the facilities and tools offered by GDLN Affiliates. Clients also benefit from the ability of Affiliates to help them choose and apply these tools effectively, and to tap development practitioners and experts worldwide. GDLN Affiliates facilitate around 1000 videoconference-based activities a year on behalf of their clients, reaching some 90,000 people worldwide. Most of these activities bring together participants in two or more countries over a series of sessions. A majority of GDLN activities are organized by small government agencies and NGOs.\n\nThe GDLN in the East Asia and Pacific region has experienced rapid growth and Distance Learning Centers now operate, or are planned in 20 countries: Australia, Mongolia, Cambodia, China, Indonesia, Singapore, Philippines, Sri Lanka, Japan, Papua New Guinea, South Korea, Thailand, Laos, Timor Leste, Fiji, Afghanistan, Bangladesh, India, Nepal and New Zealand. With over 180 Distance Learning Centers, it is the largest development learning network in the Asia and Pacific region. The Secretariat Office of GDLN Asia Pacific is located in the Center of Academic Resources of Chulalongkorn University, Bangkok, Thailand.\n\nGDLN Asia Pacific was launched at the GDLN's East Asia and Pacific regional meeting held in Bangkok from 22 to 24 May 2006. Its vision is to become \"the premier network exchanging ideas, experience and know-how across the Asia Pacific Region\". GDLN Asia Pacific is a separate entity to The World Bank. It has endorsed its own Charter and Business Plan and, in accordance with the Charter, a GDLN Asia Pacific Governing Committee has been appointed.\n\nThe committee comprises China (2), Australia (1), Thailand (1), The World Bank (1) and finally, a nominee of the Government of Japan (1). The organization is currently hosted by Chulalongkorn University in Bangkok, Thailand, founding member of the GDLN Asia Pacific.\n\nThe Governing Committee has determined that the most appropriate legal status for the GDLN AP in Thailand is a \"Foundation\". The World Bank is currently engaging a solicitor in Thailand to process all documentation in order to obtain this legal status.\n\nGDLN Asia Pacific is built on the principle of shared resources among partners engaged in a common task, and this is visible in the organizational structures that exist, as the network evolves. Physical space for its headquarters is provided by the host of the GDLN Centre in Thailand – Chulalongkorn University; Technical expertise and some infrastructure is provided by the Tokyo Development Learning Centre (TDLC); Fiduciary services are provided by Australian National University (ANU) Until the GDLN Asia Pacific is established as a legal entity tin Thailand, ANU, has offered to assist the governing committee, by providing a means of managing the inflow and outflow of funds and of reporting on them. This admittedly results in some complexity in contracting arrangements, which need to be worked out on a case by case basis and depends to some extent on the legal requirements of the countries involved.\n\nA Justice Sector Peer-Assisted Learning (JUSTPAL) Network was launched in April 2011 by the Poverty Reduction and Economic Management (PREM) Department of the World Bank's Europe and Central Asia (ECA) Region. The JUSTPAL objective is to provide an online and offline platform for justice professionals to exchange knowledge, good practices and peer-driven improvements to justice systems and thereby support countries to improve their justice sector performance, quality of justice and service delivery to citizens and businesses.\n\nThe JUSTPAL Network includes representatives of judiciaries, ministries of justice, prosecutors, anti-corruption agencies and other justice-related entities from across the globe. The Network currently has active members from more than 50 countries.\n\nTo facilitate fruitful exchange of reform experiences and sharing of applicable good practices, the JUSTPAL Network has organized its activities under (currently) five Communities of Practice (COPs): (i) Budgeting for the Justice Sector; (ii) Information Systems for Justice Services; (iii) Justice Sector Physical Infrastructure; (iv) Court Management and Administration; and (v) Prosecution and Anti-Corruption Agencies.\n\nAs a guideline to the World Bank's operations in any particular country, a Country Assistance Strategy is produced, in cooperation with the local government and any interested stakeholders and may rely on analytical work performed by the Bank or other parties.\n\nClean Air Initiative (CAI) is a World Bank initiative to advance innovative ways to improve air quality in cities through partnerships in selected regions of the world by sharing knowledge and experiences. It includes electric vehicles. Initiatives like this help address and tackle pollution-related diseases.\n\nBased on an agreement between the United Nations and the World Bank in 1981, \"Development Business\" became the official source for World Bank Procurement Notices, Contract Awards, and Project Approvals.\n\nIn 1998, the agreement was re-negotiated, and included in this agreement was a joint venture to create an electronic version of the publication via the World Wide Web. Today, \"Development Business\" is the primary publication for all major multilateral development banks, United Nations agencies, and several national governments, many of whom have made the publication of their tenders and contracts in \"Development Business\" a mandatory requirement.\n\nThe World Bank or the World Bank Group is also a sitting observer in the United Nations Development Group.\n\nThe World Bank collects and processes large amounts of data and generates them on the basis of economic models. These data and models have gradually been made available to the public in a way that encourages reuse, whereas the recent publications describing them are available as open access under a Creative Commons Attribution License, for which the bank received the SPARC Innovator 2012 award.\n\nThe World Bank also endorses the Principles for Digital Development.\n\nThe following table lists the top 15 DAC 5 Digit Sectors to which the World Bank has committed funding, as recorded by it in its International Aid Transparency Initiative (IATI) publications. The World Bank states on the IATI Registry website that the amounts \"will cover 100% of IBRD and IDA development flows\" but will not cover other development flows.\n\nThe World Bank hosts the Open Knowledge Repository (OKR) as an official open access repository for its research outputs and knowledge products.\nThe World Bank's repository is listed in the Registry of Research Data Repositories re3data.org.\n\nThe World Bank has long been criticized by non-governmental organizations, such as the indigenous rights group Survival International, and academics, including its former Chief Economist Joseph Stiglitz, Henry Hazlitt and Ludwig Von Mises. Henry Hazlitt argued that the World Bank along with the monetary system it was designed within would promote world inflation and \"a world in which international trade is State-dominated\" when they were being advocated. Stiglitz argued that the so-called free market reform policies that the Bank advocates are often harmful to economic development if implemented badly, too quickly (\"shock therapy\"), in the wrong sequence or in weak, uncompetitive economies. Similarly, Carmine Guerriero notices that these reforms have introduced in developing countries regulatory institutions typical of the common law legal tradition because allegedly more efficient according to the legal origins theory. The latter however has been fiercely criticized since it does not take into account that the legal institutions transplanted during the European colonization have been then reformed. This issue makes the legal origins theory's inference unreliable and the World Bank reforms detrimental.\n\nOne of the strongest criticisms of the World Bank has been the way in which it is governed. While the World Bank represents 188 countries, it is run by a small number of economically powerful countries. These countries (which also provide most of the institution's funding) choose the leadership and senior management of the World Bank, and their interests dominate the bank. Titus Alexander argues that the unequal voting power of western countries and the World Bank's role in developing countries makes it similar to the South African Development Bank under apartheid, and therefore a pillar of global apartheid.\n\nIn the 1990s, the World Bank and the IMF forged the Washington Consensus, policies that included deregulation and liberalization of markets, privatization and the downscaling of government. Though the Washington Consensus was conceived as a policy that would best promote development, it was criticized for ignoring equity, employment and how reforms like privatization were carried out. Joseph Stiglitz argued that the Washington Consensus placed too much emphasis on the growth of GDP, and not enough on the permanence of growth or on whether growth contributed to better living standards.\n\nThe United States Senate Committee on Foreign Relations report criticized the World Bank and other international financial institutions for focusing too much \"on issuing loans rather than on achieving concrete development results within a finite period of time\" and called on the institution to \"strengthen anti-corruption efforts\".\n\nJames Ferguson has argued that the main effect of many development projects carried out by the World Bank and similar organizations is not the alleviation of poverty. Instead the projects often serve to expand the exercise of bureaucratic state power. Through his case-studies of development projects in Thaba-Tseka he shows that the World Bank's characterization of the economic conditions in Lesotho was flawed, and the Bank ignored the political and cultural character of the state in crafting their projects. As a result, the projects failed to help the poor, but succeeded in expanding the government bureaucracy.\n\nCriticism of the World Bank and other organizations often takes the form of protesting as seen in recent events such as the World Bank Oslo 2002 Protests, the October Rebellion, and the Battle of Seattle. Such demonstrations have occurred all over the world, even among the Brazilian Kayapo people.\n\nAnother source of criticism has been the tradition of having an American head the bank, implemented because the United States provides the majority of World Bank funding. \"When economists from the World Bank visit poor countries to dispense cash and advice\", observed \"The Economist\" in 2012, \"they routinely tell governments to reject cronyism and fill each important job with the best candidate available. It is good advice. The World Bank should take it.\" Jim Yong Kim, a Korean-American, is the most recently appointed president of the World Bank.\n\nThe effect of structural adjustment policies on poor countries has been one of the most significant criticisms of the World Bank. The 1979 energy crisis plunged many countries into economic crisis. The World Bank responded with structural adjustment loans, which distributed aid to struggling countries while enforcing policy changes in order to reduce inflation and fiscal imbalance. Some of these policies included encouraging production, investment and labour-intensive manufacturing, changing real exchange rates and altering the distribution of government resources. Structural adjustment policies were most effective in countries with an institutional framework that allowed these policies to be implemented easily. For some countries, particularly in Sub-Saharan Africa, economic growth regressed and inflation worsened. The alleviation of poverty was not a goal of structural adjustment loans, and the circumstances of the poor often worsened, due to a reduction in social spending and an increase in the price of food, as subsidies were lifted.\n\nBy the late 1980s, international organizations began to admit that structural adjustment policies were worsening life for the world's poor. The World Bank changed structural adjustment loans, allowing for social spending to be maintained, and encouraging a slower change to policies such as transfer of subsidies and price rises. In 1999, the World Bank and the IMF introduced the Poverty Reduction Strategy Paper approach to replace structural adjustment loans. The Poverty Reduction Strategy Paper approach has been interpreted as an extension of structural adjustment policies as it continues to reinforce and legitimize global inequities. Neither approach has addressed the inherent flaws within the global economy that contribute to economic and social inequities within developing countries.\n\nSome critics, most prominently the author Naomi Klein, are of the opinion that the World Bank Group's loans and aid have unfair conditions attached to them that reflect the interests, financial power and political doctrines (notably the Washington Consensus) of the Bank and, by extension, the countries that are most influential within it. Among other allegations, Klein says the Group's credibility was damaged \"when it forced school fees on students in Ghana in exchange for a loan; when it demanded that Tanzania privatise its water system; when it made telecom privatisation a condition of aid for Hurricane Mitch; when it demanded labour 'flexibility' in Sri Lanka in the aftermath of the Asian tsunami; when it pushed for eliminating food subsidies in post-invasion Iraq\".\n\nThe World Bank requires sovereign immunity from countries it deals with. Sovereign immunity waives a holder from all legal liability for their actions. It is proposed that this immunity from responsibility is a \"shield which The World Bank wants to resort to, for escaping accountability and security by the people.\" As the United States has veto power, it can prevent the World Bank from taking action against its interests.\n\nWorld Bank favored PricewaterhouseCoopers as a consultant in a bid for privatizing the water distribution in Delhi, India\n\n\n\n"}
{"id": "12978936", "url": "https://en.wikipedia.org/wiki?curid=12978936", "title": "Zinsco", "text": "Zinsco\n\nZinsco was a manufacturer of electrical distribution panels and circuit breakers, founded by Emile Martin Zinsmeyer and his son Martin Emile Zinsmeyer in the early 1930s.\n\nFrank Adam Electric, a manufacturer of electrical panels and circuit breakers, was founded in 1891 as part of the Blacker and Adam Watch Company in St. Louis, Missouri. In 1928, a young sales manager named Emile Zinsmeyer took charge of Frank Adam Electric's western division in Los Angeles. In the latter part of the Great Depression, Emile negotiated to buy Frank Adam's west coast stock, forming Zinsmeyer Company. Frank Adam Electric would continue manufacturing panels and breakers through the 1950s, and they are now extremely rare.\n\nLocated at 729 Turner Street in Los Angeles, the Zinsmeyer Company would soon undergo some changes. Zinsmeyer employed two of his sons, Wilford, who went by Bill, and Martin. In 1943, Martin bought the company from his father and renamed it Zinsco Electric. Almost immediately, Zinsco began development of new panel and circuit breaker designs, with patent applications dating back to 1946. The first Zinsco panels contained copper bus-bars and copper breaker clips. Original breakers were patented in 1950 and labeled “Magnatrip.” Five additional patents would be issued for subsequent designs. These early breakers employed copper bus-clips, which matched the copper bus-bars in these panels.\n\nPrior to World War II, the only permissible electrical conductors were copper, gold, and silver. A copper shortage during the war promoted code changes. Although aluminum was initially expensive to manufacture, advancements in the processing and production of aluminum wire made it available by 1942 and it was added to the National Electrical Code. In 1952, the Korean War would cause another major copper shortage in the U.S. Although some companies, such as FPE and Square-D, continued to use an all-copper design, other manufacturers switched to an aluminum alloy for bus bars and breakers. Zinsco would remain with copper in both their panels and breakers until the third major copper shortage in the early 1960s, when they would switch to an aluminum bus. In 1963, Zinsco introduced the R-38 twin breaker, which was the only twin circuit breaker that also made contact on both bus-bars for 240 volts in a single breaker space. This made the Zinsco brand hugely popular with contractors and millions of Zinsco service panels and load centers were installed through the 60s and 70s.\n\nIn 1973, Martin Zinsmeyer sold the Zinsco Electric Company to GTE-Sylvania. During that ownership, the product line remained the same, with new labeling and branding, while dropping the “Magnatrip” label. In 1978, the line changed names again, and was re-labeled Challenger.\n\nBy 1981, GTE-Sylvania divested itself of the electrical distribution business and sold off its product lines and manufacturing facilities. The Challenger line, mostly manufactured at the time in Jackson, Mississippi, was sold to a former officer of GTE, who used the Challenger name as the name of his new company, Challenger Electrical Equipment Corporation.\n\nThe new Challenger Brand immediately ceased production of any new Zinsco frame panels and load-centers, but still produced the re-branded Zinsco circuit breaker line. Challenger concentrated on a new line of panel equipment using the same bus configuration proven with Murray, Crouse-Hinds, and others. But, from 1982 until 1994, the 200-amp service panels used a Zinsco frame main breaker. Challenger flourished through the 80s, and was eventually received by Westinghouse in a multi-asset deal, in order for Westinghouse to sell its remaining electrical manufacturing facilities to Eaton Corporation in 1994. Both Challenger and Thomas & Betts (T&B) had manufacturing facilities in Mississippi, where T&B received the Zinsco circuit breaker molds. T&B then continued to fabricate and sell the breakers under their own label, tripling the price at that time, from about $7.00 for the popular R-38 twin breaker to $19.00. T&B would eventually close the Zinsco circuit breaker production altogether in 2005. Today, Connecticut Electric continues making Zinsco replacement aluminum bus kits and breakers.\n\nThe copper shortage of the 1960s was primarily caused by the US government, which had huge contracts out, many of which were for air-conditioning, requiring over a million new copper coils. This action depressed the market of available copper for other manufacturing needs and promoted the first real demand for aluminum NM cable. When Zinsco changed to an aluminum bus, the aluminum selected was Alloy 6061. First developed in 1935, Alloy 6061 is a good conductor, a hard aluminum, and still a popular alloy for welding today. But in 1961, another aluminum alloy was developed: 6101. This alloy has a higher tensile strength and is the best conductor for electrical use. It is the aluminum used in the current aluminum busing of panels and load-centers.\n\nBut Alloy 6061 continued to be used, both in wiring and in electrical panels. From 1964 to 1972, 6061 aluminum wire and quick-wire receptacles were being installed in over two million homes, particularly tract homes. This wire was inferior, and the aluminum had tremendous expansion and contraction between power consumption and rest. Since power outlets were also constructed of dissimilar metals, the receptacles caused the aluminum to oxidize. Oxidization of aluminum creates aluminum oxide, which insulates rather than conducts. This caused numerous fires, outlet burn-outs, and nearly 100 deaths as published by the NFPA. Because of this, aluminum wiring received a very bad reputation.\n\nIn 1972 aluminum was changed from an inferior product and into a modified allow. Modern aluminum wire is an AA800 series alloy, which has a higher tensile strength, so it undergoes less expansion and contraction. This change was also followed in the late 90s by COLAR-rated receptacles as a requirement in aluminum device wiring. Since that time, aluminum for anything smaller than 8-gauge wiring has virtually disappeared.\n\nCurrently, Zinsco electrical equipment is considered obsolete due to a design flaw in which the circuit breaker's connection to the bus bar becomes loose, causing arcing and subsequent overheating. Long-term exposure to this heat can cause the breaker to fuse to the bus bar, making it impossible to remove. Even worse, it can cause the breaker's contacts to fuse together, thus preventing the breaker from tripping even in an overcurrent situation, thereby causing a potential fire hazard.\n\nAftermarket replacements for the Zinsco breakers are available; however, it may be more cost effective simply to replace the entire panel with a more modern and safer design from another manufacturer (such as Eaton, GE, Siemens, or Square-D), depending on the number of breakers to be replaced. If the bus bar shows signs of corrosion, or if any of the breakers show signs of overheating, the panel should be replaced entirely. Many electricians advocate replacement of the panel in any case, due to its historically poor reliability.\n\nManufactures of Zinsco Style Breakers:\n\n"}
