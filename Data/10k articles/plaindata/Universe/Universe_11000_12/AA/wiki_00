{"id": "17760201", "url": "https://en.wikipedia.org/wiki?curid=17760201", "title": "(Z)-Stilbene", "text": "(Z)-Stilbene\n\n(\"Z\")-Stilbene is a diarylethene, that is, a hydrocarbon consisting of a cis ethene double bond substituted with a phenyl group on both carbon atoms of the double bond. The name stilbene was derived from the Greek word , which means shining.\n\nStilbene exists as two possible isomers known as (\"E\")-stilbene and (\"Z\")-stilbene. (\"Z\")-Stilbene is sterically hindered and less stable because the steric interactions force the aromatic rings out-of-plane and prevent conjugation. (\"Z\")-Stilbene has a melting point of , while (\"E\")-stilbene melts around , illustrating that the two compounds are quite different.\n\n\n\nMany stilbene derivatives (stilbenoids) are present naturally in plants. An example is resveratrol and its cousin, pterostilbene. \n"}
{"id": "1207128", "url": "https://en.wikipedia.org/wiki?curid=1207128", "title": "1991 West Virginia derecho", "text": "1991 West Virginia derecho\n\nThe 1991 West Virginia derecho was a serial derecho (storm) that started in Arkansas in the early morning hours of April 9, 1991, and made its way northeast, finally dying out over Pennsylvania late that evening.\n\nTwo people were killed and 145 were injured in the event, mainly from falling trees, flying debris, and mobile homes and trailers being overturned. Western and central West Virginia were affected by hail and several roads were blocked. Most of the destructive damage occurred in Tennessee, Kentucky, West Virginia, western Maryland, and Western Pennsylvania. The fatalities associated with this storm occurred in Charleston, West Virginia and Huntington, West Virginia as a result of the high winds.\n\nIn West Virginia alone, there were 8,000 insurance damage claims for homes and businesses. Over 200,000 people lost power in the derecho. Many people experienced flickering lights and power surges. This derecho was the worst severe weather event for West Virginia since the 1974 Super Outbreak. \n\nThere are several criteria for a mesoscale convective system (MCS) to be considered a derecho. The system needs to have sustained winds of 58 mph, and stretch along a boundary at least 250 miles long. There must also be isolated wind gusts of 75 mph or greater. \n\nThis is a rather simplified list of conditions needed to form a derecho. However, when all of these conditions occur simultaneously in the atmosphere, a derecho storm system has the \"potential\" to form. \n\n\n"}
{"id": "23692600", "url": "https://en.wikipedia.org/wiki?curid=23692600", "title": "Act of God (film)", "text": "Act of God (film)\n\nAct of God is a 2009 Canadian documentary film that investigates the \"metaphysical\" effects of being struck by lightning. It was directed by Jennifer Baichwal (\"Manufactured Landscapes\") and distributed by Zeitgeist Films. The film's world premier was at the 2009 Hot Docs Canadian International Documentary Festival at the Royal Ontario Museum in Toronto on 30 April 2009. It went on general release in Canada on 1 May 2009, and limited release in the United States on 31 July 2009. The film's European premiere was at the 44th Karlovy Vary International Film Festival in the Czech Republic on 11 July 2009.\n\nIn \"Act of God\" director Jennifer Baichwal questions whether being struck by lightning is a \"random natural occurrence or a predestined event\". The film contains seven stories in which Baichwal interviews people about their personal experiences with lightning strikes. She speaks to American novelist and screenwriter Paul Auster, Canadian dramatist James O'Reilly, and US Marine veteran and author Dannion Brinkley. She also interviews a storm chaser in France, and a group of Mexican mothers who accept the loss of their children to lightning at a religious festival as \"God's will\". She also investigates a Yoruba religious community in Rwanda (the lightning capital of the world) who worship the lightning god Shango. The reactions in each of Baichwal's subjects varies considerably, from an \"act of God\" to the \"mechanics of reality\".\n\nAuster, the \"philosophical anchor of the film\", relates how he saw his friend being struck by lightning a short distance from him at a summer camp. Auster, 14 years old at the time, survived the incident while his friend died. Auster said \"It opened up a whole realm of speculation that I've continued to live with ever since.\" Yet in spite of the deep effect this event had on him, Auster insists in the film that it was \"nothing more than a random occurrence\".\n\nO'Reilly wrote a play called \"Act of God\" which was based on his experience with lightning. In the film O'Reilly says \"I can't accept that it happened for a reason, nor can I really accept that there is no reason. The only way to carry on is to be humble, and a little bit in awe of these things you can't really understand.\" Brinkley was also struck by lightning and described it as \"dying for 28 minutes and going up to heaven and having a completely life-transforming experience.\"\n\nAlso present in the film is English experimental and improvisational guitarist Fred Frith. Frith loosely ties up the stories by demonstrating that \"we are electrical beings, our brains work electrically\". In the laboratory of his brother, neuroscientist Chris Frith, Frith improvises music on his guitar while electrical impulses in his head are recorded with a brain scan, showing that \"our very thoughts are akin to tiny lightning strikes in the cerebral cortex.\" Baichwal described improvisation as \"the state of being between meaning and chance\" and \"it was the perfect metaphor for being struck by lightning\".\n\nFrith provides the music for his segment of the film, while the score for the rest of the film comes from musicians Martin Tielli, Dave Bidini and Selina Martin.\n\nJennifer Baichwal was born in Montreal where she studied for her master's degree in philosophy and theology at McGill University. Her debut feature-length film was \"Let It Come Down: The Life of Paul Bowles\" in 1998, which won her Best Biography at the 1999 Hot Docs Canadian International Documentary Festival, and the 1999 International Emmy for Best Arts Documentary. Baichwal is best known for her 2006 multiple award-winning documentary, \"Manufactured Landscapes\", which was about the Canadian artist Edward Burtynsky.\n\nMany of Baichwal's films are about artists and the creative process, but she has also explored philosophical and spiritual themes, for example in her award-winning documentary \"The Holier It Gets\", which records her journey to the source of the Ganges River in India. \"Act of God\" looks at both the artistic and \"metaphysical\" side of life.\n\nInstead of using the \"traditional scientific approach\", Baichwal tackled the issue lightning strikes from a \"philosophical point of view\". She said \"Our main challenge was figuring out how to make a film about something that's totally ephemeral. I love unanswerable questions; questions like 'Is there such a thing as destiny', and 'what does it all mean?'\" While research is done into the role of electricity in our brains, Baichwal wanted to find out the effect of electricity on the mind. She said that \"Lightning is always cast in a scientific light [in film], unless it's used as a joke. We wanted stories that embodied that tension between meaning and chance.\"\n\nThe idea of making \"Act of God\" came to Baichwal before she started working on her previous film, \"Manufactured Landscapes\". It was during that film's development and her travels around the world to attend its screenings at film festivals that she started doing research and conducting interviews for \"Act of God\". But the final inspiration to begin work on the film came from her partner Nicholas de Pencier, \"a weather nerd\", and the writings of lightning survivor, Paul Auster.\n\n\"Act of God\" took three years to make, which included several years of research and collecting stories from around the world of people \"whose lives [were] changed by lightning\". De Pencier was the film's cinematographer and co-producer.\n\n\"Act of God\" was selected as the Opening Night Gala feature at the 2009 Hot Docs Canadian International Documentary Festival, the largest documentary film festival in North America.\n\nChris Jancelewicz of \"AOL Canada Entertainment\" said the footage taken by Jennifer Baichwal's partner, Nick de Pencier is \"nothing short of remarkable\". \"One of the amazing things about \"Act of God\" is Baichwal's ability to resist this urge to dictate what lightning, and ultimately chance, fate, and destiny mean in the bigger picture.\" Jancelewicz said that throughout the film Baichwal never imposes her views, and never comes to any conclusion, simply because \"there is no conclusion to be found\".\n\nSusan Noakes of \"CBC News\" described the film as an \"enigmatic meditation on being struck by lightning\". Jessica Werb at \"The Georgia Straight\" said the film was \"one hell of a thesis\". She added that while \"most of us live [...] somewhere 'in the continuum of meaning and randomness'\", the lives of those appearing in the film, \"in one terrifying instant, swung to one extreme or the other\".\n\nNorman Wilner of NOW Magazine gave the film 4 'N's out of 5, and Nathan Southern at Allmovie gave the film 2.5 stars out of 5.\n\n"}
{"id": "649724", "url": "https://en.wikipedia.org/wiki?curid=649724", "title": "Affine plane (incidence geometry)", "text": "Affine plane (incidence geometry)\n\nIn geometry, an affine plane is a system of points and lines that satisfy the following axioms:\n\nIn an affine plane, two lines are called \"parallel\" if they are equal or disjoint. Using this definition, Playfair's axiom above can be replaced by:\n\nParallelism is an equivalence relation on the lines of an affine plane.\n\nSince no concepts other than those involving the relationship between points and lines are involved in the axioms, an affine plane is an object of study belonging to incidence geometry. They are non-degenerate linear spaces satisfying Playfair's axiom.\n\nThe familiar Euclidean plane is an affine plane. There are many finite and infinite affine planes. As well as affine planes over fields (and division rings), there are also many non-Desarguesian planes, not derived from coordinates in a division ring, satisfying these axioms. The Moulton plane is an example of one of these.\n\nIf the number of points in an affine plane is finite, then if one line of the plane contains points then:\nThe number is called the \"order\" of the affine plane.\n\nAll known finite affine planes have orders that are prime or prime power integers. The smallest affine plane (of order 2) is obtained by removing a line and the three points on that line from the Fano plane. A similar construction, starting from the projective plane of order three, produces the affine plane of order three sometimes called the Hesse configuration. An affine plane of order exists if and only if a projective plane of order exists (however, the definition of order in these two cases is not the same). Thus, there is no affine plane of order 6 or order 10 since there are no projective planes of those orders. The Bruck–Ryser–Chowla theorem provides further limitations on the order of a projective plane, and thus, the order of an affine plane.\n\nThe lines of an affine plane of order fall into equivalence classes of lines apiece under the equivalence relation of parallelism. These classes are called \"parallel classes\" of lines. The lines in any parallel class form a partition the points of the affine plane. Each of the lines that pass through a single point lies in a different parallel class.\n\nThe parallel class structure of an affine plane of order may be used to construct a set of mutually orthogonal latin squares. Only the incidence relations are needed for this construction.\n\nAn affine plane can be obtained from any projective plane by removing a line and all the points on it, and conversely any affine plane can be used to construct a projective plane by adding a line at infinity, each of whose points is that point at infinity where an equivalence class of parallel lines meets.\n\nIf the projective plane is non-Desarguesian, the removal of different lines could result in non-isomorphic affine planes. For instance, there are exactly four projective planes of order nine, and seven affine planes of order nine. There is only one affine plane corresponding to the Desarguesian plane of order nine since the collineation group of that projective plane acts transitively on the lines of the plane. Each of the three non-Desarguesian planes of order nine have collineation groups having two orbits on the lines, producing two non-isomorphic affine planes of order nine, depending on which orbit the line to be removed is selected from.\n\nA line in a projective plane is a translation line if the group of elations with axis acts transitively on the points of the affine plane obtained by removing from the plane . A projective plane with a translation line is called a translation plane and the affine plane obtained by removing the translation line is called an affine translation plane. While in general it is often easier to work with projective planes, in this context the affine planes are preferred and several authors simply use the term translation plane to mean affine translation plane.\n\nAn alternate view of affine translation planes can be obtained as follows: Let be a -dimensional vector space over a field . A spread of is a set of -dimensional subspaces of that partition the non-zero vectors of . The members of are called the components of the spread and if and are distinct components then . Let be the incidence structure whose points are the vectors of and whose lines are the cosets of components, that is, sets of the form where is a vector of and is a component of the spread . Then:\n\nAn incidence structure more general than a finite affine plane is a -\"net of order\" . This consists of points and lines such that:\n\nAn -net of order is precisely an affine plane of order .\n\nA -\"net of order\" is equivalent to a set of mutually orthogonal Latin squares of order .\n\nFor an arbitrary field , let be a set of -dimensional subspaces of the vector space , any two of which intersect only in {0} (called a partial spread). The members of , and their cosets in , form the lines of a translation net on the points of . If this is a -net of order . Starting with an affine translation plane, any subset of the parallel classes will form a translation net.\n\nGiven a translation net, it is not always possible to add parallel classes to the net to form an affine plane. However, if is an infinite field, any partial spread with fewer than members can be extended and the translation net can be completed to an affine translation plane.\n\nGiven the \"line/point\" incidence matrix of any finite incidence structure, , and any field, the row space of over is a linear code that we can denote by . Another related code that contains information about the incidence structure is the Hull of which is defined as:\nwhere is the orthogonal code to .\n\nNot much can be said about these codes at this level of generality, but if the incidence structure has some \"regularity\" the codes produced this way can be analyzed and information about the codes and the incidence structures can be gleaned from each other. When the incidence structure is a finite affine plane, the codes belong to a class of codes known as \"geometric codes\". How much information the code carries about the affine plane depends in part on the choice of field. If the characteristic of the field does not divide the order of the plane, the code generated is the full space and does not carry any information. On the other hand, \n\nFurthermore, \n\nWhen the geometric code generated is the -ary Reed-Muller Code.\n\nAffine spaces can be defined in an analogous manner to the construction of affine planes from projective planes. It is also possible to provide a system of axioms for the higher-dimensional affine spaces which does not refer to the corresponding projective space.\n\n\n"}
{"id": "3581245", "url": "https://en.wikipedia.org/wiki?curid=3581245", "title": "Aposymbiotic", "text": "Aposymbiotic\n\nAposymbiosis occurs when symbiotic organisms live apart from one another (for example, a clownfish living independently of a sea anemone). Studies have shown that the lifecycles of both the host and the symbiont are affected in some way, usually negative, and that for obligate symbiosis the effects can be drastic. Aposymbiosis is distinct from exsymbiosis, which occurs when organisms are recently separated from a symbiotic association. Because symbionts can be vertically transmitted from parent to offspring or horizontally transmitted from the environment, the presence of an aposymbiotic state suggests that transmission of the symbiont is horizontal. A classical example of a symbiotic relationship with an aposymbiotic state is the Hawaiian bobtail squid \"Euprymna scolopes\" and the bioluminescent bacteria \"Vibrio fischeri.\" While the nocturnal squid hunts, the bacteria emit light of similar intensity of the moon which camouflages the squid from predators. Juveniles are colonized within hours of hatching and \"Vibrio\" must outcompete other bacteria in the seawater through a system of recognition and infection. \nAposymbiotic organisms can be used as models to observe a variety of processes. Aposymbiotic \"Euprymna\" juveniles have been studied throughout colonization in order to determine the system of recognizing \"Vibrio fischeri\" in seawater. Coral polyps without their symbiont algae are models for coral calcification and the effects of the algae on coral pH regulation. \n"}
{"id": "45693651", "url": "https://en.wikipedia.org/wiki?curid=45693651", "title": "Application of silicon-germanium thermoelectrics in space exploration", "text": "Application of silicon-germanium thermoelectrics in space exploration\n\nSilicon-germanium (SiGe) thermoelectrics have been used for converting heat into power in spacecraft designed for deep-space NASA missions since 1976. This material is used in the radioisotope thermoelectric generators (RTGs) that power \"Voyager 1\", \"Voyager 2\", \"Galileo\", \"Ulysses\", \"Cassini\", and \"New Horizons\" spacecraft. SiGe thermoelectric material converts enough radiated heat into electrical power to fully meet the power demands of each spacecraft. The properties of the material and the remaining components of the RTG contribute towards the efficiency of this thermoelectric conversion.\n\nHeavily doped semiconductors, such as silicon-germanium (SiGe) thermoelectric couples (also called thermocouples or unicouples), are used in space exploration.\n\nSiGe alloys present good thermoelectric properties. Their performance in thermoelectric power production is characterized by high dimensionless figures-of-merit (ZT) under high temperatures, which has been shown to be near 2 in some nanostructured-SiGe models.\n\nSiGe alloy devices are mechanically rugged and can withstand severe shock and vibration due to its high tensile strength (i.e. >7000 psi) and low dislocation density. SiGe material is malleable with standard metallurgical equipment and bonds easily to construct components. SiGe alloy devices can operate under high temperatures (i.e. >1300 ˚C) without degradation due to their electronic stability, low thermal expansion coefficient and high oxidation resistance.\n\nNear the sun, solar cell performance deteriorates from high incident particle flux and high temperatures from heat flux. However, thermoelectric energy conversion systems that use thermoelectric materials (e.g. SiGe alloys) as a supplemental source of power for missions near the sun can operate unprotected in vacuum and air environments under high temperatures due to their low sensitivity to radiation damage. Such properties have made SiGe thermoelectrics convenient for power generation in space.\nThe multifoil cold stack assembly, composed of molybdenum, tungsten, stainless steel, copper, and alumina materials, provides the insulation between the electrical and thermal currents of the system. The SiGe n-leg doped with boron and SiGe p-leg doped with phosphorus act as the intermediary between the heat source and electrical assembly.\n\nSiGe thermocouples in an RTG convert heat directly into electricity. Thermoelectric power generation requires a constantly maintained temperature difference among the junctions of the two dissimilar metals (i.e. Si and Ge) to produce a low power closed circuit electric current without extra circuitry or external power sources.\n\nA large array of SiGe thermocouples/unicouples form a thermopile that was incorporated into the design of radioisotope thermoelectric generators (RTGs) used in the missions \"Voyager\", \"Galileo\", \"Ulysses\", \"Cassini\", and \"New Horizons\". On these spacecraft, Pu-238 dioxide fuel undergoes natural decay. The SiGe thermocouples/unicouples convert this heat to hundreds of Watts of electrical power.\n\nThe thermocouples/unicouples attached to the outer shell consist of a SiGe alloy n-leg doped with boron and a SiGe p-leg doped with phosphorus to provide thermoelectric polarity to the couple. The electrical and thermal currents of the system are separated by bonding the SiGe alloy thermocouple to a multifoil cold stack assembly of molybdenum, tungsten, stainless steel, copper, and alumina components. Several layers of Astroquartz silica fiber yarn electrically insulate the legs of the SiGe thermocouples. In between the inner insulation system and the outer shell, copper connectors form the electrical circuit, which uses a two-string, series-parallel wiring design to connect the unicouples. The circuit loop arrangement minimizes the net magnetic field of the generator.\n\nSiGe has been used as a material in RTGs since 1976. Each mission that has used RTG technology involves exploration of far-reaching regions of the solar system. The most recent mission, \"New Horizons\" (2005), was originally set for a 3-year exploration, but was extended to 17 years.\n\n\"Voyager 1\" and \"Voyager 2\" spacecraft launched in August and September 1977 required multi-hundred-watt (MHW) RTG containing plutonium oxide fuel spheres for an operational life appropriate for exploration of Jupiter, Saturn, Uranus, and Neptune. Conversion of the decay heat of the plutonium to electrical power was accomplished through 312 silicon-germanium (SiGe) thermoelectric couples. A hot junction temperature of 1273 K (1832 °F) with a cold junction temperature of 573 K (572 °F) compose the temperature gradient in the thermoelectric couple in the RTG. This mechanism provided the total electrical power to operate the spacecraft’s instruments, communications and other power demands. The RTG on \"Voyager\" will produce adequate electrical power for spacecraft operation until about the year 2020. Similar MHW-RTG models are also used on the two U.S. Air Force communications Lincoln Experimental Satellites 8 and 9 (\"LES-8/9\").\n\nThe \"Galileo\" spacecraft launched on October 18, 1989, the \"Ulysses\" on October 6, 1990, the \"Cassini\" on October 15, 1997 and the \"New Horizons\" on January 19, 2006. All of these spacecraft contain the general purpose heat source (GPHS) RTG commissioned by the U.S. Department of Energy. The GPHS-RTG employs identical heat-to-electrical conversion technology used in the MHW-RTGs from the \"Voyager\" missions, using SiGe thermocouples/unicouples and the Pu-238–fueled GPHS. \"New Horizons\" made its historic flyby past Pluto and its moons on July 14, 2015 (see JHU Applied Physics website). The spacecraft's next destination will be a small Kuiper Belt object (KBO) known as 2014 MU69 that orbits nearly a billion miles beyond Pluto. Based on performance, data and modeling for the SiGe alloy RTGs, the GPHS-RTGs on \"Ulysses\", \"Cassini\" and \"New Horizons\" are expected to meet or exceed the remaining power performance requirements for their deep-space missions.\n\nMissions after 2010 requiring RTGs will instead use the Multi-Mission Radioisotope Thermoelectric Generator (MMRTG) containing lead telluride (PbTe) thermocouples and Pu-238 dioxide for spacecraft power applications.\n\n"}
{"id": "16901885", "url": "https://en.wikipedia.org/wiki?curid=16901885", "title": "Bakhtiari Dam", "text": "Bakhtiari Dam\n\nThe Bakhtiari Dam is an arch dam currently under construction on the Bakhtiari River within the Zagros Mountains on the border of Lorestan and Khuzestan Provinces, Iran. At a planned height of , it will be the world's tallest dam once completed and withhold the second largest reservoir in Iran after the Karkheh reservoir. The main purpose of the dam is hydroelectric power production and it will support a 1,500 MW power station. By trapping sediment, the dam is also expected to extend the life of the Dez Dam downstream.\n\nPreliminary studies for the dam began in 1996 and were carried out by Mahab Qods Consulting Engineers. The studies were carried out over a period of 33 months and in March 2000, the results were given to Iran Water & Power Resources Development Co (IWPCO). In May 2005, IWPCO awarded consultation services for the project to Moshanir Consulting Engineers, Dezab Consulting Engineers, Econo-Electrowatt/Boyri and Stucky Pars Consulting Engineers. On April 30, 2007 the construction contract was awarded to China's Sinohydro Corporation. The contract is worth $2 billion and was to be funded with direct investment from China. Sinohydro signed the 118-month contract on March 15, 2011 and was expected to be working with Iran's Farab. But the Iranian government rejected Sinohydro's bid in late May 2012 and handed the project over to Khatam-al-Anbiya (KAA), which is controlled by the Iranian Revolutionary Guard Corps. The KAA commander announced on 19 December 2012 that construction of the dam had begun with access roads leading to the project site. On 25 March 2013, Iranian President Mahmood Ahmadinejad attended a groundbreaking ceremony for the dam, initiating its construction.\n\nDuring construction, a total of six bridges will be built to support workers, vehicles and equipment in addition to various access roads. To divert the river, two tunnels, and in length will be constructed at the dam's left abutment. They will have discharge capacities of and respectively. To divert the water, two roller-compacted concrete cofferdams will be constructed. The upstream cofferdam will be high and the downstream . Material to construct the dam including aggregate will come from the actual excavation of the dam site along with three quarries in the area.\n\nThe Bakhtiari will be a tall and long variable-radius arch dam. It will be wide at its crest and wide at its base while being composed of of concrete. The dam's reservoir will have a normal capacity of and an active or \"useful\" capacity of . At a normal elevation of above sea level, the reservoir will have a surface area of , maximum width of and length of . Its catchment area will be .\n\nThe dam will contain two spillways. The main service spillway will be an diameter tunnel in the right abutment with two flood gates. The discharge capacity of this spillway will be . The second spillway will be two radial gates on the dam's orifice with a discharge capacity of . The dam's powerhouse will be located underground at the left abutment. It will be long, high and wide; containing 6 x 250 MW vertical Francis turbine-generators. Before reaching the power station, water will be transferred by six long penstocks. Feeding water to the penstocks is a long headrace tunnel with a three gate intake structure.\n\n"}
{"id": "21257471", "url": "https://en.wikipedia.org/wiki?curid=21257471", "title": "Bottom eta meson", "text": "Bottom eta meson\n\nThe bottom eta meson () or eta-b meson is a flavourless meson formed from a bottom quark and its antiparticle. It was first observed by the BaBar experiment at SLAC in 2008, and is the lightest particle containing a bottom and anti-bottom quark.\n\n\n"}
{"id": "3701829", "url": "https://en.wikipedia.org/wiki?curid=3701829", "title": "Bushing (electrical)", "text": "Bushing (electrical)\n\nIn electric power, a bushing is an insulated device that allows an electrical conductor to pass safely through a grounded conducting barrier such as the case of a transformer or circuit breaker. Bushings are typically made from porcelain, though other materials are possible.\n\nAll materials carrying an electric charge generate an electric field. When an energized conductor is near a material at earth potential, it can form very high field strengths, especially where the field lines are forced to curve sharply around the earthed material. The bushing controls the shape and strength of the field and reduces the electrical stresses in the insulating material.\n\nA bushing must be designed to withstand the electrical field strength produced in the insulation, when any earthed material is present. As the strength of the electrical field increases, leakage paths may develop within the insulation. If the energy of the leakage path overcomes the dielectric strength of the insulation, it may puncture the insulation and allow the electrical energy to conduct to the nearest earthed material causing burning and arcing.\n\nA typical bushing design has a 'conductor', (usually of copper or aluminium, occasionally of other conductive materials), surrounded by insulation, except for the terminal ends.\n\nIn the case of a busbar, the conductor terminals will support the busbar in its location. In the case of a bushing, a fixing device will also be attached to the insulation to hold it in its location. Usually, the fixing point is integral or surrounds the insulation over part of the insulated surface. The insulated material between the fixing point and the conductor is the most highly stressed area.\n\nThe design of any electrical bushing must ensure that the electrical strength of the insulated material is able to withstand the penetrating 'electrical energy' passing through the conductor, via any highly stressed areas. It must also be capable of enduring, occasional and exceptional high voltage moments as well as the normal continual service withstand voltage, as it is the voltage that directs and controls the development of leakage paths and not current.\n\nInsulated bushings can be installed either indoor, or outdoor, and the selection of insulation will be determined by the location of the installation and the electrical service duty on the bushing.\n\nFor a bushing to work successfully over many years, the insulation must remain effective both in composition and design shape and will be key factors in its survival. Bushings can therefore vary considerably in both material and design style.\n\nThe earliest bushing designs use electro porcelain for both indoor and outdoor applications. Porcelain is impervious to moisture once sealed by fired glaze and is low cost and flexible to manufacture. The main disadvantage with porcelain is that its small value of linear expansion has to be accommodated by using flexible seals and substantial metal fittings, both of which present manufacturing and operational problems.\n\nA basic porcelain bushing is a hollow porcelain shape that fits through a hole in a wall or metal case, allowing a conductor to pass through its center, and connect at both ends to other equipment. Bushings of this type are often made of wet-process fired porcelain, which is then glazed. A semi-conducting glaze may be used to assist in equalizing the electrical potential gradient along the length of the bushing.\n\nThe inside of the porcelain bushing is often filled with oil to provide additional insulation and bushings of this construction are widely used up to 36 KV where higher partial discharges are permitted.\n\nWhere partial discharge is required to conform to IEC60137, paper and resin insulated conductors are used in conjunction with porcelain, for unheated indoor and outdoor applications.\n\nThe use of resin (polymer, polymeric, composite) insulated bushings for high voltage applications is common, although most high-voltage bushings are usually made of resin impregnated paper insulation around the conductor with porcelain or polymer weather sheds, for the outdoor end and occasionally for the indoor end.\n\nAnother early form of insulation was paper, however, paper is hygroscopic and absorbs moisture which is detrimental and is disadvantaged by the inflexible linear designs. Cast resin technology, has dominated insulated products since the 1960s, due to its flexibility of shape and its higher dielectrical strength.\n\nTypically, paper insulation is later impregnated either with oil (historically), or more commonly today with resin. In the case of resin, the paper is film coated with a Phenolic resin to become Synthetic Resin Bonded Paper, (SRBP) or impregnated after dry winding with epoxy resins, to become Resin Impregnated Paper or Epoxy Resin Impregnated Paper (RIP, ERIP).\n\nSRBP insulated bushings are typically used up to voltages around 72.5 kV. However, above 12 kV, there is a need to control the external electrical field and to even out the internal energy storage which marginalises the dielectric strength of paper insulation.\n\nTo improve the performance of paper insulated bushings, metallic foils can be inserted during the winding process. These act to stabilize the generated electrical fields, homogenising the internal energy using the effect of capacitance. This feature resulted in the Condenser/Capacitor bushing.\n\nThe condenser bushing is made by inserting very fine layers of metallic foil into the paper during the winding process. The inserted conductive foils produce a capacitive effect which dissipates the electrical energy more evenly throughout the insulated paper and reduces the electric field stress between the energised conductor and any earthed material.\n\nCondenser bushings produce electric stress fields which are significantly less potent around the fixing flange than designs without foils and, when used in conjunction with resin impregnation, produce bushings which can be used at service voltages over one million with great success.\n\nSince the 1965s, resin materials have been used for all types of bushing up to the highest voltages. The flexibility of using a castable form of insulation has replaced paper insulation in many product areas and dominates the existing insulated bushing market.\n\nAs with paper insulation, the control of electric stress fields remains important. Resin insulation has greater dielectric strength than paper and requires less stress control at voltages below 25 kV. However, some compact and higher rated switchgear designs, have earthed materials closer to bushings than in the past and these designs may require stress control screens in resin bushings operating as low as 12 kV Fixing points are often integral with the main resin form, and present fewer problems to earthed materials than the metal flanges used on paper bushings. However, care must be observed in resin insulated bushings designs which use internally cast screens such that the benefit of electrical stress field control is not off set by increasing partial discharge caused by the difficulties of eliminating micro voids in the resin around the screens during the casting process. The need to eliminate voids in resin becomes more sensitive as voltages increases, and it is normal to revert to resin impregnated, foiled paper insulation for bushings rated over 72.5 kV.\n\nBushings sometimes fail due to partial discharge. This is sometimes due to the slow and progressive degradation of the insulation over many years of energized service; however it may also be a rapid degeneration which destroys a good bushing in a matter of hours. At present, there is great interest by the electricity supply industry in monitoring the condition of high voltage bushings. However, some bushings failing early in service are due to failures to control voltage or carry out essential maintenance, while others relate to incipient failure mechanisms built in at manufacture. This view is evidenced by the minority of bushing failures worldwide.\n\n"}
{"id": "740885", "url": "https://en.wikipedia.org/wiki?curid=740885", "title": "Calcium fluoride", "text": "Calcium fluoride\n\nCalcium fluoride is the inorganic compound of the elements calcium and fluorine with the formula CaF. It is a white insoluble solid. It occurs as the mineral fluorite (also called fluorspar), which is often deeply coloured owing to impurities.\n\nThe compound crystallizes in a cubic motif called a fluorite structure.\n\nCa centres are eight-coordinate, being centered in a \"box\" for eight F centres. Each F centre is coordinated to four Ca centres. Although perfectly packed crystalline samples are colorless, the mineral is often deeply colored due to the presence of F-centers.\nThe same crystal structure is found in numerous ionic compounds with formula AB, such as CeO, cubic ZrO, UO, ThO, and PuO. A related structure is the antifluorite structure, where the anions and cations are swapped, such as BeC.\n\nThe mineral fluorite is abundant, widespread, and mainly of interest as a precursor to HF. Thus, little motivation exists for the industrial production of CaF. High purity CaF is produced by treating calcium carbonate with hydrofluoric acid:\n\nNaturally occurring CaF is the principal source of hydrogen fluoride, a commodity chemical used to produce a wide range of materials.\nCalcium fluoride in the fluorite state is of significant commercial importance as a fluoride source. Hydrogen fluoride is liberated from the mineral by the action of concentrated sulfuric acid:\n\nCalcium fluoride is used to manufacture optical components such as windows and lenses, used in thermal imaging systems, spectroscopy, telescopes, and excimer lasers. It is transparent over a broad range from ultraviolet (UV) to infrared (IR) frequencies. Its low refractive index reduces the need for anti-reflection coatings. Its insolubility in water is convenient as well. Doped calcium fluoride, like natural fluorite, exhibits thermoluminescence and is used in thermoluminescent dosimeters.\n\nCaF is classified as \"not dangerous\", although reacting it with sulfuric acid produces toxic hydrofluoric acid. With regards to inhalation, the NIOSH-recommended concentration of fluorine-containing dusts is 2.5 mg/m in air.\n\n\n"}
{"id": "14301817", "url": "https://en.wikipedia.org/wiki?curid=14301817", "title": "Cire Trudon", "text": "Cire Trudon\n\nCire Trudon is a French candlemaker and the oldest wax-producing factory worldwide. Founded in 1643, it was the provider of the royal court of Louis XIV, as well as most of the great churches of France.\n\nCire Trudon was the biggest wax-producing factory in the French Kingdom during the 17th and 18th centuries. In 1762, in his encyclopaedia \"The art of the wax producer\", engineer Duhamel du Monceau, praises the skills of the Trudons and gives the manufacture as an example. Such a level of quality led to King Louis XIV elevating Charles Trudon to the French nobility as Count Trudon des Ormes.\n\nIn 1643, a salesman named Claude Trudon arrived in Paris. He soon became the owner of a shop on Saint-Honoré, which provided its customers with wax, candles for any domestic usage as well as church candles to the neighbouring Saint Roch parish. The candles were homemade, developing and building on a specific manufacturing process. On the eve of the reign of Louis XIV, M. Trudon established his first family-owned factory which was to bear his name and make the fortune of his heirs.\n\nClaude's son Jacques took over, becoming a grocer and wax producer and joined the Versailles royal court in 1687 as the apothecary and distiller of Queen Marie-Thérèse.\n\nAt the time, wax was under high scrutiny. Carefully collected on the hive, it was bleached through a series of pure water baths that washed off all the impurities. Dried in the open air, the wax was whitened by sunlight. When burning, the flame lit the translucent edges generating the glowing aura of the candle.\n\nIn 1737, Jérôme Trudon, heir of the family, purchased one of the most famous wax producing factories of the times that belonged to Lord Pean de Saint Gilles. Pean de Saint Gilles was then the official wax provider to the King. Drawing from the family expertise, Hierosme devoted his skills to the development of a vast factory. Skilful and very demanding, he produced a wax of very high quality, collecting it from the best hives of the kingdom and trading directly with the producers. The wax was then treated with the utmost attention: it was washed with the purest water after being filtered with gypsum, guaranteeing the highest quality. The factory also imported the finest cotton to manufacture wicks whose combustion was clean and regular. The Trudon candles, so white and so perfect, could burn for hours without crackling; their flame neither trembled nor smoked.\n\nMaison Trudon furnished candles to the royal court, and cathedrals and churches over France. More than one hundred people worked at the time in a very large building – now registered in the French inventory of historical monuments – in the city of Antony, Hauts-de-Seine. Its Latin motto and its blazon are engraved on a stone board of the factory building: a depiction of hives and bees bordered by the saying: \"Deo regique laborant\" (\"they work for God and for the King\" — they meaning the bees).\n\nCire Trudon still keep records of recipe and tools of wax whitening: wrought iron, 17th century pans. The moulds used to form candles bearing the royal blazons still remain: \"cierge pascal pour la Chapelle du Roy à Versailles, Bougies de nuit pour le Roy...\" (\"Easter candle church for the Royal Chapel in Versailles, night candle for the King...\")\n\nCire Trudon supplied the Versailles castle until the end of the monarchy. During his captivity, Louis XVI used the candles of his royal wax manufacturer. The blazon and the motto would be hidden under a layer of mortar to avoid the furies of the Revolution.\n\nIt is still today the candle provider of many churches, like Saint-Roch church in Paris, which has burned their candles since 1643. \nThe brand was successfully relaunched in 2006 .\n\nThe manufacturer distributes its products in France and abroad.\n\n\n"}
{"id": "7717738", "url": "https://en.wikipedia.org/wiki?curid=7717738", "title": "Contact resistance", "text": "Contact resistance\n\nThe term contact resistance refers to the contribution to the total resistance of a system which can be attributed to the contacting interfaces of electrical leads and connections as opposed to the intrinsic resistance, which is an inherent property, independent of the measurement method. This effect is often described by the term Electrical Contact Resistance or ECR and may vary with time, most often decreasing, in a process known as resistance creep. The idea of potential drop on the injection electrode was introduced by William Shockley to explain the difference between the experimental results and the model of gradual channel approximation. In addition to the term ECR, \"Interface resistance\", \"transitional resistance\", or just simply \"correction term\" are also used. The term \"parasitic resistance\" has been used as a more general term, where it is usually still assumed that the contact resistance has a major contribution.\n\nHere we need to distinguish the contact resistance evaluation in two-electrode systems (e.g. diodes) and three-electrode systems (e.g. transistors).\n\nFor two electrode systems the specific contact resistivity is experimentally defined as the slope of the I-V curve at V=0:\n\nwhere J is the current density = current/area. The units of specific contact resistivity are typically therefore in formula_2 where formula_3 stands for ohms. When the current is a linear function of the voltage, the device is said to have ohmic contacts.\n\nThe resistance of contacts can be crudely estimated by comparing the results of a four terminal measurement to a simple two-lead measurement made with an ohmmeter. In a two-lead experiment, the measurement current causes a potential drop across both the test leads and the contacts so that the resistance of these elements is inseparable from the resistance of the actual device, with which they are in series. In a four-point probe measurement, one pair of leads is used to inject the measurement current while a second pair of leads, in parallel with the first, is used to measure the potential drop across the device. In the four-probe case, there is no potential drop across the voltage measurement leads so the contact resistance drop is not included. The difference between resistance derived from two-lead and four-lead methods is a reasonably accurate measurement of contact resistance assuming that the leads resistance is much smaller. Specific contact resistance can be obtained by multiplying by contact area. It should also be noted that the contact resistance may vary with temperature.\n\nInductive and capacitive methods could be used in principle to measure an intrinsic impedance without the complication of contact resistance. In practice, direct current methods are more typically used to determine resistance.\n\nThe three electrode systems such as transistors require more complicated methods for the contact resistance approximation. The most common approach is the transmission line model (TLM). Here, the total device resistance formula_4 is plotted as a function of the channel length:\n\nwhere formula_6 and formula_7 are contact and channel resistances, respectively, formula_8 is the channel length/width, formula_9 is gate insulator capacitance (per unit of area), formula_10 is carrier mobility, and formula_11 and formula_12 are gate-source and drain-source voltages. Therefore, the linear extrapolation of total resistance to the zero channel length provides the contact resistance. The slope of the linear function is related to the channel transconductance and can be used for estimation of the ”contact resistance-free” carrier mobility. The approximations used here (linear potential drop across the channel region, constant contact resistance...) lead sometimes to the channel dependent contact resistance.\n\nBeside the TLM it was proposed the gated four-probe measurement and the modified time-of-flight method (TOF). The direct methods able to measure potential drop on the injection electrode directly are the Kelvin probe force microscopy (KFM) and the electric-field induced second harmonic generation.\n\nFor given physical and mechanical material properties, parameters that govern the magnitude of electrical contact resistance (ECR) and its variation at an interface relate primarily to surface structure and applied load (Contact mechanics). Surfaces of metallic contacts generally exhibit an external layer of oxide material and adsorbed water molecules, which lead to capacitor-type junctions at weakly contacting asperities and resistor type contacts at strongly contacting asperities. Over time, the contact resistance at an interface relaxes, particularly at weakly contacting surfaces, through current induced welding and dielectric breakdown, in a process also known as resistance creep. The coupling of surface chemistry, contact mechanics and charge transport mechanisms needs to be accounted for in the mechanistic evaluation of ECR phenomena.\n\nWhen a conductor has spatial dimensions close to formula_13, where formula_14 is Fermi wavevector of the conducting material, Ohm's law does not hold anymore. These small devices are called quantum point contacts. Their conductance must be an integer multiple of the value formula_15, where formula_16 is the electronic charge and formula_17 is Planck's constant. Quantum point contacts behave more like waveguides than the classical wires of everyday life and may be described by the Landauer scattering formalism. Point-contact tunneling is an important technique for characterizing superconductors.\n\nMeasurements of thermal conductivity are also subject to contact resistance, with particular significance in heat transport through granular media. Similarly, a drop in hydrostatic pressure (analogous to electrical voltage) occurs when fluid flow transitions from one channel to another.\n\nBad contacts are the cause of failure or poor performance in a wide variety of electrical devices. For example, corroded jumper cable clamps can frustrate attempts to start a vehicle that has a low battery. Dirty or corroded contacts on a fuse or its holder can give the false impression that the fuse is blown. A sufficiently high contact resistance can cause substantial heating in a high current device. Unpredictable or noisy contacts are a major cause of the failure of electrical equipment.\n\n\n"}
{"id": "13209889", "url": "https://en.wikipedia.org/wiki?curid=13209889", "title": "Dacia Sandero", "text": "Dacia Sandero\n\nThe Dacia Sandero is a subcompact car produced jointly by the French manufacturer Renault and its Romanian subsidiary Dacia since 2007, currently at its second generation. It is also marketed as the Renault Sandero in certain markets, such as Russia, Egypt, South Africa, Mexico, and South America. It was introduced in September 2007, and is based on the Logan platform.\nIt is also produced in Iran by Pars Khodro and marketed as Renault Sandero.\n\nWith a slightly shorter wheelbase than the sedan from which it derives, the Sandero was developed at Renault's Technocentre near Paris, France, in conjunction with the regional engineering centers based in Brazil and Romania. It was revealed for the first time at the 2007 Frankfurt Motor Show, and made its formal market debut in Brazil, as a Renault model, in December 2007, being the first Renault model to debut outside Europe. \n\nIt was launched subsequently in Europe as a Dacia model at the Geneva Motor Show in March 2008. Renault began manufacturing the Sandero in South Africa in February 2009, and in December 2009, in Russia. A Renault version is also manufactured in Colombia for its home market and for export to countries including Chile.\n\nIn May 2011, Renault launched in Brazil a facelifted version of Sandero, which enjoys a new face and a revised interior.\n\nIn Colombia, the facelifted versions of the Renault Sandero and the Renault Stepway were revealed at the beginning of 2012 with some differences from the other versions sold, such as the location of the doors locks and the passenger's airbag.\n\nOn the passive safety front, Sandero has been designed to meet the requirements of European regulations. Depending on equipment level, Dacia Sandero comes with up to four airbags. In terms of active safety Dacia Sandero features the latest generation Bosch 8.1 ABS which incorporates EBD (Electronic Brakeforce Distribution) and EBA (Emergency Brake Assist).\n\nEuro NCAP rated the Dacia Sandero fitted with the basic level of safety equipment and also crash tested the car equipped with the 'safety pack', which is standard on some variants, and optional on others. The crash test for basic level Dacia Sandero equipped with front seatbelt load limiters, driver frontal airbag and front passenger frontal airbag, scored 3 stars for adults, 4 stars for children occupants and 1 star for pedestrians.\n\n\nThe EuroNCAP test for the 'safety pack' model equipped with side body and head airbags and front seatbelt pretensioners, received a score of 31 for adults, 38 for children occupants and 6 for pedestrians, these results being rated as 4 from 5 stars for adults and children occupants.\n\n\nRenault do Brasil, which is the Brazilian outfit of French car manufacturer Renault, released in October 2008 the Sandero-based crossover Stepway, ten months after launching the Sandero brand there. The Brazilian Stepway has a 1.6 litre 16 valve engine, the Hi-Flex one with bio-ethanol abilities, and it is marketed in Brazil, Colombia, Argentina and Mexico.\n\nThe European version, unveiled on May 7, 2009 at Barcelona International Motor Show under the Dacia brand, is available in most of the European markets as of September 2009. Dacia Sandero Stepway comes with a 1.6 litre and petrol engine or 1.5 dCi diesel engine.\n\nThe second generation Sandero was revealed by Dacia at the 2012 Paris Motor Show. The new Stepway variant was also presented. The hatchback model and the mini crossover version were spotted covered in camouflage during 2012, in the months of June, July, and September, and CGI impressions of the new model were released by car magazines Auto Bild and Za Rulem. \n\nOfficial photos with the new Sandero were released by Dacia on 17 September 2012, showing an exterior design theme similar to the new Logan and a dashboard inspired from Lodgy.\n\nIn Romania, the new Sandero and Sandero Stepway could be ordered from 1 October 2012. It also became available in the United Kingdom, where it joined the Duster in dealerships from 2013, being the most affordable car on the market.\n\nIn June 2014, it was launched as the new Renault Sandero in Brazil, where it is also manufactured for the South American markets. Sales in Russia began in September 2014, the Sandero being locally assembled at the AvtoVAZ plant.\n\nThe current Sandero model (produced from 2012) is produced in Mioveni, Romania (near Pitesti) for RHD markets such as United Kingdom, Ireland, Cyprus and South Africa (as Renault Sandero), it is also produced in Algeria by Renault Algeria since beginning of 2016 for the local market. (Only Stepway extreme edition)\n\nIn May 2013, the second generation Dacia Sandero achieved a four star EuroNCAP overall rating for basic level, improving on the previous basic model’s three star score.\n\nThe car received a score of 29 pts (80%) for adults, 39 pts (79%) for children occupants, 21 pts (57%) for pedestrians and 5 pts (55%) for safety assist, these results being rated as 5/5 stars for adult and child occupant protections, and 4/5 stars for pedestrian protection and safety assist.\n\n\nIn August 2014, Renault Sport CEO Patrice Ratti revealed to Autocar magazine that a hot hatch RS version of Sandero was in the works, following test cars being spotted in early to mid 2015. Using the 150 PS (148 hp) 2.0 16v F4R engine, and capable of accelerating from 0 to 62 mph (100 kph) in 8.0 seconds, the Sandero RS is the first Renault Sport to be manufactured outside France. It was released in September 2015 in Brazil, different from the normal versions with three types of ECU control: normal, sport and sport+, four disc brakes with ABS, Clio RS steering wheel, electronic stability program and a six speed manual transmission.\n\nIn January 2013, British magazine \"What Car?\" awarded the second generation Sandero as the \"Best supermini less than £12,000\", noting that \"it offers something genuinely new and different in that it brings real space for bargain prices\". \"What Car?\" awarded the Sandero again in 2014 and 2015.\n\n\"Auto Express\" assessed a 4 out of 5 to the Sandero Stepway.\n\nThe Sandero was the focus of a running gag on the British television programme \"Top Gear\". In Series 11 and Series 12, after Dacia sent the show a press kit, presenter James May would often exclaim \"Good News!\" and explain a fact about the Sandero during the show's news segment, to which Jeremy Clarkson would reply \"Great!\" before abruptly changing the subject. \n\nThe gag was also featured in \"The Big Book of Top Gear\", with a page proclaiming \"Good News! The Dacia Sandero is in this book!\" In later episodes, the presenters switched sides of the gag, with Clarkson bringing up news about the car and May shrugging it off. In the first episode of Series 13, when May said he had \"Good News\", Clarkson immediately asked \"Is it the Dacia Sandero?\", to which a seemingly bewildered May replied, \"No...\" The car was not mentioned for the remainder of the series.\n\nIn Series 14, during a visit to Romania, Clarkson bought May a used Sandero as a gift. After returning from a test drive, May parked the car behind an idling truck, and exited. As May praised the car to his co-presenters, the lorry reversed into the Sandero, damaging the passenger side. The joke was continued in Series 15, except this time referring to the Dacia Duster, and in Series 18, when May brought up the new Dacia Lodgy. The gag returned in the first and third episodes of Series 19, as well as the second and fifth episodes of Series 20. \n\nThe second generation Sandero was featured alongside the Ford Fiesta and Volkswagen Up! in series 21 as part of a 1.0L three cylinder cars challenge, which ended with Clarkson (Up!) and May (Sandero) having to drive into the abandoned city of Chernobyl, with Hammond's Fiesta having already run out of fuel. The Sandero was the only car to make it back out and complete all the challenges. May pointed out the large price difference between the Fiesta and the Sandero, stating that at £17,500 Vs £7,500 he could afford to lose his car, buy another - and still be better off than Hammond.\n\nDespite the comical and sarcastic nature of the recurring bit, May has stated that he has a genuine affinity for the Sandero. According to some sources, its second generation was intended to become a third \"Reasonably Priced Car\" on \"Top Gear\", however its use was prevented due to its delayed release in Britain.\n\n\n\n"}
{"id": "55598200", "url": "https://en.wikipedia.org/wiki?curid=55598200", "title": "Daylight redirecting film", "text": "Daylight redirecting film\n\nDaylight redirecting film (DRF) is a thin, flexible plastic film which can be applied to a window to refract or reflect incoming light upwards, so that the deeper parts of the room are lit more evenly. It can be used as a substitute for opaque blinds. It is a form of prism lighting.\n\nThe human eye's response to light is non-linear: halving the light level does not halve the perceived brightness of a space, it makes it look only slightly dimmer. If light is redistributed from the brightest parts of a room to the dimmest, the room therefore appears brighter overall, and more space can be given a useful and comfortable level of illumination (see before and after images from an 1899 article, left). This can reduce the need for artificial lighting.\n\nRefraction and total internal reflection inside optical prisms can bend beams of light. This bending of the light allows it to be redistributed.\n\nWhile the films do save energy from lighting, the savings vary substantially by climate, aspect, electricity costs, and existing lighting type. At 2014 costs, including labour, payback time may be measured in decades.\n\nThis film increases the sunlight area by 200%.\n\nIt is effective not only for lighting energy but also for cooling and heating energy saving. With the growing interest in green energy buildings, the interest in these Daylight redirecting films is also growing.\n\nDaylight redirecting film is made of acrylic on a flexible polyester backing, one side coated with a pressure-sensitive adhesive (to make it peel-and-stick).\n\nThere are two types of film. Some film is moulded with tiny prisms, making a flexible peel-and-stick miniature prismatic panel. The prisms are joined at the edges into a sheet. A prism sheet is somewhat like a linear Fresnel lens, but each ridge may be identical. Unlike a Fresnel lens, the light is not intended to be focused, but used for anidolic lighting.\n\nOther film is moulded with thin near-horizontal voids protruding into or through the acrylic; the slits reflect light hitting their top surfaces upwards. Refraction is minimized, to avoid colouring the light.\n\nThe reflection-based films are more transparent (both are translucent), but they tend to send the light up at the ceiling, not deeper into the room. Refraction-based films are translucent rather than transparent, but offer finer control over the direction of the outgoing light beam; the film can be made in a variety of prism shapes to refract light by a variety of angles. Refraction-based films are a lighter modern version of glass prism tiles.\n\nDaylight redirecting window film was initially made of one redirecting film and one glare-reducing diffusing film, often located on different interior surfaces of a double-glazed window, but integrated single films are now available.\nDue to the prism structure, it works only at certain angles. By making DRF blind, you can always keep the same effect. \nAdd UV blocking filters to the film to block harmful UV rays.\n\n"}
{"id": "36683093", "url": "https://en.wikipedia.org/wiki?curid=36683093", "title": "Distribution of lightning", "text": "Distribution of lightning\n\nThe distribution of lightning, or the incidence of individual strikes, in any particular place is highly variable, but lightning does have an underlying spatial distribution. High quality lightning data has only recently become available, but the data indicates that lightning occurs on average 44 (± 5) times every second over the entire Earth, making a total of about 1.4 billion flashes per year.\n\nThe lightning flash rate averaged over the Earth for intra-cloud (IC) + cloud-to-cloud (CC) to cloud-to-ground (CG) is in the ratio: (IC+CC):CG = 3:1. The base of the negative region in a cloud is normally at roughly the elevation where freezing occurs. The closer this region is to the ground, the more likely cloud-to-ground strikes are. In the tropics, where the freeze zone is higher, the (IC+CC):CG ratio is about 9:1. In Norway, at latitude 60° N, where the freezing elevation is lower, the (IC+CC):CG ratio is about 1:1.\n\nThe map on the right shows that lightning is not distributed evenly around the planet. About 70% of lightning occurs on land in the Tropics, where the majority of thunderstorms occur. The North and South Poles and the areas over the oceans have the fewest lightning strikes. The place where lightning occurs most often (according to the data from 2004 to 2005) is near the small village of Kifuka in the mountains of eastern Democratic Republic of the Congo, where the elevation is around . This region received 158 lightning strikes per square kilometer (409 per sq mi) a year.\n\nAbove the Catatumbo river, which feeds Lake Maracaibo in Venezuela, Catatumbo lightning flashes several times per minute and this place has the highest number of lightning strikes per square kilometer in the world. Singapore has one of the highest rates of lightning activity in the world. The city of Teresina in northern Brazil has the third-highest rate of occurrences of lightning strikes in the world. The surrounding region is referred to as the \"Chapada do Corisco\" (\"Flash Lightning Flatlands\").\n\nIn the United States, the west coast has the fewest lightning strikes, and Central Florida sees more lightning than any other area. Florida has the largest number of recorded strikes during summer. Much of Florida is a peninsula, bordered by the ocean on three sides with a subtropical climate. The result is the nearly daily development of clouds that produce thunderstorms. For example, in what is called \"Lightning Alley\", an area from Tampa to Orlando, there are as many as 50 strikes per square mile (about 20 per km) per year. The Empire State Building in New York City is struck by lightning on average 23 times each year, and was once struck 8 times in 24 minutes.\n\nBefore technology was developed to accurately detect and record lightning flashes, climatologies were based upon the number of audible detection of thunder. The keraunic (or ceraunic) level was the average number of days per year when thunder was heard in a given area. A map of isokeraunic contours was used to give a rough estimate of relative lightning frequencies. However, variations in population, the distance sound travels due to terrain made such maps quite spurious, and human hearing made such maps imprecise. It also could not hope to differentiate between different types of lightning.\n\nElectronic lightning sensors advanced during the 20th century using radio wave disruptions. Originally the expense of such instruments caused only sporadic development. However a small set of sensors in the U.S. employed during a 1979 project by NOAA’s National Severe Storms Laboratory grew by bits and pieces into the National Lightning Detection Network (NLDN), achieving nationwide coverage in 1989. Vaisala is now the operator and primary distributor of data from the NLDN, and developed the Canadian Lightning Detection Network (CLDN) as of 1998. The EUCLID network is the European shared network, covering most of the continent apart from some far eastern nations. Collaborative amateur development spurred the formation of the Blitzortung community, which offers real-time lightning strike data from most of the world (as well as historical data dating back to 2008) under the Creative Commons license.\n\nSatellite lightning measurements began in 1997 when NASA and National Space Development Agency (NASDA) of Japan launched the Lightning Imaging Sensor (LIS) aboard the aboard the TRMM satellite, providing periodic scan swaths over tropical and sub-tropical portions of the globe until the satellite's was lost in 2015. In 2017 NOAA started deployment of Geostationary Lightning Mappers aboard their GOES-R class satellites, offering continual coverage of much of the land within the western Hemisphere.\n\nMaps of the U.S. lightning strike/kmyr averaged from 1997-2010 are available from Vaisala's webpage for a fee. More detailed U.S. regional lightning maps based on the National Oceanic and Atmospheric Administration (NOAA) and the National Weather Service (NWS) data centered on different cities are put out by the Cooperative Institute for Applied Meteorological Studies at Texas A&M University.\n\n"}
{"id": "13651046", "url": "https://en.wikipedia.org/wiki?curid=13651046", "title": "Double layer (plasma physics)", "text": "Double layer (plasma physics)\n\nA double layer is a structure in a plasma consisting of two parallel layers of opposite electrical charge. The sheets of charge, which are not necessarily planar, produce localised excursions of electric potential, resulting in a relatively strong electric field between the layers and weaker but more extensive compensating fields outside, which restore the global potential. Ions and electrons within the double layer are accelerated, decelerated, or deflected by the electric field, depending on their direction of motion.\n\nDouble layers can be created in discharge tubes, where sustained energy is provided within the layer for electron acceleration by an external power source. Double layers are claimed to have been observed in the aurora and are invoked in astrophysical applications. Similarly, a double layer in the auroral region requires some external driver to produce electron acceleration.\n\nElectrostatic double layers are especially common in current-carrying plasmas, and are very thin (typically ten Debye lengths), compared to the sizes of the plasmas that contain them. Other names for a double layer are electrostatic double layer, electric double layer, plasma double layers. The term ‘electrostatic shock’ in the magnetosphere has been applied to electric fields oriented at an oblique angle to the magnetic field in such a way that the perpendicular electric field is much stronger than the parallel electric field, In laser physics, a double layer is sometimes called an ambipolar electric field.\n\nDouble layers are conceptually related to the concept of a 'sheath' (\"see\" Debye sheath). An early review of double layers from laboratory experiment and simulations is provided by Torvén.\n\nDouble layers may be classified in the following ways:\nPotential imbalance will be neutralised by electron (1&3) and ion (2&4) migration, unless the potential gradients are sustained by an external energy source. Under most laboratory situations, unlike outer space conditions, charged particles may effectively originate within the double layer, by ionization at the anode or cathode, and be sustained.\n\nThe figure shows the localised perturbation of potential produced by an idealised double layer consisting of two oppositely charged discs. The perturbation is zero at a distance from the double layer in every direction.\n\nIf an incident charged particle, such as a precipitating auroral electron, encounters such a static or quasistatic structure in the magnetosphere, provided that the particle energy exceeds half the electric potential difference within the double layer, it will pass through without any net change in energy. Incident particles with less energy than this will also experience no net change in energy but will undergo more overall deflection.\n\nFour distinct regions of a double layer can be identified, which affect charged particles passing through it, or within it:\n\nDouble layers will tend to be transient in the magnetosphere, as any charge imbalance will become neutralised, unless there is a sustained external source of energy to maintain them as there is under laboratory conditions.\n\nThe details of the formation mechanism depend on the environment of the plasma (e.g. double layers in the laboratory, ionosphere, solar wind, nuclear fusion, etc.). Proposed mechanisms for their formation have included:\n\n\n\nThis brief account of the advent of double layers in research elaborates on some information in the chronological list of events provided above.\nIt was already known in the 1920s that a plasma has a limited capacity for current maintenance, Irving Langmuir characterized double layers in the laboratory and called these structures double-sheaths. In the 1950s a thorough study of double layers started in the laboratory. Many groups are still working on this topic theoretically, experimentally and numerically. It was first proposed by Hannes Alfvén (the developer of magnetohydrodynamics from laboratory experiments) that the polar lights or Aurora Borealis are created by electrons accelerated in the magnetosphere of the Earth. He supposed that the electrons were accelerated electrostatically by an electric field localized in a small volume bounded by two charged regions, and the so-called double layer would accelerate electrons earthwards. Since then other mechanisms involving wave-particle interactions have been proposed as being feasible, from extensive spatial and temporal in situ studies of auroral particle characteristics.\n\nMany investigations of the magnetosphere and auroral regions have been made using rockets and satellites. McIlwain discovered from a rocket flight in 1960 that the energy spectrum of auroral electrons exhibited a peak that was thought then to be too sharp to be produced by a random process and which suggested, therefore, that an ordered process was responsible. It was reported in 1977 that satellites had detected the signature of double layers as electrostatic shocks in the magnetosphere. indications of electric fields parallel to the geomagnetic field lines was obtained by the Viking satellite, which measures the differential potential structures in the magnetosphere with probes mounted on 40m long booms. These probes measured the local particle density and the potential difference between two points 80m apart. Asymmetric potential excursions with respect to 0 V were measured, and interpreted as a double layer with a net potential within the region. Magnetospheric double layers typically have a strength formula_1 (where the electron temperature is assumed to lie in the range formula_2) and are therefore weak. A series of such double layers would tend to merge, much like a string of bar magnets, and dissipate, even within a rarefied plasma. It has yet to be explained how any overall localised charge distribution in the form of double layers might provide a source of energy for auroral electrons precipitated into the atmosphere.\n\nInterpretation of the FAST spacecraft data proposed strong double layers in the auroral acceleration region. Strong double layers have also been reported in the downward current region by Andersson et al. Parallel electric fields with amplitudes reaching nearly 1 V/m were inferred to be confined to a thin layer of approximately 10 Debye lengths. It is stated that the structures moved ‘at roughly the ion acoustic speed in the direction of the accelerated electrons, i.e., anti-earthward.’ That raises a question of what role, if any, double layers might play in accelerating auroral electrons that are precipitated downwards into the atmosphere from the magnetosphere.\n\nThe possible role of precipitating electrons from 1-10keV themselves generating such observed double layers or electric fields has seldom been considered or analysed. Equally, the general question of how such double layers might be generated from an alternative source of energy, or what the spatial distribution of electric charge might be to produce net energy changes, is seldom addressed. Under laboratory conditions an external power supply is available. \nIn the laboratory, double layers can be created in different devices. They are investigated in double plasma machines, triple plasma machines, and Q-machines. The stationary potential structures that can be measured in these machines agree very well with what one would expect theoretically. An example of a laboratory double layer can be seen in the figure below, taken from Torvén and Lindberg (1980), where we can see how well-defined and confined is the potential drop of a double layer in a double plasma machine.\nOne of the interesting aspects of the experiment by Torvén and Lindberg (1980) is that not only did they measure the potential structure in the double plasma machine but they also found high-frequency fluctuating electric fields at the high-potential side of the double layer (also shown in the figure). These fluctuations are probably due to a beam-plasma interaction outside the double layer, which excites plasma turbulence. Their observations are consistent with experiments on electromagnetic radiation emitted by double layers in a double plasma machine by Volwerk (1993), who, however, also observed radiation from the double layer itself.\n\nThe power of these fluctuations has a maximum around the plasma frequency of the ambient plasma. It was later reported that the electrostatic high-frequency fluctuations near the double layer can be concentrated in a narrow region, sometimes called the hf-spike. Subsequently, both radio emissions, near the plasma frequency, and whistler waves at much lower frequencies were seen to emerge from this region. Similar whistler wave structures were observed together with electron beams near Saturn's moon Enceladus, suggesting the possible presence of a double layer at lower altitude.\n\nA recent development in double layer experiments in the laboratory is the investigation of so-called stairstep double layers. It has been observed that a potential drop in a plasma column can be divided into different parts. Transitions from a single double layer into two-, three-, or greater-step double layers are strongly sensitive to the boundary conditions of the plasma.\n\nUnlike experiments in the laboratory, the concept of such double layers in the magnetosphere, and any role in creating the aurora, suffers from there so far being no identified steady source of energy. The electric potential characteristic of double layers might however indicate that, those observed in the auroral zone are a secondary product of precipitating electrons that have been energized in other ways, such as by electrostatic waves. \nSome scientists have suggested a role of double layers in solar flares. Establishing such a role indirectly is even harder to verify than postulating double layers as accelerators of auroral electrons within the earth’s magnetosphere. Serious questions have been raised on their role even there.\n\n\n\n"}
{"id": "32723441", "url": "https://en.wikipedia.org/wiki?curid=32723441", "title": "Dual-Stage 4-Grid", "text": "Dual-Stage 4-Grid\n\nThe Dual-Stage 4-Grid (DS4G) is an electrostatic ion thruster design developed by the European Space Agency, in collaboration with the Australian National University. The design was derived by D. Fern from Controlled Thermonuclear Reactor experiments that use a 4-grid mechanism to accelerate ion beams.\n\nA 4-grid ion thruster with only 0.2 m diameter is projected to absorb 250 kW power. With that energy input rate, the thruster could produce a thrust of 2.5 N. The specific impulse (a measure of fuel efficiency), could reach 19,300 s at an exhaust velocity of 210 km/s if xenon propellant were used. The potentially attainable power and thrust densities would substantially extend the power absorption of current ion thrusters to far more than 100 kW. These characteristics facilitate the development of ion thrusters that can result in extraordinary high-end velocities.\n\nLike with thruster concepts such as VASIMR, the dual-stage-4-grid ion thrusters are mainly limited by the necessary power supply for their operation. For example, if solar panels were to supply more than 250 kW, the size of the solar array would surpass the size of the solar panels of the International Space Station. To provide 250 kW with Stirling radioisotope generators would require roughly 1 tonne of plutonium-238 (for which the US stockpile as of 2013 was no more than 20 kg), and so a nuclear thermal reactor would be needed.\n"}
{"id": "1848930", "url": "https://en.wikipedia.org/wiki?curid=1848930", "title": "Duroplast", "text": "Duroplast\n\nDuroplast is a composite thermosetting plastic, a close relative of formica and bakelite. It is a resin plastic reinforced with fibers (either cotton or wool) making it a fiber-reinforced plastic similar to fiberglass.\n\nDuroplast was used by the German Democratic Republic state owned automobile manufacturer called VEB Sachsenring Automobilwerke Zwickau from 1955 to until 1991, just after German Reunification. It was the material used to produce the body of the Trabant. There were four main versions of the Trabant, the 1963-1990 Trabant 601 was the longest running. The successor company is called HQM Sachsenring GmbH.\n\nThe product was first used in the body of the IFA F8 and later also the AWZ P70 or Zwickau P70 and the Trabant. It was also used to make suitcases.\n\nDuroplast is light and strong. It is made of recycled material, cotton waste and phenol resins. Because it can be made in a press similar to shaping steel, it is more suitable for volume car production than fiberglass.\n\nSimilar to fiberglass, Duroplast has limited possibilities for efficient disposal. As discarded Trabants began to fill junkyards, creative solutions sprung up for disposing of them. One of these was developed by a Berlin biotechnology company, which experimented with a bacterium to consume the body in 20 days. Urban legends, depicted in the movie \"Black Cat, White Cat\" and described in a song by the Serbian band Atheist Rap, described recycling Duroplast by feeding the cars to pigs, sheep and other farm animals. In the late 1990s, the same Trabant factory in Zwickau developed a solution for disposing of the Duroplast shells by shredding them and using them as an aggregate in cement blocks for pavement construction. This was featured in an episode of the program \"Scientific American Frontiers\" on the American PBS TV channel.\n\nThe use of Duroplast in Trabants and subsequent GDR jokes and mockery in western auto magazines such as \"Car and Driver\" gave rise to an urban myth that the Trabant is made of corrugated cardboard.\n"}
{"id": "11401059", "url": "https://en.wikipedia.org/wiki?curid=11401059", "title": "Environmental impact of reservoirs", "text": "Environmental impact of reservoirs\n\nThe environmental impact of reservoirs comes under ever-increasing scrutiny as the global demand for water and energy increases and the number and size of reservoirs increases.\n\nDams and reservoirs can be used to supply drinking water, generate hydroelectric power, increase the water supply for irrigation, provide recreational opportunities, and flood control. In 1960 the construction of Llyn Celyn and the flooding of Capel Celyn provoked political uproar which continues to this day. More recently, the construction of Three Gorges Dam and other similar projects throughout Asia, Africa and Latin America have generated considerable environmental and political debate.\n\nA dam also acts as a barrier between the upstream and downstream movement of migratory river animals, such as salmon and trout. \nSome communities have also begun the practice of transporting migratory fish upstream to spawn via a barge.\n\nRivers carry sediment down their riverbeds, allowing for the formation of depositional features such as river deltas, alluvial fans, braided rivers, oxbow lakes, levees and coastal shores. The construction of a dam blocks the flow of sediment downstream, leading to downstream erosion of these sedimentary depositional environments, and increased sediment build-up in the reservoir. While the rate of sedimentation varies for each dam and each river, eventually all reservoirs develop a reduced water-storage capacity due to the exchange of \"live storage\" space for sediment. Diminished storage capacity results in decreased ability to produce hydroelectric power, reduced availability of water for irrigation, and if left unaddressed, may ultimately result in the expiration of the dam and river.\n\nAs all dams result in reduced sediment load downstream, a dammed river is greatly demanding for sediment as it will not have enough sediment. This is because the rate of deposition of sediment is greatly reduced since there is less to deposit but the rate of erosion remains nearly constant, the water flow erodes the river shores and riverbed, threatening shoreline ecosystems, deepening the riverbed, and narrowing the river over time. This leads to a compromised water table, reduced water levels, homogenization of the river flow and thus reduced ecosystem variability, reduced support for wildlife, and reduced amount of sediment reaching coastal plains and deltas. This prompts coastal erosion, as beaches are unable to replenish what waves erode without the sediment deposition of supporting river systems. Downstream channel erosion of dammed rivers is related to the morphology of the riverbed, which is different from directly studying the amounts of sedimentation because it is subject to specific long term conditions for each river system. For example, the eroded channel could create a lower water table level in the affected area, impacting bottomland crops such as alfalfa or corn, and resulting in a smaller supply.\nIn the case of the Three Gorges Dam in China the changes described above now appears to have arrived at a new balance of erosion and sedimentation over a 10-year period in the lower reaches of the river. The impacts on the tidal region have also been linked to the upstream effects of the dam.\n\nThe water of a deep reservoir in temperate climates typically stratifies with a large volume of cold, oxygen poor water in the hypolimnion. Analysis of temperature profiles from 11 large dams in the Murray Darling Basin (Australia) indicated differences between surface water and bottom water temperatures up to 16.7 degrees Celsius. If this water is released to maintain river flow, it can cause adverse impacts on the downstream ecosystem including fish populations. Under worse case conditions (such as when the reservoir is full or near full), the stored water is strongly stratified and large volumes of water are being released to the downstream river channel via bottom level outlets, depressed temperatures can be detected 250 - 350 kilometres downstream. The operators of Burrendong Dam on the Macquarie River (eastern Australia) are attempting to address thermal suppression by hanging a geotextile curtain around the existing outlet tower to force the selective release of surface water.\n\nMany dams are built for irrigation and although there is an existing dry ecosystem downstream, it is deliberately destroyed in favor of irrigated farming. After the Aswan Dam was constructed in Egypt it protected Egypt from the droughts in 1972–73 and 1983–87 that devastated East and West Africa. The dam allowed Egypt to reclaim about 840,000 hectares in the Nile Delta and along the Nile Valley, increasing the country's irrigated area by a third. The increase was brought about both by irrigating what used to be desert and by bringing under cultivation 385,000 hectares that were natural flood retention basins. About half a million families were settled on these new lands.\n\nIn many low lying developing countries the savanna and forest ecology adjacent to floodplains and river deltas are irrigated by wet season annual floods. Farmers annually plant flood recession crops, where the land is cultivated after floods recede to take advantage of the moist soil. Dams generally discourage this cultivation and prevent annual flooding, creating a dryer downstream ecology while providing a constant water supply for irrigation.\n\"Case studies\"\n\nDams occasionally break causing catastrophic damage to communities downstream. Dams break due to engineering errors, attack or natural disaster. The greatest dam break disaster happened in China killing 200,000 Chinese citizens. However, they have happened in California killing 600 people, Germany during World War II and other countries.\n\nThe controversial Three Gorges Dam in China is able to store 22 cubic kilometres of floodwaters on the Yangtze River. The 1954 Yangtze River floods killed 33,000 people and displaced 18 million people from their homes. In 1998 a flood killed 4000 people and 180 million people were affected. The flooding of the reservoir caused over a million people to relocate, then a flood in August 2009 was completely captured by the new reservoir, protecting hundreds of millions of people downstream.\n\nDiseases\nWhilst reservoirs are helpful to humans, they can also be harmful as well. One negative effect is that the reservoirs can become breeding grounds for disease vectors. This holds true especially in tropical areas where mosquitoes (which are vectors for malaria) and snails (which are vectors for Schistosomiasis) can take advantage of this slow flowing water.\nResettlement\nDams and the creation of reservoirs also require relocation of potentially large human populations if they are constructed close to residential areas. The record for the largest population relocated belongs to the Three Gorges dam built in China. Its reservoir submerged a large area of land, forcing over a million people to relocate. \"Dam related relocation affects society in three ways: an economic disaster, human trauma, and social catastrophe\", states Dr. Michael Cernea of the World Bank and Dr. Thayer Scudder, a professor at the California Institute of Technology. \nAs well, as resettlement of communities, care must also be taken not to irreparably damage sites of historical or cultural value. The Aswan Dam forced the movement of the Temple at Aswan to prevent its destruction by the flooding of the reservoir.\n\nReservoirs may contribute to changes in the Earth's climate. Warm climate reservoirs generate methane, a greenhouse gas when the reservoirs are stratified, in which the bottom layers are anoxic (i.e. they lack oxygen), leading to degradation of biomass through anaerobic processes. At a dam in Brazil, where the flooded basin is wide and the biomass volume is high the methane produced results in a pollution potential 3.5 times more than an oil-fired power plant would be. A theoretical study has indicated that globally hydroelectric reservoirs may emit 104 million metric tonnes of methane gas annually. Methane gas is a significant contributor to global climate change.\n\nThe following table indicates reservoir emissions in milligrams per square meter per day for different bodies of water.\n\n\n"}
{"id": "25876669", "url": "https://en.wikipedia.org/wiki?curid=25876669", "title": "Fine paper", "text": "Fine paper\n\nFine papers are printing and writing paper grades based mainly on chemical pulps. Normally the content of mechanical pulps are below 10% and the amount of fillers in the range 5–25%.\n\nFine papers are normally surface sized or pigmented with calcium carbonate. Uncoated fine papers are calendered in the paper machine with an online calender.\n\n"}
{"id": "57291700", "url": "https://en.wikipedia.org/wiki?curid=57291700", "title": "Flathead motorcycles", "text": "Flathead motorcycles\n\nFlathead motorcycles are a type of bike that was a standard for pre-war motorcycles, in particular US V-twins such as Harley-Davidson and Indian, some British singles, BMW flat twins and Russian copies thereof.\n\nFlathead motorcycles have side-valve cylinder heads, an early engine design that has mostly fallen into disuse. \nIn 1925 Cleveland Motorcycle Manufacturing Company released a motorcycle with a T-head four-cylinder engine designed by L. E. Fowler.\n\nThe flathead engine saw service in Harley-Davidson motorcycles beginning with the Model W's flat-twin, produced from 1919 to 1923, and continuing in 1924 with single-cylinder export-model and singles and continued in Servi-Cars until 1973. In the domestic U.S. market, the (primary VIN letter) D model (1929 to 1931) and its technical descendant, the (primary VIN letter) R model (1932 to 1936), started Harley's side-valve tradition in the 45-cubic-inch displacement class. The D and R models featured a total-loss oiling system and were succeeded in 1937 by the (primary VIN letter) W 45, which had recirculating oil lubrication. The WLA (W = return oil, L = performance, A = army) went on to serve in World War II as the U.S. and Canadian Army's WLC (C = Canada) primary two-wheeled mount and subsequently as a civilian middleweight through 1952. The engine continued virtually unchanged with 2.745\" bore and 3.8125\" stroke with various G-based designations in the three-wheeled Servi-Car until production ceased in 1973.\n\nIn 1952, the K series flatheads was introduced with the same bore and stroke, selling in parallel with the W series (which was discontinued after 1952), designed to compete with British sporting motorcycles of the time. The K models featured a 750cc unit construction engine and transmission case, right side foot shift and left side foot brake. From 1954 to 1956, the KH received an increase in stroke to 4-9/16\" to bring displacement to 888cc (54 cubic inches). \n\nThe American Motorcycle Association (AMA) class C rules of 1952 allowed sidevalves of 750cc to compete against 500cc overhead valve bikes. The 750cc KR factory racer was highly competitive in dirt track and road racing, and was produced in limited numbers until 1969\n, when the AMA changed the rules by increasing the Class C displacement limit to 750 cc. Without the displacement advantage, the KR flatheads were not competitive against 750 cc overhead valve bikes.\n\nIn 1930, the VL flathead replaced the JD Big Twin, which had featured intake-over-exhaust (IoE) valve configuration. The VL had a single downtube frame and total loss oiling, culminating in an version (VLH) in 1935. In 1937, that engine was redesigned to include a recirculating lubrication system, and designated the model U, and it went into the same frame and running gear configuration as the model E Knucklehead, which had originated in 1936. The U continued to be produced in varying configurations as a 74 cubic inch U & UL (1937 to 1948), and 80 cubic inch UH & ULH engine (1937 to 1941). By that time, the first year of the aluminum-head Panhead, it had been thoroughly superseded and outsold in the marketplace by the superior performance of the overhead valve model Big Twins.\n"}
{"id": "3804656", "url": "https://en.wikipedia.org/wiki?curid=3804656", "title": "Floating body effect", "text": "Floating body effect\n\nThe floating body effect is the effect of dependence of the body potential of a transistor realized by the silicon on insulator (SOI) technology on the history of its biasing and the carrier recombination processes. The transistor's body forms a capacitor against the insulated substrate. The charge accumulates on this capacitor and may cause adverse effects, for example, opening of parasitic transistors in the structure and causing off-state leakages, resulting in higher current consumption and in case of DRAM in loss of information from the memory cells. It also causes the history effect, the dependence of the threshold voltage of the transistor on its previous states. In analog devices, the floating body effect is known as the kink effect.\n\nOne countermeasure to floating body effect involves use of fully depleted devices. The insulator layer in FD devices is significantly thinner than the channel depletion width. The charge and thus also the body potential of the transistors is therefore fixed. However, the short-channel effect is worsened in the FD devices, the body may still charge up if both source and drain are high, and the architecture is unsuitable for some analog devices that require contact with the body. Hybrid trench isolation is another approach.\n\nWhile floating body effect presents a problem in SOI DRAM chips, it is exploited as the underlying principle for Z-RAM and T-RAM technologies. For this reason, the effect is sometimes called the Cinderella effect in the context of these technologies, because it transforms a disadvantage into an advantage. AMD and Hynix licensed Z-RAM, but as of 2008 had not put it into production.\nAnother similar technology (and Z-RAM competitor) developed at Toshiba and refined at Intel is Floating Body Cell (FBC).\n"}
{"id": "1208145", "url": "https://en.wikipedia.org/wiki?curid=1208145", "title": "Fuel Cell Bus Club", "text": "Fuel Cell Bus Club\n\nThe Fuel Cell Bus Club comprised the participants of the projects CUTE (2001-2006), ECTOS (2001-2005) and STEP (2001-2005) which were pioneering demonstration projects for fuel cell bus fleets in Europe and Australia. The projects have been successfully completed. There were three buses in each of the 11 cities in the trial. The buses were a Mercedes-Benz Citaro and used hydrogen fuel cells from Ballard Power Systems. At the time they claimed to be the largest fleet of fuel cell buses in the world. The buses were estimated to cost US$ 1.2 million each and have a range of and carry around 70 passengers.\n\nCUTE stands for \"Clean Urban Transport for Europe\". This European Union initiative was responsible for the fuel cell buses in all but two of the cities: Hamburg, London, Barcelona, Stockholm, Porto, Stuttgart, Amsterdam, Luxembourg, and Madrid. It was supported by a consortium of transportation operators, hydrogen infrastructure and fuel cell developers, universities and city authorities. The project ran from 2001-2006. The project was \"deemed a success.\"\n\nECTOS stands for \"Ecological City Transport System\". Icelandic New Energy was responsible for this project, the aim of which was to demonstrate \"state-of-the-art\" hydrogen technology by running part of the public transport system with fuel cell buses in the city Reykjavík, the capital of Iceland. Hydrogen was produced from domestic geothermal and hydro-powered energy sources by electrolysis. The project ran from 2001-2005.\n\nSTEP stands for \"Sustainable Transport Energy for Perth\". This initiative of the Government of Western Australia's Department for Planning and Infrastructure (DPI), was the responsibility of the public transport organisation Transperth, though it was run by contracted operator Path Transit. They were operated in the city Perth, the capital of Western Australia. These three buses are called \"EcoBuses\". The project ran from 2001-2005, with the first buses in service in September 2004.\n\nThe Perth trial received A$2.5 million funding from the Department of the Environment and Heritage and the Australian Greenhouse Office. It was endorsed by the United Nations Environment Programme and the United Nations Industrial Development Organization.\n\nBP produced the hydrogen as a by-product at its Kwinana oil refinery ( south of Perth). The hydrogen was then transported by road in specially designed road tankers to a bus depot in the northern suburbs of Perth. Perth's buses achieved greater reliability and better fuel economy than in any other city in the trial.\n\nBy June 2005, the Perth buses had covered more than and completed almost 3,000 operational hours, with almost 60,000 passengers having used the service.\n\n\nThe buses were manufactured by DaimlerChrysler, the manufacturer of Mercedes-Benz vehicles, and use fuel cell engines manufactured by XCELLSIS Fuel Cell Engines, now a division of Ballard Power Systems, developed as an alliance of Ballard, DaimlerChrysler, and Ford. A number of the cities are receiving their hydrogen from BP. The trial is being independently evaluated, mostly by Murdoch University.\n\n"}
{"id": "21490547", "url": "https://en.wikipedia.org/wiki?curid=21490547", "title": "Google PowerMeter", "text": "Google PowerMeter\n\nGoogle PowerMeter was a software project of Google's philanthropic arm, Google.org, to help consumers track their home electricity usage. The development of the software was part of an effort by Google to invest in renewable energy, electricity grid upgrades, and other measures that would reduce greenhouse gas emissions. It was launched on October 5, 2009 and ended on September 16, 2011.\n\nThe software was designed to record the user's electricity usage in near real-time. According to the company, if half of America's homes' energy use was cut by ten percent, it would equal the average energy used by eight million cars.\n\nIt was hoped that this tool would raise the home-owner's awareness of how much energy they use and make users more energy efficient. PowerMeter was intended for use with smart meters able to track electricity usage in more detail than standard electric meters. According to Google, in 2009 there were approximately 40 million smart meters in use worldwide. By early 2009, approximately 7% of US homes had a smart meter installed.\n\nSome other types of electricity meters and in-home energy use displays could also be used with PowerMeter.\n\nIn October 2009 Google PowerMeter announced their first \"device partner\", The Energy Detective (TED 5000), an energy monitor from Energy Inc then only available only in North America, and their first UK partnership which was with AlertMe.\n\nIn 2010 UK company Current Cost announced a collaboration with Google PowerMeter. San Diego Gas and Electric's Sempra Energy company announced plans to install 1.4 million smart meters in San Diego County and Southern Orange County by the end of 2011 and said that after they sent out 100,000 post cards to let consumers know they could use the Google PowerMeter service, about 6% had started to use it.\n\nIn June 2011 Google announced the service would cease.\n\n\n"}
{"id": "55164169", "url": "https://en.wikipedia.org/wiki?curid=55164169", "title": "Growian", "text": "Growian\n\nGrowian or GROWIAN (short for German \"Große Windenergieanlage\" - \"Large wind turbine\") was a publicly funded wind turbine built in the Kaiser-Wilhelm-Koog near Marne for purposes of technology testing in the 1980s. It was a two-bladed \"lee runner\" (the rotor was situated on the downwind side of the tower) with a hub height of about .\n\nFor a long time Growian was the world's largest wind turbine. Many features of the installation were novel and had not previously been trialled at this scale. Due to manufacturing faults in the casing, the turbine could not be run at full performance, and various issues with materials and construction prevented continuous testing. Consequently, the installation was idle for the greater part of the period between the first test run on 6 July 1983 (official start of operations was 4 October 1983) and end of operations in August 1987. Growian was decommissioned over the course of 1987, and dismantled in summer 1988. \n\nGrowian's power rating was 3,000 kW, at the time the highest in the world. The rotor had an oscillating nave, a diameter of , and revolved at approximately 19.5 rpm. The orientation of the two blades was regulated by a mechanical-electrical mechanism. In contrast to most modern turbines, the blades rotated on the leeward side of the tower.\n\nThe turbine house at a height of weighed 340 t, and each blade weighed 23 t.\n\nThe turbine had a switching on speed of and a rated speed of . It would cut out at a speed of and was rated for a maximum survivable speed of . Projected annual energy yield at a mean wind speed of was approximately 12 GWh.\n\nRotor and induction generator were mechanically coupled by a gearing mechanism consisting of one spur gear and two epicyclic gears. Feed-in to the power grid was realized using a set of motor–generators that was largely identical to that later used at , one of the few electrical substations that allowed for electricity import from the GDR. The rotors were constructed using steel walers, and in cross section consisted of a steel core, an outer skin and fibreglass reinforcement rods. \n\nThe tower and one of the rotors are on display at the Auto & Technik Museum Sinsheim.\n\nThe total cost of the installation was about 87 million Deutsche Mark.\n\nTowards the end of 1976, the German Federal Ministry of Education and Research (BMFT) decided to use research contracts and expert consultation to investigate the development of large wind turbines. This decision, taken as a result of public pressure, ran contrary to the stalling efforts of the major energy providers. Contracts were awarded to MAN SE , the Institute for Aerodynamics and Gas Dynamics at the University of Stuttgart, and the University of Regensburg. In 1978 the BMFT decided to construct the world's largest wind turbine with a tower height and blade diameter of 100 m. MAN SE was chosen as main contractor, and the formation of a construction and operation company was given into the charge of the reluctant (HEW). This led to the formation of the Growian GmbH on 8 January 1980, of which HEW held 46.7%, held 30.1%, and RWE held 23.2%. \n\nOverall and technical direction were the responsibility of HEW, while Schleswag dealt with commercial management. The basic contract between the partners of 3 January 1978 stipulated that after the project's conclusion, the installation was \"anticipated to be dismantled and scrapped\".\n\nThe partners as well as the BMFT also had political motives connected with the project. Günther Klätte, management board member of RWE, stated during a general business meeting: \"We require Growian [in the general sense of large wind turbines] as a proof of failure of concept\", and he noted that \"the Growian is a kind of pedagogical tool to convert the anti-nuclear energy crowd to the true faith\". A similar statement regarding the incurred financial burdens was reported of Minister of Finance and former Minister of Research Hans Matthöfer: \"We know it won't do anything for us. But we do it to demonstrate to the wind energy advocates that it doesn't work.\" After the Green Party had derided the installation as the electricity provider's \"fig leaf\" on the occasion of groundbreaking in May 1981, the RWE took internal measures to make sure that publicly a position of open-mindedness towards alternative energy production was emphasized while public interest in wind energy was allayed. \n\nInsuperable structural load and material problems occurred, not least due to the two-bladed lee runner configuration. The installation turned out to be a failure in most respects, spent substantially more time under repair than up and running, and was not even capable of sustained test operation. When it was decommissioned it had only logged a total of 420 hours in active operation.\n\nGrowian is regarded as one of the largest failures in the history of wind power and was unable to fulfill any of the expectations riding on its conception. What few insights were gained found little application in wind turbine construction. Some lessons were however learned from conceptional mistakes made in its construction, e.g., the futility of trying to reach profitable installation sizes without taking intermediate steps.\n\nThe point of view that multi-MW-yield wind turbines were technically and commercially infeasible gained some currency after the failure of the project, but was eventually superseded by technical progress. Beginning with the late 2000s, twenty-five years after Growian was decommissioned, installations with identical dimensions and yield (100 m rotor diameter, 3 MW net yield) were being produced in large numbers, a class of turbines that has continued to dominate the market and to push forward the mean net yield of newly installed turbines. As of 2015, substantially larger installations with yields up to 8 MW and rotor diameters of up to 170 m are present in the offshore sector. In contrast to Growian, however, these turbine types were incrementally developed from smaller installations in the 0.1 MW range. \n\nGrowian's former locations is still being used for wind power generation. In 1988, Germany's first wind farm, the , was built on a 20 hectare section of the former test area. It initially comprised 30 smaller turbines with net yields of 10-25 kW, provided by three different wind turbine manufacturers. After being repowered twice, the wind farm today consists of four major installations with 1-2 MW yield, in addition to a test area for small wind turbines and an information center presenting the history of wind power.\n\n"}
{"id": "6975327", "url": "https://en.wikipedia.org/wiki?curid=6975327", "title": "HVDC Russia–Finland", "text": "HVDC Russia–Finland\n\nThe HVDC Russia–Finland (also: Kernovo-Mussalo cable) was a project to build a HVDC submarine power cable between Kernovo, Leningrad Oblast (Russia) and Mussalo, Kotka (Finland). The main purpose of this project was to export Russian nuclear energy to Sweden and Finland.\n\nThe cable has been suggested originally back in the 1990s by the Russian State Nuclear Power Company Rosenergoatom. In 2004, Finland based company United Power Oy, controlled by Baltenergo, a subsidiary of Rosenergoatom, submitted an official application for the submarine cable and a converter station. On 21 December 2005, a preliminary agreement of 15-years of electricity supply was signed between United Power and BasEl, representing 16 Swedish and Finnish companies. \n\nIn December 2006, the Finnish Government rejected the project. In May 2007, United Power announced it will give up its effort to build an undersea electric cable from Russia to Finland and will look instead for direct link from Russia to Sweden across the Baltic Sea.\n\nAfter several years of efforts, the project was abandoned for political reasons. Politicians decided to drop support to this project in exchange to solve other bilateral issues, like the Nord Stream, Russian export duties on timber, or the leasing of the Saimaa Canal.\n\nIn January 2008, United Power filed for insolvency at the Kotka district court.\n\nThe submarine cable was to have a capacity of 1,000 MW for the transmission of up to 8.7 TWh of electricity per year. It was to consist of two ironclad cables at a distance of from each other, and one ground metal cable. It was to be linked with the Leningrad Nuclear Power Plant at Sosnovy Bor.\n\nThe overall cost of the project was estimated at €300 million. The financing agreement of the project was signed with Russia's state-run foreign economic bank Vnesheconombank (VEB) in June 2006. The pay-off period of the project was calculated to take six to nine years. The construction was to be completed in 2009–2010.\n\nThe main route proposal foresaw a cable from Kernovo in Russia to Mussalo in Finland. There were also alternative options. One possible option was to replace the 1000 MW cable with two 500 MW cables connecting Kernovo with different destinations in Finland. Other considered destinations in Finland were Loviisa, Sipoo, Espoo and Ingå.\n\nAfter rejection by the Finnish authorities, United Power prepared to apply a new application for a cable with a lower capacity of up to 450 MW. It also considered an alternative route from Vyborg in Russia to Lappeenranta in Finland. United Power and Baltenergo also tried to proceed with alternative projects to export Russian electricity to Finland through Estonia, or directly from Russia to Sweden. In February 2007, Baltenergo suggested an undersea power cable from Sosnovy Bor to Estonia instead of Finland and to sell electricity to the Nordic market through Estonia. In January 2007, United Power proposed a submarine cable from Russia directly to Sweden. This proposal was repeated by Baltenergo in May 2007. However, none of these proposals proceeded.\n\nUnited Power Oy was a Finnish-Russian energy company established in 2003 as a special purpose company for transferring electricity from Russia to Finland and other European countries. The shareholders of United Power were Baltenergo, Kotkan Energia and a consortium of private investors. Chairman of the Board was András Szép and Finnish members of the board were Jaakko Ihamuotila and Pertti Salolainen.\n\nAfter the construction permit was rejected by the Finnish Ministry of Trade and Industry, United Power suspended its activities. In January 2008, United Power filed for insolvency at the Kotka district court.\n\nThe project was backed by the Russian Government and supported by Finnish and Swedish industries. At the same time, the project was criticized by the Finnish national transmission grid operator Fingrid and also by some Russian energy companies. The Russian Federal Grid Company stated that there will not be enough electricity for export in the coming years as the Saint Petersburg area (Leningrad Oblast) is suffering from undercapacity, and the sea cable will worsen the current situation, as electricity would go abroad instead of to the Russian regions. The CEO of RAO UES Anatoly Chubais said that the project is unrealistic, and possibly even non-profitable. The Finnish concerns related to the Finnish grid ability to adapt the Russian power transmission and with the amount of necessary investments into transmission grid. Fingrid said that the regional grid in the southeast of Finland is operating at maximum capacity, and could not handle the additional power. According to the Finnish Mister of Trade and Industry Mauri Pekkarinen the undersea cable project would have required €1.5 billion in investments in strengthening the carrying capacity of the Finnish electricity grid. \n\nSome Nordic NGO's expressed a view that the power generated in Sosnovy Bor is not suitable to be used because this nuclear power plant is old-fashioned and could pose an environmental threat. \n\nUnited Power argued that the sea cable would increase competition at the Finnish energy market and decrease electricity prices by 6-8%. It also offered to build two gas-fired thermal power plants near at Sosnovy Bor with a total capacity of 900 MW as a reserve capacity. It also offered to consider alternative routes to decrease the necessity of upgrading Finnish transmission system.\n\n"}
{"id": "255447", "url": "https://en.wikipedia.org/wiki?curid=255447", "title": "Helmholtz free energy", "text": "Helmholtz free energy\n\nIn thermodynamics, the Helmholtz free energy is a thermodynamic potential that measures the useful work obtainable from a closed thermodynamic system at a constant temperature and volume. The negative of the change in the Helmholtz energy during a process is equal to the maximum amount of work that the system can perform in a thermodynamic process in which volume is held constant. If the volume were not held constant, part of this work would be performed as boundary work. This makes the Helmholtz energy useful for systems held at constant volume. Furthermore, at constant temperature, the Helmholtz energy is minimized at equilibrium.\n\nIn contrast, the Gibbs free energy or free enthalpy is most commonly used as a measure of thermodynamic potential (especially in chemistry) when it is inconvenient for applications that do not occur at constant pressure. For example, in explosives research Helmholtz free energy is often used, since explosive reactions by their nature induce pressure changes. It is also frequently used to define fundamental equations of state of pure substances.\n\nThe concept of free energy was developed by Hermann von Helmholtz, a German physician and physicist, and first presented in 1882 in a lecture called \"On the thermodynamics of chemical processes\". From the German word \"Arbeit\" (work), the International Union of Pure and Applied Chemistry (IUPAC) recommends the symbol \"A\" and the name \"Helmholtz energy\". In physics, the symbol \"F\" is also used in reference to \"free energy\" or \"Helmholtz function\".\n\nThe Helmholtz energy is defined as\n\nwhere\n\nThe Helmholtz energy is the Legendre transformation of the internal energy \"U\", in which temperature replaces entropy as the independent variable.\n\nThe first law of thermodynamics in a closed system provides\nwhere formula_3 is the internal energy, formula_4 is the energy added as heat, and formula_5 is the work done on the system. The second law of thermodynamics for a reversible process yields formula_6. In case of a reversible change, the work done can be expressed as formula_7 (ignoring electrical and other non-\"PV\" work):\n\nApplying the product rule for differentiation to d(\"TS\") = \"T\" d\"S\" + \"S\" d\"T\", it follows\nand\n\nThe definition of \"A\" = \"U\" − \"TS\" enables to rewrite this as \n\nBecause \"A\" is a thermodynamic function of state, this relation is also valid for a process (without electrical work or composition change) that is not reversible, as long as the system pressure and temperature are uniform.\n\nThe laws of thermodynamics are most easily applicable to systems undergoing reversible processes or processes that begin and end in thermal equilibrium, although irreversible quasistatic processes or spontaneous processes in systems with uniform temperature and pressure (u\"PT\" processes) can also be analyzed based on the fundamental thermodynamic relation as shown further below. First, if we wish to describe phenomena like chemical reactions, it may be convenient to consider suitably chosen initial and final states in which the system is in (metastable) thermal equilibrium. If the system is kept at fixed volume and is in contact with a heat bath at some constant temperature, then we can reason as follows.\n\nSince the thermodynamical variables of the system are well defined in the initial state and the final state, the internal energy increase formula_12, the entropy increase formula_13, and the total amount of work that can be extracted, performed by the system, formula_14, are well defined quantities. Conservation of energy implies\n\nThe volume of the system is kept constant. This means that the volume of the heat bath does not change either, and we can conclude that the heat bath does not perform any work. This implies that the amount of heat that flows into the heat bath is given by\n\nThe heat bath remains in thermal equilibrium at temperature \"T\" no matter what the system does. Therefore, the entropy change of the heat bath is\n\nThe total entropy change is thus given by\n\nSince the system is in thermal equilibrium with the heat bath in the initial and the final states, \"T\" is also the temperature of the system in these states. The fact that the system's temperature does not change allows us to express the numerator as the free energy change of the system:\n\nSince the total change in entropy must always be larger or equal to zero, we obtain the inequality\n\nWe see that the total amount of work that can be extracted in an isothermal process is limited by the free-energy decrease, and that increasing the free energy in a reversible process requires work to be done on the system. If no work is extracted from the system, then\n\nand thus for a system kept at constant temperature and volume and not capable of performing electrical or other non-\"PV\" work, the total free energy during a spontaneous change can only decrease.\n\nThis result seems to contradict the equation d\"A\" = −\"S\" d\"T\" − \"P\" d\"V\", as keeping \"T\" and \"V\" constant seems to imply d\"A\" = 0, and hence \"A\" = constant. In reality there is no contradiction: In a simple one-component system, to which the validity of the equation d\"A\" = −\"S\" d\"T\" − \"P\" d\"V\" is restricted, no process can occur at constant \"T\" and \"V\", since there is a unique \"P\"(\"T\", \"V\") relation, and thus \"T\", \"V\", and \"P\" are all fixed. To allow for spontaneous processes at constant \"T\" and \"V\", one needs to enlarge the thermodynamical state space of the system. In case of a chemical reaction, one must allow for changes in the numbers \"N\" of particles of each type \"j\". The differential of the free energy then generalizes to\n\nwhere the formula_23 are the numbers of particles of type \"j\", and the formula_24 are the corresponding chemical potentials. This equation is then again valid for both reversible and non-reversible u\"PT\" changes. In case of a spontaneous change at constant \"T\" and \"V\" without electrical work, the last term will thus be negative.\n\nIn case there are other external parameters, the above relation further generalizes to\n\nHere the formula_26 are the external variables, and the formula_27 the corresponding generalized forces.\n\nA system kept at constant volume, temperature, and particle number is described by the canonical ensemble. The probability to find the system in some energy eigenstate \"r\" is given by\n\nwhere\n\n\"Z\" is called the partition function of the system. The fact that the system does not have a unique energy means that the various thermodynamical quantities must be defined as expectation values. In the thermodynamical limit of infinite system size, the relative fluctuations in these averages will go to zero.\n\nThe average internal energy of the system is the expectation value of the energy and can be expressed in terms of \"Z\" as follows:\n\nIf the system is in state \"r\", then the generalized force corresponding to an external variable \"x\" is given by\n\nThe thermal average of this can be written as\n\nSuppose that the system has one external variable formula_35. Then changing the system's temperature parameter by formula_36 and the external variable by formula_37 will lead to a change in formula_38:\n\nIf we write formula_40 as\n\nwe get\n\nThis means that the change in the internal energy is given by\n\nIn the thermodynamic limit, the fundamental thermodynamic relation should hold:\n\nThis then implies that the entropy of the system is given by\n\nwhere \"c\" is some constant. The value of \"c\" can be determined by considering the limit \"T\" → 0. In this limit the entropy becomes formula_46, where formula_47 is the ground-state degeneracy. The partition function in this limit is formula_48, where formula_49 is the ground-state energy. Thus, we see that formula_50 and that\n\nCombining the definition of Helmholtz free energy\n\nalong with the fundamental thermodynamic relation\n\nformula_53\n\none can find expressions for entropy, pressure and chemical potential:\n\nThese three equations, along with the free energy in terms of the partition function,\n\nallow an efficient way of calculating thermodynamic variables of interest given the partition function and are often used in density of state calculations. One can also do Legendre transformations for different systems. For example, for a system with a magnetic field or potential, it is true that\n\nComputing the free energy is an intractable problem for all but the simplest models in statistical physics. A powerful approximation method is mean-field theory, which is a variational method based on the Bogoliubov inequality. This inequality can be formulated as follows.\n\nSuppose we replace the real Hamiltonian formula_57 of the model by a trial Hamiltonian formula_58, which has different interactions and may depend on extra parameters that are not present in the original model. If we choose this trial Hamiltonian such that\n\nwhere both averages are taken with respect to the canonical distribution defined by the trial Hamiltonian formula_58, then\n\nwhere formula_62 is the free energy of the original Hamiltonian, and formula_63 is the free energy of the trial Hamiltonian. By including a large number of parameters in the trial Hamiltonian and minimizing the free energy, we can expect to get a close approximation to the exact free energy.\n\nThe Bogoliubov inequality is often formulated in a slightly different but equivalent way. If we write the Hamiltonian as\n\nwhere formula_65 is exactly solvable, then we can apply the above inequality by defining\n\nHere we have defined formula_67 to be the average of \"X\" over the canonical ensemble defined by formula_65. Since formula_58 defined this way differs from formula_65 by a constant, we have in general\n\nTherefore,\n\nand thus the inequality\n\nholds. The free energy formula_63 is the free energy of the model defined by formula_65 plus formula_76. This means that\n\nand thus\n\nFor a classical model we can prove the Bogoliubov inequality as follows. We denote the canonical probability distributions for the Hamiltonian and the trial Hamiltonian by formula_79 and formula_80, respectively. From Gibbs' inequality we know that:\n\nholds. To see this, consider the difference between the left hand side and the right hand side. We can write this as:\n\nSince\n\nit follows that:\n\nwhere in the last step we have used that both probability distributions are normalized to 1.\n\nWe can write the inequality as:\n\nwhere the averages are taken with respect to formula_80. If we now substitute in here the expressions for the probability distributions:\n\nand\n\nwe get:\n\nSince the averages of formula_57 and formula_58 are, by assumption, identical we have:\n\nHere we have used that the partition functions are constants with respect to taking averages and that the free energy is proportional to minus the logarithm of the partition function.\n\nWe can easily generalize this proof to the case of quantum mechanical models. We denote the eigenstates of formula_58 by formula_94. We denote the diagonal components of the density matrices for the canonical distributions for formula_57 and formula_58 in this basis as:\n\nand\n\nwhere the formula_99 are the eigenvalues of formula_58\n\nWe assume again that the averages of H and formula_58 in the canonical ensemble defined by formula_58 are the same:\n\nwhere\n\nThe inequality\n\nstill holds as both the formula_79 and the formula_80 sum to 1. On the l.h.s. we can replace:\n\nOn the right-hand side we can use the inequality\n\nwhere we have introduced the notation\n\nfor the expectation value of the operator Y in the state r. See here for a proof. Taking the logarithm of this inequality gives:\n\nThis allows us to write:\n\nThe fact that the averages of H and formula_58 are the same then leads to the same conclusion as in the classical case:\n\nIn the more general case, the mechanical term (\"p\" d\"V\") must be replaced by the product of volume, stress, and an infinitesimal strain:\n\nwhere formula_116 is the stress tensor, and formula_117 is the strain tensor. In the case of linear elastic materials that obey Hooke's law, the stress is related to the strain by\n\nwhere we are now using Einstein notation for the tensors, in which repeated indices in a product are summed. We may integrate the expression for formula_119 to obtain the Helmholtz energy:\n\nThe Helmholtz free energy function for a pure substance (together with its partial derivatives) can be used to determine all other thermodynamic properties for the substance. See, for example, the equations of state for water, as given by the IAPWS in their IAPWS-95 release.\n\nHinton and Zelem \"derive an objective function for training autoencoders based on the minimum description length (MDL) principle\". \"The description length of an input vector using a particular code is the sum of the code cost and reconstruction cost. [They] define this to be the energy of the code, for reasons that will become clear later. Given an input vector, [they] define the energy of a code to be the sum of the code cost and the reconstruction cost.\" The true expected combined cost is\n\"which has exactly the form of Helmholtz free energy\".\n\n\n"}
{"id": "20114039", "url": "https://en.wikipedia.org/wiki?curid=20114039", "title": "Langmuir adsorption model", "text": "Langmuir adsorption model\n\nThe Langmuir adsorption model explains adsorption by assuming an adsorbate behaves as an ideal thing at isothermal conditions. At these conditions the adsorbate's partial pressure, formula_1, is related to the volume of it, , adsorbed onto a solid adsorbent. The adsorbent, as indicated in the figure, is assumed to be an ideal solid surface composed of series of distinct sites capable of binding the adsorbate. The adsorbate binding is treated as a chemical reaction between the adsorbate molecule formula_2 and an empty site, . This reaction yields an adsorbed complex formula_3 with an associated equilibrium constant formula_4:\n\nFrom these assumptions the Langmuir isotherm can be derived (see below), which states that it is good \n\nwhere formula_6 is the fractional occupancy of the adsorption sites, and formula_7 is the volume of the monolayer. A continuous monolayer of adsorbate molecules surrounding a homogeneous solid surface is the conceptual basis for this adsorption model.\n\nIn 1916, Irving Langmuir presented his model for the adsorption of species onto simple surfaces. Langmuir was awarded the Nobel Prize in 1932 for his work concerning surface chemistry. He hypothesized that a given surface has a certain number of equivalent sites to which a species can “stick”, either by physisorption or chemisorption. His theory began when he postulated that gaseous molecules do not rebound elastically from a surface, but are held by it in a similar way to groups of molecules in solid bodies.\n\nLangmuir published two papers that proved the assumption that adsorbed films do not exceed one molecule in thickness. The first experiment involved observing electron emission from heated filaments in gases. \nThe second, a more direct proof, examined and measured the films of liquid on an adsorbent surface layer. He also noted that generally the attractive strength between the surface and the first layer of adsorbed substance is much greater than the strength between the first and second layer. However, there are instances where the subsequent layers may condense given the right combination of temperature and pressure.\n\nInherent within this model, the following assumptions are valid specifically for the simplest case: the adsorption of a single adsorbate onto a series of equivalent sites on the surface of the solid.\n\n\nThis section provides a kinetic derivation for a single adsorbate case. The multiple adsorbate case is covered in the Competitive adsorption sub-section.\nThe model assumes adsorption and desorption as being elementary processes, where the rate of adsorption \"r\" and the rate of desorption \"r\" are given by\n\nwhere \"P\" is the partial pressure of \"A\" over the surface, [\"S\"] is the concentration of bare sites in number/m, [\"A\"] is the surface concentration of \"A\" in molecules/m, and \"k\" and \"k\" are constants of forward adsorption reaction and backward desorption reaction in the above reactions.\n\nAt equilibrium, the rate of adsorption equals the rate of desorption. Setting \"r\" = \"r\" and rearranging, we obtain\n\nThe concentration of sites is given by dividing the total number of sites (S_0) by the area of the adsorbate (a):\n\nWe can then calculate the concentration of all sites by summing the concentration of free sites [\"S\"] and occupied sites:\n\nCombining this with the equilibrium equation, we get\n\nWe define now the fraction of the surface sites covered with \"A\", θ, as\n\nThis, applied to the previous equation that combined site balance and equilibrium, yields the Langmuir adsorption isotherm:\n\nThis derivation\nwas originally provided by Volmer and Mahnert in 1925. The partition function of the finite number of adsorbents adsorbed on a surface, in a canonical ensemble, is given by\n\nwhere formula_17 is the partition function of a single adsorbed molecule, formula_18 is the number of sites available for adsorption, and formula_19 is the number of molecules adsorbed and should be less than or equal to formula_18. The terms in the bracket give the total partition function of the formula_19 adsorbed molecules by taking a product of the individual partition functions (refer to Partition function of subsystems). The formula_22 factor accounts for the overcounting arising due to the indistinguishable nature of the adsorption sites. The grand canonical partition function is given by\n\nformula_24 is the chemical potential of adsorbed molecule. As it has the form of binomial series, the summation is reduced to\nwhere formula_26\n\nThe grand potential is\n\nbased on which the average number of occupied sites is calculated\n\nwhich gives the coverage \n\nNow, invoking the condition that the system is in equilibrium, that is, the chemical potential of the adsorbed molecules is equal to that of the molecules in gas phase, we have\n\nThe chemical potential of an ideal gas is\n\nwhere formula_32 is the Helmholtz free energy of an ideal gas with its partition function\n\nformula_34 is the partition function of a single particle in the volume of formula_35 (only consider the translational freedom here).\n\nWe thus have formula_37.\n\nPlugging formula_38 to the expression of formula_39, we have\n\nwhich gives the coverage\n\nBy defining\n\nand using the identity formula_43, finally, we have\n\nIt is plotted in the figure alongside demonstrating that the surface coverage increases quite rapidly with the partial pressure of the adsorbants, but levels off after \"P\" reaches \"P\".\n\nThe previous derivations assumes that there is only one species, \"A\", adsorbing onto the surface. This section considers the case when there are two distinct adsorbates present in the system. Consider two species \"A\" and \"B\" that compete for the same adsorption sites. The following assumptions are applied here:\n\nAs derived using kinetical considerations, the equilibrium constants for both \"A\" and \"B\" are given by\n\nand\n\nThe site balance states that the concentration of total sites [\"S\"] is equal to the sum of free sites, sites occupied by \"A\" and sites occupied by \"B\":\n\nInserting the equilibrium equations and rearranging in the same way we did for the single-species adsorption, we get similar expressions for both θ and θ:\n\nThe other case of special importance is when a molecule \"D\" dissociates into two atoms upon adsorption. Here, the following assumptions would be held to be valid:\n\nUsing similar kinetic considerations, we get\n\nThe 1/2 exponent on \"p\" arises because one gas phase molecule produces two adsorbed species. Applying the site balance as done above,\n\nThe formation of Langmuir monolayers by adsorption onto a surface dramatically reduces the entropy of the molecular system. This conflicts with the second law of thermodynamics, which states that entropy will increase in an isolated system. This implies that either another locally active force is stronger than the thermodynamic potential, or that our expression of the entropy of the system is incomplete.\n\nTo find the entropy decrease, we find the entropy of the molecule when in the adsorbed condition.\n\nUsing Stirling's approximation, we have\n\nOn the other hand, the entropy of a molecule of an ideal gas is\n\nwhere formula_58 is the thermal de Broglie wavelength of the gas molecule.\n\nThe Langmuir adsorption model deviates significantly in many cases, primarily because it fails to account for the surface roughness of the adsorbent. Rough inhomogeneous surfaces have multiple site-types available for adsorption, with some parameters varying from site to site, such as the heat of adsorption. Moreover, specific surface area is a scale dependent quantity and no single true value exists for this parameter. Thus, the use of alternative probe molecules will often result in different obtained numerical values for surface area, rendering comparison problematic.\n\nThe model also ignores adsorbate/adsorbate interactions. Experimentally, there is clear evidence for adsorbate/adsorbate interactions in heat of adsorption data. There are two kinds of adsorbate/adsorbate interactions: direct interaction and indirect interaction. Direct interactions are between adjacent adsorbed molecules, which could make adsorbing near another adsorbate molecule more or less favorable and greatly affects high-coverage behavior. In indirect interactions, the adsorbate changes the surface around the adsorbed site, which in turn affects the adsorption of other adsorbate molecules nearby.\n\nThe modifications try to account for the points mentioned in above section like surface roughness, inhomogeneity, and adsorbate-adsorbate interactions.\n\nThe Freundlich isotherm is the most important multisite adsorption isotherm for rough surfaces.\n\nwhere \"α\" and \"C\" are fitting parameters. This equation implies that if one makes a log-log plot of adsorption data, the data will fit a straight line. The Freundlich isotherm has two parameters while Langmuir's equations has only one: as a result, it often fits the data on rough surfaces better than the Langmuir's equations.\n\nA related equation is the \"Toth equation\". Rearranging the Langmuir equation, one can obtain:\n\nToth modified this equation by adding two parameters, \"α\" and \"C\" to formulate the Toth equation:\n\nThis isotherm takes into accounts of indirect adsorbent-adsorbate interactions on adsorption isotherms. Temkin noted experimentally that heats of adsorption would more often decrease than increase with increasing coverage.\n\nThe heat of adsorption \"ΔH\" is defined as:\n\nHe derived a model assuming that as the surface is loaded up with adsorbate, the heat of adsorption of all the molecules in the layer would decrease linearly with coverage due to adsorbent/adsorbate interactions:\n\nwhere \"α\" is a fitting parameter. Assuming the Langmuir Adsorption isotherm still applied to the adsorbed layer, formula_64 is expected to vary with coverage, as follows:\n\nLangmuir's isotherm can be rearranged to this form:\n\nSubstituting the expression of the equilibrium constant and taking the natural logarithm:\n\nBrunauer, Emmett and Teller derived the first isotherm for multilayer adsorption. It assumes a random distribution of sites that are empty or that are covered with by one monolayer, two layers and so on, as illustrated alongside. The main equation of this model is:\n\nwhere\n\nformula_69\n\nand \"[A]\" is the total concentration of molecules on the surface, given by:\n\nwhere\n\nformula_71\n\nin which \"[A]\" is the number of bare sites, and \"[A]\" is the number of surface sites covered by \"i\" molecules.\n\nThis section describes the surface coverage when the adsorbate is in liquid phase and is a binary mixture\n\nFor ideal both phases - no lateral interactions, homogeneous surface - the composition of a surface phase for a binary liquid system in contact with solid surface is given by a classic Everett isotherm equation (being a simple analogue of Langmuir equation), where the components are interchangeable (i.e. \"1\" may be exchanged to \"2\") without change of eq. form:\n\nwhere the normal definition of multicomponent system is valid as follows :\nBy simple rearrangement, we get \n\nThis equation describes competition of components \"1\" and \"2\".\n\n\n\n"}
{"id": "1883888", "url": "https://en.wikipedia.org/wiki?curid=1883888", "title": "Live steam", "text": "Live steam\n\nLive steam is steam under pressure, obtained by heating water in a boiler. The steam is used to operate stationary or moving equipment.\n\nA live steam machine or device is one powered by steam, but the term is usually reserved for those that are replicas, scale models, toys, or otherwise used for heritage, museum, entertainment, or recreational purposes, to distinguish them from similar devices powered by electricity or some other more convenient method but designed to look as if they are steam-powered. Revenue-earning steam-powered machines such as mainline and narrow gauge steam locomotives, steamships, and power-generating steam turbines are not normally referred to as \"live steam\".\n\nSteamrollers and traction engines are popular, in 1:4 or 1:3 scale, as are model stationary steam engines, ranging from pocket-size to 1:2 scale.\n\nRidable, large-scale live steam railroading on a backyard railroad is a popular aspect of the live steam hobby, but it is time-consuming to build a locomotive from scratch and it can be costly to purchase one already built. Garden railways, in smaller scales (that cannot pull a \"live\" person nor be ridden on), offer the benefits of real steam engines (and at lower cost and in less space), but do not provide the same experience as operating one's own locomotive in the larger scales and riding on (or behind) it, while doing so.\n\nOne of the most famous live steam railroads was Walt Disney's Carolwood Pacific Railroad around his California home; it later inspired Walt Disney to surround his planned Disneyland amusement park with a working, narrow gauge railroad.\n\nThe live steam hobby is especially popular in the UK, US, and Australia. All over the world, there are hundreds of clubs and associations as well as many thousands of private backyard railroads. The world's largest live steam layout, with over of trackage is \"Train Mountain Railroad\" in Chiloquin, Oregon. Other notable layouts are operated by the Los Angeles Live Steamers Railroad Museum and the Riverside Live Steamers.\n\nAlthough not technically live steam, the hobby embraces other scale model locomotives\nwhose prototypes are diesel (gasoline) and electric (battery). A few tracks restrict operation to only\nlive steam engines, such as the Riverside Live Steamers mentioned above, but a visit to most\ntracks will reveal a mixture of locomotive types.\n\nA live steam locomotive is often an exact, hand-crafted scale model. Live steam railroad scales are generally referred to by the number of inches of scale per foot. For example, a 1:8 scale locomotive will often be referred to as a 1½\" scale locomotive. Common modelling scales are Gauge 1 (), 1/2\" (), 3/4\" (1:16), 1\" (1:12), 1½\" (1:8), 2½\" (~1:5) and 3\" (1:4).\n\nTrack gauge refers to the distance between the rails. The ridable track gauges range from to , the most popular being \", \", 5\", \" and \" (see Rail transport modelling scales). Gauges from and up are called \"Miniature Railways\" (in the US these are known as \"Grand Scale Railroads\"), and are used mostly in amusement park rides and commercial settings.\n\nOften the gauge has little to do with the scale of a locomotive since larger equipment can be built in a narrow gauge railway configuration. For instance, scales of 1.5, 1.6, 2.5, and 3 inches per foot (corresponding to scales of 1:8 to 1:4) have been used on a track gauge.\n\nThe generally accepted smallest gauge for a live steam locomotive is Gauge 1, although O scale live steam models are also common. Producing smaller-scale models remains problematic, as the laws of physics do not themselves scale: creating a small-scale boiler that produces useful quantities of steam requires careful engineering. Hornby Railways has produced commercial live steam-powered locomotives in OO scale by utilising an electrically heated boiler mounted in the tender, with cylinders in the locomotive, and control provided by electrical- signals fed through the track from a remote control unit. They are less mechanically realistic than models in larger scales; the visible valve gear is a dummy, as on the electric-motor-powered models, and steam admission to the cylinders is controlled by a rotary-valve servo inside the boiler casing, which is also a dummy. Nevertheless, the locomotive is driven by steam that is created on board the locomotive and is hence a genuine \"steam\" locomotive.\n\nIt is technically possible to build even smaller operating steam engines. Hand-made examples, as small as Z scale (1:220), with a gauge of only , have been produced. These are fired with a butane flame from a burner in the engine's tender. AA Sherwood of Australia, an engineering lecturer, produced some miniature scale model live steam engines in the late 1960s and early 1970s. His smallest live steam engines were 1:240 scale which is smaller than the 1:220 of Z Scale. The smallest scale Sherwood worked in was 1:480, though that was not live steam.\n\nA wide variety of boiler designs are available, ranging from simple externally fired pot boilers to sophisticated multi-flue internally fired boilers and even superheater boilers usually found only on larger, more complex models.\n\nFor basic locomotive models, a simple valve gear can be used, with the reversing (if any) performed by a valve, or by using a \"slip\" eccentric.\n\nMore complex locomotive models can use valve gear similar to real steam machine with the reversing done mechanically, most frequently the Walschaerts type.\n\nThere are several common fuels used to boil water in live steam models:\n\n\nLive steam road vehicles are popular with model engineers because they are not restricted to running on tracks or water and can be easily transported for rallies and exhibitions. They include traction engines & rollers, wagons, cars, road-making & agricultural machines, often seen with ancillary equipment like threshing machines.\n\nMost types of boats and ships that were powered by steam in real life can be found as live steam ship models. These include, amongst others, speed boats, launches, tugboats, ocean liners, warships, paddle steamers and freight carriers. A specialized type is the tethered hydroplane. When steam-powered, these often have flash boilers.\n\nStationary engines tend to be less popular with modelers than mobile engines; probably because they are less easily transportable. They are more popular with toy makers. They can be anything from small farm engines to winding engines and mill engines.\n\nIn the late 19th and early-mid 20th centuries, live steam toys were extremely popular, with some large manufacturers like Bing selling hundreds of different models in large quantities. There were very many smaller manufacturers all over the world. Some of these, like Mamod, Wilesco and Jensen, are still in business making live steam toys, although they are now mainly marketed as collectables and novelties rather than toys. Toys tend to be less accurate representations of real life equipment than are models and many are somewhat generic in nature. The range includes all those seen as models and some of a purely novelty kind.\n\nA live steam festival (often called a \"Steam Fair\" in the UK and a live steam \"meet\" in the US) is a gathering of people interested in steam technology. Locomotives, trains, traction engines, steam rollers and tractors, steam boats and cars, and stationary steam engines may be on display, both full-sized and in miniature. Rides may also be offered.\n\nThere are several magazines devoted to the live steam hobby:\n\n\"Model Engineer\" is an English publication that is published twice a month and was founded in 1898. Most locomotive articles have an \"English\" flavour popular in the UK, Australia, New Zealand and South Africa. The magazine is aimed at constructors but also covers non-steam and non-rail model engineering interests.\n\n\"Live Steam & Outdoor Railroading\" is a U.S. magazine, founded in 1966 and devoted to the live steam hobby, as well as to other uses of miniature and full-size steam. Originally, it was a mimeographed newsletter, but soon expanded into magazine format. In 2005, the name was changed from \"Live Steam\". It is currently published bi-monthly, with a press run of slightly over 10,000 (Dec. 2004).\n\nA more recent publication, launched in 2006, is \"The Home Railway Journal\", which is specifically aimed at enthusiasts with ride-on railways (although not just steam-powered) in North America. It is published quarterly in Sacramento, California, US.\n\n\n"}
{"id": "16055188", "url": "https://en.wikipedia.org/wiki?curid=16055188", "title": "Mahindra Electric", "text": "Mahindra Electric\n\nMahindra Electric Mobility Limited, formerly known as the Reva Electric Car Company, is an Indian company based in Bangalore, involved in designing and manufacturing of compact electric vehicles. The company's first vehicle was the REVAi electric car, available in 26 countries with more than 4,000 of its different versions sold worldwide by mid March 2011. Reva was acquired by Indian conglomerate Mahindra & Mahindra in May 2010. After the acquisition, the company launched the electric hatchback e2o in 2013. Today, the company sells electric vehicles in different segments – the electric CitySmart hatchback e2oPlus, the electric sedan eVerito and electric commercial vehicle eSupro (passenger and cargo).\n\nThe Reva Electric Car Company (RECC) was founded in 1994 by Chetan Maini, as a joint venture between the Maini Group of Bangalore and Amerigon Electric Vehicle Technologies (AEVT Inc.) of the USA. The company's sole aim was to develop and produce an affordable compact electric car. Several other automakers were also aiming to do so, but in 2001 RECC launched the REVA.\n\nREVA was an acronym for \"Revolutionary Electric Vehicle Alternative\".\n\nRECC joined up with several automotive experts to develop components for REVA. Curtis Instruments, Inc. of USA developed a Motor Controller specifically for the car. The car had a power pack for which Tudor India Limited supplied customized Prestolite batteries. The Charger for Reva was developed by Modular Power Systems of USA (a division of TDI Power). Later, RECC started manufacturing the charger themselves through a technical collaboration agreement between MPS and the Maini Group.\n\nIn 2004 GoinGreen of the UK entered into an agreement with RECC to import REVA cars and market them under the G-Wiz moniker.\n\nIn 2008 a revamped REVA model was launched called the REVAi. The company started production of a Lithium-ion variant called the REVA L-ion in 2009.\n\nIn 2009 at the Frankfurt Motor Show, Reva presented its future models Reva NXR and Reva NXG. During the event Reva and General Motors India declared a technical collaboration to develop affordable EV for the Indian market. As a result of this General Motors India announced an electric version of their hatchback in the New Delhi Auto Expo 2010: named the e-Spark, Reva was to provide battery technology.\n\nOn 26 May 2010, India's largest sports utility vehicles and tractor maker Mahindra & Mahindra bought a 55.2% controlling stake in Reva. Following the deal, the company was renamed Mahindra Reva Electric Vehicles Private Limited. Mahindra’s president of automotive business, Pawan Goenka, became the new company’s chairman. As a result of the ownership change General Motors pulled out of the tie-up with Reva that was to produce the e-spark.\n\nIn Feb 2011 GoinGreen, the UK's exclusive importer of the G-Wiz, announced that it was no longer stocking the model (although it would order them on a 4-6-week lead time when requested by customers).\n\nIn 2016, the company was rebranded as Mahindra Electric Mobility Ltd. With the intention to reflect not just the business line of producing vehicles, but also developing powertrains and integrated mobility solutions.\n\nMahindra Reva currently produces two versions of the REVAi, an urban electric micro-car seating two adults and two children:\n\n\nThe REVA went on sale in India in 2001 and in the UK since 2003. The different versions of the REVA have sold more than 4,000 vehicles worldwide by mid March 2011 and is also available in the following countries: Bhutan, Brazil, Chile, Colombia, Costa Rica, Cyprus, France, Germany, Greece, Hungary, Iceland, Ireland, Japan, Malta, Monaco, Nepal, Norfolk Islands, Norway, Peru, the Philippines, Portugal, Spain, and Sri Lanka. The REVA is exempt from most European crash test rules, because its low weight and power registers it in the European \"heavy quadricycle\" category instead of the \"car\" category.\n\nIn 2005, Reva showcased the REVA-NXG, a two-seater roadster concept car with a nominal range of per charge and a top speed of .\nThe Mahindra e2o, previously REVA NXR, is an urban electric car hatchback manufactured by the Mahindra Group. The e2o is the REVA G-Wiz successor and was developed using REVA's technology. The REVA NXR electric concept car was unveiled at the 2009 Frankfurt Motor Show. Export production was initially scheduled for 2012. Production was initially scheduled for late 2010 with deliveries slated for early 2011. The e2o was launched in India in March 2013 at a price of Rs 596,000 () after a 29% government subsidy granted by the state of Delhi. The e2o was also launched in Mumbai, Bangalore, Pune, Ahmedabad, Hyderabad, Chandigarh, and Kochi. Mahindra also launched the vehicle in the UK but later, in May 2017 withdrew from the market.\n\nThe electric car has a lithium-ion battery pack that takes five hours for a full charge, and with a weight of , delivers a range of and a top speed of . The product was eventually pulled from the market following the launch of its four door successor.\n\nPrior to the Mahindra acquisition, Reva had partnered with Bannon Automotive to set up an assembly plant in upstate New York to produce the NXR for the US market.\n\nFormula E car was showcased by Mahindra Electric, as a part of the Mahindra group in the 2014 Auto Expo at Delhi. Mahindra Electric team worked closely with the Mahindra Racing, the racing division of Mahindra Group, to get the Formula E cars ready for the Inaugural edition of Formula E World Championship which took place at Beijing in September 2014. Mahindra Racing Team is the only Indian team to race in Inaugural Formula E championship. Following a successful first ever Formula E race in Beijing, Mahindra Racing explored the possibility of bringing the electrically-powered car series to India in the 2016-2017 season.\n\nElectric vehicle industry in India\n"}
{"id": "22751476", "url": "https://en.wikipedia.org/wiki?curid=22751476", "title": "Meredith effect", "text": "Meredith effect\n\nThe Meredith effect is a phenomenon whereby the aerodynamic drag produced by a cooling radiator may be offset by careful design of the cooling duct such that useful thrust is produced. The effect was discovered in the 1930s and became more important as the speeds of piston-engined aircraft increased over the next decade.\n\nThe Meredith effect occurs when air flowing through a duct is heated by a heat-exchanger or radiator containing a hot working fluid such as ethylene glycol. Typically the fluid is a coolant carrying waste heat from an internal combustion engine.\n\nFor the effect to occur, the duct must be travelling at a significant speed with respect to the air. Air flowing into the duct meets drag resistance from the radiator surface and is compressed due to the ram air effect. As the air flows through the radiator it is heated, raising its temperature slightly and further increasing its volume. The hot, pressurised air then exits through the exhaust duct which is shaped to be convergent, i.e. to narrow towards the rear. This accelerates the air backwards and the reaction of this acceleration against the installation provides a small forward thrust. The air expands and decreases temperature as it passes along the duct, before emerging to join the external air flow. Thus, the three processes of an open Brayton cycle are achieved: compression, heat addition at constant pressure and expansion. The thrust obtainable depends upon the pressure ratio between the inside and outside of the duct and the temperature of the coolant. The higher boiling point of ethylene glycol compared to water allows the air to attain a higher temperature increasing the specific thrust. \n\nIf the generated thrust is less than the aerodynamic drag of the ducting and radiator, then the arrangement serves to reduce the net aerodynamic drag of the radiator installation. If the generated thrust exceeds the aerodynamic drag of the installation, then the entire assemblage contributes a net forward thrust to the vehicle.\n\nF. W. Meredith was a British engineer working at the Royal Aircraft Establishment (RAE), Farnborough. Reflecting on the principles of liquid cooling, he realized that what was conventionally regarded as waste heat, to be transferred to the atmosphere by a coolant in a radiator, need not be lost. The heat adds energy to the airflow and, with careful design, this may be used to generate thrust. The work was published in 1936.\n\nThe phenomenon became known as the \"Meredith effect\" and was quickly adopted by the designers of prototype fighter aircraft then under development, including the Supermarine Spitfire and Hawker Hurricane whose Rolls-Royce PV-12 engine, later named the Merlin, was cooled by ethylene glycol. An early example of a Meredith effect radiator was incorporated in the design of the Spitfire for the first flight of the prototype on 5 March 1936.\n\nMany engineers did not understand the operating principles of the effect. A common mistake was the idea that the air-cooled radial engine would benefit most, because its fins ran hotter than the radiator of a liquid-cooled engine, with the mistake persisting even as late as 1949.\n\nThe North American P-51 Mustang, which first flew in 1940, adopted both the Merlin engine and the Meredith principle.\n\nAround this time the Meredith effect also inspired early American work on the aero-thermodynamic duct or ramjet, due to the similarity of their principles of operation.\n\nThe Meredith effect was incorporated into the AIAA award-winning design of the Cratus racing airplane by University of Kansas aerospace engineering student Samantha Schueler in 2012.\n\n"}
{"id": "31804393", "url": "https://en.wikipedia.org/wiki?curid=31804393", "title": "Micathermic heater", "text": "Micathermic heater\n\nA micathermic heater is a type of space heater in which the heating element is covered in thin sheets of mica. Micathermic heaters produce both convection heat and radiant heat, and are usually thinner than other types of heaters. The 1500 watt SoleusAir 360 Micathermic Heater was recalled in 2008 due to it being a fire hazard.\n"}
{"id": "17114734", "url": "https://en.wikipedia.org/wiki?curid=17114734", "title": "Micropump", "text": "Micropump\n\nMicropumps are devices that can control and manipulate small fluid volumes. Although any kind of small pump is often referred to as micropump, a more accurate definition restricts this term to pumps with functional dimensions in the micrometer range. Such pumps are of special interest in microfluidic research, and have become available for industrial product integration in recent years. Their miniaturized overall size, potential cost and improved dosing accuracy compared to existing miniature pumps fuel the growing interest for this innovative kind of pump.\n\nNote that the below text is very incomplete in terms of providing a good overview of the different micropump types and applications, and therefore please refer to good review articles on the topic.\n\nFirst true micropumps were reported in the mid-1970s, but attracted interest only in the 1980s, when Jan Smits and Harald Van Lintel developed MEMS micropumps. Most of the fundamental MEMS micropump work was done in the 1990s. More recently, efforts have been made to design non-mechanical micropumps that are functional in remote locations due to their non-dependence on external power.\nWithin the microfluidic world, physical laws change their appearance. As an example, volumetric forces, such as weight or inertia, often become negligible, whereas surface forces can dominate fluidical behaviour, especially when gas inclusion in liquids is present. With only a few exceptions, micropumps rely on micro-actuation principles, which can reasonably be scaled up only to a certain size.\n\nMicropumps can be grouped into mechanical and non-mechanical devices. Mechanical systems contain moving parts, which are usually actuation and microvalve membranes or flaps. The driving force can be generated by utilizing piezoelectric, electrostatic, thermo-pneumatic, pneumatic or magnetic effects. Non-mechanical pumps function with electro-hydrodynamic, electro-osmotic, electrochemical or ultrasonic flow generation, just to name a few of the actuation mechanisms that are currently studied.\n\nA diaphragm micropump uses the repeated actuation of a diaphragm to drive a fluid. The membrane is positioned above a main pump valve, which is centered between inlet and outlet microvalves. When the membrane is deflected upwards through some driving force, fluid is pulled into the inlet valve into the main pump valve. The membrane is then lowered, expelling the fluid through the outlet valve. This process is repeated to pump fluid continuously.\n\nA peristaltic micropump is a micropump composed of at least three microvalves in series. These three valves are opened and closed sequentially in order to pull fluid from the inlet to the outlet in a process known as peristalsis.\n\nStatic valves are defined as valves which have fixed geometry without any moving parts. These valves provide flow rectification through addition of energy (active) or inducing desired flow behavior by fluid inertia (passive). Two most common types of static geometry passive valves are Diffuser-Nozzle Elements and Tesla valves. Micropumps having nozzle-diffuser elements as flow rectification device are commonly known as Valveless Micropumps.\n\nIn microfluidics, capillary pumping plays an important role because the pumping action does not require external actuation power. Glass capillaries and porous media, including nitrocellulose paper and synthetic paper, can be integrated into microfluidic chips. Capillary pumping is widely used in lateral flow testing. Recently, novel capillary pumps, with a constant pumping flow rate independent of the liquid viscosity and surface energy, were developed, which have a significant advantage over the traditional capillary pump (of which the flow behaviour is Washburn behaviour, namely the flow rate is not constant) because their performance does not depend on the sample viscosity.\n\nChemically powered non-mechanical pumps have been fabricated by affixing nanomotors to surfaces, driving fluid flow through chemical reactions. A wide variety of pumping systems exist including biological enzyme based pumps, organic photocatalyst pumps, and metal catalyst pumps. These pumps generate flow through a number of different mechanisms including self-diffusiophoresis, electrophoresis, bubble propulsion and the generation of density gradients. Moreover, these chemically powered micropumps can be used as sensors for the detection of toxic agents.\n\nMicropumps have potential industrial applications, such as delivery of small amounts of glue during manufacturing processes, and biomedical applications, including portable or implanted drug delivery devices. Bio-inspired applications include a flexible electromagnetic micropump using magnetorheological elastomer to replace lymphatic vessels. Chemically powered micropumps also demonstrate potential for applications in chemical sensing in terms of detecting chemical warfare agents and environmental hazards, such as mercury and cyanide. \n\n"}
{"id": "2272780", "url": "https://en.wikipedia.org/wiki?curid=2272780", "title": "Paperweight", "text": "Paperweight\n\nA paperweight is a small solid object heavy enough, when placed on top of papers, to keep them from blowing away in a breeze or from moving under the strokes of a painting brush (as with Japanese calligraphy). While any object (like a stone) can serve as a paperweight, decorative paperweights of glass are produced, either by individual artisans or factories, usually in limited editions, and are collected as works of fine glass art, some of which are exhibited in museums. First produced in about 1845, particularly in France, such decorative paperweights declined in popularity before undergoing a revival in the mid-twentieth century.\n\nDecorative glass paperweights have a flat or slightly concave base, usually polished but sometimes frosted, cut in one of several variations (e.g. star-cut bases have a multi-pointed star, while a diamond cut base has grooves cut in a criss-cross pattern), although a footed weight has a flange in the base. The ground on which the inner parts rest may be clear or colored, made of unfused sand, or resemble lace (latticinio). The domed top is usually faceted or cut and made of lead glass and may be coated with one or more thin layers of colored glass, and have windows cut through it to reveal the interior motif. The exact shape or profile of the dome varies from one artist or factory to another, but in fine examples will act as a lens that, as one moves the weight about, attractively various the inner design's appearance. A magnifying glass is often used to gain appreciation of the fine detail of the work within. In a modern piece, an identifying mark and date are imperative.\n\nPaperweights are made by individual artisans or in factories where many artists and technicians collaborate; both may produce inexpensive as well as \"collector\" weights. \n\nWorkmanship, design, rarity, and condition determine a paperweight's value: its glass should not have a yellow or greenish cast, and there should be no unintentional asymmetries, or unevenly spaced or broken elements. Visible flaws, such as bubbles, striations and scratches lessen the value. \n\nAntique paperweights, of which perhaps 10,000 or so survive (mostly in museums), generally appreciate steadily in value; as of August 2018 the record price was the $258,500 paid in 1990 for an antique French weight.\n\nAntique paperweights were made in the \"classic\" years between 1845 and 1860 primarily in three French factories named Baccarat, St. Louis, and Clichy. Together, they made between 15,000 and 25,000 weights in the classic period. Weights (mainly of lesser quality) were also made in the United States, United Kingdom, and elsewhere, though Bacchus (UK) and New England Glass Company (US) produced some that equaled the best of the French. Modern weights have been made from about 1950 to the present.\n\nIn the US, Charles Kaziun started in 1940 to produce buttons, paperweights, inkwells and other bottles, using lamp-work of elegant simplicity. In Scotland, the pioneering work of Paul Ysart from the 1930s onward preceded a new generation of artists such as William Manson, Peter McDougall, Peter Holmes and John Deacons. A further impetus to reviving interest in paperweights was the publication of Evangiline Bergstrom's book, \"Old Glass Paperweights\", the first of a new genre.\n\nA number of small studios appeared in the middle 20th century, particularly in the US. These may have several to some dozens of workers with various levels of skill cooperating to produce their own distinctive \"line\". Notable examples are Lundberg Studios, Orient and Flume, Correia Art Glass, St.Clair, Lotton, and Parabelle Glass.\n\nStarting in the late 1960s and early 70s, artists such as Francis Whittemore, Paul Stankard, his former assistant Jim D'Onofrio, Chris Buzzini, Delmo and daughter Debbie Tarsitano, Victor Trabucco and sons, Gordon Smith, Rick Ayotte and his daughter Melissa, the father and son team of Bob and Ray Banford, and Ken Rosenfeld began breaking new ground and were able to produce fine paperweights rivaling anything produced in the classic period.\n\nCollectors may specialize in one of several types of paperweights, but more often they wind up with an eclectic mix.\n\nMillefiori (Italian - \"thousand flowers\") paperweights contain thin cross-sections of cylindrical composite canes made from colored rods and usually resemble little flowers, although they can be designed after anything, even letters and dates. These are usually made in a factory setting. They exist in many variations such as scattered, patterned, close concentric or carpet ground. Sometimes the canes are formed into a sort of upright tuft shaped like a mushroom that is encased in the dome. The year of manufacture is sometimes enclosed in one of the canes.\n\nLampwork paperweights have objects such as flowers, fruit, butterflies or animals constructed by shaping and working bits of colored glass with a gas burner or torch and assembling them into attractive compositions, which are then incorporated into the dome. This is a form particularly favored by studio artists. The objects are often stylized, but may be highly realistic.\n\nSulfide paperweights have an encased cameo-like medallion or portrait plaque made from a special ceramic that is able to reproduce very fine detail. These are known as incrustations, cameo incrustations, or sulphides. They often are produced to commemorate some person or event. From the late 1700s through the end of the 1900s, an amazing variety of glass objects, including paperweights, were made with incrustations. The finest collection of incrustations ever assembled was by Paul Jokelson, collector, author and founder of the Paperweight Collectors' Association. A part of his collection was gifted to the Corning Museum of Glass, with the remaining portion being sold in London in the 1990s. Although still produced today, their heyday was before the classic period.\n\nMost paperweights, which are considered works of art, use one of the above techniques; millefiori, lampwork or sulphide — all techniques that had been around long before the advent of paperweights. A fourth technique, a crimp flower, usually a rose, originated in the Millville, New Jersey area in the first decade of the twentieth century. Often called a Millville rose, these weights range from simple folk art to fine works of art, depending on the maker. \n\nFine weights not made with any of the major techniques include swirls, marbries and crowns. Swirl paperweights have opaque rods of two or three colors radiating like a pinwheel from a central millefiori floret. A similar style, the marbrie, is a paperweight that has several bands of color close to the surface that descend from the apex in a looping pattern to the bottom of the weight. Crown paperweights have twisted ribbons, alternately colored and white filigree which radiate from a central millefiori floret at the top, down to converge again at the base. This was first devised in the Saint Louis factory and remains popular today.\nMiniature weights have a diameter of less than two inches or so, and magnums have a diameter greater than about 3.25 inches.\n\nCalifornia-style paperweights are made by \"painting\" the surface of the dome with colored molten glass (torchwork), and manipulated with picks or other tools. They may also be sprayed while hot with various metallic salts to achieve an iridescent look.\n\nVictorian portrait and advertising paperweights were dome glass paperweights first made in Pittsburgh, Pennsylvania using a process patented in 1882 by William H. Maxwell. The portrait paperweights contained pictures of ordinary people reproduced on a milk glass disk and encased within clear glass. This same process was also used to produce paperweights with the owner's name encased or an advertisement of a business or product. Pittsburgher Albert A. Graeser, patented a different process for making advertising paperweights in 1892. The Graeser process involved sealing an image to the underside of a rectangular glass blank using a milk glass or enamel-like glaze. Many paperweights of the late 19th century are marked either J. N. Abrams or Barnes and Abrams and may list either the 1882 Maxwell or 1892 Graeser patent date. It has been theorized that Barnes and Abrams did not actually manufacture advertising paperweights for their customers, but instead subcontracted the actual manufacturing task out to Pittsburgh area glasshouses. The Paperweight Collectors Association Annual Bulletins published in 2000, 2001 and 2002 describe these in detail.\n\nBohemian paperweights were particularly popular in Victorian times. Large engraved or cut hollow spheres of ruby glass were a common form.\n\nThe United States has a number of museums exhibiting outstanding paperweight collections. Many collectors consider the finest of these to be the Arthur Rubloff collection at the Art Institute of Chicago, which expanded its exhibition in 2012. The Bergstrom-Mahler Museum in Neenah, Wisconsin, exhibits the Evangeline Bergstrom collection. The Corning Museum of Glass in Corning, New York, exhibits the Amory Houghton collection. The Yelverton Paperweight Centre in Devon, England, a collection of over 1,000 paperweights, closed in 2013.\n\nAnother museum with a notable exhibition of outstanding American paperweights is in the Museum of American Glass at the Wheaton Arts and Cultural Center in Millville, New Jersey. In 1998, Henry Melville Fuller donated 330 twentieth-century paperweights to the Currier Museum of Art in Manchester, New Hampshire.\n\nThere are many paperweight collectors worldwide. Several collectors' associations hold national or regional conventions, and sponsor activities such as tours, lectures, and auctions. Famous collectors include the literary figures Colette, Oscar Wilde and Truman Capote. Empress Eugenie (Napoleon III's wife), Empress Carlotta (wife of Maximilian I of Mexico) and Farouk, King of Egypt were also avid collectors. The collecting histories of Rubloff, Bergstrom, and Houghton were similar. They had two things in common — a passion for their collecting, and the privilege of having sufficient financial resources to build extensive collections of very rare and expensive weights. Another famous collector was Lothar-Günther Buchheim, the German author and painter. He is best known for his novel \"Das Boot\" (1973), which became an international bestseller and was adapted in 1981 as an Oscar-nominated film. His collection of about 3,000 paperweights can be seen at his museum in Germany - Museum der Phanthasie - in Bernried, Bavaria, Starnberger See (Lake). Marnie Bjornson has amassed one of the most notable collections in Canada, focusing on the history of the Icelandic communities of Manitoba.\n\n\n"}
{"id": "3872258", "url": "https://en.wikipedia.org/wiki?curid=3872258", "title": "Parasitic capacitance", "text": "Parasitic capacitance\n\nParasitic capacitance, or stray capacitance is an unavoidable and usually unwanted capacitance that exists between the parts of an electronic component or circuit simply because of their proximity to each other. When two electrical conductors at different voltages are close together, the electric field between them causes electric charge to be stored on them; this effect is parasitic capacitance. All actual circuit elements such as inductors, diodes, and transistors have internal capacitance, which can cause their behavior to depart from that of 'ideal' circuit elements. Additionally, there is always non-zero capacitance between any two conductors; this can be significant at higher frequencies with closely spaced conductors, such as wires or printed circuit board traces. \n\nThe parasitic capacitance between the turns of an inductor or other wound component is often described as \"self-capacitance\". However, the term self-capacitance more correctly refers to a different phenomenon; the capacitance of a conductive object without reference to another object.\n\nWhen two conductors at different potentials are close to one another, they are affected by each other's electric field and store opposite electric charges like a capacitor. Changing the potential \"v\" between the conductors requires a current \"i\" into or out of the conductors to charge or discharge them.\nwhere \"C\" is the capacitance between the conductors. For example, an inductor often acts as though it includes a parallel capacitor, because of its closely spaced windings. When a potential difference exists across the coil, wires lying adjacent to each other are at different potentials. They act like the plates of a capacitor, and store charge. Any change in the voltage across the coil requires extra current to charge and discharge these small 'capacitors'. When the voltage changes only slowly, as in low-frequency circuits, the extra current is usually negligible, but when the voltage changes quickly the extra current is larger and can affect the operation of the circuit.\n\nCoils for high frequencies are often basket-wound to minimise parasitic capacitance.\n\nAt low frequencies parasitic capacitance can usually be ignored, but in high frequency circuits it can be a major problem. In amplifier circuits with extended frequency response, parasitic capacitance between the output and the input can act as a feedback path, causing the circuit to oscillate at high frequency. These unwanted oscillations are called \"parasitic oscillations\". \n\nIn high frequency amplifiers, parasitic capacitance can combine with stray inductance such as component leads to form resonant circuits, also leading to parasitic oscillations. In all inductors, the parasitic capacitance will resonate with the inductance at some high frequency to make the inductor \"self-resonant\"; this is called the self-resonant frequency. Above this frequency, the inductor actually has capacitive reactance. \n\nThe capacitance of the load circuit attached to the output of op amps can reduce their bandwidth. High-frequency circuits require special design techniques such as careful separation of wires and components, guard rings, ground planes, power planes, shielding between input and output, termination of lines, and striplines to minimise the effects of unwanted capacitance. \n\nIn closely spaced cables and computer busses, parasitic capacitive coupling can cause crosstalk, which means the signal from one circuit bleeds into another, causing interference and unreliable operation.\n\nElectronic design automation computer programs, which are used to design commercial printed circuit boards, can calculate the parasitic capacitance and other parasitic effects of both components and circuit board traces, and include them in simulations of circuit operation. This is called parasitic extraction.\n\nThe parasitic capacitance between the input and output electrodes of inverting amplifying devices, such as between the base and collector of transistors, is particularly troublesome because it is multiplied by the gain of the device. This \"Miller capacitance\" (first noted in vacuum tubes by John Milton Miller, 1920) is the major factor limiting the high frequency performance of active devices like transistors and vacuum tubes. The screen grid was added to triode vacuum tubes in the 1920s to reduce parasitic capacitance between the control grid and the plate, creating the tetrode, which resulted in a great increase in operating frequency.\nThe diagram, right, illustrates how Miller capacitance comes about. Suppose the amplifier shown is an ideal inverting amplifier with voltage gain of A, and Z = C is a capacitance between its input and output. The output voltage of the amplifier is\nThe current into the input terminal is \nSo the capacitance at the input of the amplifier is\nThe input capacitance is multiplied by the gain of the amplifier. This is the Miller capacitance.\nIf the input circuit has an impedance to ground of R, then (assuming no other amplifier poles) the output of the amplifier is\nThe bandwidth of the amplifier is limited by the high frequency roll-off at\nSo the bandwidth is reduced by the factor (1 + A), approximately the voltage gain of the device. The voltage gain of modern transistors can be 10 - 100 or even higher, so this is a significant limitation.\n\n"}
{"id": "51919074", "url": "https://en.wikipedia.org/wiki?curid=51919074", "title": "Poovulagin Nanbargal", "text": "Poovulagin Nanbargal\n\nPoovulagin Nanbargal is an environmental organization based in Tamil nadu, India. It spearheaded legal battle against Kudankulam Nuclear Power Plant.\n"}
{"id": "29873363", "url": "https://en.wikipedia.org/wiki?curid=29873363", "title": "Rudolf Amenga-Etego", "text": "Rudolf Amenga-Etego\n\nRudolf Amenga-Etego is a Ghanaian lawyer and environmentalist. He was awarded the Goldman Environmental Prize in 2004, for his efforts on keeping water supplies affordable for the population, and campaigning against privatization of water in Ghana.\n"}
{"id": "13606026", "url": "https://en.wikipedia.org/wiki?curid=13606026", "title": "Shell balance", "text": "Shell balance\n\nIn fluid mechanics, it may be necessary to determine how a fluid velocity changes across the flow. This can be done with a shell balance.\n\nA shell is a differential element of the flow. By looking at the momentum and forces on one small portion, it is possible to integrate over the flow to see the larger picture of the flow as a whole. The balance is determining what goes into and out of the shell. Momentum enters and leaves the shell through fluid entering and leaving the shell and through shear stress. In addition, there are pressure and gravity forces on the shell. The goal of a shell balance is to determine the velocity profile of the flow. The velocity profile is an equation to calculate the velocity based on a specific location in the flow. From this, it is possible to find a velocity for any point across the flow.\n\nShell Balances can be used for many situations. For example, flow in a pipe, flow of multiple fluids around each other, or flow due to pressure difference. Although terms in the shell balance and boundary conditions will change, the basic set up and process is the same. This system is useful to analyze any fluid flow that holds true for the requirements listed below.\n\nIn order for a shell balance to work, the flow must:\n\n\nBoundary Conditions are used to find constants of integration.\n\n\nThe following is an outline of how to perform a basic shell balance.\nIf fluid is flowing between two horizontal surfaces, each with area A touching the fluid, a differential shell of height Δy can be drawn between them as shown in the diagram below.\n\nIn this example,\nthe top surface is moving at velocity U and the bottom surface is stationary\ndensity of fluid = ρ \nviscosity of fluid = μ\nvelocity in x direction = formula_1, shown by the diagonal line above. This is what a shell balance is solving for.\n\nConservation of Momentum is the Key of a Shell Balance\n\nrate of momentum in - rate of momentum out + sum of all forces = 0\n\nTo perform a shell balance, follow the following basic steps:\n\n1. Find momentum from shear stress\n\nMomentum from Shear Stress goes into the shell at \"y\" and leaves the system at \"y\" + Δ\"y\".\n\nShear stress = \"τ\", area = \"A\", momentum = \"τ\"\"A\"\n\n2. Find momentum from flow\n\nMomentum flows into the system at \"x\" = 0 and out at \"x\" = \"L\"\n\nThe flow is steady state. Therefore, the momentum flow at \"x\" = 0 is equal to the moment of flow at \"x\" = \"L\". Therefore, these cancel out.\n\n3. Find gravity force on the shell\n\n4. Find pressure forces\n\n5. Plug into conservation of momentum and solve for \"τ\"\n\n6. Apply Newton's law of viscosity for a Newtonian fluid\n\n\"τ\" = -\"μ\"(\"dV\"/\"dy\")\n\n7. Integrate to find equation for velocity and use Boundary Conditions to find constants of integration\n\nBoundary 1: Top Surface: y = 0 and V = U\n\nBoundary 2: Bottom Surface: y = D and V = 0\n\nFor examples of performing shell balances, visit the resources listed below.\n"}
{"id": "8710679", "url": "https://en.wikipedia.org/wiki?curid=8710679", "title": "Snapper (puzzle)", "text": "Snapper (puzzle)\n\nA snapper is a mechanical puzzle consisting of a pointed piece attached to a notched dowel and a hollow block with a secondary hole along its length through which a rubber-band is stretched (across the shaft) and tied off. A demonstrator inserts the dowel into the hollow block, hooks the rubber-band on the notch, pulls on the pointed end, and allows the dowel to snap back into place. The demonstrator hands the puzzle to an observer who cannot hook the rubber-band onto the dowel's notch. The demonstrator takes the puzzle back and makes it snap immediately. The trick is that the ends of the dowel are tapered so that the demonstrator can squeeze the fingers holding it together, causing it to snap into the block.\n\nTraditionally made of wood, contemporary varieties come in plastic or metal for enhanced durability.\n\nA variation on this puzzle inserts the rubber-band into the shaft of the block opposite the pointed piece. At least one variation allows the demonstrator to hook the rubber band and show it to observers.\n\n"}
{"id": "28802225", "url": "https://en.wikipedia.org/wiki?curid=28802225", "title": "Suessite", "text": "Suessite\n\nSuessite is a rare iron silicide mineral with chemical formula: FeSi. The mineral was named after Professor Hans E. Suess. It was discovered in 1982 during the chemical analysis of The North Haig olivine pigeonite achondrite (ureilite). It is a cream white color in reflected light, and ranges in size from 1 μm \"blebs\" to elongated grains that can reach up to 0.45 cm in length. This mineral belongs in the isometric crystal class. The isometric class has crystallographic axes that are all the same length and each of the three axes perpendicular to the other two. It is isotropic, has a structural type of DO3 and a crystal lattice of BiF. \n\nSuessite is an isotropic mineral, Isotropism is defined as an optical property of a mineral that stays the same from whatever direction it is observed. In thin-section microscopy, an isotropic mineral has only one refractive index. This means that light that passes through the mineral is not split into two different directions, but it passes through unchanged. Suessite, as determined from the previous definition, only has one index of refraction. When Keil, Fuchs, and Berkley first discovered the mineral they described it as having a relatively low optical relief, but there was no determination of the index of refraction. In plane polarized light, suessite is a reddish-brown color that shows no pleochroism. \n\n\"Suessite can form under highly reducing conditions\" say the scientists who discovered this mineral. Only one out of eight ureilites studied (the North Haig ureilite) by this group contained suessite. Most contained trace amounts of kamacite which is the mineral from which Suessite is formed. In this particular study, the meteorite that contained suessite contained the highest amounts of shock metamorphism, which can be determined from the size of a shatter cone created from the impact. This could mean that suessite is formed due to the extreme increase in temperature combined with reduction of silicate rims, shortly followed by a rapid decrease in temperature. This means that, in meteorites, the abundance of suessite can be used to identify deformation associated with shock metamorphism, which could be used to determine various characteristics of the studied meteorites. \n\nThe other natural iron silicides include gupeiite (FeSi), hapkeite (FeSi), linzhiite (FeSi), luobusaite (FeSi), naquite (FeSi), xifengite (FeSi), and zangboite (TiFeSi).\n"}
{"id": "19871579", "url": "https://en.wikipedia.org/wiki?curid=19871579", "title": "Thorvald Mellingen", "text": "Thorvald Mellingen\n\nThorvald L. Mellingen (8 October 1935 – 29 October 2016) was a Norwegian engineer.\n\nHe graduated from the Norwegian Institute of Technology in 1961, and then worked in the companies Norsk Bergverk and Sydvaranger. He was then hired in Norsk Hydro in 1965, and took further education at the Institut français du pétrole. In 1969 he went on to Norges Teknisk-Naturvitenskapelige Forskningsråd. From 1976 to 1982 he worked in Saga Petroleum, overseeing construction at the Sèmè oil field in Benin. In 1983 he was hired as assistant director of NUTEC. He was then CEO from 1984 to 1998, except for a period between 1991 and 1992 when he was CEO of Statoil Nigeria. From 2001 he was an advisor for the Angolan Ministry of Petroleum.\n\nThorvald Mellingen was a volunteer and of DIDI International Therapeutic Children Theater for war affected children. In 2005 Mellingen empowered the creation of the women professional center in Bamako, Mali.\n\nMellingen was a member of the board of the Norwegian Petroleum Society, and was involved in the Norwegian Support Committee for Chechnya. He was a Knight, 1 class, of the Swedish Order of the Polar Star, and lived in Laksevåg.\n"}
{"id": "30275938", "url": "https://en.wikipedia.org/wiki?curid=30275938", "title": "Topology (chemistry)", "text": "Topology (chemistry)\n\nIn chemistry, topology provides a convenient way of describing and predicting the molecular structure within the constraints of three-dimensional (3-D) space. Given the determinants of chemical bonding and the chemical properties of the atoms, topology provides a model for explaining how the atoms ethereal wave functions must fit together. Molecular topology is a part of mathematical chemistry dealing with the algebraic description of chemical compounds so allowing a unique and easy characterization of them.\n\nTopology is insensitive to the details of a scalar field, and can often be determined using simplified calculations. Scalar fields such as electron density, Madelung field, covalent field and the electrostatic potential can be used to model topology.\nEach scalar field has its own distinctive topology and each provides different information about the nature of chemical bonding and structure. The analysis of these topologies, when combined with simple electrostatic theory and a few empirical observations, leads to a quantitative model of localized chemical bonding. In the process, the analysis provides insights into the nature of chemical bonding.\n\nApplied topology explains how large molecules reach their final shapes and how biological molecules achieve their activity.\n\nCircuit topology is a topological property of folded linear polymers. This notion has been applied to structural analysis of biomolecules such as proteins and RNAs. \n\nIt is possible to set up equations correlating direct quantitative structure activity relationships with experimental properties, usually referred to as topological indices (TIs). Topological indices are used in the development of quantitative structure-activity relationships (QSARs) in which the biological activity or other properties of molecules are correlated with their chemical structure.\n\n\n"}
{"id": "3300780", "url": "https://en.wikipedia.org/wiki?curid=3300780", "title": "Turbo generator", "text": "Turbo generator\n\nA turbo generator is the combination of a turbine directly connected to an electric generator for the generation of electric power. Large steam-powered turbo generators provide the majority of the world's electricity and are also used by steam-powered turbo-electric ships.\n\nSmaller turbo-generators with gas turbines are often used as auxiliary power units. For base loads diesel generators are usually preferred, since they offer better fuel efficiency, but, on the other hand, diesel generators have a lower power density and hence, require more space.\n\nThe efficiency of larger gas turbine plants can be enhanced by using a combined cycle, where the hot exhaust gases are used to generate steam which drives another turbo generator.\n\nThe first turbo-generators were water turbines which propelled electric generators. Irish engineer Charles Algernon Parsons demonstrated a DC steam-powered turbogenerator using a dynamo in 1887. and by 1901 had supplied the first large industrial AC turbogenerator of megawatt power to a plant in Eberfeld, Germany.\n\nTurbo generators were also used on steam locomotives as a power source for coach lighting and heating systems.\n\nUnlike hydraulic turbines which usually operate at lower speeds (100 to 600 rpm), the efficiency of a steam turbine is higher at higher speeds and therefore a turbo generator is used for steam turbines. The rotor of a turbo generator is a non-salient pole type usually with two poles.\n\nThe normal speed of a turbo generator is 1500 or 3000 rpm with four or two poles at 50 Hz (1800 or 3600 rpm with four or two poles at 60 Hz). Salient rotors will be very noisy and with a lot of windage loss. The rotating parts of a turbo generator are subjected to high mechanical stresses because of the high operation speed. To make the rotor mechanically resistant in large turbo-alternators, the rotor is normally forged from solid steel and alloys like chromium-nickel-steel or chromium-nickel-molybdenum are used. The overhang of windings at the periphery will be secured by steel retaining rings. Heavy non-magnetic metal wedges on top of the slots hold the field windings against centrifugal forces. Hard composition insulating materials, like mica and asbestos, are normally used in the slots of rotor. These material can withstand high temperatures and high crushing forces.\n\nThe stator of large turbo generators may be built of two or more parts while in smaller turbo-generators it is built up in one complete piece.\n\nBased on the air-cooled turbo generator, gaseous hydrogen first went into service as the coolant in a hydrogen-cooled turbo generator in October 1937, at the Dayton Power & Light Co. in Dayton, Ohio. Hydrogen is used as the coolant in the rotor and sometimes the stator, allowing an increase in specific utilization and a 99.0% efficiency. Because of the high thermal conductivity, high specific heat and low density of hydrogen gas, this is the most common type in its field today. The hydrogen can be manufactured on-site by electrolysis.\n\nThe generator is hermetically sealed to prevent escape of the hydrogen gas. The absence of oxygen in the atmosphere within significantly reduces the damage of the windings insulation by eventual corona discharges. The hydrogen gas is circulated within the rotor enclosure, and cooled by a gas-to-water heat exchanger.\nElectric Turbo Compounding (ETC) is a technology solution to the challenge of improving energy efficiency for the stationary power generation industry.\n\nFossil fuel based power generation is predicted to continue for decades, especially in developing economies. This is against the controversially claimed global need to reduce carbon emissions, of which, a high percentage is produced by the power sector worldwide.\n\nETC works by making gas and diesel-powered gensets (Electric Generators) work more effectively and cleaner, by recovering waste energy from the exhaust to improve power density and fuel efficiency.\n\n\n\n\n"}
{"id": "40248", "url": "https://en.wikipedia.org/wiki?curid=40248", "title": "Variable Specific Impulse Magnetoplasma Rocket", "text": "Variable Specific Impulse Magnetoplasma Rocket\n\nThe Variable Specific Impulse Magnetoplasma Rocket (VASIMR) is an electromagnetic thruster under development for possible use in spacecraft propulsion. It uses radio waves to ionize and heat a propellant. Then a magnetic field accelerates the resulting plasma to generate thrust (plasma propulsion engine). It is one of several types of spacecraft electric propulsion systems.\n\nThe VASIMR method for heating plasma was originally developed from nuclear fusion research. It is intended to bridge the gap between high-thrust, low-specific impulse and low-thrust, high-specific impulse systems, and is capable of functioning in either mode. Former NASA astronaut Franklin Chang Díaz created the VASIMR concept and has been developing it since 1977.\n\nVASIMRs units for development and test are assembled by Ad Astra Rocket Company in Costa Rica.\n\nVASIMR, sometimes referred to as the Electro-thermal Plasma Thruster or Electro-thermal Magnetoplasma Rocket, uses radio waves to ionize and heat the propellant, which is then accelerated with magnetic fields to generate thrust. This engine is electrodeless, of the same propulsion family as the electrodeless plasma thruster, the microwave arcjet, or the pulsed inductive thruster class. It can be thought of as an electrodeless version of an arcjet rocket that can reach higher propellant temperature by limiting the heat flux from the plasma to the structure. Neither type of engine uses electrodes; this eliminates the electrode erosion that shortens the life of other ion thruster designs. Since every part of a VASIMR engine is magnetically shielded and does not directly contact plasma, the durability of this engine is predicted to be greater than many other ion/plasma engines.\n\nVASIMR has been described as a convergent-divergent nozzle for ions and electrons. The propellant (a neutral gas such as argon or xenon) is injected into a hollow cylinder surfaced with electromagnets. On entering the engine, the gas is first heated to a “cold plasma” by a helicon RF antenna (also known as a “coupler”) that bombards the gas with electromagnetic waves, stripping electrons off the propellant atoms and producing a plasma of ions and loose electrons that flow down the engine compartment. By varying the amount of energy dedicated to RF heating and the amount of propellant delivered for plasma generation, VASIMR is capable of generating either low-thrust, high–specific impulse exhaust or relatively high-thrust, low–specific impulse exhaust. The second phase of the engine is a strong electromagnet positioned to compress the ionized plasma in a similar fashion to a convergent-divergent nozzle that compresses gas in traditional rocket engines.\n\nA second coupler, known as the Ion Cyclotron Heating (ICH) section, emits electromagnetic waves in resonance with the orbits of ions and electrons as they travel through the engine. Resonance is achieved through a reduction of the magnetic field in this portion of the engine that slows the orbital motion of the plasma particles. This section further heats the plasma to greater than 1,000,000 kelvin—about 173 times the temperature of the Sun’s surface.\n\nThe path of ions and electrons through the engine approximates lines parallel to the engine walls; however, the particles actually orbit those lines while traveling linearly through the engine. The final, diverging, section of the engine contains an expanding magnetic field that drives the ions and electrons in steadily expanding spirals and ejects them from the engine, parallel and opposite to the direction of motion at velocities as great as 50,000 m/s.\n\nIn contrast to the typical cyclotron resonance heating processes, VASIMR ions are immediately ejected from the magnetic nozzle before they achieve thermalized distribution. Based on novel theoretical work in 2004 by Alexey V. Arefiev and Boris N. Breizman of University of Texas at Austin, virtually all of the energy in the ion cyclotron wave is uniformly transferred to ionized plasma in a single-pass cyclotron absorption process. This allows for ions to leave the magnetic nozzle with a very narrow energy distribution, and for significantly simplified and compact magnet arrangement in the engine.\n\nVASIMR does not use electrodes; instead, it magnetically shields plasma from most hardware parts, thus eliminating electrode erosion, a major source of wear in ion engines. Compared to traditional rocket engines with very complex plumbing, high performance valves, actuators and turbopumps, VASIMR has almost no moving parts (apart from minor ones, like gas valves), maximizing long term durability.\n\nHowever, new problems emerge, such as interaction with strong magnetic fields and thermal management. The relatively large power at which VASIMR operates generates substantial waste heat that needs to be channeled away without creating thermal overload and thermal stress. Powerful superconducting electromagnets, necessary to contain hot plasma, generate tesla-range magnetic fields that can cause problems with other onboard devices and produce unwanted torque by interaction with the magnetosphere. To counter this latter effect, the VF-200 consists of two 100 kW thruster units packaged with magnetic fields oriented in opposite directions, making a net zero-torque magnetic quadrupole.\n\nThe first VASIMR experiment was conducted at Massachusetts Institute of Technology in 1983 on the magnetic mirror plasma device. Important refinements were introduced to the rocket concept in the 1990s, including the use of the \"helicon\" plasma source, which replaced the plasma gun originally envisioned and made the rocket completely \"electrodeless\"—adding to durability and long life. A new patent was granted in 2002.\n\nIn 1995, the Advanced Space Propulsion Laboratory (ASPL) was founded at NASA Lyndon B. Johnson Space Center, in the Sonny Carter Training Facility. The magnetic mirror device was brought from MIT. The first plasma experiment in Houston was conducted with a microwave plasma source. Collaboration was established with University of Houston, UT-Austin, Rice University and other academic institutions.\n\nIn 1998, the first helicon plasma experiment was performed at the ASPL. VASIMR experiment (VX) 10 in 1998 achieved a helicon RF plasma discharge as great as 10 kW, VX-25 in 2002 as great as 25 kW, and VX-50 as great as 50 kW. In March 2000, the VASIMR group was given a Rotary National Award for Space Achievement/Stellar Award. By 2005 breakthroughs were obtained at ASPL including full/efficient plasma production and acceleration of the plasma ions. VX-50 proved capable of of thrust. Published data on VX-50, capable of 50 kW of total radio frequency power, showed ICRF (second stage) efficiency to be 59% calculated by 90% \"N\" coupling efficiency × 65% \"N\" ion speed boosting efficiency.\n\nAd Astra Rocket Company (AARC) was incorporated on January 14, 2005. On June 23, 2005, Ad Astra and NASA signed the first Space Act Agreement to privatize VASIMR Technology. On July 8, 2005, Díaz retired from NASA after 25 years. Ad Astra’s Board of Directors was formed and Díaz became chairman and CEO on July 15, 2005. In July 2006, AARC opened its Costa Rica subsidiary in Liberia on the campus of Earth University. In December 2006, AARC-Costa Rica performed its first plasma experiment on the VX-CR device, using helicon ionization of argon.\n\nThe 100 kilowatt VASIMR experiment was successfully running by 2007 and demonstrated efficient plasma production with an ionization cost below 100eV. VX-100 plasma output tripled the prior record of the VX-50.\n\nModel VX-100 was expected to have \"N\" ion speed boosting efficiency of 80%. Instead, efficiency losses emerged from the conversion of DC electric current to radio frequency power and the energy consumption of the auxiliary equipment for the superconducting magnet. By comparison, 2009 state-of-the-art, proven ion engine designs such as NASA's High Power Electric Propulsion (HiPEP) operated at 80% total thruster/PPU energy efficiency.\n\nOn October 24, 2008 the company announced that the plasma generation component of the VX-200 engine—helicon first stage or solid-state high frequency power transmitter—had reached operational status. The key enabling technology, solid-state DC-RF power-processing, reached 98% efficiency. The helicon discharge used 30 kW of radio waves to turn argon gas into plasma. The remaining 170 kW of power was allocated for acceleration of plasma in the second part of the engine, via ion cyclotron resonance heating.\n\nBased on data from VX-100 testing, it was expected that the VX-200 engine would have a system efficiency of 60–65% and thrust level of 5 N. Optimal specific impulse appeared to be around 5,000 s using low cost argon propellant. One of the remaining untested issues was potential vs actual thrust—whether the hot plasma actually detached from the rocket. Another issue was waste heat management. About 60% of input energy became useful kinetic energy. Much of the remaining 40% is secondary ionizations from plasma crossing magnetic field lines and exhaust divergence. A significant portion of that 40% was waste heat (see energy conversion efficiency). Managing and rejecting that waste heat is critical.\nBetween April and September 2009, tests were performed on the VX-200 prototype with integrated 2-tesla superconducting magnets. They expanded the power range of the VASIMR to its operational capability of 200 kW.\n\nDuring November 2010, long duration, full power firing tests were performed, reaching steady state operation for 25 seconds and validating basic design characteristics.\n\nResults presented in January 2011 confirmed that the design point for optimal efficiency on the VX-200 is 50 km/s exhaust velocity, or an \"I\" of 5000s. Based on these data, thruster efficiency of 72% was achieved, yielding overall system efficiency (DC electricity to thruster power) of 60% (since the DC to RF power conversion efficiency exceeds 95%) with argon propellant. VX-200 generates a thrust of around 5.4 N at 200 kW total RF power, and 3.2 N at 100 kW RF power.\n\nThe 200 kW VX-200 had executed more than 10,000 engine firings by 2013, while demonstrating greater than 70% thruster efficiency—relative to RF power input—with argon propellant at full power.\n\nThe VF-200 flight-rated thruster consists of two 100 kW VASIMR units with opposite magnetic dipoles so that no net torque is applied to the space station when the thruster magnets are working. The VF-200-1 is the first flight unit and was slated to be tested in space attached to the ISS.\n\nIn June 2005, Ad Astra signed its first Space Act Agreement with NASA, which led to the development of the VASIMR engine. In December 10, 2007, AARC and NASA signed an Umbrella Space Act Agreement relating to the space agency's potential interest in the engine . In December 8, 2008, NASA and AARC entered into a Space Act Agreement that could lead to conducting a space flight test of the engine on the ISS.\n\nFrom 2008 Ad Astra was working on placing and testing a flight version of the VASIMR thruster for the International Space Station (ISS). The first related agreement with NASA was signed on December 8, 2008; . a formal preliminary design review took plaace on 26 June 2013.\n\nIn March 2, 2011, Ad Astra and NASA Johnson Space Center signed a Support Agreement to collaborate on research, analysis and development on space-based cryogenic magnet operations and electric propulsion systems currently under development by Ad Astra. , NASA had assigned 100 people to the project to work with Ad Astra to integrate the VF-200 onto the space station. On December 16, 2013, AARC and NASA signed another five-year Umbrella Space Act Agreement.\n\nHowever, in 2015 NASA ended plans for flying the VF-200 to the ISS. A NASA spokesperson stated that the ISS \"was not an ideal demonstration platform for the desired performance level of the engines\". Ad Astra stated that tests of a VASIMR thruster on the ISS would remain an option after a future in-space demonstration. Work with NASA continued in 2015 under NASA's NextSTEP program with planning for a 100-hour vacuum chamber test of the VX-200SSTM thruster.\nSince the available power from the ISS is less than 200 kW, the ISS VASIMR would have included a trickle-charged battery system, allowing for 15-minute pulses of thrust. Testing of the engine on the ISS would have been valuable, because it orbits at a relatively low altitude and experiences fairly high levels of atmospheric drag, making periodic boosts of altitude necessary. Currently, altitude reboosting by chemical rockets fulfills this requirement. The VASIMR test on the ISS might lead to a capability of maintaining the ISS, or a similar space station, in a stable orbit at 1/20th of the approximately $210 million/year present estimated cost.\n\nIn March 2015, Ad Astra announced the award of a $10 million award from NASA to advance the technology readiness of the next version of the VASIMR engine, the VX-200SS (SS stands for steady state) to meet the needs of deep space missions.\n\nIn August 2016, Ad Astra announced completion of the milestones for the first year of its 3-year contract with NASA. This will allow first high-power plasma firings of the engines, with a stated goal to reach 100hr and 100 kW by mid-2018. In August 2017, the company reported completing its Year 2 milestones for the VASIMR electric plasma rocket engine. NASA gave approval for Ad Astra to proceed with Year 3 after reviewing completion of a 10-hour cumulative test of the 200SS™ rocket at 100kW.\n\nVASIMR is not suitable to launch payloads from the Earth's surface because it has a low thrust-to-weight ratio and requires an ambient vacuum. Instead, the engine would function as an upper stage for cargo, reducing fuel requirements for in-space transport. The engine is anticipated to perform the following functions at a fraction of the cost of chemical technologies: drag compensation for space stations, lunar cargo delivery, satellite repositioning, satellite refueling, maintenance and repair, in space resource recovery, and deep space robotic missions.\n\nOther applications for VASIMR such as the rapid transportation of people to Mars would require a very high power, low mass energy source, such as a nuclear reactor (see nuclear electric rocket). In 2010 NASA Administrator Charles Bolden said that VASIMR technology could be the breakthrough technology that would reduce the travel time on a Mars mission from 2.5 years to 5 months.\n\nIn August 2008, Tim Glover, Ad Astra director of development, publicly stated that the first expected application of VASIMR engine is \"hauling things [non-human cargo] from low-Earth orbit to low-lunar orbit\" supporting NASA's return to Moon efforts.\n\nThe most important near-term application of VASIMR-powered spacecraft is cargo transport. Studies have shown that, despite longer transit times, VASIMR-powered spacecraft will be much more efficient than traditional integrated chemical rockets when moving goods through space. An orbital transfer vehicle (OTV)—essentially a \"space tug\"—powered by a single VF-200 engine would be capable of transporting about 7 metric tons of cargo from low Earth orbit (LEO) to low Lunar orbit (LLO) with about a six-month transit time.\n\nNASA envisions delivering about 34 metric tons of useful cargo to LLO in a single flight with a chemically propelled vehicle. To make that trip, about 60 metric tons of LOX-LH2 propellant would be expended. A comparable OTV would employ 5 VF-200 engines powered by a 1 MW solar array. To do the same job, a VASIMR-powered OTV would need to expend only about 8 metric tons of argon propellant. The total mass of such an electric OTV would be in the range of 49 t (outbound & return fuel: 9 t, hardware: 6 t, cargo 34 t).\n\nOTV transit times can be reduced by carrying lighter loads and/or expending more argon propellant with VASIMR throttled up to higher thrust at less efficient (lower \"I\") operating conditions. For instance, an empty OTV on the return trip to Earth covers the distance in about 23 days at optimal specific impulse of 5,000 s (50 kN·s/kg) or in about 14 days at \"I\" of 3,000 s (30 kN·s/kg). The total mass of the NASA specifications' OTV (including structure, solar array, fuel tank, avionics, propellant and cargo) was assumed to be 100 metric tons (98.4 long tons; 110 short tons) allowing almost double the cargo capacity compared to chemically propelled vehicles but requiring even bigger solar arrays (or other source of power) capable of providing 2 MW.\n\n, Ad Astra Rocket Company was targeting space tug missions to help \"clean up the ever-growing problem of space trash\". As of 2016 no such commercial product had reached the market.\n\nIn order to conduct a manned trip to \"Mars in just 39 days\", the VASIMR would require an electrical power level available only by nuclear propulsion (specifically the nuclear electric type) by way of nuclear power in space. This kind of nuclear fission reactor might use a traditional Rankine/Brayton/Stirling conversion engine such as that used by the SAFE-400 reactor (Brayton cycle) or the DUFF Kilopower reactor (Stirling cycle) to convert heat to electricity. However, the vehicle might be better served with non-moving parts and non-steam based power conversion using a thermocell technology of the thermoelectric (including graphene-based thermal power conversion), pyroelectric, thermophotovoltaic, or thermionic magnetohydrodynamic type. Thermoelectric materials are also an option for converting heat energy (being both black-body radiation and the kinetic thermal vibration of molecules and other particles) to electric current energy (electrons flowing through a circuit). Avoiding the need for \"football-field sized radiators\" (Zubrin quote) for a \"200,000 kilowatt (200 megawatt) reactor with a power-to-mass density of 1,000 watts per kilogram\" (Díaz quote) this reactor would require efficient waste heat capturing technology. For comparison, a Seawolf-class nuclear-powered fast attack submarine uses a 34 megawatt reactor, and the Gerald R. Ford-class aircraft carrier uses a 300 megawatt A1B reactor.\n\nMars manned mission advocate Robert Zubrin has called VASIMR a hoax, claiming that it is less efficient than other electric thrusters that are now operational. He also believes that electric propulsion is not necessary to get to Mars; therefore, budgets should not be assigned to develop it. His second critique concentrates on the lack of a suitable power source. Ad Astra responded in a press release:\n\nAs a response to VASIMR being labeled as a hoax by Zubrin, Ad Astra added a section to their FAQ: \n\n\n"}
{"id": "19270903", "url": "https://en.wikipedia.org/wiki?curid=19270903", "title": "Voices from Chernobyl", "text": "Voices from Chernobyl\n\nChernobyl Prayer: A Chronicle of the Future (UK title) / Voices from Chernobyl: The Oral History of a Nuclear Disaster (US title) is a book by Nobel Laureate Svetlana Alexievich. Alexievich was a journalist living in Minsk, the capital of Belarus, in 1986 at the time of the Chernobyl disaster. (At the time Belarus was part of the Soviet Union as the Byelorussian Soviet Socialist Republic.) \n\nAlexievich interviewed more than 500 eyewitnesses, including firefighters, liquidators (members of the cleanup team), politicians, physicians, physicists and ordinary citizens over a period of 10 years. The book relates the psychological and personal tragedy of the Chernobyl accident, and explores the experiences of individuals and how the disaster affected their lives.\n\n\"Chernobyl Prayer\" was first published in Russian in 1997 as \"Чернобыльская молитва\" and a revised, updated edition came out in 2013. The American translation was awarded the 2005 National Book Critics Circle Award for general non-fiction.\n\n"}
{"id": "8348989", "url": "https://en.wikipedia.org/wiki?curid=8348989", "title": "Volkswagen Golf Mk4", "text": "Volkswagen Golf Mk4\n\nThe Volkswagen Golf Mk4 (or VW \"Type 1J\") is a compact car, the fourth generation of the Volkswagen Golf and the successor to the Volkswagen Golf Mk3. Launched in October 1997, it was the best selling car in Europe in 2001 (though it slipped to second place, behind the Peugeot 206, in 2002).\n\nThe Mk4 was a deliberate attempt to take the Volkswagen Golf series further upmarket, with a high-quality interior and higher equipment levels.\n\nIt was replaced in 2004 by the Volkswagen Golf Mk5 in European Markets. However, manufacturing continued in South America, Mexico and China for developing markets until 2010.\n\nThe Mk4 was sold in Japan, but starting with this generation and subsequent generations, it no longer complied with Japanese Government dimension regulations which affected sales, imposing an annual tax on Japanese consumers for owning a vehicle that exceeded the maximum width limit.\n\nThe Golf Mk4 was a significant car in its class. As with its big brother the Passat, not only was it the first step of Volkswagen moving its products upmarket to plug a gap between the mainstream machines and the premium cars, with SEAT and Škoda taking over as the mainstream in a new level of interior quality and sophistication never seen before from a mainstream brand in the class. In fact, the quality of the Golf was on a par with its sister Audi A3 from the year before, but cost considerably more than other cars in its class.\n\nThe latest model remained faithful to the Golf concept but included some of the new \"arched\" styling themes first seen on the B4 Passat.\nAs with the Mk2 Golf, Volkswagen did not make a convertible version of the Mk4 Golf. Instead, they face-lifted the front bumper, fenders, grille, and hood to resemble Mk4 Golf styling but to fit a Mk3 chassis. VW managed to incorporate some non-structural Mk4 parts as well such as fender repeaters, headlights, side mirror caps, rear license tag lights, 3-spoke steering wheel airbag, etc. The rear also received a redesigned bumper with the number plate tub moved from the hatch and a Mk4 handle with a larger VW emblem above it to resemble the rear of a Mk4 Golf. The interior largely remained the same as a Mk3 interior save for a Mk4 style 3-spoke leather steering wheel, a textured dashboard (also known as \"dimpled dash\" or \"shark skin dash\"), heavily bolstered front seats with incorporated side airbags, and the hazard switch relocated from the steering column to the instrument panel. The interior lighting in the cabin was switched to the blue and red hue found in the Mk4 and some of the more familiar Mk3 parts were chromed such as the inner door handles, emergency brake button, door strikers, front seat belt anchors, key lock cylinders, and shifter button in automatic transmission equipped cars. There are some technical carryovers, as well, the main one being the immobilizer and engine computer from the Mk4 Golf being used with the older Mk3 engine mechanicals.\n\nAlthough the redesigned Golf Cabriolet looks like a Mk4 Golf, it is based on the Mk3 chassis and this causes a bit of confusion on how it should be designated. VW enthusiasts in Eastern Europe call it a Mk4 Golf Cabriolet while VW enthusiasts in the United Kingdom and United States call it a Mk3.5 Cabrio.\n\nThe Volkswagen Golf Mk4 Variant was introduced in 1999. It was discontinued in 2006, and succeeded in 2007 by the Volkswagen Golf Mk5 Variant. Unlike the Mk3, it was offered in North America with the \"Jetta\" name and front sheet metal were used. The \"JETTA WAGON\" was used in North America instead of the \"BORA\" name.\n\nVolkswagen produced a saloon version of the Mk4 Golf. As with previous incarnations of the Golf, it had its own identity, and this time was called the Volkswagen Bora although the name Jetta remained in North America and South Africa. Unlike its predecessors, the Bora/Jetta featured unique rear doors, front wings and bonnet. The front doors were the only body panels it shared with the Golf. The interior, though, was almost identical to the Golf, featuring very minor styling changes like its predecessor.\nGermany, South Africa, Slovakia, Brazil, Belgium, and China all made the Golf 4. Eastern European locations making the Golf 4 included Bosnia and Herzegovina, in Vogošća (near Sarajevo), which also made Mk1 and Mk2 models. However, although the Bosnian Mk4 was popular it was only available in the local market.\n\nThe Golf/Jetta Mk4 engine choices included 1.4, 1.6, 1.8, 2.0, 2.3  litre VR5, 2.8  litre V6 and 3.2  litre R32 petrol engines, 1.9-litre naturally aspirated diesel SDI engine, and a 1.9-litre turbodiesel, with power ranging from 90 to 150 PS (66 to 110 kW).\n\nVolkswagen made a choice of three and five-door hatchback or a five-door station wagon available. The European Golf wagon was nearly identical to the North American Jetta Wagon. The only difference was the use of the Golf front headlights, bumpers, grille, hood, and fenders as these parts are interchangeable between the Mk4 Golf and Bora/Jetta.\n\nThe Golf 4 was introduced to North America in mid-1999. Available engines for the Golf at its introduction to the American market were a 2.0 L gasoline engine, and a thrifty (48mpg) 1.9 L TDI engine. The latter soon developed a reputation for good low-speed torque and fuel economy, and can operate on alternative biofuels. In 2004 the updated 1.9L TDI PD or \"Pumpe-Düse\" engine was installed in the Golf and Jetta. The \"Pumpe-Düse\" or Pump Nozzle was a Robert Bosch extreme high pressure fuel injection system for direct cylinder injection. A 1.8 L turbocharged gas engine was introduced in 2000, along with the 12-valve 2.8 L VR6. At the same time, the 1.6 L 8-valve unit was replaced with the 16-valve unit from the Polo GTI, but detuned to 77 kW (105 PS). The 2.0 L gasoline engine was the base engine in the sportier GTI only as a 1999.5 model. For 2000, Volkswagen opted for the relatively new 1.8 L turbocharged gasoline engine as a base engine for the GTI. The top-of-the-line GLX model was equipped with Volkswagen's torquey 2.8 L VR6, which put out an impressive . The VR6 engine, with its narrow 15-degree Vee design, was unique to Volkswagen. This engine is shorter and lighter (featuring a single cylinder head) than other V6 engines which benefits the handling characteristics of this front-wheel drive car. For the 2002.5 model year Volkswagen introduced a 24-valve version of its VR6 engine to the North American market under engine code BDE. This engine had the same torque characteristics of the older 12-valve version which had been carried over from the Mk3 Golf under engine codes AAA and AFP. The 24-valve version gained an extra over the 12-valve to reach 204BHP. In Europe, the VR6-engined V6 4Motion variant was produced from 1999 with 204BHP and a 24-valve engine from the outset, using engine codes AUE and AQP. In 2002, the European market began using the BDE-code engine at the same time as the North American market. This had the same 204BHP power output but now featured variable valve timing on the exhaust valves which allowed the engine to rev more freely in the higher ranges and now had \"coil-on-plug\" ignition coils. The 1.8T and VR6 models continued until 2005, when the Mk4 platform came to an end in North America. Both the Mk4 Golf and the Mk4 Jetta are still in production in Brazil, Mexico, and China as of 2008.\n\nThe Brazilian Golf TDI PD was sold in Canada due to its popularity as a full 2006 models in base, GL and GLS trim levels for the full model year as there were no diesel engine versions for the North American 2007 Mk5 Golf (Rabbit).\n\nIn Europe, trim levels were country-specific, although the United Kingdom got E, S, SE, GTI and V5/V6/V6 4MOTION versions. The V5 was available in 150 bhp/110 kW (1997-2000) and 170 bhp/125 kW (1999-2003) versions.\n\nThe \"GTI 25th Anniversary Edition\" was a special version of the Golf GTI, for the European Market to commemorate the first GTI, launched in 1976. This model had three paint colour options: Tornado Red, Reflex Silver & Diamond Black.\n\nAt the time of its launch, it wasn't confirmed whether Volkswagen was going to sell this special edition model in the United States.\n\nA similarly equipped version of the GTI, called the GTI 337 Edition, was officially introduced at the New York Auto Show and made it to dealers late May 2002 to the US & Canadian markets. The price of the GTI 337 was $22,225 in the U.S. and $32,900 in Canada. Only 1,500 units were produced for the US market with an additional 250 for the Canadian market. This model came painted exclusively in metallic Reflex Silver.\nBoth the GTI 25th anniversary and the 337 editions were equipped with many extra features not included in the standard GTI. They included: 18x7.5\" BBS RC Wheels with special ball peen finish, perforated leather shift boot and handbrake, red and black seat belts, Red and Black upholstered Recaro seats, factory body kit (front valance, sideskirts, hatch spoiler, and rear valance), 02M 6-speed manual transmission, larger front brakes (312mm) with Red Calipers, lowered sport tuned suspension, and brushed aluminum interior trim. These models were never equipped with a sunroof as to take the car back to its roots, the Mk1 GTI, and improve handling and performance. However, one instance of the 337 edition is known to have been special ordered with a sunroof.\n\nFollowing the initial commemorative anniversary edition GTI produced in Europe in 1996 celebrating the introduction of the GTI model in 1976, and the overwhelming popularity of the 25th anniversary edition GTI produced 2001 (released as the GTI 337 in North America in 2002), Volkswagen of America produced 4,200 \"20th Anniversary Edition\" GTIs for 2003; 4,000 were shipped to the United States and 200 to Canada. This marked the 20th anniversary of the GTI's 1983 introduction to the U.S. and Canadian market, some 7 years after the GTI was introduced to the European market. Several special features distinguish this new GTI from the rest of the pack, most of which were shared with the 337 of 2002.\n\nOn the outside, the 20th Anniversary edition came with throwback red-lettered GTI logos on the left front and right rear. The rear was also accompanied by a vintage-look chrome rabbit. Blackened headlights added a distinctive look, while Votex front, rear, and side skirts along with a hatch spoiler and special edition 18\" OZ Aristo alloy wheels wrapped in Michelin Pilot Sports complete the exterior transformation. These models were produced in only three colours: Imola Yellow, Jazz Blue and Black Magic Pearl. Distribution of production was 50% Black magic pearl, 25% Jazz Blue and 25% Imola Yellow.\nInside, a few accents were noticeable. The only option was ESP, Volkswagen's stability control feature. All 20ths had a sunroof, black headliner, golf ball shift knob, black leather steering wheel with silver stitching, black leather shifter boot with silver stitching, perforated leather covered hand brake handle, and sporty black cloth Recaro bucket seats with silver stitching accents and red GTI emblems embroidered in the middle of the back rests. Aluminum trim came standard, complete with a numbered nameplate above the radio identifying the exact production number (US production only) of the vehicle. Volkswagen's premium 8-speaker Monsoon stereo system was also standard.\n\nMechanically, the 20th Anniversary Edition GTI is nearly identical to the GTI 337 Edition. A 6-speed manual 02M transmission marked the most notable departure from the norm, and upgraded suspension stiffened up the ride and lowered the car approximately 30 mm. Upgraded disc brakes front (12.3\" vented rotors) and rear (10.3\" vented rotors) helped bring things to a stop, while red powder-coated calipers added a bit of flair to the package.\n\nStarting in 2002 with engine code AWP, all of the models of the GTI's 1.8T increased in factory boost pressure and horsepower from to and had a quick 0-60 of 6.4 seconds. The 20th Anniversary GTI was not available with the VR6 engine.\n\nIn 2002, Volkswagen produced the Golf R32 in Europe as a 2003 model year. It was the world's first production car with a dual-clutch gearbox (DSG) — available for the German market. Due to unexpected popularity, Volkswagen decided to sell the car in the United States and Australia as the 2004 model year Volkswagen R32. Billed as the pinnacle of the Golf IV platform, the R32 included every performance, safety, and luxury feature Volkswagen had to offer, including the all new DOHC 4 valves per cylinder VR6 engine (ID codes: BFH/BML), which produced a rated motive power output of @ 6250 rpm and @ 2800 rpm of torque. Further additions included Haldex Traction-based 4motion on-demand four-wheel drive system, a new six-speed manual transmission, independent rear suspension, Climatronic automatic climate control, sport seats from König with R32 badging, 18\" OZ Aristo alloy wheels (Ronal produced the wheels towards the end of production), Electronic Stability Programme, larger disc brakes with gloss blue painted calipers, sunroof (for the US), Xenon Headlamps (for Europa), and model-specific bodywork additions.\n\nFor Australia, two hundred \"Edition 200\" cars were produced, each uniquely plaqued and available in three colours: Black Magic Pearl, Deep Blue Pearl and Reflex Silver.\n\nFor the US, Tornado Red was an available fourth colour. The distribution of US-spec R32 colours were:\n\nAlthough the R32 looked similar to the 20th Anniversary GTI, it shared most major components with the 3.2-litre Audi TT, including the engine, four-wheel drive system, and both front and rear suspension geometries. For the US, five thousand cars were produced and intended to be sold over a two-year period. The allotment sold out in 13 months.\n\nThe R32 is capable of 0- in 6.6 seconds, reduced to 6.4 seconds with the dual clutch gearbox. Clearing the quarter mile in 14.1 seconds at , the R32 edges out its third fastest sibling, the top-of-the-line Phaeton 6.0-litre W12 (414 bhp), by a tenth of a second at the mark.\n\nIt has a high resale and used-car value; the Kelley Blue Book used car retail price (the price an individual might expect to pay for one from a dealer) for a model in excellent condition with low mileage exceeds the original retail price of the car in many cases, making it one of a few recent cars that have actually approached an increase in value. This premium can be explained mostly due to scarcity, both of the cars themselves due to low production and importation, and especially ones that still have low mileage.\n\nIn China, FAW-VW launched a new version of the Golf IV, with FAW-VW's facelifted Bora front, at the Beijing International Automobile Exhibition in 2006. It is named Bora HS to complement the Bora Mk IV range, as the Golf name has reserved for the Golf VI, manufactured by FAW-VW in Q3, 2009. The Bora HS ended production in 2008.\n\nThe Golf Mk4 also continues to be sold in countries such as Brazil and Argentina. However, in Argentina the range is available with a 1.6 L four-cylinder petrol, a 2.0 L petrol inline four, a 1.8 L Turbocharged petrol unit, or with a 1.9-litre turbodiesel. In Chile, it was also sold until 2010, with a 1.6 to 2.0 L petrol range. All of these are Brazilian built models, although the diesels are only for export markets.\n\nIn Brazil, the Mk4 Golf, marketed as the Gol, has a 1.6l engine (with Volkswagen's Total Flex system which accepts both gasoline and ethanol), or a 2.0l engine (the 1.8 turbo engine was discontinued in 2009). It is available in two trim levels: the basic Sportline model with the 1.6 engine, and the 2.0 version with a Tiptronic 6-speed transmission.\nIn 2007, Volkswagen Brazil introduced a major restyling of the fourth generation Golf. The front takes styling cues from the current Volkswagen Polo and the back of the car is inspired by the Golf 5. It is exported in most Latin America countries, since it is produced in Brazil. It was also sold in Canada, where it was sold alongside the Mk5 model (badged as the Rabbit). The lack of diesel models for the 2007 model year led Volkswagen Canada to continue sales of an entry-level car that was designed as an alternative to the TDI models for budget-minded shoppers. The Canadian model was originally rebadged as the City Golf, but in 2009 it was renamed the Golf City alongside the Jetta City. It was not sold in the United States. Pricing of the Golf City started at C$15,300 as of 2008. As an entry-level alternative to the Rabbit, it offered only one engine: the 2.0L SOHC 8 valve with . It was not related to the South African Volkswagen CitiGolf, despite the similarity in name. The Mk4 Jetta was similarly reintroduced in Canada for the 2007 model year as the City Jetta. This was the first entry-level car from a previous-generation car since the 1992 Eagle Vista, which was a rebadged 1980s Mitsubishi Mirage. Although the Golf City was dated, its attractive price enabled good sales. The Golf City was discontinued after the 2010 model year.\n\nIn 1999, Joseph Cardinal Ratzinger, prefect of the Catholic Church's Congregation of the \"Doctrine of the Faith\" under Pope John Paul II, purchased a fourth-generation Golf in which to drive around Rome, selling it in 2005 after John Paul's death in anticipation of retiring and returning to Germany.\n\nInstead, he became Pope Benedict XVI. Shortly after John Paul's death, his 1999 Golf sold to a German owner for $13,000, who then sold the car for €188,938.88 via eBay to GoldenPalace.com.\n\nThe Golf, undriven since the sale, was subsequently sold for £14,300 via eBay.\n\n\n"}
{"id": "44952828", "url": "https://en.wikipedia.org/wiki?curid=44952828", "title": "Waste management in Thailand", "text": "Waste management in Thailand\n\nThais per capita generate an estimated 1.14 kg of solid waste per day—50 percent of it biodegradable. \nAccording to Interior Ministry statistics, refuse nationwide in 2016 amounted to 27 million tonnes, up about 0.7 percent from the previous year. Of this, 4.2 million tonnes was generated in Bangkok. The yearly figure in 2009 was 15.1 million tonnes. About twenty percent of total is generated in the Bangkok Metropolitan Area (BMA) Of the waste generated in 2015, only about five million tonnes were recycled. Only about eight million tonnes were handled in accordance with global best practices. Of Thailand's 2,500 dump sites, only about 20 percent are managed properly. According to the Pollution Control Department (PCD), Thailand's primary waste watchdog, the nation faces serious solid waste management issues. Those issues are increasing. Wichan Simachaya, director-general of the PCD, said the volume of waste could continue to grow by 600,000 tonnes a year, due to increasing population and tourism.\n\nThailand's waste management plan calls for 75 percent of Thailand's total solid waste to be properly disposed of or recycled in some way by 2021, up from the current 49 percent. By 2021, the government and private sector plan to spend a total of 177 billion baht (US$5.1 billion) on waste management technology and public awareness campaigns. \"We have fines for littering but no one seems to care,\" Wijarn said. \"We need to step up law enforcement as well as teach people to recycle, reuse and reduce waste.\"\n\nIn Thailand the roles in solid waste management (MSW) and industrial waste management are organized by the Royal Thai Government, which is then divided among the central government, regional governments, and local governments. Each government is responsible for different tasks. The central government is responsible to stimulate regulation, policies, and standards. The regional governments are responsible for coordinating central and local governments. Local governments are responsible for waste management in their governed area. Local governments themselves do not dispose of waste, but instead hire private companies that have been granted that right by the PCD. A major problem is lack of funding for waste management—the average Thai household pays less than one dollar a month to dispose of their solid waste. The main companies are Bangpoo Industrial Waste Management Center; General Environmental Conservation Public Company Limited (GENCO); SGS Thailand; Waste Management Siam Ltd (WMS);; Better World Green Public Company Limited (BWG).and Professional Waste Technology (1999) Public Company Ltd A leading resource recovery company is Wongpanit, who purchase mixed recyclables (paper, plastics, glass, aluminum, steel) at about 11,300 baht per tonne. These companies are responsible for the waste they have picked up from their customers before disposal.\n\nThailand is a profligate user of one-time use plastics. Thais use 70 billion plastic bags a year. The country is a major contributor, along with China, Indonesia, the Philippines, Vietnam and Sri Lanka, of up to 60 percent of plastic pollution in oceans. Thailand's 23 coastal provinces dump an estimated one million tonnes of garbage into the sea each year. Plastic bags make up 15 percent, plastic straws account for seven percent, and cigarette butts five percent. According to the Ministry of Natural Resources and Environment, 150 sea turtles, 100 whales and dolphins, and 12 dugongs die each year from discarded trash, half of which die from eating plastic bags.\n\nIn June 2017, Thailand pledged at an international forum to reduce plastic use. Thailand admitted waste mismanagement was the major cause of Thailand's poor record. Delegates representing Thailand's military government at the conference committed to put an end to the problem. Accordingly, it has included waste management in its 20-year national strategy.\n\nIn an easy step forward, Thailand's Pollution Control Department (PCD) got agreements from five major water bottlers to cease using plastic cap seals on drinking water bottles by 1 April 2018. A cap seal is the small plastic wrap molded over the bottle cap that must be peeled off before the bottle can be opened. Studies have found that bottles without them pose no hygienic health risk. The PCD aims to have them removed from all bottled water containers by the end of 2018. According to the PCD, Thailand produces 4.4 billion plastic drinking water bottles per year. Sixty percent, or 2.6 billion, of these bottles have cap seals. The weight of the plastic cap seals alone is around 520 tonnes per year.\n\nOn 21 July 2018 the Thai government kicked off a campaign to reduce the use of foam containers and single-use plastic bags at fresh markets countrywide. Early indications are that those efforts have not been embraced by the Thai public.\n\n Thailand collected and processed the industrial waste of 68,261 companies. Its capacity to process industrial and toxic waste is 37.6 million tonnes annually, an estimated 2.8 million tonnes of which is toxic waste. Total capacity in 2015 was 25.8 million tonnes. The Ministry of Industry's Department of Industrial Works (DIW) plans to establish 15 regional waste management facilities throughout the country as detailed in its five-year waste management plan for 2015-2019.\n\nThe Bangkok Metropolitan Administration's 500-bed General Hospital produces about 196,000 pieces of \"medical waste\" per month. About half of this waste consists of \"clean products\": packaging, PVC bags for dialysis solution, and other non-infectious items that could potentially be recycled or upcycled. Thailand has thirty-three 500-or more-bed hospitals, as well as 321 other hospitals and health centres with between 11 and 250 bed nationwide, meaning the quantity of medical waste is significant.\n\nIn a 2004 study commissioned by the World Bank to examine the state of Thailand's infrastructure, the authors concluded that, \"The worst infrastructure provision performance in Thailand is in waste water treatment...Virtually none of the...[waste water treatment] systems are operating...only 3 waste water plants operate sporadically. The problem is that no user fees are assessed or collected and that households and firm [sic] are not required to connect to the systems. Water supply authorities...have consistently refused to add waste water charges to their water supply bills, despite clear evidence from international experience that this is best practice.\"\n\nAt the end of 2016, the Bangkok Metropolitan Administration (BMA) is considering the imposition of a waste water fee, pending endorsement by the BMA council. If approved, the move will impose fees on waste water release:\n\n\nThe fees collected will be used to improve waste water facilities operated by the BMA.\n\nIn 2015 Thailand produced 9.5 million m of waste water. This was the equivalent of 150 litres per day per person. Only 34 percent of the waste water was treated at one of Thailand's 93 treatment facilities before being returned to the environment. One direct result was a corresponding deterioration of the quality of Thai coastal waters.\n\nAccording to the United Nations Ocean Conference Thailand produces about 50.000 tonnes of solid waste that finds its way into the sea each year. Large quantities of coastal rubbish, especially in the upper reaches of the Gulf of Thailand impact sea life and coastal mangroves. Mangrove swamps are cluttered with garbage. At one site, the \"Bangkok Post\" reported that, \"Plastic bags, bottles, ropes, discarded noodle containers and even a rusty cooking appliance float in the water or are stuck in the mud under the green canopy.\"\n\nThe Thai Pollution Control Department (PCD) reports that the water quality of major rivers flowing into the upper Gulf of Thailand has seriously deteriorated in the past decade. The department found the lower Chao Phraya River, which flows through Bangkok, contains bacteria and nutrient pollution from phosphates, phosphorus, and nitrogen. Nutrient pollution causes algae to grow faster than ecosystems can handle, harming water quality, food resources for aquatic animals, and marine habitats. It also decreases the oxygen that fish need to survive. PCD categorised water quality at the mouth of Chao Phraya at Bangkok's Bang Khun Thian District as \"very poor\" and worse than in 2014. Nearby rivers, such as the Tha Chin in Samut Sakhon, were rated \"poor\". PCD findings indicated large amounts of wastewater were discharged into the river from households, industry, and agriculture.\n\n\n"}
{"id": "2358446", "url": "https://en.wikipedia.org/wiki?curid=2358446", "title": "ZBLAN", "text": "ZBLAN\n\nHeavy metal fluoride glasses were accidentally discovered in 1975 by Poulain and Lucas at the University of Rennes in France, including a family of glasses ZBLAN with a composition ZrF-BaF-LaF-AlF-NaF.\n\nZBLAN has a broad optical transmission window extending from 0.3 micrometers in the UV to 7 micrometers in the infrared, low refractive index (1.50), a relatively low glass transition temperature (\"T\") of 260 °C, low dispersion and a low and negative \"dn\"/\"dT\" (temperature dependence of refractive index).\nZBLAN glass is the most stable fluoride glass known and is most commonly used to make optical fiber. Recent advances by ZBLAN fiber manufacturers have demonstrated significant increases in mechanical properties (>100 kpsi or 700 MPa for 125 µm fiber) and attenuation as low as 3 dB/km at 2.6 µm. ZBLAN optical fibers are used in different applications such as spectroscopy and sensing, laser power delivery and fiber lasers and amplifiers.\n\nThe advantages of ZBLAN over other glasses available in 1988, such as silica, is superior infrared transmittance. Their drawbacks are fragility and sensitivity to acids. ZBLAN glass resistance to water depends strongly on the acidity of the aqueous solution. Acidic solutions will attack it, whereas basic solutions will have no effect. Atmospheric moisture has a very limited effect on fluoride glasses in general, and fluoride glass/fibers can be used in a wide range of operating environments over extended periods of time without any material degradation.\n\nA large variety of multicomponent fluoride glasses have been fabricated but few can be drawn into optical fiber. The fiber fabrication is similar to any glass-fiber drawing technology. All methods involve fabrication from the melt, which creates\ninherent problems such as the formation of bubbles, core-clad interface irregularities, and small preform sizes. The process occurs at 310 °C in a controlled atmosphere (to minimize contamination by moisture or oxygen impurities which significantly\nweaken the fiber) using a narrow heat zone compared to silica. Drawing is complicated by a small difference (only 124 °C) between the glass transition temperature and the crystallization temperature. As a result, ZBLAN fibers often contain undesired crystallites. The concentration of crystallites was shown in 1998 to be reduced by growing ZBLAN in zero gravity (see figure) which reduces convection processes.\n\nIn July 2016, a Silicon Valley aerospace company, Made In Space, announced that they plan to begin manufacturing ZBLAN in space for commercial purposes starting in 2017. \n\nAndy Weir's novel, Artemis features a fictional fiber optic material \"ZAFO\" which has very low attenuation and can be manufactured only under reduced gravity.\n"}
