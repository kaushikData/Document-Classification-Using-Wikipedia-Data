{"id": "31767051", "url": "https://en.wikipedia.org/wiki?curid=31767051", "title": "Ad Dimaniyat Islands", "text": "Ad Dimaniyat Islands\n\nThe Ad Dimaniyat Islands is a protected area in Oman.\n\nThe Nature Reserve is located in Wilayat AlSeeb in the Muscat Governorate and lies about off the coast of Barka ( west of Muscat, the capital). It is composed of nine islands with a total area of . The reserve has a rich natural heritage and is replete with several kinds of coral reefs, including some examples that are quite rare. The island is home to a large number of sea turtles that lay their eggs and nest there, as well as a magnet for migratory and indigenous birds.\n\nLocally, the islands go by the following names: Kharabah, Huyoot, Al Jabal Al Kabeer (Um As Sakan). The latter is divided into two islands: Um Al Liwahah (Minaret) and Al Jawn that includes three islands.\n\n\nFirst published online: 2015 (online scan, online text)\n"}
{"id": "1839", "url": "https://en.wikipedia.org/wiki?curid=1839", "title": "Allotropy", "text": "Allotropy\n\nAllotropy or allotropism () is the property of some chemical elements to exist in two or more different forms, in the same physical state, known as \"allotropes\" of these elements. Allotropes are different structural modifications of an element; the atoms of the element are bonded together in a different manner. For example, the allotropes of carbon include diamond (the carbon atoms are bonded together in a tetrahedral lattice arrangement), graphite (the carbon atoms are bonded together in sheets of a hexagonal lattice), graphene (single sheets of graphite), and fullerenes (the carbon atoms are bonded together in spherical, tubular, or ellipsoidal formations). The term \"allotropy\" is used for elements only, not for compounds. The more general term, used for any crystalline material, is polymorphism. Allotropy refers only to different forms of an element within the same phase (i.e.: solid, liquid or gas states); differences in these states alone would not constitute examples of allotropy.\n\nFor some elements, allotropes have different molecular formulae despite difference in phase; for example, two allotropes of oxygen (dioxygen, O, and ozone, O) can both exist in the solid, liquid and gaseous states. Conversely, some elements do not maintain distinct allotropes in different phases; for example, phosphorus has numerous solid allotropes, which all revert to the same P form when melted to the liquid state.\n\nThe concept of allotropy was originally proposed in 1841 by the Swedish scientist Baron Jöns Jakob Berzelius (1779–1848). The term is derived . After the acceptance of Avogadro's hypothesis in 1860, it was understood that elements could exist as polyatomic molecules, and two allotropes of oxygen were recognized as O and O. In the early 20th century, it was recognized that other cases such as carbon were due to differences in crystal structure.\n\nBy 1912, Ostwald noted that the allotropy of elements is just a special case of the phenomenon of polymorphism known for compounds, and proposed that the terms allotrope and allotropy be abandoned and replaced by polymorph and polymorphism. Although many other chemists have repeated this advice, IUPAC and most chemistry texts still favour the usage of allotrope and allotropy for elements only.\n\nAllotropes are different structural forms of the same element and can exhibit quite different physical properties and chemical behaviours. The change between allotropic forms is triggered by the same forces that affect other structures, i.e., pressure, light, and temperature. Therefore, the stability of the particular allotropes depends on particular conditions. For instance, iron changes from a body-centered cubic structure (ferrite) to a face-centered cubic structure (austenite) above 906 °C, and tin undergoes a modification known as tin pest from a metallic form to a semiconductor form below 13.2 °C (55.8 °F). As an example of allotropes having different chemical behaviour, ozone (O) is a much stronger oxidizing agent than dioxygen (O).\n\nTypically, elements capable of variable coordination number and/or oxidation states tend to exhibit greater numbers of allotropic forms. Another contributing factor is the ability of an element to catenate.\n\nExamples of allotropes include:\n\nAmong the metallic elements that occur in nature in significant quantities (56 up to U, without Tc and Pm), almost half (27) are allotropic at ambient pressure: Li, Be, Na, Ca, Ti, Mn, Fe, Co, Sr, Y, Zr, Sn, La, Ce, Pr, Nd, Sm, Gd, Tb, Dy, Yb, Hf, Tl, Th, Pa and U. Some phase transitions between allotropic forms of technologically relevant metals are those of Ti at 882 °C, Fe at 912 °C and 1394 °C, Co at 422 °C, Zr at 863 °C, Sn at 13 °C and U at 668 °C and 776 °C.\n\n\nIn 2017, the concept of nanoallotropy was proposed by Prof. Rafal Klajn of the Organic Chemistry Department of the Weizmann Institute of Science. Nanoallotropes, or allotropes of nanomaterials, are nanoporous materials that have the same chemical composition (e.g., Au), but differ in their architecture at the nanoscale (that is, on a scale 10 to 100 times the dimensions of individual atoms). Such nanoallotropes may help create ultra-small electronic devices and find other industrial applications. The different nanoscale architectures translate into different properties, as was demonstrated for surface-enhanced Raman scattering performed on several different nanoallotropes of gold. A two-step method for generating nanoallotropes was also created.\n\n\n"}
{"id": "33428711", "url": "https://en.wikipedia.org/wiki?curid=33428711", "title": "Analysis Situs (book)", "text": "Analysis Situs (book)\n\nAnalysis Situs is a book by the Princeton mathematician Oswald Veblen, published in 1922. It is based on his 1916 lectures at the Cambridge Colloquium of the American Mathematical Society. The book, which went into a second edition in 1931, was the first English-language textbook on topology, and served for many years as the standard reference for the domain. Its contents were based on the work of Henri Poincaré as well as Veblen's own work with his former student and colleague, James Alexander.\n\nAmong the many innovations in the book was the first definition of a topological manifold, and systematisations of Betti number, torsion, the fundamental group, and the topological classification problem.\n\n"}
{"id": "9552389", "url": "https://en.wikipedia.org/wiki?curid=9552389", "title": "Anna Tambour", "text": "Anna Tambour\n\nAnna Tambour is an author of satire, fable and other strange and hard-to-categorize fiction and poetry.\n\nHer novel \"Crandolin\" was shortlisted for the 2013 World Fantasy Award. Tambour's collection \"Monterra's Deliciosa & Other Tales &\" was published in 2003, and \"Spotted Lily\", a novel, in 2005. Ebook editions of both of these were published by infinity plus in 2011.\n\n\"Locus\" listed both Tambour's collections and both novels in their Recommended Reading lists. Her 2015 collection \"The Finest Ass in the Universe\" was shortlisted for an Aurealis Award for Best Collection. \"Spotted Lily\" was shortlisted in 2006 for the William L. Crawford Fantasy Award, and was recommended for a British Fantasy Society Award (Best Novel). In 2008, \"The Jeweller of Second-hand Roe\" won the Aurealis Award for best horror short story.\n\nTambour lives in the Australian bush, but has lived all over the world and is, in Tambour's words, \"of no fixed nationality\". In addition to writing fiction, Tambour also writes about and takes photographs of what she calls \"magnificants — magnificent insignificants\".\n\n\n"}
{"id": "36560134", "url": "https://en.wikipedia.org/wiki?curid=36560134", "title": "Australian Gas Networks", "text": "Australian Gas Networks\n\nAustralian Gas Networks Limited, formerly Envestra Limited, is an Australian energy company that operates natural gas transmission pipelines and distribution networks in South Australia, Victoria, Queensland, New South Wales, and the Northern Territory. The company owns distribution systems in a number of towns and metropolitan areas including Adelaide, Brisbane, Melbourne, Rockhampton, Albury, Alice Springs, Bundaberg and Whyalla. It outsources the operation and management of the assets to the APA Group.\n\nIn October 1993, Boral acquired from the South Australian government SAGASCO, its vertically integrated natural gas monopoly. Boral combined SAGASCO's distribution network with businesses it owned in Queensland to form Envestra, which was floated in early 1997. In March 1999, Envestra acquired part of the former Gas and Fuel Corporation's distribution network in Victoria, known as the Stratus distribution network, and renamed it Envestra (Vic).\n\nIn 2013, APA Group announced an approach to the board of Envestra with an all-share merger proposal. In September 2014, Hong Kong-based Cheung Kong Group bought all the shares in Envestra, including APA's 33.4% stake, while APA retained the operation and management of Envestra's assets until 2027. In October 2014, the company's name was changed to Australian Gas Networks Limited.\n"}
{"id": "2637151", "url": "https://en.wikipedia.org/wiki?curid=2637151", "title": "Batten", "text": "Batten\n\nA batten is most commonly a strip of solid material, historically wood but can also of plastic, metal, or fiberglass. Battens are variously used in construction, sailing, and other fields. \n\nIn the lighting industry, battens refer to linear fittings, commonly LED strips or using fluorescent tubes. Batten luminaires are typically of low cost and meant to be fixed directly to structural battens in loft spaces or to ceilings and soffits in back-of-house areas where aesthetic value is not required. The fluorescent options can include a low-spec diffuser cover, or simply be procured with the fluorescent tube exposed.\n\nIn the steel industry, battens used as furring may also be referred to as \"top hats\", in reference to the profile of the metal.\n\n\"Roofing battens\" or \"battening\", also called \"roofing lath\", are used to provide the fixing point for roofing materials such as shingles or tiles. The spacing of the battens on the trusses or rafters depend on the type of roofing material and are applied horizontally like purlins.\n\nBattens are also used in metal roofing to secure the sheets called a \"batten-seam roof\" and are covered with a \"batten roll joint\".\n\nSome roofs may use a grid of battens in both directions, known as a \"counter-batten system\", which improves ventilation.\n\n\"Wall battens\" like roofing battens are used to fix siding materials such as tile or shingles. Rainscreen construction uses battens (furring) as part of a system which allows walls to dry out more quickly than normal.\n\n\"Board-and-batten\" siding is an exterior treatment of vertical boards with battens covering the seams. Board-and-batten roofing is a type of board roof with battens covering the gaps between boards on a roof as the roofing material. Board-and-batten is also a synonym for single-wall construction, a method of building with vertical, structural boards, the seams sometimes covered with battens.\n\nBattens may be used as spacers, sometimes called furring, to raise the surface of a material. In flooring the sometimes large battens support the finish flooring in a similar manner to a joist but with the batten resting on a solid sub-floor as a \"floating floor\" and sometimes cushioned.\n\n\"Batten trim\" or \"batten molding\" is a thin strip of trim typically with a rectangular cross-section similar to lath used in lattice, used to cover seams between panels of exterior siding or interior paneling.\n\nIn flooring a batten may be relatively large, up to thick by wide and more than long.\n\nIn door construction ledges and sometimes also braces are used to strengthen the vertical battens.\n\nBattens are used for solid wall insulation. Regularly spaced battens are fitted to the wall, the spaces between them filled with insulation, and plasterboard or drywall screwed to the battens. This method is no longer the most popular, as rigid insulation sheets give better insulation (with battens bridging the insulation) and take less time to fit.\n\nIn concrete work a \"screed batten\" is fixed to the formwork to smoothly guide a screed smoothing tool.\n\nIn sailing, battens are long, narrow and flexible inserts used in sails, to improve their qualities as airfoils.\n"}
{"id": "37885032", "url": "https://en.wikipedia.org/wiki?curid=37885032", "title": "Cadjan", "text": "Cadjan\n\nCadjan are woven mats made from coconut palm leaves, used for roofing and walls. Cadjan houses were available in many Asian countries in past, but with development these houses are now limited to very rural areas in India, Sri Lanka and a few other Asian countries.\n"}
{"id": "2115745", "url": "https://en.wikipedia.org/wiki?curid=2115745", "title": "Cadmium selenide", "text": "Cadmium selenide\n\nCadmium selenide is an inorganic compound with the formula CdSe. It is a black to red-black solid that is classified as a II-VI semiconductor of the n-type. Much of the current research on cadmium selenide is focused on its nanoparticles.\n\nThree crystalline forms of CdSe are known: wurtzite (hexagonal), sphalerite (cubic) and rock-salt (cubic). The sphalerite CdSe structure is unstable and converts to the wurtzite form upon moderate heating. The transition starts at about 130 °C, and at 700 °C it completes within a day. The rock-salt structure is only observed under high pressure.\n\nThe production of cadmium selenide has been carried out in two different ways. The preparation of bulk crystalline CdSe is done by the High-Pressure Vertical Bridgman method or High-Pressure Vertical Zone Melting.\n\nCadmium selenide may also be produced in the form of nanoparticles. (see applications for explanation) Several methods for the production of CdSe nanoparticles have been developed: arrested precipitation in solution, synthesis in structured media, high temperature pyrolysis, sonochemical, and radiolytic methods are just a few.\n\nProduction of cadmium selenide by arrested precipitation in solution is performed by introducing alkylcadmium and trioctylphosphine selenide (TOPSe) precursors into a heated solvent under controlled conditions.\n\nCdSe nanoparticles can be modified by production of two phase materials with ZnS coatings. The surfaces can be further modified, e.g. with mercaptoacetic acid, to confer solubility.\nSynthesis in structured environments refers to the production of cadmium selenide in liquid crystal or surfactant solutions. The addition of surfactants to solutions often results in a phase change in the solution leading to a liquid crystallinity. A liquid crystal is similar to a solid crystal in that the solution has long range translational order. Examples of this ordering are layered alternating sheets of solution and surfactant, micelles, or even a hexagonal arrangement of rods.\n\nHigh temperature pyrolysis synthesis is usually carried out using an aerosol containing a mixture of volatile cadmium and selenium precursors. The precursor aerosol is then carried through a furnace with an inert gas, such as hydrogen, nitrogen, or argon. In the furnace the precursors react to form CdSe as well as several by-products.\n\nCdSe-derived nanoparticles with sizes below 10 nm exhibit a property known as quantum confinement. Quantum confinement results when the electrons in a material are confined to a very small volume. Quantum confinement is size dependent, meaning the properties of CdSe nanoparticles are tunable based on their size. One type of CdSe nanoparticle is a CdSe quantum dot. This discretization of energy states results in electronic transitions that vary by quantum dot size. Larger quantum dots have closer electronic states than smaller quantum dots which means that the energy required to excite an electron from HOMO to the LUMO is lower than the same electronic transition in a smaller quantum dot. This quantum confinement effect can be observed as a red shift in absorbance spectra for nanocrystals with larger diameters.\n\nCdSe quantum dots have been implemented in a wide range of applications including solar cells, light emitting diodes, and biofluorescent tagging. CdSe-based materials also have potential uses in biomedical imaging. Human tissue is permeable to near infra-red light. By injecting appropriately prepared CdSe nanoparticles into injured tissue, it may be possible to image the tissue in those injured areas.\n\nCdSe quantum dots are usually composed of a CdSe core and a ligand shell. Ligands play important roles in the stability and solubility of the nanoparticles. During synthesis, ligands stabilize growth to prevent aggregation and precipitation of the nanocrystals. These capping ligands also affect the quantum dot’s electronic and optical properties by passivating surface electronic states. An application that depends on the nature of the surface ligands is the synthesis of CdSe thin films. The density of the ligands on the surface and the length of the ligand chain affect the separation between nanocrystal cores which in turn influence stacking and conductivity. Understanding the surface structure of CdSe quantum dots in order to investigate the structure’s unique properties and for further functionalization for greater synthetic variety requires a rigorous description of the ligand exchange chemistry on the quantum dot surface.\n\nA prevailing belief is that trioctylphosphine oxide (TOPO) or trioctylphosphine (TOP), a neutral ligand derived from a common precursor used in the synthesis of CdSe dots, caps the surface of CdSe quantum dots. However, results from recent studies challenge this model. Using NMR, quantum dots have been shown to be nonstoichiometric meaning that the cadmium to selenide ratio is not one to one. CdSe dots have excess cadmium cations on the surface that can form bonds with anionic species such as carboxylate chains. The CdSe quantum dot would be charge unbalanced if TOPO or TOP were indeed the only type of ligand bound to the dot.\n\nThe CdSe ligand shell may contain both X type ligands which form covalent bonds with the metal and L type ligands that form dative bonds. It has been shown that these ligands can undergo exchange with other ligands. Examples of X type ligands that have been studied in the context of CdSe nanocrystal surface chemistry are sulfides and thiocyanates. Examples of L type ligands that have been studied are amines and phosphines (ref). A ligand exchange reaction in which tributylphosphine ligands were displaced by primary alkylamine ligands on chloride terminated CdSe dots has been reported. Stoichiometry changes were monitored using proton and phosphorus NMR. Photoluminescence properties were also observed to change with ligand moiety. The amine bound dots had significantly higher photoluminescent quantum yields than the phosphine bound dots.\n\nCdSe material is transparent to infra-red (IR) light and has seen limited use in photoresistors and in windows for instruments utilizing IR light. The material is also highly luminescent.\n\nCadmium is a toxic heavy metal and appropriate precautions should be taken when handling it and its compounds. Selenides are toxic in large amounts. Cadmium selenide is a known carcinogen to humans and medical attention should be sought if swallowed or if contact with skin or eyes occurs.\n\n"}
{"id": "24860148", "url": "https://en.wikipedia.org/wiki?curid=24860148", "title": "Capa vehicle", "text": "Capa vehicle\n\nA capacitor vehicle or capa vehicle is a traction vehicle that uses supercapacitors (also called ultracapacitors) to store electricity.\n\n, the best ultracapacitors can only store about 5% of the energy that lithium-ion rechargeable batteries can, limiting them to a couple of miles per charge. This makes them ineffective as a general energy storage medium for passenger vehicles. But ultracapacitors can charge much faster than batteries, so in vehicles such as buses that have to stop frequently at known points where charging facilities can be provided, energy storage based exclusively on ultracapacitors becomes viable\n\nChina is experimenting with a new form of electric bus, known as \"Capabus\", which runs without continuous overhead lines (is an autonomous vehicle) by using power stored in large onboard electric double-layer capacitors (EDLCs), which are quickly recharged whenever the vehicle stops at any bus stop (under so-called electric umbrellas), and fully charged in the terminus.\n\nA few prototypes were being tested in Shanghai in early 2005. In 2006 two commercial bus routes began to use electric double-layer capacitor buses; one of them is route 11 in Shanghai. In 2009 Sinautec Automobile Technologies, based in Arlington, Virginia, and its Chinese partner Shanghai Aowei Technology Development Company are testing, with 17 forty-one seat Ultracap Buses serving the Greater Shanghai area since 2006 without any major technical problems. During the Shanghai Expo in 2010, however, 40 super-capacitor buses were being used on a special Expo bus service and owing to the super-capacitors becoming overheated some of the buses broke down. Buses in the Shanghai pilot are made by Germantown, Tennessee-based Foton America Bus Company Another 60 buses will be delivered early next year with ultracapacitors that supply 10 watt-hours per kilogram.\n\nThe buses have very predictable routes and need to stop regularly every or less, allowing quick recharging at charging stations at bus stops. A collector on the top of the bus rises a few feet and touches an overhead charging line at the stop; within a couple of minutes the ultracapacitor banks stored under the bus seats are fully charged. The buses can also capture energy from braking, and the company says that recharging stations can be equipped with solar panels. A third generation of the product, which will give of range per charge or better is planned.\n\nSinautec estimates that one of its buses has one-tenth the energy cost of a diesel bus and can achieve lifetime fuel savings of $200,000. The buses use 40% less electricity even when compared to an electric trolley bus, mainly because they are lighter. The ultracapacitors are made of activated carbon and have an energy density of six watt-hours per kilogram (for comparison a high-performance lithium-ion battery can achieve 200 watt-hours per kilogram, but the ultracapacitor bus is about 40% cheaper than a lithium-ion battery bus and far more reliable).\n\nThere is also a plug-in hybrid version, which also uses ultracaps.\n\nRATP, the public-owned company that manages most of Paris' public transport system, is currently performing tests using a hybrid bus outfitted with ultracapacitors. The model, called Lion's City Hybrid, is supplied by German manufacturer MAN.\n\nFoton America Bus is in talks with New York City, Chicago, and some towns in Florida about trialing the buses.\n\nGSP Belgrade, Serbia has launched the first bus line operated solo by super-capacitor buses from Chinese manufacturer Higer Bus. Since 2014 the public transport authority of the city of Sofia, Bulgaria is also performing tests on a capabus made by Higer.\n\nPantographs and underbody collectors are integrated in bus stops to quick electric bus recharge, making possible to use a smaller battery on the bus, which reduces the investment and subsequent costs.\n\nIn a subway car or tram, an insulator at a track switch may cut off power from the car for a few feet along the line and use a large capacitor to store energy to drive the subway car through the insulator in the power feed.\n\nThe new Nanjing tram uses the supercapacitor technology, with charging hardware at each stop instead of continuous catenary. The first line started operating in 2014. The rail vehicles were produced by CSR Zhuzhou; according to the manufacturers, they are the world's first low-floor tram completely powered by supercapacitors.\n\nIn 2001 and 2002 VAG, the public transport operator in Nuremberg, Germany, tested a hybrid bus which uses a diesel-electric drive system with electric double-layer capacitors.\n\nSince 2003 Mannheim Stadtbahn in Mannheim, Germany has operated a capa vehicle, an LRV (light-rail vehicle), which uses electric double-layer capacitors to store braking energy.\n\nOther companies from the public transport manufacturing sector are developing electric double-layer capacitor technology: The Transportation Systems division of Siemens AG is developing a mobile energy storage based on ELDCs called Sibac Energy Storage and also Sitras SES, a stationary version integrated into the trackside power supply.\nAdetel Group has developed its own energy saver named ″NeoGreen″ for LRV, LRT and metros. \nThe company Cegelec is also developing an ELDC-based energy storage system.\n\nProton Power Systems has created the world's first triple hybrid forklift truck, which uses fuel cells and batteries as primary energy storage with ELDCs to supplement them.\n\nUniversity of Southampton spin-out Nanotecture has received a Government grant to develop supercapacitors for hybrid vehicles. The company is set to receive £376,000 from the DTI in the UK for a project entitled “next generation supercapacitors for hybrid vehicle applications”. The project also involves Johnson Matthey and HILTech Developments. The project will use supercapacitor technology to improve hybrid electric vehicles and increase overall energy efficiency\n\nSinautec is in discussions with MIT's Schindall about developing ultracapacitors of higher energy density using vertically aligned carbon nanotube structures that give the devices more surface area for holding a charge. So far they are able to get twice the energy density of an existing ultracapacitor, but they are trying to get about five times. This would create an ultracapacitor with one-quarter of the energy density of a lithium-ion battery.\n\nFuture developments includes the use of inductive charging under the street, to avoid overhead wiring. A pad under each bus stop and at each stop light along the way would be used.\n\nThe FIA, the governing body for many motor racing events, proposed in the \"Power-Train Regulation Framework for Formula 1\" version 1.3 of 23 May 2007 that a new set of power train regulations be issued that includes a hybrid drive of up to 200 kW input and output power using \"superbatteries\" made with both batteries and supercapacitors.\n\nUltracapacitors are used in some electric vehicles, such as AFS Trinity's concept prototype, to store rapidly available energy with their high power density, in order to keep batteries within safe resistive heating limits and extend battery life. The Ultrabattery combines a supercapacitor and a battery in a single unit, creating an electric vehicle battery that lasts longer, costs less and is more powerful than current technologies used in plug-in hybrid electric vehicles (PHEVs).\n\n\n"}
{"id": "14510126", "url": "https://en.wikipedia.org/wiki?curid=14510126", "title": "Central Research Institute of Electric Power Industry", "text": "Central Research Institute of Electric Power Industry\n\nThe Central Research Institute of Electric Power Industry (CRIEPI; 電力中央研究所) is a Japanese non-profit foundation that conducts research and development of technologies in a variety of scientific and technical fields related to the electric power industry. Also, CRIEPI researches many aspects of social matters through subordinate laboratories. CRIEPI not only engages in research and education in Japan, but also provides education, training and technology transfer worldwide.\n\nIt is similar to the U.S. EPRI, though its energy research extends into areas that could be considered to be in the domain of the USDOE's national laboratories in the United States, such as research and design of fission reactor concepts.\n\nIn 2004, CRIEPI developed a U.S. presence due to the proposed Galena Nuclear Power Plant in Galena, Alaska, that is proposed to use an advanced reactor design – the Toshiba 4S – developed by Toshiba Corporation and CRIEPI.\n\n\n\n"}
{"id": "28979212", "url": "https://en.wikipedia.org/wiki?curid=28979212", "title": "Choji oil", "text": "Choji oil\n\nChoji oil is a traditional Japanese blade preserving compound. It is used in the maintenance of high quality collectible blades. It contains clove oil extract and mineral oil, usually at a ratio of 1:10 to 1:100.\n"}
{"id": "30101288", "url": "https://en.wikipedia.org/wiki?curid=30101288", "title": "Chugoku Electric Power Company", "text": "Chugoku Electric Power Company\n\nIn 1982, Chugoku Electric Power Company proposed building a nuclear power plant near the island of Iwaishima, but many residents opposed the idea, and the island’s fishing cooperative voted overwhelmingly against the plans. In January 1983, almost 400 islanders staged a protest march, which was the first of more than 1,000 protests the islanders carried out. Since the Fukushima nuclear disaster in March 2011 there has been wider opposition to construction plans for the plant.\n\n"}
{"id": "14515809", "url": "https://en.wikipedia.org/wiki?curid=14515809", "title": "Colored-particle-in-cell", "text": "Colored-particle-in-cell\n\nA particle in cell simulation for non-Abelian (colored) particles and fields. Can be used to simulate an equilibrium or non-equilibrium quark-gluon plasma.\n\n"}
{"id": "46250924", "url": "https://en.wikipedia.org/wiki?curid=46250924", "title": "Cubical set", "text": "Cubical set\n\nIn topology, a branch of mathematics, a cubical set is a set-valued contravariant functor on the category of (various) \"n\"-cubes. See the references for the more precise definitions.\n\n\n"}
{"id": "46901507", "url": "https://en.wikipedia.org/wiki?curid=46901507", "title": "Electric motorsport", "text": "Electric motorsport\n\nElectric motorsport (also known as electric racing or electric motor racing) is a category of motor sport that consists of the racing of electric powered vehicles for competition, either in all-electric series, or in open-series against vehicles with different power trains. Very early in the history of automobiles, electric cars held several performance records over internal combustion engine cars, such as land speed records, but fell behind in performance during the first decade of the 20th century. With the renaissance of electric vehicles during the early 21st century, notable electric-only racing series have been developed, for both cars and motorcycles, including for example, the FIA Formula E Championship. In other racing events, electric vehicles are competing alongside combustion engine vehicles, for example in the Isle of Man TT and the Pikes Peak International Hill Climb, and in some cases winning outright.\n\nsee also: History of the steam engine and Origins of the locomotive\n\nEarly mechanically powered vehicles used steam power, a technology first developed for static applications (notably, Thomas Newcomen 1712 and James Watt 1765) (see History of the steam engine). Steam for vehicle traction was taken up both for road vehicles and for rail by Richard Trevithick who creating the Puffing devil for transporting passengers by road in 1801, and later rail transport, initially for coal (1804) and then for people (Trevethick 1808, George Stephenson 1824 onwards). By the 1830s steam began to be more widely adopted for transportation, with steam carriages for road (e.g. the 1827 Goldsworthy Gurney Steam bus) and for rail, although the latter quickly became more established for medium and longer distance travel. Mechanically powered road vehicles were largely limited to utilitarian vehicles such as traction engines during this period (especially 1850s onwards, see History of steam road vehicles).\n\nDuring the 1860s diverse small experiments with personal transportation and different powertrains blossomed, with steam buggies (e.g. Henry Taylor 1867) and even steam motorcycles (Michaex-Perreux and Sylvester Roper, both around 1867-69). Amédée Bollée developed several long distance (Le Mans to Paris, 210 km) steam vehicles from 1873 onwards, including the 1878 La Mancelle of which 50 were produced, and the 1881 La Rapide capable of 62 km/h (39 mph). An early electric powertrain was exhibited in November 1881 by French inventor Gustave Trouvé at the International Exhibition of Electricity in Paris.\n\nEnglish inventor Thomas Parker, who was responsible for innovations such as electrifying the London Underground, overhead tramways in Liverpool and Birmingham, and the smokeless fuel coalite, built the first production electric car in London in 1884, using his own specially designed high-capacity rechargeable batteries. Parker's long-held interest in the construction of more fuel-efficient vehicles led him to experiment with electric vehicles. He also may have been concerned about the malign effects smoke and pollution were having in London.\n\nEarly petrol/gasoline-powered internal combustion engine automobiles were completed almost simultaneously by several German inventors working independently: Karl Benz built his first automobile in 1885 in Mannheim. Benz was granted a patent for his automobile on 29 January 1886, and began the first production of automobiles in 1888, after Bertha Benz, his wife, had proved - with the first long-distance trip in August 1888, from Mannheim to Pforzheim and back (194 km) - that the horseless coach was absolutely suitable for daily use.\n\nOverall, there were a variety of powertrains and vehicle forms experimented with during this period, each with different advantages and disadvantages, range, reliability and speed. In terms of outright performance, different powertrains competed for the land speed record through the turn of the 20th century (see below), and it was not until 1924 onwards that internal combustion powertrains began to dominate this aspect.\n\nThe table below details the early history of land speed records from 1898 into the early decades of the 20th century. \"La Jamais Contente\" () was the first road vehicle to go over . It was an electric vehicle with a light alloy torpedo shaped bodywork and with Fulmen batteries. The vehicle established the land speed record on April 29 or May 1, 1899 at Achères, Yvelines near Paris, France. The vehicle had two direct drive Postel-Vinay 25 kW motors, running at 200 V drawing 124 Amperes each for about 68 hp, and was equipped with Michelin tires.\n\nAs of 1900, 38% of US automobiles, 33,842 cars, were powered by electricity (40% by steam, and 22% by gasoline). However, as combustion powertrains developed, they offered a superior range than electrics, and (especially after the 1908 Ford Model T, and its mass production from 1913 onwards) a much lower purchase price. In 1912 the electric starter motor was invented by Charles Kettering leading to easier and faster starting of internal combustion powertrains, and removing what had been perceived as one of their main drawbacks (having to use a hand crank). Electric and subsequently steam still had some performance advantages and dominated the outright speed record until 1924. Yet the combustion engine technology benefitted from much greater market penetration and thus more development, and began to achieve greater speed performance than electrics and stream from 1924 onwards.\n\nThe emergence of higher volume manufacturing of electric powertrain vehicles has allowed for economies of scale and increased research and development of electrical batteries, the critical technology in electric powertrains. Battery power density (the amount of energy stored per unit weight) has typically been the greatest drawback of electric powertrains in comparison to gasoline combustion engine powertrains. Steady advances in battery technology, especially lithium-ion battery technologies first commercialized in the early 1990s have driven innovations in electric powertrains, and further battery advances allow them to once again compete with the combustion engine powertrains in motorsports events.\n\nElectric drag racing is a sport where electric vehicles start from standstill and attempt the highest possible speed over a short given distance. They sometimes race and usually beat gasoline sports cars. Organizations such as NEDRA keep track of records worldwide using certified equipment.\n\nAt the Formula Student competition at the Silverstone Circuit in July 2013, the electric powered car of the ETH Zurich won against all cars with internal combustion engines. It is believed to be the first time that an electric vehicle has beaten cars powered by combustion engines in any accredited motorsport competition.\n\nIn 2015, an electric car won all places of the Pikes Peak International Hill Climb. Also in that year the second place on all classes was won by an electric car. Already in 2014, electric cars had won second and third place.\n\nIn January 2017, a pure electric car participated in the Paris-Dakar Rally and completed the entire route of through Argentina, Paraguay and Bolivia. The vehicle had been specially designed and built for the race. The car had a 250 kW engine (340 hp) and a 150 kWh battery. The battery consisted of several modules. Each module could be charged separately by power cable to speed up the charging process.\n\nThe Pikes Peak International Hill Climb, also known as \"The Race to the Clouds\", is an annual automobile and motorcycle hillclimb to the summit of Pikes Peak in Colorado, USA. The track measures over 156 turns, climbing from the start at Mile 7 on Pikes Peak Highway, to the finish at , on grades averaging 7.2%.\n\nThe race is self sanctioned and has taken place since 1916, making it the second oldest motorsport event in the Western Hemisphere behind the Indianapolis 500. It is currently contested by a variety of classes of cars, trucks, motorcycles and quads. There are often numerous new classes tried and discarded year-to-year. In the modern era, electric vehicles have competed in the event since 1983 (Joe Balls, Sears Electric car). In the 2-wheeled divisions, the electric powertrains are already able to outcompete the combustion engines; the 2013 overall winner was an electric bike, the Lightning Motorcycle LS-218 electric Superbike ridden by Carlin Dunne in a time of 10:00.694 minutes, a new course record for the 2-wheeled class.\n\nIn the 4-Wheeled Divisions, the results of recent years show that the relative improvements in race time are advancing at different rates in the electric classes compared to the unlimited/combustion engine classes. Unlimited class vehicles, with combustion engines, broke the 16 minute barrier in 1938, and have steadily improved over the subsequent decades, breaking the 10-minute barrier in 2011. Meanwhile, the electric vehicle class broke the 16 minute barrier in 1994 (Katy Endicott, Honda Civic Shuttle finishing in 15:44:71 minutes), and the 10-minute barrier in 2013 (Nabuhiro 'Monster' Tajima, E-Runner finishing in 9:46:53 minutes). In the 2014 event, the Mitsubishi team (MiEV Evolution III) achieved 2nd (Greg Tracey, 9:08.188 minutes) and 3rd position (Hiroshi Masuoka, 9:12:204 minutes) - within 3 seconds and 7 seconds (respectively) of the overall fastest vehicle, a gasoline powered Norma race car. The graph to the right shows the relative rates of improvement of these two classes since the 1980s.\n\n2015 for the first time in the history of the race was an electric car to win the race outright. The winning car was the Latvian team Drive eO's 3rd generation vehicle, eO PP03 with peak power of 1020 kW and peak torque of 2160 Nm, weighing 1200 kg. The time achieved was 9:07.222 minutes, just faster than Greg Tracey's 9:08.188 time in 2014. Second place was also earned by an electric car, the Tajima Rimac Automobili E-Runner Concept_One with peak power of 1100 kW, peak torque of 1500 Nm, and weighing 1500 kg.\n\nThe winning driver Rhys Millen said in an interview that the vehicle had lost half its tractive power (due to heat) from around the halfway point. Based on testing the team had expected a run 30 seconds faster.\n\nBoth the two above teams and drivers raced again in 2016, with evolutions of the vehicles, and were joined by a prototype '4-motor EV' from Acura based on the 2016 NSX production car, with heavy modifications to all-electric drive. The Acura, driven by Tetsuya Yamano, achieved a remarkable time for its first outing, finishing in 9:06:015 minutes, and third place overall. Drive eO's 4th generation vehicle, eO PP100 achieved a time of 8:57:118 minutes, improving on the previous year's time and coming second overall. The winning 2016 vehicle was a combustion engine Norma driven by Romain Dumas with a time 8:51:445 minutes. The Norma has similar power to weight ratio to the PP100, but weighs half as much and as a result has better traction in corners. Dumas' team also won in an earlier version of the same vehicle in 2014 with a time of 9:05:801 minutes, thus improving by some 14 seconds over 2 years technological evolution. The Drive eO vehicle improved by some 10 seconds in one year, over its 2015 time.\n\nIn 2018, the outright winner was the all electric I.D. R Pikes Peak Prototype driven by Romain Dumas, in new overall record time 7:57.148 minutes. The previous track record stood at 8:13.878 minutes, driven by Sébastien Loeb in the 2013 Peugeot 208 T16 Pikes Peak. Now that electrics have overtaken the performance of gasoline cars on Pike's Peak, it seems likely that electric cars will continue to dominate this race from 2018 onwards. Electric cars are still relatively early in their motorsport development with much room for further improvement, whereas gasoline powered cars have had over a century of development, with only incremental improvements likely to come in the future.\n\nTT Zero is part of the Isle of Man TT and races for 1 lap (37.733 miles) of the Snaefell Mountain Course. The TT Zero event as an officially sanctioned Isle of Man TT race is for racing motorcycles where \"\"The technical concept is for motorcycles (two wheeled) to be powered without the use of carbon based fuels and have zero toxic/noxious emissions\".\"\n\nThe inaugural 2010 TT Zero race was won by Mark Miller riding a MotoCzysz E1pc motor-cycle in 23 minutes and 22.89 seconds at an average race speed of 96.820 mph for 1 lap (37.733 miles) of the Mountain Course and the first United States winner since Dave Roper won the 1984 Historic TT riding a 500cc Matchless. The TT Zero race replaced the TTXGP franchise with the simplification of the regulations and the emphasis on electric powered motor-cycles. The MotoCzysz E1pc was also the first American manufactured motor-cycle to win an Isle of Man TT Race since Oliver Godfrey won the 1911 Senior TT with an Indian V-Twin motor-cycle.\n\nThe riders of the TT Zero bikes are typically those that also compete in the combustion engine classes and are very experienced on the circuit. Comparing the experience of the different powertrains, Lee Johnston said as he climbed off his electric bike in the 2015 practice sessions: “That was just mint. It feels so stable, it’s unbelievable. It’s just so peaceful. No revving.” Asked for what was memorable, he responded “I think just the peace and quiet and riding over the mountain, no noise and seeing the sunset...\"\n\n\"(practice & qualifying session laps not included)\"\n\nThe TT Zero lap speeds have been improving at an average rate of around 2 mph in recent years (121.824 mph as of 2018). With more mature technology, the combustion engine bikes' lap speeds in the mainstream TT have been improving at a lower rate of around 1 mph each year in recent years (135.452 mph as of 2018). It's likely that the electric bikes will surpass the combustion bikes at some point, but it may not happen until the mid to late 2020s.\n\nThe FIA World Endurance Championship is an auto racing world championship organized by the Automobile Club de l'Ouest (ACO) and sanctioned by the Fédération Internationale de l'Automobile (FIA). The series usurps the ACO's former Intercontinental Le Mans Cup which began in 2010, and is the first endurance series of world championship status since the demise of the World Sportscar Championship at the end of 1992. The World Endurance Championship name was previously used by the FIA from 1981 to 1985.\n\nThe series feature multiple classes of cars competing in endurance races, with sports prototypes competing in the Le Mans Prototype categories, and production-based grand tourers competing in the LM GTE categories. World champion titles are awarded to the top scoring manufacturers and drivers over the season, while other cups and trophies will be awarded for drivers and private teams.\n\nThe Nissan ZEOD RC was designed by Ben Bowlby, who previously designed the 2012 Garage 56 entry DeltaWing as an employee for DeltaWing Project 56 LLC, a consortium led by Don Panoz. He previously worked for DeltaWing LLC, a Chip Ganassi company created to develop a DeltaWing concept race car for IndyCar. Nissan provided an engine and received naming rights on the Garage 56 entry at the 2012 Le Mans race, as well as and other 2012 American Le Mans Series races\n\nThe ZEOD RC had a hybrid electric drivetrain with lithium ion battery packs in a chassis similar in design to the DeltaWing. \nIn a June 22, 2013 article at Autosport.com, Bowlby said: \"This is a new car, but it uses the narrow track technology of the DeltaWing and that gives us great efficiency. It is something we understand and it is an efficient way of getting around Le Mans.\"\n\nAt the 2014 24 Hours of Le Mans, the car had to retire during the race's early hours due to a gearbox failure. However it managed to achieve its goals of reaching a speed above 300 km/h and completing a lap in Le Mans using electric power only.\n\nIn January 2017, a pure electric car participated in the Paris-Dakar Rally and completed the entire route of 9000 km through Argentina, Paraguay and Bolivia. The vehicle had been specially designed and built for the race. The car had a 250 kW engine (340 hp) and a 150 kWh battery. The battery consisted of several modules. Each module could be charged separately by power cable to speed up the charging process.\n\nFormula E is a class of auto racing, sanctioned by the Fédération Internationale de l'Automobile (FIA), and is the highest class of competition for one-make, single-seater, electrically powered racing cars. The series was conceived in 2012, and the inaugural championship started in Beijing in September 2014.\n\nThe World Rallycross Championship (WRX) will be all-electric starting with the 2020 season. The cars will be four wheel drive with an electric motor on each axle for a full power output of 500kW (671bhp). Manufacturers will provide a power-train and bodywork for a common chassis and battery provided by Oreca and Williams respectively.\n\nA series with all-electric GT cars (in the first season all Tesla Model S), called \"Electric GT\" (EGT) is planned to start in 2019.\n\nA new series for all-electric touring cars was announced ahead of the Geneva Motor Show by TCR promoter WSC Ltd. New SEAT performance brand CUPRA are lined up as the first entrant to the series. The series is planned to start in 2020.\n\nThe Andros Trophy, a French ice racing series, began experimenting with electric cars in 2007. An electric car class was added in 2010. The car, developed by Exagon, features a 67 kW engine and a total weight of 800 kg.\n\nThe World Solar Challenge is a biennial solar-powered car race which covers through the Australian Outback, from Darwin, Northern Territory to Adelaide, South Australia.\n\nThe race attracts teams from around the world, most of which are fielded by universities or corporations although some are fielded by high schools. The race has a 28-year history spanning twelve races, with the inaugural event taking place in 1987.\n\nAmerican-based Global Rallycross Championship announced in 2016 that they want to add an electric class. However, Global Rallycross folded in 2018.\n\nThe Targa West in Western Australia has featured an Electric Vehicle since 2016 and has several road registered Electric Vehicles competing in the Targa South West and Quit Targa West 2018.\n\nThe MotoGP motorcycle world championship will have an all-electric support series called \"MotoE World Cup\" by 2019.\n\nElectric powertrains have advantages over combustion engines in power delivery and vehicle dynamics (especially on motorbikes), but still have range disadvantages in longer races (note that combustion engine vehicle often have to refill energy supply also, e.g. Isle of Man TT bikes refill every two laps). Early electric challengers to combustion engine vehicles are therefore typically in shorter more intensive races such as hill climbs or other limited distance races, or simply in fastest lap times (e.g. around the Isle of Man Snaefell circuit). Nevertheless, for endurance racing, hybrid electric powertrains have also proven their advantages over pure combustion engine powertrains, with recent years at the 24 Hours of Le Mans all won by the hybrid electric Audi R18 e-tron quattro.\n\n"}
{"id": "14798494", "url": "https://en.wikipedia.org/wiki?curid=14798494", "title": "Electrochemiluminescence", "text": "Electrochemiluminescence\n\nElectrochemiluminescence or electrogenerated chemiluminescence (ECL) is a kind of luminescence produced during electrochemical reactions in solutions. In electrogenerated chemiluminescence, electrochemically generated intermediates undergo a highly exergonic reaction to produce an electronically excited state that then emits light upon relaxation to a lower-level state. This wavelength of the emitted photon of light corresponds to the energy gap between these two states. ECL excitation can be caused by energetic electron transfer (redox) reactions of electrogenerated species. Such luminescence excitation is a form of chemiluminescence where one/all reactants are produced electrochemically on the electrodes.\n\nECL is usually observed during application of potential (several volts) to electrodes of electrochemical cell that contains solution of luminescent species (polycyclic aromatic hydrocarbons, metal complexes, Quantum Dots or Nanoparticles ) in aprotic organic solvent (ECL composition).\nIn organic solvents both oxidized and reduced forms of luminescent species can be produced at different electrodes simultaneously or at a single one by sweeping its potential between oxidation and reduction. The excitation energy is obtained from recombination of oxidized and reduced species.\n\nIn aqueous medium, which is mostly used for analytical applications, simultaneous oxidation and reduction of luminescent species is difficult to achieve due to electrochemical splitting of water itself so the ECL reaction with the coreactants is used. In the later case luminescent species are oxidized at the electrode together with the coreactant which gives a strong reducing agent after some chemical transformations (the oxidative reduction mechanism).\n\nECL proved to be very useful in analytical applications as a highly sensitive and selective method. It combines analytical advantages of chemiluminescent analysis (absence of background optical signal) with ease of reaction control by applying electrode potential. As an analytical technique it presents outstanding advantages over other common analytical methods due to its versatility, simplified optical setup compared with photoluminescence (PL), and good temporal and spatial control compared with chemiluminescence (CL). Enhanced selectivity of ECL analysis is reached by variation of electrode potential thus controlling species that are oxidized/reduced at the electrode and take part in ECL reaction (see electrochemical analysis).\n\nIt generally uses Ruthenium complexes, especially [Ru (Bpy)] (which releases a photon at ~620 nm) regenerating with TPrA (Tripropylamine) in liquid phase or liquid–solid interface. It can be used as monolayer immobilized on an electrode surface (made e.g. of nafion, or special thin films made by Langmuir–Blogett technique or self-assembly technique) or as a coreactant or more commonly as a tag and used in HPLC, Ru tagged antibody based immunoassays, Ru Tagged DNA probes for PCR etc., NADH or HO generation based biosensors, oxalate and organic amine detection and many other applications and can be detected from picomolar sensitivity to dynamic range of more than six orders of magnitude. Photon detection is done with photomultiplier tubes (PMT) or silicon photodiode or gold coated fiber-optic sensors.The importance of ECL techniques detection for bio-related applications has been well established. ECL is heavily used commercially for many clinical lab applications.\n\n"}
{"id": "1633227", "url": "https://en.wikipedia.org/wiki?curid=1633227", "title": "Flownet", "text": "Flownet\n\nA flownet is a graphical representation of two-dimensional steady-state groundwater flow through aquifers.\n\nConstruction of a flownet is often used for solving groundwater flow problems where the geometry makes analytical solutions impractical. The method is often used in civil engineering, hydrogeology or soil mechanics as a first check for problems of flow under hydraulic structures like dams or sheet pile walls. As such, a grid obtained by drawing a series of equipotential lines is called a flownet. The flownet is an important tool in analysing two-dimensional irrotational flow problems. Flow net technique is a graphical representation method.\n\nThe method consists of filling the flow area with stream and equipotential lines, which are everywhere perpendicular to each other, making a curvilinear grid. Typically there are two surfaces (boundaries) which are at constant values of potential or hydraulic head (upstream and downstream ends), and the other surfaces are no-flow boundaries (i.e., impermeable; for example the bottom of the dam and the top of an impermeable bedrock layer), which define the sides of the outermost streamtubes (see figure 1 for a stereotypical flownet example).\n\nMathematically, the process of constructing a flownet consists of contouring the two harmonic or analytic functions of potential and stream function. These functions both satisfy the Laplace equation and the contour lines represent lines of constant head (equipotentials) and lines tangent to flowpaths (streamlines). Together, the potential function and the stream function form the complex potential, where the potential is the real part, and the stream function is the imaginary part.\n\nThe construction of a flownet provides an approximate solution to the flow problem, but it can be quite good even for problems with complex geometries by following a few simple rules (initially developed by Philipp Forchheimer around 1900, and later formalized by Arthur Casagrande in 1937) and a little practice:\n\nThe first flownet pictured here (modified from Craig, 1997) illustrates and quantifies the flow which occurs under the dam (flow is assumed to be invariant along the axis of the dam — valid near the middle of the dam); from the pool behind the dam (on the right) to the tailwater downstream from the dam (on the left). \n\nThere are 16 green equipotential lines (15 equal drops in hydraulic head) between the 5 m upstream head to the 1m downstream head (4 m / 15 head drops = 0.267 m head drop between each green line). The blue streamlines (equal changes in the streamfunction between the two no-flow boundaries) show the flowpath taken by water as it moves through the system; the streamlines are everywhere tangent to the flow velocity.\nThe second flownet pictured here (modified from Ferris, et al., 1962) shows a flownet being used to analyze map-view flow (invariant in the vertical direction), rather than a cross-section. Note that this problem has symmetry, and only the left or right portions of it needed to have been done. To create a flownet to a point sink (a singularity), there must be a recharge boundary nearby to provide water and allow a steady-state flowfield to develop.\n\nDarcy's law describes the flow of water through the flownet. Since the head drops are uniform by construction, the gradient is inversely proportional to the size of the blocks. Big blocks mean there is a low gradient, and therefore low discharge (hydraulic conductivity is assumed constant here). \n\nAn equivalent amount of flow is passing through each streamtube (defined by two adjacent blue lines in diagram), therefore narrow streamtubes are located where there is more flow. The smallest squares in a flownet are located at points where the flow is concentrated (in this diagram they are near the tip of the cutoff wall, used to reduce dam underflow), and high flow at the land surface is often what the civil engineer is trying to avoid, being concerned about or dam failure.\n\nIrregular points (also called singularities) in the flow field occur when streamlines have kinks in them (the derivative doesn't exist at a point). This can happen where the bend is outward (e.g., the bottom of the cutoff wall in the figure above), and there is infinite flux at a point, or where the bend is inward (e.g., the corner just above and to the left of the cutoff wall in the figure above) where the flux is zero. \n\nThe second flownet illustrates a well, which is typically represented mathematically as a point source (the well shrinks to zero radius); this is a singularity because the flow is converging to a point, at that point the Laplace equation is not satisfied. \n\nThese points are mathematical artifacts of the equation used to solve the real-world problem, and do not actually mean that there is infinite or no flux at points in the subsurface. These types of points often do make other types of solutions (especially numeric) to these problems difficult, while the simple graphical technique handles them nicely.\n\nTypically flownets are constructed for homogeneous, isotropic porous media experiencing saturated flow to known boundaries. There are extensions to the basic method to allow some of these other cases to be solved:\n\nAlthough the method is commonly used for these types of groundwater flow problems, it can be used for any problem which is described by the Laplace equation (formula_1), for example electric current flow through the earth.\n\n\n"}
{"id": "48349793", "url": "https://en.wikipedia.org/wiki?curid=48349793", "title": "France Nature Environnement", "text": "France Nature Environnement\n"}
{"id": "10821", "url": "https://en.wikipedia.org/wiki?curid=10821", "title": "Francium", "text": "Francium\n\nFrancium is a chemical element with symbol Fr and atomic number 87. It used to be known as eka-caesium. It is extremely radioactive; its most stable isotope, francium-223 (originally called actinium K after the natural decay chain it appears in), has a half-life of only 22 minutes. It is the second-least electronegative element, behind only caesium, and is the second rarest naturally occurring element (after astatine). The isotopes of francium decay quickly into astatine, radium, and radon. As an alkali metal, it has one valence electron.\n\nBulk francium has never been viewed. Because of the general appearance of the other elements in its periodic table column, it is assumed that francium would appear as a highly reactive metal, if enough could be collected together to be viewed as a bulk solid or liquid. Obtaining such a sample is highly improbable, since the extreme heat of decay (the half-life of its longest-lived isotope is only 22 minutes) would immediately vaporize any viewable quantity of the element.\n\nFrancium was discovered by Marguerite Perey in France (from which the element takes its name) in 1939. It was the last element first discovered in nature, rather than by synthesis. Outside the laboratory, francium is extremely rare, with trace amounts found in uranium and thorium ores, where the isotope francium-223 continually forms and decays. As little as 20–30 g (one ounce) exists at any given time throughout the Earth's crust; the other isotopes (except for francium-221) are entirely synthetic. The largest amount produced in the laboratory was a cluster of more than 300,000 atoms.\n\nFrancium is one of the most unstable of the naturally occurring elements: its longest-lived isotope, francium-223, has a half-life of only 22 minutes. The only comparable element is astatine, whose most stable natural isotope, astatine-219 (the alpha daughter of francium-223), has a half-life of 56 seconds, although synthetic astatine-210 is much longer-lived with a half-life of 8.1 hours. All isotopes of francium decay into astatine, radium, or radon. Francium-223 also has a shorter half-life than the longest-lived isotope of each synthetic element up to and including element 105, dubnium.\n\nFrancium is an alkali metal whose chemical properties mostly resemble those of caesium. A heavy element with a single valence electron, it has the highest equivalent weight of any element. Liquid francium—if created—should have a surface tension of 0.05092 N/m at its melting point. Francium's melting point was calculated to be around 27 °C (80 °F, 300 K). The melting point is uncertain because of the element's extreme rarity and radioactivity. Thus, the estimated boiling point value of 677 °C (1250 °F, 950 K) is also uncertain.\n\nLinus Pauling estimated the electronegativity of francium at 0.7 on the Pauling scale, the same as caesium; the value for caesium has since been refined to 0.79, but there are no experimental data to allow a refinement of the value for francium. Francium has a slightly higher ionization energy than caesium, 392.811(4) kJ/mol as opposed to 375.7041(2) kJ/mol for caesium, as would be expected from relativistic effects, and this would imply that caesium is the less electronegative of the two. Francium should also have a higher electron affinity than caesium and the Fr ion should be more polarizable than the Cs ion. The CsFr molecule is predicted to have francium at the negative end of the dipole, unlike all known heterodiatomic alkali metal molecules. Francium superoxide (FrO) is expected to have a more covalent character than its lighter congeners; this is attributed to the 6p electrons in francium being more involved in the francium–oxygen bonding.\n\nFrancium coprecipitates with several caesium salts, such as caesium perchlorate, which results in small amounts of francium perchlorate. This coprecipitation can be used to isolate francium, by adapting the radiocaesium coprecipitation method of Lawrence E. Glendenin and Nelson. It will additionally coprecipitate with many other caesium salts, including the iodate, the picrate, the tartrate (also rubidium tartrate), the chloroplatinate, and the silicotungstate. It also coprecipitates with silicotungstic acid, and with perchloric acid, without another alkali metal as a carrier, which provides other methods of separation. Nearly all francium salts are water-soluble.\n\nThere are 34 known isotopes of francium ranging in atomic mass from 199 to 232. Francium has seven metastable nuclear isomers. Francium-223 and francium-221 are the only isotopes that occur in nature, though the former is far more common.\n\nFrancium-223 is the most stable isotope, with a half-life of 21.8 minutes, and it is highly unlikely that an isotope of francium with a longer half-life will ever be discovered or synthesized. Francium-223 is the fifth product of the actinium decay series as the daughter isotope of actinium-227. Francium-223 then decays into radium-223 by beta decay (1.149 MeV decay energy), with a minor (0.006%) alpha decay path to astatine-219 (5.4 MeV decay energy).\n\nFrancium-221 has a half-life of 4.8 minutes. It is the ninth product of the neptunium decay series as a daughter isotope of actinium-225. Francium-221 then decays into astatine-217 by alpha decay (6.457 MeV decay energy).\n\nThe least stable ground state isotope is francium-215, with a half-life of 0.12 μs: it undergoes a 9.54 MeV alpha decay to astatine-211. Its metastable isomer, francium-215m, is less stable still, with a half-life of only 3.5 ns.\n\nDue to its instability and rarity, there are no commercial applications for francium. It has been used for research purposes in the fields of chemistry\nand of atomic structure. Its use as a potential diagnostic aid for various cancers has also been explored, but this application has been deemed impractical.\n\nFrancium's ability to be synthesized, trapped, and cooled, along with its relatively simple atomic structure, has made it the subject of specialized spectroscopy experiments. These experiments have led to more specific information regarding energy levels and the coupling constants between subatomic particles. Studies on the light emitted by laser-trapped francium-210 ions have provided accurate data on transitions between atomic energy levels which are fairly similar to those predicted by quantum theory.\n\nAs early as 1870, chemists thought that there should be an alkali metal beyond caesium, with an atomic number of 87. It was then referred to by the provisional name \"eka-caesium\". Research teams attempted to locate and isolate this missing element, and at least four false claims were made that the element had been found before an authentic discovery was made.\n\nSoviet chemist D. K. Dobroserdov was the first scientist to claim to have found eka-caesium, or francium. In 1925, he observed weak radioactivity in a sample of potassium, another alkali metal, and incorrectly concluded that eka-caesium was contaminating the sample (the radioactivity from the sample was from the naturally occurring potassium radioisotope, potassium-40). He then published a thesis on his predictions of the properties of eka-caesium, in which he named the element \"russium\" after his home country. Shortly thereafter, Dobroserdov began to focus on his teaching career at the Polytechnic Institute of Odessa, and he did not pursue the element further.\n\nThe following year, English chemists Gerald J. F. Druce and Frederick H. Loring analyzed X-ray photographs of manganese(II) sulfate. They observed spectral lines which they presumed to be of eka-caesium. They announced their discovery of element 87 and proposed the name \"alkalinium\", as it would be the heaviest alkali metal.\n\nIn 1930, Fred Allison of the Alabama Polytechnic Institute claimed to have discovered element 87 when analyzing pollucite and lepidolite using his magneto-optical machine. Allison requested that it be named \"virginium\" after his home state of Virginia, along with the symbols Vi and Vm. In 1934, H.G. MacPherson of UC Berkeley disproved the effectiveness of Allison's device and the validity of his discovery.\n\nIn 1936, Romanian physicist Horia Hulubei and his French colleague Yvette Cauchois also analyzed pollucite, this time using their high-resolution X-ray apparatus. They observed several weak emission lines, which they presumed to be those of element 87. Hulubei and Cauchois reported their discovery and proposed the name \"moldavium\", along with the symbol Ml, after Moldavia, the Romanian province where Hulubei was born. In 1937, Hulubei's work was criticized by American physicist F. H. Hirsh Jr., who rejected Hulubei's research methods. Hirsh was certain that eka-caesium would not be found in nature, and that Hulubei had instead observed mercury or bismuth X-ray lines. Hulubei insisted that his X-ray apparatus and methods were too accurate to make such a mistake. Because of this, Jean Baptiste Perrin, Nobel Prize winner and Hulubei's mentor, endorsed moldavium as the true eka-caesium over Marguerite Perey's recently discovered francium. Perey took pains to be accurate and detailed in her criticism of Hulubei's work, and finally she was credited as the sole discoverer of element 87. All other previous purported discoveries of element 87 were ruled out due to francium's very limited half-life.\n\nEka-caesium was discovered in 1939 by Marguerite Perey of the Curie Institute in Paris, when she purified a sample of actinium-227 which had been reported to have a decay energy of 220 keV. Perey noticed decay particles with an energy level below 80 keV. Perey thought this decay activity might have been caused by a previously unidentified decay product, one which was separated during purification, but emerged again out of the pure actinium-227. Various tests eliminated the possibility of the unknown element being thorium, radium, lead, bismuth, or thallium. The new product exhibited chemical properties of an alkali metal (such as coprecipitating with caesium salts), which led Perey to believe that it was element 87, produced by the alpha decay of actinium-227. Perey then attempted to determine the proportion of beta decay to alpha decay in actinium-227. Her first test put the alpha branching at 0.6%, a figure which she later revised to 1%.\n\nPerey named the new isotope \"actinium-K\" (it is now referred to as francium-223) and in 1946, she proposed the name \"catium\" (Cm) for her newly discovered element, as she believed it to be the most electropositive cation of the elements. Irène Joliot-Curie, one of Perey's supervisors, opposed the name due to its connotation of \"cat\" rather than \"cation\"; furthermore, the symbol coincided with that which had since been assigned to curium. Perey then suggested \"francium\", after France. This name was officially adopted by the International Union of Pure and Applied Chemistry (IUPAC) in 1949, becoming the second element after gallium to be named after France. It was assigned the symbol Fa, but this abbreviation was revised to the current Fr shortly thereafter. Francium was the last element discovered in nature, rather than synthesized, following hafnium and rhenium. Further research into francium's structure was carried out by, among others, Sylvain Lieberman and his team at CERN in the 1970s and 1980s.\n\nFr is the result of the alpha decay of Ac and can be found in trace amounts in uranium minerals. In a given sample of uranium, there is estimated to be only one francium atom for every 1 × 10 uranium atoms. It is also calculated that there is at most 30 g of francium in the Earth's crust at any given time.\nFrancium can be synthesized by a fusion reaction when a gold-197 target is bombarded with a beam of oxygen-18 atoms from a linear accelerator in a process originally developed at in the physics department at the State University of New York at Stony Brook in 1995. Depending on the energy of the oxygen beam, the reaction can yield francium isotopes with masses of 209, 210, and 211.\n\nThe francium atoms leave the gold target as ions, which are neutralized by collision with yttrium and then isolated in a magneto-optical trap (MOT) in a gaseous unconsolidated state. Although the atoms only remain in the trap for about 30 seconds before escaping or undergoing nuclear decay, the process supplies a continual stream of fresh atoms. The result is a steady state containing a fairly constant number of atoms for a much longer time. The original apparatus could trap up to a few thousand atoms, while a later improved design could trap over 300,000 at a time. Sensitive measurements of the light emitted and absorbed by the trapped atoms provided the first experimental results on various transitions between atomic energy levels in francium. Initial measurements show very good agreement between experimental values and calculations based on quantum theory. The research project using this production method relocated to TRIUMF in 2012, where over 10 francium atoms have been held at a time, including large amounts of Fr in addition to Fr and Fr.\n\nOther synthesis methods include bombarding radium with neutrons, and bombarding thorium with protons, deuterons, or helium ions.\n\nFr can also be isolated from samples of its parent Ac, the francium being milked via elution with NHCl–CrO from an actinium-containing cation exchanger and purified by passing the solution through a silicon dioxide compound loaded with barium sulfate.\n\nIn 1996, the Stony Brook group trapped 3000 atoms in their MOT, which was enough for a video camera to capture the light given off by the atoms as they fluoresce. Francium has not been synthesized in amounts large enough to weigh.\n\n"}
{"id": "1276374", "url": "https://en.wikipedia.org/wiki?curid=1276374", "title": "Fretwork", "text": "Fretwork\n\nFretwork is an interlaced decorative design that is either carved in low relief on a solid background, or cut out with a fretsaw, coping saw, jigsaw or scroll saw. Most fretwork patterns are geometric in design. The materials most commonly used are wood and metal. Fretwork is used to adorn furniture and musical instruments. The term is also used for tracery on glazed windows and doors. Fretwork is also used to adorn/decorate architecture, where specific elements of decor are named according to their use such as eave bracket, gable fretwork or baluster fretwork, which may be of metal, especially cast iron or aluminum.\n\nFretwork patterns originally were ornamental designs used to decorate objects with a grid or a lattice. Designs have developed from the rectangular wave Greek fret to intricate intertwined patterns. A common misconception is that fretwork must be done with a fretsaw. However, a fretwork pattern is considered a fretwork whether or not it was cut out with a fretsaw.\n\nComputer numerical control (CNC) has brought about change in the method of timber fretwork manufacture. Lasers or router/milling cutting implements can now fashion timber and various other materials into flat and even 3D decorative items.\n"}
{"id": "69176", "url": "https://en.wikipedia.org/wiki?curid=69176", "title": "Guarana", "text": "Guarana\n\nGuarana ( from the Portuguese \"guaraná\" ), \"Paullinia cupana\", syns. \"P. crysan, P. sorbilis\") is a climbing plant in the maple family, Sapindaceae, native to the Amazon basin and especially common in Brazil. Guarana has large leaves and clusters of flowers and is best known for the seeds from its fruit, which are about the size of a coffee bean.\n\nAs a dietary supplement or herb, guarana seed is an effective stimulant: it contains about twice the concentration of caffeine found in coffee seeds (about 2–4.5% caffeine in guarana seeds, compared to 1–2% for coffee seeds). The additive has gained notoriety for being used in energy drinks. As with other plants producing caffeine, the high concentration of caffeine is a defensive toxin that repels herbivores from the berry and its seeds.\n\nThe colour of the fruit ranges from brown to red and it contains black seeds that are partly covered by white arils. The colour contrast when the fruit is split open has been compared with the appearance of eyeballs and has become the basis of an origin myth among the Sateré-Mawé people.\n\nThe word \"guarana\" comes from the Guaraní word \"guara-ná,\" which has its origins in the Sateré-Maué word for the plant, \"warana\", that in Tupi-Guarani means \"fruit like the eyes of the people.\"\n\nGuarana plays an important role in Tupi and Guaraní Paraguayan culture. According to a myth attributed to the Sateré-Maué tribe, guarana's domestication originated with a deity killing a beloved village child. To console the villagers, a more benevolent god plucked the left eye from the child and planted it in the forest, resulting in the wild variety of guarana. The god then plucked the right eye from the child and planted it in the village, giving rise to domesticated guarana.\n\nThe Guaranís would make an herbal tea by shelling, washing and drying the seeds, followed by pounding them into a fine powder. The powder is kneaded into a dough and then shaped into cylinders. This product is known as guarana bread, which would be grated and then immersed into hot water along with sugar.\n\nThis plant was introduced to European colonizers and to Europe in the 16th century by Felip Betendorf, Oviedo, Hernández, Cobo and other Spaniard chroniclers. By 1958, guarana was commercialized.\n\nThe table contains a partial listing of some of the chemicals found in guarana seeds, although other parts of the plant may contain them as well in varying quantities.\n\nAccording to the Biological Magnetic Resonance Data Bank, guaranine (better known as caffeine) is found in guarana and is identical to caffeine derived from other sources, like coffee, tea and mate. Guaranine, theine and mateine are all synonyms for caffeine when the definitions of those words include none of the properties and chemicals of their host plants except caffeine. Natural sources of caffeine contain widely varying mixtures of xanthine alkaloids other than caffeine, including the cardiac stimulants theophylline, theobromine and other substances such as polyphenols, which can form insoluble complexes with caffeine.\nThe main natural phenols found in guarana are (+)-catechin and (-)-epicatechin.\n\nIn the United States, guarana fruit powder and seed extract have not been determined for status as \"generally recognized as safe\" (GRAS) by the Food and Drug Administration, but rather are approved as food additives for flavor (but not non-flavor) uses.\n\nGuarana is used in sweetened or carbonated soft drinks and energy drinks, an ingredient of herbal teas or contained in dietary supplement capsules. Generally, South America obtains most of its caffeine from guarana.\n\nBrazil, the third-largest consumer of soft drinks in the world, produces several soft drink brands from guarana extract. Paraguay is also a producer of guarana soft drinks with several brands operating in its market. The word \"guaraná\" is widely used in Brazil and Paraguay as a reference to soft drinks containing guarana extract.\n\n"}
{"id": "22871216", "url": "https://en.wikipedia.org/wiki?curid=22871216", "title": "Hannes Alfvén Prize", "text": "Hannes Alfvén Prize\n\nThe Hannes Alfvén Prize is a prize established by the European Physical Society (EPS) Plasma Physics Division in 2000. The Prize is awarded annually by the European Physical Society at the EPS Conference on Plasma Physics for outstanding work in the field of plasma physics: \"for achievements which have shaped the plasma physics field or are expected to do so in future.\"\n\nIt is named after the Swedish physicist Hannes Alfvén.\n\nSource: European Physical Society Plasma Physics Division\n\n\n"}
{"id": "57156804", "url": "https://en.wikipedia.org/wiki?curid=57156804", "title": "Kammwamba Thermal Power Station", "text": "Kammwamba Thermal Power Station\n\nThe Khammwamba Thermal Power Station, also Khammwamba Power Station, is a proposed coal-fired power plant in Malawi, with planned installed capacity of .\n\nThe power station would be located in the \"Kammwamba Area\" in the town of Zalewa, in Neno District, in the Southern Region of Malawi, approximately , by road, north-west of the city of Blantyre, the largest city and financial capital of Malawi.\n\nAs of November 2016, Malawi had installed capacity of 351 megawatts of electric power generation. Peak demand stood at 350 megawatts, leaving very little room for flexibility. This has exposed the Malawi grid to recurrent, crippling load-shedding. It has also starved big industrial and manufacturing projects, especially in the areas of mining and processing, slowing national development and job creation.\n\nAs far back as 2013, the government of Malawi began engaging the China Gezhouba Group Company (CGGC), to design, build and operate a coal-fired electricity-generation power plant at Kammwamba, in Southern Malawi. The money needed for construction would be borrowed from the Export–Import Bank of China. At that time, it was expected that the coal would be sourced within Malawi, where coal reserves are estimated at 22,000,000,000 tons. In January 2014, the cost of the project was valued at US$295 million.\n\nIn May 2014, plans to build this power station were concretized. The new plans called for starting with a 300 megawatts development, expandable in the future to 1,000 megawatts. The primary source of the coal to fire the power station, was identified as the Moatize coalfields in neighboring Mozambique. The imported coal would be hauled to the site via the nearby railway line.\n\nThe power generated at this station, would be relayed to a 400kV substation at Phombeya, in Balaka District, where it would be integrated into the Malawian national electricity grid.\n\nIn September 2017, \"The Nation (Malawi)\" reported that the government of Malawi would own 85 percent of the power station, while CGGC of China would own 15 percent of the power station. Vale Logistics of Brazil would provide the coal needed, over a 30-year contract period. The challenge at that time was that the agreements called for the government to put down US$104 million in \"commitment money\". However, the government had only US$12.4 million available, and had requested to pay the balance in installments. Construction was expected to start in November 2018, with commissioning of the 300 megawatts plant expected in 2022.\n\n\n"}
{"id": "918877", "url": "https://en.wikipedia.org/wiki?curid=918877", "title": "Levitated dipole", "text": "Levitated dipole\n\nA levitated dipole is a type of nuclear fusion reactor design using a superconducting torus which is magnetically levitated inside the reactor chamber. The name refers to the magnetic dipole that forms within the reaction chamber, similar to Earth's or Jupiter's magnetospheres. It is believed that such an apparatus could contain plasma more efficiently than other fusion reactor designs.\n\nThe Levitated Dipole Experiment (LDX) was funded by the US Department of Energy's Office of Fusion Energy. The machine was run in a collaboration between MIT and Columbia University. Funding for the LDX was ended in November 2011 to concentrate resources on tokamak designs.\n\nThe Earth's magnetic field is generated by the circulation of charges in the Earth's molten core. The resulting magnetic dipole field forms a shape with magnetic field lines passing through the Earth's center, reaching the surface near the poles and extending far into space above the equator. Charged particles entering the field will tend to follow the lines of force, moving north or south. As they reach the polar regions, the magnetic lines begin to cluster together, and this increasing field can cause particles below a certain energy threshold to reflect, and begin travelling in the opposite direction. Such particles bounce back and forth between the poles until the collide with other particles. Particles with greater energy continue towards the Earth, impacting the atmosphere and causing the aurora.\n\nThis basic concept is used in the magnetic mirror approach to fusion energy. The mirror uses a solenoid to confine the plasma in the center of a cylinder, and then two magnets at either end to force the magnetic lines closer together to create reflecting areas. One of the most promising of the early approaches to fusion, the mirror ultimately proved to be very \"leaky\", with the fuel refusing to properly reflect from the ends as the density and energy were increased. Annoyingly, it was the particles with the most energy, those most likely to undergo fusion, that preferentially escaped. Research into large mirror machines ended in the 1980s as it became clear they would not reach fusion breakeven in a practically sized device.\n\nThe levitated dipole can be thought of, in some ways, as a toroidal mirror, much more similar to the Earth's field than the linear system in a traditional mirror. In this case, the confinement area is not the linear area between the mirrors, but the toroidal area around the outside of the central magnet, similar to the area around the Earth's equator. Particles in this area that move up or down see increasing magnetic density and tend to move back towards the equator area again. This gives the system some level of natural stability. Particles with higher energy, the ones that would escape a traditional mirror, instead follow the field lines through the hollow center of the magnet, recirculating back into the equatorial area again.\n\nThis makes the Levitated Dipole unique when compared with other magnetic confinement machines. In those experiments, small fluctuations can cause significant energy loss. By contrast, in a dipolar magnetic field, fluctuations tend to compress the plasma, without energy loss. This compression effect was first noticed by Akira Hasegawa (of the Hasegawa-Mima equation) after participating in the Voyager 2 encounter with Uranus.\n\nAdapting this concept to a fusion experiment was first proposed by Dr. Jay Kesner (MIT) and Dr. Michael Mauel (Columbia) in the mid to late nineties. The pair assembled a team and raised money to build the machine. They achieved first plasma on Friday, August 13, 2004 at 12:53 PM. First plasma was done by (1) successfully levitating the dipole magnet and (2) RF heating the plasma. The LDX team has since successfully conducted several levitation tests, including a 40-minute suspension of the superconducting coil on February 9, 2007. Shortly after, the coil was damaged in a control test in February 2007 and replaced in May 2007. The replacement coil was inferior, a copper wound electromagnet, that was also water cooled. Scientific results, including the observation of an inward turbulent pinch, were reported in Nature Physics.\n\nThis experiment needed a very special free-floating electromagnet, which created the unique \"toilet-bowl\" magnetic field. The magnetic field was originally made of two counter-wound rings of currents. Each ring contained a 19-strand niobium-tin Rutherford cable (common in superconducting magnets). These looped around inside a Inconel magnet; a magnet that looked like an oversized donut. The donut was charged using induction. Once charged, it generated a magnetic field for roughly an 8-hour period. Overall, the ring weighed 450 kilograms and levitated 1.6 meters above a superconducting ring. The ring produced roughly a 5-tesla field. This superconductor was encased inside a liquid helium, which kept the electromagnet below 10 kelvins. This design is similar to the D20 dipole experiment at Berkeley and the RT-1 experiment at the University of Tokyo.\n\nThe dipole was suspended inside a mushroom-shaped vacuum chamber, which was about 5 meters in diameter and ~3 meters high. At the base of the chamber was a charging coil. This coil is used to charge the dipole, using induction. The coil exposing the dipole to a varying magnetic field. Next, the dipole is raised into the center of the chamber. This could be done with supports or using the field itself. Around the outside of this chamber were Helmholtz coils, which were used to produce a uniform surrounding magnetic field. This external field would interact with the dipole field, suspending the dipole. It was in this surrounding field that plasma moved. The plasma forms around the dipole and inside the chamber. The plasma is formed by heating a low pressure gas. The gas is heated using a radio frequency, essentially microwaving the plasma in a 17-kilowatt field.\n\nThe machine was monitored using diagnostics fairly standard to all of fusion. These included: \n\nSingle particles corkscrew along the field lines, flowing around the dipole electromagnet. This leads to a giant encapsulation of the electromagnet. As material passes through the center, the density spikes. This is because lots of plasma is trying to squeeze through a limited area. This is where most of the fusion reactions occur. This behavior has been called a turbulent pinch. \nIn large amounts, the plasma formed two shells around the dipole: a low density shell, occupying a large volume and a high density shell, closer to the dipole. This is shown here. The plasma was trapped fairly well. It gave a maximum beta number of 0.26. A value of 1 is ideal.\n\nThere were two modes of operation observed: \nThese had been proposed by Nicholas Krall in the 1960s.\n\nIn the case of deuterium fusion (the cheapest and most straightforward fusion fuel) the geometry of the LDX has the unique advantage over other concepts. Deuterium fusion makes two products, that occur with near equal probability:\n\nIn this machine, the secondary tritium could be partially removed, a unique property of the dipole. Another fuel choice is tritium and deuterium. This reaction can be done at lower heats and pressures. But it has several drawbacks. First, tritium is far more expensive than deuterium. This is because tritium is rare. It has a short half-life making it hard to produce and store. It is also considered a hazardous material, so using it is a hassle from a health, safety and environmental perspective. Finally, tritium and deuterium produces fast neutrons which means any reactor burning it would require heavy shielding.\n\n\n"}
{"id": "31580571", "url": "https://en.wikipedia.org/wiki?curid=31580571", "title": "Liquid-crystal laser", "text": "Liquid-crystal laser\n\nA liquid-crystal laser is a laser that uses a liquid crystal as the resonator cavity, allowing selection of emission wavelength and polarization from the active laser medium. The lasing medium is usually a dye doped into the liquid crystal. Liquid-crystal lasers are comparable in size to diode lasers, but provide the continuous wide spectrum tunability of dye lasers while maintaining a large coherence area. The tuning range is typically several tens of nanometers. Self-organization at micrometer scales reduces manufacturing complexity compared to using layered photonic metamaterials. Operation may be either in continuous wave mode or in pulsed mode.\n\nDistributed feedback lasing using Bragg reflection of a periodic structure instead of external mirrors was first proposed in 1971, predicted theoretically with cholesteric liquid crystals in 1978, achieved experimentally in 1980, and explained in terms of a photonic band gap in 1998.\nA United States Patent issued in 1973 described a liquid-crystal laser that uses \"a liquid lasing medium having internal distributed feedback by virtue of the molecular structure of a cholesteric liquid-crystal material.\"\n\nStarting with a liquid crystal in the nematic phase, the desired helical pitch (the distance along the helical axis for one complete rotation of the nematic plane subunits) can be achieved by doping the liquid crystal with a chiral molecule. For light circularly polarized with the same handedness, this regular modulation of the refractive index yields selective reflection of the wavelength given by the helical pitch, allowing the liquid-crystal laser to serve as its own resonator cavity. Photonic crystals are amenable to band theory methods, with the periodic dielectric structure playing the role of the periodic electric potential and a photonic band gap (reflection notch) corresponding to forbidden frequencies. The lower photon group velocity and higher density of states near the photonic bandgap suppresses spontaneous emission and enhances stimulated emission, providing favorable conditions for lasing. If the electronic band edge falls in the photonic bandgap, electron-hole recombination is strictly suppressed. This allows for devices with high lasing efficiency, low lasing threshold, and stable frequency, where the liquid-crystal laser acts its own waveguide. \"Colossal\" nonlinear change in refractive index is achievable in doped nematic-phase liquid crystals, that is the refractive index can change with illumination intensity at a rate of about 10cm/W of illumination intensity. Most systems use a semiconductor pumping laser to achieve population inversion, though flash lamp and electrical pumping systems are possible.\n\nTuning of the output wavelength is achieved by smoothly varying the helical pitch: as the winding changes, so does the length scale of the crystal. This in turn shifts the band edge and changes the optical path length in the lasing cavity. Applying a static electric field perpendicular to the dipole moment of the local nematic phase rotates the rod-like subunits in the hexagonal plane and reorders the chiral phase, winding or unwinding the helical pitch. Similarly, optical tuning of the output wavelength is available using laser light far from the pick-up frequency of the gain medium, with degree of rotation governed by intensity and the angle between the polarization of the incident light and the dipole moment. Reorientation is stable and reversible. The chiral pitch of a cholesteric phase tends to unwind with increasing temperature, with a disorder-order transition to the higher symmetry nematic phase at the high end. By applying a temperature gradient perpendicular to the direction of emission varying the location of stimulation, frequency may be selected across a continuous spectrum. Similarly, a quasi-continuous doping gradient yields multiple laser lines from different locations on the same sample. Spatial tuning may also be accomplished using a wedge cell. The boundary conditions of the narrower cell squeeze the helical pitch by requiring a particular orientation at the edge, with discrete jumps where the outer cells rotate to the next stable orientation; frequency variation between jumps is continuous.\n\nIf a defect is introduced into the liquid crystal to disturb the periodicity, a single allowed mode may be created inside of the photonic bandgap, reducing power leeching by spontaneous emission at adjacent frequencies. Defect mode lasing was first predicted in 1987, and was demonstrated in 2003.\n\nWhile most such thin films lase on the axis normal to the film's surface, some will lase on a conic angle around that axis.\n\n\n\n"}
{"id": "58575012", "url": "https://en.wikipedia.org/wiki?curid=58575012", "title": "List of pipeline accidents in the United States in 2011", "text": "List of pipeline accidents in the United States in 2011\n\nThe following is a list of pipeline accidents in the United States in 2011. It is one of several lists of U.S. pipeline accidents. See also list of natural gas and oil production accidents in the United States.\n\n\n \n"}
{"id": "13010605", "url": "https://en.wikipedia.org/wiki?curid=13010605", "title": "Load management", "text": "Load management\n\nLoad management, also known as demand side management (DSM), is the process of balancing the supply of electricity on the network with the electrical load by adjusting or controlling the load rather than the power station output. This can be achieved by direct intervention of the utility in real time, by the use of frequency sensitive relays triggering the circuit breakers (ripple control), by time clocks, or by using special tariffs to influence consumer behavior. Load management allows utilities to reduce demand for electricity during peak usage times (peak shaving), which can, in turn, reduce costs by eliminating the need for peaking power plants. In addition, some peaking power plants can take more than an hour to bring on-line which makes load management even more critical should a plant go off-line unexpectedly for example. Load management can also help reduce harmful emissions, since peaking plants or backup generators are often dirtier and less efficient than base load power plants. New load-management technologies are constantly under development — both by private industry and public entities.\n\nIn 1972, Theodore George “Ted” Paraskevakos, while working for Boeing in Huntsville, Alabama, developed a sensor monitoring system which used digital transmission for security, fire, and medical alarm systems as well as meter-reading capabilities for all utilities. This technology was a spin-off of his patented automatic telephone line identification system, now known as caller ID. In, 1974, Paraskevakos was awarded a U.S. patent for this technology.\n\nAt the request of the Alabama Power Company, Paraskevakos developed a load-management system along with automatic meter-reading technology. In doing so, he utilized the ability of the system to monitor the speed of the watt power meter disc and, consequently, power consumption. This information, along with the time of day, gave the power company the ability to instruct individual meters to manage water heater and air conditioning consumption in order to prevent peaks in usage during the high consumption portions of the day. For this approach, Paraskevakos was awarded multiple patents.\n\nSince electrical energy is a form of energy that cannot be effectively stored in bulk, it must be generated, distributed, and consumed immediately. When the load on a system approaches the maximum generating capacity, network operators must either find additional supplies of energy or find ways to curtail the load, hence load management. If they are unsuccessful, the system will become unstable and blackouts can occur.\n\nLong-term load management planning may begin by building sophisticated models to describe the physical properties of the distribution network (i.e. topology, capacity, and other characteristics of the lines), as well as the load behavior. The analysis may include scenarios that account for weather forecasts, the predicted impact of proposed load-shed commands, estimated time-to-repair for off-line equipment, and other factors.\n\nThe utilization of load management can help a power plant achieve a higher capacity factor, a measure of average capacity utilization. Capacity factor is a measure of the output of a power plant compared to the maximum output it could produce. Capacity factor is often defined as \"the ratio of average load to capacity\" or \"the ratio of average load to peak load in a period of time\". A higher load factor is advantageous because a power plant may be less efficient at low load factors, a high load factor means fixed costs are spread over more kWh of output (resulting in a lower price per unit of electricity), and a higher load factor means greater total output. If the power load factor is affected by non-availability of fuel, maintenance shut-down, unplanned breakdown, or reduced demand (as consumption pattern fluctuate throughout the day), the generation has to be adjusted, since grid energy storage is often prohibitively expensive.\n\nSmaller utilities that buy power instead of generating their own find that they can also benefit by installing a load control system. The penalties they must pay to the energy provider for peak usage can be significantly reduced. Many report that a load control system can pay for itself in a single season.\n\nWhen the decision is made to curtail load, it is done so on the basis of system \"reliability\". The utility in a sense \"owns the switch\" and sheds loads only when the stability or reliability of the electrical distribution system is threatened. The utility (being in the business of generating, transporting, and delivering electricity) will not disrupt their business process without due cause. Load management, when done properly, is non-invasive, and imposes no hardship on the consumer. The load should be shifted to off peak hours.\n\nDemand response places the \"on-off switch\" in the hands of the consumer using devices such as a smart grid controlled load control switch. While many residential consumers pay a flat rate for electricity year-round, the utility's costs actually vary constantly, depending on demand, the distribution network, and composition of the company's electricity generation portfolio. In a free market, the wholesale price of energy varies widely throughout the day. Demand response programs such as those enabled by smart grids attempt to incentivize the consumer to limit usage based upon \"cost\" concerns. As costs rise during the day (as the system reaches peak capacity and more expensive peaking power plants are used), a free market economy should allow the price to rise. A corresponding drop in demand for the commodity should meet a fall in price. While this works for predictable shortages, many crises develop within seconds due to unforeseen equipment failures. They must be resolved in the same time-frame in order to avoid a power blackout. Many utilities who are interested in demand response have also expressed an interest in load control capability so that they might be able to operate the \"on-off switch\" before price updates could be published to the consumers.\n\nThe application of load control technology continues to grow today with the sale of both radio frequency and powerline communication based systems. Certain types of smart meter systems can also serve as load control systems. Charge control systems can prevent the recharging of electric vehicles during peak hours. Vehicle-to-grid systems can return electricity from an electric vehicle's batteries to the utility, or they can throttle the recharging of the vehicle batteries to a slower rate.\n\nThe largest residential load control system in the world is found in Florida and is managed by Florida Power and Light. It utilizes 800,000 load control transponders (LCTs) and controls 1,000 MW of electrical power (2,000 MW in an emergency). FPL has been able to avoid the construction of numerous new power plants due to their load management programs.\n\nRipple control is the most common form of load control, and is used in many countries around the world, including Australia, New Zealand, the United Kingdom, Germany, the Netherlands, and South Africa. Ripple control involves superimposing a higher-frequency signal (usually between 100 and 1600 Hz) onto the standard 50–60 Hz of the main power signal. When receiver devices attached to non-essential residential or industrial loads receive this signal, they shut down the load until the signal is disabled or another frequency signal is received.\n\nEarly implementations of ripple control occurred during World War II in various parts of the world using a system that communicates over the electrical distribution system. Ripple control systems are generally paired with a two- (or more) tiered pricing system, whereby electricity is more expensive during peak times (evenings) and cheaper during low-usage times (early morning).\n\nAffected residential devices will vary by region, but may include residential electric hot-water heaters, air conditioners, pool pumps, or crop-irrigation pumps. In a distribution network outfitted with load control, these devices are outfitted with communicating controllers that can run a program that limits the duty cycle of the equipment under control. Consumers are usually rewarded for participating in the load control program by paying a reduced rate for energy. Proper load management by the utility allows them to practice load shedding to avoid rolling blackouts and reduce costs.\n\nZellweger off-peak is one common brand of ripple control systems.\n\nGreater loads physically slow the rotors of a grid's synchronized generators. This causes AC mains to have a slightly reduced frequency when a grid is heavily loaded. The reduced frequency is immediately sensible across the entire grid. Inexpensive local electronics can easily and precisely measure mains frequencies and turn off sheddable loads. In some cases, this feature is nearly free, e.g. if the controlling equipment (such as an electric power meter, or the thermostat in an air-conditioning system) already has a microcontroller. Most electronic electric power meters internally measure frequency, and require only demand control relays to turn off equipment. In other equipment, often the only needed extra equipment is a resistor divider to sense the mains cycle and a schmitt trigger (a small integrated circuit) so the microcontrollers' digital input can sense a reliable fast digital edge. A schmitt trigger is already standard equipment on many microcontrollers.\n\nThe main advantage over ripple control is greater customer convenience: Unreceived ripple control telegrams can cause a water heater to remain off, causing a cold shower. Or, they can cause an airconditioner to remain off, resulting in a sweltering home. In contrast, as the grid recovers, its frequency naturally rises to normal, so frequency-controlled load control automatically enables water heaters, air-conditioners and other comfort equipment. The cost of equipment can be less, and there are no concerns about overlapping or unreached ripple control regions, mis-received codes, transmitter power, etc.\n\nThe main disadvantage compared to ripple control is a less fine-grained control. For example, a grid authority has only a limited ability to select which loads are shed. In controlled war-time economies, this can be a substantial disadvantage.\n\nThe system was invented in PNNL in the early 21st century, and has been shown to stabilize grids.\n\nIn many countries, including United States, United Kingdom and France, the power grids routinely use privately held, emergency diesel generators in load management schemes\n\nSince the 1950s, New Zealand has had a system of load management based on ripple control, allowing the electricity supply for domestic and commercial water storage heaters to be switched off and on, as well as allowing remote control of nightstore heaters and street lights. Ripple injection equipment located within each local distribution network signals to ripple control receivers at the customer's premises. Control may either done manually by the local distribution network company in response to local outages or requests to reduce demand from the transmission system operator (i.e. Transpower), or automatically when injection equipment detects mains frequency falling below 49.2 Hz. Ripple control receivers are assigned to one of several ripple channels to allow the network company to only turn off supply on part of the network, and to allow staged restoration of supply to reduce the impact of a surge in demand when power is restored to water heaters after a period of time off.\n\nDepending on the area, the consumer may have two electricity meters, one for normal supply (\"Anytime\") and one for the load-managed supply (\"Controlled\"), with Controlled supply billed at a lower rate per kilowatt-hour than Anytime supply. For those with load-managed supply but only a single meter, electricity is billed at the \"Composite\" rate, priced between Anytime and Controlled.\n\nFrance has an EJP tariff, which allows it to disconnect certain loads and to encourage consumers to disconnect certain loads. This tariff is no longer available for new clients (as of July 2009). The \"Tempo\" tariff also includes different types of days with different prices, but has been discontinued for new clients as well (as of July 2009). Reduced prices during nighttime are available for customers for a higher monthly fee.\n\nRltec in the UK in 2009 reported that domestic refrigerators are being sold fitted with their dynamic load response systems. In 2011 it was announced that the Sainsbury supermarket chain will use dynamic demand technology on their heating and ventilation equipment.\n\nIn the UK, night storage heaters are used to increase the load by about 5 GW to accommodate the nuclear programme. There is also a programme that allows industrial loads to be disconnected using circuit breakers triggered automatically by frequency sensitive relays fitted on site. This operates in conjunction with Standing Reserve, a programme using diesel generators. These can also be remotely switched using BBC Radio 4 Longwave Radio teleswitch.\n\nSP transmission deployed Dynamic Load Management scheme in Dumfries and Galloway area using real time monitoring of embedded generation and disconnecting them, should an overload being detected on transmission Network.\n\n\n"}
{"id": "2464563", "url": "https://en.wikipedia.org/wiki?curid=2464563", "title": "Manila paper", "text": "Manila paper\n\nManila paper is a relatively inexpensive type of paper, generally made through a less refined process than other types of paper. It is typically made from semi-bleached wood fibres. It is not as strong as craft paper but has better printing qualities. Manila is buff-coloured and the fibres are usually visible to the naked eye. Because this paper is generally inexpensive, it is commonly given to children for making art.\n\nManila paper was originally made out of old Manila hemp ropes which were extensively used on ships, having replaced true hemp. It was made from Manila hemp (also called abacá) or \"Musa textilis\" which is grown in The Philippines and hence the association with Manila, the capital of that country. Abaca is an exceptionally strong fibre, nowadays used for special papers like teabag tissue. It is also very expensive, priced several times higher than woodpulp, hence the change to that fiber for what is still called Manilla—usually with two \"ll\"s. More recently new woodpulp has often been replaced with a high proportion of recycled fibre. True manila hemp folders would have been much tougher and long lasting than modern folders.\n\nManila is most commonly used for making file folders. Some fashion schools and people in the fashion industry use large rolls of Manila to create finalised clothing patterns.\n\n"}
{"id": "105875", "url": "https://en.wikipedia.org/wiki?curid=105875", "title": "Maranta arundinacea", "text": "Maranta arundinacea\n\nMaranta arundinacea, also known as arrowroot, maranta, West Indian arrowroot, obedience plant, Bermuda arrowroot, araru, ararao or hulankeeriya, is a large, perennial herb found in rainforest habitats. Arrowroot flour is now produced commercially mostly in St. Vincent and the Grenadines. Arrowroot was one of the earliest plants to be domesticated for food in northern South America, with evidence of exploitation or cultivation of the plant dating back to 8200 BCE.\n\nArrowroot is a perennial plant growing to a height of between and . Its leaves are lanceolate. The edible part of the plant is the rhizome. Twin clusters of small white flowers bloom about 90 days after planting. The plant rarely produces seed and reproduction is typically by planting part of a rhizome with a bud. Rhizomes are ready for harvesting 10–12 months after planting as leaves of the plant begin to wilt and die. The rhizomes are fleshy, cylindrical, and grow from to long.\n\nThe arrowroot plant probably originated in the Amazon rainforest of north-western Brazil and neighboring countries. It grows best between temperatures of and with annual precipitation between and . The dormant rhizomes can withstand temperatures as low as .\n\nIn the continental United States, arrowroot is cultivated as an outside plant only in southern Florida.\n\n\"Maranta arundinacea\" is native to Mexico, Central America, the West Indies (Cuba, Hispaniola, Puerto Rico, Trinidad, Lesser Antilles) and South America (Brazil, Peru, Ecuador, Colombia, Venezuela, Suriname, Guyana, French Guiana). It is widely cultivated in the many warm countries and is considered naturalized in Jamaica, Bahamas, Bermuda, the Netherlands Antilles, India, Sri Lanka, China (Guangdong, Guangxi, Hainan, Yunnan), Taiwan, Kazan Rettō (火山列島), Mauritius, Réunion, Equatorial Guinea, Gabon, Florida, Cambodia, Indonesia and the Philippines.\n\nThe Caribbean island nation of St. Vincent and the Grenadines is the world's largest grower of arrowroot and producer of arrowroot flour. In Kerala, India, arrowroot, locally called bilathi koova, is cultivated to produce an easily digestible starch.\n\nRadio-carbon dating has established that \"M. arundinacea\" was one of the first plants domesticated in prehistoric South America. Arrowroot, along with leren (\"Calathea allouia\"), squash (\"Cucurbita moschata\"), and bottle gourd (\"Lagenaria siceraria\") became cultivated plants in northern South American and Panama between 8200 BCE and 5600 BCE. Some archaeologists believe that arrowroot was first used by Native Americans not as food but as a poultice to extract poison from wounds caused by spears or arrows.\n\nEvidence of the use of arrowroot as food has been found dating from 8200 BCE at the San Isidro archaeological site in the upper Cauca River valley of Colombia near the city of Popayán. Starch grains from arrowroot were found on grinding tools. It is unclear whether the arrowroot had been gathered or grown, although the elevation of the site of is probably outside the normal range of elevations at which \"M. arundinacea\" grows in the wild. Thus, the plant may have been introduced at San Isidro from nearby lowland rain forest areas in a pioneering effort to cultivate it. Stone hoes for cultivation of plants have been found which date as old as 7700 BCE in the middle Cauca valley, north of San Isidro.\n\nDomestication of arrowroot at these early dates was probably on a small-scale with gardens of only a few plants being planted in alluvial soils near streams to ensure the steady supply of moisture needed during the growing season by arrowroot and other similar root crops. The exploitation of arrowroot foot was probably complicated by the difficulty of extracting the starch from the fibrous roots. The roots must first be pounded or ground then soaked in water to separate the starch from the fibers. The starch is excellent for digestibility.\n\nCurrently arrowroot starch is used in food preparations and confectionery, and for industrial applications such as cosmetics and glue. The residue of starch extraction has a high fibre content and can be fed to livestock.\n"}
{"id": "42530377", "url": "https://en.wikipedia.org/wiki?curid=42530377", "title": "Matchboard", "text": "Matchboard\n\nMatchboard by definition is \"a board with a groove cut along one edge and a tongue along the other so as to fit snugly with the edges of similarly cut boards.\"\nMatchboarding can be used both internally and externally, and can be layered in many different styles including: square edge, feather edge, ship lap and tongue and groove. \n\nMatchboard was most popular in the late Victorian period, when woodworking machinery had developed that could cut the edge joints quickly and cheaply. In the 1930s, further developments in glues and veneer-cutting machinery made plywood affordable. This also gave a cleanly smooth-surfaced Modernist look that suited the taste for new styles. Matchboard then became much less popular. In the 1970s there was a resurgence of interest in the style as a retro feature, but this was usually provided, for cost reasons, as a faux matchboard effect pressed into the surface of a plywood board.\n"}
{"id": "36688650", "url": "https://en.wikipedia.org/wiki?curid=36688650", "title": "Moisture expansion", "text": "Moisture expansion\n\nMoisture expansion is the tendency of matter to change in volume in response to a change in moisture content. The macroscopic effect is similar to that of thermal expansion but the microscopic causes are very different. Moisture expansion is caused by hygroscopy.\n"}
{"id": "4406110", "url": "https://en.wikipedia.org/wiki?curid=4406110", "title": "Nanofiber", "text": "Nanofiber\n\nNanofibers are fibers with diameters in the nanometer range. Nanofibers can be generated from different polymers and hence have different physical properties and application potentials. Examples of natural polymers include collagen, cellulose, silk fibroin, keratin, gelatin and polysaccharides such as chitosan and alginate. Examples of synthetic polymers include poly(lactic acid) (PLA), polycaprolactone (PCL), polyurethane (PU), poly(lactic-co-glycolic acid) (PLGA), poly(3-hydroxybutyrate-co-3-hydroxyvalerate) (PHBV), and poly(ethylene-co-vinylacetate) (PEVA). Polymer chains are connected via covalent bonds. The diameters of nanofibers depend on the type of polymer used and the method of production. All polymer nanofibers are unique for their large surface area-to-volume ratio, high porosity, appreciable mechanical strength, and flexibility in functionalization compared to their microfiber counterparts.\n\nThere exist many different methods to make nanofibers, including drawing, electrospinning, self-assembly, template synthesis, and thermal-induced phase separation. Electrospinning is the most commonly used method to generate nanofibers because of the straightforward setup, the ability to mass-produce continuous nanofibers from various polymers, and the capability to generate ultrathin fibers with controllable diameters, compositions, and orientations. This flexibility allows for controlling the shape and arrangement of the fibers so that different structures (\"i.e.\" hollow, flat and ribbon shaped) can be fabricated depending on intended application purposes.\n\nNanofibers have many possible technological and commercial applications. They are used in tissue engineering, drug delivery, cancer diagnosis, lithium-air battery, optical sensors and air filtration.\n\nNanofibers were first produced via electrospinning more than four centuries ago. Beginning with the development of the electrospinning method, English physicist William Gilbert (1544-1603) first documented the electrostatic attraction between liquids by preparing an experiment in which he observed a spherical water drop on a dry surface warp into a cone shape when it was held below an electrically charged amber. This deformation later came to be known as the Taylor cone. In 1882, English physicist Lord Rayleigh (1842-1919) analyzed the unstable states of liquid droplets that were electrically charged, and noted that the liquid was ejected in tiny jets when equilibrium was established between the surface tension and electrostatic force. In 1887, British physicist Charles Vernon Boys (1855-1944) published a manuscript about nanofiber development and production. In 1900, American inventor John Francis Cooley (1861-1903) filed the first modern electrospinning patent.\n\nAnton Formhals was the first person to attempt nanofiber production between 1934 and 1944 and publish the first patent describing the experimental production of nanofibers. In 1966, Harold Simons published a patent for a device that could produce thin and light nanofiber fabrics with diverse motifs.\n\nOnly at the end of the 20th century have the words electrospinning and nanofiber become common language among scientists and researchers. Electrospinning continues to be developed today.\n\nMany chemical and mechanical techniques for preparing nanofibers exist.\n\nElectrospinning is the most commonly used method to fabricate nanofibers. \n\nThe instruments necessary for electrospinning include a high voltage supplier, a capillary tube with a pipette or needle with a small diameter, and a metal collecting screen. One electrode is placed into the polymer solution and the other electrode is attached to the collector. An electric field is applied to the end of the capillary tube that contains the polymer solution held by its surface tension and forms a charge on the surface of the liquid. As the intensity of the electric field increases, the hemispherical surface of the fluid at the tip of the capillary tube elongates to form a conical shape known as the Taylor cone. A critical value is attained upon further increase in the electric field in which the repulsive electrostatic force overcomes the surface tension and the charged jet of fluid is ejected from the tip of the Taylor cone. The discharged polymer solution jet is unstable and elongates as a result, allowing the jet to become very long and thin. Charged polymer fibers solidifies with solvent evaporation. Randomly-oriented nanofibers are collected on the collector. Nanofibers can also be collected in a highly aligned fashion by using specialized collectors such as the rotating drum, metal frame, or a two-parallel plates system. Parameters such as jet stream movement and polymer concentration have to be controlled to produce nanofibers with uniform diameters and morphologies.\n\nThe electrospinning technique transforms many types of polymers into nanofibers. An electrospun nanofiber network resembles the extracellular matrix (ECM) well. This resemblance is a major advantage of electrospinning because it opens up the possibility of mimicking the ECM with regards to fiber diameters, high porosity, and mechanical properties. Electrospinning is being further developed for mass production of one-by-one continuous nanofibers.\n\nThermal-induced phase separation separates a homogenous polymer solution into a multi-phase system via thermodynamic changes. The procedure involves five steps: polymer dissolution, liquid-liquid or liquid-solid phase separation, polymer gelation, extraction of solvent from the gel with water, and freezing and freeze-drying under vacuum. Thermal-induced phase separation method is widely used to generate scaffolds for tissue regeneration.\n\nThe homogenous polymer solution in the first step is thermodynamically unstable and tends to separate into polymer-rich and polymer-lean phases under appropriate temperature. Eventually after solvent removal, the polymer-rich phase solidifies to form the matrix and the polymer-lean phase develops into pores. Next, two types of phase separation can be carried out on the polymer solution depending on the desired pattern. Liquid-liquid separation is usually used to form bicontinuous phase structures while solid-liquid phase separation is used to form crystal structures. The gelation step plays a crucial role in controlling the porous morphology of the nanofibrous matrices. Gelation is influenced by temperature, polymer concentration, and solvent properties. Temperature regulates the structure of the fiber network: low gelation temperature results in formation of nanoscale fiber networks while high gelation temperature leads to the formation of a platelet-like structure. Polymer concentration affects fiber properties: an increase in polymer concentration decreases porosity and increases mechanical properties such as tensile strength. Solvent properties influence morphology of the scaffolds. After gelation, gel is placed in distilled water for solvent exchange. Afterwards, the gel is removed from the water and goes through freezing and freeze-drying. It is then stored in a desiccator until characterization.\n\nThe drawing method makes long single strands of nanofibers one at a time. The pulling process is accompanied by solidification that converts the dissolved spinning material into a solid fiber. A cooling step is necessary in the case of melt spinning and evaporation of solvent in the case of dry spinning. A limitation, however, is that only a viscoelastic material that can undergo extensive deformations while possessing sufficient cohesion to survive the stresses developed during pulling can be made into nanofibers through this process.\n\nThe template synthesis method uses a nanoporous membrane template composed of cylindrical pores of uniform diameter to make fibrils (solid nanofiber) and tubules (hollow nanofiber). This method can be used to prepare fibrils and tubules of many types of materials, including metals, semiconductors and electronically conductive polymers. The uniform pores allow for control of the dimensions of the fibers so nanofibers with very small diameters can be produced through this method. However, a drawback of this method is that it cannot make continuous nanofibers one at a time.\n\nThe self-assembly technique is used to generate peptide nanofibers and peptide amphiphiles. The method was inspired by the natural folding process of amino acid residues to form proteins with unique three-dimensional structures. The self-assembly process of peptide nanofibers involves various driving forces such as hydrophobic interactions, electrostatic forces, hydrogen bonding and van der Waals forces and is influenced by external conditions such as ionic strength and pH.\n\nDue to their high porosity and large surface area-to-volume ratio, nanofibers are widely used to construct scaffolds for biological applications. Major examples of natural polymers used in scaffold production are collagen, cellulose, silk fibroin, keratin, gelatin and polysaccharides such as chitosan and alginate. Collagen is a natural extracellular component of many connective tissues. Its fibrillary structure, which varies in diameter from 50-500 nm, is important for cell recognition, attachment, proliferation and differentiation. Using type I collagen nanofibers produced via electrospinning, Shih et al. found that the engineered collagen scaffold showed an increase in cell adhesion and decrease in cell migration with increasing fiber diameter. Using silk scaffolds as a guide for growth for bone tissue regeneration, Kim et al. observed complete bone union after 8 weeks and complete healing of defects after 12 weeks whereas the control in which the bone did not have the scaffold displayed limited mending of defects in the same time period. Similarly, keratin, gelatin, chitosan and alginate demonstrate excellent biocompatibility and bioactivity in scaffolds.\n\nHowever, cellular recognition of natural polymers can easily initiate an immune response. Consequently, synthetic polymers such as poly(lactic acid) (PLA), polycaprolactone (PCL), polyurethane (PU), poly(lactic-co-glycolic acid) (PLGA), poly(L-lactide) (PLLA), and poly(ethylene-co-vinylacetate) (PEVA) have been developed as alternatives for integration into scaffolds. Being biodegradable and biocompatible, these synthetic polymers can be used to form matrices with a fiber diameter within the nanometer range. Out of these synthetic polymers, PCL has generated considerable enthusiasm among researchers. PCL is a type of biodegradable polyester that can be prepared via ring-opening polymerization of ε-caprolactone using catalysts. It shows low toxicity, low cost and slow degradation. PCL can be combined with other materials such as gelatin, collagen, chitosan, and calcium phosphate to improve the differentiation and proliferation capacity (2, 17). PLLA is another popular synthetic polymer. PLLA is well known for its superior mechanical properties, biodegradability and biocompatibility. It shows efficient cell migration ability due to its high spatial interconnectivity, high porosity and controlled alignment. A blend of PLLA and PLGA scaffold matrix has shown proper biomimetic structure, good mechanical strength and favorable bioactivity.\n\nIn tissue engineering, a highly porous artificial extracellular matrix is needed to support and guide cell growth and tissue regeneration. Natural and synthetic biodegradable polymers have been used to create such scaffolds.\n\nSimon, in a 1988 NIH SBIR grant report, showed that electrospinning could be used to produced nano- and submicron-scale polystyrene and polycarbonate fibrous mats specifically intended for use as in vitro cell substrates. This early use of electrospun fibrous lattices for cell culture and tissue engineering showed that Human Foreskin Fibroblasts (HFF), transformed Human Carcinoma (HEp-2), and Mink Lung Epithelium (MLE) would adhere to and proliferate upon the fibers.\n\nNanofiber scaffolds are used in bone tissue engineering to mimic the natural extracellular matrix of the bones. The bone tissue is arranged either in a compact or trabecular pattern and composed of organized structures that vary in length from the centimeter range all the way to the nanometer scale. Nonmineralized organic component (i.e. type 1 collagen), mineralized inorganic component (i.e. hydroxyapatite), and many other noncollagenous matrix proteins (i.e. glycoproteins and proteoglycans) make up the nanocomposite structure of the bone ECM. The organic collagen fibers and the inorganic mineral salts provide flexibility and toughness, respectively, to ECM.\n\nAlthough the bone is a dynamic tissue that can self-heal upon minor injuries, it cannot regenerate after experiencing large defects such as bone tumor resections and severe nonunion fractures because it lacks the appropriate template. Currently, the standard treatment is autografting which involves obtaining the donor bone from a non-significant and easily accessible site (i.e. iliac crest) in the patient own body and transplanting it into the defective site. Transplantation of autologous bone has the best clinical outcome because it integrates reliably with the host bone and can avoid complications with the immune system. But its use is limited by its short supply and donor site morbidity associated with the harvest procedure. Furthermore, autografted bones are avascular and hence are dependent on diffusion for nutrients, which affects their viability in the host. The grafts can also be resorbed before osteogenesis is complete due to high remodeling rates in the body. Another strategy for treating severe bone damage is allografting which transplants bones harvested from a human cadaver. However, allografts introduce the risk of disease and infection in the host.\n\nBone tissue engineering presents a versatile response to treat bone injuries and deformations. Nanofibers produced via electrospinning mimics the architecture and characteristics of natural extracellular matrix particularly well. These scaffolds can be used to deliver bioactive agents that promote tissue regeneration. These bioactive materials should ideally be osteoinductive, osteoconductive, and osseointegratable. Bone substitute materials intended to replace autologous or allogeneic bone consist of bioactive ceramics, bioactive glasses, and biological and synthetic polymers. The basis of bone tissue engineering is that the materials will be resorbed and replaced over time by the body’s own newly regenerated biological tissue.\n\nTissue engineering is not only limited to the bone: a large amount of research is devoted to cartilage, ligament, skeletal muscle, skin, blood vessel, and neural tissue engineering as well.\n\nSuccessful delivery of therapeutics to the intended target largely depends on the choice of the drug carrier. The criteria for an ideal drug carrier include maximum effect upon delivery of the drug to the target organ, evasion of the immune system of the body in the process of reaching the organ, retention of the therapeutic molecules from preparatory stages to the final delivery of the drug, and proper release of the drug for exertion of the intended therapeutic effect.\n\nNanofibers have grabbed the attention of researchers as a favorable carrier candidate. Natural polymers such as gelatin and alginate make for good fabrication biomaterials for carrier nanofibers because of their biocompatibility and biodegradability that result in no harm to the tissue of the host and no toxic accumulation in the human body, respectively. Additionally, the physical properties of the nanofibers are well suited for drug delivery system. Due to their cylindrical morphology, they possess a high surface area-to-volume ratio. As a result, the fibers possess high drug-loading capacity and can release therapeutic molecules over a large surface area in the medium. Whereas surface area to volume ratio can only be controlled by adjusting the radius for spherical vesicles, nanofibers have more degrees of freedom in controlling the ratio by varying both the length and the cross-sectional radius. This adjustability is important for their application in drug delivery system in which the functional parameters need to be precisely controlled.\n\nAntibiotics and anticancer drugs have been successfully encapsulated in electrospun nanofibers by adding the drug into the polymer solution prior to electrospinning. Ignatova et al. showed that various antibiotics such as tetracycline hydrochloride, ciprofloxacin and levofloxacin can be incorporated into the synthetic polymer solution that can be electrospun to produce fibers that can be delivered to the site of interest. Drugs and other biopolymers such as DNA can be loaded onto the surfaces of nanofibers via simple adsorption, nanoparticles adsorption, and multilayer assembly. Surface-loaded nanofiber scaffolds are useful as adhesion barriers between internal organs and tissues post-surgery. Adhesion occurs during the healing process and can bring on complications such as chronic pain and reoperation failure. Nanofibers can separate the surgery-operated site from nearby tissues and organs while simultaneously deliver therapeutics such as antibiotics that are physically adsorbed onto their surfaces. Drugs can also be constructed in the form of nanoparticles and attached to the surface of nanofibers, allowing for loading of high concentration of the drugs in a given nanofiber. Rujitanaroj et al. showed that nanofibers functionalized with silver nanoparticles can be used as effective antibiotics against bacteria such as \"E. coli\", \"Pseudomonas aeruginosa\", \"Staphylococcus aureus\", and \"methicillin-resistant Staphylococcus aureus\" found in burn wounds. Finally, charged biopolymers such as DNA can be incorporated into the multilayer assembly of nanofibers. Such modification can provide diverse drug releasing surface profiles of nanofibers for different medical contexts.\n\nAlthough pathologic examination is the current standard method for molecular characterization in testing for the presence of biomarkers in tumors, these single-sample analyses fail to account for the diverse genomic nature of tumors. Considering the invasive nature, psychological stress, and the financial burden resulting from repeated tumor biopsies in patients, biomarkers that could be judged through minimally invasive procedures, such as blood draws, constitute an opportunity for progression in precision medicine.\n\nLiquid biopsy is an option that is becoming increasingly popular as an alternative to solid tumor biopsy. This is simply a blood draw that contains circulating tumor cells (CTCs) which are shed into the bloodstream from solid tumors. Patients with metastatic cancer are more likely to have detectable CTCs in the bloodstream but CTCs also exist in patients with localized diseases. It has been found that the number of CTCs present in the bloodstream of patients with metastatic prostate and colorectal cancer is prognostic of the overall survival of tumors. CTCs also have been demonstrated to inform prognosis in earlier stages of the disease.\n\nRecently, Ke et al. developed a NanoVelcro chip that captures the CTCs from the blood samples. When blood is passed through the chip, the nanofibers coated with protein antibodies bind to the proteins expressed on the surface of cancer cells and act like Velcro to trap CTCs for analysis. The NanoVelcro CTC assays underwent three generations of development. The first generation NanoVelcro Chip was created for CTC enumeration for cancer prognosis, staging, and dynamic monitoring. The second generation NanoVelcro-LCM was developed for single-cell CTC isolation. The individually isolated CTCs can be subjected to single-CTC genotyping. The third generation Thermoresponsive Chip allowed for CTC purification. The nanofiber polymer brushes undergo temperature-dependent conformational changes to capture and release CTCs.\n\nAmong many advanced electrochemical energy storage devices, rechargeable lithium-air batteries are of particular interest due to their considerable energy storing capacities and high power densities. As the battery is being used, lithium ions combine with oxygen from the air to form particles of lithium oxides, which attach to carbon fibers on the electrode. During recharging, the lithium oxides separate again into lithium and oxygen which is released back into the atmosphere. This conversion sequence is highly inefficient because there is significant voltage difference of more than 1.2 volts between the output voltage and the charging voltage of the battery meaning that approximately 30% of the electrical energy is lost as heat when the battery is charging. Also the large volume changes resulting from continuous conversion of oxygen between its gaseous and solid state puts stress on the electrode and limits its lifetime. \n\nThe performance of these batteries depends on the characteristics of the material that makes up the cathode. Carbon materials have been widely used as cathodes because of their excellent electrical conductivities, large surface areas, and chemical stability. Especially relevant for lithium-air batteries, carbon materials act as substrates for supporting metal oxides. Binder-free electrospun carbon nanofibers are particularly good potential candidates to be used in electrodes in lithium-oxygen batteries because they have no binders, have open macroporous structures, have carbons that support and catalyze the oxygen reduction reactions, and have versatility.\n\nZhu et al. developed a novel cathode that can store lithium and oxygen in the electrode they named nanolithia which is a matrix of carbon nanofibers periodically embedded with cobalt oxide. These cobalt oxides provide stability to the normally unstable superoxide-containing nanolithia. In this design, oxygen is stored as LiO and does not convert between gaseous and solid forms during charging and discharging. When the battery is discharging, lithium ions in nanolithia and react with superoxide oxygen the matrix to form LiO, and LiO. The oxygen remains in its solid state as it transitions among these forms. The chemical reactions of these transitions provide electrical energy. During charging, the transitions occur in reverse.\n\nPolymer optical fibers have generated increasing interest in recent years. Because of low cost, ease of handling, long wavelength transparency, great flexibility, and biocompatibility, polymer optical fibers show great potential for short-distance networking, optical sensing and power delivery.\n\nElectrospun nanofibers are particularly well-suitable for optical sensors because sensor sensitivity increases with increasing surface area per unit mass. Optical sensing works by detecting ions and molecules of interest via fluorescence quenching mechanism. Wang et al. successfully developed nanofibrous thin film optical sensors for metal ion (Fe and Hg) and 2,4-dinitrotoluene (DNT) detection using the electrospinning technique.\n\nQuantum dots show useful optical and electrical properties, including high optical gain and photochemical stability. A variety of quantum dots have been successfully incorporated into polymer nanofibers. Meng et al. showed that quantum dot-doped polymer nanofiber sensor for humidity detection shows fast response, high sensitivity, and long-term stability while requiring low power consumption.\n\nKelly et al. developed a sensor that warns first responders when the carbon filters in their respirators have become saturated with toxic fume particles. The respirators typically contain activated charcoal that traps airborne toxins. As the filters become saturated, chemicals begin to pass through and render the respirators useless. In order to easily determine when the filter is spent, Kelly and his team developed a mask equipped with a sensor composed of carbon nanofibers assembled into repeating structures called photonic crystals that reflect specific wavelengths of light. The sensors exhibit an iridescent color that changes when the fibers absorb toxins.\n\nElectrospun nanofibers are useful for removing volatile organic compounds (VOC) from the atmosphere. Scholten et al. showed that adsorption and desorption of VOC by electrospun nanofibrous membrane were faster than the rates of conventional activated carbon.\n\nAirborne contamination in the personnel cabins of mining equipment is of concern to the mining workers, mining companies, and government agencies such as the Mine Safety and Health Administration (MSHA). Recent work with mining equipment manufacturers and the MSHA has shown that nanofiber filter media can reduce cabin dust concentration to a greater extent compared to standard cellulose filter media.\n\nNanofibre has the capabilities in oil–water separation, most particularly in sorption process when the material in use has the oleophilic and hydrophobic surfaces. These characteristic enable the nanofiber to be used as a tool to combat either oily waste- water from domestic household and industrial activities, or oily seawater due to the oil run down to the ocean from oil transportation activities and oil tank cleaning on a vessel.\n\n"}
{"id": "2419157", "url": "https://en.wikipedia.org/wiki?curid=2419157", "title": "Nd:GdVO4", "text": "Nd:GdVO4\n\nNd:GdVO is one of the active laser medium for diode laser-pumped solid-state lasers. Several advantages over crystals include a larger emission cross-section, a pump power threshold, a wider absorption bandwidth, and a polarized output.\n"}
{"id": "30155900", "url": "https://en.wikipedia.org/wiki?curid=30155900", "title": "Nordic Windpower", "text": "Nordic Windpower\n\nNordic Windpower was a privately owned manufacturer of wind turbines based in Kansas City, Missouri in the United States. Nordic Windpower filed for liquidation in the Kansas City Bankruptcy court in October 2012.\n\nThe company had roots in a Swedish government wind turbine research and development program started 1975. Early prototypes in this program were erected in Maglarp, Skåne and Näsudden, Gotland. Nordic Windpower's N1000 turbine was designed with inspiration from the Maglarp prototype and has two blades rather than the more conventional three.\n\nIn the 1990s the company became privately owned Nordic Windpower AB. After selling only 5 turbines it became a United States entity in 2005 and promptly sold 7 turbines. It received a $16 million loan guarantee from the United States Department of Energy. Its private owners were Khosla Ventures, New Enterprise Associates and Novus Energy Partners, Goldman Sachs, Impax Asset Management, I2BF Venture Capital, Pulsar Energy Capital and other investors.\n\nBefore moving to Kansas City it had its corporate headquarters in Berkeley, California with a production facility in Pocatello, Idaho. Engineering design was carried out in Bristol, United Kingdom. In December 2010 Nordic announced its plans to move to Missouri to be closer to its target market and was offered $5.6 million in conditional incentive packages to relocate to Kansas City International Airport. Nordic temporarily occupied of the former American Airlines Overhaul facility at the airport which the airline had abandoned. It was scheduled to be the first tenant at the KCI Intermodal Business Centre being developed by Trammell Crow Company at the airport.\n\n"}
{"id": "14052233", "url": "https://en.wikipedia.org/wiki?curid=14052233", "title": "Nygårdsfjellet Wind Farm", "text": "Nygårdsfjellet Wind Farm\n\nNygårdsfjellet Wind Farm is a windfarm located in Narvik, Norway. The wind turbines are located 400 m above sea level. The farm is owned by Nordkraft Vind, a joint venture between Narvik Energi and DONG Energy. It started production in 2006 with three 2.3 MW turbines, an annual production of 26 GWh and with plans to add more turbines.\n\nIn 2011 an additional 11 turbines of the same capacity were commissioned, bringing the total capacity to 32.2 MW and the annual production to 104.2 GWh corresponding to the average consumption of 5200 Norwegian households.\n\n"}
{"id": "14659653", "url": "https://en.wikipedia.org/wiki?curid=14659653", "title": "Oil megaprojects (2012)", "text": "Oil megaprojects (2012)\n\nThis page summarizes projects that propose to bring more than of new liquid fuel capacity to market, with the first production of fuel beginning in 2012. This is part of the Wikipedia summary of oil megaprojects.\n\nTerminology\n"}
{"id": "49023", "url": "https://en.wikipedia.org/wiki?curid=49023", "title": "Propulsion", "text": "Propulsion\n\nPropulsion means to push forward or drive an object forward . The term is derived from two Latin words: \"pro\", meaning\" before\" or \"forward\"; and \"pellere\", meaning \"to drive\". \nA propulsion system consists of a source of mechanical power, and a \"propulsor\" (means of converting this power into propulsive force). \n\nA technological system uses an engine or motor as the power source (commonly called a powerplant), and wheels and axles, propellers, or a propulsive nozzle to generate the force. Components such as clutches or gearboxes may be needed to connect the motor to axles, wheels, or propellers.\n\nBiological propulsion systems use an animal's muscles as the power source, and limbs such as wings, fins or legs as the propulsors.\n\nA technological/biological system may use human, or trained animal, muscular work to power a mechanical device.\n\nAn aircraft propulsion system generally consists of an aircraft engine and some means to generate thrust, such as a propeller or a propulsive nozzle.\n\nAn aircraft propulsion system must achieve two things. First, the thrust from the propulsion system must balance the drag of the airplane when the airplane is cruising. And second, the thrust from the propulsion system must exceed the drag of the airplane for the airplane to accelerate. The greater the difference between the thrust and the drag, called the excess thrust, the faster the airplane will accelerate.\n\nSome aircraft, like airliners and cargo planes, spend most of their life in a cruise condition. For these airplanes, excess thrust is not as important as high engine efficiency and low fuel usage. Since thrust depends on both the amount of gas moved and the velocity, we can generate high thrust by accelerating a large mass of gas by a small amount, or by accelerating a small mass of gas by a large amount. Because of the aerodynamic efficiency of propellers and fans, it is more fuel efficient to accelerate a large mass by a small amount, which is why high-bypass turbofans and turboprops are commonly used on cargo planes and airliners.\n\nSome aircraft, like fighter planes or experimental high speed aircraft, require very high excess thrust to accelerate quickly and to overcome the high drag associated with high speeds. For these airplanes, engine efficiency is not as important as very high thrust. Modern combat aircraft usually have an afterburner added to a low bypass turbofan. Future hypersonic aircraft may use some type of ramjet or rocket propulsion.\n\nGround propulsion is any mechanism for propelling solid bodies along the ground, usually for the purposes of transportation. The propulsion system often consists of a combination of an engine or motor, a gearbox and wheel and axles in standard applications.\n\nMaglev (derived from magnetic levitation) is a system of transportation that uses magnetic levitation to suspend, guide and propel vehicles with magnets rather than using mechanical methods, such as wheels, axles and bearings. With maglev a vehicle is levitated a short distance away from a guide way using magnets to create both lift and thrust. Maglev vehicles are claimed to move more smoothly and quietly and to require less maintenance than wheeled mass transit systems. It is claimed that non-reliance on friction also means that acceleration and deceleration can far surpass that of existing forms of transport. The power needed for levitation is not a particularly large percentage of the overall energy consumption; most of the power used is needed to overcome air resistance (drag), as with any other high-speed form of transport.\n\nMarine propulsion is the mechanism or system used to generate thrust to move a ship or boat across water. While paddles and sails are still used on some smaller boats, most modern ships are propelled by mechanical systems consisting a motor or engine turning a propeller, or less frequently, in jet drives, an impeller. Marine engineering is the discipline concerned with the design of marine propulsion systems.\n\nSteam engines were the first mechanical engines used in marine propulsion, but have mostly been replaced by two-stroke or four-stroke diesel engines, outboard motors, and gas turbine engines on faster ships.Nuclear reactors producing steam are used to propel warships and icebreakers, and there have been attempts to utilize them to power commercial vessels.Electric motors have been used on submarines and electric boats and have been proposed for energy-efficient propulsion. Recent development in liquified natural gas (LNG) fueled engines are gaining recognition for their low emissions and cost advantages.\n\nSpacecraft propulsion is any method used to accelerate spacecraft and artificial satellites. There are many different methods. Each method has drawbacks and advantages, and spacecraft propulsion is an active area of research. However, most spacecraft today are propelled by forcing a gas from the back/rear of the vehicle at very high speed through a supersonic de Laval nozzle. This sort of engine is called a rocket engine.\n\nAll current spacecraft use chemical rockets (bipropellant or solid-fuel) for launch, though some (such as the Pegasus rocket and SpaceShipOne) have used air-breathing engines on their first stage. Most satellites have simple reliable chemical thrusters (often monopropellant rockets) or resistojet rockets for orbital station-keeping and some use momentum wheels for attitude control. Soviet bloc satellites have used electric propulsion for decades, and newer Western geo-orbiting spacecraft are starting to use them for north-south stationkeeping and orbit raising. Interplanetary vehicles mostly use chemical rockets as well, although a few have used ion thrusters and Hall effect thrusters (two different types of electric propulsion) to great success.\n\nA cable car is any of a variety of transportation systems relying on cables to pull vehicles along or lower them at a steady rate. The terminology also refers to the vehicles on these systems. The cable car vehicles are motor-less and engine-less and they are pulled by a cable that is rotated by a motor off-board.\n\nAnimal locomotion, which is the act of self-propulsion by an animal, has many manifestations, including running, swimming, jumping and flying. Animals move for a variety of reasons, such as to find food, a mate, or a suitable microhabitat, and to escape predators. For many animals the ability to move is essential to survival and, as a result, selective pressures have shaped the locomotion methods and mechanisms employed by moving organisms. For example, migratory animals that travel vast distances (such as the Arctic tern) typically have a locomotion mechanism that costs very little energy per unit distance, whereas non-migratory animals that must frequently move quickly to escape predators (such as frogs) are likely to have costly but very fast locomotion. The study of animal locomotion is typically considered to be a sub-field of biomechanics.\n\nLocomotion requires energy to overcome friction, drag, inertia, and gravity, though in many circumstances some of these factors are negligible. In terrestrial environments gravity must be overcome, though the drag of air is much less of an issue. In aqueous environments however, friction (or drag) becomes the major challenge, with gravity being less of a concern. Although animals with natural buoyancy need not expend much energy maintaining vertical position, some will naturally sink and must expend energy to remain afloat. Drag may also present a problem in flight, and the aerodynamically efficient body shapes of birds highlight this point. Flight presents a different problem from movement in water however, as there is no way for a living organism to have lower density than air. Limbless organisms moving on land must often contend with surface friction, but do not usually need to expend significant energy to counteract gravity.\n\nNewton's third law of motion is widely used in the study of animal locomotion: if at rest, to move forwards an animal must push something backwards. Terrestrial animals must push the solid ground; swimming and flying animals must push against a fluid (either water or air). The effect of forces during locomotion on the design of the skeletal system is also important, as is the interaction between locomotion and muscle physiology, in determining how the structures and effectors of locomotion enable or limit animal movement.\n\n"}
{"id": "23317", "url": "https://en.wikipedia.org/wiki?curid=23317", "title": "Proton", "text": "Proton\n\nA proton is a subatomic particle, symbol or , with a positive electric charge of +1\"e\" elementary charge and a mass slightly less than that of a neutron. Protons and neutrons, each with masses of approximately one atomic mass unit, are collectively referred to as \"nucleons\".\n\nOne or more protons are present in the nucleus of every atom; they are a necessary part of the nucleus. The number of protons in the nucleus is the defining property of an element, and is referred to as the atomic number (represented by the symbol \"Z\"). Since each element has a unique number of protons, each element has its own unique atomic number.\n\nThe word \"proton\" is Greek for \"first\", and this name was given to the hydrogen nucleus by Ernest Rutherford in 1920. In previous years, Rutherford had discovered that the hydrogen nucleus (known to be the lightest nucleus) could be extracted from the nuclei of nitrogen by atomic collisions. Protons were therefore a candidate to be a fundamental particle, and hence a building block of nitrogen and all other heavier atomic nuclei.\n\nIn the modern Standard Model of particle physics, protons are hadrons, and like neutrons, the other nucleon (particles present in atomic nuclei), are composed of three quarks. Although protons were originally considered fundamental or elementary particles, they are now known to be composed of three valence quarks: two up quarks of charge +\"e\" and one down quark of charge –\"e\". The rest masses of quarks contribute only about 1% of a proton's mass, however. The remainder of a proton's mass is due to quantum chromodynamics binding energy, which includes the kinetic energy of the quarks and the energy of the gluon fields that bind the quarks together. Because protons are not fundamental particles, they possess a physical size, though not a definite one; the root mean square charge radius of a proton is about 0.84–0.87 fm or to .\n\nAt sufficiently low temperatures, free protons will bind to electrons. However, the character of such bound protons does not change, and they remain protons. A fast proton moving through matter will slow by interactions with electrons and nuclei, until it is captured by the electron cloud of an atom. The result is a protonated atom, which is a chemical compound of hydrogen. In vacuum, when free electrons are present, a sufficiently slow proton may pick up a single free electron, becoming a neutral hydrogen atom, which is chemically a free radical. Such \"free hydrogen atoms\" tend to react chemically with many other types of atoms at sufficiently low energies. When free hydrogen atoms react with each other, they form neutral hydrogen molecules (H), which are the most common molecular component of molecular clouds in interstellar space.\n\nProtons are spin-½ fermions and are composed of three valence quarks, making them baryons (a sub-type of hadrons). The two up quarks and one down quark of a proton are held together by the strong force, mediated by gluons.A modern perspective has a proton composed of the valence quarks (up, up, down), the gluons, and transitory pairs of sea quarks. Protons have a positive charge distribution which decays approximately exponentially, with a mean square radius of about 0.8 fm.\n\nProtons and neutrons are both nucleons, which may be bound together by the nuclear force to form atomic nuclei. The nucleus of the most common isotope of the hydrogen atom (with the chemical symbol \"H\") is a lone proton. The nuclei of the heavy hydrogen isotopes deuterium and tritium contain one proton bound to one and two neutrons, respectively. All other types of atomic nuclei are composed of two or more protons and various numbers of neutrons.\n\nThe concept of a hydrogen-like particle as a constituent of other atoms was developed over a long period. As early as 1815, William Prout proposed that all atoms are composed of hydrogen atoms (which he called \"protyles\"), based on a simplistic interpretation of early values of atomic weights (see Prout's hypothesis), which was disproved when more accurate values were measured.\nIn 1886, Eugen Goldstein discovered canal rays (also known as anode rays) and showed that they were positively charged particles (ions) produced from gases. However, since particles from different gases had different values of charge-to-mass ratio (e/m), they could not be identified with a single particle, unlike the negative electrons discovered by J. J. Thomson. Wilhelm Wien in 1898 identified the hydrogen ion as particle with highest charge-to-mass ratio in ionized gases.\n\nFollowing the discovery of the atomic nucleus by Ernest Rutherford in 1911, Antonius van den Broek proposed that the place of each element in the periodic table (its atomic number) is equal to its nuclear charge. This was confirmed experimentally by Henry Moseley in 1913 using X-ray spectra.\n\nIn 1917 (in experiments reported in 1919), Rutherford proved that the hydrogen nucleus is present in other nuclei, a result usually described as the discovery of protons. Rutherford had earlier learned to produce hydrogen nuclei as a type of radiation produced as a product of the impact of alpha particles on nitrogen gas, and recognize them by their unique penetration signature in air and their appearance in scintillation detectors. These experiments were begun when Rutherford had noticed that, when alpha particles were shot into air (mostly nitrogen), his scintillation detectors showed the signatures of typical hydrogen nuclei as a product. After experimentation Rutherford traced the reaction to the nitrogen in air, and found that when alphas were produced into pure nitrogen gas, the effect was larger. Rutherford determined that this hydrogen could have come only from the nitrogen, and therefore nitrogen must contain hydrogen nuclei. One hydrogen nucleus was being knocked off by the impact of the alpha particle, producing oxygen-17 in the process. This was the first reported nuclear reaction, N + α → O + p. (This reaction would later be observed happening directly in a cloud chamber in 1925).\n\nRutherford knew hydrogen to be the simplest and lightest element and was influenced by Prout's hypothesis that hydrogen was the building block of all elements. Discovery that the hydrogen nucleus is present in all other nuclei as an elementary particle led Rutherford to give the hydrogen nucleus a special name as a particle, since he suspected that hydrogen, the lightest element, contained only one of these particles. He named this new fundamental building block of the nucleus the \"proton,\" after the neuter singular of the Greek word for \"first\", πρῶτον. However, Rutherford also had in mind the word \"protyle\" as used by Prout. Rutherford spoke at the British Association for the Advancement of Science at its Cardiff meeting beginning 24 August 1920. Rutherford was asked by Oliver Lodge for a new name for the positive hydrogen nucleus to avoid confusion with the neutral hydrogen atom. He initially suggested both \"proton\" and \"prouton\" (after Prout). Rutherford later reported that the meeting had accepted his suggestion that the hydrogen nucleus be named the \"proton\", following Prout's word \"protyle\". The first use of the word \"proton\" in the scientific literature appeared in 1920.\n\nRecent research has shown that thunderstorms can produce protons with energies of up to several tens of MeV.\n\nProtons are routinely used for accelerators for proton therapy or various particle physics experiments, with the most powerful example being the Large Hadron Collider.\n\nIn a July 2017 paper, researchers measured the mass of a proton to be (the values in parentheses being the statistical and systematic uncertainties, respectively), which is lower than measurements from the CODATA 2014 value by three standard deviations.\n\nThe free proton (a proton not bound to nucleons or electrons) is a stable particle that has not been observed to break down spontaneously to other particles. Free protons are found naturally in a number of situations in which energies or temperatures are high enough to separate them from electrons, for which they have some affinity. Free protons exist in plasmas in which temperatures are too high to allow them to combine with electrons. Free protons of high energy and velocity make up 90% of cosmic rays, which propagate in vacuum for interstellar distances. Free protons are emitted directly from atomic nuclei in some rare types of radioactive decay. Protons also result (along with electrons and antineutrinos) from the radioactive decay of free neutrons, which are unstable.\n\nThe spontaneous decay of free protons has never been observed, and protons are therefore considered stable particles according to the Standard Model. However, some grand unified theories (GUTs) of particle physics predict that proton decay should take place with lifetimes between 10 to 10 years and experimental searches have established lower bounds on the mean lifetime of a proton for various assumed decay products.\n\nExperiments at the Super-Kamiokande detector in Japan gave lower limits for proton mean lifetime of for decay to an antimuon and a neutral pion, and for decay to a positron and a neutral pion.\nAnother experiment at the Sudbury Neutrino Observatory in Canada searched for gamma rays resulting from residual nuclei resulting from the decay of a proton from oxygen-16. This experiment was designed to detect decay to any product, and established a lower limit to a proton lifetime of .\n\nHowever, protons are known to transform into neutrons through the process of electron capture (also called inverse beta decay). For free protons, this process does not occur spontaneously but only when energy is supplied. The equation is:\n\nThe process is reversible; neutrons can convert back to protons through beta decay, a common form of radioactive decay. In fact, a free neutron decays this way, with a mean lifetime of about 15 minutes.\n\nIn quantum chromodynamics, the modern theory of the nuclear force, most of the mass of protons and neutrons is explained by special relativity. The mass of a proton is about 80–100 times greater than the sum of the rest masses of the quarks that make it up, while the gluons have zero rest mass. The extra energy of the quarks and gluons in a region within a proton, as compared to the rest energy of the quarks alone in the QCD vacuum, accounts for almost 99% of the mass. The rest mass of a proton is, thus, the invariant mass of the system of moving quarks and gluons that make up the particle, and, in such systems, even the energy of massless particles is still measured as part of the rest mass of the system.\n\nTwo terms are used in referring to the mass of the quarks that make up protons: \"current quark mass\" refers to the mass of a quark by itself, while \"constituent quark mass\" refers to the current quark mass plus the mass of the gluon particle field surrounding the quark. These masses typically have very different values. As noted, most of a proton's mass comes from the gluons that bind the current quarks together, rather than from the quarks themselves. While gluons are inherently massless, they possess energy—to be more specific, quantum chromodynamics binding energy (QCBE)—and it is this that contributes so greatly to the overall mass of protons (see mass in special relativity). A proton has a mass of approximately 938 MeV/c, of which the rest mass of its three valence quarks contributes only about 9.4 MeV/c; much of the remainder can be attributed to the gluons' QCBE. \n\nThe internal dynamics of protons are complicated, because they are determined by the quarks' exchanging gluons, and interacting with various vacuum condensates. Lattice QCD provides a way of calculating the mass of a proton directly from the theory to any accuracy, in principle. The most recent calculations claim that the mass is determined to better than 4% accuracy, even to 1% accuracy (see Figure S5 in Dürr \"et al.\"). These claims are still controversial, because the calculations cannot yet be done with quarks as light as they are in the real world. This means that the predictions are found by a process of extrapolation, which can introduce systematic errors. It is hard to tell whether these errors are controlled properly, because the quantities that are compared to experiment are the masses of the hadrons, which are known in advance.\n\nThese recent calculations are performed by massive supercomputers, and, as noted by Boffi and Pasquini: \"a detailed description of the nucleon structure is still missing because ... long-distance behavior requires a nonperturbative and/or numerical treatment...\"\nMore conceptual approaches to the structure of protons are: the topological soliton approach originally due to Tony Skyrme and the more accurate AdS/QCD approach that extends it to include a string theory of gluons, various QCD-inspired models like the bag model and the constituent quark model, which were popular in the 1980s, and the SVZ sum rules, which allow for rough approximate mass calculations. These methods do not have the same accuracy as the more brute-force lattice QCD methods, at least not yet.\n\nThe problem of defining a radius for an atomic nucleus (proton) is similar to the problem of atomic radius, in that neither atoms nor their nuclei have definite boundaries. However, the nucleus can be modeled as a sphere of positive charge for the interpretation of electron scattering experiments: because there is no definite boundary to the nucleus, the electrons \"see\" a range of cross-sections, for which a mean can be taken. The qualification of \"rms\" (for \"root mean square\") arises because it is the nuclear cross-section, proportional to the square of the radius, which is determining for electron scattering.\n\nThe internationally accepted value of a proton's charge radius is (see orders of magnitude for comparison to other sizes). This value is based on measurements involving a proton and an electron (namely, electron scattering measurements and complex calculation involving scattering cross section based on Rosenbluth equation for momentum-transfer cross section), and studies of the atomic energy levels of hydrogen and deuterium.\n\nHowever, in 2010 an international research team published a proton charge radius measurement via the Lamb shift in muonic hydrogen (an exotic atom made of a proton and a negatively charged muon). As a muon is 200 times heavier than an electron, its de Broglie wavelength is correspondingly shorter. This smaller atomic orbital is much more sensitive to the proton's charge radius, so allows more precise measurement. Their measurement of the root-mean-square charge radius of a proton is \", which differs by 5.0 standard deviations from the CODATA value of \". In January 2013, an updated value for the charge radius of a proton——was published. The precision was improved by 1.7 times, increasing the significance of the discrepancy to 7σ.\nThe 2014 CODATA adjustment slightly reduced the recommended value for the proton radius (computed using electron measurements only) to , but this leaves the discrepancy at (87510-84087)/sqrt(610^2 + 39^2) round 1σ.\n\nThe international research team that obtained this result at the Paul Scherrer Institut in Villigen includes scientists from the Max Planck Institute of Quantum Optics, Ludwig-Maximilians-Universität, the Institut für Strahlwerkzeuge of Universität Stuttgart, and the University of Coimbra, Portugal. The team is now attempting to explain the discrepancy, and re-examining the results of both previous high-precision measurements and complex calculations involving scattering cross section. If no errors are found in the measurements or calculations, it could be necessary to re-examine the world's most precise and best-tested fundamental theory: quantum electrodynamics. The proton radius remains a puzzle as of 2017. Perhaps the discrepancy is due to new physics, or the explanation may be an ordinary physics effect that has been missed.\n\nThe radius is linked to the form factor and momentum transfer cross section. The atomic form factor G modifies the cross section corresponding to point-like proton.\n\nThe atomic form factor is related to the wave function density of the target:\n\nThe form factor can be split in electric and magnetic form factors. These can be further written as linear combinations of Dirac and Pauli form factors.\n\nSince the proton is composed of quarks confined by gluons, an equivalent pressure which acts on the quarks can be defined. This allows calculation of their distribution as a function of distance from the centre using Compton scattering of high-energy electrons (DVCS, for \"deeply virtual Compton scattering\"). The pressure is maximum at the centre, about 10 Pa which is greater than the pressure inside a neutron star. It is positive (repulsive) to a radial distance of about 0.6 fm, negative (attractive) at greater distances, and very weak beyond about 2 fm.\n\nThe radius of hydrated proton appears in the Born equation for calculating the hydration enthalpy of hydronium.\n\nAlthough protons have affinity for oppositely charged electrons, this is a relatively low-energy interaction and so free protons must lose sufficient velocity (and kinetic energy) in order to become closely associated and bound to electrons. High energy protons, in traversing ordinary matter, lose energy by collisions with atomic nuclei, and by ionization of atoms (removing electrons) until they are slowed sufficiently to be captured by the electron cloud in a normal atom.\n\nHowever, in such an association with an electron, the character of the bound proton is not changed, and it remains a proton. The attraction of low-energy free protons to any electrons present in normal matter (such as the electrons in normal atoms) causes free protons to stop and to form a new chemical bond with an atom. Such a bond happens at any sufficiently \"cold\" temperature (i.e., comparable to temperatures at the surface of the Sun) and with any type of atom. Thus, in interaction with any type of normal (non-plasma) matter, low-velocity free protons are attracted to electrons in any atom or molecule with which they come in contact, causing the proton and molecule to combine. Such molecules are then said to be \"protonated\", and chemically they often, as a result, become so-called Brønsted acids.\n\nIn chemistry, the number of protons in the nucleus of an atom is known as the atomic number, which determines the chemical element to which the atom belongs. For example, the atomic number of chlorine is 17; this means that each chlorine atom has 17 protons and that all atoms with 17 protons are chlorine atoms. The chemical properties of each atom are determined by the number of (negatively charged) electrons, which for neutral atoms is equal to the number of (positive) protons so that the total charge is zero. For example, a neutral chlorine atom has 17 protons and 17 electrons, whereas a Cl anion has 17 protons and 18 electrons for a total charge of −1.\n\nAll atoms of a given element are not necessarily identical, however, as the number of neutrons may vary to form different isotopes, and energy levels may differ forming different nuclear isomers. For example, there are two stable isotopes of chlorine: with 35 − 17 = 18 neutrons and with 37 − 17 = 20 neutrons.\n\nIn chemistry, the term proton refers to the hydrogen ion, . Since the atomic number of hydrogen is 1, a hydrogen ion has no electrons and corresponds to a bare nucleus, consisting of a proton (and 0 neutrons for the most abundant isotope \"protium\" ). The proton is a \"bare charge\" with only about 1/64,000 of the radius of a hydrogen atom, and so is extremely reactive chemically. The free proton, thus, has an extremely short lifetime in chemical systems such as liquids and it reacts immediately with the electron cloud of any available molecule. In aqueous solution, it forms the hydronium ion, HO, which in turn is further solvated by water molecules in clusters such as [HO] and [HO].\n\nThe transfer of in an acid–base reaction is usually referred to as \"proton transfer\". The acid is referred to as a proton donor and the base as a proton acceptor. Likewise, biochemical terms such as proton pump and proton channel refer to the movement of hydrated ions.\n\nThe ion produced by removing the electron from a deuterium atom is known as a deuteron, not a proton. Likewise, removing an electron from a tritium atom produces a triton.\n\nAlso in chemistry, the term \"proton NMR\" refers to the observation of hydrogen-1 nuclei in (mostly organic) molecules by nuclear magnetic resonance. This method uses the spin of the proton, which has the value one-half. The name refers to examination of protons as they occur in protium (hydrogen-1 atoms) in compounds, and does not imply that free protons exist in the compound being studied.\n\nThe Apollo Lunar Surface Experiments Packages (ALSEP) determined that more than 95% of the particles in the solar wind are electrons and protons, in approximately equal numbers.\n\nProtons also have extrasolar origin from galactic cosmic rays, where they make up about 90% of the total particle flux. These protons often have higher energy than solar wind protons, and their intensity is far more uniform and less variable than protons coming from the Sun, the production of which is heavily affected by solar proton events such as coronal mass ejections.\n\nResearch has been performed on the dose-rate effects of protons, as typically found in space travel, on human health. To be more specific, there are hopes to identify what specific chromosomes are damaged, and to define the damage, during cancer development from proton exposure. Another study looks into determining \"the effects of exposure to proton irradiation on neurochemical and behavioral endpoints, including dopaminergic functioning, amphetamine-induced conditioned taste aversion learning, and spatial learning and memory as measured by the Morris water maze. Electrical charging of a spacecraft due to interplanetary proton bombardment has also been proposed for study. There are many more studies that pertain to space travel, including galactic cosmic rays and their possible health effects, and solar proton event exposure.\n\nThe American Biostack and Soviet Biorack space travel experiments have demonstrated the severity of molecular damage induced by heavy ions on micro organisms including Artemia cysts.\n\nCPT-symmetry puts strong constraints on the relative properties of particles and antiparticles and, therefore, is open to stringent tests. For example, the charges of a proton and antiproton must sum to exactly zero. This equality has been tested to one part in . The equality of their masses has also been tested to better than one part in . By holding antiprotons in a Penning trap, the equality of the charge-to-mass ratio of protons and antiprotons has been tested to one part in . The magnetic moment of antiprotons has been measured with error of nuclear Bohr magnetons, and is found to be equal and opposite to that of a proton.\n\n"}
{"id": "1967137", "url": "https://en.wikipedia.org/wiki?curid=1967137", "title": "QCD vacuum", "text": "QCD vacuum\n\nThe Quantum Chromodynamic Vacuum or QCD vacuum is the vacuum state of quantum chromodynamics (QCD). It is an example of a \"non-perturbative\" vacuum state, characterized by non-vanishing condensates such as the gluon condensate and the quark condensate in the complete theory which includes quarks. The presence of these condensates characterizes the confined phase of quark matter.\n\nLike any relativistic quantum field theory, QCD enjoys Poincaré symmetry including the discrete symmetries CPT (each of which is realized). Apart from these space-time symmetries, it also has internal symmetries. Since QCD is an SU(3) gauge theory, it has local SU(3) gauge symmetry.\n\nSince it has many flavours of quarks, it has approximate flavour and chiral symmetry. This approximation is said to involve the chiral limit of QCD. Of these chiral symmetries, the baryon number symmetry is exact. Some of the broken symmetries include the axial U(1) symmetry of the flavour group. This is broken by the chiral anomaly. The presence of instantons implied by this anomaly also breaks CP symmetry.\n\nIn summary, the QCD Lagrangian has the following symmetries:\n\nThe following classical symmetries are broken in the QCD Lagrangian:\n\nWhen the Hamiltonian of a system (or the Lagrangian) has a certain symmetry, but the vacuum does not, then one says that spontaneous symmetry breaking (SSB) has taken place.\n\nA familiar example of SSB is in ferromagnetic materials. Microscopically, the material consists of atoms with a non-vanishing spin, each of which acts like a tiny bar magnet, i.e., a magnetic dipole. The Hamiltonian of the material, describing the interaction of neighbouring dipoles, is invariant under rotations. At high temperature, there is no magnetization of a large sample of the material. Then one says that the symmetry of the Hamiltonian is realized by the system. However, at low temperature, there could be an overall magnetization. This magnetization has a \"preferred direction\", since one can tell the north magnetic pole of the sample from the south magnetic pole. In this case, there is spontaneous symmetry breaking of the rotational symmetry of the Hamiltonian.\n\nWhen a continuous symmetry is spontaneously broken, massless bosons appear, corresponding to the remaining symmetry. This is called the Goldstone phenomenon and the bosons are called Goldstone bosons.\n\nThe SU() × SU() chiral flavour symmetry of the QCD Lagrangian is broken in the vacuum state of the theory. The symmetry of the vacuum state is the diagonal SU() part of the chiral group. The diagnostic for this is the formation of a non-vanishing chiral condensate , where is the quark field operator, and the flavour index is summed. The Goldstone bosons of the symmetry breaking are the pseudoscalar mesons.\n\nWhen N=2, i.e., only the up and down quarks are treated as massless, the three pions are the Goldstone bosons. When the strange quark is also treated as massless, i.e., , all eight pseudoscalar mesons of the quark model become Goldstone bosons. The actual masses of these mesons are obtained in chiral perturbation theory through an expansion in the (small) actual masses of the quarks.\n\nIn other phases of quark matter the full chiral flavour symmetry may be recovered, or broken in completely different ways.\n\nThe evidence for QCD condensates comes from two eras, the pre-QCD era 1950–1973 and the post-QCD era, after 1974. The pre-QCD results established that the strong interactions vacuum contains a quark chiral condensate, while the post-QCD results established that the vacuum also contains a gluon condensate.\n\nIn the 1950s, there were many attempts to produce a field theory to describe the interactions of pions and nucleons. The obvious renormalizable interaction between the two objects is the Yukawa coupling to a pseudoscalar:\n\nAnd this is clearly theoretically correct, since it is leading order and it takes all the symmetries into account. But it doesn't match experiment. The interaction that does couples the nucleons to the \"gradient\" of the pion field.\n\nThis is the gradient-coupling model. This interaction has a very different dependence on the energy of the pion—it vanishes at zero momentum.\nThis type of coupling means that a coherent state of low momentum pions barely interacts at all. This is a manifestation of an approximate symmetry, a shift symmetry of the pion field. The replacement\n\nleaves the gradient coupling alone, but not the pseudoscalar coupling.\n\nThe modern explanation for the shift symmetry was first proposed by Yoichiro Nambu. The pion field is a Goldstone boson, and the shift symmetry is the lowest order approximation to moving along the flat directions.\n\nThere is a mysterious relationship between the strong interaction coupling of the pions to the nucleons, the coefficient in the gradient coupling model, and the axial vector current coefficient of the nucleon which determines the weak decay rate of the neutron. The relation is\n\nand it is obeyed to 10% accuracy.\n\nThe constant is the coefficient that determines the neutron decay rate. It gives the normalization of the weak interaction matrix elements for the nucleon. On the other hand, the pion-nucleon coupling is a phenomenological constant describing the scattering of bound states of quarks and gluons.\n\nThe weak interactions are current-current interactions ultimately because they come from a non-Abelian gauge theory. The Goldberger–Treiman relation suggests that the pions for some reason interact as if they are related to the same symmetry current.\n\nThe phenomenon which gives rise to the Goldberger–Treiman relation was called the \"partially conserved axial current\" (PCAC) hypothesis. Partially conserved is an archaic term for spontaneously broken, and the axial current is now called the chiral symmetry current.\n\nThe idea is that the symmetry current which performs axial rotations on the fundamental fields does not preserve the vacuum. This means that the current applied to the vacuum produces particles. The particles must be spinless, otherwise the vacuum wouldn't be Lorentz invariant. By index matching, the matrix element is:\n\nwhere is the momentum carried by the created pion. Since the divergence of the axial current operator is zero, we must have\n\nHence the pions are massless, , in accordance with Goldstone's theorem.\n\nNow if the scattering matrix element is considered, we have\n\nUp to a momentum factor, which is the gradient in the coupling, it takes the same form as the axial current turning a neutron into a proton in the current-current form of the weak interaction.\n\nExtensions of the PCAC ideas allowed Steven Weinberg to calculate the amplitudes for collisions which emit low energy pions from the amplitude for the same process with no pions. The amplitudes are those given by acting with symmetry currents on the external particles of the collision.\n\nThese successes established the basic properties of the strong interaction vacuum well before QCD.\n\nExperimentally it is seen that the masses of the octet of pseudoscalar mesons is very much lighter than the next lightest states; i.e., the octet of vector mesons (such as the rho meson). The most convincing evidence for SSB of the chiral flavour symmetry of QCD is the appearance of these pseudo-Goldstone bosons. These would have been strictly massless in the chiral limit. There is convincing demonstration that the observed masses are compatible with chiral perturbation theory. The internal consistency of this argument is further checked by lattice QCD computations which allow one to vary the quark mass and check that the variation of the pseudoscalar masses with the quark mass is as required by chiral perturbation theory.\n\nThis pattern of SSB solves one of the earlier \"mysteries\" of the quark model, where all the pseudoscalar mesons should have been of nearly the same mass. Since , there should have been nine of these. However, one (the SU(3) singlet η′ meson) has quite a larger mass than the SU(3) octet. In the quark model, this has no natural explanation – a mystery named the η−η′ mass splitting (the η is one member of the octet, which should have been degenerate in mass with the η′).\n\nIn QCD, one realizes that the η′ is associated with the axial U(1) which is \"explicitly broken\" through the chiral anomaly, and thus its mass is not \"protected\" to be small, like that of the η. The η–η′ mass splitting can be explained\n\nPCAC and current algebra also provide evidence for this pattern of SSB. Direct estimates of the chiral condensate also come from such analysis.\n\nAnother method of analysis of correlation functions in QCD is through an operator product expansion (OPE). This writes the vacuum expectation value of a non-local operator as a sum over VEVs of local operators, i.e., condensates. The value of the correlation function then dictates the values of the condensates. Analysis of many separate correlation functions gives consistent results for several condensates, including the gluon condensate, the quark condensate, and many mixed and higher order condensates. In particular one obtains\n\nHere refers to the gluon field tensor, to the quark field, and to the QCD coupling.\n\nThese analyses are being refined further through improved sum rule estimates and direct estimates in lattice QCD. They provide the \"raw data\" which must be explained by models of the QCD vacuum.\n\nA full solution of QCD should give a full description of the vacuum, confinement and the hadron spectrum. Lattice QCD is making rapid progress towards providing the solution as a systematically improvable numerical computation. However, approximate models of the QCD vacuum remain useful in more restricted domains. The purpose of these models is to make quantitative sense of some set of condensates and hadron properties such as masses and form factors.\n\nThis section is devoted to models. Opposed to these are systematically improvable computational procedures such as large QCD and lattice QCD, which are described in their own articles.\n\nThe Savvidy vacuum is a model of the QCD vacuum which at a basic level is a statement that it cannot be the conventional Fock vacuum empty of particles and fields. In 1977, George Savvidy showed that the QCD vacuum with zero field strength is unstable, and decays into a state with a calculable non vanishing value of the field. Since condensates are scalar, it seems like a good first approximation that the vacuum contains some non-zero but homogeneous field which gives rise to these condensates. However, Stanley Mandelstam showed that a homogeneous vacuum field is also unstable. The instability of a homogeneous gluon field was argued by Niels Kjær Nielsen and Poul Olesen in their 1978 paper. These arguments suggest that the scalar condensates are an effective long-distance description of the vacuum, and at short distances, below the QCD scale, the vacuum may have structure.\n\nIn a type II superconductor, electric charges condense into Cooper pairs. As a result, magnetic flux is squeezed into tubes. In the dual superconductor picture of the QCD vacuum, chromomagnetic monopoles condense into dual Cooper pairs, causing chromoelectric flux to be squeezed into tubes. As a result, confinement and the \"string picture\" of hadrons follows. This dual superconductor picture is due to Gerard 't Hooft and Stanley Mandelstam. 't Hooft showed further that an Abelian projection of a non-Abelian gauge theory contains magnetic monopoles.\n\nWhile the vortices in a type II superconductor are neatly arranged into a hexagonal or occasionally square lattice, as is reviewed in Olesen's 1980 seminar one may expect a much more complicated and possibly dynamical structure in QCD. For example, nonabelian Abrikosov-Nielsen-Olesen vortices may vibrate wildly or be knotted.\n\nString models of confinement and hadrons have a long history. They were first invented to explain certain aspects of crossing symmetry in the scattering of two mesons. They were also found to be useful in the description of certain properties of the Regge trajectory of the hadrons. These early developments took on a life of their own called the dual resonance model (later renamed string theory). However, even after the development of QCD string models continued to play a role in the physics of strong interactions. These models are called \"non-fundamental strings\" or QCD strings, since they should be derived from QCD, as they are, in certain approximations such as the strong coupling limit of lattice QCD.\n\nThe model states that the colour electric flux between a quark and an antiquark collapses into a string, rather than spreading out into a Coulomb field as the normal electric flux does. This string also obeys a different force law. It behaves as if the string had constant tension, so that separating out the ends (quarks) would give a potential energy increasing linearly with the separation. When the energy is higher than that of a meson, the string breaks and the two new ends become a quark-antiquark pair, thus describing the creation of a meson. Thus confinement is incorporated naturally into the model.\n\nIn the form of the Lund model Monte Carlo program, this picture has had remarkable success in explaining experimental data collected in electron-electron and hadron-hadron collisions.\n\nStrictly, these models are not models of the QCD vacuum, but of physical single particle quantum states — the hadrons. The model proposed originally in 1974 by A. Chodos \"et al.\" \nconsists of inserting a quark model in a \"perturbative vacuum\" inside a volume of space called a bag. Outside this bag is the real QCD vacuum, whose effect is taken into account through the difference between energy density of the true QCD vacuum and the perturbative vacuum (bag constant ) and boundary conditions imposed on the quark wave functions and the gluon field. The hadron spectrum is obtained by solving the Dirac equation for quarks and the Yang–Mills equations for gluons. The wave functions of the quarks satisfy the boundary conditions of a fermion in an infinitely deep potential well of scalar type with respect to the Lorentz group. The boundary conditions for the gluon field are those of the dual color superconductor. The role of such a superconductor is attributed to the physical vacuum of QCD. Bag models strictly prohibit the existence of open color (free quarks, free gluons, etc.) and lead in particular to string models of hadrons.\n\nThe chiral bag model couples the axial vector current of the quarks at the bag boundary to a pionic field outside of the bag. In the most common formulation, the chiral bag model basically replaces the interior of the skyrmion with the bag of quarks. Very curiously, most physical properties of the nucleon become mostly insensitive to the bag radius. Prototypically, the baryon number of the chiral bag remains an integer, independent of bag radius: the exterior baryon number is identified with the topological winding number density of the Skyrme soliton, while the interior baryon number consists of the valence quarks (totaling to one) plus the spectral asymmetry of the quark eigenstates in the bag. The spectral asymmetry is just the vacuum expectation value summed over all of the quark eigenstates in the bag. Other values, such as the total mass and the axial coupling constant , are not precisely invariant like the baryon number, but are mostly insensitive to the bag radius, as long as the bag radius is kept below the nucleon diameter. Because the quarks are treated as free quarks inside the bag, the radius-independence in a sense validates the idea of asymptotic freedom.\n\nAnother view states that BPST-like instantons play an important role in the vacuum structure of QCD. These instantons were discovered in 1975 by Alexander Belavin, Alexander Markovich Polyakov, Albert S. Schwarz and Yu. S. Tyupkin as topologically stable solutions to the Yang-Mills field equations. They represent tunneling transitions from one vacuum state to another. These instantons are indeed found in lattice calculations. The first computations performed with instantons used the dilute gas approximation. The results obtained did not solve the infrared problem of QCD, making many physicists turn away from instanton physics. Later, though, an instanton liquid model was proposed, turning out to be more promising an approach.\n\nThe dilute instanton gas model departs from the supposition that the QCD vacuum consists of a gas of BPST-like instantons. Although only the solutions with one or few instantons (or anti-instantons) are known exactly, a dilute gas of instantons and anti-instantons can be approximated by considering a superposition of one-instanton solutions at great distances from one another. Gerard 't Hooft calculated the effective action for such an ensemble, and he found an infrared divergence for big instantons, meaning that an infinite amount of infinitely big instantons would populate the vacuum.\n\nLater, an instanton liquid model was studied. This model starts from the assumption that an ensemble of instantons cannot be described by a mere sum of separate instantons. Various models have been proposed, introducing interactions between instantons or using variational methods (like the \"valley approximation\") endeavoring to approximate the exact multi-instanton solution as closely as possible. Many phenomenological successes have been reached. Whether an instanton liquid can explain confinement in 3+1 dimensional QCD is not known, but many physicists think that it is unlikely.\n\nA more recent picture of the QCD vacuum is one in which center vortices play an important role. These vortices are topological defects carrying a center element as charge. These vortices are usually studied using lattice simulations, and it has been found that the behavior of the vortices is closely linked with the confinement–deconfinement phase transition: in the confinement phase vortices percolate and fill the spacetime volume, in the deconfinement phase they are much suppressed. Also it has been shown that the string tension vanished upon removal of center vortices from the simulations, hinting at an important role for center vortices.\n\n\n"}
{"id": "45696716", "url": "https://en.wikipedia.org/wiki?curid=45696716", "title": "Sheaf of planes", "text": "Sheaf of planes\n\nIn mathematics, a sheaf of planes is the set of all planes that have the same common line. It may also be known as a pencil or fan of planes.\n\nWhen extending the concept of line to the line at infinity, a set of parallel planes can be seen as a \"sheaf of planes\" intersecting in a \"line at infinity\". To distinguish it from the more common definition the adjective \"parallel\" can be added to it, resulting in the expression: \"parallel sheaf of planes\".\n\n"}
{"id": "46674487", "url": "https://en.wikipedia.org/wiki?curid=46674487", "title": "Small Arms Weapons Effects Simulator", "text": "Small Arms Weapons Effects Simulator\n\nThe Small Arms Weapons Effect Simulator (SAWES) was a training device used by the British Army during the 1980s. It consisted of an infrared projector mounted on the L1A1 self-loading rifle and later the SA80, and a harness with receptors to receive the beams to simulate hits. The sight had a similar reticle as the SUSAT and a cable attached to the trigger that activated the device when using blank ammunition. An unusual 'Umpire gun' existed made from L1A1 components was used by range staff.\n\n"}
{"id": "18106366", "url": "https://en.wikipedia.org/wiki?curid=18106366", "title": "Storrun wind farm", "text": "Storrun wind farm\n\nStorrun Wind Farm is a 30 MW wind farm in Krokom in Jämtland, Sweden. The wind farm was opened on 26 September 2009. It consists of 12 Nordex 2.5 MW N90 wind turbines. The developer and operator of the wind farm is Storrun Vindkraft AB, a joint venture of DONG Energy (80%) and Borevind AB (20%). The wind farm covers an area of approximately and average wind speed at the hub height is . Due to the northern location, the project includes anti-ice coated blades and estimation of production losses caused by icing.\n\n"}
{"id": "14248468", "url": "https://en.wikipedia.org/wiki?curid=14248468", "title": "Stéphane Lhomme", "text": "Stéphane Lhomme\n\nStéphane Lhomme (born 4 November 1965 in Bordeaux) is president of Tchernoblaye association, and was speaker of \"Sortir du nucléaire\" Network during 2002-2010.\n\nStéphane Lhomme was arrested in May 2006 and in April 2008 by French police for allegedly leaking a confidential report saying that the European Pressurized Reactor French nuclear reactor would not resist to an airplane crash.\n"}
{"id": "13313000", "url": "https://en.wikipedia.org/wiki?curid=13313000", "title": "Substellar object", "text": "Substellar object\n\nA substellar object, sometimes called a substar, is an astronomical object whose mass is smaller than the smallest mass at which hydrogen fusion can be sustained (approximately 0.08 solar masses). This definition includes brown dwarfs and former stars similar to EF Eridani B, and can also include objects of planetary mass, regardless of their formation mechanism and whether or not they are associated with a primary star.\n\nAssuming that a substellar object has a composition similar to the Sun's and at least the mass of Jupiter (approximately 10 solar masses), its radius will be comparable to that of Jupiter (approximately 0.1 solar radii) regardless of the mass of the substellar object (brown dwarfs are less than 75 Jupiter masses). This is because the center of such a substellar object at the top range of the mass (just below the hydrogen-burning limit) is quite degenerate, with a density of ≈10 g/cm, but this degeneracy lessens with decreasing mass until, at the mass of Jupiter, a substellar object has a central density less than 10 g/cm. The density decrease balances the mass decrease, keeping the radius approximately constant.\n\nSubstellar objects like brown dwarfs can live forever even though they do not have enough mass to fuse hydrogen and helium.\n\nA substellar object with a mass just below the hydrogen-fusing limit may ignite hydrogen fusion temporarily at its center. Although this will provide some energy, it will not be enough to overcome the object's ongoing gravitational contraction. Likewise, although an object with mass above approximately 0.013 solar masses will be able to fuse deuterium for a time, this source of energy will be exhausted in approximately 10 to 10 years. Apart from these sources, the radiation of an isolated substellar object comes only from the release of its gravitational potential energy, which causes it to gradually cool and shrink. A substellar object in orbit about a star will shrink more slowly as it is kept warm by the star, evolving towards an equilibrium state where it emits as much energy as it receives from the star.\n\nSubstellar objects are cool enough to have water vapor in their atmosphere. Infrared spectroscopy can detect the distinctive color of water in gas giant size substellar objects, even if they are not in orbit about a star.\n\nWilliam Duncan MacMillan proposed in 1918 the classification of substellar objects into three categories based on their density and phase state: solid, transitional and dark (non-stellar) gaseous. Solid objects include Earth, smaller terrestrial planets and moons; with Uranus and Neptune (as well as later mini-Neptune and Super Earth planets) as transitional objects between solid and gaseous. Saturn, Jupiter and large gas giant planets are in a fully \"gaseous\" state.\n\n\n"}
{"id": "896811", "url": "https://en.wikipedia.org/wiki?curid=896811", "title": "Tornado Alley", "text": "Tornado Alley\n\nTornado Alley is a colloquial term for the area of the United States (or by some definitions extending into Canada) where tornadoes are most frequent. The term was first used in 1952 as the title of a research project to study severe weather in areas of Texas, Oklahoma, Kansas, South Dakota, Iowa, Missouri, Nebraska, Colorado, North Dakota, and Minnesota. It is largely a media-driven term although tornado climatologists distinguish peaks in activity in certain areas and storm chasers have long recognized the Great Plains tornado belt.\n\nAlthough the official boundaries of Tornado Alley are not clearly defined, its core extends from northern Texas, Oklahoma, Kansas, into Nebraska. Some research suggests that tornadoes are becoming more frequent in the northern parts of Tornado Alley where it reaches the Canadian prairies.\n\nActive tornado regions have also been discovered in other areas of the world, such as in parts of Europe and Australia and especially in the Pampas lowlands of Argentina extending into Uruguay and adjacent areas of Paraguay and extreme southern Brazil. The Pampas has a high consistency of tornadoes, whereas Bangladesh and adjacent East India have the highest frequency of violent tornadoes outside the central and southern United States.\n\nOver the years, the location(s) of Tornado Alley have not been clearly defined. No definition of tornado alley has ever been officially designated by the National Weather Service (NWS). Thus, differences in location are the result of the different criteria used.\n\nAccording to the National Severe Storms Laboratory (NSSL) FAQ, \"Tornado Alley\" is a term used by the media as a reference to areas that have higher numbers of tornadoes. A study of 1921–1995 tornadoes concluded almost one-fourth of all significant tornadoes occur in this area.\n\nNo state is entirely free of tornadoes; however, they occur more frequently in the Central United States, between the Rocky Mountains and Appalachian Mountains. Texas reports the most tornadoes of any state due to its large size, in addition to its proximity to Tornado Alley. Kansas and Oklahoma ranked first and second respectively in the number of tornadoes per area, per data collected through 2007, however in 2013 statistics from the National Climatic Data Center show Florida ranked first. Although Florida reports a high number and density of tornado occurrences, tornadoes there rarely reach the strength of those that sometimes occur in the southern plains. Regionally, the frequency of tornadoes in the United States is closely tied with the progression of the warm season when warm and cold air masses often clash.\n\nAnother criterion for the location of Tornado Alley (or Tornado Alleys) can be where the strongest tornadoes occur more frequently.\n\nTornado Alley can also be defined as an area reaching from central Texas to the Canadian prairies and from eastern Colorado to western Pennsylvania.\n\nIt has also been asserted that there are numerous Tornado Alleys. In addition to the Texas/Oklahoma/Kansas core, such areas include the Upper Midwest, the lower Ohio Valley, the Tennessee Valley and the lower Mississippi valley. Some studies suggest that there are also smaller tornado alleys located across the United States.\n\nThe tornado alleys in the southeastern U.S., notably the lower Mississippi Valley and the upper Tennessee Valley, are sometimes called by the nickname \"Dixie Alley\", coined in 1971 by Allen Pearson, former director of the National Severe Storms Forecasting Center (NSSFC).\n\nIn Tornado Alley, warm, humid air from the equator meets cool, dry air from Canada and the Rocky Mountains. This creates an ideal environment for tornadoes to form within developed thunderstorms and super cells.\n\nThe term \"tornado alley\" was first used in 1952 by U.S. Air Force meteorologists Major Ernest J. Fawbush (1915–1982) and Captain Robert C. Miller (1920–1998) as the title of a research project to study severe weather in parts of Texas and Oklahoma.\n\nDespite the elevated frequency of destructive tornadoes, building codes, such as requiring strengthened roofs and more secure connections between the building and its foundation, are not necessarily stricter compared to other areas of the United States and are markedly weaker than some hurricane prone areas such as south Florida. One particular tornado-afflicted town, Moore, Oklahoma, managed to increase its building requirements in 2014. Other common precautionary measures include the construction of storm cellars, and the installation of tornado sirens. Tornado awareness, preparedness, and media weather coverage are also high.\n\nThe southeastern United States is particularly prone to violent, long track tornadoes. Much of the housing in this region is less robust compared to other areas in the United States, and many people live in mobile homes. As a result, tornado-related casualties in the southern United States are particularly higher. Significant tornadoes occur less frequently than in the traditionally recognized tornado alley, however, very severe and expansive outbreaks occur every few years.\n\nThe average tornado usually only stays on ground for 5 minutes but but these figures, reported by the National Climatic Data Center for the period between 1991 and 2010, show the seventeen U.S. states with the highest average number of tornadoes per per year.\nAfter the U.S., Canada gets the most tornadoes in the world. The average number of tornadoes per is highest in the southern parts of Alberta, Saskatchewan, and Manitoba, as well as Southern Ontario. Northern Ontario between the Manitoba border and the Lake Superior lakehead is also prone to severe tornadoes, but tornadoes in this area are believed to be underestimated due to the extremely low population in this region.\n\nRoughly half of all Canadian tornadoes strike the Canadian prairies and Northern Ontario as far east as Lake Superior. Together, these regions make up the northernmost border of the U.S. Tornado Alley. Tornadoes up to EF5 in strength have been documented in this region.\n\nAnother third of Canadian tornadoes strike Southern Ontario, especially in the region halfway between the Great Lakes from Lake St. Clair to Ottawa. This happens because tornadoes in this region are triggered or augmented by lake breeze fronts from Lake Huron, Lake Erie, Lake Ontario, and Georgian Bay. Tornadoes do not often hit lake shadow regions, although they are not unknown, and some, such as the 2011 Goderich, Ontario tornado, have been violent. However, most Ontario tornadoes are concentrated in a narrow corridor from Windsor and Sarnia through London, and then northeast to Barrie and Ottawa. Tornadoes up to EF4 in strength have been documented in this region.\n\nSouthwestern Ontario weather is strongly influenced by its peninsular position between the Great Lakes. As a result, increases in temperature in this region are likely to increase the amount of precipitation in storms due to lake evaporation. Increased temperature contrasts may also increase the violence and possibly the number of tornadoes.\n\n\n"}
{"id": "2839367", "url": "https://en.wikipedia.org/wiki?curid=2839367", "title": "Traditional dyes of the Scottish Highlands", "text": "Traditional dyes of the Scottish Highlands\n\nTraditional dyes of the Scottish Highlands are the\nnative vegetable dyes used in Scottish Gaeldom.\n\nThe following are the principal dyestuffs with the colours they produce. Several of the tints are very bright, but have now been superseded for convenience of usage by various mineral dyes. The Latin names are given where known and also the Scottish Gaelic names for various ingredients. Amateurs may wish to experiment with some of the suggestions, as urine (human or animal) is used in many recipes as a mordant. A number of the recipes used are for more than one colour, and that this chart is only a guide, and also that Scottish Gaelic spelling is subject to variations. Many of the dyes are made from lichens, the useful ones for this purpose being known as crottle.\n\n\n\n\n\n\n\n\n\n\n\n\nThe process employed is to wash the thread thoroughly in urine long kept (\"fual\"), rinse and wash in pure water, then put into the boiling pot of dye which is kept boiling hot on the fire. The thread is lifted now and again on the end of a stick, and again plunged in until it is all thoroughly dyed. If blue, the thread is then washed in salt water but any other colour uses fresh water.\n\n\n\n"}
{"id": "3867349", "url": "https://en.wikipedia.org/wiki?curid=3867349", "title": "Uranyl acetate", "text": "Uranyl acetate\n\nUranyl acetate (UO(CHCOO)·2HO) is the acetate salt of uranyl and is a yellow-green crystalline solid made up of yellow-green rhombic crystals and has a slight acetic odor. This compound is a nuclear fuel derivative, and its use and possession are sanctioned by international law.\n\nCommercial preparations of uranyl acetate are usually made from depleted uranium and are prepared by reacting metallic uranium with acetic acid.\n\nUranyl acetate is extensively used as a negative stain in electron microscopy. Most procedures in electron microscopy for biology require the use of uranyl acetate. Negative staining protocols typically treat the sample with 1% to 5% aqueous solution. Uranyl acetate staining is simple and quick to perform and one can examine the sample within a few minutes after staining. Some biological samples are not amenable to uranyl acetate staining and, in these cases, alternative staining techniques and or low-voltage electron microscopy technique may be more suitable.\n\n1% and 2% uranyl acetate solutions are used as an indicator, and a titrant in stronger concentrations in analytical chemistry, as it forms an insoluble salt with sodium (the vast majority of sodium salts are water-soluble). Uranyl acetate solutions show evidence of being sensitive to light, especially UV, and will precipitate if exposed.\n\nUranyl acetate is also used in a standard test—American Association of State Highway and Transportation Officials (AASHTO) Designation T 299—for alkali-silica reactivity in aggregates (crushed stone or gravel) being considered for use in cement concrete. \n\nUranyl acetate dihydrate has been used as a starting reagent in experimental inorganic chemistry, for example, [UOCl(THF)] (THF = tetrahydrofuran).\n\nUranyl acetate is both radioactive and toxic. Normal commercial stocks prepared from depleted uranium have a typical specific activity of per gram. This is a very mild level of radioactivity and is not sufficient to be harmful while the material remains external to the body.\n\nUranyl acetate is very toxic if ingested, inhaled as dust or by skin contact if skin is cut or abraded. The toxicity is due to the combined effect of chemical toxicity and mild radioactivity and there is a danger of cumulative effects from long term exposure.\n\n"}
{"id": "14921763", "url": "https://en.wikipedia.org/wiki?curid=14921763", "title": "Vankusawade Wind Park", "text": "Vankusawade Wind Park\n\nVankusawade Wind Park is a wind farm located on a high mountain plateau at 1,150 m above the Koyana Reservoir, around 40 km from the town of Satara, Satara District in Maharashtra.\n\nWind power is generated from Suzlon S33/350 turbines of 350 kW each, resulting in a total power output of 210 MW.\n"}
{"id": "4186087", "url": "https://en.wikipedia.org/wiki?curid=4186087", "title": "Water dimer", "text": "Water dimer\n\nThe water dimer consists of two water molecules loosely bound by a hydrogen bond. It is the smallest water cluster. Because it is the simplest model system for studying hydrogen bonding in water, it has been the target of many theoretical (and later experimental) studies that it has been called a \"theoretical Guinea pig\".\n\nThe ab initio binding energy between the two water molecules is estimated to be 5-6 kcal/mol, although values between 3 and 8 have been obtained depending on the method. The experimentally measured dissociation energy (including nuclear quantum effects) of (HO) and (DO) are 3.16 ± 0.03 kcal/mol (13.22 ± 0.12 kJ/mol) and 3.56 ± 0.03 kcal/mol (14.88 ± 0.12 kJ/mol), respectively. The values are in excellent agreement with calculations. The O-O distance of the vibrational ground-state is experimentally measured at ca. 2.98 Å; the hydrogen bond is almost linear, but the angle with the plane of the acceptor molecule is about 57°. The vibrational ground-state is known as the linear water dimer (shown in the figure to the right), which is a near prolate top (viz., in terms of rotational constants, A > B ≈ C). Other configurations of interest include the cyclic dimer and the bifurcated dimer.\n\nThe first theoretical study of the water dimer was an ab initio calculation published in 1968 by Morokuma and Pedersen. Since then, the water dimer has been the focus of sustained interest by theoretical chemists concerned with hydrogen bonding—a search of the CAS database up to 2006 returns over 1100 related references (73 of them in 2005). The water dimer is a hotly studied topic in Physical Chemistry for several reasons. (HO) is thought to play a significant role in many atmospheric processes, including acid rain formation, absorption of excess solar radiation, condensation of water droplets, and chemical reactions. In addition, a complete understanding of the water dimer is thought to play a key role in a more thorough understanding of hydrogen bonding in liquid and solid forms of water.\n"}
