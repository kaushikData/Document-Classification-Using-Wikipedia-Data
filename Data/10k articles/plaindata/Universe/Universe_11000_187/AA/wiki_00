{"id": "4754114", "url": "https://en.wikipedia.org/wiki?curid=4754114", "title": "Abandoned footwear", "text": "Abandoned footwear\n\nAbandoned footwear, such as a lone boot or shoe, has often been noted in out-of-the-way places like ponds or by the side of roads. Sometimes the shoes may even be new and fashionable.\n\nThere are many hypotheses about why this phenomenon seems to more often involve footwear than other types of clothing. Shoes, being more sturdily constructed than most other types of clothing, will last longer after being abandoned outdoors. Leather shoes, for instance, are estimated to last for 25–40 years outside. Some shoe abandonment is intentional, as in shoe tossing, in which shoes are tied together by their laces and thrown in great numbers into trees, over power lines, or over fences.\n\nAbandoned footwear is a feature in a number of artistic works, including: \n\n\n\n\n\n\nLeaving behind shoes can be a symbol of retirement in sport. For example, as ESPN's Sherry Skalko describes about Rulon Gardner's last wrestling bout in Athens, Greece: \nAn emotional Rulon Gardner prepares to leave his shoes on the mat -- a symbol of retirement.\n\nAfter the referee raised Gardner's hand in victory -- first to one side of the arena, then to the other -- Gardner grabbed an American flag, wiped away tears and parked himself in the middle of Mat B like \"a 33-year-old kid\" and took off his size 13 shoes. First the right one, the one that contains the constant reminder of the snowmobiling accident that almost took his life two years ago, then the left.\n\nThen the super heavyweight bronze medalist stood up, bowed his head at each side of the mat and walked off, leaving his shoes behind, a wrestler's signal that he had fought his final bout.\n\nSeattle Seahawks running back Marshawn Lynch announced his retirement on February 7, 2016 by tweeting a picture of a pair of football cleats abandoned on a telephone wire.\n\nAn unusual abundance of abandoned shoes was found on Miami's Palmetto Expressway on Friday, 2 January 2009. Thousands of assorted shoes of all kinds and conditions were scattered across the highway, disrupting traffic for many hours. The shoes were collected for the charity Soles4Souls which redistributes shoes to needy people. This unusually large batch of shoes was expected to go to Haiti.\n\n\n"}
{"id": "4033683", "url": "https://en.wikipedia.org/wiki?curid=4033683", "title": "Battery (vacuum tube)", "text": "Battery (vacuum tube)\n\nIn the early days of electronics, vacuum tube (called \"valves\" in British contexts) devices (such as radios) were powered by batteries. Each battery had a different designation depending on which vacuum tube element it was associated with.\n\nInitially, the only such device was a diode with only a filament (cathode) and a plate (anode). Following the direction of electron flow, these electrodes were identified as \"A\" and \"B\", respectively and thus the associated batteries were referred to as the \"A\" battery and \"B\" battery, respectively. Later, when the control grid element was added to create the triode tube, it was logically assigned the letter \"C\" and supplied from a \"C\" battery. Subsequent addition of further internal elements to improve the performance of the triode did not require an extension to this series of batteries – these elements were either resistively-biased from the existing batteries, connected to ground or to the cathode.\n\nThis nomenclature was used primarily within North America. Different battery names were used elsewhere in the English speaking world.\n\nAn A battery is any battery used to provide power to the filament of a vacuum tube. It is sometimes colloquially referred to as a \"wet battery\". (A dry cell could be used for the purpose, but the ampere-hour capacity of dry cells was too low at the time to be of practical use in this service). The term comes from the days of valve (tube) radios when it was common practice to use a dry battery for the plate (anode) voltage and a rechargeable lead/acid \"wet\" battery for the filament voltage. (The filaments in vacuum tubes consumed much more current than the anodes, and so the \"A\" battery would drain much more rapidly than the \"B\" battery; therefore, using a rechargeable \"A\" battery in this role reduced the need for battery replacement. In contrast, a non-rechargeable \"B\" battery would need to be replaced relatively infrequently.)\n\nA batteries were initially 2 volts, being lead acid accumulators, but with the introduction of all dry battery radios, 1.4 volts became more common. Other voltages can be encountered. For example, 7.5 volt batteries were sometimes used to power a series connected set of 1.4 volt valves (tubes).\n\nIn Britain and some other countries, the \"A\" battery was known as the \"LT\" (low tension) battery if dry, and simply the \"accumulator\" if wet.\n\nA B battery is any battery used to provide the plate voltage of a vacuum tube. It is sometimes colloquially referred to as a \"dry battery\" (although there's no reason why a \"wet\" battery of suitable voltage couldn't be utilised for the purpose).\n\nThe filament is primarily a heat source and therefore the A battery supplies significant current and rapidly discharges. The B battery experiences comparatively little current draw and retains its stored capacity far longer than an A battery. Early B batteries used with bright emitter tubes were 120 volts, but these quickly became obsolete as they were replaced with examples having voltages of typically 45 volts, 67½ volts, or 90 volts as more efficient tubes became available. Some examples had taps every 22½ volts.\n\nEven when the plate voltage rail is fed by a power supply rather than a battery, it is generally referred to as the \"B+\" line in American schematics.\n\nBecause plate voltages can be as high as 300V DC, multiple B batteries may be connected together in series to additively provide the required operating voltages. The much higher available voltage of B batteries means that they must be handled more carefully than other battery types due to their ability to shock and/or burn the person handling them.\n\nIn Britain and in some other countries, the \"B\" battery was known as the \"HT\" (high tension) battery.\n\nIn electronics, a C battery may refer to any battery used to provide bias to the control grid of a vacuum tube. Until the early 1930s this was common practice in valve (tube) radio sets but was largely superseded by grid leak resistors or voltage divider biasing. Because the tube grids drew no current, the C battery provided the bias voltage with no current draw. The battery's life in the radio was essentially its shelf life.\n\nGrid bias batteries are still manufactured today, but not for radio use. They are popular in schools and colleges as a convenient variable voltage source in science classes. The most popular battery is the 9 volt type with taps every 1½ volts that accept banana plugs.\n\nA rare form of \"C\" battery was the \"bias cell\", a button-size miniature battery designed to deliver a constant voltage with no current drain. These were briefly popular between 1936 and 1945 as the bias cell was less costly than a resistor/capacitor bias network.\n\nIn Britain and in some other countries, the \"C\" battery was known as the \"GB\" (grid bias) battery.\n\n\n"}
{"id": "11012831", "url": "https://en.wikipedia.org/wiki?curid=11012831", "title": "Bioenergetic systems", "text": "Bioenergetic systems\n\nBioenergetic systems are metabolic processes that relate to the flow of energy in living organisms. Those processes convert energy into adenosine triphosphate (ATP), which is the form suitable for muscular activity. There are two main forms of synthesis of ATP: \"aerobic\", which involves oxygen from the bloodstream, and \"anaerobic\", which does not. Bioenergetics is the field of biology that studies bioenergetic systems.\n\nThe cellular respiration process that converts food energy into ATP (a form of energy) is largely dependent on oxygen availability. During exercise, the supply and demand of oxygen available to muscle cells is affected by duration and intensity and by the individual's cardiorespiratory fitness level. Three exercise energy systems can be selectively recruited, depending on the amount of oxygen available, as part of the cellular respiration process to generate the ATP for the muscles. They are ATP, the anaerobic system and the aerobic system.\n\nATP is the usable form of chemical energy for muscular activity. It is stored in most cells, particularly in muscle cells. Other forms of chemical energy, such as those available from food, must be transformed into ATP before they can be utilized by the muscle cells.\n\nSince energy is released when ATP is broken down, energy is required to rebuild or resynthesize it. The building blocks of ATP synthesis are the by-products of its breakdown; adenosine diphosphate (ADP) and inorganic phosphate (Pi). The energy for ATP resynthesis comes from three different series of chemical reactions that take place within the body. Two of the three depend upon the type of food eaten, whereas the other depends upon a chemical compound called phosphocreatine. The energy released from any of these three series of reactions is coupled with the energy needs of the reaction that resynthesizes ATP. The separate reactions are functionally linked together in such a way that the energy released by the one is always used by the other.\n\nThree methods can synthesize ATP:\n\nAerobic and anaerobic systems usually work concurrently. When describing activity, it is not a question of which energy system is working, but which predominates.\n\nThe term metabolism refers to the various series of chemical reactions that take place within the body. Aerobic refers to the presence of oxygen, whereas anaerobic means with series of chemical reactions that does not require the presence of oxygen. The ATP-CP series and the lactic acid series are anaerobic, whereas the oxygen series is aerobic.\n\nCreatine phosphate (CP), like ATP, is stored in muscle cells. When it is broken down, a large amount of energy is released. The energy released is coupled to the energy requirement necessary for the resynthesis of ATP.\n\nThe total muscular stores of both ATP and CP are small. Thus, the amount of energy obtainable through this system is limited. The phosphogen stored in the working muscles is typically exhausted in seconds of vigorous activity. However, \"the usefulness of the ATP-CP system lies in the rapid availability of energy rather than quantity\". This is important with respect to the kinds of physical activities that humans are capable of performing.\n\nThis system is known as anaerobic glycolysis. \"Glycolysis\" refers to the breakdown of sugar. In this system, the breakdown of sugar supplies the necessary energy from which ATP is manufactured. When sugar is metabolized anaerobically, it is only partially broken down and one of the byproducts is lactic acid. This process creates enough energy to couple with the energy requirements to resynthesize ATP.\n\nWhen H+ ions accumulate in the muscles causing the blood pH level to reach very low levels, temporary muscle fatigue results. Another limitation of the lactic acid system that relates to its anaerobic quality is that only a few moles of ATP can be resynthesized from the breakdown of sugar as compared to the yield possible when oxygen is present. This system cannot be relied on for extended periods of time.\n\nThe lactic acid system, like the ATP-CP system, is important primarily because it provides a rapid supply of ATP energy. For example, exercises that are performed at maximum rates for between 1 and 3 minutes depend heavily upon the lactic acid system for ATP energy. In activities such as running 1500 meters or a mile, the lactic acid system is used predominately for the \"kick\" at the end of a race.\n\n\nThis stage of the aerobic system occurs on the cristae (infoldings on the membrane of the mitochondria). The NADH+ from glycolysis and the Krebs cycle, and the FADH+ from the Krebs cycle produce electron carriers at decreasing energy levels, in which energy is released to reform ATP. Each NADH+ that travels this electron transport chain provides enough energy for 3 molecules of ATP, and each molecule of FADH+ provides enough energy for 2 molecules of ATP. If you do your math this means that 10 total NADH+ molecules allow the rejuvenation of 30 ATP, and 2 FADH+ molecules allow for 4 ATP molecules to be rejuvenated (the total being 34 from oxidative phosphorylation, plus the 4 from the previous 2 stages meaning a total of 38 ATP being produced during the aerobic system). The NADH+ and FADH+ get oxidized to allow the NAD and FAD to return to be used in the aerobic system again, and electrons and hydrogen ions are accepted by oxygen to produce water, a harmless byproduct.\n\n"}
{"id": "6423", "url": "https://en.wikipedia.org/wiki?curid=6423", "title": "Calorie", "text": "Calorie\n\nA calorie or calory (archaic) is a unit of energy. Various definitions exist but fall into two broad categories. The first, the small calorie, or gram calorie (symbol: cal), is defined as the amount of heat energy needed to raise the temperature of one gram of water by one degree Celsius at a pressure of one atmosphere. The second, the large calorie or kilogram calorie (symbols: Cal, kcal), also known as the food calorie and similar names, is defined as the heat energy required to raise the temperature of one kilogram (rather than a gram) of water by one degree Celsius. It is equal to small calories.\n\nAlthough these units relate to the metric system, all of them have been considered obsolete in science since the adoption of the SI system. The unit of energy in the International System of Units is the joule. One small calorie is approximately 4.2 joules (so one large calorie is about 4.2 kilojoules). The factor used to convert calories to joules at a given temperature is numerically equivalent to the specific heat capacity of water expressed in joules per kelvin per gram (or per kilogram, for kilocalories). The precise conversion factor depends on the definition adopted.\n\nIn spite of its non-official status, the large calorie is still widely used as a unit of food energy. The small calorie is also often used for measurements in chemistry, although the amounts involved are typically recorded in kilocalories.\n\nThe (large) calorie was first defined by Nicolas Clément in 1824 as a unit of heat energy. It entered French and English dictionaries between 1841 and 1867. The word comes . The small calorie was introduced by P. A. Favre and J. T. Silbermann in 1852. In 1879, Marcellin Berthelot introduced the convention of capitalizing the large Calorie to distinguish the senses. The use of the (large) calorie for nutrition was introduced to the American public by Wilbur Olin Atwater, a professor at Wesleyan University, in 1887.\n\nThe energy needed to increase the temperature of a given mass of water by 1 °C depends on the atmospheric pressure and the starting temperature. Accordingly, several different precise definitions of the calorie have been used.\n\nThe pressure is usually taken to be the standard atmospheric pressure (). The temperature increase can be expressed as one kelvin, which means the same as an increment of one degree Celsius.\n\nThe two definitions most common in older literature appear to be the \"15 °C calorie\" and the \"thermochemical calorie\". Until 1948, the latter was defined as 4.1833 international joules; the current standard of 4.184 J was chosen to have the new thermochemical calorie represent the same quantity of energy as before.\n\nThe calorie was first defined specifically to measure energy in the form of heat, especially in experimental calorimetry.\n\nIn a nutritional context, the kilojoule (kJ) is the SI unit of food energy, although the kilocalorie is still in common use. The word \"calorie\" is popularly used with the number of kilocalories of nutritional energy measured. As if to avoid confusion, it is sometimes written \"Calorie\" (with a capital \"C\") in an attempt to make the distinction, although this is not widely understood. Capitalization contravenes the rule that the initial letter of a unit name or its derivative shall be lower case in English.\n\nTo facilitate comparison, specific energy or energy density figures are often quoted as \"calories per serving\" or \"kilocalories per 100 g\". A nutritional requirement or consumption is often expressed in calories per day. One gram of fat in food contains nine calories, while a gram of either a carbohydrate or a protein contains approximately four calories. Alcohol in a food contains seven calories per gram.\n\nIn other scientific contexts, the term \"calorie\" almost always refers to the small calorie. Even though it is not an SI unit, it is still used in chemistry. For example, the energy released in a chemical reaction per mole of reagent is occasionally expressed in kilocalories per mole. Typically, this use was largely due to the ease with which it could be calculated in laboratory reactions, especially in aqueous solution: a volume of reagent dissolved in water forming a solution, with concentration expressed in moles per liter (1 liter weighing 1 kg), will induce a temperature change in degrees Celsius in the total volume of water solvent, and these quantities (volume, molar concentration and temperature change) can then be used to calculate energy per mole. It is also occasionally used to specify energy quantities that relate to reaction energy, such as enthalpy of formation and the size of activation barriers. However, its use is being superseded by the SI unit, the joule, and multiples thereof such as the kilojoule.\n\nIn the past a bomb calorimeter was utilised to determine the energy content of food by burning a sample and measuring a temperature change in the surrounding water. Today this method is not commonly used in the USA and has been succeeded by calculating the energy content indirectly from adding up the energy provided by energy-containing nutrients of food (such as protein, carbohydrates and fats). The fibre content is also subtracted to account for the fact fibre is not digested by the body.\n"}
{"id": "1342988", "url": "https://en.wikipedia.org/wiki?curid=1342988", "title": "Capacitor plague", "text": "Capacitor plague\n\nThe capacitor plague was a problem related to a higher-than-expected failure rate of non-solid aluminum electrolytic capacitors, between 1999 and 2007, especially those from some Taiwanese manufacturers, due to faulty electrolyte composition that caused corrosion accompanied by gas generation, often rupturing the case of the capacitor from the build-up of pressure.\n\nHigh failure rates occurred in many well-known brands of electronics, and was particularly evident in motherboards, video cards, and power supplies of personal computers, leading to premature failure of these devices.\n\nFaulty capacitors have been a problem since capacitors' initial development, but the first flawed capacitors linked to Taiwanese raw material problems were reported by the specialist magazine \"Passive Component Industry\" in September 2002. Shortly thereafter, two mainstream electronics journals reported the discovery of widespread prematurely failing capacitors, from Taiwanese manufacturers, in motherboards.\n\nThese publications informed engineers and other technically interested specialists, but the issue did not receive widespread public exposure until Carey Holzman published his experiences about \"leaking capacitors\" in the overclocking performance community.\n\nThe news from the Holzman publication spread quickly on the Internet and in newspapers, partly due to the spectacular images of the failures — bulging or burst cans, expelled sealing rubber and leaking electrolyte on countless circuit boards. Many PC users were affected, and caused an avalanche of reports and comments on thousands of blogs and other web communities.\n\nThe quick spread of the news also resulted in many misinformed users and blogs posting pictures of capacitors that had failed due to reasons other than faulty electrolyte.\n\nMost of the affected capacitors were produced from 1999 to 2003 and failed between 2002 and 2005. Problems with capacitors produced with an incorrectly formulated electrolyte have affected equipment manufactured up to at least 2007.\n\nMajor vendors of motherboards such as Abit, IBM, Dell, Apple, HP, and Intel were affected by capacitors with faulty electrolytes.\n\nIn 2005, Dell spent some US$420 million replacing motherboards outright and on the logistics of determining whether a system was in need of replacement.\n\nMany other equipment manufacturers unknowingly assembled and sold boards with faulty capacitors, and as a result the effect of the capacitor plague could be seen in all kinds of devices worldwide.\n\nBecause not all manufactures had offered recalls or repairs, do-it-yourself repair instructions were written and published on the Internet.\n\nIn the November/December 2002 issue of \"Passive Component Industry\", following its initial story about defective electrolyte, reported that some large Taiwanese manufacturers of electrolytic capacitors were denying responsibility for defective products.\n\nWhile industrial customers confirmed the failures, they were not able to trace the source of the faulty components. The defective capacitors were marked with previously unknown brands such as \"Tayeh\", \"Choyo\", or \"Chhsi\". The marks were not easily linked to familiar companies or product brands. Failed e-caps with well known brands may have had failures not related to defective electrolyte.\n\nThe motherboard manufacturer ABIT Computer Corp. was the only affected manufacturer that publicly admitted to defective capacitors obtained from Taiwan capacitor makers being used in its products. However, the company would not reveal the name of the capacitor maker that supplied the faulty products.\n\nThe non-solid aluminum electrolytic capacitors with improperly formulated electrolyte mostly belonged to the so-called \"low ESR\", \"low impedance\", or \"high ripple current\" e-cap series. The advantage of e-caps using an electrolyte composed of 70% water or more is, in particular, a low ESR, which allows a higher ripple current, and decreased production costs, water being the least costly material in a capacitor.\n\nAll electrolytic capacitors with non-solid electrolyte age over time, due to evaporation of the electrolyte. The capacitance usually decreases and the equivalent series resistance (ESR) usually increases. The normal lifespan of a non-solid electrolytic capacitor of consumer quality, typically rated at 2000 h/85 °C and operating at 40 °C, is roughly 6 years. It can be more than 10 years for a 1000 h/105 °C capacitor operating at 40 °C. Electrolytic capacitors that operate at a lower temperature can have a considerably longer lifespan.\n\nThe capacitance should normally degrade to as low as 70% of the rated value, and the ESR increase to twice the rated value, over the normal life span of the component, before it should be considered as a \"degradation failure\". The life of an electrolytic capacitor with defective electrolyte can be as little as two years. The capacitor may fail prematurely after reaching approximately 30% to 50% of its expected lifetime.\n\nThe electrical characteristics of a failed electrolytic capacitor with an open vent are the following:\n\nElectrolytic capacitors with an open vent are in the process of drying out, regardless of whether they have good or bad electrolyte. They always show low capacitance values and very high ohmic ESR values. Dry e-caps are therefore electrically useless.\n\nE-caps can fail without any visible symptoms. Since the electrical characteristics of electrolytic capacitors are the reason for their use, these parameters must be tested with instruments to definitively decide if the devices have failed. But even if the electrical parameters are out of their specifications, the assignment of failure to the electrolyte-problem is not a certainty.\n\nNon-solid aluminum electrolytic capacitors without visible symptoms, which have improperly formulated electrolyte, typically show two electrical symptoms:\n\nWhen examining a failed electronic device, the failed capacitors can easily be recognized by clearly visible symptoms that include the following:\n\n\nThe first electrolytic capacitor developed was an aluminum electrolytic capacitor with a liquid electrolyte, invented by Charles Pollak in 1896. Modern electrolytic capacitors are based on the same fundamental design. After roughly 120 years of development billions of these inexpensive and reliable capacitors are used in electronic devices.\n\nAluminum electrolytic capacitors with non-solid electrolyte are generally called \"electrolytic capacitors\" or \"e-caps\". The components consist of two strips of aluminum foil, separated by a paper spacer, which is saturated with a liquid or gel-like electrolyte. One of the aluminum foil strips, called the anode, chemically roughened and oxidized in a process called \"forming\", holds a very thin oxide layer on its surface as an electrical insulator serving as the dielectric of the capacitor. The liquid electrolyte, which is the cathode of the capacitor, covers the irregular surface of the oxide layer of the anode perfectly, and makes the increased anode surface effectual, thus increasing the effective capacitance.\n\nA second aluminum foil strip, called the \"cathode foil\", serves to make electrical contact with the electrolyte. The spacer separates the foil strips to avoid direct metallic contact which would produce a short circuit. Lead wires are attached to both foils which are then rolled with the spacer into a wound cylinder which will fit inside an aluminum case or \"can\". The winding is impregnated with liquid electrolyte. This provides a reservoir of electrolyte to extend the lifetime of the capacitor. The assembly is inserted into an aluminum can and sealed with a plug. Aluminum electrolytic capacitors with non-solid electrolyte have grooves in the top of the case, forming a vent, which is designed to split open in the event of excessive gas pressure caused by heat, short circuit, or failing electrolyte.\n\nThe aluminum foil used in non-solid aluminum electrolytic capacitors must have a purity of 99.99%. The foil is roughened by electrochemical etching to enlarge the effective capacitive surface. This etched anode aluminum foil is oxidized (called \"forming\"). Forming creates a very thin oxide barrier layer on the anode surface. This oxide layer is electrically insulating and serves as the dielectric of the capacitor. The forming takes place whenever a positive voltage is applied to the anode, and generates an oxide layer whose thickness varies according to the applied voltage. This electrochemical behavior explains the self-healing mechanism of non-solid electrolytic capacitors.\n\nThe normal process of oxide formation or self-healing is carried out in two reaction steps. First, a strongly exothermic reaction transforms metallic aluminum (Al) into aluminum hydroxide, Al(OH):\n\nThis reaction is accelerated by a high electric field and by high temperatures, and is accompanied by a pressure buildup in the capacitor housing, caused by the released hydrogen gas. The gel-like aluminum hydroxide Al(OH) (also called alumina trihydrate (ATH), aluminic hydroxide, aluminum(III) hydroxide, or hydrated alumina) is converted, via a second reaction step (usually slowly over a few hours at room temperature, more rapidly in a few minutes at higher temperatures), into the amorphous or crystalline form of aluminum oxide, AlO:\n\nThis oxide serves as dielectric and also protects the capacitor from the aggressive reactions of metallic aluminum to parts of the electrolyte. One problem of the forming or self-healing processes in non-solid aluminum electrolytics is that of corrosion, the electrolyte having to deliver enough oxygen to generate the oxide layer, with water, corrosive of aluminum, being the most efficient way.\n\nThe name \"electrolytic capacitor\" derives from the electrolyte, the conductive liquid inside the capacitor. As a liquid it can conform to the etched and porous structure of the anode and the grown oxide layer, and form a \"tailor-made\" cathode.\n\nFrom an electrical point of view the electrolyte in an electrolytic capacitor is the actual cathode of the capacitor and must have good electrical conductivity, which is actually ion-conductivity in liquids. But it is also a chemical mixture of solvents with acid or alkali additives, which must be non-corrosive (chemically inert) so that the capacitor, whose inner components are made of aluminum, remains stable over its expected lifetime. In addition to the good conductivity of operating electrolytes, there are other requirements, including chemical stability, chemical compatibility with aluminum, and low cost. The electrolyte should also provide oxygen for the forming processes and self-healing. This diversity of requirements for the liquid electrolyte results in a broad variety of proprietary solutions, with thousands of patented electrolytes.\n\nUp to the mid 1990s electrolytes could be roughly placed into two main groups:\n\nIt was known that water is a very good solvent for low ohmic electrolytes. However, the corrosion problems linked to water hindered, up to that time, the use of it in amounts larger than 20% of the electrolyte, the water-driven corrosion using the above-mentioned electrolytes being kept under control with chemical inhibitors that stabilize the oxide layer.\n\nIn the 1990s a third class of electrolytes was developed by Japanese researchers.\n\nAt the beginning of the 1990s, some Japanese manufacturers started the development of a new, low-ohmic water-based class of electrolytes. Water, with its high permittivity of ε = 81, is a powerful solvent for electrolytes, and possesses high solubility for conductivity-enhancing concentrations of salt ions, resulting in significantly improved conductivity compared to electrolytes with organic solvents like GBL. But water will react quite aggressively and even violently with unprotected aluminum, converting metallic aluminum (Al) into aluminum hydroxide (Al(OH)), via a highly exothermic reaction that gives off heat, causing gas expansion that can lead to an explosion of the capacitor. Therefore, the main problem in the development of water-based electrolytes is achieving long-term stability by hindering the corrosive action of water on aluminum.\n\nNormally the anode foil is covered by the dielectric aluminum oxide (AlO) layer, which protects the base aluminum metal against the aggressiveness of aqueous alkali solutions. However, some impurities or weak points in the oxide layer offer the possibility for water-driven anodic corrosion that forms aluminum hydroxide (Al(OH)). In e-caps using an alkaline electrolyte this aluminum hydroxide will not be transformed into the desired stable form of aluminum oxide. The weak point remains and the anodic corrosion is ongoing. This corrosive process can be interrupted by protective substances in the electrolyte known as inhibitors or passivators. Inhibitors—such as chromates, phosphates, silicates, nitrates, fluorides, benzoates, soluble oils, and certain other chemicals—can reduce the anodic and cathodic corrosion reactions. However, if inhibitors are used in an insufficient amount, they tend to increase pitting.\n\nThe aluminum oxide layer in the electrolytic capacitor is resistant to chemical attacks, as long as the pH value of the electrolyte is in the range of pH 4.5 to 8.5. However, the pH value of the electrolyte is ideally about 7 (neutral); and measurements carried out as early as the 1970s have shown that leakage current is increased, due to chemically induced defects, when the pH value deviates from this ideal value. It is known that water is extremely corrosive to pure aluminum and introduces chemical defects. It is further known that unprotected aluminum oxide dielectrics can be slightly dissolved by alkaline electrolytes, weakening the oxide layer.\n\nThe fundamental issue of water-containing-electrolyte systems lies in the control of aggressiveness of the water towards metallic aluminum. This issue has dominated the development of electrolytic capacitors over many decades. The first commercially used electrolytes in the mid-twentieth century were mixtures of ethylene glycol and boric acid. But even these glycol electrolytes had an unwanted chemical water-crystal reaction, according to the scheme: \"acid + alcohol\" → \"ester + water\". Thus, even in the first apparently water-free electrolytes, esterification reactions could generate a water content of up to 20 percent. These electrolytes had a voltage-dependent life span, because at higher voltages the leakage current based on the aggressiveness of the water would increase exponentially; and the associated increased consumption of electrolyte would lead to a faster drying out. Otherwise, the electrolyte has to deliver the oxygen for self-healing processes, and water is the best chemical substance to do that.\n\nAttempt at a pictorial representation of the formation of aluminum hydroxidein a pore of a roughened electrolytic capacitor anode foil\n\nIt is known that the \"normal\" course of building a stable aluminum oxide layer by the transformation of aluminum, through the intermediate step of aluminum hydroxide, can be interrupted by an excessively alkaline or basic electrolyte. For example, alkaline disruption of the chemistry of this reaction results instead in the following reaction:\n\nIn this case, it may happen that the hydroxide formed in the first step becomes mechanically detached from the metallic aluminum surface and will not be transformed into the desired stable form of aluminum oxide. The initial self-healing process for building a new oxide layer is prevented by a defect or a weak dielectric point, and generated hydrogen gas escapes into the capacitor. Then, at the weak point, further formation of aluminum hydroxide is started, and prevented from converting into stable aluminum oxide. The self-healing of the oxide layer inside the electrolytic capacitor can't take place. However, reactions do not come to a standstill, as more and more hydroxide grows in the pores of the anode foil, and the first reaction step produces more and more hydrogen gas in the can, increasing the pressure.\n\nThe Japanese manufacturer Rubycon became a leader in the development of new water-based electrolyte systems with enhanced conductivity in the late 1990s. After several years of development, researchers led by Shigeru Uzawa had found a mixture of inhibitors that suppressed the aluminum hydration. In 1998, Rubycon announced two series, ZL and ZA, of the first production capacitors using an electrolyte with a water content of about 40%, which were suitable for temperatures ranging from to . Later, electrolytes were developed to work with water of up to 70% by weight. Other manufacturers, such as NCC, Nichicon, and Elna followed with their own products a short time later.\n\nThe improved conductivity of the new electrolyte can be seen by comparing two capacitors, both of which have a nominal capacitance of 1000 µF at 16 V rated voltage, in a package with a diameter of 10 mm and a height of 20 mm. The capacitors of the Rubycon YXG series are provided with an electrolyte based on an organic solvent and can attain an impedance of 46 mΩ when loaded with a ripple current of 1400 mA. ZL series capacitors with the new water-based electrolyte can attain an impedance of 23 mΩ with a ripple current of 1820 mA, an overall improvement of 30%.\n\nThe new type of capacitor was called \"Low-ESR\" or \"Low-Impedance\", \"Ultra-Low-Impedance\" or \"High-Ripple Current\" series in the data sheets. The highly competitive market in digital data technology and high-efficiency power supplies rapidly adopted these new components because of their improved performance. Furthermore, by improving the conductivity of the electrolyte, capacitors not only can withstand a higher ripple current rating, they are much cheaper to produce since water is much cheaper than other solvents. Better performance and low cost drove widespread adoption of the new capacitors for high volume products such as PCs, LCD screens, and power supplies.\n\nIndustrial espionage was implicated in the capacitor plague, in connection with the theft of an electrolyte formula. A materials scientist working for Rubycon in Japan left the company, taking the secret water-based electrolyte formula for Rubycon's ZA and ZL series capacitors, and began working for a Chinese company. The scientist then developed a copy of this electrolyte. Then, some staff members who defected from the Chinese company copied an incomplete version of the formula and began to market it to many of the aluminium electrolytic manufacturers in Taiwan, undercutting the prices of the Japanese manufacturers. This incomplete electrolyte lacked important proprietary ingredients which were essential to the long-term stability of the capacitors and was unstable when packaged in a finished aluminum capacitor. This faulty electrolyte allowed the unimpeded formation of hydroxide and produced hydrogen gas.\n\nThere are no known public court proceedings related to alleged theft of electrolyte formulas. However, one independent laboratory analysis of defective capacitors has shown that many of the premature failures appear to be associated with high water content and missing inhibitors in the electrolyte, as described below.\n\nUnimpeded formation of hydroxide (hydration) and associated hydrogen gas production, occurring during \"capacitor plague\" or \"bad capacitors\" incidents involving the failure of large numbers of aluminum electrolytic capacitors, has been demonstrated by two researchers at the Center for Advanced Life Cycle Engineering of the University of Maryland who analyzed the failed capacitors.\n\nThe two scientists initially determined, by ion chromatography and mass spectrometry, that there was hydrogen gas present in failed capacitors, leading to bulging of the capacitor's case or bursting of the vent. Thus it was proved that the oxidation takes place in accordance with the first step of aluminum oxide formation.\n\nBecause it has been customary in electrolytic capacitors to bind the excess hydrogen by using reducing or depolarizing compounds, such as aromatic nitrogen compounds or amines, to relieve the resulting pressure, the researchers then searched for compounds of this type. Although the analysis methods were very sensitive in detecting such pressure-relieving compounds, no traces of such agents were found within the failed capacitors.\n\nIn capacitors in which the internal pressure build-up was so great that the capacitor case was already bulging but the vent had not opened yet, the pH value of the electrolyte could be measured. The electrolyte of the faulty Taiwanese capacitors was alkaline, with a pH of between 7 and 8. Good comparable Japanese capacitors had an electrolyte that was acidic, with a pH of around 4. As it is known that aluminum can be dissolved by alkaline liquids, but not that which is mildly acidic, an energy dispersive X-ray spectroscopy (EDX or EDS) fingerprint analysis of the electrolyte of the faulty capacitors was made, which detected dissolved aluminum in the electrolyte.\n\nTo protect the metallic aluminum against the aggressiveness of the water, some phosphate compounds, known as inhibitors or passivators, can be used to produce long-term stable capacitors with high-aqueous electrolytes. Phosphate compounds are mentioned in patents regarding electrolytic capacitors with aqueous electrolytic systems. Since phosphate ions were missing and the electrolyte was also alkaline in the investigated Taiwanese electrolytes, the capacitor evidently lacked any protection against water damage, and the formation of more-stable alumina oxides was inhibited. Therefore, only aluminum hydroxide was generated.\n\nThe results of chemical analysis were confirmed by measuring electrical capacitance and leakage current in a long-term test lasting 56 days. Due to the chemical corrosion, the oxide layer of these capacitors had been weakened, so after a short time the capacitance and the leakage current increased briefly, before dropping abruptly when gas pressure opened the vent. The report of Hillman and Helmold proved that the cause of the failed capacitors was a faulty electrolyte mixture used by the Taiwanese manufacturers, which lacked the necessary chemical ingredients to ensure the correct pH of the electrolyte over time, for long-term stability of the electrolytic capacitors. Their further conclusion, that the electrolyte with its alkaline pH value had the fatal flaw of a continual buildup of hydroxide without its being converted into the stable oxide, was verified on the surface of the anode foil both photographically and with an EDX-fingerprint analysis of the chemical components.\n\n\n"}
{"id": "26466782", "url": "https://en.wikipedia.org/wiki?curid=26466782", "title": "Casselman Wind Power Project", "text": "Casselman Wind Power Project\n\nThe Casselman Wind Power Project is a wind farm in Somerset County, Pennsylvania with 23 GE 1.5 MW Wind Turbines that began commercial operation in 2007. The wind farm has a combined total nameplate capacity of 34.5 megawatts, but actually produces about 90,666 megawatt-hours of electricity annually. The wind farm was developed by PPM Energy and constructed by Iberdrola, based in Spain. Power produced at the wind farm is distributed by First Energy.\n\nEight of the project's 23 wind turbines sit atop a rehabilitated surface mine. In addition, the former mining site also hosts the wind farm's operation center, collector transformer and interconnection facility.\n\n\n"}
{"id": "53150792", "url": "https://en.wikipedia.org/wiki?curid=53150792", "title": "Chrysler minivan (AS)", "text": "Chrysler minivan (AS)\n\nThe second-generation Chrysler minivans are a series of minivans that were manufactured and marketed by Chrysler Corporation in North America and Europe from 1991 to 1995. Officially designated the AS platform by Chrysler, the second-generation minivans were again sold in passenger and cargo configurations by the Chrysler, Plymouth, and Dodge divisions. \n\nLaunched in late 1990 for the 1991 model year, the second-generation minivans were an extensive revision of the first-generation chassis and body. The model range became the first minivans offered with driver-side airbags (1991) and with optional integrated child safety seats (1992). While adding all-wheel drive as an option for the first time, the AS platform would also mark the last offering of a manual transmission. \n\nAs with its predecessor, Chrysler assembled second-generation minivans at Windsor Assembly in Windsor, Ontario, Canada, with additional production at Saint Louis (North) Assembly in Fenton, Missouri from 1990 to 1994. In 1992, to supplement exports from the United States, the Chrysler Voyager began production in Graz, Austria (in the Eurostar joint venture factory between Chrysler and Steyr-Daimler-Puch).\n\nFor the 1996 model year, the AS-generation minivans were replaced by the NS platform, marking the first complete redesign of the Chrysler minivans since their introduction. \n\nIntroduced in November 1990, the second-generation Chrysler minivans were marketed by the Dodge, Plymouth, and Chrysler divisions. The Dodge Caravan and Plymouth Voyager nameplates returned, with both short-wheelbase and long-wheelbase (Grand) body configurations. As with the first-generation minivans, base-trim examples were equipped with 5-passenger seating, with 7-passenger seating as standard in higher-trim versions (SE, LE, ES/LX, and all Town & Country vans).\n\nFor 1994, the model range underwent a mid-cycle revision, primarily intended to comply with 1998 upgrades to US federal safety standards for cars (although the entire model range was officially considered a light truck). Alongside minor exterior styling changes, the minivans were given given dual airbags, four-wheel disc brakes with ABS, and side impact beams in the front and sliding doors. In another set of revisions, Chrysler sought to make the vans quieter, improving fit and finish.\n\nThe second-generation Chrysler minivans are officially designated the Chrysler AS platform (matching the Chrysler shift to two-letter platform designations in 1990). Largely a substantial revision of the first-generation S-platform minivans, the AS chassis retained the 112.0 inch wheelbase for standard-wheelbase vans and 119.1 inches for extended-wheelbase \"Grand\" vans. While structurally unrelated to the (discontinued) Chrysler K-cars, the second-generation minivans shifted mechanical commonality towards its larger derivatives, adopting engines and transmissions from the Chrysler AA platform (Dodge Spirit/Plymouth Acclaim) and the Chrysler AC/AY platform (Chrysler New Yorker/Fifth Avenue/Imperial and Dodge Dynasty). \n\nRetaining the unibody construction of the first-generation minivans, the second-generation minivans are fitted with a MacPherson strut front suspension with coil rear springs; all-wheel drive versions have 4-wheel independent suspension with a leaf-sprung rear axle. During the development of the platform, attention was paid on refining the handling over the first-generation vans, in order to improve stability, maneuverability, and steering feel.\n\nFor 1991 to 1993, a disc front/rear drum brake configuration was utilized, with a four-wheel disc brake system introduced in 1994. All versions were fitted with anti-lock brakes. \n\nWith the lone exception of the Chrysler 2.5L Turbo I inline-4, the second-generation minivans returned the powertrain from their 1990 predecessors. The standard engine was a naturally-aspirated Chrysler 2.5L inline-4, producing 100 hp; this engine was standard on standard-wheelbase passenger vans and all cargo vans. A Mitsubishi-produced 3.0L V6 (producing 142 hp) was an option on short-wheelbase vans. Introduced in 1990, the Chrysler 3.3L V6 (producing 150 hp; increased to 162 hp in 1994) was standard on all Grand Voyagers/Grand Caravans, Town & Countrys, and vans with all-wheel drive; the 3.3L V6 was offered on short-wheelbase as a second option.\n\nFor 1994, the AS-platform minivans adopted the 3.8L V6 engine from the Chrysler Imperial. A larger-bore version of the 3.3L V6 tuned for additional torque output, while producing the same 162 hp of the 3.3L V6, produced 213 lb-ft of torque. The standard engine of the Chrysler Town & Country and all-wheel drive vans, the 3.8L engine became an option on the Grand Voyager/Grand Caravan. \n\nInitially deleted from the model line for 1991, the 5-speed manual transmission made a return for 1992 through 1994 on base-trim or cargo vans. The 2.5L engine was available with a 3-speed TorqueFlite automatic, along with the 3.0L V6. The 4-speed Ultradrive overdrive automatic transmission was fitted to both the 3.3L and 3.8L V6 engines.\nIn an extensive redesign, the second-generation minivans featured a major revision of the exterior, lowering the exterior coefficient of drag from 0.43 to 0.39. While the styling changes were largely evolutionary, the only shared body panels from the first-generation vans were the front doors and the sliding door. \n\nThe front fascia saw the largest degree of change, with redesigned front fenders allowing for a lower hoodline. Introduced in 1987, composite headlamp units made their return, wrapping into the front fenders. With the lower hoodline, a smaller grille was used. As part of the model change, stylists sought to differentiate the three model ranges from each other. The Plymouth Voyager was given chrome-trimmed headlights and a sleeker version of its 1987-1990 grille; the Dodge Caravan was given headlamps without chrome trim and adopted the Dodge divisional four-segment grille for the first time. The Chrysler Town & Country shared its headlamps with the Plymouth Voyager with a model-specific waterfall-style grille. In an effort to lessen rust and improve fit and finish, front and rear bumpers were redesigned, with wraparound plastic bumpers replacing steel and rubber-ended bumpers.\n\nFor 1991, the exterior door handles were carried over from the 1984-1990 vans; for the 1992 model year, the door handles were replaced by a larger design, fitted flush with the body. \n\nIn line with the front fascia, the rear fascia underwent several major changes, with the rear liftgate redesigned. In addition to rounded edges to improve aerodynamics, the liftgate added a larger rear window and a center/third rear brake lamp; the rear window wiper was updated to feature intermittent modes. \n\nIn addition to the extensive exterior changes, the interior saw a complete redesign; as with the previous generation, the Dodge Caravan and Plymouth Voyager shared nearly identical interiors (across comparable trim levels). Coinciding with the addition of a driver-side airbag, the dashboard was reconfigured, placing many primary and secondary controls within closer reach of the driver. Dodge and Plymouth vans were offered with two different analog instrument panels, with the Chrysler Town & Country fitted with a digital instrument panel. For the first time, a glovebox was added to the dashboard (the underseat storage drawer made its return). \n\nThe 1991 redesign adopted the seating configurations offered on the 1989-1990 vans. While 5-passenger seating remained available on base-trim vans, 7-passenger seating became standard for all SE and LE-trim vans (regardless of wheelbase). To meet federal safety standards, all outboard seating positions were fitted with 3-point seatbelts. The Chrysler Town & Country (in line with other high-content minivans) shifted from a second-row bench seat to second-row bucket seats as part of the redesign. Marketed as \"Quad Command\" seats, the configuration was available on higher-trim (LE and LX/ES) Dodge and Plymouth vans. For 1992, for vans with second-row bench seats, Chrysler introduced integrated child safety seats as an optional feature. For 1994, the integrated child safety seats were modified, allowing for a recline position (closer to a standard carseat). \n\nFrom the first-generation minivans, Chrysler carried over the previous trim levels essentially unchanged (the Mini Ram Van was replaced by the Dodge Caravan C/V). Although once a distinguishing feature of the Chrysler minivans in the 1980s, simulated woodgrain trim began to fall out of favor in the 1990s. In 1992, woodgrain became a delete option from the Chrysler Town & Country and removed entirely from the Dodge Caravan for 1994. \n\nAs part of the 1994 model revision, the liftgate badging was changed from a stamped silver-on-black badge to a smaller embossed silver font; this was the first change to badging since 1984. Chrysler Town & Country versions retained their script-style badging.\n\nAs part of the introduction of the second-generation minivans, the Chrysler Town & Country became a permanent member of the minivan model range, intended as its flagship. Produced only in the long-wheelbase body configuration, the Town & Country was equipped with woodgrain trim, body-color mirrors, and alloy wheels; the Town & Country is the only version of this generation produced with a digital instrument panel. As with its Chrysler New Yorker and Imperial counterparts, the Town and Country was fitted with a crystal Pentastar hood ornament. Along with tufted leather interior trim, the Town & Country was fitted with \"Quad Command\" seating, replacing the second row bench seat with bucket seats matching those of the front row.\n\nAlthough woodgrain trim had been associated with the Town & Country nameplate since the 1940s, in 1992, the feature became a delete option (replaced by a gold pinstripe and a Town & Country fender script) as it began to fall out of favor with buyers. In 1993 for MY1994, Chrysler introduced a version of the Town & Country with a body-color grille alongside the woodgrain side trim.\nOutside North America, as Chrysler does not own the rights to the Dodge or Plymouth brands, exports of the Chrysler minivans were done under the Chrysler nameplate. In Europe, Chrysler began sales of the Voyager in 1988. As with its first generation counterpart, the second-generation Chrysler Voyager is largely related to the Dodge Caravan in trim. Initially produced by St. Louis Assembly, in 1992, Chrysler began sourcing the Voyager from Eurostar in Austria. In 1994, the Chrysler Voyager began production with a 2.5L VM Motori diesel engine; as of 2017, these are the only diesel-powered Chrysler minivans.\nAs with the first generation minivans, the Dodge Caravan and Plymouth Voyager were trimmed nearly identically, with the exception of cargo vans marketed exclusively by the Dodge division (as the Dodge Caravan C/V and Grand Caravan C/V. With the exception of the steering wheel, second-generation Dodge and Plymouth minivans are nearly identical internally across comparable trim levels; Dodge Caravans are fitted with a four-spoke steering wheel while Plymouth and Chrysler minivans use a two-spoke steering wheel.\n\nAlong with the base-trim 5-passenger vans, both divisions continued the mid-grade SE and deluxe-trim LE, with the LX serving as the top trim for the short-wheelbase Voyager and the ES serving as the top.\n\nIn a change from the first-generation vans, all SE and LE vans were offered with 7-passenger seating, regardless of wheelbase. LE-trim vans included many features available on SE-trim vans as option as standard, along with additional sound insulation; long-wheelbase (\"Grand\") vans and all-wheel drive models were equipped with 15-inch wheels.\n\nThe top trim in the Caravan and the Voyager was the ES (for both versions of the Caravan) and the LX (for the short-wheelbase Voyager). Sharing its model features with LE-trim models included as standard equipment, the LX/ES trims served as a cosmetic upgrade similar to namesake trims in the Plymouth Acclaim and Dodge Spirit. In place of woodgrain trim, the LX was given color-keyed two-tone exterior; the ES was given nearly monochrome trim (as with the Dodge Spirit, ES-trim Caravans were fitted with white-painted wheels). In line with the European Chrysler Voyager, the Dodge Caravan ES was not fitted with a stand-up hood ornament (with a body-color Chrysler Pentastar affixed directly onto the hood).\n\nFollowing the production of CNG-fuel (compressed natural gas) van prototypes derived from the Dodge Ram Van in 1992, Chrysler began production of a CNG-fuel version of its minivan in 1994. Using the 3.3L V6 and 4-speed Ultradrive transmission powertrain, the CNG minivans underwent several modifications to accommodate the change in fuel. Along with modifications to the engine valvetrain, four 3000PSI CNG fuel tanks were added to the vehicle, taking the place of the spare tire well and gasoline tank, holding the energy equivalent of 8.5 gallons of gasoline.\n\nThe Chrysler TEVan is a battery electric vehicle developed between Chrysler and the Electric Power Research Institute. First unveiled as a concept in 1992, an unknown number were produced between 1993 and 1995 (between 56 and 80). Deriving its name from the original T-115 codename for the Chrysler minivans and EV, the TEVan was sold nearly exclusively to fleet buyers; the vehicle was based on a standard-wheelbase five-passenger Dodge Caravan.\n\nThe TEVan used a 27 hp, 65 hp max (48 kW) Separately-Excited GE DC traction motor coupled to a two-speed FWD trans-axle that featured Hi, Lo, Reverse and Park. The owner's manual referred to it as a 'semi-automatic transmission' although it used a clutch. The motor controller was also manufactured by GE.\n\nTwo different battery types were available for the TEVan during its production; weighing in at , the battery pack brought the curb weight the minivan to . The 180 V nickel-cadmium pack consisted of 30 SAFT STM5-180 6 V 180 Ah batteries in six removable pods under the floor of the car, delivering over of range, and used an automatic watering system for easy battery maintenance. The nickel-iron pack consisted of 30 Eagle-Pitcher 6 V 200 Ah batteries in six pods under the floor and delivered over of range. The TEVan owner's manual stated of range. The TEVan's on-board charger was a PFC Martin-Marietta and accepted 120 V AC@20 A or 40 A, 240 V AC@20 A or 40 A, and as high as 220 V AC@40 A three-phase inputs.\n\nThe TEVan had an 8.8 kW three-stage ceramic electric heater. The 120 A DC/DC converter provided all the 12 V power, there was no auxiliary (12 V) battery. Gauges included motor temperature and SOC (state of charge, akin to \"Fuel Level\") using the stock instruments. It was also equipped with electric air conditioning (R-134a), regenerative braking, power brakes using a Delco electric vacuum pump, power steering, AM/FM Stereo, and airbags. The original equipment tires were LRR, (Low Rolling Resistance), Goodyear P205/75R15 Momentum at 50 PSI.\n\nAs a successor, Chrysler would produce the 1997-1999 EPIC (Electric Powered Interurban Commuter Vehicle), based upon the third-generation minivans.\n\n"}
{"id": "40562160", "url": "https://en.wikipedia.org/wiki?curid=40562160", "title": "Convention on Assistance in the Case of a Nuclear Accident or Radiological Emergency", "text": "Convention on Assistance in the Case of a Nuclear Accident or Radiological Emergency\n\nThe Convention on Assistance in the Case of a Nuclear Accident or Radiological Emergency is a 1986 treaty of the International Atomic Energy Agency (IAEA) whereby states have agreed to provide notification to the IAEA of any assistance that they can provide in the case of a nuclear accident that occurs in another state that has ratified the treaty. Along with the Convention on Early Notification of a Nuclear Accident, it was adopted in direct response to the April 1986 Chernobyl disaster.\n\nThe Convention was concluded and signed at a special session of the IAEA general conference on 26 September 1986; the special session was called because of the Chernobyl disaster, which had occurred five months before. Significantly, the Soviet Union and the Ukrainian SSR—the states that were responsible for the Chernobyl disaster—both signed the treaty at the conference and quickly ratified it. It was signed by 68 states and the Convention entered into force on 26 February 1987 after the third ratification.\n\nAs of 2016, there are 112 states that have ratified or acceded to the Convention, plus the European Atomic Energy Community, the Food and Agriculture Organization, the World Health Organization, and the World Meteorological Organization. The states that have signed the Convention but not ratified it are Afghanistan, Côte d'Ivoire, Democratic Republic of the Congo, Holy See, Niger, North Korea, Sierra Leone, Sudan, Syria, and Zimbabwe. The states that have ratified the Convention but have since denounced it and withdrawn from the agreement are Bulgaria, Hungary, Mongolia, and Poland.\n\n"}
{"id": "24577221", "url": "https://en.wikipedia.org/wiki?curid=24577221", "title": "De Historia piscium", "text": "De Historia piscium\n\nDe Historia Piscium, Latin for \"The History of Fish,\" was a scientific book written by Francis Willughby and published by the Royal Society in 1686. It was unpopular and sold poorly, causing severe strain on the finances of the society. This resulted in the society being unable to meet its promise to finance the publication of the very wealthy Newton's \"Philosophiae Naturalis Principia Mathematica\" (\"Mathematical Principles of Natural Philosophy\", better known simply as \"Principia\"), leaving this to Edmond Halley, who was then the clerk of the society. After Halley had personally financed the publication of \"Principia\", he was informed that the society could no longer afford to provide him the promised annual salary of £50. Instead, the similarly wealthy Halley was paid with left-over copies of \"De Historia Piscium\".\n"}
{"id": "1679935", "url": "https://en.wikipedia.org/wiki?curid=1679935", "title": "Electric power quality", "text": "Electric power quality\n\nElectric power quality, or simply power quality, involves voltage, frequency, and waveform. Good power quality can be defined as a steady supply voltage that stays within the prescribed range, steady a.c. frequency close to the rated value, and smooth voltage curve waveform (resembles a sine wave). In general, it is useful to consider power quality as the compatibility between what comes out of an electric outlet and the load that is plugged into it. The term is used to describe electric power that drives an electrical load and the load's ability to function properly. Without the proper power, an electrical device (or load) may malfunction, fail prematurely or not operate at all. There are many ways in which electric power can be of poor quality and many more causes of such poor quality power.\n\nThe electric power industry comprises electricity generation (AC power), electric power transmission and ultimately electric power distribution to an electricity meter located at the premises of the end user of the electric power. The electricity then moves through the wiring system of the end user until it reaches the load. The complexity of the system to move electric energy from the point of production to the point of consumption combined with variations in weather, generation, demand and other factors provide many opportunities for the quality of supply to be compromised.\n\nWhile \"power quality\" is a convenient term for many, it is the quality of the voltage—rather than power or electric current—that is actually described by the term. Power is simply the flow of energy and the current demanded by a load is largely uncontrollable.\n\nThe quality of electrical power may be described as a set of values of parameters, such as:\n\n\nIt is often useful to think of power quality as a compatibility problem: is the equipment connected to the grid compatible with the events on the grid, and is the power delivered by the grid, including the events, compatible with the equipment that is connected? Compatibility problems always have at least two solutions: in this case, either clean up the power, or make the equipment tougher.\n\nThe tolerance of data-processing equipment to voltage variations is often characterized by the CBEMA curve, which give the duration and magnitude of voltage variations that can be tolerated.\nIdeally, AC voltage is supplied by a utility as sinusoidal having an amplitude and frequency given by national standards (in the case of mains) or system specifications (in the case of a power feed not directly attached to the mains) with an impedance of zero ohms at all frequencies.\n\nNo real-life power source is ideal and generally can deviate in at least the following ways:\n\n\n\n\nEach of these power quality problems has a different cause. Some problems are a result of the shared infrastructure. For example, a fault on the network may cause a dip that will affect some customers; the higher the level of the fault, the greater the number affected. A problem on one customer’s site may cause a transient that affects all other customers on the same subsystem. Problems, such as harmonics, arise within the customer’s own installation and may propagate onto the network and affect other customers. Harmonic problems can be dealt with by a combination of good design practice and well proven reduction equipment.\n\nPower conditioning is modifying the power to improve its quality.\n\nAn uninterruptible power supply can be used to switch off of mains power if there is a transient (temporary) condition on the line. However, cheaper UPS units create poor-quality power themselves, akin to imposing a higher-frequency and lower-amplitude square wave atop the sine wave. High-quality UPS units utilize a double conversion topology which breaks down incoming AC power into DC, charges the batteries, then remanufactures an AC sine wave. This remanufactured sine wave is of higher quality than the original AC power feed.\n\nA Dynamic Voltage Regulator (DVR) and static synchronous series compensator or (SSSC) are utilized for series voltage sag compensation.\n\nA surge protector or simple capacitor or varistor can protect against most overvoltage conditions, while a lightning arrester protects against severe spikes.\n\nElectronic filters can remove harmonics.\n\nModern systems use sensors called phasor measurement units (PMU) distributed throughout their network to monitor power quality and in some cases respond automatically to them. Using such smart grids features of rapid sensing and automated self healing of anomalies in the network promises to bring higher quality power and less downtime while simultaneously supporting power from intermittent power sources and distributed generation, which would if unchecked degrade power quality.\n\nA power quality compression algorithm is an algorithm used in the analysis of power quality. To provide high quality electric power service, it is essential to monitor the quality of the electric signals also termed as power quality (PQ) at different locations along an electrical power network. Electrical utilities carefully monitor waveforms and currents at various network locations constantly, to understand what lead up to any unforeseen events such as a power outage and blackouts. This is particularly critical at sites where the environment and public safety are at risk (institutions such as hospitals, sewage treatment plants, mines, etc.).\n\nEngineers have at their disposal many meters, that are able to read and display electrical power waveforms and calculating parameters of the waveforms. These parameters may include, for example, current and voltage RMS, phase relationship between waveforms of a multi-phase signal, power factor, frequency, THD, active power (kW), reactive power (kVAr), apparent power (kVA) and active energy (kWh), reactive energy (kVArh) and apparent energy (kVAh) and many more. In order to sufficiently monitor unforeseen events, Ribeiro et al. explains that it is not enough to display these parameters, but to also capture voltage waveform data at all times. This is impracticable due to the large amount of data involved, causing what is known the “bottle effect”. For instance, at a sampling rate of 32 samples per cycle, 1,920 samples are collected per second. For three-phase meters that measure both voltage and current waveforms, the data is 6-8 times as much. More practical solutions developed in recent years store data only when an event occurs (for example, when high levels of power system harmonics are detected) or alternatively to store the RMS value of the electrical signals. This data, however, is not always sufficient to determine the exact nature of problems.\n\nNisenblat \"et al.\" proposes the idea of power quality compression algorithm (similar to lossy compression methods) that enables meters to continuously store the waveform of one or more power signals, regardless whether or not an event of interest was identified. This algorithm referred to as PQZip empowers a processor with a memory that is sufficient to store the waveform, under normal power conditions, over a long period of time, of at least a month, two months or even a year. The compression is performed in real time, as the signals are acquired; it calculates a compression decision before all the compressed data is received. For instance should one parameter remain constant, and various others fluctuate, the compression decision retains only what is relevant from the constant data, and retains all the fluctuation data. It then decomposes the waveform of the power signal of numerous components, over various periods of the waveform. It concludes the process by compressing the values of at least some of these components over different periods, separately. This real time compression algorithm, performed independent of the sampling, prevents data gaps and has a typical 1000:1 compression ratio.\n\nA typical function of a power analyzer is generation of data archive aggregated over given interval. Most typically 10 minute or 1 minute interval is used as specified by the IEC/IEEE PQ standards. A significant archive sizes are created during an operation of such instrument. As Kraus \"et al.\" have demonstrated the compression ratio on such archives using Lempel–Ziv–Markov chain algorithm, bzip or other similar lossless compression algorithms can be significant. By using prediction and modeling on the stored time series in the actual power quality archive the efficiency of post processing compression is usually further improved. This combination of simplistic techniques implies savings in both data storage and data acquisition processes.\n\n\n"}
{"id": "41396916", "url": "https://en.wikipedia.org/wiki?curid=41396916", "title": "Electricity sector in Singapore", "text": "Electricity sector in Singapore\n\nThe electricity sector in Singapore ranges from generation, transmission, distribution and retailing of electricity in Singapore.\n\nElectricity sector in Singapore is regulated by Singapore Power ().\n\nAs of 2015, Singapore uses natural gas (95%) and waste (4%) for power stations' fuel. Oil used to contribute 23% in 2005 but now is down to 1%. The fossil fuel basis of Singapore's electricity system affects the way that electric cars are taxed.\n\n\n"}
{"id": "23005420", "url": "https://en.wikipedia.org/wiki?curid=23005420", "title": "Energy Commission (Malaysia)", "text": "Energy Commission (Malaysia)\n\nThe Energy Commission (), abbreviated ST, was created under the Energy Commission Act 2001 as a new regulator for the energy industry in Peninsular Malaysia and Sabah. The Commission was established to ensure that the energy industry is developed in an efficient manner so that Malaysia is ready to meet the new challenges of globalisation and liberalisation, particularly in the energy supply industry.\n\nThe commission regulates and promotes all matters relating to the electricity and gas supply industry within the scope of applicable legislation namely Electricity Supply Act 1990, License Supply Regulation 1990, Gas Supply Act 1993, Electricity Regulation 1994, and Gas Supply Regulation 1997. In performing its role the commission takes the self-regulation approach.\n\nPrior to privatisation in 1990, the responsibility for planning and operation of the electricity supply industry in Peninsular Malaysia and Sabah vested in the National Electricity Board and the Sabah Electricity Board respectively while the Electrical Inspectorate Department, under the Ministry of Energy was responsible for licensing of private generation and the safety of electrical installations and equipment. Whereas in Sarawak, the Sarawak Electricity Supply Corporation (SESCO) was the supply authority while the State Inspectorate was responsible for licensing and safety matters in the state.\n\nIn 1990, the Electrical Inspectorate Department was abolished and the Department of Electricity Supply formed under the Electricity Supply Act 1990 as the industry and safety regulator of the electricity supply industry in Peninsular and Sabah. However, in Sarawak, the State Electricity Ordinance is still in force providing the State Electrical Inspectorate with the legal power to continue with its regulatory functions.\n\nIn 1993, the Department of Gas Supply under the Prime Minister’s Department was formed for regulating the gas distribution industry. The Director General of Electricity Supply was also appointed as the Director General of Gas Supply. Administratively, the two departments are jointly known as the Department of Electricity and Gas Supply.\n\nWith the anticipation of industry deregulation, the Energy Commission Act 2001 was approved by the parliament to take over the functions of the Department of Electricity and Gas Supply. The Energy Commission was established under this act on 1 May 2001 and became fully operational on 2 January 2002.\n\nThe Energy Commission is the regulator for the electricity and gas supply industry based on the powers provided by the Energy Commission Act 2001 and other related acts. The following are the roles and responsibilities identified by the Energy Commission:\n\n\nSeveral energy efficiency projects have been started by the commission, mostly related to efficient electricity generation and use: \n\n\n"}
{"id": "34123042", "url": "https://en.wikipedia.org/wiki?curid=34123042", "title": "Energy in North Korea", "text": "Energy in North Korea\n\nEnergy in North Korea describes energy and electricity production, consumption and import in North Korea.\n\nNorth Korea is a net energy exporter. Primary energy use in North Korea was 224 TWh and 9 TWh per million people in 2009.\n\nNorth Korea energy production in relation to population was about same as is in South Korea in 2004–2009. The difference is in the energy import. North Korea is almost self-sufficient in energy. The energy import is very small in North Korea and 86% of primary energy use in South Korea. North Korea's population in 2009 was 23,91 million, and 48,75 million in South Korea.\n\nAccording to statistics compiled by the South Korean agency, Statistics Korea, based on International Energy Agency (IEA) data, per capita electricity consumption fell from its peak in 1990 of 1247 kilowatt hours to a low of 712 kilowatt hours in 2000. It has slowly risen since to 819 kilowatt hours in 2008, a level below that of 1970.\n\nIn 2017 many homes were using small standalone photovoltaic systems.\n\nNorth Korea imports crude oil from an aging pipeline that originates in Dandong, China. The crude oil is refined at the Ponghwa Chemical Factory in Sinuiju, North Korea. North Korea has a smaller oil refinery, the Sŭngri Refinery, on its Russian border. \n\nNorth Korea imports jet fuel, diesel fuel, and gasoline from two refineries in Dalian, China, which arrive at the North Korean port of Nampo.\n\n\n"}
{"id": "1880117", "url": "https://en.wikipedia.org/wiki?curid=1880117", "title": "Expeller pressing", "text": "Expeller pressing\n\nExpeller pressing (also called oil pressing) is a mechanical method for extracting oil from raw materials. The raw materials are squeezed under high pressure in a single step. When used for the extraction of food oils, typical raw materials are nuts, seeds and algae, which are supplied to the press in a continuous feed. As the raw material is pressed, friction causes it to heat up; in the case of harder nuts (which require higher pressures) the material can exceed temperatures of .\n\nAn expeller press is a screw-type machine that mainly presses oil seeds through a caged barrel-like cavity. Other materials used with an expeller press include but are not limited to meat by-products, synthetic rubber and animal feeds. Raw materials enter one side of the press and waste products exit the other side. The machine uses friction and continuous pressure from the screw drives to move and compress the seed material. The oil seeps through small openings that do not allow seed fiber solids to pass through. Afterward, the pressed seeds are formed into a hardened cake, which is removed from the machine. Pressure involved in expeller pressing creates heat in the range of . Some companies claim that they use a cooling apparatus to reduce this temperature to protect certain properties of the oils being extracted.\n\nExpeller processing cannot remove every last trace of liquid (usually oil) from the raw material. A significant amount remains trapped inside of the cake left over after pressing. In most small-scale rural situations this is of little or no importance, as the cake that remains after the oil has been removed finds uses in local dishes, in the manufacture of secondary products, or for animal feed. Some raw materials, however, do not release oil by simple expelling, the most notable being rice bran. In order to remove oil from commodities that do not respond to expelling or to extract the final traces of oil after expelling, it is necessary to use solvent extraction.\n\nThe earliest expeller presses utilized a continuous screw design. The compression screws were much like the screws of a screw conveyor—that is, the helicoid flighting started at one end and ended at the other.\n\nValerius Anderson invented the interrupted screw design and patented it in the year 1900. Anderson observed that in the continuous flighting arrangement of a compression screw, there are tendencies for slippery materials either to co-rotate with the screw or to pass through with minimal dewatering. He wrote that \"brewers' slops, slaughterhouse refuse\" and other \"soft and mushy\" materials dewater poorly in continuous screw presses.\n\nHis invention consisted of putting interruptions in the flighting of a compression screw. It was much like having a hanger bearing in a screw conveyor: there is no flighting on the shaft at that point, so material tends to stop moving and pile up. It is only after solids accumulate in the gap that the downstream flighting catches material. When this happens, material is forced along its way. The result is better dewatering and thus a more consistent press cake.\n\nAfter the 1900 patent, a major improvement was made with the addition of resistor teeth. Fitted into the gaps where there is no flighting, these teeth increase the agitation within the press, further diminishing co-rotation tendencies.\n\nAs the years went by, applications of the interrupted screw design were expanded beyond slippery and slimy materials. This took place because competing continuous screw presses worked best only under conditions of constant feed, at constant consistency. If either the consistency or the flow rate diminished, squeezing would diminish until it was inadequate for proper moisture removal. At the same time, if the consistency increased, the press could jam. To counteract these tendencies it was necessary to build a very heavy press, frequently with an expensive variable speed drive.\n\nIn contrast, it was found that the interruptions in the flighting of the Anderson screw would provide cushion within the press. If consistency went down, compression was still effective. A plug of sufficiently solid material had to build up at each interruption before solids could progress towards the discharge. This self-correcting performance prevents wet material from purging at the cake discharge. It is achieved without varying the speed of the screw.\n\nThe economic advantages of these characteristics led to interrupted screw presses being used to dewater fibrous materials that are neither slippery nor slimy. Examples would be alfalfa, corn husk, and, more recently, paper mill fibers.\n\n"}
{"id": "2023440", "url": "https://en.wikipedia.org/wiki?curid=2023440", "title": "FR-2", "text": "FR-2\n\nFR-2 (Flame Resistant 2) is a NEMA designation for synthetic resin bonded paper, a composite material made of paper impregnated with a plasticized phenol formaldehyde resin, used in the manufacture of printed circuit boards. Its main properties are similar to NEMA grade XXXP (MIL-P-3115) material, and can be substituted for the latter in many applications.\n\nFR-2 sheet with copper foil lamination on one or both sides is widely used to build low-end consumer electronic equipment. While its electrical and mechanical properties are inferior to those of epoxy-bonded fiberglass, FR-4, it is significantly cheaper. It is not suitable for devices installed in vehicles, as continuous vibration can make cracks propagate, causing hairline fractures in copper circuit traces. Without copper foil lamination, FR-2 is sometimes used for simple structural shapes and electrical insulation.\n\nFR-2 can be machined by drilling, sawing, milling and hot punching. Cold punching and shearing are not recommended, as they leave a ragged edge and tend to cause cracking. Tools made of high-speed steel can be used, although tungsten carbide tooling is preferred for high volume production.\n\nAdequate ventilation or respiration protection are mandatory during high-speed machining, as it gives off toxic vapors.\n\n\n"}
{"id": "19609885", "url": "https://en.wikipedia.org/wiki?curid=19609885", "title": "Friedel's salt", "text": "Friedel's salt\n\nFriedel's salt is an anion exchanger mineral belonging to the family of the layered double hydroxides (LDHs). It has affinity for anions as chloride and iodide and is capable to retain them to a certain extent in its crystallographical structure.\n\nFriedel's salt general formula is:\n\nIn the cement chemist notation, considering that\n\nand doubling all the stoichiometry, it can also be written as follows:\n\nFriedel's salt can also be considered as an AF phase in which chloride ions have replaced sulfate ions and is formed in cements initially rich in tri-calcium aluminate (CA).\n\nIt plays a main role in the retention of chloride anions in cement and concrete. However, Friedel's salt remains a poorly understood phase in the CaO-AlO-CaCl-HO system, and is critical for the stability of salt-saturated Portland cement-based grouts.\n\nFriedel's salt discovery is relatively difficult to trace back from the recent literature, simply because it is an ancient finding of a poorly known and non-natural product. It has been synthesised and identified in 1897 by Georges Friedel, mineralogist and crystallographer, son of the famous French chemist Charles Friedel. Georges Friedel also synthesised Calcium aluminate (1903) in the framework of his work on the Macles theory (twin crystals). This point requires further verification.\n\n\n\n\n\n\n\n"}
{"id": "47723295", "url": "https://en.wikipedia.org/wiki?curid=47723295", "title": "Garbage", "text": "Garbage\n\nGarbage, trash, rubbish, or refuse is waste material that is discarded by humans, usually due to a perceived lack of utility. The term generally does not encompass bodily waste products, purely liquid or gaseous wastes, nor toxic waste products. Garbage is commonly sorted and classified into kinds of material suitable for specific kinds of disposal.\n\nWhat constitutes garbage is highly subjective, with some individuals or societies tending to discard things that others find useful or restorable. The words garbage, refuse, rubbish, trash, and waste are generally treated as interchangeable when used to describe \"substances or objects which the holder discards or intends or is required to discard\". Some of these terms have historic distinctions that are no longer present. In the 1880s, material to be disposed of was divided into four general categories: ashes (derived from the burning of coal or wood), garbage, rubbish, and street-sweepings. This scheme of categorization reduced the terms to more specific concepts:\n\nThe distinction between terms used to describe wet and dry discarded material \"was important in the days when cities slopped garbage to pigs, and needed to have the wet material separated from the dry\", but has since dissipated.\n\nIn urban areas, garbage of all kinds is collected and treated as municipal solid waste; garbage that is discarded in ways that cause it to end up in the environment, rather than in facilities designed to receive garbage, is considered litter. Litter is a form of garbage, and municipal solid waste that is improperly disposed of, and which therefore enters the environment, is treated as litter. Notably, however, only a small fraction of garbage that is generated becomes litter, with the vast majority being disposed of in ways intended to secure it from entering the environment.\n\nMan has been creating garbage throughout history, beginning with bone fragments left over from using animal parts and stone fragments discarded from toolmaking. The degree to which groups of early humans began engaging in agriculture can be estimated by examining the type and quality of animal bones in their garbage. Garbage from prehistoric or pre-civilization humans was often collected into mounds called middens, which might contain things such as \"a mix of discarded food, charcoal, shell tools, and broken pottery\".\n\n"}
{"id": "238560", "url": "https://en.wikipedia.org/wiki?curid=238560", "title": "Ilya Prigogine", "text": "Ilya Prigogine\n\nViscount Ilya Romanovich Prigogine (; ; 28 May 2003) was a physical chemist and Nobel laureate noted for his work on dissipative structures, complex systems, and irreversibility.\n\nPrigogine was born in Moscow a few months before the Russian Revolution of 1917, into a Jewish family. His father, Roman (Ruvim Abramovich) Prigogine, was a chemical engineer at the Imperial Moscow Technical School; his mother, Yulia Vikhman, was a pianist. Because the family was critical of the new Soviet system, they left Russia in 1921. They first went to Germany and in 1929, to Belgium, where Prigogine received Belgian nationality in 1949. His brother Alexandre (1913–1991) became an ornithologist.\n\nPrigogine studied chemistry at the Université Libre de Bruxelles, where in 1950, he became professor. In 1959, he was appointed director of the International Solvay Institute in Brussels, Belgium. In that year, he also started teaching at the University of Texas at Austin in the United States, where he later was appointed Regental Professor and Ashbel Smith Professor of Physics and Chemical Engineering. From 1961 until 1966 he was affiliated with the Enrico Fermi Institute at the University of Chicago. In Austin, in 1967, he co-founded the Center for Thermodynamics and Statistical Mechanics, now the Center for Complex Quantum Systems. In that year, he also returned to Belgium, where he became director of the Center for Statistical Mechanics and Thermodynamics.\n\nHe was a member of numerous scientific organizations, and received numerous awards, prizes and 53 honorary degrees. In 1955, Ilya Prigogine was awarded the Francqui Prize for Exact Sciences. For his study in irreversible thermodynamics, he received the Rumford Medal in 1976, and in 1977, the Nobel Prize in Chemistry. In 1989, he was awarded the title of Viscount in the Belgian nobility by the King of the Belgians. Until his death, he was president of the International Academy of Science, Munich and was in 1997, one of the founders of the International Commission on Distance Education (CODE), a worldwide accreditation agency. In 1998 he was awarded an \"honoris causa\" doctorate by the UNAM in Mexico City.\n\nPrigogine was first married to Belgian poet Hélène Jofé (as an author also known as Hélène Prigogine) and in 1945 they had a son Yves. After their divorce, he married Polish-born chemist Maria Prokopowicz (also known as Maria Prigogine) in 1961. In 1970 they had a son Pascal. In 2003 he was one of 22 Nobel Laureates who signed the Humanist Manifesto.\n\nPrigogine received an Honorary Doctorate from Heriot-Watt University in 1985 \n\nPrigogine is best known for his definition of dissipative structures and their role in thermodynamic systems far from equilibrium, a discovery that won him the Nobel Prize in Chemistry in 1977. In summary, Ilya Prigogine discovered that importation and dissipation of energy into chemical systems could result in the emergence of new structures (hence dissipative structures) due to internal self reorganization. In his 1955 text, Prigogine drew connections between dissipative structures and the Rayleigh-Benard instability and the Turing mechanism.\n\nDissipative structure theory led to pioneering research in self-organizing systems, as well as philosophical inquiries into the formation of complexity on biological entities and the quest for a creative and irreversible role of time in the natural sciences. See the criticism by Joel Keizer and Ronald Fox.\n\nWith professor Robert Herman, he also developed the basis of the two fluid model, a traffic model in traffic engineering for urban networks, analogous to the two fluid model in classical statistical mechanics.\n\nPrigogine's formal concept of self-organization was used also as a \"complementary bridge\" between General Systems Theory and thermodynamics, conciliating the cloudiness of some important systems theory concepts with scientific rigour.\n\nIn his later years, his work concentrated on the fundamental role of indeterminism in nonlinear systems on both the classical and quantum level. Prigogine and coworkers proposed a Liouville space extension of quantum mechanics. A Liouville space is the vector space formed by the set of (self-adjoint) linear operators, equipped with an inner product, that act on a Hilbert space. There exists a mapping of each linear operator into Liouville space, yet not every self-adjoint operator of Liouville space has a counterpart in Hilbert space, and in this sense Liouville space has a richer structure than Hilbert space. The Liouville space extension proposal by Prigogine and co-workers aimed to solve the arrow of time problem of thermodynamics and the measurement problem of quantum mechanics.\n\nPrigogine co-authored several books with Isabelle Stengers, including \"The End of Certainty\" and \"La Nouvelle Alliance\" (\"Order out of Chaos\").\n\nIn his 1996 book, \"La Fin des certitudes\", co-authored by Isabelle Stengers and published in English in 1997 as \"The End of Certainty: Time, Chaos, and the New Laws of Nature\", Prigogine contends that determinism is no longer a viable scientific belief: \"The more we know about our universe, the more difficult it becomes to believe in determinism.\" This is a major departure from the approach of Newton, Einstein and Schrödinger, all of whom expressed their theories in terms of deterministic equations. According to Prigogine, determinism loses its explanatory power in the face of irreversibility and instability.\n\nPrigogine traces the dispute over determinism back to Darwin, whose attempt to explain individual variability according to evolving populations inspired Ludwig Boltzmann to explain the behavior of gases in terms of populations of particles rather than individual particles. This led to the field of statistical mechanics and the realization that gases undergo irreversible processes. In deterministic physics, all processes are time-reversible, meaning that they can proceed backward as well as forward through time. As Prigogine explains, determinism is fundamentally a denial of the arrow of time. With no arrow of time, there is no longer a privileged moment known as the \"present,\" which follows a determined \"past\" and precedes an undetermined \"future.\" All of time is simply given, with the future as determined or as undetermined as the past. With irreversibility, the arrow of time is reintroduced to physics. Prigogine notes numerous examples of irreversibility, including diffusion, radioactive decay, solar radiation, weather and the emergence and evolution of life. Like weather systems, organisms are unstable systems existing far from thermodynamic equilibrium. Instability resists standard deterministic explanation. Instead, due to sensitivity to initial conditions, unstable systems can only be explained statistically, that is, in terms of probability.\n\nPrigogine asserts that Newtonian physics has now been \"extended\" three times: first with the introduction of spacetime in general relativity, then with the use of the wave function in quantum mechanics, and finally with the recognition of indeterminism in the study of unstable systems (chaos theory).\n\n\n\n\n"}
{"id": "36694087", "url": "https://en.wikipedia.org/wiki?curid=36694087", "title": "InstaLoad", "text": "InstaLoad\n\nInstaLoad is a patented technology developed by Microsoft which allows cylindrical batteries to function in a battery holder regardless of the batteries polarity.\n\nThe InstaLoad technology can be licensed from the Microsoft hardware Intellectual Property Licensing program. Microsoft provides royalty-free licensing for manufacturers of accessibility devices. InstaLoad is purely mechanical.\n\nThe device is designed to save time when swapping out batteries and reduce confusion from hard to read battery diagrams.\n\nInstaLoad was designed to address the large number of phone calls to the Microsoft customer help desk in which consumers were improperly placing batteries in Microsoft wireless devices (such as the keyboard and mouse).\n\n"}
{"id": "20956847", "url": "https://en.wikipedia.org/wiki?curid=20956847", "title": "Interamerican Association for Environmental Defense", "text": "Interamerican Association for Environmental Defense\n\nThe Interamerican Association for Environmental Defense (Spanish: \"Asociacion Interamericana para la Defensa del Ambiente\") (AIDA) is a non-profit international environmental law organization founded in 1996 by a collaboration of five environmental organizations in the Americas including Earthjustice.\n\nAIDA's headquarters is in San Francisco, California. The organization works internationally with partners in many different countries including Argentina, Canada, Chile, Colombia, Costa Rica, Ecuador, Mexico, and Peru.\n\nAIDA works primarily to improve and protect human health and the environment. AIDA's most notable work has been in La Oroya, Peru where they have fought the poisoning of local people by heavy metals and other contaminants emitted by a local smelter. AIDA has also made significant impacts protecting the Leatherback turtle in Costa Rica through a partnership with Cedarena.\n\nAIDA conducts its efforts according to four basic principles:\n\n1. Encourage Transnational Collaboration - In many cases, environmental crises can’t be boxed into individual nations. Pesticide spraying in Colombia threatens forests in Ecuador; polluted waters from Bolivia damage fragile wetlands in Brazil; overfishing by boats registered in Panama causes global disruptions in marine ecosystems; and consumer excess in the United States strains environmental resources throughout the hemisphere. AIDA doesn’t let political boundaries dictate the scope of our efforts. We do what it takes to win, no matter how many borders we have to cross along the way.\n\n2. Protect human rights - Environmental health and human rights are two sides of the same coin. Without the services provided by functioning ecosystems – clean water, breathable air, and productive soil – human communities cannot thrive. When human rights are violated, democracy fails. When significant disparities in economic capacity and political influence are involved, AIDA protects poor communities struggling against powerful corporate or state interests.\n\n3. Cultivate the power of international law - Many international treaties make lofty promises that lead to little action. Commitments on paper are meaningless without real-world incentives and mechanisms for enforcement. AIDA designs international strategies that lead to measurable results – we hold governments accountable and build the capacity of key players in positions to make a difference.\n\n4. Encourage citizen enforcement and public participation - Lasting change comes from the ground up. AIDA works to empower the communities and organizations that we represent. Sometimes, governments cannot be relied upon to protect basic environmental and human rights. When the authorities don’t deliver, AIDA helps nonprofit organizations enforce the law.\n\nAIDA partners with local groups to field multinational teams of lawyers and scientists to tackle a range of environmental and human rights crises – including the decline of freshwater resources, the proliferation of toxins, climate change, and the decimation of vulnerable biodiversity.\n\nAIDA’s efforts are divided into five strategic themes:\n\n1. Human Rights and the Environment- AIDA works to establish the connection between environmental degradation and harm to under-resourced communities throughout the Americas. When big business moves in to extract natural resources or develop infrastructure, local people are often left with contaminated water, polluted air, and no way to feed their families.\n\n2. Marine Biodiversity and Coastal Protection- Marine ecosystems are among the most threatened environmental resources in the Americas. AIDA works to implement legal, administrative, and political strategies to help protect endangered species, encourage the sustainable harvesting of delicate marine resources, and protect coastal areas that provide essential habitat to threatened biodiversity and human communities.\n\n3. Climate Change - Global warming is the most systemic and long-range threat to environmental health. AIDA has recently expanded its efforts to include work on climate change, with a focus on developing legal tools and regulatory frameworks that will help move human societies toward energy sustainability and protect those most harmed by rising sea levels and changing weather patterns.\n\n4. Freshwater Preservation- Clean water is a cornerstone of human and environmental health. AIDA implements legal strategies to protect ecosystems that serve as vital freshwater resources for nearby communities and biodiversity. AIDA also works to prevent companies from polluting freshwater supplies with poisonous toxins.\n\n5. Strengthening Environmental Governance and Public Participation - AIDA works to build capacity in Latin and Central America by educating decision makers, distributing information to nonprofit organizations, and building alliances among communities and lawyers working for environmental protection. We work to protect the ability of the public to participate meaningfully in important environmental decisions.\n\nAIDA periodically offers training workshops to legal advocates throughout the hemisphere. The workshops emphasize the inseparable link between human rights and the environment, reinforce participants’ understanding of the Inter-American System of Human Rights, and promote discussion on protecting human rights and the environment within this hemisphere. Workshops include presentations by experts, case studies, and participatory discussions.\n\nAIDA works on projects in collaboration with environmental and human rights groups throughout the hemisphere, including the following participating organizations:\n\n"}
{"id": "448161", "url": "https://en.wikipedia.org/wiki?curid=448161", "title": "Ivan Visin", "text": "Ivan Visin\n\nIvan \"Ivo\" Visin (1806 - 1868) was a naval captain and explorer.\n\nVisin was born in Prčanj, in the Bay of Kotor, then under occupation of the French Empire. On request of the government of the Habsburg monarchy, he circumnavigated the globe in a vessel called \"Splendido\" between 1852 and 1859. The journey started in Antwerp and ended successfully in Trieste. The ship was 30 m long with 311 tonnes of cargo. For this undertaking of historical importance for the empire, he had been decorated with a flag of honour \"Merito navali\" by the Austrian Emperor Franz Josef. The trophy is on display in Birth of Our Lady church in Prčanj. Later, Visin became an honorary citizen of Trieste.\n"}
{"id": "30106405", "url": "https://en.wikipedia.org/wiki?curid=30106405", "title": "Jupiter Stone", "text": "Jupiter Stone\n\nIn the Roman tradition, oaths were sworn upon \"Iuppiter Lapis\" or the Jupiter Stone located in the Temple of Jupiter, Capitoline Hill. \"Iuppiter Lapis\" was held in the Roman tradition to be an Oath Stone, an aspect of Jupiter in his role as divine lawmaker responsible for order and used principally for the investiture of the oathtaking of office.\n\nAccording to Cyril Bailey, in \"The Religion of Ancient Rome\" (1907): \n\n"}
{"id": "9060791", "url": "https://en.wikipedia.org/wiki?curid=9060791", "title": "Jürgen Gmehling", "text": "Jürgen Gmehling\n\nJürgen Gmehling (born January 13, 1946 in Duisburg) is a retired German professor of technical and industrial chemistry at the Carl von Ossietzky University of Oldenburg.\n\nHis career started with an apprenticeship as a laboratory assistant at the Duisburg copper works before he studied chemical engineering at the engineering school in Essen and then chemistry in Dortmund and Clausthal. He received his diploma from the University of Dortmund in 1970 and his PhD (Dr. rer. nat., inorganic chemistry) in 1973. After this he worked as a scientific coworker in Dortmund before he became a private lecturer and, after his habilitation, an assistant professor.\n\nGmehling was appointed a full professor for technical chemistry at the University of Oldenburg in 1989 and retired in 2011.\n\nGmehling's main focus is the process development. This includes the development of software for process synthesis and process simulation as well as measurement, collection, and estimation of thermophysical properties of pure components and component mixtures. The following list summarizes fields of his scientific work but is in no way complete.\n\n\nGmehling began in the 1970s with the systematic evaluation of the scientific literature, aiming to build a data bank for vapor-liquid equilibria. These data were needed for the development of a new method for the prediction of activity coefficients named UNIFAC. This data bank is still named the Dortmund Data Bank.\n\nGmehling developed with colleagues models for the estimation of several thermodynamic and thermophysical properties:\n\nThe following implementation of his methods are also developments:\n\nGmehling has received some awards:\n\n\nGmehling has published scientific articles and books.\n\n"}
{"id": "4391913", "url": "https://en.wikipedia.org/wiki?curid=4391913", "title": "Magnesium diuranate", "text": "Magnesium diuranate\n\nMagnesium diuranate (MgUO) is a compound of uranium. It is known in the uranium refining industry as \"MDU\" and forms the major part of some yellowcake mixtures. Yellowcakes are an intermediate product in the uranium refining process.\n\nTo produce this form of yellowcake, crushed ore is mixed with hot water to a 58% solids slurry. The solids slurry is then processed through a series of tanks, where sulfuric acid, sodium chlorate, and steam are used to extract the uranium from the solids slurry. The average leaching efficiency for this process is 98.5%. The uranium-bearing solution is then decanted and directed to a solvent extraction (SX) process for further purification. In this extraction step, the dissolved uranium is transferred from the feed solution into the organic solvent. Next a stripping step recovers the uranium into a sodium chloride aqueous phase after which the barren solvent is recycled. The average efficiency of the SX circuit is 99.9%. The high-grade “pregnant” strip solution from SX goes to the next stage where magnesia slurry is added to precipitate magnesium diuranate. The yellow cake precipitate is then thickened, dried, re-crushed and packed into industry standard 220 litre steel drums for shipment to customers.\n\nAs with all yellowcake process the selection of this end product is dependent on the properties of the ore to be processed. The point of using magnesia is that you need an alkali and magnesia can be cheap.\n"}
{"id": "4643400", "url": "https://en.wikipedia.org/wiki?curid=4643400", "title": "Majorana fermion", "text": "Majorana fermion\n\nA Majorana fermion (), also referred to as a Majorana particle, is a fermion that is its own antiparticle. They were hypothesized by Ettore Majorana in 1937. The term is sometimes used in opposition to a Dirac fermion, which describes fermions that are not their own antiparticles.\n\nWith the exception of the neutrino, all of the Standard Model fermions are known to behave as Dirac fermions at low energy (after electroweak symmetry breaking), and none are Majorana fermions. The nature of the neutrinos is not settled – they may be either Dirac or Majorana fermions.\n\nIn condensed matter physics, bound Majorana fermions can appear as quasiparticle excitations – the collective movement of several individual particles, not a single one, and they are governed by non-abelian statistics.\n\nThe concept goes back to Majorana's suggestion in 1937 that neutral spin- particles can be described by a real wave equation (the Majorana equation), and would therefore be identical to their antiparticle (because the wave functions of particle and antiparticle are related by complex conjugation).\n\nThe difference between Majorana fermions and Dirac fermions can be expressed mathematically in terms of the creation and annihilation operators of second quantization: The creation operator formula_1 creates a fermion in quantum state formula_2 (described by a \"real\" wave function), whereas the annihilation operator formula_3 annihilates it (or, equivalently, creates the corresponding antiparticle). For a Dirac fermion the operators formula_1 and formula_3 are distinct, whereas for a Majorana fermion they are identical. The ordinary fermionic annihilation and creation operators formula_6 and formula_7 can be written in terms of two Majorana operators formula_8 and formula_9 by\n\nIn supersymmetry models, neutralinos — superpartners of gauge bosons and Higgs bosons — are Majorana.\n\nAnother common convention for the normalization of the Majorana fermion operator is\nThis convention has the advantage that the Majorana operator squares to the identity.\n\nUsing this convention, a collection of Majorana fermions formula_14 (formula_15) obey the following commutation identities\nwhere formula_18 and formula_19 are antisymmetric matrices.\n\nBecause particles and antiparticles have opposite conserved charges, Majorana fermions have zero charge. All of the elementary fermions of the Standard Model have gauge charges, so they cannot have fundamental Majorana masses.\n\nHowever, the right-handed sterile neutrinos introduced to explain neutrino oscillation could have Majorana masses. If they do, then at low energy (after electroweak symmetry breaking), by the seesaw mechanism, the neutrino fields would naturally behave as six Majorana fields, with three of them expected to have very high masses (comparable to the GUT scale) and the other three expected to have very low masses (below 1 eV). If right-handed neutrinos exist but do not have a Majorana mass, the neutrinos would instead behave as three Dirac fermions and their antiparticles with masses coming directly from the Higgs interaction, like the other Standard Model fermions.\nThe seesaw mechanism is appealing because it would naturally explain why the observed neutrino masses are so small. However, if the neutrinos are Majorana then they violate the conservation of lepton number and even of B − L.\n\nNeutrinoless double beta decay, which has not (yet) been observed. It can be viewed as two ordinary beta decay events whose resultant antineutrinos immediately annihilate with each other, and is only possible if neutrinos are their own antiparticles.\n\nThe high-energy analog of the neutrinoless double beta decay process is the production of same-sign charged lepton pairs in hadron colliders; it is being searched for by both the ATLAS and CMS experiments at the Large Hadron Collider. In theories based on left–right symmetry, there is a deep connection between these processes. In the currently most-favored explanation of the smallness of neutrino mass, the seesaw mechanism, the neutrino is “naturally” a Majorana fermion.\n\nMajorana fermions cannot possess intrinsic electric or magnetic moments, only toroidal moments. Such minimal interaction with electromagnetic fields makes them potential candidates for cold dark matter.\n\nIn superconducting materials, Majorana fermions can emerge as (non-fundamental) quasiparticles (more commonly referred to as Bogoliubov quasiparticles in condensed matter physics). This becomes possible because a quasiparticle in a superconductor is its own antiparticle.\n\nMathematically, the superconductor imposes electron hole \"symmetry\" on the quasiparticle excitations, relating the creation operator formula_20 at energy formula_21 to the annihilation operator formula_22 at energy formula_23. Majorana fermions can be bound to a defect at zero energy, and then the combined objects are called Majorana bound states or Majorana zero modes. This name is more appropriate than Majorana fermion (although the distinction is not always made in the literature), because the statistics of these objects is no longer fermionic. Instead, the Majorana bound states are an example of non-abelian anyons: interchanging them changes the state of the system in a way that depends only on the order in which the exchange was performed. The non-abelian statistics that Majorana bound states possess allows them to be used as a building block for a topological quantum computer.\n\nA quantum vortex in certain superconductors or superfluids can trap midgap states, so this is one source of Majorana bound states. Shockley states at the end points of superconducting wires or line defects are an alternative, purely electrical, source. An altogether different source uses the fractional quantum Hall effect as a substitute for the superconductor.\n\nIn 2008, Fu and Kane provided a groundbreaking development by theoretically predicting that Majorana bound states can appear at the interface between topological insulators and superconductors. Many proposals of a similar spirit soon followed, where it was shown that Majorana bound states can appear even without any topological insulator. An intense search to provide experimental evidence of Majorana bound states in superconductors first produced some positive results in 2012. A team from the Kavli Institute of Nanoscience at Delft University of Technology in the Netherlands reported an experiment involving indium antimonide nanowires connected to a circuit with a gold contact at one end and a slice of superconductor at the other. When exposed to a moderately strong magnetic field the apparatus showed a peak electrical conductance at zero voltage that is consistent with the formation of a pair of Majorana bound states, one at either end of the region of the nanowire in contact with the superconductor.. Simultaneously, a group from Purdue University and University of Notre Dame reported observation of fractional Josephson effect (decrease of the Josephson frequency by a factor of 2) in indium antimonide nanowires connected to two superconducting contacts and subjected to a moderate magnetic field, another signature of Majorana bound states. Bound state with zero energy was soon detected by several other groups in similar hybrid devices, and fractional Josephson effect was observed in topological insulator HgTe with superconducting contacts\n\nThe aforementioned experiments marks a possible verification of independent 2010 theoretical proposals from two groups predicting the solid state manifestation of Majorana bound states in semiconducting wires. However, it was also pointed out that some other trivial non-topological bounded states could highly mimic the zero voltage conductance peak of Majorana bound state. The subtle relation between those trivial bound states and Majorana bound states was reported by the researchers in Niels Bohr Institute, who can directly \"watch\" coalescing Andreev bound states evolving into Majorana bound states, thanks to a much cleaner semiconductor-superconductor hybrid system.\n\nIn 2014, evidence of Majorana bound states was also observed using a low-temperature scanning tunneling microscope, by scientists at Princeton University. It was suggested that Majorana bound states appeared at the edges of a chain of iron atoms formed on the surface of superconducting lead. The detection was not decisive because of possible alternative explanations.\n\nMajorana fermions may also emerge as quasiparticles in quantum spin liquids, and were observed by researchers at Oak Ridge National Laboratory, working in collaboration with Max Planck Institute and University of Cambridge on 4 April 2016.\n\nChiral Majorana fermions were detected in 2017, in a quantum anomalous Hall effect/superconductor hybrid device. In this system, Majorana fermions edge mode will give a rise to a formula_24 conductance edge current.\n\nOn 16 August 2018, a strong evidence for the existence of Majorana bound states (or Majorana anyons) in an iron-based superconductor, which many alternative trivial explanations cannot account for, was reported by researchers in Prof. Gao Hong-jun's team and Prof. Ding Hong's team at Institute of Physics, Chinese Academy of Sciences and University of Chinese Academy of Sciences, when they used scanning tunneling spectroscopy on the superconducting Dirac surface state of the iron-based superconductor. It was the first time that Majorana particles were observed in a bulk of pure substance.\n\nMajorana bound states can also be realized in quantum error correcting codes. This is done by creating so called 'twist defects' in codes such as the Toric code which carry unpaired Majorana modes. The braiding of Majoranas realized in such a way forms a projective representation of the braid group.\n\nSuch a realization of Majoranas would allow them to be used to store and process quantum information within a quantum computation. Though the codes typically have no Hamiltonian to provide suppression of errors, fault-tolerance would be provided by the underlying quantum error correcting code.\n"}
{"id": "8145633", "url": "https://en.wikipedia.org/wiki?curid=8145633", "title": "Motor soft starter", "text": "Motor soft starter\n\nA motor soft starter is a device used with AC electrical motors to temporarily reduce the load and torque in the power train and electric current surge of the motor during start-up. This reduces the mechanical stress on the motor and shaft, as well as the electrodynamic stresses on the attached power cables and electrical distribution network, extending the lifespan of the system.\n\nIt can consist of mechanical or electrical devices, or a combination of both. Mechanical soft starters include clutches and several types of couplings using a fluid, magnetic forces, or steel shot to transmit torque, similar to other forms of torque limiter. Electrical soft starters can be any control system that reduces the torque by temporarily reducing the voltage or current input, or a device that temporarily alters how the motor is connected in the electric circuit.\n\nAcross-the line starting of induction motors is accompanied by inrush currents up to 7-10 times higher than running current, and starting torque up to 3 times higher than running torque. The increased torque results in sudden mechanical stress on the machine which leads to a reduced service life. Moreover, the high inrush current stresses the power supply, which may lead to voltage dips. As a result, lifespan of sensitive equipment may be reduced.\n\nA soft starter eliminates the undesired side effects. Several types based on control of the supply voltage or mechanical devices such as slip clutches were developed. The list provides an overview of the various electric start-up types. The current and torque characteristic curves show the behavior of the respective starter solution.\nTorque surges entail high mechanical stress on the machine, which results in higher service costs and increased wear. High currents and current peaks lead to high fixed costs charged by the power supply companies (peak current calculation) and to increased mains and generator loads.\n\nA soft starter continuously controls the three-phase motor’s voltage supply during the start-up phase. This way, the motor is adjusted to the machine’s load behavior. Mechanical operating equipment is accelerated smoothly. This lengthens service life, improves operating behavior, and smooths work flows. Electrical soft starters can use solid state devices to control the current flow and therefore the voltage applied to the motor. They can be connected in series with the line voltage applied to the motor, or can be connected inside the delta (Δ) loop of a delta-connected motor, controlling the voltage applied to each winding. Solid state soft starters can control one or more phases of the voltage applied to the induction motor with the best results achieved by three-phase control. Typically, the voltage is controlled by reverse-parallel-connected silicon-controlled rectifiers (thyristors), but in some circumstances with three-phase control, the control elements can be a reverse-parallel-connected SCR and diode.\n\nAnother way to limit motor starting current is a series reactor. If an air core is used for the series reactor then a very efficient and reliable soft starter can be designed which is suitable for all types of 3 phase induction motor [ synchronous / asynchronous ] ranging from 25 kW 415 V to 30 MW 11 kV. Using an air core series reactor soft starter is very common practice for applications like pump, compressor, fan etc. Usually high starting torque applications do not use this method.\n\nSoft starters can be set up to the requirements of the individual application. In pump applications, a soft start can avoid pressure surges. Conveyor belt systems can be smoothly started, avoiding jerk and stress on drive components. Fans or other systems with belt drives can be started slowly to avoid belt slipping. Soft starts are seen in electrical R/C helicopters, and allow the rotor blades to spool-up in a smooth, controlled manner rather than a sudden surge. In all systems, a soft start limits the inrush current and so improves stability of the power supply and reduces transient voltage drops that may affect other loads. \n"}
{"id": "50452877", "url": "https://en.wikipedia.org/wiki?curid=50452877", "title": "Nuki (joinery)", "text": "Nuki (joinery)\n\nThe Nuki joint is a Japanese style of carpentry connection. Nuki joints are common in Japanese and oriental carpentry, and comprise one of the simplest structural connectors. They are similar to mortise and tenon joints, and have been used traditionally in historic buildings, such as Shinto shrines and Buddhist temples, and also in modern domestic houses. The basic principle involves penetrating one element through another (i.e., embedment); in Japan and other Asian countries this method is used to connect wooden posts and beams.\n"}
{"id": "22320319", "url": "https://en.wikipedia.org/wiki?curid=22320319", "title": "Oak–heath forest", "text": "Oak–heath forest\n\nAn oak-heath forest is a plant community association and type of forest ecology. It is a deciduous forest type of well-drained, acidic soils, characterized by oaks (\"Quercus\") and plants of the heath family, Ericaceae. It is commonly found in the high elevations of the eastern United States. Such forest areas typically have a dense fibrous root layer at the surface of the soil, and in many areas predominate on south-facing or southwest-facing slopes. Many of the existing oak-heath forests once featured American chestnut as an important canopy species.\n\nOaks (\"Quercus\") characteristic of an oak-heath associations include white oak, \"Quercus alba\"; black oak, \"Quercus velutina\"; scarlet oak, \"Quercus coccinea\"; chestnut oak, \"Quercus prinus\", and red oak, \"Quercus rubra\".\n\nHeath plants common to this ecology include mountain-laurel, \"Kalmia latifolia\", various blueberries, genus \"Vaccinium\", huckleberries, genus \"Gaylussacia\", sourwood (or sorrel-tree), \"Oxydendron arboreum\", and azaleas and rhododendrons, genus \"Rhododendron\". These are all usually shrubs, except for \"Oxydendron\", which is usually a small tree. There are also heaths that are sub-shrubs, usually trailing on the ground, including teaberry, \"Gaultheria procumbens\" and trailing arbutus, \"Epigaea repens\".\n"}
{"id": "9146267", "url": "https://en.wikipedia.org/wiki?curid=9146267", "title": "Optical solar reflector", "text": "Optical solar reflector\n\nAn optical solar reflector (OSR) consists of a top layer made out of quartz, over a reflecting layer made of metal.\n\nOSRs are used for radiators on spacecraft. The quartz outer layer lets the solar light through which reflects on the metal layer. This results in a low absorption coefficient. The quartz layer is a good IR emitter. The result of these properties is a good emitting, low absorbing material, thus making it a cold material.\n\nOptical solar reflectors are a type of second surface mirror.\n"}
{"id": "6773523", "url": "https://en.wikipedia.org/wiki?curid=6773523", "title": "PPS-1350", "text": "PPS-1350\n\nPPS-1350 is a Hall-effect thruster, a kind of ion propulsion system for spacecraft. It was used in the SMART-1 mission to the moon and two geostationary satellites: Inmarsat-4A F4 and Hispasat AG1.\n\nIt creates a stream of electrically charged xenon ions accelerated by an electric field and confined by a magnetic field. The PPS-1350 is built by Snecma, a French aerospace firm, in cooperation with Fakel, who designed the SPT-100, on which the PPS 1350 is based.\n\n"}
{"id": "4593428", "url": "https://en.wikipedia.org/wiki?curid=4593428", "title": "Pitching moment", "text": "Pitching moment\n\nIn aerodynamics, the pitching moment on an airfoil is the moment (or torque) produced by the aerodynamic force on the airfoil if that aerodynamic force is considered to be applied, not at the center of pressure, but at the aerodynamic center of the airfoil. The pitching moment on the wing of an airplane is part of the total moment that must be balanced using the lift on the horizontal stabilizer. More generally, a pitching moment is any moment acting on the pitch axis of a moving body.\n\nThe lift on an airfoil is a distributed force that can be said to act at a point called the center of pressure. However, as angle of attack changes on a cambered airfoil, there is movement of the center of pressure forward and aft. This makes analysis difficult when attempting to use the concept of the center of pressure. One of the remarkable properties of a cambered airfoil is that, even though the center of pressure moves forward and aft, if the lift is imagined to act at a point called the aerodynamic center the moment of the lift force changes in proportion to the square of the airspeed. If the moment is divided by the dynamic pressure, the area and chord of the airfoil, the result is known as the pitching moment coefficient. This coefficient changes only a little over the operating range of angle of attack of the airfoil. The combination of the two concepts of \"aerodynamic center\" and \"pitching moment coefficient\" make it relatively simple to analyse some of the flight characteristics of an aircraft.\n\nThe aerodynamic center of an airfoil is usually close to 25% of the chord behind the leading edge of the airfoil. When making tests on a model airfoil, such as in a wind-tunnel, if the force sensor is not aligned with the quarter-chord of the airfoil, but offset by a distance x, the pitching moment about the quarter-chord point, formula_1 is given by\nwhere the indicated values of \"D\" and \"L\" are the drag and lift on the model, as measured by the force sensor.\n\nThe pitching moment coefficient is important in the study of the longitudinal static stability of aircraft and missiles.\n\nThe \"pitching moment coefficient\" formula_3 is defined as follows \nwhere \"M\" is the pitching moment, \"q\" is the dynamic pressure, \"S\" is the wing area, and \"c\" is the length of the chord of the airfoil.\nformula_3 is a dimensionless coefficient so consistent units must be used for \"M\", \"q\", \"S\" and \"c\".\n\nPitching moment coefficient is fundamental to the definition of aerodynamic center of an airfoil. The \"aerodynamic center\" is defined to be the point on the chord line of the airfoil at which the \"pitching moment coefficient\" does not vary with angle of attack, or at least does not vary significantly over the operating range of angle of attack of the airfoil.\n\nIn the case of a symmetric airfoil, the lift force acts through one point for all angles of attack, and the \"center of pressure\" does not move as it does in a cambered airfoil. Consequently, the \"pitching moment coefficient\" for a symmetric airfoil is zero.\n\nThe pitching moment is, by convention, considered to be positive when it acts to pitch the airfoil in the nose-up direction. Conventional cambered airfoils supported at the aerodynamic center pitch nose-down so the \"pitching moment coefficient\" of these airfoils is negative.\n\n\n"}
{"id": "32451837", "url": "https://en.wikipedia.org/wiki?curid=32451837", "title": "Ramp generator", "text": "Ramp generator\n\nIn electronics and electrical engineering, a ramp generator is a circuit that creates a linear rising or falling output with respect to time. The output variable is usually voltage, although current ramps can be created.\nLinear ramp generators are also known as sweep generators\n\nRamp generator produces a Sawtooth wave form, Suppose a 3V is applied to input of a comparator of X terminal and ramp generator at Y terminal. the ramp generator starts increasing its voltage but, still lower than input X terminal of the comparator the output shall be 1, As soon as the ramp voltage is equal to or more than X, comparator output goes low.\nVoltage and current linear ramp generator find wide application in instrumentation and communication systems. \nRamp generators used in electrical generators or electric motors to avoid transients when changing a load. Some ramp generators present also the possibility to change the start-up and return flow time.\n\nOriginally, ramp generators were implemented as analog hardware devices.\n"}
{"id": "44028105", "url": "https://en.wikipedia.org/wiki?curid=44028105", "title": "Rhenish-Westphalian Coal Syndicate", "text": "Rhenish-Westphalian Coal Syndicate\n\nThe Rhenish-Westphalian Coal Syndicate (ger.: Rheinisch-Westfälisches Kohlen-Syndikat -RWKS) was a cartel established in 1893 in Essen bringing together the major coal producers in the Ruhr.\n\nThe syndicate was set up as coal producers moved towards using shipping rather than railways to deliver their coal to Rotterdam. The cartel co-operated with the Dutch Coal Trade Union, to whom they gave the sole distribution rights for Westphalian coal. Daniël George van Beuningen of the Steenkolen Handels Vereniging was a leading figure in this relationship, greatly increasing the amount of coal imported to Rotterdam and resulting in the cost of using Rhine based barges dropping as their greater use also stimulated technical innovation.\n\nThis arrangement led to Rotterdam becoming not just the leading coal transhipment port in the Netherlands but also evolving into the major bunker port in Europe. In 1913 this coal transhipment accounted for over two thirds of the total shipping on the Rhine. By this time the Rhenish-Westphalian Coal Syndicate accounted for 93% of the coal output in the Ruhr and 54% of Germany as a whole.\n"}
{"id": "6304757", "url": "https://en.wikipedia.org/wiki?curid=6304757", "title": "SNOLAB", "text": "SNOLAB\n\nSNOLAB is a Canadian underground physics laboratory at a depth of 2 km in Vale's Creighton nickel mine in Sudbury, Ontario. The original Sudbury Neutrino Observatory (SNO) experiment has ended, but the facilities have been expanded into a permanent underground laboratory.\n\nAlthough accessed through a mine, the laboratory proper is maintained as a class-2000 cleanroom, with very low levels of dust and background radiation.\n\nSNOLAB is the world's second-deepest underground lab facility; its overburden of rock provides 6010 metre water equivalent (MWE) shielding from cosmic rays, providing a low-background environment for experiments requiring high sensitivities and extremely low counting \n\nThe Sudbury Neutrino Observatory was the world's deepest underground experiment since the Kolar Gold Fields experiments ended with the closing of that mine in 1992. With the deepest underground laboratory in North America at 2100 metre water equivalent depth, and the deepest in the world at 4800 m.w.e., many other groups were interested in conducting experiments in the 6000 m.w.e. location.\n\nIn 2002, funding was approved by the Canada Foundation for Innovation to expand the SNO facilities into a general-purpose laboratory, and more funding was received in 2007 and 2008.\n\nConstruction of the major laboratory space was completed in 2009, with the entire lab entering operation as a 'clean' space in March 2011.\n\nSNOLAB was briefly the world's deepest underground laboratory, until it was surpassed by the 2.4 km-deep China Jinping Underground Laboratory at the end of 2010. CJPL achieves a muon flux of less than 0.2 μ/m²/day, slightly less than SNOLAB's 0.27 μ/m²/day. (For comparison, the rate on the surface, at sea level, is about 15 million μ/m²/day.)\n\nThe planned DUSEL laboratory in the United States, which would have been deeper, was greatly scaled back after the National Science Foundation refused major funding in 2010.\n\n SNOLAB hosts five operating physics experiments:\n\nFour more experiments are currently under construction:\n\nFive experiments have completed and are no longer operating:\n\nAdditional planned experiments have requested laboratory space such as the next-generation nEXO, COBRA Experiment searches for neutrinoless double beta decay, and the second generation New Experiments With Spheres–Gas (NEWS-G) electrostatic dark matter detector (a 60 cm diameter prototype is currently operating at LSM, and a 140 cm diameter version is scheduled for installation at SNOLAB by summer 2018). There are also plans for a larger PICO-250L detector.\n\nThe total size of the SNOLAB underground facilities, including utility spaces and personnel spaces, is:\n"}
{"id": "3079521", "url": "https://en.wikipedia.org/wiki?curid=3079521", "title": "Saudi Aramco World", "text": "Saudi Aramco World\n\nAramco World (formerly \"Saudi Aramco World\") is a bi-monthly magazine published by Aramco Services Company, a US-based subsidiary of Saudi Aramco, the state-owned oil company of the Kingdom of Saudi Arabia. The first issue of the magazine appeared in November 1949. The bimonthly magazine is published in Houston, Texas.\n\nWhile Saudi Arabia is still frequently the main feature of articles, the magazine also covers the wider Arabic and Muslim world, and is aimed at both company employees and other interested readers. The website also allows free access to back issues going back to the early 1960s, including photography.\n\nIn 2004, the magazine's website was awarded \"Best Magazine Website\" by the Web Marketing Association.\n\n"}
{"id": "17135491", "url": "https://en.wikipedia.org/wiki?curid=17135491", "title": "Seed paper", "text": "Seed paper\n\nSeed paper is a type of handmade paper that includes any number of different plant seeds. The seeds themselves can still germinate after the papermaking process and they can sprout when the paper is planted in soil.\n\nPapermakers have been producing paper including seeds in the United States since 1941, but international papermakers have practiced seed inclusion in the paper for centuries. Seed paper has traditionally been handmade in smaller batches and is often made-to-order for clients.\n\nSprouting seed paper has enjoyed a resurgence of popularity in the United States recently. Seed paper can be used for stationery, cards, invitations, and for decorative wraps.\n\nA wide variety of flower, vegetable, and tree seeds can also be used in seed paper for decorative effect. The seeds and flowers in the paper can create decorative effects and colors. Depending on the type of seed and the process used, different colors, thickness, and patterns can be created.\n"}
{"id": "22267813", "url": "https://en.wikipedia.org/wiki?curid=22267813", "title": "Skojec", "text": "Skojec\n\nSkojec was a medieval central European unit of account as well as a unit of mass. It was also used as a unit of currency. 1 skojec was equal to 1/24 of a grzywna.\n\n1 skojec = 30 pfennigs<br>\n1 wiardunek = 6 skojecs<br>\n1 skojec = 2 groschen\n\nThe etymology of the word skojec derives from the word \"skot\" which in old Polish described cattle; cattle were used as the reference unit of exchange since at least the time of ancient Greece, as there are references in the Iliad to a suit of armor costing 9 oxen. Athenian lawgiver Draco, when establishing his laws in 621 BCE, defined all the fines in terms of how many cattle must be paid. The Latin word for money, \"pecunia\", is proof that Romans always saw cattle (\"pecus\") as the source of wealth. In time, a slab of copper with an imprint of an ox became equivalent to the worth of the animal. In India, the word \"rupia\" means both cattle and money. Similarly, in Poland \"skot\" meant cattle and its extension \"skojec\" described money.\n\n \n"}
{"id": "20577798", "url": "https://en.wikipedia.org/wiki?curid=20577798", "title": "Snake Wells", "text": "Snake Wells\n\nSnake Wells is a term used by Shell Oil to refer to a series of oil wells drilled in the Champion West oil field offshore Brunei. The wells used a combination of technologies including extended reach drilling, swellable wellbore packers and remotely operated zonal isolation and control. The directional drilling technique allows the path of the well to be directed to achieve contact with as many potentially producing features as possible. This results in a \"snake like\" well path which weaves up and down through multiple geological features in order to achieve maximum reservoir drainage.\n\nShells marketing suggests that this approach is an invention of Shell, however in reality the technologies involved have all been used before elsewhere.\n\nShell's marketing states:\n\nThe creative minds at Shell refused to accept that so much perfectly good oil was going to waste - so they decided to do something about it. The snake well drill is a ground breaking piece of technology that allows us to extract oil from these previously hard to reach places. Unlike conventional drills, state of the art software allows the snake well drill to follow complex horizontal paths, cutting through shale and sand to reach a number of different reservoir pockets from a single drilling platform.\nHowever, despite the high complexity of the wells, the articles published in the industry publications refer only to existing technologies.\n"}
{"id": "772868", "url": "https://en.wikipedia.org/wiki?curid=772868", "title": "Solar thermal collector", "text": "Solar thermal collector\n\nA solar thermal collector collects heat by absorbing sunlight. The term \"solar collector\" commonly refers to solar hot water panels, but may refer to installations such as solar parabolic troughs and solar towers; or basic installations such as solar air heaters. Concentrated solar power plants usually use the more complex collectors to generate electricity by heating a fluid to drive a turbine connected to an electrical generator. Simple collectors are typically used in residential and commercial buildings for space heating. The first solar thermal collector designed for building roofs was patented by William H. Goettl and called the \"Solar heat collector and radiator for building roof\".\n\nSolar collectors are either non-concentrating or concentrating. In the non-concentrating type, the collector area (i.e., the area that intercepts the solar radiation) is the same as the absorber area (i.e., the area absorbing the radiation). In these types the whole solar panel absorbs light. Concentrating collectors have a bigger interceptor than absorber.\n\nFlat-plate and evacuated-tube solar collectors are used to collect heat for space heating, domestic hot water or cooling with an absorption chiller.\n\nFlat-plate collectors are the most common solar thermal technology. They consist of an (1) enclosure containing (2) a dark colored absorber plate with fluid circulation passageways, and (3) a transparent cover to allow transmission of solar energy into the enclosure. The sides and back of the enclosure are typically insulated to reduce heat loss to the outside air. A fluid is circulated through the absorber's fluid passageways to remove heat from the solar collector. The circulation fluid in tropical and sub-tropical climates is typically water. In climates where freezing is likely, a heat-transfer fluid similar to an automotive antifreeze solution may be used instead of water, or in a mixture with water. If a heat transfer fluid is used, a heat exchanger is typically employed to transfer heat from the solar collector fluid to a hot water storage tank. The most common absorber design consists of copper tubing attached to thermally conductive copper or aluminum fins. A dark coating is applied to the sun-facing side of the absorber assembly to increase it absorption of solar energy. A common absorber coating is flat black enamel paint. \n\nIn higher performance solar collector designs, the transparent cover is tempered glass with reduced iron oxide content (the green color visible when viewing a pane of window glass from the side). The glass may also have a stippling pattern an anti-reflective coating to trap more solar energy by reducing reflection. The absorber coating is typically a selective coating. Selective coatings have special optical properties to improve efficiency by reducing the emittance of infrared energy from the absorber.\n\nSome manufacturers have introduced inexpensive flat plate solar collectors that employ polycarbonate transparent covers and polypropylene absorber assemblies. \n\nMost air heat fabricators and some water heat manufacturers have a completely flooded absorber consisting of two sheets of metal which the fluid passes between. Because the heat exchange area is greater, they may be marginally more efficient than traditional absorbers. \n\nIn locations with average available solar energy, flat plate collectors are sized approximately one-half to one square foot per gallon of one day's hot water use. Absorber piping configurations include:\nPolymer flat plate collectors are an alternative to metal collectors and are now being produced in Europe. These may be wholly polymer, or they may include metal plates in front of freeze-tolerant water channels made of silicone rubber. Polymers are flexible and therefore freeze-tolerant and can employ plain water instead of antifreeze, so that they may be plumbed directly into existing water tanks instead of needing heat exchangers that lower efficiency. By dispensing with a heat exchanger, temperatures need not be quite so high for the circulation system to be switched on, so such direct circulation panels, whether polymer or otherwise, can be more efficient, particularly at low light levels. Some early selectively coated polymer collectors suffered from overheating when insulated, as stagnation temperatures can exceed the polymer's melting point. For example, the melting point of polypropylene is , while the stagnation temperature of insulated thermal collectors can exceed if control strategies are not used. For this reason polypropylene is not often used in glazed selectively coated solar collectors. Increasingly polymers such as high temperate silicones (which melt at over ) are being used. Some non polypropylene polymer based glazed solar collectors are matte black coated rather than selectively coated to reduce the stagnation temperature to or less.\n\nIn areas where freezing is a possibility, freeze-tolerance (the capability to freeze repeatedly without cracking) can be achieved by the use of flexible polymers. Silicone rubber pipes have been used for this purpose in UK since 1999. Conventional metal collectors are vulnerable to damage from freezing, so if they are water filled they must be carefully plumbed so they completely drain using gravity before freezing is expected, so that they do not crack. Many metal collectors are installed as part of a sealed heat exchanger system. Rather than having potable water flow directly through the collectors, a mixture of water and antifreeze such as propylene glycol is used. A heat exchange fluid protects against freeze damage down to a locally determined risk temperature that depends on the proportion of propylene glycol in the mixture. The use of glycol lowers the water's heat carrying capacity marginally, while the addition of an extra heat exchanger may lower system performance at low light levels.\n\nA pool or unglazed collector is a simple form of flat-plate collector without a transparent cover. Typically polypropylene or EPDM rubber or silicone rubber is used as an absorber. Used for pool heating it can work quite well when the desired output temperature is near the ambient temperature (that is, when it is warm outside). As the ambient temperature gets cooler, these collectors become less effective. Most flat plate collectors have a life expectancy of over 25 years.\n\nMost vacuum tube collectors are used in middle Europe use heat pipes for their core instead of passing liquid directly through them. Direct flow is more popular in China. Evacuated heat pipe tubes (EHPTs) are composed of multiple evacuated glass tubes each containing an absorber plate fused to a heat pipe. The heat is transferred to the transfer fluid (water or an antifreeze mix—typically propylene glycol) of a domestic hot water or hydronic space heating system in a heat exchanger called a \"manifold\". The manifold is wrapped in insulation and covered by a protective sheet metal or plastic case.\nThe vacuum inside of the evacuated tube collectors have been proven to last more than 25 years, the reflective coating for the design is encapsulated in the vacuum inside of the tube, which will not degrade until the vacuum is lost.\nThe vacuum that surrounds the outside of the tube greatly reduces convection and conduction heat loss, therefore achieving greater efficiency than flat-plate collectors, especially in colder conditions. This advantage is largely lost in warmer climates, except in those cases where very hot water is desirable, e.g., for commercial processes. The high temperatures that can occur may require special design to prevent overheating.\n\nSome evacuated tubes (glass-metal) are made with one layer of glass that fuses to the heat pipe at the upper end and encloses the heat pipe and absorber in the vacuum. Others (glass-glass) are made with a double layer of glass fused together at one or both ends with a vacuum between the layers (like a vacuum bottle or flask), with the absorber and heat pipe contained at normal atmospheric pressure. Glass-glass tubes have a highly reliable vacuum seal, but the two layers of glass reduce the light that reaches the absorber. Moisture may enter the non-evacuated area of the tube and cause absorber corrosion. Glass-metal tubes allow more light to reach the absorber, and protect the absorber and heat pipe from corrosion even if they are made from dissimilar materials (see galvanic corrosion).\n\nThe gaps between the tubes may allow for snow to fall through the collector, minimizing the loss of production in some snowy conditions, though the lack of radiated heat from the tubes can also prevent effective shedding of accumulated snow.\n\nA longstanding argument exists between proponents of these two technologies. Some of this can be related to the physical structure of evacuated tube collectors which have a discontinuous absorbance area. An array of evacuated tubes on a roof has open space between the collector tubes, and vacuum between the two concentric glass tubes of each collector. Collector tubes cover only a fraction of a unit area on a roof. If evacuated tubes are compared with flat-plate collectors on the basis of area of roof occupied, a different conclusion might be reached than if the areas of absorber were compared. In addition, the ISO 9806 standard is ambiguous in describing the way in which the efficiency of solar thermal collectors should be measured, since these could be measured either in terms of gross area or in terms of absorber area. Unfortunately, power output is not given for thermal collectors as it is for PV panels. This makes it difficult for purchasers and engineers to make informed decisions.\n\nFlat-plate collectors usually lose more heat to the environment than evacuated tubes, as an increasing function of temperature. They are inappropriate for high temperature applications such as process steam production. Evacuated tube collectors have a lower absorber plate area to gross area ratio (typically 60–80% of gross area) compared to flat plates. Based on absorber plate area, most evacuated tube systems are more efficient per square meter than equivalent flat plate systems. This makes them suitable where roof space is limiting, for example where the number of occupants of a building is higher than the number of square metres of suitable and available roof space. In general, per installed square metre, evacuated tubes deliver marginally more energy when the ambient temperature is low (e.g. during winter) or when the sky is overcast. However, even in areas without much sunshine and solar heat, some low cost flat plate collectors can be more cost efficient than evacuated tube collectors. Although several European companies manufacture evacuated tube collectors, the evacuated tube market is dominated by manufacturers in the East. Several Chinese companies have track records of 15–30 years. There is no unambiguous evidence that the two designs differ in long term reliability. However, evacuated tube technology is younger and (especially for newer variants with sealed heat pipes) still need to demonstrate competitive lifetimes. The modularity of evacuated tubes can be advantageous in terms of extensibility and maintenance, for example if the vacuum in one tube diminishes.\nFor a given absorber area, evacuated tubes can therefore maintain their efficiency over a wide range of ambient temperatures and heating requirements. In most climates, flat-plate collectors will generally be more cost-effective than evacuated tubes. When employed in arrays and considered instead on a per square metre basis, the efficient but costly evacuated tube collectors can have a net benefit in winter and summer. They are well-suited to cold ambient temperatures and work well in situations of consistently low sunshine, providing heat more consistently than flat plate collectors per square metre. Heating of water by a medium to low amount (i.e. Tm-Ta) is much more efficiently performed by flat plate collectors. Domestic hot water frequently falls into this medium category. Glazed or unglazed flat collectors are the preferred devices for heating swimming pool water. Unglazed collectors may be suitable in tropical or subtropical environments if domestic hot water needs to be heated by less than 20 °C. A contour map can show which type is more effective (both thermal efficiency and energy/cost) for any geographic region.\n\nEHPTs work as a thermal one-way valve due to their heat pipes. This gives them an inherent maximum operating temperature that acts as a safety feature. They have less aerodynamic drag, which may allow them to be placed onto the roof without being tied down. They can collect thermal radiation from the bottom in addition to the top. Tubes can be replaced individually without stopping the entire system. There is no condensation or corrosion within the tubes. One hurdle to wider adoption of evacuated tube collectors in some markets is their inability to pass internal thermal shock tests where ISO 9806-2 section 9 class b is a requirement for durability certification. This means that if unprotected evacuated tube collectors are exposed to full sun for too long prior to being filled with cold water the tubes may shatter due to the rapid temperature shift. There is also the question of vacuum leakage. Flat panels have been around much longer and are less expensive. They may be easier to clean. Other properties, such as appearance and ease of installation are more subjective.\n\nThe main use of this technology is in residential buildings where the demand for hot water has a large impact on energy bills. This generally means a situation with a large family, or a situation in which the hot water demand is excessive due to frequent laundry washing. Commercial applications include laundromats, car washes, military laundry facilities and eating establishments. The technology can also be used for space heating if the building is located off-grid or if utility power is subject to frequent outages. Solar water heating systems are most likely to be cost effective for facilities with water heating systems that are expensive to operate, or with operations such as laundries or kitchens that require large quantities of hot water. Unglazed liquid collectors are commonly used to heat water for swimming pools but can also be applied to large scale water pre-heating. When loads are large relative to available collector area the bulk of the water heating can be done at low temperature, lower than swimming pool temperatures where unglazed collectors are well established in the marketplace as the right choice. Because these collectors need not withstand high temperatures, they can use less expensive materials such as plastic or rubber. Many unglazed collectors are made of polypropylene and must be drained fully to avoid freeze damage when air temperatures drop below 44F on clear nights. A smaller but growing percentage of unglazed collectors are flexible meaning they can withstand water freezing solid inside their absorber. The freeze concern only need be the water filled piping and collector manifolds in a hard freeze condition. Unglazed solar hot water systems should be installed to \"drainback\" to a storage tank whenever solar radiation is insufficient. There are no thermal shock concerns with unglazed systems. Commonly used in swimming pool heating since solar energy's early beginnings, unglazed solar collectors heat swimming pool water directly without the need for antifreeze or heat exchangers. Hot water solar systems require heat exchangers due to contamination possibilities and in the case of unglazed collectors, the pressure difference between the solar working fluid (water) and the load (pressurized cold city water). Large scale unglazed solar hot water heaters like the one at the Minoru Aquatic Center in Richmond BC operate at lower temperatures than evacuated tube or boxed and glazed collector systems so they require larger more expensive heat exchangers but all other components including vented storage tanks and uninsulated plastic PVC piping reduce costs of this alternative dramatically compared to the higher temperature collector types. When heating hot water, we are actually heating cold to warm and warm to hot. We can heat cold to warm as efficiently with unglazed collectors, just as we can heat warm to hot with high temperature collectors\n\nA \"solar bowl\" is a type of solar thermal collector that operates similarly to a parabolic dish, but instead of using a tracking parabolic mirror with a fixed receiver, it has a fixed spherical mirror with a tracking receiver. This reduces efficiency, but makes it cheaper to build and operate. Designers call it a \"fixed mirror distributed focus solar power system\". The main reason for its development was to eliminate the cost of moving a large mirror to track the sun as with parabolic dish systems.\n\nA fixed parabolic mirror creates a variously shaped image of the sun as it moves across the sky. Only when the mirror is pointed directly at the sun does the light focus on one point. That is why parabolic dish systems track the sun. A fixed spherical mirror focuses the light in the same place independent of the sun's position. The light, however, is not directed to one point but is distributed on a line from the surface of the mirror to one half radius (along a line that runs through the sphere center and the sun).\n\nAs the sun moves across the sky, the aperture of any fixed collector changes. This causes changes in the amount of captured sunlight, producing what is called the \"sinus effect\" of power output. Proponents of the solar bowl design claim the reduction in overall power output compared with tracking parabolic mirrors is offset by lower system costs.\n\nThe sunlight concentrated at the focal line of a spherical reflector is collected using a tracking receiver. This receiver is pivoted around the focal line and is usually counterbalanced. The receiver may consist of pipes carrying fluid for thermal transfer or photovoltaic cells for direct conversion of light to electricity.\n\nThe solar bowl design resulted from a project of the Electrical Engineering Department of the Texas Technical University, headed by Edwin O'Hair, to develop a 5 MWe power plant. A solar bowl was built for the town of Crosbyton, Texas as a pilot facility. The bowl had a diameter of , tilted at a 15° angle to optimize the cost/yield relation (33° would have maximized yield). The rim of the hemisphere was \"trimmed\" to 60°, creating a maximum aperture of . This pilot bowl produced electricity at a rate of 10 kW peak.\n\nA 15-meter diameter Auroville solar bowl was developed from an earlier test of a 3.5-meter bowl in 1979–1982 by the Tata Energy Research Institute. That test showed the use of the solar bowl in the production of steam for cooking. The full-scale project to build a solar bowl and kitchen ran from 1996, and was fully operational by 2001.\n\nA simple solar air collector consists of an absorber material, sometimes having a selective surface, to capture radiation from the sun and transfers this thermal energy to air via conduction heat transfer. This heated air is then ducted to the building space or to the process area where the heated air is used for space heating or process heating needs. Functioning in a similar manner as a conventional forced air furnace, solar-thermal-air systems provide heat by circulating air over an energy collecting surface, absorbing the sun’s thermal energy, and ducting air coming in contact with it. Simple and effective collectors can be made for a variety of air conditioning and process applications.\n\nA variety of applications can utilize solar air heat technologies to reduce the carbon footprint from use of conventional heat sources, such as fossil fuels, to create a sustainable means to produce thermal energy. Applications such as space heating, greenhouse season extension, pre-heating ventilation makeup air, or process heat can be addressed by solar air heat devices. In the field of ‘solar co-generation,' solar thermal technologies are paired with photovoltaics (PV) to increase the efficiency of the system by taking heat away from the PV collectors, cooling the PV panels to improve their electrical performance while simultaneously warming air for space heating.\n\nSpace heating for residential and commercial applications can be done through the use of solar air heating panels. This configuration operates by drawing air from the building envelope or from the outdoor environment and passing it through the collector where the air warms via conduction from the absorber and is then supplied to the living or working space by either passive means or with the assistance of a fan. A pioneering figure of this type of system was George Löf, who built a solar heated air system in 1945 for a house in Boulder, Colorado. He later included a gravel bed for heat storage.\n\nVentilation, fresh air or makeup air is required in most commercial, industrial and institutional buildings to meet code requirements. By drawing air through a properly designed unglazed transpired air collector or an air heater, the solar heated fresh air can reduce the heating load during daytime operation. Many applications are now being installed where the transpired collector preheats the fresh air entering a heat recovery ventilator to reduce the defrost time of HRV's. The higher your ventilation and temperature the better your payback time will be.\n\nSolar air heat is also used in process applications such as drying laundry, crops (i.e. tea, corn, coffee) and other drying applications. Air heated through a solar collector and then passed over a medium to be dried can provide an efficient means by which to reduce the moisture content of the material.\n\nCollectors are commonly classified by their air-ducting methods as one of three types:\n\nCollectors can also be classified by their outer surface:\n\nOffering the highest efficiency of any solar technology the through-pass configuration, air ducted onto one side of the absorber passes through a perforated material and is heated from the conductive properties of the material and the convective properties of the moving air. Through-pass absorbers have the most surface area which enables relatively high conductive heat transfer rates, but significant pressure drop can require greater fan power, and deterioration of certain absorber material after many years of solar radiation exposure can additionally create problems with air quality and performance.\n\nIn back-pass, front-pass, and combination type configurations the air is directed on either the back, the front, or on both sides of the absorber to be heated from the return to the supply ducting headers. Although passing the air on both sides of the absorber will provide a greater surface area for conductive heat transfer, issues with dust (fouling) can arise from passing air on the front side of the absorber which reduces absorber efficiency by limiting the amount of sunlight received. In cold climates, air passing next to the glazing will additionally cause greater heat loss, resulting in lower overall performance of the collector.\nGlazed systems usually have a transparent top sheet and insulated side and back panels to minimize heat loss to ambient air. The absorber plates in modern panels can have absorptivity of more than 93%. Glazed Solar Collectors (recirculating types that are usually used for space heating). Air typically passes along the front or back of the absorber plate while scrubbing heat directly from it. Heated air can then be distributed directly for applications such as space heating and drying or may be stored for later use. Payback for glazed solar air heating panels can be less than 9–15 years depending on the fuel being replaced.\n\nUnglazed systems, or transpired air systems have been used to heat make-up or ventilation air in commercial, industrial, agriculture and process applications. They consist of an absorber plate which air passes across or through as it scrubs heat from the absorber. Non-transparent glazing materials are less expensive, and decrease expected payback periods. Transpired collectors are considered \"unglazed\" because their collector surfaces are exposed to the elements, are often not transparent and not hermetically sealed.\n\nThe term \"unglazed air collector\" refers to a solar air heating system that consists of a metal absorber without any glass or glazing over top. The most common type of unglazed collector on the market is the transpired solar collector. The technology has been extensively monitored by these government agencies, and Natural Resources Canada developed the feasibility tool RETScreen™ to model the energy savings from transpired solar collectors. Since that time, several thousand transpired solar collector systems have been installed in a variety of commercial, industrial, institutional, agricultural, and process applications in countries around the world. This technology was originally used primarily in industrial applications such as manufacturing and assembly plants where there were high ventilation requirements, stratified ceiling heat, and often negative pressure in the building. With the increasing drive to install renewable energy systems on buildings, transpired solar collectors are now used across the entire building stock because of high energy production (up to 750 peak thermal Watts/square metre), high solar conversion (up to 90%) and lower capital costs when compared against solar photovoltaic and solar water heating.\n\nUnglazed air collectors heat ambient (outside) air instead of recirculated building air. Transpired solar collectors are usually wall-mounted to capture the lower sun angle in the winter heating months as well as sun reflection off the snow and achieve their optimum performance and return on investment when operating at flow rates of between 4 and 8 CFM per square foot (72 to 144 m3/h.m2) of collector area.\n\nThe exterior surface of a transpired solar collector consists of thousands of tiny micro-perforations that allow the boundary layer of heat to be captured and uniformly drawn into an air cavity behind the exterior panels. This heated ventilation air is drawn under negative pressure into the building’s ventilation system where it is then distributed via conventional means or using a solar ducting system.\n\nHot air that may enter an HVAC system connected to a transpired collector that has air outlets positioned along the top of the collector, particularly if the collector is west facing. To counter this problem, Matrix Energy has patented a transpired collector with a lower air outlet position and perforated cavity framing to perpetrate increased air turbulence behind the perforated absorber for increased performance.\n\nThis cutaway view shows the MatrixAir transpired solar collector components and air flow. The lower air inlet mitigates the intake of heated air to the HVAC system during summer operation.\n\nThe extensive monitoring by Natural Resources Canada and NREL has shown that transpired solar collector systems reduce between 10-50% of the conventional heating load and that RETScreen is an accurate predictor of system performance.\nTranspired solar collectors act as a rainscreen and they also capture heat loss escaping from the building envelope which is collected in the collector air cavity and drawn back into the ventilation system. There is no maintenance required with solar air heating systems and the expected lifespan is over 30 years.\n\nUnglazed transpired collectors can also be roof-mounted for applications in which there is not a suitable south facing wall or for other architectural considerations. Matrix Energy Inc. has patented a roof mounted product called the “Delta” a modular, roof-mounted solar air heating system where southerly, east or west facing facades are simply not available.\n\nEach ten foot (3.05 m) module will deliver 250 CFM (425 m3/h)of preheated fresh air typically providing annual energy savings of 1100 kWh (4 GJ) annually. This unique two stage, modular roof mounted transpired collector operating a nearly 90% efficiency each module delivering over 118 l/s of preheated air per two square meter collector. Up to seven collectors may be connected in series in one row, with no limit to the number of rows connected in parallel along one central duct typically yielding 4 CFM of preheated air per square foot of available roof area. +\n\nTranspired collectors can be configured to heat the air twice to increase the delivered air temperature making it suitable for space heating applications as well as ventilation air heating. In a 2-stage system, the first stage is the typical unglazed transpired collector and the second stage has glazing covering the transpired collector. The glazing allows all of that heated air from the first stage to be directed through a second set of transpired collectors for a second stage of solar heating.\n\nParabolic troughs, dishes and towers described in this section are used almost exclusively in solar power generating stations or for research purposes. Parabolic troughs have been used for some commercial solar air conditioning systems. Although simple, these solar concentrators are quite far from the theoretical maximum concentration. For example, the parabolic trough concentration is about 1/3 of the theoretical maximum for the same acceptance angle, that is, for the same overall tolerances for the system. Approaching the theoretical maximum may be achieved by using more elaborate concentrators based on nonimaging optics. Solar thermal collectors may also be used in conjunction with photovoltaic collectors to obtain combined heat and power.\n\nThis type of collector is generally used in solar power plants. A trough-shaped parabolic reflector is used to concentrate sunlight on an insulated tube (Dewar tube) or heat pipe, placed at the focal point, containing coolant which transfers heat from the collectors to the boilers in the power station.\n\nWith a parabolic dish collector, one or more parabolic dishes concentrate solar energy at a single focal point, similar to the way a reflecting telescope focuses starlight, or a dish antenna focuses radio waves. This geometry may be used in solar furnaces and solar power plants.\n\nThe shape of a parabola means that incoming light rays which are parallel to the dish's axis will be reflected toward the focus, no matter where on the dish they arrive. Light from the sun arrives at the Earth's surface almost completely parallel, and the dish is aligned with its axis pointing at the sun, allowing almost all incoming radiation to be reflected towards the focal point of the dish. Most losses in such collectors are due to imperfections in the parabolic shape and imperfect reflection.\n\nLosses due to atmospheric scattering are generally minimal. However, on a hazy or foggy day, light is diffused in all directions through the atmosphere, which significantly reduces the efficiency of a parabolic dish.\n\nIn dish stirling power plant designs, a stirling engine coupled to a dynamo, is placed at the focus of the dish. This absorbs the energy focused onto it and converts it into electricity.\n\nA power tower is a large tower surrounded by tracking mirrors called heliostats. These mirrors align themselves and focus sunlight on the receiver at the top of tower, collected heat is transferred to a power station below. This design reaches very high temperatures. High temperatures are suitable for electricity generation using conventional methods like steam turbine or a direct high temperature chemical reaction such as liquid salt. By concentrating sunlight, current systems can get better efficiency than simple solar cells. A larger area can be covered by using relatively inexpensive mirrors rather than using expensive solar cells. Concentrated light can be redirected to a suitable location via optical fiber cable for such uses as illuminating buildings. Heat storage for power production during cloudy and overnight conditions can be accomplished, often by underground tank storage of heated fluids. Molten salts have been used to good effect. Other working fluids, such as liquid metals, have also been proposed due to their superior thermal properties.\n\nHowever, concentrating systems require sun tracking to maintain sunlight focus at the collector. They are unable to provide significant power in diffused light conditions. Solar cells are able to provide some output even if the sky becomes cloudy, but power output from concentrating systems drops drastically in cloudy conditions as diffused light cannot be concentrated.\n\n\n\n"}
{"id": "1450728", "url": "https://en.wikipedia.org/wiki?curid=1450728", "title": "Stephen King (conservationist)", "text": "Stephen King (conservationist)\n\nStephen King is a New Zealand conservationist.\n\nHe is well known for his tree top protest in what is now the Pureora Forest Park in order to halt native forest logging during campaigning in the 1970s and 1980s. King also is involved in the protection of kauri trees in the Waipoua Forest.\n\nKing is one of the patrons of the New Zealand Trust for Conservation Volunteers and is one of the founding trustees of the Waipoua Forest Trust.\n\nStephen King was sentenced to four months community detention in 2010 for possession of objectionable images, including child pornography.\n"}
{"id": "42009369", "url": "https://en.wikipedia.org/wiki?curid=42009369", "title": "Stockyard Hill Wind Farm", "text": "Stockyard Hill Wind Farm\n\nStockyard Hill Wind Farm is a wind farm project being developed and constructed by Goldwind Australia, in Victoria (Australia). \n\nThe proposed facility will have 149 wind turbines at a site approximately 35 km west of Ballarat between Beaufort and Skipton. The forecast cost is $900 million ($750 million US dollars) In December 2009, the original plan for 242 turbines, the plan was changed to only have 157 turbines. The Victorian minister of planning approved the 157 turbines, in October 2010.\n\nIn 2013 Origin contemplated selling the project before production would have begun, but decided to continue the project.\n\nConstruction commenced in May 2018 on a design that has 149 wind turbines. Origin Energy has a power purchase agreement to buy all electricity until 2030. Goldwind Australia is providing Engineering, Procurement and Construction during development and Warranty, Operations and Maintenance services following commissioning.\n\n\n"}
{"id": "41808229", "url": "https://en.wikipedia.org/wiki?curid=41808229", "title": "Tetranitratoborate", "text": "Tetranitratoborate\n\nTetranitratoborate is an anion composed of boron with four nitrate groups. It has formula [B(NO)]. It can form salts with large cations such as tetramethyl ammonium nitratoborate, or tetraethyl ammonium tetranitratoborate.\n\nThe ion was first discovered by C. R. Guibert, M. D. Marshall in 1966 after failed attempts to make neutral (non-ionic) boron nitrate.\n\nThe related molecule, boron nitrate B(NO), has resisted attempts to make it, and if it exists it is unstable above −78 °C.\n\nOther related ions are the slightly more stable tetraperchloratoborates, with perchlorate groups instead of nitrate, and tetranitratoaluminate with the next atom down the periodic table, aluminium instead of boron ([Al(NO)]).\n\nTetramethyl ammonium chloride reacts with BCl to make (CH)NBCl. Then the tetrachloroborate is reacted with NO at around −20 °C to form tetramethyl ammonium nitratoborate, and other gases such as NOCl and Cl.\n\nAnother mechanism to make tetranitratoborate salts is to shake a metal nitrate with BCl in chloroform at 20 °C for several days. Chloronitratoborate [ClBNO] is an unstable intermediate.\n\nThe infrared spectrum of tetramethyl ammonium nitratoborate includes a prominent line at 1,612 cm with shoulders at 1582 and 1,626 cm attributed to \"v\". Also prominent is 1,297 and 1,311 cm attributed to \"v\", with these vibrations due to the nitrate bonded via one oxygen.\n\nThe density of tetramethyl ammonium nitratoborate is 1.555. It is colourless and crystalline. As tetramethyl ammonium nitratoborate is heated it has some sort of transition between 51 and 62 °C. It decomposes above 75 °C producing gas. Above 112 °C it is exothermic, and a solid is left if it is heated to 160 °C.\n\nTetramethyl ammonium nitratoborate is insoluble in cold water but slightly soluble in hot water. It does not react with water. It also dissolves in liquid ammonia, acetonitrile, methanol, and dimethylformamide. It reacts with liquid sulfur dioxide.\n\nAt room temperature tetramethyl ammonium nitratoborate is stable for months. It does not explode with impact.\n\nAlkali metal tetranitratoborates are unstable at room temperature and decompose.\n\n1-Ethyl-3-methyl-imidazolimium tetranitratoborate was discovered in 2002. It is an ionic liquid that turns solid at −25 °C.\n"}
{"id": "2396574", "url": "https://en.wikipedia.org/wiki?curid=2396574", "title": "The Formula (1980 film)", "text": "The Formula (1980 film)\n\nThe Formula is a 1980 American mystery film directed by John G. Avildsen and released by Metro-Goldwyn-Mayer. It features a preeminent cast including Marlon Brando, George C. Scott, John Gielgud, and Marthe Keller. Craig T. Nelson also makes a brief appearance as a geologist.\n\nThe film opens in the final days of World War II as Soviet forces close in on the outskirts of Berlin. Panzer Korps General Helmut Kladen (Richard Lynch) is dispatched to the Swiss frontier with top secret documents to be used as a bargaining chip with the Allies to save Germany from the Soviets. He is subsequently intercepted by the U.S. Army and turned over to Army Intelligence.\n\nIn contemporary Los Angeles, Lt. Barney Caine (George C. Scott) is assigned to solve the murder of his former boss and friend Tom Neeley, which presumably occurred during a drug deal gone wrong. However, Neeley has written \"Gene\" on a newspaper in his own blood, and Caine finds a map of Germany with the name \"Obermann\" on it. Caine is surprised to learn that Neeley provided drugs at parties hosted by the tycoon Adam Steiffel (Marlon Brando). When he interviews Neeley's ex-wife (Beatrice Straight), he quickly catches her in several lies, and when he returns to interview her a second time, he finds her shot dead in her hot tub.\n\nSteiffel reveals in his interview that Neeley was working for him as a bagman. Neeley was sent overseas by the company to deliver money to business partners. Caine becomes convinced he must go to Germany to solve Neeley's murder, he convinces his Chief (Alan North) to allow him to go to Germany to continue the investigation. Later, the Chief is seen phoning one of Steiffel's cronies (G. D. Spradlin) to tell him that Caine has taken the bait.\n\nOnce in Berlin, Caine meets Paul Obermann (David Byrd) at the Berlin Zoo. Obermann explains operation \"Genesis\". This confirms Caine's hunch that Neeley was killed over Genesis. Obermann is then murdered outside the zoo. At his apartment, his niece Lisa (Marthe Keller) shows up to be interviewed by the police. At Obermann's memorial service, Caine asks Lisa to accompany him to act as his interpreter. Lisa agrees and they follow up on a lead that Obermann gave him regarding Professor Siebold who worked on the formula.\n\nDuring their interview with Siebold (Ferdy Mayne), he reveals that the inventor of the formula, Dr. Abraham Esau (John Gielgud), is still alive. After they leave his apartment, Siebold is shot in the head through a window. When they meet up with Esau, he writes down the formula for Caine, after he makes Caine promise to make it public. Lisa and Caine make photocopies and send them to the LAPD and a Swiss energy company. Caine also hides two copies from Lisa, depositing them in the hotel's safe. Subsequently, he reveals that he has deduced that she is not Obermann's niece at all, but a spy sent to keep tabs on him. Lisa admits it, but claims she didn't sleep with him because of her orders.\n\nAt the border with East Berlin, Caine confronts Tadesco who relates how he knew Neeley, and what transpired after his capture by the Americans. As Tadesco walks towards his car, Lisa kills Tadesco, then walks towards East Berlin. At the airport before flying home to Los Angeles, Caine realizes the two copies of the formula in the hotel safe were replaced with fakes by Lisa, and that the only real copies are with the LAPD and the Swiss.\n\nAfter landing in Los Angeles, he heads straight to Steiffel's office. Steiffel has kidnapped Caine's partner (Yosuta) and is holding him to exchange for the copy of the formula.\n\nAfter exchanging the formula for Yosuta's release, Caine demands answers from Steiffel. Steiffel then outlines the cartel's plan since the end of the war, to keep the formula secret. They had been able to keep it secret until Swiss businessman, Tauber, began searching for the members of the original Genesis team, in the hope the team could recreate the formula. Tauber's actions made the members of the Genesis team a liability to the cartel, so Steiffel had pulled strings to get Caine sent on a trip to Germany, which would serve as a cover for the cartel's plot to eliminate the remaining members.\n\nJust before leaving, Caine reveals that he sent the formula to Tauber.\n\nAfter their meeting, Steiffel makes a phone call to Tauber, asking him to keep the formula secret for another 10 years in exchange for a 25% share of his anthracite holdings. They negotiate briefly, and Tauber agrees to not use the formula for 10 years.\n\n\nThe film opened to mixed to negative reviews from critics. It was said that behind the scenes, John G. Avildsen was after a print of the film he liked to be released. Studio infighting led him to demand the film to be left alone, but writer-producer Steve\nShagan decided to cut the film into a different version that ultimately ended in the theaters. Avildsen was bitter. He also \nwanted the print to be released to have little of Marlon Brando to emphasize the mystery. While critics liked the acting, they were not stunned by the plot and its twists. It also won Golden Raspberry nominations.\n\n\n\n\n"}
{"id": "931370", "url": "https://en.wikipedia.org/wiki?curid=931370", "title": "Tropical rainforest", "text": "Tropical rainforest\n\nTropical rainforests are rainforests that occur in areas of tropical rainforest climate in which there is no dry season – all months have an average precipitation of at least 60 mm – and may also be referred to as \"lowland equatorial evergreen rainforest\". True rainforests are typically found between 10 degrees north and south of the equator (see map); they are a sub-set of the tropical forest biome that occurs roughly within the 28 degree latitudes (in the equatorial zone between the Tropic of Cancer and Tropic of Capricorn). Within the World Wildlife Fund's biome classification, tropical rainforests are a type of tropical moist broadleaf forest (or tropical wet forest) that also includes the more extensive seasonal tropical forests.\n\nTropical rainforests can be characterized in two words: hot and wet. Mean monthly temperatures exceed during all months of the year. Average annual rainfall is no less than and can exceed although it typically lies between and . This high level of precipitation often results in poor soils due to leaching of soluble nutrients in the ground.\n\nTropical rainforests exhibit high levels of biodiversity. Around 40% to 75% of all biotic species are indigenous to the rainforests. Rainforests are home to half of all the living animal and plant species on the planet. Two-thirds of all flowering plants can be found in rainforests. A single hectare of rainforest may contain 42,000 different species of insect, up to 807 trees of 313 species and 1,500 species of higher plants. Tropical rainforests have been called the \"world's largest pharmacy\", because over one quarter of natural medicines have been discovered within them. It is likely that there may be many millions of species of plants, insects and microorganisms still undiscovered in tropical rainforests.\n\nTropical rainforests are among the most threatened ecosystems globally due to large-scale fragmentation as a result of human activity. Habitat fragmentation caused by geological processes such as volcanism and climate change occurred in the past, and have been identified as important drivers of speciation. However, fast human driven habitat destruction is suspected to be one of the major causes of species extinction. Tropical rain forests have been subjected to heavy logging and agricultural clearance throughout the 20th century, and the area covered by rainforests around the world is rapidly shrinking.\n\nTropical rainforests have existed on earth for hundreds of millions of years. Most tropical rainforests today are on fragments of the Mesozoic era supercontinent of Gondwana. The separation of the landmass resulted in a great loss of amphibian diversity while at the same time the drier climate spurred the diversification of reptiles. The division left tropical rainforests located in five major regions of the world: tropical America, Africa, Southeast Asia, Madagascar, and New Guinea, with smaller outliers in Australia. However, the specifics of the origin of rainforests remain uncertain due to an incomplete fossil record.\n\nSeveral biomes may appear similar-to, or merge via ecotones with, tropical rainforest:\n\n\nMoist seasonal tropical forests receive high overall rainfall with a warm summer wet season and a cooler winter dry season. Some trees in these forests drop some or all of their leaves during the winter dry season, thus they are sometimes called \"tropical mixed forest\". They are found in parts of South America, in Central America and around the Caribbean, in coastal West Africa, parts of the Indian subcontinent, and across much of Indochina.\n\nThese are found in cooler-climate mountainous areas, becoming known as cloud forests at higher elevations. Depending on latitude, the lower limit of montane rainforests on large mountains is generally between 1500 and 2500 m while the upper limit is usually from 2400 to 3300 m.\n\nTropical freshwater swamp forests, or \"flooded forests\", are found in Amazon basin (the Várzea) and elsewhere.\n\nRainforests are divided into different strata, or layers, with vegetation organized into a vertical pattern from the top of the soil to the canopy. Each layer is a unique biotic community containing different plants and animals adapted for life in that particular strata. Only the emergent layer is unique to tropical rainforests, while the others are also found in temperate rainforests.\n\nThe forest floor, the bottom-most layer, receives only 2% of the sunlight. Only plants adapted to low light can grow in this region. Away from riverbanks, swamps and clearings, where dense undergrowth is found, the forest floor is relatively clear of vegetation because of the low sunlight penetration. This more open quality permits the easy movement of larger animals such as: ungulates like the okapi (\"Okapia johnstoni\"), tapir (\"Tapirus\" sp.), Sumatran rhinoceros (\"Dicerorhinus sumatrensis\"), and apes like the western lowland gorilla (\"Gorilla gorilla\"), as well as many species of reptiles, amphibians, and insects. The forest floor also contains decaying plant and animal matter, which disappears quickly, because the warm, humid conditions promote rapid decay. Many forms of fungi growing here help decay the animal and plant waste.\n\nThe understory layer lies between the canopy and the forest floor. The understory is home to a number of birds, small mammals, insects, reptiles, and predators. Examples include leopard (\"Panthera pardus\"), poison dart frogs (\"Dendrobates\" sp.), ring-tailed coati (\"Nasua nasua\"), boa constrictor (\"Boa constrictor\"), and many species of Coleoptera. The vegetation at this layer generally consists of shade-tolerant shrubs, herbs, small trees, and large woody vines which climb into the trees to capture sunlight. Only about 5% of sunlight breaches the canopy to arrive at the understory causing true understory plants to seldom grow to 3 m (10 feet). As an adaptation to these low light levels, understory plants have often evolved much larger leaves. Many seedlings that will grow to the canopy level are in the understory.\n\nThe canopy is the primary layer of the forest forming a roof over the two remaining layers. It contains the majority of the largest trees, typically 30–45 m in height. Tall, broad-leaved evergreen trees are the dominant plants. The densest areas of biodiversity are found in the forest canopy, as it often supports a rich flora of epiphytes, including orchids, bromeliads, mosses and lichens. These epiphytic plants attach to trunks and branches and obtain water and minerals from rain and debris that collects on the supporting plants. The fauna is similar to that found in the emergent layer, but more diverse. It is suggested that the total arthropod species richness of the tropical canopy might be as high as 20 million. Other species habituating this layer include many avian species such as the yellow-casqued wattled hornbill (\"Ceratogymna elata\"), collared sunbird (\"Anthreptes collaris\"), grey parrot (\"Psitacus erithacus\"), keel-billed toucan (\"Ramphastos sulfuratus\"), scarlet macaw (\"Ara macao\") as well as other animals like the spider monkey (\"Ateles\" sp.), African giant swallowtail (\"Papilio antimachus\"), three-toed sloth (\"Bradypus tridactylus\"), kinkajou (\"Potos flavus\"), and tamandua (\"Tamandua tetradactyla\").\n\nThe emergent layer contains a small number of very large trees, called \"emergents\", which grow above the general canopy, reaching heights of 45–55 m, although on occasion a few species will grow to 70–80 m tall. Some examples of emergents include: \"Balizia elegans\", \"Dipteryx panamensis\", \"Hieronyma alchorneoides\", \"Hymenolobium mesoamericanum\", \"Lecythis ampla\" and \"Terminalia oblonga\". These trees need to be able to withstand the hot temperatures and strong winds that occur above the canopy in some areas. Several unique faunal species inhabit this layer such as the crowned eagle (\"Stephanoaetus coronatus\"), the king colobus (\"Colobus polykomos\"), and the large flying fox (\"Pteropus vampyrus\").\n\nHowever, stratification is not always clear. Rainforests are dynamic and many changes affect the structure of the forest. Emergent or canopy trees collapse, for example, causing gaps to form. Openings in the forest canopy are widely recognized as important for the establishment and growth of rainforest trees. It’s estimated that perhaps 75% of the tree species at La Selva Biological Station, Costa Rica are dependent on canopy opening for seed germination or for growth beyond sapling size, for example.\n\nTropical rainforests are located around and near the equator, therefore having what is called an equatorial climate characterized by three major climatic parameters: temperature, rainfall, and dry season intensity. Other parameters that affect tropical rainforests are carbon dioxide concentrations, solar radiation, and nitrogen availability. In general, climatic patterns consist of warm temperatures and high annual rainfall. However, the abundance of rainfall changes throughout the year creating distinct moist and dry seasons. Tropical forests are classified by the amount of rainfall received each year, which has allowed ecologists to define differences in these forests that look so similar in structure. According to Holdridge’s classification of tropical ecosystems, true tropical rainforests have an annual rainfall greater than 2 m and annual temperature greater than 24 degrees Celsius, with a potential evapotranspiration ratio (PET) value of <0.25. However, most lowland tropical forests can be classified as tropical moist or wet forests, which differ in regards to rainfall. Tropical forest ecology- dynamics, composition, and function- are sensitive to changes in climate especially changes in rainfall.\n\nSoil types are highly variable in the tropics and are the result of a combination of several variables such as climate, vegetation, topographic position, parent material, and soil age. Most tropical soils are characterized by significant leaching and poor nutrients, however there are some areas that contain fertile soils. Soils throughout the tropical rainforests fall into two classifications which include the ultisols and oxisols. Ultisols are known as well weathered, acidic red clay soils, deficient in major nutrients such as calcium and potassium. Similarly, oxisols are acidic, old, typically reddish, highly weathered and leached, however are well drained compared to ultisols. The clay content of ultisols is high, making it difficult for water to penetrate and flow through. The reddish color of both soils is the result of heavy heat and moisture forming oxides of iron and aluminium, which are insoluble in water and not taken up readily by plants.\n\nSoil chemical and physical characteristics are strongly related to above ground productivity and forest structure and dynamics. The physical properties of soil control the tree turnover rates whereas chemical properties such as available nitrogen and phosphorus control forest growth rates. The soils of the eastern and central Amazon as well as the Southeast Asian Rainforest are old and mineral poor whereas the soils of the western Amazon (Ecuador and Peru) and volcanic areas of Costa Rica are young and mineral rich. Primary productivity or wood production is highest in western Amazon and lowest in eastern Amazon which contains heavily weathered soils classified as oxisols. Additionally, Amazonian soils are greatly weathered, making them devoid of minerals like phosphorus, potassium, calcium, and magnesium, which come from rock sources. However, not all tropical rainforests occur on nutrient poor soils, but on nutrient rich floodplains and volcanic soils located in the Andean foothills, and volcanic areas of Southeast Asia, Africa, and Central America.\n\nOxisols, infertile, deeply weathered and severely leached, have developed on the ancient Gondwanan shields. Rapid bacterial decay prevents the accumulation of humus. The concentration of iron and aluminium oxides by the laterization process gives the oxisols a bright red color and sometimes produces minable deposits (e.g., bauxite). On younger substrates, especially of volcanic origin, tropical soils may be quite fertile.\n\nThis high rate of decomposition is the result of phosphorus levels in the soils, precipitation, high temperatures and the extensive microorganism communities. In addition to the bacteria and other microorganisms, there are an abundance of other decomposers such as fungi and termites that aid in the process as well. Nutrient recycling is important because below ground resource availability controls the above ground biomass and community structure of tropical rainforests. These soils are typically phosphorus limited, which inhibits net primary productivity or the uptake of carbon. The soil contains microbial organisms such as bacteria, which break down leaf litter and other organic matter into inorganic forms of carbon usable by plants through a process called decomposition. During the decomposition process the microbial community is respiring, taking up oxygen and releasing carbon dioxide. The decomposition rate can be evaluated by measuring the uptake of oxygen. High temperatures and precipitation increase decomposition rate, which allows plant litter to rapidly decay in tropical regions, releasing nutrients that are immediately taken up by plants through surface or ground waters. The seasonal patterns in respiration are controlled by leaf litter fall and precipitation, the driving force moving the decomposable carbon from the litter to the soil. Respiration rates are highest early in the wet season because the recent dry season results in a large percentage of leaf litter and thus a higher percentage of organic matter being leached into the soil.\n\nA common feature of many tropical rainforests is the distinct buttress roots of trees. Instead of penetrating to deeper soil layers, buttress roots create a widespread root network at the surface for more efficient uptake of nutrients in a very nutrient poor and competitive environment. Most of the nutrients within the soil of a tropical rainforest occur near the surface because of the rapid turnover time and decomposition of organisms and leaves. Because of this, the buttress roots occur at the surface so the trees can maximize uptake and actively compete with the rapid uptake of other trees. These roots also aid in water uptake and storage, increase surface area for gas exchange, and collect leaf litter for added nutrition. Additionally, these roots reduce soil erosion and maximize nutrient acquisition during heavy rains by diverting nutrient rich water flowing down the trunk into several smaller flows while also acting as a barrier to ground flow. Also, the large surface areas these roots create provide support and stability to rainforests trees, which commonly grow to significant heights. This added stability allows these trees to withstand the impacts of severe storms, thus reducing the occurrence of fallen trees.\n\nSuccession is an ecological process that changes the biotic community structure over time towards a more stable, diverse community structure after an initial disturbance to the community. The initial disturbance is often a natural phenomenon or human caused event. Natural disturbances include hurricanes, volcanic eruptions, river movements or an event as small as a fallen tree that creates gaps in the forest. In tropical rainforests, these same natural disturbances have been well documented in the fossil record, and are credited with encouraging speciation and endemism.\n\nTropical rainforests exhibit a vast diversity in plant and animal species. The root for this remarkable speciation has been a query of scientists and ecologists for years. A number of theories have been developed for why and how the tropics can be so diverse.\n\nInterspecific competition results from a high density of species with similar niches in the tropics and limited resources available. Species which \"lose\" the competition may either become extinct or find a new niche. Direct competition will often lead to one species dominating another by some advantage, ultimately driving it to extinction. Niche partitioning is the other option for a species. This is the separation and rationing of necessary resources by utilizing different habitats, food sources, cover or general behavioral differences. A species with similar food items but different feeding times is an example of niche partitioning.\n\nThe theory of Pleistocene refugia was developed by Jürgen Haffer in 1969 with his article \"Speciation of Amazonian Forest Birds\". Haffer proposed the explanation for speciation was the product of rainforest patches being separated by stretches of non forest vegetation during the last glacial period. He called these patches of rainforest areas refuges and within these patches allopatric speciation occurred. With the end of the glacial period and increase in atmospheric humidity, rainforest began to expand and the refuges reconnected. This theory has been the subject of debate. Scientists are still skeptical of whether or not this theory is legitimate. Genetic evidence suggests speciation had occurred in certain taxa 1–2 million years ago, preceding the Pleistocene.\n\nTropical rainforests have harboured human life for many millennia, with many Indian tribes in South- and Central America, who belong to the Indigenous peoples of the Americas, the Congo Pygmies in Central Africa, and several tribes in South-East Asia, like the Dayak people and the Penan people in Borneo. Food resources within the forest are extremely dispersed due to the high biological diversity and what food does exist is largely restricted to the canopy and requires considerable energy to obtain. Some groups of hunter-gatherers have exploited rainforest on a seasonal basis but dwelt primarily in adjacent savanna and open forest environments where food is much more abundant. Other people described as rainforest dwellers are hunter-gatherers who subsist in large part by trading high value forest products such as hides, feathers, and honey with agricultural people living outside the forest.\n\nA variety of indigenous people live within the rainforest as hunter-gatherers, or subsist as part-time small scale farmers supplemented in large part by trading high-value forest products such as hides, feathers, and honey with agricultural people living outside the forest. Peoples have inhabited the rainforests for tens of thousands of years and have remained so elusive that only recently have some tribes been discovered. These indigenous peoples are greatly threatened by loggers in search for old-growth tropical hardwoods like Ipe, Cumaru and Wenge, and by farmers who are looking to expand their land, for cattle(meat), and soybeans, which are used to feed cattle in Europe and China. On 18 January 2007, FUNAI reported also that it had confirmed the presence of 67 different uncontacted tribes in Brazil, up from 40 in 2005. With this addition, Brazil has now overtaken the island of New Guinea as the country having the largest number of uncontacted tribes. The province of Irian Jaya or West Papua in the island of New Guinea is home to an estimated 44 uncontacted tribal groups.\n\nThe pygmy peoples are hunter-gatherer groups living in equatorial rainforests characterized by their short height (below one and a half meters, or 59 inches, on average). Amongst this group are the Efe, Aka, Twa, Baka, and Mbuti people of Central Africa. However, the term pygmy is considered pejorative so many tribes prefer not to be labeled as such.\n\nSome notable indigenous peoples of the Americas, or Amerindians, include the Huaorani, Ya̧nomamö, and Kayapo people of the Amazon. The traditional agricultural system practiced by tribes in the Amazon is based on swidden cultivation (also known as slash-and-burn or shifting cultivation) and is considered a relatively benign disturbance. In fact, when looking at the level of individual swidden plots a number of traditional farming practices are considered beneficial. For example, the use of shade trees and fallowing all help preserve soil organic matter, which is a critical factor in the maintenance of soil fertility in the deeply weathered and leached soils common in the Amazon.\n\nThere is a diversity of forest people in Asia, including the Lumad peoples of the Philippines and the Penan and Dayak people of Borneo. The Dayaks are a particularly interesting group as they are noted for their traditional headhunting culture. Fresh human heads were required to perform certain rituals such as the Iban \"kenyalang\" and the Kenyah \"mamat\". Pygmies who live in Southeast Asia are, amongst others, referred to as \"Negrito\".\n\nYam, coffee, chocolate, banana, mango, papaya, macadamia, avocado, and sugarcane all originally came from tropical rainforest and are still mostly grown on plantations in regions that were formerly primary forest. In the mid-1980s and 1990s, 40 million tons of bananas were consumed worldwide each year, along with 13 million tons of mango. Central American coffee exports were worth US$3 billion in 1970. Much of the genetic variation used in evading the damage caused by new pests is still derived from resistant wild stock. Tropical forests have supplied 250 cultivated kinds of fruit, compared to only 20 for temperate forests. Forests in New Guinea alone contain 251 tree species with edible fruits, of which only 43 had been established as cultivated crops by 1985.\n\nIn addition to extractive human uses, rain forests also have non-extractive uses that are frequently summarized as ecosystem services. Rain forests play an important role in maintaining biological diversity, sequestering and storing carbon, global climate regulation, disease control, and pollination. Half of the rainfall in the Amazon area is produced by the forests. The moisture from the forests is important to the rainfall in Brazil, Paraguay, Argentina Deforestation in the Amazon rainforest region was one of the main reason that cause the severe Drought of 2014-2015 in Brazil\n\nDespite the negative effects of tourism in the tropical rainforests, there are also several important positive effects.\n\nDeposits of precious metals (gold, silver, coltan) and fossil fuels (oil and natural gas) occur underneath rainforests globally. These resources are important to developing nations and their extraction is often given priority to encourage economic growth. Mining and drilling can require large amounts of land development, directly causing deforestation. In Ghana, a West African nation, deforestation from decades of mining activity left about 12% of the country's original rainforest intact.\n\nWith the invention of agriculture, humans were able to clear sections of rainforest to produce crops, converting it to open farmland. Such people, however, obtain their food primarily from farm plots cleared from the forest and hunt and forage within the forest to supplement this. The issue arising is between the independent farmer providing for his family and the needs and wants of the globe as a whole. This issue has seen little improvement because no plan has been established for all parties to be aided.\n\nAgriculture on formerly forested land is not without difficulties. Rainforest soils are often thin and leached of many minerals, and the heavy rainfall can quickly leach nutrients from area cleared for cultivation. People such as the Yanomamo of the Amazon, utilize slash-and-burn agriculture to overcome these limitations and enable them to push deep into what were previously rainforest environments. However, these are not rainforest dwellers, rather they are dwellers in cleared farmland that make forays into the rainforest. Up to 90% of the typical Yanamomo diet comes from farmed plants.\n\nSome action has been taken by suggesting fallow periods of the land allowing secondary forest to grow and replenish the soil. Beneficial practices like soil restoration and conservation can benefit the small farmer and allow better production on smaller parcels of land.\n\nThe tropics take a major role in reducing atmospheric carbon dioxide. The tropics (most notably the Amazon rainforest) are called carbon sinks. As major carbon reducers and carbon and soil methane storages, their destruction contributes to increasing global energy trapping, atmospheric gases. Climate change has been significantly contributed to by the destruction of the rainforests. A simulation was performed in which all rainforest in Africa were removed. The simulation showed an increase in atmospheric temperature by 2.5 to 5 degrees Celsius.\n\nEfforts to protect and conserve tropical rainforest habitats are diverse and widespread. Tropical rainforest conservation ranges from strict preservation of habitat to finding sustainable management techniques for people living in tropical rainforests. International policy has also introduced a market incentive program called Reducing Emissions from Deforestation and Forest Degradation (REDD) for companies and governments to outset their carbon emissions through financial investments into rainforest conservation.\n\n"}
{"id": "38610197", "url": "https://en.wikipedia.org/wiki?curid=38610197", "title": "Ukishima Solar Power Plant", "text": "Ukishima Solar Power Plant\n\nThe Ukishima Solar Power Plant (浮島太陽光発電所) is a 7 MW solar photovoltaic power station located on the waterfront in Kawasaki. It is the first solar plant built by Tepco, and was completed on August 10, 2011. In the first year of operation, it produced 9,453 MWh, a capacity factor of 0.15, which was about 30% greater than anticipated. An unusual feature of the plant is that the panels are mounted at a fixed angle of 10°, instead of the 30°, which would normally be considered optimal for this latitude.\n\n"}
{"id": "30284183", "url": "https://en.wikipedia.org/wiki?curid=30284183", "title": "Vinča Nuclear Institute", "text": "Vinča Nuclear Institute\n\nThe Vinča Institute of Nuclear Sciences is a nuclear physics research institution near Belgrade, Serbia. Since its founding, the institute has also conducted research in the fields in physics, chemistry and biology. The scholarly institute is part of the University of Belgrade.\n\nThe institute was established in 1948 as the Institute for Physics. Several different research groups started in the 1950s, and two research reactors were built.\n\nThe institute operates two research reactors; RA and RB. The research reactors were supplied by the USSR. The larger of the two reactors was rated at 6.5 MW and used Soviet-supplied 80% enriched uranium fuel.\n\nThe nuclear research program ended in 1968, while the reactors were switched off in 1984.\n\nOn 15 October 1958, there was a criticality accident at one of the research reactors. Six workers received large doses of radiation; one died shortly afterwards. The other five received the first ever bone marrow transplants in Europe.\n\nSix young researchers, all between 24 and 26 years, were conducting an experiment on the reactor, and the results were to be used by one student for his thesis. At some point, they smelled the strong scent of ozone. It took them 10 minutes to discover the origin of the ozone, but by that time they were already irradiated. The news was briefly broadcast by the state agency Tanjug, but the news on the incident were then suppressed. The reasons included the position of state in the atmosphere of the Cold war division, but also a fact that the state commission concluded that the incident was caused by the carelessness and indiscipline of the researchers. The patients were first treated in Belgrade, under care of dr Vasa Janković. Thanks to the personal connections of the Institute director Pavle Savić, who was collaborator of Irène and Frédéric Joliot-Curie, they were transferred to the Curie Institute in Paris.\n\nIn Paris, they were treated by oncologist Georges Mathé. Five researchers were heavily radiated: Rosanda Dangubić, Života Vranić, Radojko Maksić, Draško Grujić and Stijepo Hajduković, while Živorad Bogojević received a low dose of radiation. Mathé operated on all five of them, performing the first successful allogeneic bone marrow transplant ever performed on unrelated human beings. The donors were all French: Marcel Pabion, Albert Biron, Raymond Castanier and Odette Draghi, a mother of four young children. The fifth donor was Dr. , member of Mathé's team. On 11 November 1958, Maksić became the first man to receive a graft from an unrelated donor (Pabion). Out of five treated patients, only Vranić, died. Other recovered and returned to Belgrade to continue working in Vinča or other institutes. Several years later, Dangubić gave birth to a healthy baby girl.\n\nIn the 1980s, the waste was kept in the open. The waste was then transferred into two hangars, H1 and H2, while the ground was remediated. Until 1990, the waste from the entire country of Yugoslavia was stashed in Vinča. H2 also harbors the barrels with the depleted uranium and DU bullets, remnants of the ammunition collected on four locations in south Serbia after the 1999 NATO bombing of Serbia.\n\nIn 2009, it was reported that the nuclear fuel storage pool, containing large quantities of radioactive waste, was in poor condition.\n\nIn 2010, 2.5 tonnes of waste, including 13 kg of 80% enriched uranium, were transported from Vinča to a reprocessing facility at Mayak, Russia. This was the IAEA's largest ever technical cooperation project, and thousands of police protected the convoys.\n\nRemoval of the nuclear waste allows decommissioning of Vinča's remaining reactor to be completed.\n\nIn 2012 the Law on radiation protection and nuclear safety was adopted. It envisioned that in maximum 10 years, that is by 2022, the waste from Vinča has to be transferred to the permanent and safe depository location. A new and modern hangar, H3, was built in the meantime but due to the legal procedures and licensing problems it is still closed. Though, it is meant to be only a transition location where the processed waste from H1 is to be kept before being transported to the permanent location. Still, as of 2018, a large quantities of nuclear waste remain in the Institute, the permanent location hasn't been selected, while the waste is not being treated and processed at all.\n\nThe waste in Vinča is of low to mid-level radioactivity, which means it is potentially hazardous for the health and safety of wider area of Serbia, not just for Belgrade. Additionally, after removing all the radioactive waste, the institute can truly be transformed into the modern scientific-business park.\n\n"}
{"id": "22578564", "url": "https://en.wikipedia.org/wiki?curid=22578564", "title": "Widowmaker (forestry)", "text": "Widowmaker (forestry)\n\nIn forestry, the term widowmaker or fool killer describes a detached or broken limb or tree top and denotes the hazards that such features cause, being responsible for causing fatalities to forest workers. The U.S. Occupational Safety and Health Administration describes widowmakers as \"broken off limbs that are hanging freely in the tree to be felled or in the trees close by.\" \n\nWidowmakers are often caused by fungal growth over a sustained period. Other causes include damage from other falling trees or stress on a branch.\n\nWidowmakers may pose a risk to equipment or personnel working under or around the tree. They can become dislodged by wind or during tree felling, and are responsible for 11% of all fatal chainsaw accidents. The U.S. National Institute for Occupational Safety and Health (NIOSH) offers ways to eliminate risks by avoiding working beneath widowmakers, knocking them down, or pulling them down with a machine. \n\n"}
{"id": "719984", "url": "https://en.wikipedia.org/wiki?curid=719984", "title": "Yellowcake", "text": "Yellowcake\n\nYellowcake (also called urania) is a type of uranium concentrate powder obtained from leach solutions, in an intermediate step in the processing of uranium ores. It is a step in the processing of uranium after it has been mined but before fuel fabrication or uranium enrichment. Yellowcake concentrates are prepared by various extraction and refining methods, depending on the types of ores. Typically, yellowcakes are obtained through the milling and chemical processing of uranium ore forming a coarse powder that has a pungent odor, is insoluble in water, and contains about 80% uranium oxide, which melts at approximately 2880 °C.\n\nThe ore is first crushed to a fine powder by passing raw uranium ore through crushers and grinders to produce \"pulped\" ore. This is further processed with concentrated acid, alkaline, or peroxide solutions to leach out the uranium. Yellowcake is what remains after drying and filtering. The yellowcake produced by most modern mills is actually brown or black, not yellow; the name comes from the color and texture of the concentrates produced by early mining operations.\n\nInitially, the compounds formed in yellowcakes were not identified; in 1970, the U.S. Bureau of Mines still referred to yellowcakes as the final precipitate formed in the milling process and considered it to be ammonium diuranate or sodium diuranate. The compositions were variable and depended upon the leachant and subsequent precipitating conditions. The compounds identified in yellowcakes include uranyl hydroxide, uranyl sulfate, sodium para-uranate, and uranyl peroxide, along with various uranium oxides. Modern yellowcake typically contains 70% to 90% triuranium octoxide (UO) by weight. Other oxides such as uranium dioxide (UO) and uranium trioxide (UO) exist.\n\nYellowcake is produced by all countries in which uranium ore is mined.\n\nYellowcake is used in the preparation of uranium fuel for nuclear reactors, for which it is smelted into purified UO for use in fuel rods for pressurized heavy-water reactors and other systems that use natural unenriched uranium.\n\nPurified uranium can also be enriched into the isotope U-235. In this process, the uranium oxides are combined with fluorine to form uranium hexafluoride gas (UF). Next, that undergoes isotope separation through the process of gaseous diffusion, or in a gas centrifuge. This can produce low-enriched uranium containing up to 20% U-235 that is suitable for use in most large civilian electric-power reactors. With further processing, one obtains highly enriched uranium, containing 20% or more U-235, that is suitable for use in compact nuclear reactors—usually used to power naval warships and submarines. Further processing can yield weapons-grade uranium with U-235 levels usually above 90%, suitable for nuclear weapons.\n\nThe uranium in yellowcake is almost exclusively (>99%) U-238, with very low radioactivity. U-238 has an extremely long half-life, over 4 billion years, meaning that it emits radiation at a slow rate. This stage of processing is before the more radioactive U-235 is concentrated, so by definition, this stage of uranium has the same radioactivity as it did in nature when it was underground, as the proportions of isotopes are at their native relative concentration. Despite its long radioactive half-life, it has a shorter biological half-life, 15 days. Yellowcake is as radiologically harmless as natural potassium-carrying minerals or thorium-oxide mantles used in paraffin fuel lanterns.\n\n"}
{"id": "9886550", "url": "https://en.wikipedia.org/wiki?curid=9886550", "title": "Young Electric Sign Company", "text": "Young Electric Sign Company\n\nYESCO is a privately owned manufacturer of electric signs based in Salt Lake City, founded by Thomas Young in 1920. The company provides design, fabrication, installation and maintenance of signs.\n\nMany notable sign projects have been produced by YESCO, including the NBC Experience globe in New York City, the historic El Capitan Theatre and Wax Museum marquees in Hollywood, the Reno Arch, and in Las Vegas, Vegas Vic, the Fremont Street Experience, the Astrolabe in The Venetian, the Wynn Las Vegas resort sign, and the Aria Resort & Casino.\n\nThe company was created by Thomas Young on March 20, 1920. The young sign painter had left England just a decade earlier to immigrate with his family to Ogden, Utah. In the beginning, his shop specialized in coffin plates, gold leaf window lettering, lighted signs and painted advertisements. As the science of lighting and sign-making advanced, so did Tom Young’s signs.\n\nIn 1933, YESCO opened a branch office in the Apache Hotel in Las Vegas. The company erected their first neon sign in Las Vegas for the Boulder Club.\n\nYESCO – soon became recognized as a leader in the sign industry, tackling large and complex sign projects. For example, it erected the first neon spectacular sign in Las Vegas for the Boulder Club in the late ’30s, and in 1995 it completed the four-block-long Fremont Street Experience canopy in Las Vegas.\n\nYESCO continues to design, build, install and maintain signs and interior displays in areas. In recent years, YESCO has built a substantial outdoor digital media (billboard) division of its business. \n\nYESCO has approximately 1,000 employees, more than 40 offices, and operates three manufacturing plants featuring automated and custom equipment. Additional smaller manufacturing and service facilities are located through the United States and Canada.\n\nYESCO offers sign and lighting service franchises in states east of Colorado and throughout Canada.\n\nIn 2015 Young Electric Sign Company sold YESCO Electronics, a subsidiary company, to Samsung Electronics of America, Inc. \nNBC ushered in the millennium with a new YESCO “message globe” in its NBC Experience store, located at Rockefeller Center in New York City. The electronic sign quickly became recognized as one of the most distinctive electronic displays in the world.\n\nFrom the outside of the building, it looks like a brilliant illuminated globe. The 35’-diameter hemisphere is covered with thousands of full-color LEDs. Colorful video and special effects, along with animations provided by YESCO’s media services group, are displayed on the globe’s surface, telling the NBC story. When it was first turned on, it literally stopped traffic on West 49th Street.\n\nPerhaps the world’s most recognized electronic sign, Vegas Vic was designed by and built by YESCO. Upon its installation in 1951 over the Pioneer Club on historical Fremont Street, the 40'-tall electronic cowboy immediately became Las Vegas’s unofficial greeter.\n\nThe tall marquee features a high, wide, concave, double-faced LED message center with a first-of-its-kind “moving eraser.” Conceived by Steve Wynn, the massive eraser glides silently and smoothly up and down over the LED message center, appearing to change the graphics as it goes. The eraser weighs 62,000 pounds, and is counterbalanced by a 62,000-pound weight inside the sign.\n\nThe sign uses 4,377,600 LEDs and the eraser is powered by a motor at its base that runs a gear and cable system. The firm of FTSI engineered the 62,000-pound eraser’s movement, which is capable of speeds up to .The company has been instrumental in supporting the Neon Museum, which is dedicated to preserving the neon signs and associated artifacts of Las Vegas. \n\nSome of the retired signs include the sign for the Silver Slipper casino and Aladdin's lamp from the first version of Aladdin Casino. In 2004 the Binions Horseshoe sign Harrah's.\n\nYESCO installed the vaulted canopy arching above four blocks of Fremont Street.\n\nYESCO owns the Welcome to Fabulous Las Vegas sign.\n\nBorn in Sunderland, England, in 1895, Thomas Young was 15 years old when his family emigrated to Ogden, Utah. Hard-working and talented, the boy applied his passion to making signs, becoming a Master Sign Writer. He began by creating wall-lettering and gold-leaf window signs, working for the Electric Service Company and the Redfield-King Sign Company in Ogden.\n\nYoung married Elmina Carlisle in 1916. Four years later, in 1920, he founded his own sign company: Thomas Young Sign Company, which specialized in coffin plates, gold window lettering, lighted signs and painted advertisements.\n\nIn 1932 Young expanded his business to Las Vegas, and within two years purchased the Ogden Armory for $12,000 to expand production capacity. He also started a branch in Salt Lake City in that year.\n\nYoung was elected president of the National Sign Association in 1936, serving for two terms. A year later, in 1937, he moved his family and YESCO headquarters to Salt Lake City, Utah, and continued expanding the business.\n\nIn 1969 Young turned over the reins of company leadership to his son who currently serves as the Chairman of the Board. The company is now managed by third and fourth generations of the Young family.\n\nSome of YESCO's most prominent signage designers have included:\n\n"}
