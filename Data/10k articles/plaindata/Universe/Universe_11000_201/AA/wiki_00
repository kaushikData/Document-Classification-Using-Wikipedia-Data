{"id": "38644338", "url": "https://en.wikipedia.org/wiki?curid=38644338", "title": "A Community Speaks", "text": "A Community Speaks\n\nA Community Speaks is a 2004 feature-length documentary about modern-day land stewardship. The documentary was produced and directed by Bruce Campbell and his wife, Ida Gearon.\n\nThe documentary was shot in Oregon and was Campbell's second directorial debut, behind 2002's short documentary, \"Fanalysis\".\n\n\n"}
{"id": "57923863", "url": "https://en.wikipedia.org/wiki?curid=57923863", "title": "BLC Leather Technology Centre", "text": "BLC Leather Technology Centre\n\nBLC Leather Technology Centre or BLC is a testing, auditing and consulting business that specialises in chemicals, leather, footwear and other associated products and related materials, and is based in Northamptonshire . An additional specialism is the safe use and management of chemicals in the supply chain; a concept from which BLC developed the ZDHC approved BLC Chem-MAP certification process for chemicals, chemical companies, brands and their suppliers. \n\nA significant item of note is that BLC is the official facilitator of the Leather Working Group (LWG) which is a significant environmental stewardship programme within the leather industry. The LWG has over 600 member companies that includes over 100 major clothing and footwear brands.\n\nIn March 2018, BLC was acquired by Eurofins Scientific of Belgium who now have a majority share holding in the company..\n\n\n"}
{"id": "4964", "url": "https://en.wikipedia.org/wiki?curid=4964", "title": "Bernoulli number", "text": "Bernoulli number\n\nIn mathematics, the Bernoulli numbers are a sequence of rational numbers which occur frequently in number theory. The values of the first 20 Bernoulli numbers are given in the adjacent table. For every even other than 0, is negative if is divisible by 4 and positive otherwise. For every odd other than 1, .\n\nThe superscript ± used in this article designates the two sign conventions for Bernoulli numbers. Only the term is affected: \n\nIn the formulas below, one can switch from one sign convention to the other with the relation formula_1.\n\nThe Bernoulli numbers are special values of the Bernoulli polynomials formula_2, with formula_3 and formula_4.\n\nSince for all odd , and many formulas only involve even-index Bernoulli numbers, some authors write \"\" to mean . This article does not follow this notation.\n\nThe Bernoulli numbers appear in the Taylor series expansions of the tangent and hyperbolic tangent functions, in Faulhaber's formula for the sum of powers of the first positive integers, in the Euler–Maclaurin formula, and in expressions for certain values of the Riemann zeta function.\n\nThe Bernoulli numbers were discovered around the same time by the Swiss mathematician Jacob Bernoulli, after whom they are named, and independently by Japanese mathematician Seki Kōwa. Seki's discovery was posthumously published in 1712 in his work \"Katsuyo Sampo\"; Bernoulli's, also posthumously, in his \"Ars Conjectandi\" of 1713. Ada Lovelace's note G on the Analytical Engine from 1842 describes an algorithm for generating Bernoulli numbers with Babbage's machine. As a result, the Bernoulli numbers have the distinction of being the subject of the first published complex computer program.\n\nThe Bernoulli numbers are rooted in the early history of the computation of sums of integer powers, which have been of interest to mathematicians since antiquity.\n\nMethods to calculate the sum of the first positive integers, the sum of the squares and of the cubes of the first positive integers were known, but there were no real 'formulas', only descriptions given entirely in words. Among the great mathematicians of antiquity to consider this problem were Pythagoras (c. 572–497 BCE, Greece), Archimedes (287–212 BCE, Italy), Aryabhata (b. 476, India), Abu Bakr al-Karaji (d. 1019, Persia) and Abu Ali al-Hasan ibn al-Hasan ibn al-Haytham (965–1039, Iraq).\n\nDuring the late sixteenth and early seventeenth centuries mathematicians made significant progress. In the West Thomas Harriot (1560–1621) of England, Johann Faulhaber (1580–1635) of Germany, Pierre de Fermat (1601–1665) and fellow French mathematician Blaise Pascal (1623–1662) all played important roles.\n\nThomas Harriot seems to have been the first to derive and write formulas for sums of powers using symbolic notation, but even he calculated only up to the sum of the fourth powers. Johann Faulhaber gave formulas for sums of powers up to the 17th power in his 1631 \"Academia Algebrae\", far higher than anyone before him, but he did not give a general formula.\n\nBlaise Pascal in 1654 proved \"Pascal's identity\" relating the sums of the th powers of the first positive integers for .\n\nThe Swiss mathematician Jakob Bernoulli (1654–1705) was the first to realize the existence of a single sequence of constants which provide a uniform formula for all sums of powers .\n\nThe joy Bernoulli experienced when he hit upon the pattern needed to compute quickly and easily the coefficients of his formula for the sum of the th powers for any positive integer can be seen from his comment. He wrote:\n\nBernoulli's result was published posthumously in \"Ars Conjectandi\" in 1713. Seki Kōwa independently discovered the Bernoulli numbers and his result was published a year earlier, also posthumously, in 1712. However, Seki did not present his method as a formula based on a sequence of constants.\n\nBernoulli's formula for sums of powers is the most useful and generalizable formulation to date. The coefficients in Bernoulli's formula are now called Bernoulli numbers, following a suggestion of Abraham de Moivre.\n\nBernoulli's formula is sometimes called Faulhaber's formula after Johann Faulhaber who found remarkable ways to calculate sum of powers but never stated Bernoulli's formula. To call Bernoulli's formula Faulhaber's formula does injustice to Bernoulli and simultaneously hides the genius of Faulhaber as Faulhaber's formula is in fact more efficient than Bernoulli's formula. According to Knuth a rigorous proof of Faulhaber's formula was first published by Carl Jacobi in 1834 . Knuth's in-depth study of Faulhaber's formula concludes (the nonstandard notation on the LHS is explained further on):\n\nThe Bernoulli numbers (n)/(n) were introduced by Jakob Bernoulli in the book \"Ars Conjectandi\" published posthumously in 1713 page 97. The main formula can be seen in the second half of the corresponding facsimile. The constant coefficients denoted , , and by Bernoulli are mapped to the notation which is now prevalent as , , , . The expression means – the small dots are used as grouping symbols. Using today's terminology these expressions are falling factorial powers . The factorial notation as a shortcut for was not introduced until 100 years later. The integral symbol on the left hand side goes back to Gottfried Wilhelm Leibniz in 1675 who used it as a long letter for \"summa\" (sum). (The \"Mathematics Genealogy Project\"\nshows Leibniz as the academic advisor of Jakob Bernoulli. See also the \"Earliest Uses of Symbols of Calculus\".) The letter on the left hand side is not an index of summation but gives the upper limit of the range of summation which is to be understood as . Putting things together, for positive , today a mathematician is likely to write Bernoulli's formula as:\n\nThis formula suggests setting when switching from the so-called 'archaic' enumeration which uses only the even indices 2, 4, 6… to the modern form (more on different conventions in the next paragraph). Most striking in this context is the fact that the falling factorial has for the value . \nThus Bernoulli's formula can be written\n\nif , recapturing the value Bernoulli gave to the coefficient at that position.\n\nNote that the formula for formula_9 in the first half contains an error at the last term: it must be formula_10 instead of formula_11.\n\nMany characterizations of the Bernoulli numbers have been found in the last 300 years, and each could be used to introduce these numbers. Here only three of the most useful ones are mentioned:\n\n\nFor the proof of the equivalence of the three approaches see or .\n\nThe Bernoulli numbers obey the sum formulas \nwhere formula_13 and denotes the Kronecker delta. Solving for formula_14 gives the recursive formulas\n\nIn 1893 Louis Saalschütz listed a total of 38 explicit formulas for the Bernoulli numbers , usually giving some reference in the older literature. One of them is:\n\nThe exponential generating functions are\n\nThe (normal) generating function\n\nis an asymptotic series. It contains the trigamma function .\n\nThe Bernoulli numbers can be expressed in terms of the Riemann zeta function:\n\nHere the argument of the zeta function is 0 or negative.\n\nBy means of the zeta functional equation and the gamma reflection formula the following relation can be obtained:\n\nNow the argument of the zeta function is positive.\n\nIt then follows from () and Stirling's formula that\n\nIn some applications it is useful to be able to compute the Bernoulli numbers through modulo , where is a prime; for example to test whether Vandiver's conjecture holds for , or even just to determine whether is an irregular prime. It is not feasible to carry out such a computation using the above recursive formulae, since at least (a constant multiple of) arithmetic operations would be required. Fortunately, faster methods have been developed which require only operations (see big notation).\n\nDavid Harvey describes an algorithm for computing Bernoulli numbers by computing modulo for many small primes , and then reconstructing via the Chinese remainder theorem. Harvey writes that the asymptotic time complexity of this algorithm is and claims that this implementation is significantly faster than implementations based on other methods. Using this implementation Harvey computed for . Harvey's implementation has been included in SageMath since version 3.1. Prior to that, Bernd Kellner computed to full precision for in December 2002 and Oleksandr Pavlyk for with Mathematica in April 2008.\n\nArguably the most important application of the Bernoulli number in mathematics is their use in the Euler–Maclaurin formula. Assuming that is a sufficiently often differentiable function the Euler–Maclaurin formula can be written as \n\nThis formulation assumes the convention . Using the convention the formula becomes\n\nHere (i.e. the zeroth-order derivative of a is just ). Moreover, let denote an antiderivative of . By the fundamental theorem of calculus,\n\nThus the last formula can be further simplified to the following succinct form of the Euler–Maclaurin formula\n\nThis form is for example the source for the important Euler–Maclaurin expansion of the zeta function\n\nHere denotes the rising factorial power.\n\nBernoulli numbers are also frequently used in other kinds of asymptotic expansions. The following example is the classical Poincaré-type asymptotic expansion of the digamma function .\n\nBernoulli numbers feature prominently in the closed form expression of the sum of the th powers of the first positive integers. For define\n\nThis expression can always be rewritten as a polynomial in of degree . The coefficients of these polynomials are related to the Bernoulli numbers by Bernoulli's formula:\nwhere denotes the binomial coefficient.\n\nFor example, taking to be 1 gives the triangular numbers .\n\nTaking to be 2 gives the square pyramidal numbers .\n\nSome authors use the alternate convention for Bernoulli numbers and state Bernoulli's formula in this way:\n\nBernoulli's formula is sometimes called Faulhaber's formula after Johann Faulhaber who also found remarkable ways to calculate sums of powers.\n\nFaulhaber's formula was generalized by V. Guo and J. Zeng to a -analog .\n\nThe Bernoulli numbers appear in the Taylor series expansion of many trigonometric functions and hyperbolic functions.\n\n\n\n\nThe Bernoulli numbers appear in the following Laurent series:\n\nDigamma function: formula_36\n\nThe Kervaire–Milnor's formula for the order of the cyclic group of diffeomorphism classes of exotic -spheres which bound parallelizable manifolds involves Bernoulli numbers. Let be the number of such exotic spheres for , then\n\nThe Hirzebruch signature theorem for the genus of a smooth oriented closed manifold of dimension 4\"n\" also involves Bernoulli numbers.\n\nThe connection of the Bernoulli number to various kinds of combinatorial numbers is based on the classical theory of finite differences and on the combinatorial interpretation of the Bernoulli numbers as an instance of a fundamental combinatorial principle, the inclusion–exclusion principle.\n\nThe definition to proceed with was developed by Julius Worpitzky in 1883. Besides elementary arithmetic only the factorial function and the power function is employed. The signless Worpitzky numbers are defined as\n\nThey can also be expressed through the Stirling numbers of the second kind\n\nA Bernoulli number is then introduced as an inclusion–exclusion sum of Worpitzky numbers weighted by the harmonic sequence 1, , , …\n\nThis representation has .\n\nConsider the sequence , . From Worpitzky's numbers , applied to is identical to the Akiyama–Tanigawa transform applied to (see Connection with Stirling numbers of the first kind). This can be seen via the table:\n\nThe first row represents .\n\nHence for the second fractional Euler numbers () / ():\n\nA second formula representing the Bernoulli numbers by the Worpitzky numbers is for \n\nThe simplified second Worpitzky's representation of the second Bernoulli numbers is:\n\nwhich links the second Bernoulli numbers to the second fractional Euler numbers. The beginning is:\n\nThe numerators of the first parentheses are (see Connection with Stirling numbers of the first kind).\n\nIf denotes Stirling numbers of the second kind then one has:\n\nwhere denotes the falling factorial.\n\nIf one defines the Bernoulli polynomials as:\n\nwhere for are the Bernoulli numbers.\n\nThen after the following property of binomial coefficient:\n\none has,\n\nOne also has following for Bernoulli polynomials,\n\nThe coefficient of in is .\n\nComparing the coefficient of in the two expressions of Bernoulli polynomials, one has:\n\n(resulting in ) which is an explicit formula for Bernoulli numbers and can be used to prove Von-Staudt Clausen theorem.\n\nThe two main formulas relating the unsigned Stirling numbers of the first kind to the Bernoulli numbers (with ) are\n\nand the inversion of this sum (for , )\n\nHere the number are the rational Akiyama–Tanigawa numbers, the first few of which are displayed in the following table.\n\nThe Akiyama–Tanigawa numbers satisfy a simple recurrence relation which can be exploited to iteratively compute the Bernoulli numbers. This leads to the algorithm shown in the section 'algorithmic description' above. See /.\n\nAn \"autosequence\" is a sequence which has its inverse binomial transform equal to the signed sequence. If the main diagonal is zeroes = , the autosequence is of the first kind. Example: , the Fibonacci numbers. If the main diagonal is the first upper diagonal multiplied by 2, it is of the second kind. Example: /, the second Bernoulli numbers (see ). The Akiyama–Tanigawa transform applied to = 1/ leads to (\"n\") / (\"n\" + 1). Hence:\n\nSee and . () / () are the second (fractional) Euler numbers and an autosequence of the second kind.\n\nAlso valuable for / (see Connection with Worpitzky numbers).\n\nThere are formulas connecting Pascal's triangle to Bernoulli numbers \nwhere formula_51 is the determinant of a n-by-n square matrix part of Pascal’s triangle whose elements are: formula_52\n\nExample:\n\nThere are formulas connecting Eulerian numbers to Bernoulli numbers:\n\nBoth formulae are valid for if is set to . If is set to − they are valid only for and respectively.\n\nThe Stirling polynomials are related to the Bernoulli numbers by . S. C. Woon described an algorithm to compute as a binary tree:\n\nWoon's recursive algorithm (for ) starts by assigning to the root node . Given a node of the tree, the left child of the node is and the right child . A node is written as in the initial part of the tree represented above with ± denoting the sign of .\n\nGiven a node the factorial of is defined as\n\nRestricted to the nodes of a fixed tree-level the sum of is , thus\n\nFor example:\n\nThe integral\nhas as special values for .\n\nFor example, and . Here, is the Riemann zeta function, and is the imaginary unit. Leonhard Euler (\"Opera Omnia\", Ser. 1, Vol. 10, p. 351) considered these numbers and calculated\n\nThe Euler numbers are a sequence of integers intimately connected with the Bernoulli numbers. Comparing the\nasymptotic expansions of the Bernoulli and the Euler numbers shows that the Euler numbers are in magnitude approximately times larger than the Bernoulli numbers . In consequence:\n\nThis asymptotic equation reveals that lies in the common root of both the Bernoulli and the Euler numbers. In fact could be computed from these rational approximations.\n\nBernoulli numbers can be expressed through the Euler numbers and vice versa. Since, for odd , (with the exception ), it suffices to consider the case when is even.\n\nThese conversion formulas express an inverse relation between the Bernoulli and the Euler numbers. But more important, there is a deep arithmetic root common to both kinds of numbers, which can be expressed through a more fundamental sequence of numbers, also closely tied to . These numbers are defined for as\n\nand by convention . The magic of these numbers lies in the fact that they turn out to be rational numbers. This was first proved by Leonhard Euler in a landmark paper \"‘De summis serierum reciprocarum’\" (On the sums of series of reciprocals) and has fascinated mathematicians ever since. The first few of these numbers are\n\nThese are the coefficients in the expansion of .\n\nThe Bernoulli numbers and Euler numbers are best understood as \"special views\" of these numbers, selected from the sequence and scaled for use in special applications.\n\nThe expression [ even] has the value 1 if is even and 0 otherwise (Iverson bracket).\n\nThese identities show that the quotient of Bernoulli and Euler numbers at the beginning of this section is just the special case of when is even. The are rational approximations to and two successive terms always enclose the true value of . Beginning with the sequence starts ( / ):\n\nThese rational numbers also appear in the last paragraph of Euler's paper cited above.\n\nConsider the Akiyama–Tanigawa transform for the sequence () / ():\n\nFrom the second, the numerators of the first column are the denominators of Euler's formula. The first column is − × .\n\nThe sequence \"S\" has another unexpected yet important property: The denominators of \"S\" divide the factorial (\"n\" − 1)<nowiki>!</nowiki>. In other words: the numbers \"T\" = \"S\"(\"n\" − 1)!, sometimes called Euler zigzag numbers, are integers.\n\nThus the above representations of the Bernoulli and Euler numbers can be rewritten in terms of this sequence as\n\nThese identities make it easy to compute the Bernoulli and Euler numbers: the Euler numbers are given immediately by and the Bernoulli numbers are obtained from by some easy shifting, avoiding rational arithmetic.\n\nWhat remains is to find a convenient way to compute the numbers . However, already in 1877 Philipp Ludwig von Seidel published an ingenious algorithm which makes it extremely simple to calculate .\n\n\nSeidel's algorithm is in fact much more general (see the exposition of Dominique Dumont ) and was rediscovered several times thereafter.\n\nSimilar to Seidel's approach D. E. Knuth and T. J. Buckholtz gave a recurrence equation for the numbers and recommended this method for computing and ‘on electronic computers using only simple operations on integers’.\n\nV. I. Arnold rediscovered Seidel's algorithm in and later Millar, Sloane and Young popularized Seidel's algorithm under the name boustrophedon transform.\n\nTriangular form:\n\nOnly , with one 1, and , with two 1s, are in the OEIS.\n\nDistribution with a supplementary 1 and one 0 in the following rows:\n\nThis is , a signed version of . The main andiagonal is . The main diagonal is . The central column is . Row sums: 1, 1, −2, −5, 16, 61…. See . See the array beginning with 1, 1, 0, −2, 0, 16, 0 below.\n\nThe Akiyama–Tanigawa algorithm applied to () / () yields:\n\n1. The first column is . Its binomial transform leads to:\n\nThe first row of this array is . The absolute values of the increasing antidiagonals are . The sum of the antidiagonals is \n\n2. The second column is . Its binomial transform yields:\n\nThe first row of this array is . The absolute values of the second bisection are the double of the absolute values of the first bisection.\n\nConsider the Akiyama-Tanigawa algorithm applied to () / ( () = abs( ()) + 1 = .\n\nThe first column whose the absolute values are could be the numerator of a trigonometric function.\n\nThe first two upper diagonals are =  × . The sum of the antidiagonals is = 2 × (\"n\" + 1).\n\n− is an autosequence of the second kind, like for instance / . Hence the array:\n\nThe main diagonal, here , is the double of the first upper one, here . The sum of the antidiagonals is = 2 × (1). Note that  −  = 2 × .\n\nAround 1880, three years after the publication of Seidel's algorithm, Désiré André proved a now classic result of combinatorial analysis & . Looking at the first terms of the Taylor expansion of the trigonometric functions\n\nThe coefficients are the Euler numbers of odd and even index, respectively. In consequence the ordinary expansion of has as coefficients the rational numbers .\n\nAndré then succeeded by means of a recurrence argument to show that the alternating permutations of odd size are enumerated by the Euler numbers of odd index (also called tangent numbers) and the alternating permutations of even size by the Euler numbers of even index (also called secant numbers).\n\nThe arithmetic mean of the first and the second Bernoulli numbers are the associate Bernoulli numbers: \n, , , , , / . Via the second row of its inverse Akiyama–Tanigawa transform , they lead to Balmer series / .\n\nThe Akiyama–Tanigawa algorithm applied to () / () leads to the Bernoulli numbers / , / , or without , named intrinsic Bernoulli numbers .\n\nHence another link between the intrinsic Bernoulli numbers and the Balmer series via ().\n\nThe terms of the first row are f(n) = . 2, f(n) is an autosequence of the second kind. 3/2, f(n) leads by its inverse binomial transform to 3/2 −1/2 1/3 −1/4 1/5 ... = 1/2 + log 2.\n\nConsider g(n) = 1/2 - 1 / (n+2) = 0, 1/6, 1/4, 3/10, 1/3. The Akiyama-Tanagiwa transforms gives:\n\n0, g(n), is an autosequence of the second kind.\n\nEuler () / () without the second term () are the fractional intrinsic Euler numbers The corresponding Akiyama transform is:\n\nThe first line is . preceded by a zero is an autosequence of the first kind. It is linked to the Oresme numbers. The numerators of the second line are preceded by 0. The difference table is:\n\nThe Bernoulli numbers for \"n\" = 0, 1, 2, 3, ... are\n\nThis is where is the th coefficient of (/).\n\nSee . The following fractional numbers are an autosequence of the first kind.\n\nApply = ()/():\n\nThe rows are alternatively autosequences of the first and of the second kind. The second row is /. For the third row, see .\n\nThe first column is from Mersenne primes, see . For the second column see .\n\nConsider the triangle () = =\n\nThis is Pascal's triangle bordered by zeroes. The antidiagonals' sums are , the Fibonacci numbers. Two elementary transforms yield the array ASPEC0, a companion to ASPEC in .\n\nMultiplying the SBD array in by ASPEC0, we have by row sums /:\n\nThis triangle is unreduced.\n\nThe Bernoulli numbers can be expressed in terms of the Riemann zeta function as for integers provided for and the expression is understood as the limiting value and the convention is used. This intimately relates them to the values of the zeta function at negative integers. As such, they could be expected to have and do have deep arithmetical properties. For example, the Agoh–Giuga conjecture postulates that is a prime number if and only if is congruent to −1 modulo . Divisibility properties of the Bernoulli numbers are related to the ideal class groups of cyclotomic fields by a theorem of Kummer and its strengthening in the Herbrand-Ribet theorem, and to class numbers of real quadratic fields by Ankeny–Artin–Chowla.\n\nThe Bernoulli numbers are related to Fermat's Last Theorem (FLT) by Kummer's theorem , which says:\n\nPrime numbers with this property are called regular primes. Another classical result of Kummer are the following congruences.\n\nA generalization of these congruences goes by the name of -adic continuity.\n\nIf , and are positive integers such that and are not divisible by and , then\n\nSince , this can also be written\n\nwhere and , so that and are nonpositive and not congruent to 1 modulo . This tells us that the Riemann zeta function, with taken out of the Euler product formula, is continuous in the -adic numbers on odd negative integers congruent modulo to a particular , and so can be extended to a continuous function for all -adic integers , the -adic zeta function.\n\nThe following relations, due to Ramanujan, provide a method for calculating Bernoulli numbers that is more efficient than the one given by their original recursive definition:\n\nThe von Staudt–Clausen theorem was given by Karl Georg Christian von Staudt and Thomas Clausen independently in 1840. The theorem states that for every ,\nis an integer. The sum extends over all primes for which divides .\n\nA consequence of this is that the denominator of is given by the product of all primes for which divides . In particular, these denominators are square-free and divisible by 6.\n\nThe sum\n\ncan be evaluated for negative values of the index . Doing so will show that it is an odd function for even values of , which implies that the sum has only terms of odd index. This and the formula for the Bernoulli sum imply that is 0 for even and ; and that the term for is cancelled by the subtraction. The von Staudt–Clausen theorem combined with Worpitzky's representation also gives a combinatorial answer to this question (valid for \"n\" > 1).\n\nFrom the von Staudt–Clausen theorem it is known that for odd the number is an integer. This seems trivial if one knows beforehand that the integer in question is zero. However, by applying Worpitzky's representation one gets\n\nThe generalized Bernoulli numbers are certain algebraic numbers, defined similarly to the Bernoulli numbers, that are related to special values of Dirichlet -functions in the same way that Bernoulli numbers are related to special values of the Riemann zeta function.\n\nLet be a Dirichlet character modulo . The generalized Bernoulli numbers attached to are defined by\n\nApart from the exceptional , we have, for any Dirichlet character , that if .\n\nGeneralizing the relation between Bernoulli numbers and values of the Riemann zeta function at non-positive integers, one has the for all integers :\n\nwhere is the Dirichlet -function of .\n\nChoosing or results in the Bernoulli number identity in one or another convention.\n\nand\n\n\n\n"}
{"id": "29763781", "url": "https://en.wikipedia.org/wiki?curid=29763781", "title": "Blythe Photovoltaic Power Plant", "text": "Blythe Photovoltaic Power Plant\n\nThe 21 megawatt Blythe Photovoltaic Power Plant is a photovoltaic (PV) solar project in California. It is located in Blythe, California, in Riverside County about east of Los Angeles. Commercial operation began in December 2009. Electricity generated by the power plant is being sold to Southern California Edison under a 20-year power purchase agreement. Another 20 MW plant called NRG Solar Blythe II came online in April 2017.\n\n"}
{"id": "37047010", "url": "https://en.wikipedia.org/wiki?curid=37047010", "title": "Boiler blowdown", "text": "Boiler blowdown\n\nBoiler blowdown is water intentionally wasted from a boiler to avoid concentration of impurities during continuing evaporation of steam. The water is blown out of the boiler with some force by steam pressure within the boiler. Bottom blowdown used with early boilers caused abrupt downward adjustment of boiler water level and was customarily expelled downward to avoid the safety hazard of showering hot water on nearby individuals.\nA steam boiler evaporates liquid water to form steam, or gaseous water; and requires frequent replenishment of boiler feedwater for the continuous production of steam required by most boiler applications. Water is a capable solvent, and will dissolve small amounts of solids from piping and containers including the boiler. Continuing evaporation of steam concentrates dissolved impurities until they reach levels potentially damaging to steam production within the boiler. Without blowdown, impurities would reach saturation levels and begin to precipitate within the boiler. Impurity concentrations are highest where steam is being produced near heat exchange surfaces. Precipitation would be expected to occur in the form of scale deposits on those heat exchange surfaces. Scale deposits thermally insulate heat exchange surfaces initially decreasing the rate of steam generation, and potentially causing boiler metals to reach failure temperatures.\n\nSurface blowdown continuously bleeds off a low volume of water from within the boiler as a means of ridding the boiler of dissolved impurities. It is most effective to remove water with the highest level of impurities; and such water is found where steam separates in the steam drum at the top of the boiler. There are two types of surface blowdown fittings. The simpler is a pipe entering the steam drum at the normal water level. A more sophisticated skimmer arrangement attaches a swivel joint to the pipe for a short length of pipe suspended by a float. The skimmer may be more effective at removing floating oil which might otherwise cause foaming. Surface blowdown is normally fed to a flash tank and heat exchanger for heat recovery. The flashed steam can be used in the feedwater heater. These energy economies are seldom practical with infrequent bottom blowdown. Surface blowdown offers the additional advantage of steady-state operating conditions.\n\nSome boiler water treatments cause precipitation of impurities as insoluble particles anticipating those particles will settle to the bottom of the boiler before they become entrained in water circulating past the heat exchange surfaces. These water treatments often include compounds forming a sludge to entrap such particles; and boilers intended for such water treatment include a structure called a mud drum at the lowest part of the boiler. Bottom blowdown involves periodically opening valves in the mud drum to allow boiler pressure to force accumulated sludge out of the boiler. Similar blowdown connections at the bottom of water wall headers are blown down less frequently. Several short blowdown events remove sludge more effectively than a single continuous blowdown. Shorter blowdown events cause less significant changes in boiler water level, and are safer during periods of high steam demand.\n\nBottom blowdown piping drains the lowest parts of the boiler so it can be used to drain the boiler for servicing. Bottom blowdown piping must be of a diameter large enough to minimize the risk of being plugged with baked sludge. Modern boilers discharge bottom blowdown to a blowoff tank where the blowdown can flash and vent steam upwards without entraining water which might cause burns. A pipe near the bottom of the blowoff tank maintains a water level below the blowdown entry point and allows cooler water remaining from earlier blowdown events to drain from the tank first. Two bottom blowdown valves are often used in series to minimize erosion. One valve serves as the sealing valve, and the other as the blowdown valve. The sealing valve is customarily opened first and closed last. Both are opened rapidly and fully to minimize erosion on the seat and disk faces. Care is taken to avoid trapping scale or rust particles within the valve by reopening a valve to flush the particles through if resistance is encountered when attempting to close it. Bottom blowdown valves are often rebuilt or replaced whenever the boiler is taken out of service for maintenance.\n\n"}
{"id": "46183", "url": "https://en.wikipedia.org/wiki?curid=46183", "title": "Butter", "text": "Butter\n\nButter is a dairy product containing up to 80% butterfat (in commercial products) which is solid when chilled and at room temperature in some regions, and liquid when warmed. It is made by churning fresh or fermented cream or milk to separate the butterfat from the buttermilk. It is generally used as a spread on plain or toasted bread products and a condiment on cooked vegetables, as well as in cooking, such as baking, sauce making, and pan frying. Butter consists of butterfat, milk proteins and water, and often added salt.\n\nMost frequently made from cow's milk, butter can also be manufactured from the milk of other mammals, including sheep, goats, buffalo, and yaks. Salt (such as dairy salt), flavorings (such as garlic) and preservatives are sometimes added to butter. Rendering butter, removing the water and milk solids, produces clarified butter or \"ghee\", which is almost entirely butterfat.\n\nButter is a water-in-oil emulsion resulting from an inversion of the cream, where the milk proteins are the emulsifiers. Butter remains a firm solid when refrigerated, but softens to a spreadable consistency at room temperature, and melts to a thin liquid consistency at . The density of butter is 911 grams per Litre (0.950 lb per US pint). It generally has a pale yellow color, but varies from deep yellow to nearly white. Its natural, unmodified color is dependent on the source animal's feed and genetics, but the commercial manufacturing process commonly manipulates the color with food colorings like annatto or carotene.\n\nThe word \"butter\" derives (via Germanic languages) from the Latin \"butyrum\", which is the latinisation of the Greek βούτυρον (\"bouturon\"). This may be a compound of βοῦς (\"bous\"), \"ox, cow\" + τυρός (\"turos\"), \"cheese\", that is \"cow-cheese\". The word \"turos\" (\"cheese\") is attested in Mycenaean Greek. The unlatinized form is found in the name butyric acid, a compound found in rancid butter and dairy products such as Parmesan cheese.\n\nIn general use, the term \"butter\" refers to the spread dairy product when unqualified by other descriptors. The word commonly is used to describe puréed vegetable or seed and nut products such as peanut butter and almond butter. It is often applied to spread fruit products such as apple butter. Fats such as cocoa butter and shea butter that remain solid at room temperature are also known as \"butters\". Non-dairy items that have a dairy-butter consistency may use \"butter\" to call that consistency to mind, including food items such as maple butter and witch's butter and nonfood items such as baby bottom butter, hyena butter, and rock butter.\n\nUnhomogenized milk and cream contain butterfat in microscopic globules. These globules are surrounded by membranes made of phospholipids (fatty acid emulsifiers) and proteins, which prevent the fat in milk from pooling together into a single mass. Butter is produced by agitating cream, which damages these membranes and allows the milk fats to conjoin, separating from the other parts of the cream. Variations in the production method will create butters with different consistencies, mostly due to the butterfat composition in the finished product. Butter contains fat in three separate forms: free butterfat, butterfat crystals, and undamaged fat globules. In the finished product, different proportions of these forms result in different consistencies within the butter; butters with many crystals are harder than butters dominated by free fats.\n\nChurning produces small butter grains floating in the water-based portion of the cream. This watery liquid is called buttermilk—although the buttermilk most common today is instead a directly fermented skimmed milk. The buttermilk is drained off; sometimes more buttermilk is removed by rinsing the grains with water. Then the grains are \"worked\": pressed and kneaded together. When prepared manually, this is done using wooden boards called scotch hands. This consolidates the butter into a solid mass and breaks up embedded pockets of buttermilk or water into tiny droplets.\n\nCommercial butter is about 80% butterfat and 15% water; traditionally made butter may have as little as 65% fat and 30% water. Butterfat is a mixture of triglyceride, a triester derived from glycerol and three of any of several fatty acid groups. Butter becomes rancid when these chains break down into smaller components, like butyric acid and diacetyl. The density of butter is 0.911 g/cm (0.527 oz/in), about the same as ice.\n\nIn some countries, butter is given a grade before commercial distribution.\n\nBefore modern factory butter making, cream was usually collected from several milkings and was therefore several days old and somewhat fermented by the time it was made into butter. Butter made from a fermented cream is known as cultured butter. During fermentation, the cream naturally sours as bacteria convert milk sugars into lactic acid. The fermentation process produces additional aroma compounds, including diacetyl, which makes for a fuller-flavored and more \"buttery\" tasting product. Today, cultured butter is usually made from pasteurized cream whose fermentation is produced by the introduction of \"Lactococcus\" and \"Leuconostoc\" bacteria.\nAnother method for producing cultured butter, developed in the early 1970s, is to produce butter from fresh cream and then incorporate bacterial cultures and lactic acid. Using this method, the cultured butter flavor grows as the butter is aged in cold storage. For manufacturers, this method is more efficient, since aging the cream used to make butter takes significantly more space than simply storing the finished butter product. A method to make an artificial simulation of cultured butter is to add lactic acid and flavor compounds directly to the fresh-cream butter; while this more efficient process is claimed to simulate the taste of cultured butter, the product produced is not cultured but is instead flavored.\n\nDairy products are often pasteurized during production to kill pathogenic bacteria and other microbes. Butter made from pasteurized fresh cream is called sweet cream butter. Production of sweet cream butter first became common in the 19th century, with the development of refrigeration and the mechanical cream separator. Butter made from fresh or cultured unpasteurized cream is called raw cream butter. While butter made from pasteurized cream may keep for several months, raw cream butter has a shelf life of roughly ten days.\n\nThroughout continental Europe, cultured butter is preferred, while sweet cream butter dominates in the United States and the United Kingdom. Cultured butter is sometimes labeled \"European-style\" butter in the United States, although cultured butter is made and sold by some, especially Amish, dairies. Commercial raw cream butter is virtually unheard-of in the United States. Raw cream butter is generally only found made at home by consumers who have purchased raw whole milk directly from dairy farmers, skimmed the cream themselves, and made butter with it. It is rare in Europe as well.\n\nSeveral \"spreadable\" butters have been developed. These remain softer at colder temperatures and are therefore easier to use directly out of refrigeration. Some methods modify the makeup of the butter's fat through chemical manipulation of the finished product, some manipulate the cattle's feed, and some incorporate vegetable oil into the butter. \"Whipped\" butter, another product designed to be more spreadable, is aerated by incorporating nitrogen gas—normal air is not used to avoid oxidation and rancidity.\n\nAll categories of butter are sold in both salted and unsalted forms. Either granular salt or a strong brine are added to salted butter during processing. In addition to enhanced flavor, the addition of salt acts as a preservative. The amount of butterfat in the finished product is a vital aspect of production. In the United States, products sold as \"butter\" must contain at least 80% butterfat. In practice, most American butters contain slightly more than that, averaging around 81% butterfat. European butters generally have a higher ratio—up to 85%.\nClarified butter is butter with almost all of its water and milk solids removed, leaving almost-pure butterfat. Clarified butter is made by heating butter to its melting point and then allowing it to cool; after settling, the remaining components separate by density. At the top, whey proteins form a skin, which is removed. The resulting butterfat is then poured off from the mixture of water and casein proteins that settle to the bottom.\n\nGhee is clarified butter that has been heated to around 120 °C (250 °F) after the water evaporated, turning the milk solids brown. This process flavors the ghee, and also produces antioxidants that help protect it from rancidity. Because of this, ghee can keep for six to eight months under normal conditions.\n\nCream may be separated (usually by a centrifugal separator) from whey instead of milk, as a byproduct of cheese-making. Whey butter may be made from whey cream. Whey cream and butter have a lower fat content and taste more salty, tangy and \"cheesy\". They are also cheaper than \"sweet\" cream and butter. The fat content of whey is low, so 1000 pounds of whey will typically give 3 pounds of butter.\n\nThere are several butters produced in Europe with protected geographical indications; these include:\n\nThe earliest butter would have been from sheep or goat's milk; cattle are not thought to have been domesticated for another thousand years. An ancient method of butter making, still used today in parts of Africa and the Near East, involves a goat skin half filled with milk, and inflated with air before being sealed. The skin is then hung with ropes on a tripod of sticks, and rocked until the movement leads to the formation of butter.\n\nIn the Mediterranean climate, unclarified butter spoils quickly— unlike cheese, it is not a practical method of preserving the nutrients of milk. The ancient Greeks and Romans seemed to have considered butter a food fit more for the northern barbarians. A play by the Greek comic poet Anaxandrides refers to Thracians as \"boutyrophagoi\", \"butter-eaters\". In his \"Natural History\", Pliny the Elder calls butter \"the most delicate of food among barbarous nations\", and goes on to describe its medicinal properties. Later, the physician Galen also described butter as a medicinal agent only.\n\nHistorian and linguist Andrew Dalby says most references to butter in ancient Near Eastern texts should more correctly be translated as ghee. Ghee is mentioned in the Periplus of the Erythraean Sea as a typical trade article around the first century CE Arabian Sea, and Roman geographer Strabo describes it as a commodity of Arabia and Sudan. In India, ghee has been a symbol of purity and an offering to the gods—especially Agni, the Hindu god of fire—for more than 3000 years; references to ghee's sacred nature appear numerous times in the \"Rigveda\", circa 1500–1200 BCE. The tale of the child Krishna stealing butter remains a popular children's story in India today. Since India's prehistory, ghee has been both a staple food and used for ceremonial purposes, such as fueling holy lamps and funeral pyres.\n\nIn the cooler climates of northern Europe, people could store butter longer before it spoiled. Scandinavia has the oldest tradition in Europe of butter export trade, dating at least to the 12th century. After the fall of Rome and through much of the Middle Ages, butter was a common food across most of Europe—but had a low reputation, and so was consumed principally by peasants. Butter slowly became more accepted by the upper class, notably when the early 16th century Roman Catholic Church allowed its consumption during Lent. Bread and butter became common fare among the middle class, and the English, in particular, gained a reputation for their liberal use of melted butter as a sauce with meat and vegetables.\n\nIn antiquity, butter was used for fuel in lamps as a substitute for oil. The \"Butter Tower\" of Rouen Cathedral was erected in the early 16th century when Archbishop Georges d'Amboise authorized the burning of butter instead of oil, which was scarce at the time, during Lent.\n\nAcross northern Europe, butter was sometimes treated in a manner unheard-of today: it was packed into barrels (firkins) and buried in peat bogs, perhaps for years. Such \"bog butter\" would develop a strong flavor as it aged, but remain edible, in large part because of the unique cool, airless, antiseptic and acidic environment of a peat bog. Firkins of such buried butter are a common archaeological find in Ireland; the National Museum of Ireland – Archaeology has some containing \"a grayish cheese-like substance, partially hardened, not much like butter, and quite free from putrefaction.\" The practice was most common in Ireland in the 11th–14th centuries; it ended entirely before the 19th century.\n\nLike Ireland, France became well known for its butter, particularly in Normandy and Brittany. By the 1860s, butter had become so in demand in France that Emperor Napoleon III offered prize money for an inexpensive substitute to supplement France's inadequate butter supplies. A French chemist claimed the prize with the invention of margarine in 1869. The first margarine was beef tallow flavored with milk and worked like butter; vegetable margarine followed after the development of hydrogenated oils around 1900.\nUntil the 19th century, the vast majority of butter was made by hand, on farms. The first butter factories appeared in the United States in the early 1860s, after the successful introduction of cheese factories a decade earlier. In the late 1870s, the centrifugal cream separator was introduced, marketed most successfully by Swedish engineer Carl Gustaf Patrik de Laval. This dramatically sped up the butter-making process by eliminating the slow step of letting cream naturally rise to the top of milk. Initially, whole milk was shipped to the butter factories, and the cream separation took place there. Soon, though, cream-separation technology became small and inexpensive enough to introduce an additional efficiency: the separation was accomplished on the farm, and the cream alone shipped to the factory. By 1900, more than half the butter produced in the United States was factory made; Europe followed suit shortly after.\n\nIn 1920, Otto Hunziker authored \"The Butter Industry, Prepared for Factory, School and Laboratory\", a well-known text in the industry that enjoyed at least three editions (1920, 1927, 1940). As part of the efforts of the American Dairy Science Association, Professor Hunziker and others published articles regarding: causes of tallowiness (an odor defect, distinct from rancidity, a taste defect); mottles (an aesthetic issue related to uneven color); introduced salts; the impact of creamery metals and liquids; and acidity measurement. These and other ADSA publications helped standardize practices internationally.\n\nButter also provided extra income to farm families. They used wood presses with carved decoration to press butter into pucks or small bricks to sell at nearby markets or general stores. The decoration identified the farm that produced the butter. This practice continued until production was mechanized and butter was produced in less decorative stick form. Today, butter presses remain in use for decorative purposes.\n\nPer capita butter consumption declined in most western nations during the 20th century, in large part because of the rising popularity of margarine, which is less expensive and, until recent years, was perceived as being healthier. In the United States, margarine consumption overtook butter during the 1950s, and it is still the case today that more margarine than butter is eaten in the U.S. and the EU.\n\nIn the United States, butter has traditionally been made into small, rectangular blocks by means of a pair of wooden butter paddles. It is usually produced in sticks that are individually wrapped in waxed or foiled paper, and sold as a package of 4 sticks. This practice is believed to have originated in 1907, when Swift and Company began packaging butter in this manner for mass distribution.\nDue to historical differences in butter printers (machines that cut and package butter), 4-ounce sticks are commonly produced in two different shapes:\n\n\nMost butter dishes are designed for Elgin-style butter sticks.\n\nButter stick wrappers are usually marked with divisions for , which is less than their actual volume: the Elgin-pack shape is , while the Western-pack shape is . The printing on unsalted (\"sweet\") butter wrappers is typically red, while that for salted butter is typically blue.\n\nOutside of the United States, the shape of butter packages is approximately the same, but the butter is measured for sale and cooking by mass (rather than by volume or unit/stick), and is sold in and packages. The wrapper is usually a foil and waxed-paper laminate. (The waxed paper is now a siliconised substitute, but is still referred to in some places as parchment, from the wrapping used in past centuries; and the term 'parchment-wrapped' is still employed where the paper alone is used, without the foil laminate.)\n\nButter for commercial and industrial use is packaged in plastic buckets, tubs, or drums, in quantities and units suited to the local market.\n\nIn 1997, India produced of butter, most of which was consumed domestically. Second in production was the United States (), followed by France (), Germany (), and New Zealand (). France ranks first in per capita butter consumption with 8 kg per capita per year. In terms of absolute consumption, Germany was second after India, using of butter in 1997, followed by France (), Russia (), and the United States (). New Zealand, Australia, and the Ukraine are among the few nations that export a significant percentage of the butter they produce.\n\nDifferent varieties are found around the world. \"Smen\" is a spiced Moroccan clarified butter, buried in the ground and aged for months or years. A similar product is \"maltash\" of the Hunza Valley, where cow and yak butter can be buried for decades, and is used at events such as weddings. Yak butter is a specialty in Tibet; \"tsampa\", barley flour mixed with yak butter, is a staple food. Butter tea is consumed in the Himalayan regions of Tibet, Bhutan, Nepal and India. It consists of tea served with intensely flavored—or \"rancid\"—yak butter and salt. In African and Asian developing nations, butter is traditionally made from sour milk rather than cream. It can take several hours of churning to produce workable butter grains from fermented milk.\n\nNormal butter softens to a spreadable consistency around 15 °C (60 °F), well above refrigerator temperatures. The \"butter compartment\" found in many refrigerators may be one of the warmer sections inside, but it still leaves butter quite hard. Until recently, many refrigerators sold in New Zealand featured a \"butter conditioner\", a compartment kept warmer than the rest of the refrigerator—but still cooler than room temperature—with a small heater. Keeping butter tightly wrapped delays rancidity, which is hastened by exposure to light or air, and also helps prevent it from picking up other odors. Wrapped butter has a shelf life of several months at refrigerator temperatures. Butter can also be frozen to further extend its storage life. \n\n\"French butter dishes\" or \"Acadian butter dishes\" have a lid with a long interior lip, which sits in a container holding a small amount of water. Usually the dish holds just enough water to submerge the interior lip when the dish is closed. Butter is packed into the lid. The water acts as a seal to keep the butter fresh, and also keeps the butter from overheating in hot temperatures. This method lets butter sit on a countertop for several days without spoiling.\n\nOnce butter is softened, spices, herbs, or other flavoring agents can be mixed into it, producing what is called a \"compound butter\" or \"composite butter\" (sometimes also called \"composed butter\"). Compound butters can be used as spreads, or cooled, sliced, and placed onto hot food to melt into a sauce. Sweetened compound butters can be served with desserts; such hard sauces are often flavored with spirits.\nMelted butter plays an important role in the preparation of sauces, most obviously in French cuisine. \"Beurre noisette\" (hazelnut butter) and \"Beurre noir\" (black butter) are sauces of melted butter cooked until the milk solids and sugars have turned golden or dark brown; they are often finished with an addition of vinegar or lemon juice. Hollandaise and béarnaise sauces are emulsions of egg yolk and melted butter; they are in essence mayonnaises made with butter instead of oil. Hollandaise and béarnaise sauces are stabilized with the powerful emulsifiers in the egg yolks, but butter itself contains enough emulsifiers—mostly remnants of the fat globule membranes—to form a stable emulsion on its own. \"Beurre blanc\" (white butter) is made by whisking butter into reduced vinegar or wine, forming an emulsion with the texture of thick cream. \"Beurre monté\" (prepared butter) is melted but still emulsified butter; it lends its name to the practice of \"mounting\" a sauce with butter: whisking cold butter into any water-based sauce at the end of cooking, giving the sauce a thicker body and a glossy shine—as well as a buttery taste.\n\nIn Poland, the butter lamb (\"Baranek wielkanocny\") is a traditional addition to the Easter Meal for many Polish Catholics. Butter is shaped into a lamb either by hand or in a lamb-shaped mould. Butter is also used to make edible decorations to garnish other dishes.\n\nButter is used for sautéing and frying, although its milk solids brown and burn above 150 °C (250 °F)—a rather low temperature for most applications. The smoke point of butterfat is around 200 °C (400 °F), so clarified butter or ghee is better suited to frying. Ghee has always been a common frying medium in India, where many avoid other animal fats for cultural or religious reasons.\n\nButter fills several roles in baking, where it is used in a similar manner as other solid fats like lard, suet, or shortening, but has a flavor that may better complement sweet baked goods. Many cookie doughs and some cake batters are leavened, at least in part, by creaming butter and sugar together, which introduces air bubbles into the butter. The tiny bubbles locked within the butter expand in the heat of baking and aerate the cookie or cake. Some cookies like shortbread may have no other source of moisture but the water in the butter. Pastries like pie dough incorporate pieces of solid fat into the dough, which become flat layers of fat when the dough is rolled out. During baking, the fat melts away, leaving a flaky texture. Butter, because of its flavor, is a common choice for the fat in such a dough, but it can be more difficult to work with than shortening because of its low melting point. Pastry makers often chill all their ingredients and utensils while working with a butter dough.\n\nAs butter is essentially just the milk fat, it contains only traces of lactose, so moderate consumption of butter is not a problem for lactose intolerant people. People with milk allergies may still need to avoid butter, which contains enough of the allergy-causing proteins to cause reactions. Whole milk, butter and cream have high levels of saturated fat. Butter is a good source of Vitamin A.\n\nThe molecular composition of butter which contribute to butter's distinct flavor includes: fatty acids, lactones, methyl ketones, diacetyl and dimethyl sulfide. When foods containing butter are baked, the concentrations of methyl ketones and lactones increase to provide the flavor of butter.\n\n\n"}
{"id": "12869072", "url": "https://en.wikipedia.org/wiki?curid=12869072", "title": "Charge controller", "text": "Charge controller\n\nA charge controller, charge regulator or battery regulator limits the rate at which electric current is added to or drawn from electric batteries.\nIt prevents overcharging and may protect against overvoltage, which can reduce battery performance or lifespan, and may pose a safety risk. It may also prevent completely draining (\"deep discharging\") a battery, or perform controlled discharges, depending on the battery technology, to protect battery life.\nThe terms \"charge controller\" or \"charge regulator\" may refer to either a stand-alone device, or to control circuitry integrated within a battery pack, battery-powered device, or battery charger.\n\nCharge controllers are sold to consumers as separate devices, often in conjunction with solar or wind power generators, for uses such as RV, boat, and off-the-grid home battery storage systems.\nIn solar applications, charge controllers may also be called solar regulators. Some charge controllers / solar regulators have additional features, such as a low voltage disconnect (LVD), a separate circuit which powers down the load when the batteries become overly discharged (some battery chemistries are such that over-discharge can ruin the battery).\n\nA series charge controller or series regulator disables further current flow into batteries when they are full. A shunt charge controller or shunt regulator diverts excess electricity to an auxiliary or \"shunt\" load, such as an electric water heater, when batteries are full.\n\nSimple charge controllers stop charging a battery when they exceed a set high voltage level, and re-enable charging when battery voltage drops back below that level. Pulse width modulation (PWM) and maximum power point tracker (MPPT) technologies are more electronically sophisticated, adjusting charging rates depending on the battery's level, to allow charging closer to its maximum capacity.\n\nA charge controller with MPPT capability frees the system designer from closely matching available PV voltage to battery voltage. Considerable efficiency gains can be achieved, particularly when the PV array is located at some distance from the battery. By way of example, a 150 volt PV array connected to an MPPT charge controller can be used to charge a 24 or 48 volt battery. Higher array voltage means lower array current, so the savings in wiring costs can more than pay for the controller.\n\nCharge controllers may also monitor battery temperature to prevent overheating. Some charge controller systems also display data, transmit data to remote displays, and data logging to track electric flow over time.\n\nCircuitry that functions as a charge regulator controller may consist of several electrical components, or may be encapsulated in a single microchip, an integrated circuit (IC) usually called a charge controller IC or charge control IC.\n\nCharge controller circuits are used for rechargeable electronic devices such as cell phones, laptop computers, portable audio players, and uninterruptible power supplies, as well as for larger battery systems found in electric vehicles and orbiting space satellites\n\n"}
{"id": "6580748", "url": "https://en.wikipedia.org/wiki?curid=6580748", "title": "Chloryl fluoride", "text": "Chloryl fluoride\n\nChloryl fluoride is the chemical compound with the formula ClOF. It is commonly encountered as side-product in reactions of chlorine fluorides with oxygen sources. It is the acyl fluoride of chloric acid.\n\nClOF was first reported by Schmitz and Schumacheb in 1942, who prepared it by the fluorination of ClO. The compound is more conveniently prepared by treatment of sodium chlorate and chlorine trifluoride and purified by vacuum fractionation, i.e. selectively condensing this species separately from other products. This species is a gas boiling at −6 °C:\n\nIn contrast to OF, ClOF is a pyramidal molecule. This structure is predicted by VSEPR. The differing structures reflects the greater tendency of chlorine to exist in positive oxidation states with oxygen and fluorine ligands. The related Cl-O-F compound perchloryl fluoride, ClOF, is tetrahedral.\nThe related bromine compound \"bromyl fluoride\" (BrOF) adopts the same structure as ClOF, whereas \"iodyl fluoride\" (IOF) forms a polymeric substance under standard conditions.\n\n"}
{"id": "34418057", "url": "https://en.wikipedia.org/wiki?curid=34418057", "title": "Climate Change and Global Energy Security", "text": "Climate Change and Global Energy Security\n\nClimate Change and Global Energy Security: Technology and Policy Options is a 2011 book by Marilyn A. Brown and Benjamin K. Sovacool. In this book, Brown and Sovacool offer detailed assessments of commercially available technologies for strengthening global energy security and climate change mitigation. They also evaluate the barriers to the deployment of these technologies and critically review public policy options for their commercialization. Arguing that society has all the technologies necessary for the task, the authors discuss an array of options available today, including high-efficiency transportation, renewable energy, carbon sequestration, and demand side management.\n\n\n"}
{"id": "47521", "url": "https://en.wikipedia.org/wiki?curid=47521", "title": "Condensation", "text": "Condensation\n\nCondensation is the change of the physical state of matter from gas phase into liquid phase, and is the reverse of vapourisation. The word most often refers to the water cycle. It can also be defined as the change in the state of water vapour to liquid water when in contact with a liquid or solid surface or cloud condensation nuclei within the atmosphere. When the transition happens from the gaseous phase into the solid phase directly, the change is called deposition.\n\nCondensation is initiated by the formation of atomic/molecular clusters of that species within its gaseous volume—like rain drop or snow flake formation within clouds—or at the contact between such gaseous phase and a liquid or solid surface. In clouds, this can be catalyzed by water-nucleating proteins, produced by atmospheric microbes, which are capable of binding gaseous or liquid water molecules.\nA few distinct reversibility scenarios emerge here with respect to the nature of the surface.\n\nCondensation commonly occurs when a vapor is cooled and/or compressed to its saturation limit when the molecular density in the gas phase reaches its maximal threshold. Vapor cooling and compressing equipment that collects condensed liquids is called a \"condenser\".\n\nPsychrometry measures the rates of condensation through evaporation into the air moisture at various atmospheric pressures and temperatures. Water is the product of its vapor condensation—condensation is the process of such phase conversion.\n\nCondensation is a crucial component of distillation, an important laboratory and industrial chemistry application.\n\nBecause condensation is a naturally occurring phenomenon, it can often be used to generate water in large quantities for human use. Many structures are made solely for the purpose of collecting water from condensation, such as air wells and fog fences. Such systems can often be used to retain soil moisture in areas where active desertification is occurring—so much so that some organizations educate people living in affected areas about water condensers to help them deal effectively with the situation.\n\nIt is also a crucial process in forming particle tracks in a cloud chamber. In this case, ions produced by an incident particle act as nucleation centers for the condensation of the vapor producing the visible \"cloud\" trails.\n\nCommercial applications of condensation, by consumers as well as industry, include power generation, water desalination, thermal management, refrigeration, and air conditioning.\n\nNumerous living beings use water made accessible by condensation. A few examples of these are the Australian thorny devil, the darkling beetles of the Namibian coast, and the coast redwoods of the West Coast of the United States.\n\nCondensation in building construction is an unwanted phenomenon as it may cause dampness, mold health issues, wood rot, corrosion, weakening of mortar and masonry walls, and energy penalties due to increased heat transfer. To alleviate these issues, the indoor air humidity needs to be lowered, or air ventilation in the building needs to be improved. This can be done in a number of ways, for example opening windows, turning on extractor fans, using dehumidifiers, drying clothes outside and covering pots and pans whilst cooking. Air conditioning or ventilation systems can be installed that help remove moisture from the air, and move air throughout a building. The amount of water vapour that can be stored in the air can be increased simply by increasing the temperature. However, this can be a double edged sword as most condensation in the home occurs when warm, moisture heavy air comes into contact with a cool surface. As the air is cooled, it can no longer hold as much water vapour. This leads to deposition of water on the cool surface. This is very apparent when central heating is used in combination with single glazed windows in winter.\n\nInterstructure condensation may be caused by thermal bridges, insufficient or lacking insulation, damp proofing or insulated glazing.\n\n"}
{"id": "48181134", "url": "https://en.wikipedia.org/wiki?curid=48181134", "title": "Cårrujavrit Hydroelectric Power Station", "text": "Cårrujavrit Hydroelectric Power Station\n\nThe Cårrujavrit Hydroelectric Power Station ( or \"Čårrujavrit kraftverk\") is a hydroelectric power station in the municipality of Kvænangen in Troms county, Norway. It utilizes a drop of between its intake reservoir on the Njemenaiko River (, , ) and Little Lakes (, , ), which is also the reservoir for the Kvænangsbotn Hydroelectric Power Station. Lake Tjoika ( or \"Hyttysenjärvi\", ) serves as the reservoir for the plant and is regulated at a level between and . The plant has a Francis turbine and operates at an installed capacity of , with an average annual production of about 11 GWh. The plant is controlled by Kvænangen Kraftverk AS, with a 48.2% share owned by Troms Kraft.\n"}
{"id": "83412", "url": "https://en.wikipedia.org/wiki?curid=83412", "title": "D1G reactor", "text": "D1G reactor\n\nThe D1G reactor was a prototype naval reactor designed for the United States Navy to provide electricity generation and propulsion on warships. The D1G designation stands for:\n\n\nThis prototype nuclear reactor was constructed for the United States Department of Energy's Office of Naval Reactors as part of the Naval Nuclear Propulsion Program. The reactor was built by General Electric and operated by the Knolls Atomic Power Laboratory at the Kesselring Site Operation in West Milton, New York. It was used for testing components and as a training tool for the Nuclear Power Training Unit. The reactor operated from 1962 to 1996, when it was shut down in March of that year. It was later defuelled, with the pressure vessel eventually removed in 2002.\n\nThe containment structure — which housed both the primary (nuclear reactor) and secondary (steam plant) systems — is referred to as the \"DIG-ball\" due to its unique shape: a Horton Sphere. The sphere was originally constructed by Chicago Bridge and Iron Works to house the liquid metal cooled reactor of the USS \"Seawolf\" (SSN-575), with the dome designed to contain a liquid sodium explosion.\n"}
{"id": "58161", "url": "https://en.wikipedia.org/wiki?curid=58161", "title": "Dust devil", "text": "Dust devil\n\nA dust devil is a strong, well-formed, and relatively long-lived whirlwind, ranging from small (half a metre wide and a few metres tall) to large (more than 10 metres wide and more than 1000 metres tall). The primary vertical motion is upward. Dust devils are usually harmless, but can on rare occasions grow large enough to pose a threat to both people and property.\n\nThey are comparable to tornadoes in that both are a weather phenomenon involving a vertically oriented rotating column of wind. Most tornadoes are associated with a larger parent circulation, the mesocyclone on the back of a supercell thunderstorm. Dust devils form as a swirling updraft under sunny conditions during fair weather, rarely coming close to the intensity of a tornado.\n\nDust devils form when a pocket of hot air near the surface rises quickly through cooler air above it, forming an updraft. If conditions are just right, the updraft may begin to rotate. As the air rapidly rises, the column of hot air is stretched vertically, thereby moving mass closer to the axis of rotation, which causes intensification of the spinning effect by conservation of angular momentum. The secondary flow in the dust devil causes other hot air to speed horizontally inward to the bottom of the newly forming vortex. As more hot air rushes in toward the developing vortex to replace the air that is rising, the spinning effect becomes further intensified and self-sustaining. A dust devil, fully formed, is a funnel-like chimney through which hot air moves, both upwards and in a circle. As the hot air rises, it cools, loses its buoyancy and eventually ceases to rise. As it rises, it displaces air which descends outside the core of the vortex. This cool air returning acts as a balance against the spinning hot-air outer wall and keeps the system stable.\n\nThe spinning effect, along with surface friction, usually will produce a forward momentum. The dust devil is able to sustain itself longer by moving over nearby sources of hot surface air.\n\nAs available hot air near the surface is channeled up the dust devil, eventually surrounding cooler air will be sucked in. Once this occurs, the effect is dramatic, and the dust devil dissipates in seconds. Usually this occurs when the dust devil is not moving fast enough (depletion) or begins to enter a terrain where the surface temperatures are cooler.\n\nCertain conditions increase the likelihood of dust devil formation.\n\nOn Earth, most dust devils are very small and weak, often less than 3 feet (0.9 m) in diameter with maximum winds averaging about 45 miles per hour (70 km/h), and they often dissipate less than a minute after forming. On rare occasions, a dust devil can grow very large and intense, sometimes reaching a diameter of up to 300 feet (90 m) with winds in excess of 60 mph (100 km/h+) and can last for upwards of 20 minutes before dissipating.\n\nDust devils typically do not cause injuries, but rare, severe dust devils have caused damage and even deaths in the past. One such dust devil struck the Coconino County Fairgrounds in Flagstaff, Arizona, on September 14, 2000, causing extensive damage to several temporary tents, stands and booths, as well as some permanent fairgrounds structures. Several injuries were reported, but there were no fatalities. Based on the degree of damage left behind, it is estimated that the dust devil produced winds as high as 75 mph (120 km/h), which is equivalent to an EF-0 tornado. On May 19, 2003, a dust devil lifted the roof off a two-story building in Lebanon, Maine, causing it to collapse and kill a man inside. In East El Paso, Texas in 2010, three children in an inflatable jump house were picked up by a dust devil and lifted over 10 feet (3 m), traveling over a fence and landing in a backyard three houses away. In Commerce City, Colorado in 2018, a powerful dust devil hurtled two porta-potties into the air. No one was injured in the incident.\n\nDust devils have been implicated in around 100 aircraft accidents. While many incidents have been simple taxiing problems, a few have had fatal consequences. Dust devils are also considered major hazards among skydivers and paragliding pilots as they can cause a parachute or a glider to collapse with little to no warning, at altitudes considered too low to cut away, and contribute to the serious injury or death of parachutists.\n\nThere is an endemic health issue in some arid areas, such as the southwestern United States, California, northwestern Mexico, Central America, and South America, called Valley fever. “Cocci” fungus grows naturally in alkaline soil and the fungus spores, that lie dormant and can be picked up by dust devils, are blown around. When a person breathes these spores in, they cause fungal pneumonia. While usually not a serious threat, the disease is still dangerous to some people.\n\nDust devils, even small ones (on Earth), can produce radio noise and electrical fields greater than 10,000 volts per meter. A dust devil picks up small dirt and dust particles. As the particles whirl around, they bump and scrape into each other and become electrically charged. The whirling charged particles also create a magnetic field that fluctuates between 3 and 30 times each second.\n\nThese electrical fields assist the vortices in lifting materials off the ground and into the atmosphere. Field experiments indicate that a dust devil can lift 1 gram of dust per second from each square metre (10 lb/s from each acre) of ground over which it passes. A large dust devil measuring about 100 metres (330 ft) across at its base can lift about 15 metric tonnes (17 short tons) of dust into the air in 30 minutes. Giant dust storms that sweep across the world's deserts contribute 8% of the mineral dust in the atmosphere each year during the handful of storms that occur. In comparison, the significantly smaller dust devils that twist across the deserts during the summer lift about three times as much dust, thus having a greater combined impact on the dust content of the atmosphere. When this occurs, they are often called sand pillars.\n\nDust devils also occur on Mars (see dust devil tracks) and were first photographed by the Viking orbiters in the 1970s. In 1997, the Mars Pathfinder lander detected a dust devil passing over it. In the image shown here, photographed by the Mars Global Surveyor, the long dark streak is formed by a moving swirling column of Martian atmosphere. The dust devil itself (the black spot) is climbing the crater wall. The streaks on the right are sand dunes on the crater floor.\n\nMartian dust devils can be up to fifty times as wide and ten times as high as terrestrial dust devils, and large ones may pose a threat to terrestrial technology sent to Mars.\n\nMission members monitoring the Spirit rover on Mars reported on March 12, 2005, that a lucky encounter with a dust devil had cleaned the solar panels of that robot. Power levels dramatically increased and daily science work was anticipated to be expanded. A similar phenomenon (solar panels mysteriously cleaned of accumulated dust) had previously been observed with the Opportunity rover, and dust devils had also been suspected as the cause.\n\nA fire whirl or swirl, sometimes called fire devils or fire tornadoes, can be seen during intense fires in combustible building structures or, more commonly, in forest or bush fires. A fire whirl is a vortex-shaped formation of burning gases being released from the combustible material. The genesis of the vortex is probably similar to that of a dust devil. As distinct from the dust devil, it is improbable that the height reached by the fire gas vortex is greater than the visible height of the vertical flames because of turbulence in the surrounding gases that inhibit creation of a stable boundary layer between the rotating/rising gases relative to the surrounding gases.\n\nHot cinders underneath freshly deposited ash in recently burned areas may sometimes generate numerous dust devils. The lighter weight and the darker color of the ash may create dust devils that are visible hundreds of feet into the air.\n\nAsh devils form similar to dust devils and are often seen on unstable days in burn scar areas of recent fires. \nSteam devils are phenomena often observed in the steam rising from power plants.\n\nThe same conditions can produce a snow whirlwind, although differential heating is more difficult in snow-covered areas.\n\nCoal devils are common at the coal town of Tsagaan Khad in South Gobi Province, Mongolia. They occur when dust devils pick up large amounts of stockpiled coal. Their dark color makes them resemble some tornados.\n\n\n"}
{"id": "41975588", "url": "https://en.wikipedia.org/wiki?curid=41975588", "title": "ETE (tokamak)", "text": "ETE (tokamak)\n\nThe Spherical Tokamak Experiment () is a machine dedicated to plasma studies in low aspect ratio tokamaks. The ETE was entirely designed and assembled at the Associated Plasma Laboratory (Laboratório Associado de Plasma, LAP) of Brazil's National Institute for Space Research (INPE).\n"}
{"id": "57139943", "url": "https://en.wikipedia.org/wiki?curid=57139943", "title": "Flame stretch", "text": "Flame stretch\n\nIn combustion, flame stretch (formula_1) is a quantity which measures the amount of stretch of the flame surface due to curvature and due to the outer velocity field strain. The concept of flame stretch was introduced by Karlovitz in 1953 and George H. Markstein studied the flame stretch by treating the flame surface as a hydrodynamic discontinuity (known as flame front) and also discussed by Bernard Lewis and Guenther von Albe in their book. All these discussions were treated flame stretch as an effect of flow velocity gradients. But the stretch can be found even if there is no velocity gradient, but due to the flame curvature. So, the definition required a more general formulation and its precise definition was first introduced by Forman A. Williams in 1975 as the ratio of rate of change of flame surface area to the area itself\n\nWhen formula_3, the flame is stretched, otherwise compressed. Sometimes the flame stretch is defined as non-dimensional quantity\n\nwhere formula_5 is the laminar flame thickness and formula_6 is the laminar propagation speed of unstretched premixed flame.\n\n"}
{"id": "72839", "url": "https://en.wikipedia.org/wiki?curid=72839", "title": "Foucault pendulum", "text": "Foucault pendulum\n\nThe Foucault pendulum ( ; ) or Foucault's pendulum is a simple device named after French physicist Léon Foucault and conceived as an experiment to demonstrate the Earth's rotation. The pendulum was introduced in 1851 and was the first experiment to give simple, direct evidence of the earth's rotation. Today, Foucault pendulums are popular displays in science museums and universities.\n\nThe first public exhibition of a Foucault pendulum took place in February 1851 in the Meridian of the Paris Observatory. A few weeks later, Foucault made his most famous pendulum when he suspended a 28-kg brass-coated lead bob with a 67-m-long wire from the dome of the Panthéon, Paris. The plane of the pendulum's swing rotated clockwise approximately 11.3° per hour, making a full circle in approximately 31.8 hours. The original bob used in 1851 at the Panthéon was moved in 1855 to the Conservatoire des Arts et Métiers in Paris. A second temporary installation was made for the 50th anniversary in 1902.\n\nDuring museum reconstruction in the 1990s, the original pendulum was temporarily displayed at the Panthéon (1995), but was later returned to the Musée des Arts et Métiers before it reopened in 2000. On April 6, 2010, the cable suspending the bob in the Musée des Arts et Métiers snapped, causing irreparable damage to the pendulum and to the marble flooring of the museum.\n\nAn exact copy of the original pendulum has been operating under the dome of the Panthéon, Paris since 1995.\n\nAt either the North Pole or South Pole, the plane of oscillation of a pendulum remains fixed relative to the distant masses of the universe while Earth rotates underneath it, taking one sidereal day to complete a rotation. So, relative to Earth, the plane of oscillation of a pendulum at the North Pole – viewed from above – undergoes a full clockwise rotation during one day; a pendulum at the South Pole rotates counterclockwise.\n\nWhen a Foucault pendulum is suspended at the equator, the plane of oscillation remains fixed relative to Earth. At other latitudes, the plane of oscillation precesses relative to Earth, but more slowly than at the pole; the angular speed, \"ω\" (measured in clockwise degrees per sidereal day), is proportional to the sine of the latitude, formula_1:\n\nwhere latitudes north and south of the equator are defined as positive and negative, respectively. For example, a Foucault pendulum at 30° south latitude, viewed from above by an earthbound observer, rotates counterclockwise 360° in two days.\n\nTo demonstrate rotation directly rather than indirectly via the swinging pendulum, Foucault used a gyroscope in an 1852 experiment. The inner gimbal of the Foucault gyroscope was balanced on knife edge bearings on the outer gimbal and the outer gimbal was suspended by a fine, torsion-free thread in such a manner that the lower pivot point carried almost no weight. The gyro was spun to 9000–12000 revolutions per minute with an arrangement of gears before being placed into position, which was sufficient time to balance the gyroscope and carry out 10 minutes of experimentation. The instrument could be observed either with a microscope viewing a tenth of a degree scale or by a long pointer. At least three more copies of a Foucault gyro were made in convenient travelling and demonstration boxes and copies survive in the UK, France, and the US.\n\nA Foucault pendulum requires care to set up because imprecise construction can cause additional veering which masks the terrestrial effect. The initial launch of the pendulum is critical; the traditional way to do this is to use a flame to burn through a thread which temporarily holds the bob in its starting position, thus avoiding unwanted sideways motion (see a ).\nAir resistance damps the oscillation, so some Foucault pendulums in museums incorporate an electromagnetic or other drive to keep the bob swinging; others are restarted regularly, sometimes with a launching ceremony as an added attraction.\nA 'pendulum day' is the time needed for the plane of a freely suspended Foucault pendulum to complete an apparent rotation about the local vertical. This is one sidereal day divided by the sine of the latitude.\n\nIn a near-inertial frame moving in tandem with Earth, but not sharing the rotation of the earth about its own axis, the suspension point of the pendulum traces out a circular path during one sidereal day.\n\nAt the latitude of Paris, 48 degrees 51 minutes north, a full precession cycle takes just under 32 hours, so after one sidereal day, when the Earth is back in the same orientation as one sidereal day before, the oscillation plane has turned by just over 270 degrees. If the plane of swing was north-south at the outset, it is east-west one sidereal day later.\n\nThis also implies that there has been exchange of momentum; the Earth and the pendulum bob have exchanged momentum. The Earth is so much more massive than the pendulum bob that the Earth's change of momentum is unnoticeable. Nonetheless, since the pendulum bob's plane of swing has shifted, the conservation laws imply that an exchange must have occurred.\n\nRather than tracking the change of momentum, the precession of the oscillation plane can efficiently be described as a case of parallel transport. For that, it can be demonstrated, by composing the infinitesimal rotations, that the precession rate is proportional to the projection of the angular velocity of Earth onto the normal direction to Earth, which implies that the trace of the plane of oscillation will undergo parallel transport. After 24 hours, the difference between initial and final orientations of the trace in the Earth frame is , which corresponds to the value given by the Gauss–Bonnet theorem. \"α\" is also called the holonomy or geometric phase of the pendulum. When analyzing earthbound motions, the Earth frame is not an inertial frame, but rotates about the local vertical at an effective rate of radians per day. A simple method employing parallel transport within cones tangent to the Earth's surface can be used to describe the rotation angle of the swing plane of Foucault's pendulum.\n\nFrom the perspective of an Earth-bound coordinate system with its \"x\"-axis pointing east and its \"y\"-axis pointing north, the precession of the pendulum is described by the Coriolis force. Consider a planar pendulum with natural frequency \"ω\" in the small angle approximation. There are two forces acting on the pendulum bob: the restoring force provided by gravity and the wire, and the Coriolis force. The Coriolis force at latitude \"φ\" is horizontal in the small angle approximation and is given by\n\nwhere Ω is the rotational frequency of Earth, \"F\" is the component of the Coriolis force in the \"x\"-direction and \"F\" is the component of the Coriolis force in the \"y\"-direction.\n\nThe restoring force, in the small-angle approximation, is given by\nUsing Newton's laws of motion this leads to the system of equations\nSwitching to complex coordinates , the equations read\nTo first order in Ω/\"ω\" this equation has the solution\nIf time is measured in days, then and the pendulum rotates by an angle of −2π sin(\"φ\") during one day.\nMany physical systems precess in a similar manner to a Foucault pendulum. As early as 1836, the Scottish mathematician Edward Sang contrived and explained the precession of a spinning top. In 1851, Charles Wheatstone\n\nSimilarly, consider a nonspinning, perfectly balanced bicycle wheel mounted on a disk so that its axis of rotation makes an angle formula_2 with the disk. When the disk undergoes a full clockwise revolution, the bicycle wheel will not return to its original position, but will have undergone a net rotation of formula_5.\n\nFoucault-like precession is observed in a virtual system wherein a massless particle is constrained to remain on a rotating plane that is inclined with respect to the axis of rotation.\n\nSpin of a relativistic particle moving in a circular orbit precesses similar to the swing plane of Foucault pendulum. The relativistic velocity space in Minkowski spacetime can be treated as a sphere \"S\" in 4-dimensional Euclidean space with imaginary radius and imaginary timelike coordinate. Parallel transport of polarization vectors along such sphere gives rise to Thomas precession, which is analogous to the rotation of the swing plane of Foucault pendulum due to parallel transport along a sphere \"S\" in 3-dimensional Euclidean space.\n\nIn physics, the evolution of such systems is determined by geometric phases. Mathematically they are understood through parallel transport.\n\nThere are numerous Foucault pendulums at universities, science museums, and the like throughout the world. The United Nations headquarters in New York City has one; the largest is at the Oregon Convention Center.\n\nThe experiment has also been carried out at the South Pole, where it was assumed that the rotation of the earth would have maximum effect at the Amundsen–Scott South Pole Station, in a six-story staircase of a new station under construction. The pendulum had a length of 33 m and the bob weighed 25 kg. The location was ideal: no moving air could disturb the pendulum, and the low viscosity of the cold air reduced air resistance. The researchers confirmed about 24 hours as the rotation period of the plane of oscillation.\n\n\n"}
{"id": "17597582", "url": "https://en.wikipedia.org/wiki?curid=17597582", "title": "Gardenia taitensis", "text": "Gardenia taitensis\n\nGardenia taitensis, also called Tahitian gardenia or Tiaré flower, is a species of plant in the Rubiaceae family. It is an evergreen tropical shrub that grows to 4 m tall and has glossy dark green leaves (5–16 cm long) that are oppositely arranged along the stem. The flower is creamy white and pinwheel-shaped with 5–9 lobes (each lobe 2–4 cm long) and fragrant. Native to the highland shores of the South Pacific, it has the distinction of being one of the few cultivated plants native to Polynesia. It is the national flower of French Polynesia and the Cook Islands.\n\nThe name Tahitian gardenia is somewhat a misnomer because it is neither native nor naturalised in Tahiti. The first acceptable scientific name for the plant was based on Tahitian specimens collected by Jules Dumont d'Urville in 1824. Hence the scientific name of \"Gardenia taitensis\", and the English name of Tahitian gardenia or Tiaré flower. It was first collected in Tahiti, by the Forsters on Captain Cook's first Pacific voyage (1768–1771), although it was misidentified as \"Gardenia florida\".\n\nThe plant originates from Melanesia and Western Polynesia. It is an aboriginal introduction to the Cook Islands and French Polynesia and possibly Hawaii.\n\n\n\n\n"}
{"id": "14686079", "url": "https://en.wikipedia.org/wiki?curid=14686079", "title": "Geiseltalsee Solarpark", "text": "Geiseltalsee Solarpark\n\nGeiseltalsee Solarpark, also known as Geiseltalsee, is a 4 MWp photovoltaic power plant located in Merseburg, Germany. The power plant was constructed by BP Solar using 24,864 BP solar modules. The power station was completed in 2004\n\n"}
{"id": "7177687", "url": "https://en.wikipedia.org/wiki?curid=7177687", "title": "Gross–Pitaevskii equation", "text": "Gross–Pitaevskii equation\n\nThe Gross–Pitaevskii equation (GPE, named after Eugene P. Gross\n) describes the ground state of a quantum system of identical bosons using the Hartree–Fock approximation and the pseudopotential interaction model.\n\nIn the Hartree–Fock approximation the total wave-function formula_1 of the system of formula_2 bosons is taken as a product of single-particle functions formula_3,\nwhere formula_5 is the coordinate of the formula_6-th boson.\n\nThe pseudopotential model Hamiltonian of the system is given as\nwhere formula_8 is the mass of the boson, formula_9 is the external potential, formula_10 is the boson-boson scattering length, and formula_11 is the Dirac delta-function.\n\nIf the single-particle wave-function satisfies the Gross–Pitaevski equation,\nthe total wave-function minimizes the expectation value of the model Hamiltonian under normalization condition formula_13\n\nIt is a model equation for the single-particle wavefunction in a Bose–Einstein condensate. It is similar in form to the Ginzburg–Landau equation and is sometimes referred to as a nonlinear Schrödinger equation.\n\nA Bose–Einstein condensate (BEC) is a gas of bosons that are in the same quantum state, and thus can be described by the same wavefunction. A free quantum particle is described by a single-particle Schrödinger equation. Interaction between particles in a real gas is taken into account by a pertinent many-body Schrödinger equation. If the average spacing between the particles in a gas is greater than the scattering length (that is, in the so-called dilute limit), then one can approximate the true interaction potential that features in this equation by a pseudopotential. The non-linearity of the Gross–Pitaevskii equation has its origin in the interaction between the particles. This is made evident by setting the coupling constant of interaction in the Gross–Pitaevskii equation to zero (see the following section): thereby, the single-particle Schrödinger equation describing a particle inside a trapping potential is recovered.\n\nThe equation has the form of the Schrödinger equation with the addition of an interaction term. The coupling constant, g, is proportional to the scattering length formula_10 of two interacting bosons:\n\nwhere formula_16 is the reduced Planck's constant and m is the mass of the boson.\nThe energy density is\n\nwhere formula_18 is the wavefunction, or order parameter, and V is an external potential.\nThe time-independent Gross–Pitaevskii equation, for a conserved number of particles, is\n\nwhere formula_20 is the chemical potential. The chemical potential is found from the condition that the number of particles is related to the wavefunction by\n\nFrom the time-independent Gross–Pitaevskii equation, we can find the structure of a Bose–Einstein condensate in various external potentials (e.g. a harmonic trap).\n\nThe time-dependent Gross–Pitaevskii equation is\n\nFrom the time-dependent Gross–Pitaevskii equation we can look at the dynamics of the Bose–Einstein condensate. It is used to find the collective modes of a trapped gas.\n\nSince the Gross–Pitaevskii equation is a nonlinear partial differential equation, exact solutions are hard to come by. As a result, solutions have to be approximated via myriad techniques.\n\nThe simplest exact solution is the free particle solution, with formula_23,\n\nThis solution is often called the Hartree solution. Although it does satisfy the Gross–Pitaevskii equation, it leaves a gap in the energy spectrum due to the interaction:\n\nAccording to the Hugenholtz–Pines theorem,\n\nA one-dimensional soliton can form in a Bose–Einstein condensate, and depending upon whether the interaction is attractive or repulsive, there is either a bright or dark soliton. Both solitons are local disturbances in a condensate with a uniform background density.\n\nIf the BEC is repulsive, so that formula_26, then a possible solution of the Gross–Pitaevskii equation is,\n\nwhere formula_28 is the value of the condensate wavefunction at formula_29, and formula_30, is the coherence length. This solution represents the dark soliton, since there is a deficit of condensate in a space of nonzero density. The dark soliton is also a type of topological defect, since formula_31 flips between positive and negative values across the origin, corresponding to a formula_32 phase shift.\n\nFor formula_33\n\nwhere the chemical potential is formula_35. This solution represents the bright soliton, since there is a concentration of condensate in a space of zero density.\n\nIn systems where an exact analytical solution may not be feasible, one can make a variational approximation. The basic idea is to make a variational ansatz for the wavefunction with free parameters, plug it into the free energy, and minimize the energy with respect to the free parameters.\n\nThe Gross-Pitaevskii equation is a partial differential equation in space and time variables. Usually it does not have analytic solution and numerical methods, such as the split-step Crank–Nicolson and Fourier spectral methods, are used for its solution. There are different Fortran and C programs for its solution for the contact interaction and long-range dipolar interaction.\n\nIf the number of particles in a gas is very large, the interatomic interaction becomes large so that the kinetic energy term can be neglected from the Gross–Pitaevskii equation. This is called the Thomas–Fermi approximation.\n\nBogoliubov treatment of the Gross–Pitaevskii equation is a method that finds the elementary excitations of a Bose–Einstein condensate. To that purpose, the condensate wavefunction is approximated by a sum of the equilibrium wavefunction formula_37 and a small perturbation formula_38,\n\nThen this form is inserted in the time dependent Gross–Pitaevskii equation and its complex conjugate, and linearized to first order in formula_38\n\nAssuming the following for formula_38\n\none finds the following coupled differential equations for formula_45 and formula_46 by taking the formula_47 parts as independent components\n\nFor a homogeneous system, i.e. for formula_50, one can get formula_51 from the zeroth order equation. Then we assume formula_45 and formula_46 to be plane waves of momentum formula_54, which leads to the energy spectrum\n\nFor large formula_54, the dispersion relation is quadratic in formula_54 as one would expect for usual non interacting single particle excitations. For small formula_54, the dispersion relation is linear\n\nwith formula_60 being the speed of sound in the condensate. The fact that formula_61 shows, according to Landau's criterion, that the condensate is a superfluid, meaning that if an object is moved in the condensate at a velocity inferior to s, it will not be energetically favorable to produce excitations and the object will move without dissipation, which is a characteristic of a superfluid. Experiments have been done to prove this superfluidity of the condensate, using a tightly focused blue-detuned laser.\n\nThe optical potential well formula_62 might be formed by two counter propagating optical vortices with wavelengths formula_63, effective width formula_64 and topological charge formula_65 :\n\nwhere formula_67.\n\nIn cylindrical coordinate system \nformula_68 \nthe potential well \nhave a remarkable \"double helix geometry\": \n\nIn a reference frame rotating with angular velocity formula_70, time-dependent Gross–Pitaevskii equation with helical potential is as follows:\n\nwhere \nformula_72 is the angular momentum operator.\n\nThe solution for condensate wavefunction \nformula_73 is a superposition of two phase-conjugated matter-wave vortices:\n\nThe macroscopically observable momentum of condensate is :\n\nwhere formula_77 is number of atoms in condensate. \nThis means that atomic ensemble moves coherently along \nformula_78 axis with group \nvelocity whose direction is defined by signs of topological \ncharge formula_65 and angular velocity \nformula_80:\nThe angular momentum of helically trapped condensate is exactly zero:\n\n\n"}
{"id": "596482", "url": "https://en.wikipedia.org/wiki?curid=596482", "title": "Hair wax", "text": "Hair wax\n\nHair wax is a thick hairstyling product containing wax, used to assist with holding the hair. In contrast with hair gel, most of which contain alcohol, hair wax remains appliable and has less chance of drying out. Consequently, hair wax is currently experiencing an increase in popularity, often under names such as pomade, putty, glue, whip, molding gum, or styling paste. The texture, consistency, and purpose of these products varies widely and each has a different purported purpose depending on the manufacturer. Traditionally, pomade is a type of hair wax that also adds shine to one's hair. \n\nHair wax has been used for many years and a waxy soap-like substance was invented by the ancient Gauls as a hair styling agent and was not used as a cleaning agent until many years later.\n\nThe following are some of the ingredients typically found in commercial hair wax products.\n\nSome stylists prefer making their own blends of hair wax customized for their clientele. Various recipes exist, including some with \"secret\" ingredients.\n\n"}
{"id": "39443092", "url": "https://en.wikipedia.org/wiki?curid=39443092", "title": "Hsingneng Power Plant", "text": "Hsingneng Power Plant\n\nThe Hsingneng Power Plant, Star Energy Power Plant or Changbin Power Plant () is a gas-fired power plant in Chang-Bin Industrial Park, Lukang Township, Changhua County, Taiwan.\n\nThe power plant was commissioned in March 2004 and started its operation in April 2004.\n\n"}
{"id": "11174664", "url": "https://en.wikipedia.org/wiki?curid=11174664", "title": "John Ghazvinian", "text": "John Ghazvinian\n\nJohn Ghazvinian (born 1974) is an American journalist and historian. He was raised in London and Los Angeles, born in Iran and currently lives in Philadelphia. He is known for his writing on African oil politics as the author of \"\" (Harcourt, 2007), an exposé of the petroleum industry in Africa. Ghazvinian is currently a Senior Fellow at the Center for Programs in Contemporary Writing at the University of Pennsylvania.\n\n\"Untapped\" has received widespread praise, particularly among progressives. Andrew Leonard at salon.com wrote of the book that it \"should be must reading for anyone who still believes that unregulated markets are the best way to cure all the ills of the poor nations of the world.\" \"The Boston Globe\" called \"Untapped\" a \"riveting account and superb analysis of what African oil means to a fuel-hungry world and to the African nations involved.\" \"The New York Times\" called the book \"perceptive\" and said that it \"drills home the point...that a thoughtful strategy to lift the neglected bottom billion must compete against the global oil giants going about their business.\"\n\nGhazvinian also writes for The Nation, Newsweek, GQ and The Virginia Quarterly Review.\n\nRaised in London and later in Los Angeles, he currently lives in Philadelphia.\n\n"}
{"id": "57528074", "url": "https://en.wikipedia.org/wiki?curid=57528074", "title": "Kilmalkedar", "text": "Kilmalkedar\n\nKilmalkedar is a medieval ecclesiastical site and National Monument located in County Kerry, Ireland.\n\nKilmalkedar is on the Dingle Peninsula, east of Ballyferriter and northwest of Dingle.\n\nKilmalkedar is traditionally associated with Saint Brendan (c. AD 484 – c. 577), but also with a local saint, Maolcethair (Maol Céadair, Maol Céaltair, Malkedar; died 636).\n\nThe surviving church dates to the mid-12th century, with the chancel extended c. 1200.\n\nIt was a traditional assembly site for pilgrims, who followed the Saint's Road (\"Casán na Naomh\") northeast to Mount Brandon.\n\nSome of the rituals carried out by locals, like performing nine clockwise circuits of the site on Easter Sunday, or the boring of holes in standing stones, suggest remnants of Celtic religion; Kilmalkedar may well have been a religious site long before Christianity arrived.\n\nThe church resembles Cormac's Chapel on the Rock of Cashel (built 1127–1134). Its nave is with antae and steep gables. The chancel is externally. The doorway is a notable Hiberno-Romanesque piece. A hole in the east wall of the chancel is called \"the eye of the needle\"; if one can fit through it, one is certain to go to heaven.\n\nPre-Romanesque remains include a corbelled building, perhaps a monastic cell; an alphabet stone; an Ogham stone; a sundial; a stone cross; and some bullauns. One of the bullauns is associated with the mythical cow Glas Gaibhnenn.\n\nThe alphabet stone is carved with \"DNI\" (\"domini\") and the Latin alphabet in uncial script, carved c. AD 550–600.\n\nThe Ogham stone (CIIC 187) reads (\"Name of Máel-Inbher son of Broccán\") and dates to c. AD 600.\n"}
{"id": "1638040", "url": "https://en.wikipedia.org/wiki?curid=1638040", "title": "Laminated veneer lumber", "text": "Laminated veneer lumber\n\nLaminated veneer lumber (LVL) is an engineered wood product that uses multiple layers of thin wood assembled with adhesives. It is typically used for headers, beams, rimboard, and edge-forming material. LVL offers several advantages over typical milled lumber: Made in a factory under controlled specifications, it is stronger, straighter, and more uniform. Due to its composite nature, it is much less likely than conventional lumber to warp, twist, bow, or shrink. LVL is a type of structural composite lumber, comparable to Glued laminated timber (Gluelam) but with a higher allowable stress. \n\nStructural composite lumber, including LVL, are a relatively recent innovation. They are the result of new technology and economic pressure to make use of new species and smaller trees that cannot be used to make solid sawn lumber. While plywood became widespread by the early twentieth century, the invention of LVL wasn't until the 1980s after the invention of oriented strand board. The National Design Specification for Wood Construction is generally updated on a 3- to 5-year cycle. The 1991 release is especially significant in that it is the first release which mentions LVL. LVL is mentioned as a subcategory of structural glued laminated timber. The first explorations into engineered lumber happened during World War II in America. In 1942, an increased demand for wood caused a sudden timber shortage. The war industry utilized a panel material developed by the Homasote Company of Trenton, New Jersey made of wood pulp and ground newspaper to be used in place of siding and sheathing for buildings. The invention of laminated veneer lumber as known today can be attributed to Arthur Troutner. While glue laminated wood veneers were in use since the middle of the 19th century on a small scale for furniture and pianos, Troutner was the first to develop a laminated veneer lumber of a scale large enough to be used in construction. In 1971 \"Micro=Lam LVL\" was introduced. \"Micro=Lam LVL\" consisted of laminated veneer lumber billets 4 feet wide, 3-1/2 inches thick, and 80 feet long. Troutner proved the structural capabilities of his Micro=Lam product by building a house in Hagerman, Idaho using beams made of Micro=Lam. Most corporations considered Troutner's invention to be a niche product and it was not until the mid 1980s when logging became an environmental concern and corporations moved toward engineered lumber that LVL became widespread in use. \n\nLaminated veneer lumber is similar in appearance to plywood, although in plywood the veneers switch direction while stacking and in LVL the veneers all stack in the same direction. In LVL, the direction of the wood grain is always parallel to the length of the billet. The stacking of these veneers into a complete board, called a billet, creates a single piece of LVL sharing a common direction of wood grain. LVL is typically rated by the manufacturer for elastic modulus and allowable bending stress. Common elastic moduli are ; ; and ; and common allowable bending stress values are ; and . Although the creation of LVL is often proprietary and thus its make-up is largely dependent on individual manufacturers, in general one cubic meter of North American Lumber is composed of 97.54% Wood, 2.41% of Phenol formaldehyde resin, 0.02% of Phenol resorcinol formaldehyde resin, and 0.03% fillers. \n\nLVL is commonly manufactured in North America by companies that also manufacture I-joists. LVL is manufactured to sizes compatible with the depth of I-joist framing members for use as beams and headers. Additionally, some manufacturers further cut LVL into sizes for use as chord-members on I-joists. In 2012, North American LVL manufacturers produced more than 1.2 million cubic metres (43.4 million cubic feet) of LVL in 18 different facilities, and in 2013 the production increased with more than 14%. It is not coincidental that LVL mills are often co-located with I-joist manufacturing facilities as many builders use a combination of I-joists and LVL in floor and roof assemblies. \n\nBecause it is specifically sized to be compatible with I-joist floor framing, residential builders and building designers like the combination of I-joist and LVL floor and roof assemblies. LVL is considered to be a highly reliable building material that provides many of the same attributes associated with large sized timbers. LVL can also be used in combination with gluelam as an outer gluelam tension lam to increase the strength of the gluelam beam. However, due to the fact that the assembly adhesives limit the penetration of chemicals typically used to treat outdoor-rated lumber, LVL may not be suitable for outdoor load-bearing use. A deck built using pressure-treated LVL collapsed due to internal rotting of the twelve-year-old LVL components, although the LVL beams had passed regular visual inspections. The breakdown of LVL end uses in North America is 33% new single family residential construction, 25% residential renovations and upkeep, 8% new non-residential construction and 34% manufacturing furniture and other products.\n\nLVL belongs to the category of engineered wood called structural composite lumber. Other members of this category are parallel strand lumber (PSL) and laminated strand lumber (LSL). All members of this category are strong and predictable, and are thus interchangeable for some applications. PSL is made from veneers that are cut up into long strands and oriented parallel to its length before being compressed into its final shape. LSL is also made from strands rather than veneer, although the strands are shorter and aligned with less precision than PSL and is created as billets that are like a thick version of oriented strand board. Billets of PSL and LVL are very similar although their sizes are different. Billets of PSL can be as large as 12 inches wide and 60 feet long while LVL can range up to 4 feet wide and 80 feet long.\n\n\n\"Deck Collapse Conclusions\"\n"}
{"id": "60828", "url": "https://en.wikipedia.org/wiki?curid=60828", "title": "Lepton", "text": "Lepton\n\nIn particle physics, a lepton is an elementary particle of half-integer spin (spin ) that does not undergo strong interactions. Two main classes of leptons exist: charged leptons (also known as the electron-like leptons), and neutral leptons (better known as neutrinos). Charged leptons can combine with other particles to form various composite particles such as atoms and positronium, while neutrinos rarely interact with anything, and are consequently rarely observed. The best known of all leptons is the electron.\n\nThere are six types of leptons, known as \"flavours\", grouped in three \"generations\". The first-generation Leptons, also called \"electronic leptons\", comprise the electron () and the electron neutrino (); the second are the \"muonic leptons\", comprising the muon () and the muon neutrino (); and the third are the \"tauonic leptons\", comprising the tau () and the tau neutrino (). Electrons have the least mass of all the charged leptons. The heavier muons and taus will rapidly change into electrons and neutrinos through a process of particle decay: the transformation from a higher mass state to a lower mass state. Thus electrons are stable and the most common charged lepton in the universe, whereas muons and taus can only be produced in high energy collisions (such as those involving cosmic rays and those carried out in particle accelerators).\n\nLeptons have various intrinsic properties, including electric charge, spin, and mass. Unlike quarks however, leptons are not subject to the strong interaction, but they are subject to the other three fundamental interactions: gravitation, the weak interaction, and to electromagnetism, of which the latter is proportional to charge, and is thus zero for the electrically neutral neutrinos.\n\nFor every lepton flavor there is a corresponding type of antiparticle, known as an antilepton, that differs from the lepton only in that some of its properties have equal magnitude but opposite sign. According to certain theories, neutrinos may be their own antiparticle. It is not currently known whether this is the case.\n\nThe first charged lepton, the electron, was theorized in the mid-19th century by several scientists and was discovered in 1897 by J. J. Thomson. The next lepton to be observed was the muon, discovered by Carl D. Anderson in 1936, which was classified as a meson at the time. After investigation, it was realized that the muon did not have the expected properties of a meson, but rather behaved like an electron, only with higher mass. It took until 1947 for the concept of \"leptons\" as a family of particle to be proposed. The first neutrino, the electron neutrino, was proposed by Wolfgang Pauli in 1930 to explain certain characteristics of beta decay. It was first observed in the Cowan–Reines neutrino experiment conducted by Clyde Cowan and Frederick Reines in 1956. The muon neutrino was discovered in 1962 by Leon M. Lederman, Melvin Schwartz, and Jack Steinberger, and the tau discovered between 1974 and 1977 by Martin Lewis Perl and his colleagues from the Stanford Linear Accelerator Center and Lawrence Berkeley National Laboratory. The tau neutrino remained elusive until July 2000, when the DONUT collaboration from Fermilab announced its discovery.\n\nLeptons are an important part of the Standard Model. Electrons are one of the components of atoms, alongside protons and neutrons. Exotic atoms with muons and taus instead of electrons can also be synthesized, as well as lepton–antilepton particles such as positronium.\n\nThe name \"lepton\" comes from the Greek \"leptós\", \"fine, small, thin\" (neuter nominative/accusative singular form: λεπτόν \"leptón\"); the earliest attested form of the word is the Mycenaean Greek , \"re-po-to\", written in Linear B syllabic script. \"Lepton\" was first used by physicist Léon Rosenfeld in 1948:\n\nFollowing a suggestion of Prof. C. Møller, I adopt—as a pendant to \"nucleon\"—the denomination \"lepton\" (from λεπτός, small, thin, delicate) to denote a particle of small mass.\n\nThe etymology incorrectly implies that all the leptons are of small mass. When Rosenfeld named them, the only known leptons were electrons and muons, whose masses are indeed small compared to nucleons—the mass of an electron () and the mass of a muon (with a value of ) are fractions of the mass of the \"heavy\" proton (). However, the mass of the tau (discovered in the mid 1970s) () is nearly twice that of the proton, and about 3,500 times that of the electron.\n\nThe first lepton identified was the electron, discovered by J.J. Thomson and his team of British physicists in 1897. Then in 1930 Wolfgang Pauli postulated the electron neutrino to preserve conservation of energy, conservation of momentum, and conservation of angular momentum in beta decay. Pauli theorized that an undetected particle was carrying away the difference between the energy, momentum, and angular momentum of the initial and observed final particles. The electron neutrino was simply called the neutrino, as it was not yet known that neutrinos came in different flavours (or different \"generations\").\n\nNearly 40 years after the discovery of the electron, the muon was discovered by Carl D. Anderson in 1936. Due to its mass, it was initially categorized as a meson rather than a lepton. It later became clear that the muon was much more similar to the electron than to mesons, as muons do not undergo the strong interaction, and thus the muon was reclassified: electrons, muons, and the (electron) neutrino were grouped into a new group of particles—the leptons. In 1962, Leon M. Lederman, Melvin Schwartz, and Jack Steinberger showed that more than one type of neutrino exists by first detecting interactions of the muon neutrino, which earned them the 1988 Nobel Prize, although by then the different flavours of neutrino had already been theorized.\n\nThe tau was first detected in a series of experiments between 1974 and 1977 by Martin Lewis Perl with his colleagues at the SLAC LBL group. Like the electron and the muon, it too was expected to have an associated neutrino. The first evidence for tau neutrinos came from the observation of \"missing\" energy and momentum in tau decay, analogous to the \"missing\" energy and momentum in beta decay leading to the discovery of the electron neutrino. The first detection of tau neutrino interactions was announced in 2000 by the DONUT collaboration at Fermilab, making it the latest particle of the Standard Model to have been directly observed, apart from the Higgs boson, which has been discovered in 2012.\n\nAlthough all present data is consistent with three generations of leptons, some particle physicists are searching for a fourth generation. The current lower limit on the mass of such a fourth charged lepton is , while its associated neutrino would have a mass of at least .\n\nLeptons are spin- particles. The spin-statistics theorem thus implies that they are fermions and thus that they are subject to the Pauli exclusion principle: No two leptons of the same species can be in exactly the same state at the same time. Furthermore, it means that a lepton can have only two possible spin states, namely up or down.\n\nA closely related property is chirality, which in turn is closely related to a more easily visualized property called helicity. The helicity of a particle is the direction of its spin relative to its momentum; particles with spin in the same direction as their momentum are called \"right-handed\" and otherwise they are called \"left-handed\". When a particle is massless, the direction of its momentum relative to its spin is frame independent, while for massive particles it is possible to 'overtake' the particle by a Lorentz transformation flipping the helicity. Chirality is a technical property (defined through the transformation behaviour under the Poincaré group) that agrees with helicity for (approximately) massless particles and is still well defined for massive particles.\n\nIn many quantum field theories, such as quantum electrodynamics and quantum chromodynamics, left- and right-handed fermions are identical. However, the Standard Model's Weak interaction, treats left-handed and right-handed fermions are asymmetrically: Only left-handed fermions (and right-handed anti-fermions) participate in the weak interaction. This is an example of parity violation explicitly written into the model. In the literature, left-handed fields are often denoted by a capital \"L\" subscript (e.g. the normal electron: e) and right-handed fields are denoted by a capital \"R\" subscript (e.g. a positron e).\n\nRight-handed neutrinos and left-handed anti-neutrinos have no possible interaction with other particles (\"see\" sterile neutrinos) and so are not a functional part of the Standard Model, although their exclusion is not a strict requirement; they are sometimes listed in particle tables to emphasize that they would have no active role if included in the model. Even though electrically charged particles (electron, muon, or tau) do not engage in the weak interaction specifically, they can still interact electrically, and hence still participate in the combined electro-weak force, although with a different strengths (\"Y\").\n\nOne of the most prominent properties of leptons is their electric charge, \"Q\". The electric charge determines the strength of their electromagnetic interactions. It determines the strength of the electric field generated by the particle (see Coulomb's law) and how strongly the particle reacts to an external electric or magnetic field (see Lorentz force). Each generation contains one lepton with \"Q\" = −\"e\" (conventionally the charge of a particle is expressed in units of the elementary charge) and one lepton with zero electric charge. The lepton with electric charge is commonly simply referred to as a 'charged lepton' while the neutral lepton is called a neutrino. For example, the first generation consists of the electron with a negative electric charge and the electrically neutral electron neutrino .\n\nIn the language of quantum field theory, the electromagnetic interaction of the charged leptons is expressed by the fact that the particles interact with the quantum of the electromagnetic field, the photon. The Feynman diagram of the electron-photon interaction is shown on the right.\n\nBecause leptons possess an intrinsic rotation in the form of their spin, charged leptons generate a magnetic field. The size of their magnetic dipole moment \"μ\" is given by\nwhere \"m\" is the mass of the lepton and \"g\" is the so-called g-factor for the lepton. First order approximation quantum mechanics predicts that the g-factor is 2 for all leptons. However, higher order quantum effects caused by loops in Feynman diagrams introduce corrections to this value. These corrections, referred to as the anomalous magnetic dipole moment, are very sensitive to the details of a quantum field theory model and thus provide the opportunity for precision tests of the standard model. The theoretical and measured values for the electron anomalous magnetic dipole moment are within agreement within eight significant figures.\n\nIn the Standard Model, the left-handed charged lepton and the left-handed neutrino are arranged in doublet that transforms in the spinor representation (\"T\" = ) of the weak isospin SU(2) gauge symmetry. This means that these particles are eigenstates of the isospin projection \"T\" with eigenvalues and − respectively. In the meantime, the right-handed charged lepton transforms as a weak isospin scalar (\"T\" = 0) and thus does not participate in the weak interaction, while there is no evidence that a right-handed neutrino exists at all.\n\nThe Higgs mechanism recombines the gauge fields of the weak isospin SU(2) and the weak hypercharge U(1) symmetries to three massive vector bosons (, , ) mediating the weak interaction, and one massless vector boson, the photon, responsible for the electromagnetic interaction. The electric charge \"Q\" can be calculated from the isospin projection \"T\" and weak hypercharge \"Y\" through the Gell-Mann–Nishijima formula,\nTo recover the observed electric charges for all particles, the left-handed weak isospin doublet must thus have \"Y\" = −1, while the right-handed isospin scalar e must have \"Y\" = −2. The interaction of the leptons with the massive weak interaction vector bosons is shown in the figure on the left.\n\nIn the Standard Model, each lepton starts out with no intrinsic mass. The charged leptons (i.e. the electron, muon, and tau) obtain an effective mass through interaction with the Higgs field, but the neutrinos remain massless. For technical reasons, the masslessness of the neutrinos implies that there is no mixing of the different generations of charged leptons as there is for quarks. This is in close agreement with current experimental observations.\n\nHowever, it is known from experiments—most prominently from observed neutrino oscillations—that neutrinos do in fact have some very small mass, probably less than . This implies the existence of physics beyond the Standard Model. The currently most favoured extension is the so-called seesaw mechanism, which would explain both why the left-handed neutrinos are so light compared to the corresponding charged leptons, and why we have not yet seen any right-handed neutrinos.\n\nThe members of each generation's weak isospin doublet are assigned leptonic numbers that are conserved under the Standard Model. Electrons and electron neutrinos have an \"electronic number\" of \"L\" = 1, while muons and muon neutrinos have a \"muonic number\" of \"L\" = 1, while tau particles and tau neutrinos have a \"tauonic number\" of \"L\" = 1. The antileptons have their respective generation's leptonic numbers of −1.\n\nConservation of the leptonic numbers means that the number of leptons of the same type remains the same, when particles interact. This implies that leptons and antileptons must be created in pairs of a single generation. For example, the following processes are allowed under conservation of leptonic numbers:\n\nbut not these:\n\nHowever, neutrino oscillations are known to violate the conservation of the individual leptonic numbers. Such a violation is considered to be smoking gun evidence for physics beyond the Standard Model. A much stronger conservation law is the conservation of the total number of leptons (\"L\"), conserved even in the case of neutrino oscillations, but even it is still violated by a tiny amount by the chiral anomaly.\n\nThe coupling of the leptons to gauge bosons are flavour-independent (i.e., the interactions between leptons and gauge bosons are the same for all leptons). This property is called lepton universality and has been tested in measurements of the tau and muon lifetimes and of Z boson partial decay widths, particularly at the Stanford Linear Collider (SLC) and Large Electron-Positron Collider (LEP) experiments.\n\nThe decay rate () of muons through the process → + + is approximately given by an expression of the form (see muon decay for more details)\n\nwhere \"K\" is some constant, and \"G\" is the Fermi coupling constant. The decay rate of tau particles through the process → + + is given by an expression of the same form\n\nwhere \"K\" is some constant. Muon–Tauon universality implies that \"K\" = \"K\". On the other hand, electron–muon universality implies\n\nThis explains why the branching ratios for the electronic mode (17.85%) and muonic (17.36%) mode of tau decay are equal (within error).\n\nUniversality also accounts for the ratio of muon and tau lifetimes. The lifetime of a lepton (\"τ\") is related to the decay rate by\n\nwhere \"B\"(x → y) and (x → y) denotes the branching ratios and the resonance width of the process x → y .\n\nThe ratio of tau and muon lifetime is thus given by\n\nUsing the values of the 2008 \"Review of Particle Physics\" for the branching ratios of muons and tau yields a lifetime ratio of ~, comparable to the measured lifetime ratio of ~. The difference is due to \"K\" and \"K\" not actually being constants: They depend on the mass of leptons.\n\nRecent tests of lepton universality in B meson decays, performed by the LHCb, BaBar and Belle experiments, have shown consistent deviations from the Standard Model predictions. However the combined statitical and systematic significance is not yet high enough to claim an observation of new physics.\n\n\n\n"}
{"id": "2055346", "url": "https://en.wikipedia.org/wiki?curid=2055346", "title": "Lyoluminescence", "text": "Lyoluminescence\n\nLyoluminescence refers to the emission of light while dissolving a solid into a liquid solvent. It is actually a form of chemiluminescence. The most common lyoluminescent effect is seen when solid samples which have been heavily irradiated by ionizing radiation are dissolved in water. The total amount of light emitted by the material increases proportionally with the total radiation dose received by the material up to a certain level called the saturation value. \n\nMany gamma-irradiated substances are known to lyoluminescence; these include spices, powdered milk, soups, cotton and paper. While the broad variety of materials which exhibit lyoluminescence confounds explanation by a single common mechanism there is a common feature to the phenomenon, the production of free radicals in solution. Lyoluminescence intensity can be increased by performing the dissolution of the solid in a solution containing conventionally chemiluminescent compounds such as luminol. These are thus called lyoluminescence sensitizers.\n"}
{"id": "1693080", "url": "https://en.wikipedia.org/wiki?curid=1693080", "title": "Magnetosheath", "text": "Magnetosheath\n\nThe magnetosheath is the region of space between the magnetopause and the bow shock of a planet's magnetosphere. The regularly organized magnetic field generated by the planet becomes weak and irregular in the magnetosheath due to interaction with the incoming solar wind, and is incapable of fully deflecting the highly charged particles. The density of the particles in this region is considerably lower than what is found beyond the bow shock, but greater than within the magnetopause, and can be considered a transitory state.\n\nScientific research into the exact nature of the magnetosheath has been limited due to a longstanding misconception that it was a simple byproduct of the bow shock/magnetopause interaction and had no inherently important properties of its own. Recent studies indicate, however, that the magnetosheath is a dynamic region of turbulent plasma flow that may play an important role in the structure of the bow shock and the magnetopause, and may help to dictate the flow of energetic particles across those boundaries.\n\nThe Earth's magnetosheath typically occupies the region of space approximately 10 Earth radii on the upwind (Sun-facing) side of the planet, extending significantly farther out on the downwind side due to the pressure of the solar wind. The exact location and width of the magnetosheath does depend on variables such as solar activity.\n\n\n"}
{"id": "17937110", "url": "https://en.wikipedia.org/wiki?curid=17937110", "title": "Maritsa 3 Power Plant", "text": "Maritsa 3 Power Plant\n\nMaritsa 3 Thermal Power Plant () is a power plant situated near the city of Dimitrovgrad, southern Bulgaria. It has an installed capacity of 120 MW.\n\n\n"}
{"id": "9206499", "url": "https://en.wikipedia.org/wiki?curid=9206499", "title": "Metal–insulator transition", "text": "Metal–insulator transition\n\nMetal–insulator transitions are transitions from a metal (material with good electrical conductivity of electric charges) to an insulator (material where conductivity of charges is quickly suppressed). These transitions can be achieved by tuning various ambient parameters such as pressure or, in case of a semiconductor, doping.\n\nThe basic distinction between metals and insulators was proposed by Bethe, Sommerfeld and Bloch in 1928/1929. It distinguished between conducting metals and nonconducting insulators. However, in 1937 de Boer and Evert Verwey reported that many transition-metal oxides (such as NiO) with a partially filled d-band were poor conductors, often insulating. In the same year, the importance of the electron-electron correlation was stated by Peierls. Since then, these materials as well as others exhibiting a transition between a metal and an insulator have been extensively studied, e.g. by Sir Nevill Mott, after whom the insulating state is named Mott insulator.\n\nThe classical band structure of solid state physics predicts the Fermi level to lie in a band gap for insulators and in the conduction band for metals, which means metallic behavior is seen for compounds with partially filled bands. However, some compounds have been found which show insulating behavior even for partially filled bands. This is due to the electron-electron correlation, since electrons cannot be seen as noninteracting. Mott considers a lattice model with just one electron per site. Without taking the interaction into account, each site could be occupied by two electrons, one with spin up and one with spin down. Due to the interaction the electrons would then feel a strong Coulomb repulsion, which Mott argued splits the band in two: The lower band is then occupied by the first electron per site, the upper by the second. If each site is only occupied by a single electron the lower band is completely filled and the upper band completely empty, the system thus a so-called Mott insulator.\n\n"}
{"id": "35430753", "url": "https://en.wikipedia.org/wiki?curid=35430753", "title": "Ministry of Petroleum and Mining", "text": "Ministry of Petroleum and Mining\n\nThe Ministry of Petroleum is a ministry of the Government of South Sudan. The incumbent minister is Ezekiel Lul Gatkuoth, the ministry of petroleum contributes more than 90% of the south Sudan total income through oil production and exportation to different counties through north sudan pipe line from the oil fields in south sudan to the port sudan in red sea. the government of sudan is heavily taxing south sudan for using the pipe line, and with the recent declined in the oil prices in the international market, this declined in the oil prices has really affected the economy of the country. \n"}
{"id": "24642315", "url": "https://en.wikipedia.org/wiki?curid=24642315", "title": "Mootral", "text": "Mootral\n\nMootral is the name given to a programme to reduce methane emissions from ruminant animals, chiefly cows and sheep, but also goats. Methane is a major target greenhouse gas and in the 4th protocol report of the Intergovernmental Panel on Climate Change (IPCC) is recommended to increase from a x23 to x72 multiplier because of the magnitude of its effect relative to carbon dioxide and short longevity in Earth's atmosphere.\n\nThe Mootral natural feed supplement is produced by Neem Biotech. The active ingredient is an organic organosulfur compound (normally found in garlic), Research at the University of Aberystwyth, Wales has demonstrated up to a 94% reduction in methane production.\n\nMootral is the brand for the project, designed to certify approved methane reduction programmes in which corporates may participate and prove the provenance of their involvement. Proceeds of emission trading credits are part donated to farmers, who will receive Mootral without charge for adding to their cattle feed. Further surpluses return to programme participants and charities allied to greenhouse gas reduction research.\n\nMootral has attracted much recent attention as runner-up in the FT Global Climate Challenge and Dutch Postcode Lottery. \n\nMootral is a finalist in the Shell/BBC/Newsweek World Challenge 2009 as one of the 12 most promising solutions to climate change.\n\n"}
{"id": "11047210", "url": "https://en.wikipedia.org/wiki?curid=11047210", "title": "Nayakrishi", "text": "Nayakrishi\n\nThe New Agriculture Movement () is an agricultural movement in Bangladesh that opposes the use of Western pesticides and genetically altered seeds.\n\nThe Movement began in response to environmental hazards that were believed to have been started by the use of insecticides and nematicides in the growing of crops.\n\nIn addition to health concerns the movement strongly promotes organic farming, and the use for food and animal fodder of plants which are often regarded as weeds. This is seen as both furthering self-sufficiency and distancing Bangladesh from Western development firms and the International Monetary Fund.\n\nNayakrishi has a special emphasis on supporting women, with a programme of supplying cattle to poor female-headed households which are kept until a calf is born, when the original animal is passed on to another family, and the organisation of the Specialised Women's Seed Network to collect seeds from local varieties of crops.\n\n"}
{"id": "44483722", "url": "https://en.wikipedia.org/wiki?curid=44483722", "title": "Newstead Gasworks", "text": "Newstead Gasworks\n\nNewstead Gasworks is a heritage-listed former gasometer at 70 Longland Street, Teneriffe, City of Brisbane, Queensland, Australia. It was built from 1873 to 1887. It is also known as Brisbane Gas Company Gasworks and Newstead Gasworks No.2 gasholder. It was added to the Queensland Heritage Register on 24 June 2005.\n\nThe Newstead Gasworks was established in 1887, as the second Brisbane gasworks. The surviving Newstead Gasworks No.2 gasholder (remnants) and guide framing was erected at that time and was probably moved from the original gas works site at Petrie Bight where it was erected in 1873.\n\nFrom 1861 to 1864, Brisbane's population more than doubled, to 12,551. In the mid-1860s, Brisbane's infrastructure blossomed, with construction of the first cross-river bridge, a new Brisbane Town Hall, a vastly improved water supply and its first gasworks. Commercial gas supply originated in London in 1812, Sydney in 1841 and Melbourne in 1856.\n\nThe Brisbane Municipal Council was anxious to provide street lighting, for which gas was seen as the only feasible system. Earlier in the decade, the Colonial Government, supposedly for health reasons, refused permission for the Council to establish a gasworks on a site at Petrie's Bight. On the same site, however, in 1864, the Government authorised private enterprise to establish this new public utility. Central to these decisions was the Minister for Lands and Works, Arthur Macalister, then at the epicentre of friction between the Government and the Brisbane Council, friction which has erupted on several occasions ever since.\n\nOne of the founding directors of the Company in 1864 was Lewis Adolphus Bernays, secretary for the first 37 years, and eventually a board member, of the (Brisbane) Board of Waterworks. Bernays, also listed in Who's Who from 1851-1905, was prominent in a number of Queensland organisations including the Acclimatisation Society, and held the position of clerk of the Queensland Legislative Assembly.\n\nBrisbane Gas Company production started at Petrie Bight in November 1865. The streets of Brisbane were lit. Soon after, however, council's arrears of payment resulted in its gas being turned off, with the streets plunged back into darkness. The streets remained unlit until the council's (and the colony's) financial situation had recovered sufficiently, in 1870. By 1873, the boot was on the other foot. The council's cash flow had recovered, but gas flow could not satisfy demand. The Company duly solved the problem by installing another, larger (No.2) gasholder (probably known then as a gasometer).\n\nThe 81-year-long association of the Cowlishaw family with the board of the company also started in 1873. James Cowlishaw, later the Hon. James Cowlishaw, Member of the Queensland Legislative Council, an architect, was chairman for 41 years. He served on the board for 45 years. His brother George Cowlishaw also served on the board for 28 years from 1884. James Cowlishaw was succeeded in the chair by his son Thomas Owen Cowlishaw, who served for 34 years until 1954. A fourth Cowlishaw, George Owen Cowlishaw, was involved with the Company in 1939.\n\nThe main demand for gas during the 1870s was probably for lighting, and the Municipal Council was almost certainly the Brisbane Gas Company's biggest customer. After the financial woes of the later 1860s there followed two decades of steady growth in Brisbane, reflected in an increase of demand for gas, for both lighting and for domestic and industrial fuel. Supply was expanded to, among other places, South Brisbane. The Company had outgrown the Petrie's Bight site, and acquired of suitable land at Newstead, in 1883. Further adjoining land was bought in 1885 to facilitate the company's first major expansion.\n\nThe company's engineer for the shift from Petrie Bight to Newstead was JH Tomlinson, arriving in 1880 from a previous appointment in Birmingham. He presumably managed the design, supply and erection of the new gasworks. The equipment, much of it imported from England, was landed at the Company's wharf at the Brisbane River frontage of the site. The company first manufactured and stored town gas at Newstead in 1887 after a period of 22 years manufacture at Petrie Bight.\n\nAs part of the shift to Newstead, the Petrie's Bight gasholder No.2, constructed in 1873 from puddled iron, was re-erected on the recently acquired land at Longland Street. No.1 gasholder remained at Petrie Bight. At this time, the first weight-controlled governor would have been installed in its governor house.\n\nJust as had occurred in the early 1860s, the land boom of the 1880s was reflected in booming demand for gas. Events proved that there was room for a second gasworks. The South Brisbane Gas and Light Company Limited was formed in 1885, entered the market very aggressively, and produced its first gas at the end of 1886. By this time, gas was increasingly in use for domestic fuel as well as for lighting. A price-cutting war lasted until 1889 when the companies, without any concerns about restrictive trade practices, carved Brisbane up into north and south of the river as their respective marketing territories. It was not until the Gas Act of 1916 that the government saw fit to look more closely at the cosy arrangements between gas companies, which by that stage included the Wynnum and Manly Gas and Lighting Company Limited, formed in 1912. The Act dealt with product quality and pressure, and provided for gas examiners. Price was also dealt with. (A later piece of legislation, the Profiteering Prevention Act, also encompassed gas).\n\nA further consideration in the 1880s was the emergence in Brisbane of electricity generation for power and lighting. Eventually, as electricity technology advanced, gas lighting would literally fade into the background, the demand for gas as fuel outstripping the demand for use for lighting. Active defence was mounted to protect the gas companies' interests in lighting, and it was not until 1917 that the first permanent electric lighting took to the streets of Brisbane.\n\nThe demise of gas lighting would not have been immediate, and the demand for gas continued to grow. In 1929, No.3 gasholder at Doggett Street was commissioned, a four-lift frame-guided steel tank gasholder, again made from puddled iron, with a capacity of , by far the largest in Brisbane.\n\nIn 1937, gasholder No.2 was fitted with a new crown of riveted steel plate.\n\nIn the post-World War II period, demand for gas again boomed, to such an extent that at one stage some 5000 newly constructed northside dwellings were completed without their gas having been connected. The core of the problem lay in English gasworks manufacturers' inability to deliver. A major retort augmentation planned for completion in 1950 finally came on stream in 1954, but not until an emergency measure had been taken to erect a Humphreys and Glasgow water-gas plant, for which coke was the feedstock. This came on stream just in time for the winter of 1953.\n\nWhen the Queensland Railways were converted to diesel fuel in the 1960s, it was judged that there would be a risk of serious damage if a locomotive were to derail near No.2 gasholder. Accordingly, a reinforced concrete wall was built as a shield between the railway spur line and the gasholder. Diesel locomotives were seen as a greater risk than their apparently cuddly coal-fired steam counterparts that had used the same track, in the same proximity, for the previous half-century.\n\nBy 1965, No.1 gasholder in Ann Street had been dismantled. In 1999, so had No.3, and No.2 has been disabled by splitting a hole in the crown plating, leaving the inside of the gasholder exposed to a moist oxidising atmosphere, ideal conditions for rapid rust.\n\nIn 2008, planning approval was given for the Gasworks Newstead urban redevelopment project, replacing the former industrial area in Newstead. The project incorporated the gasometer site and repurposed it as an outdoor amphitheatre.\n\nThe foregoing history has been compiled in the context of gas storage, with the emphasis on the remaining No.2 gasholder (remnants) and guide framing, which has stored and delivered both town gas and natural gas.\n\nThe sources of gasholder feed have been a series of gas manufacturing plants on the site, of which there are none left. The size and programming of the manufacturing plants have been determined by several factors:\n\nThe following is a brief history of the major manufacturing installations at Newstead since 1887. The first manufacture at Newstead in 1887 was in a retort house containing ten benches of manually operated horizontal retorts. Feedstock was Ipswich coking coal. Output was \"town\" gas.\n\nA new per day West's vertical retort house, embarked upon during World War I, was eventually completed in 1920, with six settings of eight vertical retorts long.\n\nIn 1926, a Whessoe washer was imported, used to remove ammonia and naphthalene after completion of all other purification.\n\nA further ten benches were added to the horizontal retort house in 1927 when the horizontal retort house was revamped to mechanical operation.\n\nSeveral additions were made in 1947, in preparation for a major increase in plant capacity planned to be on stream by 1950. A Woodhall-Duckham electrostatic de-tarrer was imported from England, and a Livesay washer was constructed locally. These were of 4,000,000 cu. ft. per day capacity, matching the capacity of the proposed new Glover-West plant on order from England. Also, constructed locally to (English) Holmes design, multi-film washers. Additional Connersville meters were also installed.\n\nThe new Glover-West vertical retort carbonising plant, with a capacity of 4,000,000 cu. ft. per day, was eventually opened in 1954. In view of delays in supply of specialised plant from post-war England, a Humphreys-Glasgow carburetted water-gas plant was installed as an emergency measure by MR Hornibrook Pty Ltd, prior to the winter of 1953. While less efficient than the carbonisation process, this process, one of several known as gasification, used coke as its feedstock and was more easily automated. It is probable that the Bryan-Donkin diaphragm-operated governors, housed immediately to the north of and in series with the original governor, were installed as part of the 1954 revamp.\n\nWith the arrival of natural gas, the retort houses and the water-gas plant were duly retired from service, to be replaced, in 1968 with a Vickers-Zimmer plant, supplemented in 1973 by an Onia-Geigy plant ex Adelaide. These gasification plants synthesised town gas from oil refinery material and natural gas, until the eventual 1996 conversion of the older suburbs to natural gas.\n\nThe older carbonisation plants were demolished in 1994, and the gasification plants in 1999, along with gasholder No.3 in Doggett Street. In 2002, approval was given to demolish the brick governor houses and to remove associated machinery and pipe work, to demolish the storage shed and concrete safety wall, and to construct new commercial buildings and an adjacent road surrounding the gasometer. Approval was also given to excavate the site and remove its crown and inner gaslifts.\n\nAll items of gas manufacture, such as retort houses, exhausters, condensers, gasification plants, scrubbers, washers, purifiers, have been demolished. Most of these lay on the eastern side of the site. The weighbridge and the compressor house have also been demolished.\n\nThe only remaining items of historical engineering interest remaining on site include the Newstead Gasworks No.2 gasholder (remnants) and guide framing. The facility originally had a double-lift 500,000 cu. ft. capacity and was re-crowned in 1937.\n\nNewstead Gasworks No.2 gasholder (remnants and guide framing) was listed on the Queensland Heritage Register on 24 June 2005 having satisfied the following criteria.\n\nThe place is important in demonstrating the evolution or pattern of Queensland's history.\n\nThe Newstead Gasworks No.2 gasholder demonstrates the growth and development pattern of the reticulation of a major public utility, the gas supply and its infrastructure, in Brisbane and northern Brisbane suburbs from the 1860s through to the 1990s. Town gas was among the earliest of innovations that significantly improved the quality of life in the burgeoning city of Brisbane, particularly in the boom periods of the 1860s and 1880s.\n\nThe place demonstrates rare, uncommon or endangered aspects of Queensland's cultural heritage.\n\nThe remnants of the gasholder and guide framing represents the last of the frame-guided type remaining in Brisbane, and possibly in Queensland. The Newstead No.2 gasholder (remnants) and guide framing, constructed in 1873 from puddled iron and part of the former Petrie Bight gasworks (1865-1887), is significant for its rarity, its robustness and its association with Queensland's first gasworks and a major public utility for 125 years.\n\nThe place has potential to yield information that will contribute to an understanding of Queensland's history.\n\nThe place has the potential to yield information about the evolving technology of manufacture and distribution of town and natural gas in Brisbane and northern Brisbane suburbs over a period of 125 years.\n\nThe place is important in demonstrating the principal characteristics of a particular class of cultural places.\n\nOne of the principal characteristics of a gasworks was its gasholder storage and other necessarily robust but sensitive and reliable machinery and apparatus required for drawing from the storage. At the Newstead site, the remaining gasholder (remnants) and guide framing demonstrates some of these characteristics of the gasworks.\n\nThe place is important because of its aesthetic significance.\n\nLarge round structures are unusual and attractive, particularly in areas of predominantly angular commercial and industrial construction such as Newstead. Aesthetically, the cylindrical gasholder with its spherical crown, even when empty, was both interesting and elegant. The aesthetic is further enhanced by the slightly ornate frame-guide structure.\n\nThe place has a strong or special association with a particular community or cultural group for social, cultural or spiritual reasons.\n\nFor over 100 years, passers-by along Breakfast Creek Road, Longland and Ann Streets, as well as residents of and workers in the area, would have seen No.2 gasholder in various states of filling, silently performing its important function of storing and feeding out the gas supply. For these communities, the gasholder (remnants) and guide framing was and is an important landmark. The material remaining on the site carries with it a strong association with these communities.\n\nThe place has a special association with the life or work of a particular person, group or organisation of importance in Queensland's history.\n\nThe gasworks site has a special association with a vital public utility, Brisbane's gas supply, for over a century. Prominent figures with long and close associations with the gasworks included Lewis Adolphus Bernays (associated between 1864-1908) and the Hon James Cowlishaw (associated between 1873-1920).\n"}
{"id": "9107990", "url": "https://en.wikipedia.org/wiki?curid=9107990", "title": "Oleg Khinsagov", "text": "Oleg Khinsagov\n\nOleg Khinsagov () is a Russian citizen from Vladikavkaz, North Ossetia–Alania. On January 25, 2007 he was sentenced by a Georgian court for 8.5 years for smuggling 100 grams of highly enriched uranium.\n\nAccording to the Georgian authorities, in January 2006, Khinsagov together with a few Georgian citizens from the separatist region of South Ossetia was trying to sell 100 grams of highly enriched uranium. He claimed that the material is only a sample and he has more than 3 kilograms of the substance in his Vladikavkaz garage. Georgian police arranged meeting of Khinsagov with their Turkish-speaking agent introduced as a representative of a rich Muslim organization willing to buy the sample for $1 million US. At the meeting held on February 1, 2006 Khinsagov was arrested with 100 grams of a substance in two plastic pouches. The chemical analysis performed by an American Department of Energy Lab confirmed the substance as being a U-235 purity of 89.451 percent enriched Uranium that makes it a weapons-grade material.\n\nThe Georgian side accused Russian investigators in the lack of cooperation with the investigation. According to Shota Utiashvili, the head of the Georgian Interior Ministry's analytical department: \"We received the test results from Russian specialists. They confirmed that the substance was high-enriched uranium, but did not say anything about its origin.\". According to Rosatom the Georgian side did not provided enough material to pinpoint its origin, still the FSB report provided for the Georgian investigators confirmed the substance as being the highly enriched uranium and indicated it was processed more than ten years ago.\n\n"}
{"id": "15610058", "url": "https://en.wikipedia.org/wiki?curid=15610058", "title": "Omega-3 acid ethyl esters", "text": "Omega-3 acid ethyl esters\n\nOmega-3 acid ethyl esters are the omega-3 fatty acids eicosapentaenoic acid (EPA) and docosahexaenoic acid (DHA), i.e., the ester part, attached at one end to an ethanol molecule - the ethyl part. These FDA-approved prescription products are used in combination with changes in diet to lower blood triglyceride levels in adults with severe (≥ 500 mg/dL) hypertriglyceridemia. This was the first fish oil-derived product to be approved for use as a drug. The first approvals came in Europe in 2001. The first approval in the US came in 2004. These prescription products have been tested in clinical trials. In the US, omega-3 ethyl esters are also manufactured and sold as dietary supplements.\n\nOmega-3 acid ethyl esters are used in addition to changes in diet to reduce triglyceride levels in adults with severe (≥ 500 mg/dL) hypertriglyceridemia. In the European markets and other major markets outside the US, omega-3 acid ethyl esters are indicated for hypertriglyceridemia as a monotherapy, or in combination with a statin for patients with mixed dyslipidemia and as secondary prevention after heart attack in addition to other standard therapy (e.g. statins, anticoagulants, beta-blockers, and ACE-I).\n\nIntake of large doses (2.0 to 4.0 g/day) of long-chain omega-3 fatty acids as prescription drugs or dietary supplements are generally required to achieve significant (> 15%) lowering of triglycerides, and at those doses the effects can be significant (from 20% to 35% and even up to 45% in individuals with levels greater that 500 mg/dL). It appears that both eicosapentaenoic acid (EPA) and docosahexaenoic acid (DHA) lower triglycerides, but DHA appears to raise LDL-C (\"bad cholesterol\") more than EPA, while DHA raises HDL-C (\"good cholesterol\") while EPA does not.\n\nThere are other omega-3 fish oil based prescription drugs on the market that have similar uses and mechanisms of action.\n\n\nThere are many fish oil dietary supplements on the market. There appears to be little difference in effect between dietary supplement and prescription forms of omega-3 fatty acids as to ability to lower triglycerides, but the ethyl ester products work less well when taken on an empty stomach or with a low-fat meal. The ingredients of dietary supplements are not as carefully controlled as prescription products and have not been tested in clinical trials as such drugs have. Prescription omega-3 products are more concentrated, requiring fewer softgels for the same daily dose.\n\nSpecial caution should be taken with people who have fish and shellfish allergies. In addition, as with other omega-3 fatty acids, taking omega-3 acid ethyl esters puts people who are on anticoagulants at risk for prolonged bleeding time.\n\nSide effects include stomach ache, burping, and a bad taste; some people on very high doses (8g/day) in clinical trials had atrial fibrillation.\n\nOmega-3 acid ethyl esters have not been tested in pregnant women and are rated pregnancy category C; it is excreted in breast milk and the effects on infants are not known.\n\nAfter ingestion, omega-3 acid ethyl esters are metabolized mostly in the liver like other dietary fatty acids.\n\nOmega-3 acid ethyl esters, like other omega-3 fatty acid based drugs, appears to reduce production of triglycerides in the liver, and to enhance clearance of triglycerides from circulating very low-density lipoprotein (VLDL) particles; the way it does that is not clear, but potential mechanisms include increased breakdown of fatty acids; inhibition of diglyceride acyltransferase which is involved in biosynthesis of triglycerides in the liver; and increased activity of lipoprotein lipase in blood. \n\nThe active ingredient is concentrated omega-3-acid ethyl esters that are made from fish body oils that are purified and esterified, For the Lovaza product, each 1000 mg softgel capsule contains 840 mg omega-3 fatty acids: eicosapentaenoic acid ethyl ester (460 mg) and docosahexaenoic acid ethyl ester (380 mg).\n\nPronova BioPharma ASA had its roots in Norway's codfish liver oil industry; it was founded in 1991 as a spinout from the JC Martens company, which in turn was founded in \n1838 in Bergen, Norway. Pronova developed the concentrated omega-3 acid ethyl esters formulation that is the active pharmaceutical ingredient of Lovaza.\n\nIt won approvals to market the drug, called Omacor in Europe (and initially in the US) in several European countries in 2001 after conducting a three and a half year trial in 11,000 subjects; it partnered with other companies like Pierre Fabre in France. In 2004 Pronova licensed the US and Puerto Rican rights to Reliant Therapeutics, the business model of which was in-licensing cardiovascular drugs. In that same year, Reliant and Pronova won FDA approval for the drug and it was launched in the US and Europe in 2005; global sales in 2005 were $144M and by 2008 they were $778M. In 2007 GlaxoSmithKline acquired Reliant for $1.65 billion in cash.\n\nIn 2009 generic companies Teva Pharmaceuticals and Par Pharmaceutical made clear their intentions to file Abbreviated New Drug Applications to bring generics to market, and in April 2009 Pronova sued them from infringing the key US patents covering Lovaza, US 5,656,667 (due to expire in April 2017) US 5,502,077 (exp March 2013) and in May 2012 a district court ruled in Pronova's favor, saying that the patents were valid. The generic companies appealed and in September 2013 the Federal Circuit reversed, saying that because more than one year before Pronova's predecessor company applied for a patent, it had sent samples of the fish oil used in Lovaza to a researcher for testing, and this constituted \"public use\" that made the patent invalid. Generic versions of Lovaza were introduced in America in April 2014.\n\nPronovo has continued to manufacture the ingredients in Lovaza, and in 2012 BASF announced it would acquire Pronova for $844 million, and the deal closed in 2013.\n\n"}
{"id": "14314003", "url": "https://en.wikipedia.org/wiki?curid=14314003", "title": "Palladium black", "text": "Palladium black\n\nPalladium black is a coarse, sponge-like form of elemental palladium which offers a large surface area for catalytic activity. It is used in organic synthesis as a catalyst for hydrogenation reactions.\n\nThe term palladium black is also used colloquially to refer to a black precipitate of elemental palladium, which forms via decomposition of various palladium complexes.\n\nPalladium black is typically prepared from palladium(II) chloride or palladium(II)-ammonium chloride. The palladium chloride process entails the formation of palladium hydroxide using lithium hydroxide followed by reduction under hydrogen gas while the palladium(II)-ammonium chloride route employs a solution formic acid followed by the precipitation of the catalyst using potassium hydroxide.\n\n"}
{"id": "13621696", "url": "https://en.wikipedia.org/wiki?curid=13621696", "title": "Pathatrix", "text": "Pathatrix\n\nPathatrix is a high volume recirculating immuno magnetic-capture system for the detection of pathogens in food and environmental samples. Pathatrix was used in 2006, linking an outbreak to contaminated spinach.\n\nThe Pathatrix system is being used by regulatory agencies and food companies around the world to enhance their pathogen detection capabilities.\n\nUnlike other detection methods Pathatrix allows the entire pre-enriched sample or large pooled samples to be recirculated over antibody-coated paramagnetic beads. It can specifically isolate pathogens directly from food samples and in conjunction with quantitative PCR can provide results within hours. It is also used to improve the performance of other rapid methods such as PCR, lateral flow, ELISA and chromgenic media by reducing or eliminating the need for lengthy pre enrichment and/or selective enrichment steps. The Pathatrix is useful in pathogen labs that would be running food samples and looking for foodborne diseases.\n\nThe Pathatrix is a rapid test method and Pathatrix pooling allows the screening of large numbers of food samples in a highly cost-effective way for specific pathogens such as E. coli O157, Salmonella or Listeria monocytogenes.\n\nThe Pathatrix will selectively bind and purify the target organism from a comprehensive range of complex food matrices (including raw ground beef, chocolate, peanut butter, leafy greens, spinach, tomatoes). The Pathatrix is the only microbial detection system that allows for the entire sample to be analyzed.\n\nTo perform a test using Pathatrix there are 4 major steps including sample preparation, pre-incubation, placing the sample in the Pathatrix, and washing and elution.\n\nOnce the food samples are ready they are placed in the five independent incubation pots providing heat that makes the Pathatrix more sensitive for the detection of the target organism. Tubing, made of silicone rubber, is then connected to form a loop. Finally magnetic beads particles, coated with an antibody specific to the target pathogen, are added.\n\nThe whole process runs for 180 minutes at a temperature between 30-37 degrees Celsius. After the incubation has taken place the target organism, now captured by the reagent, are now retained on the magnetic while unwanted debris is taken away. During the capture phase the microorganisms are removed from the system and further processed, otherwise colonies can be seen within as little as 16 hours.\n\n\n"}
{"id": "1391009", "url": "https://en.wikipedia.org/wiki?curid=1391009", "title": "Plasma stealth", "text": "Plasma stealth\n\nPlasma stealth is a proposed process to use ionized gas (plasma) to reduce the radar cross-section (RCS) of an aircraft. Interactions between electromagnetic radiation and ionized gas have been extensively studied for many purposes, including concealing aircraft from radar as stealth technology. Various methods might plausibly be able to form a layer or cloud of plasma around a vehicle to deflect or absorb radar, from simpler electrostatic or radio frequency discharges to more complex laser discharges. It is theoretically possible to reduce RCS in this way, but it may be very difficult to do so in practice.\n\nIn 1956, Arnold Eldredge, of General Electric, filed a patent application for an \"Object Camouflage Method and Apparatus,\" which proposed using a particle accelerator in an aircraft to create a cloud of ionization that would \"...refract or absorb incident radar beams.\" It is unclear who funded this work or whether it was prototyped and tested. U.S. Patent 3,127,608 was granted in 1964.\n\nDuring Project OXCART, the operation of the Lockheed A-12 reconnaissance aircraft, the CIA funded an attempt to reduce the RCS of the A-12's inlet cones. Known as Project KEMPSTER, this used an electron beam generator to create a cloud of ionization in front of each inlet. The system was flight tested but was never deployed on operational A-12s or SR-71s.\n\nIn 1992, Hughes Research Laboratory conducted a research project to study electromagnetic wave propagation in unmagnetized plasma. A series of high voltage spark gaps were used to generate UV radiation, which creates plasma via photoionization in a waveguide. Plasma filled missile radome were tested in an anechoic chamber for attenuation of reflection. At about the same time, R. J. Vidmar study the use of atmospheric pressure plasma as electromagnetic reflectors and absorbers. Other investigators also studied the case of a non-uniform magnetized plasma slab.\n\nDespite the apparent technical difficulty of designing a plasma stealth device for combat aircraft, there are claims that a system was offered for export by Russia in 1999. In January 1999, the Russian ITAR-TASS news agency published an interview with Doctor Anatoliy Koroteyev, the director of the Keldysh Research Center (FKA Scientific Research Institute for Thermal Processes), who talked about the plasma stealth device developed by his organization. The claim was particularly interesting in light of the solid scientific reputation of Dr. Koroteyev and the Institute for Thermal Processes, which is one of the top scientific research organizations in the world in the field of fundamental physics.\n\nThe \"Journal of Electronic Defense\" reported that \"plasma-cloud-generation technology for stealth applications\" developed in Russia reduces an aircraft's RCS by a factor of 100 (20 dB). According to this June 2002 article, the Russian plasma stealth device has been tested aboard a Sukhoi Su-27IB fighter-bomber. The Journal also reported that similar research into applications of plasma for RCS reduction is being carried out by Accurate Automation Corporation (Chattanooga, Tennessee) and Old Dominion University (Norfolk, Virginia) in the U.S.; and by Dassault Aviation (Saint-Cloud, France) and Thales (Paris, France).\n\nA plasma is a \"quasineutral\" (total electrical charge is close to zero) mix of ions (atoms which have been ionized, and therefore possess a net positive charge), electrons, and neutral particles (un-ionized atoms or molecules). Most plasmas are only partially ionized, in fact, the ionization degree of common plasma devices like fluorescent lamp is fairly low ( less than 1%). Almost all the matter in the universe is very low density plasma: solids, liquids and gases are uncommon away from planetary bodies. Plasmas have many technological applications, from fluorescent lighting to plasma processing for semiconductor manufacture.\n\nPlasmas can interact strongly with electromagnetic radiation: this is why plasmas might plausibly be used to modify an object's radar signature. Interaction between plasma and electromagnetic radiation is strongly dependent on the physical properties and parameters of the plasma, most notably the electron temperature and plasma density.\n\n\nPlasmas can have a wide range of values in both temperature and density; plasma temperatures range from close to absolute zero and to well beyond 10 kelvins (for comparison, tungsten melts at 3700 kelvins), and plasma may contain less than one particle per cubic metre, or be denser than lead. Electron temperature is usually expressed as electronvolt (eV), and 1 eV is equivalent to 11,604 K. Common plasmas temperature and density in fluorescent light tubes and semiconductor manufacturing processes are around several eV and 10per cm. For a wide range of parameters and frequencies, plasma is electrically conductive, and its response to low-frequency electromagnetic waves is similar to that of a metal: a plasma simply reflects incident low-frequency radiation. Low-frequency means it is lower than the characteristic electron plasma frequency. The use of plasmas to control the reflected electromagnetic radiation from an object (Plasma stealth) is feasible at suitable frequency where the conductivity of the plasma allows it to interact strongly with the incoming radio wave, and the wave can either be absorbed and converted into thermal energy, or reflected, or transmitted depending on the relationship between the radio wave frequency and the characteristic plasma frequency. If the frequency of the radio wave is lower than the plasma frequency, it is reflected. if it is higher, it transmit. If these two are equal, then resonance occurs. There are also another mechanism where reflection can be reduced. If the electromagnetic wave passes through the plasma, and is reflected by the metal, and the reflected wave and incoming wave are roughly equal in power, then they may form two phasors. When these two phasors are of opposite phase they can cancel each other out. In order to obtain substantial attenuation of radar signal, the plasma slab needs adequate thickness and density.\n\nPlasmas support a wide range of waves, but for unmagnetised plasmas, the most relevant are the Langmuir waves, corresponding to a dynamic compression of the electrons. For magnetised plasmas, many different wave modes can be excited which might interact with radiation at radar frequencies.\n\nPlasma layers around aircraft have been considered for purposes other than stealth. There are many research papers on the use of plasma to reduce aerodynamic drag. In particular, electrohydrodynamic coupling can be used to accelerate air flow near an aerodynamic surface. One paper considers the use of a plasma panel for boundary layer control on a wing in a low-speed wind tunnel. This demonstrates that it is possible to produce a plasma on the skin of an aircraft. Radioactive Xenon nuclear poison or Polonium isotopes when successfully suspended in generated plasma layers or doped into vehicle hulls, may be utilized in order for a reduction in radar cross-section by generating a plasma layer on the surface. If tunable this could shield against HMP/EMP and HERF weaponry or act as optical radiation pressure actuators.\n\nBoeing filed a series of patents related to the concept of plasma stealth. In US 7,744,039 B2, Jun. 2010, a system to control air flow with electrical pulses is described. In US 7,988,101 B2, Aug. 2011, a plasma generating device is used to create a plasma flow on the trailing edge, which can change its RCS. In US 8,016,246 B2 Sep. 2011, a plasma actuator system is used to camouflage weapon bay on a fighter when it is open. In US 8,016,247 B2, the plasma actuator system is described in detail, which is basically a dielectric barrier discharge device. In US 8,157,528 B1 Apr. 2012, a plasma actuating cascade array for use on rotor blade is described. In US 8,220,753 B2 Jul. 2012, a system for controlling airflow on wing surface with pulsed discharge is described.\n\nWhen electromagnetic waves, such as radar signals, propagate into a conductive plasma, ions and electrons are displaced as a result of the time varying electric and magnetic fields. The wave field gives energy to the particles. The particles generally return some fraction of the energy they have gained to the wave, but some energy may be permanently absorbed as heat by processes like scattering or resonant acceleration, or transferred into other wave types by mode conversion or nonlinear effects. A plasma can, at least in principle, absorb all the energy in an incoming wave, and this is the key to plasma stealth. However, plasma stealth implies a substantial reduction of an aircraft's RCS, making it more difficult (but not necessarily impossible) to detect. The mere fact of detection of an aircraft by a radar does not guarantee an accurate targeting solution needed to intercept the aircraft or to engage it with missiles. A reduction in RCS also results in a proportional reduction in detection range, allowing an aircraft to get closer to the radar before being detected.\n\nThe central issue here is frequency of the incoming signal. A plasma will simply reflect radio waves below a certain frequency (characteristic electron plasma frequency). This is the basic principle of short wave radios and long-range communications, because low-frequency radio signals bounce between the Earth and the ionosphere and may therefore travel long distances. Early-warning over-the-horizon radars utilize such low-frequency radio waves (typically lower than 50 MHz). Most military airborne and air defense radars, however, operate in VHF, UHF, and microwave band, which have frequencies higher than the characteristic plasma frequency of ionosphere, therefore microwave can penetrate the ionosphere and communication between the ground and communication satellites demonstrates is possible. (\"Some\" frequencies can penetrate the ionosphere).\n\nPlasma surrounding an aircraft might be able to absorb incoming radiation, and therefore reduces signal reflection from the metal parts of the aircraft: the aircraft would then be effectively invisible to radar at long range due to weak signals received. A plasma might also be used to modify the reflected waves to confuse the opponent's radar system: for example, frequency-shifting the reflected radiation would frustrate Doppler filtering and might make the reflected radiation more difficult to distinguish from noise.\n\nControl of plasma properties like density and temperature is important for a functioning plasma stealth device, and it may be necessary to dynamically adjust the plasma density, temperature, or combinations, or the magnetic field, in order to effectively defeat different types of radar systems. The great advantage Plasma Stealth possesses over traditional radio frequency stealth techniques like shape morphing into LO geometry and use of radar-absorbent materials is that plasma is tunable and wideband. When faced with frequency hopping radar, it is possible, at least in principle, to change the plasma temperature and density to deal with the situation. The greatest challenge is to generate a large area or volume of plasma with good energy efficiency.\n\nPlasma stealth technology also faces various technical problems. For example, the plasma itself emits EM radiation, although it is usually weak and noise-like in spectrum. Also, it takes some time for plasma to be re-absorbed by the atmosphere and a trail of ionized air would be created behind the moving aircraft, but at present there is no method to detect this kind of plasma trail at long distance. Thirdly, plasmas (like glow discharges or fluorescent lights) tend to emit a visible glow: this is not compatible with overall low observability concept. However, present optical detection devices like FLIR has a shorter range than radar, so Plasma Stealth still has an operational range space. Last but not least, it is extremely difficult to produce a radar-absorbent plasma around an entire aircraft traveling at high speed, the electrical power needed is tremendous. However, a substantial reduction of an aircraft's RCS may be still be achieved by generating radar-absorbent plasma around the most reflective surfaces of the aircraft, such as the turbojet engine fan blades, engine air intakes, vertical stabilizers, and airborne radar antenna.\n\nThere have been several computational studies on plasma-based radar cross section reduction technique using three-dimensional finite-difference time-domain simulations. Chaudhury et al. studied the electromagnetic wave attenuation of an Epstein profile plasma using this method. Chung studied the radar cross change of a metal cone when it is covered with plasma, a phenomenon that occurs during reentry into the atmosphere. Chung simulated the radar cross section of a generic satellite, and also the radar cross section when it is covered with artificially generated plasma cones.\n\nDue to the obvious military applications of the subject, there are few readily available experimental studies of plasma's effect on the radar cross section (RCS) of aircraft, but plasma interaction with microwaves is a well explored area of general plasma physics. Standard plasma physics reference texts are a good starting point and usually spend some time discussing wave propagation in plasmas.\nOne of the most interesting articles related to the effect of plasma on the RCS of aircraft was published in 1963 by the IEEE. The article is entitled \"Radar cross sections of dielectric or plasma coated conducting spheres and circular cylinders\" (IEEE Transactions on Antennas and Propagation, September 1963, pp. 558–569). Six years earlier, in 1957, the Soviets had launched the first artificial satellite. While trying to track Sputnik it was noticed that its electromagnetic scattering properties were different from what was expected for a conductive sphere. This was due to the satellite's traveling inside of a plasma shell: the ionosphere.\n\nThe Sputnik's simple shape serves as an ideal illustration of plasma's effect on the RCS of an aircraft. Naturally, an aircraft would have a far more elaborate shape and be made of a greater variety of materials, but the basic effect should remain the same. In the case of the Sputnik flying through the ionosphere at high velocity and surrounded by a naturally occurring plasma shell, there are two separate radar reflections: the first from the conductive surface of the satellite, and the second from the dielectric plasma shell.\n\nThe authors of the paper found that a dielectric (plasma) shell may either decrease or increase the echo area of the object. If either one of the two reflections is considerably greater, then the weaker reflection will not contribute much to the overall effect. The authors also stated that the EM signal that penetrates the plasma shell and reflects off the object's surface will drop in intensity while traveling through plasma, as was explained in the prior section.\n\nThe most interesting effect is observed when the two reflections are of the same order of magnitude. In this situation the two components (the two reflections) will be added as phasors and the resulting field will determine the overall RCS. When these two components are out of phase relative to each other, cancellation occurs. This means that under such circumstances the RCS becomes null and the object is completely invisible to the radar.\n\nIt is immediately apparent that performing similar numeric approximations for the complex shape of an aircraft would be difficult. This would require a large body of experimental data for the specific airframe, properties of plasma, aerodynamic aspects, incident radiation, etc. In contrast, the original computations discussed in this paper were done by a handful of people on an IBM 704 computer made in 1956, and at the time, this was a novel subject with very little research background. So much has changed in science and engineering since 1963, that differences between a metal sphere and a modern combat jet pale in comparison.\n\nA simple application of plasma stealth is the use of plasma as an antenna: metal antenna masts often have large radar cross sections, but a hollow glass tube filled with low pressure plasma can also be used as an antenna, and is entirely transparent to radar when not in use.\n\n"}
{"id": "1464101", "url": "https://en.wikipedia.org/wiki?curid=1464101", "title": "Pyroligneous acid", "text": "Pyroligneous acid\n\nPyroligneous acid, also called wood vinegar or wood acid, is a dark liquid produced by the destructive distillation of wood and other plant materials.\n\nThe principal components of pyroligneous acid are acetic acid, acetone and methanol. It was once used as a commercial source for acetic acid. In addition, the vinegar often contains 80-90% water along with some 200 organic compounds.\n\nPyroligneous acid (acetum lignorum) was investigated by German chemist Johann Rudolph Glauber. The acid was eaten as a substitute for vinegar. It was also used topically for treating wounds, ulcers and other ailments. A salt can be made by neutralizing the acid with a lye made from the ashes of the burnt wood.\n\nDuring the United States Civil War it became increasingly difficult for the Confederate States of America to obtain much needed salt. Curing meat and fish with pyroligneous acid was attempted by cooks to compensate for this deficiency. Unfortunately for the Confederate States Army it was not a comparable method of food preservation.\n\nIn the nineteenth century, pyroligneous acid was used to prepare an impure aluminium sulfacetate mordant for use with cotton, but the resulting mixture imparted a burnt odor to the cotton, and Ganswindt recommended its use be abandoned in favour of purer preparations in 1899.\n\nIn 1895, pyroligneous acid was first marketed under the brand Wright's Liquid Smoke, a liquid smoke product intended to impart the flavor and some of the preservative effects of wood smoking to meats and vegetables at a time when fear about the carcinogenic effects of wood smoking was becoming prevalent.\n\n"}
{"id": "1953299", "url": "https://en.wikipedia.org/wiki?curid=1953299", "title": "Pyrolytic carbon", "text": "Pyrolytic carbon\n\nPyrolytic carbon is a material similar to graphite, but with some covalent bonding between its graphene sheets as a result of imperfections in its production.\n\nPyrolytic carbon is man-made and is not thought to be found in nature. Generally it is produced by heating a hydrocarbon nearly to its decomposition temperature, and permitting the graphite to crystallise (pyrolysis). One method is to heat synthetic fibers in a vacuum. Another method is to place seeds on a plate in the very hot gas to collect the graphite coating. It is used in high temperature applications such as missile nose cones, rocket motors, heat shields, laboratory furnaces, in graphite-reinforced plastic, and in biomedical prostheses.\n\nPyrolytic carbon samples usually have a single cleavage plane, similar to mica, because the graphene sheets crystallize in a planar order, as opposed to graphite, which forms microscopic randomly oriented zones. Because of this, pyrolytic carbon exhibits several unusual anisotropic properties. It is more thermally conductive along the cleavage plane than graphite, making it one of the best planar thermal conductors available.\n\nPyrolitic graphite forms mosaic crystals with controlled mosaicities up to a few degrees.\n\nIt is also more diamagnetic (χ = −4×10) against the cleavage plane, exhibiting the greatest diamagnetism (by weight) of any room-temperature diamagnet. In comparison, pyrolitic graphite has a relative permeability of 0.9996, whereas bismuth has a relative permeability of 0.9998 (table).\n\nFew materials can be made to magnetically levitate stably above the magnetic field from a permanent magnet. Although magnetic repulsion is obviously and easily achieved between any two magnets, the shape of the field causes the upper magnet to push off sideways, rather than remaining supported, rendering stable levitation impossible for magnetic objects (see Earnshaw's theorem). Strongly diamagnetic materials, however, can levitate above powerful magnets.\n\nWith the easy availability of rare-earth permanent magnets in recent years, the strong diamagnetism of pyrolytic carbon makes it a convenient demonstration material for this effect.\n\nIn 2012, a research group in Japan demonstrated that pyrolytic carbon can respond to laser light or sufficiently powerful natural sunlight by spinning or moving in the direction of the field gradient. The carbon's magnetic susceptibility weakens upon sufficient illumination, leading to an unbalanced magnetization of the material and movement when using a specific geometry.\n\n\nBecause blood clots do not easily form on it, it is often advisable to line a blood-contacting prosthesis with this material in order to reduce the risk of thrombosis. For example, it finds use in artificial hearts and artificial heart valves. Blood vessel stents, by contrast, are often lined with a polymer that has heparin as a pendant group, relying on drug action to prevent clotting. This is at least partly because of pyrolytic carbon's brittleness and the large amount of permanent deformation, which a stent undergoes during expansion.\n\nPyrolytic carbon is also in medical use to coat anatomically correct orthopaedic implants, a.k.a. replacement joints. In this application it is currently marketed under the name \"PyroCarbon\". These implants have been approved by the U.S. Food and Drug Administration for use in the hand for metacarpophalangeal (knuckle) replacements. They are produced by two companies: Tornier (BioProfile) and Ascension Orthopedics. (On September 23, 2011, Integra LifeSciences acquired Ascension Orthopedics.) The FDA has also approved PyroCarbon interphalangeal joint replacements under the Humanitarian Device Exemption.\n\n\n"}
{"id": "41447786", "url": "https://en.wikipedia.org/wiki?curid=41447786", "title": "Renewable energy in Bhutan", "text": "Renewable energy in Bhutan\n\nRenewable energy in Bhutan is the use of renewable energy for electricity generation in Bhutan. The renewable energy sources include hydropower.\n\nWhile Bhutan has seen great successes with developing its large hydropower projects through technical and financial assistance from India, little or no private sector participation with other forms of renewable energy has been evident. In part because of the Sustainable development goals, Bhutan has established a minimum goal of 20 megawatts of renewable energy product by 2025, through a mix of renewable energy technologies. Bhutan's Department of Renewable Energy helped formulate and launch its Alternative Renewable Energy Policy in order to promote in Bhutan a mix of clean Renewable Energy (RE) technologies - solar, wind, bio-mass, geo-thermal, pico/micro/mini/small hydropower plants up to 25 MW in size and waste-to-energy technologies.\n\nBhutan's commitment to renewable energy started in 1980. Six years later, the first hydropower plant opened in Chukha, followed by a plant in Kurichhu in 2001. Soon after that two more plants opened in Basochhu in 2005 and Tala in 2009. At COP 15 in 2009 (2009 United Nations Climate Change Conference), Bhutan made its first promise to remain completely carbon neutral ; they reaffirmed this promise at COP 21 in 2015 (2015 United Nations Climate Change Conference).\n\nBhutan's first step into renewable energy was hydroelectric power. They first started by opening the first hydroelectric power plant in Chukha in 1986. They now have three more plants open: Kurichhu (2001), Basochhu (2005), and Tala (2009). Currently approximately 70% or 4.4 million tons of the hydroelectric power Bhutan produces is exported to India. Despite efforts to expand the types of renewable energy used in Bhutan, hydroelectric power is still the leading source of clean energy in the nation.\n\nAfter the 2015 United Nations Climate Change Conference, Bhutan has been moving towards other forms of renewable energy so as to decrease their reliance on hydroelectric power during winter and dry months. Bhutan has increased their focus specifically in the areas of: windmills, biogas plants, solar power, and smaller hydropower plants. The initial plan was to install 24 wind turbines, however installation was halted after only two turbines due to citizens raising concerns about possible noise pollution. Since 2015, Bhutan has installed a 30 MW solar energy plant in Shingkhar in the Bumthang district; this plant was the first of its kind. There have also been approximately 13,500 stoves and 2,800 biogas plants installed throughout the country. In an effort to further spread the use of renewable energy and to decrease the country's carbon emissions, Bhutan also provides free electricity to rural farmers; this reduces the amount of fires/ gas they use to do their farm work. The government also subsidizes LED light bulbs and electric vehicles. Currently Bhutan's clean energy exports offset approximately 6 million tons of carbon dioxide.\n\nIn a 2016 TED Talk, the Bhutanese Prime Minister Tshering Tobgay spoke about how Bhutan is the only country able to claim the title of \"carbon negative\". This means that though the nation produces about 2.2 million tons of CO , the forests offset more than 4 million tons of CO. They are able to do this because over 72% of their country is under the cover of their forests, a constitutional mandate of the nation.\n\nAt the 2009 United Nations Climate Change Conference, Bhutan made their first promise to remain carbon neutral; they again made this promise at the 2015 United Nations Climate Change Conference. As of 2016, the clean energy Bhutan exported offset roughly 6 million tons of CO; it is their goal to export enough clean energy to offset 17 million tons of carbon dioxide by 2020. In his 2016 TED Talk, Prime Minister Tshering Tobgay stated that if Bhutan was able to harness even half of the potential hydroelectric power, they would be able to offset roughly 50 million tons of carbon dioxide, more than New York City produces in one year.\n\n"}
{"id": "25599", "url": "https://en.wikipedia.org/wiki?curid=25599", "title": "Rubidium", "text": "Rubidium\n\nRubidium is a chemical element with symbol Rb and atomic number 37. Rubidium is a soft, silvery-white metallic element of the alkali metal group, with a standard atomic weight of 85.4678. Elemental rubidium is highly reactive, with properties similar to those of other alkali metals, including rapid oxidation in air. On Earth, natural rubidium comprises two isotopes: 72% is the stable isotope, Rb; 28% is the slightly radioactive Rb, with a half-life of 49 billion years—more than three times longer than the estimated age of the universe.\n\nGerman chemists Robert Bunsen and Gustav Kirchhoff discovered rubidium in 1861 by the newly developed technique, flame spectroscopy.\n\nRubidium's compounds have various chemical and electronic applications. Rubidium metal is easily vaporized and has a convenient spectral absorption range, making it a frequent target for laser manipulation of atoms.\n\nRubidium is not a known nutrient for any living organisms. However, rubidium ions have the same charge as potassium ions, and are actively taken up and treated by animal cells in similar ways.\n\nRubidium is a very soft, ductile, silvery-white metal. It is the second most electropositive of the stable alkali metals and melts at a temperature of . Like other alkali metals, rubidium metal reacts violently with water. As with potassium (which is slightly less reactive) and caesium (which is slightly more reactive), this reaction is usually vigorous enough to ignite the hydrogen gas it produces. Rubidium has also been reported to ignite spontaneously in air. It forms amalgams with mercury and alloys with gold, iron, caesium, sodium, and potassium, but not lithium (even though rubidium and lithium are in the same group).\n\nRubidium has a very low ionization energy of only 406 kJ/mol. Rubidium and potassium show a very similar purple color in the flame test, and distinguishing the two elements requires more sophisticated analysis, such as spectroscopy.\n\nRubidium chloride (RbCl) is probably the most used rubidium compound: among several other chlorides, it is used to induce living cells to take up DNA; it is also used as a biomarker, because in nature, it is found only in small quantities in living organisms and when present, replaces potassium. Other common rubidium compounds are the corrosive rubidium hydroxide (RbOH), the starting material for most rubidium-based chemical processes; rubidium carbonate (RbCO), used in some optical glasses, and rubidium copper sulfate, RbSO·CuSO·6HO. Rubidium silver iodide (RbAgI) has the highest room temperature conductivity of any known ionic crystal, a property exploited in thin film batteries and other applications.\n\nRubidium forms a number of oxides when exposed to air, including rubidium monoxide (RbO), RbO, and RbO; rubidium in excess oxygen gives the superoxide RbO. Rubidium forms salts with halides, producing rubidium fluoride, rubidium chloride, rubidium bromide, and rubidium iodide.\n\nAlthough rubidium is monoisotopic, rubidium in the Earth's crust is composed of two isotopes: the stable Rb (72.2%) and the radioactive Rb (27.8%). Natural rubidium is radioactive, with specific activity of about 670 Bq/g, enough to significantly expose a photographic film in 110 days. \n\nTwenty four additional rubidium isotopes have been synthesized with half-lives of less than 3 months; most are highly radioactive and have few uses.\n\nRubidium-87 has a half-life of  years, which is more than three times the age of the universe of  years, making it a primordial nuclide. It readily substitutes for potassium in minerals, and is therefore fairly widespread. Rb has been used extensively in dating rocks; Rb beta decays to stable Sr. During fractional crystallization, Sr tends to concentrate in plagioclase, leaving Rb in the liquid phase. Hence, the Rb/Sr ratio in residual magma may increase over time, and the progressing differentiation results in rocks with elevated Rb/Sr ratios. The highest ratios (10 or more) occur in pegmatites. If the initial amount of Sr is known or can be extrapolated, then the age can be determined by measurement of the Rb and Sr concentrations and of the Sr/Sr ratio. The dates indicate the true age of the minerals only if the rocks have not been subsequently altered (see rubidium–strontium dating).\n\nRubidium-82, one of the element's non-natural isotopes, is produced by electron-capture decay of strontium-82 with a half-life of 25.36 days. With a half-life of 76 seconds, rubidium-82 decays by positron emission to stable krypton-82.\n\nRubidium is the twenty-third most abundant element in the Earth's crust, roughly as abundant as zinc and rather more common than copper. It occurs naturally in the minerals leucite, pollucite, carnallite, and zinnwaldite, which contain as much as 1% rubidium oxide. Lepidolite contains between 0.3% and 3.5% rubidium, and is the commercial source of the element. Some potassium minerals and potassium chlorides also contain the element in commercially significant quantities.\n\nSeawater contains an average of 125 µg/L of rubidium compared to the much higher value for potassium of 408 mg/L and the much lower value of 0.3 µg/L for caesium.\n\nBecause of its large ionic radius, rubidium is one of the \"incompatible elements.\" During magma crystallization, rubidium is concentrated together with its heavier analogue caesium in the liquid phase and crystallizes last. Therefore, the largest deposits of rubidium and caesium are zone pegmatite ore bodies formed by this enrichment process. Because rubidium substitutes for potassium in the crystallization of magma, the enrichment is far less effective than that of caesium. Zone pegmatite ore bodies containing mineable quantities of caesium as pollucite or the lithium minerals lepidolite are also a source for rubidium as a by-product.\n\nTwo notable sources of rubidium are the rich deposits of pollucite at Bernic Lake, Manitoba, Canada, and the rubicline ((Rb,K)AlSiO) found as impurities in pollucite on the Italian island of Elba, with a rubidium content of 17.5%. Both of those deposits are also sources of caesium.\n\nAlthough rubidium is more abundant in Earth's crust than caesium, the limited applications and the lack of a mineral rich in rubidium limits the production of rubidium compounds to 2 to 4 tonnes per year. Several methods are available for separating potassium, rubidium, and caesium. The fractional crystallization of a rubidium and caesium alum (Cs,Rb)Al(SO)·12HO yields after 30 subsequent steps pure rubidium alum. Two other methods are reported, the chlorostannate process and the ferrocyanide process.\n\nFor several years in the 1950s and 1960s, a by-product of potassium production called Alkarb was a main source for rubidium. Alkarb contained 21% rubidium, with the rest being potassium and a small amount of caesium. Today the largest producers of caesium, such as the Tanco Mine, Manitoba, Canada, produce rubidium as a by-product from pollucite.\n\nRubidium was discovered in 1861 by Robert Bunsen and Gustav Kirchhoff, in Heidelberg, Germany, in the mineral lepidolite through spectroscopy. Because of the bright red lines in its emission spectrum, they chose a name derived from the Latin word \"rubidus\", meaning \"deep red\".\n\nRubidium is a minor component in lepidolite. Kirchhoff and Bunsen processed 150 kg of a lepidolite containing only 0.24% rubidium oxide (RbO). Both potassium and rubidium form insoluble salts with chloroplatinic acid, but those salts show a slight difference in solubility in hot water. Therefore, the less-soluble rubidium hexachloroplatinate (RbPtCl) could be obtained by fractional crystallization. After reduction of the hexachloroplatinate with hydrogen, the process yielded 0.51 grams of rubidium chloride for further studies. Bunsen and Kirchhoff began their first large-scale isolation of caesium and rubidium compounds with of mineral water, which yielded 7.3 grams of caesium chloride and 9.2 grams of rubidium chloride. Rubidium was the second element, shortly after caesium, to be discovered by spectroscopy, just one year after the invention of the spectroscope by Bunsen and Kirchhoff.\n\nThe two scientists used the rubidium chloride to estimate that the atomic weight of the new element was 85.36 (the currently accepted value is 85.47). They tried to generate elemental rubidium by electrolysis of molten rubidium chloride, but instead of a metal, they obtained a blue homogeneous substance which \"neither under the naked eye nor under the microscope showed the slightest trace of metallic substance.\" They presumed it was a subchloride (); however, the product was probably a colloidal mixture of the metal and rubidium chloride. In a second attempt to produce metallic rubidium, Bunsen was able to reduce rubidium by heating charred rubidium tartrate. Although the distilled rubidium was pyrophoric, they were able to determine the density and the melting point. The quality of this research in the 1860s can be appraised by the fact that their determined density differs less than 0.1 g/cm and the melting point by less than 1 °C from the presently accepted values.\n\nThe slight radioactivity of rubidium was discovered in 1908, but that was before the theory of isotopes was established in 1910, and the low level of activity (half-life greater than 10 years) made interpretation complicated. The now proven decay of Rb to stable Sr through beta decay was still under discussion in the late 1940s.\n\nRubidium had minimal industrial value before the 1920s. Since then, the most important use of rubidium is research and development, primarily in chemical and electronic applications. In 1995, rubidium-87 was used to produce a Bose–Einstein condensate, for which the discoverers, Eric Allin Cornell, Carl Edwin Wieman and Wolfgang Ketterle, won the 2001 Nobel Prize in Physics.\n\nRubidium compounds are sometimes used in fireworks to give them a purple color. Rubidium has also been considered for use in a thermoelectric generator using the magnetohydrodynamic principle, where hot rubidium ions are passed through a magnetic field. These conduct electricity and act like an armature of a generator thereby generating an electric current. Rubidium, particularly vaporized Rb, is one of the most commonly used atomic species employed for laser cooling and Bose–Einstein condensation. Its desirable features for this application include the ready availability of inexpensive diode laser light at the relevant wavelength, and the moderate temperatures required to obtain substantial vapor pressures. For cold atom applications requiring tunable interactions, Rb is preferable due to its rich Feshbach spectrum.\n\nRubidium has been used for polarizing He, producing volumes of magnetized He gas, with the nuclear spins aligned rather than random. Rubidium vapor is optically pumped by a laser and the polarized Rb polarizes He through the hyperfine interaction. Such spin-polarized He cells are useful for neutron polarization measurements and for producing polarized neutron beams for other purposes.\n\nThe resonant element in atomic clocks utilizes the hyperfine structure of rubidium's energy levels, and rubidium is useful for high-precision timing. It is used as the main component of secondary frequency references (rubidium oscillators) in cell site transmitters and other electronic transmitting, networking, and test equipment. These rubidium standards are often used with GPS to produce a \"primary frequency standard\" that has greater accuracy and is less expensive than caesium standards. Such rubidium standards are often mass-produced for the telecommunication industry.\n\nOther potential or current uses of rubidium include a working fluid in vapor turbines, as a getter in vacuum tubes, and as a photocell component. Rubidium is also used as an ingredient in special types of glass, in the production of superoxide by burning in oxygen, in the study of potassium ion channels in biology, and as the vapor in atomic magnetometers. In particular, Rb is used with other alkali metals in the development of spin-exchange relaxation-free (SERF) magnetometers.\n\nRubidium-82 is used for positron emission tomography. Rubidium is very similar to potassium and tissue with high potassium content will also accumulate the radioactive rubidium. One of the main uses is myocardial perfusion imaging. As a result of changes in the blood–brain barrier in brain tumors, rubidium collects more in brain tumors than normal brain tissue, allowing the use of radioisotope rubidium-82 in nuclear medicine to locate and image brain tumors. Rubidium-82 has a very short half-life of 76 seconds, and the production from decay of strontium-82 must be done close to the patient.\n\nRubidium was tested for the influence on manic depression and depression. Dialysis patients suffering from depression show a depletion in rubidium and therefore a supplementation may help during depression. In some tests the rubidium was administered as rubidium chloride with up to 720 mg per day for 60 days.\n\nRubidium reacts violently with water and can cause fires. To ensure safety and purity, this metal is usually kept under a dry mineral oil or sealed in glass ampoules in an inert atmosphere. Rubidium forms peroxides on exposure even to small amount of air diffused into the oil, and storage is subject to similar precautions as the storage of metallic potassium.\n\nRubidium, like sodium and potassium, almost always has +1 oxidation state when dissolved in water, even in biological contexts. The human body tends to treat Rb ions as if they were potassium ions, and therefore concentrates rubidium in the body's intracellular fluid (i.e., inside cells). The ions are not particularly toxic; a 70 kg person contains on average 0.36 g of rubidium, and an increase in this value by 50 to 100 times did not show negative effects in test persons. The biological half-life of rubidium in humans measures 31–46 days. Although a partial substitution of potassium by rubidium is possible, when more than 50% of the potassium in the muscle tissue of rats was replaced with rubidium, the rats died.\n\n\n\n"}
{"id": "17294984", "url": "https://en.wikipedia.org/wiki?curid=17294984", "title": "SN Power", "text": "SN Power\n\nSN Power AS invests in clean, renewable energy on a commercial basis in emerging markets. The company has operations in Southeast Asia, Africa and Central America and the focus is to acquire, develop, construct and operate hydropower assets. SN Power has several running plants in the Philippines, Zambia, Laos and Panama, and the company consist of a multinational team of people employed globally in its operations and projects. In addition to operating assets, SN Power has office in Netherlands and is Head-quartered in Oslo Norway. \n\nThe company is fully owned by Norfund, the Norwegian Investment Fund for Developing Countries. \n\nThe company was established in 2002. Along with Norfund, the Norwegian Investment Fund for Developing Countries, Statkraft created the international power company Statkraft Norfund Power Invest (in short called SN Power). The company was formed to promote economic growth and sustainable development in new and emerging markets. In May 2003, SN Power made its first investment, with the acquisition of 30 per cent of Nividhu in Sri Lanka, a venture partner that operates the power plants Assupiniella and Belihuloya.\n\nSN Power AS was reestablished as a new company in 2013 after its parent company Statkraft and Norfund decided to make changes to their share portfolios. \n\nOn 27th September 2017 Statkraft and Norfund closed an agreement to swap shares in their jointly owned international hydropower assets. \nThe agreement implies that Statkraft sells all its shares in SN Power to Norfund, while Norfund sells all its shares in Statkraft IH Invest AS (SKIHI) to Statkraft. \nUp until that date, Statkraft and Norfund owned 50 percent each in SN Power, which owns hydropower plants in Panama, Laos, Philippines and Zambia. At the same time Statkraft has owned 81.9% and Norfund 18.1% of the shares in SKIHI, the company owning mainly hydropower assets in Peru, Brazil, Chile, India and Nepal.\n\nThrough these transactions, Norfund increases its presence in Africa and Southeast Asia. \n"}
{"id": "16168699", "url": "https://en.wikipedia.org/wiki?curid=16168699", "title": "Satellite tornado", "text": "Satellite tornado\n\nA satellite tornado is a tornado that rotates around a larger, primary tornado and interacts with the same mesocyclone. Satellite tornadoes occur apart from the primary tornado and are not considered subvortices; the primary tornado and satellite tornadoes are considered to be separate tornadoes. The cause of satellite tornadoes is not known. Such tornadoes are more often anticyclonic than are typical tornadoes and these pairs may be referred to as tornado couplets. Satellite tornadoes most commonly form in association with very large and intense tornadoes.\n\nSatellite tornadoes are relatively uncommon. When a satellite tornado does occur there is often more than one orbiting satellite spawned during the life cycle of the tornado or with successive primary tornadoes spawned by the parent supercell (a process known as cyclic tornadogenesis and leading to a tornado family). On tornado outbreak days, if satellite tornadoes occur with one supercell, there is an elevated probability of their occurrence with other supercells.\n\nSatellite tornadoes may merge into their companion tornado although often the appearance of this occurring is an illusion caused when an orbiting tornado revolves around the backside of a primary tornado obscuring view of the satellite. During the March 1990 Central United States tornado outbreak, one member of a tornado family (rated F5) constricted and became a satellite tornado of the next tornado of the family before merging into the new primary tornado which soon also intensified to F5.\n\nSome examples of tornado couplets include the Tri-State Tornado, the Chickasha tornado during the 1999 Oklahoma tornado outbreak, the 2007 Greensburg tornado, and the 2013 El Reno tornado. Satellite tornadoes are more likely to be recognized in recent decades than in the far past as eyewitness accounts as well as damage survey information are often available for later events. The advent of storm chasing, in particular, boosts the likelihood that satellite tornadoes are noticed visually and/or on mobile radar. These tornadoes may remain over open country and thus cause less structural damage and consequently are less widely known. Such examples include near Beloit, Kansas on 15 May 1990 and during Project VORTEX near Allison, Texas on 8 June 1995, among other events.\n\n\n"}
{"id": "28847871", "url": "https://en.wikipedia.org/wiki?curid=28847871", "title": "Submission (2010 film)", "text": "Submission (2010 film)\n\nSubmission (Swedish: Underkastelsen) is a 2010 Swedish documentary film directed by Stefan Jarl and narrated by Stellan Skarsgård. In the film, director Jarl has his blood drawn for a series of tests to show how much of a \"chemical burden\" is in his body.\n\nJarl convinces actress Eva Röse, who is pregnant, to have the blood tests also. The film goes on to describe the issue of chemicals and plastics invented since World War II and how they affect the health of people around the world.\n\nThe film had its North American premiere at the Mill Valley Film Festival on 15 October 2010.\n\n"}
{"id": "4576338", "url": "https://en.wikipedia.org/wiki?curid=4576338", "title": "Tantite", "text": "Tantite\n\nTantite is a rare tantalum oxide mineral with formula: TaO. Tantite forms transparent microscopic colorless triclinic - pedial crystals with an adamantine luster. It has a Mohs hardness of 7 and a high specific gravity of 8.45. Chemical analyses show minor inclusion (1.3%) of niobium oxide.\n\nIt was first described in 1983 for an occurrence in a pegmatite in the Kola peninsula, Russia. It has also been reported from a pegmatite complex in Florence County, Wisconsin. Associated mineral species include elbaite, lepidolite, spodumene, columbite-tantalite, wodginite, and microlite.\n\n"}
{"id": "2275470", "url": "https://en.wikipedia.org/wiki?curid=2275470", "title": "Tropical Storm Knut", "text": "Tropical Storm Knut\n\nThe name Knut was used for two tropical cyclones in the Eastern Pacific Ocean. The name was retired in the spring of 1992, and replaced with Kenneth in the 1993 season.\n\n"}
{"id": "42443821", "url": "https://en.wikipedia.org/wiki?curid=42443821", "title": "Tudor batteries", "text": "Tudor batteries\n\nTudor is a lead-acid battery brand founded by Henri Tudor in 1890 and is now owned by Exide Technologies. \n\nHenri Tudor from Rosport created in 1890 what will become later the Tudor batteries brand: the \"Société anonyme Franco-Belge pour la fabrication de l’accumulateur Tudor\" (Franco-Belgian anonymous society for the manufacture of the Tudor Accumulator). At that time, more than 1200 Tudor batteries were in service in Belgium and abroad including Germany, France, Austria, Spain, Egypt, Denmark, Finland, Sweden, Swiss, Hungary, Nederland, Italy, Poland and Argentina. The emblem was the name of the brand with the head of a wyvern griffon from Wales. Tudor had licensed his brand in many countries like Germany (\"Akkumulatoren Fabrik AG\" (AFA)) and Spain (\"la Sociedad Española Del Acumulador\").\n\nIn 1901, Henri Tudor decided to change the name of the firm to \"Société anonyme des accumulateurs Tudor\" (Anonymous society of the Tudor Accumulator) with operating sites in Lille and Florival. Later in 1908, the Rosport site was closed. As Tudor Spain became more profitable, the shareholding shifted and the majority shareholder became the Germany Company (AFA) and Tudor imported equipment from Germany.\nWhen the first world war broke out in 1914, AFA removed from Tudor Spain's capital. Three directors resigned and the situation was critical. However, in 1916, Tudor Spain became the supplier of two starting batteries models manufactured in Spain. In others European countries during WWI, plants were completely plundered, like in Belgium and in Luxembourg. These events will not hinder the company to have, between the two world wars nearly 25,000 employees at the Florival plant. In 1920, Tudor Spain supplied the first battery for a submarine (Isaac Peral), with funding from The Akkumulator Fabrik AG.\n\nIn 1928, after Henri Tudor’s death, his son John took the succession as Managing director of “Accumulateurs Tudor”\n\nDuring the following years, the traction battery market will strongly rise. However, the first expansion of their plants were based on stationary batteries. For these facilities, the company proposed service contracts under which it bound up with keeping the battery in good working order for a period of ten years against a fixed remuneration. From 1931, «Accumulateurs Tudor» introduced widely the traction battery which is especially used for the industry handling equipment.\n\nIn 1939, Tudor batteries France made the acquisition of the Belgian Society of Fulmen accumulator S.A. with its 5900 m2 site located in Leeuw-St.Pierre.\n\n\nThe 1974 recession was keenly felt. The first half of 1981 was characterized by a new crisis resulting in technical unemployment and a deficit. In 1982, the plant underwent thirteen days strike while production was one million pieces per year. The company suffered from the global overcapacity in the starter battery, which represented 65% of revenues.\n\nThe goal was to reduce this proportion to 50%. Sales were divided between Germany, Benelux and France, with a predominance of the French market. The competition was Japanese, but also Europe with the establishment of a major General Motors production plant in Sarreguemines. The lifetime of the batteries significantly lengthened, and replacements became less frequent.\n\nA continuous casting installation of grids, representing an investment of one million dollars, was commissioned in 1982. At the trays, antique ebony gave way to lighter and more resistant polymers. The stationary battery was the subject of high demand in the area of preventing interruptions (telephone exchanges, operating rooms, computer rooms). Batteries handling equipment however accounted for 70% of turnover of industrial battery.\n\nEventually, The Florival site suffered from its initial geographical limits. In 1990, the “Société Anonyme Tudor accumulators\" was absorbed by the C.E.A.C.(European Accumulators Company) based in Gennevilliers (France) which was a French-Italian company. C.E.A.C., well-known for its Fulmen brand, thus became also owner of the Tudor brand in some country. The production activities of the Florival site were reduced gradually.\n\nIn October 1994, Exide Technologies took over the “Societad Española del Accumulador Tudor”. The Spanish company was, with Tudor India and Tudor Swedish, one of the last social reasons to keep using Tudor brand. In 1995 Exide Technologies also took C.E.A.C. The Florival site became a distribution center.\n\n\n"}
{"id": "12868735", "url": "https://en.wikipedia.org/wiki?curid=12868735", "title": "World Energy Council", "text": "World Energy Council\n\nThe World Energy Council is a global and inclusive forum for thought-leadership and tangible engagement with headquarters in London. Its mission is 'To promote the sustainable supply and use of energy for the greatest benefit of all people'.\n\nThe idea for the foundation of the Council came from Daniel Nicol Dunlop in the 1920s. He wanted to gather experts from all around the world to discuss current and future energy issues. He organised in 1923 first national committees, which organised the first World Power Conference (WPC) in 1924. 1700 experts from 40 countries met in London to discuss energy issues. The meeting was a success and the participants decided on July 11, 1924 to establish a permanent organisation named \"World Power Conference\". Dunlop was elected as its first Secretary General.\nIn 1968 the name was changed to \"World Energy Conference\" and in 1989 it became the \"World Energy Council\".\n\nThe World Energy Council is the principal impartial network of leaders and practitioners promoting an affordable, stable and environmentally sensitive energy system for the greatest benefit of all. Formed in 1923, the Council is the UN-accredited global energy body, representing the entire energy spectrum, with more than 3000 member organisations located in over 90 countries and drawn from governments, private and state corporations, academia, NGOs and energy-related stakeholders. The World Energy Council informs global, regional and national energy strategies by hosting high-level events, publishing authoritative studies, and working through its extensive member network to facilitate the world’s energy policy dialogue. Today, the Council has Member Committees established in over 90 countries, which represent over 3000 member organizations including governments, industry and expert institutions. The World Energy Council covers all energy resources and technologies of energy supply and demand.\n\nThe World Energy Council hosts the World Energy Congress, which is the world’s largest and most influential energy event covering all aspects of the energy agenda. Staged every three years, the Congress provides a platform for energy leaders and experts in all aspects of the sector to address the challenges and opportunities facing suppliers and consumers of energy.\n\nThe World Energy Council's publications include an annual country-by-country Energy and Climate Policy Assessment, the Survey of Energy Resources. \n\nAs of July 2018 the World Energy Council has 92 member committees and 2 countries which have direct membership\n\n\n\n\n"}
