{"id": "21848654", "url": "https://en.wikipedia.org/wiki?curid=21848654", "title": "Absorption hardening", "text": "Absorption hardening\n\nIn the field of nuclear engineering, absorption hardening is the increase in average energy of neutrons in a population by preferential absorption of lower-energy neutrons. This occurs because absorption cross-sections typically increase for lower neutron energies.\n\n"}
{"id": "3199351", "url": "https://en.wikipedia.org/wiki?curid=3199351", "title": "Adiabatic flame temperature", "text": "Adiabatic flame temperature\n\nIn the study of combustion, there are two types of adiabatic flame temperature depending on how the process is completed, constant volume and constant pressure, describing the temperature that the combustion products theoretically reach if no energy is lost to the outside environment.\n\nThe constant volume adiabatic flame temperature is the temperature that results from a complete combustion process that occurs without any work, heat transfer or changes in kinetic or potential energy. Its temperature is higher than the constant pressure process because none of the energy is utilized to change the volume of the system (i.e., generate work).\n\nIn daily life, the vast majority of flames one encounters are those of organic compounds including wood, wax, fat, common plastics, propane, and gasoline. The constant-pressure adiabatic flame temperature of such substances in air is in a relatively narrow range around 1950 °C. This is because, in terms of stoichiometry, the combustion of an organic compound with \"n\" carbons involves breaking roughly 2\"n\" C–H bonds, \"n\" C–C bonds, and 1.5\"n\" O bonds to form roughly \"n\" CO molecules and \"n\" HO molecules.\n\nBecause most combustion processes that happen naturally occur in the open air, there is nothing that confines the gas to a particular volume like the cylinder in an engine. As a result, these substances will burn at a constant pressure allowing the gas to expand during the process.\n\nAssuming initial atmospheric conditions (1 bar and 20 °C), the following table lists the adiabatic flame temperature for various gases under constant pressure conditions. The temperatures mentioned here are for a stoichiometric fuel-oxidizer mixture (i.e. equivalence ratio \"φ\" = 1).\n\nNote these are theoretical, not actual, flame temperatures produced by a flame that loses no heat. The closest will be the hottest part of a flame, where the combustion reaction is most efficient. This also assumes complete combustion (e.g. perfectly balanced, non-smokey, usually bluish flame)\n\nFrom the first law of thermodynamics for a closed reacting system we have,\nwhere, formula_2 and formula_3 are the heat and work transferred from the system to the surroundings during the process respectively, and formula_4 and formula_5 are the internal energy of the reactants and products respectively. \nIn the constant volume adiabatic flame temperature case, the volume of the system is held constant hence there is no work occurring,\nand there is no heat transfer because the process is defined to be adiabatic: formula_7. As a result, the internal energy of the products is equal to the internal energy of the reactants: formula_8. \nBecause this is a closed system, the mass of the products and reactants is constant and the first law can be written on a mass basis,\n\nIn the constant pressure adiabatic flame temperature case, the pressure of the system is held constant which results in the following equation for the work,\nAgain there is no heat transfer occurring because the process is defined to be adiabatic: formula_11. From the first law, we find that,\nRecalling the definition of enthalpy we recover: formula_13. Because this is a closed system, the mass of the products and reactants is constant and the first law can be written on a mass basis,\n\nWe see that the adiabatic flame temperature of the constant pressure process is lower than that of the constant volume process. This is because some of the energy released during combustion goes into changing the volume of the control system. One analogy that is commonly made between the two processes is through combustion in an internal combustion engine. For the constant volume adiabatic process, combustion is thought to occur instantaneously when the piston reaches the top of its apex (Otto cycle or constant volume cycle). For the constant pressure adiabatic process, while combustion is occurring the piston is moving in order to keep the pressure constant (Diesel cycle or constant pressure cycle).\n\nIf we make the assumption that combustion goes to completion (i.e. formula_15 and formula_16), we can calculate the adiabatic flame temperature by hand either at stoichiometric conditions or lean of stoichiometry (excess air). This is because there are enough variables and molar equations to balance the left and right hand sides,\n\nRich of stoichiometry there are not enough variables because combustion cannot go to completion with at least formula_18 and formula_19 needed for the molar balance (these are the most common incomplete products of combustion),\n\nHowever, if we include the Water gas shift reaction,\nand use the equilibrium constant for this reaction, we will have enough variables to complete the calculation.\n\nDifferent fuels with different levels of energy and molar constituents will have different adiabatic flame temperatures.\n\nWe can see by the following figure why nitromethane (CHNO) is often used as a power boost for cars. Since each mole of nitromethane contains two moles of oxygen, it can burn much hotter because it provides its own oxidant along with fuel. This in turn allows it to build up more pressure during a constant volume process. The higher the pressure, the more force upon the piston creating more work and more power in the engine. It stays relatively hot rich of stoichiometry because it contains its own oxidant. However, continual running of an engine on nitromethane will eventually melt the piston and/or cylinder because of this higher temperature.\n\nIn real world applications, complete combustion does not typically occur. Chemistry dictates that dissociation and kinetics will change the relative constituents of the products. There are a number of programs available that can calculate the adiabatic flame temperature taking into account dissociation through equilibrium constants (Stanjan, NASA CEA, AFTP). The following figure illustrates that the effects of dissociation tend to lower the adiabatic flame temperature. This result can be explained through Le Chatelier's principle.\n\n\n\n\n"}
{"id": "1545928", "url": "https://en.wikipedia.org/wiki?curid=1545928", "title": "Aggregate (composite)", "text": "Aggregate (composite)\n\nAggregate is the component of a composite material that resists compressive stress and provides bulk to the composite material. For efficient filling, aggregate should be much smaller than the finished item, but have a wide variety of sizes. For example, the particles of stone used to make concrete typically include both sand and gravel.\n\n\"Aggregate composites\" tend to be much easier to fabricate, and much more predictable in their finished properties, than \"fiber composites\". Fiber orientation and continuity can have an overwhelming effect, but can be difficult to control and assess. Fabrication aside, aggregate materials themselves also tend to be less expensive; the most common aggregates mentioned above are found in nature and can often be used with only minimal processing.\n\nNot all composite materials include aggregate. Aggregate particles tend to have about the same dimensions in every direction (that is, an of about one), so that aggregate composites do not display the level of synergy that fiber composites often do. A strong aggregate held together by a weak matrix will be weak in tension, whereas fibers can be less sensitive to matrix properties, especially if they are properly oriented and run the entire length of the part (i.e., a \"continuous filament\").\n\nMost composites are filled with particles whose aspect ratio lies somewhere between oriented filaments and spherical aggregates. A good compromise is \"chopped fiber\", where the performance of filament or cloth is traded off in favor of more aggregate-like processing techniques. Ellipsoid and plate-shaped aggregates are also used.\n\nIn most cases, the ideal finished piece would be 100% aggregate. A given application's most desirable quality (be it high strength, low cost, high dielectric constant, or low density) is usually most prominent in the aggregate itself; all the aggregate lacks is the ability to flow on a small scale, and form attachments between particles. The matrix is specifically chosen to serve this role, but its abilities should not be abused.\n\nExperiments and mathematical models show that more of a given volume can be filled with hard spheres if it is first filled with large spheres, then the spaces between () are filled with smaller spheres, and the new interstices filled with still smaller spheres as many times as possible. For this reason, control of \"particle size distribution\" can be quite important in the choice of aggregate; appropriate simulations or experiments are necessary to determine the optimal proportions of different-sized particles.\n\nThe upper limit to particle size depends on the amount of flow required before the composite sets (the gravel in paving concrete can be fairly coarse, but fine sand must be used for tile mortar), whereas the lower limit is due to the thickness of matrix material at which its properties change (clay is not included in concrete because it would \"absorb\" the matrix, preventing a strong bond to other aggregate particles). Particle size distribution is also the subject of much study in the fields of ceramics and powder metallurgy.\n\nMultiply the length of the area (in feet) by the width of the area (in feet) = Square Feet\nMultiply Square Feet by the Depth (in feet)* = Cubic Feet\nDivide Cubic Feet by 27 = Cubic Yards\nMultiply Cubic Yards by the material density (usually between 1 to 1.5 tons per cubic yard) = Tons Needed\n\nThere are many source of calculation of aggregate of crushing material but the known methods are:\n\n1. By metric calculations.\nIt is simple way of calculation containing human involvement by meter taps. Furthermore, in this type of calculation, we take length, width and height as an average value. Product of all these component give cubic meter value. This was the oldest method and its results can vary between 15-20%.\n\n2. By Surveying.\nIn this type of calculation we measure through levels of aggregate stored in stockpile. this was the advanced method of calculation than meters. the results can vary between 12-18%\n\n3. By GPS:\nIt is the latest technique to calculate the aggregates, apparently, the closest to accurate values because it is controlled through GPS(Global Positioning System) by satellite. In this technique, the receiver is fixed at the location where signals reception is acceptable to the local network and the values are calculated by meters. The results can be obtained via Autocad and 3D softwares. Consequently, the results can vary between 10-12%.\n\nToughness is a compromise between the (often contradictory) requirements of strength and plasticity. In many cases, the aggregate will have one of these properties, and will benefit if the matrix can add what it lacks. Perhaps the most accessible examples of this are composites with an organic matrix and ceramic aggregate, such as asphalt concrete (\"tarmac\") and filled plastic (i.e., Nylon mixed with powdered glass), although most metal matrix composites also benefit from this effect. In this case, the correct balance of hard and soft components is necessary or the material will become either too weak or too brittle.\n\nMany materials properties change radically at small length scales (see nanotechnology). In the case where this change is desirable, a certain range of aggregate size is necessary to ensure good performance. This naturally sets a lower limit to the amount of matrix material used.\n\nUnless some practical method is implemented to orient the particles in micro- or nano-composites, their small size and (usually) high strength relative to the particle-matrix bond allows any macroscopic object made from them to be treated as an aggregate composite in many respects.\n\nWhile bulk synthesis of such nanoparticles as carbon nanotubes is currently too expensive for widespread use, some less extreme nanostructured materials can be synthesized by traditional methods, including electrospinning and spray pyrolysis. One important aggregate made by spray pyrolysis is glass microspheres. Often called \"microballoons\", they consist of a hollow shell several tens of nanometers thick and approximately one micrometer in diameter. Casting them in a polymer matrix yields syntactic foam, with extremely high compressive strength for its low density.\n\nMany traditional nanocomposites escape the problem of aggregate synthesis in one of two ways:\n\nNatural aggregates: By far the most widely used aggregates for nano-composites are naturally occurring. Usually these are ceramic materials whose crystalline structure is extremely directional, allowing it to be easily separated into flakes or fibers. The nanotechnology touted by General Motors for automotive use is in the former category: a fine-grained clay with a laminar structure suspended in a thermoplastic olefin (a class which includes many common plastics like polyethylene and polypropylene). The latter category includes fibrous asbestos composites (popular in the mid-20th century), often with matrix materials such as linoleum and Portland cement.\n\nIn-situ aggregate formation: Many micro-composites form their aggregate particles by a process of self-assembly. For example, in high impact polystyrene, two immiscible phases of polymer (including brittle polystyrene and rubbery polybutadiene) are mixed together. Special molecules (graft copolymers) include separate portions which are soluble in each phase, and so are only stable at the interface between them, in the manner of a detergent. Since the number of this type of molecule determines the interfacial area, and since spheres naturally form to minimize surface tension, synthetic chemists can control the size of polybutadiene droplets in the molten mix, which harden to form rubbery aggregates in a hard matrix. Dispersion strengthening is a similar example from the field of metallurgy. In glass-ceramics, the aggregate is often chosen to have a negative coefficient of thermal expansion, and the proportion of aggregate to matrix adjusted so that the overall expansion is very near zero. Aggregate size can be reduced so that the material is transparent to infrared light.\n\n"}
{"id": "12696423", "url": "https://en.wikipedia.org/wiki?curid=12696423", "title": "Aoyama Plateau Wind Farm", "text": "Aoyama Plateau Wind Farm\n\nThe , is a large wind farm in Japan. \nThe wind farm is located in the Aoyama Plateau, Muroo-Akame-Aoyama Quasi-National Park.\nWith a total capacity of 95 MW, it is the largest wind farm in the country.\n\nThe wind farm was built in stages. \nThe first phase consisted of 20 750 kW turbines manufactured by JFE Engineering, commissioned in 2003.\nThese turbines have a rotor diameter of 50.5 m and a tower height of 50 m.\nThis original site covers 10.5 hectares and produced a maximum of 15 MW.\nThe second phase was commissioned between 2016 and 2017 and comprises an additional 40 larger 2 MW turbines with a rotor diameter of 65.4 m and a tower height of 80 m, manufactured by Hitachi.\n\n\n"}
{"id": "57040011", "url": "https://en.wikipedia.org/wiki?curid=57040011", "title": "Bituminous Coal Operators Association", "text": "Bituminous Coal Operators Association\n\nBituminous Coal Operators Association (BCOA) is a coal mining lobbying organization. It was founded in 1950 by various companies to deal with the UMWA and unionizing of mines during the change from human labor to mechanical labor. The BCOA would strike deals between miners, mine companies, and coal buying companies to provide a steady flow of continuous labor and a steady purchasing price for coal. The main deals normally contained negotiations of some miners being put out of work by mechanizations while the miners left would be guaranteed a steady job and pay as long as they agreed to not hold up progress with strikes and other activities. In addition, the BCOA hears requests from the UMWA employees for pay raises but often results in unprotected employees being laid off after a deal has been reached. The current president of BCOA is David M Young. He is the main representative for the BCOA and lobbyist.\n\nBCOA was formed in 1950 as a group to negotiate with the UMWA on behalf of large coal companies located in the Appalachian region. BCOA agreed to provide job security to workers in mining companies who were part of BCOA and also negotiate wages and benefits. UMWA and BCOA came to an agreement that miners would not strike against layoffs if they were allowed health benefits and a fair wage. BCOA still exists today but is much smaller organization and is now mainly used for lobbying for important coal issues. BCOA recently has been working with the UMWA and Murray mines to negotiate a new contract between thousands of workers in West Virginia, Ohio, and Pennsylvania. \n\nIn 1946, under President John L. Lewis, the union created The Welfare and Retirement Fund. This was an entirely new method for benefits and pensions because it introduced health care for the union workers and their families. \n\nIn May, of 1946 the National Bituminous Coal Wage Agreement established a health, welfare, and retirement fund backed up by a five-cents-per-ton levy on all coal produced for the use by bituminous coal companies. \n\nFrom the 1950s to the 1970s there were various updates to the agreement with the goal to completely satisfy miners enough to end random wildcat strikes throughout coal mines across the country. By 1974, the coal industry led the country for the rate of work stoppages in a year, ten times the rate in other industries.\n\nOn December 6, of 1974 a new National Bituminous Coal Wage Agreement resulted in the development of four separate trust funds, replacing the single Welfare and Retirement Funds. \nThe 1950 Benefit Pension Trust was created for workers retiring before 1976; the 1974 Pension Trust, for workers retiring after 1975; the 1950 Benefit Trust, provides medical and death benefits for workers retiring after 1975 and their dependents; and the 1974 Benefit trust, providing medical and death benefits for workers retiring after 1975 and their dependents. These four funds operate collectively under the title Health and Retirement Funds.\n\nThis agreement has sparked an unending conflict between the BCOA and the UMWA. The main problem being the fact that the UMWA cannot present an agreement that can stand, and after time the BCOA does not deem the agreement any longer acceptable, which leads to multiple modifications as time progresses. \n\nFor taxpayers, the agreement seems entirely unfair. As the UMWA makes modifications to the agreements, such as the changes it made in 1974 which resulted in 5.57 billion dollars worth of unfunded liabilities. These liabilities and money that is unaccounted for is coming out of taxpayers paychecks. This original agreement in 1940 has stemmed many arguments between the UMWA and the BCOA even until today (2018).\n\nThe majority of this has been work on specific acts like the Safety acts ,which deals with health benefits to miners, and other smaller health acts or protection bills like the Federal mine and health and safety act of 1977 which was created by the Mine Safety and Health Administration (MSHA) who deal with the BCOA on many accounts. The BCOA also works on an average of one land restoration act a year like the Crow Tribe Land Restoration Act which was passed in 2009.\n\nhttps://libraries.psu.edu/findingaids/1827.htm\n\nhttps://isreview.org/issue/74/miners-strike-1977-78\n\n"}
{"id": "4744787", "url": "https://en.wikipedia.org/wiki?curid=4744787", "title": "Blue-listed", "text": "Blue-listed\n\nBlue-listed species includes any indigenous species or subspecies (taxa) considered to be vulnerable in their locale. Vulnerable taxa are of special concern because of characteristics that make them particularly sensitive to human activities or natural events. Blue-listed taxa are at risk, but are not extirpated, endangered or threatened.\n\n\n"}
{"id": "54147", "url": "https://en.wikipedia.org/wiki?curid=54147", "title": "Bremsstrahlung", "text": "Bremsstrahlung\n\nBremsstrahlung (), from \"to brake\" and \"radiation\"; i.e., \"braking radiation\" or \"deceleration radiation\", is electromagnetic radiation produced by the deceleration of a charged particle when deflected by another charged particle, typically an electron by an atomic nucleus. The moving particle loses kinetic energy, which is converted into radiation (i.e., a photon), thus satisfying the law of conservation of energy. The term is also used to refer to the process of producing the radiation. Bremsstrahlung has a continuous spectrum, which becomes more intense and whose peak intensity shifts toward higher frequencies as the change of the energy of the decelerated particles increases.\n\nBroadly speaking, bremsstrahlung or braking radiation is any radiation produced due to the deceleration (negative acceleration) of a charged particle, which includes synchrotron radiation (i.e. photon emission by a relativistic particle), cyclotron radiation (i.e. photon emission by a non-relativistic particle), and the emission of electrons and positrons during beta decay. However, the term is frequently used in the more narrow sense of radiation from electrons (from whatever source) slowing in matter.\n\nBremsstrahlung emitted from plasma is sometimes referred to as free/free radiation. This refers to the fact that the radiation in this case is created by charged particles that are free; i.e., not part of an ion, atom or molecule, both before and after the deflection (acceleration) that caused the emission.\n\nA charged particle accelerating in a vacuum radiates power, as described by the Larmor formula and its relativistic generalizations. Although the term, \"bremsstrahlung\", is \"usually\" reserved for charged particles accelerating in matter, not vacuum, the formulas are similar. (In this respect, bremsstrahlung differs from Cherenkov radiation, another kind of braking radiation which occurs \"only\" in matter, and not in a vacuum.)\n\nThe most established relativistic formula for total radiated power is given by\n\nwhere formula_2 (the velocity of the particle divided by the speed of light), formula_3 is the Lorentz factor, formula_4 signifies a time derivative of formula_5, and \"q\" is the charge of the particle. This is commonly written in the mathematically equivalent form using formula_6:\n\nIn the case where velocity is parallel to acceleration (for example, linear motion), the formula simplifies to\n\nwhere formula_9 is the acceleration. For the case of acceleration perpendicular to the velocity formula_10 (a case that arises in circular particle accelerators known as synchrotrons), the total power radiated reduces to\n\nradiated in the two limiting cases is proportional to formula_12 formula_13 or formula_14 formula_15. Since formula_16, we see that the total radiated power goes as formula_17 or formula_18, which accounts for why electrons lose energy to bremsstrahlung radiation much more rapidly than heavier charged particles (e.g., muons, protons, alpha particles). This is the reason a TeV energy electron-positron collider (such as the proposed International Linear Collider) cannot use a circular tunnel (requiring constant acceleration), while a proton-proton collider (such as the Large Hadron Collider) can utilize a circular tunnel. The electrons lose energy due to bremsstrahlung at a rate formula_19 times higher than protons do.\n\nThe most general formula for radiated power as a function of angle is:\nwhere formula_21 is a unit vector pointing from the particle towards the observer, and formula_22 is an infinitesimal bit of solid angle.\n\nIn the case where velocity is parallel to acceleration (for example, linear motion), this simplifies to\nwhere formula_24 is the angle between formula_25 and the direction of observation.\n\nNOTE: this article currently gives formulas that apply in the Rayleigh-Jeans limit formula_26, and does not use a quantized (Planck) treatment of radiation. Thus a usual factor like formula_27 does not appear. The appearance of formula_28in y below is due to the quantum-mechanical treatment of collisions.\n\nIn a plasma, the free electrons continually collide with the ions, producing bremsstrahlung. A complete analysis requires accounting for both binary Coulomb collisions as well as collective (dielectric) behavior. A detailed treatment is given by Bekefi, while a simplified one is given by Ichimaru. In this section we follow Bekefi's dielectric treatment, with collisions included approximately via the cutoff wavenumber, formula_29.\n\nConsider a uniform plasma, with thermal electrons distributed according to the Maxwell–Boltzmann distribution with the temperature formula_30. Following Bekefi, the power spectral density (power per angular frequency interval per volume, integrated over the whole formula_31 sr of solid angle, and in both polarizations) of the bremsstrahlung radiated, is calculated to be\n\nwhere formula_33 is the electron plasma frequency, formula_34 is the photon frequency, formula_35 is the number density of electrons and ions, and other symbols are physical constants. The second bracketed factor is the index of refraction of a light wave in a plasma, and shows that emission is greatly suppressed for formula_36 (this is the cutoff condition for a light wave in a plasma; in this case the light wave is evanescent). This formula thus only applies for formula_37. This formula should be summed over ion species in a multi-species plasma.\n\nThe special function formula_38 is defined in the exponential integral article, and the unitless quantity formula_39 is\n\nformula_29 is a maximum or cutoff wavenumber, arising due to binary collisions, and can vary with ion species. Roughly, formula_42 when formula_43 (typical in plasmas that are not too cold), where formula_44 eV is the Hartree energy, and formula_45 is the electron thermal de Broglie wavelength. Otherwise, formula_46 where formula_47 is the classical Coulomb distance of closest approach.\n\nFor the usual case formula_42, we find\n\nThe formula for formula_50 is approximate, in that it neglects enhanced emission occurring for formula_34 slightly above formula_52.\n\nIn the limit formula_53, we can approximate E1 as\nformula_54\nwhere formula_55 is the Euler–Mascheroni constant. The leading, logarithmic term is frequently used, and resembles the Coulomb logarithm that occurs in other collisional plasma calculations. For formula_56 the log term is negative, and the approximation is clearly inadequate. Bekefi gives corrected expressions for the logarithmic term that match detailed binary-collision calculations.\n\nThe total emission power density, integrated over all frequencies, is\n\nNote the appearance of formula_62 due to the quantum nature of formula_63. In practical units, a commonly used version of this formula for formula_64 is \n\nThis formula is 1.59 times the one given above, with the difference due to details of binary collisions. Such ambiguity is often expressed by introducing Gaunt factor formula_66, e.g. in one finds\n\nwhere everything is expressed in the CGS units.\n\nFor very high temperatures there are relativistic corrections to this formula, that is, additional terms of the order of formula_68\n\nIf the plasma is optically thin, the bremsstrahlung radiation leaves the plasma, carrying part of the internal plasma energy. This effect is known as the \"bremsstrahlung cooling\". It is a type of radiative cooling. The energy carried away by bremsstrahlung is called \"bremsstrahlung losses\" and represents a type of radiative losses. One generally uses the term \"bremsstrahlung losses\" in the context when the plasma cooling is undesired, as e.g. in fusion plasmas.\n\nPolarizational bremsstrahlung (sometimes referred to as \"atomic bremsstrahlung\") is the radiation emitted by the target's atomic electrons as the target atom is polarized by the Coulomb field of the incident charged particle. Polarizational bremsstrahlung contributions to the total bremsstrahlung spectrum have been observed in experiments involving relatively massive incident particles, resonance processes, and free atoms. However, there is still some debate as to whether or not there are significant polarizational bremsstrahlung contributions in experiments involving fast electrons incident on solid targets.\n\nIt is worth noting that the term \"polarizational\" is not meant to imply that the emitted bremsstrahlung is polarized. Also, the angular distribution of polarizational bremsstrahlung is theoretically quite different than ordinary bremsstrahlung.\n\nIn an X-ray tube, electrons are accelerated in a vacuum by an electric field towards a piece of metal called the \"target\". X-rays are emitted as the electrons slow down (decelerate) in the metal. The output spectrum consists of a continuous spectrum of X-rays, with additional sharp peaks at certain energies. The continuous spectrum is due to bremsstrahlung, while the sharp peaks are characteristic X-rays associated with the atoms in the target. For this reason, bremsstrahlung in this context is also called continuous X-rays.\n\nThe shape of this continuum spectrum is approximately described by Kramers' law.\n\nThe formula for Kramers' law is usually given as the distribution of intensity (photon count) formula_69 against the wavelength formula_70 of the emitted radiation:\n\nThe constant \"K\" is proportional to the atomic number of the target element, and formula_72 is the minimum wavelength given by the Duane–Hunt law.\n\nThe spectrum has a sharp cutoff at formula_72, which is due to the limited energy of the incoming electrons. For example, if an electron in the tube is accelerated through 60 kV, then it will acquire a kinetic energy of 60 keV, and when it strikes the target it can create X-rays with energy of at most 60 keV, by conservation of energy. (This upper limit corresponds to the electron coming to a stop by emitting just one X-ray photon. Usually the electron emits many photons, and each has an energy less than 60 keV.) A photon with energy of at most 60 keV has wavelength of at least 21 pm, so the continuous X-ray spectrum has exactly that cutoff, as seen in the graph. More generally the formula for the low-wavelength cutoff, the Duane-Hunt law, is:\nwhere \"h\" is Planck's constant, \"c\" is the speed of light, \"V\" is the voltage that the electrons are accelerated through, \"e\" is the elementary charge, and \"pm\" is picometres.\n\nBeta particle-emitting substances sometimes exhibit a weak radiation with continuous spectrum that is due to bremsstrahlung (see the \"outer bremsstrahlung\" below). In this context, bremsstrahlung is a type of \"secondary radiation\", in that it is produced as a result of stopping (or slowing) the primary radiation (beta particles). It is very similar to X-rays produced by bombarding metal targets with electrons in X-ray generators (as above) except that it is produced by high-speed electrons from beta radiation.\n\nThe \"inner\" bremsstrahlung (also known as \"internal bremsstrahlung\") arises from the creation of the electron and its loss of energy (due to the strong electric field in the region of the nucleus undergoing decay) as it leaves the nucleus. Such radiation is a feature of beta decay in nuclei, but it is occasionally (less commonly) seen in the beta decay of free neutrons to protons, where it is created as the beta electron leaves the proton.\n\nIn electron and positron emission by beta decay the photon's energy comes from the electron-nucleon pair, with the spectrum of the bremsstrahlung decreasing continuously with increasing energy of the beta particle. In electron capture, the energy comes at the expense of the neutrino, and the spectrum is greatest at about one third of the normal neutrino energy, decreasing to zero electromagnetic energy at normal neutrino energy. Note that in the case of electron capture, bremsstrahlung is emitted even though no charged particle is emitted. Instead, the bremsstrahlung radiation may be thought of as being created as the captured electron is accelerated toward being absorbed. Such radiation may be at frequencies that are the same as soft gamma radiation, but it exhibits none of the sharp spectral lines of gamma decay, and thus is not technically gamma radiation.\n\nThe internal process is to be contrasted with the \"outer\" bremsstrahlung due to the impingement on the nucleus of electrons coming from the outside (i.e., emitted by another nucleus), as discussed above.\n\nIn some cases, \"e.g.\" , the bremsstrahlung produced by shielding the beta radiation with the normally used dense materials (\"e.g.\" lead) is itself dangerous; in such cases, shielding must be accomplished with low density materials, \"e.g.\" Plexiglas (Lucite), plastic, wood, or water; as the atomic number is lower for these materials, the intensity of bremsstrahlung is significantly reduced, but a larger thickness of shielding is required to stop the electrons (beta radiation).\n\nThe dominant luminous component in a cluster of galaxies is the 10 to 10 kelvin intracluster medium. The emission from the intracluster medium is characterized by thermal bremsstrahlung. This radiation is in the energy range of X-rays and can be easily observed with space-based telescopes such as Chandra X-ray Observatory, XMM-Newton, ROSAT, ASCA, EXOSAT, Suzaku, RHESSI and future missions like IXO and Astro-H .\n\nBremsstrahlung is also the dominant emission mechanism for H II regions at radio wavelengths.\n\nIn electric discharges, for example as laboratory discharges between two electrodes or as lightning discharges between cloud and ground or within clouds, electrons produce Bremsstrahlung photons while scattering off air molecules. These photons become manifest in terrestrial gamma-ray flashes and are the source for beams of electrons, positrons, neutrons and protons. The appearance of Bremsstrahlung photons also influences the propagation and morphology of discharges in nitrogen-oxygen mixtures with low percentages of oxygen.\n\nThe complete quantum mechanical description was first performed by Bethe and Heitler. They assumed plane waves for electrons which scatter at the nucleus of an atom, and derived a cross section which relates the complete geometry of that process to the frequency of the emitted photon. The quadruply differential cross section which shows a quantum mechanical symmetry to pair production, is:\n\nThere formula_76 is the atomic number, formula_77 the fine structure constant, formula_62 the reduced Planck's constant and formula_79 the speed of light. The kinetic energy formula_80 of the electron in the initial and final state is connected to its total energy formula_81 or its momenta formula_82 via\n\nwhere formula_84 is the mass of an electron. Conservation of energy gives\n\nwhere formula_86 is the photon energy. The directions of the emitted photon and the scattered electron are given by\n\nwhere formula_88 is the momentum of the photon.\n\nThe differentials are given as\n\nThe absolute value of the virtual photon between the nucleus and electron is\n\nThe range of validity is given by the Born approximation\n\nwhere this relation has to be fulfilled for the velocity formula_92 of the electron in the initial and final state.\n\nFor practical applications (e.g. in Monte Carlo codes) it can be interesting to focus on the relation between the frequency formula_34 of the emitted photon and the angle between this photon and the incident electron. Köhn and Ebert integrated the quadruply differential cross section by Bethe and Heitler over formula_94 and formula_95 and obtained:\n\nwith\n\nand\n\nHowever, a much simpler expression for the same integral can be found in (Eq. 2BN) and in (Eq. 4.1).\n\nAn analysis of the doubly differential cross section above shows that electrons whose kinetic energy is larger than the rest energy (511 keV) emit photons in forward direction while electrons with a small energy emit photons isotropically.\n\nOne mechanism, considered important for small atomic numbers formula_76, is the scattering of a free electron at the shell electrons of an atom or molecule. Since electron–electron bremsstrahlung is a function of formula_76 and the usual electron-nucleus bremsstrahlung is a function of formula_101, electron–electron bremsstrahlung is negligible for metals. For air, however, it plays an important role in the production of terrestrial gamma-ray flashes.\n\n\n"}
{"id": "9591722", "url": "https://en.wikipedia.org/wiki?curid=9591722", "title": "Buddam (unit)", "text": "Buddam (unit)\n\nA buddam (also known as a chow) is an obsolete unit of mass used in the pearl trade in Mumbai (formerly Bombay) in the 19th century. A buddam equalled 1/1600 chow, or 1/16 docra.\n\n"}
{"id": "3134585", "url": "https://en.wikipedia.org/wiki?curid=3134585", "title": "Charge density", "text": "Charge density\n\nIn electromagnetism, charge density is a measure of the amount of electric charge per unit length, surface area, or volume. \"Volume charge density\" (symbolized by the Greek letter ρ) is the quantity of charge per unit volume, measured in the SI system in coulombs per cubic meter (C•m), at any point in a volume. \"Surface charge density\" (σ) is the quantity of charge per unit area, measured in coulombs per square meter (C•m), at any point on a surface charge distribution on a two dimensional surface. \"Linear charge density\" (λ) is the quantity of charge per unit length, measured in coulombs per meter (C•m), at any point on a line charge distribution. Charge density can be either positive or negative, since electric charge can be either positive or negative.\n\nLike mass density, charge density can vary with position. In classical electromagnetic theory charge density is idealized as a \"continuous\" scalar function of position formula_1, like a fluid, and formula_2, formula_3, and formula_4 are usually regarded as continuous charge distributions, even though all real charge distributions are made up of discrete charged particles. Due to the conservation of electric charge, the charge density in any volume can only change if an electric current of charge flows into or out of the volume. This is expressed by a continuity equation which links the rate of change of charge density formula_2 and the current density formula_6.\n\nSince all charge is carried by subatomic particles, which can be idealized as points, the concept of a \"continuous\" charge distribution is an approximation, which becomes inaccurate at small length scales. A charge distribution is ultimately composed of individual charged particles separated by regions containing no charge. For example the charge in an electrically charged metal object is made up of conduction electrons moving randomly in the metal's crystal lattice. Static electricity is caused by surface charges consisting of ions on the surface of objects, and the space charge in a vacuum tube is composed of a cloud of free electrons moving randomly in space. The charge carrier density in a conductor is equal to the number of mobile charge carriers (electrons, ions, etc.) per unit volume. The charge density at any point is equal to the charge carrier density multiplied by the elementary charge on the particles. However because the elementary charge on an electron is so small (1.6•10 C) and there are so many of them in a macroscopic volume (there are about 10 conduction electrons in a cubic centimeter of copper) the continuous approximation is very accurate when applied to macroscopic volumes, and even microscopic volumes above the nanometer level.\n\nAt atomic scales, due to the uncertainty principle of quantum mechanics, a charged particle does not \"have\" a precise position but is represented by a probability distribution, so the charge of an individual particle is not concentrated at a point but is 'smeared out' in space and acts like a true continuous charge distribution. This is the meaning of 'charge distribution' and 'charge density' used in chemistry and chemical bonding. An electron is represented by a \"wavefunction\" formula_7 whose square is proportional to the probability of finding the electron at any point formula_1 in space, so formula_9 is proportional to the charge density of the electron at any point. In atoms and molecules the charge of the electrons is distributed in clouds called orbitals which surround the atom or molecule, and are responsible for chemical bondings.\n\nFollowing are the definitions for continuous charge distributions.\n\nThe linear charge density is the ratio of an infinitesimal electric charge d\"Q\" (SI unit: C) to an infinitesimal line element, \nsimilarly the surface charge density uses a surface area element d\"S\" \nand the volume charge density uses a volume element d\"V\"\n\nIntegrating the definitions gives the total charge \"Q\" of a region according to line integral of the linear charge density λ(r) over a line or 1d curve \"C\", \nsimilarly a surface integral of the surface charge density σ(r) over a surface \"S\",\nand a volume integral of the volume charge density ρ(r) over a volume \"V\", \n\nwhere the subscript \"q\" is to clarify that the density is for electric charge, not other densities like mass density, number density, probability density, and prevent conflict with the many other uses of λ, σ, ρ in electromagnetism for wavelength, electrical resistivity and conductivity.\n\nWithin the context of electromagnetism, the subscripts are usually dropped for simplicity: λ, σ, ρ. Other notations may include: ρ, ρ, ρ, ρ, ρ, ρ etc.\n\nThe total charge divided by the length, surface area, or volume will be the average charge densities:\n\nIn dielectric materials, the total charge of an object can be separated into \"free\" and \"bound\" charges.\n\nBound charges set up electric dipoles in response to an applied electric field E, and polarize other nearby dipoles tending to line them up, the net accumulation of charge from the orientation of the dipoles is the bound charge. They are called bound because they cannot be removed: in the dielectric material the charges are the electrons bound to the nuclei.\n\nFree charges are the excess charges which can move into electrostatic equilibrium, i.e. when the charges are not moving and the resultant electric field is independent of time, or constitute electric currents.\n\nIn terms of volume charges densities, the total charge density is:\n\nas for surface charge densities:\n\nwhere subscripts \"f\" and \"b\" denote \"free\" and \"bound\" respectively.\n\nThe bound surface charge is the charge piled up at the surface of the dielectric, given by the dipole moment perpendicular to the surface:\n\nwhere s is the separation between the point charges constituting the dipole, formula_20 is the electric dipole moment, formula_21 is the unit normal vector to the surface.\n\nTaking infinitesimals:\n\nand dividing by the differential surface element \"dS\" gives the bound surface charge density:\n\nSee Maxwell's equations and constitutive relation for more details.\n\nFor the special case of a homogeneous charge density ρ, independent of position i.e. constant throughout the region of the material, the equation simplifies to:\n\nThe proof of this is immediate. Start with the definition of the charge of any volume:\n\nThen, by definition of homogeneity, ρ(r) is a constant denoted by ρ (to differ between the constant and non-constant densities), and so by the properties of an integral can be pulled outside of the integral resulting in:\n\nso,\n\nThe equivalent proofs for linear charge density and surface charge density follow the same arguments as above.\n\nFor a single point charge \"q\" at position r inside a region of 3d space \"R\", like an electron, the volume charge density can be expressed by the Dirac delta function:\n\nwhere r is the position to calculate the charge.\n\nAs always, the integral of the charge density over a region of space is the charge contained in that region. The delta function has the \"shifting property\" for any function \"f\":\n\nso the delta function ensures that when the charge density is integrated over \"R\", the total charge in \"R\" is \"q\":\n\nThis can be extended to \"N\" discrete point-like charge carriers. The charge density of the system at a point r is a sum of the charge densities for each charge \"q\" at position r, where :\n\nThe delta function for each charge \"q\" in the sum, \"δ\"(r − r), ensures the integral of charge density over \"R\" returns the total charge in \"R\":\n\nIf all charge carriers have the same charge \"q\" (for electrons \"q\" = −\"e\", the electron charge) the charge density can be expressed through the number of charge carriers per unit volume, \"n\"(r), by\n\nSimilar equations are used for the linear and surface charge densities.\n\nIn special relativity, the length of a segment of wire depends on velocity of observer because of length contraction, so charge density will also depend on velocity. Anthony French\nhas described how the magnetic field force of a current-bearing wire arises from this relative charge density. He used (p 260) a Minkowski diagram to show \"how a neutral current-bearing wire appears to carry a net charge density as observed in a moving frame.\" When a charge density is measured in a moving frame of reference it is called proper charge density.\n\nIt turns out the charge density \"ρ\" and current density J transform together as a four current vector under Lorentz transformations.\n\nIn quantum mechanics, charge density \"ρ\" is related to wavefunction \"ψ\"(r) by the equation\n\nwhere \"q\" is the charge of the particle and |ψ(r)| = \"ψ\"*(r)\"ψ\"(r) is the probability density function i.e. probability per unit volume of a particle located at r.\n\nWhen the wavefunction is normalized - the average charge in the region r ∈ \"R\" is\n\nwhere dr is the integration measure over 3d position space.\n\nThe charge density appears in the continuity equation for electric current, and also in Maxwell's Equations. It is the principal source term of the electromagnetic field, when the charge distribution moves this corresponds to a current density. The charge density of molecules impacts chemical and separation processes. For example, charge density influences metal-metal bonding and hydrogen bonding. For separation processes such as nanofiltration, the charge density of ions influences their rejection by the membrane.\n\n\n\n"}
{"id": "3333683", "url": "https://en.wikipedia.org/wiki?curid=3333683", "title": "Combined gas and steam", "text": "Combined gas and steam\n\nCombined gas and steam (COGAS) is the name given to marine compound powerplants comprising gas and steam turbines, the latter being driven by steam generated using the heat from the exhaust of the gas turbines. In this way, some of the otherwise lost energy can be reclaimed and the specific fuel consumption of the plant can be decreased. Large (land-based) electric powerplants built using this combined cycle can reach conversion efficiencies of over 60%. \n\nIf the turbines do not drive a propeller shafts directly and instead a turbo-electric transmission is used, the system is also known as COGES.\n\nCOGAS differs from many other combined marine propulsion systems in that it is not intended to operate on one system alone. While this is possible, it will not operate efficiently this way, as with Combined diesel and gas systems when run solely on diesel engines. Especially COGAS should not be confused with Combined steam and gas (COSAG) power plants, which employ traditional, oil-fired boilers for steam turbine propulsion for normal cruising, and supplement this with gas turbines for faster reaction times and higher dash speed.\n\nCOGAS has been proposed as upgrade for ships that use gas turbines as their main (or only) engines, e.g. in COGOG or COGAG mode, such as the s, but currently no naval ship uses this concept. However some modern cruise ships are equipped with COGES. E.g. Celebrity Cruises' \"Millennium\" and other ships of her class use turbo-electric plants with two General Electric LM2500+ gas turbines and one steam-turbine.\n\nBMW is currently researching combined gas and steam for automotive use, using their turbosteamer system. This uses the waste heat of combustion from the exhaust and turns it into steam to produce torque which is input into the crankshaft.\n\n\n"}
{"id": "2331297", "url": "https://en.wikipedia.org/wiki?curid=2331297", "title": "Critical radius", "text": "Critical radius\n\nCritical radius is the minimum size that must be formed by atoms or molecules clustering together (in a gas, liquid or solid matrix) before a new-phase inclusion (a bubble, a droplet, or a solid particle) is stable and begins to grow. Formation of such stable \"nuclei\" is called nucleation. \n\nIn precipitation models this is generally a prelude to models of the growth process itself. Sometimes precipitation is rate-limited by the nucleation process. This happens for example before one takes a cup of superheated water from a microwave and, when jiggling it against dust particles on the wall of the cup, enables \"heterogeneous\" nucleation that then rapidly converts much of that water into steam.\n\nIf the change in phase forms a crystalline solid in a liquid matrix, the atoms might then form a dendrite. The crystal growth continues in three dimensions, the atoms attaching themselves in certain preferred directions, usually along the axes of a crystal, forming a characteristic tree-like structure of a dendrite.\n\nExample: the critical radius for spherical dendrite in an ideal system can be determined from its Gibbs free energy\n\nformula_1 \n\nwhere formula_2 is the Gibbs volume energy and formula_3 is the interfacial energy. The critical radius formula_4 is found by setting the derivative of formula_5 equal to zero\n\nformula_6 \n\nyielding\n\nformula_7,\n\nwhere formula_3 is the surface energy, and formula_2 is Gibbs energy per volume.\n\n\n"}
{"id": "3997462", "url": "https://en.wikipedia.org/wiki?curid=3997462", "title": "Di-tert-butyl peroxide", "text": "Di-tert-butyl peroxide\n\nDi-\"tert\"-butyl peroxide or DTBP is an organic compound consisting of a peroxide group bonded to two tert-butyl groups. It is one of the most stable organic peroxides. It is a colorless liquid.\n\nThe peroxide bond undergoes homolysis at temperatures >100 °C. For this reason di-\"tert\"-butyl peroxide is commonly used as a radical initiator in organic synthesis and polymer chemistry. The decomposition reaction proceeds via the generation of methyl radicals.\n\nDTBP can in principle be used in engines where oxygen is limited, since the molecule supplies both the oxidizer and the fuel.\n\n"}
{"id": "225270", "url": "https://en.wikipedia.org/wiki?curid=225270", "title": "Electrophorus", "text": "Electrophorus\n\nAn electrophorus or electrophore is a simple manual capacitive electrostatic generator used to produce electrostatic charge via the process of electrostatic induction. A first version of it was invented in 1762 by Swedish professor Johan Carl Wilcke, but Italian scientist Alessandro Volta improved and popularized the device in 1775, and is sometimes erroneously credited with its invention. The word \"electrophorus\" was coined by Volta from the Greek ήλεκτρον ('elektron'), and ϕέρω ('phero'), meaning 'electricity bearer'.\n\nThe electrophorus consists of a dielectric plate (originally a 'cake' of resinous material such as pitch or wax, but in modern versions plastic is used) and a metal plate with an insulating handle. The dielectric plate is first charged through the triboelectric effect by rubbing it with fur or cloth. For this discussion, imagine the dielectric gains negative charge by rubbing, as in the illustration below. The metal plate is then placed onto the dielectric plate. The dielectric does not transfer a significant fraction of its surface charge to the metal because the microscopic contact is poor. Instead the electrostatic field of the charged dielectric causes the charges in the metal plate to separate. It develops two regions of charge—the positive charges in the plate are attracted to the side facing down toward the dielectric, charging it positively, while the negative charges are repelled to the side facing up, charging it negatively, with the plate remaining electrically neutral as a whole. Then, the side facing up is momentarily grounded (which can be done by touching it with a finger), draining off the negative charge. Finally, the metal plate, now carrying only one sign of charge (positive in our example), is lifted.\n\nSince the charge on the dielectric is not depleted in this process, the charge on the metal plate can be used for experiments, for example by touching it to metal conductors allowing the charge to drain away, and the uncharged metal plate can be placed back on the dielectric and the process repeated to get another charge. This can be repeated as often as desired, so in principle an unlimited amount of induced charge can be obtained from a single charge on the dielectric. For this reason Volta called it \"elettroforo perpetuo\" (the perpetual electricity bearer). In actual use the charge on the dielectric will eventually (within a few days at most) leak off through the surface of the cake or the atmosphere to recombine with opposite charges around to restore neutrality.\n\nOne of the largest examples of an electrophorus was built in 1777 by German scientist Georg Christoph Lichtenberg. It was 6 feet (2 m) in diameter, with the metal plate raised and lowered using a pulley system. It could reportedly produce 15 inch (38 cm) sparks. Lichtenberg used its discharges to create the strange treelike marks known as Lichtenberg figures.\n\nCharge in the universe is conserved. The electrophorus simply separates positive and negative charges. A positive or negative charge ends up on the metal plate (or other storage conductor), and the opposite charge is stored in another object after grounding (in the earth or the person touching the metal plate). This separation takes work since the lowest energy state implies uncharged objects. Work is done by raising the charged metal plate away from the oppositely charged resinous plate. This additional energy put into the system is converted to potential energy in the form of charge separation (opposite charges that were originally on the plate), so raising the metal plate actually increases its voltage relative to the dielectic plate.\n\nThe electrophorus is thus actually a manually operated electrostatic generator, using the same principle of electrostatic induction as electrostatic machines such as the Wimshurst machine and the Van de Graaff generator.\n\n"}
{"id": "49244155", "url": "https://en.wikipedia.org/wiki?curid=49244155", "title": "Entegrus Powerlines", "text": "Entegrus Powerlines\n\nEntegrus Powerlines is an electric distributor located in southwestern Ontario, Canada. The utility provides electrical supply to approximately 40,000 customers. \nIts service territory covers 96 square kilometers of urban areas, encompassed within a 5,000 square kilometer geographic area between Windsor (to the west), London (to the east) and Sarnia (to the north). \nThe Entegrus Powerlines service territory is more specifically described as encompassing the following:\n\nThose parts of the following former municipalities (including the former Police Village of Merlin) that the former dissolved public utilities served on December 31, 1997\n"}
{"id": "11391769", "url": "https://en.wikipedia.org/wiki?curid=11391769", "title": "Erlasee Solar Park", "text": "Erlasee Solar Park\n\nThe Erlasee Solar Park, or \"Solarstrompark Gut Erlasee\", is an 11.4 megawatt (MW) photovoltaic power station located in Bavaria, southern Germany, in one of the sunniest regions of the country. Constructed on a former vineyard by the company Solon SE in 2006, it was then the world's largest photovoltaic power station.\n\nThe project uses 1,464 double-axis solar trackers to increase the annual electricity yield by 30 percent. Each tracker shoulders twelve conventional solar panels made of crystalline silicon. The plant generates about 14,000 megawatt-hours (MWh) annually, or as much as the average consumption of the nearby town of Arnstein.\n\nThe plant cost €70 million and covers an area of 77 hectares (190 acres). The project was officially commissioned on 1 September 2006. The inauguration party included a concert given by the popular German rock bands BAP and The BossHoss.\n\n\n"}
{"id": "37915634", "url": "https://en.wikipedia.org/wiki?curid=37915634", "title": "Feret diameter", "text": "Feret diameter\n\nThe Feret diameter or Feret's diameter is a measure of an object size along a specified direction. In general, it can be defined as the distance between the two parallel planes restricting the object perpendicular to that direction. It is therefore also called the caliper diameter, referring to the measurement of the object size with a caliper. This measure is used in the analysis of particle sizes, for example in microscopy, where it is applied to projections of a three-dimensional (3D) object on a 2D plane. In such cases, the Feret diameter is defined as the distance between two parallel tangential \"lines\" rather than \"planes\".\n\nFrom Cauchy's theorem it follows that for a 2D convex body, the Feret diameter averaged over all directions (〈F〉) is equal to the ratio of the object perimeter (P) and pi, i.e., 〈F〉 = P/. There is no such relation between 〈F〉 and P for a concave object.\n\nFeret diameter is used in the analysis of particle size and its distribution, e.g. in a powder or a polycrystalline solid; Alternative measures include Martin diameter, Krumbein diameter and Heywood diameter. The term first became common in scientific literature in the 1970s.\n\nIt is also used in biology as a method to analyze the size of cells in tissue sections.\n"}
{"id": "607495", "url": "https://en.wikipedia.org/wiki?curid=607495", "title": "Freezing-point depression", "text": "Freezing-point depression\n\nFreezing-point depression is the decrease of the freezing point of a solvent on addition of a non-volatile solute. Examples include salt in water, alcohol in water, or the mixing of two solids such as impurities into a finely powdered drug. In the last case, the added compound is the solute, and the original solid is thought of as the solvent. The resulting solution or solid–solid mixture has a lower freezing point than the pure solvent or solid. This phenomenon is what causes sea water, (a mixture of salt [and other things] in water) to remain liquid at temperatures below , the freezing point of pure water.\n\nThe freezing point is the temperature at which the liquid solvent and solid solvent are at equilibrium, so that their vapor pressures are equal. When a non-volatile solute is added to a volatile liquid solvent, the solution vapor pressure will be lower than that of the pure solvent. As a result the solid will reach equilibrium with the solution at a lower temperature than with the pure solvent.\n\nThe phenomenon of freezing-point depression has many practical uses. The radiator fluid in an automobile is a mixture of water and ethylene glycol. As a result of freezing-point depression, radiators do not freeze in winter (unless it is extremely cold, e.g. ). Road salting takes advantage of this effect to lower the freezing point of the ice it is placed on. Lowering the freezing point allows the street ice to melt at lower temperatures, preventing the accumulation of dangerous, slippery ice. Commonly used sodium chloride can depress the freezing point of water to about . If the road surface temperature is lower, NaCl becomes ineffective and other salts are used, such as calcium chloride, magnesium chloride or a mixture of many. These salts are somewhat aggressive to metals, especially iron, so in airports safer media such as sodium formate, potassium formate, sodium acetate, potassium acetate are used instead.\n\nFreezing-point depression is used by some organisms that live in extreme cold. Such creatures have evolved means through which they can produce high concentration of various compounds such as sorbitol and glycerol. This elevated concentration of solute decreases the freezing point of the water inside them, preventing the organism from freezing solid even as the water around them freezes, or as the air around them becomes very cold. Examples of organisms that produce antifreeze compounds include some species of arctic-living fish such as the rainbow smelt, which produces glycerol and other molecules to survive in frozen-over estuaries during the winter months. In other animals, such as the spring peeper frog (\"Pseudacris crucifer\"), the molality is increased temporarily as a reaction to cold temperatures. In the case of the peeper frog, freezing temperatures trigger a large-scale breakdown of glycogen in the frog's liver and subsequent release of massive amounts of glucose into the blood.\n\nWith the formula below, freezing-point depression can be used to measure the degree of dissociation or the molar mass of the solute. This kind of measurement is called cryoscopy (Greek \"cryo\" = cold, \"scopos\" = observe; \"observe the cold\") and relies on exact measurement of the freezing point. The degree of dissociation is measured by determining the van 't Hoff factor \"i\" by first determining \"m\" and then comparing it to \"m\". In this case, the molar mass of the solute must be known. The molar mass of a solute is determined by comparing \"m\" with the amount of solute dissolved. In this case, \"i\" must be known, and the procedure is primarily useful for organic compounds using a nonpolar solvent. Cryoscopy is no longer as common a measurement method as it once was, but it was included in textbooks at the turn of the 20th century. As an example, it was still taught as a useful analytic procedure in Cohen's \"Practical Organic Chemistry \" of 1910, in which the molar mass of naphthalene is determined using a \"Beckmann freezing apparatus\".\n\nFreezing-point depression can also be used as a purity analysis tool when analysed by differential scanning calorimetry. The results obtained are in mol%, but the method has its place, where other methods of analysis fail.\n\nThis is also the same principle acting in the melting-point depression observed when the melting point of an impure solid mixture is measured with a melting-point apparatus, since melting and freezing points both refer to the liquid–solid phase transition (albeit in different directions).\n\nIn principle, the boiling-point elevation and the freezing-point depression could be used interchangeably for this purpose. However, the cryoscopic constant is larger than the ebullioscopic constant, and the freezing point is often easier to measure with precision, which means measurements using the freezing-point depression are more precise.\n\nAlso this phenomenon is applicable in preparing a freezing mixture for use in an ice-cream machine. For this purpose, NaCl or another salt is used to lower the melting point of ice.\n\nLast but not the least, FPD measurements are used in the dairy industry to ensure that milk has not had extra water added. Milk with a FPD of over 0.509 °C is considered to be unadulterated.\n\nConsider the problem in which the solvent freezes to a very nearly pure crystal, regardless of the presence of the nonvolatile solute. This typically occurs simply because the solute molecules do not fit well in the crystal, i.e. substituting a solute for a solvent molecule in the crystal has high enthalpy. In this case, for low solute concentrations, the freezing point depression depends solely on the concentration of solute particles, not on their individual properties. The freezing point depression thus is called a colligative property.\n\nThe explanation for the freezing point depression is then simply that as solvent molecules leave the liquid and join the solid, they leave behind a smaller volume of liquid in which the solute particles can roam. The resulting reduced entropy of the solute particles thus is independent of their properties. This approximation ceases to hold when the concentration becomes large enough for solute-solute interactions to become important. In that case, the freezing point depression depends on particular properties of the solute other than its concentration.\n\nIf the solution is treated as an ideal solution, the extent of freezing-point depression depends only on the solute concentration that can be estimated by a simple linear relationship with the cryoscopic constant (\"Blagden's Law\"):\nwhere:\n\nThis simple relation doesn't include the nature of the solute, so this is only effective in a diluted solution. For a more accurate calculation at a higher concentration, for ionic solutes, Ge and Wang (2010) proposed a new equation:\n\nIn the above equation, \"T\" is the normal freezing point of the pure solvent (273 K for water, for example); \"a\" is the activity of the solution (water activity for aqueous solution); Δ\"H\" is the enthalpy change of fusion of the pure solvent at \"T\", which is 333.6 J/g for water at 273 K; Δ\"C\" is the differences of heat capacity between the liquid and solid phases at \"T\", which is 2.11 J/(g·K) for water.\n\nThe solvent activity can be calculated from Pitzer model or modified TCPC model, which typically requires 3 adjustable parameters. For the TCPC model, these parameters are available for many single salts.\n\n"}
{"id": "9142889", "url": "https://en.wikipedia.org/wiki?curid=9142889", "title": "Georges Jean Marie Darrieus", "text": "Georges Jean Marie Darrieus\n\nGeorges Jean Marie Darrieus (24 September 1888 – 15 July 1979) was a French aeronautical engineer in the 20th century. He is perhaps most famous for his invention of the Darrieus rotor, a wind turbine capable of operating from any direction and under adverse weather conditions, and the vertical-axis giromill.\n\nThe invention is described in the 1931 US patent 1,835,018\n\n"}
{"id": "10088945", "url": "https://en.wikipedia.org/wiki?curid=10088945", "title": "Henry Fajemirokun", "text": "Henry Fajemirokun\n\nChief Henry Oloyede Fajemirokun, CON (July 26, 1926 - February 15, 1978) was a trade unionist who later became a prominent Nigerian Industrialist and businessman and one of the country's dynamic indigenous entrepreneurs who had established and built one of the foremost indigenous private sector business concerns in his time. He was a strong believer in, and promoted West Africa's economic integration alongside Adebayo Adedeji which subsequently led to the formation of the Economic Community of West African States (ECOWAS). \n\nHe saw the need for a well-organized private sector and was devoted to developing and strengthening the private sector, he expended considerable resources to advance the activities of the Chamber of Commerce and Industry Movement within Nigeria, West Africa and the Commonwealth. \n\nHe held various positions in the Chamber of Commerce and industry Movement. He was the 4th Presidentof the Nigerian Association of Chambers of Commerce, Industry, Mines and Agriculture (NACCIMA), The 6th President of the Lagos Chamber of Commerce and Industry, the 1st President of the Federation of West African Chambers of Commerce (1972–1978) and the co-founder and founding President of the Nigerian-British Chamber of Commerce (NBCC) alongside Sir Adam Thomson, the Chairman of British Caledonian Airways now part of British Airways). He was a member of the Board of Governors of and former President of Nigerian-American Chamber of Commerce (NACC). He was also the Vice-President, Federation of Commonwealth Chambers of Commerce.\n\nHenry was born in Ile Oluji, Ondo State to the family of Daniel Famakinwa Fajemirokun (fondly called 'Dani Ado') and Felicia Adebumi Fajemirokun (née Akinsuroju) (b.1891), daughter of High Chief Odofin Oganbule Akinsuroju and Madam Adesemi Akinsuroju, and granddaughter of Omoba Adebamigbei (whose father was the Jegun of Ile Oluji).\n\nHe was educated at St.Peter's School, Ile-Oluji and then at St Luke's School, Oke-igbo for his primary education. For his secondary education, he educated at CMS Grammar School, Lagos and then at Ondo Boys High School (1942–1944). After Ondo Boys school, he joined the Royal West African Frontier Force at the age of 18 and served in India during World War II. After the war, he joined the Post and Telegraph Department as a clerk and studied privately for his Cambridge School Certificate.\n\nAfter serving during World War II, he began his career at the Post and Telegraph Department. He was a member of the workers' union in the department and rose to become President of the department's Ex-Servicemen's Union in 1948. In 1952, he became President of the Post and Telegraph's Clerical and Workers Allied Union. \n\nHe started private business in 1955 from a loan he received from a maternal relative. He entered the agricultural export sector, exporting cattle bones first then hides and skin, cocoa, rubber, coffee and shea nuts and imported primarily cement. He earned the trust of buyers who extended letters of credit to finance his business. He added other ventures into his business activities, in 1962, he started a maritime services firm. By 1960 he had become one of the largest importers of cement from Egypt and Poland. He received funding for this particular venture from a credit facility that had been provided to him by a British bank in London. \n\nAs the founder, Chairman and largest shareholder of the Henry Stephens Group of companies, under his direction the company grew rapidly and diversified into several spheres such as cement and building materials, engineering, banking, insurance, shipping, ship broking and oil prospecting. He was a leading voice in industry and championed greater participation by Nigerians in the economy which was still dominated by foreign concerns primarily in industry.Following a decision to in 1972, an indigenization decree was promulgated by the Federal government to effect greater indigenous participation in different sectors of the economy. Although a strong proponent of greater indigenous participation, Fajemirokun was also cognizant of the need for continued foreign investment in the country and was vocal on maintaining a balance of both. Barring his failed bid to purchase a significant stake in the Daily Times. Fajemirokun did not participate in taking over any foreign concerns as a result of the indigenization decrees in 1972 and 1977. Before 1972, he had already pioneered industries and expanded his business interests and was able to form partnerships and joint ventures with foreign firms. Fajemirokun's hard work and masterful timing served him well and enabled him to expand organically into other industries. For more than a decade the Henry Stephens Group had managed to successfully build up a solid network trading with firms in foreign countries such as the United Kingdom, United States of America, Poland, Japan, Belgium, Egypt, Germany and Brazil; and had built up strong relationships and business partnerships, representing or acquiring stakes in foreign owned firms in Nigeria prior to the euphoria of the contemporary nationalism at the time in form of the indigenization decrees by the government.\n\nThe Group was well positioned to expand into other industries and by 1977 was in the process of establishing hotels, truck assembly plants, breweries and other manufacturing concerns including the acquisition and development of mining quarries for cement.\n\nGroups such as his served to counter the competition from the Federal Military Government and foreign firms. The crowding out of the organized private sector by the FMG was vocally criticized because the belief was that the organized private sector was better suited to both manage the relationships with the foreign firms ensuring a longer term commitment to the country rather than triggering an exodus and to negotiate a fair price for the value of the equity being sold down rather than capitulating to the demands of the FMG the time.\n\nHe led the negotiations to establish the first private merchant bank in the country Nigerian-American Merchant Bank Limited (NAMBL) bringing in First Bank of Boston (later Bank of Boston) alongside other indigenous investors. The bank was setup after his sudden death with the technical assistance and shareholding of Bank of Boston and Boston Overseas Financial Corporation (subsequently merged with Bank of America).\n\nHe was a pioneer in the Nigerian maritime industry and was the first indigenous company to own an interest in a shipping line. He established Henry Stephens Shipping Company and the Nigeria Shipping Company with two lines: Nigeria-Far East Line and the Nigeria-South America Line and acquired three drybulk ships. The Nigeria- South America Line provided liner services between Brazil and the Nigerian Far East Line provided liner services to Japan and the Far East.\n\nHe also ventured into commodity brokerage and in 1969 he bought and held a seat on the London Stock Exchange. \n\nHis company Henry Stephens was a pioneer in Nigerian Oil industry as the first indigenous private oil company to get an offshore exploration license in the country's first ever offshore licensing round of oil prospecting licenses (OPL) in 1970. Bidding was opened between 26 February and 18 March on about 7000 square miles of the continental shelf. Twenty seven blocks were offered composed of areas relinquished according to leasing regulations 30 November 1968 plus additional deeper water acreage up to the 200 fathom line on the outer shelf. A total of 106 bids from over 30 oil companies or groups were made for these continental shelf OPLs. In July, Newcomers to Nigeria were offered blocks. Only 15 blocks were offered covering only half of the open area. The remaining blocks said to be the most prospective, were reserved for the proposed national oil company Nigerian National Oil Corporation (NNOC) which was established officially in 1972 .Other winners of the licensing round were Deminex a German Consortium, Occidental Petroleum, Japan Petroleum Company and Monsanto.\n\nThe Group acquired the sole distributorship for Rank Xerox (now Xerox), forming a joint venture with the Anglo-American company Rank Xerox (itself a joint venture between Rank Organisation of the United Kingdom and the Xerox Corporation of the United States of America). \n\nHe was a co-founder and played a key role in developing and promoting ECOWAS Bankwhat was to become Ecobank Transnational (ETI), however he passed away before its launch was finalized .\n\nAs an active investor he became a major shareholder in leading companies in Nigeria and abroad. He was denied the opportunity to acquire the \"Daily Times of Nigeria\" from the British International Publishing Corporation, (IPC) known as the \"Daily Mirror Group\" who owned the title by the Obasanjo junta. Henry Stephens also was a core investor in Johnson Wax and acquired stakes Fan Milk and two notable construction companies. \n\nHe was also a director of First Bank Nigeria Limited, director Nigeria Krafts Bags Limited, non-executive director Nigerian Diversified Investments, director Nigeria Sewing Machine Manufacturing Company Limited. He was Chairman of the Lagos State Tourism Advisory Committee.\n\nHe was an active philanthropist in the community and church, giving to charitable causes both in Nigeria and abroad, particularly South Africa. During the Nigerian civil war he was active in supporting the Red Cross and led other initiatives to help those in need. He made several donations to educational and community institutions.. He gave and donated generously to individuals, communities, the Red Cross Movement, Religious Organizations, Universities, State Development Funds, Students’ Clubs, Youth Organizations and the Southern African Relief Fund to which he personally donated ₦110,000 (One Hundred and Ten Thousand Naira) in January 1977 .\n\nHis contributions were recognized and in 1968, Fajemirokun was awarded the Chieftaincy title of Yegbata of Ile-Oluji by his traditional sovereign, the Jegun, thus making him a tribal aristocrat of the Yoruba people and the Asiwaju of Oke-Igbo (1971), Lijoka of Ondos (1973), High Chief Orunta of Ifewara (1974) and the Obaloro of Ado Ekiti (1977). In 1972, The University of Ife (now Obafemi Awolowo University, Ife) also conferred on him the Honorary Degree of Doctor of Science in Business Administration in recognition of his significant contributions to the academic and business communities. \n\nFor his contributions to the development of Commerce and Industry in the country, five years after his death in 1983 the Federal Government honoured him with a posthumous national award of the Commander of the Order of the Niger (CON).\n\nOn the 15th of February 1978, whilst leading a trade mission he died in Abidjan, Cote D’Ivoire. \n\n"}
{"id": "4538124", "url": "https://en.wikipedia.org/wiki?curid=4538124", "title": "Integrated gasification combined cycle", "text": "Integrated gasification combined cycle\n\nAn integrated gasification combined cycle (IGCC) is a technology that uses a high pressure gasifier to turn coal and other carbon based fuels into pressurized gas—synthesis gas (syngas). It can then remove impurities from the syngas prior to the power generation cycle. Some of these pollutants, such as sulfur, can be turned into re-usable byproducts through the Claus process. This results in lower emissions of sulfur dioxide, particulates, mercury, and in some cases carbon dioxide. With additional process equipment, a water-gas shift reaction can increase gasification efficiency and reduce carbon monoxide emissions by converting it to carbon dioxide. The resulting carbon dioxide from the shift reaction can be separated, compressed, and stored through sequestration. Excess heat from the primary combustion and syngas fired generation is then passed to a steam cycle, similar to a combined cycle gas turbine. This process results in improved thermodynamic efficiency compared to conventional pulverized coal combustion.\n\nCoal can be found in abundance in the USA and many other countries and its price has remained relatively constant in recent years. Of the traditional fossil fuels - oil, coal, and natural gas - coal is used as a feedstock for 40% of global electricity generation. Fossil fuel consumption and its contribution to large-scale, detrimental environmental changes is becoming a pressing issue, especially in light of the Paris Agreement. In particular, coal contains more CO per BTU than oil or natural gas and is responsible for 43% of CO emissions from fuel combustion. Thus, the lower emissions that IGCC technology allows through gasification and pre-combustion carbon capture is discussed as a way to addressing aforementioned concerns.\n\nBelow is a schematic flow diagram of an IGCC plant:\nThe gasification process can produce syngas from a wide variety of carbon-containing feedstocks, such as high-sulfur coal, heavy petroleum residues, and biomass.\n\nThe plant is called \"integrated\" because (1) the syngas produced in the gasification section is used as fuel for the gas turbine in the combined cycle and (2) the steam produced by the syngas coolers in the gasification section is used by the steam turbine in the combined cycle.\nIn this example the syngas produced is used as fuel in a gas turbine which produces electrical power. In a normal combined cycle, so-called \"waste heat\" from the gas turbine exhaust is used in a Heat Recovery Steam Generator (HRSG) to make steam for the steam turbine cycle. An IGCC plant improves the overall process efficiency by adding the higher-temperature steam produced by the gasification process to the steam turbine cycle. This steam is then used in steam turbines to produce additional electrical power.\n\nIGCC plants are advantageous in comparison to conventional coal power plants due to their high thermal efficiency, low non-carbon greenhouse gas emissions, and capability to process low grade coal. The disadvantages include higher capital and maintenance costs, and the amount of CO2 released without pre-combustion capture.\n\n\nA major drawback of using coal as a fuel source is the emission of carbon dioxide and other pollutants, including sulfur dioxide, nitrogen oxide, mercury, and particulates. Almost all coal-fired power plants use pulverized coal combustion, which grinds the coal to increase the surface area, burns it to make steam, and runs the steam through a turbine to generate electricity. Pulverized coal plants can only capture carbon dioxide after combustion when it is diluted and harder to separate. In comparison, gasification in IGCC allows for separation and capture of the concentrated and pressurized carbon dioxide before combustion. Syngas cleanup includes filters to remove bulk particulates, scrubbing to remove fine particulates, and solid adsorbents for mercury removal. Additionally, hydrogen gas is used as fuel, which produces no pollutants under combustion.\n\nIGCC also consumes less water than traditional pulverized coal plants. In a pulverized coal plant, coal is burned to produce steam, which is then used to create electricity using a steam turbine. Then steam exhaust must then be condensed with cooling water, and water is lost by evaporation. In IGCC, water consumption is reduced by combustion in a gas turbine, which uses the generated heat to expand air and drive the turbine. Steam is only used to capture the heat from the combustion turbine exhaust for use in a secondary steam turbine. Currently, the major drawback is the high capital cost compared to other forms of power production.\n\nThe DOE Clean Coal Demonstration Project helped construct 3 IGCC plants: Edwarsport Power Station in Edwardsport, Indiana, Polk Power Station in Tampa, Florida (online 1996), and Pinon Pine in Reno, Nevada. In the Reno demonstration project, researchers found that then-current IGCC technology would not work more than 300 feet (100m) above sea level. The DOE report in reference 3 however makes no mention of any altitude effect, and most of the problems were associated with the solid waste extraction system. The Wabash River and Polk Power stations are currently operating, following resolution of demonstration start-up problems, but the Piñon Pine project encountered significant problems and was abandoned.\n\nThe US DOE's Clean Coal Power Initiative (CCPI Phase 2) selected the Kemper Project as one of two projects to demonstrate the feasibility of low emission coal-fired power plants. Mississippi Power began construction on the Kemper Project in Kemper County, Mississippi, in 2010 and is poised to begin operation in 2016, though there have been many delays. In March, the projected date was further pushed back from early 2016 to August 31, 2016, adding $110 million to the total and putting the project 3 years behind schedule. The electrical plant is a flagship Carbon Capture and Storage (CCS) project that burns lignite coal and utilizes pre-combustion IGCC technology with a projected 65% emission capture rate.\n\nThe first generation of IGCC plants polluted less than contemporary coal-based technology, but also polluted water; for example, the Wabash River Plant was out of compliance with its water permit during 1998–2001\nbecause it emitted arsenic, selenium and cyanide. The Wabash River Generating Station is now wholly owned and operated by the Wabash River Power Association.\n\nIGCC is now touted as \"capture ready\" and could potentially be used to capture and store carbon dioxide. (See FutureGen)Poland's Kędzierzyn will soon host a Zero-Emission Power & Chemical Plant that combines coal gasification technology with Carbon Capture & Storage (CCS). This installation had been planned, but there has been no information about it since 2009. Other operating IGCC plants in existence around the world are the Alexander (formerly Buggenum) in the Netherlands, Puertollano in Spain, and JGC in Japan.\n\nThe Texas Clean Energy project plans to build a 400 MW IGCC facility that will incorporate carbon capture, utilization and storage (CCUS) technology. The project will be the first coal power plant in the United States to combine IGCC and 90% carbon capture and storage. Commercial operation is due to start in 2018.\n\nThere are several advantages and disadvantages when compared to conventional post combustion carbon capture and various variations \nA key issue in implementing IGCC is its high capital cost, which prevents it from competing with other power plant technologies. Currently, ordinary pulverized coal plants are the lowest cost power plant option. The advantage of IGCC comes from the ease of retrofitting existing power plants that could offset the high capital cost. In a 2007 model, IGCC with CCS is the lowest-cost system in all cases. This model compared estimations of levelized cost of electricity, showing IGCC with CCS to cost 71.9 $US2005/MWh, pulverized coal with CCS to cost 88 $US2005/MWh, and natural gas combined cycle with CCS to cost 80.6 $US2005/MWh. The levelized cost of electricity was noticeably sensitive to the price of natural gas and the inclusion of carbon storage and transport costs.\n\nThe potential benefit of retrofitting has so far, not offset the cost of IGCC with carbon capture technology. A 2013 report by the U.S. Energy Information Administration demonstrates that the overnight cost of IGCC with CCS has increased 19% since 2010. Amongst the three power plant types, pulverized coal with CCS has an overnight capital cost of $5,227 (2012 dollars)/kW, IGCC with CCS has an overnight capital cost of $6,599 (2012 dollars)/kW, and natural gas combined cycle with CCS has an overnight capital cost of $2,095 (2012 dollars)/kW. Pulverized coal and NGCC costs did not change significantly since 2010. The report further relates that the 19% increase in IGCC cost is due to recent information from IGCC projects that have gone over budget and cost more than expected.\n\nRecent testimony in regulatory proceedings show the cost of IGCC to be twice that predicted by Goddell, from $96 to 104/MWhr. That's before addition of carbon capture and sequestration (sequestration has been a mature technology at both Weyburn in Canada (for enhanced oil recovery) and Sleipner in the North Sea at a commercial scale for the past ten years)—capture at a 90% rate is expected to have a $30/MWh additional cost.\n\nWabash River was down repeatedly for long stretches due to gasifier problems. The gasifier problems have not been remedied—subsequent projects, such as Excelsior's Mesaba Project, have a third gasifier and train built in. However, the past year has seen Wabash River running reliably, with availability comparable to or better than other technologies.\n\nThe Polk County IGCC has design problems. First, the project was initially shut down because of corrosion in the slurry pipeline that fed slurried coal from the rail cars into the gasifier. A new coating for the pipe was developed. Second, the thermocoupler was replaced in less than two years; an indication that the gasifier had problems with a variety of feedstocks; from bituminous to sub-bituminous coal. The gasifier was designed to also handle lower rank lignites. Third, unplanned down time on the gasifier because of refractory liner problems, and those problems were expensive to repair. The gasifier was originally designed in Italy to be half the size of what was built at Polk. Newer ceramic materials may assist in improving gasifier performance and longevity. Understanding the operating problems of the current IGCC plant is necessary to improve the design for the IGCC plant of the future. (Polk IGCC Power Plant, http://www.clean-energy.us/projects/polk_florida.html.) Keim, K., 2009, IGCC A Project on Sustainability Management Systems for Plant Re-Design and Re-Image. This is an unpublished paper from Harvard University)\n\nGeneral Electric is currently designing an IGCC model plant that should introduce greater reliability. GE's model features advanced turbines optimized for the coal syngas. Eastman's industrial gasification plant in Kingsport, TN uses a GE Energy solid-fed gasifier. Eastman, a fortune 500 company, built the facility in 1983 without any state or federal subsidies and turns a profit.\n\nThere are several refinery-based IGCC plants in Europe that have demonstrated good availability (90-95%) after initial shakedown periods. Several factors help this performance:\n\n\nAnother IGCC success story has been the 250 MW Buggenum plant in The Netherlands, which was commissioned in 1994 and closed in 2013, had good availability. This coal-based IGCC plant was originally designed to use up to 30% biomass as a supplemental feedstock. The owner, NUON, was paid an incentive fee by the government to use the biomass. NUON has constructed a 1,311 MW IGCC plant in the Netherlands, comprising three 437 MW STEG units. The Nuon Magnum IGCC power plant was commissioned in 2011, and was officially opened in June 2013. Mitsubishi Heavy Industries has been awarded to construct the power plant. Following a deal with environmental organizations, NUON has been prohibited from using the Magnum plant to burn coal and biomass, until 2020. Because of high gas prices in the Netherlands, two of the three units are currently offline, whilst the third unit sees only low usage levels. The relatively low 59% efficiency of the Magnum plant means that more efficient CCGT plants (such as the Hemweg 9 plant) are preferred to provide (backup) power.\n\nA new generation of IGCC-based coal-fired power plants has been proposed, although none is yet under construction. Projects are being developed by AEP, Duke Energy, and Southern Company in the US, and in Europe by ZAK/PKE, Centrica (UK), E.ON and RWE (both Germany) and NUON (Netherlands). In Minnesota, the state's Dept. of Commerce analysis found IGCC to have the highest cost, with an emissions profile not significantly better than pulverized coal. In Delaware, the Delmarva and state consultant analysis had essentially the same results.\n\nThe high cost of IGCC is the biggest obstacle to its integration in the power market; however, most energy executives recognize that carbon regulation is coming soon. Bills requiring carbon reduction are being proposed again both the House and the Senate, and with the Democratic majority it seems likely that with the next President there will be a greater push for carbon regulation. The Supreme Court decision requiring the EPA to regulate carbon (Commonwealth of Massachusetts et al. v. Environmental Protection Agency et al.)[20] also speaks to the likelihood of future carbon regulations coming sooner, rather than later. With carbon capture, the cost of electricity from an IGCC plant would increase approximately 33%. For a natural gas CC, the increase is approximately 46%. For a pulverized coal plant, the increase is approximately 57%. This potential for less expensive carbon capture makes IGCC an attractive choice for keeping low cost coal an available fuel source in a carbon constrained world. However, the industry needs a lot more experience to reduce the risk premium. IGCC with CCS requires some sort of mandate, higher carbon market price, or regulatory framework to properly incentivize the industry.\n\nIn Japan, electric power companies, in conjunction with Mitsubishi Heavy Industries has been operating a 200 t/d IGCC pilot plant since the early '90s. In September 2007, they started up a 250 MW demo plant in Nakoso. It runs on air-blown (not oxygen) dry feed coal only. It burns PRB coal with an unburned carbon content ratio of <0.1% and no detected leaching of trace elements. It employs not only \"F\" type turbines but \"G\" type as well. (see gasification.org link below)\n\nNext generation IGCC plants with CO capture technology will be expected to have higher thermal efficiency and to hold the cost down because of simplified systems compared to conventional IGCC. The main feature is that instead of using oxygen and nitrogen to gasify coal, they use oxygen and CO. The main advantage is that it is possible to improve the performance of cold gas efficiency and to reduce the unburned carbon (char).\n\nAs a reference for powerplant efficiency:\n\nThe CO extracted from gas turbine exhaust gas is utilized in this system. Using a closed gas turbine system capable of capturing the CO by direct compression and liquefication obviates the need for a separation and capture system.\n\nPre-combustion CO removal is much easier than CO removal from flue gas in post-combustion capture due to the high concentration of CO after the water-gas-shift reaction and the high pressure of the syngas. During pre-combustion in IGCC, the partial pressure of CO is nearly 1000 times higher than in post-combustion flue gas. Due to the high concentration of CO pre-combustion, physical solvents, such as Selexol and Rectisol, are preferred for the removal of CO vs that of chemical solvents. Physical solvents work by absorbing the acid gases without the need of a chemical reaction as in traditional amine based solvents. The solvent can then be regenerated, and the CO desorbed, by reducing the pressure. The biggest obstacle with physical solvents is the need for the syngas to be cooled before separation and reheated afterwards for combustion. This requires energy and decreases overall plant efficiency.\n\nNational and international test codes are used to standardize the procedures and definitions used to test IGCC Power Plants. Selection of the test code to be used is an agreement between the purchaser and the manufacturer, and has some significance to the design of the plant and associated systems. In the United States, The American Society of Mechanical Engineers published the Performance Test Code for IGCC Power Generation Plants (PTC 47) in 2006 which provides procedures for the determination of quantity and quality of fuel gas by its flow rate, temperature, pressure, composition, heating value, and its content of contaminants.\n\nIn 2007, the New York State Attorney General's office demanded full disclosure of \"financial risks from greenhouse gases\" to the shareholders of electric power companies proposing the development of IGCC coal-fired power plants. \"Any one of the several new or likely regulatory initiatives for CO emissions from power plants - including state carbon controls, EPA's regulations under the Clean Air Act, or the enactment of federal global warming legislation - would add a significant cost to carbon-intensive coal generation\"; U.S. Senator Hillary Clinton from New York has proposed that this full risk disclosure be required of all publicly traded power companies nationwide. This honest disclosure has begun to reduce investor interest in all types of existing-technology coal-fired power plant development, including IGCC.\n\nSenator Harry Reid (Majority Leader of the 2007/2008 U.S. Senate) told the 2007 Clean Energy Summit that he will do everything he can to stop construction of proposed new IGCC coal-fired electric power plants in Nevada. Reid wants Nevada utility companies to invest in solar energy, wind energy and geothermal energy instead of coal technologies. Reid stated that global warming is a reality, and just one proposed coal-fired plant would contribute to it by burning seven million tons of coal a year. The long-term healthcare costs would be far too high, he claimed (no source attributed). \"I'm going to do everything I can to stop these plants.\", he said. \"There is no clean coal technology. There is cleaner coal technology, but there is no clean coal technology.\"\n\nOne of the most efficient ways to treat the HS gas from an IGCC plant is by converting it into sulphuric acid in a wet gas sulphuric acid process WSA process. However, the majority of the HS treating plants utilize the modified Claus process, as the sulphur market infrastructure and the transportation costs of sulphuric acid versus sulphur are in favour of sulphur production.\n\n\n"}
{"id": "25350481", "url": "https://en.wikipedia.org/wiki?curid=25350481", "title": "Kita-Iwaki powerline", "text": "Kita-Iwaki powerline\n\nThe Kita-Iwaki Powerline is the largest double-circuit powerline for three-phase electric power in the world. Built in 1999, it runs from Minami-Iwaki switch (Tamura, Fukushima) to Higashi-Yamanashi substation (Ōtsuki, Yamanashi) and has 2 circuits, which are operated at present with 500 kV, but can be switched over to 1100 kV if necessary equipment is installed. The conductors of the lines consist of 8*31.5 mm ACSR ropes providing for a total current capacity of 4000 Amperes.\nThe line is supported by lattice towers with a typical height of 108 meters. These have three crossbars of spanning 31, 32 and 33 meters.\n\nThere are two such lines: the first one is 190 kilometers long and starts at Kashiwazaki-Kariwa Nuclear Power Plant and runs over Nishi-Gunma switch to Higashi-Yamanashi substation. It was built in 1993.\nThe second 240 kilometers long line, which was built in 1999 starts at Nishi-Gunma substation and runs over Higashi-Gunma substation to Minami-Iwaki switch, whereby it passes close to Shin-Imaichi switch, which is not connected to the line.\n\n"}
{"id": "743458", "url": "https://en.wikipedia.org/wiki?curid=743458", "title": "Lightning arrester", "text": "Lightning arrester\n\nA lightning arrester (alternative spelling lightning arrestor) (also called lightning diverter) is a device used on electric power systems and telecommunication systems to protect the insulation and conductors of the system from the damaging effects of lightning. The typical lightning arrester has a high-voltage terminal and a ground terminal. When a lightning surge (or switching surge, which is very similar) travels along the power line to the arrester, the current from the surge is diverted through the arrester, in most cases to earth.\n\nIn telegraphy and telephony, a lightning arrester is placed where wires enter a structure, preventing damage to electronic instruments within and ensuring the safety of individuals near them. Smaller versions of lightning arresters, also called surge protectors, are devices that are connected between each electrical conductor in power and communications systems and the Earth. These prevent the flow of the normal power or signal currents to ground, but provide a path over which high-voltage lightning current flows, bypassing the connected equipment. Their purpose is to limit the rise in voltage when a communications or power line is struck by lightning or is near to a lightning strike.\n\nIf protection fails or is absent, lightning that strikes the electrical system introduces thousands of kilovolts that may damage the transmission lines, and can also cause severe damage to transformers and other electrical or electronic devices. Lightning-produced extreme voltage spikes in incoming power lines can damage electrical home appliances or even produce death.\n\nLightning arresters are used to protect electric fences. They consist of a spark gap and sometimes a series inductor.\n\nLightning arresters can form part of large electrical transformers and can fragment during transformer ruptures. High-voltage transformer fire barriers are required to defeat ballistics from small arms as well as projectiles from transformer bushings and lightning arresters, per NFPA 850.\n\nA potential target for a lightning strike, such as an outdoor television antenna, is attached to the terminal labeled A in the photograph. Terminal E is attached to a long rod buried in the ground. Ordinarily no current will flow between the antenna and the ground because there is extremely high resistance between B and C, and also between C and D. The voltage of a lightning strike, however, is many times higher than that needed to move electrons through the two air gaps. The result is that electrons go through the lightning arresters rather than traveling on to the television set and destroying it.\n\nA lightning arrester may be a spark gap or may have a block of a semiconducting material such as silicon carbide or zinc oxide. \"Thyrite\" was once a trade name for the silicon carbide used in arresters. Some spark gaps are open to the air, but most modern varieties are filled with a precision gas mixture, and have a small amount of radioactive material to encourage the gas to ionize when the voltage across the gap reaches a specified level. Other designs of lightning arresters use a glow-discharge tube (essentially like a neon glow lamp) connected between the protected conductor and ground, or voltage-activated solid-state switches called varistors or MOVs.\n\nLightning arresters used in power substations are large devices, consisting of a porcelain tube several feet long and several inches in diameter, typically filled with discs of zinc oxide. A safety port on the side of the device vents the occasional internal explosion without shattering the porcelain cylinder.\n\nLightning arresters are rated by the peak current they can withstand, the amount of energy they can absorb, and the breakover voltage that they require to begin conduction. They are applied as part of a lightning protection system, in combination with air terminals and bonding.\n\n\n"}
{"id": "57850264", "url": "https://en.wikipedia.org/wiki?curid=57850264", "title": "List of Menhirs", "text": "List of Menhirs\n\nThe following is an incomplete list of Menhirs\n\nIn Scandinavia, menhirs are called bautasteiner or bautastenar and continued to be erected during the Pre-Roman Iron Age and later, usually over the ashes of the dead. They were raised both as solitary stones and in formations, such as the stone ships and few stone circles. Sometimes, they were raised only as commemoration to great people, a tradition which was continued as the runestones.\n\nFrostating, with its seat at Tinghaugen in Frosta municipality in the county of Nord-Trøndelag, was the site of an early Norwegian court. The site is represented by the Frostatinget bautasten.\n\nThe tradition was strongest in Bornholm, Gotland and Götaland and appears to have followed the Goths, during the 1st century, to the southern shore of the Baltic Sea, (now Northern Poland) where they are a characteristic of the Wielbark culture.\n\nOverall 1,176 menhirs are registered in Norway. The stones are often included as part of a tomb construction. The introduction to \"Snorre Sagas\" points out that it was the custom to \"burn all dead and raise monoliths for them\" and that this custom was maintained in Norway and Sweden for a long time. As a rule, each grave was marked with a single stone, but there were also instances where several stones were used, including the burial facility De fem dårlige jomfruer at Karmsundet in Rogaland, with five raised stones. It is especially prevalent in Østlandet to find several monoliths arranged in a circle. \nSometimes standing stone monuments are unrelated to known graves. It may be that they served as boundary markers. These include several stones in Fana, Bergen that can be linked to an important historical boundary between Sunnhordland and Nordhordland, as it was in medieval times.\n\nIn Norway, standing stones usually dated to the Migration Period, the Viking Age or early Middle Ages.\n\nIn Sweden by the 13th century menhirs were erected as markers for the graves of warriors. The following lines are taken from the introduction of the Heimskringla by Snorri Sturluson;\n\nIn the same work, Snorri wrote that the Swedes burnt their dead king Vanlade and raised a stone over his ashes by the River Skyt (one of the tributaries of the River Fyris):\n\nThe tradition is also mentioned in Hávamál.\n\nIn the French-speaking canton of Vaud in Switzerland, several menhirs form linear patterns in Yverdon-les-Bains. These are situated in Clendy and date back to the third millennium BC.\n\nIn Welsh, menhirs are called 'Maen Hîr' and they are scattered throughout Wales. \n\nIreland is rich in menhirs, standing stones which are usually located in farmer's fields and are heavily worn due to poor weather conditions and exposure to livestock.\n\nA number of menhirs exist in the Czech Republic. These standing stones include the Menhir Kamenný pastýř (stony shepherd) near Klobuky.\n\nBrittany stands out in the distribution of menhirs by virtue of both the density of monuments and the diversity of types. The largest surviving menhir in the world is located in Locmariaquer, Brittany, and it is known as the \"Grand Menhir Brisé\" (\"Great Broken Menhir\"). Once nearly high, today, it lies fractured into four pieces, but it would have weighed near 330 tons when intact. It is placed third after the Thunder Stone in St. Petersburg and the Western Stone in the Western Wall as the heaviest object moved by humans without powered machinery.\n\nA 4.5 meter menhir can be seen on the side of Le Mans Cathedral. It was moved there in 1778 when the dolmen it was associated with was destroyed.\n\nAlignments of menhirs are common, the most famous being the Carnac stones in Brittany, where more than 3000 individual menhirs are arranged in four groups and arrayed in rows stretching across four kilometres. Each set is organised with the tallest stones at the western end and shorter ones at the eastern end. Some end with a semicircular cromlech, but many have since fallen or been destroyed.\n\nThe second largest concentration of menhirs in France is at the \"Cham des Bondons\", which is located on high open limestone plain in the granitic Cévennes. Today, the site is protected by the Parc National des Cévennes. From the time pastoralism was established, the site was kept open by controlled burning and grazing.\n\nOn the island of Corsica, menhirs are found in Filitosa, a megalithic site in southern Corsica. The period of occupation spans from the end of the Neolithic era and the beginning of the Bronze Age, until around the Roman times in Corsica.\n\nMenhirs are especially common in Sardinia. It is possible to see at least 332 such standing stones on the island, including especially elaborate \"statue-menhirs\" that show a human face at the top and other gendered symbols on the flat front sides. Over a hundred examples of this standardized type have been found, most of them around the village of Laconi.\nIn the Sardinian language they are known as \"perdas fittas\" or \"perdas lungas\".\n\nIn Portugal, there are also found several ancient menhirs. Among these megalithic structures is the great Almendres Menhir within the Almendres Cromlech complex near Évora.\n\nIn Spain, menhirs associated with the western European megalithic industry are relatively unusual compared to dolmens, but still are common sights in the northern half of the country, where at least 500 menhirs have been reported. They are particularly common in the Basque Country, Navarre, northern Burgos and Palencia, Cantabria, and the Pyrenees, where they are usually encountered standing alone or in small groups (cromlech) in elevated locations; the Arlobi menhir is one of the most recent examples of a menhir. Most of the menhirs in northern Spain appear to date back to the stone age; they are not usually associated with burials, but in at least one instance (the Menhir of Cuesta del Molino in Burgos) burials dating at least 2000 years after the menhir was originally built have been found.\n\nIn mediterranean Spain and, particularly the Balearic islands, megalithic structures consisting of standing stones such as the Taulas, but associated with Bronze age and Iron age cultures, are also common.\n\nThe graves of the \"Latins\" and the \"Jidovs\" near the village of Balwan (Bovan), north of Aleksinac in Serbia were marked by large boulders.\n\nNumerous menhirs dot the lands across Armenia, where they are called \"vishapakar\" (). \"Vishap\" translates to \"dragon\" or \"serpent\" and \"kar\" translates to \"stone\". The stones are cigar-shaped, and are typically tall. They are often found in the mountains near the sources of rivers or lakes. A large number of them have been carved in the shape of a fish. The earliest known vishapakar is thought to date from between the 18th to 16th centuries BC. An inscription in ancient Urartian cuneiform written upon a vishap at the temple of Garni shows that they were created prior to the Urartian Kingdom (pre-8th century).\n\nThe Hebrew term for \"standing stone\" is \"masseba\", pl. \"massebot\" (also written \"matseva\", \"matsevot\"). The most famous examples are from the Canaanite High Place at Tel Gezer, comprising a straight row of ten stone stelae and a square stone basin, all erected simultaneously during the Middle Bronze Age.\nMenhirs in Iran are found in different villages and areas of East Azarbaijan Province, meshkin shahr(pirazmian) and Amlash and Deylaman areas in Gilan. A double menhir is also situated on Kharg Island in the Persian Gulf.\n\nMenhirs are called \"Sang-Afrāsht\" (سنگ‌افراشت) in Persian, and there are different studies published in Iranian periodicals about the details of the Iranian menhirs, specially in the periodical \"Barrasiha-yi Tarikhi\" (Historical studies).\nMenhirs are found all across India. They can be as tall as 20 to 14 feet (over 4.2 m), and several hundred smaller menhirs scattered all over the agricultural fields, mountains and various geographical areas. Rao and his team visited the menhir site in Telangana on the days of summer and winter solstice and equinox and found that particular rows of stones were aligned to the rising and setting aun on these days. \"This suggests the megalithic community here was aware of the solar trajectories,\" he said.\n\nAncient standing stones are found throughout the Horn of Africa. Several of these old menhirs exist in Qohaito, Eritrea, and date to a period before the founding of the Kingdom of Axum. The Axumites themselves also erected a number of large stelae, which served a religious purpose in pre-Christian times. One of these granite columns is the largest such structure in the world, standing at 90 feet.\n\nIn northeastern Somalia, on the coastal plain 20 km to Alula's east are found ruins of an ancient monument in a platform style. The structure is formed by a rectangular drystone wall that is low in height; the space in between is filled with rubble and manually covered with small stones. Relatively large standing stones are also positioned on the edifice's corners. Near the platform are graves, which are outlined in stones. 24 m by 17 m in dimension, the structure is the largest of a string of ancient platform and enclosed platform monuments exclusive to far northeastern Somalia. Additionally, around 200 stone monuments (\"taalos\") are found in the northeastern Botiala site, most of which consist of cairns. There are a number of rows of standing stones on the eastern side of the structures, which are similar to those at Salweyn, a great cairn-held situated close to Heis. Besides cairns, the Botiala area also features a few other drystone monuments. These include disc monuments with circular, ground-level features, as well as low, rectangular platform monuments. Burial sites near Burao in the northwestern part of the country likewise feature a number of old stelae.\n\nAdditionally, between Djibouti City and Loyada in Djibouti are a number of anthropomorphic and phallic stelae. The structures are associated with graves of rectangular shape flanked by vertical slabs, as also found in central Ethiopia. The Djibouti-Loyada stelae are of uncertain age, and some of them are adorned with a T-shaped emblem.\n\nIn Ethiopia, the town of Tiya contains 36 menhirs (standing stones) or stelae. Of these, 32 are engraved with swords and other mysterious symbols. The ancient structures suggest the presence of a large, prehistoric burial complex. The archaeological site was designated a World Heritage Site in 1980.\n\nMenhirs were erected by the U'wa people of Colombia in their ancestral territory. They believe that the menhirs are the ancients of the U'wa clans who were turned into the stone piers of the world. Menhirs can be found in Chita and Chiscas, Boyacá.\n\nThere are 114 menhirs in the Provincial Park Los Menhires in Argentina. They were erected by the Tafí people, an indigenous culture of Tucumán province, and were used in fertility rites.\n\n\n"}
{"id": "30123340", "url": "https://en.wikipedia.org/wiki?curid=30123340", "title": "List of off-season Pacific hurricanes", "text": "List of off-season Pacific hurricanes\n\nThere have been 24 recorded tropical and subtropical cyclones in the eastern Pacific basin outside the official Pacific hurricane season. The National Hurricane Center (NHC) monitors the area from North America westward to 140°W, while the Central Pacific Hurricane Center is from 140°W to the International date line, north of the equator. The National Oceanic and Atmospheric Administration (NOAA) currently defines the season as starting May 15 in the eastern Pacific and June 1 for the central Pacific and ending on November 30 for both regions in each calendar year. Occasionally, however, storms develop in late November and persist until December, such as Hurricane Nina of 1957.\n\nFew off-season tropical cyclones in the east Pacific have affected land, and none of them have made landfall. Only Hurricane Nina caused both property damage and fatalities. The strongest hurricane between December and May was Hurricane Ekeka in 1992, which reached winds of . However, after Tropical Storm Paka crossed the International Date Line, it intensified into a typhoon with winds equivalent to a Category 5 hurricane on the Saffir-Simpson hurricane wind scale. The most recent off-season storm is Tropical Depression One-E in early-May 2018.\n\nThe beginning of HURDAT, the official Pacific hurricane database maintained by the NHC, is 1949. Since then, thirteen storms have occurred outside the official bounds of hurricane season in the eastern and central north Pacific, respectively. In addition, the CPHC reports nine off-season storms from 1900–1952 with another off-season tropical cyclone occurring in 1832. There have been documents published in the Monthly Weather Review reporting additional off-season storms within off the Mexican coastline, including one in December.\n\nThe wind speeds listed are maximum one-minute average sustained winds. The category refers to the intensity on the Saffir–Simpson hurricane wind scale; TS stands for tropical storm, and TD for tropical depression.\n\nSeveral off-season tropical cyclones have affected land. Hurricane Nina in early December 1957 prompted evacuations in Hawaii and caused $100,000 (1957 USD) in damage in the state. The storm also killed four people and produced waves. Hurricane Winnie in December 1983 caused minor rainfall in parts of Mexico. The unnamed tropical storm of 1996 was assumed to have killed two people when it sank a trimaran called the \"Solar Wind\". After becoming a typhoon, Paka caused significant damage in the Marshall Islands, Guam, and the Northern Mariana Islands. Overall, Paka caused $580 million (1997 USD) in damage, enough to warrant retirement of the name. None of these impacting systems made landfall.\n\nIn the official east Pacific hurricane database, which dates back to 1949, the first storm to occur outside of the current season was Hurricane Nina in 1957. In the database, thirteen tropical cyclones have existed between December and May, most recently Hurricane Pali in 2016. Tropical Storm Winona in January 1989 was not listed in the database, despite forming south of Hawaii. In addition, there were at least eight tropical cyclones before the start of the official database, many of which existed near Hawaii. Storms were most likely to occur in December, followed by January and May. Only one cyclone each was reported in the two months of March and April. Of all off-season tropical cyclones, the \"Froc Cyclone\" lasted longest, spanning 12 days and two calendar years. The year with the most off-season storms was tied between 1904 and 1992, with a total of two tropical cyclones. No Pacific hurricane season had both a pre-season and post-season storm.\n\n"}
{"id": "5302267", "url": "https://en.wikipedia.org/wiki?curid=5302267", "title": "List of refrigerants", "text": "List of refrigerants\n\nChemical refrigerants are assigned an R number which is determined systematically according to molecular structure. Common refrigerants are frequently referred to as Freon (a registered trademark of DuPont). The following is a list of refrigerants with their Type/Prefix, ASHRAE designated numbers, IUPAC chemical name, molecular formula, CAS registry number / Blend Name, Atmospheric Lifetime in years, Semi-Empirical Ozone depletion potential, net Global warming potential over a 100-year time horizon, Occupational exposure limit/Permissible exposure limit in parts per million (volume per volume) over a time-weighted average (TWA) concentration for a normal eight-hour work day and a 40-hour work week, ASHRAE 34 Safety Group in Toxicity & Flammability (in Air @ 60 °C & 101.3 kPa) classing, Refrigerant Concentration Limit / Immediately Dangerous to Life or Health in parts per million (volume per volume) & grams per cubic meter, Molecular mass in Atomic mass units, Normal Boiling Point (or Bubble & Dew Points for the Zeotrope(400)-series)(or Normal Boiling Point & Azeotropic Temperature for the Azeotrope(500)-series) at 101,325 Pa (1 atmosphere) in degrees Celsius, Critical Temperature in degrees Celsius and Critical Pressure (absolute) in kiloPascals.\n\nCompounds used as refrigerants may be described using either the appropriate prefix above or with the prefixes \"R-\" or \"Refrigerant.\" Thus, CFC-12 may also be written as R-12 or Refrigerant 12.<br>An alkene, olefin, or olefine is an unsaturated chemical compound containing at least one carbon-to-carbon double bond.\n\n"}
{"id": "14641222", "url": "https://en.wikipedia.org/wiki?curid=14641222", "title": "Longitudinal static stability", "text": "Longitudinal static stability\n\nIn flight dynamics, longitudinal static stability is the stability of an aircraft in the longitudinal, or pitching, plane under steady flight conditions. This characteristic is important in determining whether a human pilot will be able to control the aircraft in the pitching plane without requiring excessive attention or excessive strength.\n\nAs any vehicle moves it will be subjected to minor changes in the forces that act on it, and in its speed.\n\nFor a vehicle to possess positive static stability it is not necessary for its speed and orientation to return to exactly the speed and orientation that existed before the minor change that caused the upset. It is sufficient that the speed and orientation do not continue to diverge but undergo at least a small change back towards the original speed and orientation.\n\nThe longitudinal stability of an aircraft refers to the aircraft's stability in the pitching plane - the plane which describes the position of the aircraft's nose in relation to its tail and the horizon. (Other stability modes are directional stability and lateral stability.)\n\nIf an aircraft is longitudinally stable, a small increase in angle of attack will cause the pitching moment on the aircraft to change so that the angle of attack decreases. Similarly, a small decrease in angle of attack will cause the pitching moment to change so that the angle of attack increases.\n\nThe pilot of an aircraft with positive longitudinal stability, whether it is a human pilot or an autopilot, has an easy task to fly the aircraft and maintain the desired pitch attitude which, in turn, makes it easy to control the speed, angle of attack and fuselage angle relative to the horizon. The pilot of an aircraft with negative longitudinal stability has a more difficult task to fly the aircraft. It will be necessary for the pilot devote more effort, make more frequent inputs to the elevator control, and make larger inputs, in an attempt to maintain the desired pitch attitude.\n\nMost successful aircraft have positive longitudinal stability, providing the aircraft's center of gravity lies within the approved range. Some acrobatic and combat aircraft have low-positive or neutral stability to provide high maneuverability. Some advanced aircraft have a form of low-negative stability called relaxed stability to provide extra-high maneuverability.\n\nThe longitudinal static stability of an aircraft is significantly influenced by the distance (moment arm or lever arm) between the centre of gravity (c.g.) and the aerodynamic centre of the airplane. The c.g. is established by the design of the airplane and influenced by its loading, as by payload, passengers, etc. The aerodynamic centre (a.c.) of the airplane can be located approximately by taking the algebraic sum of the plan-view areas fore and aft of the c.g. multiplied by their blended moment arms and divided by their areas, in a manner analogous to the method of locating the c.g. itself. In conventional aircraft, this point is aft of, but close to, the one-quarter-chord point of the wing. In unconventional aircraft, e.g. the Quickie, it is between the two wings because the aft wing is so large. The pitching moment at the a.c. is typically negative and constant.\n\nThe a.c. of an airplane typically does not change with loading or other changes; but the c.g. does, as noted above. If the c.g. moves forward, the airplane becomes more stable (greater moment arm between the a.c. and the c.g.), and if too far forward will cause the airplane to be difficult for the pilot to bring nose-up as for landing. If the c.g. is too far aft, the moment arm between it and the a.c. diminishes, reducing the inherent stability of the airplane and in the extreme going negative and rendering the airplane longitudinally unstable; see the diagram below.\n\nAccordingly, the operating handbook for every airplane specifies the range over which the c.g. is permitted to move. Inside this range, the airplane is considered to be inherently stable, which is to say that it will self-correct longitudinal (pitch) disturbances without pilot input.\n\nNear the cruise condition most of the lift force is generated by the wings, with ideally only a small amount generated by the fuselage and tail. We may analyse the longitudinal static stability by considering the aircraft in equilibrium under wing lift, tail force, and weight. The moment equilibrium condition is called trim, and we are generally interested in the longitudinal stability of the aircraft about this trim condition.\n\nEquating forces in the vertical direction:\nwhere W is the weight, formula_2 is the wing lift and formula_3 is the tail force. \n\nFor a thin airfoil at low angle of attack, the wing lift is proportional to the angle of attack:\n\nwhere formula_5 is the wing area formula_6 is the (wing) lift coefficient, formula_7 is the angle of attack. The term formula_8 is included to account for camber, which results in lift at zero angle of attack. Finally formula_9 is the dynamic pressure:\nwhere formula_11 is the air density and formula_12 is the speed.\n\nThe force from the tailplane is proportional to its angle of attack, including the effects of any elevator deflection and any adjustment the pilot has made to trim-out any stick force. In addition, the tail is located in the flow field of the main wing, and consequently experiences downwash, reducing its angle of attack. \n\nIn a statically stable aircraft of conventional (tail in rear) configuration, the tailplane force may act upward or downward depending on the design and the flight conditions. In a typical canard aircraft both fore and aft planes are lifting surfaces. The fundamental requirement for static stability is that the aft surface must have greater authority (leverage) in restoring a disturbance than the forward surface has in exacerbating it. This leverage is a product of moment arm from the center of mass and surface area. Correctly balanced in this way, the partial derivative of pitching moment with respect to changes in angle of attack will be negative: a momentary pitch up to a larger angle of attack makes the resultant pitching moment tend to pitch the aircraft back down. (Here, pitch is used casually for the angle between the nose and the direction of the airflow; angle of attack.) This is the \"stability derivative\" d(M)/d(alpha), described below. \n\nThe tail force is, therefore:\nwhere formula_14 is the tail area, formula_15 is the tail force coefficient, formula_16 is the elevator deflection, and formula_17 is the downwash angle.\n\nA canard aircraft may have its foreplane rigged at a high angle of incidence, which can be seen in a canard catapult glider from a toy store; the design puts the c.g. well forward, requiring nose-up lift.\n\nViolations of the basic principle are exploited in some high performance \"relaxed static stability\" combat aircraft to enhance agility; artificial stability is supplied by active electronic means.\n\nThere are a few classical cases where this favourable response was not achieved, notably some early T-tail jet aircraft. In the event of a very high angle of attack, the horizontal stabilizer became immersed in downwash from the wing and fuselage, causing excessive download on the stabilizer, increasing the angle of attack still further. The only way an aircraft could recover from this situation was by jettisoning tail ballast or deploying a special tail parachute. The phenomenon became known as 'deep stall'.\n\nTaking moments about the center of gravity, the net nose-up moment is:\nwhere formula_19 is the location of the center of gravity behind the aerodynamic center of the main wing, formula_20 is the tail moment arm.\nFor trim, this moment must be zero. For a given maximum elevator deflection, there is a corresponding limit on center of gravity position at which the aircraft can be kept in equilibrium. When limited by control deflection this is known as a 'trim limit'. In principle trim limits could determine the permissible forwards and rearwards shift of the centre of gravity, but usually it is only the forward cg limit which is determined by the available control, the aft limit is usually dictated by stability.\n\nIn a missile context 'trim limit' more usually refers to the maximum angle of attack, and hence lateral acceleration which can be generated.\n\nThe nature of stability may be examined by considering the increment in pitching moment with change in angle of attack at the trim condition. If this is nose up, the aircraft is longitudinally unstable; if nose down it is stable. Differentiating the moment equation with respect to formula_7:\nNote: formula_23 is a stability derivative.\n\nIt is convenient to treat total lift as acting at a distance h ahead of the centre of gravity, so that the moment equation may be written:\nApplying the increment in angle of attack:\nEquating the two expressions for moment increment:\nThe total lift formula_27 is the sum of formula_2 and formula_3 so the sum in the denominator can be simplified and written as the derivative of the total lift due to angle of attack, yielding:\nWhere c is the mean aerodynamic chord of the main wing. The term:\nis known as the tail volume ratio. Its rather complicated coefficient, the ratio of the two lift derivatives, has values in the range of 0.50 to 0.65 for typical configurations, according to Piercy. Hence the expression for h may be written more compactly, though somewhat approximately, as:\nh is known as the static margin. For stability it must be negative. (However, for consistency of language, the static margin is sometimes taken as formula_33, so that positive stability is associated with positive static margin.)\n\nA mathematical analysis of the longitudinal static stability of a complete aircraft (including horizontal stabilizer) yields the position of center of gravity at which stability is neutral. This position is called the neutral point. (The larger the area of the horizontal stabilizer, and the greater the moment arm of the horizontal stabilizer about the aerodynamic center, the further aft is the neutral point.)\n\nThe static center of gravity margin (c.g. margin) or static margin is the distance between the center of gravity (or mass) and the neutral point. It is usually quoted as a percentage of the Mean Aerodynamic Chord. The center of gravity must lie ahead of the neutral point for positive stability (positive static margin). If the center of gravity is behind the neutral point, the aircraft is longitudinally unstable (the static margin is negative), and active inputs to the control surfaces are required to maintain stable flight. Some combat aircraft that are controlled by fly-by-wire systems are designed to be longitudinally unstable so they will be highly maneuverable. Ultimately, the position of the center of gravity relative to the neutral point determines the stability, control forces, and controllability of the vehicle.\n\nFor a tailless aircraft formula_34, the neutral point coincides with the aerodynamic center, and so for longitudinal static stability the center of gravity must lie ahead of the aerodynamic center.\n\nAn aircraft’s static stability is an important, but not sufficient, measure of its handling characteristics, and whether it can be flown with ease and comfort by a human pilot. In particular, the longitudinal dynamic stability of a statically stable aircraft will determine whether or not it is finally able to return to its original position.\n\n\n"}
{"id": "24573220", "url": "https://en.wikipedia.org/wiki?curid=24573220", "title": "Mein-ma-hla Kyun Wildlife Sanctuary", "text": "Mein-ma-hla Kyun Wildlife Sanctuary\n\nMein-ma-hla Kyun Wildlife Sanctuary () is a wetland reserve in Myanmar's Ayeyarwady Delta, located in Bogale Township, Ayeyarwady Division. It was established in 1986, is classified as a mangrove ecosystem reserve and occupies an area of .\n\n\"Mein-ma-hla\" means beautiful woman in Burmese.\n\nThe protected area has a total of 29 mangrove tree species, saltwater crocodiles and a wide range of birds.\n"}
{"id": "2324595", "url": "https://en.wikipedia.org/wiki?curid=2324595", "title": "Metal foam", "text": "Metal foam\n\nA metal foam is a cellular structure consisting of a solid metal (frequently aluminium) with gas-filled pores comprising a large portion of the volume. The pores can be sealed (closed-cell foam) or interconnected (open-cell foam). The defining characteristic of metal foams is a high porosity: typically only 5–25% of the volume is the base metal, making these ultralight materials. The strength of the material is due to the square-cube law. \n\nMetallic foams typically retain some physical properties of their base material. Foam made from non-flammable metal remains non-flammable and can generally be recycled as the base material. Its coefficient of thermal expansion is similar while thermal conductivity is likely reduced.\n\nOpen celled metal foam, also called metal sponge, can be used in heat exchangers (compact electronics cooling, cryogen tanks, PCM heat exchangers), energy absorption, flow diffusion, and lightweight optics. The high cost of the material generally limits its use to advanced technology, aerospace, and manufacturing.\n\nFine-scale open-cell foams, with cells smaller than can be seen unaided, are used as high-temperature filters in the chemical industry.\n\nMetallic foams are used in compact heat exchangers to increase heat transfer at the cost of reduced pressure. However, their use permits substantial reduction in physical size and fabrication costs. Most models of these materials use idealized and periodic structures or averaged macroscopic properties.\n\nMetal sponge has very large surface area per unit weight and catalysts are often formed into metal sponge, such as palladium black, platinum sponge, and spongy nickel. Metals such as osmium and palladium hydride are metaphorically called \"metal sponges\", but this term is in reference to their property of binding to hydrogen, rather than the physical structure.\n\nOpen cell foams are manufactured by foundry or powder metallurgy. In the powder method, \"space holders\" are used; as their name suggests, they occupy the pore spaces and channels. In casting processes, foam is cast with an open-celled polyurethane foam skeleton.\n\nClosed-cell metal foam was first reported in 1926 by Meller in a French patent where foaming of light metals, either by inert gas injection or by blowing agent, was suggested. Two patents on sponge-like metal were issued to Benjamin Sosnik in 1948 and 1951 who applied mercury vapor to blow liquid aluminium.\n\nClosed-cell metal foams were developed in 1956 by John C. Elliott at Bjorksten Research Laboratories. Although the first prototypes were available in the 1950s, commercial production began in the 1990s by Shinko Wire company in Japan. Closed-cell metal foams are primarily used as an impact-absorbing material, similarly to the polymer foams in a bicycle helmet but for higher impact loads. Unlike many polymer foams, metal foams remain deformed after impact and can therefore only be deformed once. They are light (typically 10–25% of the density of an identical non-porous alloy; commonly those of aluminium) and stiff and are frequently proposed as a lightweight structural material. However, they have not been widely used for this purpose.\n\nClosed-cell foams retain the fire resistance and recycling potential of other metallic foams, but add the property of flotation in water.\n\nFoams are commonly made by injecting a gas or mixing a foaming agent into molten metal. Melts can be foamed by creating gas bubbles in the material. Normally, bubbles in molten metal are highly buoyant in the high-density liquid and rise quickly to the surface. This rise can be slowed by increasing the viscosity of the molten metal by adding ceramic powders or alloying elements to form stabilizing particles in the melt, or by other means. Metallic melts can be foamed in one of three ways:\nTo stabilize the molten metal bubbles, high temperature foaming agents (nano- or micrometer- sized solid particles) are required. The size of the pores, or cells, is usually 1 to 8 mm. When foaming or blowing agents are used, they are mixed with the powdered metal before it is melted. This is the so-called \"powder route\" of foaming, and it is probably the most established (from an industrial standpoint). After metal (e.g. aluminium) powders and foaming agent (e.g.TiH) have been mixed, they are compressed into a compact, solid precursor, which can be available in the form of a billet, a sheet, or a wire. Production of precursors can be done by a combination of materials forming processes, such as powder pressing, extrusion (direct or conform) and flat rolling.\n\nComposite metal foam (CMF) is formed from hollow beads of one metal within a solid matrix of another, such as steel within aluminium, show 5 to 6 times greater strength to density ratio and more than 7 times greater energy absorption than previous metal foams.\n\nA less than one inch thick plate has enough resistance to turn a 7.62 x 63 mm standard-issue M2 armor piercing bullet to dust. The test plate outperformed a solid metal plate of similar thickness, while weighing far less. Other potential applications include nuclear waste (shielding X-rays, gamma rays and neutron radiation) transfer and thermal insulation for space vehicle atmospheric re-entry, with twice the resistance to fire and heat as the plain metals.\n\nCMF can replace rolled steel armor with the same protection for one-third the weight. It can block fragments and the shock waves that are responsible for brain injuries. Stainless steel CMF can block blast pressure and fragmentation at 5,000 feet per second from high explosive incendiary (HEI) rounds that detonate 18 inches from the shield. Steel CMF plates (9.5 mm or 16.75 mm thick) were placed 18 inches from the strikeplate held up against the wave of blast pressure and against the copper and steel fragments created by a 23×152 mm HEI round (as in anti-aircraft weapons) as well as a 2.3mm aluminum strikeplate.\n\nA foam is said to be stochastic when the porosity distribution is random. Most foams are stochastic because of the method of manufacture:\n\n\nA foam is said to be regular when the structure is ordered. Direct molding is one technology that produces regular foams with open pores. In the alternate, regular metal foams can be produced by additive processes such as selective laser melting (SLM).\n\nPlates can be used as casting cores. The shape is customized for each application. This manufacturing method allows for \"perfect\" foam, so-called because it satisfies Plateau's laws and has conducting pores of the shape of a truncated octahedron Kelvin cell (body-centered cubic structure).\n\nMetal foam can be used in product or architectural composition.\n\nFoam metal has been used in experimental animal prosthetics. In this application, a hole is drilled into the bone and the metal foam inserted, letting the bone grow into the metal for a permanent junction. For orthopedic applications, tantalum or titanium foams are common for their tensile strength, corrosion resistance and biocompatibility.\n\nThe back legs of Siberian Husky named Triumph received foam metal prostheses. Mammalian studies showed that porous metals, such as titanium foam, may allow vascularization within the porous area.\n\nOrthopedic device manufacturers use foam construction or metal foam coatings to achieve desired levels of osseointegration.\n\nThe primary functions of metallic foams in vehicles are to increase sound damping, reduce weight, increase energy absorption in case of crashes, and (in military applications) to combat the concussive force of IEDs. As an example, foam filled tubes could be used as anti-intrusion bars. Because of their low density (0.4–0.9 g/cm), aluminium and aluminium alloy foams are under particular consideration. These foams are stiff, fire resistant, nontoxic, recyclable, energy absorbent, less thermally conductive, less magnetically permeable, and more efficiently sound dampening, especially when compared to hollow parts. Metallic foams in hollow car parts decrease weakness points usually associated with car crashes and vibration. These foams are inexpensive to cast with powder metallurgy, compared to casting other hollow parts.\n\nCompared to polymer foams in vehicles, metallic foams are stiffer, stronger, more energy absorbent, and resistant to fire and the weather adversities of UV light, humidity, and temperature variation. However, they are heavier, more expensive, and non-insulating.\n\nMetal foam technology has been applied to automotive exhaust gas. Compared to traditional catalytic converters that use cordierite ceramic as substrate, metal foam substrate offers better heat transfer and exhibits excellent mass-transport properties (high turbulence) and may reduce the quantity of platinum catalyst required.\n\nMetal foams are used for stiffening a structure without increasing its mass. For this application, metal foams are generally closed pore and made of aluminium. Foam panels are glued to the aluminium plate to obtain a resistant composite sandwich locally (in the sheet thickness) and rigid along the length depending on the foam's thickness.\n\nThe advantage of metal foams is that the reaction is constant, regardless of the direction of the force. Foams have a plateau of stress after deformation that is constant for as much as 80% of the crushing.\n\nTian et al. listed several criteria to assess a foam in a heat exchanger. The comparison of thermal-performance metal foams with materials conventionally used in the intensification of exchange (fins, coupled surfaces, bead bed) first shows that the pressure losses caused by foams are much more important than with conventional fins, yet are significantly lower than those of beads. The exchange coefficients are close to beds and ball and well above the blades.\n\nFoams offer other thermophysical and mechanical features:\n\n\nCommercialization of foam-based compact heat exchangers, heat sinks and shock absorbers is limited due to the high cost of foam replications. Their long-term resistance to fouling, corrosion and erosion are insufficiently characterized. From a manufacturing standpoint, the transition to foam technology requires new production and assembly techniques and heat exchanger design.\n\n\n"}
{"id": "3498695", "url": "https://en.wikipedia.org/wiki?curid=3498695", "title": "National Atomic Energy Commission", "text": "National Atomic Energy Commission\n\nThe National Atomic Energy Commission (, CNEA) is the Argentine government agency in charge of nuclear energy research and development.\n\nThe agency was created on May 31, 1950 with the mission of developing and controlling nuclear energy for peaceful purposes in the country.\n\nCNEA's facilities include the Bariloche Atomic Centre (in San Carlos de Bariloche), Constituyentes Atomic Centre (in Buenos Aires), and Ezeiza Atomic Centre (in Ezeiza, Buenos Aires Province). CNEA operates research reactors at each of these sites.\n\nOfficially established by President Juan Perón's Decree No 10.936, CNEA filled the need for a state organ to oversee the funding of the Huemul Project in Bariloche. Before CNEA came into being, the project was funded by the Dirección de Migraciones. In practice CNEA had only four members (Juan Domingo Perón, González, Mendé and Ronald Richter). In 1951, decree 9697 created another agency, the Dirección Nacional de la Energía Atómica (DNEA), also under González, to do research on atomic energy in Buenos Aires (González left CNEA in April 1952 and was replaced by Iraolagoitía) until 1955. After being assessed by two review panels in 1952, the Huemul Project was closed and Richter was no longer a CNEA member. Research in physics and technology continued in Bariloche, but no longer along Richter's original line.\n\nAdmiral Oscar Armando Quihillalt was the first director of the National Atomic Energy Commission, and was also an important participant in the International Atomic Energy Agency.\n\nIn 1955, José Antonio Balseiro, a research scientist and member of the first review panel on the Huemul Project, took over the recently created Instituto de Física de Bariloche, now Instituto Balseiro, which used Richter's facilities in the mainland, but abandoned the buildings in Huemul Island.\n\nIn 1956 Argentina's uranium resources were nationalized with the Commission controlling prospecting, production, and marketing. A yellowcake (uranium oxide) production capability was created that could support future plans for reactors. However, in accord with three presidential decrees of 1960, 1962 and 1963, Argentina supplied its initial production of about 90 tons of unsafeguarded yellowcake to Israel to fuel its Dimona reactor, creating the fissile material for Israel's first nuclear weapons.\n\nThe facilities in Buenos Aires were expanded after the closure of the Huemul Project, and by the 1960s became larger in terms of size and expenditure than those in Bariloche. It was the Constituyentes research centre that the first Latin American research reactor was built (1957), the RA-1 Enrico Fermi.\n\nThe 335-MWe Atucha I designed by Germany's (Siemens) was completed in 1974. In 1984 the 600-MWe Embalse designed by Canadian CANDU started its commercial operation.\n\nIn 1995 CNEA's functions where divided into newly created separate organizations such as the Nuclear Regulatory Authority (\"Autoridad Regulatoria Nuclear - ARN\"), which supervises nuclear safety in the country, and the \"Nucleoeléctrica Argentina - NASA\", the company that operates the two completed nuclear power reactors. Today, CNEA mainly focuses on research and development of nuclear and related technologies.\n\nIn 2001 the original 1956 Directory structure of a President and five members designated by the National Executive authority was reduced, eliminating four positions of the Directory by Decrees 1065/01 and 1066/01.\n\nOn 2010 CNEA reopened the Enriched uranium plant at Pilcaniyeu inaugurated on 1983 but closed in the 1990s\n\nIn 2014 692-MWe Atucha II, also a Siemens design, started operating besides Atucha I. Since then Argentina operates three pressurized heavy-water reactors (PHWR), that use heavy water as moderator and coolant and unenrciched natural uranium as fuel, for the generation of electricity.\n\nAlso in 2014, CNEA begun construction of the world’s first small modular reactor (SMR), the CAREM-25 a small (25 MWe) pressurized water reactor (PWR) that unlike all previous power reactors in the country has been totally designed and developed domestically. It will use low-enrichment uranium as fuel and light water as coolant and moderator, a first for the country that traditionally used PHWRs designs.\n\nIn December 2015 a new uranium enrichment plant to manufacture fuel for Argentina's nuclear plants, located in Pilcaniyeu, was inaugurated. The plant will use both gaseous diffusion and more modern laser techniques.\n\nAlthough CNEA HQ is located in Buenos Aires city most of CNEA's activities are concentrated at three other sites\nIts premises contain the first nuclear reactor in Latin America RA-1 that went critical in 17 January 1958. Its researchers not only pioneered the construction of research reactors in Argentina, but also formed its leading groups in metallurgy, physics and chemistry. The Tandar accelerator is a large tandem Van de Graaff type facility dedicated to nuclear physics, condensed matter physics and medical research. It houses facilities for the development and building of research nuclear reactors fuels. Its academic unit, the Instituto Sábato, focused in Materials Science grants Licenciado, MSc and PhD degrees in association with the Universidad de San Martín.\n\nIts academic unit the Instituto Balseiro, an association with the Universidad Nacional de Cuyo trained hundred of physicists and nuclear engineers since its creation as Instituto de Física de Bariloche in 1955. Its researchers published hundreds of peer-reviewed scientific papers and provided important contributions to applied and basic science. Its RA-6 reactor was built to train Balseiro's nuclear engineering students as well to conduct basic and applied research.\n\nIts RA-3 research/production reactor produces all nuclear isotopes for medicine used in Argentina and supplies neighbouring countries as well. Other research facilities include . Its academic unit Instituto Beninson in association with the Universidad de San Martín is focused on the applications of nuclear technologies, has an engineering degree on the subject, has many post-graduate degrees as well as granting PhDs on connected areas.\n\nThere are other smaller sites around the country concerned with particular activities each\n\nThe CNEA currently holds two U.S. patents(see external links)\n\nAlthough power reactors in the 100-megawatt range have not been built by Argentina on its own, \nINVAP S.E., a company owned by the Río Negro Province, started by graduates of the Instituto Balseiro in 1976, has exported research reactors to Peru, Algeria, Egypt and most recently Australia (2000). The Australian reactor, OPAL, featuring a core power of 20 MW became operational in 2006, reaching full power in November of that year.\n\nusd$206 Million\n\n\n"}
{"id": "49470219", "url": "https://en.wikipedia.org/wiki?curid=49470219", "title": "National Petroleum Council (Brazil)", "text": "National Petroleum Council (Brazil)\n\nThe National Petroleum Council (Conselho Nacional do Petróleo or CNP) is a Brazilian organization that was established in 1938 to \"supervise, regulate, and carry out the oil industry activities previously executed by the SFPM (de Oliveira, 2012) The SFPM (Service for Promotion of Mineral Production, according to de Oliveira) was a previous governmental organization established to encourage the search and production of minerals and oil but proved to fail. The CNP was headed by General Horta Barbosa and its first mission was to find oil in the Bahia state.\n"}
{"id": "8442558", "url": "https://en.wikipedia.org/wiki?curid=8442558", "title": "Nickel oxide hydroxide", "text": "Nickel oxide hydroxide\n\nNickel oxide hydroxide is the inorganic compound with the chemical formula NiO(OH). It is a black solid that is insoluble in all solvents but attacked by base and acid. It is a component of the nickel-metal hydride battery. \n\nNickel(III) oxides are often poorly characterized and are assumed to be nonstoichiometric compounds. Nickel(III) oxide (NiO) has not been verified crystallographically. For applications in organic chemistry, nickel oxides or peroxides are generated in situ and lack crystallographic characterization. For example, \"nickel peroxide\" (CAS# 12035-36-8) is also closely related to or even identical with NiO(OH).\n\nIts layered structure resembles that of the brucite polymorph of nickel(II) hydroxide, but with half as many hydrogens. The oxidation state of nickel is 3+. It can be prepared by the reaction of nickel(II) nitrate with aqueous potassium hydroxide and bromine as the oxidant:\n\nNickel(III) oxides catalyze the oxidation of benzyl alcohol to benzoic acid using bleach:\n\nSimilarly it catalyzes the double oxidation of 3-butenoic acid to fumaric acid:\n"}
{"id": "16510408", "url": "https://en.wikipedia.org/wiki?curid=16510408", "title": "Nuclear entombment", "text": "Nuclear entombment\n\nNuclear entombment (also referred to as \"safe enclosure\") is entombment of a radioactive site and a method of nuclear decommissioning in which radioactive contaminants are encased in a structurally long-lived material, such as concrete. The idea is that the entombment will last for a period of time to ensure the remaining radioactivity is no longer of significant concern. The method has usually been used in relation to nuclear reactors, but also in relation to some nuclear test sites. Regarding decommission of nuclear power plants, entombment is one of three various ways: dismantling, safe enclosure and entombment. Nuclear entombment is the least used of the three options. The use of nuclear entombment is more practical for larger nuclear power plants that are in need of both long and short term burials. Entombment is used as a case by case basis because of its major commitment with years of surveillance and complexity until the radioactivity is no longer a major concern, permitting decommissioning and ultimate unrestricted release of the property.\n\nThe first procedure is to properly shutdown the site and stow any spent fuel/waste. The waste and reactors are often at extremely high temperatures due to the fission reaction that occurs. The waste is often placed in cooling pools filled with treated water where they await to cool to handling temperatures. Once the waste is cool enough it will often be stored in radioactive resistant containers to await being disposed of. The reactors are shutdown using special control rods to deter the fission reaction and allow for the cooling of the reactor and fuel inside. Once cool the fuel is taken out and dealt with like the waste, while the reactor is sealed in order to allow no escape of radioactive particles or gases. Lastly the heating water is then pumped out and put in containers to await proper decontamination; the coolant is also removed and stored for proper disposal. This procedure is often performed by the company that owned the plant, and if the company is unable to then properly qualified contractors are brought in. After this procedure comes the next one which deals with the radioactivity and radioactive waste.\n\nThe second procedure is the dismantling of the site. First, the radioactive fuel is removed, some of which can be recycled and refined for further use in a reactor. The most dangerous waste is placed inside a radioactive resistant containers, after which the containers are transported to storage facilities. The rest of the site can then be decontaminated through various means. The site is then checked thoroughly for any signs of radiation. Most of the remaining waste onsite can be disposed of normally as it is either not contaminated or radioactivity levels have dropped to within safe limits. This process is often completed using robots, which are able to access the difficult to reach areas deemed too radioactive for human workers.\nThe procedure of entombment is a time extensive process. The simplest of the procedures is entombing the radioactive waste source at the site itself. After containment and disposal of lower-level radioactive spent fuel sources, the entombment process of high-level radioactive parts of the plant may begin. The first step is to cover the area with a protective shield which is usually made up of radioactive-resistant materials, this allows workers to continue working with a significantly lower radioactive environment. The next step is the most crucial and time consuming, it involves encasing the site in cement, absorbent grout, or infills. Each layer of cement, grout or infills must set and cure before the next layer is added and thus, time and proper testing is required to ensure the safe containment of radiation. Once the encasement is complete the final step is often to surround the site in a clay or sand/gravel mixture and then the soil is laid on top of the site. The use of many layers allows for the maximum shielding of radioactivity. The site must be routinely checked for breaches in the containment barrier for decades. Therefore, entombment is often considered as a last resort solution to the decommissioning of a nuclear power plant or nuclear disaster site. An example would be a waste disposal facility in El Cabril Spain that uses a multi-concrete barrier concept. The concept includes placing the radioactive waste drums inside concrete boxes and placing those boxes inside a reinforced concrete vault. The Vault is then sealed with a waterproof coating to prevent any hazardous liquid from escaping the drums.\n\nMuch of the concern using nuclear entombment is the worry of nuclear leakage. Nuclear leakage occurs when there is a slight crack in the cementitious material or the clay for any radiation to seep through. Entombment does not solve the problem for every type of isotope of waste. Many different types of nuclides outlive the lifetime of the materials containing it by a thousand years or more. There are some disagreements among state governments on nuclear entombment, as it does not uphold the polluter pays principle. It is often not possible to make an exact estimate of total decommissioning costs, leaving a certain financial liability for a future generation. It is also difficult to guarantee that the necessary expertise for final decommissioning will be available in due time, or that the decommissioning fund has earned sufficient interest. The main concern for many of the infrastructures being used to entomb the waste, is that the facilities being used to entomb the radioactive waste were built for operations and not for disposal and burial of the waste. Some of the facilities may be too small for nuclear waste to be entombed because of the financial aspect going into all the materials needed to build safe housing for the hazardous material. The only other option in the case of the facility being too small is for the nuclear waste to be transferred to another disposing facility or the facility will have to be reinforced. Many U.S. state governments chose to steer clear of entombment because of its potential hazards of not being properly contained. If any of the nuclear waste were to come in contact with any water source, it has the potential to spread through rivers and lakes into highly populated cities to radioactively contaminate it and anyone who drinks from it.\n\nThe surveillance cost will be lower than the surveillance cost for SAFSTOR (safe storage) option. For example, Yankee Rowe Nuclear Power Station was finally decommissioned in 1960 at the cost of 39 million with the idea of making it to a safe storage. The cost to surveillance the power plant will add up to 8 million every year. In 2007 The Yankee Nuclear Power Station in Rowe was fully dismantled. The cost for entombment is less than the cost or dismantling which rends it to be more of a financial choice. By using the facility in which the waste came from helps the entombment process cost to be less. The use of entombment requires fewer workers and prevents them from being in major contact with the nuclear waste. In addition to using the same facility to reduce cost, it also reduces public interaction with the project and the amount of nuclear radiation emitted from the waste. By disposing of the nuclear waste in the same facility it will allow engineers to reinforce the facility to ensure safety for the public and the environment. By using the entombment method nuclear waste will be able to decompose, which will then be allowed to be transferred and the facility will be able to be dismantled.\n\nThe USNRC provides licensing for the entombment process, as well as providing license they research and develop programs to help decommission nuclear power plants. USNRC will continue the development of rule making for entombment. NRC will ask the company that are running power plants to set some money aside while the power plant is being run for future shut down cost. The NRC has decided that in order for nuclear entombment to be a possible, a long-term structure must be created specifically for the encasing of the radioactive waste. If the structures are not correctly built water can seep into them and come out with radioactive waste that can infect the public. The NRC itself imposes acts such as the Nuclear Waste Policy Act of 1982 and the Low-level radioactive waste policy to help regulate state governments on the procedures and precautions needed to dispose of the nuclear waste. The Nuclear Waste Policy of 1982 states that both the federal government's are responsible to provide a permanent disposal facility for high-level radioactive waste and spent nuclear fuel. As well with the Nuclear Waste Policy Act of 1982, which gives states the responsibility to dispose of low-level waste and provides facilities to the states that will be regulated by the NRC or by states who have agreed to follow §274 of the Atomic Energy Act.\n\nThe Chernobyl Nuclear Incident is one of the most notable nuclear disasters. The current containment building, known as the sarcophagus, does not classify as a proper entombment device. However, due to lack of repair and maintenance, because of extremely high levels of radiation, a new tomb is in the process of construction and will be considered a proper tomb. Upon completion in 2017 the structure will measure 110 meters tall and span 260 meters. The main arch will be composed of triple-layered radiation resistant panels made up of stainless steel coated in polycarbonate, this will provide the shielding necessary for radioactive containment. The structure will weigh over 30,000 tons and completely cover Reactor number 4. This new tomb is designed to last over 100 years and has special ventilation and temperature systems to prevent condensation of radioactive fluids on the inside which could result in a compromised containment. Overall this new containment structure will act as band-aid and only buy time for the Ukrainian Government to develop ways of properly cleaning up the site.\n\n\n\n"}
{"id": "14659610", "url": "https://en.wikipedia.org/wiki?curid=14659610", "title": "Oil megaprojects (2010)", "text": "Oil megaprojects (2010)\n\nFollowing is a list of Oil megaprojects in the year 2010, projects that propose to bring more than of new liquid fuel capacity to market with the first production of fuel. This is part of the Wikipedia summary of Oil Megaprojects.\n\nTerminology\n"}
{"id": "5090040", "url": "https://en.wikipedia.org/wiki?curid=5090040", "title": "Oil of catechumens", "text": "Oil of catechumens\n\nThe Oil of Catechumens is the oil used in some traditional Christian churches during baptism; it is believed to strengthen the one being baptized to turn away from evil, temptation and sin.\n\nThe catechumen, the person prepared for baptism, is also anointed as a symbol of being the heir of the Kingdom of God, as kings and queens were anointed at coronations, and empowered for their Christian life as prophets were anointed for their ministry.\n\nThe Oil of Catechumens is intended to help strengthen the person about to be baptized, and prepare them for the struggle (\"ascesis\") of the Christian life, the way a wrestler in ancient Greece and Rome was anointed before a wrestling match.\n\nIn the Eastern Orthodox Church, the oil of catechumens is blessed by the priest during the baptismal rite. After the consecration of the baptismal water, a vessel of pure olive oil is brought to the priest by the deacon. The priest breathes on the oil three times and blesses it thrice, and says the prayer of blessing.\n\nThe priest then pours a portion of the oil into the baptismal font, making the Sign of the Cross with the oil three times, as all sing Alleluia. The priest gathers some of the oil floating on the surface of the water onto the first two fingers of his right hand and anoints the catechumen, making the Sign of the Cross on the brow, breast, between the shoulders, on the ears, hands and feet. The catechumen is then immediately baptized.\n\nThis anointing before baptism should not be confused with chrismation, which is a separate Sacred Mystery (Sacrament), though it is usually performed immediately after Baptism\n\nIn the Roman Catholic Church, the Oil of Catechumens is specially blessed by a bishop or a priest along with Chrism and oil of the sick at the Mass of Chrism which takes place on Holy Thursday.\n\nDuring the baptismal rite, the priest says the following words as he anoints with the oil in the shape of a cross, \"We anoint you with the oil of salvation in the name of Christ our Savior; may he strengthen you with his power, who lives and reigns for ever and ever.\"\n\n\n"}
{"id": "2658674", "url": "https://en.wikipedia.org/wiki?curid=2658674", "title": "Parasitic loss", "text": "Parasitic loss\n\nParasitic loss is that which a parasite consumes from its host, for whom the loss may or may not be beneficial.\n"}
{"id": "24470288", "url": "https://en.wikipedia.org/wiki?curid=24470288", "title": "Plumber's putty", "text": "Plumber's putty\n\nPlumber's putty is a type of putty used as a sealant in plumbing. It is a pliable substance used to make watertight seals around faucets and drains. The putty is a basic component of a plumber's toolkit and is often used when replacing plumbing fixtures. Plumber's putty formulations vary but commonly include powdered clay and linseed oil. Other formulas use limestone, talc, or fish oil. RTV silicone or epoxy sealants may be used in place of putty.\n"}
{"id": "335132", "url": "https://en.wikipedia.org/wiki?curid=335132", "title": "Polyphase coil", "text": "Polyphase coil\n\nPolyphase coils are electromagnetic coils connected together in a polyphase system such as a generator or motor. In modern systems, the number of phases is usually three or a multiple of three. Each phase carries a sinusoidal alternating current whose phase is delayed relative to one of its neighbours and advanced relative to its other neighbour. The phase currents are separated in time evenly within each period of the alternating current. For example, in a three-phase system, the phases are separated from each other by one-third of the period.\n\nLike all coils used in electrical machinery, polyphase coils (made from insulated conducting wire) are wound around ferromagnetic armatures with radial projections and maximum core-surface exposure to the magnetic field.\n\nThe windings are physically separated around the circumference of an electrical machine. The result of such an arrangement is a rotating magnetic field that is used to convert electrical power to rotary mechanical work, or vice versa.\n\nCompared to single-phase motors and generators, polyphase motors are simpler, because they do not require external circuitry (using capacitors and inductors) to produce a starting torque. Polyphase machines can deliver constant power over each period of the alternating current, eliminating the pulsations found in a single-phase machine as the current passes through zero amplitude.\n\nThe use of polyphase coils in electrical power systems was pioneered by the engineers Nikola Tesla, Galileo Ferraris, and Michail Dolivo-Dobrovolsky.\n\n"}
{"id": "11400437", "url": "https://en.wikipedia.org/wiki?curid=11400437", "title": "Priority board", "text": "Priority board\n\nA priority board is a group of elected citizen volunteers who meet on a regular basis to address neighborhood concerns and to take action to improve the quality of their neighborhoods. They often act as the official citizen voice for their neighborhoods, and advise the city on neighborhood concerns and problems. The priority board members take a role in planning and make recommendations to the city for development, revitalization, and the allocation of city funds.\n\nThe seven Priority Boards in Dayton are: Downtown, F.R.O.C., Innerwest, Northwest, Northeast, Southeast and Southwest.\n\n\n"}
{"id": "24027000", "url": "https://en.wikipedia.org/wiki?curid=24027000", "title": "Properties of water", "text": "Properties of water\n\nWater () is a polar inorganic compound that is at room temperature a tasteless and odorless liquid, which is nearly colorless apart from an inherent hint of blue. It is by far the most studied chemical compound and is described as the \"universal solvent\" and the \"solvent of life\". It is the most abundant substance on Earth and the only common substance to exist as a solid, liquid, and gas on Earth's surface. It is also the third most abundant molecule in the universe.\n\nWater molecules form hydrogen bonds with each other and are strongly polar. This polarity allows it to separate ions in salts and strongly bond to other polar substances such as alcohols and acids, thus dissolving them. Its hydrogen bonding causes its many unique properties, such as having a solid form less dense than its liquid form, a relatively high boiling point of 100 °C for its molar mass, and a high heat capacity.\n\nWater is amphoteric, meaning that it exhibits properties of both an acid and a base; it readily produces both and ions. Related to its amphoteric character, it undergoes self-ionization. The product of the activities, or approximately, the concentrations of and is a constant.\n\nWater is the chemical substance with chemical formula ; one molecule of water has two hydrogen atoms covalently bonded to a single oxygen atom.\nWater is a tasteless, odorless liquid at ambient temperature and pressure, and appears colorless in small quantities, although it has its own intrinsic very light blue hue. Ice also appears colorless, and water vapor is essentially invisible as a gas.\n\nUnlike other analogous hydrides of the oxygen family, water is primarily a liquid under standard conditions due to hydrogen bonding. The molecules of water are constantly moving in relation to each other, and the hydrogen bonds are continually breaking and reforming at timescales faster than 200 femtoseconds (2×10 seconds).\nHowever, these bonds are strong enough to create many of the peculiar properties of water, some of which make it integral to life.\n\nWithin the Earth's atmosphere and surface, the liquid phase is the most common and is the form that is generally denoted by the word \"water\". The solid phase of water is known as ice and commonly takes the structure of hard, amalgamated crystals, such as ice cubes, or loosely accumulated granular crystals, like snow. Aside from common hexagonal crystalline ice, other crystalline and amorphous phases of ice are known. The gaseous phase of water is known as water vapor (or steam). Visible steam and clouds are formed from minute droplets of water suspended in the air.\n\nWater also forms a supercritical fluid. The critical temperature is 647 K and the critical pressure is 22.064 MPa. In nature this only rarely occurs in extremely hostile conditions. A likely example of naturally occurring supercritical water is in the hottest parts of deep water hydrothermal vents, in which water is heated to the critical temperature by volcanic plumes and the critical pressure is caused by the weight of the ocean at the extreme depths where the vents are located. This pressure is reached at a depth of about 2200 meters: much less than the mean depth of the ocean (3800 meters).\n\nWater has a very high specific heat capacity of 4.1814 J/(g·K) at 25 °C – the second highest among all the heteroatomic species (after ammonia), as well as a high heat of vaporization (40.65 kJ/mol or 2257 kJ/kg at the normal boiling point), both of which are a result of the extensive hydrogen bonding between its molecules. These two unusual properties allow water to moderate Earth's climate by buffering large fluctuations in temperature. Most of the additional energy stored in the climate system since 1970 has accumulated in the oceans.\n\nThe specific enthalpy of fusion (more commonly known as latent heat) of water is 333.55 kJ/kg at 0 °C: the same amount of energy is required to melt ice as to warm ice from −160 °C up to its melting point or to heat the same amount of water by about 80 °C. Of common substances, only that of ammonia is higher. This property confers resistance to melting on the ice of glaciers and drift ice. Before and since the advent of mechanical refrigeration, ice was and still is in common use for retarding food spoilage.\n\nThe specific heat capacity of ice at −10 °C is 2.03 J/(g·K)\nand the heat capacity of steam at 100 °C is 2.08 J/(g·K).\n\nThe density of water is about : this relationship was originally used to define the gram. The density varies with temperature, but not linearly: as the temperature increases, the density rises to a peak at and then decreases. This unusual negative thermal expansion below is also observed in molten silica. Regular, hexagonal ice is also less dense than liquid water—upon freezing, the density of water decreases by about 9%.\n\nThese effects are due to the reduction of thermal motion with cooling, which allows water molecules to form more hydrogen bonds that prevent the molecules from coming close to each other. While below 4 °C the breakage of hydrogen bonds due to heating allows water molecules to pack closer despite the increase in the thermal motion (which tends to expand a liquid), above 4 °C water expands as the temperature increases. Water near the boiling point is about 4% less dense than water at .\n\nUnder increasing pressure, ice undergoes a number of transitions to other polymorphs with higher density than liquid water, such as ice II, ice III, high-density amorphous ice (HDA), and very-high-density amorphous ice (VHDA).\n\nThe unusual density curve and lower density of ice than of water is vital to life—if water were most dense at the freezing point, then in winter the very cold water at the surface of lakes and other water bodies would sink, the lake could freeze from the bottom up, and all life in them would be killed. Furthermore, given that water is a good thermal insulator (due to its heat capacity), some frozen lakes might not completely thaw in summer. The layer of ice that floats on top insulates the water below. Water at about 4 °C (39 °F) also sinks to the bottom, thus keeping the temperature of the water at the bottom constant (see diagram).\n\nThe density of salt water depends on the dissolved salt content as well as the temperature. Ice still floats in the oceans, otherwise they would freeze from the bottom up. However, the salt content of oceans lowers the freezing point by about 1.9 °C (see here for explanation) and lowers the temperature of the density maximum of water to the former freezing point at 0 °C. This is why, in ocean water, the downward convection of colder water is \"not\" blocked by an expansion of water as it becomes colder near the freezing point. The oceans' cold water near the freezing point continues to sink. So creatures that live at the bottom of cold oceans like the Arctic Ocean generally live in water 4 °C colder than at the bottom of frozen-over fresh water lakes and rivers.\n\nAs the surface of salt water begins to freeze (at −1.9 °C for normal salinity seawater, 3.5%) the ice that forms is essentially salt-free, with about the same density as freshwater ice. This ice floats on the surface, and the salt that is \"frozen out\" adds to the salinity and density of the sea water just below it, in a process known as \"brine rejection\". This denser salt water sinks by convection and the replacing seawater is subject to the same process. This produces essentially freshwater ice at −1.9 °C on the surface. The increased density of the sea water beneath the forming ice causes it to sink towards the bottom. On a large scale, the process of brine rejection and sinking cold salty water results in ocean currents forming to transport such water away from the Poles, leading to a global system of currents called the thermohaline circulation.\n\nWater is miscible with many liquids, including ethanol in all proportions. Water and most oils are immiscible usually forming layers according to increasing density from the top. This can be predicted by comparing the polarity. Water being a relatively polar compound will tend to be miscible with liquids of high polarity such as ethanol and acetone, whereas compounds with low polarity will tend to be immiscible and poorly soluble such as with hydrocarbons.\n\nAs a gas, water vapor is completely miscible with air. On the other hand, the maximum water vapor pressure that is thermodynamically stable with the liquid (or solid) at a given temperature is relatively low compared with total atmospheric pressure.\nFor example, if the vapor's partial pressure is 2% of atmospheric pressure and the air is cooled from 25 °C, starting at about 22 °C water will start to condense, defining the dew point, and creating fog or dew. The reverse process accounts for the fog burning off in the morning. If the humidity is increased at room temperature, for example, by running a hot shower or a bath, and the temperature stays about the same, the vapor soon reaches the pressure for phase change, and then condenses out as minute water droplets, commonly referred to as steam.\n\nA saturated gas or one with 100% relative humidity is when the vapor pressure of water in the air is at equilibrium with vapor pressure due to (liquid) water; water (or ice, if cool enough) will fail to lose mass through evaporation when exposed to saturated air. Because the amount of water vapor in air is small, relative humidity, the ratio of the partial pressure due to the water vapor to the saturated partial vapor pressure, is much more useful.\nVapor pressure above 100% relative humidity is called super-saturated and can occur if air is rapidly cooled, for example, by rising suddenly in an updraft.\n\nThe compressibility of water is a function of pressure and temperature. At 0 °C, at the limit of zero pressure, the compressibility is . At the zero-pressure limit, the compressibility reaches a minimum of around 45 °C before increasing again with increasing temperature. As the pressure is increased, the compressibility decreases, being at 0 °C and .\n\nThe bulk modulus of water is about 2.2 GPa. The low compressibility of non-gases, and of water in particular, leads to their often being assumed as incompressible. The low compressibility of water means that even in the deep oceans at 4 km depth, where pressures are 40 MPa, there is only a 1.8% decrease in volume.\n\n The temperature and pressure at which ordinary solid, liquid, and gaseous water coexist in equilibrium is a triple point of water. Since 1954, this point had been used to define the base unit of temperature, the kelvin but, starting in 2019, the kelvin will be defined using the Boltzmann constant, rather than the triple point of water.\n\nDue to the existence of many polymorphs (forms) of ice, water has other triple points, which have either three polymorphs of ice or two polymorphs of ice and liquid in equilibrium. Gustav Heinrich Johann Apollon Tammann in Göttingen produced data on several other triple points in the early 20th century. Kamb and others documented further triple points in the 1960s.\nThe melting point of ice is at standard pressure; however, pure liquid water can be supercooled well below that temperature without freezing if the liquid is not mechanically disturbed. It can remain in a fluid state down to its homogeneous nucleation point of about . The melting point of ordinary hexagonal ice falls slightly under moderately high pressures, by /atm or about /70 atm as the stabilization energy of hydrogen bonding is exceeded by intermolecular repulsion, but as ice transforms into its allotropes (see crystalline states of ice) above , the melting point increases markedly with pressure, i.e., reaching at (triple point of Ice VII).\n\nPure water containing no exogenous ions is an excellent insulator, but not even \"deionized\" water is completely free of ions. Water undergoes auto-ionization in the liquid state, when two water molecules form one hydroxide anion () and one hydronium cation ().\n\nBecause water is such a good solvent, it almost always has some solute dissolved in it, often a salt. If water has even a tiny amount of such an impurity, then it can conduct electricity far more readily.\n\nIt is known that the theoretical maximum electrical resistivity for water is approximately 18.2 MΩ·cm (182 kΩ·m) at 25 °C. This figure agrees well with what is typically seen on reverse osmosis, ultra-filtered and deionized ultra-pure water systems used, for instance, in semiconductor manufacturing plants. A salt or acid contaminant level exceeding even 100 parts per trillion (ppt) in otherwise ultra-pure water begins to noticeably lower its resistivity by up to several kΩ·m.\n\nIn pure water, sensitive equipment can detect a very slight electrical conductivity of 0.05501 ± 0.0001 µS/cm at 25.00 °C. Water can also be electrolyzed into oxygen and hydrogen gases but in the absence of dissolved ions this is a very slow process, as very little current is conducted. In ice, the primary charge carriers are protons (see proton conductor). Ice was previously thought to have a small but measurable conductivity of 1 S/cm, but this conductivity is now thought to be almost entirely from surface defects, and without those, ice is an insulator with an immeasurably small conductivity.\n\nAn important feature of water is its polar nature. The structure has a bent molecular geometry for the two hydrogens from the oxygen vertex. The oxygen atom also has two lone pairs of electrons. One effect usually ascribed to the lone pairs is that the H–O–H gas phase bend angle is 104.48°, which is smaller than the typical tetrahedral angle of 109.47°. The lone pairs are closer to the oxygen atom than the electrons sigma bonded to the hydrogens, so they require more space. The increased repulsion of the lone pairs forces the O–H bonds closer to each other.\n\nAnother consequence of its structure is that water is a polar molecule. Due to the difference in electronegativity, a bond dipole moment points from each H to the O, making the oxygen partially negative and each hydrogen partially positive. A large molecular dipole, points from a region between the two hydrogen atoms to the oxygen atom. The charge differences cause water molecules to aggregate (the relatively positive areas being attracted to the relatively negative areas). This attraction, hydrogen bonding, explains many of the properties of water, such as its solvent properties.\n\nAlthough hydrogen bonding is a relatively weak attraction compared to the covalent bonds within the water molecule itself, it is responsible for a number of water's physical properties. These properties include its relatively high melting and boiling point temperatures: more energy is required to break the hydrogen bonds between water molecules. In contrast, hydrogen sulfide (), has much weaker hydrogen bonding due to sulfur's lower electronegativity. is a gas at room temperature, in spite of hydrogen sulfide having nearly twice the molar mass of water. The extra bonding between water molecules also gives liquid water a large specific heat capacity. This high heat capacity makes water a good heat storage medium (coolant) and heat shield.\n\nWater molecules stay close to each other (cohesion), due to the collective action of hydrogen bonds between water molecules. These hydrogen bonds are constantly breaking, with new bonds being formed with different water molecules; but at any given time in a sample of liquid water, a large portion of the molecules are held together by such bonds.\n\nWater also has high adhesion properties because of its polar nature. On extremely clean/smooth glass the water may form a thin film because the molecular forces between glass and water molecules (adhesive forces) are stronger than the cohesive forces.\nIn biological cells and organelles, water is in contact with membrane and protein surfaces that are hydrophilic; that is, surfaces that have a strong attraction to water. Irving Langmuir observed a strong repulsive force between hydrophilic surfaces. To dehydrate hydrophilic surfaces—to remove the strongly held layers of water of hydration—requires doing substantial work against these forces, called hydration forces. These forces are very large but decrease rapidly over a nanometer or less. They are important in biology, particularly when cells are dehydrated by exposure to dry atmospheres or to extracellular freezing.\nWater has an unusually high surface tension of 71.99 mN/m at 25 °C which is caused by the strength of the hydrogen bonding between water molecules. This allows insects to walk on water.\n\nBecause water has strong cohesive and adhesive forces, it exhibits capillary action. Strong cohesion from hydrogen bonding and adhesion allows trees to transport water more than 100 m upward.\n\nWater is an excellent solvent due to its high dielectric constant. Substances that mix well and dissolve in water are known as hydrophilic (\"water-loving\") substances, while those that do not mix well with water are known as hydrophobic (\"water-fearing\") substances. The ability of a substance to dissolve in water is determined by whether or not the substance can match or better the strong attractive forces that water molecules generate between other water molecules. If a substance has properties that do not allow it to overcome these strong intermolecular forces, the molecules are precipitated out from the water. Contrary to the common misconception, water and hydrophobic substances do not \"repel\", and the hydration of a hydrophobic surface is energetically, but not entropically, favorable.\n\nWhen an ionic or polar compound enters water, it is surrounded by water molecules (hydration). The relatively small size of water molecules (~ 3 angstroms) allows many water molecules to surround one molecule of solute. The partially negative dipole ends of the water are attracted to positively charged components of the solute, and vice versa for the positive dipole ends.\n\nIn general, ionic and polar substances such as acids, alcohols, and salts are relatively soluble in water, and non-polar substances such as fats and oils are not. Non-polar molecules stay together in water because it is energetically more favorable for the water molecules to hydrogen bond to each other than to engage in van der Waals interactions with non-polar molecules.\n\nAn example of an ionic solute is table salt; the sodium chloride, NaCl, separates into cations and anions, each being surrounded by water molecules. The ions are then easily transported away from their crystalline lattice into solution. An example of a nonionic solute is table sugar. The water dipoles make hydrogen bonds with the polar regions of the sugar molecule (OH groups) and allow it to be carried away into solution.\n\nThe quantum tunneling dynamics in water was reported as early as 1992. At that time it was known that there are motions which destroy and regenerate the weak hydrogen bond by internal rotations of the substituent water monomers. On 18 March 2016, it was reported that the hydrogen bond can be broken by quantum tunneling in the water hexamer. Unlike previously reported tunneling motions in water, this involved the concerted breaking of two hydrogen bonds. Later in the same year, the discovery of the quantum tunneling of water molecules was reported.\n\nWater is relatively transparent to visible light, near ultraviolet light, and far-red light, but it absorbs most ultraviolet light, infrared light, and microwaves. Most photoreceptors and photosynthetic pigments utilize the portion of the light spectrum that is transmitted well through water. Microwave ovens take advantage of water's opacity to microwave radiation to heat the water inside of foods. Water's light blue colour is caused by weak absorption in the red part of the visible spectrum.\n\nA single water molecule can participate in a maximum of four hydrogen bonds because it can accept two bonds using the lone pairs on oxygen and donate two hydrogen atoms. Other molecules like hydrogen fluoride, ammonia and methanol can also form hydrogen bonds. However, they do not show anomalous thermodynamic, kinetic or structural properties like those observed in water because none of them can form four hydrogen bonds: either they cannot donate or accept hydrogen atoms, or there are steric effects in bulky residues. In water, intermolecular tetrahedral structures form due to the four hydrogen bonds, thereby forming an open structure and a three-dimensional bonding network, resulting in the anomalous decrease in density when cooled below 4 °C. This repeated, constantly reorganizing unit defines a three-dimensional network extending throughout the liquid. This view is based upon neutron scattering studies and computer simulations, and it makes sense in the light of the unambiguously tetrahedral arrangement of water molecules in ice structures.\n\nHowever, there is an alternative theory for the structure of water. In 2004, a controversial paper from Stockholm University suggested that water molecules in liquid form typically bind not to four but to only two others; thus forming chains and rings. The term \"string theory of water\" (which is not to be confused with the string theory of physics) was coined. These observations were based upon X-ray absorption spectroscopy that probed the local environment of individual oxygen atoms.\n\nAt standard conditions, water is a polar liquid that slightly dissociates disproportionately or self ionizes into an hydronium ion and hydroxide ion.\n\nThe dissociation constant for this dissociation is commonly symbolized as \"K\" and has a value of about at 25 °C; see here for values at other temperatures. Pure water has a concentration of the hydroxide ion () equal to that of the hydrogen ion (), which gives a pH of 7 at 25 °C.\n\nAction of water on rock over long periods of time typically leads to weathering and water erosion, physical processes that convert solid rocks and minerals into soil and sediment, but under some conditions chemical reactions with water occur as well, resulting in metasomatism or mineral hydration, a type of chemical alteration of a rock which produces clay minerals. It also occurs when Portland cement hardens.\n\nWater ice can form clathrate compounds, known as clathrate hydrates, with a variety of small molecules that can be embedded in its spacious crystal lattice. The most notable of these is methane clathrate, 4 , naturally found in large quantities on the ocean floor.\n\nRain is generally mildly acidic, with a pH between 5.2 and 5.8 if not having any acid stronger than carbon dioxide. If high amounts of nitrogen and sulfur oxides are present in the air, they too will dissolve into the cloud and rain drops, producing acid rain.\n\nSeveral isotopes of both hydrogen and oxygen exist, giving rise to several known isotopologues of water. Vienna Standard Mean Ocean Water is the current international standard for water isotopes. Naturally occurring water is almost completely composed of the neutron-less hydrogen isotope protium. Only 155 ppm include deuterium ( or D), a hydrogen isotope with one neutron, and fewer than 20 parts per quintillion include tritium ( or T), which has two neutrons. Oxygen also has three stable isotopes, with present in 99.76%, in 0.04%, and in 0.2% of water molecules.\n\nDeuterium oxide, , is also known as heavy water because of its higher density. It is used in nuclear reactors as a neutron moderator. Tritium is radioactive, decaying with a half-life of 4500 days; exists in nature only in minute quantities, being produced primarily via cosmic ray-induced nuclear reactions in the atmosphere. Water with one protium and one deuterium atom occurs naturally in ordinary water in low concentrations (~0.03%) and in far lower amounts (0.000003%) and any such molecules are temporary as the atoms recombine.\n\nThe most notable physical differences between and , other than the simple difference in specific mass, involve properties that are affected by hydrogen bonding, such as freezing and boiling, and other kinetic effects. This is because the nucleus of deuterium is twice as heavy as protium, and this causes noticeable differences in bonding energies. The difference in boiling points allows the isotopologues to be separated. The self-diffusion coefficient of at 25 °C is 23% higher than the value of . Because water molecules exchange hydrogen atoms with one another, hydrogen deuterium oxide (DOH) is much more common in low-purity heavy water than pure dideuterium monoxide .\n\nConsumption of pure isolated may affect biochemical processes – ingestion of large amounts impairs kidney and central nervous system function. Small quantities can be consumed without any ill-effects; humans are generally unaware of taste differences, but sometimes report a burning sensation or sweet flavor. Very large amounts of heavy water must be consumed for any toxicity to become apparent. Rats, however, are able to avoid heavy water by smell, and it is toxic to many animals.\n\n\"Light water\" refers to deuterium-depleted water (DDW), water in which the deuterium content has been reduced below the standard level.\n\nIt is the most abundant substance on Earth and also the third most abundant molecule in the universe, after and . 0.23 ppm of the earth's mass is water and 97.39% of the global water volume of 1.38 km is found in the oceans.\n\nWater is amphoteric: it has the ability to act as either an acid or a base in chemical reactions. According to the Brønsted-Lowry definition, an acid is a proton () donor and a base is a proton acceptor. When reacting with a stronger acid, water acts as a base; when reacting with a stronger base, it acts as an acid. For instance, water receives an ion from HCl when hydrochloric acid is formed:\n\nIn the reaction with ammonia, , water donates a ion, and is thus acting as an acid:\n\nBecause the oxygen atom in water has two lone pairs, water often acts as a Lewis base, or electron pair donor, in reactions with Lewis acids, although it can also react with Lewis bases, forming hydrogen bonds between the electron pair donors and the hydrogen atoms of water. HSAB theory describes water as both a weak hard acid and a weak hard base, meaning that it reacts preferentially with other hard species:\n\nWhen a salt of a weak acid or of a weak base is dissolved in water, water can partially hydrolyze the salt, producing the corresponding base or acid, which gives aqueous solutions of soap and baking soda their basic pH:\n\nWater's Lewis base character makes it a common ligand in transition metal complexes, examples of which include metal aquo complexes such as to perrhenic acid, which contains two water molecules coordinated to a rhenium center. In solid hydrates, water can be either a ligand or simply lodged in the framework, or both. Thus, consists of [Fe(HO)] centers and one \"lattice water\". Water is typically a monodentate ligand, i.e., it forms only one bond with the central atom.\nAs a hard base, water reacts readily with organic carbocations; for example in an hydration reaction, a hydroxyl group () and an acidic proton are added to the two carbon atoms bonded together in the carbon-carbon double bond, resulting in an alcohol. When addition of water to an organic molecule cleaves the molecule in two, hydrolysis is said to occur. Notable examples of hydrolysis are the saponification of fats and the digestion of proteins and polysaccharides. Water can also be a leaving group in S2 substitution and E2 elimination reactions; the latter is then known as a dehydration reaction.\n\nWater contains hydrogen in the oxidation state +1 and oxygen in the oxidation state −2. It oxidizes chemicals such as hydrides, alkali metals, and some alkaline earth metals. One example of an alkali metal reacting with water is:\n\nSome other reactive metals, such as aluminum and beryllium, are oxidized by water as well, but their oxides adhere to the metal and form a passive protective layer. Note that the rusting of iron is a reaction between iron and oxygen that is dissolved in water, not between iron and water.\n\nWater can be oxidized to emit oxygen gas, but very few oxidants react with water even if their reduction potential is greater than the potential of . Almost all such reactions require a catalyst.\nAn example of the oxidation of water is:\n\nWater can be split into its constituent elements, hydrogen and oxygen, by passing an electric current through it. This process is called electrolysis.\nThe cathode half reaction is:\n\nThe anode half reaction is:\n\nThe gases produced bubble to the surface, where they can be collected. The required potential for the electrolysis of pure water is 1.23 V at 25 °C. The operating potential is actually 1.48 V or higher in practical electrolysis.\n\nHenry Cavendish showed that water was composed of oxygen and hydrogen in 1781. The first decomposition of water into hydrogen and oxygen, by electrolysis, was done in 1800 by English chemist William Nicholson and Anthony Carlisle. In 1805, Joseph Louis Gay-Lussac and Alexander von Humboldt showed that water is composed of two parts hydrogen and one part oxygen.\n\nGilbert Newton Lewis isolated the first sample of pure heavy water in 1933.\n\nThe properties of water have historically been used to define various temperature scales. Notably, the Kelvin, Celsius, Rankine, and Fahrenheit scales were, or currently are, defined by the freezing and boiling points of water. The less common scales of Delisle, Newton, Réaumur and Rømer were defined similarly. The triple point of water is a more commonly used standard point today.\n\nThe accepted IUPAC name of water is \"oxidane\" or simply \"water\", or its equivalent in different languages, although there are other systematic names which can be used to describe the molecule. Oxidane is only intended to be used as the name of the mononuclear parent hydride used for naming derivatives of water by substituent nomenclature. These derivatives commonly have other recommended names. For example, the name hydroxyl is recommended over \"oxidanyl\" for the –OH group. The name oxane is explicitly mentioned by the IUPAC as being unsuitable for this purpose, since it is already the name of a cyclic ether also known as tetrahydropyran.\n\nThe simplest systematic name of water is \"hydrogen oxide\". This is analogous to related compounds such as hydrogen peroxide, hydrogen sulfide, and deuterium oxide (heavy water). Using chemical nomenclature for type I ionic binary compounds, water would take the name \"hydrogen monoxide\", but this is not among the names published by the International Union of Pure and Applied Chemistry (IUPAC). Another name is \"dihydrogen monoxide\", which is a rarely used name of water, and mostly used in the dihydrogen monoxide hoax.\n\nOther systematic names for water include \"hydroxic acid\", \"hydroxylic acid\", and \"hydrogen hydroxide\", using acid and base names. None of these exotic names are used widely. The polarized form of the water molecule, , is also called hydron hydroxide by IUPAC nomenclature.\n\n\"Water substance\" is a term used for hydrogen oxide (HO) when one does not wish to specify whether one is speaking of liquid water, steam, some form of ice, or a component in a mixture or mineral.\n\n\n"}
{"id": "40167957", "url": "https://en.wikipedia.org/wiki?curid=40167957", "title": "SN2 Palmitate", "text": "SN2 Palmitate\n\nSN2 Palmitate is a structured triglyceride where palmitic acid bonded to the middle position (sn-2) of the glycerol backbone. Structured triglycerides are achieved through an enzymatic process using vegetable oils. Current usage of structured triglycerides is mainly for infant formula providing human milk fat substitute.\n\nFats in human breast milk provides about 50% of energy needed for the development and growth newborn infant. About 98% of the fats provided by the human milk are in the form of triglycerides, which themselves are molecules consisting of mixtures of three fatty acids bonded to sn-1, sn-2, and sn-3 positions of a glycerol backbone.\nThe human mammary gland provides the baby with a unique fat composition where the fatty acids arranged in specific combinations, different from the triglycerides in other human tissues and plasma, or from common dietary fats and oils. Palmitic acid (C16:0) is the major saturated fatty acid in human milk, accounting for 17-25% of the total fatty acids, with over 70% of 16:0 is esterified at the milk triglyceride sn-2 position. The major unsaturated fatty acid in human milk is oleic acid (18:1n-9) and this is mostly esterified at the triglyceride sn-1,3 (outer) positions. The positioning of palmitic acid at the sn-2 position is conserved in all women, regardless of race or nutrition, unlike the general fatty acid profile of human milk.\n\nThe development of sn-2 Palmitate structured triglycerides enables mimicking both the composition as well as the structure of human milk fat. Vegetable oils that are commonly used as source for infant formula fat have the opposite structure where the palmitic is located mainly at sn-1 and sn-3 positions. Enzymatic process on vegetable oils enables changing the position of palmitic acid to the sn-2 position. Clinical studies in preterm and term infants, as well as preclinical animal model studies, show that enrichment of infant formula with sn-2 Palmitate results in increased fat absorption, reduced calcium soaps formation and stool hardness, increased calcium retention and larger skeletal mineral deposition.\n\nTriglyceride digestion by endogenous lipases leads to hydrolysis of fatty acids from the triacylglyceride sn-1,3 positions, to release two fatty acids and one sn-2 monoglyceride into the intestinal lumen. The fatty acids configuration on the triglyceride has a major contribution to the efficacy of this nutrient absorption. While the unsaturated and short chain saturated free fatty acids are well absorbed regardless of their position, the absorption of free long chain saturated fatty acids, i.e. palmitic acid and longer, is relatively low. The main cause for this low absorption is their melting point above body temperature (~630 C), and thus high tendency to create complexes with dietary minerals, such as calcium or magnesium which are secreted into feces leading to loss of both fatty acids (energy) and calcium. These complexes, also known as fatty acids soaps, are insoluble and therefore indigestible and positively related to stool hardness.\n\nLitmanovitz et al. applied the bone speed of sound (SOS) ultrasound technology in a randomized, controlled, double-blind clinical study of bone parameters in term infants and showed that infants fed formula containing sn-2 Palmitate (INFAT®) had higher bone SOS compared to infants fed formula with standard vegetable oil blends at age of 12 weeks. The bone SOS measures for infants fed the sn-2 Palmitate formula were also comparable to those of the group of breast-fed infants \n\nThe intestinal microflora is an essential “organ” which serves numerous important functions, including protection against pathogens and modulation of inflammatory and immune responses, provision of metabolic intermediates and some vitamins, and regulation of intestinal epithelial proliferation and intestinal maturation. Yaron et al. showed that infants fed formula containing sn-2 Palmitate had higher numbers of Lactobacilli and Bifidobacteria after 6 weeks of feeding than infants fed a control formula with standard vegetable oils.\nAnother recent experimental study published by Lu et al. has used the MUC2 deficient mice to address the possible role of milk palmitic acid content and positioning in triglycerides on intestinal inflammation.\n\n\"Stereo-specific positioning of fatty acids in human milk triglycerides involves preferential positioning of the saturated fatty acid palmitic acid (16:0) at the sn-2 position, rather than at the sn-1,3 positions, as is typical of human tissue and plasma lipids, and vegetable oils common in human diets.\" Early infant crying is considered to reflect basic, instinctive responses governed by neurochemical mechanisms similar to those that control feeding and drinking (i.e., spontaneous behaviors).\nTerm infants fed formula with sn-2 Palmitate for the 12 weeks after birth demonstrated lower crying duration during the day and night compared to infants fed a standard vegetable oil\n\n"}
{"id": "41258747", "url": "https://en.wikipedia.org/wiki?curid=41258747", "title": "Serpent (software)", "text": "Serpent (software)\n\nSerpent is a continuous-energy multi-purpose three-dimensional Monte Carlo particle transport code. It is under development at VTT Technical Research Centre of Finland since 2004. Serpent was originally known as Probabilistic Scattering Game (PSG) from 2004 to the first pre-release of Serpent 1 in October 2008. The development of Serpent 2 was started in 2010. The active development of Serpent 1 has been discontinued even though Serpent 2 is not officially released yet. Serpent 2 is however available for registered users of Serpent 1.\n\nSerpent was originally developed to be a simplified neutron transport code for reactor physics applications. Its main focus was on group constant generation with two-dimensional lattice calculations. Burnup calculation capability was included early on. Nowadays Serpent is used in a wide range of applications from the group constant generation to coupled multi-physics applications, fusion neutronics and radiation shielding. In addition to the original neutron transport capabilities, Serpent is able to perform photon transport.\n\n"}
{"id": "49837417", "url": "https://en.wikipedia.org/wiki?curid=49837417", "title": "Shahpurkandi dam project", "text": "Shahpurkandi dam project\n\nLocated in Pathankot district, the project will be constructed on the canal from Shahpurkandi Barrage to Madhopur headworks, downstream of the existing Ranjit Sagar Dam. The water released by Ranjit Sagar Dam shall be utilised for generating power from this project.\n\nThe project comprises seven hydro generating sets located in two power houses 6 nos each of 33 MW and one of 8 MW.\n\nAs per decision of Govt. of Punjab, civil works of this project are to be executed by the Irrigation Deptt. Govt. of Punjab and Electro & Mechanical (E&M) works of the project are to be carried out by the PSPCL. Notification of Award for Electro-Mechanical (E & M) works of 206 MW Shahpurkandi Hydro Electric Project (HEP) on EPC mode basis to M/S BHEL, New Delhi has been issued on 29.01.2014. The Contract agreement was signed on 28.02.2014. Turbine Model Test witness test has been carried out w.e.f. 18.09.2014 to 27.09.2014 at Hydro Lab, BHEL, Bhopal.\n"}
{"id": "285522", "url": "https://en.wikipedia.org/wiki?curid=285522", "title": "Superheating", "text": "Superheating\n\nIn physics, superheating (sometimes referred to as boiling retardation, or boiling delay) is the phenomenon in which a liquid is heated to a temperature higher than its boiling point, without boiling. This is a so-called metastable state or metastate, where boiling might occure at any time, induced by external or internal effects. Superheating is achieved by heating a homogeneous substance in a clean container, free of nucleation sites, while taking care not to disturb the liquid.\n\nWater is said to \"boil\" when bubbles of water vapor grow without bound, bursting at the surface. For a vapor bubble to expand, the temperature must be high enough that the vapor pressure exceeds the ambient pressure (the atmospheric pressure, primarily). Below that temperature, a water vapor bubble will shrink and vanish.\n\nSuperheating is an exception to this simple rule; a liquid is sometimes observed not to boil even though its vapor pressure does exceed the ambient pressure. The cause is an additional force, the surface tension, which suppresses the growth of bubbles.\n\nSurface tension makes the bubble act like a rubber balloon (more precisely, one that is under-inflated so that the rubber is still elastic). The pressure inside is raised slightly by the \"skin\" attempting to contract. For the bubble to expand, the temperature must be raised slightly above the boiling point to generate enough vapor pressure to overcome both surface tension and ambient pressure.\n\nWhat makes superheating so explosive is that a larger bubble is easier to inflate than a small one; just as when blowing up a balloon, the hardest part is getting started. It turns out the excess pressure due to surface tension is inversely proportional to the diameter of the bubble. This means if the largest bubbles in a container are only a few micrometres in diameter, overcoming the surface tension may require exceeding the boiling point by several degrees Celsius. Once a bubble does begin to grow, the pressure due to the surface tension reduces, so it expands explosively. In practice, most containers have scratches or other imperfections which trap pockets of air that provide starting bubbles. But a container of liquid with only microscopic bubbles can superheat dramatically.\n\nSuperheating can occur when an undisturbed container of water is heated in a microwave oven. At the time the container is removed, the lack of nucleation sites prevents boiling, leaving the surface calm. However, once the water is disturbed, some of it violently flashes to steam, potentially spraying boiling water out of the container. The boiling can be triggered by jostling the cup, inserting a stirring device, or adding a substance like instant coffee or sugar. The chance of superheating is greater with smooth containers, because scratches or chips can house small pockets of air, which serve as nucleation points. Superheating is more likely after repeated heating and cooling cycles of an undisturbed container, as when a forgotten coffee cup is re-heated without being removed from a microwave oven. This is due to heating cycles releasing dissolved gases such as oxygen and nitrogen from the solvent. There are ways to prevent superheating in a microwave oven, such as putting a popsicle stick in the glass or using a scratched container.\n\nSuperheating of hydrogen liquid is used in bubble chambers.\n\nThere is a common belief that superheating can occur only in pure substances. This is untrue, as superheating has been observed in coffee and other impure liquids. Impurities do prevent superheating if they introduce nucleation sites (rough areas where gas is trapped); for example, sand tends to suppress superheating in water. Dissolved gas can also provide nucleation sites when it comes out of solution and forms bubbles. However, an impurity such as salt or sugar, dissolved in water to form a homogeneous solution, does not prevent superheating. Other liquids are known to superheat including 2% milk and almond milk.\n\n"}
{"id": "17533699", "url": "https://en.wikipedia.org/wiki?curid=17533699", "title": "Symbion Power", "text": "Symbion Power\n\nSymbion Power is a US power engineering and construction firm that has extensive operations in Iraq and Afghanistan. Since 2005 Symbion has been responsible for the construction of transmission and distribution facilities throughout Iraq. In 2005/2006 the company was awarded $250m of US funded competitively bid reconstruction work. Symbion is rated in the top 10 US firms using IRRF funds.\n\nSymbion Power LLC, an engineering contractor, engages in the design, engineering, procurement, and construction of electrical infrastructure projects. Its projects include transmission or transmission and distribution lines, air and gas insulated substations, and power plants in the United States, Africa, the Middle East, Europe, Asia, and Australia. The company also owns and operates a power plant in Tanzania for the generation of electricity. In addition, it constructs and operates training schools to help develop a skilled local workforce capable of constructing and installing an electrical power infrastructure in northern Iraq; Tanzania; and Kabul, Afghanistan. Symbion Power LLC was founded in 2000 and is based in Washington, District of Columbia with additional offices and subsidiaries in Delaware; Limassol, Cyprus; Cape Town, South Africa; Dubai, United Arab Emirates; Baghdad, Iraq; Erbil, Kurdistan; Port-au-Prince, Haiti; and Dar es Salaam and Morogoro, Tanzania. In December 2015, Symbion Power signed 25-year power purchase agreement (PPA) with Rwanda Energy Group.\n\n"}
{"id": "9651385", "url": "https://en.wikipedia.org/wiki?curid=9651385", "title": "Toledo do Brasil Balanças", "text": "Toledo do Brasil Balanças\n\nToledo do Brasil is a manufacturer and distributor of weighing scales for professional use, based in São Bernardo do Campo, Brazil.\n\nIt is affiliated with Mettler Toledo.\n\n"}
{"id": "24288498", "url": "https://en.wikipedia.org/wiki?curid=24288498", "title": "Wadja Egnankou", "text": "Wadja Egnankou\n\nWadja Egnankou is a scientist from Côte d'Ivoire, a researcher at the University of Abidjan. He received the Goldman Environmental Prize in 1992 for his efforts to protect the mangrove forests of the country.\n"}
{"id": "8719690", "url": "https://en.wikipedia.org/wiki?curid=8719690", "title": "Walter M. Urbain", "text": "Walter M. Urbain\n\nWalter Mathias Urbain (1910 – January 15, 2002) is a distinguished American scientist who helped pioneer food science through innovative research during World War II. His contributions include new patents and methodologies in food engineering, irradiation, and meat science. Because of his contributions, the US government, especially the US Army and the former US Atomic Energy Commission, developed national programs on food irradiation during the 1950s which led to the development of international standards and the application of his methods on a global basis.\n\nDr. Urbain graduated Phi Beta Kappa from the University of Chicago in 1934, earning his Doctorate (PhD) in Chemistry. After obtaining his degree, he joined Swift & Company where he became head (Director) of Engineering Research. In 1966 he joined Michigan State University where he taught physics and chemistry, and also focused his research on food irradiation.\n\nDr. Urbain's breakthrough findings led him to the forefront of food science, consulting on projects for the US government and acting as lead consultant to the International Atomic Energy Agency (IAEA) in Vienna, Austria. Dr. Urbain also helped kick start the food irradiation program at the United Nations, namely the Food and Agriculture Organization (FAO) and World Health Organization (WHO). Dr Urbain became Director of the first FAO/IAEA International Training Course on Food Irradiation and Techniques which led to the creation of an International Training Manual on Food Irradiation Technology and Techniques in 1968 as well as an international certification program labelled the \"Food Irradiation Process Control School\".\n\nThe success of food irradiation today is a result of Walter Urbain's dedication and determination on this subject. Dr. Urbain's many honors include, the \"Outstanding Civilian Service Award\" from the U.S. Army in 1962 as well as the \"Industrial Achievement Award\" from The Institute of Food Technologists in 1963 and the \"International Food Engineering Award\" in 1976.\n\nOutside of his duties, Dr. Urbain played a significant role in several scientific organizations including the American Chemical Society, the American Society of Agricultural and Biological Engineers (American Society of Agricultural Engineers: 1907-2005), and the Institute of Food Technologists where he was a Chartered Member. Numerous patents are registered in his name; he published 36 scientific papers, wrote hundreds of articles and published two books on food irradiation.\n\nUrbain died in Dallas, Texas on January 15, 2002.\n\n\n\n"}
{"id": "89547", "url": "https://en.wikipedia.org/wiki?curid=89547", "title": "Water vapor", "text": "Water vapor\n\nWater vapor, water vapour or aqueous vapor is the gaseous phase of water. It is one state of water within the hydrosphere. Water vapor can be produced from the evaporation or boiling of liquid water or from the sublimation of ice. Unlike other forms of water, water vapor is invisible. Under typical atmospheric conditions, water vapor is continuously generated by evaporation and removed by condensation. It is less dense than air and triggers convection currents that can lead to clouds.\n\nBeing a component of Earth's hydrosphere and hydrologic cycle, it is particularly abundant in Earth's atmosphere where it is also a potent greenhouse gas along with other gases such as carbon dioxide and methane. Use of water vapor, as steam, has been important to humans for cooking and as a major component in energy production and transport systems since the industrial revolution.\n\nWater vapor is a relatively common atmospheric constituent, present even in the solar atmosphere as well as every planet in the Solar System and many astronomical objects including natural satellites, comets and even large asteroids. Likewise the detection of extrasolar water vapor would indicate a similar distribution in other planetary systems. Water vapor is significant in that it can be indirect evidence supporting the presence of extraterrestrial liquid water in the case of some planetary mass objects.\n\nWhenever a water molecule leaves a surface and diffuses into a surrounding gas, it is said to have evaporated. Each individual water molecule which transitions between a more associated (liquid) and a less associated (vapor/gas) state does so through the absorption or release of kinetic energy. The aggregate measurement of this kinetic energy transfer is defined as thermal energy and occurs only when there is differential in the temperature of the water molecules. Liquid water that becomes water vapor takes a parcel of heat with it, in a process called evaporative cooling. The amount of water vapor in the air determines how frequently molecules will return to the surface. When a net evaporation occurs, the body of water will undergo a net cooling directly related to the loss of water.\n\nIn the US, the National Weather Service measures the actual rate of evaporation from a standardized \"pan\" open water surface outdoors, at various locations nationwide. Others do likewise around the world. The US data is collected and compiled into an annual evaporation map. The measurements range from under 30 to over 120 inches per year. Formulas can be used for calculating the rate of evaporation from a water surface such as a swimming pool. In some countries, the evaporation rate far exceeds the precipitation rate.\n\nEvaporative cooling is restricted by atmospheric conditions. Humidity is the amount of water vapor in the air. The vapor content of air is measured with devices known as hygrometers. The measurements are usually expressed as specific humidity or percent relative humidity. The temperatures of the atmosphere and the water surface determine the equilibrium vapor pressure; 100% relative humidity occurs when the partial pressure of water vapor is equal to the equilibrium vapor pressure. This condition is often referred to as complete saturation. Humidity ranges from 0 gram per cubic metre in dry air to 30 grams per cubic metre (0.03 ounce per cubic foot) when the vapor is saturated at 30 °C.\n\nSublimation is when water molecules directly leave the surface of ice without first becoming liquid water. Sublimation accounts for the slow mid-winter disappearance of ice and snow at temperatures too low to cause melting. Antarctica shows this effect to a unique degree because it is by far the continent with the lowest rate of precipitation on Earth. As a result, there are large areas where millennial layers of snow have sublimed, leaving behind whatever non-volatile materials they had contained. This is extremely valuable to certain scientific disciplines, a dramatic example being the collection of meteorites that are left exposed in unparalleled numbers and excellent states of preservation.\n\nSublimation is important in the preparation of certain classes of biological specimens for scanning electron microscopy. Typically the specimens are prepared by cryofixation and freeze-fracture, after which the broken surface is freeze-etched, being eroded by exposure to vacuum till it shows the required level of detail. This technique can display protein molecules, organelle structures and lipid bilayers with very low degrees of distortion.\n\nWater vapor will only condense onto another surface when that surface is cooler than the dew point temperature, or when the water vapor equilibrium in air has been exceeded. When water vapor condenses onto a surface, a net warming occurs on that surface. The water molecule brings heat energy with it. In turn, the temperature of the atmosphere drops slightly. In the atmosphere, condensation produces clouds, fog and precipitation (usually only when facilitated by cloud condensation nuclei). The dew point of an air parcel is the temperature to which it must cool before water vapor in the air begins to condense concluding water vapor is a type of water or rain.\n\nAlso, a net condensation of water vapor occurs on surfaces when the temperature of the surface is at or below the dew point temperature of the atmosphere. Deposition is a phase transition separate from condensation which leads to the direct formation of ice from water vapor. Frost and snow are examples of deposition.\n\nA number of chemical reactions have water as a product. If the reactions take place at temperatures higher than the dew point of the surrounding air the water will be formed as vapor and increase the local humidity, if below the dew point local condensation will occur. Typical reactions that result in water formation are the burning of hydrogen or hydrocarbons in air or other oxygen containing gas mixtures, or as a result of reactions with oxidizers.\n\nIn a similar fashion other chemical or physical reactions can take place in the presence of water vapor resulting in new chemicals forming such as rust on iron or steel, polymerization occurring (certain polyurethane foams and cyanoacrylate glues cure with exposure to atmospheric humidity) or forms changing such as where anhydrous chemicals may absorb enough vapor to form a crystalline structure or alter an existing one, sometimes resulting in characteristic color changes that can be used for measurement.\n\nMeasuring the quantity of water vapor in a medium can be done directly or remotely with varying degrees of accuracy. Remote methods such electromagnetic absorption are possible from satellites above planetary atmospheres. Direct methods may use electronic transducers, moistened thermometers or hygroscopic materials measuring changes in physical properties or dimensions.\n\nWater vapor is lighter or less dense than dry air. At equivalent temperatures it is buoyant with respect to dry air, whereby the density of dry air at standard temperature and pressure (273.15 K, 101.325 kPa) is 1.27 g/L and water vapor at standard temperature has a vapor pressure of 0.6 kPa and the much lower density of 4.85 mg/L.\n\nWater vapor and dry air density calculations at 0 °C:\n\nAt the same temperature, a column of dry air will be denser or heavier than a column of air containing any water vapor, the molar mass of diatomic nitrogen and diatomic oxygen both being greater than the molar mass of water. Thus, any volume of dry air will sink if placed in a larger volume of moist air. Also, a volume of moist air will rise or be buoyant if placed in a larger region of dry air. As the temperature rises the proportion of water vapor in the air increases, and its buoyancy will increase. The increase in buoyancy can have a significant atmospheric impact, giving rise to powerful, moisture rich, upward air currents when the air temperature and sea temperature reaches 25 °C or above. This phenomenon provides a significant driving force for cyclonic and anticyclonic weather systems (typhoons and hurricanes).\n\nWater vapor is a by-product of respiration in plants and animals. Its contribution to the pressure, increases as its concentration increases. Its partial pressure contribution to air pressure increases, lowering the partial pressure contribution of the other atmospheric gases (Dalton's Law). The total air pressure must remain constant. The presence of water vapor in the air naturally dilutes or displaces the other air components as its concentration increases.\n\nThis can have an effect on respiration. In very warm air (35 °C) the proportion of water vapor is large enough to give rise to the stuffiness that can be experienced in humid jungle conditions or in poorly ventilated buildings.\n\nWater vapor has lower density than that of air and is therefore buoyant in air but has lower vapor pressure than that of air. When water vapor is used as a lifting gas by a thermal airship the water vapor is heated to form steam so that its vapor pressure is greater than the surrounding air pressure in order to maintain the shape of a theoretical \"steam balloon\", which yields approximately 60% the lift of helium and twice that of hot air.\n\nThe amount of water vapor in an atmosphere is constrained by the restrictions of partial pressures and temperature. Dew point temperature and relative humidity act as guidelines for the process of water vapor in the water cycle. Energy input, such as sunlight, can trigger more evaporation on an ocean surface or more sublimation on a chunk of ice on top of a mountain. The \"balance\" between condensation and evaporation gives the quantity called vapor partial pressure.\n\nThe maximum partial pressure (\"saturation pressure\") of water vapor in air varies with temperature of the air and water vapor mixture. A variety of empirical formulas exist for this quantity; the most used reference formula is the Goff-Gratch equation for the SVP over liquid water below zero degree Celsius:\n\nThe formula is valid from about −50 to 102 °C; however there are a very limited number of measurements of the vapor pressure of water over supercooled liquid water. There are a number of other formulae which can be used.\n\nUnder certain conditions, such as when the boiling temperature of water is reached, a net evaporation will always occur during standard atmospheric conditions regardless of the percent of relative humidity. This immediate process will dispel massive amounts of water vapor into a cooler atmosphere.\n\nExhaled air is almost fully at equilibrium with water vapor at the body temperature. In the cold air the exhaled vapor quickly condenses, thus showing up as a fog or mist of water droplets and as condensation or frost on surfaces. Forcibly condensing these water droplets from exhaled breath is the basis of exhaled breath condensate, an evolving medical diagnostic test.\n\nControlling water vapor in air is a key concern in the heating, ventilating, and air-conditioning (HVAC) industry. Thermal comfort depends on the moist air conditions. Non-human comfort situations are called refrigeration, and also are affected by water vapor. For example, many food stores, like supermarkets, utilize open chiller cabinets, or \"food cases\", which can significantly lower the water vapor pressure (lowering humidity). This practice delivers several benefits as well as problems.\n\nGaseous water represents a small but environmentally significant constituent of the atmosphere. The percentage water vapor in surface air varies from 0.01% at -42 °C (-44 °F) to 4.24% when the dew point is 30 °C (86 °F). Approximately 99.13% of it is contained in the troposphere. The condensation of water vapor to the liquid or ice phase is responsible for clouds, rain, snow, and other precipitation, all of which count among the most significant elements of what we experience as weather. Less obviously, the latent heat of vaporization, which is released to the atmosphere whenever condensation occurs, is one of the most important terms in the atmospheric energy budget on both local and global scales. For example, latent heat release in atmospheric convection is directly responsible for powering destructive storms such as tropical cyclones and severe thunderstorms. Water vapor is the most potent greenhouse gas owing to the presence of the hydroxyl bond which strongly absorbs in the infra-red region of the light spectrum.\n\nWater in Earth's atmosphere is not merely below its boiling point (100 °C), but at altitude it goes below its freezing point (0 °C), due to water's highly polar attraction. When combined with its quantity, water vapor then has a relevant dew point and frost point, unlike e. g., carbon dioxide and methane. Water vapor thus has a scale height a fraction of that of the bulk atmosphere, as the water condenses and exits, primarily in the troposphere, the lowest layer of the atmosphere. Carbon dioxide () and methane, being non-polar, rise above water vapor. The absorption and emission of both compounds contribute to Earth's emission to space, and thus the planetary greenhouse effect. This greenhouse forcing is directly observable, via distinct spectral features versus water vapor, and observed to be rising with rising levels. Conversely, adding water vapor at high altitudes has a disproportionate impact, which is why methane (rising, then oxidizing to and two water molecules) and jet traffic have disproportionately high warming effects.\n\nIt is less clear how cloudiness would respond to a warming climate; depending on the nature of the response, clouds could either further amplify or partly mitigate warming from long-lived greenhouse gases.\n\nIn the absence of other greenhouse gases, Earth's water vapor would condense to the surface;<ref name=\"http://www.astronomycafe.net/qadir/q1209.html Astronomy Cafe\">What is the maximum and minimum distance for the Earth that is compatible with life?</ref><ref name=\"http://www.astronomynotes.com/solarsys/s3c.htm Astronomy Notes\">\"for the Earth, the albedo is 0.306 and the distance is 1.000 AU, so the expected temperature is 254 K or -19 C – significantly below the freezing point of water!\"</ref> this has likely happened, possibly more than once. Scientists thus distinguish between non-condensable (driving) and condensable (driven) greenhouse gases, i.e., the above water vapor feedback.\n\nFog and clouds form through condensation around cloud condensation nuclei. In the absence of nuclei, condensation will only occur at much lower temperatures. Under persistent condensation or deposition, cloud droplets or snowflakes form, which precipitate when they reach a critical mass.\n\nThe water content of the atmosphere as a whole is constantly depleted by precipitation. At the same time it is constantly replenished by evaporation, most prominently from seas, lakes, rivers, and moist earth. Other sources of atmospheric water include combustion, respiration, volcanic eruptions, the transpiration of plants, and various other biological and geological processes. The mean global content of water vapor in the atmosphere is roughly sufficient to cover the surface of the planet with a layer of liquid water about 25 mm deep. The mean annual precipitation for the planet is about 1 meter, which implies a rapid turnover of water in the air – on average, the residence time of a water molecule in the troposphere is about 9 to 10 days.\n\nEpisodes of surface geothermal activity, such as volcanic eruptions and geysers, release variable amounts of water vapor into the atmosphere. Such eruptions may be large in human terms, and major explosive eruptions may inject exceptionally large masses of water exceptionally high into the atmosphere, but as a percentage of total atmospheric water, the role of such processes is trivial. The relative concentrations of the various gases emitted by volcanoes varies considerably according to the site and according to the particular event at any one site. However, water vapor is consistently the commonest volcanic gas; as a rule, it comprises more than 60% of total emissions during a subaerial eruption.\n\nAtmospheric water vapor content is expressed using various measures. These include vapor pressure, specific humidity, mixing ratio, dew point temperature, and relative humidity.\n\nBecause water molecules absorb microwaves and other radio wave frequencies, water in the atmosphere attenuates radar signals. In addition, atmospheric water will reflect and refract signals to an extent that depends on whether it is vapor, liquid or solid.\n\nGenerally, radar signals lose strength progressively the farther they travel through the troposphere. Different frequencies attenuate at different rates, such that some components of air are opaque to some frequencies and transparent to others. Radio waves used for broadcasting and other communication experience the same effect.\n\nWater vapor reflects radar to a lesser extent than do water's other two phases. In the form of drops and ice crystals, water acts as a prism, which it does not do as an individual molecule; however, the existence of water vapor in the atmosphere causes the atmosphere to act as a giant prism.\n\nA comparison of GOES-12 satellite images shows the distribution of atmospheric water vapor relative to the oceans, clouds and continents of the Earth. Vapor surrounds the planet but is unevenly distributed. The image loop on the right shows monthly average of water vapor content with the units are given in centimeters, which is the precipitable water or equivalent amount of water that could be produced if all the water vapor in the column were to condense. The lowest amounts of water vapor (0 centimeters) appear in yellow, and the highest amounts (6 centimeters) appear in dark blue. Areas of missing data appear in shades of gray. The maps are based on data collected by the Moderate Resolution Imaging Spectroradiometer (MODIS) sensor on NASA's Aqua satellite. The most noticeable pattern in the time series is the influence of seasonal temperature changes and incoming sunlight on water vapor. In the tropics, a band of extremely humid air wobbles north and south of the equator as the seasons change. This band of humidity is part of the Intertropical Convergence Zone, where the easterly trade winds from each hemisphere converge and produce near-daily thunderstorms and clouds. Farther from the equator, water vapor concentrations are high in the hemisphere experiencing summer and low in the one experiencing winter. Another pattern that shows up in the time series is that water vapor amounts over land areas decrease more in winter months than adjacent ocean areas do. This is largely because air temperatures over land drop more in the winter than temperatures over the ocean. Water vapor condenses more rapidly in colder air.\n\nAs water vapour absorbs light in the visible spectral range, its absorption can be used in spectroscopic applications (such as DOAS) to determine the amount of water vapor in the atmosphere. This is done operationally, e.g. from the GOME spectrometers on ERS and MetOp. The weaker water vapor absorption lines in the blue spectral range and further into the UV up to its dissociation limit around 243 nm are mostly based on quantum mechanical calculations and are only partly confirmed by experiments.\n\nWater vapor plays a key role in lightning production in the atmosphere. From cloud physics, usually clouds are the real generators of static charge as found in Earth's atmosphere. The ability of clouds to hold massive amounts of electrical energy is directly related to the amount of water vapor present in the local system.\n\nThe amount of water vapor directly controls the permittivity of the air. During times of low humidity, static discharge is quick and easy. During times of higher humidity, fewer static discharges occur. Permittivity and capacitance work hand in hand to produce the megawatt outputs of lightning.\n\nAfter a cloud, for instance, has started its way to becoming a lightning generator, atmospheric water vapor acts as a substance (or insulator) that decreases the ability of the cloud to discharge its electrical energy. Over a certain amount of time, if the cloud continues to generate and store more static electricity, the barrier that was created by the atmospheric water vapor will ultimately break down from the stored electrical potential energy. This energy will be released to a local oppositely charged region, in the form of lightning. The strength of each discharge is directly related to the atmospheric permittivity, capacitance, and the source's charge generating ability.\n\nWater vapor is common in the Solar System and by extension, other planetary systems. Its signature has been detected in the atmospheres of the Sun, occurring in sunspots. The presence of water vapor has been detected in the atmospheres of all seven extraterrestrial planets in the solar system, the Earth's Moon, and the moons of other planets, although typically in only trace amounts.\n\nGeological formations such as cryogeysers are thought to exist on the surface of several icy moons ejecting water vapor due to tidal heating and may indicate the presence of substantial quantities of subsurface water. Plumes of water vapor have been detected on Jupiter's moon Europa and are similar to plumes of water vapor detected on Saturn's moon Enceladus. Traces of water vapor have also been detected in the stratosphere of Titan. Water vapor has been found to be a major constituent of the atmosphere of dwarf planet, Ceres, largest object in the asteroid belt The detection was made by using the far-infrared abilities of the Herschel Space Observatory. The finding is unexpected because comets, not asteroids, are typically considered to \"sprout jets and plumes.\" According to one of the scientists, \"The lines are becoming more and more blurred between comets and asteroids.\" Scientists studying Mars hypothesize that if water moves about the planet, it does so as vapor.\n\nThe brilliance of comet tails comes largely from water vapor. On approach to the Sun, the ice many comets carry sublimes to vapor, and this vapor's in-vacuo \"deposition-conversion\" to ice particles reflects light from the Sun. (The trailing vapor is itself invisible.) Knowing a comet's distance from the sun, astronomers may deduce a comet's water content from its brilliance.\n\nWater vapor has also been confirmed outside the Solar System. Spectroscopic analysis of HD 209458 b, an extrasolar planet in the constellation Pegasus, provides the first evidence of atmospheric water vapor beyond the Solar System. A star called CW Leonis was found to have a ring of vast quantities of water vapor circling the aging, massive star. A NASA satellite designed to study chemicals in interstellar gas clouds, made the discovery with an onboard spectrometer. Most likely, \"the water vapor was vaporized from the surfaces of orbiting comets.\" HAT-P-11b a relatively small exoplanet has also been found to possess water vapour.\n\n\n\n"}
{"id": "8438818", "url": "https://en.wikipedia.org/wiki?curid=8438818", "title": "Winter of 1962–63 in the United Kingdom", "text": "Winter of 1962–63 in the United Kingdom\n\nThe winter of 1962–63 (also known as the Big Freeze of 1963) was one of the coldest winters on record in the United Kingdom. Temperatures plummeted and lakes and rivers began to freeze over.\n\nIn the Central England Temperature (CET) record, extending back to 1659, only the winter (defined as the months of December, January and February) of 1683–84 has been significantly colder, with 1739–40 being slightly colder than 1962–63. Over Scotland and Northern Ireland, where data does not extend back to 1740, the winters of 1813–14 and 1878–79 were certainly colder than 1962–63, as was the winter of 1779–80 in Scotland and 1894–95 in Northern Ireland.\n\nThis winter remains the coldest since at least 1895 in all meteorological districts of the United Kingdom except Scotland North, where the two winters of 1978–79 and 2009–10 were marginally colder.\n\nThe beginning of December was very foggy, with London suffering its last great smog before clean air legislation and the reduction in the use of coal fires had their full effect. A short wintry outbreak brought snow to the country on 12–13 December. A very cold easterly set in on 22 December as an anticyclone formed over Scandinavia, drawing cold continental winds from Russia. Over the Christmas period, the Scandinavian High collapsed, but a new one formed near Iceland, bringing northerly winds. Significant snowfall occurred as the air mass moved south and parts of Southern England in particular had heavy snow late on 26 December (Boxing Day) continuing into 27 December. The cold air became firmly established.\n\nOn 29 and 30 December 1962 a blizzard swept across South West England and Wales. Snow drifted to over 20 feet deep in places, driven on by gale force easterly winds, blocking roads and railways. The snow stranded villagers and brought down powerlines. The near-freezing temperatures meant that the snow cover lasted for over two months in some areas. Snow lay to 6 inches depth in Manchester city centre, 9 inches in Wythenshawe, and about 18 inches at Keele University in Staffordshire. By the end of the month, there were snow drifts 8 feet deep in Kent and 15 feet deep in the west.\n\nWith an average temperature of , January 1963 remains the coldest month since January 1814 over Central England, although over Northern England, Scotland and Northern Ireland February 1947 was generally colder, while December 2010 was also colder over Northern Ireland. Much of England and Wales was snow-covered throughout the month. The country started to freeze solid, with temperatures as low as at Achany in Sutherland on the 11th. Freezing fog was a hazard for most of the country.\n\nIn January 1963 the sea froze for a mile out from shore at Herne Bay, Kent. The sea also froze inshore in many places, removing many British inland waterbirds’ usual last resort of finding food in estuaries and shallow sea. The sea froze 4 miles out to sea from Dunkirk, and BBC Television news expressed a fear that the Strait of Dover would freeze across. The upper reaches of the River Thames also froze over, though it did not freeze in Central London, partly due to the hot effluent from two thermal power stations, Battersea and Bankside; the removal of the old multi-arched London Bridge, which had obstructed the river’s free flow, and the addition of the river embankments, made the river less likely to freeze in London than in earlier times (see River Thames frost fairs). On 20 January, 283 workers had to be rescued by RAF helicopters from Fylingdales, where they had been snowbound for several days. The ice was thick enough in some places that people were skating on it, and on 22 January a car was driven across the frozen Thames at Oxford. Icicles hung from many roof gutterings; some of these were over 3 feet long.\n\nStarting on 25 January there was a brief thaw but it only lasted 3 days. \n\nIn February 1963, more snow came. It was also stormy with winds reaching Force 8 on the Beaufort scale (gale-force winds).\n\nA 36-hour blizzard caused heavy drifting snow in most parts of the country. Drifts reached 20 feet in some areas and there were gale-force winds reaching up to 81 mph. On the Isle of Man, wind speeds were recorded at 119 mph.\n\nThe thaw set in during early March; 6 March was the first morning of the year without any frost anywhere in Britain. The temperatures soon soared to and the remaining snow rapidly disappeared.\n\nOne of the most noticeable consequences of the freezing conditions which hit the UK in the winter of 1962–63 was the enormous disruption to the national sporting calendar. For many weeks football matches in both the English and Scottish leagues suffered because of the severe effects of the winter weather. Several ties in the 1962–63 FA Cup were rescheduled ten or more times, to the point that the 5th and 6th rounds originally scheduled for 16 February and 9 March respectively, were finally played on 16 March and 30 March respectively, with the semi-finals rescheduled from 30 March to 27 April. A board known as the \"Pools Panel\" was set up; postponed matches were adjudicated by it, to provide the football pool results. From 8 December to 16 February, Bolton Wanderers played no competitive matches. Both codes of rugby, union and league, suffered much the same fate. All this occurred in the days well before under-soil heating became widespread at major venues. When the thaw arrived, a huge backlog of fixtures had to be hastily dealt with. The Football League season was extended by four weeks from its original finishing point of 27 April, with the final league fixtures (scheduled sports events) taking place one day before the rescheduled FA Cup Final. Some lower-level competitions did not complete their season at all.\n\nNational Hunt horse racing was also badly affected, with 94 meetings cancelled during the freeze. There was no racing in England between 23 December and 7 March inclusive, although a meeting at Ayr in Scotland went ahead on 5 January.\n\nThe cold of the winter of 1962–63 is alluded to in the Dream Academy's 1985 hit single \"Life in a Northern Town\": \"In winter 1963 / It felt like the world would freeze / With John F. Kennedy and the Beatles\". However, the events referenced i.e. the assassination of John F. Kennedy, which took place in November 1963 and Beatlemania in the United Kingdom also initially observed in 1963, mean they took place the following winter of 1963–64.\n\nThe 2017 Christmas special and first episode of series seven of \"Call the Midwife\" were set during the winter of 1962–63. The cold was a factor in several of the episodes' plot points. Actress Jenny Agutter, who plays Sister Julienne, wrote an article in \"The Times\" about her memories of that winter to coincide with the Christmas special.\n\n\nThe oldest instrumental climatic data for Scotland are from Edinburgh in 1764, and for Northern Ireland from Armagh in 1796.\nOver the UK as a whole February 1947 was colder than January 1963. The months of February 1895, January 1881 and February 1855 were probably colder than January 1963 when averaged over the entire UK, though precise data have never been compiled.\n\n\n\n"}
