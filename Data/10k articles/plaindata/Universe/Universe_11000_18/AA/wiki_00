{"id": "1193842", "url": "https://en.wikipedia.org/wiki?curid=1193842", "title": "1990 oil price shock", "text": "1990 oil price shock\n\nThe 1990 oil price shock occurred in response to the Iraqi invasion of Kuwait on August 2, 1990, Saddam Hussein's second invasion of a fellow OPEC member. Lasting only nine months, the price spike was less extreme and of shorter duration than the previous oil crises of 1973–1974 and 1979–1980, but the spike still contributed to the recession of the early 1990s. Average monthly price of oil rose from $17 per barrel in July to $36 per barrel in October. As the U.S.-led coalition experienced military success against Iraqi forces, concerns about long-term supply shortages eased and prices began to fall.\n\nOn August 2, 1990, The Republic of Iraq invaded the State of Kuwait, leading to a 7-month occupation of Kuwait and an eventual U.S.-led military intervention. While Iraq officially claimed Kuwait was stealing its oil via slant drilling, its true motives were more complicated and less clear. At the time of the invasion, Iraq owed Kuwait $14 billion of outstanding debt that Kuwait had loaned it during the 1980–1988 Iran–Iraq War. In addition, Iraq felt Kuwait was overproducing oil, lowering prices and hurting Iraqi oil profits in a time of financial stress.\n\nIn the buildup to the invasion, Iraq and Kuwait had been producing a combined of oil a day. The potential loss of these supplies, coupled with threats to Saudi Arabian oil production, led to a rise in prices from $21 per barrel at the end of July to $28 per barrel on August 6. On the heels of the invasion, prices rose to a peak of $46 per barrel in mid-October.\n\nThe United States' rapid intervention and subsequent military success helped to mitigate the potential risk to future oil supplies, thereby calming the market and restoring confidence. After only nine months, the spike had subsided, although the Kuwaiti oil fires set by retreating Iraqi forces were not completely extinguished until November 1991, and it took years for the two countries' combined production to regain its former level.\n\nThe U.S. Federal Reserve's monetary tightening in 1988 targeted the rapid inflation of the 1980s. By raising interest rates and lowering growth expectations, the Fed hoped to slow and eventually reduce inflationary pressures, creating greater price stability. The August 6 invasion was seen as a direct threat to the price stability the Fed sought. In fact, the Council of Economic Advisors published a consensus estimate that a one-year, 50 percent increase in the price of oil could temporarily raise the price level of the economy by 1 percent and potentially lower real output by the same amount.\n\nDespite the potential for inflation, the U.S. Fed and central banks around the globe decided it would not be necessary to raise interest rates to counteract the rise in oil prices. Rather, the U.S. Federal Reserve decided to maintain interest rates as if the oil price spike were not occurring. This decision to refrain from action stemmed from confidence in the future success of Desert Storm to protect major oil-producing facilities in the Middle East and a will to maintain the long-term credibility of economy policy that had been built up during the 1980s.\n\nTo avoid being accused of inaction in the face of potential economic turbulence, the U.S. revised the Gramm-Rudman-Hollings Balanced Budget Act. Initially, the act prohibited the U.S. from changing budget deficit targets even in the event of a negative shock to the economy. When oil prices rose, revision of this act allowed the U.S. government to adjust its budget for changes in the economy, further mitigating the risk of rising prices. The result was a peak in prices at $46 per barrel in mid-October, followed by a steady decline in prices until 1994.\n\n"}
{"id": "1984978", "url": "https://en.wikipedia.org/wiki?curid=1984978", "title": "Acetogen", "text": "Acetogen\n\nAn acetogen is a microorganism that generates acetate (CHCOO) as an end product of anaerobic respiration or fermentation. However, this term is usually employed in a more narrow sense only to those bacteria and archaea that perform anaerobic respiration and carbon fixation simultaneously through the reductive acetyl coenzyme A (acetyl-CoA) pathway (also known as the Wood-Ljungdahl pathway). These genuine acetogens are also known as \"homoacetogens\" and they can produce acetyl-CoA (and from that, in most cases, acetate as the end product) from two molecules of carbon dioxide (CO) and four molecules of molecular hydrogen (H). This process is known as acetogenesis, and is different from acetate fermentation, although both occur in the absence of molecular oxygen (O) and produce acetate. Although previously thought that only bacteria are acetogens, some archaea can be considered to be acetogens.\n\nAcetogens are found in a variety of habitats, generally those that are anaerobic (lack oxygen). Acetogens can use a variety of compounds as sources of energy and carbon; the best studied form of acetogenic metabolism involves the use of carbon dioxide as a carbon source and hydrogen as an energy source. Carbon dioxide reduction is carried out by the key enzyme acetyl-CoA synthase. Together with methane-forming archaea, acetogens constitute the last limbs in the anaerobic food web that leads to the production of methane from polymers in the absence of oxygen. Acetogens may represent ancestors of the first bioenergetically active cells in evolution.\n\nAcetogens have diverse metabolic roles, which help them thrive in different environments. One of their metabolic products is acetate which is an important nutrient for the host and its inhabiting microbial community, most seen in termite's guts. Acetogens also serve as \"Hydrogen sinks\" in termite's GI tract. Hydrogen gas inhibits biodegradation and acetogens use up these hydrogen gases in the anaerobic environment to favor the biodegradative capacity of the host by reacting hydrogen gas and carbon dioxide to make acetate. Acetogens have the ability to use variety of substrates in an event where another competitor such as a methanogen makes Hydrogen gas a limiting substrate. Acetogens can use and convert alcohols, lactates and fatty acids, which are usually restricted to syntrophs, instead of just carbon dioxide and hydrogen. This enables them to take on the roles of important contributors of food chain such as primary fermenters. Acetogens can work together with methanogens, as exemplified by the conversion of carbohydrates by a Methanosarcina barkeri and coculture of A. woodii. The methanogen takes up acetate to favor the acetogen. Sometimes the interspecies transfer of Hydrogen gas between A. woodii and an H2-consuming methanogen results in hydrogen gas being released from the acetogen instead of going toward acetogenesis by WLP. Acetogens are also one of the contributors to corrosion of steel. Acetobacterium woodii utilize hydrogen gas and CO2 to make the acetate that is used as carbon source for many of the Sulfate-reducing bacteria growing with hydrogen gas and sulfate.\n"}
{"id": "6301567", "url": "https://en.wikipedia.org/wiki?curid=6301567", "title": "Alkalide", "text": "Alkalide\n\nAn alkalide is a chemical compound in which alkali metals are anions (that is, they bear a negative charge). Such species are notable because alkali metals were previously thought to appear in salts only as cations. Alkalide compounds have been synthesized containing a cation of the alkaline earth metal barium.\n\nAlkali metals are well known to form salts. Table salt, or sodium chloride NaCl, illustrates the usual role of an alkali metal such as sodium: its positive charge is balanced by a negatively charged ion in the empirical formula for this ionic compound. The traditional explanation for this phenomenon is that the loss of one electron from elemental sodium to produce a cation with a single positive charge produces a stable closed-shell electron configuration. Sodium was thought to always form singly charged cations until the discovery of alkalides and the same arguments apply to the remainder of the alkali metals.\n\nKnown alkalides include Na, K, Rb, and Cs. These species are called sodide or natride, potasside or kalide, rubidide, and caeside, respectively. “Lithides” and \"francides\", compounds containing Li or Fr, respectively, are not currently known. The known alkalides, first discovered in the 1970s, are of theoretical interest due to their unusual stoichiometry and low ionization potentials. Alkalide species are chemically related to the electrides, salts containing trapped electrons as the \"anions\".\n\nA typical alkalide is the sodium natride salt [Na(2,2,2-crypt)]Na. This salt contains both Na and Na. The cryptand isolates and stabilizes the Na, preventing its reduction by the Na. Dimers of cationic and anionic sodium have also been observed, as has an HNa salt known as \"inverse sodium hydride\".\n\nNormally, alkalides are thermally labile due to the high reactivity of the alkalide anion, which is theoretically able to break most covalent bonds including the C–O bonds in a typical cryptand. The introduction of a special cryptand ligand containing amines instead of ether linkages has allowed the isolation of kalide and natrides that are stable at room temperature.\n"}
{"id": "24428035", "url": "https://en.wikipedia.org/wiki?curid=24428035", "title": "Alpha oxidation", "text": "Alpha oxidation\n\nAlpha-oxidation of phytanic acid is believed to take place entirely within peroxisomes. \n(Propionyl-CoA is released as a result of beta oxidation when the beta carbon is substituted)\n\nEnzymatic deficiency in alpha-oxidation (most frequently in phytanoyl-CoA dioxygenase) leads to Refsum's disease, in which the accumulation of phytanic acid and its derivatives leads to neurological damage. Other disorders of peroxisome biogenesis also prevent alpha-oxidation from occurring.\n\n"}
{"id": "8584125", "url": "https://en.wikipedia.org/wiki?curid=8584125", "title": "Anthropic rock", "text": "Anthropic rock\n\nAnthropic rock is rock that is made, modified and moved by humans. Concrete is the most widely known example of this. The new category has been proposed to recognise that man-made rocks are likely to last for long periods of Earth's future geological time, and will be important in humanity's long-term future.\n\nHistorically, anthropogenic lithogenesis is a new event or process on Earth. For millennia humans dug and built only with natural rock. Archaeologists, during 1998, reported that artificial rock was made in ancient Mesopotamia. The ancient Romans developed and widely used concrete, much of which is intact today. British Victorians were very familiar with the durable mock-rock surface formations used in public parks, constructed of Pulhamite and Coade stone. Concrete as we know it today dates from the development of modern cement in 1756. Worldwide, the preparation of concrete adds at least 0.2 gigatonnes yearly to the atmosphere's CO gas stock and, thereby affects Earth's Greenhouse Effect. In 2007, 7.5–8 cubic kilometers of concrete were created annually by humans.\n\nThe US geologist James Ross Underwood, Jr. advocated a fourth class of rocks to be added to Earth and planetary materials studies which would supplement geology's long-identified igneous, sedimentary and metamorphic groups. His practical proposal for an \"anthropic rocks\" category recognizes the pervading spread of humankind and its industrial products.\n\nFuture macro-engineering of the Earth may involve the total envelopment of the planet with, among other materials, concrete.\nNASA and others have offered many settlement proposals that entail the use of in-situ resources of the Moon and Mars, such as brick, by astronauts.\n\nThe relatively inert nature of rocks has been exploited in many methods to immobilize chemical and/or radioactive wastes; the Australian researcher, A.E. Ringwood, developed a titanate ceramic called Synroc, his acronym for \"synthetic rock\". D.J. Sheppard proposed Sun-orbiting space colonies, interplanetary and interstellar spaceships ought to be manufactured of concrete. There have also been proposals for deep-diving submarines constructed of concrete and concrete ships.\n\nAlan Weisman in \"The World Without Us\" (2007) noted that anthropic rocks of all kinds, among other artifacts, will exist far into our planet's future even should our species disappear \"tomorrow\".\n"}
{"id": "37139601", "url": "https://en.wikipedia.org/wiki?curid=37139601", "title": "Apollo's Fire (book)", "text": "Apollo's Fire (book)\n\nApollo's Fire: Igniting America's Clean Energy Economy is a 2007 book by Washington State Governor Jay Inslee and researcher Bracken Hendricks. Inslee first proposed an Apollo-scale program, designed to galvanize the nation around the urgent goal of solving the environmental and energy crisis, in the \"Seattle Post-Intelligencer\" in 2002. Eventually Inslee co-authored \"Apollo's Fire\", in which he says that through improved Federal policies the United States can wean itself off of its dependence on foreign oil and fossil fuel, create millions of Green-collar worker jobs, and stop global warming. Along these lines, he has been a prominent supporter of the Apollo Alliance.\n\nIn \"Chapter 2: Reinventing the car\", \"Apollo's Fire\" highlights such innovative efforts such as CalCars, founded in 2002 to promote plug-in hybrid electric vehicles (PHEVs), charged by off-peak electricity from renewable energy sources, as a key to addressing oil dependence and global warming worldwide.\n\n\n"}
{"id": "3394375", "url": "https://en.wikipedia.org/wiki?curid=3394375", "title": "Association of Electricity Producers", "text": "Association of Electricity Producers\n\nThe Association of Electricity Producers (AEP) was a trade association for the United Kingdom electricity market. It had approximately 90 members, including electricity generation companies and those offering support services.\n\nThe Association was founded in 1987 as the Association of Independent Electricity Producers, with the initial remit of lobbying the UK government's Department of Energy to remove restrictions on private companies operating within the electricity industry. The idea for the AIEP came from David Andrews who was an engineer promoting small scale Combined Heat and Power - mini- CHP who was frustrated by the blatant attempts of the then CEGB to manipulate the market against mini-CHP. He called the first meeting held at Orchard Partners in London, then also active in promoting CHP, with John Macadam of Orchard Partners and Martin Alder then of Wessex Water and Trevor Dooley of Electrical Review who provided valuable initial publicity. Early on Andrews suggested that a contact, David Porter, who had a small desk top publishing business provide a newsletter - Porter went on to become the Director and Chief Executive.\n\nThis early goal, against the CEGB, was achieved with the passing of the Electricity Act in 1989, and the subsequent full privatisation of the industry (with the exception of nuclear power).\n\nIn 1995, recognising that little remained from which the organisation could be \"independent\", it changed its name to the Association of Electricity Producers. In 1999 it moved to its current offices in London. It used to be based further towards Pall Mall on Waterloo Place.\n\nThe Association employed 12 staff. Its President was Sir Michael Spicer former MP, and David Porter was its Chief Executive from 1991.\n\nThe UK electricity industry is worth £54 million, and directly employs 87,000 people.\n\nIn April 2012, the Association of Electricity Producers merged with the Energy Retail Association and the UK Business Council for Sustainable Energy to become Energy UK.\n\n\n"}
{"id": "335054", "url": "https://en.wikipedia.org/wiki?curid=335054", "title": "Axion", "text": "Axion\n\nThe axion () is a hypothetical elementary particle postulated by the Peccei–Quinn theory in 1977 to resolve the strong CP problem in quantum chromodynamics (QCD). If axions exist and have low mass within a specific range, they are of interest as a possible component of cold dark matter.\n\nAs shown by Gerard 't Hooft, strong interactions of the standard model, QCD, possess a non-trivial vacuum structure that in principle permits violation of the combined symmetries of charge conjugation and parity, collectively known as CP. Together with effects generated by weak interactions, the effective periodic strong CP-violating term, , appears as a Standard Model input – its value is not predicted by the theory, but must be measured. However, large CP-violating interactions originating from QCD would induce a large electric dipole moment (EDM) for the neutron. Experimental constraints on the currently unobserved EDM implies CP violation from QCD must be extremely tiny and thus must itself be extremely small. Since a priori could have any value between 0 and 2\"π\", this presents a “naturalness” problem for the standard model. Why should this parameter find itself so close to 0? (Or, why should QCD find itself CP-preserving?) This question constitutes what is known as the strong CP problem.\n\nOne simple solution exists: If at least one of the quarks of the standard model is massless, becomes unobservable. However, empirical evidence strongly suggests that none of the quarks are massless.\n\nIn 1977, Roberto Peccei and Helen Quinn postulated a more elegant solution to the strong CP problem, the Peccei–Quinn mechanism. The idea is to effectively promote to a field. This is accomplished by adding a new global symmetry (called a Peccei–Quinn symmetry) that becomes spontaneously broken. This results in a new particle, as shown by Frank Wilczek and Steven Weinberg, that fills the role of , naturally relaxing the CP-violation parameter to zero. This hypothesized new particle is called the axion. The original Weinberg–Wilczek axion was ruled out. Current literature discusses the mechanism as the \"invisible axion\", which has two forms: KSVZ (Kim–Shifman–Vainshtein– and DFSZ (Dine–Fischler––\n\nIt had been thought that the invisible axion solves the \nstrong CP problem without being amenable to verification by experiment. Axion models choose coupling that does not appear in any of the prior experiments. The very weakly coupled axion is also very light because axion couplings and mass are proportional. The situation changed when it was shown that a very light axion is overproduced in the early universe and therefore excluded. The critical mass is of order 10 times the electron mass, where axions may account for the dark matter. The axion is thus a dark-matter candidate, as well as a solution to the strong CP problem. Furthermore, in 1983, Pierre Sikivie wrote down the modification of Maxwell's equations from a light stable axion and showed that axions can be detected on Earth by converting them to photons, using a strong magnetic field, the principle of the ADMX. Solar axions may be converted to X-rays, as in CAST. Many experiments are searching laser light for signs of axions.\n\nA mass value between 50 and 1,500 µeV for the axion was reported in a paper published in November 2016 (Borsanyi, S. et al.). The result was calculated by simulating the formation of axions during the post-inflation period on a supercomputer.\n\nIf magnetic monopoles exist then there is a symmetry in Maxwell's equations where the electric and magnetic fields can be rotated into each other with the new fields still satisfying Maxwell's equations. Luca Visinelli showed that the duality symmetry can be carried over to the axion-electromagnetic theory as well. Assuming the existence of both magnetic charges and axions, Maxwell's equations read\n\nIf magnetic monopoles do not exist, then the same equations hold with the density formula_1 and current formula_2 replaced by zero. Incorporating the axion has the effect of rotating the electric and magnetic fields into each other.\n\nwhere the mixing angle formula_3 depends on the coupling constant formula_4 and the axion field strength formula_5\n\nBy plugging the new values for electromagnetic field formula_6 and formula_7 into Maxwell's equations we obtain the axion-modified Maxwell equations above. Incorporating the axion into the electromagnetic theory also gives a new differential equation – the axion law – which is simply the Klein-Gordon Equation (the quantum field theory equation for massive spin-zero particles) with an formula_8 source term.\n\nA term analogous to the one that would be added to Maxwell's equations to account for axions also appears in recent (2008) theoretical models for topological insulators giving an effective axion description of the electrodynamics of these materials. This term leads to several interesting predicted properties including a quantized magnetoelectric effect. Evidence for this effect has recently been given in THz spectroscopy experiments performed at the Johns Hopkins University.\n\nThe Italian PVLAS experiment searches for polarization changes of light propagating in a magnetic field. The concept was first put forward in 1986 by Luciano Maiani, Roberto Petronzio and Emilio Zavattini. A rotation claim in 2006 was excluded by an upgraded setup. An optimized search began in 2014.\n\nAnother technique is so called \"light shining through walls\", where light passes through an intense magnetic field to convert photons into axions, that pass through metal. Experiments by BFRS and a team led by Rizzo ruled out an axion cause. GammeV saw no events in a 2008 PRL. ALPS-I conducted similar runs, setting new constraints in 2010; ALPS-II will run in 2019. OSQAR found no signal, limiting coupling and will continue.\n\nSeveral experiments search for astrophysical axions by the Primakoff effect, which converts axions to photons and vice versa in electromagnetic fields. Axions can be produced in the Sun's core when X-rays scatter in strong electric fields. The CAST solar telescope is underway, and has set limits on coupling to photons and electrons. ADMX searches the galactic dark matter halo for resonant axions with a cold microwave cavity and has excluded optimistic axion models in the 1.9-3.53 μeV range. It is amidst a series of upgrades and is taking new data, including at 4.9-6.2 µeV. Other experiments of this type include HAYSTAC, CULTASK, and ORGAN. HAYSTAC recently completed the first scanning run of a haloscope above 20 µeV.\n\nResonance effects may be evident in Josephson junctions from a supposed high flux of axions from the galactic halo with mass of 0.11 meV and density compared to the implied dark matter density , indicating said axions would not have enough mass to be the sole component of dark matter. The ORGAN experiment plans to conduct a direct test of this result via the haloscope method.\n\nDark matter cryogenic detectors have searched for electron recoils that would indicate axions. CDMS published in 2009 and EDELWEISS set coupling and mass limits in 2013. UORE and XMASS also set limits on solar axions in 2013. XENON100 used a 225-day run to set the best coupling limits to date and exclude some parameters.\n\nAxion-like bosons could have a signature in astrophysical settings. In particular, several recent works have proposed axion-like particles as a solution to the apparent transparency of the Universe to TeV photons. It has also been demonstrated in a few recent works that, in the large magnetic fields threading the atmospheres of compact astrophysical objects (e.g., magnetars), photons will convert much more efficiently. This would in turn give rise to distinct absorption-like features in the spectra detectable by current telescopes. A new promising means is looking for quasi-particle refraction in systems with strong magnetic gradients. In particular, the refraction will lead to beam splitting in the radio light curves of highly magnetized pulsars and allow much greater sensitivities than currently achievable. The International Axion Observatory (IAXO) is a proposed fourth generation helioscope.\n\nAxions may be produced within neutron stars, by nucleon-nucleon bremsstrahlung. The subsequent decay of axions to gamma rays allows constraints on the axion mass to be placed from observations of neutron stars in gamma-rays using the Fermi LAT. From an analysis of four neutron stars, Berenji et al. obtained a 95% CL upper limit on the axion mass of 0.079 eV.\n\nIt was reported in 2014 that evidence for axions may have been detected as a seasonal variation in observed X-ray emission that would be expected from conversion in the Earth's magnetic field of axions streaming from the Sun. Studying 15 years of data by the European Space Agency's XMM-Newton observatory, a research group at Leicester University noticed a seasonal variation for which no conventional explanation could be found. One potential explanation for the variation, described as \"plausible\" by the senior author of the paper, is the known seasonal variation in visibility to XMM-Newton of the sunward magnetosphere in which X-rays may be produced by axions from the Sun's core. This interpretation of the seasonal variation is disputed by two Italian researchers, who identify flaws in the arguments of the Leicester group that are said to rule out an interpretation in terms of axions. Most importantly, the scattering in angle assumed by the Leicester group to be caused by magnetic field gradients during the photon production, necessary to allow the X-rays to enter the detector that cannot point directly at the sun, would dissipate the flux so much that the probability of detection would be negligible.\n\nIn 2013, Christian Beck suggested that axions might be detectable in Josephson junctions; and in 2014, he argued that a signature, consistent with a mass ≈110 μeV, had in fact been observed in several preexisting experiments.\n\nIn 2016 a theoretical team from MIT devised a possible way of detecting axions using a strong magnetic field. The magnetic field need be no stronger than that produced in a MRI scanning machine and it should show a slight wavering variation that is linked to the mass of the axion. The experiment is now being implemented by experimentalists at the university. Another approach being used by the University of Washington uses a strong magnetic field to detect the possible weak conversion of axions to microwaves.\n\nOne theory of axions relevant to cosmology had predicted that they would have no electric charge, a very small mass in the range from 10 to , and very low interaction cross-sections for strong and weak forces. Because of their properties, axions would interact only minimally with ordinary matter. Axions would also change to and from photons in magnetic fields.\n\nIn supersymmetric theories the axion has both a scalar and a fermionic superpartner. The fermionic superpartner of the axion is called the axino, the scalar superpartner is called the saxion or dilaton.\nThey are all bundled up in a chiral superfield.\n\nThe axino has been predicted to be the lightest supersymmetric particle in such a model. In part due to this property, it is considered a candidate for dark matter.\n\nInflation suggests that axions were created abundantly during the Big Bang. Because of a unique coupling to the instanton field of the primordial universe (the \"misalignment mechanism\"), an effective dynamical friction is created during the acquisition of mass following cosmic inflation. This robs all such primordial axions of their kinetic energy.\n\nIf axions have low mass, thus preventing other decay modes (since there's no lighter particles to decay into), theories predict that the universe would be filled with a very cold Bose–Einstein condensate of primordial axions. Hence, axions could plausibly explain the dark matter problem of physical cosmology. Observational studies are underway, but they are not yet sufficiently sensitive to probe the mass regions if they are the solution to the dark matter problem. High mass axions of the kind searched for by Jain and Singh (2007) would not persist in the modern universe. Moreover, if axions exist, scatterings with other particles in the thermal bath of the early universe unavoidably produce a population of hot axions.\n\nLow mass axions could have additional structure at the galactic scale. If they continuously fall into galaxies from the intergalactic medium, they would be denser in \"caustic\" rings, just as the stream of water in a continuously-flowing fountain is thicker at its peak. The gravitational effects of these rings on galactic structure and rotation might then be observable. Other cold dark matter theoretical candidates, such as WIMPs and MACHOs, could also form such rings, but because such candidates are fermionic and thus experience friction or scattering among themselves, the rings would be less pronounced.\n\nAxions would also have stopped interaction with normal matter at a different moment than other more massive dark particles. The lingering effects of this difference could perhaps be calculated and observed astronomically.\n\nJoão G. Rosa and Thomas W. Kephart suggested that axion clouds formed around unstable primordial black holes might initiate a chain of reactions that irradiate electromagnetic waves, allowing their detection. When adjusting the mass of the axions to explain dark matter, the pair discovered that the value would also explain the luminosity and wavelength of fast radio bursts, being a possible origin for both phenomena.\n\n\n"}
{"id": "41552346", "url": "https://en.wikipedia.org/wiki?curid=41552346", "title": "Boston Stone", "text": "Boston Stone\n\nThe Boston Stone is a stone in Boston, Massachusetts. Near —but not on— the Freedom Trail, it is a minor tourist attraction.\n\nThe stone, a flattened sphere about in diameter, hollowed out on one side, is embedded in the foundation of a building on Marshall Street (a narrow alley named for Thomas Marshall, captain of a ferry to Charlestown in the late 17th century) in the Blackstone Block Historic District. Below the stone is a plinth inscribed \"Boston Stone 1737\". There is no plaque and no further elucidation, and the Boston Stone has no official status.\n\nThe stone was originally a millstone used for grinding paint pigments. It was imported from England in 1635 or 1700 by the painter Tom Childs. in 1836, the stone was recovered while the foundations for the present building were being dug, and was placed in the brick wall in the building that still stands today.\n\nAccording to popular legend and some sources, the stone is the geographic center of Boston, as it was used in colonial times by surveyors as the zero point for outlying milestones showing the distance to Boston, analogous to the London Stone, but this is almost certainly not true – there are no contemporary records indicating this, the 1767 Milestones stretching from Boston to Springfield, Massachusetts are not based on the Boston Stone, and the Old State House would have been a more likely center point for measuring distances to the city. 19th-century advertising for the Marshall House inn describes the original inscribing of the Boston Stone's plinth, and perhaps its attribution as Boston's zero milestone, as an early 19th-century advertising ploy. Nonetheless, the 1921 Rand, McNally guide to the city opined that it was probably set up to provide directions to nearby shops in imitation of the London Stone.\n"}
{"id": "25769951", "url": "https://en.wikipedia.org/wiki?curid=25769951", "title": "Carbonaceous biochemical oxygen demand", "text": "Carbonaceous biochemical oxygen demand\n\nCarbonaceous biochemical oxygen demand or CBOD is a method defined test measured by the depletion of dissolved oxygen by biological organisms in a body of water in which the contribution from nitrogenous bacteria has been suppressed. CBOD is a method defined parameter is widely used as an indication of the pollutant removal from wastewater. It is listed as a conventional pollutant in the U.S. Clean Water Act.\n\nThe CBOD tests have the widest application in measuring waste loadings to treatment plants and in evaluating the CBOD-removal efficiency of such treatment systems. The test measures the molecular oxygen utilized during a specified incubation period for the biochemical degradation of organic material (carbonaceous demand) and the oxygen used to oxidize inorganic material such as sulfides and ferrous iron. It also may measure the amount of oxygen used to oxidize reduced forms of nitrogen (nitrogenous demand) unless their oxidation is prevented by an inhibitor. The seeding and dilution procedures provide an estimate of the CBOD at pH 6.5 to 7.5.\n\nThere are two recognized EPA methods for the measurement of CBOD:\n\nSince the publication of a simple, accurate and direct dissolved oxygen analytical procedure by Winkler, the analysis of dissolved oxygen levels for water has been key to the determination of surface water purity and ecological wellness. The Winkler method is still one of only two analytical techniques used to calibrate oxygen electrode meters; the other procedure is based on oxygen solubility at saturation as per Henry's law. Though many researchers have refined the Winkler analysis to dissolved oxygen levels in the low PPB range, the method does not lend itself to automation.\n\nThe development of an analytical instrument that utilizes the reduction-oxidation (redox) chemistry of oxygen in the presence of dissimilar metal electrodes was introduced during the 1950s. This redox electrode utilized an oxygen-permeable membrane to allow the diffusion of the gas into an electrochemical cell and its concentration determined by polarographic or galvanic electrodes. This analytical method is sensitive and accurate down to levels of ± 0.1 mg/l dissolved oxygen. Calibration of the redox electrode of this membrane electrode still requires the use of the Henry’s law table or the Winkler test for dissolved oxygen.\n\nDuring the last two decades, a new form of electrode was developed based on the luminescence emission of a photo-active chemical compound and the quenching of that emission by oxygen. This quenching photophysics mechanism is described by the Stern–Volmer equation for dissolved oxygen in a solution:\n\n\nThe determination of oxygen concentration by luminescence quenching has a linear response over a broad range of oxygen concentrations and has excellent accuracy and reproducibility. \nThere are two recognized EPA methods for the measurement of dissolved oxygen for CBOD:\n\nBring the sample to ambient room temperature. If pH of sample is <6.5 or >7.5 neutralize the sample to approximately a pH of 7.0 using either sulfuric acid or sodium hydroxide. Aliquots of the neutralized sample are transferred to 300 mL CBOD bottles. These CBOD samples must be at concentrations that will deplete by at least 2 mg/L dissolved oxygen (DO) and have at least 1 mg/L DO left after five days of incubation. Therefore, make enough dilutions (minimum of 3) of the prepared sample to bracket the predicted CBOD.\n\nThe minimum aliquot volume transferred to a 300 mL CBOD bottle will be 3 mL as set by Standard Methods. If a smaller volume is needed to meet the DO depletion requirements, then you must make dilutions to the sample. Add approximately 0.1 g of Nitrification Inhibitor (2-chloro-6-(trichloro-methyl) pyridine) to each 300mL CBOD bottle before adding CBOD dilution water. If the sample is being prepared as a seeded sample, add enough prepared seed to the sample to achieve acceptable dissolved oxygen depletion. Add CBOD Dilution water to each CBOD sample bottle so as to completely fill the bottle with no air spaces or bubbles when the stopper is placed in the bottle.\n\nPlace the dissolved oxygen probe in the bottle and allow the dissolved oxygen meter to come to equilibrium. Allow the meter to come to equilibrium prior to accepting dissolved oxygen value. Record the DO of the sample, stopper the bottle, add DI water to the water seal if needed, cap the water seal, and incubate for 5 days at 20 °C ± 1 °C. Exclude light to avoid growth of algae in the bottles during incubation.\n\nUpon completion of the 5-day incubation± 6 hours, record the DO of the depleted samples with a calibrated DO meter. Allow the meter to come to equilibrium prior to accepting dissolved oxygen value. Calculate the CBODs from the formula below. Only bottles, including seed controls, giving a minimum DO depletion of 2.0 mg/L and a residual DO of at least 1.0 mg/L after 5 days of incubation are considered to produce valid data, because at least 2.0 mg oxygen uptake per L is required to give a meaningful measure of oxygen uptake and at least 1.0 mg/L must remain throughout the test to ensure that insufficient DO does not affect the rate of oxidation of waste constituents.\n\nSeed CBOD Uptake: Typically a 10, 20, and 30 mL sample of seed added to 3 separate CBOD bottles with approximately 0.1 g Nitrification Inhibitor and diluted with CBOD dilution water. Run these QC samples with each batch of seeded CBOD. Calculate the DO uptake per mL of seed added to each bottle using either the slope method or the ratio method.\n\nFor the slope method, plot DO depletion in milligrams per liter versus mLs of seed for all seed control bottles having a 2.0 mg/L depletion and 1.0 minimum residual DO. The plot should present a straight line for which the slope indicates DO depletion per mL of seed. The DO-axis intercept is oxygen depletion caused by the dilution water and should be less than 0.20 mg/L.\n\nFor the ratio method, divide the DO depletion by the volume of seed in mLs for each seed control bottle having a 2.0 mg/L depletion and greater than 1.0 mg/L minimum residual DO and average the results.\n\nThe CBOD test is method defined. Factors such as bacterial seed viability, anoxic stress during the 5 days, and nitrogenous inhibition efficacy will produce method variability between duplicates, analysts and laboratories. Clear quality assurance and quality control limits must be developed to produce valid results.\n\nWastewater by definition may contain pollutants that inhibit bacterial seed metabolisms or are toxic to the seed. In these cases, all samples should be seeded with a known amount of viable bacteria for the MBOD analysis. Toxicity or inhibition is observed in CBOD analysis when the calculated CBOD increases with progressive dilutions of the sample.\n\nSelection of a viable microbial population for the CBOD analysis is key in obtaining valid results. The bacterial population needs both carbonaceous and nitrogenous strains present. Sources of viable bacterial seed can be primary clarifier effluent, non-disinfected secondary clarifier effluent or a commercial seed preparation. Each source should have clear quality assurance and quality control requirements set by the glucose-glutamic acid check sample.\n\nTransfer a known amount of glucose-glutamic acid solution to a CBOD bottle and add sufficient seed to achieve acceptable dissolved oxygen depletion. Fill CBOD bottle with CBOD dilution water and nitrification inhibitor. Determine the 5 Day CBOD. Passing results will have a CBOD of 198 (+ 30.5) mg/L. Run these check samples with each batch of CBOD samples. It is important to realize that glucose-glutamic acid is not intended to be an accuracy check in the test. Its sole purpose is to demonstrate that the seed is viable and metabolizing in the proper range of activity under the conditions of the test.\n\nIn order to reduce a wastewater plants BOD values to meet regulatory compliance requirements, some plant operators try to suppress nitrification when they are not required to meet ammonia limits. This practice usually results in increased effluent toxicity and oxygen demand on the receiving waters. Therefore, to eliminate this situation and because the BOD test is not reflective of effluent quality under nitrifying conditions, the wastewater plant should:\n\nThe results of these analysis can show that CBOD should be utilized for regulatory compliance with wastewater discharge requirements.\n\n\n"}
{"id": "43236523", "url": "https://en.wikipedia.org/wiki?curid=43236523", "title": "China University of Petroleum (Beijing)", "text": "China University of Petroleum (Beijing)\n\nThe China University of Petroleum - Beijing (CUPB) () is a Chinese university with campuses in Beijing and Karamay. It is part of the China University of Petroleum.\n\nChina University of Petroleum-Beijing has two campuses, one in Changping, Beijing and the other in Karamay, Xinjiang. It also has its own graduate school under the administration of Ministry of Education. In 1997, it became of member of the first “Project 211” universities and in 2006 it was nominated as one of the universities to construct National Innovation Platform for Advantageous Disciplines.\n\nBeijing campus is situated in Changping, Beijing, covers an area of 0.331 km. Until January 2017, there are 1436 teachers, in which 237 professors including two academicians of Chinese Academy of Science (Wang Tieguan and Gao Deli) and two academicians of Chinese Academy of Engineering (Shen Zhonghou and Li Gensheng), and 350 associate professors, in Beijing campus. There are 9 professors received the title of Changjiang Distinguished Professor for Changjiang Scholars Program. \n\nBeijing campus currently has 7,714 undergraduate students, 5,538 Master students, 1,142 PhDs and approximately 800 overseas students from 52 countries. \n\nThe Secretary of the Party Committee of China University of Petroleum - Beijing is Shan Honghong (since January 2017) who is the former president of China University of Petroleum (East China). The President of Beijing Campus is Zhang Laibin (since June 2005).\n\nKaramay campus opened in October 2015, with the support from Beijing campus, Ministry of Education, Government on both provincial and municipal levels, as well as the China National Petroleum Corporation. \n\nKaramay campus is located in Karamay, Xinjiang and covers an area of 4.8 km. The Principal of Karamay campus is Zhang Shicheng (since March 2016) who is the Vice President in Beijing campus as well (since September 2008). \n\nKaramay campus is geographically close to several countries in China’s Belt and Road Initiative, namely, Kazakhstan, Uzbekistan, Turkmenistan, Kyrgyzstan, Tajikistan, Afghanistan, Pakistan, and Mongolia. It teaches engineering, economics, finance, business, management, computer science, information technology, TCFL (Teaching Chinese as a Foreign Language).\n\nAt present, the teachers are constituted by two parts: about one half are assigned by Beijing campus, who are either professors or associate professors, and the other half are employed by Karamay campus, of whom two thirds are PhDs. The scope of teaching and research covers areas like Geological Engineering, Petroleum Engineering, Geophysical Prospecting, Well Testing, Chemical Engineering, Mechanics, Storage and Transport, and Electric Engineering, Mathematics, Physics, Chemistry, Materials, Computer, Ideological Politics, Humanities, Foreign Languages, and P.E.. \n\nIn the fall semester of 2016, it enrolled the first batch of 461 undergraduate students from 16 nationwide.\n\n\n"}
{"id": "56098220", "url": "https://en.wikipedia.org/wiki?curid=56098220", "title": "Cryobiology (journal)", "text": "Cryobiology (journal)\n\nCryobiology is a bimonthly peer-reviewed scientific journal covering cryobiology. It was established in 1964 and is published by Elsevier on behalf of the Society for Cryobiology, of which it is the official journal. The editor-in-chief is D.M. Rawson (University of Bedfordshire). According to the \"Journal Citation Reports\", the journal has a 2017 impact factor of 2.050. \n"}
{"id": "749012", "url": "https://en.wikipedia.org/wiki?curid=749012", "title": "Current source", "text": "Current source\n\nA current source is an electronic circuit that delivers or absorbs an electric current which is independent of the voltage across it.\n\nA current source is the dual of a voltage source. The term \"current sink\" is sometimes used for sources fed from a negative voltage supply. Figure 1 shows the schematic symbol for an ideal current source driving a resistive load. There are two types. An \"independent current source\" (or sink) delivers a constant current. A \"dependent current source\" delivers a current which is proportional to some other voltage or current in the circuit.\n\nAn ideal current source generates a current that is independent of the voltage changes across it. An ideal current source is a mathematical model, which real devices can approach very closely. If the current through an ideal current source can be specified independently of any other variable in a circuit, it is called an \"independent\" current source. Conversely, if the current through an ideal current source is determined by some other voltage or current in a circuit, it is called a dependent or controlled current source. Symbols for these sources are shown in Figure 2.\n\nThe internal resistance of an ideal current source is infinite. An independent current source with zero current is identical to an ideal open circuit. The voltage across an ideal current source is completely determined by the circuit it is connected to. When connected to a short circuit, there is zero voltage and thus zero power delivered. When connected to a load resistance, the voltage across the source approaches infinity as the load resistance approaches infinity (an open circuit). \n\nNo physical current source is ideal. For example, no physical current source can operate when applied to an open circuit. There are two characteristics that define a current source in real life. One is its internal resistance and the other is its compliance voltage. The compliance voltage is the maximum voltage that the current source can supply to a load. Over a given load range, it is possible for some types of real current sources to exhibit nearly infinite internal resistance. However, when the current source reaches its compliance voltage, it abruptly stops being a current source.\n\nIn circuit analysis, a current source having finite internal resistance is modeled by placing the value of that resistance across an ideal current source (the Norton equivalent circuit). However, this model is only useful when a current source is operating within its compliance voltage.\n\nThe simplest non-ideal current source consists of a voltage source in series with a resistor. The amount of current available from such a source is given by the ratio of the voltage across the voltage source to the resistance of the resistor (Ohm's law; I = V/R). This value of current will only be delivered to a load with zero voltage drop across its terminals (a short circuit, an uncharged capacitor, a charged inductor, a virtual ground circuit, etc.) The current delivered to a load with nonzero voltage (drop) across its terminals (a linear or nonlinear resistor with a finite resistance, a charged capacitor, an uncharged inductor, a voltage source, etc.) will always be different. It is given by the ratio of the voltage drop across the resistor (the difference between the exciting voltage and the voltage across the load) to its resistance. For a nearly ideal current source, the value of the resistor should be very large but this implies that, for a specified current, the voltage source must be very large (in the limit as the resistance and the voltage go to infinity, the current source will become ideal and the current will not depend at all on the voltage across the load). Thus, efficiency is low (due to power loss in the resistor) and it is usually impractical to construct a 'good' current source this way. Nonetheless, it is often the case that such a circuit will provide adequate performance when the specified current and load resistance are small. For example, a 5 V voltage source in series with a 4.7 kilohm resistor will provide an \"approximately\" constant current of to a load resistance in the range of 50 to 450 ohm.\n\nA Van de Graaff generator is an example of such a high voltage current source. It behaves as an almost constant current source because of its very high output voltage coupled with its very high output resistance and so it supplies the same few microamperes at any output voltage up to hundreds of thousands of volts (or even tens of megavolts) for large laboratory versions.\n\nIn these circuits the output current is not monitored and controlled by means of negative feedback.\n\nThey are implemented by active electronic components (transistors) having current-stable nonlinear output characteristic when driven by steady input quantity (current or voltage). These circuits behave as dynamic resistors changing their present resistance to compensate current variations. For example, if the load increases its resistance, the transistor decreases its present output resistance (and \"vice versa\") to keep up a constant total resistance in the circuit.\n\nActive current sources have many important applications in electronic circuits. They are often used in place of ohmic resistors in analog integrated circuits (e.g., a differential amplifier) to generate a current that depends slightly on the voltage across the load.\n\nThe common emitter configuration driven by a constant input current or voltage and common source (common cathode) driven by a constant voltage naturally behave as current sources (or sinks) because the output impedance of these devices is naturally high. The output part of the simple current mirror is an example of such a current source widely used in integrated circuits. The common base, common gate and common grid configurations can serve as constant current sources as well.\n\nA JFET can be made to act as a current source by tying its gate to its source. The current then flowing is the of the FET. These can be purchased with this connection already made and in this case the devices are called current regulator diodes or constant current diodes or current limiting diodes (CLD). An enhancement mode N channel MOSFET can be used in the circuits listed below.\n\nAn example: bootstrapped current source.\n\nThe simple resistor passive current source is ideal only when the voltage across it is 0; so voltage compensation by applying parallel negative feedback might be considered to improve the source. Operational amplifiers with feedback effectively work to minimise the voltage across their inputs. This results in making the inverting input a virtual ground, with the current running through the feedback, or load, and the passive current source. The input voltage source, the resistor, and the op-amp constitutes an \"ideal\" current source with value, . The op-amp voltage-to-current converter in Figure 3, a transimpedance amplifier and an op-amp inverting amplifier are typical implementations of this idea.\n\nThe floating load is a serious disadvantage of this circuit solution.\n\nA typical example are Howland current source and its derivative Deboo integrator. In the last example (Fig. 1), the Howland current source consists of an input voltage source, , a positive resistor, R, a load (the capacitor, C, acting as impedance ) and a negative impedance converter INIC ( and the op-amp). The input voltage source and the resistor R constitute an imperfect current source passing current, through the load (Fig. 3 in the source). The INIC acts as a second current source passing \"helping\" current, , through the load. As a result, the total current flowing through the load is constant and the circuit impedance seen by the input source is increased. However the Howland current source isn't widely used because it requires the four resistors to be perfectly matched, and its impedance drops at high frequencies.\n\nThe grounded load is an advantage of this circuit solution.\n\nThey are implemented as a voltage follower with series negative feedback driven by a constant input voltage source (i.e., a \"negative feedback voltage stabilizer\"). The voltage follower is loaded by a constant (current sensing) resistor acting as a simple current-to-voltage converter connected in the feedback loop. The external load of this current source is connected somewhere in the path of the current supplying the current sensing resistor but out of the feedback loop.\n\nThe voltage follower adjusts its output current flowing through the load so that to make the voltage drop across the current sensing resistor R equal to the constant input voltage . Thus the voltage stabilizer keeps up a constant voltage drop across a constant resistor; so, a constant current flows through the resistor and respectively through the load.\n\nIf the input voltage varies, this arrangement will act as a voltage-to-current converter (voltage-controlled current source, VCCS); it can be thought as a reversed (by means of negative feedback) current-to-voltage converter. The resistance R determines the transfer ratio (transconductance).\n\nCurrent sources implemented as circuits with series negative feedback have the disadvantage that the voltage drop across the current sensing resistor decreases the maximal voltage across the load (the \"compliance voltage\").\n\nThe simplest constant-current source or sink is formed from one component: a JFET with its gate attached to its source. Once the drain-source voltage reaches a certain minimum value, the JFET enters saturation where current is approximately constant. This configuration is known as a constant-current diode, as it behaves much like a dual to the constant voltage diode (Zener diode) used in simple voltage sources.\n\nDue to the large variability in saturation current of JFETs, it is common to also include a source resistor (shown in the adjacent image) which allows the current to be tuned down to a desired value.\n\nIn this bipolar junction transistor (BJT) implementation (Figure 4) of the general idea above, a \"Zener voltage stabilizer\" (R1 and DZ1) drives an \"emitter follower\" (Q1) loaded by a \"constant emitter resistor\" (R2) sensing the load current. The external (floating) load of this current source is connected to the collector so that almost the same current flows through it and the emitter resistor (they can be thought of as connected in series). The transistor, Q1, adjusts the output (collector) current so as to keep the voltage drop across the constant emitter resistor, R2, almost equal to the relatively constant voltage drop across the Zener diode, DZ1. As a result, the output current is almost constant even if the load resistance and/or voltage vary. The operation of the circuit is considered in details below.\n\nA Zener diode, when reverse biased (as shown in the circuit) has a constant voltage drop across it irrespective of the current flowing through it. Thus, as long as the Zener current () is above a certain level (called holding current), the voltage across the Zener diode () will be constant. Resistor, R1, supplies the Zener current and the base current () of NPN transistor (Q1). The constant Zener voltage is applied across the base of Q1 and emitter resistor, R2.\n\nVoltage across R2 () is given by , where is the base-emitter drop of Q1. The emitter current of Q1 which is also the current through R2 is given by\n\nSince is constant and is also (approximately) constant for a given temperature, it follows that is constant and hence is also constant. Due to transistor action, emitter current, , is very nearly equal to the collector current, , of the transistor (which in turn, is the current through the load). Thus, the load current is constant (neglecting the output resistance of the transistor due to the Early effect) and the circuit operates as a constant current source. As long as the temperature remains constant (or doesn't vary much), the load current will be independent of the supply voltage, R1 and the transistor's gain. R2 allows the load current to be set at any desirable value and is calculated by\n\nwhere is typically 0.65 V for a silicon device.\n\n( is also the emitter current and is assumed to be the same as the collector or required load current, provided is sufficiently large). Resistance, , at resistor, R1, is calculated as\n\nwhere = 1.2 to 2 (so that is low enough to ensure adequate ),\n\nand is the lowest acceptable current gain for the particular transistor type being used.\n\nThe Zener diode can be replaced by any other diode; e.g., a light-emitting diode LED1 as shown in Figure 5. The LED voltage drop () is now used to derive the constant voltage and also has the additional advantage of tracking (compensating) changes due to temperature. is calculated as\n\nand as\n\nTemperature changes will change the output current delivered by the circuit of Figure 4 because is sensitive to temperature. Temperature dependence can be compensated using the circuit of Figure 6 that includes a standard diode, D, (of the same semiconductor material as the transistor) in series with the Zener diode as shown in the image on the left. The diode drop () tracks the changes due to temperature and thus significantly counteracts temperature dependence of the CCS.\n\nResistance is now calculated as\n\nSince ,\n\nThis method is most effective for Zener diodes rated at 5.6 V or more. For breakdown diodes of less than 5.6 V, the compensating diode is usually not required because the breakdown mechanism is not as temperature dependent as it is in breakdown diodes above this voltage.\n\nSeries negative feedback is also used in the . Negative feedback is a basic feature in some current mirrors using multiple transistors, such as the Widlar current source and the Wilson current source.\n\nOne limitation with the circuits in Figures 5 and 6 is that the thermal compensation is imperfect. In bipolar transistors, as the junction temperature increases the drop (voltage drop from base to emitter) decreases. In the two previous circuits, a decrease in will cause an increase in voltage across the emitter resistor, which in turn will cause an increase in collector current drawn through the load. The end result is that the amount of 'constant' current supplied is at least somewhat dependent on temperature. This effect is mitigated to a large extent, but not completely, by corresponding voltage drops for the diode, D1, in Figure 6, and the LED, LED1, in Figure 5. If the power dissipation in the active device of the CCS is not small and/or insufficient emitter degeneration is used, this can become a non-trivial issue.\n\nImagine in Figure 5, at power up, that the LED has 1 V across it driving the base of the transistor. At room temperature there is about 0.6 V drop across the junction and hence 0.4 V across the emitter resistor, giving an approximate collector (load) current of amps. Now imagine that the power dissipation in the transistor causes it to heat up. This causes the drop (which was 0.6 V at room temperature) to drop to, say, 0.2 V. Now the voltage across the emitter resistor is 0.8 V, twice what it was before the warmup. This means that the collector (load) current is now twice the design value! This is an extreme example of course, but serves to illustrate the issue.\n\nThe circuit to the left overcomes the thermal problem (see also, current limiting). To see how the circuit works, assume the voltage has just been applied at V+. Current runs through R1 to the base of Q1, turning it on and causing current to begin to flow through the load into the collector of Q1. This same load current then flows out of Q1's emitter and consequently through to ground. When this current through to ground is sufficient to cause a voltage drop that is equal to the drop of Q2, Q2 begins to turn on. As Q2 turns on it pulls more current through its collector resistor, R1, which diverts some of the injected current in the base of Q1, causing Q1 to conduct less current through the load. This creates a negative feedback loop within the circuit, which keeps the voltage at Q1's emitter almost exactly equal to the drop of Q2. Since Q2 is dissipating very little power compared to Q1 (since all the load current goes through Q1, not Q2), Q2 will not heat up any significant amount and the reference (current setting) voltage across will remain steady at ~0.6 V, or one diode drop above ground, regardless of the thermal changes in the drop of Q1. The circuit is still sensitive to changes in the ambient temperature in which the device operates as the BE voltage drop in Q2 varies slightly with temperature.\n\nThe simple transistor current source from Figure 4 can be improved by inserting the base-emitter junction of the transistor in the feedback loop of an op-amp (Figure 7). Now the op-amp increases its output voltage to compensate for the drop. The circuit is actually a buffered non-inverting amplifier driven by a constant input voltage. It keeps up this constant voltage across the constant sense resistor. As a result, the current flowing through the load is constant as well; it is exactly the Zener voltage divided by the sense resistor. The load can be connected either in the emitter (Figure 7) or in the collector (Figure 4) but in both the cases it is floating as in all the circuits above. The transistor is not needed if the required current doesn't exceed the sourcing ability of the op-amp. The article on current mirror discusses another example of these so-called \"gain-boosted\" current mirrors.\n\nThe general negative feedback arrangement can be implemented by an IC voltage regulator (LM317 voltage regulator on Figure 8). As with the bare emitter follower and the precise op-amp follower above, it keeps up a constant voltage drop (1.25 V) across a constant resistor (1.25 Ω); so, a constant current (1 A) flows through the resistor and the load. The LED is on when the voltage across the load exceeds 1.8 V (the indicator circuit introduces some error). The grounded load is an important advantage of this solution.\n\nNitrogen-filled glass tubes with two electrodes and a calibrated Becquerel (fissions per second) amount of Ra offer a constant number of charge carriers per second for conduction, which determines the maximum current the tube can pass over a voltage range from 25 to 500 V.\n\nMost sources of electrical energy (mains electricity, a battery, etc.) are best modeled as voltage sources. Such sources provide constant voltage, which means that as long as the current drawn from the source is within the source's capabilities, its output voltage stays constant. An ideal voltage source provides no energy when it is loaded by an open circuit (i.e., an infinite impedance), but approaches infinite power and current when the load resistance approaches zero (a short circuit). Such a theoretical device would have a zero ohm output impedance in series with the source. A real-world voltage source has a very low, but non-zero output impedance: often much less than 1 ohm.\n\nConversely, a current source provides a constant current, as long as the load connected to the source terminals has sufficiently low impedance. An ideal current source would provide no energy to a short circuit and approach infinite energy and voltage as the load resistance approaches infinity (an open circuit). An \"ideal\" current source has an infinite output impedance in parallel with the source. A \"real-world\" current source has a very high, but finite output impedance. In the case of transistor current sources, impedances of a few megohms (at DC) are typical.\n\nAn \"ideal\" current source cannot be connected to an \"ideal\" open circuit because this would create the paradox of running a constant, non-zero current (from the current source) through an element with a defined zero current (the open circuit). Also, a current source should not be connected to another current source if their currents differ but this arrangement is frequently used (e.g., in amplifying stages with dynamic load, CMOS circuits, etc.)\n\nSimilarly, an \"ideal\" voltage source cannot be connected to an \"ideal\" short circuit (R = 0), since this would result a similar paradox of finite non-zero voltage across an element with defined zero voltage (the short circuit). Also, a voltage source should not be connected to another voltage source if their voltages differ but again this arrangement is frequently used (e.g., in common base and differential amplifying stages).\n\nContrary, current and voltage sources can be connected to each other without any problems, and this technique is widely used in circuitry (e.g., in cascode circuits, differential amplifier stages with common emitter current source, etc.)\n\nBecause no ideal sources of either variety exist (all real-world examples have finite and non-zero source impedance), any current source can be considered as a voltage source with the \"same\" source impedance and vice versa. These concepts are dealt with by Norton's and Thévenin's theorems.\n\nCharging of capacitor by constant current source and by voltage source is different. Linearity is maintained for constant current source charging of capacitor with time, whereas voltage source charging of capacitor is exponential with time. This particular property of constant current source helps for proper signal conditioning with nearly zero reflection from load.\n\n\n\n"}
{"id": "949096", "url": "https://en.wikipedia.org/wiki?curid=949096", "title": "Geissler tube", "text": "Geissler tube\n\nA Geissler tube is an early gas discharge tube used to demonstrate the principles of electrical glow discharge, similar to modern neon lighting. The tube was invented by the German physicist and glassblower Heinrich Geissler in 1857. It consists of a sealed, partially evacuated glass cylinder of various shapes with a metal electrode at each end, containing rarefied gasses such as neon, argon, or air; mercury vapor or other conductive fluids; or ionizable minerals or metals, such as sodium. When a high voltage is applied between the electrodes, an electrical current flows through the tube. The current dissociates electrons from the gas molecules, creating ions, and when the electrons recombine with the ions, the gas emits light by fluorescence. The color of light emitted is characteristic of the material within the tube, and many different colors and lighting effects can be achieved. The first gas-discharge lamps, Geissler tubes were novelty items, made in many artistic shapes and colors to demonstrate the new science of electricity. In the early 20th century, the technology was commercialized and evolved into neon lighting.\n\nGeissler tubes were mass-produced from the 1880s as novelty and entertainment devices, with various spherical chambers and decorative serpentine paths formed into the glass tube. Some tubes were very elaborate and complex in shape and would contain chambers within an outer casing. A novel effect could be obtained by spinning a glowing tube at high speed with a motor; a disk of color was seen due to persistence of vision. When an operating tube was touched by the hand the shape of the glowing discharge inside often changed, due to the capacitance of the body. \n\nSimple straight Geissler tubes were used in early-20th-century scientific research as high voltage indicators. When a Geissler tube was brought near a source of high voltage alternating current such as a Tesla coil or Ruhmkorff coil, it would light up even without contact with the circuit. They were used to tune the tank circuits of radio transmitters to resonance. Another example of their use was to find nodes of standing waves on transmission lines, such as Lecher lines used to measure the frequency of early radio transmitters. \n\nAnother use around 1900 was as the light source in Pulfrich refractometers.\n\nGeissler tubes are sometimes still used in physics education to demonstrate the principles of gas discharge tubes.\n\nGeissler tubes were the first gas discharge tubes, and have had a large impact on the development of many instruments and devices which depend on electric discharge through gases.\n\nOne of the most significant consequences of Geissler tube technology was the discovery of the electron and the invention of electronic vacuum tubes. By the 1870s better vacuum pumps enabled scientists to evacuate Geissler tubes to a higher vacuum; these were called Crookes tubes after William Crookes. When current was applied, it was found that the glass envelope of these tubes would glow at the end opposite to the cathode. Observing that sharp-edged shadows were cast on the glowing tube wall by obstructions in the tube in front of the cathode, Johann Hittorf realized that the glow was caused by some type of ray travelling in straight lines through the tube from the cathode. These were named cathode rays. In 1897 J. J. Thomson showed that cathode rays consisted of a previously unknown particle, which was named the \"electron\". The technology of controlling electron beams resulted in the invention of the amplifying vacuum tube in 1907, which created the field of \"electronics\" and dominated it for 50 years, and the cathode ray tube which was used in radar and television displays.\n\nSome of the devices which evolved from Geissler tube technology:\n\n\n"}
{"id": "31915555", "url": "https://en.wikipedia.org/wiki?curid=31915555", "title": "Gobius leucomelas", "text": "Gobius leucomelas\n\nGobius leucomelas is a species of goby native to the western Indian Ocean where it is only known from off of the coasts of Eritrea.\n"}
{"id": "8904811", "url": "https://en.wikipedia.org/wiki?curid=8904811", "title": "Gold Standard (carbon offset standard)", "text": "Gold Standard (carbon offset standard)\n\nThe Gold Standard, or Gold Standard for the Global Goals, is a standard and logo certification mark program for non-governmental emission reductions projects in the Clean Development Mechanism (CDM) Joint Implementation (JI) and Voluntary Carbon Market. It is published and administered by the Gold Standard Foundation, a non-profit foundation headquartered in Geneva, Switzerland. It was designed with an intent to ensure that carbon credits are not only real and verifiable but that they make measurable contributions to sustainable development worldwide. Its objective is to add branding, with a logo label, to existing and new Carbon Credits generated by projects which can then be bought and traded by countries that have a binding legal commitment according to the Kyoto Protocol.\n\nThe Gold Standard for CDM (GS CER) was developed in 2003 by World Wide Fund for Nature (WWF), SouthSouthNorth, and Helio International. The Voluntary Gold Standard (GS VER), a methodology for use within the voluntary carbon market, was launched in May 2006. The programs were created following a 12-month consultation period that included workshops and web-based consultation conducted by an independent Standards Advisory Board composed of NGOs, scientists, project developers and government representatives.\n\nThe Gold Standard certification program is open to any non-government, community-based organization, especially those with an interest in the promotion of sustainable development or a focus on climate and energy issues. As of October 2018, more than 80 non-profit organizations internationally had officially endorsed the Gold Standard program.\n\nThe program is administered by the Gold Standard Foundation, a non-profit foundation under that is headquartered in Geneva, Switzerland. It also employs local experts in Brazil, India and South Africa.\n\nIn July 2008 the Gold Standard Version 2.0 was released with sets of guidelines and manuals on the GS requirements, toolkits and other supporting documents to be used by project developers and DOEs. This relegated the previously applicable manuals to Version 1.0. The Version 2.0 also supports Program of Activities (PoA).\n\nIn July 2017, a new version called the Gold Standard for the Global Goals was released, superseding previous Gold Standards.\n\nThe Gold Standard is recognized by carbon market and climate change politics scholars as a prime instance of the group of voluntary standards. Independent empirical evidence for how the Gold Standard is actually performing is rare. Existing evidence suggests that the Gold Standard does not always effectively police companies' misuse of the standard's label (making it difficult to know whether carbon credits associated with the Gold Standard labels have undergone the Gold Standard certification process). As a program certifying emissions trading programs, criticisms of the general practice of emissions trading may also generally apply to the Gold Standard certification program.\n\nTo be eligible for Gold Standard Certification, a project must:\n\n\nStatus of projects that apply for Gold Standard can be tracked on its registry. The Project Developers, Designated Operational Entities (DOEs) (also known as Validators), and Traders can open accounts with the registry. There are various publicly available reports .\n\n\n"}
{"id": "18714480", "url": "https://en.wikipedia.org/wiki?curid=18714480", "title": "Govora Power Station", "text": "Govora Power Station\n\nThe Govora Power Station is a large thermal power plant located in Râmnicu Vâlcea, having 4 generation groups of 50 MW each having a total electricity generation capacity of 200 MW.\n\nCoordinates: \n\n"}
{"id": "1380940", "url": "https://en.wikipedia.org/wiki?curid=1380940", "title": "Green vehicle", "text": "Green vehicle\n\nA green vehicle, or clean vehicle, or eco-friendly vehicle or environmentally friendly vehicle is a road motor vehicle that produces less harmful impacts to the environment than comparable conventional internal combustion engine vehicles running on gasoline or diesel, or one that uses certain alternative fuels. Presently, in some countries the term is used for any vehicle complying or surpassing the more stringent European emission standards (such as Euro6), or California's zero-emissions vehicle standards (such as ZEV, ULEV, SULEV, PZEV), or the low-carbon fuel standards enacted in several countries.\n\nGreen vehicles can be powered by alternative fuels and advanced vehicle technologies and include hybrid electric vehicles, plug-in hybrid electric vehicles, battery electric vehicles, compressed-air vehicles, hydrogen and fuel-cell vehicles, neat ethanol vehicles, flexible-fuel vehicles, natural gas vehicles, clean diesel vehicles, and some sources also include vehicles using blends of biodiesel and ethanol fuel or gasohol. In November 2016, with an EPA-rated fuel economy of 136 miles per gallon gasoline equivalent (mpg-e) (), the 2017 Hyundai Ioniq Electric became the most efficient EPA-certified vehicle considering all fuels and of all years, surpassing the 2014-2016 model year all-electric BMW i3.\n\nSeveral author also include conventional motor vehicles with high fuel economy, as they consider that increasing fuel economy is the most cost-effective way to improve energy efficiency and reduce carbon emissions in the transport sector in the short run. As part of their contribution to sustainable transport, these vehicles reduce air pollution and greenhouse gas emissions, and contribute to energy independence by reducing oil imports.\n\nAn environmental analysis extends beyond just the operating efficiency and emissions. A life-cycle assessment involves production and post-use considerations. A cradle-to-cradle design is more important than a focus on a single factor such as energy efficiency.\n\nCars with similar production of energy costs can obtain, during the life of the car (operational phase), large reductions in energy costs through several measures:\n\n\nGreen vehicles include vehicles types that function fully or partly on alternative energy sources other than fossil fuel or less carbon intensive than gasoline or diesel.\n\nAnother option is the use of alternative fuel composition in conventional fossil fuel-based vehicles, making them function partially on renewable energy sources. Other approaches include personal rapid transit, a public transportation concept that offers automated, on-demand, non-stop transportation on a network of specially built guideways.\n\nExamples of vehicles with reduced petroleum consumption include electric cars, plug-in hybrids and fuel cell-powered hydrogen cars.\n\nElectric cars are typically more efficient than fuel cell-powered vehicles on a Tank-to-wheel basis. They have better fuel economy than conventional internal combustion engine vehicles but are hampered by range or maximum distance attainable before discharging the battery. The electric car batteries are their main cost. They provide a 0% to 99.9% reduction in CO emissions compared to an ICE (gasoline, diesel) vehicle, depending on the source of electricity.\n\nHybrid cars may be partly fossil fuel (or biofuel) powered and partly electric or hydrogen-powered. Most combine an internal combustion engine with an electric engine, though other variations too exist. The internal combustion engine is often either a gasoline or Diesel engine (in rare cases a Stirling engine may even be used). They are more expensive to purchase but cost redemption is achieved in a period of about 5 years due to better fuel economy.\n\nCompressed air cars, stirling-powered vehicles, Liquid nitrogen vehicles are even less polluting than electrical vehicles, as the vehicle and its components can be made more environmentally friendly.\n\nSolar car races are held on a regular basis in order to promote green vehicles and other \"green technology\". These sleek driver-only vehicles can travel long distances at highway speeds using only the electricity generated instantaneously from the sun.\n\nA conventional vehicle can become a greener vehicle by mixing in renewable fuels or using less carbon intensive fossil fuel. Typical gasoline-powered cars can tolerate up to 10% ethanol. Brazil manufactured cars that run on neat ethanol, though there were discontinued. Another available option is a flexible-fuel vehicle which allows any blend of gasoline and ethanol, up to 85% in North America and Europe, and up to 100% in Brazil. Another existing option is to convert a conventional gasoline-powered to allow the alternative use of CNG. Pakistan, Argentina, Brazil, Iran, India, Italy, and China have the largest fleets of natural gas vehicles in the world.\n\nDiesel-powered vehicles can often transition completely to biodiesel, though the fuel is a very strong solvent, which can occasionally damage rubber seals in vehicles built before 1994. More commonly, however, biodiesel causes problems simply because it removes all of the built-up residue in an engine, clogging filters, unless care is taken when switching from dirty fossil-fuel derived diesel to bio-diesel. It is very effective at 'de-coking' the diesel engines combustion chambers and keeping them clean. Biodiesel is the lowest emission fuel available for diesel engines. Diesel engines are the most efficient car internal combustion engines. Biodiesel is the only fuel allowed in some North American national parks because spillages will completely bio-degrade within 21 days. Biodiesel and vegetable oil fuelled, diesel engined vehicles have been declared amongst the greenest in the US \"Tour de Sol\" competition.\n\nThis presents problems, as biofuels can use food resources in order to provide mechanical energy for vehicles. Many experts point to this as a reason for growing food prices, particularly US Bio-ethanol fuel production which has affected maize prices. In order to have a low environmental impact, biofuels should be made only from waste products, or from new sources like algae.\nMultiple companies are offering and developing two, three, and four wheel vehicles combining the characteristics of a bicycle with electric motors. US Federal, State and Local laws do not clearly nor consistently classify these vehicles as bicycles, electric bicycles, motorcycles, electric motorcycles, mopeds, Neighborhood Electric Vehicle, motorised quadricycle or as a car. Some laws have limits on top speeds, power of the motors, range, etc. while others do not.\n\n\nHorse and carriage are just one type of animal propelled vehicle. Once a common form of transportation, they became far less common as cities grew and automobiles took their place. In dense cities, the waste produced by large numbers of transportation animals was a significant health problem. Oftentimes the food is produced for them using diesel powered tractors, and thus there is some environmental impact as a result of their use.\n\nHuman powered transport includes walking, bicycles, velomobiles, row boats, and other environmentally friendly ways of getting around. In addition to the health benefits of the exercise provided, they are far more environmentally friendly than most other options. The only downside is the speed limitations, and how far one can travel before getting exhausted.\n\nVehicle emissions contribute to the increasing concentration of gases linked to climate change. In order of significance, the principal greenhouse gases associated with road transport are carbon dioxide (CO), methane (CH) and nitrous oxide (NO). Road transport is the third largest source of greenhouse gases emitted in the UK, and accounts for over 20% of total emissions, and 33% in the United States. Of the total greenhouse gas emissions from transport, over 85% are due to CO emissions from road vehicles. The transport sector is the fastest growing source of greenhouse gases.\n\nVehicle pollutants have been linked to human ill health including the incidence of respiratory and cardiopulmonary disease and lung cancer. A 1998 report estimated that up to 24,000 people die prematurely each year in the UK as a direct result of air pollution. According to the World Health Organization, up to 13,000 deaths per year among children (aged 0–4 years) across Europe are directly attributable to outdoor pollution. The organization estimates that if pollution levels were returned to within EU limits, more than 5,000 of these lives could be saved each year.\n\nHybrid taxi fleet operators in New York have also reported that reduced fuel consumption saves them thousands of dollars per year.\n\nA study by CNW Marketing Research suggested that the extra energy cost of manufacture, shipping, disposal, and the short lives of some of these types of vehicle (particularly gas-electric hybrid vehicles) outweighs any energy savings made by their using less petroleum during their useful lifespan. This type of argument is the long smokestack argument. Critics of the report note that the study prorated all of Toyota's hybrid research-and-development costs across the relatively small number of Priuses on the road, rather than using the incremental cost of building a vehicle; used for the length of life of a Prius (Toyota offers a warranty on the Prius' hybrid components, including the battery), and calculated that a majority of a car's cradle-to-grave energy gets expended during the vehicle's production, not while it is driven.\nNorwegian Consumer Ombudsman official Bente Øverli stated that \"Cars cannot do anything good for the environment except less damage than others.\" Based on this opinion, Norwegian law severely restricts the use of \"greenwashing\" to market automobiles, strongly prohibiting advertising a vehicle as being environmentally friendly, with large fines issued to violators.\n\nSome studies try to compare environmental impact of electric and petrol vehicles over complete life cycle, including production, operation, and dismantling. \n\nIn general, results differ vastly dependent on the region considered, due to difference in energy sources to produce electricity that fuels electric vehicles. When considering only CO emissions, it is noted that production of electric cars generate about twice as much emissions as that of internal combustion cars. However, emissions of CO during operation are much larger (on average) than during production. For electric cars, emissions caused during operation depend on energy sources used to produce electricity and thus vary a lot geographically. Studies suggest that when taking into account both production and operation, electric cars would cause more emissions in economies where production of electricity is not clean, e.g., it is mostly coal based.. For this reason, some studies found that driving electric cars is less environmentally damaging in western US states than in eastern ones, where less electricity is produced using cleaner sources. Similarly, in countries like India, Australia or China, where large portion of electricity is produced by using coal, driving electric vehicles would cause larger environmental damage than driving petrol vehicles. When justifying use of electric cars over petrol cars, these kinds of studies do not provide sufficiently clear results. Environmental impact is calculated based on fuel mix used to produce electricity that powers electric cars. However, when a gas vehicle is replaced by an equivalent electric vehicle, additional power must be installed in electrical grid. This additional capacity would normally not be based on the same ratios of energy sources (\"clean\" versus fossil fuels) than the current capacity. Only when additional electricity production capacity installed to switch from petrol to electric vehicles would predominantly consist of clean sources, switch to electric vehicles could reduce environmental damage. Another common problem in methodology used in comparative studies is that it only focuses on specific kinds of environmental impact. While some studies focus only on emission of gas pollutants over life cycle or only on greenhouse gas emissions such as CO, comparison should also account for other environmental impacts such as pollutants released otherwise during production and operation or ingredients that can not be effectively recycled. Examples include use of lighter high performing metals, lithium batteries and more rare metals in electric cars, which all have high environmental impact.\n\nA study that also looked at factors other than energy consumption and carbon emissions has suggested that there is no such thing as an environmentally friendly car.\n\nThe use of vehicles with increased fuel efficiency is usually considered positive in the short term but criticism of any hydrocarbon-based personal transport remains. The Jevons paradox suggests that energy efficiency programs are often counter-productive, even increasing energy consumption in the long run. Many environmental researchers believe that sustainable transport may require a move away from hydrocarbon fuels and from our present automobile and highway paradigm.\n\nThe European Union is promoting the marketing of greener cars via a combination of binding and non-binding measures. As of April 2010, 15 of the 27 member states of the European Union provide tax incentives for electrically chargeable vehicles and some alternative fuel vehicles, which includes all Western European countries except Italy and Luxembourg, plus the Czech Republic and Romania. The incentives consist of tax reductions and exemptions, as well as of bonus payments for buyers of electric cars, plug-in hybrids, hybrid electric vehicles and natural gas vehicles.\n\nThe United States Environmental Protection Agency (EPA) is promoting the marketing of greener cars via the SmartWay program. The SmartWay and SmartWay Elite designation mean that a vehicle is a better environmental performer relative to other vehicles. This US EPA designation is arrived at by taking into account a vehicle's Air Pollution Score and Greenhouse Gas Score. Higher Air Pollution Scores indicate vehicles that emit lower amounts of pollutants that cause smog relative to other vehicles. Higher Greenhouse Gas Scores indicate vehicles that emit lower amounts of carbon dioxide and have improved fuel economy relative to other vehicles.\n\nTo earn the SmartWay designation, a vehicle must earn at least a 6 on the Air Pollution Score and at least a 6 on the Greenhouse Gas Score, but have a combined score of at least 13. SmartWay Elite is given to those vehicles that score 9 or better on both the Greenhouse Gas and Air Pollution Scores.\n\nA Green Vehicle Marketing Alliance, in conjunction with the Oak Ridge National Laboratory (ONRL), periodically meets, and coordinates marketing efforts.\n\nThe Progressive Insurance Automotive X PRIZE (PIAXP) is a set of competitions, programs and events, from the X PRIZE Foundation to \"inspire a new generation of super-efficient vehicles that help break America's addiction to oil and stem the effects of climate change.\" Progressive Insurance is the title sponsor of the prize, the centerpiece of which is the Competition Division, within which a $10 million purse will be divided between the winners of three competitions.\n\nThe essence of each competition is to design, build and race super-efficient vehicles that will achieve 100 MPGe (2.35 liter/100 kilometer) and can be produced for the mass market. Within the Competition Division, there are two vehicle classes: Mainstream and Alternative. The mainstream class has a prize of $5 million. The alternate class has 2 separate prizes of $2.5 million, one for side-by-side seating and one for tandem seating.\n\nSome of the competitors, such as Aptera and Tesla, are already taking deposits for 'green' vehicles from customers.\n\nSeveral automobile magazines, motor vehicle specialized publications and environmental groups publish annual rankings or listings of the best green cars of a given year. The following table presents a selection of the annual top pickings.\n\nDedicated electric and green vehicle motor shows:\n\n\n\n"}
{"id": "11854214", "url": "https://en.wikipedia.org/wiki?curid=11854214", "title": "Heelstone Ditch", "text": "Heelstone Ditch\n\nHeelstone Ditch is a roughly circular feature surrounding the Heelstone at Stonehenge. It is not known if there was an intended relationship between the ditch and the heelstone although it is likely that the stone was in place either before or at the same time as the ditch. It has steep sloping sides which end at a narrow flat base, and is approximately 4 ft (1.2m) deep and 3.5 ft (1.1m) wide. It is some 12 ft (3.7m) from base the base of the Heelstone, with a diameter of roughly 32 ft (9.7m). A broad arcing trench found in 1923 by Lt-Col William Hawley 9 ft (2.7m) wide cuts this ditch from the West, deepening towards the stone. Against the Heelstone Ditch (inside circle) is rammed chalk filled Stonehole 97, whose missing stone is known as Heelstone's twin although it is possible that the stone in Stonehole 97 was moved and is now the stone known as the Heelstone. The ditch was probably dug after the stone in Stonehole 97 was moved but possibly before that.\n\nOnly a small part of the Heelstone Ditch, immediately behind the Highways Agency A344 roadside fence, now remains unexplored or undisturbed. General nature of the ditch and fill described in 1979 by Michael W. Pitts, \"et al.\" , compares well with Hawley's (1923, 1925).\n\n\n"}
{"id": "28831825", "url": "https://en.wikipedia.org/wiki?curid=28831825", "title": "Hornslet wind-turbine collapse", "text": "Hornslet wind-turbine collapse\n\nThe Hornslet Wind Turbine Collapse was a spectacular collapse of a wind turbine on February 22, 2008. It is one of only a few structural collapses that have been captured on film.\n\nThe collapsed wind turbine was a Nordtank NKT 600-180/43 with a maximum generation capacity of 600 kW at Hornslet, Denmark at . It was operated by Syddjurs and inaugurated on December 23, 1996. The rotor of this turbine, which carried in the Danish wind turbine register the designation 570715000000021503, had a diameter of 43 metres. It was mounted on a free-standing steel-tube tower 44.5 metres above ground.\n\nThe collapsed wind turbine was the first and northernmost turbine of Hyacintvej Wind Park, which consists of five similar wind turbines, arranged in a line running south-southeastly - north-northwesterly.\n\nAfter a malfunction from a worn brake mechanism, a service team from Vestas were called. Vestas engineers checked and repaired the wind turbine brake on the morning of February 22, 2008. At the last routine inspection it was noted that the main gear of the turbine was also making unusual noises and a sophisticated endoscopic inspection of the gear was planned, but as result of its high cost it was not undertaken immediately.\n\nAfter repair and several checks of the brake, the turbine was restarted in order to bring it back into normal operation. At this time the wind was very strong. The airbrakes at the tip of the blades were turned on to control the speed of the turbine before it reached operational speed. After its generator was synchronized to the grid a noise from the nacelle prompted an attempt to stop the turbine manually.\nA large crashing sound occurred, possibly as a result of the gear failing, at which point the turbine began to oscillate strongly. The rotor then suddenly stopped but immediately started turning again. The rotor did not at first turn very fast, but it was now impossible to control the speed of rotation.\n\nThe tower was evacuated immediately, the airbrakes of the turbine had failed and as a strong wind blew the turbine started rotating faster and faster quickly reaching a speed far beyond its design tolerances. Service personnel contacted the police who established a security cordon of 400 metres around the turbine. 2.5 hours later, at about 3:20 pm, the blades began to disintegrate. One of the blades hit halfway along the tower which bent in the direction of the wind. The top half of the tower then sheared off at the bend and fell to the ground. The base of the tower remained standing. The debris of the turbine flew 200–500 metres away. No injuries were caused. \n\nThe collapse was filmed from a nearby farmhouse. The film was shown on several TV stations and is available online. The turbine was out of service until June 2008 and was eventually replaced by a new wind turbine of the same design.\n\n\n"}
{"id": "12201787", "url": "https://en.wikipedia.org/wiki?curid=12201787", "title": "Hutchinson metric", "text": "Hutchinson metric\n\nIn mathematics, the Hutchinson metric is a function which measures \"the discrepancy between two images for use in fractal image processing\" and \"can also be applied to describe the similarity between DNA sequences expressed as real or complex genomic signals\".\n\nConsider only nonempty, compact, and finite metric spaces. For a space formula_1, let formula_2 denote the space of Borel probability measures on formula_1, with \n\nthe embedding associating to formula_5 the point measure formula_6. The support formula_7 of a measure in P(X) is the smallest closed subset of measure 1. \n\nIf \n\nis Borel measurable then the induced map \n\nassociates to formula_10 the measure formula_11 defined by\n\nfor all formula_13 Borel in formula_14.\n\nThen the Hutchinson metric is given by\n\nwhere the formula_16 is taken over all real-valued functions \"u\" with Lipschitz constant\nformula_17 \n\nThen formula_18 is an isometric embedding of formula_1 into formula_2, and if \n\nis Lipschitz then \n\nis Lipschitz with the same Lipschitz constant.\n\n"}
{"id": "14946771", "url": "https://en.wikipedia.org/wiki?curid=14946771", "title": "Ikom monoliths", "text": "Ikom monoliths\n\nThe Ikom monoliths are a series of volcanic-stone monoliths from the area of Ikom, Cross River State, Nigeria. They are estimated to have been made between 200 and 1850 AD.\n\nNumbering about 300 in total, the monoliths are between 0.3 and 1.8 metres (1 and 6 feet) high, and are laid out in some 30 circles located around Alok in the Ikom area of Cross River State. Since Cross River was divided, it now belongs to Akwa Ibom State. The monoliths are phallic in form and some feature stylized faces as well as decorative patterns and inscriptions. Although the carvings have not been deciphered, researchers and linguists believe that the inscriptions may represent a form of writing and visual communication.\n\nExposure to extreme weather conditions have put these monoliths at risk of erosion and deterioration. The monoliths are also located in an area where the nearby people do not commonly see their worth as tourist attractions. They were recently added to the World Monuments Fund's list of sites in danger and are being considered for inclusion onto UNESCO's World Heritage Site list.\n\nA medium-sized example of an Ikom monolith with human facial features can be found in the British Museum's collection.\n\n"}
{"id": "18713928", "url": "https://en.wikipedia.org/wiki?curid=18713928", "title": "Ișalnița Power Station", "text": "Ișalnița Power Station\n\nThe Işalniţa Power Station is a large thermal power plant located in Işalniţa, Dolj County having 2 generating groups of 315 MW each for a total of 630MW of installed capacity. The power station formerly had a total of eight generation groups, three of 50 MW, one of 55 MW, two of 100 MW and two groups of 315 MW for a total of 1,035MW but six of these groups were decommissioned thus leaving only the two 315MW units.\n\nA consortium formed by Alstom, Sumitomo and IHI Corporation was commissioned to upgrade the 315 MW group to 345 MW each at a total cost of US$315 million.\n\nThere are plans to add another generating group of 500 MWh at Işalniţa Power Station that will result a total power generating capacity of 1,535 MWh at a cost of US$750 million.\n\nThe tallest chimney of the power station is 200 metres tall.\n"}
{"id": "58959352", "url": "https://en.wikipedia.org/wiki?curid=58959352", "title": "List of tallest monolithic statues", "text": "List of tallest monolithic statues\n\nList of tallest monolithic statues erected around the world.\n\n"}
{"id": "40310", "url": "https://en.wikipedia.org/wiki?curid=40310", "title": "Magnetohydrodynamics", "text": "Magnetohydrodynamics\n\nMagnetohydrodynamics (MHD; also magneto-fluid dynamics or hydro­magnetics) is the study of the magnetic properties and behaviour of electrically conducting fluids. Examples of such magneto­fluids include plasmas, liquid metals, salt water, and electrolytes. The word \"magneto­hydro­dynamics\" is derived from \"magneto-\" meaning magnetic field, \"hydro-\" meaning water, and \"dynamics\" meaning movement. The field of MHD was initiated by Hannes Alfvén, for which he received the Nobel Prize in Physics in 1970.\n\nThe fundamental concept behind MHD is that magnetic fields can induce currents in a moving conductive fluid, which in turn polarizes the fluid and reciprocally changes the magnetic field itself. The set of equations that describe MHD are a combination of the Navier–Stokes equations of fluid dynamics and Maxwell’s equations of electro­magnetism. These differential equations must be solved simultaneously, either analytically or numerically.\n\nThe first recorded use of the word \"magnetohydrodynamics\" is by Hannes Alfvén in 1942:\n\nThe ebbing salty water flowing past London's Waterloo Bridge interacts with the Earth's magnetic field to produce a potential difference between the two river-banks. Michael Faraday called this effect \"magneto-electric induction\" and tried this experiment in 1832 but the current was too small to measure with the equipment at the time, and the river bed contributed to short-circuit the signal. However, by a similar process the voltage induced by the tide in the English Channel was measured in 1851.\n\nThe simplest form of MHD, Ideal MHD, assumes that the fluid has so little resistivity that it can be treated as a perfect conductor. This is the limit of infinite magnetic Reynolds number. In ideal MHD, Lenz's law dictates that the fluid is in a sense \"tied\" to the magnetic field lines. To explain, in ideal MHD a small rope-like volume of fluid surrounding a field line will continue to lie along a magnetic field line,\neven as it is twisted and distorted by fluid flows in the system. This is sometimes referred to as the magnetic field lines being \"frozen\" in the fluid.\nThe connection between magnetic field lines and fluid in ideal MHD fixes the topology of the magnetic field in the fluid—for example, if a set of magnetic field lines are tied into a knot, then they will remain so as long as the fluid/plasma has negligible resistivity. This difficulty in reconnecting magnetic field lines makes it possible to store energy by moving the fluid or the source of the magnetic field. The energy can then become available if the conditions for ideal MHD break down, allowing magnetic reconnection that releases the stored energy from the magnetic field.\n\nThe ideal MHD equations consist of the continuity equation, the Cauchy momentum equation, Ampere's Law neglecting displacement current, and a temperature evolution equation. As with any fluid description to a kinetic system, a closure approximation must be applied to highest moment of the particle distribution equation. This is often accomplished with approximations to the heat flux through a condition of adiabaticity or isothermality.\n\nThe main quantities which characterize the electrically conducting fluid are the bulk plasma velocity field v, the current density J, the mass density \"ρ\", and the plasma pressure \"p\". The flowing electric charge in the plasma is the source of a magnetic field B and electric field E. All quantities generally vary with time \"t\". Vector operator notation will be used, in particular ∇ is gradient, ∇⋅ is divergence, and ∇× is curl.\n\nThe mass continuity equation is\nThe Cauchy momentum equation is\nThe Lorentz force term J×B can be expanded using Ampere's law and the vector calculus identity\n\nto give\nwhere the first term on the right hand side is the magnetic tension force and the second term is the magnetic pressure force.\nThe ideal Ohm's law for a plasma is given by\nFaraday's law is\nThe low-frequency Ampere's law neglects displacement current and is given by\nThe magnetic divergence constraint is\nThe energy equation is given by\nwhere formula_10 is the ratio of specific heats for an adiabatic equation of state. This energy equation is, of course, only applicable in the absence of shocks or heat conduction as it assumes that the entropy of a fluid element does not change.\n\nIdeal MHD is only strictly applicable when:\n\n\nIn an imperfectly conducting fluid the magnetic field can generally move through the fluid following a diffusion law with the resistivity of the plasma serving as a diffusion constant. This means that solutions to the ideal MHD equations are only applicable for a limited time for a region of a given size before diffusion becomes too important to ignore. One can estimate the diffusion time across a solar active region (from collisional resistivity) to be hundreds to thousands of years, much longer than the actual lifetime of a sunspot—so it would seem reasonable to ignore the resistivity. By contrast, a meter-sized volume of seawater has a magnetic diffusion time measured in milliseconds.\n\nEven in physical systems – which are large and conductive enough that simple estimates of the Lundquist number suggest that the resistivity can be ignored – resistivity may still be important: many instabilities exist that can increase the effective resistivity of the plasma by factors of more than a billion. The enhanced resistivity is usually the result of the formation of small scale structure like current sheets or fine scale magnetic turbulence, introducing small spatial scales into the system over which ideal MHD is broken and magnetic diffusion can occur quickly. When this happens, magnetic reconnection may occur in the plasma to release stored magnetic energy as waves, bulk mechanical acceleration of material, particle acceleration, and heat.\n\nMagnetic reconnection in highly conductive systems is important because it concentrates energy in time and space, so that gentle forces applied to a plasma for long periods of time can cause violent explosions and bursts of radiation.\n\nWhen the fluid cannot be considered as completely conductive, but the other conditions for ideal MHD are satisfied, it is possible to use an extended model called resistive MHD. This includes an extra term in Ohm's Law which models the collisional resistivity. Generally MHD computer simulations are at least somewhat resistive because their computational grid introduces a numerical resistivity.\n\nAnother limitation of MHD (and fluid theories in general) is that they depend on the assumption that the plasma is strongly collisional (this is the first criterion listed above), so that the time scale of collisions is shorter than the other characteristic times in the system, and the particle distributions are Maxwellian. This is usually not the case in fusion, space and astrophysical plasmas. When this is not the case, or the interest is in smaller spatial scales, it may be necessary to use a kinetic model which properly accounts for the non-Maxwellian shape of the distribution function. However, because MHD is relatively simple and captures many of the important properties of plasma dynamics it is often qualitatively accurate and is therefore often the first model tried.\n\nEffects which are essentially kinetic and not captured by fluid models include double layers, Landau damping, a wide range of instabilities, chemical separation in space plasmas and electron runaway. In the case of ultra-high intensity laser interactions, the incredibly short timescales of energy deposition mean that hydrodynamic codes fail to capture the essential physics.\n\nIn many MHD systems most of the electric current is compressed into thin nearly-two-dimensional ribbons termed current sheets. These can divide the fluid into magnetic domains, inside of which the currents are relatively weak. Current sheets in\nthe solar corona are thought to be between a few meters and a few kilometers in thickness, which is quite thin compared to the magnetic domains (which are thousands to hundreds of thousands of kilometers across). Another example is in the Earth's magnetosphere, where current sheets separate topologically distinct domains, isolating most of the Earth's ionosphere from the solar wind.\n\nThe wave modes derived using MHD plasma theory are called magnetohydrodynamic waves or MHD waves. In general there are three MHD wave modes:\n\nAll these waves have constant phase velocities for all frequencies, and hence there is no dispersion. At the limits when the angle between the wave propagation vector k and magnetic field B is either 0 (180) or 90 degrees, the wave modes are called:\n\nThe phase velocity depends on the angle between the wave vector k and the magnetic field B. An MHD wave propagating at an arbitrary angle formula_11 with respect to the time independent or bulk field formula_12 will satisfy the dispersion relation\nformula_13where formula_14is the Alfvén speed. This branch corresponds to the shear Alfvén mode. Additionally the dispersion equation givesformula_15whereformula_16is the ideal gas sound speed. The plus branch corresponds to the fast-MHD wave mode and the minus branch corresponds to the slow-MHD wave mode.\n\nThe MHD oscillations will be damped if the fluid is not perfectly conducting but has a finite conductivity, or if viscous effects are present.\n\nMHD waves and oscillations are a popular tool for the remote diagnostics of laboratory and astrophysical plasmas, e.g. the corona of the Sun (Coronal seismology).\n\n\nBeneath the Earth's mantle lies the core, which is made up of two parts: the solid inner core and liquid outer core. Both have significant quantities of iron. The liquid outer core moves in the presence of the magnetic field and eddies are set up into the same due to the Coriolis effect. These eddies develop a magnetic field which boosts Earth's original magnetic field—a process which is self-sustaining and is called the geomagnetic dynamo.\n\nBased on the MHD equations, Glatzmaier and Paul Roberts have made a supercomputer model of the Earth's interior. After running the simulations for thousands of years in virtual time, the changes in Earth's magnetic field can be studied. The simulation results are in good agreement with the observations as the simulations have correctly predicted that the Earth's magnetic field flips every few hundred thousand years. During the flips, the magnetic field does not vanish altogether—it just gets more complex.\nSome monitoring stations have reported that earthquakes are sometimes preceded by a spike in ultra low frequency (ULF) activity. A remarkable example of this occurred before the 1989 Loma Prieta earthquake in California, although a subsequent study indicates that this was little more than a sensor malfunction. On December 9, 2010, geoscientists announced that the DEMETER satellite observed a dramatic increase in ULF radio waves over Haiti in the month before the magnitude 7.0 M 2010 earthquake. Researchers are attempting to learn more about this correlation to find out whether this method can be used as part of an early warning system for earthquakes.\n\nMHD applies to astrophysics, including stars, the interplanetary medium (space between the planets), and possibly within the interstellar medium (space between the stars) and jets. Most astrophysical systems are not in local thermal equilibrium, and therefore require an additional kinematic treatment to describe all the phenomena within the system (see Astrophysical plasma).\n\nSunspots are caused by the Sun's magnetic fields, as Joseph Larmor theorized in 1919. The solar wind is also governed by MHD. The differential solar rotation may be the long-term effect of magnetic drag at the poles of the Sun, an MHD phenomenon due to the Parker spiral shape assumed by the extended magnetic field of the Sun.\n\nPreviously, theories describing the formation of the Sun and planets could not explain how the Sun has 99.87% of the mass, yet only 0.54% of the angular momentum in the solar system. In a closed system such as the cloud of gas and dust from which the Sun was formed, mass and angular momentum are both conserved. That conservation would imply that as the mass concentrated in the center of the cloud to form the Sun, it would spin faster, much like a skater pulling their arms in. The high speed of rotation predicted by early theories would have flung the proto-Sun apart before it could have formed. However, magnetohydrodynamic effects transfer the Sun's angular momentum into the outer solar system, slowing its rotation.\n\nBreakdown of ideal MHD (in the form of magnetic reconnection) is known to be the likely cause of solar flares. The magnetic field in a solar active region over a sunspot can store energy that is released suddenly as a burst of motion, X-rays, and radiation when the main current sheet collapses, reconnecting the field.\n\nMagnetohydrodynamic sensors are used for precision measurements of angular velocities in inertial navigation systems such as in aerospace engineering. Accuracy improves with the size of the sensor. The sensor is capable of surviving in harsh environments.\n\nMHD is related to engineering problems such as plasma confinement, liquid-metal cooling of nuclear reactors, and electromagnetic casting (among others).\n\nA magnetohydrodynamic drive or MHD propulsor is a method for propelling seagoing vessels using only electric and magnetic fields with no moving parts, using magnetohydrodynamics. The working principle involves electrification of the propellant (gas or water) which can then be directed by a magnetic field, pushing the vehicle in the opposite direction. Although some working prototypes exist, MHD drives remain impractical.\n\nThe first prototype of this kind of propulsion was built and tested in 1965 by Steward Way, a professor of mechanical engineering at the University of California, Santa Barbara. Way, on leave from his job at Westinghouse Electric, assigned his senior-year undergraduate students to develop a submarine with this new propulsion system. In the early 1990s, a foundation in Japan (Ship & Ocean Foundation (Minato-ku, Tokyo)) built an experimental boat, the \"Yamato-1\", which used a magnetohydrodynamic drive incorporating a superconductor cooled by liquid helium, and could travel at 15 km/h.\n\nMHD power generation fueled by potassium-seeded coal combustion gas showed potential for more efficient energy conversion (the absence of solid moving parts allows operation at higher temperatures), but failed due to cost-prohibitive technical difficulties. One major engineering problem was the failure of the wall of the primary-coal combustion chamber due to abrasion.\n\nIn microfluidics, MHD is studied as a fluid pump for producing a continuous, nonpulsating flow in a complex microchannel design.\n\nMHD can be implemented in the continuous casting process of metals to suppress instabilities and control the flow.\n\nAn important task in cancer research is developing more precise methods for delivery of medicine to affected areas. One method involves the binding of medicine to biologically compatible magnetic particles (e.g. ferrofluids), which are guided to the target via careful placement of permanent magnets on the external body. Magnetohydrodynamic equations and finite element analysis are used to study the interaction between the magnetic fluid particles in the bloodstream and the external magnetic field.\n\n"}
{"id": "39827212", "url": "https://en.wikipedia.org/wiki?curid=39827212", "title": "Monographiae Biologicae", "text": "Monographiae Biologicae\n\nMonographiae Biologicae () is a scholarly scientific literature review series, consisting of monographs published by Kluwer Academic Publishers, an imprint of Springer Science+Business Media. The series subject area generally covers ecology, zoology, and biology. More specifically, the book series covers the biogeography of continental areas, including whole continents; differentiated stand-alone ecosystems such as islands, island groups, mountains or mountain chains; aquatic or marine ecosystems such as coastal systems, mangroves, coral reefs, and other related ecosystems. Fresh water environments are also included in this series such as major river basins, lakes, and groups of lakes.\n\nTaxonomic studies include the main groups of animals, plants, fungi and the comparative ecology of major biomes.\n\nThe series continues \"Physiologia comparata et oecologia\", ().\n\nThis series is indexed by the following services:\n\n"}
{"id": "30258850", "url": "https://en.wikipedia.org/wiki?curid=30258850", "title": "Montanic acid", "text": "Montanic acid\n\nMontanic acid is a saturated fatty acid isolated and detected mainly in montan wax. It also occurs in beeswax and Chinese wax. Montanic acid ethylene glycol esters and glycerol esters are used as protective layer on fruit skins and coating on foods.\n"}
{"id": "31990026", "url": "https://en.wikipedia.org/wiki?curid=31990026", "title": "Nuclear energy in Kenya", "text": "Nuclear energy in Kenya\n\nIn 2017, the Kenya Nuclear Electrification Board (Kneb) estimated that a 1,000 MW nuclear plant could be operational by 2027 and cost Ksh500-600 billion ($5-$6 billion).\n\nIn September 2010 Former Energy and Petroleum Ministry PS Patrick Nyoike announced that Kenya aims to build a 1,000 MW nuclear power plant between 2017 and 2022. \nThe projected cost using South Korean technology was US$3.5 billion.\nNuclear and renewable sources of energy such as wind, solar and geothermal plants could play a major role in helping Kenya achieve middle income status, as the reduction of carbon emissions becomes a higher priority.\n\nKenya has embarked on a programme to see the country generate 1 GW (1,000 MW) from Nuclear sources between 2020 and 2022. By 2030 Kenya is slated to have installed a capacity of 4 GW of nuclear energy, generating about 19% of Kenya's energy needs. Meaning that nuclear power would be the second largest source of energy in Kenya coming second after geothermal power which is a clean form of energy.\n\nThe Kenya Nuclear Electricity Board is in charge of spearheading this sector in the country.\n\n\n\n"}
{"id": "2127679", "url": "https://en.wikipedia.org/wiki?curid=2127679", "title": "Number density", "text": "Number density\n\nIn physics, astronomy, chemistry, biology and geography, number density (symbol: \"n\" or \"ρ\") is an intensive quantity used to describe the degree of concentration of countable objects (particles, molecules, phonons, cells, galaxies, etc.) in physical space: three-dimensional volumetric number density, two-dimensional areal number density, or one-dimensional line number density. Population density is an example of areal number density. The term number concentration (symbol: \"C\", to avoid confusion with amount of substance \"n\") is sometimes used in chemistry for the same quantity, particularly when comparing with other concentrations.\n\nVolume number density is the number of specified objects per unit volume:\nwhere \"N\" is the total number of objects in a volume \"V\". \n\nHere it is assumed that \"N\" is large enough that rounding of the count to the nearest integer does not introduce much of an error, however \"V\" is chosen to be small enough that the resulting \"n\" does not depend much on the size or shape of the volume \"V\".\n\nIn SI units, number density is measured in m, although cm is often used. However, these units are not quite practical when dealing with atoms or molecules of gases, liquids or solids at room temperature and atmospheric pressure, because the resulting numbers are extremely large (on the order of 10). Using the number density of an ideal gas at and as a yardstick: is often introduced as a unit of number density, for any substances at any conditions (not necessarily limited to an ideal gas at and ).\n\nUsing the number density as a function of spatial coordinates, the total number of objects \"N\" in the entire volume \"V\" can be calculated as\nwhere d\"V\" = d\"x\" d\"y\" d\"z\" is a volume element. If each object possesses the same mass \"m\", the total mass \"m\" of all the objects in the volume \"V\" can be expressed as\n\nSimilar expressions are valid for electric charge or any other extensive quantity associated with countable objects. For example, replacing \"m\" with \"q\" (total charge) and \"m\" with \"q\" (charge of each object) in the above equation will lead to a correct expression for charge.\n\nThe number density of solute molecules in a solvent is sometimes called concentration, although usually concentration is expressed as a number of moles per unit volume (and thus called molar concentration).\n\nFor any substance, the number density can be expressed in terms of its amount concentration \"c\" (in mol/m) as\nwhere is the Avogadro constant. This is still true if the spatial dimension unit, metre, in both \"n\" and \"c\" is consistently replaced by any other spatial dimension unit, e.g. if \"n\" is in cm and \"c\" is in mol/cm, or if \"n\" is in L and \"c\" is in mol/L, etc.\n\nFor atoms or molecules of a well-defined molar mass \"M\" (in kg/mol), the number density can be expressed in terms of their mass density \"ρ\" (in kg/m) as\nNote that the ratio \"M\"/\"N\" is the mass of a single atom or molecule in kg.\n\nThe following table lists common examples of number densities at and , unless otherwise noted.\n\n"}
{"id": "6002168", "url": "https://en.wikipedia.org/wiki?curid=6002168", "title": "P-series fuels", "text": "P-series fuels\n\nP-series fuels are a family of renewable, non-petroleum, liquid fuels that can substitute for gasoline. The blend of methyl tetrahydrofuran (MTHF), ethanol, and hydrocarbon constitute the P-series fuel.\n\n"}
{"id": "19645922", "url": "https://en.wikipedia.org/wiki?curid=19645922", "title": "Palierne equation", "text": "Palierne equation\n\nPalierne equation connects the dynamic modulus of emulsions with the dynamic modulus of the two phases, size of the droplets and the interphase surface tension. The equation can also be used for suspensions of viscoelastic solid particles in viscoelastic fluids. The equation is named after French rheologist Jean-François Palierne who has proposed the equation in 1991.\n\nFor the dilute emulsions Palierne equation looks like:\nwhere formula_2 is the dynamic modulus of the emulsion, formula_3 is the dynamic modulus of the continuous phase (matrix), formula_4 is the volume fraction of the disperse phase and the formula_5 is given as\nwhere formula_7 is the dynamic modulus of the disperse phase, formula_8 is the surface tension between the phases and formula_9 is the radius of the droplets.\n\nFor the suspension of solid particles the value of formula_5 is given as\n\nThe Palierne equation is usually extended for the finite volume concentrations of the disperse phase formula_4 as:\n"}
{"id": "19889751", "url": "https://en.wikipedia.org/wiki?curid=19889751", "title": "Phi meson", "text": "Phi meson\n\nIn particle physics, the phi meson or meson is a vector meson formed of a strange quark and a strange antiquark. It was the meson's unusual propensity to decay into and that led to the discovery of the OZI rule. It has a mass of and a mean lifetime of .\n\nThe most common decay modes of the meson are at , at , and various indistinguishable combinations of s and pions at . In all cases, it decays via the strong force. The pion channel would naïvely be the dominant decay channel because the collective mass of the pions is smaller than that of the kaons, making it energetically favorable; however, it is suppressed by the OZI rule.\n\nThe quark composition of the meson can be thought of as a mix between , , and states, but it is very nearly a pure state. This can be shown by deconstructing the wave function of the into its component parts. We see that the and mesons are mixtures of the SU(3) wave functions as follows.\n\nformula_1,\n\nformula_2,\n\nwhere\n\nformula_3 is the nonet mixing angle,\n\nformula_4 and\n\nformula_5.\n\nThe mixing angle at which the components decouple completely can be calculated to be about 35.3˚. The mixing angle of the and states is calculated from the masses of each state to be about 35˚, which is very close to maximum decoupling. Therefore, the meson is nearly a pure state.\n\nThe existence of the meson was first proposed by the Japanese American particle physicist, J. J. Sakurai, in 1962 as a resonance state between the and the . It was discovered later in 1962 by Connolly, et al. in a 20-inch hydrogen bubble chamber at the Alternating Gradient Synchrotron (AGS) in Brookhaven National Laboratory in Uptown, NY while they were studying collisions at approximately 2.23 GeV/c. In essence, the reaction involved a beam of s being accelerated to high energies to collide with protons.\n\nThe meson has several possible decay modes. The most energetically favored mode involves the meson decaying into 3 pions, which is what would naïvely be expected. However, we instead observe that it decays most frequently into 2 kaons. Between 1963 and 1966, 3 people, Susumu Okubo, George Zweig and Jugoro Iizuka, each independently proposed a rule to account for the observed suppression of the 3 pion decay. This rule is now known as the OZI rule and is also the currently accepted explanation for the unusually long lifetimes of the and mesons. Namely, on average they last and respectively. This is compared to the normal mean lifetime of a meson decaying via the strong force, which is on the order of .\n\nIn 1999, a factory named DAFNE (or DANE since the F stands for \" Factory\") began operation to study the decay of the meson in Frascati, Italy. It produces mesons via electron-positron collisions. It has numerous detectors, including the KLOE detector which was in operation at the beginning of its operation.\n\n"}
{"id": "10414501", "url": "https://en.wikipedia.org/wiki?curid=10414501", "title": "Platinum black", "text": "Platinum black\n\nPlatinum black (Pt black) is a fine powder of platinum with good catalytic properties. The name of platinum black is due to its black color.\n\nPlatinum black is widely used as a thin film covering solid platinum metal, forming platinum electrodes for applications in electrochemistry. The process of covering platinum electrodes with such a layer of platinum black is called \"platinization of platinum\". The platinized platinum has a true surface area much higher than the geometrical surface area of the electrode and, therefore, exhibits action superior to that of shiny platinum.\n\nPlatinum black powder is used as a catalyst in proton exchange membrane fuel cells. In common practice, the platinum black is either sprayed using an ultrasonic nozzle or hot pressed onto the membrane or gas diffusion layer. A suspension of platinum black and carbon powder in ethanol-water solutions serves to optimize the uniformity of the coating, electrical conductivity, and in the case of application to the membrane, to prevent dehydration of the membrane during the application.\n\nHistorically many \"self-lighting\" gas lamps, ovens, and stove burners used platinum black to catalyze the oxidation of a small amount of gas, lighting the device without a match or spark. This works particularly well for producer gas, town gas, and wood gas which contain a substantial fraction of hydrogen gas (H) which is particularly well catalyzed by platinum black.\n\nPlatinum black powder can be manufactured from ammonium chloroplatinate by heating at 500 °C in molten sodium nitrate for 30 minutes, followed by pouring the melt into water, boiling, washing, and reduction of the brown powder (believed to be platinum dioxide) with gaseous hydrogen to platinum black.\n\nBefore platinization, the platinum surface is cleaned by immersion in aqua regia (50% solution, i.e., 3 volumes of 12 mol/kg of HCl, 1 volume of 16 mol/kg HNO, 4 volumes of water).\n\nPlatinization is often conducted from water solution of 0.072 mol/kg of chloroplatinic acid and 0.00013 mol/kg of lead acetate, at a current density of 30 mA/cm for up to 10 minutes. The process evolves chlorine at the anode; the interaction of the chlorine with the cathode is prevented by employing a suitable separation (e.g., a glass frit).\n\nAnother author recommends electroplating with the current density of 5 mA/cm while reversing the polarity every 30 seconds for 15 minutes.\n\nAfter platinization, the electrode should be rinsed and stored in distilled water. The electrode loses its catalytic properties on prolonged exposure to air. \n\nPlatinum sponge is a porous, grayish-black form of platinum that can adsorb a large amount of gas, such as hydrogen or oxygen gas, allowing it to be used as a catalyst in many gas reactions such as the oxidation of ammonium. It can also be used for the ignition of combustible gases. It is used as the raw material for electronic instrument, chemical industry, and precision alloys. It can also be used as a surface active agent. It is soluble in chloroazotic acid and is formed from a mass of metallic particles.\n\nIt is made of a mass of platinum particles with the following characteristics:\n\nIt is prepared by dipping asbestos into chloroplatinic acid or ammonium chloroplatinate. The substance is then burned to produce platinum sponge. Alternatively, it can be made by strongly heating ammonium chloroplatinate. Its catalytic properties vary depending on the specifics of the manufacturing.\n\nIn hydrogen saturated hydrochloric acid, the shiny platinum electrode is observed to assume positive potential versus that of platinum black at zero net current (+ 340 mV at room temperature). With the temperature increasing to 70 °C, the difference in potentials dropped to zero. The reason for this is not perfectly clear, although several explanations have been proposed.\n\n\n"}
{"id": "1060865", "url": "https://en.wikipedia.org/wiki?curid=1060865", "title": "Pre-preg", "text": "Pre-preg\n\nPre-preg is \"pre-impregnated\" composite fibers where a thermoset polymer matrix material, such as epoxy, or a thermoplastic resin is already present. The fibers often take the form of a weave and the matrix is used to bond them together and to other components during manufacture. The thermoset matrix is only partially cured to allow easy handling; this B-Stage material requires cold storage to prevent complete curing. B-Stage pre-preg is always stored in cooled areas since heat accelerates complete polymerization. Hence, composite structures built of pre-pregs will mostly require an oven or autoclave to cure. \n\nPre-preg allows one to impregnate the fibers on a flat workable surface, or rather in an industrial process, and then later form the impregnated fibers to a shape which could prove to be problematic for the hot injection process. Pre-preg also allows one to impregnate a bulk amount of fiber and then store it in a cooled area (below 20°C) for an extended period of time to cure later. The process can also be time consuming in comparison to the hot injection process and the added value for pre-preg preparation is at the stage of the material supplier.\n\nThis technique can be utilized in the aviation industry. As in principle, prepreg has the potential to be processed batch sizes. Despite fiber glass has high applicability in aircraft specifically small aircraft motors, carbon fiber is employed in this type of industry with higher rate, and the demand on it is i increase. For example, the characterization of Airbus A380 is handled by means of a mass fraction. This mass fractionb is about 20%, and the Airbus A350XWB by a mass fraction of about 50% of carbon fiber prepregs. Carbon fiber prepregs have been used in the airfoils of the Airbus fleet for more than 20 years.\n\nThe usage of prepreg in automotive industry is used at relatively limitted quantities in comparison with other techniques like automated tape lay-up and automated fiber placement. The main resean behined this is the relative high cost of prepreg fibers as well as the compounds used in molds. Example of such tools areBMC or SMC.\nThere are many products that utilize the concept of prepreg among which is the following.\n\n\n\n\n\n\n\n\nThere are many fiber types that can be excellent candidates for the preparation of preimpregnated fibers. The most common fibers among these candidates are the following fibers.\n\n\nOne distinguishes the matrix systems according to their hardening temperature and the type of resin. The curing temperature greatly influences the glass transition temperature and thus the operating temperature. Military aircraft mainly use 180 ° C systems\n\nThe prepreg matrix consists of a mixture of resin and hardener, in some cases an accelerator. Freezing at -20 ° C prevents the resin from reacting with the hardener. If the cold chain is interrupted, the reaction starts and the prepreg becomes unusable. There are also high-temperature prepregs which can be stored for a certain time at room temperature. These prepregs can then be cured only in an autoclave at elevated temperature.\n\nIt is mainly used resins based on epoxy resin. Vinyl ester-based prepregs are also available. Since vinyl ester resins must be pre-accelerated with amine accelerator or cobalt, their processing time at room temperature is shorter than with epoxy-based prepregs. Catalysts (also called hardeners) include peroxides such as methyl ethyl ketone peroxide (MEKP), acetyl acetone peroxide (AAP) or cyclohexanone peroxide (CHP). Vinylesterharz is used under high impact stress.\n\nPrepregs are cured at elevated temperature. They can be processed with the hot pressing technique or the autoclave technique. In both techniques increases by the pressure of the fiber volume fraction.\n\nThe best qualities can be produced with the autoclave technique. The combination of pressure and vacuum results in components with very low air inclusions.\n\nThe curing can be followed by a tempering process, which serves for complete crosslinking.\n\nRecent advances in out of autoclave (OOA) processes hold promise for improving performance and lowering costs for composite structures. Using vacuum-bag-only (VBO) for atmospheric pressures, the new OOA processes promise to deliver the less than 1 percent void content required for aerospace primary structures. Led by material scientists at Air Force Research Lab, the technique would save the costs of constructing and installing large structure autoclaves ($100M saved at NASA) and making small production runs of 100 aircraft economically viable.\n\n"}
{"id": "25082257", "url": "https://en.wikipedia.org/wiki?curid=25082257", "title": "Reach (mathematics)", "text": "Reach (mathematics)\n\nIn mathematics, the reach of a subset of Euclidean space R is a real number that roughly describes how curved the boundary of the set is.\n\nLet \"X\" be a subset of R. Then reach of \"X\" is defined as \n\nShapes that have reach infinity include\n\nThe graph of \"ƒ\"(\"x\") = |\"x\"| has reach zero.\n\nA circle of radius \"r\" has reach \"r\".\n"}
{"id": "60569", "url": "https://en.wikipedia.org/wiki?curid=60569", "title": "Rectifier", "text": "Rectifier\n\nA rectifier is an electrical device that converts alternating current (AC), which periodically reverses direction, to direct current (DC), which flows in only one direction. The process is known as \"rectification\", since it \"straightens\" the direction of current. Physically, rectifiers take a number of forms, including vacuum tube diodes, mercury-arc valves, stacks of copper and selenium oxide plates, semiconductor diodes, silicon-controlled rectifiers and other silicon-based semiconductor switches. Historically, even synchronous electromechanical switches and motors have been used. Early radio receivers, called crystal radios, used a \"cat's whisker\" of fine wire pressing on a crystal of galena (lead sulfide) to serve as a point-contact rectifier or \"crystal detector\".\n\nRectifiers have many uses, but are often found serving as components of DC power supplies and high-voltage direct current power transmission systems. Rectification may serve in roles other than to generate direct current for use as a source of power. As noted, detectors of radio signals serve as rectifiers. In gas heating systems flame rectification is used to detect presence of a flame.\n\nBecause of the alternating nature of the input AC sine wave, the process of rectification alone produces a DC that, though unidirectional, consists of pulses of current. Many applications of rectifiers, such as power supplies for radio, television and computer equipment, require a \"steady\" constant DC current (as would be produced by a battery). In these applications the output of the rectifier is smoothed by an electronic filter, which may be a capacitor, choke, or set of capacitors, chokes and resistors, possibly followed by a voltage regulator to produce a steady current.\n\nMore complex circuitry that performs the opposite function, converting DC to AC, is called an inverter.\n\nBefore the development of silicon semiconductor rectifiers, vacuum tube thermionic diodes and copper oxide- or selenium-based metal rectifier stacks were used. With the introduction of semiconductor electronics, vacuum tube rectifiers became obsolete, except for some enthusiasts of vacuum tube audio equipment. For power rectification from very low to very high current, semiconductor diodes of various types (junction diodes, Schottky diodes, etc.) are widely used.\n\nOther devices that have control electrodes as well as acting as unidirectional current valves are used where more than simple rectification is required—e.g., where variable output voltage is needed. High-power rectifiers, such as those used in high-voltage direct current power transmission, employ silicon semiconductor devices of various types. These are thyristors or other controlled switching solid-state switches, which effectively function as diodes to pass current in only one direction.\n\nRectifier circuits may be single-phase or multi-phase (three phase being the most common number of phases). Most low power rectifiers for domestic equipment are single-phase, but three-phase rectification is very important for industrial applications and for the transmission of energy as DC (HVDC).\n\nIn half-wave rectification of a single-phase supply, either the positive or negative half of the AC wave is passed, while the other half is blocked. Mathematically, it is a ramp function (for positive pass, negative block): passing positive corresponds to the ramp function being the identity on positive inputs, blocking negative corresponds to being zero on negative inputs. Because only one half of the input waveform reaches the output, mean voltage is lower. Half-wave rectification requires a single diode in a single-phase supply, or three in a three-phase supply. Rectifiers yield a unidirectional but pulsating direct current; half-wave rectifiers produce far more ripple than full-wave rectifiers, and much more filtering is needed to eliminate harmonics of the AC frequency from the output.\n\nThe no-load output DC voltage of an ideal half-wave rectifier for a sinusoidal input voltage is:\n\nwhere:\n\nA full-wave rectifier converts the whole of the input waveform to one of constant polarity (positive or negative) at its output. Mathematically, this corresponds to the absolute value function. Full-wave rectification converts both polarities of the input waveform to pulsating DC (direct current), and yields a higher average output voltage. Two diodes and a center tapped transformer, or four diodes in a bridge configuration and any AC source (including a transformer without center tap), are needed. Single semiconductor diodes, double diodes with common cathode or common anode, and four-diode bridges, are manufactured as single components.\n\nFor single-phase AC, if the transformer is center-tapped, then two diodes back-to-back (cathode-to-cathode or anode-to-anode, depending upon output polarity required) can form a full-wave rectifier. Twice as many turns are required on the transformer secondary to obtain the same output voltage than for a bridge rectifier, but the power rating is unchanged.\n\nThe average and RMS no-load output voltages of an ideal single-phase full-wave rectifier are:\n\nVery common double-diode rectifier vacuum tubes contained a single common cathode and two anodes inside a single envelope, achieving full-wave rectification with positive output. The 5U4 and the 80/5Y3 (4 pin)/(octal)were popular examples of this configuration.\n\nSingle-phase rectifiers are commonly used for power supplies for domestic equipment. However, for most industrial and high-power applications, three-phase rectifier circuits are the norm. As with single-phase rectifiers, three-phase rectifiers can take the form of a half-wave circuit, a full-wave circuit using a center-tapped transformer, or a full-wave bridge circuit.\n\nThyristors are commonly used in place of diodes to create a circuit that can regulate the output voltage. Many devices that provide direct current actually \"generate\" three-phase AC. For example, an automobile alternator contains six diodes, which function as a full-wave rectifier for battery charging.\n\nAn uncontrolled three-phase, half-wave midpoint circuit requires three diodes, one connected to each phase. This is the simplest type of three-phase rectifier but suffers from relatively high harmonic distortion on both the AC and DC connections. This type of rectifier is said to have a pulse-number of three, since the output voltage on the DC side contains three distinct pulses per cycle of the grid frequency:\n\nThe peak values formula_2 of this three-pulse DC voltage are calculated from the RMS value formula_3 of the input phase voltage (line to neutral voltage, 120 V in North America, 230 V within Europe at mains operation): formula_4. The average no-load output voltage formula_5 results from the integral under the graph of a positive half-wave with the period duration of formula_6 (from 30° to 150°):\n\nIf the AC supply is fed via a transformer with a center tap, a rectifier circuit with improved harmonic performance can be obtained. This rectifier now requires six diodes, one connected to each end of each transformer secondary winding. This circuit has a pulse-number of six, and in effect, can be thought of as a six-phase, half-wave circuit.\n\nBefore solid state devices became available, the half-wave circuit, and the full-wave circuit using a center-tapped transformer, were very commonly used in industrial rectifiers using mercury-arc valves. This was because the three or six AC supply inputs could be fed to a corresponding number of anode electrodes on a single tank, sharing a common cathode.\n\nWith the advent of diodes and thyristors, these circuits have become less popular and the three-phase bridge circuit has become the most common circuit.\n\nFor an uncontrolled three-phase bridge rectifier, six diodes are used, and the circuit again has a pulse number of six. For this reason, it is also commonly referred to as a six-pulse bridge. The B6 circuit can be seen simplified as a series connection of two three-pulse center circuits.\n\nFor low-power applications, double diodes in series, with the anode of the first diode connected to the cathode of the second, are manufactured as a single component for this purpose. Some commercially available double diodes have all four terminals available so the user can configure them for single-phase split supply use, half a bridge, or three-phase rectifier.\n\nFor higher-power applications, a single discrete device is usually used for each of the six arms of the bridge. For the very highest powers, each arm of the bridge may consist of tens or hundreds of separate devices in parallel (where very high current is needed, for example in aluminium smelting) or in series (where very high voltages are needed, for example in high-voltage direct current power transmission).\n\nThe pulsating DC voltage results from the differences of the instantaneous positive and negative phase voltages formula_3, phase-shifted by 30°:\n\nThe ideal, no-load average output voltage formula_11 of the B6 circuit results from the integral under the graph of a DC voltage pulse with the period duration of formula_12 (from 60° to 120°) with the peak value formula_13:\n\nIf the three-phase bridge rectifier is operated symmetrically (as positive and negative supply voltage), the center point of the rectifier on the output side (or the so-called isolated reference potential) opposite the center point of the transformer (or the neutral conductor) has a potential difference in form of a triangular common-mode voltage. For this reason, the two centers must never be connected to each other, otherwise short-circuit currents would flow. The ground of the three-phase bridge rectifier in symmetrical operation is thus decoupled from the neutral conductor or the earth of the mains voltage. Powered by a transformer, earthing of the center point of the bridge is possible, provided that the secondary winding of the transformer is electrically isolated from the mains voltage and the star point of the secondary winding is not on earth. In this case, however, (negligible) leakage currents are flowing over the transformer windings.\n\nThe common-mode voltage is formed out of the respective average values of the differences between the positive and negative phase voltages, which form the pulsating DC voltage. The peak value of the delta voltage formula_17 amounts ¼ of the peak value of the phase input voltage formula_2 and is calculated with formula_2 minus half of the DC voltage at 60° of the period:\n\nThe RMS value of the common-mode voltage is calculated from the form factor for triangular oscillations:\n\nIf the circuit is operated asymmetrically (as a simple supply voltage with just one positive pole), both the positive and negative poles (or the isolated reference potential) are pulsating opposite the center (or the ground) of the input voltage analogously to the positive and negative waveforms of the phase voltages. However, the differences in the phase voltages result in the six-pulse DC voltage (over the duration of a period). The strict separation of the transformer center from the negative pole (otherwise short-circuit currents will flow) or a possible grounding of the negative pole when powered by an isolating transformer apply correspondingly to the symmetrical operation.\n\nThe controlled three-phase bridge rectifier uses thyristors in place of diodes. The output voltage is reduced by the factor cos(α):\n\nOr, expressed in terms of the line to line input voltage:\n\nWhere:\n\nThe above equations are only valid when no current is drawn from the AC supply or in the theoretical case when the AC supply connections have no inductance. In practice, the supply inductance causes a reduction of DC output voltage with increasing load, typically in the range 10–20% at full load.\n\nThe effect of supply inductance is to slow down the transfer process (called commutation) from one phase to the next. As result of this is that at each transition between a pair of devices, there is a period of overlap during which three (rather than two) devices in the bridge are conducting simultaneously. The overlap angle is usually referred to by the symbol μ (or u), and may be 20 30° at full load.\n\nWith supply inductance taken into account, the output voltage of the rectifier is reduced to:\n\nThe overlap angle μ is directly related to the DC current, and the above equation may be re-expressed as:\n\nWhere:\n\nAlthough better than single-phase rectifiers or three-phase half-wave rectifiers, six-pulse rectifier circuits still produce considerable harmonic distortion on both the AC and DC connections. For very high-power rectifiers the twelve-pulse bridge connection is usually used. A twelve-pulse bridge consists of two six-pulse bridge circuits connected in series, with their AC connections fed from a supply transformer that produces a 30° phase shift between the two bridges. This cancels many of the characteristic harmonics the six-pulse bridges produce.\n\nThe 30 degree phase shift is usually achieved by using a transformer with two sets of secondary windings, one in star (wye) connection and one in delta connection.\n\nThe simple half-wave rectifier can be built in two electrical configurations with the diodes pointing in opposite directions, one version connects the negative terminal of the output direct to the AC supply and the other connects the positive terminal of the output direct to the AC supply. By combining both of these with separate output smoothing it is possible to get an output voltage of nearly double the peak AC input voltage. This also provides a tap in the middle, which allows use of such a circuit as a split rail power supply.\n\nA variant of this is to use two capacitors in series for the output smoothing on a bridge rectifier then place a switch between the midpoint of those capacitors and one of the AC input terminals. With the switch open, this circuit acts like a normal bridge rectifier. With the switch closed, it act like a voltage doubling rectifier. In other words, this makes it easy to derive a voltage of roughly 320 V (±15%, approx.) DC from any 120 V or 230 V mains supply in the world, this can then be fed into a relatively simple switched-mode power supply. However, for a given desired ripple, the value of both capacitors must be twice the value of the single one required for a normal bridge rectifier; when the switch is closed each one must filter the output of a half-wave rectifier, and when the switch is open the two capacitors are connected in series with an equivalent value of half one of them.\n\nCascaded diode and capacitor stages can be added to make a voltage multiplier (Cockroft-Walton circuit). These circuits are capable of producing a DC output voltage potential up to about ten times the peak AC input voltage, in practice limited by current capacity and voltage regulation issues. Diode voltage multipliers, frequently used as a trailing boost stage or primary high voltage (HV) source, are used in HV laser power supplies, powering devices such as cathode ray tubes (CRT) (like those used in CRT based television, radar and sonar displays), photon amplifying devices found in image intensifying and photo multiplier tubes (PMT), and magnetron based radio frequency (RF) devices used in radar transmitters and microwave ovens. Before the introduction of semiconductor electronics, transformerless vacuum tube receivers powered directly from AC power sometimes used voltage doublers to generate roughly 300 VDC from a 100–120 V power line.\n\nSeveral ratios are used to quantify the function and performance of rectifiers or their output, including transformer utilization factor (TUF), conversion ratio (\"η\"), ripple factor, form factor, and peak factor. The two primary measures are DC voltage (or offset) and peak-peak ripple voltage, which are constituent components of the output.\n\nConversion ratio (also called \"rectification ratio\", and confusingly, \"efficiency\") \"η\" is defined as the ratio of DC output power to the input power from the AC supply. Even with ideal rectifiers, the ratio is less than 100% because some of the output power is AC power rather than DC which manifests as ripple superimposed on the DC waveform. The ratio can be improved with the use of smoothing circuits which reduce the ripple and hence reduce the AC content of the output. Conversion ratio is reduced by losses in transformer windings and power dissipation in the rectifier element itself. This ratio is of little practical significance because a rectifier is almost always followed by a filter to increase DC voltage and reduce ripple. In some three-phase and multi-phase applications the conversion ratio is high enough that smoothing circuitry is unnecessary.\nIn other circuits, like filament heater circuits in vacuum tube electronics where the load is almost entirely resistive, smoothing circuitry may be omitted because resistors dissipate both AC and DC power,so no power is lost.\n\nFor a half-wave rectifier the ratio is very modest.\n\nThus maximum conversion ratio for a half-wave rectifier is,\n\nSimilarly, for a full-wave rectifier,\n\nThree-phase rectifiers, especially three-phase full-wave rectifiers, have much greater conversion ratios because the ripple is intrinsically smaller.\n\nFor a three-phase half-wave rectifier,\n\nFor a three-phase full-wave rectifier,\n\nA real rectifier characteristically drops part of the input voltage (a voltage drop, for silicon devices, of typically 0.7 volts plus an equivalent resistance, in general non-linear)—and at high frequencies, distorts waveforms in other ways. Unlike an ideal rectifier, it dissipates some power.\n\nAn aspect of most rectification is a loss from the peak input voltage to the peak output voltage, caused by the built-in voltage drop across the diodes (around 0.7 V for ordinary silicon p–n junction diodes and 0.3 V for Schottky diodes). Half-wave rectification and full-wave rectification using a center-tapped secondary produces a peak voltage loss of one diode drop. Bridge rectification has a loss of two diode drops. This reduces output voltage, and limits the available output voltage if a very low alternating voltage must be rectified. As the diodes do not conduct below this voltage, the circuit only passes current through for a portion of each half-cycle, causing short segments of zero voltage (where instantaneous input voltage is below one or two diode drops) to appear between each \"hump\".\n\nPeak loss is very important for low voltage rectifiers (for example, 12 V or less) but is insignificant in high-voltage applications such as HVDC power transmission systems.\n\nNon-linear loads like rectifiers produce current harmonics of the source frequency on the AC side and voltage harmonics of the source frequency on the DC side, due to switching behavior.\n\n While half-wave and full-wave rectification deliver unidirectional current, neither produces a constant voltage. There is a large AC ripple voltage component at the source frequency for a half-wave rectifier, and twice the source frequency for a full-wave rectifier. Ripple voltage is usually specified peak-to-peak. Producing steady DC from a rectified AC supply requires a smoothing circuit or filter. In its simplest form this can be just a capacitor (also called a filter, reservoir, or smoothing capacitor), choke, resistor, Zener diode and resistor, or voltage regulator placed at the output of the rectifier. In practice, most smoothing filters utilize multiple components to efficiently reduce ripple voltage to a level tolerable by the circuit.\nThe filter capacitor releases its stored energy during the part of the AC cycle when the AC source does not supply any power, that is, when the AC source changes its direction of flow of current.\n\nThe above diagram shows reservoir performance from a near zero impedance source, such as a mains supply. As the rectifier voltage increases, it charges the capacitor and also supplies current to the load. At the end of the quarter cycle, the capacitor is charged to its peak value Vp of the rectifier voltage. Following this, the rectifier voltage starts to decrease to its minimum value Vmin as it enters the next quarter cycle. This initiates the discharge of the capacitor through the load.\n\nThe size of the capacitor C is determined by the amount of ripple r that can be tolerated, where r=(Vp-Vmin)/Vp.\n\nThese circuits are very frequently fed from transformers, and have significant resistance. Transformer resistance modifies the reservoir capacitor waveform, changes the peak voltage, and introduces regulation issues.\n\nFor a given load, sizing of a smoothing capacitor is a tradeoff between reducing ripple voltage and increasing ripple current. The peak current is set by the rate of rise of the supply voltage on the rising edge of the incoming sine-wave, reduced by the resistance of the transformer windings. High ripple currents increase IR losses (in the form of heat) in the capacitor, rectifier and transformer windings, and may exceed the ampacity of the components or VA rating of the transformer. Vacuum tube rectifiers specify the maximum capacitance of the input capacitor, and SS diode rectifiers also have current limitations. Capacitors for this application need low ESR, or ripple current may overheat them. To limit ripple voltage to a specified value the required capacitor size is proportional to the load current and inversely proportional to the supply frequency and the number of output peaks of the rectifier per input cycle. Full-wave rectified output requires a smaller capacitor because it is double the frequency of half-wave rectified output. To reduce ripple to a satisfactory limit with just a single capacitor would often require a capacitor that's infeasibly large.\n\nIt is also possible to put the rectified waveform into a choke-input filter. The advantage of this circuit is that the current waveform is smoother: current is drawn over the entire cycle, instead of being drawn in pulses at the peaks of AC voltage each half-cycle as in a capacitor input filter. The disadvantage is that the voltage output is much lower – the average of an AC half-cycle rather than the peak; this is about 90% of the RMS voltage versus formula_37 times the RMS voltage (unloaded) for a capacitor input filter. Offsetting this is superior voltage regulation and higher available current, which reduce peak voltage and ripple current demands on power supply components. Inductors require cores of iron or other magnetic materials, and add weight and size. Their use in power supplies for electronic equipment has therefore dwindled in favour of semiconductor circuits such as voltage regulators.\n\nIn cases where ripple voltage is insignificant, like battery chargers, the input filter may be a single series resistor to adjust the output voltage to that required by the circuit. A resistor reduces both output voltage and ripple voltage proportionately. A disadvantage of a resistor input filter is that it consumes power in the form of waste heat that is not available to the load, so it is employed only in low current circuits.\n\nTo further reduce ripple, the initial filter element may be followed by additional alternating series and shunt filter components, or by a voltage regulator. Series filter components may be resistors or chokes; shunt elements may be resistors or capacitors. The filter may raise DC voltage as\nwell as reduce ripple. Filters are often constructed from pairs of series/shunt components called RC (series resistor, shunt capacitor) or LC (series choke, shunt capacitor) sections. Two common filter geometries are known as Pi (capacitor, choke, capacitor) and T (choke, capacitor, choke) filters.\nSometimes the series elements are resistors - because resistors are smaller and cheaper - when a lower DC output is desirable or permissible. Another\nkind of special filter geometry is a series resonant choke or tuned choke filter. Unlike the other filter geometries which are low-pass filters, a resonant choke filter is a band-stop filter: it is a parallel combination of choke and capacitor which resonates at the frequency of the ripple voltage, presenting a very high impedance to the ripple. It may be followed by a shunt capacitor to complete the filter.\n\nA more usual alternative to additional filter components, if the DC load requires very low ripple voltage, is to follow the input filter with a voltage regulator. A voltage regulator operates on a different principle than a filter, which is essentially a voltage divider that shunts voltage at the ripple frequency away from the load. Rather, a regulator increases or decreases current supplied to the load in order to maintain a constant output voltage.\n\nA simple passive shunt voltage regulator may consist of a series resistor to drop source voltage to the required level and a Zener diode shunt with reverse\nvoltage equal to the set voltage. When input voltage rises, the diode dumps current to maintain the set output voltage. This kind of regulator is usually employed only in low voltage, low current circuits because Zener diodes have both voltage and current limitations. It is also very inefficient, because it dumps excess current, which is not available to the load.\n\nA more efficient alternative to a shunt voltage regulator is an active voltage regulator circuit. An active regulator employs reactive components to store and discharge energy, so that most or all current supplied by the rectifier is passed to the load. It may also use negative and positive feedback in conjunction with at least one voltage amplifying component like a transistor to maintain output voltage when source voltage drops. The input filter must prevent the troughs of the ripple dropping below the minimum voltage required by the regulator to produce the required output voltage. The regulator serves both to significantly reduce the ripple and to deal with variations in supply and load characteristics.\n\nThe primary application of rectifiers is to derive DC power from an AC supply (AC to DC converter). Rectifiers are used inside the power supplies of virtually all electronic equipment. AC/DC power supplies may be broadly divided into linear power supplies and switched-mode power supplies. In such power supplies, the rectifier will be in series following the transformer, and be followed by a smoothing filter and possibly a voltage regulator.\n\nConverting DC power from one voltage to another is much more complicated. One method of DC-to-DC conversion first converts power to AC (using a device called an inverter), then uses a transformer to change the voltage, and finally rectifies power back to DC. A frequency of typically several tens of kilohertz is used, as this requires much smaller inductance than at lower frequencies and obviates the use of heavy, bulky, and expensive iron-cored units. Another method of converting DC voltages uses a charge pump, using rapid switching to change the connections of capacitors; this technique is generally limited to supplies up to a couple of watts, owing to the size of capacitors required.\n\nRectifiers are also used for detection of amplitude modulated radio signals. The signal may be amplified before detection. If not, a very low voltage drop diode or a diode biased with a fixed voltage must be used. When using a rectifier for demodulation the capacitor and load resistance must be carefully matched: too low a capacitance makes the high frequency carrier pass to the output, and too high makes the capacitor just charge and stay charged.\n\nRectifiers supply polarised voltage for welding. In such circuits control of the output current is required; this is sometimes achieved by replacing some of the diodes in a bridge rectifier with thyristors, effectively diodes whose voltage output can be regulated by switching on and off with phase fired controllers.\n\nThyristors are used in various classes of railway rolling stock systems so that fine control of the traction motors can be achieved. Gate turn-off thyristors are used to produce alternating current from a DC supply, for example on the Eurostar Trains to power the three-phase traction motors.\n\nBefore about 1905 when tube type rectifiers were developed, power conversion devices were purely electro-mechanical in design. Mechanical rectification systems used some form of rotation or resonant vibration (e.g. vibrators) driven by electromagnets, which operated a switch or commutator to reverse the current.\n\nThese mechanical rectifiers were noisy and had high maintenance requirements. The moving parts had friction, which required lubrication and replacement due to wear. Opening mechanical contacts under load resulted in electrical arcs and sparks that heated and eroded the contacts. They also were not able to handle AC frequencies above several thousand cycles per second.\n\nTo convert alternating into direct current in electric locomotives, a synchronous rectifier may be used. It consists of a synchronous motor driving a set of heavy-duty electrical contacts. The motor spins in time with the AC frequency and periodically reverses the connections to the load at an instant when the sinusoidal current goes through a zero-crossing. The contacts do not have to \"switch\" a large current, but they must be able to \"carry\" a large current to supply the locomotive's DC traction motors.\n\nThese consisted of a resonant reed, vibrated by an alternating magnetic field created by an AC electromagnet, with contacts that reversed the direction of the current on the negative half cycles. They were used in low power devices, such as battery chargers, to rectify the low voltage produced by a step-down transformer. Another use was in battery power supplies for portable vacuum tube radios, to provide the high DC voltage for the tubes. These operated as a mechanical version of modern solid state switching inverters, with a transformer to step the battery voltage up, and a set of vibrator contacts on the transformer core, operated by its magnetic field, to repeatedly break the DC battery current to create a pulsing AC to power the transformer. Then a second set of rectifier contacts on the vibrator rectified the high AC voltage from the transformer secondary to DC.\nA \"motor-generator set\", or the similar \"rotary converter\", is not strictly a rectifier as it does not actually \"rectify\" current, but rather \"generates\" DC from an AC source. In an \"M-G set\", the shaft of an AC motor is mechanically coupled to that of a DC generator. The DC generator produces multiphase alternating currents in its armature windings, which a commutator on the armature shaft converts into a direct current output; or a homopolar generator produces a direct current without the need for a commutator. M-G sets are useful for producing DC for railway traction motors, industrial motors and other high-current applications, and were common in many high-power D.C. uses (for example, carbon-arc lamp projectors for outdoor theaters) before high-power semiconductors became widely available.\nThe electrolytic rectifier was a device from the early twentieth century that is no longer used. A home-made version is illustrated in the 1913 book \"The Boy Mechanic\" but it would only be suitable for use at very low voltages because of the low breakdown voltage and the risk of electric shock. A more complex device of this kind was patented by G. W. Carpenter in 1928 (US Patent 1671970).\n\nWhen two different metals are suspended in an electrolyte solution, direct current flowing one way through the solution sees less resistance than in the other direction. Electrolytic rectifiers most commonly used an aluminum anode and a lead or steel cathode, suspended in a solution of tri-ammonium ortho-phosphate.\n\nThe rectification action is due to a thin coating of aluminum hydroxide on the aluminum electrode, formed by first applying a strong current to the cell to build up the coating. The rectification process is temperature-sensitive, and for best efficiency should not operate above 86 °F (30 °C). There is also a breakdown voltage where the coating is penetrated and the cell is short-circuited. Electrochemical methods are often more fragile than mechanical methods, and can be sensitive to usage variations, which can drastically change or completely disrupt the rectification processes.\n\nSimilar electrolytic devices were used as lightning arresters around the same era by suspending many aluminium cones in a tank of tri-ammonium ortho-phosphate solution. Unlike the rectifier above, only aluminium electrodes were used, and used on A.C., there was no polarization and thus no rectifier action, but the chemistry was similar.\n\nThe modern electrolytic capacitor, an essential component of most rectifier circuit configurations was also developed from the electrolytic rectifier.\n\nThe development of vacuum tube technology in the early 20th century resulted in the invention of various tube-type rectifiers, which largely replaced the noisy, inefficient mechanical rectifiers.\n\nA rectifier used in high-voltage direct current (HVDC) power transmission systems and industrial processing between about 1909 to 1975 is a \"mercury-arc rectifier\" or \"mercury-arc valve\". The device is enclosed in a bulbous glass vessel or large metal tub. One electrode, the cathode, is submerged in a pool of liquid mercury at the bottom of the vessel and one or more high purity graphite electrodes, called anodes, are suspended above the pool. There may be several auxiliary electrodes to aid in starting and maintaining the arc. When an electric arc is established between the cathode pool and suspended anodes, a stream of electrons flows from the cathode to the anodes through the ionized mercury, but not the other way (in principle, this is a higher-power counterpart to flame rectification, which uses the same one-way current transmission properties of the plasma naturally present in a flame).\n\nThese devices can be used at power levels of hundreds of kilowatts, and may be built to handle one to six phases of AC current. Mercury-arc rectifiers have been replaced by silicon semiconductor rectifiers and high-power thyristor circuits in the mid 1970s. The most powerful mercury-arc rectifiers ever built were installed in the Manitoba Hydro Nelson River Bipole HVDC project, with a combined rating of more than 1 GW and 450 kV.\nThe General Electric Tungar rectifier was a mercury vapor (ex.:5B24) or argon (ex.:328) gas-filled electron tube device with a tungsten filament cathode and a carbon button anode. It operated similarly to the thermionic vacuum tube diode, but the gas in the tube ionized during forward conduction, giving it a much lower forward voltage drop so it could rectify lower voltages. It was used for battery chargers and similar applications from the 1920s until lower-cost metal rectifiers, and later semiconductor diodes, supplanted it. These were made up to a few hundred volts and a few amperes rating, and in some sizes strongly resembled an incandescent lamp with an additional electrode.\n\nThe 0Z4 was a gas-filled rectifier tube commonly used in vacuum tube car radios in the 1940s and 1950s. It was a conventional full-wave rectifier tube with two anodes and one cathode, but was unique in that it had no filament (thus the \"0\" in its type number). The electrodes were shaped such that the reverse breakdown voltage was much higher than the forward breakdown voltage. Once the breakdown voltage was exceeded, the 0Z4 switched to a low-resistance state with a forward voltage drop of about 24 V.\n\nThe thermionic vacuum tube diode, originally called the Fleming valve, was invented by John Ambrose Fleming in 1904 as a detector for radio waves in radio receivers, and evolved into a general rectifier. It consisted of an evacuated glass bulb with a filament heated by a separate current, and a metal plate anode. The filament emitted electrons by thermionic emission (the Edison effect), discovered by Thomas Edison in 1884, and a positive voltage on the plate caused a current of electrons through the tube from filament to plate. Since only the filament produced electrons, the tube would only conduct current in one direction, allowing the tube to rectify an alternating current.\n\nThermionic diode rectifiers were widely used in power supplies in vacuum tube consumer electronic products, such as phonographs, radios, and televisions, for example the All American Five radio receiver, to provide the high DC plate voltage needed by other vacuum tubes. \"Full-wave\" versions with two separate plates were popular because they could be used with a center-tapped transformer to make a full-wave rectifier. Vacuum tube rectifiers were made for very high voltages, such as the high voltage power supply for the cathode ray tube of television receivers, and the kenotron used for power supply in X-ray equipment. However, compared to modern semiconductor diodes, vacuum tube rectifiers have high internal resistance due to space charge and therefore high voltage drops, causing high power dissipation and low efficiency. They are rarely able to handle currents exceeding 250 mA owing to the limits of plate power dissipation, and cannot be used for low voltage applications, such as battery chargers. Another limitation of the vacuum tube rectifier is that the heater power supply often requires special arrangements to insulate it from the high voltages of the rectifier circuit.\n\nThe crystal detector was the earliest type of semiconductor diode. Invented by Jagadish Chandra Bose and developed by G. W. Pickard starting in 1902, it was a significant improvement over earlier detectors such as the coherer. The crystal detector was widely used prior to vacuum tubes becoming available. One popular type of crystal detector, often called a \"cat's whisker detector\", consists of a crystal of some semiconducting mineral, usually galena (lead sulfide), with a light springy wire touching its surface. Its fragility and limited current capability made it unsuitable for power supply applications. In the 1930s, researchers miniaturized and improved the crystal detector for use at microwave frequencies.\nOnce common until replaced by more compact and less costly silicon solid-state rectifiers in the 1970s, these units used stacks of oxide-coated metal plates and took advantage of the semiconductor properties of selenium or copper oxide. While selenium rectifiers were lighter in weight and used less power than comparable vacuum tube rectifiers, they had the disadvantage of finite life expectancy, increasing resistance with age, and were only suitable to use at low frequencies. Both selenium and copper oxide rectifiers have somewhat better tolerance of momentary voltage transients than silicon rectifiers.\n\nTypically these rectifiers were made up of stacks of metal plates or washers, held together by a central bolt, with the number of stacks determined by voltage; each cell was rated for about 20 V. An automotive battery charger rectifier might have only one cell: the high-voltage power supply for a vacuum tube might have dozens of stacked plates. Current density in an air-cooled selenium stack was about 600 mA per square inch of active area (about 90 mA per square centimeter).\nSilicon diodes are the most widely used rectifiers for lower voltages and powers, and have largely replaced other rectifiers. Due to their substantially lower forward voltage (0.3V versus 0.7V for silicon diodes) germanium diodes have an inherent advantage over silicon diodes in low voltage circuits. \n\nIn high-power applications, from 1975 to 2000, most mercury valve arc-rectifiers were replaced by stacks of very high power thyristors, silicon devices with two extra layers of semiconductor, in comparison to a simple diode.\n\nIn medium-power transmission applications, even more complex and sophisticated voltage sourced converter (VSC) silicon semiconductor rectifier systems, such as insulated gate bipolar transistors (IGBT) and gate turn-off thyristors (GTO), have made smaller high voltage DC power transmission systems economical. All of these devices function as rectifiers.\n\nA major area of research is to develop higher frequency rectifiers, that can rectify into terahertz and light frequencies. These devices are used in optical heterodyne detection, which has myriad applications in optical fiber communication and atomic clocks. Another prospective application for such devices is to directly rectify light waves picked up by tiny antennas, called nantennas, to produce DC electric power. It is thought that arrays of nantennas could be a more efficient means of producing solar power than solar cells.\n\nA related area of research is to develop smaller rectifiers, because a smaller device has a higher cutoff frequency. Research projects are attempting to develop a unimolecular rectifier, a single organic molecule that would function as a rectifier.\n\n"}
{"id": "20691673", "url": "https://en.wikipedia.org/wiki?curid=20691673", "title": "Screw", "text": "Screw\n\nA screw is a type of fastener, in some ways similar to a bolt (see \"Differentiation between bolt and screw\" below), typically made of metal, and characterized by a helical ridge, known as a \"male thread\" (external thread). Screws are used to fasten materials by digging in and wedging into a material when turned, while the thread cuts grooves in the fastened material that may help pull fastened materials together and prevent pull-out. There are many screws for a variety of materials; those commonly fastened by screws include wood, sheet metal, and plastic.\n\nA screw is a combination of simple machines—it is in essence an inclined plane wrapped around a central shaft, but the inclined plane (thread) also comes to a sharp edge around the outside, which acts a wedge as it pushes into the fastened material, and the shaft and helix also form a wedge in the form of the point. Some screw threads are designed to mate with a complementary thread, known as a \"female thread\" (internal thread), often in the form of a nut, or object that has the internal thread formed into it. Other screw threads are designed to cut a helical groove in a softer material as the screw is inserted. The most common uses of screws are to hold objects together and to position objects.\n\nA screw will usually have a \"head\" on one end that contains a specially formed shape that allows it to be turned, or \"driven\", with a tool. Common tools for driving screws include screwdrivers and wrenches. The head is usually larger than the body of the screw, which keeps the screw from being driven deeper than the length of the screw and to provide a \"bearing surface\". There are exceptions; for instance, carriage bolts have a domed head that is not designed to be driven; set screws often have a head smaller than the outer diameter of the screw; J-bolts have a J-shaped head which is not designed to be driven, but rather is usually sunk into concrete allowing it to be used as an anchor bolt. The cylindrical portion of the screw from the underside of the head to the tip is known as the \"shank\"; it may be fully threaded or partially threaded. The distance between each thread is called the \"pitch\".\n\nThe majority of screws are tightened by clockwise rotation, which is termed a \"right-hand thread\"; a common mnemonic device for remembering this when working with screws or bolts is \"righty-tighty, lefty-loosey\". If the fingers of the right hand are curled around a right-hand thread, it will move in the direction of the thumb when turned in the same direction as the fingers are curled. Screws with left-hand threads are used in exceptional cases, where loads would tend to loosen a right handed fastener, or when non-interchangeability with right-hand fasteners is required. For example, when the screw will be subject to counterclockwise torque (which would work to undo a right-hand thread), a left-hand-threaded screw would be an appropriate choice. The left side pedal of a bicycle has a left-hand thread.\n\nMore generally, screw may mean any helical device, such as a clamp, a micrometer, a ship's propeller, or an Archimedes' screw water pump.\n\nThere is no universally accepted distinction between a screw and a bolt. A simple distinction that is often true, although not always, is that a bolt passes through a substrate and takes a nut on the other side, whereas a screw takes no nut because it threads directly into the substrate (a screw \"screws into something\", a bolt \"bolts several things together\"). So, as a general rule, when buying a packet of \"screws\" nuts would not be expected to be included, but bolts are often sold with matching nuts. Part of the confusion over this is likely due to regional or dialectical differences. \"Machinery's Handbook\" describes the distinction as follows:\n\nThis distinction is consistent with ASME B18.2.1 and some dictionary definitions for \"screw\" and \"bolt\".\n\nThe issue of what is a screw and what is a bolt is not completely resolved with \"Machinery's Handbook\" distinction, however, because of confounding terms, the ambiguous nature of some parts of the distinction, and usage variations. Some of these issues are discussed below:\n\nEarly wood screws were made by hand, with a series of files, chisels, and other cutting tools, and these can be spotted easily by noting the irregular spacing and shape of the threads, as well as file marks remaining on the head of the screw and in the area between threads. Many of these screws had a blunt end, completely lacking the sharp tapered point on nearly all modern wood screws. Eventually, lathes were used to manufacture wood screws, with the earliest patent being recorded in 1760 in England. During the 1850s swaging tools were developed to provide a more uniform and consistent thread. Screws made with these tools have rounded valleys with sharp and rough threads. Some wood screws were made with cutting dies as early as the late 1700s (possibly even before 1678 when the book content was first published in parts).\n\nOnce screw turning machines were in common use, most commercially available wood screws were produced with this method. These cut wood screws are almost invariably tapered, and even when the tapered shank is not obvious, they can be discerned because the threads do not extend past the diameter of the shank. Such screws are best installed after drilling a pilot hole with a tapered drill bit. The majority of modern wood screws, except for those made of brass, are formed on thread rolling machines. These screws have a constant diameter, threads with a larger diameter than the shank, and are stronger because the rolling process does not cut the grain of the metal.\n\nASME standards specify a variety of \"Machine Screws\" in diameters ranging up to . These fasteners are often used with nuts but also often driven into tapped holes (without nuts). They might be considered a screw or a bolt based on the \"Machinery's Handbook\" distinction. In practice, they tend to be mostly available in smaller sizes and the smaller sizes are referred to as screws or less ambiguously as machine screws, although some kinds of machine screw can be referred to as stove bolts.\n\nASME standard B18.2.1-1996 specifies Hex Cap Screws whose size range is in diameter. These fasteners are very similar to hex bolts. They differ mostly in that they are manufactured to tighter tolerances than the corresponding bolts. \"Machinery's Handbook\" refers parenthetically to these fasteners as \"Finished Hex Bolts\". Reasonably, these fasteners might be referred to as bolts, but based on the US government document \"Distinguishing Bolts from Screws\", the US government might classify them as screws because of the tighter tolerance. In 1991 responding to an influx of counterfeit fasteners Congress passed PL 101-592 \"Fastener Quality Act\" This resulted in the rewriting of specifications by the ASME B18 committee. B18.2.1 was re-written and as a result they eliminated the \"Finished Hex Bolts\" and renamed them the \"Hex Cap Screw\"—a term that had existed in common usage long before, but was now also being codified as an official name for the ASME B18 standard.\n\nThese terms refer to fasteners that are designed to be threaded into a tapped hole that is in part of the assembly and so based on the \"Machinery's Handbook\" distinction they would be screws. Here common terms are at variance with \"Machinery's Handbook\" distinction.\n\nLag screws (US) or coach screws (UK, Australia, and New Zealand) (also referred to as lag bolts or coach bolts, although this is a misnomer) are large wood screws. Square-headed and hex-headed lag screws are covered by ASME B18.2.1 standards, and the head is typically an external hex. A typical lag screw can range in diameter from to , and lengths from or longer, with the coarse threads of a wood-screw or sheet-metal-screw threadform (but larger).\n\nThe materials are usually carbon steel substrate with a coating of zinc galvanization (for corrosion resistance). The zinc coating may be bright (electroplated), yellow (electroplated), or dull gray hot-dip galvanized. Lag screws are used to lag together lumber framing, to lag machinery feet to wood floors, and for other heavy carpentry applications. The attributive modifier \"lag\" came from an early principal use of such fasteners: the fastening of lags such as barrel staves and other similar parts.\n\nThese fasteners are \"screws\" according to the \"Machinery's Handbook\" criteria, and the obsolescent term \"lag bolt\" has been replaced by \"lag screw\" in the \"Handbook\". However, to many tradesmen, they are \"bolts\", because they are large, with hex or square heads.\n\nThe federal government of the United States made an effort to formalize the difference between a bolt and a screw, because different tariffs apply to each. The document seems to have no significant effect on common usage and does not eliminate the ambiguous nature of the distinction between screws and bolts for some threaded fasteners. The document also reflects (although it probably did not originate) significant confusion of terminology usage that differs between the legal/statutory/regulatory community and the fastener industry. The legal/statutory/regulatory wording uses the terms \"coarse\" and \"fine\" to refer to the tightness of the tolerance range, referring basically to \"high-quality\" or \"low-quality\", but this is a poor choice of terms, because those terms in the fastener industry have a different meaning (referring to the steepness of the helix's lead).\n\nOld USS and SAE standards defined cap screws as fasteners with shanks that were threaded to the head and bolts as fasteners with shanks that were partially unthreaded. The relationship of this rule to the idea that a bolt by definition takes a nut is clear (because the unthreaded section of the shank, which is called the \"grip\", was expected to pass through the substrate without threading into it). This is now an obsolete distinction, although large bolts still often have unthreaded sections of shank.\n\nAlthough there is no reason to consider this definition obsolete, because it is far from clear that \"a bolt by definition takes a nut\" . Using a coach \"bolt\" as an example (and it has been a 'bolt' for a very long time). It was not originally intended to receive a nut, but did have a shank. Its purpose was not to pass through the entire substrate but only one piece of it, while the threaded portion bit into the other in order to draw, and clamp the materials together. The 'carriage' bolt was derived from this and was employed more to speed up manufacturing than achieve a different function. The carriage bolt passes through both pieces of materials and employs a nut to provide the clamping force. Both are still, however, bolts.\n\nThe distinctions above are enforced in the controlled vocabulary of standards organizations. Nevertheless, there are sometimes differences between the controlled vocabulary and the natural language use of the words by machinists, auto mechanics and others. These differences reflect linguistic evolution shaped by the changing of technology over centuries. The words \"bolt\" and \"screw\" have both existed since before today's modern mix of fastener types existed, and the natural usage of those words has evolved retronymously in response to the technological change. (That is, the use of words as names for objects changes as the objects change.) Non-threaded fasteners predominated until the advent of practical, inexpensive screw-cutting in the early 19th century. The basic meaning of the word \"screw\" has long involved the idea of a helical screw thread, but the Archimedes screw and the screw gimlet (like a corkscrew) preceded the fastener.\n\nThe word \"bolt\" is also a very old word, and it was used for centuries to refer to metal rods that passed through the substrate to be fastened on the other side, often via nonthreaded means (clinching, forge welding, pinning, wedging, etc.). The connection of this sense to the sense of a door bolt or the crossbow bolt is apparent. In the 19th century, bolts fastened via screw threads were often called \"screw bolts\" in contradistinction to \"clench bolts\".\n\nIn common usage, the distinction (not rigorous) is often that screws are smaller than bolts, and that screws are generally tapered while bolts are not. For example, cylinder head bolts are called \"bolts\" (at least in North American usage) despite the fact that by some definitions they ought to be called \"screws\". Their size and their similarity to a bolt that would take a nut seem linguistically to overrule any other factors in this natural word choice proclivity.\n\nBolts have been defined as headed fasteners having external threads that meet an exacting, uniform bolt thread specification (such as ISO metric screw thread M, MJ, Unified Thread Standard UN, UNR, and UNJ) such that they can accept a non-tapered nut. Screws are then defined as headed, externally threaded fasteners that do not meet the above definition of bolts. These definitions of screw and bolt eliminate the ambiguity of the \"Machinery's handbook\" distinction. And it is for that reason, perhaps, that some people favor them. However, they are neither compliant with common usage of the two words nor are they compliant with formal specifications.\n\nA possible distinction is that a screw is designed to cut its own thread; it has no need for access from or exposure to the opposite side of the component being fastened to. This definition of screw is further reinforced by the consideration of the developments of fasteners such as Tek Screws, with either round or hex heads, for roof cladding, self-drilling and self-tapping screws for various metal fastening applications, roof batten screws to reinforce the connection between the roof batten and the rafter, decking screws etc.\nOn the other hand, a bolt is the male part of a fastener system designed to be accepted by a pre-equipped socket (or nut) of exactly the same thread design.\n\nThreaded fasteners either have a tapered shank or a non-tapered shank. Fasteners with tapered shanks are designed to either be driven into a substrate directly or into a pilot hole in a substrate. Mating threads are formed in the substrate as these fasteners are driven in. Fasteners with a non-tapered shank are designed to mate with a nut or to be driven into a tapped hole.\n\nA fastener with a built in washer is called a SEM or SEMS, short for pre-asSEMbled. It could be fitted on either a tapered or non-tapered shank.\n\nA superbolt, or multi-jackbolt tensioner is an alternative type of fastener that retrofits or replaces existing nuts, bolts, or studs. Tension in the bolt is developed by torquing individual jackbolts, which are threaded through the body of the nut and push against a hardened washer. Because of this, the amount of torque required to achieve a given preload is reduced. Installation and removal of any size tensioner is achieved with hand tools, which can be advantageous when dealing with large diameter bolting applications.\n\nThe field of screws and other hardware for internal fixation within the body is huge and diverse. Like prosthetics, it integrates the industrial and medicosurgical fields, causing manufacturing technologies (such as machining, CAD/CAM, and 3D printing) to intersect with the art and science of medicine. Like aerospace and nuclear power, this field involves some of the highest technology for fasteners, as well as some of the highest prices, for the simple reason that performance, longevity, and quality have to be excellent in such applications. Bone screws tend to be made of stainless steel or titanium, and they often have high-end features such as conical threads, multistart threads, cannulation (hollow core), and proprietary screw drive types (some not seen outside of these applications).\n\nThese abbreviations have jargon currency among fastener specialists (who, working with many screw types all day long, have need to abbreviate repetitive mentions). The smaller basic ones can be built up into the longer ones; for example, if you know that \"FH\" means \"flat head\", then you may be able to parse the rest of a longer abbreviation containing \"FH\".\n\nThese abbreviations are not universally standardized across corporations; each corporation can coin their own. The more obscure ones may not be listed here.\n\nThe extra spacing between linked terms below helps the reader to see the correct parsing at a glance.\n\nScrews and bolts are usually made of steel. \nWhere great resistance to weather or corrosion is required, like in very small screws or medical implants, materials such as stainless steel, brass, titanium, bronze, silicon bronze or monel may be used.\n\nGalvanic corrosion of dissimilar metals can be prevented (using aluminum screws for double-glazing tracks for example) by a careful choice of material. \nSome types of plastic, such as nylon or polytetrafluoroethylene (PTFE), can be threaded and used for fastenings requiring moderate strength and great resistance to corrosion or for the purpose of electrical insulation.\n\nOften a surface coating is used to protect the fastener from corrosion (e.g. bright zinc plating for steel screws), to impart a decorative finish (e.g. japanning) or otherwise alter the surface properties of the base material.\n\nSelection criteria of the screw materials include: size, required strength, resistance to corrosion, joint material, cost and temperature.\n\nThe American Institute of Steel Construction (AISC) 13th Edition Steel Design Manual section 16.1 chapter J-3 specifies the requirements for bolted structural connections. Structural bolts replaced rivets due to the decreasing cost and increasing strength of structural bolts in the 20th century. Connections are formed with two types of joints: slip-critical connections and bearing connections. In slip-critical connections, movement of the connected parts is a serviceability condition and bolts are tightened to a minimum required pretension. Slip is prevented through friction of the \"faying\" surface, that is the plane of shear for the bolt and where two members make contact. Because friction is proportional to the normal force, connections must be sized with bolts numerous and large enough to provide the required load capacity. However, this greatly decreases the shear capacity of each bolt in the connection. The second (and more common type) of connection is a bearing connection. In this type of connection, the bolts carry the load through shear and are only tightened to a \"snug-fit\". These connections require fewer bolts than slip-critical connections and therefore are a less expensive alternative. Slip-critical connections are more common on flange plates for beam and column splices and moment critical connections. Bearing type connections are used in lightweight structures and in member connections where slip is not important and prevention of structural failure is the design constraint. Common bearing type connections include: shear tabs, beam supports, gusset plates in trusses.\n\nThe numbers stamped on the head of the bolt are referred to the grade of the bolt used in certain application with the strength of a bolt. High-strength steel bolts usually have a hexagonal head with an ISO strength rating (called \"property class\") stamped on the head. And the absence of marking/number indicates a lower grade bolt with low strength. The property classes most often used are 5.8, 8.8, and 10.9. The number before the point is the ultimate tensile strength in MPa divided by 100. The number after the point is the multiplier ratio of yield strength to ultimate tensile strength. For example, a property class 5.8 bolt has a nominal (minimum) ultimate tensile strength of 500 MPa, and a tensile yield strength of 0.8 times ultimate tensile strength or 0.8(500) = 400 MPa.\n\nUltimate tensile strength is the tensile stress at which the bolt fails. Tensile yield strength is the stress at which the bolt will yield in tension across the entire section of the bolt and receive a permanent set (an elongation from which it will not recover when the force is removed) of 0.2% offset strain. Proof strength is the usable strength of the fastener. Tension testing of a bolt up to the proof load should not cause permanent set of the bolt and should be conducted on actual fasteners rather than calculated. If a bolt is tensioned beyond the proof load, it may behave in plastic manner due to yielding in the threads and the tension preload may be lost due to the permanent plastic deformations. When elongating a fastener prior to reaching the yield point, the fastener is said to be operating in the elastic region; whereas elongation beyond the yield point is referred to as operating in the plastic region of the bolt material. If a bolt is loaded in tension beyond its proof strength, the yielding at the net root section of the bolt will continue until the entire section is begins to yield and it has exceeded its yield strength. If tension increases, the bolt fractures at its ultimate strength.\n\nMild steel bolts have property class 4.6, with is 400 MPa ultimate strength and 0.6*400=240 MPa yield strength. High-strength steel bolts have property class 8.8, which is 800 MPa ultimate strength and 0.8*800=640 MPa yield strength or above.\n\nThe same type of screw or bolt can be made in many different grades of material. For critical high-tensile-strength applications, low-grade bolts may fail, resulting in damage or injury. On SAE-standard bolts, a distinctive pattern of marking is impressed on the heads to allow inspection and validation of the strength of the bolt. However, low-cost counterfeit fasteners may be found with actual strength far less than indicated by the markings. Such inferior fasteners are a danger to life and property when used in aircraft, automobiles, heavy trucks, and similar critical applications.\n\nThere are many standards governing the material and mechanical properties of imperial sized externally threaded fasteners. Some of the most common consensus standards for grades produced from carbon steels are ASTM A193, ASTM A307, ASTM A354, ASTM F3125, and SAE J429. Some of the most common consensus standards for grades produced from corrosion resistant steels are ASTM F593 & ASTM A193.\n\nThe international standards for metric externally threaded fasteners are ISO 898-1 for property classes produced from carbon steels and ISO 3506-1 for property classes produced from corrosion resistant steels. \n\n\nSome varieties of screw are manufactured with a break-away head, which snaps off when adequate torque is applied. This prevents tampering and also provides an easily inspectable joint to guarantee proper assembly. An example of this is the shear bolts used on vehicle steering columns, to secure the ignition switch.\n\nModern screws employ a wide variety of drive designs, each requiring a different kind of tool to drive in or extract them. The most common screw drives are the slotted and Phillips in the US; hex, Robertson, and Torx are also common in some applications, and Pozidriv has almost completely replaced Phillips in Europe. Some types of drive are intended for automatic assembly in mass-production of such items as automobiles. More exotic screw drive types may be used in situations where tampering is undesirable, such as in electronic appliances that should not be serviced by the home repair person.\n\nThe hand tool used to drive in most screws is called a \"screwdriver\". A power tool that does the same job is a \"power screwdriver\"; power drills may also be used with screw-driving attachments. Where the holding power of the screwed joint is critical, torque-measuring and \"torque-limiting screwdrivers\" are used to ensure sufficient but not excessive force is developed by the screw. The hand tool for driving hex head threaded fasteners is a \"spanner\" (UK usage) or \"wrench\" (US usage), while a \"nut setter\" is used with a power screw driver.\n\nThere are many systems for specifying the dimensions of screws, but in much of the world the ISO metric screw thread preferred series has displaced the many older systems. Other relatively common systems include the British Standard Whitworth, BA system (British Association), and the Unified Thread Standard.\n\nThe basic principles of the ISO metric screw thread are defined in international standard ISO 68-1 and preferred combinations of diameter and pitch are listed in ISO 261. The smaller subset of diameter and pitch combinations commonly used in screws, nuts and bolts is given in ISO 262. The most commonly used pitch value for each diameter is the \"coarse pitch\". For some diameters, one or two additional \"fine pitch\" variants are also specified, for special applications such as threads in thin-walled pipes. ISO metric screw threads are designated by the letter M followed by the major diameter of the thread in millimeters (e.g. \"M8\"). If the thread does not use the normal \"coarse pitch\" (e.g. 1.25 mm in the case of M8), then the pitch in millimeters is also appended with a multiplication sign (e.g. \"M8×1\" if the screw thread has an outer diameter of 8 mm and advances by 1 mm per 360° rotation).\n\nThe nominal diameter of a metric screw is the outer diameter of the thread. The tapped hole (or nut) into which the screw fits, has an internal diameter which is the size of the screw minus the pitch of the thread. Thus, an M6 screw, which has a pitch of 1 mm, is made by threading a 6 mm shank, and the nut or threaded hole is made by tapping threads into a hole of 5 mm diameter (6 mm - 1 mm).\n\nMetric hexagon bolts, screws and nuts are specified, for example, in British Standard BS 4190 (general purpose screws) and BS 3692 (precision screws). The following table lists the relationship given in these standards between the thread size and the maximal width across the hexagonal flats (wrench size):\n\nIn addition, the following non-preferred intermediate sizes are specified:\n\nThe first person to create a standard (in about 1841) was the English engineer Sir Joseph Whitworth. Whitworth screw sizes are still used, both for repairing old machinery and where a coarser thread than the metric fastener thread is required. Whitworth became \"British Standard Whitworth\", abbreviated to BSW (BS 84:1956) and the \"British Standard Fine\" (BSF) thread was introduced in 1908 because the Whitworth thread was too coarse for some applications. The thread angle was 55°, and the depth and pitch varied with the diameter of the thread (i.e., the bigger the bolt, the coarser the thread). Spanners for Whitworth bolts are marked with the size of the bolt, not the distance across the flats of the screw head.\n\nThe most common use of a Whitworth pitch nowadays is in all UK scaffolding. Additionally, the standard photographic tripod thread, which for small cameras is 1/4\" Whitworth (20 tpi) and for medium/large format cameras is 3/8\" Whitworth (16 tpi). It is also used for microphone stands and their appropriate clips, again in both sizes, along with \"thread adapters\" to allow the smaller size to attach to items requiring the larger thread. Note that while 1/4\" UNC bolts fit 1/4\" BSW camera tripod bushes, yield strength is reduced by the different thread angles of 60° and 55° respectively.\n\nBritish Association (BA) screw threads, named after the British Association for Advancement of Science, were devised in 1884 and standardised in 1903. Screws were described as \"2BA\", \"4BA\" etc., the odd numbers being rarely used, except in equipment made prior to the 1970s for telephone exchanges in the UK. This equipment made extensive use of odd-numbered BA screws, in order—it may be suspected—to reduce theft. BA threads are specified by British Standard BS 93:1951 \"Specification for British Association (B.A.) screw threads with tolerances for sizes 0 B.A. to 16 B.A.\"\n\nWhile not related to ISO metric screws, the sizes were actually defined in metric terms, a 0BA thread having a 6 mm diameter and 1 mm pitch. Other threads in the BA series are related to 0BA in a geometric series with the common factors 0.9 and 1.2. For example, a 4BA thread has pitch formula_1 mm (0.65mm) and diameter formula_2 mm (3.62mm). Although 0BA has the same diameter and pitch as ISO M6, the threads have different forms and are not compatible.\n\nBA threads are still common in some niche applications. Certain types of fine machinery, such as moving-coil meters and clocks, tend to have BA threads wherever they are manufactured. BA sizes were also used extensively in aircraft, especially those manufactured in the United Kingdom. BA sizing is still used in railway signalling, mainly for the termination of electrical equipment and cabling.\n\nBA threads are extensively used in Model Engineering where the smaller hex head sizes make scale fastenings easier to represent. As a result, many UK Model Engineering suppliers still carry stocks of BA fasteners up to typically 8BA and 10BA. 5BA is also commonly used as it can be threaded onto 1/8 rod.\n\nThe Unified Thread Standard (UTS) is most commonly used in the United States, but is also extensively used in Canada and occasionally in other countries. The size of a UTS screw is described using the following format: X-Y, where X is the nominal size (the hole or slot size in standard manufacturing practice through which the shank of the screw can easily be pushed) and Y is the threads per inch (TPI). For sizes inch and larger the size is given as a fraction; for sizes less than this an integer is used, ranging from 0 to 16. The integer sizes can be converted to the actual diameter by using the formula 0.060 + 0.013 × number. For example, a #4 screw is 0.060 + 0.013 × 4 = 0.112 inches in diameter. There are also screw sizes smaller than \"0\" (zero or ought). The sizes are 00, 000, 0000 which are usually referred to as two ought, three ought, and four ought. Most eyeglasses have the bows screwed to the frame with 00-72 (pronounced double ought – seventy two) size screws. To calculate the major diameter of \"ought\" size screws count the number of 0's and multiply this number by .013 and subtract from .060. For example, the major diameter of a 000-72 screw thread is .060 – (3 x .013) = .060-.039 = .021 inches. For most size screws there are multiple TPI available, with the most common being designated a Unified Coarse Thread (UNC or UN) and Unified Fine Thread (UNF or UF). Note: In countries other than the United States and Canada, the ISO Metric Screw Thread System is primarily used today. Unlike most other countries the United States and Canada still use the Unified (Inch) Thread System. However, both are moving over to the ISO Metric System. It is estimated that approximately 60% of screw threads in use in the United States are still inch based.[67]\n\nThere are three steps in manufacturing a screw: \"heading\", \"thread rolling\", and \"coating\". Screws are normally made from wire, which is supplied in large coils, or round bar stock for larger screws. The wire or rod is then cut to the proper length for the type of screw being made; this workpiece is known as a \"blank\". It is then cold headed, which is a cold working process. Heading produces the \"head\" of the screw. The shape of the die in the machine dictates what features are pressed into the screw head; for example a flat head screw uses a flat die. For more complicated shapes two heading processes are required to get all of the features into the screw head. This production method is used because heading has a very high production rate, and produces virtually no waste material. Slotted head screws require an extra step to cut the slot in the head; this is done on a \"slotting machine\". These machines are essentially stripped down milling machines designed to process as many blanks as possible.\nThe blanks are then polished again prior to threading. The threads are usually produced via thread rolling, however some are cut. The workpiece is then tumble finished with wood and leather media to do final cleaning and polishing. For most screws, a coating, such as electroplating with zinc (galvanizing) or applying black oxide, is applied to prevent corrosion.\n\nWhile a recent hypothesis attributes the Archimedes' screw to Sennacherib, King of Assyria, archaeological finds and pictorial evidence only appear in the Hellenistic period and the standard view holds the device to be a Greek invention, most probably by the 3rd century BC polymath Archimedes. Though resembling a screw, this is not a screw in the usual sense of the word.\n\nEarlier, the screw had been described by the Greek mathematician Archytas of Tarentum (428–350 BC). By the 1st century BC, wooden screws were commonly used throughout the Mediterranean world in screw presses for pressing olive oil from olives and pressing juice from grapes in winemaking. Metal screws used as fasteners were rare in Europe before the 15th century, if known at all.\n\nRybczynski has shown that handheld screwdrivers (formerly called \"turnscrews\" in English, in more direct parallel to their original French name, \"tournevis\") have existed since medieval times (the 1580s at the latest), although they probably did not become truly widespread until after 1800, once threaded fasteners had become commodified, as detailed below.\n\nThere were many forms of fastening in use before threaded fasteners became widespread. They tended to involve carpentry and smithing rather than machining, and they involved concepts such as dowels and pins, wedging, mortises and tenons, dovetails, nailing (with or without clenching the nail ends), forge welding, and many kinds of binding with cord made of leather or fiber, using many kinds of knots. Prior to the mid-19th century, cotter pins or pin bolts, and \"clinch bolts\" (now called rivets), were used in shipbuilding. Glues also existed, although not in the profusion seen today.\n\nThe metal screw did not become a common fastener until machine tools for their mass production were developed toward the end of the 18th century. This development blossomed in the 1760s and 1770s along two separate paths that soon converged: the mass production of \"wood\" screws (meaning screws made of metal to be used in wood) in a specialized, single-purpose, high-volume-production machine tool; and the low-count, toolroom-style production of \"machine\" screws (V-thread) with easy selection among various pitches (whatever the machinist happened to need on any given day).\n\nThe first path was pioneered by brothers Job and William Wyatt of Staffordshire, UK, who patented in 1760 a machine that we might today best call a screw machine of an early and prescient sort. It made use of a leadscrew to guide the cutter to produce the desired pitch, and the slot was cut with a rotary file while the main spindle held still (presaging live tools on lathes 250 years later). Not until 1776 did the Wyatt brothers have a wood-screw factory up and running. Their enterprise failed, but new owners soon made it prosper, and in the 1780s they were producing 16,000 screws a day with only 30 employees—the kind of industrial productivity and output volume that would later be characteristic of modern industry but was revolutionary at the time.\n\nMeanwhile, English instrument maker Jesse Ramsden (1735–1800) was working on the toolmaking and end of the screw-cutting problem, and in 1777 he invented the first satisfactory screw-cutting lathe. The British engineer Henry Maudslay (1771–1831) gained fame by popularizing such lathes with his screw-cutting lathes of 1797 and 1800, containing the trifecta of leadscrew, slide rest, and change-gear gear train, all in the right proportions for industrial machining. In a sense he unified the paths of the Wyatts and Ramsden and did for machine screws what had already been done for wood screws, i.e., significant easing of production spurring commodification. His firm would remain a leader in machine tools for decades afterward. A misquoting of James Nasmyth popularized the notion that Maudslay had \"invented\" the slide rest, but this was incorrect; however, his lathes helped to popularize it.\n\nThese developments of the 1760–1800 era, with the Wyatts and Maudslay being arguably the most important drivers, caused great increase in the use of threaded fasteners. Standardization of threadforms began almost immediately, but it was not quickly completed; it has been an evolving process ever since. Further improvements to the mass production of screws continued to push unit prices lower and lower for decades to come, throughout the 19th century.\n\nThe American development of the turret lathe (1840s) and of automatic screw machines derived from it (1870s) drastically reduced the unit cost of threaded fasteners by increasingly automating the machine tool control. This cost reduction spurred ever greater use of screws.\n\nThroughout the 19th century, the most commonly used forms of screw head (that is, drive types) were simple internal-wrenching straight slots and external-wrenching squares and hexagons. These were easy to machine and served most applications adequately. Rybczynski describes a flurry of patents for alternative drive types in the 1860s through 1890s, but explains that these were patented but not manufactured due to the difficulties and expense of doing so at the time. In 1908, Canadian P. L. Robertson was the first to make the internal-wrenching square socket drive a practical reality by developing just the right design (slight taper angles and overall proportions) to allow the head to be stamped easily but successfully, with the metal cold forming as desired rather than being sheared or displaced in unwanted ways. Practical manufacture of the internal-wrenching hexagon drive (hex socket) shortly followed in 1911.\n\nIn the early 1930s, the popular Phillips-head screw was invented by American Henry F. Phillips.\n\nThreadform standardization further improved in the late 1940s, when the ISO metric screw thread and the Unified Thread Standard were defined.\n\nPrecision screws, for controlling motion rather than fastening, developed around the turn of the 19th century, were one of the central technical advances, along with flat surfaces, that enabled the industrial revolution. They are key components of micrometers and lathes.\n\nAlternative fastening methods are:\n\n\n"}
{"id": "31857185", "url": "https://en.wikipedia.org/wiki?curid=31857185", "title": "Shaft voltage", "text": "Shaft voltage\n\nShaft voltage occurs in electric motors and generators due to leakage, induction, or capacitive coupling with the windings of the motor. It can occur in motors powered by variable-frequency drives, as often used in heating, ventilation, air conditioning and refrigeration systems. DC machines may have leakage current from the armature windings that energizes the shaft. Currents due to shaft voltage causes deterioration of motor bearings, but can be prevented with a grounding brush on the shaft, grounding of the motor frame, insulation of the bearing supports, or shielding.\n\nShaft voltage can be induced by non-symmetrical magnetic fields of the motor (or generator) itself. External sources of shaft voltage include other coupled machines, and electrostatic charging due to rubber belts rubbing on drive pulleys.\n\nEvery rotor has some degree of capacitive coupling to the motor's electrical windings, but the effective inline capacitor acts as a high-pass filter, so the coupling is often weak at 50–60 Hz line frequency. But many Variable Frequency Drives (VFD) induce significant voltage onto the shaft of the driven motor, because of the kilohertz switching of the insulated gate bipolar transistors (IGBTs), which produce the pulse width modulation used to control the motor. The presence of high frequency ground currents can cause sparks, arcing and electrical shocks and can damage bearings.\n\nTechniques used to minimise this problem include: insulation, alternate discharge paths, Faraday shield, insulated bearings, ceramic bearings, grounding brush and shaft grounding ring.\n\nAn electrostatic shielded induction motor (ESIM) is one approach to the shaft-voltage problem, as the insulation reduces voltage levels below the dielectric breakdown. This effectively stops bearing degradation and offers one solution to accelerated bearing wear caused by fluting, induced by pulsewidth modulated (PWM) inverters.\n\nGrounding the shaft by installing a grounding brush device on either the non-drive end or drive end of a VFD electric motor provides an alternate low-impedance path from the motor shaft to the motor case. This method channels the current away from the bearings. It significantly reduces shaft voltage and therefore bearing current by not allowing voltage to build up on the rotor. Low maintenance grounding brushes have been field tested and proven to run for as long as 10 years at 1800rpm while maintaining the necessary target voltage that prevents electrical bearing damage for the entire life of the brush. \n\nA shaft grounding ring (SGR) is similar to a grounding brush, except that this brush makes use of conductive micro fibers, creating a low impedance path from the motor shaft to ground.\n\nInsulated bearings eliminate the path to ground through the bearing for current to flow. However, installing insulated bearings does not eliminate the shaft voltage, which will still find the lowest impedance path to ground. This can potentially cause a problem if the path happens to be through the driven load or through some other component.\n\nHigh frequency grounding can be significantly improved by installing shielded cable with an extremely low impedance path between the VFD and the motor. One popular cable type is continuous corrugated aluminum sheath cable.\n\n\n"}
{"id": "8849460", "url": "https://en.wikipedia.org/wiki?curid=8849460", "title": "Split-ring resonator", "text": "Split-ring resonator\n\nA split-ring resonator (SRR) is an artificially produced structure common to metamaterials. Their purpose is to produce the desired magnetic susceptibility (magnetic response) in various types of metamaterials up to 200 terahertz. These media create the necessary strong magnetic coupling to an applied electromagnetic field, not otherwise available in conventional materials. For example, an effect such as negative permeability is produced with a periodic array of split ring resonators.\n\nA single cell SRR has a pair of enclosed loops with splits in them at opposite ends. The loops are made of nonmagnetic metal like copper and have a small gap between them. The loops can be concentric, or square, and gapped as needed. A magnetic flux penetrating the metal rings will induce rotating currents in the rings, which produce their own flux to enhance or oppose the incident field (depending on the SRRs resonant properties). This field pattern is dipolar. The small gaps between the rings produces large capacitance values which lower the resonating frequency. Hence the dimensions of the structure are small compared to the resonant wavelength. This results in low radiative losses, and very high quality factors.\n\nSplit ring resonators (SRRs) consist of a pair of concentric metallic rings, etched on a dielectric substrate, with slits etched on opposite sides. SRRs can produce an effect of being electrically smaller when responding to an oscillating electromagnetic field. These resonators have been used for the synthesis of left handed and negative refractive index media, where the necessary value of the negative effective permeability is due to the presence of the SRRs. When an array of electrically small SRRs is excited by means of a time varying magnetic field, the structure behaves as an effective medium with negative effective permeability in a narrow band above SRR resonance. SRRs have also been coupled to planar transmission lines, for the synthesis of transmission line metamaterials.\n\nThe split ring resonator and the metamaterial itself are composite materials. Each SRR has an individual tailored response to the electromagnetic field. However, the periodic construction of many SRR cells is such that the electromagnetic wave interacts as if these were homogeneous materials. This is similar to how light actually interacts with everyday materials; materials such as glass or lenses are made of atoms, an averaging or macroscopic effect is produced.\n\nThe SRR is designed to mimic the magnetic response of atoms, only on a much larger scale. Also, as part of periodic composite structure these are designed to have a stronger magnetic coupling than is found in nature. The larger scale allows for more control over the magnetic response, while each unit is smaller than the radiated electromagnetic wave.\n\nSRRs are much more active than ferromagnetic materials found in nature. The pronounced magnetic response in such lightweight materials demonstrates an advantage over heavier, naturally occurring materials. Each unit can be designed to have its own magnetic response. The response can be enhanced or lessened as desired. In addition, the overall effect reduces power requirements.\n\nThere are a variety of split-ring resonators and periodic structures: rod-split-rings, nested split-rings, single split rings, deformed split-rings, spiral split-rings, and extended S-structures. The variations of split ring resonators have achieved different results, including smaller and higher frequency structures. The research which involves some of these types are discussed throughout the article.\n\nTo date (December 2009) the capability for desired results in the visible spectrum has not been achieved. However, in 2005 it was noted that, physically, a nested circular split-ring resonator must have an inner radii of 30 to 40 nanometers for success in the mid-range of the visible spectrum. Microfabrication and nanofabrication techniques may utilize direct laser beam writing or electron beam lithography depending on the desired resolution.\n\nSplit-ring resonators (SRR) are one of the most common elements used to fabricate metamaterials. Split-ring resonators are non-magnetic materials The first ones were usually fabricated from circuit board material to create metamaterials.\n\nLooking at the image directly to the right, it can be seen that at first a single SRR looked like an object with a two square perimeters, and each perimeter with small segment removed, which results in squared \"C\" shapes, on fiberglass, printed circuit board material. In this type of configuration it is actually two concentric bands of non-magnetic conductor material. There is one gap in each band placed 180° relative to each other. The gap in each band gives it the distinctive \"C\" shape, rather than a totally circular or square shape. Then multiple cells of this double band configuration are fabricated onto circuit board material by an etching technique and lined with copper wire strip arrays are added. After processing, the boards are cut and assembled into an interlocking unit. It is constructed into a periodic array with a large number of SRRs.\n\nThere are now a number of different configurations that use the SRR nomenclature.\n\nA periodic array of SRRs was used for the first demonstration of a negative index of refraction. For this demonstration, \"square shaped SRRs\", with the lined wire configurations, were fabricated into a periodic, arrayed, cell structure. This is the substance of the metamaterial. Then a metamaterial prism was cut from this material. The prism experiment demonstrated a negative index of refraction for the first time in the year 2000; the paper about the demonstration was submitted to the journal Science on January 8, 2001, accepted on February 22, 2001 and published on April 6, 2001.\n\nJust before this prism experiment, Pendry et al. was able to demonstrate that a three-dimensional array of intersecting thin wires could be used to create negative values of ε. In a later demonstration, a periodic array of copper split-ring resonators could produce an effective negative μ. In 2000 Smith et al. were the first to successfully combine the two arrays and produce a LHM which had negative values of ε and μ for a band of frequencies in the GHz range.\n\nSRRs were first used to fabricate left-handed metamaterials for the microwave range, and several years later for the terahertz range. By 2007, experimental demonstration of this structure at microwave frequencies has been achieved by many groups. In addition, SRRs have been used for research in acoustic metamaterials. The arrayed SRRs and wires of the first Left-handed metamaterial were melded into alternating layers. This concept and methodology was then applied to (dielectric) materials with optical resonances producing negative effective permittivity for certain frequency intervals resulting in \"photonic bandgap frequencies\". Another analysis showed Left Handed Material to be fabricated from inhomogeneous constituents, which yet results in a macroscopically homogeneous material. SRRs had been used to focus a signal from a point source, increasing the transmission distance for near field waves. Furthermore, another analysis showed SRRs with a negative index of refraction capable of high-frequency magnetic response, which created an artificial magnetic device composed of non-magnetic materials (dielectric circuit board).\n\nThe resonance phenomena that occurs in this system is essential to achieving the desired effects.\n\nSRRs also exhibit resonant electric response in addition to their resonant magnetic response. The response, when combined with an array of identical wires is averaged over the whole composite structure which results in effective values, including the refractive index. The original logic behind SRRs specifically, and metamaterials generally was to create a structure, which imitates an arrayed atomic structure only on a much larger scale.\n\nIn research based in metamaterials, and specifically negative refractive index, there are different types of split-ring resonators. Of the examples mentioned below most all of them have a gap in each ring. In other words, with a double ring structure, each ring has a gap.\n\nThere is the \"1-D Split-Ring Structure\" with two square rings, one inside the other. One set of cited \"unit cell\" dimensions would be an outer square of 2.62 mm and an inner square of 0.25 mm. 1-D structures such as this are easier to fabricate compared with constructing a rigid 2-D structure.\n\nThe \"Symmetrical-Ring Structure\" is another classic example. Described by the nomenclature these are two rectangular square D type configurations, exactly the same size, lying flat, side by side, in the unit cell. Also these are not concentric. One set of cited dimensions are 2 mm on the shorter side, and 3.12 mm on the longer side. The gaps in each ring face each other, in the unit cell.\n\nThe \"Omega Structure\", as the nomenclature describes, has an Ω-shaped ring structure. There are two of these, standing vertical, side by side, instead of lying flat, in the unit cell. In 2005 these were considered to be a new type of metamaterial. One set of cited dimensions are annular parameters of R = 1.4 mm and r = 1 mm, and the straight edge is 3.33 mm.\n\nAnother new metamaterial in 2005 was a coupled “S” shaped structure. There are two vertical \"S\" shaped structures, side by side, in a unit cell. There is no gap as in the ring structure, however there is a space between the top and middle parts of the S and space between the middle part and bottom part of the S. Furthermore, it still has the properties of having an electric plasma frequency and a magnetic resonant frequency.\n\nOther types of split-ring resonators are the spiral resonator with 8 loops. broadside coupled\nsplit-ring resonator (BC-SRR). Two-layer multi spiral resonator (TL-MSR), the broad-side coupled spiral resonator with four turns, the complementary split ring resonator, the open split-ring resonator (OSRR), and the open\ncomplementary split-ring resonator (OCSRR). Transmission line configurations include SRR-based CRLH (composite right-left-handed) transmission line and its equivalent compliment.\n\nOn May 1, 2000 conducting wires were placed symmetrically within each cell of a periodic split-ring resonator array which achieved negative propagation of electromagnetic waves in the microwave region. The concept was and still is used to build interacting elements smaller than the applied electromagnetic radiation. In addition, the spacing between, referred to as the lattice constant, is also smaller than the applied radiation.\n\nAdditionally, the splits in the ring allow the SRR unit to achieve resonance at wavelengths much larger than the diameter of the ring. The unit is designed to generate a large capacitance, lower the resonant frequency, and concentrate the electric field. Combining units creates a design as a periodic medium. Furthermore, the multiple unit structure has strong magnetic coupling with low radiative losses. \nResearch has also covered variations in magnetic resonances for different SRR configurations. \nResearch has continued into terahertz radiations with SRRs Other related work fashioned metamaterial configurations with non-SRR structures. These can be constructed with materials such as periodic metallic crosses, or an ever-widening concentric ring structures known as Swiss rolls. Permeability for only the red wavelength at 780 nm has been analyzed and along with other related work \n\n\n"}
{"id": "37356948", "url": "https://en.wikipedia.org/wiki?curid=37356948", "title": "Stallion Bus and Transit Corp.", "text": "Stallion Bus and Transit Corp.\n\nStallion Bus is an American bus manufacturer and distributor. They are the North American distributor for Higer Bus from China and their primary products include mid-size coach body built on a Freightliner chassis, Cutaway buses, Customized Mercedes Sprinter vans, and specialty vehicles. Their equipment meets \"Buy America\" standards and is Altoona Rated. The company was founded in 2006 and now supplies buses all over the United States, Canada, and abroad.\n\n2006–present (Example: 938L)\nStallion currently produces many different product lines. All current models are wide, exclusive of mirrors.\n\n"}
{"id": "22101239", "url": "https://en.wikipedia.org/wiki?curid=22101239", "title": "Stannide", "text": "Stannide\n\nA stannide can refer to an intermetallic compound containing tin combined with one or more other metals; an anion consisting solely of tin atoms or a compound containing such an anion, or, in the field of organometallic chemistry an ionic compound containing an organotin anion (e.g.see an alternative name for such a compound is stannanide.)\n\nWhen tin is combined with an alkali or alkaline earth metal some of the compounds formed have ionic structures containing monatomic or polyatomic tin anions (Zintl ions), such as Sn in MgSn or in KSn.\nEven with these metals not all of the compounds formed can be considered to be ionic with localised bonding, for example SrSn, a metallic compound, contains {Sn} square pyramidal units.\n\nTernary (where there is an alkali or alkaline earth metal, a transition metal as well as tin e.g. LiRhSn and MgRuSn) have been investigated.\n\nBinary (involving one other metal) and ternary (involving two other metals) intermetallic stannides have been investigated. Niobium stannide, NbSn is perhaps the best known superconducting tin intermetallics. This is more commonly called \"niobium-tin\".\n\nSome examples of stannide Zintl ions are listed below. Some of them contain 2-centre 2-electron bonds (2c-2e), others are \"electron deficient\" and bonding sometimes can be described using polyhedral skeletal electron pair theory (Wade's rules) where the number of valence electrons contributed by each tin atom is considered to be 2 (the s electrons do not contribute). There are some examples of silicide and plumbide ions with similar structures, for example tetrahedral , the chain anion (Si), and .\n"}
{"id": "35262029", "url": "https://en.wikipedia.org/wiki?curid=35262029", "title": "Sustainable aviation fuel", "text": "Sustainable aviation fuel\n\nSustainable aviation fuel (SAF) is the name given to advanced aviation biofuel types used in jet aircraft and certified as being sustainable by a reputable independent third-party, such as the Roundtable on Sustainable Biomaterials (RSB). This certification is in addition to the safety and performance certification, issued by global standards body ASTM International, that all jet fuel is required to meet in order to be approved for use in regular passenger flights.\n\nA SAF sustainability certification verifies that the fuel product, mainly focussing on the biomass feedstock, has met criteria focussed around long-term global environmental, social and economic \"triple-bottom-line\" sustainability considerations. Under many carbon emission regulation schemes, such as the European Union Emissions Trading Scheme, a certified SAF product may be granted an exemption from an associated carbon compliance liability cost.  This marginally improves the economic competitiveness of environmentally favourable SAF over traditional fossil-based jet fuel. However, in the near term there are several commercialisation and regulatory hurdles that are yet to be overcome through the collaboration of a variety of stakeholders for SAF products to meet price parity with traditional jet fuel and to enable widespread uptake.\n\nThe first reputable body to launch a sustainable biofuel certification system applicable to SAF was the academic European-based Roundtable on Sustainable Biomaterials (RSB) NGO. This multi-stakeholder organization set a global benchmark standard on which the sustainability integrity of advanced aviation biofuel types seeking to use the claim of being a Sustainable Aviation Fuel can be judged. Leading airlines in the aviation industry and other signatories to the Sustainable Aviation Fuel Users Group (SAFUG) pledge support the RSB as the preferred provider of SAF certification. These airlines believe it important for any proposed aviation biofuels have independently certified sustainable biofuel long term environmental benefits compared to the status quo in order to ensure their successful uptake and marketability \n\nAs emissions trading schemes and other carbon compliance regimes are emerging globally certain biofuels are likely to be exempt, \"zero rated\", by governments from having an associated carbon compliance liability due to their closed-emissions-loop renewable nature if they can also prove their wider sustainability credentials. For example, in the European Union Emissions Trading Scheme it has been proposed by SAFUG that only aviation biofuels that have been certified as sustainable by the RSB or similar bodies would be zero rated. This proposal has been accepted.\n\nSAFUG was formed by a group of interested airlines in 2008 under the auspices of Boeing Commercial Airplanes and in cooperation with support from NGOs such as Natural Resources Defense Council. Member airlines represent more than 15% of the industry, and all member CEOs have signed a pledge to work on the development and use of Sustainable Aviation Fuel.\n\nIn addition to SAF certification, the integrity of aviation biofuel producers and their product can be assessed by further means such as by using Richard Branson's Carbon War Room Renewable Jets Fuels initiative. \nA leading independent NGO focused on this issue is the Sustainable Sky Institute \n\n\n\nRegional SAF Roadmap initiatives:\n"}
{"id": "63243", "url": "https://en.wikipedia.org/wiki?curid=63243", "title": "Symbol (chemistry)", "text": "Symbol (chemistry)\n\nIn relation to the chemical elements, a symbol is a code for a chemical element. Many functional groups have their own chemical symbol, e.g. Ph for the phenyl group, and Me for the methyl group. Chemical symbols for elements normally consist of one or two letters from the Latin alphabet, but can contain three when the element has a systematic temporary name (as of March 2017, no discovered elements have such a name), and are written with the first letter capitalized.\n\nEarlier chemical element symbols stem from classical Latin and Greek vocabulary. For some elements, this is because the material was known in ancient times, while for others, the name is a more recent invention. For example, \"He\" is the symbol for helium (New Latin name, not known in ancient Roman times), \"Pb\" for lead (\"plumbum\" in Latin), and \"Hg\" for mercury (\"hydrargyrum\" in Greek). Some symbols come from other sources, like \"W\" for tungsten (\"Wolfram\" in German, not known in Roman times).\n\nTemporary symbols assigned to newly or not-yet synthesized elements use 3-letter symbols based on their atomic numbers. For example, \"Uno\" was the temporary symbol for hassium (element 108) which had the temporary name of \"unniloctium\".\n\nChemical symbols may be modified by the use of prepended superscripts or subscripts to specify a particular isotope of an atom. Additionally, appended superscripts may be used to indicate the ionization or oxidation state of an element. They are widely used in chemistry and they have been officially chosen by the International Union of Pure and Applied Chemistry (IUPAC). There are also some historical symbols that are no longer officially used.\nAttached subscripts or superscripts specifying a nuclide or molecule have the following meanings and positions:\nIn Chinese, each chemical element has a dedicated character, usually created for the purpose (see Chemical elements in East Asian languages). However, Latin symbols are also used, especially in formulas.\nA list of current, dated, as well as proposed and historical signs and symbols is included here with its signification. Also given is each element's atomic number, atomic weight or the atomic mass of the most stable isotope, group and period numbers on the periodic table, and etymology of the symbol.\n\nHazard pictographs are another type of symbols used in chemistry.\n\nAntimatter atoms are denoted by a bar above the symbol for their matter counterpart, so e.g. H is the symbol for antihydrogen.\n\nThe following is a list of symbols and names formerly used or suggested for elements, including symbols for placeholder names and names given by discredited claimants for discovery. \nThe following is a list of pictographic symbols employed to symbolize elements known since ancient times (for example to the alchemists). Not included in this list are symbolic representations of substances previously called elements (such as certain rare earth mineral blends and the classical elements fire and water of ancient philosophy) which are known today to be multi-atomic. Also not included are symbolic representations currently used for elements in other languages such as the Chinese characters for elements. Modern alphabetic notation was introduced in 1814 by Jöns Jakob Berzelius.\nThe following is a list of isotopes of elements given in the previous tables which have been designated unique symbols. By this it is meant that a comprehensive list of current systematic symbols (in the Atom form) are not included in the list and can instead be found in the chart. The symbols for the named isotopes of hydrogen, deuterium (D) and tritium (T) are still in use today, as is thoron (Tn) for radon-220 (though not actinon; An is usually used instead for a generic actinide). Heavy water and other deuterated solvents are commonly used in chemistry, and it is convenient to use a single character rather than a symbol with a subscript in these cases. The practice also continues with tritium compounds. When the name of the solvent is given, a lowercase d is sometimes used. For example, d-benzene and CD can be used instead of [H]CH.\n\nThe symbols for isotopes of elements other than hydrogen and radon are no longer in use within the scientific community. Many of these symbols were designated during the early years of radiochemistry, and several isotopes (namely those in the actinium decay family, the radium decay family, and the thorium decay family) bear placeholder names using the early naming system devised by Ernest Rutherford.\n\nGeneral:\n\n\nFrom organic chemistry:\n\nExotic atoms:\n\n\n\n"}
{"id": "20869422", "url": "https://en.wikipedia.org/wiki?curid=20869422", "title": "Tea leaf paradox", "text": "Tea leaf paradox\n\nThe tea leaf paradox describes a phenomenon where tea leaves in a cup of tea migrate to the center and bottom of the cup after being stirred rather than being forced to the edges of the cup, as would be expected in a spiral centrifuge. The correct physical explanation of the paradox was for the first time given by James Thomson in 1857. He correctly connected the appearance of secondary flow (in both Earth atmosphere and tea cup) with ″friction on the bottom″ . The formation of secondary flows in an annular channel was theoretically treated by Boussinesq as early as in 1868. The migration of near-bottom particles in river-bend flows was experimentally investigated by A. Ya. Milovich in 1913. The solution first came from Albert Einstein in a 1926 paper in which he explained the erosion of river banks, and repudiated Baer's law.\n\nStirring the liquid makes it spin around the cup. In order to maintain this curved path, a centripetal force in towards the center is needed (similar to the tension in a string when spinning a bucket over your head). This is accomplished by a pressure gradient outward (higher pressure outside than inside).\n\nHowever, near the bottom and outer edges the liquid is slowed by the friction against the cup. There the fictitious (inertial) centrifugal force is weaker and cannot overcome the pressure gradient, so these pressure differences become more important for the water flow. This is called a boundary layer or more specifically an Ekman layer.\n\nThe inertial centrifugal force due to the bulk rotation of the liquid results in the development of an outward pressure gradient within the liquid, where the pressure is higher along the rim than in the middle. This manifests itself as the formation of a concave liquid-air interface. This pressure gradient provides the necessary centripetal forces for circular motion when summed over the entirety of the rotating liquid.\n\nHowever, within the boundary layers where fluid rotation is slowed by friction and viscous effects, the centripetal force due to pressure gradient is dominant over the inertial forces from rotation, and creates an inward secondary flow within the boundary layer. The flow converges at the bottom of the teacup (where the tea leaves are observed to gather) and flows upwards to the surface. Higher up, the liquid flow meets the surface and flows outwards. The leaves are too heavy to lift upwards and remain in the middle. Combined with the primary rotational flow, the leaves will be observed to spiral inwards along the bottom of the teacup.\n\nThe phenomenon has been used to develop a new technique to separate red blood cells from blood plasma, to understand atmospheric pressure systems, and in the process of brewing beer to separate out coagulated trub in the whirlpool.\n\n\n"}
{"id": "11255805", "url": "https://en.wikipedia.org/wiki?curid=11255805", "title": "The Carbon War", "text": "The Carbon War\n\nThe Carbon War: Global Warming and the End of the Oil Era is a 1999 book by former oil geologist Jeremy Leggett about global warming.\n\n\n\n"}
{"id": "45523123", "url": "https://en.wikipedia.org/wiki?curid=45523123", "title": "Thermie", "text": "Thermie\n\nA thermie (th) is a non-SI metric unit of heat energy, part of the metre-tonne-second system sometimes used by European engineers. The thermie is equal to the amount of energy required to raise the temperature of 1 tonne (1,000 kg) of water at 14.5 °C at standard atmospheric pressure by 1 °C. The thermie is equivalent to 1,000 kilocalories, 4.1868 megajoules or 3968.3 BTU.\n"}
{"id": "2689629", "url": "https://en.wikipedia.org/wiki?curid=2689629", "title": "Tissue-pack marketing", "text": "Tissue-pack marketing\n\nThe concept of tissue-pack marketing was first developed in Japan. Its origins date back to the late 1960s when Hiroshi Mori, the founder of a paper-goods manufacturer in Kōchi Prefecture called Meisei Industrial Co., was looking for ways to expand demand for paper products. At the time, the most common marketing freebie in Japan was boxes of matches. These were often given away at banks and then used by women in the kitchen. Mori figured tissues would have even wider appeal than the matches, and as a result he developed the machinery to fold and package tissues into easy-to-carry, pocket-size packs. The new product was marketed only as a form of advertising and was not sold to consumers.\n\nWhere the more traditional flyers are often discarded without being read or simply not accepted by the consumer, the same is not true of advertising tissue-packs. The most important reason for this is because the tissues add functionality to the advertisement. This functionality has several benefits:\n\n\n\n\nJapan is still the main market for tissue-pack advertising, but the practice has begun to spread overseas. \n\nPocket tissue advertising was introduced in Montreal, Canada, in December 2000 by Promotion Par Main. In April 2005, this marketing method was also used in Ontario, Canada, by Hold'em Promotions Inc., after the company's founders saw tissue advertising during a trip to China.\n\nIn the United States, a subsidiary of the Japanese trading company Itochu, AdPack USA, introduced tissue-pack marketing in New York in 2005, and now offers it throughout the country. \n\nIn 2012, the tissue marketing company Adtishoo launched operations in the United Kingdom.\n\n"}
{"id": "1675998", "url": "https://en.wikipedia.org/wiki?curid=1675998", "title": "Traction substation", "text": "Traction substation\n\nA traction substation, traction current converter plant or traction power substation (TPSS) is an electrical substation that converts electric power from the form provided by the electrical power industry for public utility service to an appropriate voltage, current type and frequency to supply railways, trams (streetcars) or trolleybuses with traction current.\n\nThese systems can be used to convert three-phase 50Hz or 60Hz alternating current (AC) for the supply of AC railway electrification systems at a lower frequency and single phase, as used by many older systems, or to rectify AC into direct current (DC) for those systems (primarily public transit systems) using DC for traction power.\n\nOriginally, the conversion equipment usually consisted of one or more motor-generator sets containing three-phase synchronous AC motors and single-phase AC generators, mechanically coupled to a common shaft. Rotary converters were also used, especially where the desired output was DC current from an AC source.\n\nIn the 1920s, DC was derived using electronic valves (mercury arc rectifiers). In modern systems, high-voltage DC (HVDC) \"back-to-back\" stations are used instead of mechanical equipment to convert between different frequencies and phases of AC power and solid-state thyristor rectifier systems are used for conversion from AC power to DC traction power.\n\nTraction current converter plants are either decentralized (where one plant directly supplies the overhead lines or third rail of the traction system, with no feed into a traction current distribution network) or centralized (for the supply of the traction power network, usually in addition to the direct supply of the overhead lines or third rail).\n\nCentral traction current converter plants are generally found in Germany (primarily in the cities of Neckarwestheim, Ulm, Nuremberg), Austria and Switzerland, while decentralized traction current converter plants are generally found in Norway, Sweden and the German states of Mecklenburg-Vorpommern and Brandenburg as well as parts of Great Britain. A List of railway electrification systems provides further detail.\n\n"}
