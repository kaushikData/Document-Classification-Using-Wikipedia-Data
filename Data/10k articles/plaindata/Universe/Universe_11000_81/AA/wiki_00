{"id": "39094534", "url": "https://en.wikipedia.org/wiki?curid=39094534", "title": "2013 extreme weather events", "text": "2013 extreme weather events\n\nThe 2013 extreme weather events included several all-time temperature records in Northern and Southern Hemisphere. The February extent of snow cover in Eurasia and North America was above average, while the extent of Arctic ice in the same month was 4,5% below the 1981–2010 average. The Northern Hemisphere weather extremes have been linked to the melting of Arctic sea ice, which alters atmospheric circulation in a way that leads to more snow and ice.\n\nBy January 11, 233 weather-related deaths were reported in India. Elsewhere, particularly in Russia, the Czech Republic and the United Kingdom, low temperatures affected wildlife, delaying bird breeding and disrupting the bird migration. On January 10 Bangladesh faced the lowest temperature since country's independence, at in Saidpur. While Finland and most of Northern European countries got the record high, and even the highest temperatures at Europe during May and June, Western- and Middle Europe faced much cooler weather and even their wettest May and June ever. During summer prolonged heat waves in the Northern Hemisphere set new record high temperatures.\n\nOn March 24, 2014, the secretary-general of the World Meteorological Organization Michel Jarraud announced that \"many of the extreme events of 2013 were consistent with what we would expect as a result of human-induced climate change\".\n\nIn Northern Hemisphere the weather pattern repeatedly directed cold Arctic air southward, leaving Greenland and northeastern Canada much warmer than average for March. High pressure situated over Greenland acted as a block, forcing the jet stream into a southerly flow. Observations over Greenland threatened to break the worldwide record for highest barometric pressure of 1083.3 mb, set on December 31, 1968, in Siberia, the National Centers for Environmental Prediction’s Ocean Prediction Center forecast a high pressure center of at least 1074 mb over Greenland during March. A Greenland block was also noticed during the cold weather seen in 2010. The Arctic oscillation index changed from positive to negative, weakening the pressure gradient. Throughout March the west Atlantic winds, which normally kept the winters in Europe relatively mild, have been blowing chiefly from the northeast, bringing in cold Arctic air. The westerly Atlantic winds were weakened by small air pressure difference between northern and southern latitudes. In late March, the Arctic oscillation index dropped to -5.6 and much of Northern Hemisphere experienced particularly low temperatures. The low value of the Arctic oscillation was the second lowest March value on record. The British Met Office described the cold temperatures as part of a larger-scale weather pattern in the Northern Hemisphere, associated with the negative phase of the North Atlantic Oscillation.\n\nDue to expected low temperatures overnight on February 16 and 17, an extreme cold weather alert was issued in Toronto. On February 16 Environment Canada also issued a weather statement for York, Durham, Peel and Halton areas, warning of intensifying flurries.\n\nOn December 22, a severe ice storm brought snow and freezing rain to Toronto, and much of Eastern Canada. The storm caused widespread power outages and left hundreds of thousands in the dark.\n\nThe cold wave in the United States was influenced by a low-pressure area called a \"Clipper\" which brought an Arctic cold front that caused rapidly falling temperatures and strong northwest winds with gusts of .\n\nIn Salt Lake City January, 2013 with the average temperature of became the coldest month on record since 1949 and the sixth-coldest January since 1874.\n\nOn March 5 a record 6-inch snow depth was noted in Chicago's O'Hare International Airport, exceeding the previous 1999 record for that date by 2.2 inches. On the same day 900 flights were cancelled in O'Hare Airport, while Midway Airport reportedly cancelled 240 flights. U.S. Airways reported 350 flight cancellations for March 6.\n\nIn Orlando, Florida, the temperature lowered to on March 28, 2 degrees below the record of 1955. The lowest minimum temperature record of 1971 () was also broken in West Palm Beach, Florida, where a temperature of was recorded on March 28.\n\nOn 1–3 May, a late snow storm occurred across the central United States from Arkansas to Minnesota. The storm formed from a deep upper level trough which became a cut-off low, the event was named \"Achilles\" by the Weather Channel. The storm broke records for depth of snow and lateness in the season, and was cited as the worst May snow since 1947.\n\nDuring mid- to late-May, the tornado outbreaks of May 18–21 and May 26–31 resulted in over 100 tornadoes, including an EF5 tornado in Moore, Oklahoma, and an EF3 tornado in El Reno, Oklahoma. The related storms also resulted in record-breaking rains in North Dakota, New York, and Vermont. The tornado that went through El Reno broke the record for widest tornado ever recorded, at 2.6 miles wide.\n\nThe October 2013 North American storm complex was a blizzard and tornado outbreak that affected the Northwest, Rockies, and much of the Midwest.\n\nThe incident trapped over 6 dozen people inside of their automobiles and harmed 15 people in suburban Iowa and Nebraska. Rapid City, the second largest city in South Dakota, was engulfed in close to two feet of snow, which exceeds the amount of snow that the city has ever recorded during any whole month of October. Furthermore, on October 4, 2013, the city received over 1.5 feet of snow, which exceeded the previous one day record in October by more than six inches. Over 20 000 people lost electricity in Black Hills, where more than a meter of watered down, dense snow had fallen. The storm system also included thunderstorms that brought iced precipitation, significant rain and over half a dozen tornadoes to Nebraska and Iowa. Over 200 km of Interstate 90 were shut down from South Dakota to Wyoming.\n\nTens of thousands of cattle were killed in South Dakota with ranchers reporting loss of 20 to 50% of their herds. Thousands of people were without power. Three people died in a motor vehicle accident on U. S. Route 20 in Nebraska.\n\nOn December 6, a daily record snowfall of 0.1 inches (2 mm) is set in the Dallas–Fort Worth metroplex, breaking the old record of trace amounts of snow set in 1950.\n\nDuring March a cold easterly flow across northern Europe from Russia brought intense snowfalls across the continent as it met moisture filled air masses from Ukraine to Ireland.\n\nStopped on their annual way to the north by the cold weather, thousands of European golden plovers occupied the fields of central Moravia and southeast Bohemia. The cold weather also delayed the arrival of migratory birds that spend the summer in the Czech Republic. In Příbram the temperature on March 24 broke the 1883 record of , being 1.8 degrees lower.\n\nIn Finland, a cold wave hit on the first days of March 2013. The cold wave had very sunny and dry weather for over a month-long period and brought the lowest temperatures for the 2012-13 winter season. The lowest measured temperature (-38,2 °C) was recorded on March 13 in Taivalkoski. Total rainfall was less than 50% of the average in most parts of Finland. With 2-6 °C colder temperatures from 1981-2010 average, March was the coldest winter month in Finland from early 2013 after a mild January and February, and also the coldest March for seven years.\n\nNorthern France was hit by heavy snow beginning on March 11, with Météo-France warning of \"dangerous weather of an exceptional intensity\". French Prime Minister Jean-Marc Ayrault activated a crisis committee to coordinate government efforts to \"guarantee security and movement of the country's citizens.\" Snowfall across Normandy, Brittany and Picardy was up to 40 cm deep and drifting in 100 km/h winds. Eurostar suspended its cross-channel services between Paris and London. The storm also hit the Channel Islands, producing 8 foot high drifts on Guernsey with blizzards described as the worst since 1979 according to the Jersey Met Office.\n\nOn March 15 tanks were deployed to reach thousands of motorists trapped in heavy snow. On March 22 the heaviest March snows in at least 400 years in Budapest canceled outdoor activities, related to commemoration of Hungarian Revolution of 1848.\n\nAccording to OMSZ, the country's meteorological service, this spring was the second wettest since 1901.\n\nThe northern Norwegian region of Nordland saw unusually heavy snowfall, with late March seeing falls in excess of 1 metre deep. Many places in the south mountain area like Geilo, Haukeli and Hovden have their coldest average temperature in March on record. Many places had temperatures more than 5 degrees below average.\n\nAccording to the Russian Bird Conservation Union, the cold weather in the European part of Russia stalled the bird migration, except for the rooks. The weather conditions also suspended the bird migration to Russian Far East. On March 3 thousands of snow rollers formed at the frozen surface of Sineglazovo Lake. On March 15 an all-time low temperature was recorded in Novosibirsk, surpassing the city's 1964 record by 0.5 °C. On April 1 the VVC weather station in Moscow recorded the city's highest snow cover since 1895, at 65 cm, which surpassed the previous record by 9 cm.\n\nSpain saw unusual late April snow, with weather warnings in 18 provinces. The event was widely attributed to a cut-off low.\n\nThe Kiev State Administration declared a state of emergency in the city on March 23 \"due to the deterioration of weather conditions [heavy snowfall, blizzards, snow-banks]\". The government established a weather crisis center under direct supervision of Prime Minister Mykola Azarov. The weather conditions forced the authorities to make Monday, March 25, a day off for all those employed in the government sector in Kiev and Kiev Oblast, except medical services and those tackling the aftermath of the snowfall. March 23 brought heavy snow to Ukraine with the normal monthly amount of snow falling in just 24 hours. With the state of emergency declared the military used armoured personnel carriers to drag buses out of snow drifts.\n\nThe United Kingdom experienced a very severe cold spell in March-April 2013. March was the coldest in the Central England region since 1883.\n\nThe UK then went on to experience the most notable summer heatwave since 2006 with temperatures exceeding 28 °C (82 °F) somewhere for 19 consecutive days, on the 6th to the 24th and also values exceeding 30 °C on several days. The UK overall had a rather dry month compared to normal, with nearly all of the month's rain falling during the last ten days, often as thundery outbreaks. The highest temperatures was 33.5 °C (92.3 °F) at Heathrow and Northholt on the 22nd. The heat returned at the beginning of August with Heathrow recording 34.1 °C (93.4 °F)\n\nOn December 13 a cold snap led to snow in such cities as Cairo, Alexandria and Madinaty for the first time in 112 years.\n\nIn China the average January temperature became the coldest in 28 years. In northeast China the average temperature in January 2013 decreased to , the coldest in 43 years, while in northern China it dropped to a 42-year low of . About a thousand ships were stuck in ice in Laizhou Bay, while 10,500 square miles of ice reportedly covered the surface of the Bohai Sea. About 180,000 cattle deaths were reported in northern China by January 10. On May 2, 2013, a minimum temperature of 16.6 °C was recorded at the Hong Kong Observatory, making it the coldest May temperature since 1917. It was also the third-coldest minimum temperature in May since recorded.\n\nIn January Delhi experienced night temperatures below , which was 4 to 5 degrees lower than the normal seasonal average. In the first week of January 2013, Delhi had a temperature of , the lowest in 44 years. The IFRC's Disaster Relief Emergency Fund donated 57,100 Swiss francs to support the Indian Red Cross Society in delivering immediate assistance to about 10,000 people. In Uttar Pradesh and Bihar the cold wave dropped the mercury to as low as which forced closure of all schools up to grade 13 until January 12.\n\nOn January 10, Lebanese cities and especially Beirut were flooded and snow reached the sea and the coast.\nIsrael saw its heaviest snowfall since 1992 in Jerusalem, which accumulated 10 to 15 cm (4 to 6 inches) of snow in the city centre and more than that in outlying areas. The snowfall stalled the public transport and blocked a highway linking Jerusalem to Tel Aviv. A foot of snow (30 cm) fell at Safed in northern Israel. Lake Kinneret was refilling after years of drought. Much snow fell across Syria; -6 °C (22 °F) was reported in the east suburbs of Damascus and forced camping people made homeless by the Syrian civil war to desperate measures to find shelter, and caused 17 or more deaths in the Middle East.\n\nAfter a cold March and early April in Finland and Scandinavia, a late, but very strong and widespread spring caused fast snow melting and flooding around Finland. Floods occurred mainly in the Province of Northern Ostrobothnia, where floods hit hardest in Pyhäjoki. In May the weather became exceptionally warm. Thermal summer began about a half-month earlier than usual throughout the country, and after mid-May Finland recorded nine heat-days (maximum temperature over 25 °C) for May 2013; whereas typically there are three heat-days in May. The heat of May 2013 also broke records around Finland's Lapland, which got the warmest temperatures of month. The highest temperature, 30,5 °C, was recorded at May 31, 2013, in Utsjoki. It was the first time the temperature rose over 30 °C in May since 1995, when in Lapinjärvi's Ingermaninkylä was recorded Finland's highest all-time temperature of May, 31 °C. June also had exceptionally warm weather, with seventeen heat-days for the month, where eight heat-days is the norm. The highest temperature, 32,4 °C, was recorded on June 26, 2013, in Liperi. The hot weather brought unusual numbers of thunderstorms and lightning strikes. For June, 78,000 lightning strikes were recorded; the most recorded since June 1995. The single-day record, 28,500 lightning strikes, was recorded on June 27, the highest daily count for the 2000s and 2010s. May and June 2013 were both among the warmest five Mays and Junes in Finland's history.\n\nGermany and areas of Central Europe had their wettest ever May, followed by the severe flooding during the 2013 European floods.\n\nIn late June 2013, an intense heat wave struck Southwestern United States. Various places in Southern California reached up to . On June 30, Death Valley, California, hit which is the hottest temperature ever recorded on Earth during the month of June. It was five degrees shy of the highest temperature recorded in Death Valley, which was , recorded in July 1913.\n\nAfter six days in early July with temperatures over 40 °C (104 °F), Portugal officially entered a heat wave. Temperatures reached as high as 45 °C in some places in Alentejo and Ribatejo. Rising temperatures led to heat health warnings being issued for Southern England and the Midlands in the UK's first prolonged heatwave since 2006 on 17 July. The UK recorded its hottest day since July 2006, with 33.5 °C (92.3F) recorded at Heathrow and Northolt in west London on 22 July. This was later beaten again on 1 August, when 34.1 °C was recorded at London Heathrow. Epidemiological statisticians at the London School of Hygiene & Tropical Medicine using models developed in 2011 estimated the heatwave in England and Wales would have led to the premature deaths of 650 people. London Fire Brigade reported that it had attended on average 29 grass fires a day between 1–21 July, seven times the number of call outs for grass fires than the same period in 2012. In Hungary the July heatwave set new daily records for the 27th, 28th and 29th being the hottest day of the year with 40 °C only 2 degrees from the absolute record. A day earlier the 2013 Formula One Hungarian Grand Prix was the 2nd hottest in recent Grand Prix history just behind the 2005 Bahrain Grand Prix. Track temperature reached 56 °C and air temperature was 38 °C.\nPoland had heat waves in Mid-June, Late July and Early August. Kraków broke its record high on August 8, 2013, when it reached 37.2 °C (98.96 °F), the 7th day in a row above 30 °C.\n\nIn July a heat wave struck China with alerts covering nine provinces, including Anhui, Jiangsu, Hunan, Hubei, Shanghai and Chongqing. Shanghai saw 24 days with temperatures at or above 35˚C in July. Temperatures in Shanghai reached 40.6 degrees Celsius, the highest ever temperature recorded in 140 years of weather recording in the city.\n\nThe summer of 2012-2013 in Australia resulted in many heat records being broken over a 90-day period.\n\n\"With sustained wind speeds of more than 310 kilometres per hour, Haiyan was the most powerful tropical cyclone to make landfall in recorded history.\" \"Unusually warm subsurface Pacific waters appear to have endowed Haiyan with the energy that made it the strongest typhoon ever known to make landfall.\"\n\nIn Nordic countries, a storm caused destruction from November 15 to November 18, 2013. In Norway the winds blew as strong as 49 metres per second (180 km/h) and the strongest average was 39 metres per second (140 km/h). In Sweden the strongest windspeed was 47 metres per second (170 km/h; 110 mph) speed, which surpassed the former record of 44 metres per second (160 km/h; 98 mph) recorded in association with Cyclone Gudrun in 2005. In Finland the storm was the third worst since 2000. Only Cyclone Janika in 2001 and Cyclone Dagmar (Tapani) in 2011 had been worse. Cyclone Hilde left about 230,000 households without electricity.\n\n\n"}
{"id": "16877125", "url": "https://en.wikipedia.org/wiki?curid=16877125", "title": "Automated tank cleaning machine", "text": "Automated tank cleaning machine\n\nAn automated tank cleaning machine is a machine used to clean cargo, process, underground storage tanks and similar equipment such as those found in tank trucks, railroad cars, barges, oil tankers, food and beverage manufacturing facilities, chemical processing plants, ethanol plants, and brewing facilities. Genericized trademarks such as Gamajet and Butterworth machine are often used to refer to automatic tank cleaning machines regardless of their manufacturer.\n\nTanks must be cleaned from time to time for various reasons. One reason is to change the type of product carried inside a tank to prevent cross contamination. Another is to allow the tank to be inspected or for maintenance to be performed within a tank and to prevent product build-up on tank interior walls.\n\nAutomated tank cleaning machines work in a manner similar to an irrigation sprinkler but are highly-engineered to deliver increased force. Water forced through rotary jet nozzles rotates the device on a dual axis, creating a 360° cleaning pattern. As the water sprays, the liquid is pumped out of the tank. Portable cleaning systems are commonly used for many outdoor applications while fixed or permanent Clean-in-Place (CIP) systems are used in more sanitary environments.\n\nThe Alfa Laval Gamajet 8 is widely used in the chemical industry, ethanol, transport, brewing, municipality, and oilfield/fuel storage applications. Weighing only 15 lbs., the device maintains 40 lbs. of cleaning force at 25 feet. Typical cleaning cycles are completed in 12 minutes.\n\nThe Butterworth Type K machine is widely used. This model can clean a tank of up to . It uses water with a pressure up to and a temperature of up to . The water jet reaches up to . Depending on the pressure used, a cleaning cycle can take from about 10 to 50 minutes and the machine uses between and per minute.\n\nOn most crude-oil tankers, a special crude oil washing system, or COW system, is part of the cleaning process. The COW system circulates hot crude oil through the fixed tank-cleaning system to remove wax and asphaltic deposits.\n\nAlthough machines are often used to wash tanks, a final stage of manual cleaning known as mucking, is usually performed. Mucking requires protocols for entry into confined spaces and the use of airline respirators, protective clothing and safety observers.\n\nHowever, with Gamajet tank cleaning machines, confined space entry is greatly reduced eliminating danger to workers. Manual tank cleaning is dangerous in a number of ways. While tank barges can be cleaned in port, shipboard tanks are generally cleaned at sea. This is largely due to risks of fire and explosion inside the tanks.\n\nThe first automated tank cleaning machine was invented by Arthur Butterworth and patented in 1920. His goal was to limit the amount of time that workers had to spend inside tanks, and partially relieve them of a dangerous and laborious job. In 1925, Butterworth established a company to market the machine. Standard Oil New Jersey bought the company in 1930, and it later became a subsidiary of the Exxon Corporation.\n\nIn 1986, as part of an internal restructuring at Exxon, the Butterworth company was sold to Exxon management. Today the company is privately held, and headquartered in Houston, Texas.\n\nIn 2012, Gamajet Cleaning Systems, Inc. was purchased by Alfa Laval to combine two global tank cleaning product lines, Gamajet and Toftejorg. The tank cleaning segment operates in Exton, Pennsylvania.\n\n\n"}
{"id": "5652850", "url": "https://en.wikipedia.org/wiki?curid=5652850", "title": "Biodegradable polythene film", "text": "Biodegradable polythene film\n\nPolyethylene or polythene film biodegrades naturally, albeit over a long period of time. Methods are available to make it more degradable under certain conditions of sunlight, moisture, oxygen, and composting and enhancement of biodegradation by reducing the hydrophobic polymer and increasing hydrophilic properties.\n\nIf traditional polyethylene film is littered it can be unsightly, and a hazard to wildlife. Some people believe that making plastic shopping bags biodegradable is one way to try to allow the open litter to degrade.\n\nPlastic recycling improves usage of resources. Biodegradable films need to be kept away from the usual recycling stream to prevent contaminating the polymers to be recycled.\n\nIf disposed of in a sanitary landfill, most traditional plastics do not readily decompose. The sterile conditions of a sealed landfill also deter degradation of biodegradable polymers.\n\nPolyethylene is a polymer consisting of long chains of the monomer ethylene (IUPAC name ethene). The recommended scientific name polyethene is systematically derived from the scientific name of the monomer.[1] [2] In certain circumstances it is useful to use a structure–based nomenclature. In such cases IUPAC recommends poly(methylene).[2] The difference is due to the opening up of the monomer's double bond upon polymerisation.\n\nIn the polymer industry the name is sometimes shortened to PE in a manner similar to that by which other polymers like polypropylene and polystyrene are shortened to PP and PS respectively. In the United Kingdom the polymer is commonly called polythene, although this is not recognised scientifically.\n\nThe ethene molecule (known almost universally by its common name ethylene) CH is CH=CH, Two CH groups connected by a double bond, thus:\nPolyethylene is created through polymerization of ethene. It can be produced through radical polymerization, anionic addition polymerization, ion coordination polymerization or cationic addition polymerization. This is because ethene does not have any substituent groups that influence the stability of the propagation head of the polymer. Each of these methods results in a different type of polyethylene.\n\nPolythene or polyethylene film will naturally fragment and biodegrade, but it can take many decades to do this. There are two methods to resolve this problem. One is to modify the carbon chain of polyethylene with an additive to improve its degradability and then its biodegradability; the other is to make a film with similar properties to polyethylene from a biodegradable substance such as starch. The latter are however much more expensive.\n\nThis type is made from corn (maize), potatoes or wheat. This form of biodegradable film meets the ASTM standard (American Standard for Testing Materials) and European Norm EN13432 for compostability as it degrades at least 90% within 90 days or less at 140 degrees F. However, actual products made with this type of film may not meet those standards.\n\nThe heat, moisture and aeration in an industrial composting plant are required for this type of film to biodegrade, so it will not therefore readily degrade if littered in the environment.\n\n\n\nCarrier bag, refusal sacks, vegetable bags, food films, agricultural films, mailing films. However, these applications are still very limited compared to those of petroleum based plastic films.\n\nAdditives can be added to conventional polymers to make them either oxodegradable or more hydrophilic to facilitate microbial attack.\n\nThese films are made by incorporating an additive within normal polymers to provide an oxidative and then a biological mechanism to degrade them. This typically takes 6 months to 1 year in the environment with adequate exposure to oxygen Degradation is a two-stage process; first the plastic is converted by reaction with oxygen (light, heat and/or stress accelerates the process but is not essential) to hydrophilic low molecular-weight materials and then these smaller oxidized molecules are biodegraded, i.e. converted into carbon dioxide, water and biomass by naturally occurring microorganisms. Commercial competitors and their trade associations allege that the process of biodegradation stops at a certain point, leaving fragments, but they have never established why or at what point. In fact Oxo-biodegradation of polymer material has been studied in depth at the Technical Research Institute of Sweden and the Swedish University of Agricultural Sciences. A peer-reviewed report of the work was published in Vol 96 of the journal of Polymer Degradation & Stability (2011) at page 919-928. It shows 91% biodegradation in a soil environment within 24 months, when tested in accordance with ISO 17556.\nThis is similar to the breakdown of woody plant material where lignin is broken down and forms a humus component improving the soil quality. \nThere is however a lot of controversy about these types of bags. The complete biodegradation is disputed and claimed not to take place. Many countries are now also thinking to ban this type of bags altogether\n\nThese films are inherently biodegradable over a long period of time. Enhancement of the polymer by adding in additives to change the hydrophobic nature of the resin to slightly hydrophilic allows microorganisms to consume the macromolecules of the product, these products often are confused with oxobiodegradable products, but work in a different way. Enhancing of the hydrophilicity of the polymer allows fungus and bacteria to consume the polymer at a faster rate utilizing the carbon inside the polymer chain for energy. These additives attract certain microorganisms found in nature and many tests have been completed on the mixing of synthetic and biobased materials which are inherently biodegradable for enhancing the biodegradability of synthetic polymers that are not as fast to biodegrade.\n\n\n\nTrash Bags, Garbage Bags, Compost Bags, Carrier bag, Agricultural Film, Mulch Film, produce bags, - in fact all forms of short-life plastic film packaging\n\n\n"}
{"id": "4290647", "url": "https://en.wikipedia.org/wiki?curid=4290647", "title": "Biological naturalism", "text": "Biological naturalism\n\nBiological naturalism is a theory about, among other things, the relationship between consciousness and body (i.e. brain), and hence an approach to the mind–body problem. It was first proposed by the philosopher John Searle in 1980 and is defined by two main theses: 1) all mental phenomena from pains, tickles, and itches to the most abstruse thoughts are caused by lower-level neurobiological processes in the brain; and 2) mental phenomena are higher-level features of the brain.\n\nThis entails that the brain has the right causal powers to produce intentionality. However, Searle's biological naturalism does not entail that brains and \"only\" brains can cause consciousness. Searle is careful to point out that while it appears to be the case that certain brain functions are sufficient for producing conscious states, our current state of neurobiological knowledge prevents us from concluding that they are necessary for producing consciousness. In his own words:\n\n\"The fact that brain processes cause consciousness does not imply that only brains can be conscious. The brain is a biological machine, and we might build an artificial machine that was conscious; just as the heart is a machine, and we have built artificial hearts. Because we do not know exactly how the brain does it we are not yet in a position to know how to do it artificially.\" (Biological Naturalism, 2004)\n\nSearle denies Cartesian dualism, the idea that the mind is a separate kind of substance to the body, as this contradicts our entire understanding of physics, and unlike Descartes, he does not bring God into the problem. Indeed, Searle denies any kind of dualism, the traditional alternative to monism, claiming the distinction is a mistake. He rejects the idea that because the mind is not objectively viewable, it does not fall under the rubric of physics.\n\nSearle believes that consciousness \"is a real part of the real world and it cannot be eliminated in favor of, or reduced to, something else\" whether that something else is a neurological state of the brain or a computer program. He contends, for example, that the software known as Deep Blue \"knows\" nothing about chess. He also believes that consciousness is both a cause of events in the body and a response to events in the body.\n\nOn the other hand, Searle doesn't treat consciousness as a ghost in the machine. He treats it, rather, as a state of the brain. The causal interaction of mind and brain can be described thus in naturalistic terms: Events at the micro-level (perhaps at that of individual neurons) cause consciousness. Changes at the macro-level (the whole brain) constitute consciousness. Micro-changes cause and then are impacted by holistic changes, in much the same way that individual football players cause a team (as a whole) to win games, causing the individuals to gain confidence from the knowledge that they are part of a winning team.\n\nHe articulates this distinction by pointing out that the common philosophical term 'reducible' is ambiguous. Searle contends that consciousness is \"causally reducible\" to brain processes without being \"ontologically reducible\". He hopes that making this distinction will allow him to escape the traditional dilemma between reductive materialism and substance dualism; he affirms the essentially physical nature of the universe by asserting that consciousness is completely caused by and realized in the brain, but also doesn't deny what he takes to be the obvious facts that humans really are conscious, and that conscious states have an essentially first-person nature.\n\nIt can be tempting to see the theory as a kind of property dualism, since, in Searle's view, a person's mental properties are categorically different from his or her micro-physical properties. The latter have \"third-person ontology\" whereas the former have \"first-person ontology.\" Micro-structure is accessible objectively by any number of people, as when several brain surgeons inspect a patient's cerebral hemispheres. But pain or desire or belief are accessible subjectively by the person who has the pain or desire or belief, and no one else has that mode of access. However, Searle holds mental properties to be a species of physical property—ones with first-person ontology. So this sets his view apart from a dualism of physical and non-physical properties. His mental properties are putatively physical.\n\nThere have been several criticisms of Searle's idea of biological naturalism.\n\nJerry Fodor suggests that Searle gives us no account at all of exactly \"why\" he believes that a biochemistry like, or similar to, that of the human brain is indispensable for intentionality. Fodor thinks that it seems much more plausible to suppose that it is the way in which an organism (or any other system for that matter) is connected to its environment that is indispensable in the explanation of intentionality. It is easier to see \"how the fact that my thought is causally connected to a tree might bear on its being a thought about a tree. But it's hard to imagine how the fact that (to put it crudely) my thought is made out of hydrocarbons could matter, except on the unlikely hypothesis that only hydrocarbons can be causally connected to trees in the way that brains are.\" \n\nJohn Haugeland takes on the central notion of some set of special \"right causal powers\" that Searle attributes to the biochemistry of the human brain. He asks us to imagine a concrete situation in which the \"right\" causal powers are those that our neurons have to reciprocally stimulate one another. In this case, silicon-based alien life forms can be intelligent just in case they have these \"right\" causal powers; i.e. they possess neurons with synaptics connections that have the power to reciprocally stimulate each other. Then we can take any speaker of the Chinese language and cover his neurons in some sort of wrapper which prevents them from being influenced by neurotransmitters and, hence, from having the right causal powers. At this point, \"Searle's demon\" (an English speaking nanobot, perhaps) sees what is happening and intervenes: he sees through the covering and determines which neurons would have been stimulated and which not and proceeds to stimulate the appropriate neurons and shut down the others himself. The experimental subject's behavior is unaffected. He continues to speak perfect Chinese as before the operation but now the causal powers of his neurotransmitters have been replaced by someone who does not understand the Chinese language. The point is generalizable: for any causal powers, it will always be possible to hypothetically replace them with some sort of Searlian demon which will carry out the operations mechanically. His conclusion is that Searle's is necessarily a dualistic view of the nature of causal powers, \"not intrinsically connected with the actual powers of physical objects.\" \n\nSearle himself actually does not rule out the possibility for alternate arrangements of matter bringing forth consciousness other than biological brains. He also disputes that Biological naturalism is dualistic in nature in a brief essay entitled \"Why I Am Not a Property Dualist\".\n\n\n\n"}
{"id": "64085", "url": "https://en.wikipedia.org/wiki?curid=64085", "title": "Caramel", "text": "Caramel\n\nCaramel (, or ) is a medium to dark-orange confectionery product made by heating a variety of sugars. It can be used as a flavoring in puddings and desserts, as a filling in bonbons, or as a topping for ice cream and custard.\n\nThe process of caramelization consists of heating sugar slowly to around . As the sugar heats, the molecules break down and re-form into compounds with a characteristic color and flavor.\n\nA variety of candies, desserts, and confections are made with caramel: brittles, nougats, pralines, flan, crème brûlée, crème caramel, and caramel apples. Ice creams sometimes are flavored with or contain swirls of caramel.\n\nThe English word comes from French \"caramel\", borrowed from Spanish \"caramelo\" (18th century), itself possibly from Portuguese \"caramel\". Most likely that comes from Late Latin \"calamellus\" 'sugar cane', a diminutive of \"calamus\" 'reed, cane', itself from Greek κάλαμος. Less likely, it comes from a Medieval Latin \"cannamella\", from \"canna\" 'cane' + \"mella\" 'honey'. Finally, some dictionaries connect it to an Arabic \"kora-mokhalla\" 'ball of sweet'.\n\nCaramel sauce is made by mixing caramelized sugar with cream. Depending on the intended application, additional ingredients such as butter, fruit purees, liquors, or vanilla can be used. Caramel sauce is used in a variety of desserts, though most notably as a topping for ice cream. When it is used for crème caramel or flan, it is known as clear caramel and only contains caramelized sugar and water. Butterscotch sauce is made with dark brown sugar, butter, and often a splash of whiskey. Traditionally, butterscotch is a hard candy more in line with a toffee, with the suffix \"scotch\" meaning \"to score\".\n\nToffee, or in the US \"caramel candy\", is a soft, dense, chewy candy made by boiling a mixture of milk or cream, sugar(s), glucose, butter, and vanilla (or vanilla flavoring). The sugar and glucose are heated separately to reach ; the cream and butter are then added which cools the mixture. The mixture is then stirred and reheated until it reaches . Upon completion of cooking, vanilla or any additional flavorings and salt are added. Adding the vanilla or flavorings earlier would result in them burning off at the high temperatures. Adding salt earlier in the process would result in inverting the sugars as they cooked.\n\nAlternatively, all ingredients may be cooked together. In this procedure, the mixture is not heated above the firm ball stage (), so that caramelization of the milk occurs. This temperature is not high enough to caramelize sugar and this type of candy is often called milk caramel or cream caramel.\n\nSalted caramel is a variety of caramel produced in the same way as regular caramel, but with larger amounts of salt added during preparation. Originally utilised in desserts, the confection has seen wide use elsewhere, including in hot chocolate and spirits such as vodka. A study conducted in 2017 by the University of Florida suggested that the popularity of salted caramel is due to its chemical composition, as all of its main ingredients have effects on the reward systems of the human brain, resulting in a process described as 'hedonic escalation'. \n\nCaramel colouring, a dark, bitter liquid, is the highly concentrated product of near total caramelization, used commercially as food and beverage colouring, \"e.g.\", in cola.\n\nCaramelization is the removal of water from a sugar, proceeding to isomerization and polymerization of the sugars into various high-molecular-weight compounds. Compounds such as difructose anhydride may be created from the monosaccharides after water loss. Fragmentation reactions result in low-molecular-weight compounds that may be volatile and may contribute to flavor. Polymerization reactions lead to larger-molecular-weight compounds that contribute to the dark-brown color.\n\nIn modern recipes and in commercial production, glucose (from corn syrup or wheat) or invert sugar is added to prevent crystallization, making up 10%–50% of the sugars by mass. \"Wet caramels\" made by heating sucrose and water instead of sucrose alone produce their own invert sugar due to thermal reaction, but not necessarily enough to prevent crystallization in traditional recipes.\n\nFour and Six Tenths tablespoons (i.e., 69 grams) of commercially prepared butterscotch or caramel topping contain:\n\n"}
{"id": "20827977", "url": "https://en.wikipedia.org/wiki?curid=20827977", "title": "Cerium(IV) oxide–cerium(III) oxide cycle", "text": "Cerium(IV) oxide–cerium(III) oxide cycle\n\nThe cerium(IV) oxide–cerium(III) oxide cycle or CeO/CeO cycle is a two-step thermochemical process that employs cerium(IV) oxide and cerium(III) oxide for hydrogen production. The cerium-based cycle allows the separation of H and O in two steps, making high-temperature gas separation redundant.\n\nThe thermochemical two-step water splitting process (thermochemical cycle) uses redox systems:\n\n\nFor the first endothermic step, cerium(IV) oxide is thermally dissociated in an inert gas atmosphere at and 100-200 mbar into cerium(III) oxide and oxygen. In the second exothermic step cerium(III) oxide reacts at – in a fixed bed reactor with water and produces hydrogen and cerium(IV) oxide.\n\n\n"}
{"id": "29134980", "url": "https://en.wikipedia.org/wiki?curid=29134980", "title": "Chariots in ancient China", "text": "Chariots in ancient China\n\nThe ancient Chinese chariot was used as an attack and pursuit vehicle on the open fields and plains of Ancient China from around 1200 BCE. Chariots also allowed military commanders a mobile platform from which to control troops while providing archers and soldiers armed with dagger-axes increased mobility. They reached a peak of importance during the Spring and Autumn period, but were largely superseded by cavalry during the Han Dynasty.\n\nTraditional sources attribute the invention of the chariot to the Xia dynasty minister Xi Zhong, and say they were used at the Battle of Gan (甘之战) in the 21st century BCE.\nHowever archeological evidence shows that small scale use of the chariot began around 1200 BCE in the late Shang dynasty. This corroborates the material spread of the invention from the Eurasian Grass-Steppe to the West, by Proto-Indo-Europeans (likely the Tocharians) who similarly have borne horse, agricultural, and honey making technologies through the Tarim Basin into China. Contemporary oracle bone inscriptions of the character depict a chariot-like two wheeled vehicle with a single pole for the attachment of horses.\n\nChariots reached their apogee and remained a powerful weapon until the end of the Warring States Period (471-221 BCE) when increasing use of the crossbow, massed infantry, the adoption of standard cavalry units and the adaptation of nomadic cavalry (mounted archery) took over. Chariots continued to serve as command posts for officers during the Qin and Han Dynasties while armored chariots were also used by the Han Dynasty against the Xiongnu Confederation in the Han–Xiongnu War, specifically at the Battle of Mobei in 119 CE. General Wei Qing's army, setting off from Dingxiang, encountered the Xiongnu Chanyu's army of 80,000 cavalry. Wei Qing ordered his troops to arrange heavy-armoured chariots in a ring formation, creating mobile fortresses.\n\nWith changes in the nature of warfare, as well as the increasing availability of larger breeds of horses, during the Qin and Han Dynasties (221 BCE- 220 CE) the chariot was replaced by cavalry and infantry, and the single-pole chariot became less important. At this time the double shaft chariot developed as a transport vehicle which was light and easy to handle. During the Eastern Han (25-220 CE) and later during the Three Kingdoms Period (220-280 CE), the double shaft chariot was the predominant form. This change is seen in innumerable Han Dynasty stone carvings and in many ceramic tomb models. Over time, as society evolved, the early chariot of the Pre-Qin period gradually disappeared.\n\nAncient Chinese chariots were typically two wheeled vehicles drawn by two or four horses with a single draught pole measuring around 3 m long that was originally straight but later evolved into two curved shafts. At the front end of the pole there was a horizontal draw-bar about one metere long with wooden yokes attached, to which the horses would be harnessed. Wooden wheels with a diameter of between approximately 1.2 - 1.4 m were mounted on a three meter long axle and secured at each end with a bronze hubcap. Wheels of the Shang period usually had 18 spokes, but those of the Zhou period numbered from 18 to 26. Chariot wheels of the Spring and Autumn period (8th-7th century BCE) had between 25 and 28 spokes. The carriage body was around one metre long and 0.8 metres wide with wooden walls and an opening at the back to provide access for soldiers.\nWith the arrival of the Spring and Autumn period (771-476 BCE) improvements had been made to the chariot’s design and construction. The angle of the curved draw pole had increased raising the end of the pole. This reduced the amount of effort required by the horse pulling the chariot and increased its speed. The width of the carriage body had also increased to around 1.5 m allowing soldiers greater freedom of movement. Key components such as the pole, hubcap and yoke were reinforced with decorated copper castings, increasing the chariot’s stability and durability. These chariots were variously referred to as “gold chariots” (金车), “attack chariots” (攻车) or “weapons chariots”.( 戎车)\n\nThe Chinese war chariot, like the other war chariots of Eurasia, derived its characteristic ability to perform at high speed by a combination of a light design, together with a propulsion system using horses, which were the fastest draft animals available. \n\nUsually a chariot carried three armored warriors with different tasks: one, known as the charioteer (御者) was responsible for driving, a second, the archer (射) (or sometimes multiple archers (多射)) tasked with long range shooting. The \"róngyòu\" (戎右), whose role was short range defence, made up the third member of the crew. Weapons carried on the chariot consisted of close-combat and long range weapons.\n\nThe most important close-combat weapon aboard the chariot was the dagger-axe or \"gē\" (戈), a weapon with a roughly three meter shaft. At the end of the double-headed device there was a sharp dagger on one side and an axe head on the other. This was carried by the \"róngyòu\" and could be either swung or thrust like a spear at the enemy. By the time of the Spring and Autumn Period the \"gē\" had largely been superseded by the halberd or \"jĭ\" (戟) which had a spear blade at the end of the shaft in addition to the axe head and dagger.\n\nAll chariot commanders carried a bronze dagger for protection in the case of the chariot becoming unserviceable or an enemy jumping on board the chariot. Soldiers aboard wore leather or occasionally copper armour and carried a shield or \"dùn\" (盾) made from leather or bronze. The chariot’s archer was armed either a bow (\"gōng\" 弓) or crossbow (\"nŭ 弩\") for long distance attacks. Chariot horses also began to wear armor during the Spring and Autumn Period to protect against injury. When the chariot was not engaged in a military campaign it was used as a transport vehicle.\n\nThe chariot was a large military vehicle that through its lack of flexibility was not effective as a single combat unit. Usually its commander would be allocated a number of infantrymen or \"tú zù\" (徒卒) to co-operate in battle. During the Western Zhou era, ten infantry were usually allocated to each chariot with five of them riding on the chariot, each of which was called a squadron (\"duì\" 隊/队). Five squadrons made up a \"zhèngpiān\" (正偏), four \"zhèngpiān\" formed a division (\"shī\" 师) while five divisions were known as an army (\"jūn\" 军). In the Spring and Autumn Period the chariot became the main weapon of war. Along with each state's increase in military manpower, their proportion of chariots to overall army numbers also fell with the number of men allocated to each chariot increasing to seventy. This alteration fundamentally changed the fundamentals of warfare.\n\nIn ancient China the chariot was used in a primary role from the time of the Shang Dynasty until the early years of the Han Dynasty (c. 1200–200 BCE) when it was replaced by cavalry and fell back into a secondary support role. For a millennium or more, every chariot borne soldier had used the particular combat tactics that use of the vehicle required.\n\nChariot based combat usually took place in wide-open spaces. When the two sides were within range they would first exchange arrow or crossbow fire, hoping that through superior numbers they would cause disorder and confusion in the enemy ranks. As the two opponents closed on each other they would stay about four meters apart to avoid the three meter long dagger-axes of their opponents. Only when two chariots came closer than this would an actual fight occur.\n\nOnly about three meters wide, with infantry riding on both sides, the chariot was highly inflexible as a fighting machine and difficult to turn around. Coupled with this were restrictions on the use of weapons with opponents seizing the momentary opportunity for victory or trapping their opponent with a pincer movement. These tactics required fighting in tight formation with good military discipline and control.\nWhen the Spring and Autumn period began, more attention was paid to troop formations according to the type of battle. Chariot units were trained to ensure co-ordination with the rest of the army during a military campaign.\n\nDuring the Western Zhou Era, chariots were deployed on wide open plains abreast of each other in a single line. The accompanying infantry would then be deployed forward of the chariot, a broad formation that denied the enemy the opportunity for pincer attacks. When the two sides clashed, if the chariots remained in strict formation there would be a good opportunity to encircle the enemy. During this period of chariot warfare, the use of orderly team-based combat to some extent determined the difference between victory and defeat, otherwise fighting would have to stop in order to consolidate the formation. In this type operation unified command was important. Senior officers would use drums and flags to command the army's advance and retreat, speed and to make formation adjustments. However such operations were inherently very slow paced and the speed of engagement thus hampered. Furthermore, the infantry had to remain in line which was not conducive to long-distance pursuits of retreating enemies.\n\nA typical example of the importance of disciplined forces occurred during the Zhou overthrow of Shang at the decisive Battle of Muye in 1046 BCE. As the Zhou army moved forward, the infantry and chariots were commanded to stop and regroup after every six or seven steps to maintain formation. The Shang army, despite its superior numbers, was largely composed of demoralized and forcibly conscripted troops. As a result, the troops failed to stay in formation and were defeated.\n\nAs the Spring and Autumn period dawned, chariots remained the key to victory. At the Battle of Yanling in 575 BCE between the States of Chu and Jin the disorganised nature of the Chu army's chariots and infantry led to its defeat. Both troop formations and the flexibility of the chariot subsequently underwent major developments with infantry placing a much larger role in combat. Troops were no longer deployed forward of chariots but instead around all four sides thereby increasing the vehicle's flexibility. Formations no longer involved a single line of chariots; instead they were spread out which brought the advantage of depth. In this way the chariot's movement was no longer impeded so it could counter enemy attacks as well as provide a fast pursuit vehicle.\n\n\n"}
{"id": "15698259", "url": "https://en.wikipedia.org/wiki?curid=15698259", "title": "Christopher Margules", "text": "Christopher Margules\n\nChristopher Robert Margules (AM) is the Leader of the Indo-Pacific Field Division of Conservation International. He is based in Queensland, Australia and has written extensively on the management of biological diversity and biological diversity planning.\n"}
{"id": "335109", "url": "https://en.wikipedia.org/wiki?curid=335109", "title": "Compressive strength", "text": "Compressive strength\n\nCompressive strength or compression strength is the capacity of a material or structure to withstand loads tending to reduce size, as opposed to tensile strength, which withstands loads tending to elongate. In other words, compressive strength resists compression (being pushed together), whereas tensile strength resists tension (being pulled apart). In the study of strength of materials, tensile strength, compressive strength, and shear strength can be analyzed independently.\n\nSome materials fracture at their compressive strength limit; others deform irreversibly, so a given amount of deformation may be considered as the limit for compressive load. Compressive strength is a key value for design of structures.\nCompressive strength is often measured on a universal testing machine; these range from very small table-top systems to ones with over 53 MN capacity. Measurements of compressive strength are affected by the specific test method and conditions of measurement. Compressive strengths are usually reported in relationship to a specific technical standard.\n\nWhen a specimen of material is loaded in such a way that it extends it is said to be in \"tension\". On the other hand, if the material compresses and shortens it is said to be in \"compression\".\n\nOn an atomic level, the molecules or atoms are forced apart when in tension whereas in compression they are forced together. Since atoms in solids always try to find an equilibrium position, and distance between other atoms, forces arise throughout the entire material which oppose both tension or compression. The phenomena prevailing on an atomic level are therefore similar.\n\nThe \"strain\" is the relative change in length under applied stress; positive strain characterises an object under tension load which tends to lengthen it, and a compressive stress that shortens an object gives negative strain. Tension tends to pull small sideways deflections back into alignment, while compression tends to amplify such deflection into buckling.\n\nCompressive strength is measured on materials, components, and structures.\n\nBy definition, the ultimate compressive strength of a material is that value of uniaxial compressive stress reached when the material fails completely. The compressive strength is usually obtained experimentally by means of a \"compressive test\". The apparatus used for this experiment is the same as that used in a tensile test. However, rather than applying a uniaxial tensile load, a uniaxial compressive load is applied. As can be imagined, the specimen (usually cylindrical) is shortened as well as spread laterally. A stress–strain curve is plotted by the instrument and would look similar to the following:\nThe compressive strength of the material would correspond to the stress at the red point shown on the curve. In a compression test, there is a linear region where the material follows Hooke's law. Hence, for this region, \"formula_1\", where, this time, E refers to the Young's Modulus for compression. In this region, the material deforms elastically and returns to its original length when the stress is removed.\n\nThis linear region terminates at what is known as the yield point. Above this point the material behaves plastically and will not return to its original length once the load is removed.\n\nThere is a difference between the engineering stress and the true stress. By its basic definition the uniaxial stress is given by:\n\nformula_2\n\nwhere,\nF = Load applied [N],\nA = Area [m]\n\nAs stated, the area of the specimen varies on compression. In reality therefore the area is some function of the applied load i.e. A = f(F). Indeed, stress is defined as the force divided by the area at the start of the experiment. This is known as the engineering stress and is defined by,\n\nformula_3\n\nA=Original specimen area [m]\n\nCorrespondingly, the engineering strain would be defined by:\n\nformula_4\n\nwhere\nl = current specimen length [m] and l = original specimen length [m]\n\nThe compressive strength would therefore correspond to the point on the engineering stress strain curve formula_5 defined by\n\nformula_6\n\nformula_7\n\nwhere\nF = load applied just before crushing and l = specimen length just before crushing.\n\nIn engineering design practice, professionals mostly rely on the engineering stress. In reality, the \"true stress\" is different from the engineering stress. Hence calculating the compressive strength of a material from the given equations will not yield an accurate result. This is because the cross sectional area A changes and is some function of load A = φ(F).\n\nThe difference in values may therefore be summarized as follows:\n\nAs a final note, it should be mentioned that the frictional force mentioned in the second point is not constant for the entire cross section of the specimen. It varies from a minimum at the centre, away from the clamps, to a maximum at the edges where it is clamped. Due to this, a phenomenon known as \"barrelling\" occurs where the specimen attains a barrel shape.\n\nConcrete and ceramics typically have much higher compressive strengths than tensile strengths. Composite materials, such as glass fiber epoxy matrix composite, tend to have higher tensile strengths than compressive strengths. Metals tend to have tensile and compressive strengths that are very similar.\n\nFor designers, compressive strength is one of the most important engineering properties of concrete. It is a standard industrial practice that the concrete is classified based on grades. This grade is nothing but the Compressive Strength of the concrete cube or cylinder. Cube or Cylinder samples are usually tested under a compression testing machine to obtain the compressive strength of concrete. The test requisites differ country to country based on the design code. As per Indian codes, compressive strength of concrete is defined as \n\n\"The compressive strength of concrete is given in terms of the characteristic compressive strength of 150 mm size cubes tested at 28 days (fck). The characteristic strength is defined as the strength of the concrete below which not more than 5% of the test results are expected to fall.\"\n\nFor design purposes, this compressive strength value is restricted by dividing with a factor of safety, whose value depends on the design philosophy used.\n\n\n"}
{"id": "676811", "url": "https://en.wikipedia.org/wiki?curid=676811", "title": "Conflagration", "text": "Conflagration\n\nA conflagration is a large and destructive fire that threatens human life, animal life, health, and/or property. It may also be described as a blaze or simply a (large) fire. A conflagration can begin accidentally, be naturally caused (wildfire), or intentionally created (arson). Arson can be for fraud, murder, sabotage or diversion, or due to a person's pyromania. A very large fire can produce a firestorm, in which the central column of rising heated air induces strong inward winds, which supply oxygen to the fire. Conflagrations can cause casualties including deaths or injuries from burns, trauma due to collapse of structures and attempts to escape, and smoke inhalation.\n\nFirefighting is the practice of attempting to extinguish a conflagration, protect life and property, and minimize damage and injury. One of the goals of fire prevention is to avoid conflagrations. When a conflagration is extinguished, there is often a fire investigation to determine the cause of the fire.\nDuring a conflagration a significant movement of air and combustion products occurs. Hot gaseous products of combustion move upward, causing the influx of more dense cold air to the combustion zone. Sometimes, the influx is so intense that the fire grows into a firestorm.\n       Inside a building, the intensity of gas exchange depends on the size and location of openings in walls and floors, the ceiling height, and the amount and characteristics of the combustible materials.\n\n\n\n"}
{"id": "5089577", "url": "https://en.wikipedia.org/wiki?curid=5089577", "title": "Council for Registered Gas Installers", "text": "Council for Registered Gas Installers\n\nThe Council for Registered Gas Installers (CORGI) operates a voluntary registration scheme for gas installers in the United Kingdom. From 1991 to 2009 registration with CORGI was a legal requirement for gas operatives and businesses throughout the UK, and before April 2010 in Northern Ireland and Guernsey .\n\nCORGI registration requires (beside payment of fees) that gas operatives hold a certificate of competence under the Accredited Certification Scheme (ACS) demonstrating an appropriate level of competence and experience in particular types of gas work. The ACS replaced a number of different certification schemes in 1998.\n\nCORGI lost its status as official registration body in England, Scotland and Wales on 1 April 2009 and in Northern Ireland and Guernsey in April 2010, with this role being taken on by the Gas Safe Register, run on behalf of the Health and Safety Executive by Capita Group.\n\nCORGI was originally established in 1970 as the \"Confederation for the Registration of Gas Installers\" to operate a voluntary register. This followed a gas explosion in 1968 which led to the partial collapse of Ronan Point, a tower block in London. \n\nNotwithstanding gas explosions, the greatest danger to the public in using gas is from carbon monoxide (CO), which is a highly toxic by-product of the combustion process. Most of the concern for gas safety focuses on avoiding excessive production of CO through adequate ventilation and correct combustion, and the safe dispersal of the small amounts produced by correct combustion through effective flue systems. Modern room-sealed gas appliances are much safer in this respect and the number of fatalities from CO poisoning has greatly declined. Each year in the UK around 30 people are killed as a result of CO poisoning.\n\nRegistration with CORGI was a legal requirement under the Gas Safety (Installation and Use) Regulations 1998 for any gas installation business and there were around 55,000 CORGI registered businesses in the UK employing nearly 110,000 gas operatives.\n\nCORGI was answerable to the Health and Safety Executive, which is the Government watchdog on all safety issues, including gas. The HSE has the authority to appoint CORGI and/or other agencies to operate the Register of Gas Installers. In 2006 another body (NAPIT) made an application to the HSE to operate a rival registration scheme. This application was turned down by the Health and Safety Executive. It is the CORGI view - a view that is supported by many other organisations too such as the All Party Parliamentary Gas Safety Group - that introducing another gas registration body would cause confusion. However some registered gas installers believe that CORGI's decision to run schemes for plumbing, electrics and ventilation will itself create public confusion, as well as taking away its focus from the prime consideration of gas safety. However, CORGI has been clear that gas safety is at the core of its business and will remain so. CORGI realises that gas installers often do jobs other than just gas and the decision to offer plumbing, electrical and ventilation schemes was made to help installers that are CORGI registered for gas to comply with Building Regulations in areas closely related to gas work.\n\nSome gas installers, primarily individual, independent installers through associations such as ARGI (Association of Registered Gas Installers) felt that the organisation was overbearing and an excessive financial burden, and that little was being done to stop unregistered installers operating and that they were not being supported by or listened to by the organisation. There were also concerns that it abused its monopolistic position as the sole awarding body for UK gas installers. The reality was, that CORGI used the customer and installer data to sell products and services (initially by including a sales booklet along with the Installation certificate sent to the customer), which is contrary to their remit for the care of that data (data protection).\n\nIn 2005, CORGI set up the CORGI Trust, which sees all profits from the commercial side of the business being donated to the Trust. These funds will then be used to help advance gas safety within the UK. Recommendations for the use of these funds has ranged from getting the Government to change the law so that only CORGI registered installers can buy gas appliances, through to doing a national television campaign to increase awareness on the need for customers to only use CORGI registered installers and the dangers of carbon monoxide.\n\nGas Safe Register is the official gas registration body for the United Kingdom, Isle of Man and Guernsey, appointed by the relevant Health and Safety Authority for each area. By law all gas engineers must be on the Gas Safe Register. \nGas Safe Register replaced CORGI as the gas registration body in Great Britain and Isle of Man on 1 April 2009 and Northern Ireland and Guernsey on 1 April 2010.\n\nGas Safe Register work to protect the public from unsafe gas work through;\n\nGas Safe Register is run by Capita Gas Registration and Ancillary Services Limited, a division of Capita Plc.\n\n\n"}
{"id": "238911", "url": "https://en.wikipedia.org/wiki?curid=238911", "title": "Crimson", "text": "Crimson\n\nCrimson is a strong, red color, inclining to purple. It originally meant the color of the kermes dye produced from a scale insect, \"Kermes vermilio\", but the name is now sometimes also used as a generic term for slightly bluish-red colors that are between red and rose.\n\nCrimson (NR4) is produced using the dried bodies of the kermes insect, which were gathered commercially in Mediterranean countries, where they live on the kermes oak, and sold throughout Europe. Kermes dyes have been found in burial wrappings in Anglo-Scandinavian York. They fell out of use with the introduction of cochineal, because although the dyes were comparable in quality and color intensity it needed ten to twelve times as much kermes to produce the same effect as cochineal.\n\nCarmine is the name given to the dye made from the dried bodies of the female cochineal, although the name crimson is sometimes applied to these dyes too. Cochineal appears to have been brought to Europe during the conquest of Mexico by the Spaniard Hernán Cortés, and the name 'carmine' is derived from the French \"carmin\". It was first described by Mathioli in 1549. The pigment is also called \"cochineal\" after the insect from which it is made.\n\nAlizarin (PR83) is a pigment that was first synthesized in 1868 by the German chemists Carl Gräbe and Carl Liebermann and replaced the natural pigment madder lake. Alizarin crimson is a dye bonded onto alum which is then used as a pigment and mixed with ochre, sienna and umber. It is not totally colorfast.\n\nThe word \"crimson\" has been recorded in English since 1400, and its earlier forms include \"cremesin\", \"crymysyn\" and \"cramoysin\" (cf. cramoisy, a crimson cloth). These were adapted via Old Spanish from the Medieval Latin \"cremesinus\" (also \"kermesinus\" or \"carmesinus\"), the dye produced from \"Kermes\" scale insects, and can be traced back to Arabic \"qermez\" (\"red\"), also borrowed in Turkish \"kırmızı\" and many other languages, e.g. German \"Karmesin\", Italian \"cremisi\", French \"cramoisi\", Portuguese \"carmesim\", etc. (via Latin). The ultimate source may be Sanskrit कृमिज \"kṛmi-jā\" meaning \"worm-made\".\n\nA shortened form of \"carmesinus\" also gave the Latin \"carminus\", from which comes carmine.\n\nOther cognates include the Persian \"qermez\" (or \"ghermez\") \"red\", Old Church Slavonic \"čruminu\", archaic Russian чермный (\"čermnyj\"), and Serbo-Croatian \"crven\" \"red\". Cf. also vermilion.\n\nCarmine dyes, which give crimson and related red and purple colors, are based on an aluminium and calcium salt of carminic acid. Carmine lake is an aluminium or aluminium-tin lake of cochineal extract, and crimson lake is prepared by striking down an infusion of cochineal with a 5 percent solution of alum and cream of tartar. Purple lake is prepared like carmine lake with the addition of lime to produce the deep purple tone. Carmine dyes tend to fade quickly.\n\nCarmine dyes were once widely prized in both the Americas and in Europe. They were used in paints by Michelangelo and for the crimson fabrics of the Hussars, the Turks, the British Redcoats, and the Royal Canadian Mounted Police.\n\nNowadays carmine dyes are used for coloring foodstuffs, medicines and cosmetics. As a food additive in the European Union, carmine dyes are designated E120, and are also called cochineal and Natural Red 4. Carmine dyes are also used in some oil paints and watercolors used by artists.\n\n\n\n\n‘’Crimson and Clover’’ 1968 \n\n\n\n\n\n\n\n"}
{"id": "40639288", "url": "https://en.wikipedia.org/wiki?curid=40639288", "title": "DeepOcean", "text": "DeepOcean\n\nDeepOcean is an integrated provider of safe, high quality, innovative services and technologies for the subsea industry.\nDemonstrating an extensive track record, DeepOcean offers a breadth of subsea services including Survey and Seabed-mapping, Subsea Installation, Seabed Intervention, Inspection, Maintenance and Repair (IMR), and Decommissioning.\n\nThis strong portfolio of services, coupled with a fleet of owned and controlled specialised equipment and multi-purpose support spreads, enables DeepOcean to bundle its subsea services to deliver cost-effective, tailored solutions to meet individual client needs.\n\nOperating within a challenging industry, DeepOcean is committed to protecting the health and safety of all people involved with its activities, and thus achieving a safe and incident free workplace, with high standards of environmental responsibility and pollution prevention.\nDeepOcean Group Holding (DeepOcean) was established in May 2011 after the successful completion of the restructuring of its former parent company. DeepOcean is the parent company of the subsea services company DeepOcean AS and the cable lay and trenching company DO 1 UK Ltd.\n\nDeepOcean offers the following five main service lines: (i) Inspection, Maintenance and Repair (IMR) including pipeline inspection, (ii) Seabed Intervention (incl. trenching), (iii) Subsea Installation (incl. SURF), (iv) Survey and Seabed-mapping and (v) Decommissioning.\n\nDO 1 UK Ltd., formerly known as CTC Marine Projects Ltd., was established in 1993. Its initial core business was the provision of fibre optic cable lay and seabed intervention solutions for the global telecommunication market. Later, the company diversified and added cable lay and trenching services for the oil & gas, offshore renewables and interconnectors industries.\n\nDeepOcean AS was established in 1999. It is founded on the provision of high quality equipment and subsea services combined with a team of highly experienced personnel with knowledge of deepwater operations. DeepOcean now has the track record and experience to take on deepwater assignments anywhere in the world.\n\nLate 2016 Funds advised by Triton became the largest shareholder of DeepOcean. The Triton funds invest in and support the positive development of medium-sized businesses headquartered in Northern Europe, Italy and Spain.\n\n\nDeepOcean has a Supervisory Board of Directors as well as an Executive Management Team who oversea the daily management of DeepOcean's activities.\n\nBoard of Directors:\n\n\nExecutive Management:\n\nhttp://www.deepoceangroup.com\n\n"}
{"id": "20193727", "url": "https://en.wikipedia.org/wiki?curid=20193727", "title": "Design-basis event", "text": "Design-basis event\n\nA design basis event (DBE) is a postulated event used to establish the acceptable performance requirements of the structures, systems, and components, such that a Nuclear power plant can withstand the event and not endanger the health or safety of the plant operators or the wider public. Similar terms are design basis accident (DBA) and maximum credible accident.\n\nSubtypes of DBEs are:\n\nCircumstances like the 2011 Tōhoku earthquake and tsunami were not considered within the design basis of the plant, and so the resulting Fukushima I nuclear accidents were described using this terminology as \"beyond design basis\" or \"non-design-basis\". However, some have claimed that the design basis for tsunami events at Fukushima was incorrect. \n\nAccidents caused by poor design, failure to follow listed safety procedures, or other forms of human error are not considered to be beyond design basis accidents. The terminology can be unclear, however, because a poorly handled design basis accident can result in conditions beyond what was considered likely, causing a beyond design basis accident. For this reason, some industry experts have criticized the use of design basis terminology. The Three Mile Island accident and the Chernobyl disaster are examples of design basis accidents becoming non design basis accidents largely due to failures to follow the written operating procedures.\n\nBeyond-design-basis events can reduce or eliminate the margin of safety of the structures, systems and components, possibly resulting in a catastrophic failure. \n\nThe Fukushima Daiichi nuclear disaster was caused by a \"beyond-design-basis event,\" the tsunami and associated earthquakes were more powerful than the plant was designed to accommodate. The plant was withstood the earthquake but the tsunami overflowed the seawall. Since then, the possibility of unforeseen beyond design basis events has been a major concern for plant operators.\n\n"}
{"id": "9588527", "url": "https://en.wikipedia.org/wiki?curid=9588527", "title": "Engine power", "text": "Engine power\n\nEngine power or horsepower is the maximum power that an engine can put out. It can be expressed in kilowatts or horsepower. The power output depends on the size and design of the engine, but also on the speed at which it is running and the load or torque. Maximum power is achieved at relatively high speeds and at high load.\n\nThe following tables give some examples of engine power for a wide variety of engines and vehicles.\n\nIncreased engine performance is a consideration, but also other features associated with luxury vehicles. Longitudinal engines are common. Bodies vary from hot hatches, sedans (saloons), coupés, convertibles and roadsters. Mid-range dual-sport and cruiser motorcycles tend to have similar power-to-weight ratios.\nPower-to-weight ratio is an important vehicle characteristic that affects the acceleration and handling - and therefore the driving enjoyment - of any sports vehicle.\nAircraft also depend on high power-to-weight ratio to achieve sufficient lift.\n\n\n"}
{"id": "20960842", "url": "https://en.wikipedia.org/wiki?curid=20960842", "title": "Ethylene bis(stearamide)", "text": "Ethylene bis(stearamide)\n\nEthylene bis stearamide (EBS) is an organic compound with the formula (CHNHC(O)CH). It is a waxy white solid and is also found as powder or beads that is widely used as a form release agent. The compound is derived from the reaction of ethylenediamine and stearic acid. It is a white solid of low toxicity that provides a slippery coating for a variety of applications.\n\nEBS is a synthetic wax used as a dispersing agent or internal/external lubricant for benefits in plastic applications to facilitate and stabilize the dispersion of solid compounding materials to enhance processability, to decrease friction and abrasion of the polymer surface, and to contribute color stability and polymer degradation.\n\nIt is also used in process industries as release agent, antistatic agent and antifoaming agent for the production of thermoplastics, wiring, and paper. It is used in powder metallurgy.\n"}
{"id": "33868398", "url": "https://en.wikipedia.org/wiki?curid=33868398", "title": "Fauna of Poland", "text": "Fauna of Poland\n\nFauna of Poland, according to the Polish Museum and Institute of Zoology, includes approximately 36,000 species. The most common type of fauna in Poland are arthropods (Arthropoda), and within the phyla, insects (Insecta) are most numerous.\n\nPolish fauna is represented by vertebrates including but not limited to:\n\n"}
{"id": "395193", "url": "https://en.wikipedia.org/wiki?curid=395193", "title": "Granville Woods", "text": "Granville Woods\n\nGranville Tailer Woods (April 23, 1856 – January 30, 1910) was an American inventor who held more than 50 patents. He is also the first American of African ancestry to be a mechanical and electrical engineer after the Civil War. Self-taught, he concentrated most of his work on trains and streetcars. One of his notable inventions was the Multiplex Telegraph, a device that sent messages between train stations and moving trains. His work assured a safer and better public transportation system for the cities of the United States.\nGranville T. Woods was born to Martha J. Brown and Cyrus Woods. He also had a brother named Lyates. His mother was part Native American, and his father was African American. Granville attended school in Columbus until age 10, but had to leave due to his family's poverty, which meant he needed to work; he served an apprenticeship in a machine shop and learned the trades of machinist and blacksmith. Some sources of his day asserted that he also received two years of college-level training in \"electrical and mechanical engineering,\" but little is known about where he might have studied.\n\nIn 1872, Woods obtained a job as a fireman on the Danville and Southern Railroad in Missouri, eventually becoming an engineer. In December 1874, he moved to Springfield, Illinois, and worked at a rolling mill, the Springfield Iron Works. He studied mechanical and electrical engineering in college from 1876-1878. In 1878, he took a job aboard the \"Ironsides\", and, within two years, became Chief Engineer of the steamer. When he returned to Ohio, he became an engineer with the Dayton and Southwestern Railroad in southwestern Ohio. In 1880, he moved to Cincinnati, Ohio and established his business as an electrical engineer and an inventor. After receiving the patent for the multiplex telegraph, he reorganized his Cincinnati company as the Woods Electric Co, but in 1892 he moved his own research operations to New York City, where he was joined by a brother, Lyates Woods, who also had several inventions of his own.\n\nSome internet sources claim he was married. However, the newspapers of his day generally referred to him as a \"bachelor.\" The one indication that he had been married at some point was a brief mention in 1891 that said he was being sued for divorce by a woman identified as Ada Woods. But while little more was said of his personal life, Granville T. Woods was often described as an articulate and well-spoken man, as well as meticulous and stylish in his choice of clothing, and a man who preferred to dress in black. At times, he would refer to himself as an immigrant from Australia, in the belief that he would be given more respect if people thought he was from a foreign country, as opposed to being an African American. In his day, the black newspapers frequently expressed their pride in his achievements, saying he was \"the greatest of Negro inventors\", and sometimes even calling him \"professor,\" although there is no evidence he ever received a college degree.\n\nWoods invented and patented Tunnel Construction for the electric railroad system, and was referred to by some as the \"Black Edison.\"\n\nIn 1885, Woods patented an apparatus which was a combination of a telephone and a telegraph. The device, which he called \"telegraphony\", would allow a telegraph station to send voice and telegraph messages over a single wire. He sold the rights to this device to the American Bell Telephone Company. In 1887, he patented the Synchronous Multiplex Railway Telegraph which allowed communications between train stations from moving trains, a technology pioneered by Lucius Phelps in 1884.\n\nThomas Edison later filed a claim to the ownership of this patent. In 1888, Woods manufactured a system of overhead electric conducting lines for railroads modeled after the system pioneered by Charles van Depoele, a famed inventor who had by then installed his electric railway system in thirteen U.S. cities. In 1889, he filed a patent for an improvement to the steam-boiler furnace.\n\nGranville Woods often had difficulties in enjoying his success as other inventors made claims to his devices. Thomas Edison made one of these claims, stating that he had first created a similar telegraph and that he was entitled to the patent for the device. Woods was twice successful in defending himself, proving that there were no other devices upon which he could have depended or relied upon to make his device. After Thomas Edison's second defeat, he decided to offer Granville Woods a position with the Edison Company, but Granville declined.\n\nWoods is sometimes credited with the invention of the electric third rail, however, many third rail systems were in place in both Europe and North America at the time Woods filed for his patent in 1901. Thomas Edison had been awarded a patent for the third rail almost two decades earlier, in 1882.\n\nWoods is also sometimes credited with the invention of the air brake in 1904 for trains, however, George Westinghouse patented the air brake almost 40 years prior.\n\nOver the course of his lifetime Granville Woods would obtain more than 50 patents for inventions including an automatic brake and an egg incubator and for improvements to other inventions such as safety circuits, telegraph, telephone, and phonograph. He died on January 30, 1910 in New York City, having sold a number of his devices to such companies as Westinghouse, General Electric and American Engineering. Until 1975, his resting place was an unmarked grave, but historian M.A. Harris helped to raise funds, and persuaded several of the corporations that used Woods's inventions to donate towards a headstone. It was erected at St. Michael's Cemetery in Elmhurst, Queens NY.\n\nBaltimore City Community College established the Granville T Woods scholarship in memory of the inventor.\n\n\n"}
{"id": "26412140", "url": "https://en.wikipedia.org/wiki?curid=26412140", "title": "Highland Wind Energy Center", "text": "Highland Wind Energy Center\n\nThe Highland Wind Project is a large wind power project in O'Brien County, Iowa. The 500 MW project was placed online in 2015.\n\nThe project is part of $1.9 billion in wind power construction announced by MidAmerican Energy in 2013. The project is the largest in Iowa, as well as the largest single-phase project in the world. Siemens is supplying 218 2.3 MW wind turbines for the project.\n\nThe project is located at the southern end of Buffalo Ridge. The proposed Rock Island Clean Line HVDC transmission line would originate just north of the project.\n\nAn expansion of the project, formerly known as Highland II was developed by Invenergy. This expansion was purchased by MidAmerican energy and constructed in 2016 as the 250 MW O'Brien Wind Farm. It also lies in O'Brien county, just to the north of the Highland project.\n\n\n"}
{"id": "52544801", "url": "https://en.wikipedia.org/wiki?curid=52544801", "title": "Hitrino train derailment", "text": "Hitrino train derailment\n\nOn 10 December 2016, a freight train derailed, exploded and caught fire in the village of Hitrino in Shumen Province, Bulgaria, killing at least seven people and injuring 29 others.\n\nAt 05:37 (03:37 UTC) on 10 December 2016, a Bulmarket freight train travelling from Burgas to Ruse derailed in Hitrino, Shumen Province, Bulgaria. The company specialises in transport of fuels over rail and road. Two of the waggons, which were carrying propane-butane and propylene, struck a power line pole, exploded and caught fire, engulfing at least fifty buildings, one of which collapsed, trapping several children. The three train drivers (two in the lead and the third driver in the second electric locomotive) survived the accident. Seven people were killed and 29 injured. An evacuation of the village was ordered as 150 firefighters fought the blaze; it was extinguished by midday. The injured were taken to hospitals in Shumen and Varna; some of them had sustained burns to 90% of their bodies.\n\nIt was reported that sparks had been seen coming from the locomotive of the train immediately before the accident, possibly indicative of heavy braking.\n\nThe incident occurred on the oldest railway line in Bulgaria, connecting the country's main riverine port on the Danube in the city of Ruse and the main port on the Black Sea in Varna. The line was built during Ottoman rule by British private investors in the 1860s. The railway station in Hitrino was also the site of the first railway accident in Bulgaria in 1866.\n\nThe train operator Bulmarket Rail Cargo was founded in 2004 and in the same year acquired a license for railway operations. It operates second-hand Danish (Class 86) and British (Class 87) electric locomotives like those involved in the incident (lead locomotive nr.86.003 and second locomotive nr.87.025), as its mainline motive rolling stock, as well as diesel-hydraulic locomotives for shunting duties. Bulmarket DM, the mother corporation of Bulmarket Rail Cargo, has its headquarters and main base of operations, with its own railway yard, Bulmarket Port and fuel storage facilities, in the city of Ruse.\n\nBulgaria's Chief Prosecutor opened an investigation into the accident. The owner of Bulmarket DM, Stanko Stankov, expressed his intention to involve an international team of railway incident experts from France, Germany and the Czech Republic to execute an independent investigation into the case. This was rebutted by the Republic of Bulgaria's Attorney General's Office; it issued a statement that according to Bulgarian law such an investigation has absolutely no legal basis and justification, because the Attorney General's Office (\"Главна Прокуратура\") and its special division for investigation of severe crimes and incidents — the National Inquiry Service (\"Национална Следствена Служба\") — are the only legal bodies authorized by the country's laws to execute such investigations. On that basis, the police authorities guarding the perimeter denied access to the crash site to technical experts involved by the train operator. Residents were not allowed into their homes as a state of emergency was declared, but volunteers were allowed to be in Hitrino. Bullmarket DM also had to remove cisterns within 15 days.\n\nAccording to the State Agency for National Security (semi-autonomous agency within the Bulgarian Ministry of the Interior, tasked with counter-terrorism, counter-espionage and high-profile corruption cases) the probable trigger for the incident was a railroad switch that had not been properly secured.\n\nThe investigators from the National Inquiry Service of the Attorney General's Office found that upon derailment at the railroad switch a crash between the fifth and sixth tank wagons led to the towing hook of one of them causing a rupture in the lower frontal part of the gas tank of the other. The discharged gas fumes from the liquid gas reached a bakery located in immediate proximity to the railway station, and its furnaces ignited them, causing the violent explosion. According to ammunition experts, the chain of events led to an equivalent of an oversized thermobaric weapon with blasting energy four to five times greater than the TNT-equivalent.\n\nBulgarian Prime Minister Boyko Borisov visited the scene. He called for people to donate blood as there was a shortage in the local hospitals. Local response alleviated the shortages. Bulgarian Transport Minister Ivaylo Moskovski also visited the site, along with top rail officials. A national day of mourning was declared for 12 December 2016.\n\nThe Bulgarian Government announced that a fund of 10 million leva (€5.1 million) was to be made available following a special Cabinet meeting on 12 December. The money would be distributed in two ways: 5 million leva (€2.55 million) would go to the Hitrino Municipality, and the other 5 million leva (€2.55 million) would go to the Labour and Social Policy Ministry. The Interior Ministry stated that people who had lost identity documents, passports and driving licences in the fire would have them replaced free of charge.\n\n\n"}
{"id": "12804134", "url": "https://en.wikipedia.org/wiki?curid=12804134", "title": "Humayun Rashid Choudhury", "text": "Humayun Rashid Choudhury\n\nHumayun Rasheed Choudhury (11 November 1928 – 10 July 2001) was a Bangladeshi career diplomat and Speaker of the Bangladesh National Parliament from 1996 to 2001. He was elected president of the 41st session of the UN General Assembly in 1986. He was awarded Independence Day Award in 2018 posthumously by the Government of Bangladesh.\n\nChoudhury was born in Sylhet in the then British India on 11 November 1928. He was the eldest of the seven children of Abdur Rasheed Choudhury (d. 1944) and Begum Serajunessa Choudhury (1910–1974). Abdur was a member of the Assam Legislative Assembly and later a member of the Central Legislative Assembly in Delhi. Serajunessa was elected a member of Pakistan National Assembly.\n\nChowdhury studied in St. Edmund's College in Shillong. He graduated from Aligarh Muslim University in 1947. He then studied for the English Bar and became a member of the Inner Temple in London. He obtained a diploma in International Affairs from the London Institute of World Affairs. He later graduated from The Fletcher School of Law and Diplomacy in Massachusetts, United States.\n\nChoudhury joined the Pakistan Foreign Service in 1953. During his diplomatic career with Pakistan, he held various assignments in Rome, Baghdad, Paris, Lisbon, Jakarta and New Delhi. During the Bangladesh Liberation War in 1971, he defected to the Provisional Government of Bangladesh. He negotiated the recognition of Bangladesh by over 40 countries. On Victory Day, 1971, he addressed the Indian parliament on behalf of the Bangladeshi people. He became the first Bangladeshi Ambassador to the Federal Republic of Germany in 1972, with concurrent accreditation to Switzerland, Austria and the Holy See. He was also the first Permanent Representative of Bangladesh to the International Atomic Energy Agency (IAEA) and the United Nations Industrial Development Organization (UNIDO). In 1975, Choudhury sheltered Sheikh Hasina and Sheikh Rehana at his residence in Bonn after the assassination of Sheikh Mujibur Rahman.\n\nIn 1976, Choudhury became the first Ambassador of Bangladesh to the Kingdom of Saudi Arabia. He also had concurrent accreditation to Jordan and Oman. During this assignment, he represented Bangladesh in the Organization of the Islamic Conference. He served as the Foreign Secretary of Bangladesh during 1981-1982. He was appointed Ambassador to the United States in June 1982. As a member or leader of his country's delegations, he attended the United Nations General Assembly session; the Islamic Summit Conference held in Taif, Saudi Arabia (1981); the Islamic Foreign Ministers' Conference in Tripoli (1977), Dakar (1978), Fez, Morocco (1979), Islamabad (1980) and Baghdad (1981); the North-South Summit on International Cooperation and Development held in Cancun, Mexico (1981); Meetings of the Islamic Summit-level Peace Committee to resolve disputes between Iran and Iraq; the extraordinary session on Afghanistan of the Islamic Foreign Ministers in Islamabad (1980); and the extraordinary session on Jerusalem of the Islamic Foreign Ministers held in Amman (1980).\n\nHe has also led his country's delegations to a number of bilateral meetings including talks with India on border delineation, sharing of Ganges waters, demarcation of the maritime boundary, South Asia Forum (South Asian Regional Co-operation), Bangladesh–Burma border demarcation talks, Bangladesh-Saudi Arabia Joint Economic Talks and others. As Chairman of the fourteenth Islamic Conference of Foreign Ministers (ICFM XIV), he presided over the Co-ordination Meeting of the Foreign Ministers of the Islamic Conference while attending the thirty-ninth session of the United Nations General Assembly. He also led the Bangladesh delegation to the Extraordinary Ministerial Meeting of the non-Aligned Countries on Namibia, held in New Delhi in April 1985, and the Islamic Peace Committee Meeting, held in Jeddah in May 1985. Choudhury was part of the cabinet of President Hossain Mohammad Ershad and a Jatiya Party Member of Parliament during 1986-1990. He was elected member of the National Parliament in 1996 as a nominee of the Bangladesh Awami League and was elected Speaker of the Parliament. He died in Dhaka due to a heart attack on July 10, 2001.\n\n\nChoudhury was married to Mehjabeen Choudhury. They had a daughter Nasrine R Karim (1949–2010) and a son Nauman Choudhury (1950-2017).\n"}
{"id": "26617950", "url": "https://en.wikipedia.org/wiki?curid=26617950", "title": "Hurricane dynamics and cloud microphysics", "text": "Hurricane dynamics and cloud microphysics\n\nTropical convective clouds play an important part in the Earth's climate system. Convection and release of latent heat transports energy from the surface into the upper atmosphere. Clouds have a higher albedo than the underlying ocean, which causes more incoming solar radiation to be reflected back to space. Since the tops of tropical systems are much cooler than the surface of the Earth, the presence of high convective clouds cools the climate system.\n\nThe most recognizable cloud system in the tropics is the hurricane. In addition to the important climatic effects of tropical weather systems, hurricanes possess enough energy to cause massive death and destruction. Therefore, their accurate prediction is of utmost importance.\n\nCloud microphysics describe the structure and properties of clouds on the microscopic scale.\n\nThe Tropical Rainfall Measuring Mission (TRMM) was launched in 1997 to provide quantitative estimates of rainfall over the entire tropics. The satellite uses remote sensing techniques to convert the radiance recorded at the sensor to rainfall values. The most important variable used to constrain the measurements is the properties of the hydrometeors. Hurricanes are mixed-phase clouds, meaning that liquid and solid water (ice) are both present in the cloud. Typically, liquid water dominates at altitudes lower than the freezing level and solid water at altitudes where the temperature is colder than -40 °C. Between 0 °C and -40 °C water can exists in both phases simultaneously. In addition to the phase, the solid water hydrometeors can have different shapes and types that need to be accounted for in the radiative transfer calculations.\n\nIn autumn 1999 the TRMM-Large-Scale Biosphere-Atmosphere Experiment in Amazonia (LBA) field experiment sampled continental and oceanic tropical clouds in Brazil. The goal of TRMM-LBA was to validate the rainfall in cloud resolving models. There have been several in-situ observations of cloud microphysics in tropical clouds which will be discussed here.\n\nCloud microphysics are the physical processes that describe the growth, decay, and fallout of precipitation particles. In terms of models, cloud microphysics occur on a scale smaller than the grid-scale of the model and have to be parameterized.\n\nHurricane track forecasts have been getting better in recent years. Looking at the example of Hurricane Rita, the forecast of the National Hurricane Center 36 hours before landfall shifted more than 130 kilometers from the previous forecast, causing an unneeded evacuation. There has been research that has shown that the choice of subgrid-scale parameterization schemes can influence hurricane intensity, track, speed, and precipitation rates. Microphysical assumptions may directly or indirectly modulated storm structure, which result in small changes in the hurricane track which can have societal consequences.\n\nThe shape of liquid water drops is generally spherical because of the effects of surface tension. Depending on the size of the drop the friction of the air flowing past a falling drop may squish the bottom on the drop so that it is slightly non-spherical. However, solid ice does not generally form into nice spherical shapes. Ice crystals have a preference to form hexagonal structures by deposition, but can form odd shapes in the presence riming or aggregation into graupel.\n\nThe shape of ice particles is mostly dependent on the temperature and supersaturation where the form. The supersaturation is mostly dependent upon the speed in the updraft regions. In regions of high updraft, there are more hydrometeors formed. Graupel is found mostly in regions of weak updrafts. Particle size tends to decrease with increasing altitude because at lower altitudes the larger particles collide and aggregate with the smaller particles. Because updrafts are important for cloud microphysics, it is also necessary to consider how convection parameterization schemes may influence microphysics.\n\nSmall errors in the parameterization of the particle size distribution can have large impacts on the calculation of the terminal velocity. The composition, size, and number concentration of particles varies dramatically in stratiform and convective regions. The particle fall speed derived from observations of tropical cyclones varies significantly from those derived from midlatitude systems.\n\nThere have been many studies of the feasibility of modifying hurricanes so that they would not be as destructive. Rosenfeld et al. (2007) studied possible modification of hurricane Katrina (2005). They simulated the \"seeding\" of the hurricane by suppressing all warm rain formation in the outer regions of the hurricane. They report that in their simulation the seeded hurricane initially weakened the surface winds in the region of seeding. The eye of the hurricane eventually contracted and became stronger, but the average of the total wind field was weaker. In this best case scenario, they report that seeding reduced the hurricane-force winds by 25%.\n\nRango and Hobbs (2005) obtained in situ measurements of tropical convective systems. They found that the liquid water content was below adiabatic values. This was true even in newly formed updrafts, suggesting that collision-coalescence and/or entrainment mixing are efficient methods for the removal of liquid water. They noted that the effective radius started to decrease at altitudes above 2–4 km above cloud base, which they attribute to the warm rain process. Ice processes became important at temperatures between -4 °C and -10 °C, and they photographed different shapes including needles, frozen drops, and sheaths. In growing clouds, it was noted that the particle size often grew by riming.\n\nTokay et al. (2008) studied the raindrop size distribution in topical cyclones. They found high concentrations of small and middle sized drops regardless if larger drops were present. The total number of droplets was between 600–1000 m, the liquid water content was around 1 g m, and a rain rate of approximately 20 mm per hour. The droplets had a mean mass diameter of ~1.6 mm and the maximum diameter recorded was 4.11 mm. There results indicate that prior rain rate estimates from tropical cyclones may have been underestimated due to the differences in microphysics between midlatitude and tropical storms.\n\nIn-situ measurements of the microphysics of tropical clouds in the Amazon show that in regions of stronger updrafts contained smaller supercooled water droplets or ice particles than weaker updrafts. In stratiform anvil regions, aggregation into graupel was the main growth mechanism. The speed of the updraft determines if warm rain processes, riming, or aggregation are the primary mechanism of growth in updraft regions.\n\nHeymsfield et al. (2002) also looked at the microphysics of tropical convection, but they limited themselves to the stratiform regions. They observed ice particles of many shapes and sizes. In particular, they noted that rimed particles were found near convective regions, small spheres were found in regions of \"transient convection\", and at low temperatures cirrus crystals formed. They constructed particle size distributions and noted that they fit particularly well to Gamma distributions and slightly less well to exponential distributions. They noted that their results were similar to results derived from midlatitude systems.\n\nThere are several different cloud microphysics parameterization schemes. Depending on the sophistication of the scheme, the number of ice-phase categories can vary. Many schemes use at least three categories: cloud ice, graupel, and snow. The classification of ice into categories is necessary because different forms of ice will fall at different velocities.\n\nTypically, microphysics schemes will use a mass-weighted average for the fall velocity. McFarquhar and Black (2004) showed that different parameterizing methods results in dramatically different terminal velocities of the hydrometeors.\n\nThe presence of cloud condensation nuclei (CCN) influences the number of cloud drops that form in a cloud; the more CCN there are, the more cloud droplets that will form. Changes in the CCN concentration and their associated changes in the cloud drop distribution can redistribute the energy within a hurricane. This was known in the 1960s which lead scientists to think that hurricanes could be modified by the addition of CCN to produce less intense hurricanes. It was proposed that by seeding with silver iodide outside the eyewall would freeze the supercooled water, release more latent heat, and increase convection. This method was ultimately unsuccessful because of the lack of supercooled water in the tropical cyclone. A different approach seeds the clouds with a large number of small hygroscopic aerosols. The large number of CCN leads to smaller raindrops, less collision-coalescence, and thus less rainout. This water is convected above the freezing level, leading to warming in the upper atmosphere and greater convection.\n\nThere have been several modeling studies on the effects of increased CCN on hurricane intensity. Rosenfeld \"et al.\". (2007) used the Weather Research Model (WRF) to simulate hurricane Katrina, and then turned off the warm rain processes to approximate the effects of adding a large number of CCN. They report that they were successful in simulating key features of Katrina in the control run including the minimum central pressure and maximum wind speeds. When the warm rain processes were removed, the cloud water content naturally increased and the rain water content decreased. The warm rain suppression also changes the thermodynamic structure of the hurricane: temperatures are decreased at low levels at the outer edge of the hurricane. Later, the peak winds decreased along with the central pressure.\n\nZhu and Zhang (2006) used a mesoscale model to simulate the 1998 Hurricane Bonnie. They report that their control run was reasonably able to simulate the observed hurricane. They then ran a series of sensitivity experiments to examine how changes in the microphysics influence the hurricane. The various sensitivity runs were focused on the effects of ice processes. They report that the tropical cyclones exhibit a large sensitivity in the intensity and core structures to ice phase processes. As the ice processes are removed, the intensity of the cyclone decreases without the Bergeron process. When graupel processes are removed, the storm weakens but not as much as when all ice processes are removed. And when evaporation is turned off, the storm increases in intensity dramatically. They conclude that melting and evaporation processes are important in amplifying tropical cyclones.\n\nDifferent cumulus parameterization schemes were derived for different situations. The Betts-Miller scheme (or the derivative Betts-Miller-Janjic) attempts to ensure that the local vertical temperature and moisture structures be realistic. The Betts-Miller(-Janjic) scheme is often used when simulating tropical cyclones. Davis and Bosart (2002) simulated Hurricane Diana (1984) which underwent extratropical transition. They used the Betts-Miller-Janjic cumulus parameterization scheme in two ways: one with the parameters set for midlatitude systems and the other for tropical systems. They not that when the parameterization scheme is set for midlatitude systems the simulation produces a more realistic track and storm intensity. However, the tropical simulation produces a more realistic rainfall field.\n\nPattnaik and Krishnamurtil (2007) simulated hurricane Charley of 2004 to assess the impact of cloud microphysics on hurricane intensity. They report that their control run was successful in simulating the track, intensity, speed and precipitation. They used the microphysics scheme from NASA Goddard Space Flight Center. This scheme uses five different classifications of cloud water: liquid cloud water, cloud ice, rain water, snow, and hail/graupel. It also allows for supercooled water. Their study attempts to show how fall speed and intercept parameters can influence the tropical cyclone intensity. The size distribution of precipitation particles is parametrized as:\nwhere N is the number of precipitation particles between a given diameter D and D + dD, N is the intercept parameter, λ is the slope parameter, and D is the diameter of the particles. They used the same model and microphysics scheme, turning off different microphysical mechanisms to understand which ones are the most important. They note that modifications to the microphysics scheme dramatically impacted the hurricane intensity. The most intense hurricanes were when melting was suppressed, or when no evaporation was allowed. They interpret this as meaning that the energy needed to either melt or evaporate the particles could instead be used to heat the air column, which increased convection leading to a stronger storm. During the weakest simulated storm, the fall speed of the snow and graupel particles was increased. The increased rate of fallout also increased the evaporation, leading to weakened convection. Changes in the intercept parameter showed little change. This implies that the total number of particles does not matter as much as the relative distribution between different sizes of particles.\n\nA series of simulations which principally looked at how cloud microphysics affect hurricane track also revealed that subgrid-scale turbulent mixing parameterization schemes influenced the intensity of a hurricane simulation more than its track.\n\nThough not the main goal, two work of Rosenfeld \"et al.\". (2007) noted that in their simulations that the suppression of warm rain through the addition of large amounts of aerosols will cause the tropical cyclone to divert eastward. Zhu and Zhang (2006) report that the hurricane track was not sensitive to cloud microphysical processes except for very weak storms, which were pushed to the east. In a series of sensitivity studies, Pattnaik and Krishnamurti conclude that microphysical processes have little effect on hurricane track.\n\nDavis and Bosart (2002) considered, among other things, the effects of cumulus parameterization on tropical storm track. They found that changes in potential vorticity at the tropopause can cause changes in the wind field. Specifically, different schemes have different methods of parametrizing the potential vorticity which results in different tracks. They found that the Betts-Miller-Janjic and Grill schemes produced a more westward track than Kain-Fritsch. The Kain-Fritsch scheme tended to intensify the storm too rapidly but produced the best track compared with observations. The simulated tracks of more intense storms tends to be farther to the east compared with weaker storms. Davis and Bosart also point out that their results differ from previous sensitivity studies on cumulus parameterizations which found that the Betts-Miller-Janjic scheme tended to have better results. They attribute this difference to the differences in grid spacing between the studies.\n\nThe first paper published that specifically looked at the impacts of cloud microphysics on hurricane track wa Fovell and Su (2007). They use simulations of Hurricane Rita (2005) and an idealized hurricane simulation to see how different microphysical parameterization and convection schemes change the hurricane track. They compared the effects of the Kessler (K), Lin et al. (L), and the three class WRF single moment (WFR3) schemes, coupled with the effects of Kain-Fritsch (KF), Grell-Devenyi (GD), and Betts-Miller-Janjic (BMJ) convective parameterization schemes. The hurricane that most similarly simulated Rita's track was when the WSM3 microphysics scheme was paired with BMJ convection. The worst simulated track was when the K microphysics was paired with KF convection, which produced a weaker storm that tracked well west of the actual storm. The spread from simply changing the microphysics and cumulus convection parameterization schemes produced the same spread in hurricane tracks as the National Hurricane Center ensemble.\n\nThey also note that the biggest difference between the microphysics parameterizations is that K does not include any ice phases. The differences between the crystalline nature of cloud ice and snow, compared with the spherical nature of raindrops, and the semi-spherical shape of graupel will likely produce different fall velocities when frozen water is included in the parameterizations. They used the most accurate member of the Rita simulations and changed the microphysics so that the fall speed of the ice particles would have the same fall velocity as if they were liquid raindrops with the same mass. They report that this changed the track of the hurricane so that it tracked further to the west, similar to the K scheme.\n\nIn addition to simulating Hurricane Rita, Fovell and Su (2007) also made the same simulations as before, but on a smaller grid size so that cumulus parameterization was not needed. They found that the hurricane produced by the K scheme was weaker than the rest and had the most westward track. They conclude that the different implicit microphysical assumption in the different schemes can change the hurricane track on forecasting timescales. In general, their results suggest that larger-sized hurricanes will track further westward, which is consistent with \"beta drift\".\n\nWhen an idealized set of hurricanes was produced with no large-scale flow, with variable Coriolis parameter, they found that the hurricanes still moved in the northeast to north-northeast direction. However, the different microphysical schemes tracked at different directions. Since there was no large-scale flow, they conclude that the differences in the track represent changes in the vortex motion caused by changes in the microphysics. On a constant f-plane experiment, there was no movement of the storms. They note that variations among the NWS consensus model results could be primarily due to how the different models parameterize their cloud microphysics and other subgrid-scale processes.\n\nRecently, Fovell et al. (2009) conducted a modeling study of hurricanes in idealized environments. The model had a constant sea surface temperature, no background wind, but with Earth rotation. They then inserted a vortex with varying microphysics schemes and noted that the tracks diverged. They used the same microphysics schemes as F07, and like F07 the noted that the K storm moved faster and further westward than storm produced with other microphysics schemes. An earlier study by Fiorino and Elsberry (1989) showed that hurricane track and speed can be changed by simply changing the tangential winds in the outer part of the storm because they helped determine the orientation and strength of the beta gyres. The F09 storm with the K microphysics parameterization had the largest eye and the strongest winds at large radii, while the L storm was most intense and WSM3 had the most compact eye.\n\nF09 noted that storm with stronger outer winds tracked more to the northwest than storms with weak winds. They hypothesize that this can be explained with an atmosphere in hydrostatic balance. Assuming an atmosphere that is in hydrostatic balance, the average column virtual temperature contributes the most to the surface pressure. The virtual temperatures of the three F09 storms varied with the Kessler storm having temperatures several degrees warmer than the other storms. The winds are determined by the radial pressure gradients, which are related to the temperature gradients. Therefore, storms that have a large radial variation in virtual temperature will have stronger outer winds. The temperature differences between the models can be explained by the change in radiative heating and cooling. The K microphysics scheme produced particle fall speeds that were slower than the others, thereby increasing the size of its anvil. F09 report that the most important factor that influences the size of the anvil is the terminal velocity, and that the terminal velocity of the particles depends on their geometry, density, and size. Interactions between the anvil and incoming and emitted radiation changes the radial temperature gradient, leading to changes in the track direction.\n\nFovell et al. conclude that the choice of microphysics schemes can lead to changes in the terminal velocities of the particles in the anvil which could lead to increases or decreases in the size of the anvil. Schemes that produce heavier particles that will fall faster (like K) produce worse results. They conclude by warning that any changes in storm track or speed that are initially caused by microphysics could be amplified by other dynamic factors such as the steering flow or sea surface temperatures.\n"}
{"id": "33331939", "url": "https://en.wikipedia.org/wiki?curid=33331939", "title": "Kondo model", "text": "Kondo model\n\nThe Kondo model (sometimes referred to as the s-d model) is a model for a quantum impurity coupled to a large reservoir of noninteracting electrons. The quantum impurity is represented by a spin-1/2, which is coupled to a continuous band of noninteracting electrons by an antiferromagnetic exchange coupling, J. The Kondo model is used as a model for metals containing magnetic impurities, as well as quantum dot systems.\n\nformula_1\n\nwhere formula_2 is a spin-1/2 operator representing the impurity, and formula_3 is the local spin-density of the noninteracting band at the impurity site (formula_4 are the Pauli matrices). J < 0, i.e. the exchange coupling is antiferromagnetic.\n\nJun Kondo applied third-order perturbation theory to the Kondo model and showed that the resistivity of the model diverges logarithmically as the temperature goes to zero. This explained why metal samples containing magnetic impurities have a resistance minimum (see Kondo effect). The problem of finding a solution to the Kondo model which did not contain this unphysical divergence became known as the Kondo problem.\n\nA number of methods were used to attempt to solve the Kondo problem. Phillip Anderson devised a perturbative renormalization group method, known as Poor Man's Scaling, which involves perturbatively eliminating excitations to the edges of the noninteracting band. This method indicated that, as temperature is decreased, the effective coupling between the spin and the band, formula_5, increases without limit. As this method is perturbative in J, it becomes invalid when J becomes large, so this method did not truly solve the Kondo problem, although it did hint at the way forward.\n\nThe Kondo problem was finally solved when Kenneth Wilson applied the numerical renormalization group to the Kondo model and showed that the resistivity goes to a constant as temperature goes to zero.\n\nThere are many variants of the Kondo model. For instance, the spin-1/2 can be replaced by a spin-1 or even a greater spin. The two-channel Kondo model is a variant of the Kondo model which has the spin-1/2 coupled to two independent noninteracting bands. One can also consider the ferromagnetic Kondo model (i.e. the standard Kondo model with J > 0).\n\nThe Kondo model is intimately related to the Anderson impurity model, as can be shown by Schrieffer–Wolff transformation.\n\n"}
{"id": "17345009", "url": "https://en.wikipedia.org/wiki?curid=17345009", "title": "Lease automatic custody transfer unit", "text": "Lease automatic custody transfer unit\n\nA Lease Automatic Custody Transfer unit or LACT unit measures the net volume and quality of liquid hydrocarbons. A LACT unit measures volumes in the range of 100-7000 BOPD. This system provides for the automatic measurement, sampling, and transfer of oil from the lease location into a pipeline. A system of this type is applicable where larger volumes of oil are being produced and must have a pipeline available in which to connect.\n\nAmerican Petrolum Institute (May 1991), Manual of Petroleum Measurement Standards, Chapter 6, Section 1, Lease Automatic Custody_Transfer (LACT) Systems, Second Edition. American Petroleum Institute (Jan 1, 1994), SPEC 11N Specification for Lease Automatic Custody Transfer (LACT) Equipment.\n\n"}
{"id": "30916554", "url": "https://en.wikipedia.org/wiki?curid=30916554", "title": "Maryland power plant research program", "text": "Maryland power plant research program\n\nThe Maryland Power Plant Research Program (PPRP) is a government program within the Maryland Department of Natural Resources and is charged with addressing power plant licensing issues. PPRP was established under the Power Plant Siting and Research Act of 1971. This legislation provided a model for addressing power plant licensing issues which several other states have adopted. PPRP is housed within DNR’s headquarters in Annapolis, Maryland.\n\nPPRP functions to ensure that Maryland meets its electricity demands at reasonable costs while protecting the state's valuable natural resources. It provides a continuing program for evaluating electric generation issues and recommending responsible, long-term solutions.\n\n"}
{"id": "3401385", "url": "https://en.wikipedia.org/wiki?curid=3401385", "title": "Mesoamerican Biological Corridor", "text": "Mesoamerican Biological Corridor\n\nThe Mesoamerican Biological Corridor (MBC) is a region that consists of Belize, Guatemala, El Salvador, Honduras, Nicaragua, Costa Rica, Panama, and some southern states of Mexico. The area acts as a natural land bridge from South America to North America, which is important for species who use the bridge in migration. Due to the extensive unique habitat types, Mesoamerica contains somewhere between 7 and 10% of the world’s known species.\n\nThe corridor was originally proposed in the 1990s to facilitate animal movements along the Americas without interfering with human development and land use, while promoting ecological sustainability. The Mesoamerican Biological Corridor is made of four parts: Core Zones, Buffer Zones, Corridor Zones, and Multiple-Use Zones, each with varying availability for human use.\n\nWith the increasing conversion of natural tropical ecosystems to agricultural farms and for other human use, comes growing concern over conservation of local species. Mesoamerica is considered one of many biodiversity hotspots where extinction is a significant threat. This area is the world’s third largest biodiversity hotspot. Some efforts have been made to protect organisms in the region, however, many of these protected sites are “small, fragmented, isolated, or poorly protected”\n\nIn the late 1980s, Archie Carr III envisioned a way to protect threatened and endangered wildlife native to the region by connecting fragmented patches of habitat, and to create buffer zones to allow different levels of land use near protected areas. The corridor that eventually came to be, originally called Paseo Pantera (Spanish for Path of the Panther), follows the Atlantic coastline. The MBC began in the late 1990s, by funding from the World Bank in order to promote wildlife conservation, particularly endemic, threatened, and endangered species, and ways to use the land in a sustainable fashion. It was developed by a team of biologists from the University of Florida and the Central American Commission on Environment and Development (CCAD), and was remapped by CCAD, the United Nations Development Programme (UNDP), and the Global Environment Facility (GEF) for political reason. $4 million was invested in the corridor by United States Agency for International Development (USAID) from 1990 to 1995. In 1992, all of the countries that are part of the Mesoamerican Biological Corridor joined the Central American System of Protected Areas (SICAP), which allows each country to “maintain its own ministries of the environment.” The corridor project has been successful in providing wildlife habitat; however, regional biota remained threatened due to fragmented areas and “unevenness of the region’s protected area system”\n\nThe Mesoamerican Biological Corridor incorporates multiple diverse biomes and is bordered by the Caribbean Sea to the east and the Pacific Ocean on the west. Splitting the corridor in half is the Guatemalan Mountain range, which includes active volcanoes. These environmental forces create four terrestrial biomes and 19 terrestrial ecoregions. The biomes include, tropical dry broadleaf forest, tropical wet broadleaf forests, xeric shrub lands, and tropical coniferous forests.\n\nAccording to data from 2003, roughly 57% of the Mesoamerican biological corridor is natural vegetation, with the remaining land being used mostly for cattle and crop production. The main crops produced in the MBC include sugar cane, corn, coffee, and beans. With agricultural production being such a large part of all the nations economies, there is much emphasis on adopting sustainable agricultural practices.\n\nThe Mesoamerican Biological Corridor is made of four parts: Core Zones, Buffer Zones, Corridor Zones, and Multiple-Use Zones, each with varying availability for human use. Core Zones are protected areas whose purpose is to promote and sustain biodiversity in the areas in order to maintain ecosystem services to the local people. Buffer Zones include the areas surrounding the protected Core Zones, which are made up mostly of wild land. Pathways between zones are labeled as Corridor (or Connectivity) Zones; these zones link water and land passages, allowing movement of plants and animals throughout the corridor. Finally Multi-use Zones, separate wild and protected land from land used for forestry, agriculture, and areas of direct human impacts. “Around 10.7% of Mesoamerica is currently under some category of protection for biodiversity conservation.” \n\nThe Mesoamerican Biological Corridor is a program that “integrates protection areas into a single, functional conservation area”. Their goal is to promote “regional scale connectivity of protected areas with sustainable development and improvement of human livelihoods.” The purpose of the corridor is to emphasize the conservation movement as being a social and group effort. One issue with conservation efforts arise from the discontinuity of government and politics across the corridor; areas are often fragmented and up to 40% of protected areas go unenforced because it crosses nations barriers. The rapid increase in human population growth negatively affects conservation. Although this growth has been paired with rapid urbanization, the majority of the MBC population still resides in rural areas and “depends directly on biological resources for subsistence.” This dependency has led to exploitation that is difficult to quantify and regulate by the nations’ governments and conservation groups.\nAs of 2010, SICAP (Central American System of Protected Areas) encompasses 669 protected areas that total 124,250 square kilometers. Yet, conservation efforts are hindered and negatively impacted by the fragmentation of land parcels and cross-national political differences and tension. Most of the protected areas are roughly 18,400 hectares, while only 18 areas exceeded 1,000 square kilometers. Presently, most conservation efforts are in promoting sustainable development and mitigating the damage done to the area by deforestation. Deforestation in the Mesoamerican Biological Corridor peaked between the 1970s and the 1990s. Planting native trees is the main method of restoring ecosystems after deforestation.\n\nDue to the corridor having been recently developed there has not been any studies that specifically address the benefits. Future studies should be completed to examine the differences in animal populations prior to the corridor and after implementation.\n\nWhen the Mesoamerican Biological Corridor was in the planning process there was a lack of formal functions proposed. The stakeholders did not have a clear idea of what the exact functions of the MBC were, which led to anger and an increase in the time taken to implement the corridor. The MBC was originally conceived as a way to protect threatened and endangered wildlife by connecting fragments of habitats and forming buffer zones to limit human land use. However, many of the interested stakeholders wanted to include common livelihood problems such as pollution, water and sanitation, pesticides contamination, firewood acquisition, zoonotic and infectious disease. It was finally decided that the main goals of the corridor would be to facilitate animal movements along the Americas without interfering with human development and land use, while promoting ecological sustainability. Indigenous people were barely involved in these decisions and the zone boundaries were made without their input. This lack of input led to distrust and tension between the locals and corridor implementers.\n\nIn an effort to promote ecological sustainability, payment for various environmental services are given to landowners in order to motivate reforestation on their land. A major issue with these programs is that most small landholders do not have titles to the land. These small landholders were given plots to cultivate when they worked on larger farms or many were displaced migrants who settled in unclaimed lands. Since they have no legal documentation of land ownership they can’t apply for many of the correct land use incentives, thus little consideration of long-term effects on the land is given. Another issue is that the programs don’t differentiate between small-scale and large-scale landowners. In an effort to reduce Carbon emissions the MBC offers incentives for carbon sinks. Large-scale landowners have taken advantage of these systems by planting African Oil Palms on their lands. These plants provide them with more carbon credits whereas a small landowner who is maintaining forest will receive little to no carbon credits\n"}
{"id": "44146963", "url": "https://en.wikipedia.org/wiki?curid=44146963", "title": "Metallomesogen", "text": "Metallomesogen\n\nA metallomesogen is a metal complex that exhibit liquid crystalline behavior. Thus, they adopt ordered structures in the molten phase, e.g. smectic and nematic phases. The dominant interactions responsible for their phase behavior are the nonbonding contacts between organic substituents. Two early classes of such materials are based on substituted ferrocenes and dithiolene complexes.\n"}
{"id": "2813644", "url": "https://en.wikipedia.org/wiki?curid=2813644", "title": "Micarta", "text": "Micarta\n\nMicarta is a brand name for composites of linen, canvas, paper, fiberglass, carbon fiber or other fabric in a thermosetting plastic. It was originally used in electrical and decorative applications. Micarta was developed by George Westinghouse at least as early as 1910 using phenolic resins invented by Leo Baekeland. These resins were used to impregnate paper and cotton fabric which were cured under pressure and high temperature to produce laminates. In later years this manufacturing method included the use of fiberglass fabric and other resin types were also used. Today Micarta high pressure industrial laminates are produced with a wide variety of resins and fibers. The term has been used generically for most resin impregnated fibre compounds. Common uses of modern high pressure laminates are as electrical insulators, printed circuit board substrates, and knife handles.\n\nThe Micarta trademark is a registered trademark of Industrial Laminates / Norplex, Inc. (dba *Norplex-Micarta).\n\nMicarta industrial laminates are normally phenolic, epoxy, silicone, or melamine resin based thermoset materials reinforced with fiberglass, cork, cotton cloth, paper, carbon fiber or other substrates. Micarta industrial laminate sheet is a hard, dense material made by applying heat and pressure to layers of prepreg. These layers of laminations are usually of cellulose paper, cotton fabrics, synthetic yarn fabrics, glass fabrics, or unwoven fabrics. When heat and pressure are applied to the layers, a chemical reaction (polymerization) transforms the layers into a high-pressure thermosetting industrial laminated plastic.\n\nMicarta industrial laminates are manufactured in dozens of commercial grades.\n\nThe largest use for Micarta industrial laminates is a high strength electrical insulation in power generating and distribution equipment. Laminates are also used in heavy equipment, aerospace (such as propeller blades), automotive, office equipment, tabletops, countertops, electronic, electrical insulation between pressure vessels or piping and their supports, decorative applications, including knife handles, BBQ and kitchen tool handles, and handgun grips, guitar fingerboards, nuts and bridges, pool cues, and safety gear such as hard hats. Between 1935 and 1945, Westinghouse's Power-Aire desk fans used blades made of Micarta.\n\nMicarta 259-2 was used as the ablation heat shield material in early ICBM warheads.\n\nMicarta's industrial laminate division and name was purchased by Norplex in 2003, merging two of the largest industrial laminate manufactures in the United States. Norplex still manufactures Micarta and produces over 100 different versions of Micarta offered in sheet, tube, and rod forms. *Norplex-Micarta Industrial Composites\n\n\n"}
{"id": "40333710", "url": "https://en.wikipedia.org/wiki?curid=40333710", "title": "Myrmicacin", "text": "Myrmicacin\n\nMyrmicacin (3-hydroxydecanoic acid) is a chemical compound of the β-hydroxycarboxylic acid class. It is named after the South American leaf-cutter ants (Myrmicinae) in which it was first discovered, but is also found in royal jelly. Myrmicacin is believed to act as a herbicide which prevents seeds collected by the ants from germinating within the nest.\n\n"}
{"id": "2428725", "url": "https://en.wikipedia.org/wiki?curid=2428725", "title": "North Luangwa National Park", "text": "North Luangwa National Park\n\nNorth Luangwa National Park is a national park in Zambia, the northernmost of the three in the valley of the Luangwa River. Founded as a game reserve in 1938, it became a national park in 1972 and now covers 4,636 km².\n\nLike the South Park, its eastern boundary is the Luangwa River, while it rises to cover a stretch of the Muchinga Escarpment to the west. The Mwaleshi River flows east–west through the centre of the park, the area to its south being a strict wilderness zone.\n\nWildlife is widely found, including Cookson's wildebeest, Crawshay's zebra and many antelopes and birds. Elephant numbers have recovered from poaching in the 1970s and 80s. The struggle against poaching in the park was described by Delia and Mark Owens in their book \"The Eye of the Elephant\".\n\nFor many years its wildlife suffered greatly from poaching, but recent years have seen poaching almost entirely stopped. It has generally suffered from a lack of investment and interest compared to the much more popular South Luangwa National Park, although its flora and fauna are very similar to its southern counterpart. In 2003, black rhinoceroses were re-introduced to the park.\n\nSince 2005, the protected area is considered a Lion Conservation Unit together with South Luangwa National Park.\n"}
{"id": "53573195", "url": "https://en.wikipedia.org/wiki?curid=53573195", "title": "Northern Marmara and Değirmenköy (Silivri) Depleted Gas Reservoir", "text": "Northern Marmara and Değirmenköy (Silivri) Depleted Gas Reservoir\n\nNorthern Marmara and Değirmenköy (Silivri) Depleted Gas Reservoir () are underground natural gas storages inside depleted gas fields in Istanbul Province, northwestern Turkey. Combined, it is the country's first underground natural gas storage facility.\n\nOne of the storage facilities is situated inside a depleted gas field undersea in northern Marmara Sea and the other is in neighboring Değirmenköy, a town in Silivri district of Istanbul Province. Both sites were suitable due to their proximity to Istanbul and to the gas pipeline of BOTAŞ. \n\nNorthern Marmara Gas Field was discovered in 1988 in an area west of Silivri and far off the coast at a depth of . To determine the size of the natural gas reserve, which is the first undersea natural gas reserve in Turkey, three offshore boreholes in 1995 and two more were drilled in 1996. Natural gas production started in September 1997 at the five gas wells. Gas was pumped from an offshore platform by a -long undersea pipeline to the plant at the coast for processing. Between 2003 and 2004, six directional wells were drilled, which had vertical depths of and horizontal deviation of .\n\nDeğirmenköy Natural Gas Field is located west of Silivri. The field was discovered in 1994, and the production started in 1995 from nine wells, seven of which were directional. Gas processing facility was built by a consortium of German Lurgi AG and Turkish Fernas Construction Ltd.\n\nThe storage facilities of Northern Marmara and Değirmenköy were projected by the Turkish Petroleum Corporation (TPAO) in 1996. The depleted gas reservoirs went into service in July 2007. The Northern Marmara Reservoir is connected to the main processing plant of BOTAŞ by a -long pipeline and the Değirmenköy Reservoir by a -long pipeline.\n\nThe storage capacity of the Northern Marmara Reservoir is and of the Değirmenköy Reservoir is . While the maximum daily gas injection capacity is , the maximum withdrawal capacity per day is .\n\nCurrently, the Northern Marmara and Değirmenköy (Silivri) Depleted Gas Reservoir is the only underground natural gas storage facility in Turkey. It is operated by the TPAO.\n\nThe entire natural gas storage project is planned in three phases. The second phase involves the capacity expansion for the Değirmenköy facility, and the third phase for the Northern Marmara facility. The second phase expansion project, which is scheduled to be completed in 2020, provides increasing of the daily injection capacity up to and the maximum daily withdrawal capacity to . It is planned that the total storage capacity will be , the daily injection capacity and the daily withdrawal capacity after completion of the third phase. \n\n"}
{"id": "48565516", "url": "https://en.wikipedia.org/wiki?curid=48565516", "title": "PTC rubber", "text": "PTC rubber\n\nPTC rubber is a silicone rubber which conducts electricity with a resistivity that increases exponentially with increasing temperature for all temperatures up to a temperature where the resistivity grows to infinity. Above this temperature the PTC rubber is an electrical insulator. PTC rubber is made from polydimethylsiloxane (PDMS) loaded with carbon nanoparticles. PTC stands for Positive temperature coefficient.\n\nIf the electric field strength inside the material is large enough (typically larger than 30 V/mm), the carbon nanoparticles trigger a quantum mechanical tunneling effect current to flow through the material. The contribution from a large number of small tunneling effect currents can add up to macroscopic currents in the range of amperes.\nThe quantum mechanical tunneling effect inside the material is highly temperature dependent. The current decreases exponentially with increasing temperature. This means that the resistivity of the material grows exponentially with increasing temperature. At a specific temperature the quantum mechanical tunneling effect current ceases. At temperatures higher than this the PTC rubber is an electrical insulator and no electrical current can flow through it. This temperature can be adjusted between 32 and 176 F (0 and 80 C) during the production of the PTC rubber. Hence, PTC rubber is a PTC material with very strong PTC characteristics. In fact, PTC rubber has the strongest PTC effect of all known materials, and over a wide temperature range. It has PTC properties for all temperatures.\n\nThe PTC rubber can be rolled into thin sheets and laminated with copper. The copper is in turn connected to a voltage to provide the electrical field inside the material necessary to trigger the tunneling effect. The sheets can be formed into any shape and size.\n\nPTC rubber sheets can be used as thin flexible PTC heaters. These heaters will provide high power when they are cold and rapidly heat up themselves to a constant temperature and remain there virtually unaffected by changes in the ambient conditions. They can be powered with any voltage between 5 and 230 V, AC or DC.\n\nThe PTC rubber material is a self-regulating heater. It produces the same amount of heat in each point of the heater as is conducted and radiated away from the heater to the object it is attached to, and its surroundings. It is in constant thermal equilibrium with the environment, point by point. A measure of the power produced by the heater is actually a measurement of the heat transfer between the heater and the object. Hence, it can be used as a heat transfer sensor.\n"}
{"id": "871170", "url": "https://en.wikipedia.org/wiki?curid=871170", "title": "Paint stripper", "text": "Paint stripper\n\nPaint stripper, or paint remover, is a product designed to remove paint and other finishes and also to clean the underlying surface. The removal of paint containing lead may lead to lead poisoning and is regulated in the United States. Other paint removal methods involve mechanical (scraping or sanding) or heat (hot air, radiant heat, or steam). A material safety data sheet will provide more safety information than on the product label.\n\nChemical paint removers work only on certain types of finishes, and when multiple types of finishes may have been used on any particular surface, trial and error testing is typical to determine the best stripper for each application. Two basic categories of chemical paint removers are caustic and solvent.\n\nCaustic paint removers, typically sodium hydroxide (also known as lye or caustic soda), work by breaking down the chemical bonds of the paint, usually by hydrolysis of the chain bonds of the polymers forming the paint. Caustic removers must be neutralized or the new finish will fail prematurely. In addition, several side effects and health risks must be taken into account in using caustic paint removers. Such caustic aqueous solutions are typically used by antique dealers who aim to restore old furniture by stripping off worn varnishes, for example.\n\nSolvent paint strippers penetrate the layers of paint and break the bond between the paint and the object by swelling the paint.\n\nThe principal active ingredient in historically common solvent paint strippers is dichloromethane, also called methylene chloride, which has serious health risks including death, is likely a carcinogen, and other risks.\n\nSolvent strippers may also have formulations with orange oil (or other terpene solvents), N-methylpyrrolidone, esters such as dibasic esters (often dimethyl esters of shorter dicarboxylic acids, sometimes aminated, for example, adipic acid or glutamic acid), aromatic hydrocarbons, dimethylformamide, and other solvents are known as well. The formula differs according to the type of paint and the character of the underlying surface. Nitromethane is another commonly used solvent. Dimethyl sulfoxide is a less toxic alternative solvent used in some formulations.\n\nPaint strippers come in a liquid, or a gel (\"thixotropic\") form that clings even to vertical surfaces.\n\nThe principle of paint strippers is penetration of the paint film by the molecules of the active ingredient, causing its swelling; this volume increase causes internal strains, which, together with the weakening of the layer's adhesion to the underlying surface, leads to separation of the layer of the paint from the substrate.\n\nVarious co-solvents are added to the primary active ingredient. These assist with penetration into the paint and its removal and differ according to the target paint. Ethanol is suitable for shellac, methyl ethyl ketone is used for cellulose nitrate, and phenol and cresols are employed in some industrial formulas. Benzyl alcohol is used as well.\n\nActivators increase the penetration rate; for dichloromethane water is suitable, other choices are amines, strong acids or strong alkalis. The activator's role is to disrupt the molecular and intermolecular bonds in the paint film and assist with weakening this. Its composition depends on the character of the paint to be removed. Mineral acids are used for epoxy resins to hydrolyze their ether bonds. Alkaline activators are usually based on sodium hydroxide. Some cosolvents double as activators. Amine activators, alkalines weaker than inorganic hydroxides, are favored when the substrate could be corroded by strong acids or bases.\n\nSurfactants assist with wetting the surface, increasing the area of where the solvent can penetrate the paint layer. Anionic surfactants (e.g., dodecyl benzenesulfonate or sodium xylene sulfonate) are used for acidic formulas, cationic or non-ionic are suitable for alkaline formulas.\nPaint strippers containing surfactants are excellent brush cleaners.\n\nThickeners are used for thixotropic formulas to help the mixture form gel that adheres to vertical surfaces and to reduce the evaporation of the solvents, thus prolonging the time the solvent can penetrate the paint. Cellulose-based agents, e.g., hydroxypropyl cellulose, are commonly used for mixtures that are not extremely acidic or basic; under such conditions cellulose undergoes hydrolysis and loses effectiveness, so fumed silica is used for these instead. Another possibility is using waxes (usually paraffin wax or polyethylene or polypropylene derivatives), or polyacrylate gels.\n\nCorrosion inhibitors are added to the formula to protect the underlying substrate and the paint stripper storage vessel (usually a steel can) from corrosion. Dichloromethane decomposes with time to hydrochloric acid, which readily reacts with propylene oxide or butylene oxide and therefore is removed from the solution. Chromate-based inhibitors give the mixture a characteristic yellow color. Other possibilities include polyphosphates, silicates, borates and various antioxidants.\n\nSequestrants and chelating agents are used to \"disarm\" metal ions present in the solution, which could otherwise reduce the efficiency of other components, and assist with cleaning stains, which often contain metal compounds. The most common sequestrants used in paint strippers are EDTA, tributyl phosphate, and sodium phosphate.\n\nColorants may be added.\n\nHeat guns are an alternative to chemical paint strippers. When heated, softened paint clumps and is easier to contain. High-temperature heat guns at or more create toxic lead fumes in lead-based paint, but low-temperature heat guns and infrared paint removers do not create lead fumes. Fire is a possible hazard of using heat guns.\n\nSteam can be used on large surfaces or items to be stripped, such as window sash, can be placed inside a steam box.\n\n\n"}
{"id": "1285760", "url": "https://en.wikipedia.org/wiki?curid=1285760", "title": "Penetrating oil", "text": "Penetrating oil\n\nPenetrating oil, also known as penetrating fluid, is very low-viscosity oil. It can be used to free rusted mechanical parts (such as nuts and bolts) so that they can be removed, because it can penetrate into the narrow space between the threads of two parts. It can also be used as a general-purpose lubricant, a cleaner, or a corrosion stopper. Using penetrating fluids as general-purpose lubricants is not advisable, because such oils are relatively volatile. As a result, much of the penetrating oil will evaporate in a short amount of time, leaving little residual lubricant.\n\nOther uses include: removing chewing gum and adhesive stickers, lessening friction on metal-stringed musical instruments, various gardening purposes and household repair tasks.\n\n"}
{"id": "12179151", "url": "https://en.wikipedia.org/wiki?curid=12179151", "title": "Pericopsis elata", "text": "Pericopsis elata\n\nPericopsis elata is a species of flowering plant in the family \"Fabaceae\" and is known by the common names African teak, afromosia, afrormosia, kokrodua and assamela. \n\nThe species is native to moist, semi-deciduous forests in Cameroon, Republic of the Congo, DRC, Ivory Coast, Ghana, and Nigeria.\n\nThe species grows to 30-45m tall with a trunk of 1-1.5m in diameter. Annual diameter increases between unlogged and logged areas have been shown to be similar. It is a deciduous species that flowers at the end of the main dry season. The minimum trunk diameter for reproduction is given as 32 cm, while that for effective flowering is 37 cm. The fruit take 7 months to mature.\n\nThe tree produces hardwood timber of high commercial value due to its texture, strength, density and durability. It is used in the manufacture of boats, veneer and furniture.\n\nThe DRC has the world's largest remaining stocks of Afrormosia, which are largely confined to the Équateur and Orientale Provinces.\n\nIllegal logging and habitat loss pose a realistic threat to the afrormosia, which ranks among the most valued hard tropical timber species. Following decades of extraction in the 20th and 21st century, it is ranked CITES Appendix II. This implies that it is subject to trade regulation because it is recognised that unregulated trade puts the species at risk of extinction. The factors that control its population dynamics are however imperfectly known. With a minimum logging diameter of 90 cm, full recovery is expected over a 30-year period. Selective logging of 12% of seed trees has been shown to have little influence on its survival. Sustainability is believed to be best achieved by purposeful actions to ensure regeneration after logging.\n"}
{"id": "30791", "url": "https://en.wikipedia.org/wiki?curid=30791", "title": "Polytetrafluoroethylene", "text": "Polytetrafluoroethylene\n\nPolytetrafluoroethylene (PTFE) is a synthetic fluoropolymer of tetrafluoroethylene that has numerous applications. The best known brand name of PTFE-based formulas is Teflon by Chemours. Chemours is a 2015 spin-off of DuPont Co., which discovered the compound in 1938.\n\nPTFE is a fluorocarbon solid, as it is a high-molecular-weight compound consisting wholly of carbon and fluorine. PTFE is hydrophobic: neither water nor water-containing substances wet PTFE, as fluorocarbons demonstrate mitigated London dispersion forces due to the high electronegativity of fluorine. PTFE has one of the lowest coefficients of friction of any solid.\n\nPTFE is used as a non-stick coating for pans and other cookware. It is nonreactive, partly because of the strength of carbon–fluorine bonds, and so it is often used in containers and pipework for reactive and corrosive chemicals. Where used as a lubricant, PTFE reduces friction, wear, and energy consumption of machinery. It is commonly used as a graft material in surgical interventions. Also, it is frequently employed as coating on catheters; this interferes with the ability of bacteria and other infectious agents to adhere to catheters and cause hospital-acquired infections.\n\nPTFE was accidentally discovered in 1938 by Roy Plunkett while he was working in New Jersey for DuPont. As Plunkett attempted to make a new chlorofluorocarbon refrigerant, the tetrafluoroethylene gas in its pressure bottle stopped flowing before the bottle's weight had dropped to the point signaling \"empty.\" Since Plunkett was measuring the amount of gas used by weighing the bottle, he became curious as to the source of the weight, and finally resorted to sawing the bottle apart. He found the bottle's interior coated with a waxy white material that was oddly slippery. Analysis showed that it was polymerized perfluoroethylene, with the iron from the inside of the container having acted as a catalyst at high pressure. Kinetic Chemicals patented the new fluorinated plastic (analogous to the already known polyethylene) in 1941, and registered the Teflon trademark in 1945.\n\nBy 1948, DuPont, which founded Kinetic Chemicals in partnership with General Motors, was producing over two million pounds (900 tons) of Teflon brand PTFE per year in Parkersburg, West Virginia. An early use was in the Manhattan Project as a material to coat valves and seals in the pipes holding highly reactive uranium hexafluoride at the vast K-25 uranium enrichment plant in Oak Ridge, Tennessee.\n\nIn 1954, the wife of French engineer Marc Grégoire urged him to try the material he had been using on fishing tackle on her cooking pans. He subsequently created the first PTFE-coated, non-stick pans under the brandname Tefal (combining \"Tef\" from \"Teflon\" and \"al\" from aluminium). In the United States, Marion A. Trozzolo, who had been using the substance on scientific utensils, marketed the first US-made PTFE-coated pan, \"The Happy Pan\", in 1961.\n\nHowever, Tefal was not the only company to utilize PTFE in nonstick cookware coatings. In subsequent years, many cookware manufacturers developed proprietary PTFE-based formulas, including Swiss Diamond International, which uses a diamond-reinforced PTFE formula; Scanpan, which uses a titanium-reinforced PTFE formula; and both All-Clad and Newell Rubbermaid's Calphalon, which use a non-reinforced PTFE-based nonstick. Other cookware companies, such as Meyer Corporation's Anolon, use Teflon nonstick coatings purchased from Chemours.\n\nIn the 1990s, it was found that PTFE could be radiation cross-linked above its melting point in an oxygen-free environment. Electron beam processing is one example of radiation processing. Cross-linked PTFE has improved high-temperature mechanical properties and radiation stability. This was significant because, for many years, irradiation at ambient conditions has been used to break down PTFE for recycling. This radiation-induced chain scission allows it to be more easily reground and reused.\n\nPTFE is produced by free-radical polymerization of tetrafluoroethylene. The net equation is\nBecause tetrafluoroethylene can explosively decompose to tetrafluoromethane and carbon, special apparatus is required for the polymerization to prevent hot spots that might initiate this dangerous side reaction. The process is typically initiated with persulfate, which homolyzes to generate sulfate radicals:\nThe resulting polymer is terminated with sulfate ester groups, which can be hydrolyzed to give OH end-groups.\n\nBecause PTFE is poorly soluble in almost all solvents, the polymerization is conducted as an emulsion in water. This process gives a suspension of polymer particles. Alternatively, the polymerization is conducted using a surfactant such as PFOS.\n\nPTFE is a thermoplastic polymer, which is a white solid at room temperature, with a density of about 2200 kg/m. According to Chemours, its melting point is . It maintains high strength, toughness and self-lubrication at low temperatures down to , and good flexibility at temperatures above . PTFE gains its properties from the aggregate effect of carbon-fluorine bonds, as do all fluorocarbons. The only chemicals known to affect these carbon-fluorine bonds are highly reactive metals like the alkali metals, and at higher temperatures also such metals as aluminium and magnesium, and fluorinating agents such as xenon difluoride and cobalt(III) fluoride.\n\nThe coefficient of friction of plastics is usually measured against polished steel. PTFE's coefficient of friction is 0.05 to 0.10, which is the third-lowest of any known solid material (BAM being the first, with a coefficient of friction of 0.02; diamond-like carbon being second-lowest at 0.05). PTFE's resistance to van der Waals forces means that it is the only known surface to which a gecko cannot stick. In fact, PTFE can be used to prevent insects climbing up surfaces painted with the material. PTFE is so slippery that insects cannot get a grip and tend to fall off. For example, PTFE is used to prevent ants climbing out of formicaria.\n\nBecause of its chemical inertness, PTFE cannot be cross-linked like an elastomer. Therefore, it has no \"memory\" and is subject to creep. Because of its superior chemical and thermal properties, PTFE is often used as a gasket material within industries that require resistance to aggressive chemicals such as pharmaceuticals or chemical processing. However, because of the propensity to creep, the long-term performance of such seals is worse than for elastomers which exhibit zero, or near-zero, levels of creep. In critical applications, Belleville washers are often used to apply continuous force to PTFE gaskets, ensuring a minimal loss of performance over the lifetime of the gasket.\n\nProcessing PTFE can be difficult and expensive, because the high melting temperature, , is above the initial decomposition temperature, . Even when melted, PTFE does not flow, but instead behaves as a gel due to the absence of crystalline phase and high melt viscosity.\n\nSome PTFE parts are made by cold-moulding, a form of compression molding. Here, fine powdered PTFE is forced into a mould under high pressure (10–100 MPa). After a settling period, lasting from minutes to days, the mould is heated at , allowing the fine particles to fuse into a single mass.\n\nThe major application of PTFE, consuming about 50% of production, is for wiring in aerospace and computer applications (e.g. hookup wire, coaxial cables). This application exploits the fact that PTFE has excellent dielectric properties, especially at high radio frequencies, making it suitable for use as an excellent insulator in connector assemblies and cables, and in printed circuit boards used at microwave frequencies. Combined with its high melting temperature, this makes it the material of choice as a high-performance substitute for the weaker and lower-melting-point polyethylene commonly used in low-cost applications.\n\nIn industrial applications, owing to its low friction, PTFE is used for plain bearings, gears, slide plates, seals, gaskets, bushings, and more applications with sliding action of parts, where it outperforms acetal and nylon.\nIts extremely high bulk resistivity makes it an ideal material for fabricating long-life electrets, the electrostatic analogues of permanent magnets.\n\nPTFE film is also widely used in the production of carbon fiber composites as well as fiberglass composites, notably in the aerospace industry. PTFE film is used as a barrier between the carbon or fiberglass part being built, and breather and bagging materials used to incapsulate the bondment when debulking (vacuum removal of air from between layers of laid-up plies of material) and when curing the composite, usually in an autoclave. The PTFE, used here as a film, prevents the non-production materials from sticking to the part being built, which is sticky due to the carbon-graphite or fiberglass plies being pre-pregnated with bismaleimide resin. Non-production materials such as Teflon, Airweave Breather and the bag itself would be considered F.O.D. (foreign object debris/damage) if left in layup.\n\nBecause of its extreme non-reactivity and high temperature rating, PTFE is often used as the liner in hose assemblies, expansion joints, and in industrial pipe lines, particularly in applications using acids, alkalis, or other chemicals. Its frictionless qualities allow improved flow of highly viscous liquids, and for uses in applications such as brake hoses.\n\nGore-Tex is a brand of expanded PTFE (ePTFE), a material incorporating a fluoropolymer membrane with micropores. The roof of the Hubert H. Humphrey Metrodome in Minneapolis, US, was one of the largest applications of PTFE coatings. of the material was used in the creation of the white double-layered PTFE-coated fiberglass dome.\n\nPTFE is often found in musical instrument lubrication product; most commonly, valve oil.\n\nPTFE is used in some aerosol lubricant sprays, including in micronized and polarized form. It is notable for its extremely low coefficient of friction, its hydrophobia (which serves to inhibit rust), and for the dry film it forms after application, which allows it to resist collecting particles that might otherwise form an abrasive paste. \n\nPTFE (Teflon) is best known for its use in coating non-stick frying pans and other cookware, as it is hydrophobic and possesses fairly high heat resistance.\n\nThe sole plates of some clothes irons are coated with PTFE (Teflon).\n\nOther niche applications include:\n\nPyrolysis of PTFE is detectable at , and it evolves several fluorocarbon gases and a sublimate. An animal study conducted in 1955 concluded that it is unlikely that these products would be generated in amounts significant to health at temperatures below .\n\nWhile PTFE is stable and nontoxic at lower temperatures, it begins to deteriorate after the temperature of cookware reaches about , and decomposes above . The degradation by-products can be lethal to birds, and can cause flu-like symptoms in humans—see polymer fume fever. Meat is usually fried between , and most oils start to smoke before a temperature of is reached, but there are at least two cooking oils (refined safflower oil at and avocado oil at ) that have a higher smoke point.\n\nPerfluorooctanoic acid (PFOA, or C8) has been used as a surfactant in the emulsion polymerization of PTFE, although several manufacturers have entirely discontinued its use.\n\nPFOA persists indefinitely in the environment. It is a toxicant and carcinogen in animals. PFOA has been detected in the blood of more than 98% of the general US population in the low and sub-parts per billion range, and levels are higher in chemical plant employees and surrounding subpopulations. The general population has been exposed to PFOA through massive dumping of C8 waste into the ocean and near the Ohio River Valley. PFOA has been detected in industrial waste, stain resistant carpets, carpet cleaning liquids, house dust, microwave popcorn bags, water, food and Teflon cookware.\nAs a result of a class-action lawsuit and community settlement with DuPont, three epidemiologists conducted studies on the population surrounding a chemical plant that was exposed to PFOA at levels greater than in the general population. The studies concluded that there was probably an association between PFOA exposure and six health outcomes: kidney cancer, testicular cancer, ulcerative colitis, thyroid disease, hypercholesterolemia (high cholesterol), and pregnancy-induced hypertension.\n\nOverall, PTFE cookware is considered an insignificant exposure pathway to PFOA.\n\nThe Teflon trade name is also used for other polymers with similar compositions:\n\n\nThese retain the useful PTFE properties of low friction and nonreactivity, but are more easily formable. For example, FEP is softer than PTFE and melts at ; it is also highly transparent and resistant to sunlight.\n\n"}
{"id": "21194474", "url": "https://en.wikipedia.org/wiki?curid=21194474", "title": "ReNew", "text": "ReNew\n\nReNew\" magazine (currently subtitled ReNew: technology for a sustainable future) is an Australian magazine covering domestic renewable energy technologies and sustainable culture. Originally a small magazine, printed and distributed locally in Melbourne, it was first published by the Alternative Energy Co-operative in 1980 as Soft Technology: Alternative Energy in Australia\". Although it sold for the high cover price of $0.85, the magazine's circulation increased so rapidly that by issue 35, published in February 1991, it included a full colour cover. The price was increased from $2.50 to $3 and national distribution rights secured for issue 40, published in June the following year. A total of 13,000 copies were printed. The magazine was renamed, after much deliberation, under the present title in 1996.\n\n\n"}
{"id": "1845085", "url": "https://en.wikipedia.org/wiki?curid=1845085", "title": "Siberian natural resources", "text": "Siberian natural resources\n\nSiberian natural resources refers to resources found in Russian Siberia, in the North Asian Mainland. The Siberian region is rich in resources, including coal, oil and metal ores\n\nSiberia’s contribution to the Soviet economy in percent of national output was given in Soviet statistical yearbooks for 1973 (1940 in brackets) as: Coal 33% (23%), Coking coal 30% (17%), Oil 21% (1.6%), Natural gas 8.5% (from 1.5% in 1950), Electric power output 18% (6.6%), Iron ore 6.9% (1.6%), Pig iron 9.5% (10%), Crude steel 8.3% (10%), Rolled steel 10% (9.1%). But regional breakdowns were omitted in the yearbooks from 1973, except for a few 1975 figures.\n\nIn the important Chelyabinsk coalfields, production rose from 390,000 tonnes in 1925 to 3,519,000 tonnes in 1936. The total production for the East Urals was 8,080,000 tonnes in 1937. Reserves for the following coalfields are shown in millions of tonnes:\n\n\nThe development of the coal fields of Kuznetsk Basin, (sometimes called the \"Kuzbass\") transformed the Siberian steppe. The south sector of the Trans-Siberian Railway became a great industrial center because of the activity surrounding the coalfields. These ancient reserves of 13,000,000,000 tonnes grew to 450,658,000,000 tonnes. In 1937, the 50 mines in this area produced a total of 17,300,000 tonnes. The coal production of this zone was comparable with that of all of British India, and half of that produced by Japan. Kuznetsk coal was the best in the USSR, with high energy and low sulphur content. The total anthracite reserve was 54 million tonnes; and was used in the Ural-Kuznetsk Metallurgical Combine. Siberia is also the coldest winters to survive, this includes freezing temperature such as -50 degrees.\n\nAnother important reserve is at Karaganda near the Magnitogorsk (Magnet City) Higt Ovens. Production in 1937 was 3,937,200 tonnes. Other important coal deposits are: Minusinsk near Chernogorsk, which joins the mining zone of Chulym-Yenisei at the Yenisei river; the Kansk deposits north of Krasnoyarsk; the Irkutsk deposits, which yielded 3,000,000 tonnes from the Cheremkovo mine in 1937; the Lake Baikal deposits; the Lena sector; the Norilsk sector in Tunguska mining zone; the Sangar Khai founts in the Amur River and Bureya Rivers near Vladivostok, the Artem and Suchan mines with 1937 production of 2,110,000 tonnes and 590,000 tonnes respectively; and the Sakhalin coal deposits in the Pamir and Tian Shan mountains in central Asia.\n\nIn the Minusinsk area, the estimated reserves are 20,612,000,000 tonnes; the Chulym-Yenisei mine is estimated at 43,000,000,000 tonnes; the Kansk sources estimated at 42,000,000,000 tonnes; and Cheremkhovo estimated at 79,000,000,000 tonnes. The Kuznets area has excellent coal for coke, chemical and gas production. Production in 1913 was 774,000 tonnes. In 1927, these mines extracted about 2,600,000 tonnes to maintain one extraction of 16,800,000 tonnes. The modernized Prokopyevsk mine has an expected production capacity of about 3,200,000 tonnes. The other sources are in Stalinsk (Kuznets), Lenin-Kuznets, Kemerovo, Anzhero-Sudzhensk, Chernogorsk, and Cheremkhovo\n\nThe Siberian petroleum sources follow (in metric tonnes - 1 metric tonne of petroleum is equivalent to 5 or 10 barrels, or 42 gallons, depending on the density)\n\n\nTable of Total production (for 45 oil well areas)\n\nThe most important Siberian petroleum zones are the West Siberian petroleum basin, Central Urals, Sakhalin Island, Nordvyl on the Arctic Siberian coast, and the Kamchatka peninsula. From the Caspian Sea there is one oil pipeline, which continues to the petrol camps of Emba at Orsk and ends in Omsk, in western Siberia. Sakhalin Island has the most important oil reserves in the Russian Far East. In 1936, the Ohka oil wells extracted about 470,000 tonnes; one-third were obtained for Japanese concessionaires. In the Emba River area about 466,000 tonnes were extracted from about 20 pits of a total of 300 yaciments in 1937.\n\nTotal USSR oil production was 230,700,000 tonnes and there exist other reserves of 652,000,000 tonnes.\n\nThe third source of energy is hydro electricity. The region boasts large rivers capable of accommodating in-river hydro plants of 1000 MW and more per project. This potential was realised at an early stage, leading to investigations into the hydro potential of Pamir Tien-Shan and other East Siberian hydro resources. Today these hydro systems contribute roughly 40% of the electricity produced in Russia's Second Electricity Zone (Siberia) and helps to explain why the wholesale electricity prices in Zone 2 are structurally cheaper than in Zone 1 (European Russia). \n\nIn 2011, Russia's electricity consumption totalled 1022 TWh, of which Hydropower contributed 63TWh. These energy producing and disposes in 50% of time raised in about 280.690 gigawatts (GW), between of theirs based in one disposition of 95% stay 58 GW.\n\nTo increase output, studies were made of the Lena, Yenisei and Ob river systems. In the Lenin Program in 1920, proposed construction of power systems in the Urals, Yenisei, Angara River and Lake Baikal. Some of these projects are similar to the Grand Coulee Dam in the Columbia River.\n\nA major hydroelectric powerplant was constructed in 1932 on the Dnieper River, at a high latitude in Zaporozhe. But it was destroyed in 1941 by retreating Soviet forces during World War II. The plant had a production capacity of 900 MW, was about 2,500 feet long, and rose 125 feet above water level. In 1940 the total production capacity was 2.5 GW. The new plan proposed plants on a gigantic scale, on the Angara river. Planned output was 9 GW, with four other plants in high Yenisei producing about 4 GW more.\n\nSiberian iron sources were more assorted. They are at Magnitogorsk, Nizhni Tagil deposits in the south of Kuznetsk, the Angara River reserves, and Russian Far East mines.\n\nThe mines of the Urals have been known since 1702: Magnitogorsk\nwith annual extraction of 6,000,000 tonnes in 1931, minerals being \nmagnetite and secondarily martite, with 55% or 66% of iron content. The\nother and oldest center was in Ninshi-Tagil. The total Ural iron reserves were 1,390,670,000 tonnes, of which one-third are limonite\nand about 450,000,000 correspond directly at Magnitogorsk. When the deposits\nin Kuznets began to be exploited, in 1930 recent discovered the Mountainous Shoria iron deposits, with reserves calculated as 292,412,000\ntonnes, 45% iron content, and the Karaganda deposits. The other \nimportant founts stay in Petrovsk-Zabaikal near Baikal Lake, and\nthe Little Khingan Mountains in the Soviet side of the frontier.\n\nOther iron resources in East Siberia are the Angara and Ilim river areas northwest of Baikal Lake, with production of 420,850,000 tonnes. No less than 30% of USSR iron production in the USSR was obtained in the Kuznets zone in 1937.\n\nIron deposits:\n\n\nThe existence of coal, estimated at 400,000,000,000 tonnes, was about a quarter of the Asian total, or half of the European reserves. The principal coal mining valleys and basins are:\n\nDuring the interwar years, major production was in the Kuznetsk Basin; the Carboniferous Basin of Irkutsk extends joining\nat Transiberian railway, in 480 km, and the Maritime province near the Vladivostok area.\n\nPetrol is encounter on north section of Sakhalin island, and their exploitation are accord topic between Japanese and Russians. Other sources are in the Kamchatka or Ohkostk coasts, but the rest of Siberia did not promise much, with the exception of petrol pits in Central Asia or the Urals. These last (referring to the Turkestan zone) are one extension of Caucasian petrol zone and the mentioned Ural petrol sources.\n\nGold is often found in Siberia; currently the principal mining districts are in the Olekma-Vitim region of the Lena Valley. During the period 1910-1914, the Siberian gold mines extracted an average of 46,655 kg and employed 57,000 workers. Exploited deposits are of placer gold. The joint British Russian Company, Lena Gold Mining Joint Stock Company (Lenzoloto), owned and operated the \"Lena Goldfields\" -- much of the region's gold production. Their mistreatment of workers ultimately led to the Lena massacre which ignited the Russian Revolution; the British shareholders secured compensation from the Soviet government in 1935. Russia remains the second biggest gold producer in the world, while using modern workplace standards.\n\nEspecially important in the Kirguises Steppes, in the Altai ranges and the Yenisei river basin.\n\nZinc is a natural resource.\n\nStay more distributed and are exploited. The most important are Telbes Mine (Kuznetsk coal basin), Minusinsk, Yenisei\nvalley, Olga territory (Maritime Province) and the Irkutsk area.\n"}
{"id": "3871234", "url": "https://en.wikipedia.org/wiki?curid=3871234", "title": "Static pressure", "text": "Static pressure\n\nIn fluid mechanics the term static pressure has several uses:\n\nAn aircraft’s altimeter is operated by the static pressure system. An aircraft’s airspeed indicator is operated by the static pressure system and the pitot pressure system.\n\nThe static pressure system is open to the exterior of the aircraft to sense the pressure of the atmosphere at the altitude at which the aircraft is flying. This small opening is called the static port. In flight the air pressure is slightly different at different positions around the exterior of the aircraft. The aircraft designer must select the position of the static port carefully. There is no position on the exterior of an aircraft at which the air pressure, for all angles of attack, is identical to the atmospheric pressure at the altitude at which the aircraft is flying. The difference in pressure causes a small error in the altitude indicated on the altimeter, and the airspeed indicated on the airspeed indicator. This error in indicated altitude and airspeed is called position error.\n\nWhen selecting the position for the static port, the aircraft designer’s objective is to ensure the pressure in the aircraft’s static pressure system is as close as possible to the atmospheric pressure at the altitude at which the aircraft is flying, across the operating range of weight and airspeed. Many authors describe the atmospheric pressure at the altitude at which the aircraft is flying as the \"freestream static pressure\". At least one author takes a different approach in order to avoid a need for the expression \"freestream static pressure\". Gracey has written \"The static pressure is the atmospheric pressure at the flight level of the aircraft\". Gracey then refers to the air pressure at any point close to the aircraft as the \"local static pressure\".\n\nThe concept of pressure is central to the study of fluids. A pressure can be identified for every point in a body of fluid, regardless of whether the fluid is in motion. Pressure can be measured using an aneroid, Bourdon tube, mercury column, or various other methods.\n\nThe concepts of \"total pressure\" and \"dynamic pressure\" arise from Bernoulli's equation and are significant in the study of all fluid flows. (These two pressures are not pressures in the usual sense - they cannot be measured using an aneroid, Bourdon tube or mercury column.) To avoid potential ambiguity when referring to pressure in fluid dynamics, many authors use the term \"static pressure\" to distinguish it from \"total pressure\" and \"dynamic pressure\"; the term \"static pressure\" is identical to the term \"pressure\", and can be identified for every point in a fluid flow field.\n\nIn \"Aerodynamics\", L.J. Clancy writes: \"To distinguish it from the total and dynamic pressures, the actual pressure of the fluid, which is associated not with its motion but with its state, is often referred to as the static pressure, but where the term pressure alone is used it refers to this static pressure.\"\n\nBernoulli's equation is fundamental to the dynamics of incompressible fluids. In many fluid flow situations of interest, changes in elevation are insignificant and can be ignored. With this simplification, Bernoulli’s equation for incompressible flows can be expressed as\n\nwhere:\n\nEvery point in a steadily flowing fluid, regardless of the fluid speed at that point, has its own static pressure formula_8, dynamic pressure formula_9, and total pressure formula_10. Static pressure and dynamic pressure are likely to vary significantly throughout the fluid but total pressure is constant along each streamline. In irrotational flow, total pressure is the same on all streamlines and is therefore constant throughout the flow.\n\nThe simplified form of Bernoulli's equation can be summarised in the following memorable word equation:\n\nThis simplified form of Bernoulli’s equation is fundamental to an understanding of the design and operation of ships, low speed aircraft, and airspeed indicators for low speed aircraft – that is aircraft whose maximum speed will be less than about 30% of the speed of sound.\n\nAs a consequence of the widespread understanding of the term \"static pressure\" in relation to Bernoulli’s equation, many authors in the field of fluid dynamics also use \"static pressure\" rather than \"pressure\" in applications not directly related to Bernoulli’s equation.\n\nThe British Standards Institution, in its Standard \"Glossary of Aeronautical Terms\", gives the following definition:\n\nThe term \"(hydro)static pressure\" is sometimes used in fluid statics to refer to the pressure of a fluid at a nominated depth in the fluid. In fluid statics the fluid is stationary everywhere and the concepts of dynamic pressure and total pressure are not applicable. Consequently, there is little risk of ambiguity in using the term \"pressure\", but some authors choose to use \"static pressure\" in some situations.\n\n\nAircraft design and operation\n\nFluid dynamics\n"}
{"id": "8723207", "url": "https://en.wikipedia.org/wiki?curid=8723207", "title": "Stokesian dynamics", "text": "Stokesian dynamics\n\nStokesian dynamics\nis a solution technique for the Langevin equation, which is the relevant form of Newton's 2nd law for a Brownian particle. The method treats the suspended particles in a discrete sense while the continuum approximation remains valid for the surrounding fluid, i.e., the suspended particles are generally assumed to be significantly larger than the molecules of the solvent. The particles then interact through hydrodynamic forces transmitted via the continuum fluid, and when the particle Reynolds number is small, these forces are determined through the linear Stokes equations (hence the name of the method). In addition, the method can also resolve non-hydrodynamic forces, such as Brownian forces, arising from the fluctuating motion of the fluid, and interparticle or external forces. Stokesian Dynamics can thus be applied to a variety of problems, including sedimentation, diffusion and rheology, and it aims to provide the same level of understanding for multiphase particulate systems as molecular dynamics does for statistical properties of matter. For formula_1 rigid particles of radius formula_2 suspended in an incompressible Newtonian fluid of viscosity formula_3 and density formula_4, the motion of the fluid is governed by the Navier–Stokes equations, while the motion of the particles is described by the coupled equation of motion:\nIn the above equation formula_6 is the particle translational/rotational velocity \nvector of dimension 6N. formula_7 is the hydrodynamic force, i.e., force exerted by the fluid on the particle due to relative motion between them. formula_8 is the stochastic Brownian force due to thermal motion of fluid particles. formula_9 is the deterministic nonhydrodynamic force, which may be almost any form of interparticle or external force, e.g. electrostatic repulsion between like charged particles. Brownian dynamics is one of the popular techniques of solving the Langevin equation, but the hydrodynamic interaction in Brownian dynamics is highly simplified and normally includes only the isolated body resistance. On the other hand, Stokesian dynamics includes the many body hydrodynamic interactions. Hydrodynamic interaction is very important for non-equilibrium suspensions, like a sheared suspension, where it plays a vital role in its microstructure and hence its properties. Stokesian dynamics is used primarily for non-equilibrium suspensions where it has been shown to provide results which agree with experiments.\n\nWhen the motion on the particle scale is such that the particle Reynolds number is small, the hydrodynamic force exerted on the particles in a suspension undergoing a bulk linear shear flow is:\n\nHere, formula_11 is the velocity of the bulk shear flow evaluated at the particle \ncenter, formula_12 is the symmetric part of the velocity-gradient tensor; formula_13 and formula_14 are the configuration-dependent resistance matrices that give the hydrodynamic force/torque on the particles due to their motion relative to the fluid (formula_13) and due to the imposed shear flow (formula_14). Note that the subscripts on the matrices indicate the coupling between kinematic (formula_6) and dynamic (formula_18) quantities.\n\nOne of the key features of Stokesian dynamics is its handing of the hydrodynamic interactions, which is fairly accurate without being computationally inhibitive (like boundary integral methods) for a large number of particles. Classical Stokesian dynamics requires formula_19 operations where \"N\" is the number of particles in the system (usually a periodic box). Recent advances have reduced the computational cost to about formula_20\n\nThe stochastic or Brownian force formula_8 arises from the thermal fluctuations in the fluid and is characterized by:\n\nThe angle brackets denote an ensemble average, formula_24 is the Boltzmann constant, formula_25 is the absolute temperature and formula_26 is the delta function. The amplitude of the correlation between the Brownian forces at time formula_27 and at time formula_28 results from the fluctuation-dissipation theorem for the N-body system.\n\n"}
{"id": "739501", "url": "https://en.wikipedia.org/wiki?curid=739501", "title": "Talent (measurement)", "text": "Talent (measurement)\n\nThe talent (, from Ancient Greek: , \"talanton\" \"scale, balance, sum\") was one of several ancient units of mass, a commercial weight, as well as corresponding units of value equivalent to these masses of a precious metal. The talent of gold was known to Homer, who described how Achilles gave a half-talent of gold to Antilochus as a prize. A Greek, or Attic talent, was (approximately the mass of water required to fill an amphora), a Roman talent was , an Egyptian talent was , and a Babylonian talent was . Ancient Israel, and other Levantine countries, adopted the Babylonian talent, but later revised the mass. The heavy common talent, used in New Testament times, was .\n\nThe original Homeric talent was probably the gold equivalent of the value of an ox or a cow. Based on a statement from a later Greek source that \"the talent of Homer was equal in amount to the later Daric [... i.e.] two Attic drachmas\" and analysis of finds from a Mycenaean grave-shaft, a weight of about can be established for this original talent. The later Attic talent was of a different weight than the Homeric, but represented the same value in copper as the Homeric did in gold, with the price ratio of gold to copper in Bronze Age Greece being 1:3000.\n\nAn Attic talent was the equivalent of 60 minae or 6,000 drachmae. \n\nAn Attic talent was about . Friedrich Hultsch estimated a weight of 26.196 kg, and offers a weight of 25.992 kg.\n\nAn Attic talent of silver was the value of nine man-years of skilled work. In 415 BC, an Attic talent was a month's pay for a trireme crew, Hellenistic mercenaries were commonly paid one drachma per day of military service.\n\nThe Aeginetan talent weighed about 37 kg. The German historian Friedrich Hultsch calculated a range of 36.15 to 37.2kg based on such estimates as the weight of one full Aeginetan \"metretes\" of coins, and concluded that the Aeginetan talent represented the water weight of a Babylonian \"ephah\": 36.29 kg by his reckoning (the \"metretes\" and the \"ephah\" were units of volume). Percy Gardner estimated a weight of 37.32 kg, based on extant weights and coins.\n\nAn Aeginetan talent was worth 60 Aeginetan minae, or 6,000 Aeginetan drachmae.\n\nThe Babylonians, Sumerians, and Hebrews divided a talent into 60 minas, each of which was subdivided into 60 shekels. The Greek also used the ratio of 60 minas to one talent. A Greek mina was approximately 434 ± 3 grams. A Roman talent was 100 libra. A libra is exactly three quarters of a Greek mina, so a Roman talent is 1.33 Greek talents. An Egyptian talent was 80 librae.\n\nThe talent as a unit of value is mentioned in the New Testament in Jesus' parable of the talents (Matthew 25:14-30). This parable is the origin of the use of the word \"talent\" to mean \"gift or skill\" in English and other languages. Luke includes a different parable involving the mina. According to Epiphanius, the talent is called \"mina\" (\"maneh\") among the Hebrews, and was the equivalent in weight to one-hundred denarii. The talent is found in another parable of Jesus where a servant who is forgiven a debt of ten thousand talents refuses to forgive another servant who owes him only one hundred silver denarii. The talent is also used elsewhere in the Bible, as when describing the material invested in the Ark of the Covenant. Solomon received 666 gold talents a year.\n\n\n"}
{"id": "310597", "url": "https://en.wikipedia.org/wiki?curid=310597", "title": "Thermoelectric cooling", "text": "Thermoelectric cooling\n\nThermoelectric cooling uses the Peltier effect to create a heat flux between the junction of two different types of materials. A Peltier cooler, heater, or thermoelectric heat pump is a solid-state active heat pump which transfers heat from one side of the device to the other, with consumption of electrical energy, depending on the direction of the current. Such an instrument is also called a Peltier device, Peltier heat pump, solid state refrigerator, or thermoelectric cooler (TEC). It can be used either for heating or for cooling, although in practice the main application is cooling. It can also be used as a temperature controller that either heats or cools.\n\nThis technology is far less commonly applied to refrigeration than vapor-compression refrigeration is. The primary advantages of a Peltier cooler compared to a vapor-compression refrigerator are its lack of moving parts or circulating liquid, very long life, invulnerability to leaks, small size, and flexible shape. Its main disadvantages are high cost and poor power efficiency. Many researchers and companies are trying to develop Peltier coolers that are cheap and efficient. (See Thermoelectric materials.)\n\nA Peltier cooler can also be used as a thermoelectric generator. When operated as a cooler, a voltage is applied across the device, and as a result, a difference in temperature will build up between the two sides. When operated as a generator, one side of the device is heated to a temperature greater than the other side, and as a result, a difference in voltage will build up between the two sides (the Seebeck effect). However, a well-designed Peltier cooler will be a mediocre thermoelectric generator and vice versa, due to different design and packaging requirements.\n\nThermoelectric coolers operate by the Peltier effect (which also goes by the more general name thermoelectric effect). The device has two sides, and when a DC electric current flows through the device, it brings heat from one side to the other, so that one side gets cooler while the other gets hotter. The \"hot\" side is attached to a heat sink so that it remains at ambient temperature, while the cool side goes below room temperature. In some applications, multiple coolers can be cascaded together for lower temperature.\n\nTwo unique semiconductors, one n-type and one p-type, are used because they need to have different electron densities. The semiconductors are placed thermally in parallel to each other and electrically in series and then joined with a thermally conducting plate on each side. When a voltage is applied to the free ends of the two semiconductors there is a flow of DC current across the junction of the semiconductors causing a temperature difference. The side with the cooling plate absorbs heat which is then moved to the other side of the device where the heat sink is. Thermoelectric Coolers, also abbreviated to TECs are typically connected side by side and sandwiched between two ceramic plates. The cooling ability of the total unit is then proportional to the number of TECs in it.\n\nSome benefits of using a TEC are:\n\nSome disadvantages of using a TEC are:\n\nA single-stage TEC will typically produce a maximal temperature difference of 70 °C between its hot and cold sides. The more heat moved using a TEC, the less efficient it becomes, because the TEC needs to dissipate both the heat being moved and the heat it generates itself from its own power consumption. The amount of heat that can be absorbed is proportional to the current and time.\nwhere \"P\" is the Peltier coefficient, \"I\" is the current, and \"t\" is the time. The Peltier coefficient depends on temperature and the materials the TEC is made of.\n\nIn refrigeration applications, thermoelectric junctions have about 1/4th the efficiency compared to conventional means (they offer around 10–15% efficiency of the ideal Carnot cycle refrigerator, compared with 40–60% achieved by conventional compression-cycle systems (reverse Rankine systems using compression/expansion).) Due to this lower efficiency, thermoelectric cooling is generally only used in environments where the solid-state nature (no moving parts, low maintenance, compact size, and orientation insensitivity) outweighs pure efficiency.\n\nPeltier (thermoelectric) cooler performance is a function of ambient temperature, hot and cold side heat exchanger (heat sink) performance, thermal load, Peltier module (thermopile) geometry, and Peltier electrical parameters.\n\nRequirements for thermoelectric materials:\n\nCommon thermoelectric materials used as semiconductors include bismuth telluride, lead telluride, silicon germanium, and bismuth-antimony alloys. Of these bismuth telluride is the most commonly used. New high-performance materials for thermoelectric cooling are being actively researched.\n\nThermoelectric coolers are used for applications that require heat removal ranging from milliwatts to several thousand watts. They can be made for applications as small as a beverage cooler or as large as a submarine or railroad car. TECs have limited life time. Their health strength can be measured by the change of their AC resistance (ACR). When a TEC gets \"old\" or worn out, the ACR will increase.\n\nPeltier elements are commonly used in consumer products. For example, Peltier elements are used in camping, portable coolers, cooling electronic components and small instruments. The cooling effect of Peltier heat pumps can also be used to extract water from the air in dehumidifiers. A camping/car type electric cooler can typically reduce the temperature by up to 20 °C (36 °F) below the ambient temperature. Climate-controlled jackets are beginning to use Peltier elements. Thermoelectric coolers are used to augment heat sinks for microprocessors. They are also used for wine coolers.\n\nThermoelectric coolers are used in many fields of industrial manufacturing and require a thorough performance analysis as they face the test of running thousands of cycles before these industrial products are launched to the market. Some of the applications include laser equipment, thermoelectric air conditioners or coolers, industrial electronics and telecommunications, automotive, mini refrigerators or incubators, military cabinets, IT enclosures, and more. \n\nPeltier elements are used in scientific devices. They are a common component in thermal cyclers, used for the synthesis of DNA by polymerase chain reaction (PCR), a common molecular biological technique, which requires the rapid heating and cooling of the reaction mixture for denaturation primer annealing and enzymatic synthesis cycles.\n\nWith feedback circuitry, Peltier elements can be used to implement highly stable temperature controllers that keep desired temperature within ±0.01 °C. Such stability may be used in precise laser applications to avoid laser wavelength drifting as environment temperature changes.\n\nThe effect is used in satellites and spacecraft to reduce temperature differences caused by direct sunlight on one side of a craft by dissipating the heat over the cold shaded side, where it is dissipated as thermal radiation to space. Since 1961, some unmanned spacecraft (including the Curiosity Mars rover) utilize radioisotope thermoelectric generators (RTGs) that convert thermal energy into electrical energy using the Seebeck effect. The devices can last several decades, as they are fueled by the decay of high-energy radioactive materials.\n\nPhoton detectors such as CCDs in astronomical telescopes, spectrometers, or very high-end digital cameras are often cooled by Peltier elements. This reduces dark counts due to thermal noise. A dark count occurs when a pixel registers an electron caused by thermal fluctuation rather than a photon. On digital photos taken at low light these occur as speckles (or \"pixel noise\").\n\nThermoelectric coolers can be used to cool computer components to keep temperatures within design limits or to maintain stable functioning when overclocking. A Peltier cooler with a heat sink or waterblock can cool a chip to well below ambient temperature.\n\nIn fiber-optic applications, where the wavelength of a laser or a component is highly dependent on temperature, Peltier coolers are used along with a thermistor in a feedback loop to maintain a constant temperature and thereby stabilize the wavelength of the device.\n\nSome electronic equipment intended for military use in the field is thermoelectrically cooled.\n\nThe vast majority of TECs have an ID printed on the cooled side.\n\nThese universal IDs clearly indicate the size, number of stages, number of couples, and current rating in amps, as seen in the adjacent diagram.\n\n"}
{"id": "24437450", "url": "https://en.wikipedia.org/wiki?curid=24437450", "title": "Thiopyrylium", "text": "Thiopyrylium\n\nThiopyrylium is a cation with the chemical formula CHS. It is analogous to the pyrylium cation with the oxygen atom replaced by a sulfur atom.\n\nThiopyrylium salts are less reactive than the analogous pyrylium salts due to the lower electronegativity of the sulfur atom. Among the chalcogenic 6-membered unsaturated heterocycles, thiopyrylium is the most aromatic, due to sulfur having the same Pauling electronegativity as carbon and only a slightly higher covalent radius.\n\nThiopyrylium salts can be synthesized by hydrogen abstraction from thiopyran by a hydride ion acceptor, such as trityl perchlorate.\n\nThe thiopyrylium analogue of 2,4,6-trisubstituted pyrylium salts can be synthesized by treatment with sodium sulfide followed by precipitation with acid. This reaction causes the oxygen atom in the pyrylium cation to be substituted with sulfur.\n"}
{"id": "2588632", "url": "https://en.wikipedia.org/wiki?curid=2588632", "title": "Tipped tool", "text": "Tipped tool\n\nA tipped tool is any cutting tool in which the cutting edge consists of a separate piece of material that is brazed, welded, or clamped onto a body made of another material. In the types in which the cutter portion is an indexable part clamped by a screw, the cutters are called inserts (because they are inserted into the tool body). Tipped tools allow each part of the tool, the shank and the cutter(s), to be made of the material with the best properties for its job. Common materials for the cutters (brazed tips or clamped inserts) include cemented carbide, polycrystalline diamond, and cubic boron nitride. Tools that are commonly tipped include milling cutters (such as end mills, face mills, and fly cutters), tool bits, router bits, and saw blades (especially the metal-cutting ones). \n\nThe advantage of tipped tools is only a small insert of the cutting material is needed to provide the cutting ability. The small size makes manufacturing of the insert easier than making a solid tool of the same material. This also reduces cost because the tool holder can be made of a less-expensive and tougher material. In some situations a tipped tool is better than its solid counterpart because it combines the toughness of the tool holder with the hardness of the insert.\n\nIn other situations this is less than optimal, because the joint between the tool holder and the insert reduces rigidity. However, these tools may still be used because the overall cost savings is still greater.\n\nIn industry today, insert tools are perhaps slightly more common than solid tools, but solid tools are still used in many applications. Entire catalogs of solid–high-speed steel (HSS) and solid-carbide end mills, for example, play prominent parts in some areas of milling practice, including diesinking, moldmaking, and aerospace job or batch production. Most machine shops with lathes have many solid-HSS and solid-carbide tool bits as well as many insert-tipped tool bits, and most commercial operations that involve routers (such as cabinetry and furniture shops) use plenty of solid-HSS and solid-carbide router bits as well as some tipped bits. \n\nInserts are removable cutting tips, which means they are not brazed or welded to the tool body. They are usually indexable, meaning that they can be exchanged, and often also rotated or flipped, without disturbing the overall geometry of the tool (effective diameter, tool length offset, etc.). This saves time in manufacturing by allowing fresh cutting edges to be presented periodically without the need for tool grinding, setup changes, or entering of new values into a CNC program.\n\nA \"wiper insert\" is an insert used in a milling machine or a lathe. It is designed for finish cutting, to give a smooth surface on the surface being cut. It uses special geometry to give a good finish on the workpiece at a higher-than-normal feedrate. Wiper inserts generally have a larger area in contact with the workpiece, so they exert higher force on the workpiece. This makes them unsuitable for fragile workpieces.\n\n\"Inserts used for turning and milling\" are often numbered according to ISO standard 1832. This standard aims to make the naming, specifying and ordering of inserts a simple, consistent and traceable process. This standard takes into account both metric and imperial systems of units, although certain elements differ for each unit system. The code consists of up to 13 symbols with the first 12 of them being compulsory for inserts composed of cubic boron or poly-crystalline diamond and the first 7 being compulsory for all other types of composition.\n\n"}
{"id": "2843727", "url": "https://en.wikipedia.org/wiki?curid=2843727", "title": "Turby wind turbine", "text": "Turby wind turbine\n\nThe Turby is a brand of vertical-axis Darrieus wind turbine. The three vertical aerofoil blades have a helical twist of 60 degrees, similar to Gorlov's water turbines .\n\nThe turbine consists of three vertical symmetrical airfoil blades, each having a helical twist. The helical feature spreads the torque evenly over the entire revolution, thus preventing the destructive pulsations of the straight-bladed giromill (Darrieus turbine). The wind pushes each blade around on both the windward and leeward sides of the turbine. As with a Darrieus turbine, theoretically, there is no torque on a stationary turbine, due to symmetry of the turbine and of the blades. Starting is achieved by operating the generator as a motor. Torque is caused by a change in the apparent wind direction relative to the moving blades.\n\nAnother advantage of the helical twist is that the blades generate torque well from upward-slanting airflow. This is negligible in open country, but tall buildings and cliff faces generate a bow wave which directs airflow up and over them. Turbines mounted on high building rooftops or clifftops are exposed to significantly slanting flow, and the Turby can extract more useful energy from it than a propeller-type turbine can because horizontal axis (HAWT) types cannot change their pitch to face the wind directly.\n\nThe turbine measures 2.0m (6'7\") in diameter by 2.9m (9'6\") high (including generator), and weighs 136 kg (300 lb). It is specified to generate power in winds of between 4 m/s (9 mph, 7.8kts) and 14 m/s (31 mph, 27.2kts), and can survive winds of 55 m/s (123 mph, 107kts). The rated power at 14 m/s is 2.5 kW (3.35 hp). The AC output from the synchronous generator is rectified to DC, then inverted to AC at 230V 50 Hz.\n\nCore International developed the turbine in the Netherlands with research input from the Delft University of Technology.\n\n\n"}
{"id": "1806297", "url": "https://en.wikipedia.org/wiki?curid=1806297", "title": "Virgil C. Summer Nuclear Generating Station", "text": "Virgil C. Summer Nuclear Generating Station\n\nThe Virgil C. Summer Nuclear Generating Station occupies a site near Jenkinsville, South Carolina, in Fairfield County, South Carolina, approximately northwest of Columbia. The nuclear power station includes the decommissioned experimental Carolinas-Virginia Tube Reactor (CVTR) unit, just outside the site of the old town of Parr, SC. The CVTR was a 17 MWe, heavy water reactor. Its cooling water is supplied by the Monticello Reservoir (not to be confused with the Monticello Nuclear Generating Station in Minnesota), which is also used by a pumped storage (hydroelectric) unit. The plant utilizes a once-through cooling system.\n\nThis plant has one Westinghouse 3-loop Pressurized Water Reactor, which has received approval of a 20-year license extension, taking the license expiration of Unit 1 from 2022 to 2042. South Carolina Electric and Gas was also in the process of constructing two Westinghouse AP1000 plants, which had been scheduled to go into service in 2020, but construction on these was stopped in 2017.\n\nThe plant is named after Virgil Clifton Summer, the former Chairman and CEO of SCE&G.\n\nV. C. Summer Unit 1 is a Westinghouse 3-loop Pressurized Water Reactor. The reactor first began commercial operation on January 1, 1984. The plant cost $1.3 billion to construct – 24 percent less per kilowatt than the average of 13 nuclear plants constructed over the same time period.\n\nUnit 1 generates 2,900 MW (Thermal Megawatts) of heat, supplying a net output of 966 MW (Electric Megawatts) of electricity to the grid.\n\nIn 2001, the Summer unit operated at 79.9 percent of capacity, producing 6.76 billion kilowatt-hours of electricity. In 2007 it produced 8.48 billion kilowatt-hours, increasing its capacity factor to 100.2 percent.\n\nAbout two-thirds (66.7 percent) of the Summer plant is owned by its operator, the South Carolina Electric & Gas Company (SCE&G), a subsidiary of the SCANA corporation. The remaining 33.3 percent is owned by the South Carolina Public Service Authority (Santee Cooper).\n\nOn March 27, 2008, South Carolina Electric & Gas applied to the Nuclear Regulatory Commission (NRC) for a Combined Construction and Operating License (COL) to build two 1,100 MW AP1000 pressurized water reactors at the site.\nOn May 27, 2008, SCE&G and Santee Cooper announced an engineering, procurement and construction (EPC) contract had been reached with Westinghouse. Costs were estimated to be approximately $9.8 billion for both AP1000 units, plus transmission facility and financing costs. The operators are filing an application to increase customers bills by $1.2 billion (2.5%) during the construction period to partially finance capital costs.\n\nIn March 2012, the NRC approved the construction license of the two proposed reactors at the Summer plant. \nAs with the license approval for the Vogtle plant, NRC chairman Gregory Jaczko cast the lone dissenting vote, saying \"I continue to believe that we should require that all Fukushima-related safety enhancements are implemented before these new reactors begin operating\".\nThe reactors were expected to go on-line in 2017 and 2018 respectively.\n\nThe construction of Unit 2 began officially on March 9, 2013, with the pouring of concrete for the basemat. The placement of the first concrete was completed on March 11, 2013. Unit 2 is the first reactor to start construction in the US in 30 years.\nFirst concrete for Unit 3 was completed on November 4, 2013.\n\nIn October 2014, a delay of at least one year and extra costs of $1.2 billion were announced, largely due to fabrication delays. Unit 2 was expected to be substantially complete in late 2018 or early 2019, with unit 3 about a year later.\n\nOn July 23, 2015, V.C. Summer Unit 2 reached a landmark milestone with the successful placement of the CA-01 module, one of the largest, heaviest, and most complicated modules within the Nuclear Island, also referred to as a super module because it was so large that huge submodules had to be shipped from the manufacturer and final assembly was completed on site in the twelve-story Module Assembly Building. Installation of CA-01 was long delayed due to both regulatory and production hurdles related to the module. It is the first of the US AP1000 reactors under construction to achieve placement of this critical module, beating Vogtle Unit 3 to this milestone, and allowing other construction activities in the Nuclear Island to progress that could not proceed until the module was in place. CA-01 is a large structural module that forms the internal structures of some compartments within the Containment Vessel, including the Steam Generator compartments, Reactor Vessel cavity, and Refueling Canal. The CA-01 Module is the heaviest module on site, weighing 1,200 tons, or 2.4 million pounds. Because of how much it weighs, lifting and placing the CA-01 module into the Unit 2 Nuclear Island resulted in the heaviest lift for the V.C. Summer construction project to date.\n\nIn early 2017 Westinghouse Electric Company revised in-service dates to April 2020 and December 2020 for units 2 and 3.\n\nIn March 2017, Westinghouse Electric Company filed for Chapter 11 bankruptcy because of $9 billion of losses from its two U.S. nuclear construction projects. SCANA considered its options for the project, and ultimately decided to abandon the project in July 2017. SCANA had determined that completing just Unit 2 and abandoning Unit 3 could be feasible and was leaning toward that option internally, however the project died when minority partner Santee Cooper's board voted to cease all construction and SCANA could not find another partner to take their place.\n\nOn July 31, 2017, after an extensive review into the costs of constructing Units 2 and 3, South Carolina Electric and Gas decided to stop construction of the reactors and later filed a Petition for Approval of Abandonment with the Public Service Commission of South Carolina.\n\nThe Nuclear Regulatory Commission defines two emergency planning zones around nuclear power plants: a plume exposure pathway zone with a radius of , concerned primarily with exposure to, and inhalation of, airborne radioactive contamination, and an ingestion pathway zone of about , concerned primarily with ingestion of food and liquid contaminated by radioactivity.\n\nThe 2010 U.S. population within of Summer was 17,599, an increase of 26.2 percent in a decade, according to an analysis of U.S. Census data for msnbc.com. The 2010 U.S. population within was 1,187,554, an increase of 14.3 percent since 2000. Cities within 50 miles include Columbia (30 miles to city center).\n\nThe Nuclear Regulatory Commission's estimate of the risk each year of an earthquake intense enough to cause core damage to the reactor at Summer was 1 in 26,316, according to an NRC study published in August 2010.\n\nThe Virgil C. Summer Nuclear Generating Station consists of one operational reactor. Two additional units under construction were cancelled on July 31, 2017.\n"}
{"id": "26830333", "url": "https://en.wikipedia.org/wiki?curid=26830333", "title": "Water-energy nexus", "text": "Water-energy nexus\n\nThere is no formal definition for the water-energy nexus - the concept refers to the relationship between the water used for energy production, including both electricity and sources of fuel such as oil and natural gas, and the energy consumed to extract, purify, deliver, heat/cool, treat and dispose of water (and wastewater) sometimes referred to as the energy intensity (EI). The relationship is not truly a closed loop as the water used for energy production need not be the same water that is processed using that energy, but all forms of energy production require some input of water making the relationship inextricable.\n\nAmong the first studies to evaluate the water and energy relationship was a life-cycle analysis conducted by Peter Gleick in 1994 that highlighted the interdependence and initiated the joint study of water and energy. In 2014 the US Department of Energy (DOE) released their report on the water-energy nexus citing the need for joint water-energy policies and better understanding of the nexus and its susceptibility to climate change as a matter of national security. The hybrid Sankey diagram in the DOE's 2014 water-energy nexus report summarizes water and energy flows in the US by sector, demonstrating interdependence as well as singling out thermoelectric power as the single largest user of water, used mainly for cooling. \n\nAll types of energy generation consume water either to process the raw materials used in the facility, constructing and maintaining the plant, or to just generate the electricity itself. Renewable power sources such as photovoltaic solar and wind power, which require little water to produce energy, require water in processing the raw materials to build. Water can either be \"used\" or \"consumed,\" and can be categorised as fresh, ground, surface, blue, grey or green among others. Water is considered used if it does not reduce the supply of water to downstream users, i.e. water that is taken and returned to the same source (instream use), such as in thermoelectric plants that use water for cooling and are by far the largest users of water. While used water is returned to the system for downstream uses, it has usually been degraded in some way, mainly due to thermal or chemical pollution, and the natural flow has been altered which does not factor into an assessment if only the quantity of water is considered. Water is consumed when it is removed completely from the system, such as by evaporation or consumption by crops or humans. When assessing water use all these factors must be considered as well as spatiotemporal considerations making precise determination of water use very difficult.\n\nSpang et al. (2014) conducted a study looking at the water consumption for energy production (WCEP) internationally that both showed the variation in energy types produced across countries as well as the vast differences in efficiency of energy production per unit of water use (figure 1). Operations of water distribution systems and power distribution systems under emergency conditions of limited power and water availability is an important consideration for improving the overall resilience of the water - energy nexus. Khatavkar and Mays (2017) present a methodology for control of water distribution and power distribution systems under emergency conditions of drought and limited power availability to ascertain at least minimal supply of cooling water to the power plants. Khatavkar and Mays (2017 b) applied an optimization model for water - energy nexus system for a hypothetical regional level system which showed an improved resilience for several contingency scenarios.\n\nIn 2001 operating water systems in the US consumed approximately 3% of the total annual electricity (~75 TWh). The California's State Water Project (SWP) and Central Valley Project (CVP) are together the largest water system in the world with the highest water lift, over 2000 ft. across the Tehachapi mountains, delivering water from the wetter and relatively rural north of the state, to the agriculturally intensive central valley, and finally to the arid and heavily populated south. Consequently, the SWP and CVP are the single largest consumers of electricity in California consuming approximately 5 TWh of electricity each per year. In 2001, 19% of the state’s total electricity use (~48 TWh/year) was used in processing water including end uses, with the urban sector accounting for 65% of this. In addition to electricity, 30% of California’s natural gas consumption was due to water-related processes, mainly residential water heating, and 88 million gallons of diesel was consumed by groundwater pumps for agriculture. The residential sector alone accounted for 48% of the total combined electricity and natural gas consumed for water-related processes in the state.\n\nAccording to the California Public Utilities Commission (CPUC) Energy Division’s Embedded Energy in Water Studies report:\"“'Energy Intensity' refers to the average amount of energy needed to transport or treat water or wastewater on a per unit basis\"\"Energy Intensity is sometimes used synonymously with embedded or embodied energy. In 2005, water deliveries to Southern California were assessed to have an average EI of 12.7 MWh/MG, nearly two-thirds of which was due to transportation. Following the findings that a fifth of California’s electricity is consumed in water-related processes including end-use, the CPUC responded by authorising a statewide study into the relationship between energy and water that was conducted by the California Institute for Energy and Environment (CIEE), and developed programs to save energy through water conservation.\n\nHydroelectricity is a special case of water used for energy production mainly because hydroelectric power generation is regarded as being clean and renewable, and dams (the main source of hydroelectric production) serve multiple purposes besides energy generation, including flood prevention, storage, control and recreation which make justifiable allocation analyses difficult. Furthermore, the impacts of hydroelectric power generation can be hard to quantify both in terms of evaporative consumptive losses and altered quality of water, since damming results in flows that are much colder than for flowing streams. In some cases the moderation of flows can be seen as a rivalry of water use in time may also need to accounted for in impact analysis.\n\n\n"}
{"id": "19584690", "url": "https://en.wikipedia.org/wiki?curid=19584690", "title": "Wood ash", "text": "Wood ash\n\nWood ash is the residue powder left after the combustion of wood, such as burning wood in a home fireplace or an industrial power plant. It is used traditionally by gardeners as a good source of potash.\n\nMany studies have been conducted regarding the chemical composition of wood ash, with widely varying results. Some quote calcium carbonate (CaCO) as the major constituent, others find no carbonate at all, but calcium oxide (CaO) instead. Some show as much as twelve percent iron oxide while others show none, though iron oxide is often introduced through contamination with soil. A comprehensive set of analyses of wood ash composition from many tree species has been carried out by Emil Wolff, among others.\n\nSeveral factors have a major impact on the composition:\n\nTypically between 0.43 and 1.82 percent of the mass of burned wood (dry basis) results in ash. Also the conditions of the combustion affect the composition and amount of the residue ash, thus higher temperature will reduce ash yield.\n\nMuch wood ash contains calcium carbonate as its major component, representing 25 or even 45 percent. Less than 10 percent is potash, and less than 1 percent phosphate; there are trace elements of iron, manganese, zinc, copper and some heavy metals. However, these numbers vary, as combustion temperature is an important variable in determining wood ash composition. All of these are, primarily, in the form of oxides.\n\nWood ash can be used as an organic fertilizer used to enrich agricultural soil nutrition. In this role, wood ash serves a source of potassium and calcium carbonate, the latter acting as a liming agent to neutralize acidic soils.\n\nWood ash can also be used as an amendment for organic hydroponic solutions, generally replacing inorganic compounds containing calcium, potassium, magnesium and phosphorus.\n\nWood ash is commonly disposed of in landfills, but with rising disposal costs, ecologically friendly alternatives, such as serving as compost for agricultural and forestry applications, are becoming more popular. Because wood ash has a high char content, it can be used as an odor control agent, especially in composting operations.\n\nWood ash has a very long history of being used in ceramic glazes, particularly in the Chinese, Japanese and Korean traditions, though now used by many craft potters. It acts as a flux, reducing the melting point of the glaze.\n\nPotassium hydroxide can be made directly from wood ash and in this form, is known as caustic potash or lye. Because of this property, wood ash has also traditionally been used to make wood-ash soap.\n\nThe ectomycorrhizal fungi \"Suillus granulatus\" and \"Paxillus involutus\" can release elements from wood ash.\n\n"}
{"id": "2116237", "url": "https://en.wikipedia.org/wiki?curid=2116237", "title": "Zinc selenide", "text": "Zinc selenide\n\nZinc selenide (ZnSe) is a light-yellow, solid compound comprising zinc (Zn) and selenium (Se). It is an intrinsic semiconductor with a band gap of about 2.70 eV at . ZnSe rarely occurs in nature, and is found in the mineral that was named after Hans Stille called \"stilleite.\"\n\nZnSe can be made in both hexagonal (wurtzite) and cubic (zincblende) crystal structure.\n\nIt is a wide-bandgap semiconductor of the II-VI semiconductor group (since zinc and selenium belong to the 12th and 16th groups of the periodic table, respectively). The material can be doped n-type doping with, for instance, halogen elements. P-type doping is more difficult, but can be achieved by introducing gallium.\n\n\nZnSe is insoluble in water, but reacts with acids to form toxic hydrogen selenide gas. \n\nIt can be deposited as a thin film by chemical vapour deposition techniques including MOVPE and vacuum evaporation.\n\n"}
