{"id": "15285305", "url": "https://en.wikipedia.org/wiki?curid=15285305", "title": "Allotropes of iron", "text": "Allotropes of iron\n\nIron represents perhaps the best-known example for allotropy in a metal. At atmospheric pressure, three allotropic forms of iron exist: alpha iron (α), gamma iron (γ) (also known as austenite), and delta iron (δ). At very high pressure, a fourth form exists, called epsilon iron (ε) hexaferrum. Some controversial experimental evidence exists for another high-pressure form that is stable at very high pressures and temperatures.\n\nThe phases of iron at atmospheric pressure are important because of the differences in solubility of carbon, forming different types of steel. The high-pressure phases of iron are important as models for the solid parts of planetary cores. The inner core of the Earth is generally assumed to consist essentially of a crystalline iron-nickel alloy with ε structure. The outer core surrounding the solid inner core is believed to be composed of liquid iron mixed with nickel and trace amounts of lighter elements.\n\nAs molten iron cools down, it solidifies at 1,538 °C (2,800 °F) into its δ allotrope, which has a body-centered cubic (BCC) crystal structure. δ-iron can dissolve as much as 0.08% of carbon by mass at 1,475 °C.\n\nAs the iron cools further to 1,394 °C its crystal structure changes to a face centered cubic (FCC) crystalline structure. In this form it is called gamma iron (γ-Fe) or Austenite. γ-iron can dissolve considerably more carbon (as much as 2.04% by mass at 1,146 °C). This γ form of carbon saturation is exhibited in stainless steel.\n\nBeta iron (β-Fe) are obsolete terms for the paramagnetic allotrope of iron. The primary phase of low-carbon or mild steel and most cast irons at room temperature is ferromagnetic α-Fe. As iron or steel is heated above the critical temperature A or Curie temperature of 771 °C (1044K or 1420 °F), the random thermal agitation of the atoms exceeds the oriented magnetic moment of the unpaired electron spins. The A forms the low-temperature boundary of the beta iron field in the phase diagram in Figure 1. β-Fe is crystallographically identical to α-Fe, except for magnetic domains and the expanded body-centered cubic lattice parameter as a function of temperature, and is therefore of only minor importance in steel heat treating. For this reason, the beta \"phase\" is not usually considered a distinct phase but merely the high-temperature end of the alpha phase field.\n\nSimilarly, the A is of only minor importance compared to the A (eutectoid), A and A critical temperatures. The A, where austenite is in equilibrium with cementite + γ-Fe, is beyond the right edge in Fig. 1. The α + γ phase field is, technically, the β + γ field above the A. The beta designation maintains continuity of the Greek-letter progression of phases in iron and steel: α-Fe, β-Fe, austenite (γ-Fe), high-temperature δ-Fe, and high-pressure hexaferrum (ε-Fe).\n\nβ-Fe and the A critical temperature are important in induction heating of steel, such as for surface-hardening heat treatments. Steel is typically austenitized at 900–1000 °C before it is quenched and tempered. The high-frequency alternating magnetic field of induction heating heats the steel by two mechanisms below the Curie temperature: resistance or Joule (IR) heating and ferromagnetic hysteresis losses. Above the A, the hysteresis mechanism disappears and the required amount of energy per degree of temperature increase is substantially larger than below A. Load-matching circuits may be needed to vary the impedance in the induction power source to compensate for the change.\n\nBelow 912 °C (1,674 °F) iron again adopts the BCC structure characteristic of α-iron, also called ferrite. The substance assumes a paramagnetic property. Carbon dissolves poorly in α-iron: no more than 0.021% by mass at 723 °C.\n\nAs it cools to 770 °C (1,418 °F), the Curie point (T), the iron is a fairly soft metal and becomes ferromagnetic. As iron passes below the Curie temperature, no structural change occurs, but the magnetic properties as the magnetic domains become aligned. This form of iron is stable form at room temperature. α-Fe can be subjected to pressures up to ca. 15 GPa before transforming into a high-pressure form termed ε-iron, which crystallizes in a hexagonal close-packed (hcp) structure.\n\nα-Fe is a component of steel and cast iron, conferring Ferromagnetism. It has a hardness of approximately 80 Brinell. the maximum solubility is about 0.02 wt% at and 0.001% carbon at . When it dissolves in iron, carbon atoms occupy interstitial \"holes\". Being about twice the diameter of the tetrahedral hole, the carbon introduces a strong local strain field.\n\nMild steel (carbon steel with up to about 0.2 wt% C) consist mostly of α-Fe and increasing amounts of cementite (FeC, an iron carbide). The mixture adopts a laminar structure called pearlite. Since bainite and pearlite each contain α-Fe as a component, any iron-carbon alloy will contain some amount of α-Fe if it is allowed to reach equilibrium at room temperature. The amount of α-Fe depends on the cooling process.\n\nAt pressures above approximately 10 GPa and temperatures of a few hundred kelvin or less, α-iron changes into a hexagonal close-packed (hcp) structure, which is also known as ε-iron or hexaferrum; the higher-temperature γ-phase also changes into ε-iron, but does so at a higher pressure. Antiferromagnetism in alloys of epsilon-Fe with Mn, Os and Ru has been observed.\n\nAn alternate stable form, if it exists, may appear at pressures of at least 50 GPa and temperatures of at least 1,500 K; it has been thought to have an orthorhombic or a double hcp structure. as of December 2011, recent and ongoing experiments are being conducted on high-pressure and Superdense carbon allotropes.\n\n"}
{"id": "3342763", "url": "https://en.wikipedia.org/wiki?curid=3342763", "title": "Anamirta cocculus", "text": "Anamirta cocculus\n\nAnamirta cocculus () is a Southeast Asian and Indian climbing plant. Its fruit is the source of picrotoxin, a poisonous compound with stimulant properties.\n\nThe plant is large-stemmed (up to 10 cm in diameter); the bark is \"corky gray\" with white wood. The \"small, yellowish-white, sweet-scented\" flowers vary between 6 and 10 centimeters across; the fruit produced is a drupe, \"about 1 cm in diameter when dry\".\n\nThe stem and the roots contain quaternary alkaloids, such as berberine, palmatine, magnoflorine and columbamine. The seeds deliver picrotoxin, a sesquiterpene, while the seed shells contain the tertiary alkaloids menispermine and paramenispermine.\nIts crushed seeds are an effective pediculicide (anti-lice) and are also traditionally used to stun fish or as a pesticide. In pharmacology, it is known as Cocculus Indicus.\n\nAlthough poisonous, hard multum is a preparation made from \"Cocculus Indicus\", etc., once used (by 19th century brewers) to impart a more intoxicating quality (\"giddiness\") to beer than provided by the alcoholic content alone. Charles Dickens referred to those engaging in such practices as \"brewers and beer-sellers\nof low degree... who do not understand the wholesome policy of\nselling wholesome beverage.\" Although appearing in many homeopathic volumes and at least two brewers' guides, the use of such preparations was outlawed in England, during the mid-19th century, with fines of £500 for sale and £200 for use of the drug.\n\nThe wood of the plant is used for fuel and carving.\n\nThe English common names are Indian berry, fishberry, or Levant nut (both referring to the dried fruit, and to the plant by synecdoche) and coca de Levante in Spanish; it is variously known as ligtang, aria (Mindanao), bayati (Tagalog), and variations thereof throughout its natural distribution (the Philippines, East India, Malaysia, and New Guinea).\n\nThe name \"fishberry\" comes from the use of the dried fruit as a method of fishing, in which the fish is \"stupified and captured\"; this method, however, is considered \"unsportsmanlike\".\n"}
{"id": "28640653", "url": "https://en.wikipedia.org/wiki?curid=28640653", "title": "Arizona Native Plant Society", "text": "Arizona Native Plant Society\n\nThe mission of Arizona Native Plant Society (AZNPS) is to promote knowledge, appreciation, conservation and restoration of Arizona native plants and their habitats, as well as the use of native plants in urban landscapes and gardens. Among its initiatives are the Plant Atlas Project of Arizona (PAPAZ), which trains AZNPS volunteers in botanical fieldwork; publication of booklets and brochures promoting the use of native plants; compilation and web publication of plant lists for various natural areas of Arizona and northern Mexico; grants for publication assistance and research; and pioneering work in invasive species education and removal.\n\nAZNPS began in 1977, when more than a dozen nursery owners, landscapers, and professionals created a non-profit society dedicated to educating Arizonans about the state's native plants, as well as other xeric landscape plants, including many new horticultural imports. Early on, AZNPS published a series of eight landscaping booklets designed to educate the public about the use of native and xeric plants in desert landscapes. \n\nAbout the year 2000, AZNPS began to promote the total use of native plants in the landscape, as native plants are best adapted to local habitats and soils, use the least amount of water, and are easier to maintain and keep disease-free than are imported plants. They also provide an extension of native habitat into the urban area as a \"corridor\" for native pollinators and other wildlife. \n\nAZNPS currently has five chapters that hold monthly seasonal meetings, field trips, workshops and special presentations, staff tables at local events, and network with other nongovernmental organizations and government agencies in restoration and invasive species work. The organization has two publications: The Plant Press, published biannually in themed issues offering in-depth articles on a topic, and Happenings, a quarterly listing of local chapter and state news.\n\nThe Sonoran Desert Weedwackers is an organization of Tucson-area volunteers. Since 2000, it has removed invasive buffelgrass Pennisetum ciliare from Tucson Mountain Park, and worked with other groups to remove Pennisetum setaceum (fountain grass), another fire-prone African import, from the Sonoran Desert.\n\n\n"}
{"id": "15858019", "url": "https://en.wikipedia.org/wiki?curid=15858019", "title": "Auckland Gas Company", "text": "Auckland Gas Company\n\nThe Auckland Gas Company is a company providing gas for residential or commercial customers in the Auckland area, New Zealand. It is one of the oldest still existing brand names in New Zealand, having been established as Auckland Gas Company Ltd in 1862 or 1863. It is owned since 2004 by Nova Energy.\n\nThe Auckland Gas Company was formed in 1862 as the first joint stock company in New Zealand. It was also the first private services provider in Auckland. In the 1870s, the company bought and developed a large site in Freemans Bay to build a gasworks (roughly on and east of the site of the current New World supermarket), with further buildings (mainly workshops) and offices on Beaumont Street.\n\nIn the late 1960s the Kapuni gas fields were opened, providing natural gas which was cleaner and cheaper than the locally produced coal gas variant, and the company stopped producing gas and became a supplier only. This led to most of the buildings in the Freemans Bay are being demolished.\n"}
{"id": "18208725", "url": "https://en.wikipedia.org/wiki?curid=18208725", "title": "Biorisk", "text": "Biorisk\n\nBiorisk generally refers to the risk associated with biological materials and/or infectious agents. The term has been used frequently for various purposes since the early 1990s. The term is used by regulators, laboratory personnel and industry alike and is used by WHO. WHO/Europe also provides tools and training courses in biosafety and biosecurity.\n\nAn international Laboratory Biorisk Management Standard developed under the auspices of the European Committee for Standardization, defines biorisk as the combination of the probability of occurrence of harm and the severity of that harm where the source of harm is a biological agent or toxin. The source of harm may be an unintentional exposure, accidental release or loss, theft, misuse, diversion, unauthorized access or intentional unauthorized release.\n\nIn Norway, \"Biorisk\" is trademarked by the Norwegian accredited registrar DNV.\n\n"}
{"id": "11296898", "url": "https://en.wikipedia.org/wiki?curid=11296898", "title": "BlueMotion", "text": "BlueMotion\n\nBlueMotion is a tradename for certain car models from the Volkswagen Group with an emphasis on higher fuel efficiency.\n\nVolkswagen introduced the name in 2006 on the Mk4 Polo BlueMotion, and in 2007 a version based on the current Passat was released. More recently, the technology has been used in SEAT's models like the SEAT Ibiza or the SEAT León under the name 'EcoMotive, and in the Škoda Fabia and Superb, where the technology is called \"GreenLine\". BlueMotion versions of the Golf Mk5 and Touran were released in 2008. The name refers to Volkswagen Group's corporate colour, blue, with the word 'motion' added to denote mobility, and echoes DaimlerChrysler's BlueTec engines which are equipped with advanced NOx reducing technology for diesel-powered vehicle emissions control.\n\nThe BlueMotion Polo (based on the Mk4 Polo) and Ibiza Ecomotive used a special 1.4 L three cylinder Turbocharged Direct Injection (TDI) diesel engine which develops achieves while producing just 99 grammes of CO per kilometre (base model), or 102 g/km with the higher specification trim package (with air conditioning, etc.). The Ibiza was subsequently changed to the 1.2 L TDI engine.\n\nThe BlueMotion Golf Mk6 uses a new 1.6 L TDI engine, and Polo Mk5 uses a new 1.2 L TDI engine, which produces 87 to 90 g/km of .\n\nThe cars combine a fuel efficient engine with fuel efficient design and technological strategies that include a more aerodynamic body, a low ride height, auto stop-start, programmed battery charging system (so the alternator only runs when necessary), longer gearing and low rolling resistance tires. On 30 June 2011 a Volkswagen Passat 1.6 TDI BlueMotion set a world record for the greatest distance driven on a single tank of fuel, recognized by the \"Guinness Book of Records\", achieving . The average consumption amounted to . BlueMotion vehicles are not available in North America.\n\nBlueMotionTechnologies include all products, basic technologies and innovations that significantly improve fuel economy and reduce emissions. These basic technologies represent the foundations of BlueMotion. Currently, they cover the TDI (turbocharged diesel direct injection) and TSI (boosted petrol stratified direct injection) engines as well as the extremely efficient Direct Shift Gearbox (DSG). They are supplemented by technological innovations that include drive variants such as EcoFuel (natural gas\nengines), BiFuel (liquid natural gas engines), MultiFuel (ethanol engines), hybrid systems and electric drives, as well as NOx emissions control, regenerative braking and the Stop-Start system. These basic technologies and innovations are getting incorporated into a growing range of products. These products / vehicles are currently grouped at Volkswagen Commercial Vehicles into three concept brands: ‘BlueMotion’, ‘BlueMotion Technology’ and ‘BlueTDI’.\n\nThe BlueMotion brands represents the most fuel efficient model of its line and has the following:\n\n\nVolkswagen offers BlueMotion Technology packages over the whole range and it can be configured with TSI and TDI engines.\nThe package includes start-stop system, change gear indicator and regenerative braking technology. The 2015 Golf GTI (mark VII) is available in BlueMotion version.\n\nCurrently BlueMotion in Brazil focuses on reduced rolling resistance and taller gearbox ratios, among other changes.\n\nThe Polo Bluemotion, the first model with the BlueMotion label in Brazil, uses the 1.6 TotalFlex (gasoline/alcohol) engine which is found in the other models. In April 2012 VW introduced in Brazil the Fox Bluemotion with the same engine.\n\nVolkswagen Commercial Vehicles offer a Multivan, Transporter and Crafter BlueMotion and BlueMotion Technology packages across the whole range.\n\n\n"}
{"id": "3027037", "url": "https://en.wikipedia.org/wiki?curid=3027037", "title": "Bosonization", "text": "Bosonization\n\nIn theoretical condensed matter physics and particle physics, Bosonization is a mathematical procedure by which a system of interacting fermions in (1+1) dimensions can be transformed to a system of massless, non-interacting bosons.\n\nThe basic physical idea behind bosonization is that particle-hole excitations are bosonic in character. However, it was shown by Tomonaga in 1950 that this principle is only valid in one-dimensional systems. Bosonization is an effective field theory that focuses on low-energy excitations. This is done for Luttinger liquid theory.\n\nTwo complex fermions formula_1 are written as functions of a boson formula_2\nwhile the inverse map is given by\nAll equations are normal-ordered. The changed statistics arises from anomalous dimensions of the fields.\n\n"}
{"id": "13245649", "url": "https://en.wikipedia.org/wiki?curid=13245649", "title": "Bubble point", "text": "Bubble point\n\nIn thermodynamics, the bubble point is the temperature (at a given pressure) where the first bubble of vapor is formed when heating a liquid consisting of two or more components. Given that vapor will probably have a different composition than the liquid, the bubble point (along with the dew point) at different compositions are useful data when designing distillation systems.\n\nFor a single component the bubble point and the dew point are the same and are referred to as the boiling point.\n\nAt the bubble point, the following relationship holds:<br>\n<br>\nwhere<br>\nK is the \"distribution coefficient\" or \"K factor\", defined as the ratio of mole fraction in the vapor phase formula_3 to the mole fraction in the liquid phase formula_4 at equilibrium.\n<br>\nWhen Raoult's law and Dalton's law hold for the mixture, the K factor is defined as the ratio of the vapor pressure to the total pressure of the system:<br>\n\nGiven either of formula_6 or formula_7 and either the temperature or pressure of a two-component system, calculations can be performed to determine the unknown information.\n\n"}
{"id": "7635675", "url": "https://en.wikipedia.org/wiki?curid=7635675", "title": "Bubble raft", "text": "Bubble raft\n\nA bubble raft is an array of bubbles. It demonstrates materials' microstructural and atomic length-scale behavior by modelling the {111} plane of a close-packed crystal. A material's observable and measurable mechanical properties strongly depend on its atomic and microstructural configuration and characteristics. This fact is intentionally ignored in continuum mechanics, which assumes a material to have no underlying microstructure and be uniform and semi-infinite throughout. \n\nBubble rafts assemble bubbles on a water surface, often with the help of amphiphilic soaps. These assembled bubbles act like atoms, diffusing, slipping, ripening, straining, and otherwise deforming in a way that models the behavior of the {111} plane of a close-packed crystal. The ideal (lowest energy) state of the assembly would undoubtedly be a perfectly regular single crystal, but just as in metals, the bubbles often form defects, grain boundaries, and multiple crystals. \n\nThe concept of bubble raft modelling was first presented in 1947 by Nobel Laureate Sir William Lawrence Bragg and John Nye of Cambridge University's Cavendish Laboratory in Proceedings of the Royal Society A.[1] Legend claims that Bragg conceived of bubble raft models while pouring oil into his lawn mower. He noticed that bubbles on the surface of the oil assembled into rafts resembling the {111} plane of close-packed crystals[2]. Nye and Bragg later presented a method of generating and controlling bubbles on the surface of a glycerine-water-oleic acid-triethanolamine solution, in assemblies of 100,000 or more sub-millimeter sized bubbles. In their paper [1], they go on at length about the microstructural phenomena observed in bubble rafts and hypothesized in metals.\n\nIn deforming a crystal lattice, one changes the energy and the interatomic potential felt by the atoms of the lattice. This interatomic potential is popularly (and mostly qualitatively) modeled using the Lennard-Jones potential, which consists of a balance between attractive and repulsive forces between atoms.\n\nThe \"atoms\" in Bubble Rafts also exhibit such attractive and repulsive forces:\n\nformula_1 [2]\n\nThe portion of the equation to the left of the plus sign is the attractive force, and the portion to the right represents the repulsive force.\n\nformula_2 is the interbubble potential\n\nformula_3 is the average bubble radius\n\nformula_4 is the density of the solution from which the bubbles are formed\n\nformula_5 is the gravitational constant\n\nformula_6 is the ratio of the distance between bubbles to the bubble radius\n\nformula_7 is the radius of ring contact\n\nformula_8 is the ratio R/a of the bubble radius to the Laplace constant a, where\n\nformula_9\n\nformula_10 is the surface tension \n\nformula_11 is a constant dependent upon the boundary conditions of the calculation\n\nformula_12 is a zeroth-order modified Bessel function of the second kind[2]\n\nBubble rafts can display numerous phenomena seen in the crystal lattice. This includes such things as point defects (vacancies, substitutional impurities, interstitial atoms), edge dislocations and grains. A screw dislocation can't be modeled in a 2D bubble raft because it extends outside the plane. It is even possible to replicate some microstructure treats such as annealing. The annealing process is simulated by stirring the bubble raft. This anneals out the dislocations (recovery) and promotes recrystallization. \n\n"}
{"id": "57237499", "url": "https://en.wikipedia.org/wiki?curid=57237499", "title": "Center for Energy Research", "text": "Center for Energy Research\n\nThe Center for Energy Research (formerly known as Energy Center) is a research association for studying energy research, operated by University of California San Diego. The center was founded by Sol Penner in July 1st, 1974, even though Sol Penner was already operating the research unit from Fall, 1972, who also served as the director till 1990. The major research of the Center includes fusion energy, renewable energy, fuel cells, energy storage etc.\n\nThe center has collaborations with various institutions and laboratories such as Los Alamos National Laboratory, Lawrence Livermore National Laboratory, Oak Ridge National Laboratory, Princeton Plasma Physics Laboratory, Sandia National Laboratories, etc.\n\nThe following people have been served as the director of Center for Energy Research:\n"}
{"id": "20134955", "url": "https://en.wikipedia.org/wiki?curid=20134955", "title": "Chernomorenergo", "text": "Chernomorenergo\n\nChernomorenergo is a state-owned power company of Abkhazia. It is responsible for the distribution of electricity in Abkhazia and operates the Inguri hydroelectric power station. It is currently headed by Aslan Basaria. It is only Russian plant involved in ultra-high voltage bushing development, manufacturing, and testing.\n\nIn February 1995, the Ministry for Energy was transformed into the state company Abkhazenergo. The company's first Chairman was Khuta Jinjolia, who had previously been Minister for Energy. In June of the same year, it was renamed to Chernomorenergo and turned into a state-owned private company.\n\nIn December 1999, outgoing Prime Minister Sergei Bagapsh was appointed Chairman of Chernomorenergo. On 1 May 2000, its status reversed to that of state company. After the election of Bagapsh as President of Abkhazia, he was succeeded by Rezo Zantaria on 6 May 2005.\n\nFollowing the May 2014 revolution and the election of President Raul Khajimba, Chernomorenergo was transformed into a Republican Unitary Enterprise and Aslan Basaria was appointed its new chairman.\n\n"}
{"id": "40776948", "url": "https://en.wikipedia.org/wiki?curid=40776948", "title": "Derived scheme", "text": "Derived scheme\n\nIn algebraic geometry, a derived scheme is a pair formula_1 consisting of a topological space \"X\" and a sheaf formula_2 of commutative ring spectra on \"X\" such that (1) the pair formula_3 is a scheme and (2) formula_4 is a quasi-coherent formula_5-module. The notion gives a homotopy-theoretic generalization of a scheme.\n\nA derived stack is a stacky generalization of a derived scheme.\n\nOver a field of characteristic zero, the theory is equivalent to that of a differential graded scheme. By definition, a differential graded scheme is obtained by gluing affine differential graded schemes, with respect to étale topology. It was introduced by Maxim Kontsevich \"as the first approach to derived algebraic geometry.\" and was developed further by Mikhail Kapranov and Ionut Ciocan-Fontanine.\n\nJust as affine algebraic geometry is equivalent (in categorical sense) to the theory of commutative rings (commonly called commutative algebra), affine derived algebraic geometry over characteristic zero is equivalent to the theory of commutative differential graded rings. One of the main example of derived schemes comes from the derived intersection of subschemes of a scheme, giving the Koszul complex. For example, let formula_6, then we can get a derived scheme formula_7 where\nis the étale spectrum. Since we can construct a resolution\nthe derived ring formula_10 is the koszul complex formula_11. The truncation of this derived scheme to amplitude formula_12 provides a classical model motivating derived algebraic geometry. Notice that if we have a projective scheme\nwhere formula_14 we can construct the derived scheme formula_15 where\nwith amplitude formula_12\n\nLet formula_18 be a fixed differential graded algebra defined over a field of characteristic formula_19. Then a formula_20-differential graded algebra formula_21 is called semi-free if the following conditions hold:\nIt turns out that every formula_20 differential graded algebra admits a surjective quasi-isomorphism from a semi-free formula_18 differential graded algebra, called a semi-free resolution. These are unique up to homotopy equivalence in a suitable model category. The (relative) cotangent complex of an formula_18-differential graded algebra formula_33 can be constructed using a semi-free resolution formula_34: it is defined as\nMany examples can be constructed by taking the algebra formula_36 representing a variety over a field of characteristic 0, finding a presentation of formula_37 as a quotient of a polynomial algebra and taking the Koszul complex associated to this presentation. The Koszul complex acts as a semi-free resolution of the differential graded algebra formula_38 where formula_39 is the graded algebra with the non-trivial graded piece in degree 0.\n\nThe cotangent complex of a hypersurface formula_40 can easily be computed: since we have the dga formula_41 representing the derived enhancement of formula_42, we can compute the cotangent complex as\nwhere formula_44 and formula_45 is the usual universal derivation. If we take a complete intersection, then the koszul complex\nis quasi-isomorphic to the complex\nThis implies we can construct the cotangent complex of the derived ring formula_48 as the tensor product of the cotangent complex above for each formula_49.\n\nPlease note that the cotangent complex in the context of derived geometry differs from the cotangent complex of classical schemes. Namely, if there was a singularity in the hypersurface defined by formula_50 then the cotangent complex would have infinite amplitude. These observations provide motivation for the hidden smoothness philosophy of derived geometry since we are now working with a complex of finite length.\n\nGiven a polynomial function formula_51, then consider the (homotopy) pullback diagram\nwhere the bottom arrow is the inclusion of a point at the origin. Then, the derived scheme formula_53 has tangent complex at formula_54 is given by the morphism\nwhere the complex is of amplitude formula_12. Notice that the tangent space can be recovered using formula_57 and the formula_58 measures how far away formula_59 is from being a smooth point.\nGiven a stack formula_60 there is a nice description for the tangent complex:\nIf the morphism is not injective, the formula_58 measures again how singular the space is. In addition, the euler characteristic of this complex yields the correct (virtual) dimension of the quotient stack.\n\nIn particular, if we look at the moduli stack of principal formula_63-bundles, then the tangent complex is just formula_64.\n\nDerived schemes can be used for analyzing topological properties of affine varieties. For example, consider a smooth affine variety formula_65. If we take a regular function formula_66 and consider the section of formula_67\nThen, we can take the derived pullback diagram\nwhere formula_19 is the zero section, constructing a derived critical locus of the regular function formula_50.\n\nConsider the affine variety\nand the regular function given by formula_74. Then,\nwhere we treat the last two coordinates as formula_76. The derived critical locus is then the derived scheme\nNote that since the left term in the derived intersection is a complete intersection, we can compute a complex representing the derived ring as\nwhere formula_79 is the koszul complex.\nConsider a smooth function formula_66 where formula_81 is smooth. The derived enhancement of formula_82, the derived critical locus, is given by the differential graded scheme formula_83 where the underlying graded ring are the polyvector fields\nand the differential formula_85 is defined by contraction by formula_86.\n\nFor example, if formula_87 is given by formula_74 we have the complex\nrepresenting the derived enhancement of formula_82.\n\n"}
{"id": "8524", "url": "https://en.wikipedia.org/wiki?curid=8524", "title": "Deuterium", "text": "Deuterium\n\nDeuterium (or hydrogen-2, symbol ' or ', also known as heavy hydrogen) is one of two stable isotopes of hydrogen (the other being protium, or hydrogen-1). The nucleus of deuterium, called a deuteron, contains one proton and one neutron, whereas the far more common protium has no neutron in the nucleus. Deuterium has a natural abundance in Earth's oceans of about one atom in of hydrogen. Thus deuterium accounts for approximately 0.0156% (or, on a mass basis, 0.0312%) of all the naturally occurring hydrogen in the oceans, while protium accounts for more than 99.98%. The abundance of deuterium changes slightly from one kind of natural water to another (see Vienna Standard Mean Ocean Water).\n\nThe deuterium isotope's name is formed from the Greek \"deuteros\", meaning \"second\", to denote the two particles composing the nucleus. Deuterium was discovered and named in 1931 by Harold Urey. When the neutron was discovered in 1932, this made the nuclear structure of deuterium obvious, and Urey won the Nobel Prize in 1934. Soon after deuterium's discovery, Urey and others produced samples of \"heavy water\" in which the deuterium content had been highly concentrated.\n\nDeuterium is destroyed in the interiors of stars faster than it is produced. Other natural processes are thought to produce only an insignificant amount of deuterium. Nearly all deuterium found in nature was produced in the Big Bang 13.8 billion years ago, as the basic or primordial ratio of hydrogen-1 to deuterium (about 26 atoms of deuterium per million hydrogen atoms) has its origin from that time. This is the ratio found in the gas giant planets, such as Jupiter. However, other astronomical bodies are found to have different ratios of deuterium to hydrogen-1. This is thought to be a result of natural isotope separation processes that occur from solar heating of ices in comets. Like the water cycle in Earth's weather, such heating processes may enrich deuterium with respect to protium. The analysis of deuterium/protium ratios in comets found results very similar to the mean ratio in Earth's oceans (156 atoms of deuterium per million hydrogens). This reinforces theories that much of Earth's ocean water is of cometary origin. The deuterium/protium ratio of the comet 67P/Churyumov-Gerasimenko, as measured by the Rosetta space probe, is about three times that of earth water. This figure is the highest yet measured in a comet.\n\nDeuterium/protium ratios thus continue to be an active topic of research in both astronomy and climatology.\n\nDeuterium is frequently represented by the chemical symbol D. Since it is an isotope of hydrogen with mass number 2, it is also represented by . IUPAC allows both D and , although is preferred. A distinct chemical symbol is used for convenience because of the isotope's common use in various scientific processes. Also, its large mass difference with protium (H) (deuterium has a mass of , compared to the mean hydrogen atomic weight of , and protium's mass of ) confers non-negligible chemical dissimilarities with protium-containing compounds, whereas the isotope weight ratios within other chemical elements are largely insignificant in this regard.\n\nIn quantum mechanics the energy levels of electrons in atoms depend on the reduced mass of the system of electron and nucleus. For the hydrogen atom, the role of reduced mass is most simply seen in the Bohr model of the atom, where the reduced mass appears in a simple calculation of the Rydberg constant and Rydberg equation, but the reduced mass also appears in the Schrödinger equation, and the Dirac equation for calculating atomic energy levels.\n\nThe reduced mass of the system in these equations is close to the mass of a single electron, but differs from it by a small amount about equal to the ratio of mass of the electron to the atomic nucleus. For hydrogen, this amount is about 1837/1836, or 1.000545, and for deuterium it is even smaller: 3671/3670, or 1.0002725. The energies of spectroscopic lines for deuterium and light hydrogen (hydrogen-1) therefore differ by the ratios of these two numbers, which is 1.000272. The wavelengths of all deuterium spectroscopic lines are shorter than the corresponding lines of light hydrogen, by a factor of 1.000272. In astronomical observation, this corresponds to a blue Doppler shift of 0.000272 times the speed of light, or 81.6 km/s.\n\nThe differences are much more pronounced in vibrational spectroscopy such as infrared spectroscopy and Raman spectroscopy, and in rotational spectra such as microwave spectroscopy because the reduced mass of the deuterium is markedly higher than that of protium. In nuclear magnetic resonance spectroscopy, deuterium has a very different NMR frequency (e.g. 61 MHz when protium is at 400 MHz) and is much less sensitive. Deuterated solvents are usually used in protium NMR to prevent the solvent from overlapping with the signal, although deuterium NMR on its own right is also possible.\n\nDeuterium is thought to have played an important role in setting the number and ratios of the elements that were formed in the Big Bang. Combining thermodynamics and the changes brought about by cosmic expansion, one can calculate the fraction of protons and neutrons based on the temperature at the point that the universe cooled enough to allow formation of nuclei. This calculation indicates seven protons for every neutron at the beginning of nucleogenesis, a ratio that would remain stable even after nucleogenesis was over. This fraction was in favor of protons initially, primarily because the lower mass of the proton favored their production. As the universe expanded, it cooled. Free neutrons and protons are less stable than helium nuclei, and the protons and neutrons had a strong energetic reason to form helium-4. However, forming helium-4 requires the intermediate step of forming deuterium.\n\nThrough much of the few minutes after the big bang during which nucleosynthesis could have occurred, the temperature was high enough that the mean energy per particle was greater than the binding energy of weakly bound deuterium; therefore any deuterium that was formed was immediately destroyed. This situation is known as the deuterium bottleneck. The bottleneck delayed formation of any helium-4 until the universe became cool enough to form deuterium (at about a temperature equivalent to 100 keV). At this point, there was a sudden burst of element formation (first deuterium, which immediately fused to helium). However, very shortly thereafter, at twenty minutes after the Big Bang, the universe became too cool for any further nuclear fusion and nucleosynthesis to occur. At this point, the elemental abundances were nearly fixed, with the only change as some of the radioactive products of big bang nucleosynthesis (such as tritium) decay. The deuterium bottleneck in the formation of helium, together with the lack of stable ways for helium to combine with hydrogen or with itself (there are no stable nuclei with mass numbers of five or eight) meant that an insignificant amount of carbon, or any elements heavier than carbon, formed in the Big Bang. These elements thus required formation in stars. At the same time, the failure of much nucleogenesis during the Big Bang ensured that there would be plenty of hydrogen in the later universe available to form long-lived stars, such as our Sun.\n\nDeuterium occurs in trace amounts naturally as deuterium gas, written or D, but most natural occurrence in the universe is bonded with a typical atom, a gas called hydrogen deuteride (HD or ).\n\nThe existence of deuterium on Earth, elsewhere in the solar system (as confirmed by planetary probes), and in the spectra of stars, is also an important datum in cosmology. Gamma radiation from ordinary nuclear fusion dissociates deuterium into protons and neutrons, and there are no known natural processes other than the Big Bang nucleosynthesis, which might have produced deuterium at anything close to its observed natural abundance (deuterium is produced by the rare cluster decay, and occasional absorption of naturally occurring neutrons by light hydrogen, but these are trivial sources). There is thought to be little deuterium in the interior of the Sun and other stars, as at temperatures there nuclear fusion reactions that consume deuterium happen much faster than the proton-proton reaction that creates deuterium. However, deuterium persists in the outer solar atmosphere at roughly the same concentration as in Jupiter, and this has probably been unchanged since the origin of the Solar System. The natural abundance of deuterium seems to be a very similar fraction of hydrogen, wherever hydrogen is found, unless there are obvious processes at work that concentrate it.\n\nThe existence of deuterium at a low but constant primordial fraction in all hydrogen is another one of the arguments in favor of the Big Bang theory over the Steady State theory of the universe. The observed ratios of hydrogen to helium to deuterium in the universe are difficult to explain except with a Big Bang model. It is estimated that the abundances of deuterium have not evolved significantly since their production about . Measurements of Milky Way galactic deuterium from ultraviolet spectral analysis show a ratio of as much as 23 atoms of deuterium per million hydrogen atoms in undisturbed gas clouds, which is only 15% below the WMAP estimated primordial ratio of about 27 atoms per million from the Big Bang. This has been interpreted to mean that less deuterium has been destroyed in star formation in our galaxy than expected, or perhaps deuterium has been replenished by a large in-fall of primordial hydrogen from outside the galaxy. In space a few hundred light years from the Sun, deuterium abundance is only 15 atoms per million, but this value is presumably influenced by differential adsorption of deuterium onto carbon dust grains in interstellar space.\n\nThe abundance of deuterium in the atmosphere of Jupiter has been directly measured by the Galileo space probe as 26 atoms per million hydrogen atoms. ISO-SWS observations find 22 atoms per million hydrogen atoms in Jupiter. and this abundance is thought to represent close to the primordial solar system ratio. This is about 17% of the terrestrial deuterium-to-hydrogen ratio of 156 deuterium atoms per million hydrogen atoms.\n\nCometary bodies such as Comet Hale-Bopp and Halley's Comet have been measured to contain relatively more deuterium (about 200 atoms D per million hydrogens), ratios which are enriched with respect to the presumed protosolar nebula ratio, probably due to heating, and which are similar to the ratios found in Earth seawater. The recent measurement of deuterium amounts of 161 atoms D per million hydrogen in Comet 103P/Hartley (a former Kuiper belt object), a ratio almost exactly that in Earth's oceans, emphasizes the theory that Earth's surface water may be largely comet-derived. Most recently the deuterium/protium (D/H) ratio of 67P/Churyumov–Gerasimenko as measured by \"Rosetta\" is about three times that of Earth water, a figure that is high. This has caused renewed interest in suggestions that Earth's water may be partly of asteroidal origin.\n\nDeuterium has also observed to be concentrated over the mean solar abundance in other terrestrial planets, in particular Mars and Venus.\n\nDeuterium is produced for industrial, scientific and military purposes, by starting with ordinary water—a small fraction of which is naturally-occurring heavy water—and then separating out the heavy water by the Girdler sulfide process, distillation, or other methods.\n\nIn theory, deuterium for heavy water could be created in a nuclear reactor, but separation from ordinary water is the cheapest bulk production process.\n\nThe world's leading supplier of deuterium was Atomic Energy of Canada Limited, in Canada, until 1997, when the last heavy water plant was shut down. Canada uses heavy water as a neutron moderator for the operation of the CANDU reactor design.\n\nAnother major producer of heavy water is India. All but one of India's atomic energy plants are pressurised heavy water plants, which use natural (i.e., not enriched) uranium. India has eight (seven are in operation) heavy water plants, six (five) based on D-H exchange in ammonia gas and two plants extract deuterium from natural water in a process that uses hydrogen sulphide gas at high pressure.\n\nWhile India is self-sufficient in heavy water for its own use, India now also exports reactor-grade heavy water.\n\nThe physical properties of deuterium compounds can exhibit significant kinetic isotope effects and other physical and chemical property differences from the hydrogen analogs. DO, for example, is more viscous than HO. Chemically, there are differences in bond energy and length for compounds of heavy hydrogen isotopes compared to normal hydrogen, which are larger than the isotopic differences in any other element. Bonds involving deuterium and tritium are somewhat stronger than the corresponding bonds in hydrogen, and these differences are enough to cause significant changes in biological reactions. Pharmaceutical firms are interested in the fact that deuterium is harder to remove from carbon than hydrogen.\n\nDeuterium can replace the normal hydrogen in water molecules to form heavy water (DO), which is about 10.6% denser than normal water (so that ice made from it sinks in ordinary water). Heavy water is slightly toxic in eukaryotic animals, with 25% substitution of the body water causing cell division problems and sterility, and 50% substitution causing death by cytotoxic syndrome (bone marrow failure and gastrointestinal lining failure). Prokaryotic organisms, however, can survive and grow in pure heavy water, though they develop slowly. Despite this toxicity, consumption of heavy water under normal circumstances does not pose a health threat to humans. It is estimated that a person might drink of heavy water without serious consequences. Small doses of heavy water (a few grams in humans, containing an amount of deuterium comparable to that normally present in the body) are routinely used as harmless metabolic tracers in humans and animals.\n\nThe deuteron has spin +1 (\"triplet\") and is thus a boson. The NMR frequency of deuterium is significantly different from common light hydrogen. Infrared spectroscopy also easily differentiates many deuterated compounds, due to the large difference in IR absorption frequency seen in the vibration of a chemical bond containing deuterium, versus light hydrogen. The two stable isotopes of hydrogen can also be distinguished by using mass spectrometry.\n\nThe triplet deuteron nucleon is barely bound at E = , and none of the higher energy states are bound. The singlet deuteron is a virtual state, with a negative binding energy of . There is no such stable particle, but this virtual particle transiently exists during neutron-proton inelastic scattering, accounting for the unusually large neutron scattering cross-section of the proton.\n\nThe nucleus of deuterium is called a deuteron. It has a mass of (equal to 1875.612 928(12) MeV)\n\nThe charge radius of the deuteron is . Like the proton radius, measurements using muonic deuterium produce a significantly smaller result: . This is (214130 - 212562)/sqrt(250^2 + 78^2) round 1σ less than the accepted CODATA 2014 value, measured using electrons, and confirms the unresolved proton charge radius anomaly.\n\nDeuterium is one of only five stable nuclides with an odd number of protons and an odd number of neutrons. (, , , , ; also, the long-lived radioactive nuclides , , , occur naturally.) Most odd-odd nuclei are unstable with respect to beta decay, because the decay products are even-even, and are therefore more strongly bound, due to nuclear pairing effects. Deuterium, however, benefits from having its proton and neutron coupled to a spin-1 state, which gives a stronger nuclear attraction; the corresponding spin-1 state does not exist in the two-neutron or two-proton system, due to the Pauli exclusion principle which would require one or the other identical particle with the same spin to have some other different quantum number, such as orbital angular momentum. But orbital angular momentum of either particle gives a lower binding energy for the system, primarily due to increasing distance of the particles in the steep gradient of the nuclear force. In both cases, this causes the diproton and dineutron nucleus to be unstable.\n\nThe proton and neutron making up deuterium can be dissociated through neutral current interactions with neutrinos. The cross section for this interaction is comparatively large, and deuterium was successfully used as a neutrino target in the Sudbury Neutrino Observatory experiment.\n\nDue to the similarity in mass and nuclear properties between the proton and neutron, they are sometimes considered as two symmetric types of the same object, a nucleon. While only the proton has an electric charge, this is often negligible due to the weakness of the electromagnetic interaction relative to the strong nuclear interaction. The symmetry relating the proton and neutron is known as isospin and denoted \"I\" (or sometimes \"T\").\n\nIsospin is an SU(2) symmetry, like ordinary spin, so is completely analogous to it. The proton and neutron form an isospin doublet, with a \"down\" state (↓) being a neutron, and an \"up\" state (↑) being a proton.\n\nA pair of nucleons can either be in an antisymmetric state of isospin called singlet, or in a symmetric state called triplet. In terms of the \"down\" state and \"up\" state, the singlet is\n\nThis is a nucleus with one proton and one neutron, i.e. a deuterium nucleus. The triplet is\nand thus consists of three types of nuclei, which are supposed to be symmetric: a deuterium nucleus (actually a highly excited state of it), a nucleus with two protons, and a nucleus with two neutrons. These states are not stable.\n\nThe deuteron wavefunction must be antisymmetric if the isospin representation is used (since a proton and a neutron are not identical particles, the wavefunction\nneed not be antisymmetric in general). Apart from their isospin, the two nucleons also have spin and spatial distributions of their wavefunction. The latter is symmetric if the deuteron is symmetric under parity (i.e. have an \"even\" or \"positive\" parity), and antisymmetric if the deuteron is antisymmetric under parity (i.e. have an \"odd\" or \"negative\" parity). The parity is fully determined by the total orbital angular momentum of the two nucleons: if it is even then the parity is even (positive), and if it is odd then the parity is odd (negative).\n\nThe deuteron, being an isospin singlet, is antisymmetric under nucleons exchange due to isospin, and therefore must be symmetric under the double exchange of their spin and location. Therefore, it can be in either of the following two different states:\n\nIn the first case the deuteron is a spin triplet, so that its total spin \"s\" is 1. It also has an even parity and therefore even orbital angular momentum \"l\" ; The lower its orbital angular momentum, the lower its energy. Therefore, the lowest possible energy state has , .\n\nIn the second case the deuteron is a spin singlet, so that its total spin \"s\" is 0. It also has an odd parity and therefore odd orbital angular momentum \"l\". Therefore, the lowest possible energy state has , .\n\nSince gives a stronger nuclear attraction, the deuterium ground state is in the , state.\n\nThe same considerations lead to the possible states of an isospin triplet having , or , . Thus the state of lowest energy has , , higher than that of the isospin singlet.\n\nThe analysis just given is in fact only approximate, both because isospin is not an exact symmetry, and more importantly because the strong nuclear interaction between the two nucleons is related to angular momentum in spin-orbit interaction that mixes different \"s\" and \"l\" states. That is, \"s\" and \"l\" are not constant in time (they do not commute with the Hamiltonian), and over time a state such as , may become a state of , . Parity is still constant in time so these do not mix with odd \"l\" states (such as , ). Therefore, the quantum state of the deuterium is a superposition (a linear combination) of the , state and the , state, even though the first component is much bigger. Since the total angular momentum \"j\" is also a good quantum number (it is a constant in time), both components must have the same \"j\", and therefore . This is the total spin of the deuterium nucleus.\n\nTo summarize, the deuterium nucleus is antisymmetric in terms of isospin, and has spin 1 and even (+1) parity. The relative angular momentum of its nucleons \"l\" is not well defined, and the deuteron is a superposition of mostly with some .\n\nIn order to find theoretically the deuterium magnetic dipole moment µ, one uses the formula for a nuclear magnetic moment\nwith\ng and g are g-factors of the nucleons.\n\nSince the proton and neutron have different values for g and g, one must separate their contributions. Each gets half of the deuterium orbital angular momentum formula_5 and spin formula_6. One arrives at\n\nwhere subscripts p and n stand for the proton and neutron, and .\n\nBy using the same identities as here and using the value , we arrive at the following result, in nuclear magneton units\n\nFor the , state (), we obtain\n\nFor the , state (), we obtain\n\nThe measured value of the deuterium magnetic dipole moment, is , which is 97.5% of the value obtained by simply adding moments of the proton and neutron. This suggests that the state of the deuterium is indeed to a good approximation , state, which occurs with both nucleons spinning in the same direction, but their magnetic moments subtracting because of the neutron's negative moment.\n\nBut the slightly lower experimental number than that which results from simple addition of proton and (negative) neutron moments shows that deuterium is actually a linear combination of mostly , state with a slight admixture of , state.\n\nThe electric dipole is zero as usual.\n\nThe measured electric quadrupole of the deuterium is . While the order of magnitude is reasonable, since the deuterium radius is of order of 1 femtometer (see below) and its electric charge is e, the above model does not suffice for its computation. More specifically, the electric quadrupole does not get a contribution from the \"l\" =0 state (which is the dominant one) and does get a contribution from a term mixing the \"l\" =0 and the \"l\" =2 states, because the electric quadrupole operator does not commute with angular momentum.\n\nThe latter contribution is dominant in the absence of a pure contribution, but cannot be calculated without knowing the exact spatial form of the nucleons wavefunction inside the deuterium.\n\nHigher magnetic and electric multipole moments cannot be calculated by the above model, for similar reasons.\n\nDeuterium has a number of commercial and scientific uses. These include:\n\nDeuterium is used in heavy water moderated fission reactors, usually as liquid DO, to slow neutrons without the high neutron absorption of ordinary hydrogen. This is a common commercial use for larger amounts of deuterium.\n\nIn research reactors, liquid D is used in cold sources to moderate neutrons to very low energies and wavelengths appropriate for scattering experiments.\n\nExperimentally, deuterium is the most common nuclide used in nuclear fusion reactor designs, especially in combination with tritium, because of the large reaction rate (or nuclear cross section) and high energy yield of the D–T reaction. There is an even higher-yield D– fusion reaction, though the breakeven point of D– is higher than that of most other fusion reactions; together with the scarcity of , this makes it implausible as a practical power source until at least D–T and D–D fusion reactions have been performed on a commercial scale. However, commercial nuclear fusion is not yet an accomplished technology.\n\nDeuterium is most commonly used in hydrogen nuclear magnetic resonance spectroscopy (proton NMR) in the following way. NMR ordinarily requires compounds of interest to be analyzed as dissolved in solution. Because of deuterium's nuclear spin properties which differ from the light hydrogen usually present in organic molecules, NMR spectra of hydrogen/protium are highly differentiable from that of deuterium, and in practice deuterium is not \"seen\" by an NMR instrument tuned for light-hydrogen. Deuterated solvents (including heavy water, but also compounds like deuterated chloroform, CDCl) are therefore routinely used in NMR spectroscopy, in order to allow only the light-hydrogen spectra of the compound of interest to be measured, without solvent-signal interference.\n\nNuclear magnetic resonance spectroscopy can also be used to obtain information about the deuteron's environment in isotopically labelled samples (Deuterium NMR). For example, the flexibility in the tail, which is a long hydrocarbon chains, in deuterium-labelled lipid molecules can be quantified using solid state deuterium NMR.\n\nDeuterium NMR spectra are especially informative in the solid state because of its relatively small quadrupole moment in comparison with those of bigger quadrupolar nuclei such as chlorine-35, for example.\n\nIn chemistry, biochemistry and environmental sciences, deuterium is used as a non-radioactive, stable isotopic tracer, for example, in the doubly labeled water test. In chemical reactions and metabolic pathways, deuterium behaves somewhat similarly to ordinary hydrogen (with a few chemical differences, as noted). It can be distinguished from ordinary hydrogen most easily by its mass, using mass spectrometry or infrared spectrometry. Deuterium can be detected by femtosecond infrared spectroscopy, since the mass difference drastically affects the frequency of molecular vibrations; deuterium-carbon bond vibrations are found in spectral regions free of other signals.\n\nMeasurements of small variations in the natural abundances of deuterium, along with those of the stable heavy oxygen isotopes O and O, are of importance in hydrology, to trace the geographic origin of Earth's waters. The heavy isotopes of hydrogen and oxygen in rainwater (so-called meteoric water) are enriched as a function of the environmental temperature of the region in which the precipitation falls (and thus enrichment is related to mean latitude). The relative enrichment of the heavy isotopes in rainwater (as referenced to mean ocean water), when plotted against temperature falls predictably along a line called the global meteoric water line (GMWL). This plot allows samples of precipitation-originated water to be identified along with general information about the climate in which it originated. Evaporative and other processes in bodies of water, and also ground water processes, also differentially alter the ratios of heavy hydrogen and oxygen isotopes in fresh and salt waters, in characteristic and often regionally distinctive ways. The ratio of concentration of H to H is usually indicated with a delta as δH and the geographic patterns of these values are plotted in maps termed as isoscapes. Stable isotope are incorporated into plants and animals and an analysis of the ratios in a migrant bird or insect can help suggest a rough guide to their origins.\n\nNeutron scattering techniques particularly profit from availability of deuterated samples: The H and D cross sections are very distinct and different in sign, which allows contrast variation in such experiments. Further, a nuisance problem of ordinary hydrogen is its large incoherent neutron cross section, which is nil for D. The substitution of deuterium atoms for hydrogen atoms thus reduces scattering noise.\n\nHydrogen is an important and major component in all materials of organic chemistry and life science, but it barely interacts with X-rays. As hydrogen (and deuterium) interact strongly with neutrons, neutron scattering techniques, together with a modern deuteration facility, fills a niche in many studies of macromolecules in biology and many other areas.\n\nThis is discussed below. It is notable that although most stars, including the Sun, generate energy over most of their lives by fusing hydrogen into heavier elements, such fusion of light hydrogen (protium) has never been successful in the conditions attainable on Earth. Thus, all artificial fusion, including the hydrogen fusion that occurs in so-called hydrogen bombs, requires heavy hydrogen (either tritium or deuterium, or both) in order for the process to work.\n\nA deuterated drug is a small molecule medicinal product in which one or more of the hydrogen atoms contained in the drug molecule have been replaced by deuterium. Because of the kinetic isotope effect, deuterium-containing drugs may have significantly lower rates of metabolism, and hence a longer half-life. In 2017, deutetrabenazine became the first deuterated drug to receive FDA approval.\n\nDeuterium can be used to reinforce specific oxidation-vulnerable C-H bonds within essential or conditionally essential nutrients, such as certain amino acids, or polyunsaturated fatty acids (PUFA), making them more resistant to oxidative damage. Deuterated polyunsaturated fatty acids, such as linoleic acid, slow down the chain reaction of lipid peroxidation that damage living cells. Deuterated ethyl ester of linoleic acid (RT001), developed by Retrotope, is in a compassionate use trial in infantile neuroaxonal dystrophy and has successfully completed a Phase I/II trial in Friedreich’s ataxia.\n\nThe existence of nonradioactive isotopes of lighter elements had been suspected in studies of neon as early as 1913, and proven by mass spectrometry of light elements in 1920. The prevailing theory at the time was that isotopes of an element differ by the existence of additional \"protons\" in the nucleus accompanied by an equal number of \"nuclear electrons\". In this theory, the deuterium nucleus with mass two and charge one would contain two protons and one nuclear electron. However it was expected that the element hydrogen with a measured average atomic mass very close to , the known mass of the proton, always has a nucleus composed of a single proton (a known particle), and could not contain a second proton. Thus, hydrogen could have no heavy isotopes.\n\nIt was first detected spectroscopically in late 1931 by Harold Urey, a chemist at Columbia University. Urey's collaborator, Ferdinand Brickwedde, distilled five liters of cryogenically produced liquid hydrogen to of liquid, using the low-temperature physics laboratory that had recently been established at the National Bureau of Standards in Washington, D.C. (now the National Institute of Standards and Technology). The technique had previously been used to isolate heavy isotopes of neon. The cryogenic boiloff technique concentrated the fraction of the mass-2 isotope of hydrogen to a degree that made its spectroscopic identification unambiguous.\n\nUrey created the names protium, deuterium, and tritium in an article published in 1934. The name is based in part on advice from G. N. Lewis who had proposed the name \"deutium\". The name is derived from the, Greek deuteros (second), and the nucleus to be called \"deuteron\" or \"deuton\". Isotopes and new elements were traditionally given the name that their discoverer decided. Some British scientists, such as Ernest Rutherford, wanted the isotope to be called \"diplogen\", from the Greek diploos (double), and the nucleus to be called diplon.\n\nThe amount inferred for normal abundance of this heavy isotope of hydrogen was so small (only about 1 atom in 6400 hydrogen atoms in ocean water (156 deuteriums per million hydrogens)) that it had not noticeably affected previous measurements of (average) hydrogen atomic mass. This explained why it hadn't been experimentally suspected before. Urey was able to concentrate water to show partial enrichment of deuterium. Lewis had prepared the first samples of pure heavy water in 1933. The discovery of deuterium, coming before the discovery of the neutron in 1932, was an experimental shock to theory, but when the neutron was reported, making deuterium's existence more explainable, deuterium won Urey the Nobel Prize in Chemistry in 1934. Lewis was embittered by being passed over for this recognition given to his former student.\n\nShortly before the war, Hans von Halban and Lew Kowarski moved their research on neutron moderation from France to England, smuggling the entire global supply of heavy water (which had been made in Norway) across in twenty-six steel drums.\n\nDuring World War II, Nazi Germany was known to be conducting experiments using heavy water as moderator for a nuclear reactor design. Such experiments were a source of concern because they might allow them to produce plutonium for an atomic bomb. Ultimately it led to the Allied operation called the \"Norwegian heavy water sabotage\", the purpose of which was to destroy the Vemork deuterium production/enrichment facility in Norway. At the time this was considered important to the potential progress of the war.\n\nAfter World War II ended, the Allies discovered that Germany was not putting as much serious effort into the program as had been previously thought. They had been unable to sustain a chain reaction. The Germans had completed only a small, partly built experimental reactor (which had been hidden away). By the end of the war, the Germans did not even have a fifth of the amount of heavy water needed to run the reactor, partially due to the Norwegian heavy water sabotage operation. However, even had the Germans succeeded in getting a reactor operational (as the U.S. did with a graphite reactor in late 1942), they would still have been at least several years away from development of an atomic bomb with maximal effort. The engineering process, even with maximal effort and funding, required about two and a half years (from first critical reactor to bomb) in both the U.S. and U.S.S.R., for example.\n\nThe 62-ton Ivy Mike device built by the United States and exploded on 1 November 1952, was the first fully successful \"hydrogen bomb\" or thermonuclear bomb. In this context, it was the first bomb in which most of the energy released came from nuclear reaction stages that followed the primary nuclear fission stage of the atomic bomb. The Ivy Mike bomb was a factory-like building, rather than a deliverable weapon. At its center, a very large cylindrical, insulated vacuum flask or cryostat, held cryogenic liquid deuterium in a volume of about 1000 liters (160 kilograms in mass, if this volume had been completely filled). Then, a conventional atomic bomb (the \"primary\") at one end of the bomb was used to create the conditions of extreme temperature and pressure that were needed to set off the thermonuclear reaction.\n\nWithin a few years, so-called \"dry\" hydrogen bombs were developed that did not need cryogenic hydrogen. Released information suggests that all thermonuclear weapons built since then contain chemical compounds of deuterium and lithium in their secondary stages. The material that contains the deuterium is mostly lithium deuteride, with the lithium consisting of the isotope lithium-6. When the lithium-6 is bombarded with fast neutrons from the atomic bomb, tritium (hydrogen-3) is produced, and then the deuterium and the tritium quickly engage in thermonuclear fusion, releasing abundant energy, helium-4, and even more free neutrons.\n\nIn August 2018, scientists announced the transformation of gaseous deuterium into a liquid metallic form. This may help researchers better understand giant gas planets, such as Jupiter, Saturn and related exoplanets, since such planets are thought to contain a lot of liquid metallic hydrogen, which may be responsible for their observed powerful magnetic fields.\n\nFormula: D or \n\n\nData at approximately for D (triple point):\n\nAn antideuteron is the antimatter counterpart of the nucleus of deuterium, consisting of an antiproton and an antineutron. The antideuteron was first produced in 1965 at the Proton Synchrotron at CERN and the Alternating Gradient Synchrotron at Brookhaven National Laboratory. A complete atom, with a positron orbiting the nucleus, would be called \"antideuterium\", but as of 2005 antideuterium has not yet been created. The proposed symbol for antideuterium is , that is, D with an overbar.\n\n\n"}
{"id": "46941334", "url": "https://en.wikipedia.org/wiki?curid=46941334", "title": "Difluorophosphate", "text": "Difluorophosphate\n\nDifluorophosphate or difluorodioxophosphate or phosphorodifluoridate is an anion with formula . It has a single negative charge and resembles perchlorate () and monofluorosulfonate (SOF) in shape and compounds. These ions are isoelectronic, along with tetrafluoroaluminate, phosphate, orthosilicate, and sulfate. It forms a series of compounds. The ion is toxic to mammals as it causes blockage to iodine uptake in the thyroid. However it is degraded in the body over several hours.\n\nCompounds containing difluorophosphate may have it as a simple uninegative ion, it may function as a difluorophosphato ligand where it is covalently bound to one or two metal atoms, or go on to form a networked solid. It may be covalently bound to a non metal or an organic moiety to make an ester or an amide.\n\nThe ammonium salt of difluorophosphate is formed from treating phosphorus pentoxide with ammonium fluoride. This was how the ion was first made by its discoverer, Willy Lange, in 1929.\n\nAlkali chlorides can react with dry difluorophosphoric acid to form alkali metal salts.\n\nFluoridation of dichlorophosphates can produce difluorophosphates. Another method is fluorination of phosphates or polyphosphates.\n\nTrimethylsilyl difluorophosphate reacts with metal chlorides to give difluorophosphates.\n\nThe anhydride phosphoryl difluoride oxide (POF) reacts with oxides such as UO to yield difuorophosphates. Phosphoryl difluoride oxide also reacts with alkali fluorides to yield difluorophosphates.\n\nIn ammonium difluorophosphate the difluorophosphate ion has these interatomic dimensions: P–O length 1.457 Å, P–F length 1.541 Å, O–P–O angle 118.7°, F–P–O 109.4° and F–P–F angle 98.6°. Hydrogen bonding from ammonium to oxygen atoms causes a change to the difluorophosphate ion in the ammonium salt. In potassium difluorophosphate the ion has dimensions: P–O length 1.470 Å, P–F length 1.575 Å, O–P–O angle 122.4°, F–P–O 108.6° and F–P–F angle 97.1°.\n\nOn heating the salts that are not of alkali or alkaline earths, difluorophosphates decompose firstly by giving off POF forming a monofluorophosphate (POF) compound, and then this in turn decomposes to an orthophosphate compound.\n\nDifluorophosphate salts are normally soluble and stable in water. However, in acidic or alkaline conditions they can be hydrolyzed to monofluorophosphates and hydrofluoric acid. The caesium and potassium salts are the least soluble.\n\nIrradiating potassium difluorophosphate with gamma rays can make the free radicals POF, POF and .\n\nDifluorphosphoric acid (HPOF) is one of the fluorophosphoric acids. It is produced when phosphoryl fluoride reacts with water. POF + HO → HPOF + HF. This in turn is hydrolysed more to give monofluorophosphoric acid (HPOF), and a trace of hexafluorophosphoric acid (HPF). HPOF also is produced when HF reacts with phosphorus pentoxide. Yet another method involves making difluorphosphoric acid as a side product of calcium fluoride being heated with damp phosphorus pentoxide. A method to make pure difluorphosphoric acid involves heating phosphoryl fluoride with monofluorophosphoric acid and separating the product by distillation. POF + HPOF → 2HPOF.\n\nDifluorophosphoric acid can also be produced by fluorinating phosphorus oxychlorides. POCl and POCl react with hydrogen fluoride solution to yield HPOCl and then HPOF. Yet another way is to treat orthophosphate () with fluorosulfuric acid (HSOF).\n\nDifluorphosphoric acid melts at −96.5 °C and boils at 115.9 °C. Its density at 25 °C is 1.583.\n\nDifluorophosphoric acid anhydride also known as phosphoryl difluoride oxide or diphosphoryl tetrafluoride (FOPOPOF or POF) is an anhydride of difluorphosphoric acid. It crystallises in the orthorhombic system, with space group Pcca and Z = 4. POF can be made by refluxing difluorophosphoric acid with phosphorus pentoxide. POF boils at 71 °C.\n\nIn addition to the isoelectronic series, ions related by substituting fluorine or oxygen by other elements include monofluorophosphate, difluorothiophosphate, dichlorothiophosphate, dichlorophosphate, chlorofluorothiophosphate, chlorofluorophosphate, dibromophosphate, and bromofluorophosphate.\n\nDifluorophosphate can form adducts with PF and AsF. In these the oxygen atoms form a donor-acceptor link between the P and As (or P) atoms, linking the difluorides to the pentafluorides. Example salts include KPOF·2AsF, KPOF·AsF, KPOF·2PF and KPOF·PF.\n\nAmines can react with phosphoryl fluoride to make substances with a formula RR′N–POF. The amines shown to do this include ethylamine, isopropylamine, \"n\"-butylamine, \"tert\"-butylamine, dimethylamine, and diethylamine. The monoamines can further react to yield an alkyliminophosphoricfluoride (RN=POF).\n"}
{"id": "21979868", "url": "https://en.wikipedia.org/wiki?curid=21979868", "title": "Digital micromirror device", "text": "Digital micromirror device\n\nThe digital micromirror device, or DMD, is a micro-opto-electromechanical system (MOEMS) that is the core of the trademarked DLP projection technology from Texas Instruments (TI). The DMD was invented by solid state physicist and TI Fellow Emeritus Dr. Larry Hornbeck in 1987.\n\nThe DMD project began as the Deformable Mirror Device in 1977 using micromechanical analog light modulators. The first analog DMD product was the TI DMD2000 airline ticket printer that used a DMD instead of a laser scanner.\n\nA DMD chip has on its surface several hundred thousand microscopic mirrors arranged in a rectangular array which correspond to the pixels in the image to be displayed. The mirrors can be individually rotated ±10-12°, to an on or off state. In the on state, light from the projector bulb is reflected into the lens making the pixel appear bright on the screen. In the off state, the light is directed elsewhere (usually onto a heatsink), making the pixel appear dark.\n\nTo produce greyscales, the mirror is toggled on and off very quickly, and the ratio of on time to off time determines the shade produced (binary pulse-width modulation). Contemporary DMD chips can produce up to 1024 shades of gray (10 bits). See Digital Light Processing for discussion of how color images are produced in DMD-based systems.\nThe mirrors themselves are made out of aluminum and are around 16 micrometers across. Each one is mounted on a yoke which in turn is connected to two support posts by compliant torsion hinges. In this type of hinge, the axle is fixed at both ends and twists in the middle. Because of the small scale, hinge fatigue is not a problem and tests have shown that even 1 trillion (10) operations do not cause noticeable damage. Tests have also shown that the hinges cannot be damaged by normal shock and vibration, since it is absorbed by the DMD superstructure.\n\nTwo pairs of electrodes control the position of the mirror by electrostatic attraction. Each pair has one electrode on each side of the hinge, with one of the pairs positioned to act on the yoke and the other acting directly on the mirror. The majority of the time, equal bias charges are applied to both sides simultaneously. Instead of flipping to a central position as one might expect, this actually holds the mirror in its current position. This is because attraction force on the side the mirror is already tilted towards is greater, since that side is closer to the electrodes.\n\nTo move the mirrors, the required state is first loaded into an SRAM cell located beneath each pixel, which is also connected to the electrodes. Once all the SRAM cells have been loaded, the bias voltage is removed, allowing the charges from the SRAM cell to prevail, moving the mirror. When the bias is restored, the mirror is once again held in position, and the next required movement can be loaded into the memory cell.\n\nThe bias system is used because it reduces the voltage levels required to address the pixels such that they can be driven directly from the SRAM cell, and also because the bias voltage can be removed at the same time for the whole chip, so every mirror moves at the same instant. The advantages of the latter are more accurate timing and a more cinematic moving image.\n\n"}
{"id": "1846528", "url": "https://en.wikipedia.org/wiki?curid=1846528", "title": "Edaphic", "text": "Edaphic\n\nEdaphic is a nature related to soil. Edaphic qualities may characterize the soil itself, including drainage, texture, or chemical properties such as soil pH. Edaphic may also characterize organisms, such as plant communities, where it specifies their relationships with soil. Edaphic endemics are plants or animals endemic to areas of a specific soil type.\n\nEdaphic plant communities include:\n\n"}
{"id": "51010296", "url": "https://en.wikipedia.org/wiki?curid=51010296", "title": "Electronics and semiconductor manufacturing industry in India", "text": "Electronics and semiconductor manufacturing industry in India\n\nThe Indian electronics industry is seeing growth encouraged both by government policies and incentives and by international investment. Its key and most resource-intensive segment, the semiconductor industry has substantial potential for growth since domestic demand is growing briskly. Semiconductors are required by a large number of industries, including telecommunications, information technology, industrial machinery and automation, medical electronics, automobile, engineering, power and solar photovoltaic, defense and aerospace, consumer electronics, and appliances. According to a NOVONOUS report, a steady and significant spurt in the semiconductor industry will increase the domestic market size fivefold during 2013-2020. As of 2015, however, the skill gap in Indian industry threatened progress, with 65 to 70 per cent of the market relying on imports.\n\nIndia's electronics market one of the largest in the world in terms of consumption, is predicted to grow to approximately US$400 billion by 2020 from $69.6 billion in 2012, largely led by up-surge in demand, growing at a projected compound annual growth rate of close to 25% over the period.\n\nIn 213-14, 65℅ of demand for electronic products was met through imports. According to a Frost & Sullivan-IESA data analysis, five high priority product categories together account for 60% of the overall electronic consumption. In descending order, these are mobile phones (38.85%), flat panel display television (7.91%), notebooks (5.54%) and desktops (4.39℅).\nIndia’s appliance and consumer electronics market, which was worth US$9.7 billion in 2014, is predicted to grow at a compound annual rate of 13.4%, and reach $20.6 billion by 2020. Within consumer electronics segment, set-top boxes are seen as the fastest growing category, with Y-o-Y growth predicted to be 28.8℅ between 2014-2020, followed by the television category at 20%, refrigerators at 10%, washing machines at 8-9% and air-conditioners at around 6-7%.\n\nIn 2013, demand for IT electronics in India was estimated to be valued at around US$13 billion.\n\nThe market opportunity for aerospace and defence (A&D) electronics in India is predicted to be worth upwards of US$70 billion by 2029-2030, of which around US$55 billion may be generated from electronics which are part of the platforms to be procured and rest from system-of-system projects.\n\nThe total domestic productions of electronic goods during 2012-13, 2013–14 and 2014–15 were INR 1,64,172 crore, INR 1,80,454 crore and INR 1,90,366 crore, respectively.\nThe electronics hardware manufacturing industry in India is projected to produce electronic goods worth $104 billion by 2020 from US$32.462 billion in 2013–14. In FY13, India's share in global electronics hardware production was 1.6%. The communication and broadcasting equipment segment constituted 31℅, thereby having a dominant share in the total production of electronic goods in India in FY13, followed by consumer electronics at 23℅.\nOf the smartphones shipped in the country in the April–June quarter of 2015, 24.8% were either manufactured or assembled in India, up from 19.9% in the previous quarter.\nOf the 220 million mobile sets shipped in India in 2015-16,around 110 million mobile phones have been either made or assembled in India in the last year, compared to 60 million earlier. Mobile handset manufacturing in 2015-16 grew by 185℅ in value terms to ₹54,000 crore from ₹19,000 crore in the previous year. According to an\nASSOCHAM-EY study titled \" Turning the Make in India dream into a reality for electronics and hardware industry \", the Indian electronics and hardware industry was expected to grow at a CAGR of 13%–16% in 2013–18 to reach $112–130 billion by 2018 from a 2016 level of $75 billion.\nAccording to a report of the NITI Aayog published in May 2016, the electronics industry's contribution to GDP is only 1.7% in India, compared to 15.5% in Taiwan, 15.1% in South Korea and 12.7% in China.\n\nIn 2014, the level of localized input/value addition for televisions was around 25-30% as the panels, semiconductors and the glass needed for manufacturing LCD/LED TVs had to be imported. For air-conditioners, localization was about 30-40% as the compressor, refrigerant, motor and coil were imported. About 35-40% of components for set-top boxes were sourced domestically. The localized content for washing machines and refrigerators was at around 70%.\n\nIn mobile phone assembly in India, around only 2-8℅ localized value addition is reported to be created in 2016.\n\nIndia is a net importer of electronics goods, with the majority of India's imported electronics coming from China. In 2015, electronics overtook gold and is placed immediately after crude oil as the second most valued category of imports to the country.\n\nElectronics exports from India were estimated to be around $7.66 billion in FY13, a slight decline from $8.15  billion in FY12, although in INR terms, they grew from 44000 crore rupees to 46300 crore rupees in the same period, owing to depreciation of the rupee. The telecom segment dominated India's electronics exports in 2013–14, followed by electronic components, instruments, consumer electronics, and computing. Technological improvements and competitive cost-effectiveness are thought to be the key drivers behind the growing demand for Indian electronics products abroad. In rupee terms, Indian electronic hardware exports almost doubled from 109940  crore in 2009-10 to 196103  crore in 2013–14.\nIn FY14, India's electronics exports declined to $6 billion, forming 0.28% of the global electronics trade.\n\nThe total imports of electronic goods during 2012-13, 2013–14, and 2014–15, were estimated to be worth INR 1,79,000 crore (US$28 billion), INR 1,95,900 crore (US$31 billion) and INR 2,25,600 crore (US$37 billion) respectively. The importation of phones has increased sharply from $665.47 million in 2003-04 to $10.9 billion in 2013-14, according to the commerce ministry data. Import of phones from China has grown from a $64.61 million to $7 billion during the same period. In 2013-14, India's electronics trade deficit was valued at US$23.5 billion, of which China accounted for 67℅. \nFrom around $28 billion in FY11, the importation of electronics could reach $40 billion in FY16. , local manufacturing of electronics has risen, beginning a turnaround at a time when Indian exports have been relatively weak. In January 2016, electronic imports, which accounted for 27% of India's yearly trade deficit, shrank by 2.2% to $3.2 billion, while electronic exports rose 7.8% to $0.5 billion.\n\nTo promote overall growth and open job opportunities, projected to be more than 28 million by attracting investments worth $100 billion, the Indian central government has sought to reduce the country's electronics import bill from 65% in 2014–15 to 50% in 2016 and gradually to a net-zero electronics trade by 2020. India has pursued a two-pronged strategy of import substitution and export encouragement, through the Make in India campaign coupled with the Digital India campaign, along with the Startup India and the Skill India campaigns. The government has fostered an environment conducive to foreign direct investment (FDI) inflow in a number of ways, as outlined in the National Electronics Policy and the National Telecom Policy.\n\nThe National Institution for Transforming India (NITI Aayog), a policy think-tank under the Indian central government, has suggested in a draft report that a policy be adopted to provide a tax holiday for a period of ten years to firms investing US$1 billion or more that also create 20,000 jobs. The report, hinting at a policy tilt toward the Information Technology Agreement-2 (ITA -2), also suggests that India should re-strategize its defensive policies regarding Free Trade agreements (FTAs) and aggressively pursue export-oriented policies to utilize these FTAs as opportunities to obtain duty-free access to the electronics markets of its FTA partners.\n\nThe electronics sector in India attracted foreign direct investment or FDI (equity capital component only, and after excluding the amount remitted through Reserve Bank of India's NRI schemes) worth $1.636 billion between April 2000 and March 2016, which was 0.57% of the cumulative FDI equity inflow worth $288.51 billion the country received in the same period.\n\n, the government has received 156 proposals with investment commitments worth INR1.14 lakh crore or $16.8 billion in the previous 20 months, according to the India Electronics and Semiconductor Association (IESA), an organisation that promotes local manufacture of computer hardware and electronic goods in India.\n\nAs of June 2016, the Indian electronics sector expects investments worth of US$56 billion over the next four years to fulfill its goal of generating exports worth over US$80 billion by 2020.\n\nAs of August 2016, India has attracted investments from 37 mobile manufacturing companies in the last one year, creating 40,000 direct jobs and around 125,000 indirect jobs.\nAs of May 2016, out of 195 investment proposals, worth INR 1.21 lakh crore, the government has approved 74 applications amounting to Rs 17,300 crore, while 27 proposals have been declined.\n\n\nWith the newly heralded era of Internet of Things (IoT) dictating that the new generation of interconnected devices be capable of smart-computing, Indian semiconductor industry is set for a stable upsurge with bright prospects provided India's generic obstacles like redtape-ism, fund crunch and infrastructural deficits are adequately addressed.\n\nThe fast growing electronics system design manufacturing ( ESDM ) industry in India has vibrant design capabilities with the number of units exceeding 120. As stated by the Department of Electronics and Information Technology (DeitY), approximately 2,000 chips are being designed in India every year with more than 200,000 engineers currently employed to work on various aspects of IC design and verification.\nAccording to a NOVONOUS report, the consumption of semiconductors in India, mostly import-based, is estimated to rise from $10.02 billion in 2013 to $52.58 billion by 2020 at a dynamic CAGR of 26.72%. The report estimates that the consumption of mobile devices will grow at a CAGR of 33.4% between 2013 and 2020, driving the share of mobile devices in semiconductor revenue up from 35.4% in 2013 to 50.7% in 2020. Moreover, the telecom segment is also expected to rise at a CAGR of 26.8 percent during 2013-20. The information technology and office automation segment are estimated to grow at a CAGR of 18.2% in the same period. The consumer electronics segment also is expected to grow at a CAGR of 18.8% over the seven years. The automotive electronics segment is expected to grow at 30.5% CAGR from 2013 to 2020. The EDSM industry will also grow on the back of these high consumption-led industries. Currently, almost all the semiconductor demand is met by imports from countries like the USA, Japan, and Taiwan. In the semiconductor sector, India has a significant human-capital pool which is currently concentrated in design, in the absence of an end-to-end manufacturing base. But the nascent ESDM segment in India is premised on competent domestic research by Indian universities and institutes across the entire semiconductor manufacturing value chain; namely, chip design and testing, embedded systems, process-related, EDA, MEMS and sensors, etc., which have contributed to a voluminous number of research publications.\n\nAs of 2016, the government allows 100% FDI in the ESDM sector through an automatic route to attract investments including from Original Equipment Manufacturers (OEMs) and Integrated Device Manufacturers (IDMs), and those relocating to India from other countries, in addition to EMC, MIPS and other incentives and schemes provided to the electronics sector.\n\nThe Department of Electronics and information Technology (DeitY), in line with Skill India campaign has launched an INR 49 crore scheme for capacity building in ESDM. In October 2015, Infineon Technologies, a German semiconductor firm partnered with National Skill Development Corporation (NSDC) to enhance skill and manpower in semiconductor technology, aimed at boosting the ESDM ecosystem in India.\n\nThe India Electronics & Semiconductor Association (IESA) has announced a SPEED UP and SCALE-UP of its talent development initiative to be implemented through the Centre of Excellence with Electronics Sector Skills Council of India (ESSCI) and an MoU with the Visvesvaraya Technological University (VTU) and the RV-VLSI Design Center to build human capital in the ESDM field. ESSCI, which has developed over 140 Qualification Packs (QP) / National Occupation Standards (NOS) across 14 sub-sectors of which Embedded System Design and VLSI are key domains absorbing engineers, established their first-ever Centre of Excellence (CoE) at BMS college of Engineering for VLSI and embedded system design. IESA signed an MoU with Taiwan Electrical and Electronic Manufacturers’ Association (TEEMA) to encourage co-operation in technology and knowledge transfer and investment commitment to domestic ESDM sector that can benefit both Indian and Taiwanese companies. IESA also entered into a MoU with Singapore Semiconductor Industry Association (SSIA) in February 2015, with an objective to forge trade and technical cooperation tie-ups between the electronics and semiconductor industries of both the countries.\n\nThe Department of Electronics and Information Technology (DeitY) has established an Electronics Development Fund (EDF) managed by Canara Bank ( CANBANK Venture Capital Funds or CVCFL) to provide risk capital and to attract venture funds, angel funds and seed funds for incubating R&D and fostering the innovative environment in the sector. , the establishment of “Fund of Funds for Start-ups” (FFS) approved by the union cabinet as part of the EDF for contribution to various Alternative Investment Funds (AIF) or daughter funds, registered with Securities and Exchange Board of India (SEBI) which would extend funding support to Start-ups,in line with the Start-up India Action Plan unveiled by Government in January 2016, will be beneficial to the start-ups in the ESDM space, according to IESA.\n\nThe National Centre for Flexible Electronics (NCFlexE) at IIT Kanpur, the National Centre for Excellence in Technology for Internal Security (NCETIS) at IIT Bombay and the Centre for Excellence for Internet of Things at NASSCOM, Bengaluru have been set up to promote the development of national capability in ESDM.\n\nIn 2011, Hyderabad based semiconductor chip design services entity SoCtronics completed the first 28 nm design chip to be developed in India. Bangalore-based Indian company Navika Electronics has designed GNSS/GPS SoC(System on Chip) chipsets based on ARM core processors under its own brandname for portable applications like receiving/down conversion and amplification of GPS and Galileo signals.\n\nThe Centre for Nano Science and Engineering (CeNSE), IISc, Bengaluru, in collaboration with KAS Tech, a Bengaluru-based electronics manufacturing company, has developed 'Ocean', a highly integrated and portable chemical vapour depositor that can commercially produce various two dimensional materials including graphene, in an easy 'plug and grow' approach which can have various novel applications in the ESDM sector, for both academia and industry alike.\n\nIn what could be viewed as a breakthrough for the country’s electric autimobile programme as well as indigenous electronics manufacturing, the Indian Space Research Organization (ISRO) and the Automotive Research Association of India (ARAI) together have developed and validated through tests, using ISRO's state of the art cell technology, a lithium ion battery prototype for application in electric vehicles and looks forward to commercialising the technology through mass production by partnering with automotive companies. Currently India's lithium ion battery requirements are completely met by import as there is no domestic manufacturing of these batteries. While the raw material for the batteries still has to be imported, rest of the value chain can be synthesized domestically at competitive cost, if the project clears all the barriers.\nResearchers at the Indian Institute of Technology - Bombay (IIT-B), in a collaboration with ISRO’s Semi-Conductor Labs (SCL), Chandigarh, have developed an indigenous Bipolar Junction Transistor (BJT) which can function with Bi-CMOS (Bipolar Complementary Metal Oxide Semiconductor). Analog or mixed chips based on various digital Bi-CMOS technology with integrated analog high frequency BJT based amplifiers are essential for IoT and space applications like high frequency communications as they reduce form factor, power consumption, weight, size dimensions and cost etc.\n\nIn 2014, the ESDM industry was projected to see investment proposals worth Rs 10,000 crore (US$1.5 billion) over the next two years, along with five partially state-funded start-up incubation centres of the 250 planned by the industry-body, as per IESA.\n\n\nAs of mid 2016, there are no operational commercial Semiconductor fabrication plants in India.\n\nThe Centre of Excellence in Nanoelectronics (CEN), at the Indian Institute of Technology – Bombay, has a lab-like fab facility collaborated between IIT Bombay and IISc Bangalore that offers research in design, fabrication and characterization of traditional CMOS Nano-electronic devices, Novel Material based devices (III-V Compound Semiconductor devices, Spintronics, Opto-electronics), Micro Electro-mechanical Systems (MEMS), NEMS, Bio-MEMS, polymer based devices and solar Photovoltaics to researchers across academia, industry and government laboratories, all over India. The center also offers support in device fabrication technologies using sophisticated equipment under the Indian Nano Users Program (INUP) and acts as a linchpin for developing innovative technologies that can be tweaked and commercialized for spurring the nano-industrial growth in India.\n\nIn March 2016, HSMC received ₹700 crore worth of seed investment for the project from Mumbai-based private equity fund Next Orbit Ventures (NOV).\n\n\nIn February 2014, the union cabinet approved setting up of these fab proposals with decision to extend incentives as follows:\n\nCritics and detractors of the fab projects currently underway in India, in different conceptual phases, doubt the prospects of success of these capital-intensive projects, pointing to various reasons like marginal profitability due to overcapacity of output in a saturated and fiercely competed fab market, noncompetence of these particular fabs in terms of cost and performance related to the dimensions of CMOS nodes even in attracting domestic end-use industries which have access to the more sophisticated fabs outside the country, cost prohibitive maintenance and upgrades needed every few years to weather obsolescence, nonavailability of domestically procurable semiconductor-grade materials in absence of complementing ancillary manufacturing industries and other resource-intensive strings attached to such projects, including land acquisition requirements,necessity uninterrupted deionised water and power supplies, supply of critical gases such as nitrogen and argon, absence of skilled labour force and drain of an already inadequate number of experienced domestic talent pool in electronic engineering and R&D possessing expertise to overcome the barriers of related sensitive technologies for mass production towards other attractive sectors in absence of a major Indian player in the electronics sector, especially in a developing country like India, which is still grappling with infrastructural bottlenecks.\n\nHowever, the endorsers of the fab projects, such as AMD, which partnered with HSMC for the fab project in Gujarat, stress the strategic need of developing the fabs as part of an end-to-end electronics manufacturing base in India which imports billions of dollars worth of even lower-end semiconductor nodes of 90 nm and above each year.\n\nRelevant circles within India have been advocating for investment by the central government with a long term strategic vision in the revolutionising fields of Gallium Nitride (GaN) and Mercury Cadmium Telluride (HgCdTe) based non silica semiconductor foundry and fab because of their wide ranged use like High Electron Mobility Transistor (HEMT) made from GaN in power electronics both for civilian and military applications which can switch at high speed and can handle high power and high temperature without needing any cooling and HgCdTe based high quality sensors for military space requirements.\nA foundry for producing GaN nano material proposed to be extended around the existing facility for producing gallium nitride transistors, at the IISc’s Centre for Nano Science and Engineering (CeNSE), Bangalore, at a cost of Rs. 3000 crore has received preliminary approval from the central government.\n\n"}
{"id": "37830810", "url": "https://en.wikipedia.org/wiki?curid=37830810", "title": "Fadhil Chalabi", "text": "Fadhil Chalabi\n\nFadhil Jafar al-Chalabi (born 1929) is an Iraqi economist, and was Acting Secretary General of OPEC from 1983 to 1988. He is a second cousin of the politician Ahmed Chalabi.\n\nBorn in 1929 in Baghdad to Jafar Mohamad al-Chalabi and Fatima née al -Uzri, Chalabi studied law at Baghdad University and graduated in 1951 before gaining a PhD in oil economics from the University of Paris. In 1968 he was appointed director of oil affairs in the Iraqi Ministry of Oil, and in 1973 became Iraq's permanent undersecretary of oil. He was assistant secretary general to the Organization of Arab Petroleum Exporting Countries from 1976 to 1978. From 1978 to 1988 he was deputy secretary general of OPEC, serving as acting secretary general from 1983 to 1988.\n\nIn 1987 he became Executive Director of the Centre for Global Energy Studies, a London-based think-tank founded and chaired by Ahmed Zaki Yamani. He retired in 2011.\n\nChalabi is interviewed in the 2006 documentary \"\".\n\nHe was married in 1956 to Abda Bahjat Salih and has four children.\n\n"}
{"id": "668710", "url": "https://en.wikipedia.org/wiki?curid=668710", "title": "Fire point", "text": "Fire point\n\nThe fire point of a fuel is the lowest temperature at which the vapour of that fuel will continue to burn for at least 5 seconds after ignition by an open flame. At the flash point, a lower temperature, a substance will ignite briefly, but vapor might not be produced at a rate to sustain the fire. Most tables of material properties will only list material flash points. Although in general the fire points can be assumed to be about 10 °C higher than the flash points this is no substitute for testing if the fire point is safety critical. \n\nTesting of the fire point is done by open cup apparatus.\n\n"}
{"id": "11657094", "url": "https://en.wikipedia.org/wiki?curid=11657094", "title": "Future Energy", "text": "Future Energy\n\nFuture Energy is a former accreditation scheme for green electricity in the United Kingdom, designed to support and stimulate electricity generation from renewable energy sources. The scheme was launched in 1999 and was operated by the Energy Saving Trust until funding expired in 2002.\n\nIt is thought that funding was not renewed due to few suppliers were prepared to accept the new requirements for green tariffs proposed by the Trust following the introduction of the Renewables Obligation. As of 2007 the scheme has not been replaced, although Friends of the Earth, who used to run their own scheme, have been among those calling on the government to do so.\n\n"}
{"id": "669238", "url": "https://en.wikipedia.org/wiki?curid=669238", "title": "Gelonus", "text": "Gelonus\n\nGelonus was, according to Herodotus, the capital of the Gelonians.\n\nIn his account of Scythia (\"Inquiries\" book 4), Herodotus writes that the Gelonii were formerly Greeks, having settled away from the coastal emporia among the Budini, where they \"use a tongue partly Scythian and partly Greek\":\"The Budini for their part, being a large and numerous nation, are all mightily blue-eyed and ruddy. And a city among them has been built, a wooden city, and the name of the city is Gelonus. Of its wall then in size each side is of thirty stades and high and all wooden. And their homes are wooden and their shrines. For indeed there is in the very place Greek gods’ shrines adorned in the Greek way with statues, altars and wooden shrines and for triennial Dionysus festivals in honour of Dionysus...Above the Sauromatae (Sarmatians), possessing the second region, dwell the Budini, whose territory is thickly wooded with trees of every kind. The Budini are a large and powerful nation: they have all deep blue eyes, and bright red hair. The Budini, however, do not speak the same language as the Geloni, nor is their mode of life the same. They are the aboriginal people of the country, and are nomads; unlike any of the neighbouring races, they eat 'phtheir'. Their country is thickly planted with trees of all manner of kinds. In the very woodiest part is a broad deep lake, surrounded by marshy ground with reeds growing on it. Here otters are caught, and beavers, with another sort of animal which has a square face. With the skins of this last the natives border their capotes: and they also get from them a remedy, which is of virtue in diseases of the womb...Beyond the Budini, as one goes northward, first there is a desert, seven days' journey across...\"The fortified settlement of Gelonus was reached by the Persian army of Darius in his assault on Scythia during the late 6th century BC, already burned to the ground, the Budini having abandoned it before the Persian advance. The Scythians sent a message to Darius: \"We are free as wind and what you can catch in our land is only the wind\". By employing a scorched earth strategy, they avoided battles, leaving \"earth without grass\" by burning the steppe in front of the advancing Persians (Herodotus). The Persian army returned without a single battle or any significant success.\n\nAccording to some researchers, the Budinis were a Finnic tribe ruled by the Scythians.\n\nExcavations at \"Bilske Horodyshche\" () near the village of Bilsk near Poltava in Ukraine (Coordinates ) have led to suggestions by archaeologist Boris Shramko and others identifying it as the Scythian capital Gelonus. It is strategically situated on the exact boundary between the steppe and forest-steppe. Several other locations have traditionally been named by Russian archaeologists, such as Saratov (according to Ivan Zabelin) or a location near the Don River closer to the Volga River.\n\nAccording to Herodotus each side of Gelonus is 30 stades long, the area in today's units would be about 30 square kilometres. The archeological site around Belsk, including necropolis, comprises about 80 km², and the fortifications enclose some 40 km². The north-south axis, along the Vorskla River is 17 km long. The remains of walls up to 12 metres are visible today and stretch over the horizon. The total length of the ramparts is 33 km. Inside the fortification, lay three \"keeps\", 150,000 m², 650,000 m², and 720,000 m² in area, surrounded by eroded earth walls still up to 16 metres high. Several kurgans reminded the inhabitants of the ancient Scythian burial tradition.\n\nIn Greek mythology, Gelonus was the son of Echidna and Heracles, he had an older brother Agathyrsus and a younger Scythes. Hylea is pointed to be where was the Echidna's cave between people Arimi or Harimi, the Greeks on the Euxine believed that this was somewhere in Scythia.\n\n\n"}
{"id": "1714910", "url": "https://en.wikipedia.org/wiki?curid=1714910", "title": "Grand Lake (Colorado)", "text": "Grand Lake (Colorado)\n\nGrand Lake is Colorado's largest and deepest natural lake. It is located in the headwaters of the Colorado River in Grand County, Colorado. On its north shore is located the historic and eponymous town of Grand Lake. The lake fills a glaciated valley that is dammed in part by glacial till from the Pinedale Glaciation, and is younger than about 12,000 years. Natural tributaries to the lake are the North Inlet and East Inlet, both of which flow out of Rocky Mountain National Park, which surrounds the lake on three sides: Grand Lake is located 1 mile from the Park's western entrance. Grand Lake was named Spirit Lake by the Ute Tribe because they believed the lake's cold waters to be the dwelling place of departed souls.\n\nAs part of the Colorado-Big Thompson Project,\nGrand Lake forms a continuous body of water with the man-made reservoir Shadow Mountain Lake, which under natural conditions then flows into another man-made reservoir, Lake Granby. The elevation of Grand Lake is maintained between 8,367 feet (2,550 meters) and 8,366 feet (2,549.8 meters). When the Colorado-Big Thompson (C-BT) project is diverting water to northeastern Colorado, water collected in Lake Granby can be pumped back into Shadow Mountain where it flows backward into Grand Lake, then under Rocky Mountain National Park and the Continental Divide via the Alva B. Adams Tunnel to the Big Thompson River on the eastern slope of the Rocky Mountains. From there, the water flows into the South Platte River and is used for agriculture, municipal, and industrial purposes. Diverted C-BT water provides hydroelectric power to five power stations on the eastern slope of the Colorado Rockies.\n\nThe C-BT is one of the first of many large-scale diversions of water from the Colorado River Basin between Colorado and the Gulf of California. Because the C-BT Project moves water from the Colorado Basin to the South Platte Basin, or, on a larger scale, from the Colorado River, which drains to the Gulf of California to the Mississippi River, which drains to the Gulf of Mexico, the project is considered a transbasin diversion.\n\n"}
{"id": "45382702", "url": "https://en.wikipedia.org/wiki?curid=45382702", "title": "HVDC BorWin3", "text": "HVDC BorWin3\n\nHVDC BorWin3 is a high voltage direct current (HVDC) link under construction to transmit Offshore wind power to the power grid of the German mainland. The project differs from most HVDC systems in that one of the two converter stations is built on a platform in the sea. Voltage-Sourced Converters with DC ratings of 900 MW, ±320 kV are used and the total cable length is 160 km. The project is the most recent of the German offshore HVDC projects to be awarded (in 2014). It is being built by the Siemens/ Petrofac consortium with the offshore platform contract awarded to Drydocks World in Dubai. The project is expected to be handed over to its owner, TenneT, in 2019.\n\n\n"}
{"id": "44012526", "url": "https://en.wikipedia.org/wiki?curid=44012526", "title": "Hexaperchloratoaluminate", "text": "Hexaperchloratoaluminate\n\nThe hexaperchloratoaluminate ion is a triple negative complex of perchlorate with aluminium. It is related to hexanitratoaluminate and tetraperchloratoaluminate all of which are highly oxidising energetic materials.\n\nSolid materials that have been produced in this series of hexaperchloratoaluminate salts are lithium hexaperchloratoaluminate, ammoniumhexaperchloratoaluminate, tetramethylammonium hexaperchloratoaluminate, and trinitroniumhexaperchloratoaluminate .\n\nThe hexaperchloratoaluminates can be prepared in liquid sulfur dioxide at −10 °C from aluminium trichloride combining with various perchlorates.\n\nAluminium nitrate can be heated at 125 °C with nitrosonium or nitronium perchlorate to yield hexaperchloratoaluminates.\n\nPotassium hexaperchloratoaluminate may or may not exist. Hydrazinium hexaperchloratoaluminate can only be made in an impure form with \n\nGuanidinium hexaperchloratoaluminate can be made via:\n"}
{"id": "33430996", "url": "https://en.wikipedia.org/wiki?curid=33430996", "title": "Inspectioneering Journal", "text": "Inspectioneering Journal\n\nInspectioneering Journal is a technical publication that focuses on mechanical integrity and reliability issues in the chemical and refining industries. It is published bi-monthly. The magazine was established in 1995 and is based in Houston, Texas.\n"}
{"id": "1309936", "url": "https://en.wikipedia.org/wiki?curid=1309936", "title": "Iron(II,III) oxide", "text": "Iron(II,III) oxide\n\nIron(II,III) oxide is the chemical compound with formula FeO. It occurs in nature as the mineral magnetite. It is one of a number of iron oxides, the others being iron(II) oxide (FeO), which is rare, and iron(III) oxide (FeO) also known as hematite. It contains both Fe and Fe ions and is sometimes formulated as FeO ∙ FeO. This iron oxide is encountered in the laboratory as a black powder. It exhibits permanent magnetism and is ferrimagnetic, but is sometimes incorrectly described as ferromagnetic. Its most extensive use is as a black pigment. For this purpose, it is synthesised rather than being extracted from the naturally occurring mineral as the particle size and shape can be varied by the method of production.\n\nUnder anaerobic conditions, ferrous hydroxide (Fe(OH)) can be oxidized by water to form magnetite and molecular hydrogen. This process is described by the Schikorr reaction:\nThe well-crystallized magnetite (FeO) is thermodynamically more stable than the ferrous hydroxide (Fe(OH) ).\n\nMagnetite can be prepared in the laboratory as a ferrofluid in the Massart method by mixing iron(II) chloride and iron(III) chloride in the presence of sodium hydroxide. Magnetite can also be prepared by the chemical co-precipitation in presence of ammonia, which consist in a mixture of a solution 0.1 M of FeCl·6HO and FeCl·4HO with mechanic agitation of about 2000 rpm. The molar ratio of FeCl:FeCl can be 2:1; heating this solution at 70 °C, and immediately the speed is elevated to 7500 rpm and adding quickly a solution of NHOH (10 volume %), immediately a dark precipitate will be formed, which consists of nanoparticles of magnetite. In both cases, the precipitation reaction rely on a quick transformation of acidic hydrolyzed iron ions into the spinel iron oxide structure, by hydrolysis at elevated pH values (above ca. 10).\n\nConsiderable efforts has been devoted towards controlling the particle formation process of magnetite nanoparticles due to the challenging and complex chemistry reactions involved in the phase transformations prior to the formation of the magnetite spinel structure. Magnetite particles are of interests in bioscience applications such as in magnetic resonance imaging (MRI) since iron oxide magnetite nanoparticles represent a non-toxic alternative to currently employed gadolinium-based contrast agents. However, due to lack of control over the specific transformations involved in the formation of the particles, truly superparamagnetic particles have not yet been prepared from magnetite, i.e. magnetite nanoparticles that completely lose their permanent magnetic characteristic in the absence of an external magnetic field (which by definition show a coercivity of 0 A/m). The smallest values currently reported for nanosized magnetite particles is \"Hc\" = 8.5 A m, whereas the largest reported magnetization value is 87 Am kg for synthetic magnetite.\n\nPigment quality FeO, so called synthetic magnetite, can be prepared using processes that use industrial wastes, scrap iron or solutions containing iron salts (e.g. those produced as by-products in industrial processes such as the acid vat treatment (pickling) of steel):\n\n\nReduction of FeO with hydrogen:\nReduction of FeO with CO:\n\nProduction of nano-particles can be performed chemically by taking for example mixtures of Fe and Fe salts and mixing them with alkali to precipitate colloidal FeO. The reaction conditions are critical to the process and determine the particle size.\n\nReduction of magnetite ore by CO in a blast furnace is used to produce iron as part of steel production process:\nControlled oxidation of FeO is used to produce brown pigment quality γ-FeO (maghemite):\nMore vigorous calcining (roasting in air) gives red pigment quality α-FeO (hematite):\n\nFeO has a cubic inverse spinel group structure which consists of a cubic close packed array of oxide ions where all of the Fe ions occupy half of the octahedral sites and the Fe are split evenly across the remaining octahedral sites and the tetrahedral sites.\n\nBoth FeO and γ-FeO have a similar cubic close packed array of oxide ions and this accounts for the ready interchangeability between the three compounds on oxidation and reduction as these reactions entail a relatively small change to the overall structure. FeO samples can be non-stoichiometric.\n\nThe ferrimagnetism of FeO arises because the electron spins of the Fe and Fe ions in the octahedral sites are coupled and the spins of the Fe ions in the tetrahedral sites are coupled but anti-parallel to the former. The net effect is that the magnetic contributions of both sets are not balanced and there is a permanent magnetism.\n\nFeO is ferrimagnetic with a Curie temperature of 858 K. There is a phase transition at 120K, the so-called Verwey transition where there is a discontinuity in the structure, conductivity and magnetic properties. This effect has been extensively investigated and whilst various explanations have been proposed, it does not appear to be fully understood.\n\nFeO is an electrical conductor with a conductivity significantly higher (X 10) than FeO, and this is ascribed to electron exchange between the Fe and Fe centres.\n\nFeO is used as a black pigment and is known as \"C.I pigment black 11\" (C.I. No.77499) or Mars Black.\n\nFeO is used as a catalyst in the Haber process and in the water-gas shift reaction. The latter uses an HTS (high temperature shift catalyst) of iron oxide stabilised by chromium oxide. This iron–chrome catalyst is reduced at reactor start up to generate FeO from α-FeO and CrO to CrO.\n\nNano particles of FeO are used as contrast agents in MRI scanning.\n\nFerumoxytol, also known as Feraheme and Rienso, is an intravenous FeO preparation for treatment of anemia resulting from chronic kidney disease. Ferumoxytol is manufactured and globally distributed by AMAG Pharmaceuticals.\n\nAlong with sulfur and aluminium, it is an ingredient in a specific type of thermite useful for cutting steel.\n\nBluing is a passivation process that produces a layer of FeO on the surface of steel to protect it from rust.\n\nMagnetite has been found as nano-crystals in magnetotactic bacteria (42–45 nm) and in the beak tissue of homing pigeons.\n"}
{"id": "55746198", "url": "https://en.wikipedia.org/wiki?curid=55746198", "title": "K-dron", "text": "K-dron\n\nA k-dron is an eleventh-degree polyhedron discovered by Janusz Kapusta in 1985. The name k-dron comes from the word \"dron\", which in Greek means the wall, and \"k\" is the eleventh letter of the Latin alphabet.\n\nThe shape was invented during the preparation for an exhibition by Janusza Kapusty and his colleague in a New York gallery. After printing the leaflets promoting the exhibition, which featured the image of two squares typed one in the other, Janusz Kapusta fell into the spatial form, on the basis of which he then made a spatial model of the solid.\n\nThe k-dron consists of a k-dron surface, which is a diamond with attached right triangles and a square base. Two k-drons placed tops together forms a cube. Professor Janusz Łysko (Widener University in Pennsylvania ) gave the pattern features two variables that graphs the upper surface of a k-dron within the bounds of −1 ≤ \"x\" ≤ 1 and −1 ≤ \"y\" ≤ 1, namely:\n\nK-dron systems can be arranged a great many different ways, creating a great variety of light and shadow patterns. As the angle of incidence changes, new patterns are created.\nIt is used for the design of façades of buildings, toys, games, jewelry, and even car hoods. There are an extreme variety of applications for this body – Janusz Kapusta described 50 applications in his book, and in April 2001 he stated he already found 168.\n\nIn 2009 the k-dron monument was erected before the building of the county seat in Koło.\n\n"}
{"id": "53164441", "url": "https://en.wikipedia.org/wiki?curid=53164441", "title": "Kalsnava Arboretum", "text": "Kalsnava Arboretum\n\nKalsnava Arboretum () is an arboretum in Madona Municipality, Latvia.\n\nThe arboretum was founded in 1975 with the purpose \"to conduct scientific research in dendrology and forestry and plant foreign woody plants for propagating purposes\". It today contains some 2,500 different taxa and some 45,000 individual plants. The whole arboretum is protected as a nature reserve by Latvian law. The arboretum is a member of the Nordic Association of Arboretums and the Baltic Association of Botanical Gardens. The arboretum is open to visitors and offers a variety of services, including the sale of plants.\n\n"}
{"id": "56797857", "url": "https://en.wikipedia.org/wiki?curid=56797857", "title": "Kiba Hydroelectric Power Station", "text": "Kiba Hydroelectric Power Station\n\nKiba Hydroelectric Power Station is a proposed hydroelectric power station in Uganda.\n\nThe power station would be located on the Nile River, downstream of Karuma Hydroelectric Power Station, but upstream of Murchison Falls. This location is in Nwoya District, in the Northern Region of Uganda, approximately , upstream of the boundary of Murchison Falls National Park.\n\nKiba is approximately , above sea level, and marks the point where the Kiba River enters the Nile.\n\nThe government of Uganda commissioned a study, titled \"Project for Master Plan Study on Hydropower Development in the Republic of Uganda\", that was conducted by \"Electric Power Development Company Limited\" and \"Nippon Koei Company Limited\", which was funded by the Japan International Cooperation Agency (JICA), in 2009. In the report of that study, published in 2011, three large hydroelectric power stations were identified for immediate envelopment, in the 2013 to 2023 time-frame, namely Isimba Hydroelectric Power Station, Karuma Hydroelectric Power Station and Ayago Hydroelectric Power Station. Two other stations were identified for development in the medium term, after the first three, namely Oriang Hydroelectric Power Station (400 megawatts), and Kiba Power Station (initially 200 megawatts).\n\nOn 27 May 2015 the government of Uganda signed a Memorandum of Understanding (MoU) with \"China Africa Investment and Development Company\" (CAIDC), calling for a detailed feasibility study that would lead to a Build, Own, Operate and Transfer (BOOT) agreement for the Kiba Hydroelectric Power Project and the associated transmission line works. In July 2017, more than two years from the MoU, with no tangible progress, the Uganda Ministry of Energy and Mineral Development terminated the MoU. If this dam is o be built, a new investor will have to be sourced.\n\nThe JICA report outlined a possible phased approach, where the power station is built over a number of years and commissioned in phases, to conserve resources and avoid building over-capacity.\n\n\n"}
{"id": "28677393", "url": "https://en.wikipedia.org/wiki?curid=28677393", "title": "Köthen Solar Park", "text": "Köthen Solar Park\n\nThe Köthen Solar Park is a photovoltaic power station in Köthen, Germany. It has a capacity of 45 megawatts (MW) and an expected annual electricity generation of 42 gigawatt-hours. The solar park was developed and built by RGE Energy.\n\nThe PV project is built on a former military airfield in Köthen on . In 2009, the project was the largest solar power plant in Saxony-Anhalt and the world's largest system using string inverters. The project is equipped with 205,000  solar module made of crystalline silicon by BP Solar. Total investment in the project was around €133 million. The solar park was connected to the grid in 2009. Payment was fixed to 35.49 Euro-cents per kilowatt-hour for 20 years.\n"}
{"id": "685746", "url": "https://en.wikipedia.org/wiki?curid=685746", "title": "Laterality", "text": "Laterality\n\nThe term laterality refers to the preference most humans show for one side of their body over the other. Examples include left-handedness/right-handedness and left/right-footedness; it may also refer to the primary use of the left or right hemisphere in the brain. It may also apply to animals or plants. The majority of tests have been conducted on humans, specifically to determine the effects on language.\n\nThe majority of humans are right-handed. Many are also right-sided in general (that is, they prefer to use their right eye, right foot and right ear if forced to make a choice between the two). The reasons for this are not fully understood, but it is thought that because the left cerebral hemisphere of the brain controls the right side of the body, the right side is generally stronger; it is suggested that the left cerebral hemisphere is dominant over the right in most humans because in 90-92% of all humans, the left hemisphere is the language hemisphere.\n\nHuman cultures are predominantly right-handed, and so the right-sided trend may be socially as well as biologically enforced. This is quite apparent from a quick survey of languages. The English word \"left\" comes from the Anglo-Saxon word \"lyft\" which means \"weak\" or \"useless\". Similarly, the French word for left, \"gauche\", is also used to mean \"awkward\" or \"tactless\", and \"sinistra\", the Latin word from which the English word \"sinister\" was derived, means \"left\". Similarly, in many cultures the word for \"right\" also means \"correct\". The English word \"right\" comes from the Anglo-Saxon word \"riht\" which also means \"straight\" or \"correct.\"\n\nThis linguistic and social bias is not restricted to European cultures: for example, Chinese characters are designed for right-handers to write, and no significant left-handed culture has ever been found in the world.\n\nWhen a person is forced to use the hand opposite of the hand that they would naturally use, this is known as \"forced laterality\", or more specifically \"forced dextrality\". A study done by the Department of Neurology at Keele University, North Staffordshire Royal Infirmary suggests that forced dextrality may be part of the reason that the percentage of left-handed people decreases with the higher age groups, both because the effects of pressures toward right-handedness are cumulative over time (hence increasing with age for any given person subjected to them) and because the prevalence of such pressure is decreasing, such that fewer members of younger generations face any such pressure to begin with.\n\nAmbidexterity is when a person has approximately equal skill with both hands and/or both sides of the body. True ambidexterity is very rare. Although a small number of people can write competently with both hands and use both sides of their body well, even these people usually show preference for one side of their body over the other. However, this preference is not necessarily consistent for all activities. Some people may for example use their right hand for writing, and their left hand for playing racket sports and eating (\"see also:\" cross-dominance).\n\nAlso, it is not uncommon that people preferring to use the right hand prefer to use the left leg, e.g. when using a shovel, kicking a ball, or operating control pedals. In many cases, this may be because they are disposed for left-handedness but have been trained for right-handedness. In the sport of cricket, some players may find that they are more comfortable bowling with their left or right hand, but batting with the other hand.\n\nApproximate statistics are below:\n\nLaterality of motor and sensory control has been the subject of a recent intense study and review. It turns out that the hemisphere of speech is the hemisphere of action in general and that the command hemisphere is located either in the right or the left hemisphere (never in both). Around eighty percent of people are left hemispheric for speech and the remainder are right hemispheric: ninety percent of right-handers are left hemispheric for speech, but only fifty percent of left-handers are right hemispheric for speech (the remainder are left hemispheric). The reaction time of the neurally dominant side of the body (the side opposite to the major hemisphere or the command center, as just defined) is shorter than that of the opposite side by an interval equal to the interhemispheric transfer time. Thus, one in five persons has a handedness that is the opposite for which they are wired (per laterality of command center or brainedness, as determined by reaction time study mentioned above).\n\n\nCerebral dominance or specialization has been studied in relation to a variety of human functions. With speech in particular, many studies have been used as evidence that it is generally localized in the left hemisphere. Research comparing the effects of lesions in the two hemispheres, split-brain patients, and perceptual asymmetries have aided in the knowledge of speech lateralization. In one particular study, the left hemisphere’s sensitivity to differences in rapidly changing sound cues was noted (Annett, 1991). This has real world implication, since very fine acoustic discriminations are needed to comprehend and produce speech signals. In an electrical stimulation demonstration performed by Ojemann and Mateer (1979), the exposed cortex was mapped revealing the same cortical sites were activated in phoneme discrimination and mouth movement sequences (Annett, 1991).\n\nAs suggested by Kimura (1975, 1982), left hemisphere speech lateralization might be based upon a preference for movement sequences as demonstrated by American Sign Language (ASL) studies. Since ASL requires intricate hand movements for language communication, it was proposed that skilled hand motions and speech require sequences of action over time. In deaf patients suffering from a left hemispheric stroke and damage, noticeable losses in their abilities to sign were noted. These cases were compared to studies of normal speakers with dysphasias located at lesioned areas similar to the deaf patients. In the same study, deaf patients with right hemispheric lesions did not display any significant loss of signing nor any decreased capacity for motor sequencing (Annett, 1991).\n\nOne theory, known as the acoustic laterality theory, the physical properties of certain speech sounds are what determine laterality to the left hemisphere. Stop consonants, for example t, p, or k, leave a defined silent period at the end of words that can easily be distinguished. This theory postulates that changing sounds such as these are preferentially processed by the left hemisphere. As a result of the right ear being responsible for transmission to sounds to the left hemisphere, it is capable of perceiving these sounds with rapid changes. This right ear advantage in hearing and speech laterality was evidenced in dichotic listening studies. Magnetic imaging results from this study showed greater left hemisphere activation when actual words were presented as opposed to pseudo-words (Shtyrov, Pihko, and Pulvermuller, 2005). Two important aspects of speech recognition are phonetic cues, such as format patterning, and prosody cues, such as intonation, accent, and emotional state of the speaker (Imaizumi, Koichi, Kiritani, Hosoi & Tonoike, 1998).\n\nIn a study done with both monolinguals and bilinguals, which took into account language experience, second language proficiency, and onset of bilingualism among other variables, researchers were able to demonstrate left hemispheric dominance. In addition, bilinguals that began speaking a second language early in life demonstrated bilateral hemispheric involvement. The findings of this study were able to predict differing patterns of cerebral language lateralization in adulthood (Hull & Vaid, 2006).\n\nIt has been shown that cerebral lateralization is a widespread phenomenon in the animal kingdom. Functional and structural differences between left and right brain hemispheres can be found in many other vertebrates and also in invertebrates.\n\nIt has been proposed that negative, withdrawal-associated emotions are processed predominantly by the right hemisphere, whereas the left hemisphere is largely responsible for processing positive, approach-related emotions. This has been called the \"laterality-valence hypothesis\".\n\nOne sub-set of laterality in animals is limb dominance. Preferential limb use for specific tasks has been shown in species including chimpanzees, mice, bats, wallabies, parrots, chickens and toads.\n\nAnother form of laterality is hemispheric dominance for processing conspecific vocalizations, reported for chimpanzees, sea lions, dogs, zebra finches and Bengalese finches.\n\nDomestic horses (\"Equus ferus caballus\") exhibit laterality in at least two areas of neural organization, i.e. sensory and motor. In thoroughbreds, the strength of motor laterality increases with age. Horses under 4 years old have a preference to initially use the right nostril during olfaction. Along with olfaction, French horses have an eye laterality when looking at novel objects. There is a correlation between their score on an emotional index and eye preference; horses with higher emotionality are more likely to look with their left eye. The less emotive French saddlebreds glance at novel objects using the right eye, however, this tendency is absent in the trotters, although the emotive index is the same for both breeds. Racehorses exhibit laterality in stride patterns as well. They use their preferred stride pattern at all times whether racing or not, unless they are forced to change it while turning, injured, or fatigued.\n\nIn domestic dogs (\"Canis familiaris\"), there is a correlation between motor laterality and noise sensitivity - a lack of paw preference is associated with noise-related fearfulness. (Branson and Rogers, 2006) Fearfulness is an undesirable trait in guide dogs, therefore, testing for laterality can be a useful predictor of a successful guide dog. Knowing a guide dog's laterality can also be useful for training because the dog may be better at walking to the left or the right of their blind owner.\n\nDomestic cats (\"Felis silvestris catus\") show an individual handedness when reaching for static food. In one study, 46% preferred to use the right paw, 44% the left, and 10% were ambi-lateral; 60% used one paw 100% of the time. There was no difference between male and female cats in the proportions of left and right paw preferences. In moving-target reaching tests, cats have a left-sided behavioural asymmetry. One study indicates that laterality in this species is strongly related to temperament. Furthermore, individuals with stronger paw preferences are rated as more confident, affectionate, active, and friendly.\n\nChimpanzees show right-handedness in certain conditions. This is expressed at the population level for females, but not males. The complexity of the task has a dominant effect on handedness in chimps.\n\nCattle use visual/brain lateralisation in their visual scanning of novel and familiar stimuli. Domestic cattle prefer to view novel stimuli with the left eye, (similar to horses, Australian magpies, chicks, toads and fish) but use the right eye for viewing familiar stimuli.\n\nSchreibers' long-fingered bat is lateralized at the population level and shows a left-hand bias for climbing or grasping.\n\nSome types of mastodon indicate laterality through the fossil remains having differing tusk lengths.\n\nMarsupials are fundamentally different from other mammals in that they lack a corpus callosum. However, wild kangaroos and other macropod marsupials have a left-hand preference for everyday tasks. Left-handedness is particularly apparent in the red kangaroo (\"Macropus rufus\") and the eastern gray kangaroo (\"Macropus giganteus\"). The red-necked wallaby (\"Macropus rufogriseus\") preferentially uses the left hand for behaviours that involve fine manipulation, but the right for behaviours that require more physical strength. There is less evidence for handedness in arboreal species.\n\nParrots tend to favor one foot when grasping objects (for example fruit when feeding). Some studies indicate that most parrots are left footed.\n\nThe Australian magpie (\"Gymnorhina tibicen\") uses both left-eye and right-eye laterality when performing anti-predator responses, which include mobbing. Prior to withdrawing from a potential predator, Australian magpies view the animal with the left eye (85%), but prior to approaching, the right eye is used (72%). The left eye is used prior to jumping (73%) and prior to circling (65%) the predator, as well as during circling (58%) and for high alert inspection of the predator (72%). The researchers commented that \"mobbing and perhaps circling are agonistic responses controlled by the LE[left eye]/right hemisphere, as also seen in other species. Alert inspection involves detailed examination of the predator and likely high levels of fear, known to be right hemisphere function.\"\n\nYellow-legged gull (\"Larus michahellis\") chicks show laterality when reverting from a supine to prone posture, and also in pecking at a dummy parental bill to beg for food. Lateralization occurs at both the population and individual level in the reverting response and at the individual level in begging. Females have a leftward preference in the righting response, indicating this is sex dependent. Laterality in the begging response in chicks varies according to laying order and matches variation in egg androgens concentration.\nLaterality determines the organisation of rainbowfish (\"Melanotaenia\" spp.) schools. These fish demonstrate an individual eye preference when examining their reflection in a mirror. Fish which show a right-eye preference in the mirror test prefer to be on the left side of the school. Conversely, fish that show a left-eye preference in the mirror test or were non-lateralised, prefer to be slightly to the right side of the school. The behaviour depends on the species and sex of the school.\n\nThree species of toads, the common toad (\"Bufo bufo\"), green toad (\"Bufo viridis\") and the cane toad (\"Bufo marinus\") show stronger escape and defensive responses when a model predator was placed on the toad's left side compared to their right side. Emei music frogs (\"Babina daunchina\") have a right-ear preference for positive or neutral signals such as a conspecific's advertisement call and white noise, but a left-ear preference for negative signals such as predatory attack.\n\nThe Mediterranean fruit fly (\"Ceratitis capitata\") exhibits left-biased population-level lateralisation of aggressive displays (boxing with forelegs and wing strikes) with no sex-differences. In ants, \"Temnothorax albipennis\" (rock ant) scouts show behavioural lateralization when exploring unknown nest sites, showing a population-level bias to prefer left turns. One possible reason for this is that its environment is partly maze-like and consistently turning in one direction is a good way to search and exit mazes without getting lost. This turning bias is correlated with slight asymmetries in the ants' compound eyes (differential ommatidia count).\n\n\n"}
{"id": "5230851", "url": "https://en.wikipedia.org/wiki?curid=5230851", "title": "Little Dancer of Fourteen Years", "text": "Little Dancer of Fourteen Years\n\nThe Little Fourteen-Year-Old Dancer (French: \"La Petite Danseuse de Quatorze Ans\") is a sculpture begun c. 1880 by Edgar Degas of a young student of the Paris Opera Ballet dance school, a Belgian named Marie van Goethem.\n\nThe sculpture is one-third life size and was originally sculpted in wax, a somewhat unusual choice of medium for the time. It is dressed in a real bodice, tutu and ballet slippers and has a wig of real hair. All but a hair ribbon and the tutu are covered in wax. The 28 bronze repetitions that appear in museums and galleries around the world today were cast after Degas' death. The tutus worn by the bronzes vary from museum to museum.\n\nThe exact relationship between Marie van Goethem and Edgar Degas is a matter of debate. It was common in 1880 for the \"Petits Rats\" of the Paris Opera to seek protectors from among the wealthy visitors at the back door of the opera.\n\nRealistic wax figures with real hair and real clothes had also been popular in religious, Folk, and fine arts for centuries before Degas created his \"Little Dancer\".\n\nThe arms are taut, and the legs and feet are placed in a ballet position akin to fourth position at rest, and there is tension in the pose, an image of a ballerina being put through her paces, not posing in an angelic way. Her face is – \"contorted, people thought it was a deliberate image of ugliness, but you could also say it's the image of a sickly gawky adolesecent who is being made to do something she doesn't totally want to do.\"\n\nWhen the \"La Petite Danseuse de Quatorze Ans\" was shown in Paris at the Sixth Impressionist Exhibition of 1881, it received mixed reviews. Joris-Karl Huysmans called it \"the first truly modern attempt at sculpture I know.\" Certain critics were shocked by the piece, and the dancer was compared to a monkey and an Aztec. One critic, Paul Mantz, called her the \"flower of precocious depravity,\" with a face \"marked by the hateful promise of every vice\" and \"bearing the signs of a profoundly heinous character.\" Comparisons with older art were made, perhaps partly because it was exhibited in a glass case, like classical sculpture in the Louvre, and was dressed in wig and clothes.\n\nAfter Degas' death, his heirs (brother and sister's children) made the decision to have the bronze repetitions of \"Le Petite Danseuse\" and other wax and mixed-media sculptures cast. The casting took place at the Hébrard foundry in Paris from 1920 until 1936 when the Hébrard foundry went bankrupt and closed. Thereafter, \"Hébrard\" Degas \"Little Dancer\" bronzes were cast at the Valsuani foundry in Paris until the mid-1970s. Sixty-nine of Degas' wax sculptures survived the casting process. One copy of \"Le Petite Danseuse\" is currently owned by the creator and owner of Auto Trader, John Madejski. He stated that he bought the sculpture by accident. That copy was sold for £13,257,250 ($19,077,250) at Sotheby's on February, 3rd 2009. Another Hébrard \"Little Dancer\" bronze failed to sell at a November 2011 auction at Christie's.\n\nTo construct the statue, Degas used pigmented beeswax, with a metal armature, rope, and paintbrushes covered by clay for structural support.\n\nThe \"Little Dancer\" wax sculpture we see today is a reworked version of the original sculpture that was shown in 1881. After seeing the wax sculpture in Degas’ living quarters in April 1903, the New York collector Louisine Havemeyer expressed interest in buying the wax. After proposing a bronze or cast wax of the sculpture, which Mrs. Havemeyer refused, Degas took his wax figure upstairs to his working studio and told Vollard he was reworking the sculpture for Havemeyer for 40,000 francs. Degas never sold the sculpture to Mrs. Havemeyer. After Degas died, it was found in a corner of his studio. Paul Lefond, Degas’ biographer, described the \"Little Dancer\" wax after Degas’ death as \"nothing but a ruin;\" and Mary Cassatt telegraphed Mrs. Havemeyer \"Statue Bad Condition.\" However, the wax sculpture we know today is not a ruin. It is Degas' reworked second version of his wax figure. At some point before Degas extensively reworked his sculpture, he allowed a plaster to be cast from the wax figure. This recently re-discovered plaster records the \"Little Dancer\"’s original pose, bodice, and hairdo. The plaster is now in a private collection in the United States.\n\nThe original wax sculpture was acquired by Paul Mellon in 1956. Beginning in 1985, Mr and Mrs Mellon gave the National Gallery of Art 49 Degas waxes, 10 bronzes and 2 plasters, the largest group of original Degas sculptures. \"Little Dancer\" was among the bequests. In 1997, the Airaindor-Valsuani foundry in France began casting a limited edition of Degas bronzes from the pre-1903 \"Little Dancer\" plaster. One such \"Little Dancer\" bronze is owned by the M.T. Abraham Foundation, which, at times, is lent to other institutions and museums including the State Hermitage Museum in Saint Petersburg, Russia. Like the various states of many of Degas' prints, the Valsuani bronzes record the first version of Degas' \"Little Dancer\", while the Hébrard casts record the second and final state of the sculpture.\n\nArt historian Richard Kendall published a scholarly account of the history of Degas's sculpture, \"Degas and the Little Dancer\", with contributions by Douglas Druick and Arthur Beale. The BBC Two documentary \"The Private Life of a Masterpiece: Little Dancer Aged Fourteen\", produced in 2004, closely examines the sculpture, the model, the circumstances of her life, and the critical reaction to the work. Cathy Marie Buchanan's novel, \"The Painted Girls\", presents a fictionalised account of the life of Marie van Goethem, the model for the sculpture, as does Carolyn Meyer's young adult novel \"Marie, Dancing\" and Laurence Anholt's children's picture book \"Degas and the Little Dancer\". In 2014, the Kennedy Center for the Performing Arts in Washington, D.C. premiered the stage musical, \"Little Dancer\", inspired by the story of the young ballerina immortalized by Edgar Degas in his famous sculpture. On September 12, 2016, Dr. Gregory Hedberg presented a paper entitled \"Unraveling the Mysteries of Degas’ \"Little Dancer, Aged Fourteen\",\" at the French Institute, Alliance Français, in New York City.\n\nThe piece inspired two of Damien Hirst's works, \"Virgin Mother\" and \"Verity\".\n\nThe sculpture is prominently featured in the 1993 thriller \"Malice\".\n\n"}
{"id": "39409873", "url": "https://en.wikipedia.org/wiki?curid=39409873", "title": "Mailiao Power Plant", "text": "Mailiao Power Plant\n\nThe Mailiao Power Plant () is a coal-fired power plant in Mailiao Township, Yunlin County, Taiwan. With a total installed capacity of 4,200 MW, the plant is Taiwan's third largest coal-fired power plant after Taichung Power Plant and Hsinta Power Plant.\n\nThe groundbreaking ceremony for the power plant construction was held on 12 December 1996. Invited to the ceremony were Vice Premier Hsu Li-teh, Minister of Economic Affairs Wang Chih-kang and President of Legislative Yuan Liu Sung-pan.\n\nCommissioned in June 1999 for its first two units, the power plant is the first independent power producer power plant after Taipower ended the electricity supply monopoly in Taiwan in 1994.\n\nThe power plant is owned by the Mai-Liao Power Corporation (MPC) and Formosa Petrochemical Corporation (FPCC).\n\nThe power plant consists of five 600 MW pulverized coal-fired units owned by the MPC and two 600 MW pulverized coal-fired co-generation units owned by the FPCC. The plant is designed for base load service with the capability for daily startup and shutdown operation per instructions from Taipower's dispatcher center.\n\nThe coal handling system of the power plant has a truck unloading station capacity of 600 tonnes/hour and its flexible sidewall conveyor capacity of 800 tonnes/hour. The coal is stored in coal domes in which each dome is 120 meter in diameter, 60 meter in height and 180,000 tons in capacity.\n\nOn 20 June 2012 at 11:55 am, the power plant tripped during the Tropical Storm Talim. This caused the shut down of 54 out of 66 units of the Yunlin petrochemical complex.\n\nOn 1 August 2016, a generating unit of the power plant broke down, disrupting the supply of power.\n\nOn 7 October 2017, the no. 1 generation unit of the power plant was shut down due to pipe rupture, causing 600 MW of power loss.\n\n"}
{"id": "3227061", "url": "https://en.wikipedia.org/wiki?curid=3227061", "title": "Murlen National Park", "text": "Murlen National Park\n\nMurlen National Park is a national park located in the Champhai district Mizoram in India.\nThe size of the park area is . The park is situated about 245 km east of Aizawl, and is close to the Chin Hills. It lies north of Lengteng Wildlife Sanctuary in the same district. It covers an area of approximately 100 km. The tropical, semi-evergreen and sub montane Forests of Murlen are home to a rich variety of flora and fauna. About 15 species of mammals, 150 species of birds, 35 species of Medicinal plants, 2 species of bamboos, and 4 species of orchids so far have been recorded in this Park. At present, 36 people are involved in conservation work of Murlen National Park.\n\nMurlen National Park is at Murlen Village, which is the Village of Saithuama Sailo, Chief of Hnahlan. It is a part of Hnahlan Village. Murlen National Park was declared in 1991.\n\nOnly about 1% of the sun’s ray can penetrate the forest on a sunny day. So the thickness of the forest in the park is generally compared to the forest found in the Amazon region in South America. Some of the trees found in the park are as old as 350 years. There is an area in the park where not even a single sun’s ray can penetrate. And for this reason the area has been known as ‘losing area of seven fellow-men’ or land of no return. The vegetation is admixture of \"Quercus\", \"Schima wallichii\", \"Betula\", \"Michelia champaca\", \"Pinus kesiya\", \"Prunus\", \"Myrica\", \"Rhododendron\", \"Chimonobambusa callosa\", canes and a variety of orchids and lichens.\n\nFauna found here include the tiger, leopard, sambar, barking deer, Malayan giant squirrel, Himalayan black bear, serow, hoolock gibbon, rhesus macaque, Hume's pheasant, kalij pheasant, grey partridge, hill myna, and dark-rumped swift. Cases of Hunting and poaching have been reported in Murlen National Park.\n"}
{"id": "48395", "url": "https://en.wikipedia.org/wiki?curid=48395", "title": "Navier–Stokes equations", "text": "Navier–Stokes equations\n\nIn physics, the Navier–Stokes equations (), named after Claude-Louis Navier and George Gabriel Stokes, describe the motion of viscous fluid substances.\n\nThese balance equations arise from applying Isaac Newton's second law to fluid motion, together with the assumption that the stress in the fluid is the sum of a diffusing viscous term (proportional to the gradient of velocity) and a pressure term—hence describing \"viscous flow\". The main difference between them and the simpler Euler equations for inviscid flow is that Navier–Stokes equations also factor in the Froude limit (no external field) and are not conservation equations, but rather a dissipative system, in the sense that they cannot be put into the quasilinear homogeneous form:\n\nNavier–Stokes equations are useful because they describe the physics of many phenomena of scientific and engineering interest. They may be used to model the weather, ocean currents, water flow in a pipe and air flow around a wing. The Navier–Stokes equations, in their full and simplified forms, help with the design of aircraft and cars, the study of blood flow, the design of power stations, the analysis of pollution, and many other things. Coupled with Maxwell's equations, they can be used to model and study magnetohydrodynamics.\n\nThe Navier–Stokes equations are also of great interest in a purely mathematical sense. Despite their wide range of practical uses, it has not yet been proven whether solutions always exist in three dimensions and, if they do exist, whether they are smooth – i.e. they are infinitely differentiable at all points in the domain. These are called the Navier–Stokes existence and smoothness problems. The Clay Mathematics Institute has called this one of the seven most important open problems in mathematics and has offered a US$1 million prize for a solution or a counterexample.\n\nThe solution of the Navier–Stokes equations is a flow velocity. It is a field, since it is defined at every point in a region of space and an interval of time. Once the velocity field is calculated other quantities of interest, such as pressure or temperature, may be found using additional equations and relations. This is different from what one normally sees in classical mechanics, where solutions are typically trajectories of position of a particle or deflection of a continuum. Studying velocity instead of position makes more sense for a fluid; however for visualization purposes one can compute various trajectories.\n\nThe Navier–Stokes momentum equation can be derived as a particular form of the Cauchy momentum equation, whose general convective form is\nBy setting the Cauchy stress tensor formula_3 to be the sum of a viscosity term formula_4 (the deviatoric stress) and a pressure term formula_5 (volumetric stress) we arrive at\n\n</math>\n\nwhere\n\nIn this form, it is apparent that in the assumption of an inviscid fluid -no deviatoric stress- Cauchy equations reduce to the Euler equations.\n\nAssuming conservation of mass we can use the continuity equation, formula_10 to arrive to the conservation form of the equations of motion. This is often written:\n\nwhere is the outer product:\n\nThe left side of the equation describes acceleration, and may be composed of time-dependent and convective components (also the effects of non-inertial coordinates if present). The right side of the equation is in effect a summation of hydrostatic effects, the divergence of deviatoric stress and body forces (such as gravity).\n\nAll non-relativistic balance equations, such as the Navier–Stokes equations, can be derived by beginning with the Cauchy equations and specifying the stress tensor through a constitutive relation. By expressing the deviatoric (shear) stress tensor in terms of viscosity and the fluid velocity gradient, and assuming constant viscosity, the above Cauchy equations will lead to the Navier–Stokes equations below.\n\nA significant feature of the Cauchy equation and consequently all other continuum equations (including Euler and Navier–Stokes) is the presence of convective acceleration: the effect of acceleration of a flow with respect to space. While individual fluid particles indeed experience time-dependent acceleration, the convective acceleration of the flow field is a spatial effect, one example being fluid speeding up in a nozzle.\n\nRemark: here, the Cauchy stress tensor is denoted formula_3 (instead of formula_4 as it was in the general continuum equations and in the incompressible flow section).\n\nThe compressible momentum Navier–Stokes equation results from the following assumptions on the Cauchy stress tensor:\n\n\nSince the trace of the rate-of-strain tensor in three dimensions is:\n\nThe trace of the stress tensor in three dimensions becomes:\n\nSo by alternatively decomposing the stress tensor into isotropic and deviatoric parts, as usual in fluid dynamics:\n\nIntroducing the second viscosity ,\n\nwe arrive to the linear constitutive equation in the form usually employed in thermal hydraulics:\n\nBoth second viscosity and dynamic viscosity need not be constant – in general, they depend on density, on each other (the viscosity is expressed in pressure), and in compressible flows also on temperature. Any equation expliciting one of these transport coefficient in the conservation variables is called an equation of state.\n\nBy computing the divergence of the stress tensor, since the divergence of tensor is and the divergence of tensor is , one finally arrives to the compressible (most general) Navier–Stokes momentum equation:\nThe above equation can also be written in the form\nwhere formula_19 is the material derivative.\nBulk viscosity is assumed to be constant, otherwise it should not be taken out of the last derivative. The effect of the volume viscosity is that the mechanical pressure is not equivalent to the thermodynamic pressure:\n\nThis difference is usually neglected, sometimes by explicitly assuming , but it could have an impact in sound absorption and attenuation and shock waves. The convective acceleration term can also be written as\n\nwhere the vector formula_22 is known as the Lamb vector.\n\nFor the special case of an incompressible flow, the pressure constrains the flow so that the volume of fluid elements is constant: isochoric flow resulting in a solenoidal velocity field with \n\nThe incompressible momentum Navier–Stokes equation results from the following assumptions on the Cauchy stress tensor:\n\n\nDynamic viscosity need not be constant – in incompressible flows it can depend on density and on pressure. Any equation expliciting one of these transport coefficient in the conservative variables is called an equation of state.\n\nThe divergence of the deviatoric stress is given by:\n\nbecause formula_25 for an incompressible fluid.\nIncompressibility rules out density and pressure waves like sound or shock waves, so this simplification is not useful if these phenomena are of interest. The incompressible flow assumption typically holds well with all fluids at low Mach numbers (say up to about Mach 0.3), such as for modelling air winds at normal temperatures. For incompressible (uniform density ρ) flows the following identity holds:\n\nwhere is the specific (with the sense of \"per unit mass\") thermodynamic work, the internal source term. Then the incompressible Navier–Stokes equations are best visualised by dividing for the density:\n\nwhere is called the kinematic viscosity.\nIt is well worth observing the meaning of each term (compare to the Cauchy momentum equation):\n\nThe higher-order term, namely the shear stress divergence , has simply reduced to the vector laplacian term . This laplacian term can be interpreted as the difference between the velocity at a point and the mean velocity in a small surrounding volume. This implies that – for a Newtonian fluid – viscosity operates as a \"diffusion of momentum\", in much the same way as the heat conduction. In fact neglecting the convection term, incompressible Navier–Stokes equations lead to a vector diffusion equation (namely Stokes equations), but in general the convection term is present, so incompressible Navier–Stokes equations belong to the class of convection-diffusion equations.\n\nIn the usual case of an external field being a conservative field:\n\nby defining the hydraulic head:\n\none can finally condense the whole source in one term, arriving to the incompressible Navier–Stokes equation with conservative external field:\n\nThe incompressible Navier–Stokes equations with conservative external field is the fundamental equation of hydraulics. The domain for these equations is commonly a 3 or less Euclidean space, for which an orthogonal coordinate reference frame is usually set to explicit the system of scalar partial differential equations to be solved. In 3D orthogonal coordinate systems are 3: Cartesian, cylindrical, and spherical. Expressing the Navier–Stokes vector equation in Cartesian coordinates is quite straightforward and not much influenced by the number of dimensions of the euclidean space employed, and this is the case also for the first-order terms (like the variation and convection ones) also in non-cartesian orthogonal coordinate systems. But for the higher order terms (the two coming from the divergence of the deviatoric stress that distinguish Navier–Stokes equations from Euler equations) some tensor calculus is required for deducing an expression in non-cartesian orthogonal coordinate systems.\n\nThe incompressible Navier–Stokes equation is composite, the sum of two orthogonal equations,\n\nwhere and are solenoidal and irrotational projection operators satisfying and and are the non-conservative and conservative parts of the body force. This result follows from the Helmholtz Theorem (also known as the fundamental theorem of vector calculus). The first equation is a pressureless governing equation for the velocity, while the second equation for the pressure is a functional of the velocity and is related to the pressure Poisson equation.\n\nThe explicit functional form of the projection operator in 3D is found from the Helmholtz Theorem:\n\nwith a similar structure in 2D. Thus the governing equation is an integro-differential equation similar to Coulomb and Biot-Savart law, not convenient for numerical computation.\n\nAn equivalent weak or variational form of the equation, proved to produce the same velocity solution as the Navier–Stokes equation, is given by,\n\nfor divergence-free test functions satisfying appropriate boundary conditions. Here, the projections are accomplished by the orthogonality of the solenoidal and irrotational function spaces. The discrete form of this is eminently suited to finite element computation of divergence-free flow, as we shall see in the next section. There one will be able to address the question \"How does one specify pressure-driven (Poiseuille) problems with a pressureless governing equation?\".\n\nThe absence of pressure forces from the governing velocity equation demonstrates that the equation is not a dynamic one, but rather a kinematic equation where the divergence-free condition serves the role of a conservation equation. This all would seem to refute the frequent statements that the incompressible pressure enforces the divergence-free condition.\n\nWith partitioning of the problem domain and defining basis functions on the partitioned domain, the discrete form of the governing equation is,\n\nIt is desirable to choose basis functions which reflect the essential feature of incompressible flow – the elements must be divergence-free. While the velocity is the variable of interest, the existence of the stream function or vector potential is necessary by the Helmholtz Theorem. Further, to determine fluid flow in the absence of a pressure gradient, one can specify the difference of stream function values across a 2D channel, or the line integral of the tangential component of the vector potential around the channel in 3D, the flow being given by Stokes' Theorem. Discussion will be restricted to 2D in the following.\n\nWe further restrict discussion to continuous Hermite finite elements which have at least first-derivative degrees-of-freedom. With this, one can draw a large number of candidate triangular and rectangular elements from the plate-bending literature. These elements have derivatives as components of the gradient. In 2D, the gradient and curl of a scalar are clearly orthogonal, given by the expressions,\n\nAdopting continuous plate-bending elements, interchanging the derivative degrees-of-freedom and changing the sign of the appropriate one gives many families of stream function elements.\n\nTaking the curl of the scalar stream function elements gives divergence-free velocity elements. The requirement that the stream function elements be continuous assures that the normal component of the velocity is continuous across element interfaces, all that is necessary for vanishing divergence on these interfaces.\n\nBoundary conditions are simple to apply. The stream function is constant on no-flow surfaces, with no-slip velocity conditions on surfaces.\nStream function differences across open channels determine the flow. No boundary conditions are necessary on open boundaries, though consistent values may be used with some problems. These are all Dirichlet conditions.\n\nThe algebraic equations to be solved are simple to set up, but of course are non-linear, requiring iteration of the linearized equations.\n\nSimilar considerations apply to three-dimensions, but extension from 2D is not immediate because of the vector nature of the potential, and there exists no simple relation between the gradient and the curl as was the case in 2D.\n\nRecovering pressure from the velocity field is easy. The discrete weak equation for the pressure gradient is,\n\nwhere the test/weight functions are irrotational. Any conforming scalar finite element may be used. However, the pressure gradient field may also be of interest. In this case one can use scalar Hermite elements for the pressure. For the test/weight functions one would choose the irrotational vector elements obtained from the gradient of the pressure element.\n\nThe rotating frame of reference introduces some interesting pseudo-forces into the equations through the material derivative term. Consider a stationary inertial frame of reference K, and a non-inertial frame of reference K', which is translating with velocity formula_37 and rotating with angular velocity formula_38 with respect to the stationary frame. The Navier–Stokes equation observed from the non-inertial frame then becomes\n\nHere formula_39 and formula_40 are measured in the non-inertial frame. The first term in the parenthesis represents Coriolis acceleration, the second term is due to centripetal acceleration, the third is due to the linear acceleration of K' with respect to K and the fourth term is due to the angular acceleration of K' with respect to K.\n\nThe Navier–Stokes equations are strictly a statement of the balance of momentum. To fully describe fluid flow, more information is needed, how much depending on the assumptions made. This additional information may include boundary data (no-slip, capillary surface, etc.), conservation of mass, balance of energy, and/or an equation of state.\n\nRegardless of the flow assumptions, a statement of the conservation of mass is generally necessary. This is achieved through the mass continuity equation, given in its most general form as:\n\nor, using the substantive derivative:\n\nIn the example below we can assume to have a Newtonian fluid as well as having and both be constant.\n\nRecall that mass continuity is simply the summation of the rate of mass in and the rate of mass out.\n\nSince there is no change in density over time, , we have:\n\nRecall that is a constant thus proving the divergence theorem above.\n\nTaking the curl of the Navier–Stokes equation results in the elimination of pressure. This is especially easy to see if 2D Cartesian flow is assumed (like in the degenerate 3D case with and no dependence of anything on ), where the equations reduce to:\n\nDifferentiating the first with respect to , the second with respect to and subtracting the resulting equations will eliminate pressure and any conservative force. Defining the stream function through\n\nresults in mass continuity being unconditionally satisfied (given the stream function is continuous), and then incompressible Newtonian 2D momentum and mass conservation condense into one equation:\n\nwhere is the 2D biharmonic operator and is the kinematic viscosity, . We can also express this compactly using the Jacobian determinant:\n\nThis single equation together with appropriate boundary conditions describes 2D fluid flow, taking only kinematic viscosity as a parameter. Note that the equation for creeping flow results when the left side is assumed zero.\n\nIn axisymmetric flow another stream function formulation, called the Stokes stream function, can be used to describe the velocity components of an incompressible flow with one scalar function.\n\nThe incompressible Navier–Stokes equation is a differential algebraic equation, having the inconvenient feature that there is no explicit mechanism for advancing the pressure in time. Consequently, much effort has been expended to eliminate the pressure from all or part of the computational process. The stream function formulation eliminates the pressure but only in two dimensions and at the expense of introducing higher derivatives and elimination of the velocity, which is the primary variable of interest.\n\nThe Navier–Stokes equations are nonlinear partial differential equations in the general case and so remain in almost every real situation. In some cases, such as one-dimensional flow and Stokes flow (or creeping flow), the equations can be simplified to linear equations. The nonlinearity makes most problems difficult or impossible to solve and is the main contributor to the turbulence that the equations model.\n\nThe nonlinearity is due to convective acceleration, which is an acceleration associated with the change in velocity over position. Hence, any convective flow, whether turbulent or not, will involve nonlinearity. An example of convective but laminar (nonturbulent) flow would be the passage of a viscous fluid (for example, oil) through a small converging nozzle. Such flows, whether exactly solvable or not, can often be thoroughly studied and understood.\n\nTurbulence is the time-dependent chaotic behavior seen in many fluid flows. It is generally believed that it is due to the inertia of the fluid as a whole: the culmination of time-dependent and convective acceleration; hence flows where inertial effects are small tend to be laminar (the Reynolds number quantifies how much the flow is affected by inertia). It is believed, though not known with certainty, that the Navier–Stokes equations describe turbulence properly.\n\nThe numerical solution of the Navier–Stokes equations for turbulent flow is extremely difficult, and due to the significantly different mixing-length scales that are involved in turbulent flow, the stable solution of this requires such a fine mesh resolution that the computational time becomes significantly infeasible for calculation or direct numerical simulation. Attempts to solve turbulent flow using a laminar solver typically result in a time-unsteady solution, which fails to converge appropriately. To counter this, time-averaged equations such as the Reynolds-averaged Navier–Stokes equations (RANS), supplemented with turbulence models, are used in practical computational fluid dynamics (CFD) applications when modeling turbulent flows. Some models include the Spalart–Allmaras, –, –, and SST models, which add a variety of additional equations to bring closure to the RANS equations. Large eddy simulation (LES) can also be used to solve these equations numerically. This approach is computationally more expensive—in time and in computer memory—than RANS, but produces better results because it explicitly resolves the larger turbulent scales.\n\nTogether with supplemental equations (for example, conservation of mass) and well formulated boundary conditions, the Navier–Stokes equations seem to model fluid motion accurately; even turbulent flows seem (on average) to agree with real world observations.\n\nThe Navier–Stokes equations assume that the fluid being studied is a continuum (it is infinitely divisible and not composed of particles such as atoms or molecules), and is not moving at relativistic velocities. At very small scales or under extreme conditions, real fluids made out of discrete molecules will produce results different from the continuous fluids modeled by the Navier–Stokes equations. For example, capillarity of internal layers in fluids appears for flow with high gradients. For large Knudsen number of the problem, the Boltzmann equation may be a suitable replacement. \nFailing that, one may have to resort to molecular dynamics or various hybrid methods.\n\nAnother limitation is simply the complicated nature of the equations. Time-tested formulations exist for common fluid families, but the application of the Navier–Stokes equations to less common families tends to result in very complicated formulations and often to open research problems. For this reason, these equations are usually rewritten for Newtonian fluids where the viscosity model is linear; truly general models for the flow of other kinds of fluids (such as blood) do not exist.\n\nThe Navier–Stokes equations, even when written explicitly for specific fluids, are rather generic in nature and their proper application to specific problems can be very diverse. This is partly because there is an enormous variety of problems that may be modeled, ranging from as simple as the distribution of static pressure to as complicated as multiphase flow driven by surface tension.\n\nGenerally, application to specific problems begins with some flow assumptions and initial/boundary condition formulation, this may be followed by scale analysis to further simplify the problem.\n\nAssume steady, parallel, one dimensional, non-convective pressure-driven flow between parallel plates, the resulting scaled (dimensionless) boundary value problem is:\n\nThe boundary condition is the no slip condition. This problem is easily solved for the flow field:\n\nFrom this point onward more quantities of interest can be easily obtained, such as viscous drag force or net flow rate.\n\nDifficulties may arise when the problem becomes slightly more complicated. A seemingly modest twist on the parallel flow above would be the \"radial\" flow between parallel plates; this involves convection and thus non-linearity. The velocity field may be represented by a function that must satisfy:\n\nThis ordinary differential equation is what is obtained when the Navier–Stokes equations are written and the flow assumptions applied (additionally, the pressure gradient is solved for). The nonlinear term makes this a very difficult problem to solve analytically (a lengthy implicit solution may be found which involves elliptic integrals and roots of cubic polynomials). Issues with the actual existence of solutions arise for (approximately; this is not ), the parameter R being the Reynolds number with appropriately chosen scales. This is an example of flow assumptions losing their applicability, and an example of the difficulty in \"high\" Reynolds number flows.\n\nA type of natural convection which can be described by the Navier–Stokes equation is the Rayleigh–Bénard convection. It is one of the most commonly studied convection phenomena because of its analytical and experimental accessibility.\n\nSome exact solutions to the Navier–Stokes equations exist. Examples of degenerate cases — with the non-linear terms in the Navier–Stokes equations equal to zero — are Poiseuille flow, Couette flow and the oscillatory Stokes boundary layer. But also more interesting examples, solutions to the full non-linear equations, exist such as Jeffery–Hamel flow, Von Kármán swirling flow, Stagnation point flow, Landau–Squire jet, Taylor–Green vortex.\nNote that the existence of these exact solutions does not imply they are stable: turbulence may develop at higher Reynolds numbers.\n\nUnder additional assumptions, the component parts can be separated.\n\n\\end{align}</math>\n\nwhere and are arbitrary constants. This solution is valid in the domain and for .\n\nIn Cartesian coordinates, when the viscosity is zero (), this is:\n\nA steady-state example with no singularities comes from considering the flow along the lines of a Hopf fibration. Let be a constant radius of the inner coil. One set of solutions is given by:\n\nfor arbitrary constants and . This is a solution in a non-viscous gas (compressible fluid) whose density, velocities and pressure goes to zero far from the origin. (Note this is not a solution to the Clay Millennium problem because that refers to incompressible fluids where is a constant, neither does it deal with the uniqueness of the Navier–Stokes equations with respect to any turbulence properties.) It is also worth pointing out that the components of the velocity vector are exactly those from the Pythagorean quadruple parametrization. Other choices of density and pressure are possible with the same velocity field:\n\nWyld diagrams are bookkeeping graphs that correspond to the Navier–Stokes equations via a perturbation expansion of the fundamental continuum mechanics. Similar to the Feynman diagrams in quantum field theory, these diagrams are an extension of Keldysh's technique for nonequilibrium processes in fluid dynamics. In other words, these diagrams assign graphs to the (often) turbulent phenomena in turbulent fluids by allowing correlated and interacting fluid particles to obey stochastic processes associated to pseudo-random functions in probability distributions.\n\n &\\quad = -\\frac{\\partial p}{\\partial r} + \\mu \\left(\\frac{1}{r}\\frac{\\partial}{\\partial r}\\left(r \\frac{\\partial u_r}{\\partial r}\\right) +\n\n\\end{align}</math>\n\nThe gravity components will generally not be constants, however for most applications either the coordinates are chosen so that the gravity components are constant or else it is assumed that gravity is counteracted by a pressure field (for example, flow in horizontal pipe is treated normally without gravity and without a vertical pressure gradient). The continuity equation is:\n\nThis cylindrical representation of the incompressible Navier–Stokes equations is the second most commonly seen (the first being Cartesian above). Cylindrical coordinates are chosen to take advantage of symmetry, so that a velocity component can disappear. A very common case is axisymmetric flow with the assumption of no tangential velocity (), and the remaining quantities are independent of :\n\n\\end{align}</math>\n\nMass continuity will read:\n\nThese equations could be (slightly) compacted by, for example, factoring from the viscous terms. However, doing so would undesirably alter the structure of the Laplacian and other quantities.\n\nThe Navier–Stokes equations are used extensively in video games in order to model a wide variety of natural phenomena. Simulations of small-scale gaseous fluids, such as fire and smoke, are often based on the seminal paper \"Real-Time Fluid Dynamics for Games\" by Jos Stam, which elaborates one of the methods proposed in Stam's earlier, more famous paper \"Stable Fluids\" from 1999. Stam proposes stable fluid simulation using a Navier–Stokes solution method from 1968, coupled with an unconditionally stable semi-Lagrangian advection scheme, as first proposed in 1992.\n\nMore recent implementations based upon this work run on the game systems graphics processing unit (GPU) as opposed to the central processing unit (CPU) and achieve a much higher degree of performance.\nMany improvements have been proposed to Stam's original work, which suffers inherently from high numerical dissipation in both velocity and mass.\n\nAn introduction to interactive fluid simulation can be found in the 2007 ACM SIGGRAPH course, Fluid Simulation for Computer Animation.\n\n\n"}
{"id": "1461643", "url": "https://en.wikipedia.org/wiki?curid=1461643", "title": "Nominal power (radio broadcasting)", "text": "Nominal power (radio broadcasting)\n\nNominal power is a measurement of a mediumwave radio station's output used in the United States. AM broadcasters are licensed by the Federal Communications Commission to operate at a specific nominal power, which may be (and usually is) different from the transmitter power output.\n\n\nIn both cases, nominal power excludes losses in transmission lines between the tower or phasor and the transmitter; however, it \"includes\" losses in a resistor network used to decrease the efficiency of the antenna system.\n\nNominal power is ultimately a historical artifact of the regulatory regime employed by the FCC prior to the 1980s. In the old system, rather than allowing licensees to choose any power level which would meet the efficiency and interference standards for their class, stations were restricted to a small set of power levels: 50, 100, 250, 500, 1000, 2500, 5000, 10000, 25000, and 50000 watts. A station whose maximum coverage would otherwise be available at 4500 watts (given a specific directional pattern and antenna system efficiency) had a choice of either living with 2500 watts, or reducing the antenna efficiency to a level which would allow for 5 kW. Newly-constructed stations could fairly easily design an antenna system to meet the requirements, but stations on or moving to a shared tower with higher efficiency had a problem. The resistor network exception was created to allow stations to reduce their antenna efficiency without having to modify the existing tower.\n\nRule changes in the 1980s did away with the fixed set of power choices, allowing stations to choose an appropriate power level for their antenna system (\"dial-a-power\"), so there should no longer be any need for the concept of nominal power. However, stations still take advantage of the resistor exception in some cases, simply because they perceive the marketing advantage of higher power (or at least \"round\" power) to be worth the cost of the wasted energy.\n\n"}
{"id": "22600", "url": "https://en.wikipedia.org/wiki?curid=22600", "title": "Optical brightener", "text": "Optical brightener\n\nOptical brighteners, optical brightening agents (OBAs), fluorescent brightening agents (FBAs), or fluorescent whitening agents (FWAs), are chemical compounds that absorb light in the ultraviolet and violet region (usually 340-370 nm) of the electromagnetic spectrum, and re-emit light in the blue region (typically 420-470 nm) by fluorescence. Fluorescent emission is a short-lived period of light emission by a fluorophore, unlike phosphorescence, which is long-lived. These additives are often used to enhance the appearance of color of fabric and paper, causing a \"whitening\" effect; they make intrinsically yellow/orange materials look less so, by compensating the deficit in blue and purple light reflected by the material, with the blue and purple optical emission of the fluorophore. \n\nThe most common classes of compounds with this property are the stilbenes, e.g., 4,4′-diamino-2,2′-stilbenedisulfonic acid. Older, non-commercial fluorescent chemicals include as umbelliferone, which absorbs in the UV portion of the spectrum and re-emit it in the blue portion of the visible spectrum. A white surface treated with an optical brightener can emit more visible light than that which shines on it, making it appear brighter. The blue light emitted by the brightener compensates for the diminishing blue of the treated material and changes the hue away from yellow or brown and toward white.\n\nApproximately 400 brightener types are listed in the Colour Index, but fewer than 90 are produced commercially, and only a handful are commercially important. Generically, the C.I. FBA number can be assigned to a specific substance, however, some are duplicated, since manufacturers apply for the index number when they produce it. The global OBA production for paper, textiles, and detergents is dominated by just a few di- and tetra-sulfonated triazole-stilbenes and a di-sulfonated stilbene-biphenyl derivatives. The stilbene derivatives are subject to fading upon prolonged exposure to UV, due to the formation of optically inactive cis-stilbenes. They are also degraded by oxygen in air, like most dye colorants. All brighteners have extended conjugation and/or aromaticity, allowing for electron movement. Some non-stilbene brighteners are used in more permanent applications such as whitening synthetic fiber.\n\nBrighteners can be \"boosted\" by the addition of certain polyols, such as high molecular weight polyethylene glycol or polyvinyl alcohol. These additives increase the visible blue light emissions significantly. Brighteners can also be \"quenched\". Excess brightener will often cause a greening effect as emissions start to show above the blue region in the visible spectrum.\n\nBrighteners are commonly added to laundry detergents to make the clothes appear cleaner. Normally cleaned laundry appears yellowish, which consumers do not like. Optical brighteners have replaced bluing which was formerly used to produce the same effect.\n\nBrighteners are used in many papers, especially high brightness papers, resulting in their strongly fluorescent appearance under UV illumination. Paper brightness is typically measured at 457 nm, well within the fluorescent activity range of brighteners. Paper used for banknotes does not contain optical brighteners, so a common method for detecting counterfeit notes is to check for fluorescence.\n\nOptical brighteners have also found use in cosmetics. One application is to formulas for washing and conditioning grey or blonde hair, where the brightener can not only increase the luminance and sparkle of the hair, but can also correct dull, yellowish discoloration without darkening the hair. Some advanced face and eye powders contain optical brightener microspheres that brighten shadowed or dark areas of the skin, such as \"tired eyes\".\n\nA side effect of textile optical whitening is to make the treated fabrics more visible with Night Vision Devices than non-treated ones. This may or may not be desirable for military or other applications. Optically brightened paper is often not useful in exacting photographic or art applications, since the whiteness decreases with time.\n\nEnd uses of optical brighteners include:\n\nSome brighteners can cause allergic reactions when in contact with skin, depending on the individual.\n"}
{"id": "26598031", "url": "https://en.wikipedia.org/wiki?curid=26598031", "title": "Pairwise Stone space", "text": "Pairwise Stone space\n\nIn mathematics and particularly in topology, pairwise Stone space is a bitopological space formula_1 which is pairwise compact, pairwise Hausdorff, and pairwise zero-dimensional.\n\nPairwise Stone spaces are a bitopological version of the Stone spaces.\n\nPairwise Stone spaces are closely related to spectral spaces.\n\nTheorem: If formula_2 is a spectral space, then formula_3 is a pairwise Stone space, where formula_4 is the de Groot dual topology of formula_5 . Conversely, if formula_1 is a pairwise Stone space, then both formula_7 and formula_8 are spectral spaces.\n\n"}
{"id": "17993954", "url": "https://en.wikipedia.org/wiki?curid=17993954", "title": "SNUPPS", "text": "SNUPPS\n\nSNUPPS is an acronym standing for Standardized Nuclear Unit Power Plant System. It refers to a 4-loop PWR reactor design produced by Westinghouse in the 1970s. The design was developed for 4 USA utilities, and plants were built at Callaway and Wolf Creek. The UK plant at Sizewell B was also based on SNUPPS but with significant modifications, such as a passive Emergency Boration System.\n\n"}
{"id": "1350525", "url": "https://en.wikipedia.org/wiki?curid=1350525", "title": "Single-wire transmission line", "text": "Single-wire transmission line\n\nA single-wire transmission line (or single wire method) is a method of transmitting electrical power or signals using only a \"single\" electrical conductor. This is in contrast to the usual use of a pair of wires providing a complete circuit, or an electrical cable likewise containing (at least) two conductors for that purpose.\n\nThe single-wire transmission line is not the same as the single-wire earth return system, which is not covered in this article. The latter system relies on a return current through the ground, using the earth as a second conductor between ground terminal electrodes. Thus, the earth effectively forms a second conductor. In a single-wire transmission line there is no second conductor of any form.\n\nAs early as the 1780s Luigi Galvani first observed the effect of static electricity in causing the legs of a frog to twitch, and observed the same effect produced just due to certain metallic contacts with the frog involving a complete circuit. The latter effect was correctly understood by Alessandro Volta as an electric current inadvertently produced by what would become known as a voltaic cell (battery). He understood that such a current required a complete circuit to conduct the electricity, even though the actual nature of electric currents was not at all understood (only a century later would the electron be discovered). All subsequent development of electrical motors, lights, etc. relied on the principle of a complete circuit, generally involving a pair of wires, but sometimes using the ground as the second conductor (as with commercial telegraphy).\n\nAt the end of the 19th century, Nikola Tesla demonstrated that by using an electrical network tuned to resonance it was possible to transmit electric power using only a single conductor, with no need for a return wire. This was spoken of as the \"transmission of electrical energy through one wire without return\".\n\nIn 1891, 1892, and 1893 demonstration lectures with electrical oscillators before the AIEE at Columbia College, N.Y.C., the IEE, London, the Franklin Institute, Philadelphia, and National Electric Light Association, St. Louis, it was shown that electric motors and single-terminal incandescent lamps can be operated through a single conductor without a return wire. Although apparently lacking a complete circuit, such a topology effectively obtains a return circuit by virtue of the load's self-capacitance and parasitic capacitance.\n\nThe final reference to \"burning out\" a machine was to emphasize the ability of such a system to transmit a large power given a proper impedance match, as can be obtained through electrical resonance.\n\nThis observation has been rediscovered several times, and described, for instance, in a 1993 patent. Single-wire transmission in this sense is not possible using direct current and totally impractical for low frequency alternating currents such as the standard 50–60 Hz power line frequencies. At much higher frequencies, however, it is possible for the return circuit (which would normally be connected through a second wire) to utilize the self- and parasitic capacitance of a large conductive object, perhaps the housing of the load itself. Although the self-capacitance of even large objects is rather small in ordinary terms, as Tesla himself appreciated it is possible to resonate that capacitance using a sufficiently large inductor (depending on the frequency used), in which case the large reactance of that capacitance is cancelled out. This allows a large current to flow (and a large power to be supplied to the load) without requiring an extremely high voltage source. Although this method of power transmission has long been understood, it is not clear whether there has been any commercial application of this principle for power transmission.\n\nAs early as 1899, Arnold Sommerfeld published a paper predicting the use of a single cylindrical conductor (wire) to propagate radio frequency energy as a surface wave. Sommerfeld's \"wire wave\" was of theoretical interest as a propagating mode, but this was decades before technology existed for the generation of sufficiently high radio frequencies for any such experimentation, let alone practical applications. What's more, the solution described an infinite transmission line without consideration of coupling energy into (or out of) it.\n\nOf particular practical interest, though, was the prediction of a substantially lower signal attenuation compared to using the same wire as the center conductor of a coaxial cable. Contrary to the previous explanation of the \"full\" transmitted power being due to a classical current through a wire, in this case the currents in the conductor itself are much smaller, with the energy transmitted in the form of an electromagnetic wave (radio wave). But in this case, the presence of the wire acts to guide that wave toward the load, rather than radiating away.\n\nThe reduction of ohmic losses compared to using coax (or other two-wire transmission lines) is especially an advantage at higher frequencies where these losses become very large. Practically speaking, use of this transmission mode below microwave frequencies is very problematic due to the very extended field patterns around the wire. The fields associated with the surface wave along the conductor are significant out to many conductor diameters, therefore metallic or even dielectric materials inadvertently present in these regions will distort the propagation of the mode and typically will increase propagation loss. Although there is no wavelength dependence to this dimension in the transverse direction, in the direction of propagation it is necessary to have a minimum of one half wave of conductor length to full support the propagating mode. For these reasons, and at frequencies available prior to about 1950, the practical disadvantages of such transmission completely outweighed the reduced loss due to the wire's finite conductivity.\n\nIn 1950 Georg Goubau revisited Sommerfeld's discovery of a surface wave mode along a wire, but with the intent of increasing its practicality. One major goal was to reduce the extent of the fields surrounding the conductor so that such a wire would not require an unreasonably large clearance. Another problem was that Sommerfeld's wave propagated exactly at the speed of light (or the slightly lower speed of light in air, for a wire surrounded by air). That meant that there would be radiation losses. The straight wire acts as a long wire antenna, robbing the radiated power from the guided mode. If the propagation velocity can be reduced below the speed of light then the surrounding fields become evanescent, and are thus unable to propagate energy away from the area surrounding the wire.\n\nGoubau investigated the beneficial effect of a wire whose surface is structured (rather than an exact cylinder) such as would be obtained using a threaded wire. More significantly, Goubau proposed the application of a dielectric layer surrounding the wire. Even a rather thin layer (relative to the wavelength) of a dielectric will reduce the propagation velocity sufficiently below the speed of light, eliminating radiation loss from a surface wave along the surface of a long straight wire. This modification also had the effect of greatly reducing the footprint of the electromagnetic fields surrounding the wire, addressing the other practical concern.\n\nFinally, Goubau invented a method for launching (and receiving) electrical energy from such a transmission line. The patented Goubau line (or \"G-line\") consists of a single conductor coated with dielectric material. At each end is a wide disk with a hole in the center through which the transmission line passes. The disk may be the base of a cone, with its narrow end connected typically to the shield of coaxial feed line, and the transmission line itself connecting to the center conductor of the coax.\n\nEven with the reduced extent of the surrounding fields in Goubau's design, such a device only becomes practical at UHF frequencies and above. With technological development at terahertz frequencies, where metallic losses are yet greater, the use of transmission using surface waves and Goubau lines appears promising.\n\nFrom 2003 through 2008 patents were filed for a system using Sommerfeld's original bare (uncoated) wire, but employing a launcher similar to that developed by Goubau. It was promoted under the name \"E-Line\" through 2009. Thus the resulting wave velocity is not reduced by a dielectric coating or special conditioning of the conductor as prescribed as necessary for non-radiation by Goubau for G-Line. This line is claimed to be completely non-radiating, propagating energy by way of a previously unrecognized transverse-magnetic (TM) wave. The intended application in this case is particularly for creating high information rate channels using existing power lines for communications purposes. This has been proposed for transmission of frequencies from below 50 MHz to above 20 GHz using pre-existing single or multistrand overhead power conductors.\n\nWhile Goubau-Line, which requires a conductor having an outer dielectric or special surface conditioning provided to reduce the velocity of the wave on the conductor, has long been known, this more general transverse-magnetic (TM) mode does not have this limitation. E-Line is similar to the Goubau-Line in its use of launchers to couple to and from a radially symmetric wave propagating in the space around a single conductor but different in that it can operate on insulation-free conductors, including those that are polished and completely unfeatured. The propagation velocity of the wave is not reduced and is essentially that of a wave traveling in the same medium in the absence of any conductor at all. Furthermore, practical launchers need not have a cross-section that is a large portion of a wavelength. Energy associated with the wave is confined to a region determined by the diameter and geometry of the conductor rather than the wavelength of the propagating signal. The Launcher has a low frequency cutoff limited by launcher length along the conductor.\n\nThe behavior of such a system is independent of the operating frequency, but is dependent upon details of the power conductor and its environment. \"A nearby conductor other than the line itself may provide a termination point and thereby reduce energy coupled into the TM wave\". (This has relevance to Tesla's 1891-1893 table-top demonstrations.) As for any transmission line, at very high frequencies, the increased losses of the metal conductor, despite the advantage obtained using the surface wave mode, are increased, however because conductor losses are inversely proportional to the square of line impedance, this mode can achieve much lower losses, no more than a few percent of a 50 ohm coaxial line having the same center conductor. The effects of line taps, bends, insulators and other impairments normally found on power distribution systems have been described as \"predictable and manageable\". Depending on these factors, the resulting insertion loss, along with the transmitted power and receiver sensitivity, will determine the maximum distance attained by such a system. Like CATV systems, an increased end-to-end communications path can be obtained through the use of repeaters.\n\nTo take advantage of existing lines, the conical launcher elements are built with a slot through the cone, so that they can be easily fitted over an existing power line (rather than having to be threaded through the cone). Systems can employ a launch device of only 15–20 cm in diameter from upper HF through millimeter wavelengths as long as the launch has sufficient length along the conductor. Generally structures at least one quarter wavelength long are required. A one meter long launcher with a 10 cm opening can provide under 2 dB of insertion loss from 130 MHz through many GHz.\nSystems built in this manner can provide both significant energy transfer e. g. providing motive power for electric helicopters acting as aerostats, at the same time they provide low loss transmission line connection to light weight, high altitude antennas.\n\n"}
{"id": "44243518", "url": "https://en.wikipedia.org/wiki?curid=44243518", "title": "Sivens Dam", "text": "Sivens Dam\n\nSivens Dam (Barrage de Sivens) is a dam which in 2014 was planned for construction across the Tescou, a tributary of the Tarn in the basin of the Garonne in Southern France. The construction site is 10 km north of Lisle-sur-Tarn, in the Department of Tarn (Midi-Pyrénées). The dam is named after the nearby \"Forest of Sivens\". Construction work began in 2014. The project has since been closed.\n\nOn 26 October 2014, Rémi Fraisse, a 21-year-old student protesting against the construction project, was killed after being hit in the back by a stun grenade, sparking further protests, some of which were violent.\n\nSivens is in the Tescou, a tributary of the Tarn in the basin of the Garonne. A barrage project was initiated for the formation of a water reservoir with a volume of 1.5 million cubic metres used especially for irrigation of agricultural land and the control of the low water Tescou. The impact of the project lies in retaining the flooding of 12 hectares of wetland. Countervailing measures were planned to restore a total area of 19.5 hectares of wetlands. This project was to have benefited from 30% European funds (Fedear). The main actors of the project were the general council of Tarn region, Water Agency Adour Garonne and the development company \"Slopes of Gascony\".\nIn 2011, a collective was created against this project : \"Tant qu’il y aura des bouilles\". Hundreds of people came to the zone planned for flooding to live there and to prevent the construction of the dam.\nDeclared illegal, the project is now closed.\n"}
{"id": "32385824", "url": "https://en.wikipedia.org/wiki?curid=32385824", "title": "Solar street light", "text": "Solar street light\n\nSolar street lights are raised light sources which are powered by solar panels generally mounted on the lighting structure or integrated in the pole itself. The solar panels charge a rechargeable battery, which powers a fluorescent or LED lamp during the night.\n\nMost solar lights turn on and turn off automatically by sensing outdoor light using solar panel voltage. Solar streetlights are designed to work throughout the night. Many can stay lit for more than one night if the sun is not available for a couple of days. Older models included lamps that were not fluorescent or LED. Solar lights installed in windy regions are generally equipped with flat panels to better cope with the winds.\n\nLatest designs use wireless technology and fuzzy control theory for battery management. The street lights using this technology can operate as a network with each light having the capability of performing on or off the network.\n\nSolar street lights consist of 4 main parts:\n\nThe solar panel is one of the most important parts of solar street lights, as the solar panel will convert solar energy into electricity. There are 2 types of solar panel: mono-crystalline and poly-crystalline. Conversion rate of mono-crystalline solar panel is much higher than poly-crystalline.Solar panel are varies from wattage systems.\n\nLED is usually used as lighting source of modern solar street light, as the LED will provide much higher Lumens with lower energy consumption. The energy consumption of LED fixture is at least 50% lower than HPS fixture which is widely used as lighting source in Traditional street lights. LEDs lack of warm up time also allows for use of motion detectors for additional efficiency gains.\n\nBattery will store the electricity from solar panel during the day and provide energy to the fixture during night. The life cycle of the battery is very important to the lifetime of the light and the capacity of the battery will affect the backup days of the lights. There are usually 2 types of batteries: Gel Cell Deep Cycle Battery and Lead Acid Battery and many more. Lithium-ion batteries are also popular these days as they are compact in size and not prone to theft (cannot be used in other applications like lead acid batteries).\n\nStrong Poles are necessary to all street lights, especially to solar street lights as there are often components mounted on the top of the pole: fixtures, panels and sometimes batteries. However, in some newer designs, the PV panels and all electronics are integrated in the pole itself. Wind resistance is also a factor.\n\nAlso there are some accessories, like foudation cage and battery box.\n\nEach street light can have its own photo voltaic panel, independent of other street lights. Alternately, a number of panels can be installed as a central power source on a separate location and supply power to a number of street lights. \n\nAll In One type Solar street lights are also gaining popularity. In this type the Solar panel, Lithium-ion battery and LED light are fitted together in a compact way. This enhances battery protection against theft and also the entire unit is weather proof.\n\nCity of Las Vegas was the first city in the world that tested new EnGoPlanet Solar Street lights that are coupled with kinetic tiles that produce electricity when people walk over them.\n\n\nThe charge and discharge cycles of the battery is also very important considering the overall cost of the project.\n"}
{"id": "22615422", "url": "https://en.wikipedia.org/wiki?curid=22615422", "title": "Structural material", "text": "Structural material\n\nStructural engineering depends on the knowledge of materials and their properties, in order to understand how different materials support and resist loads.\n\nCommon structural materials are:\n\nWrought iron is the simplest form of iron, and is almost pure iron (typically less than 0.15% carbon). It usually contains some slag. Its uses are almost entirely obsolete, and it is no longer commercially produced.\n\nWrought iron is very poor in fires. It is ductile, malleable and tough. It does not corrode as easily as steel.\n\nCast iron is a brittle form of iron which is weaker in tension than in compression. It has a relatively low melting point, good fluidity, castability, excellent machinability and wear resistance. Though almost entirely replaced by steel in building structures, cast irons have become an engineering material with a wide range of applications, including pipes, machine and car parts.\n\nCast iron retains high strength in fires, despite its low melting point. It is usually around 95% iron, with between 2.1% and 4% carbon and between 1% and 3% silicon. It does not corrode as easily as steel.\n\nSteel is an iron alloy with controlled level of carbon (between 0.0 and 1.7% carbon).\n\nSteel is used extremely widely in all types of structures, due to its relatively low cost, high strength-to-weight ratio and speed of construction.\n\nSteel is a ductile material, which will behave elastically until it reaches yield (point 2 on the stress–strain curve), when it becomes plastic and will fail in a ductile manner (large strains, or extensions, before fracture at point 3 on the curve). Steel is equally strong in tension and compression.\n\nSteel is weak in fires, and must be protected in most buildings. Despite its high strength to weight ratio, steel buildings have as much thermal mass as similar concrete buildings.\n\nThe elastic modulus of steel is approximately 205 GPa.\n\nSteel is very prone to corrosion (rust).\n\nStainless steel is an iron-carbon alloy with a minimum of 10.5% chromium content. There are different types of stainless steel, containing different proportions of iron, carbon, molybdenum, nickel. It has similar structural properties to steel, although its strength varies significantly.\n\nIt is rarely used for primary structure, and more for architectural finishes and building cladding.\n\nIt is highly resistant to corrosion and staining.\n\nConcrete is used extremely widely in building and civil engineering structures, due to its low cost, flexibility, durability, and high strength. It also has high resistance to fire.\n\nConcrete is a non-linear, non-elastic and brittle material. It is strong in compression and very weak in tension. It behaves non-linearly at all times. Because it has essentially zero strength in tension, it is almost always used as reinforced concrete, a composite material. It is a mixture of sand, aggregate, cement and water. It is placed in a mould, or form, as a liquid, and then it sets (goes off), due to a chemical reaction between the water and cement. The hardening of the concrete is called hydration. The reaction is exothermic (gives off heat).\n\nConcrete increases in strength continually from the day it is cast. Assuming it is not cast under water or in constantly 100% relative humidity, it shrinks over time as it dries out, and it deforms over time due to a phenomenon called creep. Its strength depends highly on how it is mixed, poured, cast, compacted, cured (kept wet while setting), and whether or not any admixtures were used in the mix. It can be cast into any shape that a form can be made for. Its colour, quality, and finish depend upon the complexity of the structure, the material used for the form, and the skill of the worker.\n\nThe elastic modulus of concrete can vary widely and depends on the concrete mix, age, and quality, as well as on the type and duration of loading applied to it. It is usually taken as approximately 25 GPa for long-term loads once it has attained its full strength (usually considered to be at 28 days after casting). It is taken as approximately 38 GPa for very short-term loading, such as footfalls.\n\nConcrete has very favourable properties in fire – it is not adversely affected by fire until it reaches very high temperatures. It also has very high mass, so it is good for providing sound insulation and heat retention (leading to lower energy requirements for the heating of concrete buildings). This is offset by the fact that producing and transporting concrete is very energy intensive. To study the material behavior plenty of numerical models were developed, e.g. the microplane model for constitutive laws of materials.\n\nReinforced concrete is concrete in which steel reinforcement bars (\"rebars\"), plates or fibers have been incorporated to strengthen a material that would otherwise be brittle. In industrialised countries, nearly all concrete used in construction is reinforced concrete. Due to its weakness in tension capacity, concrete will fail suddenly and in brittle manner under flexural (bending) or tensile force unless adequately reinforced with steel.\n\nPrestressed concrete is a method for overcoming the concrete's natural weakness in tension. It can be used to produce beams, floors or bridges with a longer span than is practical with ordinary reinforced concrete. Prestressing tendons (generally of high tensile steel cable or rods) are used to provide a clamping load which produces a compressive stress that offsets the tensile stress that the concrete compression member would otherwise experience due to a bending load.\n\nAluminium is a soft, lightweight, malleable metal. The yield strength of pure aluminium is 7–11 MPa, while aluminium alloys have yield strengths ranging from 200 MPa to 600 MPa. Aluminium has about one-third the density and stiffness of steel. It is ductile, and easily machined, cast, and extruded.\n\nCorrosion resistance is excellent due to a thin surface layer of aluminium oxide that forms when the metal is exposed to air, effectively preventing further oxidation. The strongest aluminium alloys are less corrosion resistant due to galvanic reactions with alloyed copper.\n\nAluminium is used in some building structures (mainly in facades) and very widely in aircraft engineering because of its good strength to weight ratio. It is a relatively expensive material.\n\nIn aircraft it is gradually being replaced by carbon composite materials.\n\nComposite materials are used increasingly in vehicles and aircraft structures, and to some extent in other structures. They are increasingly used in bridges, especially for conservation of old structures such as Coalport cast iron bridge built in 1818. Composites are often anisotropic (they have different material properties in different directions) as they can be laminar materials. They most often behave non-linearly and will fail in a brittle manner when overloaded.\n\nThey provide extremely good strength to weight ratios, but are also very expensive. The manufacturing processes, which are often extrusion, do not currently provide the economical flexibility that concrete or steel provide. The most commonly used in structural applications are glass-reinforced plastics.\n\nMasonry has been used in structures for thousands of years, and can take the form of stone, brick or blockwork. Masonry is very strong in compression but cannot carry tension (because the mortar between bricks or blocks is unable to carry tension). Because it cannot carry structural tension, it also cannot carry bending, so masonry walls become unstable at relatively small heights. High masonry structures require stabilisation against lateral loads from buttresses (as with the flying buttresses seen in many European medieval churches) or from windposts.\n\nHistorically masonry was constructed with no mortar or with lime mortar. In modern times cement based mortars are used. The mortar glues the blocks together, and also smooths out the interface between the blocks, avoiding localised point loads that might have led to cracking.\n\nSince the widespread use of concrete, stone is rarely used as a primary structural material, often only appearing as a cladding, because of its cost and the high skills needed to produce it. Brick and concrete blockwork have taken its place.\n\nMasonry, like concrete, has good sound insulation properties and high thermal mass, but is generally less energy intensive to produce. It is just as energy intensive as concrete to transport.\n\nTimber is the oldest of structural materials, and though mainly supplanted by steel, masonry and concrete, it is still used in a significant number of buildings. The properties of timber are non-linear and very variable, depending on the quality, treatment of wood, and type of wood supplied. The design of wooden structures is based strongly on empirical evidence.\n\nWood is strong in tension and compression, but can be weak in bending due to its fibrous structure. Wood is relatively good in fire as it chars, which provides the wood in the centre of the element with some protection and allows the structure to retain some strength for a reasonable length of time.\n\n\n"}
{"id": "16437835", "url": "https://en.wikipedia.org/wiki?curid=16437835", "title": "Thermal copper pillar bump", "text": "Thermal copper pillar bump\n\nThe thermal copper pillar bump, also known as the \"thermal bump\", is a thermoelectric device made from thin-film thermoelectric material embedded in flip chip interconnects (in particular copper pillar solder bumps) for use in electronics and optoelectronic packaging, including: flip chip packaging of CPU and GPU integrated circuits (chips), laser diodes, and semiconductor optical amplifiers (SOA). Unlike conventional solder bumps that provide an electrical path and a mechanical connection to the package, thermal bumps act as solid-state heat pumps and add thermal management functionality locally on the surface of a chip or to another electrical component. The diameter of a thermal bump is 238 μm and 60 μm high.\n\nThe thermal bump uses the thermoelectric effect, which is the direct conversion of temperature differences to electric voltage and vice versa. Simply put, a thermoelectric device creates a voltage when there is a different temperature on each side, or when a voltage is applied to it, it creates a temperature difference. This effect can be used to generate electricity, to measure temperature, to cool objects, or to heat them.\n\nFor each bump, thermoelectric cooling (TEC) occurs when a current is passed through the bump. The thermal bump pulls heat from one side of the device and transfers it to the other as current is passed through the material. This is known as the Peltier effect. The direction of heating and cooling is determined by the direction of current flow and the sign of the majority electrical carrier in the thermoelectric material. Thermoelectric power generation (TEG) on the other hand occurs when the thermal bump is subjected to a temperature gradient (i.e., the top is hotter than the bottom). In this instance, the device generates current, converting heat into electrical power. This is termed the Seebeck effect.\n\nThe thermal bump was developed by Nextreme Thermal Solutions as a method for integrating active thermal management functionality at the chip level in the same manner that transistors, resistors and capacitors are integrated in conventional circuit designs today. Nextreme chose the copper pillar bump as an integration strategy due to its widespread acceptance by Intel, Amkor and other industry leaders as the method for connecting microprocessors and other advanced electronics devices to various surfaces during a process referred to as “flip-chip” packaging. The thermal bump can be integrated as a part of the standard flip-chip process (Figure 1) or integrated as discrete devices.\n\nThe efficiency of a thermoelectric device is measured by the heat moved (or pumped) divided by the amount of electrical power supplied to move this heat. This ratio is termed the coefficient of performance or COP and is a measured characteristic of a thermoelectric device. The COP is inversely related to the temperature difference that the device produces. As you move a cooling device further away from the heat source, parasitic losses between the cooler and the heat source necessitate additional cooling power: the further the distance between source and cooler, the more cooling is required. For this reason, the cooling of electronic devices is most efficient when it occurs closest to the source of the heat generation.\n\nUse of the thermal bump does not displace system level cooling, which is still needed to move heat out of the system; rather it introduces a fundamentally new methodology for achieving temperature uniformity at the chip and board level. In this manner, overall thermal management of the system becomes more efficient. In addition, while conventional cooling solutions scale with the size of the system (bigger fans for bigger systems, etc.), the thermal bump can scale at the chip level by using more thermal bumps in the overall design.\n\nSolder bumping technology (the process of joining a chip to a substrate without shorting using solder) was first conceived and implemented by IBM in the early ‘60s. Three versions of this type of solder joining were developed. The first was to embed copper balls in the solder bumps to provide a positive stand-off. The second solution, developed by Delco Electronics (General Motors) in the late ‘60s, was similar to embedding copper balls except that the design employed a rigid silver bump. The bump provided a positive stand-off and was attached to the substrate by means of solder that was screen-printed onto the substrate. The third solution was to use a screened glass dam near the electrode tips to act as a ‘‘stop-off’’ to prevent the ball solder from flowing down the electrode. By then the Ball Limiting Metallurgy (BLM) with a high-lead (Pb) solder system and a copper ball had proven to work well. Therefore, the ball was simply removed and the solder evaporation process extended to form pure solder bumps that were approximately 125μm high. This system became known as the controlled collapse chip connection (C3 or C4).\n\nUntil the mid-90’s, this type of flip-chip assembly was practiced almost exclusively by IBM and Delco. Around this time, Delco sought to commercialize its technology and formed Flip Chip Technologies with Kulicke & Soffa Industries as a partner. At the same time, MCNC (which had developed a plated version of IBM’s C4 process) received funding from DARPA to commercialize its technology. These two organizations, along with APTOS (Advanced Plating Technologies on Silicon), formed the nascent out-sourcing market.\n\nDuring this same time, companies began to look at reducing or streamlining their packaging, from the earlier multi-chip-on-ceramic packages that IBM had originally developed C4 to support, to what were referred to as Chip Scale Packages (CSP). There were a number of companies developing products in this area. These products could usually be put into one of two camps: either they were scaled down versions of the multi-chip on ceramic package (of which the Tessera package would be one example); or they were the streamlined versions developed by Unitive Electronics, et al. (where the package wiring had been transferred to the chip, and after bumping, they were ready to be placed).\n\nOne of the issues with the CSP type of package (which was intended to be soldered directly to an FR4 or flex circuit) was that for high-density interconnects, the soft solder bump provided less of a stand-off as the solder bump diameter and pitch were decreased. Different solutions were employed including one developed by Focus Interconnect Technology (former APTOS engineers), which used a high aspect ratio plated copper post to provide a larger fixed standoff than was possible for a soft solder collapse joint.\n\nToday, flip chip is a well established technology and collapsed soft solder connections are used in the vast majority of assemblies. The copper post stand-off developed for the CSP market has found a home in high-density interconnects for advanced micro-processors and is used today by IBM for its CPU packaging.\n\nRecent trends in high-density interconnects have led to the use of copper pillar solder bumps (CPB) for CPU and GPU packaging. CPBs are an attractive replacement for traditional solder bumps because they provide a fixed stand-off independent of pitch. This is extremely important as most of the high-end products are underfilled and a smaller standoff may create difficulties in getting the underfill adhesive to flow under the die.\n\nFigure 2 shows an example of a CPB fabricated by Intel and incorporated into their Presler line of microprocessors among others. The cross section shows copper and a copper pillar (approximately 60 um high) electrically connected through an opening (or via) in the chip passivation layer at the top of the picture. At the bottom is another copper trace on the package substrate with solder between the two copper layers.\n\nThin films are thin material layers ranging from fractions of a nanometer to several micrometers in thickness. Thin-film thermoelectric materials are grown by conventional semiconductor deposition methods and fabricated using conventional semiconductor micro-fabrication techniques.\n\nThin-film thermoelectrics have been demonstrated to provide high heat pumping capacity that far exceeds the capacities provided by traditional bulk pellet TE products. The benefit of thin-films versus bulk materials for thermoelectric manufacturing is expressed in Equation 1. Here the Qmax (maximum heat pumped by a module) is shown to be inversely proportional to the thickness of the film, L.\n\nformula_1         Eq. 1\n\nAs such, TE coolers manufactured with thin-films can easily have 10x – 20x higher Qmax values for a given active area A. This makes thin-film TECs ideally suited for applications involving high heat-flux flows. In addition to the increased heat pumping capability, the use of thin films allows for truly novel implementation of TE devices. Instead of a bulk module that is 1-3 mm in thickness, a thin-film TEC can be fabricated less than 100 um in thickness.\n\nIn its simplest form, the P or N leg of a TE couple (the basic building block of all thin-film TE devices) is a layer of thin-film TE material with a solder layer above and below, providing electrical and thermal functionality.\n\nThe thermal bump is compatible with the existing flip-chip manufacturing infrastructure, extending the use of conventional solder bumped interconnects to provide active, integrated cooling of a flip-chipped component using the widely accepted copper pillar bumping process. The result is higher performance and efficiency within the existing semiconductor manufacturing paradigm. The thermal bump also enables power generating capabilities within copper pillar bumps for energy recycling applications.\n\nThermal bumps have been shown to achieve a temperature differential of 60 °C between the top and bottom headers; demonstrated power pumping capabilities exceeding 150 W/cm2; and when subjected to heat, have demonstrated the capability to generate up to 10 mW of power per bump.\n\nFigure 3 shows an SEM cross-section of a TE leg. Here it is demonstrated that the thermal bump is structurally identical to a CPB with an extra layer, the TE layer, incorporated into the stack-up. The addition of the TE layer transforms a standard copper pillar bump into a thermal bump. This element, when properly configured electrically and thermally, provides active thermoelectric heat transfer from one side of the bump to the other side. The direction of heat transfer is dictated by the doping type of the thermoelectric material (either a P-type or N-type semiconductor) and the direction of electric current passing through the material. This type of thermoelectric heat transfer is known as the Peltier effect. Conversely, if heat is allowed to pass from one side of the thermoelectric material to the other, a current will be generated in the material in a phenomenon known as the Seebeck effect. The Seebeck effect is essentially the reverse of the Peltier effect. In this mode, electrical power is generated from the flow of heat in the TE element. The structure shown in Figure 3 is capable of operating in both the Peltier and Seebeck modes, though not simultaneously.\n\nFigure 4 shows a schematic of a typical CPB and a thermal bump for comparison. These structures are similar, with both having copper pillars and solder connections. The primary difference between the two is the introduction of either a P- or N-type thermoelectric layer between two solder layers. The solders used with CPBs and thermal bumps can be any one of a number of commonly used solders including, but not limited to, Sn, SnPb eutectic, SnAg or AuSn.\n\nFigure 5 shows a device equipped with a thermal bump. The thermal flow is shown by the arrows labeled “heat.” Metal traces, which can be several micrometres high, can be stacked or interdigitated to provide highly conductive pathways for collecting heat from the underlying circuit and funneling that heat to the thermal bump.\n\nThe metal traces shown in the figure for conducting electric current into the thermal bump may or may not be directly connected to the circuitry of the chip. In the case where there are electrical connections to the chip circuitry, on-board temperature sensors and driver circuitry can be used to control the thermal bump in a closed loop fashion to maintain optimal performance. Second, the heat that is pumped by the thermal bump and the additional heat created by the thermal bump in the course of pumping that heat will need to be rejected into the substrate or board. Since the performance of the thermal bump can be improved by providing a good thermal path for the rejected heat, it is beneficial to provide high thermally conductive pathways on the backside of the thermal bump. The substrate could be a highly conductive ceramic substrate like AlN or a metal (e.g., Cu, CuW, CuMo, etc.) with a dielectric. In this case, the high thermal conductance of the substrate will act as a natural pathway for the rejected heat. The substrate might also be a multilayer substrate like a printed wiring board (PWB) designed to provide a high-density interconnect. In this case, the thermal conductivity of the PWB may be relatively poor, so adding thermal vias (e.g. metal plugs) can provide excellent pathways for the rejected heat.\n\nThermal bumps can be used in a number of different ways to provide chip cooling and power generation.\n\nThermal bumps can be evenly distributed across the surface of a chip to provide a uniform cooling effect. In this case, the thermal bumps may be interspersed with standard bumps that are used for signal, power and ground. This allows the thermal bumps to be placed directly under the active circuitry of the chip for maximum effectiveness. The number and density of thermal bumps are based on the heat load from the chip. Each P/N couple can provide a specific heat pumping (Q) at a specific temperature differential (ΔT) at a given electric current. Temperature sensors on the chip (“on board” sensors) can provide direct measurement of the thermal bump performance and provide feedback to the driver circuit.\n\nSince thermal bumps can either cool or heat the chip depending on the current direction, they can be used to provide precision control of temperature for chips that must operate within specific temperature ranges irrespective of ambient conditions. For example, this is a common problem for many optoelectronic components.\n\nIn microprocessors, graphics processors and other high-end chips, hotspots can occur as power densities vary significantly across a chip. These hotspots can severely limit the performance of the devices. Because of the small size of the thermal bumps and the relatively high density at which they can be placed on the active surface of the chip, these structures are ideally suited for cooling hotspots. In such a case, the distribution of the thermal bumps may not need to be even. Rather, the thermal bumps would be concentrated in the area of the hotspot while areas of lower heat density would have fewer thermal bumps per unit area. In this way, cooling from the thermal bumps is applied only where needed, thereby reducing the added power necessary to drive the cooling and reducing the general thermal overhead on the system.\n\nIn addition to chip cooling, thermal bumps can also be applied to high heat-flux interconnects to provide a constant, steady source of power for energy scavenging applications. Such a source of power, typically in the mW range, can trickle charge batteries for wireless sensor networks and other battery operated systems.\n\n\n"}
{"id": "5419984", "url": "https://en.wikipedia.org/wiki?curid=5419984", "title": "Torsion pendulum clock", "text": "Torsion pendulum clock\n\nA torsion pendulum clock, more commonly known as an anniversary clock or 400-day clock, is a mechanical clock which keeps time with a mechanism called a torsion pendulum. This is a weighted disk or wheel, often a decorative wheel with 3 or 4 chrome balls on ornate spokes, suspended by a thin wire or ribbon called a torsion spring (also known as \"suspension spring\"). The torsion pendulum rotates about the vertical axis of the wire, twisting it, instead of swinging like an ordinary pendulum. The force of the twisting torsion spring reverses the direction of rotation, so the torsion pendulum oscillates slowly, clockwise and counterclockwise. The clock's gears apply a pulse of torque to the top of the torsion spring with each rotation to keep the wheel going. The wheel and torsion spring function similarly to a watch's balance wheel and hairspring, as a harmonic oscillator to control the rate of the clock's hands.\n\nTorsion clocks are usually delicate, ornamental, spring-wound mantel clocks. The polished clock mechanism is exposed under a glass case or dome, to allow people to watch the torsion pendulum turn. Clocks of this style, first made by Anton Harder around 1880, are also known as 400-day or anniversary clocks, because many can run for an entire year on a single winding. This does not mean they will keep accurate time the whole year. It's best to wind the clock once a month. But some models will run up to 1000 days on a single winding.\n\nTorsion clocks are capable of running much longer between windings than clocks with an ordinary pendulum, because the torsion pendulum rotates slowly and takes little energy. However they are difficult to set up and are usually not as accurate as clocks with ordinary pendulums. One reason is that the oscillation period of the torsion pendulum changes with temperature due to temperature-dependent change in elasticity of the spring. The rate of the clock can be made faster or slower by an adjustment screw mechanism on the torsion pendulum that moves the weight balls in or out from the axis. The closer in the balls are, the smaller the moment of inertia of the torsion pendulum and the faster it will turn, like a spinning ice skater who pulls in her arms. This causes the clock to speed up.\n\nOne oscillation of the torsion pendulum usually takes 12, 15, or 20 seconds. The escapement mechanism, that changes the rotational motion of the clock's gears to pulses to drive the torsion pendulum, works rather like an anchor escapement. A crutch device at the top of the torsion spring engages a lever with two anchor-shaped arms; the arms in turn alternately engage the teeth of the escape wheel. As the anchor releases a tooth of the escape wheel, the lever, which is fixed to the anchor, moves to one side and, via the crutch, gives a small twist to the top of the torsion spring. This is just enough to keep the oscillation going.\n\nThe Atmos clock, made by Jaeger Le Coultre, is a type of torsion clock which doesn't need to be wound or powered at all. The mainspring which turns the clock's wheels is kept wound by small changes in atmospheric pressure and/or local temperature, using a bellows mechanism. Thus no winding key or battery is needed, and it can run for years without human intervention.\n\nThe torsion pendulum was invented by Robert Leslie in 1793. The torsion pendulum clock was first invented and patented by American Aaron Crane in 1841. He made clocks that would run up to one year on a winding. He also attempted to make precision astronomical regulator clocks based on the torsion pendulum, but only four sold.\n\nThe German Anton Harder apparently independently invented and patented the torsion clock in 1879-1880. He was inspired by watching a hanging chandelier rotate after a servant had turned it to light the candles. He formed the firm Jahresuhrenfabrik ('Year Clock Factory') and designed a clock that would run for a year, but its accuracy was poor. He sold the patent in 1884 to F. A. L. deGruyter of Amsterdam, who allowed the patent to expire in 1887. Other firms entered the market, beginning the German mass production of these clocks.\n\nAlthough they were successful commercially, torsion clocks remained poor timekeepers. In 1951, Charles Terwilliger of the Horolovar Co. invented a temperature compensating suspension spring, which allowed fairly accurate clocks to be made.\n\n"}
{"id": "404759", "url": "https://en.wikipedia.org/wiki?curid=404759", "title": "USS Triton (SSRN-586)", "text": "USS Triton (SSRN-586)\n\nUSS \"Triton\" (SSRN/SSN-586), a United States Navy radar picket nuclear submarine. In early 1960 it became the first vessel to execute a submerged circumnavigation of the Earth (Operation Sandblast). \"Triton\" accomplished this objective during her shakedown cruise while under the command of Captain Edward L. \"Ned\" Beach, Jr. The only member of her class, she also had the distinction of being the only Western submarine powered by two nuclear reactors.\n\n\"Triton\" was the second submarine and the fifth ship of the United States Navy to be named for the Greek god Triton. At the time of her commissioning in 1959, \"Triton\" was the largest, most powerful, and most expensive submarine ever built, at $109 million excluding the cost of nuclear fuel and reactors ($ in present-day terms).\n\nAfter operating for only two years in her designed role, \"Triton\"s mission as a radar picket submarine was made obsolete by the introduction of the carrier-based Grumman WF-2 Tracer airborne early warning aircraft. Converted to an attack submarine in 1962, she became the flagship for the Commander, Submarine Forces, U.S. Atlantic Fleet () in 1964. She was decommissioned in 1969, the first U.S. nuclear submarine to be taken out of service.\n\n\"Triton\"<nowiki>'</nowiki>s hull was moored at the St. Julien's Creek Annex of Norfolk Naval Shipyard in Portsmouth, Virginia as part of the reserve fleet until 1993, though she was struck from the Naval Vessel Register in 1986. In 1993, she was towed to Puget Sound Naval Shipyard to await the Nuclear Powered Ship and Submarine Recycling Program. The former \"Triton\" landed on the keel resting blocks in the drydock basin on 1 October 2007 to begin this recycling process which was completed effective 30 November 2009. The USS Triton's sail superstructure was saved from the recycling process and is now part of the USS Triton Submarine Memorial Park located on Port of Benton Blvd in Richland Washington .\n\n\"Triton\" is considered a first-generation U.S. nuclear-powered submarine, along with , , , and (and her sisters). While serving as fully operational units of the U.S. Navy, the vessels also played key developmental roles. \"Nautilus\" introduced the use of nuclear power for ship propulsion. \"Seawolf\" utilized a liquid-metal nuclear reactor using liquid sodium as an alternative heat exchange medium to pressurized water. \"Halibut\" was the first nuclear-powered submarine to perform a strategic nuclear deterrence patrol armed with Regulus cruise missiles. The \"Skate\"s were the first nuclear-powered submarine class with more than one ship built. \"Triton\"s unique contribution to the development of nuclear power for naval propulsion was her dual reactor plant, which provided the speed required for radar picket missions.\n\nRadar picket submarines (Navy classification \"SSR\") were developed during the post-war period to provide intelligence information, electronic surveillance, and fighter aircraft interception control for forward-deployed naval forces. Unlike destroyers used as radar picket ships during World War Two, these submarines could avoid attack by submerging if detected. The U.S. Navy's MIGRAINE program involved converting existing fleet submarines into radar picket vessels, and the Navy also ordered two purpose-built diesel-electric SSRs, and . However, these were incapable of sustaining the high submerged or surfaced speeds necessary to operate with fast carrier task forces and therefore unsuitable to the task.\n\nNuclear power offered the only possible solution. \"Triton\" was designed in the mid-1950s as a radar picket submarine capable of operating at high speed, on the surface, in advance of an aircraft carrier task force. \"Triton\"s high speed came from her twin-reactor nuclear propulsion plant, with a designed speed, surfaced and submerged, of . On 27 September 1959, \"Triton\" achieved \"well in excess of\" during her initial sea trials.\n\nTo meet her radar picket role, \"Triton\"s main air search radar initially used the AN/SPS-26, the U.S. Navy's first electronically scanned, three-dimensional search radar which was laboratory tested in 1953. The first set was installed on board the destroyer leader \"Norfolk\" prior to its installation on board \"Triton\" in 1959. Since it was scanned electronically in elevation, the AN/SPS-26 set did not need a separate height-finding radar. A submarine version of SPS-26, designated BPS-10, was under development, and it was slated for installation on \"Triton\". To process its radar, electronic, and air traffic data, \"Triton\" had a Combat Information Center (CIC) located in a separate air control compartment, situated between \"Triton\"s reactor and operations compartments.\nDesign work on a nuclear-powered radar picket submarine (SSRN) began in 1954–1955. As initially designed, it had a three-level hull, with its Combat Intelligence Center (CIC) (\"see image\") located on the middle level. Its overall length was initially , with a beam of . Also as initially designed, its displacement was 4800 tons surfaced and 6500 tons submerged. January 1955 performance estimates called for the SAR propulsion plant to produce 34,000 shaft horsepower, with a surfaced speed of and a submerged speed of . \"Triton\" initially had the same dual radar system installed on the non-nuclear \"Sailfish\"-class radar picket submarines (i.e., BPS-2 search radar and BPS-3 height-finder set) housed in a large, stepped sail (\"see image\"). Construction cost was initially estimated at $78,000,000. Subsequent growth of the SAR propulsion plant necessitated the overall increase in \"Triton\"s length and tonnage, although without any loss in speed, while the installation of the AN/SPS-26 3-D search radar allowed the elimination of a separate height-finder.\n\n\"Triton\" was to be the lead ship of a proposed class of nuclear-powered radar picket submarines. A December 1955 long-range naval planning report envisioned five carrier strike groups, each supported by two radar picket submarines. The total force included two non-nuclear \"Sailfish\" class submarines and eight nuclear submarines. With construction costs for \"Triton\" escalating, this long-range requirement was revised in 1957 to provide four nuclear-powered radar picket submarines for a single nuclear-powered carrier group, with the four remaining conventionally powered carrier groups supported by two diesel-electric radar picket submarines each.\nAt the time of her construction, \"Triton\" was the largest submarine ever built. Her knife-like bow, with its bulbous forefoot, provided improved surfaced sea-keeping for her radar picket role. Her surface sea-keeping was further enhanced by high reserve buoyancy (30%), provided by 22 ballast tanks, the most ever in an American submarine. She was the last submarine to have a conning tower, as well as the last American submarine to have twin screws or a stern torpedo room. Her sail was the largest ever aboard an American submarine, measuring long, tall, and wide, and designed to house the large AN/SPS-26 3-D air-search radar antenna when not in use. She also had a compartment solely for crew berthing, with 96 bunks, and two separate chief petty officers' (CPOs') quarters. With an overall length of , \"Triton\" was the longest submarine in the history of the United States Navy until the nuclear-powered ballistic missile submarine was commissioned in 1981.\n\n\"Triton\" was the only submarine outside of the Soviet Union designed with a two-reactor propulsion plant. Her S4G reactors were seagoing versions of the land-based S3G reactor prototype. Both reactors comprised the Submarine Advanced Reactor (SAR) program, a joint venture between the U.S. Navy, Atomic Energy Commission (AEC), and General Electric. As originally designed, \"Triton\"s total reactor output was rated at . However, \"Triton\" achieved during her sea trials (\"pictured\"), and her first commanding officer, Captain Edward L. Beach, Jr., believed \"Triton\"s plant could have reached \"had that been necessary.\"\n\nBoth of \"Triton\"s reactors share the same compartment, with its number one reactor located forward and its number two reactor located aft within that compartment. The number one reactor supplied steam to the forward engine room and the starboard propeller shaft. The number two reactor supplied steam to the after engine room and the port propeller shaft. Each reactor could individually supply steam for the entire ship, or the reactors could be cross-connected as required. It is this enhanced reliability, redundancy, and dependability of its dual-reactor plant that was a key factor in the selection of \"Triton\" to undertake the first submerged circumnavigation of the world.\n\n\"Triton\"s dual-reactor plant met a number of operational and engineering objectives, specifically the high speed requirement to meet its radar picket mission, which continues to be sources of speculation and controversy to this day. During the early 1950s, many engineers at Naval Reactors branch of the U.S. Atomic Energy Commission (AEC) were concerned about depending on single-reactor plants for submarine operations, particularly involving under-the-ice Arctic missions. The presence of two de-aerating feed tanks, which are used only on surface warships, suggested that \"Triton\"s twin-reactor plant may have served as a testbed for future multi-reactor surface warships. The SAR program was the first production naval reactor developed by General Electric for the U.S. Navy, and GE used this SAR experience for the High Power Reactor (HPR) program that led to the development of the D1G and D2G naval reactors used on , , \"California\", and \"Virginia\" classes of nuclear-powered surface ships.\n\nFinally, the U.S. Navy was debating the best approach to optimize performance, particularly underwater speed, for its nuclear submarine fleet. \"Triton\" achieved high speeds through brute horsepower, rather than the more hydrodynamically efficient teardrop-shaped hull form pioneered by which, when combined with nuclear power, allowed to achieve higher speed with less horsepower.\n\n\"Triton\"s armament consisted of six Mark 60 torpedo tubes, four bow and two stern. The Mark 60 system was a -long hydraulic-launch tube that did not have power handling capability. The standard torpedo carried by \"Triton\" was the Mark 37, with a weapon load of ten forward and five aft. \"Triton\"s first commanding officer, \"Ned\" Beach, noted the torpedo load in the forward torpedo room could have been doubled with the removal of a single support girder.\n\n\"Triton\"s main air search radar was the electronically scanned, three-dimensional AN/SPS-26. This system had a range of and was capable of tracking aircraft up to an altitude of . Since it scanned electronically in elevation, it did not need a separate height-finding set. When not in use, the SPS-26 radar was lowered into its fairwater housing for stowage within \"Triton\"s massive sail (\"pictured\"). A submarine version of SPS-26, designated BPS-10, was under development at the time of \"Triton\"s construction, slated for eventual installation on \"Triton\".\n\n\"Triton\"s long-range, passive detecting-ranging sonar was the AN/BQR-7, which had a listening range up to for surfaced or snorkeling submarines, optimized to , with target tracking capability within 5 degrees of accuracy. The chin-mounted AN/BQR-2 passive sonar array supplemented the active BQS-4, with a range up to and a bearing accuracy of 1/10 of degree, allowing the BQR-2 to be used for fire control in torpedo attacks.\n\n\"Triton\"s target fire-control system (TFCS) was the Mark 101, a post-war development that incorporated target tracking and ranging data into a position keeper, with a pair of analyzers that automatically revised torpedo gyros and settings as the target position changed. This automation greatly simplified a targeting solution for a plotting party. Previously targeting solutions were manually estimated target bearings which were then fed into the Torpedo Data Computer (TDC), a method used throughout the Pacific War. However, while entirely capable of providing efficient fire control solutions against post-war non-nuclear hunter-killer submarines, the Mark 101 proved to be less responsive to the rapid changes associated with nuclear submarine operations.\n\nThe Number One periscope was \"Triton\"s navigational periscope, and it had a built-in sextant developed by the Kollmorgen Optical Company that allowed navigators to observe celestial bodies in order to obtain an accurate star fix to plot the ship's course and position.\n\nThe U.S. Navy ordered a \"large radar picket using the advanced two-reactor system,\" designated SCB 132, in October 1955 under the U.S. Department of Defense appropriation for Fiscal Year 1956. This 1956 shipbuilding program was significant because it included authorization for the construction of eight submarines in total, the largest such order since World War II. Along with \"Triton\", the FY-56 program included four additional nuclear-powered submarines – guided missile submarine \"Halibut\", the lead ship for the \"Skipjack\" class, and the final two \"Skate\"-class attack submarines, \"Sargo\" and \"Seadragon\". The 1956 program not only completed the final authorization for all of the U.S. Navy's first-generation nuclear submarines, but with \"Skipjack\", it also marked the initial authorization for a second-generation nuclear submarine. Finally, the 1956 program included the three submarines of the diesel-electric \"Barbel\" class, the last non-nuclear attack submarines to be built for the U.S. Navy. Henceforth, the U.S. Navy submarine service would be a nuclear-powered force.\n\n\"Triton\"s keel was laid down on 29 May 1956 in Groton, Connecticut, by the Electric Boat Division of the General Dynamics Corporation. Her length presented Electric Boat with many problems during her construction. She was so long her bow obstructed the slipway's railway facility, used for transporting material around the yard. Consequently, the lower half of her bow was cut away to facilitate yard operations, and the bow was re-attached just days prior to her launch. Similarly, the last of her stern was built on an adjoining slip and attached to the rest of the hull before \"Triton\"s launch. Her sail was found to be too high to go under the scaffolding, so the top was cut away and re-attached later.\n\nEven before her launch, there was considerable discussion of \"Triton\"s role beyond her radar picket mission. An internal Navy memorandum set forth four options for the submarine's extended use. These included configuration to serve as a command ship (SSCN) for a fleet or force commander, an advanced sonar scout for the fleet, a Regulus missile submarine (SSGN), or a minelaying submarine. However, with the exception of the command ship option, all of these proposed configurations required extensive modification of her original design.\n\nAnother potential mission was as an underwater tug, able to rescue disabled submarines under the Arctic ice pack. \"Triton\"s first commanding officer, Captain Edward L. Beach, Jr., requested plans be drawn up for this modification, which he characterized as \"easy and inexpensive\". Although there was consideration for a deployment to Arctic waters, there is no evidence that \"Triton\" was ever employed as an underwater tug.\n\n\"Triton\" was launched on 19 August 1958, with Louise Willis, the wife of Vice Admiral John Wills USN (ret.), as her sponsor. The principal address was delivered by Admiral Jerauld Wright, the Commander-in-Chief of the U.S. Atlantic Command (), the Commander-in-Chief of the U.S. Atlantic Fleet () and Supreme Allied Commander Atlantic () for NATO. Over 35,000 guests attended, the largest crowd to witness a submarine launching up to that time.\n\nOn 1 February 1959, \"Triton\" was provisionally accepted for service in the U.S. Navy, with Captain Beach, the Prospective Commanding Officer (PCO), now designated as Officer-in-Charge. \"Triton\" met several key milestones before her commissioning. On 8 February 1959, reactor No. 2 achieved initial criticality, while reactor No 1 achieved this milestone on 3 April 1959.\n\nTwo shipboard accidents occurred during \"Triton\"s post-launch fitting-out. On 2 October 1958, prior to the nuclear reactor fuel being installed, a steam valve failed during testing, causing a large cloud of steam that filled the number two reactor compartment, and on 7 April 1959, a fire broke out during the testing of a deep-fat fryer and spread from the galley into the ventilation lines of the crew's mess. Both incidents, neither nuclear related, were quickly handled by ship personnel, with Lt. Commander Leslie B. Kelly, the prospective chief engineering officer, being awarded the Navy and Marine Corps Medal for his quick action during the incident on 2 October.\n\n\"Triton\" began her sea trials on 27 September 1959. Over the next five days, the ship's systems and equipment were thoroughly tested under the overall direction of Hyman G. Rickover of the Bureau of Ships' Naval Reactors branch and Captain A. C. Smith, the Supervisor of Shipbuilding at Electric Boat. \"Triton\" generated on sea trials, reaching her design surface speed of , and achieved a surface speed well in excess of . \"Triton\" subsequently executed a four-hour, full-power submerged run and a crash-back maneuver. The only significant problem encountered during her initial sea trials was the overheating of the lubricating oil system for the starboard propellor shaft spring bearing. At the recommendation of Admiral Rickover, a hose was rigged to spray the bearing housing with a steady stream of sea water to keep the shaft cool, as well as a special watch set to monitor the temperature of the lube oil.\n\n\"Triton\" began her preliminary acceptance trials (PAT) on 20 September 1959. These trials were conducted under the supervision of Rear Admiral Francis Douglas McCorkle of the U.S. Navy's Board of Inspection and Survey (INSURV). After three days of at-sea tests, \"Triton\" was passed by the INSURV as being ready to enter service as a U.S. naval vessel.\n\n\"Triton\" was commissioned on 10 November 1959 with Captain Edward L. Beach, Jr. in command. Vice Admiral Bernard L. Austin, the Deputy CNO for Plans and Policy, made the keynote address, noting:\n\nAs the largest submarine ever built, her performance will be carefully followed by naval designers and planners the world over. For many years strategists have speculated on the possibilities of tankers, cargo ships and transports that could navigate under water. Some of our more futuristic dreamers have talked of whole fleets that submerge. \"Triton\" is a bold venture into this field.\n\nThe widow of the late Rear Admiral Willis A. \"Pilly\" Lent presented the original ship's bell from the first \"Triton\" at the new commissioning ceremony. The late Admiral Lent had been the earlier \"Triton\"s first commanding officer. A watercolor painting of the ship also was presented by the American Water Color Society. The final cost of building \"Triton\", less her reactors, nuclear fuel, and other related costs paid by the AEC, was $109,000,000 USD, making \"Triton\" the most expensive submarine ever built at the time of her commissioning.\n\n\"Triton\" was assigned to Submarine Squadron 10, the U.S. Navy's first all-nuclear force, based at State Pier in New London, Connecticut, under the command of Commodore Tom Henry. \"Triton\" subsequently completed torpedo trials at Naval Station Newport and conducted other special tests at the Norfolk Navy Base before returning to Electric Boat on 7 December 1959 in order to install special communications equipment, including a prototype of the BRA-3 towed communications buoy system housed in a large fairing located on the after end of the main deck. Work on \"Triton\" at Electric Boat was delayed as priority was given to completing the Navy's first two fleet ballistic missile (FBM) submarines, and , with the objective for both vessels to start their first nuclear deterrence patrols before the end of 1960.\n\nOn 20 January 1960, \"Triton\" got underway to conduct an accelerated series of at-sea testing. \"Triton\" returned on 1 February as preparations continued for her forthcoming shakedown cruise, scheduled for departure on 16 February 1960, which involved operating with the command ship , flagship of the U.S. Second Fleet, in northern European waters. On 1 February, Captain Beach received a message from Rear Admiral Lawrence R. \"Dan\" Daspit () instructing Beach to attend a top secret meeting at The Pentagon on 4 February 1960 that led to the execution of Operation Sandblast, the first submerged circumnavigation of the world.\n\nDuring her shakedown cruise, \"Triton\" successfully executed the first submerged circumnavigation of the world, code named Operation Sandblast, following the same track as the first circumnavigation led by Ferdinand Magellan. The mission's objectives were set forth in the published ship's log (\"pictured\"):\n\nFor purposes of geophysical and oceanographic research and to determine habitability, endurance and psychological stress – all extremely important to the Polaris program – it had been decided that a rapid round-the-world trip, touching the areas of interest, should be conducted. Maximum stability of the observing platform and unbroken continuity around the world were important. Additionally, for reasons of the national interest it had been decided that the voyage should be made entirely submerged undetected by our own or other forces and completed as soon as possible. TRITON, because of her size, speed and extra dependability of her two-reactor plant, had been chosen for the mission.\n\nThe actual mission was summarized by the U.S. Navy's \"Dictionary of American Naval Fighting Ships\":\n\n\"Triton\" put to sea on her shakedown cruise on 15 February 1960, bound for the South Atlantic. She arrived in the middle Atlantic off St. Peter and St. Paul Rocks on 24 February to commence a history-making voyage. Having remained submerged since her departure from the east coast, Triton continued on south towards Cape Horn, rounded the tip of South America, and headed west across the Pacific. After transiting the Philippine and Indonesian archipelagos and crossing the Indian Ocean, she rounded the Cape of Good Hope and arrived off the St. Peter and Paul Rocks on 10 April—60 days and 21 hours after departing the mid-ocean landmark. Only once did her sail break the surface of the sea, when she transferred a sick sailor to \"Macon\" (CA-132) off Montevideo, Uruguay, on 6 March. She arrived back at Groton, Connecticut, on 10 May, having completed the first submerged circumnavigation of the earth.\n\n\"Triton\"s globe-girdling cruise proved invaluable to the United States. Politically, it enhanced the nation's prestige. From an operational viewpoint, the cruise demonstrated the great submerged endurance and sustained high-speed transit capabilities of the first generation of nuclear-powered submarines. Moreover, during the voyage, the submarine collected reams of oceanographic data. At the cruise's conclusion, \"Triton\" received the Presidential Unit Citation and Captain Beach received the Legion of Merit from President Dwight D. Eisenhower.\n\n\"Triton\"s commanding officer during Operation Sandblast, Captain Edward L. Beach, also provided a unique perspective on the circumnavigation in the published log:\n\nThe noted historian Bern Dibner placed the significance of Operation Sandblast into historical context:\n\nAlso, in his 2000 book \"Ships Of Discovery And Exploration\", historian Lincoln P. Paine further noted:\n\nThe actual submerged circumnavigation occurred between 24 February and 25 April 1960, covering in 60 days and 21 hours at the average speed of while crossing the Equator on four different occasions. Also, the total duration of \"Triton\"s shakedown cruise was 84 days 19 hours 8 minutes, covering , and \"Triton\" remained submerged for a total of 83 days 9 hours, covering during her maiden voyage.\n\nThe \"New York Times\" described \"Triton\"s submerged circumnavigation of the world as \"a triumph of human prowess and engineering skill, a feat which the United States Navy can rank as one of its bright victories in man's ultimate conquest of the seas.\"\n\nFollowing her post-shakedown availability (PSA), \"Triton\" assumed her duties as a radar picket submarine in August 1960. She deployed to northern European waters with the Second Fleet to participate in NATO exercises oriented around detecting and intercepting Soviet bombers overflying the Arctic. \"Triton\" also participated in NATO exercises against British naval forces led by the aircraft carriers and under the command of Rear Admiral Sir Charles Madden, RN. For two days during these NATO exercises, Rear Admiral Thomas H. Moorer and his flag lieutenant, Lt. William P. Lawrence, were aboard \"Triton\" to observe its submarine radar-picket operations. At the time, Moorer was serving as Commander Carrier Division Six which included the carriers and . \"Triton\" completed its first overseas deployment with a port visit to Bremerhaven, West Germany, the first by a nuclear-powered ship to a European port, from 2–9 October 1960, with an estimated 8,000 touring the boat during this port-call.\n\nFor the first half of 1961, \"Triton\" conducted operational patrols and training exercises with the Atlantic Fleet. This included an at-sea exercise involving low-power testing in support of the development of a proposed natural circulation reactor (NCR). She also deployed to monitor the Soviet 50-megaton hydrogen bomb initiation at Novaya Zemlya in the Arctic Ocean during late October 1961.\n\nDuring this period, the rising threat posed by Soviet submarine forces increased the Navy's demands for nuclear-powered attack submarines with antisubmarine warfare (ASW) capability. Following the development of the carrier-based Grumman WF-2 Tracer airborne early warning aircraft, \"Triton\"s AN/SPS-26 3-D long-range air search radar was no longer needed, and the development of the submarine version of this 3-D radar system, the BPS-10, was canceled in 1960. Accordingly, upon the demise of the Navy's radar picket submarine program, \"Triton\" was redesignated SSN-586 on 1 March 1961.\n\n\"Triton\" entered the Portsmouth Naval Shipyard in June 1962 for conversion to an attack submarine. Her crew complement was reduced from 172 to 159. She was overhauled and refueled at Groton, Connecticut, from September 1962 to January 1964, which included modification to serve as the flagship for . Since the Navy no longer had any plans to use \"Triton\"s radar picket capability, her SPS-26 radar set was replaced by a two-dimensional AN/BPS-2 air search radar, with \"Triton\" now providing the fleet with an at-sea air strike control capability.\nBecause she subsequently served as 's flagship following her overhaul, one area of continuing speculation is whether \"Triton\" was part of the National Emergency Command Post Afloat (NECPA) program. NECPA was tasked to provide afloat facilities for the President of the United States in case of an emergency or war, with the command cruisers (\"pictured\") and assigned to perform this mission.\n\n\"Triton\" had a number of attributes that made her a potential NECPA platform. Her size allowed ample room for additional shipboard systems and personnel accommodations. Her designed speed provided the capability for rapid transit, and her nuclear power plant offered virtually unlimited endurance and range. The Combat Information Center (CIC) provided substantial command and control capabilities as did the communication buoy system that could receive and send radio transmissions while submerged. As she was a submarine, \"Triton\" offered superior protection against nuclear-biological-chemical (NBC) contaminants over surface ships or an airborne command centre. However, the record remains unclear if such an explicit NECPA conversion was ever undertaken for \"Triton\".\n\nIn March 1964, upon completion of her overhaul, \"Triton\"s home port was changed from New London, to Norfolk. On 13 April 1964, she became the flagship for . On 20 January 1965, \"Triton\" rescued the pilot and a passenger of a charter aircraft that had ditched in the Atlantic Ocean off St. Croix in the Virgin Islands. \"Triton\" was relieved as 's flagship by the on 1 June 1967. Eleven days later, \"Triton\" was shifted to her original home port of New London, Connecticut.\n\n\nOn 10 May 1960, Secretary of the Navy William B. Franke presented the Presidential Unit Citation (PUC) to \"Triton\" for Operation Sandblast, the first submerged circumnavigation of the world. Chief Torpedoman's Mate Chester Raymond Fitzjarrald, the chief of the boat, accepted the PUC on behalf of \"Triton\"s officers and crew. The citation reads:\n\nFor meritorious achievement from 16 February 1960 to 10 May 1960. During this period the TRITON circumnavigated the earth submerged, generally following the route of Magellan's historic voyage. In addition to proving the ability of both crew and nuclear submarine to accomplish a mission which required almost three months of submergence, TRITON collected much data of scientific importance. The performance, determination and devotion to duty of the TRITON's crew were in keeping with the highest traditions of the naval service. All members of the crew who made this voyage are authorized to wear the Presidential Unit Citation ribbon with a special clasp in the form of a golden replica of the globe.\n\nUp to that time, this was only the second time that a U.S. Navy vessel had been awarded the Presidential Unit Citation for a peacetime mission, with the nuclear submarine receiving the first peacetime PUC in recognition of Operation Sunshine, the first submerged voyage under the North Pole in 1958. To commemorate the first submerged circumnavigation of the world, all \"Triton\" personnel who made that voyage were authorized to wear their Presidential Unit Citation ribbon with a special clasp in the form of a golden replica of the globe (\"pictured\").\n\nThe citation reads:\n\nFor exceptionally meritorious service during a period in 1967. USS \"Triton\" conducted important and arduous independent submarine operations of great importance to the national defense of the United States. The outstanding results achieved during the highly successful operations attest to the exceptional professional skill, resourcefulness and ingenuity of \"Triton\"s officers and men. Their inspiring performance of duty throughout was in keeping with the highest traditions of the naval service.\n\nDue to cutbacks in defense spending, as well as the expense of operating her twin nuclear reactors, \"Triton\"'s scheduled 1967 overhaul was canceled, and the submarine—along with 60 other vessels—was slated for inactivation. While \"Triton\"s twin reactor plant was designed to be refueled by a submarine tender like other U.S. nuclear submarines, because of the complexity of her zirconium-clad fuel elements, \"Triton\"s previous re-fueling had been done in a shipyard during her 1962–1964 overhaul. Although new fuel elements were procured and available for installation, \"Triton\"s overhaul was canceled, a source of controversy. One speculation suggests that the cancellation of \"Triton\"s overhaul allowed funds to be redirected for the repairs to the supercarrier which had been extensively damaged off Vietnam.\nFrom October 1968 through May 1969, she underwent preservation and deactivation processes, and she was decommissioned on 3 May 1969. \"Triton\" became the U.S. Navy's first nuclear-powered submarine to be taken out of service, and second in the world, after the Soviet Navy's November-class submarine \"K-27\" in 1968. On 6 May 1969, \"Triton\" departed New London under tow and proceeded to Norfolk, Virginia, where she was placed in the reserve fleet. She remained berthed at Norfolk or at the St. Julien's Creek Annex of Norfolk Naval Shipyard in Portsmouth, Virginia, into 1993. She was stricken from the Naval Vessel Registry on 30 April 1986. In August 1993, the hulks of the ex-\"Triton\" and the ex-\"Ray\" were towed by the salvage tug \"Bolster\" to the Puget Sound Naval Shipyard (PSNS), in Bremerton, Washington, arriving on 3 September 1993, to await their turn in the Nuclear Powered Ship and Submarine Recycling Program (SRP).\n\nEffective 1 October 2007, ex-\"Triton\" landed on the keel resting blocks in the drydock basin to begin recycling (\"pictured\"). The long delay in the disposal of ex-\"Triton\" has been attributed to the complexity of her dual reactor plant. Final recycling was completed effective 30 November 2009.\n\nIn the eight days prior to \"Triton\"s departure on her around-the-world submerged voyage, Captain Beach approached Lt. Tom B. Thamm, \"Triton\"s Auxiliary Division Officer, to design a commemorative plaque for their upcoming voyage, as well as the first circumnavigation led by Portuguese explorer Ferdinand Magellan. The plaque's eventual design consisted of a brass disk about in diameter, bearing a sailing ship reminiscent of Magellan's carrack, \"Trinidad\", above the submarine dolphin insignia with the years 1519 and 1960 between them, all within a laurel wreath. Outside the wreath is the motto \"AVE NOBILIS DUX, ITERUM FACTUM EST\" (\"Hail Noble Captain, It Is Done Again\").\n\nCommodore Tom Henry, commanding Submarine Squadron 10, supervised the completion of the plaque. The carving of the wooden form was done by retired Chief Electrician's Mate Ernest L. Benson at New London. The actual molding of the plaque was done by the Mystic Foundry.\n\nDuring the homeward leg of her around-the-world voyage, \"Triton\" rendezvoused with the destroyer on 2 May 1960 off Cadiz, Spain, the departure point for Magellan's earlier voyage. \"Triton\" broached, and \"John W. Weeks\" transferred the finished plaque to \"Triton\" for transport back to the United States. The plaque was subsequently presented to the Spanish government by John Davis Lodge, the United States Ambassador to Spain. This plaque is located at the City Hall in Sanlúcar de Barrameda, Spain, and it is mounted on the wall of the city hall with a marble slab memorializing the 1960 \"Triton\" submerged circumnavigation.\n\nDuplicates of the Triton Plaque were also presented to the Mystic Seaport Museum in Mystic, Connecticut, and the Naval Historical Association in Washington, D.C., as well as the U.S. Navy Submarine School and the U.S. Navy Submarine Force Library and Museum, both located in Groton, Connecticut.\n\nTriton Light is a navigational beacon on the seawall of the United States Naval Academy in Annapolis, Maryland, where the Severn River meets Spa Creek and the Annapolis harbor. It was donated to the Academy and named for the Greek god by the United States Naval Academy Class of 1945. The crew of \"Triton\" provided samples of water taken from the 22 seas through which their ship had passed during their submerged 1960 circumnavigation. These samples filled a globe built into the Triton Light, and the naming of the light and significance of the globe are explained in a commemorative marker.\n\nBeach Hall is the new headquarters for the United States Naval Institute which was dedicated on 21 April 1999 (\"pictured\"). The facility is named after Captain Edward L. Beach, Sr., who served as the Institute's secretary-treasurer, and his son, Edward L. Beach, Jr., who commanded \"Triton\" during Operation Sandblast. The dive wheel from \"Triton\"s conning tower is on display in the lobby of Beach Hall.\n\n\"Triton\" was the 2003 inductee into the Submarine Hall of Fame following her nomination by the Tidewater chapter and Hampton Roads Base of the United States Submarine Veterans, Inc. (USSVI). A shadow box filled with \"Triton\" memorabilia was placed in Alcorn Auditorium of Ramage Hall located at the U.S. Navy Submarine Learning Center, Naval Station Norfolk.\n\nUSS \"Triton\" Recruit Barracks (Ship 12) was dedicated in ceremonies at the U.S. Navy's Recruit Training Command, Naval Station Great Lakes, near North Chicago, Illinois, on 25 June 2004. The facility honors the memory of two submarines named \"Triton\" and includes memorabilia from both. Triton Hall is the fifth barracks constructed under the RTC Recapitalization Project, covering 172,000 square feet (15,979 square meters) in floor space. The facility is designed to accommodate 1056 recruits, and it includes berthing, classrooms, learning resource centers, a galley, a quarterdeck, and a modern HVAC system. On 17 May 2012, in a dedication ceremony, the long-missing ship's bell was added to the collection of artifacts in Recruit Training Command's USS Triton recruit barracks quarterdeck (\"pictured\").\n\nThe USS \"Triton\" Submarine Memorial Park is located on the Columbia River, at the end of Port of Benton Boulevard in north Richland, Washington. Its purpose is \"to establish a permanent park in north Richland in recognition of all the decommissioned reactor cores off-loaded at the Port's barge slip, transported and stored at the Hanford Site.\"\n\nThe park features \"Triton\"s sail superstructure (\"pictured\") and an information display on the history of \"Triton\". The park also serves as a tourist attraction, especially due to its location, since Hanford is the resting place of spent reactor cores from several Navy ships. Planning called for the sail to be cut up for transport and reassembled at the park site. Ground-breaking was initially scheduled to take place on 3 April 2008, with the dedication ceremony set for 19 August 2008 and a Fall 2009 start-date for construction. On 23 October 2009, the Port of Benton encased \"Triton\"s conning tower in concrete at its new USS Triton Submarine Memorial Park in north Richland, Washington. In mid-December 2009, the final pieces of \"Triton\"s sail were welded together at the park's site. During the 11 August 2010 Port of Benton commission meeting, it was reported that bids for the first phase, which includes the park's electrical lighting system and the pouring the concrete around \"Triton\"s sail, would be announced shortly by the port authority. The second phase would involve the park's landscaping, and the third phase would be the installation of a parking lot. The park is part of the Richland Riverfront Trail, a marked hiking trail that focuses on the state of Washington's contribution to the nuclear history of the United States, and it connects to the Sacagawea Heritage Trail. The USS Triton Submarine Memorial Park is located off George Washington Way near the Columbia River, and it was formally dedicated on 10 November 2011, the 52nd anniversary of the commissioning of the USS \"Triton\".\n\nIn 2011, the USS \"Triton\", Operation Sandblast, and Captain Edward L. Beach, Jr., were included in the Technology for the Nuclear Age: Nuclear Propulsion display for the Cold War exhibit at the U.S. Navy Museum in Washington, DC.\n\n\"Triton\"s submerged circumnavigation, Operation Sandblast, was the subject of the ABC television series \"Expedition!\" broadcast on Tuesday, 14 February 1961. Hosted by John D. Craig, this episode was titled \"Saga of the \"Triton\"\", and it featured film footage from Operation Sandblast with voice-over narration extracted from Captain Beach's logbook.\n\n\"Triton\" is referenced briefly in three popular Cold War novels. In \"The Last Mayday\" by Keith Wheeler (1968), \"Triton\" is depicted as participating in a submarine training exercise at the beginning of the novel, with special notice made of her large, rectangular sail. In the 1978 novel \"Cold is the Sea\" by Edward L. Beach, the second sequel to his 1955 best-seller \"Run Silent, Run Deep\", \"Triton\" is mentioned several times. Also, the under-ice towing capability that was considered for \"Triton\" served as a key plot point for the novel. Finally, in \"The Hunt for Red October\" by Tom Clancy, the biographical background for Marko Ramius mentions that, while commanding a \"Charlie\"-class submarine, Ramius had \"hounded [\"Triton\"] mercilessly for twelve hours\" in the Norwegian Sea. Subsequently, Ramius \"would note with no small satisfaction that the \"Triton\" was soon thereafter retired, because, it was said, the oversized vessel had proven unable to deal with the newer Soviet designs.\"\n\nTwo films of the period, \"Voyage to the Bottom of the Sea\" and \"Around the World Under the Sea\", dramatized globe-circling submerged voyages similar to Operation Sandblast. Also, in the teaser of the episode \"Mutiny\" of the \"Voyage to the Bottom of the Sea\" television series, broadcast on 11 January 1965, the fictional nuclear submarine \"Neptune\" is on her shakedown cruise, under the supervision of Admiral Harriman Nelson (Richard Basehart), and when the submarine's port shaft bearing begins overheating, Admiral Nelson orders a hose be rigged to cool the port shaft down with sea water, the same solution Admiral Rickover had suggested during \"Triton\"s sea trials.\n\nThe 1960 \"The Button-Down Mind of Bob Newhart\" comedy album (\"pictured\") included a sketch entitled \"The Cruise of the U.S.S. Codfish\" which was a monologue involving the final address by the captain to the crew of a nuclear-powered submarine after completing a two-year-long, around-the-world underwater voyage. Bob Newhart noted in a 2006 interview that:\n\nCaptain Beach reportedly played \"The Cruise of the U.S.S. Codfish\" over the ship's public address system during \"Triton\"s first overseas deployment in the Fall of 1960. Antigua-Barbuda issued a commemorative stamp of \"Triton\"s 1960 submerged circumnavigation. Also, \"Triton\" was the name of one of the submersibles featured in the Submarine Voyage attraction at Disneyland which operated from 1959 to 1998.\n\nThe 50th anniversary of Operation Sandblast and \"Triton\"s submerged circumnavigation of the world was celebrated on 10 April 2010, during the 2010 Submarine Birthday Ball held at the Foxwoods Resort Casino in Mashantuket, Connecticut, with Master Chief Petty Officer of the Navy (MCPON) Rick D. West delivering opening remarks (\"pictured\") to the 2,200 attendees. The U.S. Navy Submarine Force Library and Museum sponsored additional events and activities, entitled \"9,000 Leagues Under the Sea,\" between 10–12 April and 14–18 April 2010.\n\nAlso, on 9 April 2010, retired Admiral Henry G. Chiles, Jr., who served in \"Triton\" from 1963–1966, was the keynote speaker at the graduation class of the Basic Enlisted Submarine School at the New London Naval Submarine Base in Groton, Connecticut. The graduation class was named in honor of \"Triton\", and each graduate received a certificate of course completion and a commemorative coin celebrating the 50th anniversary of \"Triton\"s submerged circumnavigation. The Dolphin Scholarship Foundation used the 50th anniversary of Operation Sandblast to promote its \"Race Around the World\" fund-raising program to support its Dolphin Scholarship program. Finally, former members of \"Triton\"s crew received commemorative souvenirs of the ship's pressure hull at their 2010 re-union.\n\nOn 25 April 2010, the University of Texas Marine Science Institute posted a radio program article on its Science and the Sea web site commemorating Operation Sandblast and \"Triton\".\n\nFor the 50th anniversary of Operation Sandblast, writer-historian Carl LaVO wrote \"Incredible Voyage\" for the June 2010 edition of \"Naval History\" magazine, and John Beach wrote \"The First Submerged Circumnavigation\" for the April 1960 issue of \"The Submarine Review\", the official magazine of the Naval Submarine League. Mr. Beach is the nephew of Captain Edward L. Beach, the commanding officer of the USS \"Triton\" during Operation Sandblast. Finally, the Naval Institute Press published \"Beneath the Waves\" by Dr. Edward F. Finch, a 2010 biography of the late Captain Beach, which includes extensive coverage of Operation Sandblast.\n\nThe legacy of Operation Sandblast on its 50th anniversary was summarized by retired Captain James C. Hay who had served on \"Triton\" during its historic submerged around-the-world voyage. On the editorial page of the April 1960 issue of \"The Submarine Review\", the official magazine of the Naval Submarine League, Captain Hay noted:\n\nIt is truly a cruise which tested the crew's mettle and proved the skipper's \"tenacity\". More than that, however, it again proved to all who cared to listen that the US Navy could go anywhere, at any time, and do what ever was required. It's a good sea story about doing what had to be done. On the fiftieth anniversary of the First Submerged Circumnavigation it's a good thing to do to re-read about one of the forerunners of all we're done since.\n\n\n"}
{"id": "33010213", "url": "https://en.wikipedia.org/wiki?curid=33010213", "title": "Water of Life (Christianity)", "text": "Water of Life (Christianity)\n\nIn Christianity the term \"water of Life\" ( \"hydōr zōēs\") is used in the context of \"living water\", specific references appearing in the Book of Revelation ( and ), as well as the Gospel of John. In these references, the term \"Water of Life\" refers to the Holy Spirit.\n\nThe passages that comprise are sometimes referred to as the Water of Life Discourse. These references in the Gospel of John are also interpreted as the \"Water of Life\".\n\nThe term is also used when water is poured during Baptismal prayers, praying for the Holy Spirit, e.g., \"Give it the power to become water of life\".\n\nThe reference to Water of Life in appears in the context of New Jerusalem and states:\n\nThe Revelation reference is interpreted as the Holy Spirit. The Catechism of the Catholic Church, item 1137, considers it \"one of most beautiful symbols of the Holy Spirit\".\n\nThe common theme of thirst for the Water of Life in the Book of Revelation and the Gospel of John may be summarized as follows:\n\nThe use of the term \"Water of Life\" in Revelation 20 is part of the \"theme of life\" in the book of Revelation, other instances being the \"Book of Life\" in Revelation , and the \"Tree of Life\" in , and . John R. W. Stott relates this theme to Eternal Life in : \"And this is life eternal, that they should know thee the only true God, and him whom thou didst send, Jesus Christ\".\n\nIn the Gospel of John some references to water, as in John 4:15, are traditionally identified as the \"Water of Life\" being the Holy Spirit.\n\nThe passages that comprise , and relate the episode of the Samaritan woman are sometimes referred to as the \"Water of Life Discourse\". The Water of Life Discourse is the second among the seven discourses in the Gospel of John that pair with the seven signs in that gospel.\n\nAnother discourse, called the Bread of Life Discourse appears in . On their own, each of the discourses on the \"Water of Life\" and the \"Bread of Life\" are key examples of \"single theme discourses\" in the Gospel of John. However, these two discourses in the Gospel of John complement each other to form the theme of \"Christ as the Life\".\n\nAccording to W. E. Vine, this theme of \"Christ as the Life\" relates to\n\n"}
{"id": "6641584", "url": "https://en.wikipedia.org/wiki?curid=6641584", "title": "Xylotechnigraphy", "text": "Xylotechnigraphy\n\nXylotechnigraphy is an architectural term for a decorative treatment to wood. By staining, finishing, and graining, the wood resembles a more expensive or finer type. The process was invented by A. F. Brophy and patented in England in 1871.\n\nIn a paper read before a meeting of the Royal Institute of British Architects three years after xylotechnigraphy was patented, G. T. Robinson described it as \"in principle exceedingly simple. ... To ceilings, doors, dados in our private houses, to partitions and fittings of our banks and commercial offices, this process is, I conceive, exceedingly applicable, and to our larger and less movable pieces of furniture it is not misapplied, though I confess to a lurking dislike to it in those lesser articles to which true inlay, by reason of their smaller process, seems more aesthetically appropriate.\"\n\nIn the same paper, Robinson quoted the patent in which Brophy detailed the process:\n\nIn order to stain wood in various colours, according to any suitable design, leaving, if desired, parts of the wood unstained, so as to obtain an imitation of inlay, I proceed as follows: I first apply a varnish or solution which will fill the pores of the wood, and exclude the staining liquid from such parts of the surface as are to remain unstained, then when the varnish or solution is dry, I apply over the whole surface the lightest stain I intend to use; this stain being dry I again apply the varnish, or stopping, coating with it such parts of the surface as I desire to retain of a colour corresponding to the lightest stain, and so I proceed until the desired effect is obtained, the last stain applied being usually black or a very dark stain. The surface having been cleared off may, if desired, be varnished or polished all over, or it may remain as it is left by the last staining process.\n\n\n"}
{"id": "41614196", "url": "https://en.wikipedia.org/wiki?curid=41614196", "title": "Z Electric Vehicle", "text": "Z Electric Vehicle\n\nZ Electric Vehicle Corporation (ZEV) is an American owned and operated electric scooter manufacturer and distributor based in Morgantown, West Virginia, with test facilities in Waynesboro, Pennsylvania. Its vehicles are assembled in both the US and China with assembly shops in the USA, Australia, and Vietnam. ZEV says its electric scooters are the world's most powerful, fastest, and have the longest range.\n\nZev began as a garage project for Darus Zehrbach Jr., in his home garage in 2006. He saw the potential for a global market, while traveling the world as a mechanical engineer. In 2009 operations moved to an airport hangar in Waynesburg, PA before moving back to Morgantown, WV in 2013 where the company is currently located. Zehrbach's son, Darus Zehrbach III, joined the family business at age 24 after earning an MBA from West Virginia University.\n\nZEV has 19 electric scooter models The scooters range from the ZEV 3600 with a top speed of and a range of , to the 10 LRC, with a range (at 65% power) of , and a top speed of .\n\nZEV also makes a line of electric motorcycles, full fairing sports bikes referred to as their M-S line.\nZEV uses a gearless electromagnet powered hub style motor and powers its scooters with lithium batteries, as well as less costly lead/sodium silicate batteries. ZEV's large-diameter, multiphase motors, are cooled using an oil bath arrangement, which lets ZEV run a large motor at high speeds without the power sapping and motor-destroying effects of excess heat.\n\nZEV claims that its motors are among the world's most efficient in that they have more range than other competing bikes with up to 30% more battery capacity. ZEV competed in the 2016 Vetter Challenge, an Ohio efficiency rally—though ZEV's scooters were disqualified for failing to finish.\n\n\"What we try to do is offer higher speeds and performance than the competition, generally at 25 percent less price. We're the only company that uses hub motors, built in the back wheel. There's no chain, there's no belt drive and no air being forced through the motor, so you'll find our bikes are significantly quieter than anybody else's electric bike,\" said Zehrbach.\nIn addition to sales in the US, as of 2013 ZEV was exporting scooters to 24 countries worldwide, and has won awards for exports and marketing. With 19 models of scooter, and a motorcycle, ZEV is reported to have the widest range of models of any electric motorcycle and scooter manufacturing company in the world.\n\nIn 1994 a Third Circuit federal appellate court upheld the conviction of Darus Zehrbach, and an associate on charges of bankruptcy fraud. Mr. Zehrbach was accused of rigging an auction of the assets of a bankrupt aircraft manufacturing company, by paying off others to refrain from bidding. Mr. Zehrbach was sentenced to imprisonment for 21 months, followed by three years of supervised release.\n\nIn 2004 a Fourth Circuit federal appellate court confirmed another conviction of Mr. Zehrbach and an associate of conspiracy to defraud buyers of aircraft kit engines. Zehrbach was found to have promised to deliver aircraft engine kits that did not yet exist. While negotiating the deals Mr. Zehrbach was out on bond during the appeal of the previous bankruptcy fraud conviction. He did not tell prospective buyers that he and a co-conspirator had previous convictions for crimes involving financial dishonesty, nor that he would be incarcerated during the period he had promised delivery. The court concluded the engines did not exist, he had no intention of delivering them, and simply took the buyers’ money and never repaid it. The court affirmed his sentence of 54 months imprisonment, and required restitution of $224,148.10.\n\n"}
