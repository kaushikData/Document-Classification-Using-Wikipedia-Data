{"id": "25116192", "url": "https://en.wikipedia.org/wiki?curid=25116192", "title": "ANSI C12.19", "text": "ANSI C12.19\n\nANSI C12.19 is the American National Standard for Utility Industry End Device Data Tables\n\nThis standard defines a table structure for utility application data to be passed between an end device and a computer. The \"end device\" is typically an electricity meter, and the \"computer\" is typically a hand-held device carried by a meter reader, or a meter communication module which is part of an Automatic Meter Reading system. C12.19 does not define end device design criteria nor specify the language or protocol used to transport that data. There are however related ANSI standards which do specify the transportation of these tables. ANSI C12.18 describes the communication of C12.19 tables over an optical port. ANSI C12.21 describes the communication of C12.19 tables over a modem. ANSI C12.22 describes the communication of C12.19 tables over a network.\n\nThe purpose of the tables is to define structures for transporting data to and from end devices. A related standard, IEC 61968 defines a CIM information model for energy data.\n\nThe structures were originally known as the \"Tucker Tables\" (after Richard Tucker.) The tables were developed under the auspices of the American National Standards Institute, C12, SC17, WG2. The standard is sponsored by NEMA, and also published as MC12.19 and IEEE Std 1377.\n\nPopularity of the tables has grown over the years such that all electric meter vendors in North America now support and use the tables in their products.\n\nThe committee (working group 2) is currently chaired by Avygdor Moise and released a revision 2 of the standard in 2008, revision 2.1 of the standard in 2012.\n\nEffective 2014, the object identifiers (device classes) and XML documents (TDLs/EDLs) of this standard are managed and registered by the Energy Communications Management eXchange® (ECMX®) under the supervision of the North American End Device Registration Authority™ (NAEDRA).\n\n"}
{"id": "34019297", "url": "https://en.wikipedia.org/wiki?curid=34019297", "title": "Alkaline anion exchange membrane fuel cell", "text": "Alkaline anion exchange membrane fuel cell\n\nAn alkaline anion exchange membrane fuel cell (AAEMFC) is a type of alkaline fuel cell that uses an anion exchange membrane to separate the anode and cathode compartments.\n\nAlkaline fuel cells (AFCs) are based on the transport of alkaline anions, usually hydroxide , between the electrodes. Original AFCs used aqueous potassium hydroxide (KOH) as an electrolyte. The AAEMFCs use instead a polymer membrane that transports hydroxide anions.\n\nThis type of fuel cell is also called hydroxide exchange membrane fuel cell (HEMFC), anion-exchange membrane fuel cell (AEMFC), or alkaline membrane fuel cell (AMFC).\n\nIn an AAEMFC, the fuel, hydrogen or methanol, is supplied at the anode and oxygen through air, and water are supplied at cathode. Fuel is oxidized at anode and oxygen is reduced at cathode. At cathode, oxygen reduction produces hydroxides ions (OH) that migrate through the electrolyte towards the anode. At anode, hydroxide ions react with the fuel to produce water and electrons. Electrons go through the circuit producing current.\n\nElectrochemical reactions when hydrogen is the fuel\n\nAt Anode: H + 2OH → 2HO + 2e\n\nAt cathode: O + 2HO + 4e → 4OH\n\nElectrochemical reactions when methanol is the fuel\n\nAt anode: CHOH + 6OH → CO + 5HO + 6e-\n\nAt cathode: 3/2O + 3HO + 6e → 6OH\n\nThe alkaline fuel cell used by NASA in 1960s for Apollo and Space Shuttle program generated electricity at nearly 70% efficiency using aqueous solution of KOH as an electrolyte. In that situation, CO coming in through oxidant air stream and generated as by product from oxidation of methanol, if methanol is the fuel, reacts with electrolyte KOH forming CO/HCO. Unfortunately as a consequence, KCO or KHCO precipitate on the electrodes. However, this effect has found to be mitigated by the removal of cationic counterions from the electrode, and carbonate formation has been found to be entirely reversible by several industrial and academic groups, most notably Varcoe. Low-cost CO systems have been developed using air as the oxidant source. In alkaline anion exchange membrane fuel cell, aqueous KOH is replaced with a solid polymer electrolyte membrane, that can conduct hydroxide ions. This could overcome the problems of electrolyte leakage and carbonate precipitation, though still taking advantage of benefits of operating a fuel cell in an alkaline environment. In AAEMFCs, CO reacts with water forming HCO, which further dissociate to HCO and CO. The equilibrium concentration of CO/HCO is less than 0.07% and there is no precipitation on the electrodes in the absence of cations (K, Na). The absence of cations is, however, difficult to achieve, as most membranes are conditioned to functional hydroxide or bicarbonate forms out of their initial, chemically stable halogen form, and may significantly impact fuel cell performance by both competitively adsorbing to active sites and exerting Helmholtz-layer effects.\n\nIn comparison, against alkaline fuel cell, alkali anion exchange membrane fuel cells also protect the electrode from solid carbonate precipitation, which can cause fuel (oxygen/hydrogen) transport problem during start-up. \n\nThe large majority of membranes/ionomer that have been developed are fully hydrocarbon, allowing for much easier catalyst recycling and lower fuel crossover. Methanol has an advantage of easier storage and transportation and has higher volumetric energy density compared to hydrogen. Also, methanol crossover from anode to cathode is reduced in AAEMFCs compared to PEMFCs, due to the opposite direction of ion transport in the membrane, from cathode to anode. In addition, use of higher alcohols such as ethanol and propanol is possible in AAEMFCs, since anode potential in AAEMFCs is sufficient to oxidize C-C bonds present in alcohols.\n\nThe biggest challenge in developing AAEMFCs is the anion exchange membrane (AEM). A typical AEM is composed of a polymer backbone with tethered cationic ion-exchange groups to facilitate the movement of free OH ions. This is the inverse of Nafion used for PEMFCs, where an anion is covalently attached to the polymer and protons hop from one site to another. The challenge is to fabricate AEM with high OH ion conductivity and mechanical stability without chemical deterioration at elevated pH and temperatures. The main mechanisms of degradation are Hofmann elimination when β-hydrogens are present and direct nucleophilic attack by OH ion at the cationic site. One approach towards improving the chemical stability towards Hofmann elimination is to remove all β-hydrogens at the cationic site. All these degradation reactions limit the polymer backbone chemistries and the cations that can be incorporated for developing AEM.\n\nAnother challenge is achieving OH ion conductivity comparable to H conductivity observed in PEMFCs. Since the diffusion coefficient of OH ions is half that of H (in bulk water), a higher concentration of OH ions is needed to achieve similar results, which in turn needs higher ion exchange capacity of the polymer. However, high ion exchange capacity leads to excessive swelling of polymer on hydration and concomitant loss of mechanical properties.\n\nManagement of water in AEMFCs has also been shown to be a challenge. Recent research has shown that careful balancing of the humidity of the feed gases significantly improves fuel cell performance.\n\n"}
{"id": "27109599", "url": "https://en.wikipedia.org/wiki?curid=27109599", "title": "Bely Rast High Voltage Research Station", "text": "Bely Rast High Voltage Research Station\n\nBelvy Rast High Voltage Research Station is a facility for the development of high voltage equipment in Russia, situated at Belvy Rast in Moscovskaya Oblast. Built in 1966, the equipment of the 750 kV- and 1150 kV-lines in Russia and other parts of the former Soviet Union were first developed and tested at this facility, as well as the equipment for the never completed HVDC Ekibastuz-Centre. The facility also has an unused 1150 kV AC and a 1500 kV DC line. The DC line was the prototype of the never finished HVDC Ekibastuz–Centre.\n\n"}
{"id": "51010641", "url": "https://en.wikipedia.org/wiki?curid=51010641", "title": "Carrizo-Wilcox Aquifer", "text": "Carrizo-Wilcox Aquifer\n\nThe Carrizo-Wilcox Aquifer is an aquifer in Texas, U.S.\n"}
{"id": "12332868", "url": "https://en.wikipedia.org/wiki?curid=12332868", "title": "Columbia Non-neutral Torus", "text": "Columbia Non-neutral Torus\n\nThe Columbia Non-neutral Torus (CNT) is a small stellarator at the Columbia University Plasma Physics Laboratory designed by Thomas Sunn Pedersen with the aid of Wayne Reiersen and Fred Dahlgren of the Princeton Plasma Physics Laboratory to conduct the first investigation of non-neutral plasmas confined on magnetic surfaces. The experiment, which began operation in November 2004, is funded by the National Science Foundation and the United States Department of Energy in the form of a Faculty Early Career Development (CAREER) award.\n\nCNT, which is housed in a cylindrical vacuum chamber made of 316 stainless steel, measures 60 inches in diameter and stands 75 inches tall. The empty chamber is capable of reaching a pressure of 2x10-10 Torr. \n\nCNT is unique in its simple geometry. Magnetic surfaces are created using only 4 electromagnetic coils - two interlocking coils inside the chamber, and two poloidal field coils outside the chamber. The two interlocking coils have a radius of .405m, and the angle between them can be manually selected to be 64°, 78°, or 88°, allowing for different shear and rotational transform values, and magnetic surface configuration. The poloidal field coils have a radius of 1.08 m. The coils are powered by a 200 kW power supply and are capable of producing magnetic fields of 0.01-0.2T. The configuration of CNT creates a very low aspect ratio of 1.9, the lowest of any stellarator built.\n\nThomas Sunn Pedersen is the principal investigator of CNT, which studies several areas of theoretical and experimental non-neutral plasma physics. These include the equilibrium of non-neutral plasmas, transport and confinement, and ion-related instabilities. The CNT theory program is run by Pedersen and Prof. Allen Boozer, also at Columbia University.\n\nFirst studies on CNT showed the successful creation of magnetic\nsurfaces with the simple four coil design. At sufficiently low neutral pressures and sufficiently high magnetic field strengths, the plasmas are essentially pure electron plasmas and are macroscopically stable with confinement times of up to 20 ms. Transport is driven by collisions with neutrals as well as E x B drift along insulating\nrods inserted into the plasma. At higher neutral pressures (10 Torr and above), an ion related instability is observed, with a frequency in the 10–50 kHz range, and a poloidal mode number m=1.\n\nThe CNT group installed a conducting boundary in August 2007\nto study its effects on confinement times, and to allow measurements in the absence of internal rods. Future plans for CNT\ninclude the study of electron-positron plasmas confined on magnetic\nsurfaces and further studies of partly neutralized plasmas.\n\n\n"}
{"id": "51791674", "url": "https://en.wikipedia.org/wiki?curid=51791674", "title": "Commodity product Markup Language", "text": "Commodity product Markup Language\n\nCommodity product Markup Language (CpML) is an industry standard used in wholesale energy trading. CpML is an XML-based business mark-up language used for interoperable representation of energy trades for the purpose of post-deal-execution processes like deal confirmation and regulatory reporting. \n\nCpML 5.0 was first announced in 2013, and is based on previous standards created by European Federation of Energy Traders (EFET). CpML is governed by the CpML Foundation, a foundation under Dutch law created in 2014.\n\nThe Governance Board of the CpML Foundation consisted initially of representatives of EDF Trading, Freepoint Commodities Europe, Gazprom Marketing & Trading, RWE Supply & Trading and EFET.\n\n"}
{"id": "39672885", "url": "https://en.wikipedia.org/wiki?curid=39672885", "title": "Compact intracloud discharge", "text": "Compact intracloud discharge\n\nA compact intracloud discharge (CID), also known as a narrow bipolar event (NBE) or narrow bipolar pulse (NBP) is an intensive form of lightning that produces radio waves and scarce visible light. Lasting only a few millionths of a second (typically 20 us), these events are the most powerful known natural terrestrial source of radio waves in HF and VHF band. They are not well understood scientifically.\n\n"}
{"id": "24715055", "url": "https://en.wikipedia.org/wiki?curid=24715055", "title": "Creekology", "text": "Creekology\n\nCreekology is a petroleum prospecting method which appeared in the 19th century in US south gas-oil states. In its simplest form, it was the search for above-ground indications of oil, such as natural seeps. Creekologists also placed wells on singular points of a territory in accordance with landscape features. The placing of wells often occurred near, or on, linear objects - erosion relief forms (valleys, creeks, etc.) - giving rise to the term \"creekology\". The success rates of some creekologists in the 19th century were very high - 80-90% of their wells gave production and more recent petroleum firms can only dream about such results.\n\nIn the middle of the 20th century some geologists formed an idea about existing fault-fold systems in the earth's crust, in sediment cover, and this gave creekology a \"scientific base\". Linear forms in earth landscape connect with fault tectonics, and some folds near faults can be reservoirs of hydrocarbons. The non-fold types of traps mark by lineation too (paleo-rivers formed on faults). Now some geologists mark probable gas-oil areas with the help of space image lineation interpreting. This activity can be defined as \"new creekology\".\n"}
{"id": "47198640", "url": "https://en.wikipedia.org/wiki?curid=47198640", "title": "Crowther criterion", "text": "Crowther criterion\n\nThe conventional method to evaluate the resolution of an tomography reconstruction is determined by the Crowther criterion. The minimum number of views, \"m\", to reconstruct a particle of diameter \"D\" to a resolution of \"d\" (=1/\"R\") is given by\n"}
{"id": "55776607", "url": "https://en.wikipedia.org/wiki?curid=55776607", "title": "Cyclone Egon", "text": "Cyclone Egon\n\nCyclone Egon was a European windstorm that affected the north of France, Belgium and Germany during the night of Thursday 12 to Friday 13 January 2017. It caused three deaths, widespread power outages, and wind damage and significant snowfall, primarily France and Germany, but also in the Benelux states, Austria and Switzerland.\n\nThe storm was the first to exceed the reporting threshold of €200 million with Perils.org since Cyclone Niklas in March 2015. The total insured damages were estimated at €275m (final loss report).\n\nOn 12 January Egon developed unexpectedly from the trailing cold front of the low Dieter centred over Scandinavia. Egon formed to the southwest of Ireland reaching western France by noon that day. Egon formed “rather unexpectedly\" as a secondary low on a cold front of Windstorm Dieter, before the depression began to deepen quite quickly as it headed for northern France. The development of Egon coincided with a drop in the height of the tropopause especially between Brittany, Belgium and northeastern Germany. In more than 12 hours, the central pressure of the low pressure dropped to 980 hPa in France on 12 January. Satellite water vapour imagery of the low was described as being suggestive of a Sting jet.\n\nThe highest gust reported in France was in Dieppe which reported , a value not seen there since the Great storm of 1987.\nIn Caen a gust of was reported, the strongest since the passage of Lothar in 1999. Despite locally strong gusts, Egon was not considered by Météo-France to be one of the strongest historical storms to hit France as a whole, describing it as not comparable to the storm Lothar of 1999.\n\nEgon was accompanied by heavy rain and snow in places, with the regions of Normandy and Picardy particularly affected, where Egon caused power outages for over 330,000 households concentrated in these regions.\n\nStrong winds from Egon pushed in a significant portion of the west rose window of Soissons Cathedral to the northeast of Paris. Debris from the window landed on the organ causing severe damage.\n\nA unit of the Paluel Nuclear Power Plant, near Dieppe shut down automatically following the malfunction of a 400,000 volt transmission cable.\nSevere gusts and heavy snowfall caused considerable damage and disruption in northern France and central Germany. Some of the greatest damage was incurred to forestry.\n\nIn Germany, hurricane-force winds blew on the Saar and Rhineland-Palatinate, uprooting trees and causing power outages. Heavy snowfall covered much of the country and closed the roads, while the German Meteorological Service (DWD) indicated that some areas had received up to 30 cm in a few hours, causing accidents. The airports of Frankfurt, Leipzig and Dresden cancelled their flights. A driver died in an accident, probably because of slippery roads, in Schleswig-Holstein. Another driver was also killed in an accident in Hesse.\n\nOn 13 January in Kaiserslautern the winds of Egon brought down the scaffolding surrounding a high-rise. Hurricane-force gusts from the low Egon swept through the Saarland on 13 January up to .\n\nGermany 2\n\n"}
{"id": "11553066", "url": "https://en.wikipedia.org/wiki?curid=11553066", "title": "Daihatsu UFE-III", "text": "Daihatsu UFE-III\n\nThe UFE-III (Ultra Fuel Economy-third generation) is a mini-hybrid concept car being developed by Daihatsu. The vehicle can transport three people (one driver, and two passengers in the rear). The hybrid system comprises a 660-cubic centimeter direct-injection gasoline engine, two motors, and a nickel–metal hydride battery. Daihatsu estimates the UFE-III's fuel economy at . The body is in polymer and ultra-light aluminum with a canopy door and pointed LED headlamps. The UFE-III has an aerodynamic drag coefficient (Cd) of 0.168 and is controlled by steer-by-wire technology. Third generation of the \"Ultra Fuel Economy\" UFE, it was first shown in October 2005 at the Tokyo Motor Show.\n\n"}
{"id": "25096375", "url": "https://en.wikipedia.org/wiki?curid=25096375", "title": "Dutch Hill/Cohocton Wind Farm", "text": "Dutch Hill/Cohocton Wind Farm\n\nThe Dutch Hill/Cohocton Wind Farm is a 125 Megawatt wind farm in Cohocton, New York. It uses 50 turbines of the Clipper \"Liberty\" type, which were the largest found in the United States when they were put up for sale. The wind farm provides power for about 50,000 Northeastern homes. The wind farm is located in Steuben County. It was installed in 2008 and was developed and operated by First Wind.\n"}
{"id": "75049", "url": "https://en.wikipedia.org/wiki?curid=75049", "title": "Electrostatic discharge", "text": "Electrostatic discharge\n\nElectrostatic discharge (ESD) is the sudden flow of electricity between two electrically charged objects caused by contact, an electrical short, or dielectric breakdown. A buildup of static electricity can be caused by tribocharging or by electrostatic induction. The ESD occurs when differently-charged objects are brought close together or when the dielectric between them breaks down, often creating a visible spark.\n\nESD can create spectacular electric sparks (lightning, with the accompanying sound of thunder, is a large-scale ESD event), but also less dramatic forms which may be neither seen nor heard, yet still be large enough to cause damage to sensitive electronic devices. Electric sparks require a field strength above approximately 40 kV/cm in air, as notably occurs in lightning strikes. Other forms of ESD include corona discharge from sharp electrodes and brush discharge from blunt electrodes.\n\nESD can cause harmful effects of importance in industry, including explosions in gas, fuel vapor and coal dust, as well as failure of solid state electronics components such as integrated circuits. These can suffer permanent damage when subjected to high voltages. Electronics manufacturers therefore establish electrostatic protective areas free of static, using measures to prevent charging, such as avoiding highly charging materials and measures to remove static such as grounding human workers, providing antistatic devices, and controlling humidity.\n\nESD simulators may be used to test electronic devices, for example with a human body model or a charged device model.\n\nOne of the causes of ESD events is static electricity. Static electricity is often generated through tribocharging, the separation of electric charges that occurs when two materials are brought into contact and then separated. Examples of tribocharging include walking on a rug, rubbing a plastic comb against dry hair, rubbing a balloon against a sweater, ascending from a fabric car seat, or removing some types of plastic packaging. In all these cases, the breaking of contact between two materials results in tribocharging, thus creating a difference of electrical potential that can lead to an ESD event.\n\nAnother cause of ESD damage is through electrostatic induction. This occurs when an electrically charged object is placed near a conductive object isolated from the ground. The presence of the charged object creates an electrostatic field that causes electrical charges on the surface of the other object to redistribute. Even though the net electrostatic charge of the object has not changed, it now has regions of excess positive and negative charges. An ESD event may occur when the object comes into contact with a conductive path. For example, charged regions on the surfaces of styrofoam cups or bags can induce potential on nearby ESD sensitive components via electrostatic induction and an ESD event may occur if the component is touched with a metallic tool.\n\nESD can also be caused by energetic charged particles impinging on an object. This causes increasing surface and deep charging. This is a known hazard for most spacecraft.\n\nThe most spectacular form of ESD is the spark, which occurs when a heavy electric field creates an ionized conductive channel in air. This can cause minor discomfort to people, severe damage to electronic equipment, and fires and explosions if the air contains combustible gases or particles.\n\nHowever, many ESD events occur without a visible or audible spark. A person carrying a relatively small electric charge may not feel a discharge that is sufficient to damage sensitive electronic components. Some devices may be damaged by discharges as small as 30 V. These invisible forms of ESD can cause outright device failures, or less obvious forms of degradation that may affect the long term reliability and performance of electronic devices. The degradation in some devices may not become evident until well into their service life.\n\nA spark is triggered when the electric field strength exceeds approximately 4–30 kV/cm — the dielectric field strength of air. This may cause a very rapid increase in the number of free electrons and ions in the air, temporarily causing the air to abruptly become an electrical conductor in a process called dielectric breakdown.\n\nPerhaps the best known example of a natural spark is lightning. In this case the electric potential between a cloud and ground, or between two clouds, is typically hundreds of millions of volts. The resulting current that cycles through the stroke channel causes an enormous transfer of energy. On a much smaller scale, sparks can form in air during electrostatic discharges from charged objects that are charged to as little as 380 V (Paschen's law).\n\nEarth's atmosphere consists of 21% oxygen (O) and 78% nitrogen (N). During an electrostatic discharge, such as a lightning flash, the affected atmospheric molecules become electrically overstressed. The diatomic oxygen molecules are split, and then recombine to form ozone (O), which is unstable, or reacts with metals and organic matter. If the electrical stress is high enough, nitrogen oxides (NOx) can form. Both products are toxic to animals, and nitrogen oxides are essential for nitrogen fixation. Ozone attacks all organic matter by ozonolysis and is used in water purification.\n\nSparks are an ignition source in combustible environments that may lead to catastrophic explosions in concentrated fuel environments. Most explosions can be traced back to a tiny electrostatic discharge, whether it was an unexpected combustible fuel leak invading a known open air sparking device, or an unexpected spark in a known fuel rich environment. The end result is the same if oxygen is present and the three criteria of the fire triangle have been combined.\n\nMany electronic components, especially microchips, can be damaged by ESD. Sensitive components need to be protected during and after manufacture, during shipping and device assembly, and in the finished device. Grounding is especially important for effective ESD control. It should be clearly defined, and regularly evaluated.\n\nIn manufacturing, prevention of ESD is based on an Electrostatic Discharge Protected Area (EPA). The EPA can be a small workstation or a large manufacturing area. The main principle of an EPA is that there are no highly-charging materials in the vicinity of ESD sensitive electronics, all conductive and dissipative materials are grounded, workers are grounded, and charge build-up on ESD sensitive electronics is prevented. International standards are used to define a typical EPA and can be found for example from International Electrotechnical Commission (IEC) or American National Standards Institute (ANSI).\n\nESD prevention within an EPA may include using appropriate ESD-safe packing material, the use of conductive filaments on garments worn by assembly workers, conducting wrist straps and foot-straps to prevent high voltages from accumulating on workers' bodies, anti-static mats or conductive flooring materials to conduct harmful electric charges away from the work area, and humidity control. Humid conditions prevent electrostatic charge generation because the thin layer of moisture that accumulates on most surfaces serves to dissipate electric charges.\n\nIonizers are used especially when insulative materials cannot be grounded. Ionization systems help to neutralize charged surface regions on insulative or dielectric materials. Insulating materials prone to triboelectric charging of more than 2,000 V should be kept away at least 12 inches from sensitive devices to prevent accidental charging of devices through field induction. On aircraft, static dischargers are used on the trailing edges of wings and other surfaces.\n\nManufacturers and users of integrated circuits must take precautions to avoid ESD. ESD prevention can be part of the device itself and include special design techniques for device input and output pins. External protection components can also be used with circuit layout.\n\nDue to dielectric nature of electronics component and assemblies, electrostatic charging can not be completely prevented during handling of devices. Most of ESD sensitive electronic assemblies and components are also so small that manufacturing and handling is done with automated equipment. ESD prevention activities are therefore important with those processes where components come into direct contact with equipment surfaces. In addition, it is important to prevent ESD when an electrostatic discharge sensitive component is connected with other conductive parts of the product itself. An efficient way to prevent ESD is to use materials that are not too conductive but will slowly conduct static charges away. These materials are called static dissipative and have resistivity values below 10 ohm-meters. Materials in automated manufacturing which will touch on conductive areas of ESD sensitive electronic should be made of dissipative material, and the dissipative material must be grounded.\n\nSensitive devices need to be protected during shipping, handling, and storage. The buildup and discharge of static can be minimized by controlling the surface resistance and volume resistivity of packaging materials. Packaging is also designed to minimize frictional or triboelectric charging of packs due to rubbing together during shipping, and it may be necessary to incorporate electrostatic or electromagnetic shielding in the packaging material. A common example is that semiconductor devices and computer components are usually shipped in an antistatic bag made of a partially conductive plastic, which acts as a Faraday cage to protect the contents against ESD.\n\nFor testing the susceptibility of electronic devices to ESD from human contact, an ESD Simulator with a special output circuit, called the human body model (HBM) is often used. This consists of a capacitor in series with a resistor. The capacitor is charged to a specified high voltage from an external source, and then suddenly discharged through the resistor into an electrical terminal of the device under test. One of the most widely used models is defined in the JEDEC 22-A114-B standard, which specifies a 100 picofarad capacitor and a 1,500 ohm resistor. Other similar standards are MIL-STD-883 Method 3015, and the ESD Association's ESD STM5.1. For comportment to European Union standards for Information Technology Equipment, the IEC/EN 61000-4-2 test specification is used. Another specification (Schaffner) C = 150 pF R = 330 Ω that gives high fidelity results. Mostly the theory is there, minimum of the companies measure the real ESD survival rate. Guidelines and requirements are given for test cell geometries, generator specifications, test levels, discharge rate and waveform, types and points of discharge on the \"victim\" product, and functional criteria for gauging product survivability.\n\nA charged device model (CDM) test is used to define the ESD a device can withstand when the device itself has an electrostatic charge and discharges due to metal contact. This discharge type is the most common type of ESD in electronic devices and causes most of the ESD damages in their manufacturing. CDM discharge depends mainly on parasitic parameters of the discharge and strongly depends on size and type of component package. One of the most widely used CDM simulation test models is defined by the JEDEC.\n\nOther standardized ESD test circuits include the machine model (MM) and transmission line pulse (TLP).\n\n"}
{"id": "56628011", "url": "https://en.wikipedia.org/wiki?curid=56628011", "title": "Energi Mine", "text": "Energi Mine\n\nEnergi Mine is a United Kingdom based blockchain technology company, that develops products in the energy management sector. It uses deep learning artificial intelligence models to trade and manage energy. In February 2018, Energi Mine completed an Initial Coin Offering (ICO).\n\nEnergi Mine has a blockchain-driven platform that decentralizes the global energy market by incentivizing energy conservation. Consumers and organisations are issued with ETK Tokens to reward energy efficient behavior. The tokens can be used to pay electricity bills, buy energy-efficient appliances and take public transport.\n\nThe company was founded in 2016 by Omar Rahim, who serves as its CEO. Energi Mine was covered in the news when it appointed an AI robot, Sasha to its management board to make make algorithmic decisions.\n"}
{"id": "14858764", "url": "https://en.wikipedia.org/wiki?curid=14858764", "title": "Epibiont", "text": "Epibiont\n\nAn epibiont (from the Ancient Greek meaning \"living on top of\") is an organism that lives on the surface of another living organism. An epibiont is, by definition, harmless to its host and in this sense, the interaction between the two organisms can be considered neutralistic or commensalistic (as opposed to, for example, parasitic, in which case one organism benefits at the expense of the other, or mutualistic, in which both organisms obtain some explicit benefit from their coexistence).\n\nAlthough there is no direct effect of the epibiont to the host, there are often indirect effects resulting from this interaction and change in the surface of the host. This has been found to be especially important to marine organisms and aquatic ecosystems as surface qualities do impact necessary ecological functions such as drag, radiation absorption, nutrient uptake, etc. Examples of common epibionts are barnacles, remoras, and algae, many of which live on the surfaces of larger marine organisms such as whales, sharks, sea turtles, and mangrove trees. The host of the epibiont is typically referred to as the basibiont (\"living underneath\").\n\n\n"}
{"id": "48884622", "url": "https://en.wikipedia.org/wiki?curid=48884622", "title": "Esterlin", "text": "Esterlin\n\nThe esterlin is an obsolete netherlandic unit of mass and stands for gram. It also was a unit of mass for gold in France weighting 28 ½ Grain.\nThe place of the unit in the netherlandic unit-chain was\nIn Belgium, it was valid that 1 livre = 1000 esterlin = 1000 gram\n"}
{"id": "4642746", "url": "https://en.wikipedia.org/wiki?curid=4642746", "title": "Feynman sprinkler", "text": "Feynman sprinkler\n\nA Feynman sprinkler, also referred to as a Feynman inverse sprinkler or as a reverse sprinkler, is a sprinkler-like device which is submerged in a tank and made to suck in the surrounding fluid. The question of how such a device would turn was the subject of an intense and remarkably long-lived debate.\n\nA regular sprinkler has nozzles arranged at angles on a freely rotating wheel such that when water is pumped out of them, the resulting jets cause the wheel to rotate; both a Catherine wheel and the aeolipile (\"Hero's engine\") work on the same principle. A \"reverse\" or \"inverse\" sprinkler would operate by aspirating the surrounding fluid instead. The problem is now commonly associated with theoretical physicist Richard Feynman, who mentions it in his bestselling memoirs \"Surely You're Joking, Mr. Feynman!\" The problem did not originate with Feynman, nor did he publish a solution to it.\n\nThe first documented treatment of the problem is in chapter III, section III of Ernst Mach's textbook \"The Science of Mechanics\", first published in 1883. There, Mach reported that the device showed \"no distinct rotation.\" In the early 1940s (and apparently without awareness of Mach's earlier discussion), the problem began to circulate among members of the physics department at Princeton University, generating a lively debate. Richard Feynman, at the time a young graduate student at Princeton, built a makeshift experiment within the facilities of the university's cyclotron laboratory. The experiment ended with the explosion of the glass carboy that he was using as part of his setup.\n\nIn 1966, Feynman turned down an offer from the editor of \"Physics Teacher\" to discuss the problem in print and objected to it being called \"Feynman's problem,\" pointing instead to the discussion of it in Mach's textbook. The sprinkler problem attracted a great deal of attention after the incident was mentioned in \"Surely You're Joking, Mr. Feynman!\", a book of autobiographical reminiscences published in 1985. Feynman neither explained his understanding of the relevant physics, nor did he describe the results of the experiment. In an article written shortly after Feynman's death in 1988, John Wheeler, who had been his doctoral advisor at Princeton, revealed that the experiment at the cyclotron had shown “a little tremor as the pressure was first applied [...] but as the flow continued there was no reaction.” The sprinkler incident is also discussed in James Gleick's biography of Feynman, \"Genius,\" published in 1992, where Gleick claims that a sprinkler will not turn at all if made to suck in fluid.\n\nIn 2005, physicist Edward Creutz (who was in charge of the Princeton cyclotron at the time of the incident) revealed in print that he had assisted Feynman in setting up his experiment and that, when pressure was applied to force water out of the carboy through the sprinkler head,\n\nThe behavior of the reverse sprinkler is qualitatively quite distinct from that of the ordinary sprinkler, and one does not behave like the other \"played backwards.\" Most of the published theoretical treatments of this problem have concluded that the ideal reverse sprinkler will not experience any torque in its steady state. This may be understood in terms of conservation of angular momentum: in its steady state, the amount of angular momentum carried by the incoming fluid is constant, which implies that there is no torque on the sprinkler itself.\n\nMany experiments, going back to Mach, find no rotation of the reverse sprinkler. However, in setups with sufficiently low friction and high rate of inflow, the reverse sprinkler has been seen to turn weakly in the opposite sense to the conventional sprinkler, even in its steady state. Such behavior could be explained by the diffusion of momentum in a non-ideal (i.e., viscous) flow.\n\nHowever, careful observations of the actual behavior of experimental setups show that this turning is associated with the formation of a vortex inside the body of the sprinkler. An analysis of the actual distribution of forces and pressure in a non-ideal reverse sprinkler provides the theoretical basis to explain this:\n\n"}
{"id": "1420809", "url": "https://en.wikipedia.org/wiki?curid=1420809", "title": "Gas meter", "text": "Gas meter\n\nA gas meter is a specialized flow meter, used to measure the volume of fuel gases such as natural gas and liquefied petroleum gas. Gas meters are used at residential, commercial, and industrial buildings that consume fuel gas supplied by a gas utility. Gases are more difficult to measure than liquids, because measured volumes are highly affected by temperature and pressure. Gas meters measure a defined volume, regardless of the pressurized quantity or quality of the gas flowing through the meter. Temperature, pressure, and heating value compensation must be made to measure actual amount and value of gas moving through a meter.\n\nSeveral different designs of gas meters are in common use, depending on the volumetric flow rate of gas to be measured, the range of flows anticipated, the type of gas being measured, and other factors.\n\nGas meters that exist in colder climates in buildings built prior to the 1970s were typically located inside the home, typically in the basement or garage. Since then, the vast majority are now placed outside though there are a few exceptions especially in older cities.\n\nThese are the most common type of gas meter, seen in almost all residential and small commercial installations. Within the meter there are two or more chambers formed by movable diaphragms. With the gas flow directed by internal valves, the chambers alternately fill and expel gas, producing a nearly continuous flow through the meter. As the diaphragms expand and contract, levers connected to cranks convert the linear motion of the diaphragms into rotary motion of a crank shaft which serves as the primary flow element. This shaft can drive an odometer-like counter mechanism or it can produce electrical pulses for a flow computer.\n\nDiaphragm gas meters are positive displacement meters.\n\nRotary meters are highly machined precision instruments capable of handling higher volumes and pressures than diaphragm meters. Within the meter, two figure \"8\" shaped lobes, the rotors (also known as impellers or pistons), spin in precise alignment. With each turn, they move a specific quantity of gas through the meter. The operating principle is similar to that of a Roots blower. The rotational movement of the crank shaft serves as a primary flow element and may produce electrical pulses for a flow computer or may drive an odometer-like counter.\n\nTurbine gas meters infer gas volume by determining the speed of the gas moving through the meter. Because the volume of gas is inferred by the flow, it is important that flow conditions are good. A small internal turbine measures the speed of the gas, which is transmitted mechanically to a mechanical or electronic counter. These meters do not impede the flow of gas, but are limited at measuring lower flow rates.\n\nAn orifice gas meter consists of a straight length of pipe inside which a precisely known orifice plate creates a pressure drop, thereby affecting flow. Orifice meters are a type of differential meter, all of which infer the rate of gas flow by measuring the pressure difference across a deliberately designed and installed flow disturbance. The gas static pressure, density, viscosity, and temperature must be measured or known in addition to the differential pressure for the meter to accurately measure the fluid. Orifice meters often do not handle a large range of flow rates. They are however accepted and understood in industrial applications since they are easy to field-service and have no moving parts.\n\nUltrasonic flow meters are more complex than meters that are purely mechanical, as they require significant signal processing and computation capabilities. Ultrasonic meters measure the speed of gas movement by measuring the speed at which sound travels in the gaseous medium within the pipe. American Gas Association Report No. 9 covers the proper usage and installation of these meters, and it specifies a standardised speed-of-sound calculation which predicts the speed of sound in a gas with a known pressure, temperature, and composition.\n\nThe most elaborate types of ultrasonic flow meters average speed of sound over multiple paths in the pipe. The length of each path is precisely measured in the factory. Each path consists of an ultrasonic transducer at one end and a sensor at the other. The meter creates a 'ping' with the transducer and measures the time elapsed before the sensor receives the sonic pulse. Some of these paths point upstream so that the sum of the times of flight of the sonic pulses can be divided by the sum of the flight lengths to provide an average speed of sound in the upstream direction. This speed differs from the speed of sound in the gas by the velocity at which the gas is moving in the pipe. The other paths may be identical or similar, except that the sound pulses travel downstream. The meter then compares the difference between the upstream and downstream speeds to calculate the velocity of gas flow.\n\nUltrasonic meters are high-cost and work best with no liquids present at all in the measured gas, so they are primarily used in high-flow, high-pressure applications such as utility pipeline meter stations, where the gas is always dry and lean, and where small proportional inaccuracies are intolerable due to the large amount of money at stake. The turndown ratio of an ultrasonic meter is probably the largest of any natural gas meter type, and the accuracy and range ability of a high-quality ultrasonic meter is actually greater than that of the turbine meters against which they are proven.\n\nInexpensive varieties of ultrasonic meters are available as clamp-on flow meters, which can be used to measure flow in any diameter of pipe without intrusive modification. Such devices are based on two types of technology: (1) time of flight or transit time; and (2) cross correlation. Both technologies involve transducers that are simply clamped on to the pipe and programmed with the pipe size and schedule and can be used to calculate flow. Such meters can be used to measure almost any dry gas including natural gas, nitrogen, compressed air, and steam. Clamp-on meters are available for measuring liquid flow as well.\n\nA coriolis meter is usually one or more pipes with longitudinally or axially displaced section(s) that are excited to vibrate at resonant frequency. Coriolis meters are used with liquids and gases. When the fluid within the displaced section is at rest, both the upstream and downstream portion of the displaced section will vibrate in phase with each other. The frequency of this vibration is determined by the overall density of the pipe (including its contents). This allows the meter to measure the flowing density of the gas in real time. Once the fluid begins to flow, however, the Coriolis force comes into play. This effect implies a relationship between the phase difference in the vibration of the upstream and downstream sections and the mass flow rate of the fluid contained by the pipe.\n\nAgain, owing to the amount of inference, analog control and calculation intrinsic to a coriolis meter, the meter is not complete with just its physical components. There are actuation, sensing, electronic, and computational elements that must be present for the meter to function.\n\nCoriolis meters can handle a wide range of flow rates and have the unique ability to output mass flow - this gives the highest accuracy of flow measurement currently available for mass flow measurement. Since they measure flowing density, coriolis meters can also infer gas flow rate at flowing conditions.\n\nAmerican Gas Association Report No. 11 provides guidelines for obtaining good results when measuring natural gas with a coriolis meter.\n\nThe volume of gas flow provided by a gas meter is just that, a reading of volume. Gas volume does not take into account the quality of the gas or the amount of heat available when burned. Utility customers are billed according to the heat available in the gas. The quality of the gas is measured and adjusted for in each billing cycle. This is known by several names as the calorific value, heating value, or therm value.\n\nThe calorific value of natural gas can be obtained using a process gas chromatograph, which measures the amount of each constituent of the gas, namely:\n\nAdditionally, to convert from volume to thermal energy, the pressure and temperature of the gas must be taken into consideration. Pressure is generally not a problem; the meter is simply installed immediately downstream of a pressure regulator and is calibrated to read accurately at that pressure. Pressure compensation then occurs in the utility's billing system. Varying temperature cannot be handled as easily, but some meters are designed with built-in temperature compensation to keep them reasonably accurate over their designed temperature range. Others are corrected for temperature electronically.\n\nAny type of gas meter can be obtained with a wide variety of indicators. The most common are indicators that use multiple clock hands (pointer style) or digital readouts similar to an odometer, but remote readouts of various types are also becoming popular — see Automatic meter reading and Smart meter.\n\nGas meters are required to register the volume of gas consumed within an acceptable degree of accuracy. Any significant error in the registered volume can represent a loss to the gas supplier, or the consumer being overbilled. The accuracy is generally laid down in statute for the location in which the meter is installed. Statutory provisions should also specify a procedure to be followed should the accuracy be disputed.\nIn the UK, the permitted error for a gas meter manufactured prior to the European Measuring Instruments Directive is ±2%. However, the European Measuring Instrument Directive has harmonised gas meter errors across Europe and consequently meters manufactured since the directive came into force must read within ±3%. Meters whose accuracy is disputed by the customer have to be removed for testing by an approved meter examiner. If the meter is found to be reading outside of the prescribed limits, the supplier has to refund the consumer for gas incorrectly measured while that consumer had that meter (but not vice versa). If the meter cannot be tested or its reading is unreliable, the consumer and supplier have to negotiate a settlement. If the meter is found to be reading within limits, the consumer has to pay the costs of testing (and pay any outstanding charges). This contrasts with the position on electric meters, where the test is free and a refund is only given if the date of the meter started reading inaccurately can be determined.\n\nRemote reading is becoming popular for gas meters. It is often done through an electronic pulse output mounted on the meter. There are different styles available but most common is a contact closure switch.\n\nTurbine, rotary, and diaphragm meters can be compensated using a calculation specified in American Gas Association Report No. 7. This standardised calculation compensates the quantity of volume as measured to quantity of volume at a set of base conditions. The AGA 7 calculation itself is a simple ratio and is, in essence, a density correction approach to translating the volume or rate of gas at flowing conditions to a volume or rate at base conditions.\n\nOrifice meters are a very commonly used type of meter, and because of their widespread use, the characteristics of gas flow through an orifice meter have been closely studied. American Gas Association Report No. 3 deals with a broad range of issues relating to orifice metering of natural gas, and it specifies an algorithm for calculating natural gas flow rates based on the differential pressure, static pressure, and temperature of a gas with a known composition.\n\nThese calculations depend in part on the ideal gas law and also require a gas compressibility calculation in order to account for the fact that real gases are not ideal. A very commonly used compressibility calculation is American Gas Association Report No. 8, detail characterization.\n\nResidential, commercial and industrial gas meters have their own standard thread sizes. The gas meter is connected to customer piping through a swivel and nut, which has a dedicated set of thread sizes. These thread sizes were originally named for the amount of gas designed to flow through them in terms of gas lamps, for example a 30-Lt. meter can provide enough gas for 30 lights and was referred to in the late 19th century as a 30-light-gas-meter. These sizes are typically 10Lt, 20Lt, 30Lt, 45Lt, or 60Lt, though smaller and larger sizes are available. The thread sizes are slightly, about 1/16\", larger than the nearest size NPT size, in order to accommodate the appropriate inner diameter within the swivel.\n\n"}
{"id": "51351981", "url": "https://en.wikipedia.org/wiki?curid=51351981", "title": "Greased paper window", "text": "Greased paper window\n\nA greased paper window is a very inexpensive window made of paper coated in grease. The grease fills gaps between the paper fibers, reducing the amount of light lost to scattering. Greased paper windows provide a diffuse light source, while blocking wind and preventing insects and other small animals from entering a structure.\n\nGreased paper windows were often used by American pioneers of the early 1800s and other itinerant peoples, in lieu of relatively expensive traditional glass windows. Laura Ingalls Wilder recalled living in a home with a greased paper window in her 1937 children's novel, \"On the Banks of Plum Creek\".\n\n"}
{"id": "5218801", "url": "https://en.wikipedia.org/wiki?curid=5218801", "title": "Israel Union for Environmental Defense", "text": "Israel Union for Environmental Defense\n\nThe Israel Union for Environmental Defense (, \"Adam Teva veDin\", lit. \"Man Nature and Law\"), is an environmentalist group in Israel.\n\nIn November 2002 the Israeli Government appointed a six-member committee to explore the financial feasibility of creating two islands off the coast of Tel Aviv. One of the islands would be for an airport, similar to the Kobe airport in Japan, and the other island would have houses, commercial offices and tourism. Then IUED executive director Phil Warburg called the proposal a \"technological fantasy that is highly improper [because of] Israel’s economic and environmental constraints.\"\n\n"}
{"id": "14041249", "url": "https://en.wikipedia.org/wiki?curid=14041249", "title": "Kab 101", "text": "Kab 101\n\nKab 101 is a Sea Pony-type minimum-facilities light-production oil platform operated by Mexican state-owned oil company PEMEX, and installed about off the coast of Tabasco, near the port of Dos Bocas, in 1994. The platform was designed by British engineering firm SLP Engineering Limited. The platform also produces the wells Kab 103 and Kab 121. This platform was the site of the accident which eventually led to the death of 22 workers. Pemex would contract two independent studies and one by itself and in an exercise of transparency, post the reports on its website. On October 31, 2008, PEMEX released the result of the independent studies of the accident.\nOctober 21, 2007: The Jackup rig \"Usumacinta\" is moved to the location of Kab 101 to prepare for work on the well Kab 103.\n\nOctober 23, 2007, 0700: The arrival of Cold Front No. 4 with winds exceeding causes all personnel on the rig to cease operations.\n\n08:00 – 11:00: The Usumacinta begins moving with the seas, because its ballast and anchor points have not been properly set.\n\n11:30: The Usumacinta's auxiliary cover below its cantilever collides with the wellhead Kab 121, which begins leaking oil and gas.\n\n11:40 – 13:55: The crew of the Usumacinta attempts to close the sub-surface storm valves for both Kab 101 and the leaking Kab 121, to prevent further danger to the people on board the platform. This is only temporarily successful at stopping the leak from Kab 121.\n\n15:30: The storm valves on Kab 121 fail and hydrogen sulfide is detected, prompting the order to evacuate the platform.\n\n15:45: All 73 people from the platform were accounted for in the two lifeboats, and were wearing life jackets.\n\nAround 16:13 the lifeboat began to fill with water. This eventually led to the lifeboat crew becoming panicked and attempted to leave the lifeboat, in an attempt to board the M/V Morrison Tide. The lifeboat eventually capsized around 17:28, and a collision between the two lifeboats left most of Lifeboat #1's occupants drifting in the water. Two crew members from the Morrison Tide were lost in rescue operations; one died from injuries and another was lost at sea. Some survivors were also rescued by the M/V Isla Del Torro.\n\nThe lifeboat eventually drifted ashore east of Nuevo Campechito with nobody on board.\n\nThis lifeboat started out with poor visibility due to the oil from the leaking well Kab 121. The hatches were opened so the helmsman could see, and to allow for ventilation due to several people complaining of dizziness inside. At 17:42 the lifeboat was struck by a huge wave, and overturned. 33 to 35 people were trapped and had to fight to escape the boat.\n\nThe next day Lifeboat #2 reached the shore west of Nuevo Progreso upside down, with 12 survivors on top, and one survivor and four bodies inside.\n\nAn early estimate of the oil leak from Kab 121 was of light crude. In December 2007 Pemex estimated that of leaked oil were recovered with another 5000 completely lost.\n\nOn November 13, during attempts to control the leaks resulting from the accident, the well Kab 121 ignited and was brought under control the same day. On the 20th of November, Kab 121 ignited again, this time the fire destroyed the remains of the Derrick and was controlled on December 3.\n\nOn August 4, 2008, another fire was extinguished on the Usumacinta. This fire is suspected to have been caused by scrappers attempting to steal from the abandoned rig. This blaze was extinguished by the ships “Isla Guadalupe”, “Isla Cozumel”, “Pionero”, “Conquistador”, and \"Deep Endeavour\". The Mexican Navy also sent the interceptor \"Auriga\" to the area at the request of PEMEX.\n\n"}
{"id": "2508624", "url": "https://en.wikipedia.org/wiki?curid=2508624", "title": "Leyden ball", "text": "Leyden ball\n\nA Leyden ball is a fictional bullet used in the nineteenth century Jules Verne science fiction novel, \"Twenty Thousand Leagues Under the Sea\". It contains a capacitance charge of electrical energy, which discharges instantaneously upon the bullet's impact.\n\nA similar, but more complex non-lethal electronic weapon, meant to disable people, has been developed for police use in the modern day.\n\nIn his 1870 science fiction classic \"Twenty Thousand Leagues Under the Sea\", Jules Verne wrote about a hunting expedition using a very unusual form of bullet:\n\nthe balls sent by this gun are not ordinary balls, but little cases of glass. These glass cases are covered with a case of steel, and weighted with a pellet of lead; they are real Leyden bottles, into which the electricity is forced to a very high tension. With the slightest shock they are discharged, and the animal, however strong it may be, falls dead.\n\nIn Verne's novel, the Leyden balls were fired with special rifles powered by compressed air; it was only necessary that they touch the target. It was even possible to use them to bag game flying mere feet above the waves:\n\nI was witness to one of the finest gun shots which ever made the nerves of a hunter thrill. A large bird of great breadth of wing, clearly visible, approached, hovering over us. Captain Nemo's companion shouldered his gun and fired, when it was only a few yards above the waves. The creature fell stunned, and the force of its fall brought it within the reach of dexterous hunter's grasp. It was an albatross of the finest kind.\n\nIn 2005, the U.S. Department of Homeland Security awarded a development contract for a similar device called a Piezer, described as \"an untethered electro-muscular disruption non-lethal stun weapon based on piezoelectric technology for civil law enforcement officers and the military\".\n\nThe Extended Range Electronic Projectile, a long range untethered version of a rifle-fired Taser stun gun underwent trials in 2009. It was designed to fire non-lethal electronic dart modules at a range up to 20 meters from a 12-bore shotgun. Upon contact with its target, each dart module electrically shocks its subject for 20 seconds to immobilize it. The energy source for this device, however, is not a capacitor, but rather a conventional dry cell, or \"battery\".\n\n\n"}
{"id": "13273449", "url": "https://en.wikipedia.org/wiki?curid=13273449", "title": "Liberty Wind Turbine", "text": "Liberty Wind Turbine\n\nThe 2.5 MW Liberty Wind Turbine was the largest wind turbine manufactured in the United States when it was first installed. The turbine was developed through a partnership with U.S. Department of Energy and its National Renewable Energy Laboratory for Clipper Windpower.\n\nThe design of the turbine was meant to reduce problems with power train components that have been experienced in other machines. A two-ton crane within the nacelle simplifies maintenance thereby reducing costs. The size and weight of the liberty allow it to be constructed with crane of the same capacity as used with most 1.5 MW turbines.\n\nCommercial sales for the new Liberty turbine started in June 2006. Sales stopped in 2012 as the model was experiencing problems with power train components that had cost the company hundreds of millions of dollars in warranty repairs.\n\nLiberty Wind Turbine uses an 80 metre tall tower as a standard in its design. The rotor diameter varies amongst particular versions of turbines. The diameter for version C89 is 89 meters, 93 meters for version C93, 96 meters for version C96, and 99 meters for C99. Blade lengths are 43.2m for C89, 45.2m for C93, 46.7m for C96 and 48.2m for C96. Liberty Wind Turbines use 4 generators that have permanent magnets. Each generator delivers 660 kW at 1133 RPM at a voltage of 1350 Vdc. \n\n"}
{"id": "62198", "url": "https://en.wikipedia.org/wiki?curid=62198", "title": "Livermorium", "text": "Livermorium\n\nLivermorium is a synthetic chemical element with symbol Lv and has an atomic number of 116. It is an extremely radioactive element that has only been created in the laboratory and has not been observed in nature. The element is named after the Lawrence Livermore National Laboratory in the United States, which collaborated with the Joint Institute for Nuclear Research (JINR) in Dubna, Russia to discover livermorium during experiments made between 2000 and 2006. The name of the laboratory refers to the city of Livermore, California where it is located, which in turn was named after the rancher and landowner Robert Livermore. The name was adopted by IUPAC on May 30, 2012. 4 isotopes of livermorium are known, with mass numbers between 290 and 293 inclusive; the longest-lived among them is livermorium-293 with a half-life of about 60 milliseconds. A fifth possible isotope with mass number 294 has been reported but not yet confirmed.\n\nIn the periodic table, it is a p-block transactinide element. It is a member of the 7th period and is placed in group 16 as the heaviest chalcogen, although it has not been confirmed to behave as the heavier homologue to the chalcogen polonium. Livermorium is calculated to have some similar properties to its lighter homologues (oxygen, sulfur, selenium, tellurium, and polonium), and be a post-transition metal, although it should also show several major differences from them.\n\nThe first search for element 116, using the reaction between Cm and Ca, was performed in 1977 by Ken Hulet and his team at the Lawrence Livermore National Laboratory (LLNL). They were unable to detect any atoms of livermorium. Yuri Oganessian and his team at the Flerov Laboratory of Nuclear Reactions (FLNR) in the Joint Institute for Nuclear Research (JINR) subsequently attempted the reaction in 1978 and met failure. In 1985, a joint experiment between Berkeley and Peter Armbruster's team at GSI, the result was again negative with a calculated cross-section limit of 10–100 pb. Work on reactions with Ca, which had proved very useful in the synthesis of nobelium from the Pb+Ca reaction, nevertheless continued at Dubna, with a superheavy element separator being developed in 1989, a search for target materials and starting of collaborations with LLNL being started in 1990, production of more intense Ca beams being started in 1996, and preparations for long-term experiments with 3 orders of magnitude higher sensitivity being performed in the early 1990s. This work led directly to the production of new isotopes of elements 112 to 118 in the reactions of Ca with actinide targets and the discovery of the 5 heaviest elements on the periodic table: flerovium, moscovium, livermorium, tennessine, and oganesson.\n\nIn 1995, an international team led by Sigurd Hofmann at the Gesellschaft für Schwerionenforschung (GSI) in Darmstadt, Germany attempted to synthesise element 116 in a radiative capture reaction (in which the compound nucleus de-excites through pure gamma emission without evaporating neutrons) between a lead-208 target and selenium-82 projectiles. No atoms of element 116 were identified.\n\nIn late 1998, Polish physicist Robert Smolańczuk published calculations on the fusion of atomic nuclei towards the synthesis of superheavy atoms, including oganesson and livermorium. His calculations suggested that it might be possible to make these two elements by fusing lead with krypton under carefully controlled conditions.\n\nIn 1999, researchers at Lawrence Berkeley National Laboratory made use of these predictions and announced the discovery of livermorium and oganesson, in a paper published in \"Physical Review Letters\", and very soon after the results were reported in \"Science\". The researchers reported to have performed the reaction\n\nThe following year, they published a retraction after researchers at other laboratories were unable to duplicate the results and the Berkeley lab itself was unable to duplicate them as well. In June 2002, the director of the lab announced that the original claim of the discovery of these two elements had been based on data fabricated by principal author Victor Ninov.\n\nLivermorium was first synthesized on July 19, 2000, when scientists at Dubna (JINR) bombarded a curium-248 target with accelerated calcium-48 ions. A single atom was detected, decaying by alpha emission with decay energy 10.54 MeV to an isotope of flerovium. The results were published in December 2000.\n\nThe daughter flerovium isotope had properties matching those of a flerovium isotope first synthesized in June 1999, which was originally assigned to Fl, implying an assignment of the parent livermorium isotope to Lv. Later work in December 2002 indicated that the synthesized flerovium isotope was actually Fl, and hence the assignment of the synthesized livermorium atom was correspondingly altered to Lv.\n\nTwo further atoms were reported by the institute during their second experiment during April–May 2001. In the same experiment they also detected a decay chain which corresponded to the first observed decay of flerovium in December 1998, which had been assigned to Fl. No flerovium isotope with the same properties as the one found in December 1998 has ever been observed again, even in repeats of the same reaction. Later it was found that Fl has different decay properties and that the first observed flerovium atom may have been its nuclear isomer Fl. The observation of Fl in this series of experiments may indicate the formation of a parent isomer of livermorium, namely Lv, or a rare and previously unobserved decay branch of the already-discovered state Lv to Fl. Neither possibility is certain, and research is required to positively assign this activity. Another possibility suggested is the assignment of the original December 1998 atom to Fl, as the low beam energy used in that original experiment makes the 2n channel plausible; its parent could then conceivably be Lv, but this assignment would still need confirmation in the Cm(Ca,2n)Lv reaction.\n\nThe team repeated the experiment in April–May 2005 and detected 8 atoms of livermorium. The measured decay data confirmed the assignment of the first-discovered isotope as Lv. In this run, the team also observed the isotope Lv for the first time. In further experiments from 2004 to 2006, the team replaced the curium-248 target with the lighter curium isotope curium-245. Here evidence was found for the two isotopes Lv and Lv.\n\nIn May 2009, the IUPAC/IUPAP Joint Working Party reported on the discovery of copernicium and acknowledged the discovery of the isotope Cn. This implied the \"de facto\" discovery of the livermorium-291 isotope, from the acknowledgment of the data relating to its granddaughter Cn, although the livermorium data was not absolutely critical for the demonstration of copernicium's discovery. Also in 2009, confirmation from Berkeley and the Gesellschaft für Schwerionenforschung (GSI) in Germany came for the flerovium isotopes 286 to 289, immediate daughters of the four known livermorium isotopes. In 2011, IUPAC evaluated the Dubna team experiments of 2000–2006. Whereas they found the earliest data (not involving Lv and Cn) inconclusive, the results of 2004–2006 were accepted as identification of livermorium, and the element was officially recognized as having been discovered.\n\nThe synthesis of livermorium has been separately confirmed at the GSI (2012) and RIKEN (2014 and 2016). In the 2016 RIKEN experiment, one atom that may be assigned to Lv was seemingly detected, alpha decaying to Fl and Cn, which underwent spontaneous fission; however, the first alpha from the livermorium nuclide produced was missed, and the assignment to Lv is still uncertain though plausible.\n\nUsing Mendeleev's nomenclature for unnamed and undiscovered elements, livermorium is sometimes called \"eka-polonium\". In 1979 IUPAC recommended that the placeholder systematic element name \"ununhexium\" (\"Uuh\") be used until the discovery of the element was confirmed and a name was decided. Although widely used in the chemical community on all levels, from chemistry classrooms to advanced textbooks, the recommendations were mostly ignored among scientists in the field, who called it \"element 116\", with the symbol of \"E116\", \"(116)\", or even simply \"116\".\n\nAccording to IUPAC recommendations, the discoverer or discoverers of a new element have the right to suggest a name. The discovery of livermorium was recognized by the Joint Working Party (JWP) of IUPAC on 1 June 2011, along with that of flerovium. According to the vice-director of JINR, the Dubna team originally wanted to name element 116 \"moscovium\", after the Moscow Oblast in which Dubna is located, but it was later decided to use this name for element 115 instead. The name \"livermorium\" and the symbol \"Lv\" were adopted on May 23, 2012. The name recognises the Lawrence Livermore National Laboratory, within the city of Livermore, California, USA, which collaborated with JINR on the discovery. The city in turn is named after the American rancher Robert Livermore, a naturalized Mexican citizen of English birth. The naming ceremony for flerovium and livermorium was held in Moscow on October 24, 2012.\n\nLivermorium is expected to be near an island of stability centered on copernicium (element 112) and flerovium (element 114). The reasons for the presence of this island are still not well understood. Due to the expected high fission barriers, any nucleus within this island of stability exclusively decays by alpha decay and perhaps some electron capture and beta decay. While the known isotopes of livermorium do not actually have enough neutrons to be on the island of stability, they can be seen to approach the island, as the heavier isotopes are generally the longer-lived ones.\n\nSuperheavy elements are produced by nuclear fusion. These fusion reactions can be divided into \"hot\" and \"cold\" fusion, depending on the excitation energy of the compound nucleus produced. In hot fusion reactions, very light, high-energy projectiles are accelerated toward very heavy targets (actinides), giving rise to compound nuclei at high excitation energy (~40–50 MeV) that may either fission or evaporate several (3 to 5) neutrons. In cold fusion reactions (which use heavier projectiles, typically from the fourth period, and lighter targets, usually lead and bismuth), the produced fused nuclei have a relatively low excitation energy (~10–20 MeV), which decreases the probability that these products will undergo fission reactions. As the fused nuclei cool to the ground state, they require emission of only one or two neutrons. Hot fusion reactions tend to produce more neutron-rich products because the actinides have the highest neutron-to-proton ratios of any elements that can presently be made in macroscopic quantities.\n\nImportant information could be gained regarding the properties of superheavy nuclei by the synthesis of more livermorium isotopes, specifically those with a few neutrons more or less than the known ones – Lv, Lv, Lv, Lv, Lv, and Lv. This is possible because there are many reasonably long-lived isotopes of curium that can be used to make a target. The light isotopes can be made by fusing curium-243 with calcium-48. They would undergo a chain of alpha decays, ending at transactinide isotopes that are too light to achieve by hot fusion and too heavy to be produced by cold fusion.\n\nThe synthesis of the heavy isotopes Lv and Lv could be accomplished by fusing the heavy curium isotope curium-250 with calcium-48. The cross section of this nuclear reaction would be about 1 picobarn, though it is not yet possible to produce Cm in the quantities needed for target manufacture. After a few alpha decays, these livermorium isotopes would reach nuclides at the line of beta stability. Additionally, electron capture may also become an important decay mode in this region, allowing affected nuclei to reach the middle of the island. For example, it is predicted that Lv would alpha decay to Fl, which would undergo successive electron capture to Nh and then Cn which is expected to be in the middle of the island of stability and have a half-life of about 1200 years, affording the most likely hope of reaching the middle of the island using current technology. A drawback is that the decay properties of superheavy nuclei this close to the line of beta stability are largely unexplored.\n\nOther possibilities to synthesize nuclei on the island of stability include quasifission (partial fusion followed by fission) of a massive nucleus. Such nuclei tend to fission, expelling doubly magic or nearly doubly magic fragments such as calcium-40, tin-132, lead-208, or bismuth-209. Recently it has been shown that the multi-nucleon transfer reactions in collisions of actinide nuclei (such as uranium and curium) might be used to synthesize the neutron-rich superheavy nuclei located at the island of stability, although formation of the lighter elements nobelium or seaborgium is more favored. One last possibility to synthesize isotopes near the island is to use controlled nuclear explosions to create a neutron flux high enough to bypass the gaps of instability at Fm and at mass number 275 (atomic numbers 104 to 108), mimicking the r-process in which the actinides were first produced in nature and the gap of instability around radon bypassed. Some such isotopes (especially Cn and Cn) may even have been synthesized in nature, but would have decayed away far too quickly (with half-lives of only thousands of years) and be produced in far too small quantities (about 10 the abundance of lead) to be detectable as primordial nuclides today outside cosmic rays.\n\nIn the periodic table, livermorium is a member of group 16, the chalcogens, in the periodic table, below oxygen, sulfur, selenium, tellurium, and polonium. Every previous chalcogen has six electrons in its valence shell, forming a valence electron configuration of nsnp. In livermorium's case, the trend should be continued and the valence electron configuration is predicted to be 7s7p; therefore, livermorium will have some similarities to its lighter congeners. Differences are likely to arise; a large contributing effect is the spin–orbit (SO) interaction—the mutual interaction between the electrons' motion and spin. It is especially strong for the superheavy elements, because their electrons move much faster than in lighter atoms, at velocities comparable to the speed of light. In relation to livermorium atoms, it lowers the 7s and the 7p electron energy levels (stabilizing the corresponding electrons), but two of the 7p electron energy levels are stabilized more than the other four. The stabilization of the 7s electrons is called the inert pair effect, and the effect \"tearing\" the 7p subshell into the more stabilized and the less stabilized parts is called subshell splitting. Computation chemists see the split as a change of the second (azimuthal) quantum number \"l\" from 1 to and for the more stabilized and less stabilized parts of the 7p subshell, respectively: the 7p subshell acts as a second inert pair, though not as inert as the 7s electrons, while the 7p subshell can easily participate in chemistry. For many theoretical purposes, the valence electron configuration may be represented to reflect the 7p subshell split as 7s7p7p.\n\nThe inert pair effects in livermorium should be even stronger than for polonium and hence the +2 oxidation state becomes more stable than the +4 state, which would be stabilized only by the most electronegative ligands; this is reflected in the expected ionization energies of livermorium, where there are large gaps between the second and third ionization energies (corresponding to the breaching of the unreactive 7p shell) and fourth and fifth ionization energies. Indeed, the 7s electrons are expected to be so inert that the +6 state will not be attainable. The melting and boiling points of livermorium are expected to continue the trends down the chalcogens; thus livermorium should melt at a higher temperature than polonium, but boil at a lower temperature. It should also be denser than polonium (α-Lv: 12.9 g/cm; α-Po: 9.2 g/cm); like polonium it should also form an α and a β allotrope. The electron of the hydrogen-like livermorium atom (oxidized so that it only has one electron, Lv) is expected to move so fast that it has a mass 1.86 times that of a stationary electron, due to relativistic effects. For comparison, the figures for hydrogen-like polonium and tellurium are expected to be 1.26 and 1.080 respectively.\n\nLivermorium is projected to be the fourth member of the 7p series of chemical elements and the heaviest member of group 16 in the periodic table, below polonium. While it is the least theoretically studied of the 7p elements, its chemistry is expected to be quite similar to that of polonium. The group oxidation state of +6 is known for all the chalcogens apart from oxygen which lacks available d-orbitals for expansion of its octet and is itself one of the strongest oxidizing agents among the chemical elements. Oxygen is thus limited to a maximum +2 state, exhibited in the fluoride OF. The +4 state is known for sulfur, selenium, tellurium, and polonium, undergoing a shift in stability from reducing for sulfur(IV) and selenium(IV) through being the most stable state for tellurium(IV) to being oxidizing in polonium(IV). This suggests a decreasing stability for the higher oxidation states as the group is descended due to the increasing importance of relativistic effects, especially the inert pair effect. The most stable oxidation state of livermorium should thus be +2, with a rather unstable +4 state. The +2 state should be about as easy to form as it is for beryllium and magnesium, and the +4 state should only be achieved with strongly electronegative ligands, such as in livermorium(IV) fluoride (LvF). The +6 state should not exist at all due to the very strong stabilization of the 7s electrons, making the valence core of livermorium only four electrons. The lighter chalcogens are also known to form a −2 state as oxide, sulfide, selenide, telluride, and polonide; due to the destabilization of livermorium's 7p subshell, the −2 state should be very unstable for livermorium, whose chemistry should be essentially purely cationic, though the larger subshell and spinor energy splittings of livermorium as compared to polonium should make Lv slightly more stable than expected.\n\nLivermorane (LvH) would be the heaviest chalcogen hydride and the heaviest homolog of water (the lighter ones being HS, HSe, HTe, and PoH). Polane (polonium hydride) is a more covalent compound than most metal hydrides because polonium straddles the border between metals and metalloids and has some nonmetallic properties: it is intermediate between a hydrogen halide like hydrogen chloride (HCl) and a metal hydride like stannane (SnH). Livermorane should continue this trend: it should be a hydride rather than a livermoride, but would still be a covalent molecular compound. Spin-orbit interactions are expected to make the Lv–H bond longer than expected simply from periodic trends alone, and make the H–Lv–H bond angle larger than expected: this is theorized to be because the unoccupied 8s orbitals are relatively low in energy and can hybridize with the valence 7p orbitals of livermorium. This phenomenon, dubbed \"supervalent hybridization\", is not particularly uncommon in non-relativistic regions in the periodic table; for example, molecular calcium difluoride has 4s and 3d involvement from the calcium atom. The heavier livermorium dihalides are predicted to be linear, but the lighter ones are predicted to be bent.\n\nUnambiguous determination of the chemical characteristics of livermorium has not yet been established. In 2011, experiments were conducted to create nihonium, flerovium, and moscovium isotopes in the reactions between calcium-48 projectiles and targets of americium-243 and plutonium-244. The targets included lead and bismuth impurities and hence some isotopes of bismuth and polonium were generated in nucleon transfer reactions. This, while an unforeseen complication, could give information that would help in the future chemical investigation of the heavier homologs of bismuth and polonium, which are respectively moscovium and livermorium. The produced nuclides bismuth-213 and polonium-212m were transported as the hydrides BiH and PoH at 850 °C through a quartz wool filter unit held with tantalum, showing that these hydrides were surprisingly thermally stable, although their heavier congeners McH and LvH would be expected to be less thermally stable from simple extrapolation of periodic trends in the p-block. Further calculations on the stability and electronic structure of BiH, McH, PoH, and LvH are needed before chemical investigations take place. Moscovium and livermorium are expected to be volatile enough as pure elements for them to be chemically investigated in the near future, a property livermorium would then share with its lighter congener polonium, though the short half-lives of all presently known livermorium isotopes means that the element is still inaccessible to experimental chemistry.\n\n"}
{"id": "33332191", "url": "https://en.wikipedia.org/wiki?curid=33332191", "title": "Marsden A", "text": "Marsden A\n\nMarsden A was a 250 MW oil-fired power station near the Marsden Point Oil Refinery at Marsden Point, Ruakaka, Northland, New Zealand. It was built in the 1960s, and acted as an emergency reserve power station, serving the load centre of Auckland to the south. It was cooled through a long seawater pipe out into Bream Bay, which is now used to supply an aquaculture industry nearby.\n\nFollowing the commissioning of the dual coal- and gas-fired Huntly power station in 1982, Marsden A became less used, and it was mothballed in the 1990s due to rising oil prices. One of the generators was subsequently run as a synchronous condenser to provide reactive power support to Transpower's national grid in Northland, but were decommissioned in 2007 when they were superseded by solid state power support devices within Transpower's network.\n\nThe facility was owned and operated by Mighty River Power.\n"}
{"id": "16716910", "url": "https://en.wikipedia.org/wiki?curid=16716910", "title": "Minimum energy performance standard", "text": "Minimum energy performance standard\n\nA MEPS (Minimum Energy Performance Standard) is a specification, containing a number of performance requirements for an energy-using device, that effectively limits the maximum amount of energy that may be consumed by a product in performing a specified task.\n\nA MEPS is usually made mandatory by a government energy efficiency body. It may include requirements not directly related to energy; this is to ensure that general performance and user satisfaction are not adversely affected by increasing energy efficiency.\n\nA MEPS generally requires use of a particular test procedure that specifies how performance is measured.\n\nIn North America when addressing energy efficiency, a MEPS is sometimes referred to simply as a “standard”, as in “Co-operation on Labeling and Standards Programs”. In Latin America when addressing energy efficiency, MEPS are sometimes referred to as \"Normas\" (translated as \"norms\").\n\nA storage water heater providing hot water for sanitary purposes is required to heat up a specified quantity of water to a specified temperature and store it at that temperature for a specified time while consuming a limited amount of energy. In this example, the requirements for heating up and for maintaining the temperature may be applied as two separate energy performance requirements or there may be a single task efficiency. \nA compact fluorescent lamp is required to start and run up to near full brightness in a given time, to have a minimum life of several thousand hours, to maintain its output within specified limits, to withstand a certain number of switchings, to have a consistent colour appearance and a specified colour rendering. Its energy performance requirement is usually stated in terms of minimum efficacy (light output per electrical input).\n\nIn the U.S., the state of California was a pioneer in the introduction of MEPS. In order to reduce the growth in electricity use, the California Energy Commission (CEC) was given unique and strong authority to regulate the efficiency of appliances sold in the state. It started to adopt appliance efficiency regulations in 1978, and has updated the standards regularly over time, and expanded the list of covered appliances. \n\nIn 1988, California's standards became national standards for the U.S. through the enactment of the National Appliance Energy Conservation Act (NAECA). The federal standards preempted state standards (unless the state justified a waiver from federal preemption based on conditions in the state), and since then, the U.S. Department of Energy has had the responsibility to update the federal standards. \n\nCalifornia has continued to expand the list of appliances it regulates for appliances that are not federally regulated, and therefore not preempted. In recent years, the CEC's attention has been focused on consumer electronics, for which energy use has been growing dramatically.\n\nMEPS programs are made mandatory in Australia by state government legislation and regulations which give force to the relevant Australian Standards. It is mandatory for the following products manufactured in or imported into Australia to meet the MEPS levels specified by the relevant Australian Standards:\n\nA law was approved in 2001. MEPS have been set for three-phase electric motors and compact fluorescent lamps.\n\nOn 5 February 2002, New Zealand introduced Minimum Energy Performance Standards (MEPS) with Energy Efficiency Regulations.\nMEPS and energy rating labels help improve the energy efficiency of our products, and enable consumers to choose products that use less energy. Products covered by MEPS must meet or exceed set levels for energy performance before they can be sold to consumers.\nMEPS have been updated over the years (2002, 2003, 2004, 2008, 2011) to cover a wide range of products, and increasing levels of stringency. New Zealand works with Australia to harmonise MEPS levels. Almost all of its standards are joint standards with Australia.\nNew Zealand has mandatory Energy rating labelling for dishwashers and clothes dryers, fridges, washing machines and room air conditioners. \nMEPS apply to the following:\n\n\n"}
{"id": "34829413", "url": "https://en.wikipedia.org/wiki?curid=34829413", "title": "Moby-Duck", "text": "Moby-Duck\n\nMoby-Duck: The True Story of 28,800 Bath Toys Lost at Sea and of the Beachcombers, Oceanographers, Environmentalists, and Fools, Including the Author, Who Went in Search of Them is a book by Donovan Hohn concerning 28,800 plastic ducks and other toys, known as the Friendly Floatees, which were washed overboard from a container ship in the Pacific Ocean on 10 January 1992 and have subsequently been found on beaches around the world and used by oceanographers including Curtis Ebbesmeyer to trace ocean currents.\n\nThe book was published in the United States in March 2011 by Viking () and in the UK in February 2012 by Union Books () with a shorter subtitle: \"Moby-Duck: The True Story of 28,800 Bath Toys Lost at Sea\". It was noted by \"The New York Times\" as one of the \"100 Notable Books of 2011\", shortlisted for the 2012 Helen Bernstein Book Award for Excellence in Journalism, runner-up of the 2012 PEN/E. O. Wilson Literary Science Writing Award and runner-up of the 2013 PEN/John Kenneth Galbraith Award.\n\nThe title is a reference to Herman Melville's classic seafaring novel \"Moby-Dick\".\n"}
{"id": "13969132", "url": "https://en.wikipedia.org/wiki?curid=13969132", "title": "Neher–McGrath method", "text": "Neher–McGrath method\n\nIn electrical engineering, Neher–McGrath is a method of estimating the steady-state temperature of electrical power cables for some commonly encountered configurations. By estimating the temperature of the cables, the safe long-term current-carrying capacity of the cables can be calculated. \n\nJ. H. Neher and M. H. McGrath were two electrical engineers who wrote a paper about how to calculate the capacity of current (ampacity) of cables. The paper described two-dimensional highly symmetric simplified calculations which have formed the basis for many cable application guidelines and regulations. Complex geometries, or configurations that require three-dimensional analysis of heat flow, require more complex tools such as finite element analysis. Their article became used as reference for the ampacity in most of the standard tables. \n\nThe Neher–McGrath paper summarized years of research into analytical treatment of the practical problem of heat transfer from power cables. The methods described included all the heat generation mechanisms from a power cable (conductor loss, dielectric loss and shield loss).\n\nFrom the basic principles that electric current leads to thermal heating and thermal power transfer to the ambient environment requires some temperature difference, it follows that the current leads to a temperature rise in the conductors. The ampacity, or maximum allowable current, of an electric power cable depends on the allowable temperatures of the cable and any adjacent materials such as insulation or termination equipment. For insulated cables, the insulation maximum temperature is normally the limiting material property that constrains ampacity. For uninsulated cables (typically used in overhead installation), the tensile strength of the cable (as affected by temperature) is normally the limiting material property. The Neher–McGrath method is the electrical industry standard for calculating cable ampacity, most often employed via lookup in tables of precomputed results for common configurations.\n\nThe equation in section 310-15(C) of the National Electrical Code, called the Neher–McGrath equation (NM) (given below), may be used to estimate the effective ampacity of a cable.\n\nformula_1\n\nIn the equation, formula_2 is normally the limiting conductor temperature derived from the insulation or tensile strength limitations. formula_3 is a term added to the ambient temperature formula_4 to compensate for heat generated in the jacket and insulation for higher voltages. formula_3 is called the dielectric loss temperature rise and is generally regarded as insignificant for voltages below 2000 V. Term formula_6 is a multiplier used to convert direct current resistance (formula_7) to the effective alternating current resistance (which typically includes conductor skin effects and eddy current losses). For wire sizes smaller than AWG No. 2 (), this term is generally regarded as insignificant. formula_8 is the effective thermal resistance between the conductor and the ambient conditions, which can require significant empirical or theoretical effort to estimate. With respect to the AC-sensitive terms, tabular presentation of the NM equation results in the National Electrical Code was developed assuming the standard North American power frequency of 60 hertz and sinusoidal wave forms for current and voltage.\n\nThe challenges posed by the complexity of estimating RCA and of estimating the local increase in ambient temperature obtained by co-locating many cables (in a duct bank) create a market niche in the electric power industry for software dedicated to ampacity estimation.\n\n"}
{"id": "55890020", "url": "https://en.wikipedia.org/wiki?curid=55890020", "title": "Niles Firebrick", "text": "Niles Firebrick\n\nNiles Firebrick was manufactured by the Niles Fire brick Company since it was created in 1872 by John Rhys Thomas until the company was sold in 1953 and completely shutdown in 1960. Capital to establish the company was provided by Lizzie B. Ward to construct a small plant across from the Old Ward Mill which was run by her husband James Ward. Thomas immigrated in 1868 from Carmarthenshire in Wales with his wife and son W. Aubrey Thomas who served as secretary of the company until he was appointed as representative to the U. S. Congress in 1904. The company was managed by another son Thomas E. Thomas after J.R. Thomas died unexpectedly in 1898. The Thomases returned the favor of their original capitalization by purchasing an iron blast furnace from James Ward when he went bankrupt in 1879. Using their knowledge of firebrick they were able to make this small furnace profitable. Later they used it to showcase the value of adding hot blast to a furnace using 3 ovens packed full of firebrick. The furnace was managed by another son John Morgan Thomas.\n\nFire brick was first invented in 1822 by William Weston Young in the Neath Valley of Wales, in the next county east of Llanelli where the Thomas family lived before emigrating to Niles. It is recorded that Firebrick was made in the Llanelli area in 1870 but the market was highly cyclical and it was difficult to make a living at it. \n\nFrom 1937 to 1941 the company worked to prevent the United Brick Workers Union (CIO) from organizing the workers in preference for an independent union favored by management. The CIO union prevailed. In spite of this episode the company had good relations with the employees and tried to keep them employed during economic downturns. The \"Clingans\" mentioned in that referenced interview were Margaret Thomas Clingan, a daughter and John Rhys Thomas Clingan, a grandson, who took over management of the Company when T.E. Thomas died in 1920.\n\nPatrick J. Sheehan worked various jobs at Niles Fire Brick Company from age 13 up until 1897 when he was appointed superintendent of the plant. When Sheehan started with the company they occupied a plant covering a floor space of 3,600 square feet, two kilns, and the output was 640,000 bricks per year. The plant was moved to Langley street eighteen months afterward, and the output increased to 1,200,000. This Langley street works has constantly added to each year, until the output was 6 million and in 1905 they built the \"Falcon\" plant on the site formerly occupied by the Langley street plant. Which doubled production to 12 million per year. By 1955 the output was 25 million. The work of molding and firing brick was highly labor-intensive. Immegries from Southern States and European countries especially Italy were sought to perform the work under working conditions that were long and hard.\n"}
{"id": "56123196", "url": "https://en.wikipedia.org/wiki?curid=56123196", "title": "Nkenda–Fort Portal–Hoima High Voltage Power Line", "text": "Nkenda–Fort Portal–Hoima High Voltage Power Line\n\nThe Nkenda–Fort Portal–Hoima High Voltage Power Line is a high voltage electricity power line, in operation, connecting the high voltage substation at Nkenda, Kasese District, to another high voltage substation at Kabaale, Buseruka sub-county, Hoima District, all in the Western Region of Uganda.\n\nThe 220 kilo Volt power line starts at the 220kV substation a Nkenda, in Kasese District, approximately , by road, north of Kasese, the district headquarters, and nearest large town. The line travels in a north-easterly direction, through Fort Portal, in Kabarole District, to end at Kabaale, in Buseruka sub-county, in Hoima District, a total distance of approximately .\n\nThis power line is planned to evacuate the power from a number of mini-hydro power projects in the Western Region districts of Bundibugyo, Bunyangabu, Kasese, Hoima and Masindi. It is also planned to evacuate power from the proposed Nzizi Thermal Power Station. The power that this power line evacuates is sold to the Uganda Electricity Transmission Company Limited and integrated into the national electric grid.\n\nKEC International is the main contractor for this project. The supervising engineering company is \"Ficthner Gmbh\". The contract for construction of the associated substations was awarded to \"Shan-dong Taikai Power Engineering Company Limited. The government of Norway donated US$54 million towards the completion of this project. In 2013, the Ugandan government borrowed US$23 million from the French Development Agency to finance this power line. Construction began in 2016, with completion in July 2018 and public commissioning on 14 August 2018.\n\n\n"}
{"id": "443415", "url": "https://en.wikipedia.org/wiki?curid=443415", "title": "Parallelogram of force", "text": "Parallelogram of force\n\nThe parallelogram of forces is a method for solving (or visualizing) the results of applying two forces to an object.\n\nWhen more than two forces are involved, the geometry is no longer parallelogrammatic, but the same principles apply. Forces, being vectors are observed to obey the laws of vector addition, and so the overall (resultant) force due to the application of a number of forces can be found geometrically by drawing vector arrows for each force. For example, see Figure 1. This construction has the same result as moving F so its tail coincides with the head of F, and taking the net force as the vector joining the tail of F to the head of F. This procedure can be repeated to add F to the resultant F + F, and so forth.\n\nSuppose a particle moves at a uniform rate along a line from A to B (Figure 2) in a given time (say, one second), while in the same time, the line AB moves uniformly from its position at AB to a position at DC, remaining parallel to its original orientation throughout. Accounting for both motions, the particle traces the line AC. Because a displacement in a given time is a measure of velocity, the length of AB is a measure of the particle's velocity along AB, the length of AD is a measure of the line's velocity along AD, and the length of AC is a measure of the particle's velocity along AC. The particle's motion is the same as if it had moved with a single velocity along AC.\n\nSuppose two forces act on a particle at the origin (the \"tails\" of the vectors) of Figure 1. Let the lengths of the vectors F and F represent the velocities the two forces could produce in the particle by acting for a given time, and let the direction of each represent the direction in which they act. Each force acts independently and will produce its particular velocity whether the other force acts or not. At the end of the given time, the particle has \"both\" velocities. By the above proof, they are equivalent to a single velocity, F. By Newton's second law, this vector is also a measure of the force which would produce that velocity, thus the two forces are equivalent to a single force.\n\nWe model forces as Euclidean vectors or members of formula_1. Our first assumption is that the resultant of two forces is in fact another force, so that for any two forces formula_2 there is another force formula_3.\n\nOur final assumption is that the resultant of two forces doesn't change when rotated. If formula_4 is any rotation (any orthogonal map for the usual vector space structure of formula_1 with formula_6), then for all forces formula_7\n\nformula_8\n\nConsider two perpendicular forces formula_9 of length formula_10 and formula_11 of length formula_12, with formula_13 being the length of formula_14.\nLet formula_15 and formula_16, where formula_17 is the rotation between formula_9 and formula_14, so formula_20. Under the invariance of the rotation, we get\n\nformula_21\n\nSimilarly, consider two more forces formula_22 and formula_23. Let formula_24 be the rotation from formula_9 to formula_26: formula_27, which by inspection makes formula_28.\n\nformula_29\n\nApplying these two equations\n\nformula_30\n\nSince formula_31 and formula_32 both lie along formula_14, their lengths are equal formula_34\n\nformula_35\n\nwhich implies that formula_36 has length formula_37, which is the length of formula_38. Thus for the case where formula_9 and formula_11 are perpendicular, formula_41. However, when combining our two sets of auxiliary forces we used the associativity of formula_42. Using this additional assumption, we will form an additional proof below.\nWe model forces as Euclidean vectors or members of formula_1. Our first assumption is that the resultant of two forces is in fact another force, so that for any two forces formula_2 there is another force formula_3. We assume commutativity, as these are forces being applied concurrently, so the order shouldn't matter formula_46.\n\nConsider the map\nformula_47\n\nIf formula_42 is associative, then this map will be linear. Since it also sends formula_49 to formula_49 and formula_51 to formula_51, it must also be the identity map. Thus formula_42 must be equivalent to the normal vector addition operator.\n\nThe mathematical proof of the parallelogram of force is not generally accepted to be mathematically valid. Various proofs were developed (chiefly \"Duchayla's\" and \"Poisson's\"), and these also caused objections. That the parallelogram of force was true was not questioned, but \"why\" it was true. Today the parallelogram of force is accepted as an empirical fact, non-reducible to Newton's first principles.\n\n"}
{"id": "53001281", "url": "https://en.wikipedia.org/wiki?curid=53001281", "title": "Plastic-coated paper", "text": "Plastic-coated paper\n\nPlastic-coated paper is a coated or laminated composite material made of paper or paperboard with a plastic layer or treatment on a surface. This type of coated paper is most used in the food and drink packaging industry.\n\nThe plastic is used to improve functions such as water resistance, tear strength, abrasion resistance, ability to be heat sealed, etc.\n\nSome papers are laminated by heat or adhesive to a plastic film to provide barrier properties in use. Other papers are coated with a melted plastic layer: curtain coating is one common method. \n\nPrinted papers commonly have a top coat of a protective polymer to seal the print, provide scuff resistance, and sometimes gloss. Some coatings are processed by UV curing for stability.\n\nMost plastic coatings in the packaging industry are polyethylene (LDPE) and to a much lesser degree PET.\nShelf-stable cartons typically contain 74% paper, 22% plastic and 4% aluminum. Frozen food cartons are usually made up of a 80% paper and 20% plastic combination.\n\nThe most notable applications for plastic-coated paper are single use (disposable food packaging):\n\nPlastic coatings or layers usually make paper recycling more difficult. Some plastic laminations can be separated from the paper during the recycling process, allowing filtering out the film. If the coated paper is shredded prior to recycling, the degree of separation depends on the particular process. Some plastic coatings are water dispersible to aid recycling and repulping. Special recycling processes are available to help separate plastics. Some plastic coated papers are incinerated for heat or landfilled rather than recycled. \n\nMost plastic coated papers are not suited to composting. but do variously end up in compost bins, sometimes even legally so. In this case, the remains of the non-biodegradable plastics components form part of the global microplastics waste problem.\n\n"}
{"id": "49668604", "url": "https://en.wikipedia.org/wiki?curid=49668604", "title": "Pyramidal carbocation", "text": "Pyramidal carbocation\n\nA pyramidal carbocation is a type of carbocation with a specific configuration. This ion exists as a third class, besides the classical and non-classical ions. In these ions, a single carbon atom hovers over a four- or five-sided polygon, in effect forming a pyramid. The four-sided pyramidal ion will carry a charge of 1+, and the five-sided pyramid will carry 2+. In the images (\"at upper right)\", the black spot on the vertical line represents the hovering carbon atom.\n\nThe apparent coordination number of five, or even six, associated with the carbon atom at the top of the pyramid is a rarity as compared to the usual maximum of four.\n\nStudying these cations was sparked, at the time, by amazing results in computational chemistry. While calculating the optimal geometry of the mono-cation which arises from the extraction of chloride from 3-chlorotricyclo[2.1.0.0]pentane, the three bridges were expected to orient in space with angles of roughly 120°. The calculations however showed the four-sided pyramid to be the most stable configuration. At the top of this pyramid, there resides a carbon atom, still connected to a hydrogen. The original expected structure turned out to be not even close to an energy minimum: it represented a maximum.\n\nDepending on the method used, the ion 1c in figure 1 is an absolute or just a relative minimum.\n\nA complete theoretical discussion will use all orbitals of all contributing atoms. A first approximation might use a LCAO of the molecular orbitals in the polygon forming the base of the pyramid and the orbitals on the apical atom, as the carbon atom at the top of the pyramid. This approximation will provide insight into the intrinsic stability of the structures.\n\nThe apical carbon atom is connected to only one other substituent, so an sp-hybridisation is to be expected. The substituent will be oriented upward. Towards the basic polygon, three orbitals are available:\n\nThe approximation for the base of the pyramid is a closed ring of carbon atoms, all of them sp hybridised. The exact results depend on the ring size; overall conclusions can be formulated as:\n\n\nTo obtain bonding interactions between atoms or parts of molecules, two conditions should be met:\nThe orbitals at the apical carbon and the basic polygon are able to combine with respect to their symmetries. The result will be a more stable configuration for the pyramids. In figure 2, the symmetry aspects are depicted.\n\nFilling the atomic and molecular orbitals in pyramidal structures of different base size leads to the next table. Only bonding orbitals are accounted for.\n\nIn the case of the three-sided pyramid, clearly no ion results; a known neutral species arises: tetrahedrane. To this molecule this way of description is an alternative quantum mechanical description.\n\nThe other pyramidal structures will be charged in relation with their base size.\n\nIn 1972 Masamune describes the results of dissolving a number of precursors to \"4d\" (figure 4) at in superacid (a mixture of SOClF and FSOH). Based on both the C as well as the H-NMR-spectrum the evidence is clear: in each case the same intermediary is formed. Also, when the super acidic medium is destroyed, with either methanol or benzoic acid, the same product is formed. (see: Reaction... \"below\").\n\n\nAs described above, independent from its synthetic route, pyramidal ion 5a reacts with methanol or benzoate giving rise to products governed by reagent and the reaction medium as is clear by the substitution patterns. In 1972 Masamune is unable to explain the different behavior of the intermediate. In terms of the HSAB-theory an explanation might be given.\n\nIn 1975 Masamune calculated in the non-substituted ion most of the charge at the hydrogen atoms. Replacing hydrogen for carbon, the central atom of the methyl group, a more electronegative substituent (2.5 versus 2.1 on the Pauling scale) will concentrate charge on the skeletal carbon. This charge concentration has several effects:\n\nIn chemistry, the prefix \"homo-\" denotes a homolog, a likewise compound containing one, or as in this case two, extra CH-groups. The common aspect of the bishomo ions is the possession of a 1,4-cyclohexadiene ring instead of a cyclobutadiene one.\n\nThe stability of this ion at first may seem strange, as enlargement of the ring in general will diminish the bonding overlap between the orbitals at the center of the pyramidal structure. Here the sp hybridization, and consequently the planarity of the atoms of and those directly bonded to the sp centers, forces the tops of the p-orbitals of the basal carbons towards each other, thus creating a solid base for the apical carbon to sit on. Stiffening the configuration by a bridge between the homo-atoms, converting the base of the pyramid, to a norbornadiene, creates an even more stable structure.\n\nAccording to the results presented in Table 1, a five-sided pyramidal carbocation will be divalent. This is confirmed by theoretical and practical work by Hogeveen. In contrast to the monocation, which is described with several patterns of substitution, the dication is mainly studied by its hexamethyl derivative. The synthesis starts at hexamethyl Dewar benzene (compound I in table 4) reacting with Cl into 5,6-dichloro-1,2,3,4,5,6-hexamethylbicyclo[2.1.1]hex-2-ene (compound II in table 4). Dissolution of this compound in fluorosulfonic acid gives rise to the dication (structure III in table 4).\n\nThe presence of a pyramidal ion in the solution of fluorosulfonic acid is evidenced by the H- and C-NMR-spectrum (Table 5).\n\nThe assignment of the signals is based on their intensities and multiplicities. The assignment of the pyramidal structure is based on the observed simplicity of the spectra: five equal C-CH groups combined with one outstanding C-CH group. The only way to construct a molecular entity from this data is a five-sided pyramid. Rapid equilibriums between degenerated classical or non-classical carbocations are discarded as the position of the signals does not match the expected values for those kind of structures.\n\nThe crystal structure of [C(CH)] (SbF) • HSOF was obtained in 2017. Although the apical carbon atom is hexacoordinated, the rule of the tetravalency of carbon is still fulfilled. While the C-CH bond length of 1.479(3) Å is typical for a C-C single bond, the other five very long C-C distances of 1.694(2)-1.715(3) Å indicate a bond order of <1.\n\nThe reactions of the dication fall apart into three groups:\n\nThe product of the reaction of the dication with triethylamine offers a pathway to other substitution patterns then hexamethyl. One or both double bonds are oxidized to a keton. The keton then is reacted with an organometallic compound producing an alkylated hydroxide. The compounds formed in this way possess one or two other alkyl groups, depending on the number of oxidized double bonds. When the alcohols are dissolved in fluorosulfonic acid, they again give rise to new pyramidal dications. Both non-methyl groups occupy basal positions. Each other position at the pyramidal skeleton still carries a methyl group. Table 6 summarizes these findings.\n\nUp to this point the substitution pattern of the divalent pyramidal ion is of minor importance to its behavior. A clear difference arises when the thermal stability if the ions of type V (Table 6) is studied: the apical ethyl substituted ion is stable for 48 hours, whereas no trace of the apical \"iso\"-propyl ion is detectable anymore.\n\nAt the time of the literature survey (end of 1978), there were no reports on tervalent or higher pyramidal cations.\n"}
{"id": "5408457", "url": "https://en.wikipedia.org/wiki?curid=5408457", "title": "Quantum critical point", "text": "Quantum critical point\n\nA quantum critical point is a point in the phase diagram of a material where a continuous phase transition takes place at absolute zero. A quantum critical point is typically achieved by a continuous suppression of a nonzero temperature phase transition to zero temperature by the application of a pressure, field, or through doping.\nConventional phase transitions occur at nonzero temperature when the growth of random thermal fluctuations leads to a\nchange in the physical state of a system. Condensed matter physics research over the past few decades has revealed a new class of phase transitions called quantum phase transitions which take place at absolute zero. In the absence of the thermal fluctuations which trigger conventional phase transitions, quantum phase transitions are\ndriven by the zero point quantum fluctuations associated with Heisenberg's uncertainty principle.\nWithin the class of phase transitions, there are two main categories: at a first-order phase transition, the properties shift discontinuously, as in the melting of solid, whereas at a second order phase transition, the state of the system changes in a continuous fashion. Second-order phase transitions are marked by the growth of fluctuations on ever-longer length-scales. These fluctuations are called \"critical fluctuations\". At the critical point where a second-order transition occurs the critical fluctuations are scale invariant and extend over the entire system. At a nonzero temperature phase transition, the fluctuations that develop at a critical point are governed by classical physics, because the characteristic energy of quantum fluctuations is always smaller than the characteristic Boltzmann thermal energy formula_1.\n\nAt a quantum critical point, the critical fluctuations are quantum mechanical in nature, exhibiting scale invariance in both space and in time. Unlike classical critical points, where the critical fluctuations are limited to a narrow region around the phase transition, the influence of a quantum critical point is felt over a wide range of temperatures above the quantum critical point, so the effect of quantum criticality is felt without ever reaching absolute zero. Quantum criticality was first observed in ferroelectrics, in which the ferroelectric transition temperature is suppressed to zero.\n\nA wide variety of metallic ferromagnets and antiferromagnets have been observed to develop quantum critical behavior when their magnetic transition temperature is driven to zero through the application of pressure, chemical doping or magnetic fields. In these cases, the properties of the metal are radically transformed by the critical fluctuations, departing qualitatively from the standard Fermi liquid behavior, to form a metallic state sometimes called a non-Fermi liquid or a \"strange metal\". There is particular interest in these unusual metallic states, which are believed to exhibit a marked preponderance towards the development of superconductivity. Quantum critical fluctuations have also been shown to drive the formation of exotic magnetic phases in the vicinity of quantum critical points.\n\nQuantum critical points arise when a susceptibility diverges at zero temperature. There are a number of materials (such as CeNiGe) where this occurs serendipitously. More frequently a material has to be tuned to a quantum critical point. Most commonly this is done by taking a system with a second-order phase transition which occurs at nonzero temperature and tuning it—for example by applying pressure or magnetic field or changing its chemical composition. CePdSi is such an example, where the antiferromagnetic transition which occurs at about 10K under ambient pressure can be tuned to zero temperature by applying a pressure of 28,000 atmospheres. Less commonly a first-order transition can be made quantum critical. First-order transitions do not normally show critical fluctuations as the material moves discontinuously from one phase into another. However, if the first order phase transition does not involve a change of symmetry then the phase diagram can contain a critical endpoint where the first-order phase transition terminates. Such an endpoint has a divergent susceptibility. The transition between the liquid and gas phases is an example of a first-order transition without a change of symmetry and the critical endpoint is characterized by critical fluctuations known as critical opalescence.\n\nA quantum critical endpoint arises when a nonzero temperature critical point is tuned to zero temperature. One of the best studied examples occurs in the layered ruthenate metal, SrRuO in a magnetic field. This material shows metamagnetism with a low-temperature first-order metamagnetic transition where the magnetization jumps when a magnetic field is applied within the directions of the layers. The first-order jump terminates in a critical endpoint at about 1 kelvin. By switching the direction of the magnetic field so that it points almost perpendicular to the layers, the critical endpoint is tuned to zero temperature at a field of about 8 teslas. The resulting quantum critical fluctuations dominate the physical properties of this material at nonzero temperatures and away from the critical field. The resistivity shows a non-Fermi liquid response, the effective mass of the electron grows and the magnetothermal expansion of the material is modified all in response to the quantum critical fluctuations.\n\nAn intuitive guess of the effect of a quantum critical point being affected by noise would be that the external noise defines an effective temperature. This effective temperature would introduce a well defined energy scale in the problem and break the scale invariance of the quantum critical point. On the contrary, it was recently found that certain types of noise can induce a non-equilibrium quantum critical state. This state is out-of-equilibrium because of the continuous energy flow introduced by the noise, but it still retains the scale invariant behavior typical of critical points.\n\n"}
{"id": "40085038", "url": "https://en.wikipedia.org/wiki?curid=40085038", "title": "Regional Haze Rule", "text": "Regional Haze Rule\n\nThe Regional Haze Rule, promulgated by the United States Environmental Protection Agency in 1999, sets standards for visual air clarity in Federal Class I areas.\n\n"}
{"id": "416207", "url": "https://en.wikipedia.org/wiki?curid=416207", "title": "Rice paper", "text": "Rice paper\n\nRice paper is a product made of paper-like materials from East Asia made from different plants. These include:\n\nIn Europe, around the 1900s, a paperlike substance was originally known as rice paper, due to the mistaken notion that it is made from rice. In fact, it consists of the pith of a small tree, \"Tetrapanax papyrifer\", the \"rice paper plant\" (蓪草).\n\nThe plant grows in the swampy forests of Taiwan, and is also cultivated as an ornamental plant for its large, exotic leaves. In order to produce the paper, the boughs are boiled and freed from bark. The cylindrical core of pith is rolled on a hard flat surface against a knife, by which it is cut into thin sheets of a fine ivory-like texture.\n\nDyed in various colours, this rice paper is extensively used for the preparation of artificial flowers, while the white sheets are employed for watercolor drawings. Due to its texture, this paper is not suited for writing.\n\nThis \"rice paper\", smooth, thin, crackly, and strong, is named as a wrapper for rice, and is made from bark fibres of the paper mulberry tree. It is used for origami, calligraphy, paper screens and clothing. It is stronger than commercially made wood-pulp paper. Less commonly, the paper is made from rice straw.\n\nDepending on the type of mulberry used, it is named kozo (\"Broussonetia papyrifera\", the paper mulberry), gampi (\"Wikstroemia diplomorpha\"), or mitsumata (\"Edgeworthia chrysantha\"). The fiber comes from the bark of the paper mulberry, not the inner wood or pith, and traditionally the paper is made by hand.\n\nThe branches of the paper mulberry shrubs are harvested in the autumn, so the fibre can be processed and the paper formed during the cold winter months, because the fibre spoils easily in the heat. The branches are cut into sections two to three feet long and steamed in a large kettle, which makes the bark shrink back from the inner wood, allowing it to be pulled off like a banana peel. The bark can then be dried and stored, or used immediately. There are three layers to the bark at this stage: black bark, the outermost layer; green bark, the middle layer; and white bark, the innermost layer. All three can be made into paper, but the finest paper is made of white bark only.\n\nIf the bark strips have been dried, they are soaked in water overnight before being processed further. To clean the black and green bark from the white bark, the bark strip is spread on a board and scraped with a flat knife. Any knots or tough spots in the fibre are cut out and discarded at this stage.\n\nThe scraped bark strips are then cooked for two or three hours in a mixture of water and soda ash. The fibre is cooked enough when it can easily be pulled apart lengthwise. The strips are then rinsed several times in clean water to rinse off the soda ash. Rinsing also makes the fibre brighter and whiter—fine kozo paper is not bleached, but is naturally pure white.\n\nEach bark strip is then inspected by hand, against a white background or lit from behind by a light box. Any tiny pieces of black bark and other debris are removed with tweezers, and any knots or tough patches of fibre missed during scraping are cut out of the strips. The ultimate goal is to have completely pure white bark.\n\nThe scraped, cooked, and cleaned strips are then laid out on a table and beaten by hand. The beating tool is a wooden bat that looks like a thicker version of a cricket bat. The fibres are beaten for about half an hour, or until all the fibres have been separated and no longer resemble strips of bark.\n\nThe prepared fibre can now be made into sheets of paper. A viscous substance called formation aid is added to the vat with the fibre and water. Formation aid is polyethylene oxide, and it helps slow the flow of water, which gives the paper-maker more time to form sheets. Sheets are formed with multiple thin layers of fibre, one on top of another.\n\nEdible rice paper is used for making fresh summer rolls (salad rolls) or fried spring rolls in Vietnamese cuisine, where the rice paper is called bánh tráng or bánh đa nem. Ingredients of the food rice paper include white rice flour, tapioca flour, salt, and water. The tapioca powder makes the rice paper glutinous and smooth. It is usually sold dried in thin, crisp, translucent round sheets that are wrapped in cellophane. The sheets are dipped briefly in hot water to soften them, then wrapped around savoury or sweet ingredients.\n\nEdible paper is used in the home baking of foods such as macaroons and is often sold separately as colored sheets that are either plain or printed with images, such as bank notes and dollar bills.\n\nIn the pilot episode of the television series \"Kung Fu\", Kwai Chang Caine undergoes training to become a Shaolin priest. One of the challenges he faces is to walk on a long sheet of rice paper without tearing it or leaving any marks of his passage. His successful completion of the test is incorporated into the series' opening title sequence, with the narration, \"When you can walk its length and leave no trace, you will have learned.\"\n\n"}
{"id": "615075", "url": "https://en.wikipedia.org/wiki?curid=615075", "title": "Stone carving", "text": "Stone carving\n\nStone carving is an activity where pieces of rough natural stone are shaped by the controlled removal of stone. Owing to the permanence of the material, stone work has survived which was created during our prehistory.\n\nWork carried out by paleolithic societies to create flint tools is more often referred to as knapping. Stone carving that is done to produce lettering is more often referred to as lettering. The process of removing stone from the earth is called mining or quarrying.\n\nStone carving is one of the processes which may be used by an artist when creating a sculpture. The term also refers to the activity of masons in dressing stone blocks for use in architecture, building or civil engineering. It is also a phrase used by archaeologists, historians, and anthropologists to describe the activity involved in making some types of petroglyphs.\n\nThe earliest known works of representational art are stone carvings. Often marks carved into rock or petroglyphs will survive where painted work will not. Prehistoric Venus figurines such as the Venus of Berekhat Ram may be as old as 800,000 years, and are carved in stones such as tuff and limestone.\n\nThese earliest examples of the stone carving are the result of hitting or scratching a softer stone with a harder one, although sometimes more resilient materials such as antlers are known to have been used for relatively soft stone. Another early technique was to use an abrasive that was rubbed on the stone to remove the unwanted area.\nPrior to the discovery of steel by any culture, all stone carving was carried out by using an abrasion technique, following rough hewing of the stone block using hammers. The reason for this is that bronze, the hardest available metal until steel, is not hard enough to work any but the softest stone. The Ancient Greeks used the ductility of bronze to trap small granules of carborundum, that are naturally occurring on the island of Milos, thus making a very efficient file for abrading the stone.\n\nThe development of iron made possible stone carving tools, such as chisels, drills and saws made from steel, that were capable of being hardened and tempered to a state hard enough to cut stone without deforming, while not being so brittle as to shatter. Carving tools have changed little since then.\n\nModern, industrial, large quantity techniques still rely heavily on abrasion to cut and remove stone, although at a significantly faster rate with processes such as water erosion and diamond saw cutting.\n\nOne modern stone carving technique uses a new process: The technique of applying sudden high temperature to the surface. The expansion of the top surface due to the sudden increase in temperature causes it to break away. On a small scale, Oxy-acetylene torches are used. On an industrial scale, lasers are used. On a massive scale, carvings such as the Crazy Horse Memorial carved from the Harney Peak granite of Mount Rushmore and the Confederate Memorial Park in Albany, Georgia are produced using jet heat torches.\n\nCarving stone into sculpture is an activity older than civilization itself. Prehistoric sculptures were usually human forms, such as the Venus of Willendorf and the faceless statues of the Cycladic cultures of ancient Greece. Later cultures devised animal, human-animal and abstract forms in stone. The earliest cultures used abrasive techniques, and modern technology employs pneumatic hammers and other devices. But for most of human history, sculptors used hammer and chisel as the basic tools for carving stone.\n\nThe process begins with the selection of a stone for carving. Some artists use the stone itself as inspiration; the Renaissance artist Michelangelo claimed that his job was to free the human form trapped inside the block. Other artists begin with a form already in mind and find a stone to complement their vision. The sculptor may begin by forming a model in clay or wax, sketching the form of the statue on paper or drawing a general outline of the statue on the stone itself.\n\nWhen ready to carve, the artist usually begins by knocking off large portions of unwanted stone. This is the \"roughing out\" stage of the sculpting process. For this task they may select a point chisel, which is a long, hefty piece of steel with a point at one end and a broad striking surface at the other. A pitching tool may also be used at this early stage; which is a wedge-shaped chisel with a broad, flat edge. The pitching tool is useful for splitting the stone and removing large, unwanted chunks. Those two chisels are used in combination with a masons driving hammer.\n\nOnce the general shape of the statue has been determined, the sculptor uses other tools to refine the figure. A toothed chisel or claw chisel has multiple gouging surfaces which create parallel lines in the stone. These tools are generally used to add texture to the figure. An artist might mark out specific lines by using calipers to measure an area of stone to be addressed, and marking the removal area with pencil, charcoal or chalk. The stone carver generally uses a shallower stroke at this point in the process, usually in combination with a wooden mallet.\n\nEventually the sculptor has changed the stone from a rough block into the general shape of the finished statue. Tools called rasps and rifflers are then used to enhance the shape into its final form. A rasp is a flat, steel tool with a coarse surface. The sculptor uses broad, sweeping strokes to remove excess stone as small chips or dust. A riffler is a smaller variation of the rasp, which can be used to create details such as folds of clothing or locks of hair.\n\nThe final stage of the carving process is polishing. Sandpaper can be used as a first step in the polishing process, or sand cloth. Emery, a stone that is harder and rougher than the sculpture media, is also used in the finishing process. This abrading, or wearing away, brings out the color of the stone, reveals patterns in the surface and adds a sheen. Tin and iron oxides are often used to give the stone a highly reflective exterior.\n\nSculptures can be carved via either the direct or the indirect carving method. Indirect carving is a way of carving by using an accurate clay, wax or plaster model, which is then copied with the use of a compass or \"proportional dividers\" or a pointing machine. The direct carving method is a way of carving in a more intuitive way, without first making an elaborate model. Sometimes a sketch on paper or a rough clay draft is made.\n\nStone has been used for carving since ancient times for many reasons. Most types of stone are easier to find than metal ores, which have to be mined and smelted. Stone can be dug from the surface and carved with hand tools. Stone is more durable than wood, and carvings in stone last much longer than wooden artifacts. Stone comes in many varieties and artists have abundant choices in color, quality and relative hardness.\n\nSoft stone such as chalk, soapstone, pumice and Tufa can be easily carved with found items such as harder stone or in the case of chalk even the fingernail.\nLimestones and marbles can be worked using abrasives and simple iron tools.\nGranite, basalt and some metamorphic stone is difficult to carve even with iron or steel tools; usually tungsten carbide tipped tools are used, although abrasives still work well. Modern techniques often use abrasives attached to machine tools to cut the stone.\n\nPrecious and semi-precious gemstones are also carved into delicate shapes for jewellery or larger items, and polished; this is sometimes referred to as lapidary, although strictly speaking lapidary refers to cutting and polishing alone.\n\nWhen worked, some stones release dust that can damage lungs (silica crystals are usually to blame), so a respirator is sometimes needed.\n\nBasic stone carving tools fall into five categories:\n\n\nMore advanced processes, such as laser cutting and jet torches, use sudden high temperature with a combination of cooling water to spall flakes of stone. Other modern processes may involve diamond-wire machines or other large scale production equipment to remove large sections of undesired stone.\n\nThe use of chisels for stone carving is possible in several ways. Two are:\n\n\nThere are many types and styles of stone carving tools, each carver will decide for themselves which tools to use. Traditionalists might use hand tools only. \n\nPowered pneumatic hammers make the hard work easier. Progress on shaping stone is faster with pneumatic carving tools. Air hammers (such as Cuturi) place many thousands of impacts per minute upon the end of the tool, which would usually be manufactured or modified to suit the purpose. This type of tool creates the ability to 'shave' the stone, providing a smooth and consistent stroke, allowing for larger surfaces to be worked.\n\nAmong modern tool types, there are two main stone carving chisels:\n\n\n"}
{"id": "3922822", "url": "https://en.wikipedia.org/wiki?curid=3922822", "title": "Strontium fluoride", "text": "Strontium fluoride\n\nStrontium fluoride, SrF, also called strontium difluoride and strontium(II) fluoride, is a fluoride of strontium. It is a stable brittle white crystalline solid with melting point of 1477 °C and boiling point 2460 °C. It appears as the mineral strontiofluorite.\n\nStrontium fluoride is prepared by reaction of strontium chloride with fluorine gas, or by action of hydrofluoric acid on strontium carbonate.\n\nThe solid adopts the fluorite structure. In the vapour phase the SrF molecule is non-linear with an F−Sr−F angle of approximately 120°. This is an exception to VSEPR theory which would predict a linear structure. Ab initio calculations have been cited to propose that contributions from d orbitals in the shell below the valence shell are responsible. Another proposal is that polarization of the electron core of the strontium atom creates an approximately tetrahedral distribution of charge that interacts with the Sr−F bonds.\n\nIt is almost insoluble in water (its K value is approximately 2.0x10 at 25 degrees Celsius).\n\nIt irritates eyes and skin, and is harmful when inhaled or ingested.\nSimilar to CaF and BaF, SrF displays superionic conductivity at elevated temperatures.<ref name=\"http://www.newmet.co.uk/Products/koch/strontium.php\">https://web.archive.org/web/20051214052733/http://www.newmet.co.uk/Products/koch/strontium.php</ref>\n\nStrontium fluoride is transparent to light in the wavelengths from vacuum ultraviolet (150 nm) to infrared (11 µm). Its optical properties are intermediate to calcium fluoride and barium fluoride.<ref name=\"http://www.crystran.co.uk/strontium-fluoride-srf2.htm\"></ref>\n\nStrontium fluoride is used as an optical material for a small range of special applications, for example, as an optical coating on lenses and also as a thermoluminescent dosimeter crystal.\n\nAnother use is as a carrier of strontium-90 radioisotope in radioisotope thermoelectric generators.\n"}
{"id": "41465446", "url": "https://en.wikipedia.org/wiki?curid=41465446", "title": "Sulfoselenide", "text": "Sulfoselenide\n\nIn chemistry, a sulfoselenide is a compound containing both metal sulfides and metal selenides. Because metal sulfides and metal selenides have similar crystal structures, they exhibit some mutual solubility, forming solid solutions. Since the ionic radius sulfide of (S) is however much smaller than that for selenide (Se), the solubility ranges can be only limited. For example, pyrite (FeS) will accept only a few percent of selenium in place of sulfur. A broader range is seen for the solid solution of cadmium sulfide and cadmium selenide. CdS is yellow and CdSe is red. The sulfoselenides of cadmium are orange. They are used as an artist's pigment.\n"}
{"id": "92447", "url": "https://en.wikipedia.org/wiki?curid=92447", "title": "Superoxide", "text": "Superoxide\n\nA superoxide is a compound that contains the superoxide anion, which has the chemical formula . The systematic name of the anion is dioxide(1−). The reactive oxygen anion superoxide is particularly important as the product of the one-electron reduction of dioxygen O, which occurs widely in nature. Molecular oxygen (dioxygen) is a diradical containing two unpaired electrons, and superoxide results from the addition of an electron which fills one of the two degenerate molecular orbitals, leaving a charged ionic species with a single unpaired electron and a net negative charge of −1. Both dioxygen and the superoxide anion are free radicals that exhibit paramagnetism.\n\nThe superoxide anion, , and its protonated form, the hydroperoxyl radical HO, are in equilibrium in an aqueous solution:\n\nGiven that the hydroperoxyl radical has a p\"K\" of around 4.8, at neutral pH superoxide predominantly exists in the anionic form.\n\nSuperoxides form salts with alkali metals and alkaline earth metals. The salts CsO, RbO, KO, and NaO are prepared by the reaction of O with the respective alkali metal.\n\nThe alkali salts of are orange-yellow in color and quite stable, provided they are kept dry. Upon dissolution of these salts in water, however, the dissolved undergoes disproportionation (dismutation) extremely rapidly (in a pH-dependent manner):\n\nThis reaction (with moisture and carbon dioxide in exhaled air) is the basis of the use of potassium superoxide as an oxygen source in chemical oxygen generators, such as those used on the space shuttle and on submarines. Superoxides are also used in firefighters' oxygen tanks in order to provide a readily available source of oxygen.\n\nIn this process acts as a Brønsted base, initially forming the radical HO·. But the p\"K\" of its conjugate acid, hydrogen superoxide (HO·, also known as \"hydroperoxyl\" or \"perhydroxy radical\"), is 4.88 so that at neutral pH 7 all but 0.3% of superoxide is in the anionic form, .\n\nPotassium superoxide is soluble in dimethyl sulfoxide (facilitated by crown ethers) and is stable as long as protons are not available. Superoxide can also be generated in aprotic solvents by cyclic voltammetry.\n\nSalts also decompose in the solid state, but this process requires heating:\n\nSuperoxide and hydroperoxyl (HO) are discussed interchangeably, although superoxide predominates at physiological pHs. Both superoxide and hydroperoxyl are classified as reactive oxygen species. It is generated by the immune system to kill invading microorganisms. In phagocytes, superoxide is produced in large quantities by the enzyme NADPH oxidase for use in oxygen-dependent killing mechanisms of invading pathogens. Mutations in the gene coding for the NADPH oxidase cause an immunodeficiency syndrome called chronic granulomatous disease, characterized by extreme susceptibility to infection, especially catalase-positive organisms. In turn, micro-organisms genetically engineered to lack superoxide dismutase (SOD) lose virulence. Superoxide is also deleterious when produced as a byproduct of mitochondrial respiration (most notably by Complex I and Complex III), as well as several other enzymes, for example xanthine oxidase.\n\nBecause superoxide is toxic at high concentrations, nearly all organisms living in the presence of oxygen contain superoxide-scavenging enzymes called superoxide dismutase (SOD). SOD efficiently catalyzes the disproportionation of superoxide:\nOther proteins that can be both oxidized and reduced by superoxide (e.g., hemoglobin) have weak SOD-like activity. Genetic inactivation (\"knockout\") of SOD produces deleterious phenotypes in organisms ranging from bacteria to mice and have provided important clues as to the mechanisms of toxicity of superoxide in vivo.\n\nYeast lacking both mitochondrial and cytosolic SOD grow very poorly in air, but quite well under anaerobic conditions. Absence of cytosolic SOD causes a dramatic increase in mutagenesis and genomic instability. Mice lacking mitochondrial SOD (MnSOD) die around 21 days after birth due to neurodegeneration, cardiomyopathy, and lactic acidosis. Mice lacking cytosolic SOD (CuZnSOD) are viable but suffer from multiple pathologies, including reduced lifespan, liver cancer, muscle atrophy, cataracts, thymic involution, haemolytic anemia and a very rapid age-dependent decline in female fertility.\n\nSuperoxide may contribute to the pathogenesis of many diseases (the evidence is particularly strong for radiation poisoning and hyperoxic injury), and perhaps also to aging via the oxidative damage that it inflicts on cells. While the action of superoxide in the pathogenesis of some conditions is strong (for instance, mice and rats overexpressing CuZnSOD or MnSOD are more resistant to strokes and heart attacks), the role of superoxide in aging must be regarded as unproven for now. In model organisms (yeast, the fruit fly Drosophila and mice), genetically knocking out CuZnSOD shortens lifespan and accelerates certain features of aging (cataracts, muscle atrophy, macular degeneration, thymic involution). But the converse, increasing the levels of CuZnSOD, does not seem (except perhaps in \"Drosophila\"), to consistently increase lifespan. The most widely accepted view is that oxidative damage (resulting from multiple causes, including superoxide) is but one of several factors limiting lifespan.\n\nThe binding of O by heme proteins involves formation of Fe(III) superoxide complex.\n\nThe assay of superoxide generated in biological systems is a difficult task because of its high reactivity and short half-life. One approach that has been used in quantitative assays converts superoxide to hydrogen peroxide, which is relatively stable. Hydrogen peroxide is then assayed by a fluorimetric method. As a free radical, superoxide has a strong EPR signal, and it is possible to detect superoxide directly using this method when its abundance is high enough. For practical purposes, this can be achieved only in vitro under non-physiological conditions, such as high pH (which slows the spontaneous dismutation) with the enzyme xanthine oxidase. Researchers have developed a series of tool compounds termed \"spin traps\"(see spin trapping) that can react with superoxide, forming a meta-stable radical (half-life 1–15 minutes), which can be more readily detected by EPR. Superoxide spin trapping was initially carried out with DMPO, but, more recently, phosphorus derivatives with improved half-lives, such as DEPPMPO and DIPPMPO, have become more widely used.\n\nSuperoxides are compounds in which the oxidation number of oxygen is −. Whereas molecular oxygen (dioxygen) is a diradical containing two unpaired electrons, the addition of a second electron fills one of its two degenerate molecular orbitals, leaving a charged ionic species with single unpaired electron and a net negative charge of −1. Both dioxygen and the superoxide anion are free radicals that exhibit paramagnetism. \n\nThe derivatives of dioxygen have characteristic O–O distances that correlate with the order of the O–O bond.\n"}
{"id": "3140633", "url": "https://en.wikipedia.org/wiki?curid=3140633", "title": "TACA Flight 110", "text": "TACA Flight 110\n\nTACA Flight 110 was an international scheduled airline flight operated by TACA Airlines, traveling from Belize to New Orleans. On May 24, 1988, the Boeing 737-300 lost power in both engines, but its pilots made a successful deadstick landing on a grass levee adjacent to NASA's Michoud Assembly Facility, with no one aboard sustaining more than minor injuries, and with only minor damage to the aircraft. After engine replacement, the aircraft was able to take off from the disused runway at Michoud and subsequently returned to service.\n\nThe aircraft, a Boeing 737-3T0 (tail number N75356, serial number 23838), had first flown on January 26, 1988, and had been in service with TACA for about two weeks. On this day, the flight proceeded normally, taking off from Belize City's Philip S. W. Goldson International Airport and flying over the Gulf of Mexico toward the Louisiana coast.\n\nThe airliner was the 1,505th Boeing 737 manufactured, and was originally acquired by TACA from Polaris Aircraft Leasing in May 1988.\n\nThe captain of the flight was Carlos Dardano. At just 29 years of age, Dardano had already amassed 13,410 flight hours. Almost 11,000 of these hours were as pilot in command. Earlier in his career, he had lost an eye to crossfire on a short flight to El Salvador, where civil war was raging at the time. The first officer, Dionisio Lopez, was also very experienced, with more than 12,000 flight hours logged. Captain Arturo Soley, an instructor pilot, was also in the cockpit, monitoring the performance of the new 737.\n\nInvestigation by the National Transportation Safety Board (NTSB) revealed that during descent from FL 350 (about ) in preparation for their impending arrival at New Orleans International Airport, Captain Dardano and First Officer Lopez noticed areas of light to moderate precipitation in their path, depicted as green and yellow areas on their weather radar, as well as \"some isolated red cells\" indicative of heavy precipitation to both sides of their intended flight path.\n\nThe flight entered clouds at FL 300 (about ), the crew selecting \"continuous ignition\" and turning on engine anti-ice to protect their turbofan engines from the effects of precipitation and icing, either of which is capable of causing a flameout, where the engines lose all power. Despite flying a route between the two areas of heavy precipitation shown on radar, they encountered heavy rain, hail, and turbulence. Passing through , both engines flamed out, leaving the jet gliding with neither engine producing thrust or electrical power. The auxiliary power unit (APU) was started as the plane descended through , restoring electrical power. While attempts to \"windmill start\" the engines using the airflow generated by the plane's descent were unsuccessful, the pilots were later able to start them using the engine starters, which were powered by the APU. However, neither engine would accelerate to normal idle speed, much less to a point where it was producing meaningful thrust. Attempts to advance the throttles only resulted in overheating of the engines, so they were once more shut down to avoid catastrophic failure. \n\nAt this point, the pilots began to prepare for a ditching, as no runway was reachable with the remaining altitude. Dardano lined up with a canal and prepared the aircraft for a water landing. During this time, Lopez spotted a grass levee to the right of the canal, and suggested that the landing be attempted there. Dardano agreed, and landed the airliner in an unpowered glide on top of the narrow grass levee. Adjacent to the levee was the NASA Michoud Assembly Facility industrial complex, in the Michoud area of eastern New Orleans, near the Gulf Intracoastal Waterway.\n\nNTSB investigators determined that the aircraft had inadvertently flown into a level 4 thunderstorm and that water ingestion had caused both engines to flame out despite their being certified as meeting Federal Aviation Administration (FAA) standards for water ingestion. The aircraft suffered mild hail damage, and its right-side (number 2) engine was damaged from overheating.\n\nTo avoid similar problems in the future, the engine manufacturer, CFM International, modified the CFM56 engine by adding a sensor to force the combustor to continuously ignite under heavy rain and/or hail conditions. Other modifications were made to the engine nose cone and the spacing of the fan blades to better deflect hail away from the engine core. Also, additional bleed doors were added to drain more water from the engine.\n\nInitially, it was planned to remove the wings and transport the airplane to a repair facility by barge, but Boeing engineers and test pilots decided to perform an engine change on site. The aircraft was towed from the levee to the nearby NASA facility, fueled to the minimum amount needed and departed from Saturn Boulevard, a roadway built atop the original second world war era runway. Following takeoff, the 737 flew to Moisant Field, where further maintenance work was performed. \n\nAfter its return to service, the plane was eventually acquired by Southwest Airlines as registration N697SW. It continued in service until December 2016, when it was retired and placed into storage at Pinal Airpark.\n\nThe was featured in \"Nowhere to Land\", the 11th episode of the 11th season of \"Mayday (Air Crash Investigation)\".\n\n\n"}
{"id": "44171608", "url": "https://en.wikipedia.org/wiki?curid=44171608", "title": "The Green Bubble", "text": "The Green Bubble\n\nThe Green Bubble is a theory that the world is facing an over-investment in renewable energy and that the current levels of debts in many clean tech companies are unsustainable. As the interest rate rises many of the projects that are on the market today will go bust, which is claimed to become a big set-back for the renewable energy industry.\n\nThe term has been mentioned by several experts and articles and among them are: The book called \"The Green Bubble\" written by Per Wimmer, a Danish investor, an article written in a magazine called \"Wired\" that stresses on what happened to the solar energy companies (i.e. Solyndra for example), and the article written by Ted Nordhaus and Michael Shellenberger in 2009, \"The Green Bubble: Why Environmentalism Keeps Imploding\".\n\nThe article summarizes the history of green technology and the changes in the investments in this domain. In addition, a discussion on whether there is a bubble in green technology or not.\n\nThe history of green technology, also known as renewable energy, is not quite new. According to Alexis Madrigal, author of “Powering the Dream: The History and Promise of Green Technology,” a large number of windmills and solar heaters already existed in the early 20th century. Despite the fact that various green technologies have been used for centuries, the history of minor technologies is quite obscure since there are a few credible records.\n\nFrom the early 21st century, there has been a large development and investment in renewable energy industry such as hydroelectricity, wind power, solar thermal, and geothermal. It is known that a “mini green bubble” had already taken place between 2005 to 2007, being terminated by the recession in 2007. The bubble made a drastic fluctuation in stock prices of companies with respect to green technology; for instance, the stock price of World Water & Solar technologies Inc., which deals with solar powered water pump, once experienced a steep rise from 5 cents to $2.50 in 2007, subsequently facing a decline to 29 cents after the recession.\n\nHowever, the whole amount of global investment on renewable energy still increased until 2011, from $40 billion in 2004 to $279 billion in 2011. By contrast, the amount of investment decreased between 2011 and 2013, from $279 billion to $214 billion. This decline is said to be attributed to the improvement in the efficiency of technology costs.\n\n\"Are we Headed toward a Green Bubble\"? is the name of an article by Julie Bennet, which provides interesting facts and opinions regarding green technology markets. Bennet suggested that a little Green Bubble deflated during Sub-Prime Crisis in 2007 and the green market was in a pivotal time in 2010: If the economy was recovered, the green market may have emerged or simply a green bubble may have exploded. The article argued high promises from the green tech sector that may not be accomplished. For example, CleanEdge predicted that global markets for bio fuels, wind and solar energy will reach $ 325 billion by 2018. However, only 3.4% of total electricity expended in the U.S. came from renewal energy in 2010, while other related industries, including electric cars and solar energy, are still not competitive in their markets.\n\nJuliet Eilperin's article \"Why the Clean Tech Boom Went Bust\", provided some evidence regarding the fact that green technology sector was not growing as fast as the market expected. The article argued that The Obama administration, during 2009, tried to increase the investment in green technology markets, providing US$150 billion for development. Green Technology markets may receive more subsidies for development than any other technology industries, Eliperin indicated. Another interesting fact provided by Eilperin's article is the highly divided nature of the green technology market. According to Ernst & Young, green technology market is divided into 46 different categories and It could be possible to find some specific markets with emerging bubbles.\n\nReid Lifset in \"Beyond the Green Bubble\" provided a new point: green technology market is following the issue-attention cycle studied by Downs in \"Up and down the ecology, 1972\", which basically emphasized the cycle idea of a market governed by public policy. Lifset suggested that there was a Green Bubble before Sub-Prime Crisis, which could have provided opportunities for new investors in the market. He also argued that the green technology market will maintain its enthusiasm, mostly encouraged by government agencies.\n"}
{"id": "467047", "url": "https://en.wikipedia.org/wiki?curid=467047", "title": "Thermal energy", "text": "Thermal energy\n\nThermal energy can refer to several distinct thermodynamic quantities, such as the internal energy of a system; heat or sensible heat, which are defined as types of transfer of energy (as is work); or for the characteristic energy of a degree of freedom in a thermal system formula_1, where formula_2 is temperature and formula_3 is the Boltzmann constant.\n\nHeat is energy transferred spontaneously from a hotter to a colder system or body. Heat is energy in transfer, not a property of any one system, or 'contained' within it. On the other hand, internal energy is a property of a system. In an ideal gas, the internal energy is the sum total of the gas particles' kinetic energy, and it is this kinetic motion that is the source and the effect of the transfer of heat across a system's boundary. For this reason, the term \"thermal energy\" is sometimes used synonymously with internal energy. (Heat and work depend on the way in which an energy transfer occurred, whereas internal energy is a property of the state of a system and can thus be understood even without knowing how the energy got there.) The term \"thermal energy\" is also applied to the energy carried by a heat flow, although this quantity can also simply be called heat or amount of heat.\n\nIn many statistical physics texts, \"thermal energy\" refers to formula_1, the product of Boltzmann's constant and the absolute temperature, also written as formula_5.\n\nIn an 1847 lecture titled \"On Matter, Living Force, and Heat\", James Prescott Joule characterised various terms that are closely related to thermal energy and heat. He identified the terms latent heat and sensible heat as forms of heat each affecting distinct physical phenomena, namely the potential and kinetic energy of particles, respectively. He described latent energy as the energy of interaction in a given configuration of particles, i.e. a form of potential energy, and the sensible heat as an energy affecting temperature measured by the thermometer due to the thermal energy, which he called the living force.\n\nIf the minimum temperature of a system's environment is formula_6 and the system's entropy is formula_7, then a part of the system's internal energy amounting to formula_8 cannot be converted into useful work. This is the difference between the internal energy and the Helmholtz free energy.\n\n"}
{"id": "20405837", "url": "https://en.wikipedia.org/wiki?curid=20405837", "title": "Theta meson", "text": "Theta meson\n\nThe theta meson () is a hypothetical form of quarkonium (i.e. a flavourless meson) formed by a top quark () and top antiquark (). As a P-odd and C-odd state, it is analogous to the (), () and () mesons. Due to the top quark's short lifetime, the theta meson is not expected to be observed in nature.\n"}
{"id": "6994344", "url": "https://en.wikipedia.org/wiki?curid=6994344", "title": "Trans-Caspian Gas Pipeline", "text": "Trans-Caspian Gas Pipeline\n\nThe Trans-Caspian Gas Pipeline (, ) is a proposed subsea pipeline between Türkmenbaşy in Turkmenistan, and Baku in Azerbaijan. According to some proposals it will also include a connection between the Tengiz Field in Kazakhstan, and Türkmenbaşy. The Trans-Caspian Gas Pipeline project is purposed to transport natural gas from Turkmenistan and Kazakhstan to European Union member countries, circumventing both Russia and Iran. It is also considered as a natural eastward extension of Southern Gas Corridor. This project attracts significant interest since it will connect vast Turkmen gas resources to major consumer geographies as Turkey and Europe.\n\nA project to import natural gas from Turkmenistan through a subsea pipeline was suggested in 1996 by the United States. In February 1999, the Turkmen government entered into an agreement with General Electric and Bechtel Group for a feasibility study on the proposed pipeline. In 1999, while attending the Organisation for Economic Co-operation and Development meeting in Istanbul, Turkey, Georgia, Azerbaijan and Turkmenistan signed a number of agreements concerned with construction of pipelines. However, because of Russian and Iranian opposition to the project, an unresolved legal dispute over Caspian Sea territorial boundaries and a gas discovery on Azerbaijan's Shah Deniz field, the submarine pipeline project was shelved in the summer of 2000 and only the South Caucasus Pipeline project continued.\n\nIn January 2006, as a result of the Russia-Ukraine gas dispute, interest in the Trans-Caspian Gas Pipeline project was rekindled. On 11 January 2006, Azerbaijan's prime-minister Artur Rasizade proposed to his Kazakhstan counterpart Danial Ahmetov that Kazakhstan gas be exported through the South Caucasus Pipeline to Turkey and from there to the European market. In March 2006, Turkmen President Saparmurat Niyazov signaled his intention to rejoin possible negotiations on the pipeline. In May 2006, during his visit to Kazakhstan, the European Commissioner for Energy Andris Piebalgs professed EU support for the construction of the Trans-Caspian pipeline. Azerbaijan's Industry and Energy Minister Natig Aliyev, while addressing an international energy conference in Baku, outlined the advantages of the Trans-Caspian gas pipeline for diversifying supplies and restraining prices. On the other hand, Russia's Industry and Energy Minister Viktor Khristenko commented that existing technical, legal, environmental and other risks relating to the trans-Caspian project are so great that it would be impossible to find an investor unless there is political backing for the project. On 12 May 2007, an agreement was signed between Russia, Kazakhstan and Turkmenistan providing for Central Asian gas to be exported to Europe through the reconstructed and expanded western branch of the Central Asia-Center gas pipeline system. This was seen as a setback for the realization of the Trans-Caspian Pipeline although Turkmen President Gurbanguly Berdimuhamedow said that the Trans-Caspian pipeline project was not canceled.\n\nOn 4 September 2008, Iran's deputy foreign minister Mehti Safari confirmed that Tehran opposes the construction of any undersea pipelines in the Caspian because of environmental concerns. This jeopardizes the Trans-Caspian Gas pipeline project, according to regional expert Paul Goble. However, on 22 December 2008 Austria's OMV and Germany's RWE, both partners in Nabucco Gas Pipeline International GmbH, announced they were setting up a joint venture named the Caspian Energy Company, to carry out exploration for a gas pipeline across the Caspian Sea that would feed into the Nabucco pipeline. Based on exploration outcomes the company plans to build and operate a gas transport system across the Caspian Sea.\n\nOn 12 September 2011, the EU Foreign Affairs Council agreed to give a negotiating mandate to the European Commission for negotiations with Azerbaijan and Turkmenistan on the Trans-Caspian Gas Pipeline. On 3 September 2012, after the meeting between the European Commissioner for Energy Günther Oettinger, Turkish Energy Minister Taner Yıldız, and Azerbaijani and Turkmenistani officials in Ashgabat, Yıldız stated that Turkey will buy gas from Turkmenistan through the Trans-Caspian Gas Pipeline.\n\nAn EU proposal generally named the Southern Gas Corridor project kindled interest in the Trans+Caspian pipeline as an alternative supply route to Gasprom monopoly to European Union markets. Turkmen gas would be carried along with Azeri gas from the Shah Deniz Gas Field by this pipeline.\n\nOne part of the Southern Gas Corridor will be laid between Greece and Italy via Albania starting in 2016 and this pipeline will join with the TAP Pipeline across Turkey which is under construction since 2015 and it will meet the existing South Caucasus Pipeline at the Georgian border with Turkey\n\nThe projected capacity of the pipeline is of natural gas per year at an estimated cost of US$5 billion. In Baku, it would link to the South Caucasus Pipeline (Baku-Tbilisi-Erzurum pipeline), and through this with the planned Trans-Anatolian gas pipeline. A feasibility study for the project funded by the United States Trade and Development Agency is carried out by Granherne, a subsidiary of KBR.\n\nThe project is heavily criticized by Russia and Iran, current transit countries for Turkmen gas. Alexander Golovin, special envoy on Caspian issues, has stated that a major gas pipeline would pose a serious, dangerous risk to the prosperity of the entire region. According to the Russian Natural Resources Ministry, any gas or oil pipelines across the floor of the Caspian Sea would be environmentally unacceptable. Russia has also taken the legal position that a potential pipeline project, regardless of the route it takes on the seabed, would require the consent of all five Caspian littoral states in order to proceed. Iran has pointed out that treaties signed by Iran and the Soviet Union in 1921 and 1940 are still in force and that any action taken without the consent of all the littoral states would be illegal. In regard of the decision taken by the EU on 12 September 2011, Russia expressed its \"disappointment\" as it \"seems to have been adopted without taking into account the internationally accepted legal and geopolitical situation in the Caspian basin,\" and as Caspian Sea littoral state, Russia could veto any international agreement allowing for the pipeline to be built.\n\nIn reaction to the 1999 plans for a Trans-Caspian gas pipeline Russia and Iran collaborated in calling for a Central Asian gas cartel in 2001 and 2002. There is also a concern in the West that closer collaboration between Georgia and Azerbaijan will isolate Armenia and tempt it to strengthen ties with Russia and Iran.\n\n\n"}
{"id": "10340923", "url": "https://en.wikipedia.org/wiki?curid=10340923", "title": "Vacuum consolidation", "text": "Vacuum consolidation\n\nVacuum consolidation (or vacuum preloading) is a soft soil improvement method that has been successfully used by geotechnical engineers and specialists of ground improvement companies in countries such as Australia, China, Korea, Thailand and France for soil improvement or land reclamation (Chu et al., 2005). It does not necessarily require surcharge fill and vacuum loads of 80kPa or greater can, typically, be maintained for as long as required. However, if loads of 80kPa or greater are needed in order to achieve the target soil improvement, additional surcharge may be placed on top of the vacuum system. The vacuum preloading method is cheaper and faster than the fill surcharge method for an equivalent load in suitable areas. Where the underlying ground consists of permeable materials, such as sand or sandy clay, the cost of the technique will be significantly increased due to the requirement of cut-off walls into non-permeable layers to seal off the vacuum. It has been suggested by Carter et al. (2005) that the settlement resulting from vacuum preloading is less than that from a surcharge load of the same magnitude as vacuum consolidation is influenced by drainage boundary conditions.\n\n"}
{"id": "6949412", "url": "https://en.wikipedia.org/wiki?curid=6949412", "title": "Vectrix", "text": "Vectrix\n\nVectrix was an electric vehicle company based in Middletown, Rhode Island, United States, with research and development facilities in New Bedford, Massachusetts and an assembly plant in Wrocław, Poland. Vectrix ceased all US operations as of December 31, 2013. The company filed for bankruptcy and final liquidation in March 2014.\n\nIntroduced in 2006, the Vectrix VX-1 was a maxi-size scooter, and was the first commercially available high-performance electric scooter. \nIt was capable of over , and was reached in a little under 7 seconds, with maximum torque available from 0 rpm, a characteristic of electric motors. It has under 250 parts, compared with 2,500 for a conventionally powered scooter, and has a range of up to at .\n\nIn the United Kingdom, Italy, the Netherlands and Slovenia, the Vectrix is exempt from paying road tax.\n\nThe Vectrix scooter uses NiMH batteries with a manufacturer-claimed life of 10 years and 1,500 recharges.\nThe 125-volt battery pack has a capacity of 3.7 kW·h and can be recharged to 80% in two hours from a standard domestic power socket. The battery can also be partially recharged through regenerative braking.\nReplacement cost of the battery is estimated to be around $3,000, almost one third of the cost of the bike.\nIn June 2008, Vectrix indicated that it planned to test lithium ion battery packs based on lithium iron phosphate battery technology, in an agreement with GP Batteries International Limited of Hong Kong.\n\nIn the fall of 2008, Vectrix announced an expanded product line with two lower-priced bikes: The VX-1E was projected to arrive March 2009, with the VX-2 following in June 2009. Pre-production models of both bikes were shown at the New York International Motorcycle Show in January 2009 and also at Birmingham Motor Show, but they never entered production due to the company ceasing trading.\n\nVectrix showed a superbike concept vehicle at the 2007 Milan motorcycle show, to be produced if 500 deposits were received.\nIn 2012 New Vectrix (re)-unveiled the super bike prototype at the SWISS-MOTO 2012 show in Zürich, Switzerland, announcing that they are taking orders and may produce the bike with as little as 200 pre-orders. \nIn 2008, Vectrix announced a 3-wheeled version of the Vectrix Maxi scooter.\n\nThe New York City Police Department announced in December 2007 that it will be testing vehicles from Vectrix with the goal of replacing its current gasoline-powered scooters.\n\nThe Government of Canada purchased in August 2008 a vehicle from Vectrix with the goal of testing and evaluating a fully electric compliant open motorcycle's energy consumption, range, and additional road testing parameters. This environmental initiative is part of Transport Canada’s ecoTECHNOLOGY for Vehicles (eTV) program.\n\nIn July 2009 Vectrix Corporation laid off all but essential staff, this followed several months of announcements of financial problems. Analysis of corporate financial results showed the company listed expenses for each bike sold larger than the income earned. On September 28, 2009 the company announced a Chapter 11 bankruptcy filing in Delaware and that a New Vectrix might buy the assets of Vectrix to recapitalize a new company. In late 2009 its assets were sold to Gold Peak battery group, allowing the company to relaunch.\n\nFor much of 2010 the company rehired old and new staff and began supporting old owners with issues again. As 2011 rolled around Vectrix introduced the VX-2 and the VX-1 Li/Li+ into the product line showing at some shows and updated on their website. The VX-1 Li is the same as the original bike but using lithium batteries of 30-amp hour capacity, for similar range and performance in a lighter bike. The Li+ has a 42-amp hour capacity, giving greater range. The VX-2 is designed as a smaller, lighter and less expensive version of the original bike for those who don't need freeway speeds or the weight of the original. Though it has similar range to the original, its top speed is less than half, but so is the price. \nIn 2012, Vectrix entered into a distribution agreement with Peirspeed to distribute Vectrix electric scooters in the U.S.\n\nIn October 2013, Vectrix lost its French distributor Italmotori after allegations of failing to ship scooters, batteries, and scooter parts required for repairs under warranty. In addition, in November 2013 a French automotive magazine after confirming that Italmotori was the third distributor (after Euromotor and Vectrix France) to cease representing Vectrix, reported that Vectrix appeared to be abandoning two models, its VX-1 and VX-3. A dealer in Portugal, Fuel Free Motors, asserts that they still will offer support for Vectrix scooters.\n\nIn January 2014 Vectrix ceased all US operations. After numerous manufacturing problems resulting in failed batteries and nonfunctioning scooters, Vectrix's parent company Gold Peak, a Chinese battery manufacturer, decided to close down Vectrix's US facilities. Vectrix intended to maintain its Poland fabrication plant in order to continue to supply parts for its joint venture with Daimler's Smart division.\n\nIn March 2014 Vectrix filed for bankruptcy again, this time under Chapter 7 for liquidation of the company. The court filings indicated assets of between $1 and $10 million and liabilities of between $10 and $50 million. The bankruptcy trustee announced an auction of the company's remaining assets, including unsold scooters, parts, and lithium batteries, to take place in June 2014.\n\nDuring the liquidation process the MPTECH group acquired the majority of the Vectrix US assets from the bankruptcy trustee and relocated them to Wroclaw, Poland. Six months later, the entire assets of the Polish fabrication plant were acquired by MPTECH group and part of the heavy production equipment sold to GOVECS in order to expand their production capabilities.\nIn June 2015, the MPTECH group restarted the production of the VX-1 and VX-2 models, equipping them with completely new battery and electronics, internally design and produced.\n\nSince the restart of operations, the new Vectrix company has re-established part of the former distribution network, restored supplier network, restored the spare parts availability, and begun market expansion plans.\n\n"}
{"id": "17506189", "url": "https://en.wikipedia.org/wiki?curid=17506189", "title": "Vestas V90-2MW", "text": "Vestas V90-2MW\n\nThe Vestas V90-2MW is a three bladed upwind horizontal axis wind turbine designed and manufactured by Vestas with versions for wind classes IIA and IIIA.\n\nThe V90-2MW has a tubular steel tower between and height. The nacelle is long, wide, and high once installed. The rotor has a diameter of , with blades long.\n\n\n"}
{"id": "599950", "url": "https://en.wikipedia.org/wiki?curid=599950", "title": "White coal", "text": "White coal\n\nWhite coal is a form of fuel produced by drying chopped wood over a fire. It differs from charcoal which is carbonised wood. White coal was used in England to melt lead ore from the mid-sixteenth to the late seventeenth centuries. It produces more heat than green wood but less than charcoal and thus prevents the lead evaporating. White coal could be used mixed with charcoal for other industrial uses than lead smelting. White coal was produced in distinctive circular pits with a channel, known as Q-pits. They are frequently found in the woods of South Yorkshire.\n\nNowadays white coal is made from \n\nBenefits of white coal:\n\n\nIndia is fast becoming a major manufacturer and consumer of white coal. A large number of companies have switched their boiler fuels to use white coal instead of fossil fuels. White Coal manufacturing capacity is coming up in droves in the state of Gujarat, Maharashtra, Tamil Nadu and Rajasthan.\n\nThe production of White coal (Briquettes made of Biomass) using agricultural and forest waste is more common in North India.\n\nSolid biofuels\n\n"}
{"id": "841057", "url": "https://en.wikipedia.org/wiki?curid=841057", "title": "Yellow grease", "text": "Yellow grease\n\nYellow grease, also termed used cooking oil (UCO), used vegetable oil (UVO), recycled vegetable oil, or waste vegetable oil (WVO) is recovered from businesses and industry that use the oil for cooking.\n\nIt is used to feed livestock, and to manufacture soap, make-up, clothes, rubber, and detergents. Due to competition from these other industrial sectors, the EIA estimates that less than a third of yellow grease could be spared for biodiesel production annually.\n\nIt is distinct from brown grease; yellow grease is typically used frying oils from deep fryers, whereas brown grease is sourced from grease interceptors.\n\nYellow grease can also refer to lower-quality grades of tallow (cow or sheep fat) from animal rendering plants. \n\nThe term has been in use for some time. A source from 1896 describes it as follows: \"Yellow grease is made by packers. All the refuse materials of the packing houses go into the yellow grease tank, together with any hogs which may die on the packers' hands.\"\n\n\n"}
