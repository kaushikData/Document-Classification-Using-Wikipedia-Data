{"id": "11074125", "url": "https://en.wikipedia.org/wiki?curid=11074125", "title": "Abradable coating", "text": "Abradable coating\n\nAn abradable coating is a coating made of an abradable material – meaning if it rubs against a more abrasive material in motion, the former will be worn whereas the latter will face no wear.\n\nAbradable coatings are used in aircraft jet engines in the compressor and turbine sections where a minimal clearance is needed between the blade tips and the casing.\n\nAbradable Powder Coatings provide an economical and environmentally friendly way to improve the efficiency of engines, compressors and pumps by fine-tuning the operational fit of internal components such as pistons, rotors and cases. In typical turbo machinery, the clearance between blade tips and the casing must account for thermal and inertial expansion as well as changes in concentricity due to shock loading events. To prevent catastrophic tip to casing contact, conservatively large clearances must be employed. \n\nThe role of abradable coatings is not only to allow for closer clearances, but to automatically adjust clearances, in-situ, to accept physical events and/or thermal scenarios that may be found in a devices operational history.\n\n"}
{"id": "3209246", "url": "https://en.wikipedia.org/wiki?curid=3209246", "title": "Abraham–Lorentz force", "text": "Abraham–Lorentz force\n\nIn the physics of electromagnetism, the Abraham–Lorentz force (also Lorentz–Abraham force) is the recoil force on an accelerating charged particle caused by the particle emitting electromagnetic radiation. It is also called the radiation reaction force or the self force.\n\nThe formula predates the theory of special relativity and is not valid at velocities of the order of the speed of light. Its relativistic generalization is called the \"Abraham–Lorentz–Dirac force\". Both of these are in the domain of classical physics, not quantum physics, and therefore may not be valid at distances of roughly the Compton wavelength or below. There is, however, an analogue of the formula that is both fully quantum and relativistic, called the \"Abraham–Lorentz–Dirac–Langevin equation\".\n\nThe force is proportional to the square of the object's charge, times the jerk (rate of change of acceleration) that it is experiencing. The force points in the direction of the jerk. For example, in a cyclotron, where the jerk points opposite to the velocity, the radiation reaction is directed opposite to the velocity of the particle, providing a braking action.\n\nIt was thought that the solution of the Abraham–Lorentz force problem predicts that signals from the future affect the present, thus challenging intuition of cause and effect ( retrocausality ). For example, there are pathological solutions using the Abraham–Lorentz–Dirac equation in which a particle accelerates \"in advance\" of the application of a force, so-called \"pre-acceleration\" solutions. One resolution of this problem was discussed by Yaghjian and is further discussed by Rohrlich and Medina.\n\nMathematically, the Abraham–Lorentz force is given in SI units by\n\nor in cgs units by\n\nHere F is the force, formula_3 is the jerk (the derivative of acceleration, or the third derivative of displacement), μ is the magnetic constant, ε is the electric constant, \"c\" is the speed of light in free space, and \"q\" is the electric charge of the particle.\n\nNote that this formula is for non-relativistic velocities; Dirac simply renormalized the mass of the particle in the equation of motion, to find the relativistic version (below).\n\nPhysically, an accelerating charge emits radiation (according to the Larmor formula), which carries momentum away from the charge. Since momentum is conserved, the charge is pushed in the direction opposite the direction of the emitted radiation. In fact the formula above for radiation force can be \"derived\" from the Larmor formula, as shown below.\n\nIn classical electrodynamics, problems are typically divided into two classes:\n\n\nIn some fields of physics, such as plasma physics and the calculation of transport coefficients (conductivity, diffusivity, \"etc.\"), the fields generated by the sources and the motion of the sources are solved self-consistently. In such cases, however, the motion of a selected source is calculated in response to fields generated by all other sources. Rarely is the motion of a particle (source) due to the fields generated by that same particle calculated. The reason for this is twofold:\n\n\nThese conceptual problems created by self-fields are highlighted in a standard graduate text. [Jackson]\n\nThe Abraham–Lorentz force is the result of the most fundamental calculation of the effect of self-generated fields. It arises from the observation that accelerating charges emit radiation. The Abraham–Lorentz force is the average force that an accelerating charged particle feels in the recoil from the emission of radiation. The introduction of quantum effects leads one to quantum electrodynamics. The self-fields in quantum electrodynamics generate a finite number of infinities in the calculations that can be removed by the process of renormalization. This has led to a theory that is able to make the most accurate predictions that humans have made to date. (See precision tests of QED.) The renormalization process fails, however, when applied to the gravitational force. The infinities in that case are infinite in number, which causes the failure of renormalization. Therefore, general relativity has an unsolved self-field problem. String theory and loop quantum gravity are current attempts to resolve this problem, formally called the problem of radiation reaction or the problem of self-force.\n\nThe simplest derivation for the self-force is found for periodic motion from the Larmor formula for the power radiated from a point charge:\n\nIf we assume the motion of a charged particle is periodic, then the average work done on the particle by the Abraham–Lorentz force is the negative of the Larmor power integrated over one period from formula_5 to formula_6:\n\nThe above expression can be integrated by parts. If we assume that there is periodic motion, the boundary term in the integral by parts disappears:\n\nClearly, we can identify\n\nA more rigorous derivation, which does not require periodic motion, was found using an Effective Field Theory formulation. An alternative derivation, finding the fully relativistic expression, was found by Dirac.\n\nBelow is an illustration of how a classical analysis can lead to surprising results. The classical theory can be seen to challenge standard pictures of causality, thus signaling either a breakdown or a need for extension of the theory. In this case the extension is to quantum mechanics and its relativistic counterpart quantum field theory. See the quote from Rohrlich in the introduction concerning \"the importance of obeying the validity limits of a physical theory\".\n\nFor a particle in an external force formula_10, we have\n\nwhere\n\nThis equation can be integrated once to obtain\n\nThe integral extends from the present to infinitely far in the future. Thus future values of the force affect the acceleration of the particle in the present. The future values are weighted by the factor\n\nwhich falls off rapidly for times greater than formula_15 in the future. Therefore, signals from an interval approximately formula_15 into the future affect the acceleration in the present. For an electron, this time is approximately formula_17 sec, which is the time it takes for a light wave to travel across the \"size\" of an electron, the classical electron radius. One way to define this \"size\" is as follows: it is (up to some constant factor) the distance formula_18 such that two electrons placed at rest at a distance formula_18 apart and allowed to fly apart, would have sufficient energy to reach half the speed of light. In other words, it forms the length (or time, or energy) scale where something as light as an electron would be fully relativistic. It is worth noting that this expression does not involve Planck's constant at all, so although it indicates something is wrong at this length scale, it does not directly relate to quantum uncertainty, or to the frequency-energy relation of a photon. Although it is common in quantum mechanics to treat formula_20 as a \"classical limit\", some speculate that even the classical theory needs renormalization, no matter how Planck's constant would be fixed.\n\nTo find the relativistic generalization, Dirac renormalized the mass in the equation of motion with the Abraham–Lorentz force in 1938. This renormalized equation of motion is called the Abraham–Lorentz–Dirac equation of motion.\n\nThe expression derived by Dirac is given in signature (−, +, +, +) by \n\nWith Liénard's relativistic generalization of Larmor's formula in the co-moving frame,\n\none can show this to be a valid force by manipulating the time average equation for power:\n\nSimilar to the non-relativistic case, there are pathological solutions using the Abraham–Lorentz–Dirac equation that anticipate a change in the external force and according to which the particle accelerates \"in advance\" of the application of a force, so-called \"preacceleration\" solutions. One resolution of this problem was discussed by Yaghjian, and is further discussed by Rohrlich and Medina.\n\n\n\n"}
{"id": "2781960", "url": "https://en.wikipedia.org/wiki?curid=2781960", "title": "Air pump", "text": "Air pump\n\nAn air pump is a device for pushing air. Examples include a bicycle pump, pumps that are used to aerate an aquarium or a pond via an airstone; a gas compressor used to power a pneumatic tool, air horn or pipe organ; a bellows used to encourage a fire; a vacuum cleaner and a vacuum pump. All air pumps contain a part that moves (vane, piston, impeller, diaphragm etc.) which drives the flow of air. When the air gets moved, an area of low pressure gets created which fills up with more air. \n\nPumps and compressors use very similar mechanisms, and basically perform the same action, but in different fluid regimes. At some point there is a crossover point in terminology, but here are some stereotypes:\n\n• Compressors operate on compressible fluids, typically gases. Pumps operate on fluids, typically liquids, approximated as in-compressible.\n\n• Compressors are intended to develop a very high pressure rise against a closed system; pumps are designed to develop relatively little pressure against a free-flowing system with minimal back-pressure.\n\n• Pumps are often used in continuous-flow operation, while many lower-end compressors must have intermittent duty cycles.\n\n• Compressors usually have a feedback sensor to shut off when they reach a desired pressure; pumps have a fixed design and operate freely across their performance curve as conditions change\n\nIn 1649, Otto von Guericke invented the spool vacuum air pump. This pump was called air pump in 19th century lexicons. Additionally, Guericke's air pump decreased any potential leaks between the piston and the cylinder by utilizing washers made from leather.\n\nThe first effective air pump constructed in England for scientific purposes was made in 1658 by Robert Hooke for Robert Boyle.\n\nIn 1705, an English scientist by the name of Francis Hauksbee, developed a style of a double-barrelled air pump. Hauksbee's double-barrelled air pump was used primarily for scientific research, and had the ability to create a vacuum. \n\nDiaphragm Pumps\n\nIn the case of air pumps, diaphragm pumps are considered to be a type of pump that utilizes positive displacement. A simple diaphragm pump contains a chamber that acts like a springy diaphragm. When compressed, the air within the diaphragm gets expelled. When the diaphragm is decompressed, the chamber refills with air. A simple example for a diaphragm pump is a foot pump that requires the user to constantly step up and down on the pump to inflate something.\n\nReciprocating Pumps\n\nA simple reciprocating pump is commonly made up of a cylinder with an inlet, an outlet, and a piston within. The inlet and the outlet are used to direct the flow of air, while the piston is used to generate the flow of air. When the piston is pulled up, air gets sucked into the pump through the inlet. The pump chamber depressurizes as it fills with air. When the piston is forced down, the air becomes compressed and closes the inlet. Then the air flows out from the outlet.\n"}
{"id": "11324984", "url": "https://en.wikipedia.org/wiki?curid=11324984", "title": "Almaraz Nuclear Power Plant", "text": "Almaraz Nuclear Power Plant\n\nAlmaraz Nuclear Power Plant is a nuclear power station at Almaraz in Spain and uses the international Tagus River, that runs into Portugal for cooling.\n\nIt consists of two PWRs of 1011 and 1006 MWe. \n\nThe refrigeration of the Almaraz nuclear power plant was the first reason for the construction of the Arrocampo Reservoir in 1976.\n\nThe water is taken from the Tagus River and covers a U-shaped circuit of 25 km which allows the cooling of the heat generated by the two nuclear reactors of the plant. \n(See the illustration of the water circulation in Arrocampo) \n\nThe walls of thermic separation (\"pantallas de separación térmica\" in Spanish) (PST) are 11 km long and 8 m high. \n\nThe tops of these walls are used by great cormorants and great egret as standing, resting and sleeping areas.\n\nIn 1975 Luis E. Echávarri was made project manager of the Plant. In 1985 he became Technical Director of the Spanish Nuclear Safety Council (CSN), and in 1987 he was named Commissioner of the CSN.\n\nThe first reactor began operating in 1981 and the second in 1983. It occupies an area of 1683 hectares.\n\nAs of 2017 Spain had approved a nuclear waste warehouse at Almaraz without carrying out any consultations or impact studies. Portugal has taken the matter to the EU, protests planned on January 12 at Spanish consulates were organised by Movimiento Ibérico Antinuclear, which coincided with a meeting between Portuguese and Spanish delegates in Madrid, which ended in deadlock and Portugal to complain to the EU that Spain ignored the potential cross-border impact with no studies being carried out, which is against European Union rules.\n\nSpanish secretary of State for the EU Jorge Toledo Albiñana has said work will start regardless of Portugals complaints, and uranium bars that will remain radioactive for the next 300 years will be stored on site.\n\nIn May 2017 the Portuguese Parliament approved the Ecologist Party \"The Greens\" motion to request the closure of Spain’s Almaraz nuclear plant during the next Iberian summit. Stating that after 2020 the plant should be shut down and the Greens asked the government to take a “resolute position,” for the facility, located only 100 kilometres form the Portuguese border. Environmentalists have warned that the plan to build a nuclear waste warehouse site next to the power plant almost certainly indicates that Spain plans to extend the life of the Almaraz power plant beyond the year 2020.\n\nOn the 28 January 2016, the Spanish Nuclear Safety Council inspectors found serious failings in the water pump engines at the plant, which have potential operational issues of the cooling system and could pose a serious risk to local people and the environment in Spain and in neighboring Portugal. Greenpeace has labelled the plant as an ‘extreme case’ in its study on the application of minimum safety standards introduced in Europe after the Fukushima accident.\n\nOn the 21 September 2016, defective parts were used on unit 1’s second and third steam generators and on unit 2’s third steam generator, as well as the rim of the reactor lid in unit 2. It had already been reported that the water pump engines had been stopped twice, the plant’s cooling system was reported as not 100% reliable.\n\nOn the 10 April 2017, the Spanish Nuclear Safety Council issued a statement stating there was an unscheduled stoppage of the main number two pump at 9.57am Spanish time (8.57am Lisbon time).\n\n\n"}
{"id": "3261380", "url": "https://en.wikipedia.org/wiki?curid=3261380", "title": "Atomic hydrogen welding", "text": "Atomic hydrogen welding\n\nAtomic hydrogen welding (AHW) is an arc welding process that uses an arc between two tungsten electrodes in a shielding atmosphere of hydrogen. The process was invented by Irving Langmuir in the course of his studies of atomic hydrogen. The electric arc efficiently breaks up the hydrogen molecules, which later recombine with tremendous release of heat, reaching temperatures from 3400 to 4000 °C. Without the arc, an oxyhydrogen torch can only reach 2800 °C. This is the third-hottest flame after dicyanoacetylene at 4987 °C and cyanogen at 4525 °C. An acetylene torch merely reaches 3300 °C. This device may be called an atomic hydrogen torch, nascent hydrogen torch or Langmuir torch. The process was also known as arc-atom welding.\n\nThe heat produced by this torch is sufficient to weld tungsten (3422 °C), the most refractory metal. The presence of hydrogen also acts as a shielding gas, preventing oxidation and contamination by carbon, nitrogen or oxygen, which can severely damage the properties of many metals. It eliminates the need of flux for this purpose.\n\nThe arc is maintained independently of the workpiece or parts being welded. The hydrogen gas is normally diatomic (H), but where the temperatures are over near the arc, the hydrogen breaks down into its atomic form, absorbing a large amount of heat from the arc. When the hydrogen strikes a relatively cold surface (i.e. the weld zone), it recombines into its diatomic form, releasing the energy associated with the formation of that bond. The energy in AHW can be varied easily by changing the distance between the arc stream and the workpiece surface.\n\nIn atomic hydrogen welding, filler metal may or may not be used. In this process, the arc is maintained entirely independent of the work or parts being welded. The work is a part of the electrical circuit only to the extent that a portion of the arc comes in contact with the work, at which time a voltage exists between the work and each electrode.\n\nThis process is being replaced by gas metal-arc welding, mainly because of the availability of inexpensive inert gases.\n\n\n"}
{"id": "19334164", "url": "https://en.wikipedia.org/wiki?curid=19334164", "title": "Biomass briquettes", "text": "Biomass briquettes\n\nBiomass briquettes are a biofuel substitute to coal and charcoal. Briquettes are mostly used in the developing world, where cooking fuels are not as easily available. There has been a move to the use of briquettes in the developed world, where they are used to heat industrial boilers in order to produce electricity from steam. The briquettes are cofired with coal in order to create the heat supplied to the boiler.\n\nBiomass briquettes, mostly made of green waste and other organic materials, are commonly used for electricity generation, heat, and cooking fuel. These compressed compounds contain various organic materials, including rice husk, bagasse, ground nut shells, municipal solid waste, agricultural waste. The composition of the briquettes varies by area due to the availability of raw materials. The raw materials are gathered and compressed into briquette in order to burn longer and make transportation of the goods easier. These briquettes are very different from charcoal because they do not have large concentrations of carbonaceous substances and added materials. Compared to fossil fuels, the briquettes produce low net total greenhouse gas emissions because the materials used are already a part of the carbon cycle.\n\nOne of the most common variables of the biomass briquette production process is the way the biomass is dried out. Manufacturers can use torrefaction, carbonization, or varying degrees of pyrolysis. Researchers concluded that torrefaction and carbonization are the most efficient forms of drying out biomass, but the use of the briquette determines which method should be used.\n\nCompaction is another factor affecting production. Some materials burn more efficiently if compacted at low pressures, such as corn stover grind. Other materials such as wheat and barley-straw require high amounts of pressure to produce heat. There are also different press technologies that can be used. A piston press is used to create solid briquettes for a wide array of purposes. Screw extrusion is used to compact biomass into loose, homogeneous briquettes that are substituted for coal in cofiring. This technology creates a toroidal, or doughnut-like, briquette. The hole in the center of the briquette allows for a larger surface area, creating a higher combustion rate.\n\nPeople have been using biomass briquettes in Nepal since before recorded history. Though inefficient, the burning of loose biomass created enough heat for cooking purposes and keeping warm. The first commercial production plant was created in 1982 and produced almost 900 metric tons of biomass. In 1984, factories were constructed that incorporated vast improvements on efficiency and the quality of briquettes. They used a combination of rice husks and molasses. The King Mahendra Trust for Nature Conservation (KMTNC) along with the Institute for Himalayan Conservation (IHC) created a mixture of coal and biomass in 2000 using a unique rolling machine.\n\nIn 1925, Japan independently started developing technology to harness the energy from sawdust briquettes, known as . Between 1964 and 1969, Japan increased production fourfold by incorporating screw press and piston press technology. The member enterprise of 830 or more existed in the 1960s. The new compaction techniques incorporated in these machines made briquettes of higher quality than those in Europe. As a result, European countries bought the licensing agreements and now manufacture Japanese designed machines.\n\nCofiring relates to the combustion of two different types of materials. The process is primarily used to decrease CO emissions despite the resulting lower energy efficiency and higher variable cost. The combination of materials usually contains a high carbon emitting substance such as coal and a lesser CO emitting material such as biomass. Even though CO will still be emitted through the combustion of biomass, the net carbon emitted is nearly negligible. This is due to the fact that the material gathered for the composition of the briquettes are still contained in the carbon cycle whereas fossil fuel combustion releases CO that has been sequestered for millennia. Boilers in power plants are traditionally heated by the combustion of coal, but if cofiring were to be implemented, then the CO emissions would decrease while still maintaining the heat inputted to the boiler. Implementing cofiring would require few modifications to the current characteristics to power plants, as only the fuel for the boiler would be altered. A moderate investment would be required for implementing biomass briquettes into the combustion process.\n\nCofiring is considered the most cost-efficient means of biomass. A higher combustion rate will occur when cofiring is implemented in a boiler when compared to burning only biomass. The compressed biomass is also much easier to transport since it is more dense, therefore allowing more biomass to be transported per shipment when compared to loose biomass. Some sources agree that a near-term solution for the greenhouse gas emission problem may lie in cofiring.\n\nThe use of biomass briquettes has been steadily increasing as industries realize the benefits of decreasing pollution through the use of biomass briquettes. Briquettes provide higher calorific value per dollar than coal when used for firing industrial boilers. Along with higher calorific value, biomass briquettes on average saved 30–40% of boiler fuel cost. But other sources suggest that cofiring is more expensive due to the widespread availability of coal and its low cost. However, in the long run, briquettes can only limit the use of coal to a small extent, but it is increasingly being pursued by industries and factories all over the world. Both raw materials can be produced or mined domestically in the United States, creating a fuel source that is free from foreign dependence and less polluting than raw fossil fuel incineration.\n\nEnvironmentally, the use of biomass briquettes produces much fewer greenhouse gases, specifically, 13.8% to 41.7% and NO. There was also a reduction from 11.1% to 38.5% in emissions when compared to coal from three different leading producers, EKCC Coal, Decanter Coal, and Alden Coal. Biomass briquettes are also fairly resistant to water degradation, an improvement over the difficulties encountered with the burning of wet coal. However, the briquettes are best used only as a supplement to coal. The use of cofiring creates an energy that is not as high as pure coal, but emits fewer pollutants and cuts down on the release of previously sequestered carbon. The continuous release of carbon and other greenhouse gasses into the atmosphere leads to an increase in global temperatures. The use of cofiring does not stop this process but decreases the relative emissions of coal power plants.\n\nThe Legacy Foundation has developed a set of techniques to produce biomass briquettes through artisanal production in rural villages that can be used for heating and cooking. These techniques were recently pioneered by Virunga National Park in eastern Democratic Republic of Congo, following the massive destruction of the mountain gorilla habitat for charcoal.\n\nPangani, Tanzania, is an area covered in coconut groves. After harvesting the meat of the coconut, the indigenous people would litter the ground with the husks, believing them to be useless. The husks later became a profit center after it was discovered that coconut husks are well suited to be the main ingredient in bio briquettes. This alternative fuel mixture burns incredibly efficiently and leaves little residue, making it a reliable source for cooking in the undeveloped country. The developing world has always relied on the burning biomass due to its low cost and availability anywhere there is organic material. The briquette production only improves upon the ancient practice by increasing the efficiency of pyrolysis.\n\nTwo major components of the developing world are China and India. The economies are rapidly increasing due to cheap ways of harnessing electricity and emitting large amounts of carbon dioxide. The Kyoto Protocol attempted to regulate the emissions of the three different worlds, but there were disagreements as to which country should be penalized for emissions based on its previous and future emissions. The United States has been the largest emitter but China has recently become the largest per capita. The United States had emitted a rigorous amount of carbon dioxide during its development and the developing nations argue that they should not be forced to meet the requirements. At the lower end, the undeveloped nations believe that they have little responsibility for what has been done to the carbon dioxide levels.\nThe major use of biomass briquettes in India, is in industrial applications usually to produce steam. A lot of conversions of boilers from FO to biomass briquettes have happened over the past decade. A vast majority of those projects are registered under CDM (Kyoto Protocol), which allows for users to get carbon credits.\n\nThe use of biomass briquettes is strongly encouraged by issuing carbon credits. One carbon credit is equal to one free ton of carbon dioxide to be emitted into the atmosphere. India has started to replace charcoal with biomass briquettes in regards to boiler fuel, especially in the southern parts of the country because the biomass briquettes can be created domestically, depending on the availability of land. Therefore, constantly rising fuel prices will be less influential in an economy if sources of fuel can be easily produced domestically. Lehra Fuel Tech Pvt Ltd is approved by Indian Renewable Energy Development Agency (IREDA), is one of the largest briquetting machine manufacturers from Ludhiana, India.\n\nIn the African Great Lakes region, work on biomass briquette production has been spearheaded by a number of NGOs with GVEP (Global Village Energy Partnership) taking a lead in promoting briquette products and briquette entrepreneurs in the three Great Lakes countries; namely, Kenya, Uganda and Tanzania. This has been achieved by a five-year EU and Dutch government sponsored project called DEEP EA (Developing Energy Enterprises Project East Africa) . The main feed stock for briquettes in the East African region has mainly been charcoal dust although alternative like sawdust, bagasse, coffee husks and rice husks have also been used.\n\nCoal is the largest carbon dioxide emitter per unit area when it comes to electricity generation. It is also the most common ingredient in charcoal. There has been a recent push to replace the burning of fossil fuels with biomass. The replacement of this nonrenewable resource with biological waste would lower the carbon footprint of grill owners and lower the overall pollution of the world. Citizens are also starting to manufacture briquettes at home. The first machines would create briquettes for homeowners out of compressed sawdust, however, current machines allow for briquette production out of any sort of dried biomass.\n\nArizona has also taken initiative to turn waste biomass into a source of energy. Waste cotton and pecan material used to provide a nesting ground for bugs that would destroy the new crops in the spring. To stop this problem farmers buried the biomass, which quickly led to soil degradation. These materials were discovered to be a very efficient source of energy and took care of issues that had plagued farms.\nThe United States Department of Energy has financed several projects to test the viability of biomass briquettes on a national scale. The scope of the projects is to increase the efficiency of gasifiers as well as produce plans for production facilities.\n\nBiomass is composed of organic materials, therefore, large amounts of land are required to produce the fuel. Critics argue that the use of this land should be utilized for food distribution rather than crop degradation. Also, climate changes may cause a harsh season, where the material extracted will need to be swapped for food rather than energy. The assumption is that the production of biomass decreases the food supply, causing an increase in world hunger by extracting the organic materials such as corn and soybeans for fuel rather than food.\n\nThe cost of implementing a new technology such as biomass into the current infrastructure is also high. The fixed costs with the production of biomass briquettes are high due to the new undeveloped technologies that revolve around the extraction, production and storage of the biomass. Technologies regarding extraction of oil and coal have been developing for decades, becoming more efficient with each year. A new undeveloped technology regarding fuel utilization that has no infrastructure built around makes it nearly impossible to compete in the current market.\n\n"}
{"id": "3974", "url": "https://en.wikipedia.org/wiki?curid=3974", "title": "Biopolymer", "text": "Biopolymer\n\nBiopolymers are polymers produced by living organisms; in other words, they are polymeric biomolecules. Biopolymers contain monomeric units that are covalently bonded to form larger structures. There are three main classes of biopolymers, classified according to the monomeric units used and the structure of the biopolymer formed: polynucleotides (RNA and DNA), which are long polymers composed of 13 or more nucleotide monomers; polypeptides, which are short polymers of amino acids; and polysaccharides, which are often linear bonded polymeric carbohydrate structures.\nOther examples of biopolymers include rubber, suberin, melanin and lignin.\n\nCellulose is the most common organic compound and biopolymer on Earth. About 33 percent of all plant matter is cellulose. The cellulose content of cotton is 90 percent, for wood it is 50 percent.\n\nA major defining difference between biopolymers and synthetic polymers can be found in their structures. All polymers are made of repetitive units called monomers. Biopolymers often have a well-defined structure, though this is not a defining characteristic (example: lignocellulose): \nThe exact chemical composition and the sequence in which these units are arranged is called the primary structure, in the case of proteins. Many biopolymers spontaneously fold into characteristic compact shapes (see also \"protein folding\" as well as secondary structure and tertiary structure), which determine their biological functions and depend in a complicated way on their primary structures. Structural biology is the study of the structural properties of the biopolymers.\nIn contrast, most synthetic polymers have much simpler and more random (or stochastic) structures. This fact leads to a molecular mass distribution that is missing in biopolymers.\nIn fact, as their synthesis is controlled by a template-directed process in most \"in vivo\" systems, all biopolymers of a type (say one specific protein) are all alike: they all contain the similar sequences and numbers of monomers and thus all have the same mass. This phenomenon is called monodispersity in contrast to the polydispersity encountered in synthetic polymers. As a result, biopolymers have a polydispersity index of 1.\n\nThe convention for a polypeptide is to list its constituent amino acid residues as they occur from the amino terminus to the carboxylic acid terminus. The amino acid residues are always joined by peptide bonds. Protein, though used colloquially to refer to any polypeptide, refers to larger or fully functional forms and can consist of several polypeptide chains as well as single chains. Proteins can also be modified to include non-peptide components, such as saccharide chains and lipids.\n\nThe convention for a nucleic acid sequence is to list the nucleotides as they occur from the 5' end to the 3' end of the polymer chain, where 5' and 3' refer to the numbering of carbons around the ribose ring which participate in forming the phosphate diester linkages of the chain. Such a sequence is called the primary structure of the biopolymer.\n\nSugar-based biopolymers are often difficult with regards to convention. Sugar polymers can be linear or branched and are typically joined with glycosidic bonds. The exact placement of the linkage can vary, and the orientation of the linking functional groups is also important, resulting in α- and β-glycosidic bonds with numbering definitive of the linking carbons' location in the ring. In addition, many saccharide units can undergo various chemical modifications, such as amination, and can even form parts of other molecules, such as glycoproteins.\n\nThere are a number of biophysical techniques for determining sequence information. Protein sequence can be determined by Edman degradation, in which the N-terminal residues are hydrolyzed from the chain one at a time, derivatized, and then identified. Mass spectrometer techniques can also be used. Nucleic acid sequence can be determined using gel electrophoresis and capillary electrophoresis. Lastly, mechanical properties of these biopolymers can often be measured using optical tweezers or atomic-force microscopy. Dual polarization interferometry can be used to measure the conformational changes or self-assembly of these materials when stimulated by pH, temperature, ionic strength or other binding partners.\n\nSome biopolymers- such as PLA, naturally occurring zein, and poly-3-hydroxybutyrate can be used as plastics, replacing the need for polystyrene or polyethylene based plastics.\n\nSome plastics are now referred to as being 'degradable', 'oxy-degradable' or 'UV-degradable'. This means that they break down when exposed to light or air, but these plastics are still primarily (as much as 98 per cent) oil-based and are not currently certified as 'biodegradable' under the European Union directive on Packaging and Packaging Waste (94/62/EC). Biopolymers will break down, and some are suitable for domestic composting.\n\nBiopolymers (also called renewable polymers) are produced from biomass for use in the packaging industry. Biomass comes from crops such as sugar beet, potatoes or wheat: when used to produce biopolymers, these are classified as non food crops. These can be converted in the following pathways:\n\nSugar beet > Glyconic acid > Polyglyconic acid\n\nStarch > (fermentation) > Lactic acid > Polylactic acid (PLA)\n\nBiomass > (fermentation) > Bioethanol > Ethene > Polyethylene\n\nMany types of packaging can be made from biopolymers: food trays, blown starch pellets for shipping fragile goods, thin films for wrapping.\n\nBiopolymers can be sustainable, carbon neutral and are always renewable, because they are made from plant materials which can be grown indefinitely. These plant materials come from agricultural non food crops. Therefore, the use of biopolymers would create a sustainable industry. In contrast, the feedstocks for polymers derived from petrochemicals will eventually deplete. In addition, biopolymers have the potential to cut carbon emissions and reduce CO quantities in the atmosphere: this is because the CO released when they degrade can be reabsorbed by crops grown to replace them: this makes them close to carbon neutral.\n\nBiopolymers are biodegradable, and some are also compostable. Some biopolymers are biodegradable: they are broken down into CO and water by microorganisms. Some of these biodegradable biopolymers are compostable: they can be put into an industrial composting process and will break down by 90% within six months. Biopolymers that do this can be marked with a 'compostable' symbol, under European Standard EN 13432 (2000). Packaging marked with this symbol can be put into industrial composting processes and will break down within six months or less. An example of a compostable polymer is PLA film under 20μm thick: films which are thicker than that do not qualify as compostable, even though they are \"biodegradable\". In Europe there is a home composting standard and associated logo that enables consumers to identify and dispose of packaging in their compost heap.\n\n\n"}
{"id": "50552970", "url": "https://en.wikipedia.org/wiki?curid=50552970", "title": "Bracell Limited", "text": "Bracell Limited\n\nBracell (formerly Bracell Limited) is one of the largest specialty cellulose producers in the world. It was previously listed on the Hong Kong Stock Exchange ().\n\nBracell produces both specialty-grade and rayon-grade dissolving wood pulp from its subsidiary in Brazil, Bahia Specialty Cellulose (BSC) using wood resources grown from its own eucalyptus plantations. Dissolving wood pulp is the natural raw materials and key ingredients to a diverse range of everyday items from textiles, baby wipes and eyeglass frames, to soft ice-cream, sausage casings and pharmaceuticals, as well as industrial products such as high-performance tire cords.\n\nIt owns over 150,000 hectares of freehold timberland in Brazil, of which 84,000 hectares are reserved for eucalyptus plantation. Its harvest cycle is between six and seven years, one of the shortest globally. Its installed capacity at its Brazil operations is 485,000 tons of rayon-grade pulp or 320,000 tons of specialty-grade pulp.\n\nIts rayon-grade pulp can be used in viscose and lyocell fibers used in textiles; non-wovens including baby wipes, cosmetic masks; and viscose filament used in textiles and tissues.\n\nIts specialty-grade pulp can be used in acetate flakes for cigarette filters, plastics and others; microcrystalline cellulose for food and pharmaceuticals; cord yarns for tires and conveyor belts; and other specialty applications such as sausage casings, cellophane, sponges, etc.\n"}
{"id": "17395974", "url": "https://en.wikipedia.org/wiki?curid=17395974", "title": "Bridge Mill Power Plant", "text": "Bridge Mill Power Plant\n\nThe Bridge Mill Power Plant is an historic hydroelectric plant at 25 Roosevelt Avenue in Pawtucket, Rhode Island. It is a red brick building, with sections two and three stories in height, located on the west bank of the Seekonk River. An ashlar granite retaining wall obscures a conduit which delivers water to the facility from the Pawtucket Falls Dam. The facility has three parts: a gate house, which controls the flow of water into the power house, where five turbines were located. A boiler house housed a steam generation facility which was used to generate power when the water levels were too low for hydroelectric power generation. Built in 1893, this is probably the best-preserved 19th-century hydroelectric power station in Rhode Island.\n\nThe plant was listed on the National Register of Historic Places in 1983.\n\n"}
{"id": "5865274", "url": "https://en.wikipedia.org/wiki?curid=5865274", "title": "C2H4F2", "text": "C2H4F2\n\nThe molecular formula CHF may refer to:\n\n"}
{"id": "39470562", "url": "https://en.wikipedia.org/wiki?curid=39470562", "title": "Carbon Lighthouse", "text": "Carbon Lighthouse\n\nCarbon Lighthouse is an American environmental organization that makes it profitable for commercial and industrial buildings to become carbon neutral. Primarily, Carbon Lighthouse performs turnkey energy efficiency and renewable energy projects for office buildings, industrial plants, and schools. A portion of proceeds from projects are donated to its non-profit partner Carbon Lighthouse Association to purchase carbon allowances from government markets. The organization is headquartered in San Francisco.\n\nCarbon Lighthouse was created with the goal of using market forces to stop climate change. Solutions it offers are designed to scale to a diverse set of locations and markets, have a large potential environmental impact, and be profitably self-sustaining without government regulation or subsidies.\n\nCarbon Lighthouse was founded in 2009 by Brenden Millstein and Raphael Rosen. The organization began with energy projects in California and Oregon and through the support of various social innovation competitions including the Echoing Green Fellowship, the Stanford StartX Fellowship, the Stanford Social Innovation Fellowship, and others. On March 13, 2018 Chief Executive Officer Brenden Millstein announced they had raised $27 Million from various sources to expand its engineering and marketing efforts. \n\nCarbon Lighthouse’s business approach was designed to minimize the cost and time required to reduce energy consumption in commercial and industrial buildings. The organization does not focus on energy studies or capital-intensive retrofits, but earns income by delivering energy savings, primarily through the optimization of existing equipment already installed in buildings. Carbon Lighthouse engineers collect and analyze large amounts of data. The firm takes an entire building approach.\n\nCarbon Lighthouse offers an efficiency PPA to reduce upfront project expenses.\n\nCarbon Lighthouse’s data driven approach has led it to be known as “The Ghostbusters of Energy Efficiency.\"\n\n"}
{"id": "50415662", "url": "https://en.wikipedia.org/wiki?curid=50415662", "title": "Chrysler ETV-1", "text": "Chrysler ETV-1\n\nThe Chrysler ETV-1 was a passenger car produced by Chrysler as a test bed for motor and drive controls. With a motor produced by General Electric, it was claimed to be the \"first ground up modern day electric vehicle design.\"\n\nThe ETV-1 uses one electric motor, front-mounted driving the front axle. Chryslers ETV-1 has claimed acceleration of in 9.0 seconds and a claimed top speed of .\n\nETV-1 utilises a removable 'T' shaped battery pack, The Battery pack has a total capacity of electric vehicle battery.\n"}
{"id": "1616977", "url": "https://en.wikipedia.org/wiki?curid=1616977", "title": "Coefficient of haze", "text": "Coefficient of haze\n\nThe coefficient of haze (also known as smoke shade) is a measurement of visibility interference in the atmosphere.\n\nOne way to measure this is to draw about 1000 cubic feet of air sample through an air filter and obtain the radiation intensity through the filter. The coefficient is then calculated based on the absorbance formula\nwhere formula_2 is the radiation (400 nm light) intensity transmitted through the sampled filter, and formula_3 is the radiation intensity transmitted through a clean (control) filter.\n\n"}
{"id": "22760945", "url": "https://en.wikipedia.org/wiki?curid=22760945", "title": "Current injection technique", "text": "Current injection technique\n\nThe current injection technique is a technique developed to reduce the turn-OFF switching transient of power bipolar semiconductor devices. It was developed and published by Dr S. Eio of Staffordshire University (United Kingdom) in 2007.\n\nThe Turn-OFF switching transient of silicon-based power bipolar semiconductor devices, caused by stored charge in the device during the forward conduction state, limits switching speed of the device, which in turn limits the efficiency of the application it is used within.\n\nDifferent techniques, such as carrier lifetime control, injection efficiency and buffer layer devices, have been used to minimize turn-OFF switching transient, but all result in a trade-off between the ON-state loss and switching speed.\n\nThe current injection technique examined in Dr Eio's publications optimize the switching transient of power diodes, thyristors and insulated gate bipolar transistors (IGBTs) without the need of changing the structure of these devices. To implement the current injection technique, current injection circuit was developed with results indicating that the injection of an additional current during its switching transient can reduce the reverse recovery charge of a given power diode and thyristor, and also reduce the tail current of insulated gate bipolar transistors.\n\nPractical experimental results on diodes and thyristors showed that the amplitude of the injected current required is proportional to the peak reverse recovery current and proved that these devices experience a momentary increase in recombination of current carriers during the injection of the additional current. This help to prevent the device from conducting large negative current, which in turn reduce its reverse recovery charge and reverse recovery time. Results obtained from experiments with insulated gate bipolar transistors showed a significant reduction in the time where current falls to zero when opposing current was injected into the device during its turn-off transient. Further simulation results from numerical modeling showed that the injected opposing current temporary increase recombination in the device and therefore reduce the extracted excess carriers that stored within the device.\n\nTo prevent circuit commutation and bonding between the current injection circuit and the main test circuit where the device under test (DUT) is connected to, non-invasive circuit was developed to magnetically couple the two circuits.\n\nIn summary, current injection technique makes it possible to use devices with low forward voltage drop for high frequency applications. This also imply cheaper cost of devices as less processing steps are required during the manufacturing stages where the need of carrier lifetime control techniques are reduced. This removed the need for the semiconductor device used in the current injection circuit to have high breakdown voltage rating and also provided electrical isolation. Typical application of this technique in an inductive load chopper circuit showed a significant reduction in the tail current of insulated gate bipolar transistors, and the reverse recovery time and charge of the freewheeling diode used.\n\n"}
{"id": "32154505", "url": "https://en.wikipedia.org/wiki?curid=32154505", "title": "Drylands", "text": "Drylands\n\nDrylands are defined by a scarcity of water. Drylands are zones where precipitation is balanced by evaporation from surfaces and transpiration by plants (evapotranspiration). The United Nations Environment Program defines drylands as tropical and temperate areas with an aridity index of less than 0.65. The drylands can be classified into four sub-types: dry sub-humid lands, semi-arid lands, arid lands, and hyper-arid lands. Some authorities consider Hyper-arid lands as deserts (United Nations Convention to Combat Desertification) although a number of the world's deserts include both hyper arid and arid climate zones. The UNCCD excludes hyper-arid zones from its definition of drylands.\n\nDrylands cover 41.3% of the earth’s land surface, including 15% of Latin America, 66% of Africa, 40% of Asia and 24% of Europe. There is a significantly greater proportion of drylands in developing countries (72%), and the proportion increases with aridity: almost 100% of all Hyper Arid lands are in the developing world. Nevertheless, the United States, Australia and several countries in Southern Europe also contain significant dryland areas.\n\nDrylands are complex, evolving structures whose characteristics and dynamic properties depend on many interrelated links between climate, soil, and vegetation.\n\nThe livelihoods of millions of people in developing countries depend highly on dryland biodiversity to ensure their food security and their well-being. Drylands, unlike more humid biomes, rely mostly on above ground water runoff for redistribution of water, and almost all their water redistribution occurs on the surface Dryland inhabitants' lifestyle provides global environmental benefits which contribute to halt climate change, such as carbon sequestration and species conservation. Dryland biodiversity is equally of central importance as to ensuring sustainable development, along with providing significant global economic values through the provision of ecosystem services and biodiversity products. The UN Conference on Sustainable Development Rio+20, held in Brazil in June 2012, stressed the intrinsic value of biological diversity and recognized the severity of global biodiversity loss and degradation of ecosystems.\n\n"}
{"id": "4206527", "url": "https://en.wikipedia.org/wiki?curid=4206527", "title": "Eivind Reiten", "text": "Eivind Reiten\n\nEivind Kristofer Reiten (born 2 April 1953) is a Norwegian economist, corporate officer and politician for the Centre Party. He was a Minister of Fisheries and Minister of Petroleum and Energy during the 1980s, before entering a career in business. Reiten served as the Director General (CEO) of Norsk Hydro between 2001 and 2009, after which he took up the chairmanship of Norske Skog. Eivind Reiten was also Chairman of StatoilHydro for four days, until he resigned from his position after Norsk Hydro had been accused of corruption.\n\nReiten was born in Midsund as the son of Kristofer Reiten, a farmer and fisher, and housewife Kjellaug Opstad. He enrolled as a student in 1972, and graduated from the University of Oslo in 1978 with a degree in economics. He worked as a civil servant from 1979 to 1982, and as a secretary for the Centre Party from 1982 to 1983. He was then brought into the government as state secretary to the Minister of Finance from 1983 to 1985. He then became Minister of Fisheries from 1985 to 1986 as part of the Second cabinet Willoch, and Minister of Petroleum and Energy from 1989 to 1990 as part of Cabinet Syse. In his last political position, he was responsible for the deregulation of the electricity market in Norway. Having chaired the Centre Youth, the youth wing of the Centre Party, from 1979 to 1981, he served as a deputy representative to the Norwegian Parliament from 1985 to 1989.\n\nReiten started working for Norsk Hydro in 1988, heading the energy division from that year, and becoming senior vice president of special projects in 1991. From 1992 he led the refining and marketing division, and from 1998 the aluminum division. He was appointed executive vice president for light metals in 1999. He succeeded Egil Myklebust as chief executive officer in 2001. He was a member of the board of the Bank of Norway from 1991 to 1994 and Norske Skogindustrier from 1997 to 2000, and has chaired the board of Norway Post (1996–1999) and Telenor (2000–2001). He has attended the Bilderberg meetings. He is a fellow of the Norwegian Academy of Technological Sciences.\n\nAs part of the merger between Statoil and the oil and gas division of Norsk Hydro, Reiten was appointed chair of the merged StatoilHydro that merged on 1 October 2007. Four days later Reiten withdrew from the seat. The reason was that it was uncertain whether or not he knew about a corruption case Hydro had been accused of, where a Libyan consulting company and the consultant Abdurrazag Gammudi had been paid , used to make bribes, after the Hydro take-over of Saga Petroleum in 1999. Stated Reiten, it was in no-one's interest that he retain a conflict of interest by retaining the seat of chair in the company that would investigate himself. The case had arisen on 26 September, after a Hydro employee had leaked information about the matter; it had not been identified as part of the due diligence performed by Statoil prior to the merger. He was replaced by his deputy, Marit Arnstad, who is also a former Minister of Petroleum and Energy from the Centre Party. The investigation from StatoilHydro concluded that Reiten was informed about this during 2000 and 2001, while the investigation in Norsk Hydro concluded that Reiten did not know about the corruption. Since the Norsk Hydro investigation—that included checking 1.5 million documents—could not show that Reiten knew about the corruption, Hydro Chairman Terje Vareberg confirmed that Reiten would not be removed from his position. However, two executives of StatoilHydro were required to leave immediately.\n\nIn January 2009 Reiten announced that he was stepping down as Chief Executive of Norsk Hydro from 30 March 2009, with executive vice president Svein Richard Brandtzæg taking over. Reiten has since been nominated to replace Kim Wahl as Chairman of Norske Skog at the company's annual general meeting in April 2009.\n\nReiten is married and has two children. He resides in Oslo.\n"}
{"id": "13579251", "url": "https://en.wikipedia.org/wiki?curid=13579251", "title": "EnergyCS", "text": "EnergyCS\n\nEnergyCS is a Monrovia, California-based company specializing in integration and controls for high-energy, large format batteries. The company provides battery management systems for lithium-ion batteries and other advance energy storage technologies and is active in the electric vehicle and stationary energy storage space.\n\nEnergyCS is also a pioneer in the area of PHEV (Plug-in hybrid Electric Vehicles). The company produced the first lithium-ion powered plug-in Prius.\n\n"}
{"id": "46975952", "url": "https://en.wikipedia.org/wiki?curid=46975952", "title": "Energy Policy Institute of Australia", "text": "Energy Policy Institute of Australia\n\nThe Energy Policy Institute of Australia is an apolitical, not-for-profit energy policy body. It was created as the Energy Alliance of Australia in 1999 to collaborate with the Australian Government on the development of energy export markets. The Institute advocates technology neutrality as a core principle of energy policy. The Institute adopted the name Energy Policy Institute of Australia in 2011. \n\nFor ten years from 2008, the Institute convened an annual forum called the \"Energy State of the Nation\" but this has been superseded by more frequent events as policy issues have arisen. The Institute is governed by a Board of Directors who represent a diverse mix of corporate entities with interests in Australia's energy sector.\n\nThe Institute was established in 1999 as the Energy Alliance of Australia, in consultation with the Australian Government's Department of Resources, Energy and Tourism. It was designed as a vehicle for Australia's energy sector to liaise with the Asia Pacific Energy Cooperation (APEC) organisation. The Energy Alliance of Australia initiated the establishment of APGAS in 2005 as a forum for APEC Member States' energy policymakers, industry regulators, oil and gas suppliers and consumers, traders and pipeline and ship owners and operators. After three years APGAS closed, having fulfilled its purpose of accelerating cross-border gas trade. \n\nThe Institute aims to foster an attractive and secure energy investment climate in Australia, and promote an industry which is internationally competitive. It supports free markets and a \"resilient\" energy supply system, supported by integrated government policy. The Institute accepts \"that the world must transition to a low-carbon society as quickly as it can afford to do so.\"\n\nThe Institute conducts research into policy, technology, economics, trade and investment relevant to the energy sector. The research is disseminated through the publication of policy papers, presentation of briefings and facilitation of workshops. It provides \"a trusted communications channel between government and the private sector\" and represents the Australian industry to the APEC Energy Working Group and similar international bodies. The Institute is affiliated with the US Energy Association of Washington DC.\n\nIn 2016, the Institute commenced to publish a continually updated compendium of its policy papers. The compendium lists the key points of all past papers and is hyperlinked to each of the original papers.\n\nIn August 2016, the Institute published An Australian Energy Vision and Framework for Energy Policy Priorities.\n\nIn March 2017, the EPIA forwarded its final submission to the Finkel Review on Security of the NEM, and then in June 2017, the Institute published its position after the release of the Finkel Report.\n\nThe EPIA runs a series of Executive Briefings for its members on topical issues with an important bearing on the development of energy policy in Australia. All events are listed on the Institute's website.\n\nThe EPIA's members meet once a year, the board meets three times a year, and an Executive Committee (elected by the Board) meets monthly. Each corporate member has the right to appoint a director to the board. \n\nThe Executive Director is Robert Pritchard. Robert has over 40 years’ experience as a lawyer and adviser to industry, governments and organisations on energy projects and policies, both in Australia and overseas, and as a director of companies in the energy sector. This includes serving as Managing Director of ResourcesLaw International, Chairman of the St Baker Energy Innovation Fund and SMR Nuclear Technology. Robert was the first Chairman of the Energy Law Section of the International Bar Association and served for nine years on the Finance Committee of the World Energy Council. \n\nBoard members as of November 2017 are listed below:\n"}
{"id": "12792329", "url": "https://en.wikipedia.org/wiki?curid=12792329", "title": "Environmentalists for Nuclear", "text": "Environmentalists for Nuclear\n\nEnvironmentalists for Nuclear Energy (EFN) is a pro-nuclear power non-profit organization which aims at providing complete and straightforward information to the public on energy and the environment. It also promotes the benefits of nuclear energy for a cleaner world, and aims at uniting people in favor of clean nuclear energy.\n\nThe website of the organization states that environmental opposition to nuclear energy is the \"greatest misunderstanding and mistake of the century\".\n\nEFN is funded by the memberships and donations of its members.\n\nEFN was started by Bruno Comby in 1996 after the publication of his book \"Environmentalists For Nuclear Energy\".\n\nEFN gathers over 10,000 members and supporters, with local correspondents and a network of affiliated organizations and in more than 60 countries, to inform the public on energy and the environment.\n\nPatrick Moore co-founder of Greenpeace in 1971 and James Lovelock author of the Gaia theory, considered as the historical founder of environmental thinking are supporters of the group.\n\nThe general assembly of EFN is held once a year in Houilles.\n\nThe headquarters of EFN are located in a positive energy ecohouse in the suburbs of Paris, powered with solar energy (thermal and PV), wind energy, geothermal air-conditioning, a high efficiency heat-pump, double-flux ventilation, and just a small amount of low-carbon-emitting French nuclear energy. Conceptually designed by members of the organization, this house has an almost-nil carbon-footprint (500 times less than a standard gas-heated construction of the same size).\n\n"}
{"id": "51666513", "url": "https://en.wikipedia.org/wiki?curid=51666513", "title": "European Thermoelectric Society", "text": "European Thermoelectric Society\n\nThe European Thermoelectric Society (ETS) is a non-profit organization founded in 1995. The goals of the ETS are to promote European research and development activities and professional networking in the field of thermoelectricity.\n\nAt present, the society has hundred-some members on average and its domicile is the \"Institute for Materials Sciences (Chair of Materials Chemistry)\" at the University of Stuttgart/Germany.\n\nThe idea to establish a ″thermoelectric network″ within the European Union started to take shape at the 1st European Workshop on Thermoelectricity in Dresden/Germany in October 1994. The actual foundation assembly of the European Thermoelectric Society with participants from eight European countries was held in Cologne/Germany in February 1995. The first elected president was Hubert Scherrer (France).\n\nThe ETS aims to represent the interests of everyone in Europe involved in thermoelectricity. It offers a platform to the European thermoelectric community to improve exchange of information and pool resources. On the other hand, it acts as a single point of contact for economy, politics and the interested public. It is committed to watch trends, give directions for future activities, develop visions, and render guidance to those interested in the field. An important function is to raise awareness of thermoelectricity and its applications in Europe.\n\nIn order to live up to these purposes, the ETS is organizer of conferences in topics of interest to the community and establishes a communication network between members. In addition, the ETS is in close contact to other thermoelectric groups and societies in Europe (DTG/Germany, STS/Switzerland, GDR TE/France, AIT/Italy) and worldwide (ITS).\n\nThe organs of the society are the general assembly, the executive committee, the extended committee and the auditors.\n\nThe general assembly consists of all individual members of the society. A member can be any individual person from industry or academic institutions with activities or interests in thermoelectricity. Registration for the European Conference on Thermoelectrics (ECT) involves an obligatory one year membership. The executive committee consisting of president, secretary, and treasurer manages and represents the society, while the extended committee acts in an advisory capacity. The members of both committees are elected by the general assembly for a three-year term.\n\nThe European countries take turns at organizing the annual European Conference on Thermoelectrics (ECT) showcasing trends, developments, products and services of academia and industry in the field.\n\nEvery three years the ECT coincides with the International Conference on Thermoelectrics (ICT).\n\nTogether with the German, Italian and Swiss Thermoelectric Societies, the ETS sponsors three poster prizes every year. The prizes are endowed with a prize money of 500 Euro each and awarded at the annual ECT.\n\nAccording to the ETS bylaws, the executive committee may appoint a person as honorary member in recognition of his achievements. On this basis, the long-time active members Hubert Scherrer (France) and Harald Böttner (Germany) were awarded the honorary membership at the ECT2013 in Noordwijk/Netherlands.\n"}
{"id": "20078811", "url": "https://en.wikipedia.org/wiki?curid=20078811", "title": "Expansion tunnel", "text": "Expansion tunnel\n\nExpansion and shock tunnels are aerodynamic testing facilities with a specific interest in high speeds and high temperature testing. Shock tunnels use steady flow nozzle expansion whereas expansion tunnels use unsteady expansion with higher enthalpy, or thermal energy. In both cases the gases are compressed and heated until the gases are released, expanding rapidly down the expansion chamber. The tunnels reach speeds from Mach 3 to Mach 30 to create testing conditions that simulate hypersonic to re-entry flight. These tunnels are used by military and government agencies to test hypersonic vehicles that undergo a variety of natural phenomenon that occur during hypersonic flight.\n\nExpansion tunnels use a dual-diaphragm system where the diaphragms act as rupture discs, or a pressure relief. The tunnel is separated into three sections: drive, driven, and acceleration. The drive section is filled with high pressure helium gas. The driven section is filled with a lower pressure desired test gas, such as carbon dioxide, helium, nitrogen, or oxygen. The acceleration section is filled with an even lower pressurized test gas. Each section is divided by a diaphragm, which is meant to be ruptured in sequence causing the first diaphragm to rupture, mixing and expanding the drive and the driven. When the shock wave hits the second diaphragm, it ruptures casing the two gases to mix with the acceleration and expand down the enclosed test section. Operation time is approximately 250 microseconds.\n\nReflected shock tunnels heat and pressurize a stagnant gas by using shockwaves that are redirected back into the center; this excites the gases and produces movement, heat, and pressure. The gases are then released and expanded through the nozzle and into the test chamber. Operation time is approximately 20 milliseconds.\n\nDuring the expansion process, a variety of test are run to analyze the aerodynamic and thermal properties of the test vehicle. \n\n\nThe HET is one of the shock tunnels in the Caltech Hypersonics group at the California Institute of Technology directed by Professor Joanna Austin. It operates similarly to a shock tube where a shock formed by the primary diaphragm heats up the test gas. The novel part of this facility is when its test gas is further accelerated by an expansion shock that forms when the primary shock interacts with a second downstream diaphragm. It is a 150mm inner diameter facility with the capability to reach Mach 4-8, and was built in 2005.\n\nNASA's Hypersonic Pulse Facility (HYPULSE) is operated by the General Applied Science Laboratory (GASL) in New York. The HYPULSE facility was developed for the testing of re-entry vehicles and air-breathing engines. The specifications of the HYPULSE include a diameter of 7 feet and a 19 foot length. This facility was upgraded to have two modes, Reflected Shock Tunnel (RST) and Shock-Expansion Tunnel (SET). HYPULSE-RST generates speeds from Mach 5 to 10, whereas the HYPULSE-SET produces speeds from Mach 12 to 25.\n\nLarge Energy National Shock tunnels (LENS) were constructed over the past 15 years at the Aerothermal/Aero-optic Evaluation Center (AAEC) at CUBRC. The LENS facilities were developed for the testing of advanced missile seekerheads and scramjet engines. LENS I and LENS II have similar control, compression and data acquisition systems. LENS I facility has an 11-inch diameter by 25.5 foot long drive tube that is electrically heated with an 8-inch by 60 foot driven section capable of reaching Mach 7 to 18. Test models can have a maximum length of 12 feet and a diameter of 3 feet. The LENS I heats up the drive gas to 750 degrees F to operate at a maximum 30,000 psi. The LENS II facility integrates a 24-inch diameter to both the 60 foot drive and also the 100 foot driven tubes, which runs between Mach 3 and 9.\n\nLENS-X is an 8 foot diameter by 100 foot expansion tunnel with a top speed of Mach 30. The drive chamber, filled with helium or hydrogen gas, is compressed to 3,000 psi at 1000 degrees Fahrenheit; this breaks the first diaphragm, causing the driven chamber to experience an influx of hot gas, generating pressures over 20,000 psi before the second diaphragm is ruptured.\n\nIt is located at Kakuda Space research centre – JAXA (Japan Aerospace Exploration Agency). Both high pressure and high temperature can be simulated simultaneously in this tunnel. Major applications include Aerodynamic and aerothermodynamic tests on scale models of returnable spacecraft; and Combustion process tests on scramjet engines. HYFLEX (Hypersonic Flight Experiment) which was a re-entry demonstrator prototype vehicle of JAXA was tested in this facility. Another speciality of this tunnel is 3 pistons of different masses can be used. \n\nIt is located at University of Queensland, Australia. It is a large free piston driven shock tunnel capable of producing sub-orbital flow speeds at a range of Mach numbers.The T4 shock tunnel began operation in April 1987 and commenced routine operation, after a commissioning period, in September 1987. The 10000th shot of T4 was fired in August 2008 and it remains significantly better than X2. \n\nIt is a free piston shock tunnel located at California Institute of Technology, USA. It is the largest free-piston shock tunnel in the world at a university. It is an impulse facility capable of reaching very high stagnation enthalpies (25 MJ/kg) and pressures (40 MPa). The test time is on the order of 1 ms. It uses helium and argon as the driver gas and a .25\" steel plate as its primary diaphragm. Test gases include air, nitrogen, carbon dioxide, or mixtures thereof. The 120kg piston can reach maximum speeds in excess of 300 m/s. \n"}
{"id": "14093506", "url": "https://en.wikipedia.org/wiki?curid=14093506", "title": "Fauske Lysverk", "text": "Fauske Lysverk\n\nFauske Lysverk is a private power company that operates the power grid in Fauske, Norway with 6,000 customers and 440 kilometers of grid. The company does not have any of its own power production. The largest owner is the municipality (44.64%), Bodø Energi (12.4%) and Salten Kraftsamband. Fauske Lysverk was founded in 1913. As of May 2004 the number of shareholders in the company were 552.\n"}
{"id": "49557682", "url": "https://en.wikipedia.org/wiki?curid=49557682", "title": "Fire rock", "text": "Fire rock\n\nFire rock is manufactured lava rock that is sold in various shapes and sizes, and is used as a medium for retaining direct heat. Fire rocks are used in natural gas fireplaces or in natural gas or propane burning fire pits. It may be used as the main fuel distributor or as padding for fire glass to go on top. Fire rocks are proven to increase combustion efficiency and maintain a desirable aesthetic quality. They also are known for dispersing the flame of the fireplace or fire pit well, by allowing for gaps that act as channels for air and gas to funnel the flame through. Fire rocks come in many different colors and range in sizes from a half-inch to an inch.\n\nFire rock is noted for its ability to withstand high temperatures of direct heat for prolonged periods of time. Unlike river rock, which is non-porous and highly explosive when heated, fire rock is considered a porous substance that is capable of releasing heat through tiny holes in its outer layer. Damp fire rocks have been known to \"pop\" and throw pea sized pieces of rock as they heat. The moisture that is caught in the fire rock creates air pockets that can easily split the rock when heated. Fire rocks must be stored away from high moisture areas and kept dry to keep this problem from occurring.\n\nBecause fire rock does not emit any carbon dioxide when heated, it is considered an eco-friendly burning solution. Fire rock can be used multiple times and when cared for can be burned for years on end. It produces no ash, which cuts back on the number of toxins in the home.\n\nThe aztec civilisation named their children after these rocks to strike fear into their enemys. Many aztec soldiers were named Milintica as this was the name for fire.\n"}
{"id": "24906613", "url": "https://en.wikipedia.org/wiki?curid=24906613", "title": "Green Comm Challenge", "text": "Green Comm Challenge\n\nGreen Communication Challenge is an organization founded and led by Francesco De Leo that actively promotes the development of energy conservation technology and practices in the field of Information and Communications Technology (ICT). \nGreen Comm Challenge achieved worldwide notoriety in 2007, when it enlisted as one of the challengers in the 33rd edition of the America's Cup, an effort meant to show how researchers, technologists and entrepreneurs from around the world can be brought together by an exciting vision: building the ultimate renewable energy machine, a competitive America’s Cup boat.\n\nICT is helping society become more energy efficient: think of the positive impact on emissions of telecommuting and ecommerce for example. Computers are helping us design more energy efficient products. But there is little doubt that, while other industries strive to become more energy efficient, computers and networks themselves risk becoming the “energy hogs” of the future, unless something is done.\n\nPowering the over 1 billion personal computers, the millions of corporate data centers, the over 4 billion fixed and mobile telephones and telecommunications networks around the world requires approximately 1.4 Petawatt-hr a year () of electricity, approximately 8% of the global electrical energy produced in 2005. And consider that over 4 billion people around the world have never used a cell phone, almost three times as many as those who currently have access to one.\n\nSome estimates project that the above percentage will grow to 15% by 2020, but these projections may fail to take into account some of the disruptive trends we are witnessing today. Take Google for example: to power the over 75 billion searches performed in July 2009 the company needed an estimated one million servers, consuming an estimated 1.3 Terawatt-hr a year (). The number of searches has grown over 60% between 2008 and 2009 alone. It is no surprise that the company is planning to manage as many as 10 million servers in the future.\n\nThe explosion of video on the net is another disruptive element. The Amesterdam Internet exchange (AMS-IX), which handles approximately 20% of Europe’s traffic, saw its aggregate data traffic increase from 1.75 Petabyte per day in November 2007 to an expected 4 Petabyte per day in November 2009. Much of this rapid increase in traffic is driven by widespread use of voice and, in particular, video over the Internet.\n\nGreen Comm Challenge’s founders believe that defining a corollary to Moore’s Law is in order: increases in processor performance must be accompanied by a less-than proportional increase in energy consumption.\n\nThis, of course, is no easy undertaking. It will require a new engineering approach to designing computers, cell phones and networks. It will also require a new management culture, capable of recognizing the attractive ROIs that green technology can generate, in addition to being more sensitive to the environmental impact of management's decisions.\n\nThis is why Green Comm has fully embraced the ICT energy-efficiency challenge by establishing an interdisciplinary approach that involves some of the most innovative thinkers around the world. We are currently involved in the following four initiatives:\n\n\n"}
{"id": "24978589", "url": "https://en.wikipedia.org/wiki?curid=24978589", "title": "Hyphessobrycon sweglesi", "text": "Hyphessobrycon sweglesi\n\nHyphessobrycon sweglesi is a species of tetra that lives in the Orinoco River drainage basin in South America. The fish has a round black spot behind the gill-plate, a black band on the dorsal fin that is bordered above and below by creamy-white. The other fins are red same as the upper rim of the eye. The fish eats worms, small insects, and crustaceans. The species can lay up to 400 eggs that can hatch in a day and that are susceptible to fungus. The species' appearance is very similar to \"Hyphessobrycon megalopterus\" (black phantom tetra). The species' scientific name used to be \"Megalamphodus sweglisi\" and the species' common name is red phantom tetra.\n\nIt is recommended to keep a mixed group (males and female) of at least 8–10 specimens in a tank no smaller than 20 US gallons (76 L).\n\n"}
{"id": "35974812", "url": "https://en.wikipedia.org/wiki?curid=35974812", "title": "Ikal Angelei", "text": "Ikal Angelei\n\nIkal Angelei is a Kenyan politician and environmentalist. She was born in Kitale. She was awarded the Goldman Environmental Prize in 2012, in particular for her voicing of environmental implications of the Gilgel Gibe III Dam, speaking on behalf of Kenyan indigenous communities.\n"}
{"id": "12977073", "url": "https://en.wikipedia.org/wiki?curid=12977073", "title": "Isolated-phase bus", "text": "Isolated-phase bus\n\nIn electrical engineering, isolated-phase bus (IPB), also known as Phase-isolated Bus (PIB) in some countries, is a method of construction for circuits carrying very large currents, typically between a generator and its step-up transformer in a steam or large hydroelectric power plant.\n\nEach phase current is carried on a separate conductor, enclosed in a separate grounded metal housing. Conductors are usually hollow aluminum tubes or aluminum bars, supported within the housing on porcelain or polymer insulators. The metal housings are electrically connected so that induced current, nearly of the magnitude of the phase current, can flow through the housing, in the opposite direction from the phase current. The magnetic field produced by this current nearly exactly cancels the magnetic field produced by the phase current, so there is almost no external magnetic field produced. This also limits the amount of force produced between conductors during a short circuit. The external housings of the conductors remain at a low potential with respect to earth ground and are usually bonded to ground.\n\nBy enclosing the conductors in separate housings a high degree of protection from two-phase and three-phase faults is obtained. Almost any fault would instead be a single-phase earth fault which does not produce a large fault current. The conductors between the generator and the first circuit breaker are even more important to protect against two- and three-phase faults because there is no breaker that can stop the fault current from the generator. While most modern circuit breakers will interrupt the fault current in less than 50 ms, the fault current from the generator will take several seconds to interrupt because the field current in the rotor takes this amount of time to discharge. The consequences of a two- or three-phase fault between the generator and the first circuit breaker are therefore much more serious and often result in severe damage to the busbars and nearby equipment.\n\nIsolated-phase bus is made in ratings from 3000 amperes to 45,000 amperes, and rated for voltages from 5000 volts up to about 35,000 volts. In the larger current ratings, dry air is forced through the enclosures and within the tubular conductors for forced-air cooling of the conductors. The cooling air is recirculated through a heat exchanger. Some items of switchgear, such as circuit breakers and isolating switches, are made in housings compatible with the isolated-phase bus. Accessories such as instrument transformers, surge arresters, and capacitors are also made in compatible housings. Due to the expense of its construction and the energy loss, isolated-phase bus is usually used in short segments; a large underground powerhouse may have isolated-phase bus up to about 250 metres or so to connect generators to transformers in an underground cavern.\n\nForced-air cooling can approximately double the rating over the same size conductors used in a self-cooled system. The extra cost of losses and cooling fan power consumption must be balanced against the lower capital cost of the bus.\n\nVarious forms of flexible terminals, expansion joints, and weatherproof or fire-proof bushings and terminals are used with isolated-phase bus. Some types of apparatus such as disconnecting switches, circuit breakers, and instrument transformers are made in enclosures that can be welded to become an integral part of the isolated-phase bus system. Isolated-phase bus is usually custom manufactured for a particular project and requires accurate dimensions of the connected equipment for manufacturing.\n\nA smaller type of isolated-phase bus is manufactured for direct-current circuits; this may be used in the field circuit of a generator.\n\nCurrently, the isolated Phase Bus current world record is 52,000 Amps, for bus made by Alstom Power (since 2015 General Electric Power) and installed at the Civaux Nuclear Power Plant, in 1997. \n\nOther types of bus are:\n\nThese are used at lower ratings or where adequate protection of the circuit by overcurrent devices is possible. \n"}
{"id": "76425", "url": "https://en.wikipedia.org/wiki?curid=76425", "title": "Kater's pendulum", "text": "Kater's pendulum\n\nA Kater's pendulum is a reversible free swinging pendulum invented by British physicist and army captain Henry Kater in 1817 for use as a gravimeter instrument to measure the local acceleration of gravity. Its advantage is that, unlike previous pendulum gravimeters, the pendulum's centre of gravity and center of oscillation do not have to be determined, allowing greater accuracy. For about a century, until the 1930s, Kater's pendulum and its various refinements remained the standard method for measuring the strength of the Earth's gravity during geodetic surveys. It is now used only for demonstrating pendulum principles.\n\nA pendulum can be used to measure the acceleration of gravity \"g\" because for narrow swings its period of swing \"T\" depends only on \"g\" and its length \"L\":\n\nSo by measuring the length \"L\" and period \"T\" of a pendulum, \"g\" can be calculated.\n\nThe Kater's pendulum consists of a rigid metal bar with two pivot points, one near each end of the bar. It can be suspended from either pivot and swung. It also has either an adjustable weight that can be moved up and down the bar, or one adjustable pivot, to adjust the periods of swing. In use, it is swung from one pivot, and the period timed, and then turned upside down and swung from the other pivot, and the period timed. The movable weight (or pivot) is adjusted until the two periods are equal. At this point the period \"T\" is equal to the period of an 'ideal' simple pendulum of length equal to the distance between the pivots. From the period and the measured distance \"L\" between the pivots, the acceleration of gravity can be calculated with great precision from the equation (1) above.\n\nThe acceleration due to gravity by Kater's pendulum is given by,\n\nformula_2\n\nwhere T1 and T2 are the time periods of oscillations when it is suspended from K1 and K2 respectively and l1 and l2 are the distances of knife edges K1 and K2 from the center of gravity respectively.\n\nThe first person to discover that gravity varied over the Earth's surface was French scientist Jean Richer, who in 1671 was sent on an expedition to Cayenne, French Guiana, by the French Académie des Sciences, assigned the task of making measurements with a pendulum clock. Through the observations he made in the following year, Richer determined that the clock was 2½ minutes per day slower than at Paris, or equivalently the length of a pendulum with a swing of one second there was 1¼ Paris \"lines\", or 2.6 mm, shorter than at Paris. It was realized by the scientists of the day, and proven by Isaac Newton in 1687, that this was due to the fact that the Earth was not a perfect sphere but slightly oblate; it was thicker at the equator because of the Earth's rotation. Since the surface was farther from the Earth's center at Cayenne than at Paris, gravity was weaker there. Since that time pendulums began to be used as precision gravimeters, taken on voyages to different parts of the world to measure the local gravitational acceleration. The accumulation of geographical gravity data resulted in more and more accurate models of the overall shape of the Earth.\n\nPendulums were so universally used to measure gravity that, in Kater's time, the local strength of gravity was usually expressed not by the value of the acceleration \"g\" now used, but by the length at that location of the \"seconds pendulum\", a pendulum with a period of two seconds, so each swing takes one second. It can be seen from equation (1) that for a seconds pendulum, the length is simply proportional to \"g\":\n\nIn Kater's time, the period \"T\" of pendulums could be measured very precisely by timing them with precision clocks set by the passage of stars overhead. Prior to Kater's discovery, the accuracy of \"g\" measurements was limited by the difficulty of measuring the other factor \"L\", the length of the pendulum, accurately. \"L\" in equation (1) above was the length of an ideal mathematical 'simple pendulum' consisting of a point mass swinging on the end of a massless cord. However the 'length' of a real pendulum, a swinging rigid body, known in mechanics as a compound pendulum, is more difficult to define. In 1673 Dutch scientist Christiaan Huygens in his mathematical analysis of pendulums, \"Horologium Oscillatorium\", showed that a real pendulum had the same period as a simple pendulum with a length equal to the distance between the pivot point and a point called the \"center of oscillation\", which is located under the pendulum's center of gravity and depends on the mass distribution along the length of the pendulum. The problem was there was no way to find the location of the center of oscillation in a real pendulum accurately. It could theoretically be calculated from the shape of the pendulum if the metal parts had uniform density, but the metallurgical quality and mathematical abilities of the time didn't allow the calculation to be made accurately.\n\nTo get around this problem, most early gravity researchers, such as Jean Picard (1669), Charles Marie de la Condamine (1735), and Jean-Charles de Borda (1792) approximated a simple pendulum by using a metal sphere suspended by a light wire. If the wire had negligible mass, the center of oscillation was close to the center of gravity of the sphere. But even finding the center of gravity of the sphere accurately was difficult. In addition, this type of pendulum inherently wasn't very accurate. The sphere and wire didn't swing back and forth as a rigid unit, because the sphere acquired a slight angular momentum during each swing. Also the wire stretched elastically during the pendulum's swing, changing \"L\" slightly during the cycle.\n\nHowever, in \"Horologium Oscillatorium\", Huygens had also proved that the pivot point and the center of oscillation were interchangeable. That is, if any pendulum is suspended upside down from its center of oscillation, it has the same period of swing, and the new center of oscillation is the old pivot point. The distance between these two conjugate points was equal to the length of a simple pendulum with the same period.\n\nAs part of a committee appointed by the Royal Society in 1816 to reform British measures, Kater had been contracted by the House of Commons to determine accurately the length of the seconds pendulum in London. He realized Huygens' principle could be used to find the center of oscillation, and so the length \"L\", of a rigid (compound) pendulum. If a pendulum were hung upside down from a second pivot point that could be adjusted up and down on the pendulum's rod, and the second pivot were adjusted until the pendulum had the same period as it did when swinging right side up from the first pivot, the second pivot would be at the center of oscillation, and the distance between the two pivot points would be \"L\".\nKater wasn't the first to have this idea. French mathematician Gaspard de Prony first proposed a reversible pendulum in 1800, but his work was not published till 1889. In 1811 Friedrich Bohnenberger again discovered it, but Kater independently invented it and was first to put it in practice.\n\nKater built a pendulum consisting of a brass rod about 2 meters long, 1½ inches wide and one-eighth inch thick, with a weight \"(d)\" on one end. For a low friction pivot he used a pair of short triangular 'knife' blades attached to the rod. In use the pendulum was hung from a bracket on the wall, supported by the edges of the knife blades resting on flat agate plates. The pendulum had two of these knife blade pivots \"(a)\", facing one another, about a meter (40 in) apart, so that a swing of the pendulum took approximately one second when hung from each pivot.\n\nKater found that making one of the pivots adjustable caused inaccuracies, making it hard to keep the axis of both pivots precisely parallel. Instead he permanently attached the knife blades to the rod, and adjusted the periods of the pendulum by a small movable weight \"(b,c)\" on the pendulum shaft. Since gravity only varies by a maximum of 0.5% over the Earth, and in most locations much less than that, the weight only had to be adjusted slightly. Moving the weight toward one of the pivots decreased the period when hung from that pivot, and increased the period when hung from the other pivot. This also had the advantage that the precision measurement of the separation between the pivots only had to be made once.\n\nTo use, the pendulum was hung from a bracket on a wall, with the knife blade pivots supported on two small horizontal agate plates, in front of a precision pendulum clock to time the period. It was swung first from one pivot, and the oscillations timed, then turned upside down and swung from the other pivot, and the oscillations timed again. The small weight \"(c)\" was adjusted with the adjusting screw, and the process repeated until the pendulum had the same period when swung from each pivot. By putting the measured period \"T\", and the measured distance between the pivot blades \"L\", into the period equation (1), \"g\" could be calculated very accurately.\n\nKater performed 12 trials. He measured the period of his pendulum very accurately using the clock pendulum by the \"method of coincidences\"; timing the interval between the \"coincidences\" when the two pendulums were swinging in synchronism. He measured the distance between the pivot blades with a microscope comparator, to an accuracy of 10 in. (2.5 μm). As with other pendulum gravity measurements, he had to apply small corrections to the result for a number of variable factors:\nHe gave his result as the length of the seconds pendulum. After corrections, he found that the mean length of the solar seconds pendulum at London, at sea level, at , swinging in vacuum, was 39.1386 inches. This is equivalent to a gravitational acceleration of 9.81158 m/s. The largest variation of his results from the mean was . This represented a precision of gravity measurement of 0.7×10 (7 milligals).\n\nIn 1824, the British Parliament made Kater's measurement of the seconds pendulum the official standard of length for defining the yard.\n\nThe large increase in gravity measurement accuracy made possible by Kater's pendulum established gravimetry as a regular part of geodesy. To be useful, it was necessary to find the exact location (latitude and longitude) of the 'station' where a gravity measurement was taken, so pendulum measurements became part of surveying. Kater's pendulums were taken on the great historic geodetic surveys of much of the world that were being done during the 19th century. In particular, Kater's pendulums were used in the Great Trigonometric Survey of India.\n\nReversible pendulums remained the standard method used for absolute gravity measurements until they were superseded by free-fall gravimeters in the 1950s.\n\nRepeatedly timing each period of a Kater pendulum, and adjusting the weights until they were equal, was time consuming and error-prone. Friedrich Bessel showed in 1826 that this was unnecessary. As long as the periods measured from each pivot, T and T, are close in value, the period \"T\" of the equivalent simple pendulum can be calculated from them:\n\nHere formula_5 and formula_6 are the distances of the two pivots from the pendulum's center of gravity. The distance between the pivots, formula_7, can be measured with great accuracy. formula_5 and formula_6, and thus their difference formula_10, cannot be measured with comparable accuracy. They are found by balancing the pendulum on a knife edge to find its center of gravity, and measuring the distances of each of the pivots from the center of gravity. However, because formula_11 is so much smaller than formula_12, the second term on the right in the above equation is small compared to the first, so formula_10 doesn't have to be determined with high accuracy, and the balancing procedure described above is sufficient to give accurate results.\n\nTherefore, the pendulum doesn't have to be adjustable at all, it can simply be a rod with two pivots. As long as each pivot is close to the center of oscillation of the other, so the two periods are close, the period \"T\" of the equivalent simple pendulum can be calculated with equation (2), and the gravity can be calculated from \"T\" and \"L\" with (1).\n\nIn addition, Bessel showed that if the pendulum was made with a symmetrical shape, but internally weighted on one end, the error caused by effects of air resistance would cancel out. Also, another error caused by the finite diameter of the pivot knife edges could be made to cancel out by interchanging the knife edges.\n\nBessel didn't construct such a pendulum, but in 1864 Adolf Repsold, under contract to the Swiss Geodetic Commission, developed a symmetric pendulum 56 cm long with interchangeable pivot blades, with a period of about ¾ second. The Repsold pendulum was used extensively by the Swiss and Russian Geodetic agencies, and in the Survey of India. Other widely used pendulums of this design were made by Charles Peirce and C. Defforges.\n\n"}
{"id": "4502534", "url": "https://en.wikipedia.org/wiki?curid=4502534", "title": "Kharkiv TEC-5", "text": "Kharkiv TEC-5\n\nKharkiv TEC-5 () is a combined heat and power plant at Podvorky village in Derhachi Raion of Kharkiv Oblast, Ukraine. It has capacity of 540 MW of electric power and up 1420 Gcal/h of heat power. It has a tall chimney, built in 1979. There are also two tall cooling towers.\n\nThe power plant is owned and operated by JSC Kharkiv CHPP-5 (), a subsidiary of Naftogaz Ukrainy.\n\n"}
{"id": "6587584", "url": "https://en.wikipedia.org/wiki?curid=6587584", "title": "Mace (unit)", "text": "Mace (unit)\n\nA mace (; Hong Kong English usage: tsin; Southeast Asian English usage: chee) is a traditional Chinese measurement of weight in East Asia that was also used as a currency denomination. It is equal to 10 candareens and is of a tael or approximately 3.78 grams. A troy mace is approximately 3.7429 grams. In Hong Kong, one mace is 3.779936375 grams. and in Ordinance 22 of 1884, it is ounces avoirdupois. In Singapore, one mace (referred to as chee) is 3.77994 grams.\n\nIn imperial China, 10 candareens equaled 1 mace which was of a tael and, like the other units, was used in weight-denominated silver currency system. A common denomination was 7 mace and 2 candareens, equal to one silver Chinese yuan.\n\nLike other similar measures such as tael and catty, the English word \"mace\" derives from Malay, in this case through Dutch \"maes\", plural \"masen\", from Malay \"mas\" which, in turn, derived from Sanskrit \"\", a word related to \"mash,\" another name for the urad bean, and masha, a traditional Indian unit of weight equal to 0.97 gram. This word is unrelated to other uses of mace in English.\n\nThe Chinese word for mace is \"qian\" (), which is also a generic word for \"money\" in Mandarin Chinese. (The same Chinese character (kanji) was used for the Japanese \"sen\", the former unit equal to of a Japanese yen; the Korean \"chŏn\"(revised: jeon), the former unit equal to of a Korean won; and for the Vietnamese tiền, a currency used in late imperial Vietnam; although neither of these has ever been known as \"mace\" in English.)\n\n"}
{"id": "13015579", "url": "https://en.wikipedia.org/wiki?curid=13015579", "title": "Mesotrophic soil", "text": "Mesotrophic soil\n\nMesotrophic soils are soils with a moderate inherent fertility. An indicator of soil fertility is its base status, which is expressed as a ratio relating the major nutrient cations (calcium, magnesium, potassium and sodium) found there to the soil's clay percentage. This is commonly expressed in hundredths of a mole of cations per kilogram of clay, i.e. cmol (+) kg clay.\n\n"}
{"id": "2479157", "url": "https://en.wikipedia.org/wiki?curid=2479157", "title": "Monte Carlo N-Particle Transport Code", "text": "Monte Carlo N-Particle Transport Code\n\nMonte Carlo N-Particle Transport Code (MCNP) is a software package for simulating nuclear processes. It is developed by Los Alamos National Laboratory since at least 1957 with several further major improvements. It is distributed within the United States by the Radiation Safety Information Computational Center in Oak Ridge, TN and internationally by the Nuclear Energy Agency in Paris, France. It is used primarily for the simulation of nuclear processes, such as fission, but has the capability to simulate particle interactions involving neutrons, photons, and electrons among other particles. \"Specific areas of application include, but are not limited to, radiation protection and dosimetry, radiation shielding, radiography, medical physics, nuclear criticality safety, detector design and analysis, nuclear oil well logging, accelerator target design, fission and fusion reactor design, decontamination and decommissioning.\"\n\nMonte Carlo N-Particle eXtended (MCNPX) was also developed at Los Alamos National Laboratory, and is capable of simulating particle interactions of 34 different types of particles (nucleons and ions) and 2000+ heavy ions at nearly all energies, including those simulated by MCNP.\n\nBoth codes can be used to judge whether or not nuclear systems are critical and to determine doses from sources, among other things.\n\n\"MCNP6\" is a merger of MCNP5 and MCNPX.\n\n\n\n"}
{"id": "12082326", "url": "https://en.wikipedia.org/wiki?curid=12082326", "title": "Our Angry Earth", "text": "Our Angry Earth\n\nOur Angry Earth: A Ticking Ecological Bomb (1991) is a non-fiction book and polemic against the effects humankind is having on the environment by the science fiction writers Isaac Asimov and Frederik Pohl. \nIn his last non-fiction book, Asimov co-writes with his long-time friend science fiction author Frederik Pohl, and deals with elements of the environmental crisis such as overpopulation, oil dependence, war, global warming and the destruction of the ozone layer.\n\nIt suggests monumental disasters are threatening to destroy humankind and argues that \"it is too late to save our planet from harm\". The book has four sections: \"The Background\", \"The Problems\", \"The Technocures\" and \"The Way to Go\".\n\nIt was first published by Tor Books in 1991, .\n"}
{"id": "2129702", "url": "https://en.wikipedia.org/wiki?curid=2129702", "title": "Phototube", "text": "Phototube\n\nA phototube or photoelectric cell is a type of gas-filled or vacuum tube that is sensitive to light. Such a tube is more correctly called a 'photoemissive cell' to distinguish it from photovoltaic or photoconductive cells. Phototubes were previously more widely used but are now replaced in many applications by solid state photodetectors. The photomultiplier tube is one of the most sensitive light detectors, and is still widely used in physics research.\n\nPhototubes operate according to the photoelectric effect: Incoming photons strike a photocathode, knocking electrons out of its surface, which are attracted to an anode. Thus current is dependent on the frequency and intensity of incoming photons. Unlike photomultiplier tubes, no amplification takes place, so the current through the device is typically of the order of a few microamperes.\n\nThe light wavelength range over which the device is sensitive depends on the material used for the photoemissive cathode. A caesium-antimony cathode gives a device that is very sensitive in the violet to ultra-violet region with sensitivity falling off to blindness to red light. Caesium on oxidised silver gives a cathode that is most sensitive to infra-red to red light, falling off towards blue, where the sensitivity is low but not zero.\n\nVacuum devices have a near constant anode current for a given level of illumination relative to anode voltage. Gas filled devices are more sensitive but the frequency response to modulated illumination falls off at lower frequencies compared to the vacuum devices. The frequency response of vacuum devices is generally limited by the transit time of the electrons from cathode to anode. \n\nOne major application of the phototube was the reading of optical sound tracks for projected films. Phototubes were used in a variety of light-sensing applications until they were superseded by photoresistors and photodiodes.\n"}
{"id": "50425", "url": "https://en.wikipedia.org/wiki?curid=50425", "title": "Quantum Hall effect", "text": "Quantum Hall effect\n\nThe quantum Hall effect (or integer quantum Hall effect) is a quantum-mechanical version of the Hall effect, observed in two-dimensional electron systems subjected to low temperatures and strong magnetic fields, in which the Hall conductance undergoes quantum Hall transitions to take on the quantized values\n\nwhere is the channel current, is the Hall voltage, is the elementary charge and is Planck's constant. The prefactor is known as the filling factor, and can take on either integer () or fractional () values. The quantum Hall effect is referred to as the integer or fractional quantum Hall effect depending on whether is an integer or fraction, respectively.\n\nThe striking feature of the integer quantum Hall effect is the persistence of the quantization (i.e. the Hall plateau) as the electron density is varied. Since the electron density remains constant when the Fermi level is in a clean spectral gap, this situation corresponds to one where the Fermi level is an energy with a finite density of states, though these states are localized (see Anderson localization).\n\nThe fractional quantum Hall effect is more complicated, as its existence relies fundamentally on electron–electron interactions. The fractional quantum Hall effect is also understood as an integer quantum Hall effect, although not of electrons but of charge-flux composites known as composite fermions. In 1988, it was proposed that there was quantum Hall effect without Landau levels. This quantum Hall effect is referred to as the quantum anomalous Hall (QAH) effect. There is also a new concept of the quantum spin Hall effect which is an analogue of the quantum Hall effect, where spin currents flow instead of charge currents.\n\nThe quantization of the Hall conductance has the important property of being exceedingly precise. Actual measurements of the Hall conductance have been found to be integer or fractional multiples of to nearly one part in a billion. This phenomenon, referred to as \"exact quantization\", has been shown to be a subtle manifestation of the principle of gauge invariance. It has allowed for the definition of a new practical standard for electrical resistance, based on the resistance quantum given by the von Klitzing constant . This is named after Klaus von Klitzing, the discoverer of exact quantization. Since 1990, a fixed conventional value has been used in resistance calibrations worldwide. On 16 November 2018, the conventional value was abrogated in consequence of the decision to fix the values of h (the Planck constant) and e (the elementary charge) at the 26th meeting of the General Conference on Weights and Measures. The quantum Hall effect also provides an extremely precise independent determination of the fine structure constant, a quantity of fundamental importance in quantum electrodynamics.\n\nThe integer quantization of the Hall conductance was originally predicted by Ando, Matsumoto, and Uemura in 1975, on the basis of an approximate calculation which they themselves did not believe to be true. Several researchers subsequently observed the effect in experiments carried out on the inversion layer of MOSFETs. It was only in 1980 that Klaus von Klitzing, working at the high magnetic field laboratory in Grenoble with silicon-based samples developed by Michael Pepper and Gerhard Dorda, made the unexpected discovery that the Hall conductivity was \"exactly\" quantized. For this finding, von Klitzing was awarded the 1985 Nobel Prize in Physics. The link between exact quantization and gauge invariance was subsequently found by Robert Laughlin, who connected the quantized conductivity to the quantized charge transport in Thouless charge pump. Most integer quantum Hall experiments are now performed on gallium arsenide heterostructures, although many other semiconductor materials can be used. In 2007, the integer quantum Hall effect was reported in graphene at temperatures as high as room temperature, and in the magnesium zinc oxide ZnO–MgZnO.\n\nIn two dimensions, when classical electrons are subjected to a magnetic field they follow circular cyclotron orbits. When the system is treated quantum mechanically, these orbits are quantized. The energy levels of these quantized orbitals take on discrete values:\nwhere is the cyclotron frequency. These orbitals are known as Landau levels, and at weak magnetic fields, their existence gives rise to many \"quantum oscillations\" such as the Shubnikov–de Haas oscillations and the de Haas–van Alphen effect (which is often used to map the Fermi surface of metals). For strong magnetic fields, each Landau level is highly degenerate (i.e. there are many single particle states which have the same energy ). Specifically, for a sample of area , in magnetic field , the degeneracy of each Landau level is\nwhere represents a factor of 2 for spin degeneracy, and is the magnetic flux quantum. For sufficiently strong magnetic fields, each Landau level may have so many states that all of the free electrons in the system sit in only a few Landau levels; it is in this regime where one observes the quantum Hall effect.\n\nThe integers that appear in the Hall effect are examples of topological quantum numbers. They are known in mathematics as the first Chern numbers and are closely related to Berry's phase. A striking model of much interest in this context is the Azbel–Harper–Hofstadter model whose quantum phase diagram is the Hofstadter butterfly shown in the figure. The vertical axis is the strength of the magnetic field and the horizontal axis is the chemical potential, which fixes the electron density. The colors represent the integer Hall conductances. Warm colors represent positive integers and cold colors negative integers. Note, however, that the density of states in these regions of quantized Hall conductance is zero, hence they cannot produce the plateaus observed in the experiments. The phase diagram is fractal and has structure on all scales. In the figure there is an obvious self-similarity. In the presence of disorder, which is the source of the plateaus seen in the experiments, this diagram is very different and the fractal structure is mostly washed away.\n\nConcerning physical mechanisms, impurities and/or particular states (e.g., edge currents) are important for both the 'integer' and 'fractional' effects. In addition, Coulomb interaction is also essential in the fractional quantum Hall effect. The observed strong similarity between integer and fractional quantum Hall effects is explained by the tendency of electrons to form bound states with an even number of magnetic flux quanta, called \"composite fermions\".\n\nThe value of the von Klitzing constant may be obtained already on the level of a single atom within the Bohr model while looking at it as a single electron Hall effect. While during the cyclotron motion on a circular orbit the centrifugal force is balanced by the Lorentz force responsible for the transverse induced voltage and the Hall effect one may look at the Coulomb potential difference in the Bohr atom as the induced single atom Hall voltage and the periodic electron motion on a circle a Hall current. Defining the single atom Hall current as a rate a single electron charge formula_4 is making the Kepler revolutions with the angular frequency formula_5\n\nand the induced Hall voltage as a difference between the nucleus hydrogen Coulomb potential at the electron orbital point and at the infinity:\n\nOne obtains the quantization of the defined Bohr orbit Hall resistance in steps of the von Klitzing constant as \n\nwhich for the Bohr atom is linear but not inverse in the integer \"n\".\n\n\n"}
{"id": "30804687", "url": "https://en.wikipedia.org/wiki?curid=30804687", "title": "Region (mathematics)", "text": "Region (mathematics)\n\nIn mathematical analysis, the word region usually refers to a subset of formula_1 or formula_2 that is open (in the standard Euclidean topology), connected and non-empty. A closed region is sometimes defined to be the closure of a region.\n\nRegions and closed regions are often used as domains of functions or differential equations.\n\nAccording to Kreyszig,\nAccording to Yue Kuen Kwok,\n\n\n"}
{"id": "44367855", "url": "https://en.wikipedia.org/wiki?curid=44367855", "title": "Robert Porter Allen", "text": "Robert Porter Allen\n\nRobert Porter Allen (24 April 1905 in South Williamsport, Pennsylvania) – 28 June 1963 was an American ornithologist and environmentalist. He achieved worldwide attention for his rescue operations of the whooping crane (\"Grus Americana\") in the 1940s and 1950s.\nAllen helped save the Roseate Spoonbill from extinction. He set up a tent on Bottle Key in the Florida Bay in 1938 so that he could observe the nesting Spoonbills up close. He was the first ecologist to do this work with Roseate Spoonbills.\n\nAllen was a pioneer in early field biology and led large conservation efforts around the world to save the whooping crane (Grus americana), roseate spoonbill (Platalea ajaja), and the flamingo (Phoenicopterus ruber). Allen joined the Junior Audubon Club at a young age, and this is where his passion for birds began. He attended Lafayette College to study ornithology for a short period of time, but quickly lost interest. He secured a librarian job at Audubon, and soon became one of the youngest Audubon sanctuary directors ever appointed. Allen had a very specific approach to restoring endangered populations, and felt that widespread cooperation was necessary for success. Allen would first conduct extensive field research and documentation on the bird before any interaction. He would then publish and sell his research and include lots of illustrations and pictures for easy understanding. Allen would also capture endangered wild birds and exhibit them locally in order to raise awareness and raise money for habitat restoration. Allen fought for legislation to protect endangered species. He Helped bring awareness to all three birds, helped protect and zone off their breeding grounds, and helped generate funds from wealth families and the federal government for habitat restoration. Allen and others even flew ultra light planes to guide migratory birds to their nesting grounds.\n\nAllen began working with the whooping crane in 1946; he wanted to learn what was causing the population to decline. He noticed that the whooping cranes were not returning after plume hunting in large numbers like other birds. He began studying the whooping crane at the Aransas Refuge in Texas. He photographed and drew detailed pictures of the wild whooping crane. He studied and documented the nesting habits of the whooping crane. He also studied and photographed the various marine life and vegetation that the birds eat. He discovered that the Grus americana migrates to Aransas Refuge during the winter, because of viable food sources and mild climate. All of Allen's fieldwork was carried out with the intention of learning the ideal conditions for whooping crane reproduction. He hoped to replicate these conditions for whooping cranes in captivity, and use the knowledge to help the remaining whooping cranes to survive. The Grus americana was a bird almost driven to extinction by early Americans and their hunting habits. Allen discovered that unlike the flamingos the Grus americana can reproduce in small numbers and live a long time. This made conservation efforts much easier, once a large group was established in protected areas. \n\nLater Allen began three years of fieldwork in the Caribbean, and his studies focused on the entire range of the American Flamingo (Phoenicopterus ruber). He was working for the Society for the Protection of the Flamingo. The organization studied flamingos and brought flamingos to the New Providence Island in order to spark public interest in flamingos, and to generate donations to the foundation. Allen discovered that Flamingos work and function better in large communities, the larger the group the better. This evidence proves that the flamingos are low on the evolutionary scale, and their breeding practices a complicated and can confuse the birds. When Allen first began following the pack of flamingos the average flock size was 24 birds. Allen discovered that flamingos unlike other birds need over three hundred individuals to successfully carry out the mating ritual. It is reasonable to believe that the original number of flamingos was around 95,000 in 1600 A.D. In 1955 Allen estimated the population total to be 21,500, 80 percent less than their original number. Allen discovered that even the smallest of things could disrupt the flamingos' reproductive cycle. Gunshots, planes flying overhead, hurricanes, flooding, damming and diking bodies of water all caused the population to decline. On the island of San Lucar many people stole the eggs from flamingos for food; this could stunt the growth of a colony of flamingos for up to three decades. Allen's studies show that the same number of flamingos produced each year is equal to the number that dies off. \n\nAllen took interest in the roseate spoonbill because he felt that the U.S. government was ignoring the low population numbers. Allen wanted to secure federal funding for the roseate spoonbill, and wanted Florida Bay to be included in the Everglades National Park. Roseate spoonbills were not seen from 1865-1890 due to over hunting; they were very close to extinct. When Allen began studying the roseate spoonbill there were only 8 nesting locations in the U.S. Allen believed that public education on endangered species of birds was fundamental in fully restoring the population.\n\nAllen found that plumage hunting depleted the populations of roseate spoonbills, flamingos, and whooping cranes. Allen deducted that over hunting not only reduced the overall population of these birds, but it made them vulnerable to a whole new host of things. Once the populations were decreased the birds could no longer defend themselves from certain predators, or reproduce properly.\n\nAllen had great success growing the population of whooping cranes. The population was at 15 when Allen began his work in 1941, and the population is around 500 in 2010. Allen gained national popularity and news coverage when he spent eight years looking for the last remain nesting site for whooping cranes. Allen concluded that overdevelopment; habitat loss and unregulated hunting were the main causes for low numbers of whooping cranes, roseate spoonbills, and flamingos. Allen changed the ways Americans thought about wildlife through education programs. in 1973 his efforts ultimately lead to the passage of the Endangered Species Act.\n\n\n"}
{"id": "11410487", "url": "https://en.wikipedia.org/wiki?curid=11410487", "title": "SS Charles H. Cugle", "text": "SS Charles H. Cugle\n\nSS \"Charles H. Cugle\" was a Type Z-EC2-S-C5 Liberty ship built by J.A. Jones Construction of Panama City, Florida, launched on 13 August 1945. It was ordered by the War Shipping Administration under Maritime Commission Contract number 3145.\n\nAs part of the Army Nuclear Power Program the ship was transferred to the U.S. Army in March 1963, and fitted with a pressurized water reactor, fuelled by used low enriched uranium, designed by Martin Marietta, becoming the world's first floating nuclear power plant, at a cost of $17 million.\n\nNow renamed \"Sturgis\" (MH-1A) the reactor began operation on 24 January 1967 at Fort Belvoir, Virginia, generating 10 MWe of electrical power. The reactor barge was then towed to Gatun Lake in the Panama Canal Zone to provide power, owing to a lack of water for the hydroelectric plant. The ship returned to Fort Belvoir in early 1977, and the reactor deactivated and de-fueled. The ship was decontaminated, sealed, and assigned to the James River Reserve Fleet for an expected 50 years of SAFSTOR.\n\nHowever, 38 years later the Army Corps of Engineers deemed there were low enough levels of radioactivity in the mothballed vessel's contaminated areas for it to be scrapped. It was scheduled to be towed from Virginia to Galveston in April–May 2015 where subcontractor Malin International Ship Repair and Drydock will begin the 12- to 18-month work of removing the contaminated material and placing it in rail cars to be hauled to a hazardous materials disposal site, after which the remaining portions of the vessel will be cut up and sold for scrap value.\n\n"}
{"id": "43390294", "url": "https://en.wikipedia.org/wiki?curid=43390294", "title": "Shuakhevi Hydro Power Plant", "text": "Shuakhevi Hydro Power Plant\n\nThe Shuakhevi Hydro Power Plant (Skuakhevi HPP), is a run-of-the-river plant currently under construction in Adjara, Georgia. Construction on the project began in 2013 and it is expected to be operational in 2016. It will have an installed capacity of with expected electricity output of . The plant will have the capacity for diurnal storage in two reservoirs ( Skhalta dam with a reservoir and Didachara dam with a reservoir) allowing Shuakhevi HPP to store water for up to 12 hours and sell electricity at peak demand times. Three main tunnels are to be constructed on the Shuakhevi project; the 5.8 km Chirukhistsqali to Skhalta transfer tunnel, the Skhalta to Didachara transfer tunnel and the 17.8 km Shuakhevi headrace and pressure tunnel. It is estimated that the project will cost US$417 million. For the purposes of developing, constructing and operating the Shuakhevi HPP, ADB and EBRD extended a loan to Adjaristsqali Georgia LLC of up to $86.5 million and IFC a loan of 80m USD. Adjaristsqali Georgia LLC is owned by the Norwegian Clean Energy Invest AS (40%), India’s Tata Power (40%), and IFC Infraventures (20%), an investment fund created by the International Finance Corporation. Company Alstom was chosen to supply the electromechanical equipment for the project.\n\nElectricity demand in Georgia has been on the rise since 2009 and investment in new generation capacity is lagging behind. Georgia has about 40 billion kwt/h of potential electricity and only 18-20% is utilized today. The Shuakhevi HPP is a part of a large Georgian strategy to develop its hydropower potential. It will enable Georgia to use more of its energy resources to meet electricity demand during the winter months of December, January and February. Most of the energy will be exported to Turkey. The positive aspects of the investment according to EBRD include strengthening Georgia’s private energy sector, demonstrating new financing methods (the project will be the first power project in Georgia to rely on limited recourse financing) and setting standards for corporate governance and business conduct. It will also generate employment opportunities for local population. As of April 2016, more than 700 Georgians are working in the project, most of the\n\npeople from the local communities. To this date, almost 500 locals have been trained in a school especially designed to build required technical skills. The project company has also funded an extensive CSR programme in the valley with significant infrastructure upgrades for the local communities, support for local businesses etc. The project has also enabled the construction of a new 220 kV transmission line from Akhalsikhe to Batumi, significantly strengthening the grid connection in the whole of South Western Georgia.\n\nThe contract for the Shuakhevi HPP project is the only contract for a hydropower project in Georgia where articles related to obligations of the state and the company on electricity tariffs and economic profitability are kept confidential. It is hard to say how the project will benefit Georgia, since it is not clear how the money will end up in the state budget.\n\nThe project has responsibility to deliver electricity for three winter months in Georgia for a fixed tariff, contributing to reducing the need to import electricity and reducing the carbon emissions from electricity production in Georgia.\n\nThe project will pay 1% of invested amount in property taxes to the local municipality where the infrastructure is located, which is estimated to significantly increase the budgets in the municipalities hosting the infrastructure.\n\nThe project is designed to divert water flows from the upper parts of the Adjaristskali, Skhalta and Chirukhistskali rivers towards the reservoirs and then the powerhouse and leave only 10 percent of the average annual flow of the rivers. That will have a negative impact on the river ecosystem, including red-listed species like trout.\n\nSince there are no villages within the project site, the project does not envisage resettlement of local population. However, the project site is characterised by landslides. The tunnel, which has been drilled under the village Ghurta, which is one of three planned diversion tunnels as part of the project, did not trigger a landslide as feared by the local population. Detailed geological investigations were undertaken by Mott MacDonald, one of the leading underground geological engineers in the world before the construction started to ensure that it was safe to construct a dam in the Didachara area. The memory of the 1971 landslide which cost 22 people’s lives is still very much alive among local population. Adjaristsqali Georgia LLC is denying the risks, however they are reluctant to sign warranty contracts to offer compensation in case the constructions cause damages.\n\nOn 8 March there was a protest against the Shuakhevi HPP. About 500 villagers blocked the road to prevent the construction of a tunnel for the 187 MW Shuakhevi HPP in Didachara were dispersed by policemen and special forces. The main concern of the people is that the construction would trigger landslides.\n\n"}
{"id": "760565", "url": "https://en.wikipedia.org/wiki?curid=760565", "title": "Solar chimney", "text": "Solar chimney\n\nA solar chimney often referred to as a thermal chimney is a way of improving the natural ventilation of buildings by using convection of air heated by passive solar energy. A simple description of a solar chimney is that of a vertical shaft utilizing solar energy to enhance the natural stack ventilation through a building.\n\nThe solar chimney has been in use for centuries, particularly in the Middle east and Near East by the Persians, as well as in Europe by the Romans.\n\nIn its simplest form, the solar chimney consists of a black-painted chimney. During the day solar energy heats the chimney and the air within it, creating an updraft of air in the chimney. The suction created at the chimney's base can be used to ventilate and cool the building below. In most parts of the world it is easier to harness wind power for such ventilation as with a windcatcher, but on hot windless days a Solar chimney can provide ventilation where otherwise there would be none.\n\nThere are however a number of solar chimney variations. The basic design elements of a solar chimney are:\n\n\nA principle has been proposed for solar power generation, using a large greenhouse at the base rather than relying solely on heating the chimney itself. (For further information on this issue, see Solar updraft tower.)\n\nSolar chimneys are painted black so that they absorb the sun's heat more effectively. When the air inside the chimney is heated, it rises and pulls cold air out from under the ground via the heat exchange tubes.\n\nSolar chimneys, also called heat chimneys or heat stacks, can also be used in architectural settings to decrease the energy used by mechanical systems (systems that heat and cool the building through mechanical means). Air conditioning and mechanical ventilation have been for decades the standard method of environmental control in many building types, especially offices, in developed countries. Pollution and reallocating energy supplies have led to a new environmental approach in building design. Innovative technologies along with bioclimatic principles and traditional design strategies are often combined to create new and potentially successful design solutions. The solar chimney is one of these concepts currently explored by scientists as well as designers, mostly through research and experimentation.\n\nA solar chimney can serve many purposes. Direct gain warms air inside the chimney causing it to rise out the top and drawing air in from the bottom. This drawing of air can be used to ventilate a home or office, to draw air through a geothermal heat exchange, or to ventilate only a specific area such as a composting toilet.\n\nNatural ventilation can be created by providing vents in the upper level of a building to allow warm air to rise by convection and escape to the outside. At the same time cooler air can be drawn in through vents at the lower level. Trees may be planted on that side of the building to provide shade for cooler outside air.\n\nThis natural ventilation process can be augmented by a solar chimney. The chimney has to be higher than the roof level, and has to be constructed on the wall facing the direction of the sun. Absorption of heat from the sun can be increased by using a glazed surface on the side facing the sun. Heat absorbing material can be used on the opposing side. The size of the heat-absorbing surface is more important than the diameter of the chimney. A large surface area allows for more effective heat exchange with the air necessary for heating by solar radiation. Heating of the air within the chimney will enhance convection, and hence airflow through the chimney. Openings of the vents in the chimney should face away from the direction of the prevailing wind.\n\nTo further maximize the cooling effect, the incoming air may be led through underground ducts before it is allowed to enter the building. The solar chimney can be improved by integrating it with a trombe wall. The added advantage of this design is that the system may be reversed during the cold season, providing solar heating instead.\n\nA variation of the solar chimney concept is the solar attic. In a hot sunny climate the attic space is often blazingly hot in the summer. In a conventional building this presents a problem as it leads to the need for increased air conditioning. By integrating the attic space with a solar chimney, the hot air in the attic can be put to work. It can help the convection in the chimney, improving ventilation.\n\nThe use of a solar chimney may benefit natural ventilation and passive cooling strategies of buildings thus help reduce energy use, CO emissions and pollution in general. Potential benefits regarding natural ventilation and use of solar chimneys are:\n\nPotential benefits regarding passive cooling may include:\n\n\nThe Building Research Establishment (BRE) office building in Garston, Watford, United Kingdom, incorporates solar-assisted passive ventilation stacks as part of its ventilation strategy.\n\nDesigned by architects Feilden Clegg Bradley, the BRE offices aim to reduce energy consumption and CO emissions by 30% from current best practice guidelines and sustain comfortable environmental conditions without the use of air conditioning. The passive ventilation stacks, solar shading, and hollow concrete slabs with embedded under floor cooling are key features of this building. Ventilation and heating systems are controlled by the building management system (BMS) while a degree of user override is provided to adjust conditions to occupants' needs.\n\nThe building utilizes five vertical shafts as an integral part of the ventilation and cooling strategy. The main components of these stacks are a south facing glass-block wall, thermal mass walls and stainless steel round exhausts rising a few meters above roof level. The chimneys are connected to the curved hollow concrete floor slabs which are cooled via night ventilation. Pipes embedded in the floor can provide additional cooling utilizing groundwater.\n\nOn warm windy days air is drawn in through passages in the curved hollow concrete floor slabs. Stack ventilation naturally rising out through the stainless steel chimneys enhances the air flow through the building. The movement of air across the chimney tops enhances the stack effect.\nDuring warm, still days, the building relies mostly on the stack effect while air is taken from the shady north side of the building. Low-energy fans in the tops of the stacks can also be used to improve airflow.\n\nOvernight, control systems enable ventilation paths through the hollow concrete slab removing the heat stored during the day, which then remains cold for the following day. The exposed curved ceiling gives more surface area than a flat ceiling would, acting as a heat sink, again providing summer cooling.\nResearch based on actual performance measurements of the passive stacks found that they enhanced the cooling ventilation of the space during warm and still days and may also have the potential to assist night-time cooling due to their thermally massive structure.\n\nA technology closely related to the solar chimney is the evaporative down-draft cooltower. In areas with a hot, arid climate this approach may contribute to a sustainable way to provide air conditioning for buildings.\n\nThe principle is to allow water to evaporate at the top of a tower, either by using evaporative cooling pads or by spraying water. Evaporation cools the incoming air, causing a downdraft of cool air that will bring down the temperature inside the building. Airflow can be increased by using a solar chimney on the opposite side of the building to help in venting hot air to the outside. This concept has been used for the Visitor Center of Zion National Park. The Visitor Center was designed by the High Performance Buildings Research of the National Renewable Energy Laboratory (NREL).\n\nThe principle of the downdraft cooltower has been proposed for solar power generation as well. (See Energy tower for more information.)\n\nEvaporation of moisture from the pads on top of the Toguna buildings built by the Dogon people of Mali, Africa contribute to the coolness felt by the men who rest underneath. The women's buildings on the outskirts of town are functional as more conventional solar chimneys.\n\n\n"}
{"id": "36095989", "url": "https://en.wikipedia.org/wiki?curid=36095989", "title": "Solar power in Lithuania", "text": "Solar power in Lithuania\n\nSolar power in Lithuania is a form of renewable energy in Lithuania, and created 39 GWh of electricity in the first nine months of 2013.\n\nLithuania has 1,580 small (from several kilowatts to 2,500 kW) solar power plants with a total installed capacity of 59.4 MW which produce electricity for the country, and has an uncounted number of private power plants which make electricity only for their owners.\n\n\n"}
{"id": "1217509", "url": "https://en.wikipedia.org/wiki?curid=1217509", "title": "Tamarillo", "text": "Tamarillo\n\nThe tamarillo is a small tree or shrub in the flowering plant family Solanaceae (the nightshade family). It is best known as the species that bears the tamarillo, an egg-shaped edible fruit. It is also known as the tree tomato, tomate andino, tomate serrano, tomate de yuca, sachatomate, berenjena, tamamoro, and tomate de árbol in South America. They are popular globally, especially in New Zealand, Australia, and America.\n\nThe tamarillo is native to the Andes of Ecuador, Colombia, Peru, Chile, and Bolivia.\nToday it is still cultivated in gardens and small orchards for local production, and it is one of the most popular fruits in these regions. Other regions of cultivation are the subtropical areas throughout the world, such as Rwanda, South Africa, Darjeeling and Sikkim in India, Nepal, Hong Kong, China, the United States, Australia, Bhutan and New Zealand.\n\nThe first internationally marketed crop of tamarillos in Australia was produced around 1996, although permaculture and exotic fruit enthusiasts had increasingly grown the fruit around the country from the mid-1970s on.\n\nIn New Zealand, about 2,000 tons are produced on 200 hectares of land and exported to the United States, Japan and Europe. For the export, the existing marketing channels developed for the kiwifruit are used.\n\nThe tamarillo is also successfully grown at higher elevations of Malaysia and the Philippines, and in Puerto Rico. In the hot tropical lowlands, it develops only small fruits and fruit setting is seldom.\n\nPrior to 1967, the fruit was known as the \"tree tomato\" but the new name tamarillo (which was not the name in Spanish or any other language) was chosen by the New Zealand Tree Tomato Promotions Council in order to distinguish it from the ordinary garden tomato and increase its exotic appeal.\n\nThe plant is a fast-growing tree that grows up to 5 meters. Peak production is reached after 4 years, and the life expectancy is about 12 years. The tree usually forms a single upright trunk with lateral branches. The flowers and fruits hang from the lateral branches. The leaves are large, simple and perennial, and have a strong pungent smell. The flowers are pink-white, and form clusters of 10 to 50 flowers. They produce 1 to 6 fruits per cluster. Plants can set fruit without cross-pollination, but the flowers are fragrant and attract insects. Cross-pollination seems to improve fruit set. The roots are shallow and not very pronounced, therefore the plant is not tolerant of drought stress and can be damaged by strong winds. Tamarillos will hybridize with many other solanaceae, though the hybrid fruits will be sterile, and unpalatable in some instances.\n\nThe fruits are egg-shaped and about 4-10 centimeters long. Their color varies from yellow and orange to red and almost purple. Sometimes they have dark, longitudinal stripes. Red fruits are more acetous, yellow and orange fruits are sweeter. The flesh has a firm texture and contains more and larger seeds than a common tomato. The fruits are very high in vitamins and iron and low in calories (only about 40 calories per fruit).\n\nThe tamarillo prefers subtropical climate, with rainfall between 600 and 4000 millimeters and annual temperatures between 15 and 20 °C. It is intolerant to frost (below -2 °C) and drought stress. It is assumed that fruit set is affected by night temperatures. Areas where citrus are cultivated provide good conditions for tamarillos as well, such as in the Mediterranean climate.\nTamarillo plants grow best in light, deep, fertile soils, although they are not very demanding. However, soils must be permeable since the plants are not tolerant to water-logging. They grow naturally on soils with a pH of 5 to 8.5.\n\nPropagation is possible by both using seeds or cuttings. Seedlings first develop a straight, about 1.5 to 1.8 meters tall trunk, before they branch out. Propagation by seeds is easy and ideal in protected environments. However, in orchards with different cultivars, cross-pollination will occur and characteristics of the cultivars get mixed up. Seedlings should be kept in the nursery until they reach a height of 1 to 1.5 meters, as they are very frost-sensitive.\n\nPlants grown from cuttings branch out earlier and result in more shrub-like plants that are more suitable for exposed sites. Cuttings should be made from basal and aerial shoots, and should be free of pathogenic viruses. Plants grown from cuttings should be kept in the nursery until they reach a height of 0.5 to 1 meter.\n\nThe tree grows very quickly and is able to bear fruit after 1.5 to 2 years. The plant is daylength-insensitive. The fruits do not mature simultaneously, unless the tree has been pruned. A single tree can produce more than 20 kg of fruit per year; an orchard yields in 15 to 17 tons per hectare. One single mature tree in good soil will bear more fruit than a typical family can eat in about 3 months.\n\nTamarillos are suitable for growing as indoor container plants, though their swift growth, their light, water and humidity requirements and their large leaves can pose a challenge to those with limited space.\n\nThe tamarillo trees are adaptable and very easy to grow. However, some plant management strategies can help to stabilize and improve plant performance.\n\nPlanting distances depend on the growing system. In New Zealand, with mechanized production, single row planting distances of 1 to 1.5 meters between plants and 4.5 to 5 meters between rows are recommended. In traditional growing regions such as the Andean region, plantations are much more dense, with 1.2 to 1.5 meters between plants. Dense planting can be a strategy to protect plants against wind. On poorly drained soils, plants should be planted on ridges.\n\nPruning can help to control fruit size, plant size, harvest date and to simplify the harvesting of fruits. Cutting the tip of young plants leads to the desired branch height. Once the tree shape has been formed, pruning is reduced to the removal of old or dead wood and previously fruited branches, since branches that have already carried fruits will produce smaller fruits with lower quality the next time. Light pruning leads to medium-sized, heavy pruning to large sized fruits. Basal shoots should be removed. When plants are grown in greenhouses, pruning prevents excessive vegetative growth.\n\nWhen the tree is about 1 to 1.5 metres in height, it is advisable to cut the roots on one side and lean the tree to the other (in the direction of the midday sun at about 30 to 45 degrees). This allows fruiting branches to grow all along the trunk rather than just at the top.\n\nSince the plants are sensitive to drought stress, mulching can help to preserve moisture in the soil. It can also be a strategy to suppress weeds, as other soil management techniques, such as plowing, are not possible due to the shallow and sensitive root system.\n\nThe plants have to be protected from wind. Their shallow root system does not provide enough stability, and the lateral branches are fragile and break easily when carrying fruits.\n\nTo maximize and stabilize production, water and nutrient inputs should be provided when needed. The plants need continuous supply of water due to their shallow root system. Drought stress results in a decrease of plant growth, fruit size and productivity. Recommended fertilizer rates per hectare are 170 kg of nitrogen, 45 kg of phosphorus and 130 to 190 kg of potassium for intensive New Zealand production systems. Phosphorus and potassium are applied in the beginning of the season; nitrogen applications are distributed throughout the year.\n\nThe tamarillo tree is, compared to similar crops such as tomatoes, quite resistant to pests in general. Still, to reduce risk in intensive production systems, some pests have to be controlled to avoid major crop damage. To control pests, the same control methods as for other solanaceae can be used.\nRipening of fruits is not simultaneous. Several harvests are necessary. In climates with little annual variation, tamarillo trees can flower and set fruit throughout the year. In climates with pronounced seasons (such as New Zealand), fruits ripen in autumn. Premature harvest and ethylene induced ripening in controlled-atmosphere chambers is possible with minimal loss of fruit quality. The fragile lateral branches can break easily when loaded with fruits, so premature harvest helps to reduce this risk and allows storage of fruits up to 20 days at room temperature. A cold-water dipping process, developed by the New Zealand Department of Scientific and Industrial Research also allows further storage of 6–10 weeks.\n\nThe fruit is eaten by scooping the flesh from a halved fruit. When lightly sugared and cooled, the flesh is used for a breakfast dish. Some people in New Zealand cut the fruit in half, scoop out the pulpy flesh and spread it on toast at breakfast. Yellow-fruited cultivars have a sweeter flavor, occasionally compared to mango or apricot. The red-fruited variety, which is much more widely cultivated, is more tart, and the savory aftertaste is far more pronounced. In the Northern Hemisphere, tamarillos are most frequently available from July until November, and fruits early in the season tend to be sweeter and less astringent.\n\nThey can be made into compotes, or added to stews (e.g. Boeuf Bourguignon), hollandaise, chutneys and curries. Desserts using this fruit include bavarois and, combined with apples, a strudel.\n\nTamarillos can be added as a secondary fermentation flavouring to Kombucha Tea for a tart and tangy taste. The fruit should be mashed and added at a ratio of 3 Tamarillos to 1 Litre of Kombucha, however great care should be taken to not allow too much carbon dioxide gas to build up in sealed bottles during secondary fermentation. The sugar content of fresh Tamarillos added to Kombucha can generate a rapid carbon dioxide production in secondary fermentation within just 48–72 hours.\n\nIn Colombia, Ecuador, Panama and parts of Indonesia (including Sumatra and Sulawesi), fresh tamarillos are frequently blended together with water and sugar to make a juice. It is also available as a commercially pasteurized purée.\n\nIn Nepal, a version of the South American fruit is decently popular. It is typically consumed as a chutney or a pickle during the autumn and winter months. It is known as \"Tammatar\" and \"Ram Bheda\". Similar to Nepal, the Indian regions of Ooty, Darjeeling and Sikkim also consume Tamarillo.\n\nIn Ecuador, the tamarillo, known as \"tomate de árbol\", is blended with chili peppers to make a hot sauce commonly consumed with local dishes of the Andean region. The sauce is simply referred to as \"aji\" and is present at every meal in Ecuador.\n\nThe flesh of the tamarillo is tangy and variably sweet, with a bold and complex flavor, and may be compared to kiwifruit, tomato, guava, or passion fruit. The skin and the flesh near it have a bitter taste and are not usually eaten raw.\n\nThe tamarillo has been described as having a taste similar to that of a passion fruit and a piquant tomato combined. \n\nThe red and purple types of fruits are preferred in import countries of Europe: Even though they taste more acidic, their color is favoured by consumers.\n\nThe fruits are high in pectin and therefore have good properties for preserves. However, they oxidize and lose color when not treated. Yellow fruit types are better suited to industrial use.\n\nResearch and breeding should improve plantation management, fruit quality and postharvest treatment. A better understanding of plant physiology, nutritional requirements of plants and fruit set mechanisms will help to improve growing systems. Breeding goals are to break seed dormancy, to improve sweetness of fruits and to increase yield. For industrial uses, little \"stones\" of sodium and calcium that occasionally appear in the fruit skin form a problem. Those stones have to be eliminated by breeding.\n\n"}
{"id": "2655103", "url": "https://en.wikipedia.org/wiki?curid=2655103", "title": "Tricalcium phosphate", "text": "Tricalcium phosphate\n\nTricalcium phosphate (sometimes abbreviated TCP) is a calcium salt of phosphoric acid with the chemical formula Ca(PO). It is also known as tribasic calcium phosphate and bone phosphate of lime (BPL). It is a white solid of low solubility. Most commercial samples of \"tricalcium phosphate\" are in fact hydroxyapatite.\n\nIt exists as three crystalline polymorphs α, α', and β. The α and α' states are stable at high temperatures. As mineral, it is found in Whitlockite.\n\n\"Calcium phosphate\" refers to numerous materials consisting of calcium ions (Ca) together with orthophosphates (PO), metaphosphates or pyrophosphates (PO) and occasionally oxide and hydroxide ions. Especially, the common mineral apatite has formula Ca(PO)\"X\", where \"X\" is F, Cl, OH, or a mixture; it is hydroxyapatite if the extra ion is mainly hydroxide. Much of the \"tricalcium phosphate\" on the market is actually powdered hydroxyapatite.\n\nTricalcium phosphate is produced commercially by treating hydroxyapatite with phosphoric acid and slaked lime.\n\nIt cannot be precipitated directly from aqueous solution. Typically double decomposition reactions are employed, involving a soluble phosphate and calcium salts, e.g. (NH)HPO + Ca(NO). is performed under carefully controlled pH conditions. The precipitate will either be \"amorphous tricalcium phosphate\", ATCP, or calcium deficient hydroxyapatite, CDHA, Ca(HPO)(PO)(OH), (note CDHA is sometimes termed apatitic calcium triphosphate). Crystalline tricalcium phosphate can be obtained by calcining the precipitate. β-Ca(PO) is generally formed, higher temperatures are required to produce α-Ca(PO).\n\nAn alternative to the wet procedure entails heating a mixture of a calcium pyrophosphate and calcium carbonate:\n\nTricalcium phosphate has three recognised polymorphs, the rhombohedral β- form (shown above), and two high temperature forms, monoclinic α- and hexagonal α'-. β-tricalcium phosphate has a crystallographic density of 3.066 g cm while the high temperature forms are less dense, α-tricalcium phosphate has a density of 2.866 g cm and α'-tricalcium phosphate has a density of 2.702 g cm All forms have complex structures consisting of tetrahedral phosphate centers linked through oxygen to the calcium ions. The high temperature forms each have two types of columns, one containing only calcium ions and the other both calcium and phosphate.\n\nThere are differences in chemical and biological properties between the beta and alpha forms, the alpha form is more soluble and biodegradeable. Both forms are available commercially and are present in formulations used in medical and dental applications.\n\nCalcium phosphate is one of the main combustion products of bone (see bone ash). Calcium phosphate is also commonly derived from inorganic sources such as mineral rock.\nTricalcium phosphate occurs naturally in several forms, including:\n\nBiphasic tricalcium phosphate, BCP, was originally reported as tricalcium phosphate, but X-Ray diffraction techniques showed that the material was an intimate mixture of two phases, hydroxyapatite (HA) and β-tricalcium phosphate. It is a ceramic.\nPreparation involves the sintering causing the irreversible decomposition of calcium deficient apatites alternatively termed non-stoichiometric apatites or basic calcium phosphate, an example is:\n\nβ-TCP can contain impurities, for example calcium pyrophosphate, CaPO and apatite. β-TCP is bioresorbable. The biodegradation of BCP involves faster dissolution of the β-TCP phase followed by elimination of HA crystals. β-TCP does not dissolve in body fluids at physiological pH levels, dissolution requires cell activity producing acidic pH.\n\nTricalcium phosphate is used in powdered spices as an anticaking agent, e.g. to prevent table salt from caking. It is also found in baby powder and toothpaste.\n\nIt is also used as a nutritional supplement and occurs naturally in cow milk , although the most common and economical forms for supplementation are calcium carbonate (which should be taken with food) and calcium citrate (which can be taken without food). There is some debate about the different bioavailabilities of the different calcium salts.\n\nIt can be used as a tissue replacement for repairing bony defects when autogenous bone graft is not feasible or possible. It may be used alone or in combination with a biodegradable, resorbable polymer such as polyglycolic acid. It may also be combined with autologous materials for a bone graft.\n\nPorous beta-Tricalcium phosphate scaffolds are employed as drug carrier systems for local drug delivery in bone.\n"}
{"id": "45445352", "url": "https://en.wikipedia.org/wiki?curid=45445352", "title": "Triple phase boundary", "text": "Triple phase boundary\n\nA \"triple phase boundary\" (TPB) is a region of contact between three different phases. This concept is particularly important in the field of fuel cells, where the three phases are an electrolyte, an electrode, and a gaseous fuel. The electrochemical reactions that fuel cells use to produce electricity occur in the presence of these three phases, so the triple phase boundaries can be thought of as the active areas of the cell.\n\nThe oxygen reduction reaction that occurs at a solid oxide fuel cell's cathode, can be written as follows:\n\nDifferent mechanisms bring these reactants to a TPB to carry out this reaction. The kinetics of this reaction is one of the limiting factors in cell performance, so increasing the TPB density will increase the reaction rate, and thus increase cell performance. Analogously, TPB density will also influence the kinetics of the oxidation reaction that occurs between oxygen ions and fuel on the anode side of the cell. Transport to and from each TPB will also affect kinetics, so optimization of the pathways to get reactants and products to the active area is also an important consideration. Researchers working with fuel cells are increasingly using 3D imaging techniques like FIB-SEM to measure TPB density as a way of characterizing cell activity. Recently, processing techniques such as infiltration have been shown to substantially increase TPB density, leading to higher efficiency and, potentially, more commercially viable SOFCs.\n"}
{"id": "472972", "url": "https://en.wikipedia.org/wiki?curid=472972", "title": "Vera Rubin", "text": "Vera Rubin\n\nVera Florence Cooper Rubin (; July 23, 1928 – December 25, 2016) was an American astronomer who pioneered work on galaxy rotation rates. She uncovered the discrepancy between the predicted angular motion of galaxies and the observed motion, by studying galactic rotation curves. This phenomenon became known as the galaxy rotation problem, and was evidence of the existence of dark matter. Although initially met with skepticism, Rubin's results were confirmed over subsequent decades. Her legacy was described by \"The New York Times\" as \"ushering in a Copernican-scale change\" in cosmological theory.\n\nBeginning her academic career as the sole undergraduate in astronomy at Vassar College, Rubin went on to graduate studies at Cornell University and Georgetown University, where she observed deviations from Hubble flow in galaxies and provided evidence for the existence of galactic superclusters.\n\nRubin spent her life advocating for women in science and was known for her mentorship of aspiring women astronomers. Her data provided some of the first evidence for dark matter, which had been theorized by Fritz Zwicky in the 1930s. She was honored throughout her career for her achievements, and received the Bruce Medal, the Gold Medal of the Royal Astronomical Society, and the National Medal of Science, among others.\n\nVera Rubin was born Vera Florence Cooper, on July 23, 1928, in Philadelphia, Pennsylvania. She was the younger of two sisters. Her parents were Jewish immigrants: Philip Cooper, a Lithuanian-American electrical engineer who worked at Bell Telephone and Rose Applebaum Cooper, of Bessarabian origin, who worked at Bell until their marriage. Her father was born in Vilnius, Lithuania, as Pesach Kobchefski.\n\nThe Coopers moved to Washington, D.C. in 1938, where 10-year-old Vera developed an interest in astronomy watching the stars from her window. She built a crude telescope out of cardboard with her father, and began to observe and track meteors. She attended Coolidge Senior High School, graduating in 1944.\n\nRubin's older sister, Ruth Cooper Burge, eventually became an administrative law judge in the United States Department of Defense.\n\nRubin was inspired to pursue an undergraduate education at the currently all-women's school, Vassar College, because Maria Mitchell had been a professor there. She also ignored advice she had received from a high school science teacher to avoid a scientific career and become an artist. She graduated Phi Beta Kappa and earned her bachelor's degree in astronomy in 1948, the only graduate in astronomy that year. She attempted to enroll in a graduate program at Princeton, but was barred due to her gender. Princeton would not accept women as astronomy graduate students for 27 more years. Rubin also turned down an offer from Harvard University due to her husband, Robert Rubin's, position as a graduate student at Cornell University.\n\nShe enrolled at Cornell University, and earned a master's degree in 1951. During her graduate studies, she studied the motions of 109 galaxies and made one of the first observations of deviations from Hubble flow (how the galaxies move apart from one another). She worked with astronomer Martha Carpenter on galactic dynamics, and studied under Philip Morrison, Hans Bethe, and Richard Feynman. Though the conclusion she came to – that there was an orbital motion of galaxies around a particular pole – was disproven, the idea that galaxies were moving held true and sparked further research. Her research also provided early evidence of the supergalactic plane. This information and the data she discovered was immensely controversial, and after fighting to be allowed to present her work at the American Astronomical Society despite being pregnant, she was summarily rejected and the paper forgotten.\n\nRubin studied for her Ph.D. at Georgetown University, the only university in Washington, DC that offered a graduate degree in astronomy. She was 23 years old and pregnant when she began her doctoral studies, and the Rubins had one young child at home. She began to take classes with Francis Heyden, who recommended her to George Gamow, her eventual doctoral advisor. Her dissertation, completed in 1954, concluded that galaxies clumped together, rather than being randomly distributed through the universe, a controversial idea not pursued by others for two decades. Throughout her graduate studies, she experienced discouraging sexism, including an incident where she was not allowed to meet with her advisor in his office, because women were not allowed in that area of the university.\n\nRubin held various academic appointments for the next eleven years. She served for a year as an Instructor of Mathematics and Physics at Montgomery County Community College, then worked from 1955-1965 at Georgetown University, as a Research Associate Astronomer, Lecturer (1959-1962), and finally, Assistant Professor of Astronomy (1962-1965). She joined the Carnegie Institute in 1965, as a staff member in the Department of Terrestrial Magnetism, where she met her long-time collaborator, instrument-maker Kent Ford. Because she had young children, she did much of her work from home.\n\nIn 1963, Rubin began a year-long collaboration with Geoffrey and Margaret Burbidge, during which made her first observations of the rotation of galaxies at the McDonald Observatory's 82 inch telescope. During her work at the Carnegie Institute, Rubin applied to observe at the Palomar Observatory in 1965, despite the fact that the building did not have facilities for women. She created her own women's restroom, sidestepping the lack of facilities available for her and becoming the first female astronomer to observe there.\n\nAt the Carnegie Institution, Rubin began work related to her controversial thesis regarding galaxy clusters with Ford, making hundreds of observations using Ford's image-tube spectrograph. This instrument allowed Rubin to amplify starlight seen through the telescope so they could view astronomical objects that were previously too dim to see. The Rubin–Ford effect, an apparent anisotropy in the expansion of the Universe on the scale of 100 million light years, was discovered through studies of spiral galaxies, particularly the Andromeda Galaxy, chosen for its brightness and proximity to Earth. First appearing in journals in 1976, the idea of peculiar motion on this scale in the universe was a highly controversial proposition, dismissed by leading astronomers but ultimately shown to be valid. The effect is now known as large scale streaming. The pair also briefly studied quasars, which had been discovered in 1963 and were a popular topic of research.\n\nWishing to avoid controversial areas of astronomy, including quasars and galactic motion, Rubin began to study the rotation and outer reaches of galaxies, an interest sparked by her collaboration with the Burbidges. She investigated the rotation curves of spiral galaxies, again beginning with Andromeda, by looking at their outermost material, and observed flat rotation curves: the outermost components of the galaxy were moving as quickly as those close to the center. This was an early indication that spiral galaxies were surrounded by dark matter haloes. She further uncovered the discrepancy between the predicted angular motion of galaxies based on the visible light and the observed motion. Her research showed that spiral galaxies rotate quickly enough that they should fly apart, if the gravity of their constituent stars was all that was holding them together; because they stay intact, a large amount of unseen mass must be holding them together, a conundrum that became known as the galaxy rotation problem.\n\nRubin's calculations showed that galaxies must contain at least five to ten times as much dark matter as ordinary matter. Rubin's results were confirmed over subsequent decades, and became the first persuasive results supporting the theory of dark matter, initially proposed by Fritz Zwicky in the 1930s. This data was confirmed by radio astronomers, the discovery of the cosmic microwave background, and images of gravitational lensing. Her research also prompted a theory of non-Newtonian gravity on galactic scales, but this theory has not been widely accepted by astrophysicists.\n\nAnother area of interest for Rubin was the phenomenon of counter-rotation in galaxies. Her discovery that some gas and stars moved in the opposite direction to the rotation of the rest of the galaxy challenged the prevailing theory that all of the material in a galaxy moved in the same direction, and provided the first evidence for galaxy mergers and the process by which galaxies initially formed.\n\nRubin's perspective on the history of the work on galaxy movements was presented in a review, \"One Hundred Years of Rotating Galaxies,\" for the \"Publications of the Astronomical Society of the Pacific\" in 2000. This was an adaptation of the lecture she gave in 1996 upon receiving the Gold Medal of the Royal Astronomical Society, the second woman to be so honored, 168 years after Caroline Herschel received the Medal in 1828. She continued her research and mentorship until her death in 2016.\n\nWhen Rubin was elected to the National Academy of Science, she became the second woman astronomer in its ranks, after her colleague Margaret Burbidge. Rubin never won the Nobel Prize, though physicists such as Lisa Randall and Emily Levesque have argued that this was an oversight. She was described by Sandra Faber and Neta Bahcall as one of the astronomers who paved the way for other women in the field, as a \"guiding light\" for those who wished to have families and careers in astronomy. Rebecca Oppenheimer also recalled Rubin's mentorship as important to her early career.\n\nRubin died on the night of December 25, 2016 of complications associated with dementia. The president of the Carnegie Institution, where she performed the bulk of her work and research, called her a \"national treasure.\"\n\nThe Carnegie Institute has created a postdoctoral research fund in Rubin's honor, and the American Astronomical Society has named its Division of Dynamical Astronomy early career award after Rubin.\n\nRubin was featured in an animated segment of the 13th and final episode of \"\". An area on Mars, Vera Rubin Ridge, is named after her and Asteroid 5726 Rubin was named in her honor.\n\n\nFrom 1948 until his death in 2008, she was married to Robert Rubin. Rubin became a mother during her graduate studies at Cornell, and continued to work on her research while raising her young children. All four of her children earned Ph.D.s in the natural sciences or mathematics: David (born 1950), a geologist with the U.S. Geological Survey; Judith Young (1952–2014), an astronomer at the University of Massachusetts; Karl (born 1956), a mathematician at the University of California at Irvine; and Allan (born 1960), a geologist at Princeton University. Her children recalled later in life that their mother made a life of science appear desirable and fun, which motivated them to become scientists themselves.\n\nMotivated by her own battle to gain credibility as a woman in a field dominated by male astronomers, Rubin encouraged girls interested in investigating the universe to pursue their dreams. She faced discouraging comments on her choice of study throughout her life, but persevered, supported by family and colleagues. In addition to astronomy, Rubin was a force for greater recognition of women in the sciences and for scientific literacy. She and Burbidge advocated together for more women in the National Academy of Sciences (NAS), on review panels, and in academic searches. She said that despite her struggles with the NAS, she continued to be dissatisfied with the number of women who are elected each year, and called it \"the saddest part of [her] life\". Rubin was Jewish, and saw no conflict between science and religion. In an interview, she stated: \"In my own life, my science and my religion are separate. I'm Jewish, and so religion to me is a kind of moral code and a kind of history. I try to do my science in a moral way, and, I believe that, ideally, science should be looked upon as something that helps us understand our role in the universe.\"\n\n\nThe following are a small selection of articles selected by the scientists and historians of the \"CWP\" project, as being representative of her most important writings; Rubin published over 150 scientific papers.\n\n"}
{"id": "7091330", "url": "https://en.wikipedia.org/wiki?curid=7091330", "title": "Water-fuelled car", "text": "Water-fuelled car\n\nA water-fuelled car is an automobile that hypothetically derives its energy directly from water. Water-fuelled cars have been the subject of numerous international patents, newspaper and popular science magazine articles, local television news coverage, and websites. The claims for these devices have been found to be pseudoscience and some were found to be tied to investment frauds. These vehicles may be claimed to produce fuel from water on board with no other energy input, or may be a hybrid claiming to derive some of its energy from water in addition to a conventional source (such as gasoline).\n\nWater is fully oxidized hydrogen. Hydrogen itself is a high-energy, flammable substance, but its useful energy is released when water is formed. Water will not burn. The process of electrolysis can split water into hydrogen and oxygen, but it takes as much energy to take apart a water molecule as was released when the hydrogen was oxidized to form water. In fact, some energy would be lost in converting water to hydrogen and then burning the hydrogen because some waste heat would always be produced in the conversions. Releasing chemical energy from water, in excess or in equal proportion to the energy required to facilitate such production, would therefore violate the first or second law of thermodynamics.\n\nA water-fuelled car is not any of the following:\n\nAccording to the currently accepted laws of physics, there is no way to extract chemical energy from water alone. Water itself is highly stable—it was one of the classical elements and contains very strong chemical bonds. Its enthalpy of formation is negative (-68.3 kcal/mol or -285.8 kJ/mol), meaning that energy is required to break those stable bonds, to separate water into its elements, and there are no other compounds of hydrogen and oxygen with more negative enthalpies of formation, meaning that no energy can be released in this manner either.\n\nMost proposed water-fuelled cars rely on some form of electrolysis to separate water into hydrogen and oxygen and then recombine them to release energy; however, because the energy required to separate the elements will always be at least as great as the useful energy released, this cannot be used to produce net energy.\n\nCharles H. Garrett allegedly demonstrated a water-fuelled car \"for several minutes\", which was reported on September 8, 1935, in The Dallas Morning News. The car generated hydrogen by electrolysis as can be seen by examining Garrett's patent, issued that same year. This patent includes drawings which show a carburetor similar to an ordinary float-type carburetor but with electrolysis plates in the lower portion, and where the float is used to maintain the level of the water. Garrett's patent fails to identify a new source of energy.\n\nAt least as far back as 1980, Stanley Meyer claimed that he had built a dune buggy that ran on water, although he gave inconsistent explanations as to its mode of operation. In some cases, he claimed that he had replaced the spark plugs with a \"water splitter\", while in other cases it was claimed to rely on a \"fuel cell\" that split the water into hydrogen and oxygen. The \"fuel cell\", which he claimed was subjected to an electrical resonance, would split the water mist into hydrogen and oxygen gas, which would then be combusted back into water vapour in a conventional internal combustion engine to produce net energy. Meyer's claims were never independently verified, and in an Ohio court in 1996 he was found guilty of \"gross and egregious fraud\". He died of an aneurysm in 1998, although conspiracy theories claim that he was poisoned.\n\nIn 2002, the firm Hydrogen Technology Applications patented an electrolyser design and trademarked the term \"Aquygen\" to refer to the hydrogen oxygen gas mixture produced by the device. Originally developed as an alternative to oxyacetylene welding, the company claimed to be able to run a vehicle exclusively on water, via the production of \"Aquygen\", and invoked an unproven state of matter called \"magnegases\" and a discredited theory about magnecules to explain their results. Company founder Dennis Klein claimed to be in negotiations with a major US auto manufacturer and that the US government wanted to produce Hummers that used his technology.\n\nAt present, the company no longer claims it can run a car exclusively on water, and is instead marketing \"Aquygen\" production as a technique to increase fuel efficiency, thus making it Hydrogen fuel enhancement rather than a water-fuelled car.\n\nAlso in 2002, Genesis World Energy announced a market ready device which would extract energy from water by separating the hydrogen and oxygen and then recombining them. In 2003, the company announced that this technology had been adapted to power automobiles. The company collected over $2.5 million from investors, but none of their devices were ever brought to market. In 2006, Patrick Kelly, the owner of Genesis World Energy was sentenced in New Jersey to five years in prison for theft and ordered to pay $400,000 in restitution.\n\nIn June 2008, Japanese company Genepax unveiled a car which it claims runs on only water and air, and many news outlets dubbed the vehicle a \"water-fuel car\". The company says it \"cannot [reveal] the core part of this invention,” yet, but it has disclosed that the system uses an onboard energy generator (a \"membrane electrode assembly\") to extract the hydrogen using a \"mechanism which is similar to the method in which hydrogen is produced by a reaction of metal hydride and water\". The hydrogen is then used to generate energy to run the car. This has led to speculation that the metal hydride is consumed in the process and is the ultimate source of the car's energy, making the car a hydride-fuelled \"hydrogen on demand\" vehicle, rather than water-fuelled as claimed. On the company's website the energy source is explained only with the words \"Chemical reaction\". The science and technology magazine Popular Mechanics has described Genepax's claims as \"Rubbish.\" The vehicle that Genepax demonstrated to the press in 2008 was a REVAi electric car, manufactured in India and sold in the UK as the G-Wiz.\n\nIn early 2009, Genepax announced they were closing their website, citing large development costs.\n\nAlso in 2008, Sri Lankan news sources reported that Thushara Priyamal Edirisinghe claimed to drive a water-fuelled car about 300 kilometers on three litres of water. Like other alleged water-fuelled cars described above, energy for the car is supposedly produced by splitting water into hydrogen and oxygen using electrolysis, and then burning the gases in the engine. Thushara showed the technology to Prime Minister Ratnasiri Wickramanayaka, who \"extended the Government’s full support to his efforts to introduce the water-powered car to the Sri Lankan market.\"\n\nThushara was arrested a few months later on suspicion of investment fraud.\n\nDaniel Dingel, a Filipino inventor, has been claiming since 1969 to have developed technology allowing water to be used as fuel. In 2000, Dingel entered into a business partnership with Formosa Plastics Group to further develop the technology. In 2008, Formosa Plastics successfully sued Dingel for fraud, with the 82-year-old Dingel being sentenced to 20 years imprisonment.\n\nIn December 2011 a Pakistani doctor, Ghulam Sarwar claimed that he had invented a car that only runs on water. At the time the invented car was claimed to use 60% water and 40% Diesel or fuel, but that the inventor was working hard to make it run on only water, probably by end of June 2012. It was further claimed that the car \"emits only oxygen rather than the usual carbon\".\n\nAgha Waqar Ahmad, a Pakistani, claimed in July 2012 to have invented water-fuelled car by installing a \"water kit\" for all kind of automobiles. The kit consists of a cylindrical jar, which holds the water, a bubbler, and a pipe leading to the engine. He claims that the kit uses electrolysis to convert water into \"HHO\", which is then used as fuel. The kit requires use of distilled water to work. Ahmed claims that he has been able to achieve much higher amounts of oxyhydrogen compared to any other inventor because of \"undisclosed calculations\". He has applied for a patent in Pakistan. Some Pakistani scientists alleged that Agha's invention is nothing but a fraud as it violates the laws of thermodynamics.\n\nIn addition to claims of cars that run exclusively on water, there have also been claims that burning hydrogen or oxyhydrogen in addition to petrol or diesel fuel increases mileage. Whether such hydrogen on demand systems actually improve emissions or fuel efficiency is debated.\n\nA number of websites exist promoting the use of oxyhydrogen (which they often refer to as \"HHO\"), selling plans for do-it-yourself electrolysers or entire kits with the promise of large improvements in fuel efficiency. According to a spokesman for the American Automobile Association, \"All of these devices look like they could probably work for you, but let me tell you they don't.\"\n\nRelated to the water-fuelled car hoax are claims that additives, often a pill, convert the water into usable fuel, similar to a carbide lamp, in which a high-energy additive produces the combustible fuel. This \"gasoline pill\" has been allegedly demonstrated on a full-sized vehicle, as reported in 1980 in \"Mother Earth News\". Once again, water itself cannot contribute any energy to the process; the additive or the pill is the fuel.\n\nA hydrogen on demand vehicle uses a chemical reaction to produce hydrogen from water. The hydrogen is then burned in an internal combustion engine or used in a fuel cell to generate electricity which powers the vehicle. While these may seem at first sight to be 'water-fuelled cars', they actually take their energy from the chemical that reacts with water, and vehicles of this type are not precluded by the laws of nature. Aluminium, magnesium, and sodium borohydride are substances that react with water to generate hydrogen, and all have been used in hydrogen on demand prototypes. Eventually, the chemical runs out and has to be replenished. In all cases the energy required to produce such compounds exceeds the energy obtained from their reaction with water.\n\nOne example of a hydrogen on demand device, created by scientists from the University of Minnesota and the Weizmann Institute of Science, uses boron to generate hydrogen from water. An article in \"New Scientist\" in July 2006 described the power source under the headline \"A fuel tank full of water,\" and they quote Abu-Hamed as saying:\n\nA vehicle powered by the device would take on water and boron instead of petrol, and generate boron trioxide. Elemental boron is difficult to prepare and does not occur naturally. Boron trioxide is an example of a borate, which is the predominant form of boron on earth. Thus, a boron-powered vehicle would require an economical method of preparing elemental boron. The chemical reactions describing the oxidation of boron are:\n\nThe balanced chemical equation representing the overall process (hydrogen generation and combustion) is:\n\nAs shown above, boron trioxide is the only net byproduct, and it could be removed from the car and turned back into boron and reused. Electricity input is required to complete this process, which Al-Hamed suggests could come from solar panels. Although it is possible to obtain elemental boron by electrolysis, a substantial expenditure of energy is required. The process of converting borates to elemental boron and back might be compared with the analogous process involving carbon: carbon dioxide could be converted to charcoal (elemental carbon), then burnt to produce carbon dioxide.\n\nIt is referred to in the pilot episode for the That '70s Show sitcom, as well as in the twenty-first episode of the fifth season.\n\n\"Gashole\" (2010), a documentary film about the history of oil prices and the future of alternative mentions multiple stories regarding engines that use water to increase mileage efficiency.\n\n\"Like Water for Octane,\" an episode of \"The Lone Gunmen\", is based on a \"water-powered\" car that character Melvin Frohike saw with his own eyes back in 1962.\n\n\"The Water Engine\", a David Mamet play made into a television film in 1994, tells the story of Charles Lang inventing an engine that runs using water for fuel. The plot centers on the many obstacles the inventor must overcome to patent his device.\n\nThe plot of the 1996 action film \"Chain Reaction\" revolves around a technology to turn water (via a type of self-sustaining bubble fusion & electrolysis) into fuel and official suppression of it.\n\nA water-powered car was depicted in a 1997 episode of Team Knight Rider (a spinoff of the original Knight Rider TV series) entitled \"Oil and Water\". In the episode, the vehicle explodes after a character sabotages it by putting seltzer tablets in the fuel tank. The car shown was actually a Bricklin SV-1.\n\nThe plot of the 2017 novel \"Paradox Bound\" by Peter Clines revolves around a group of time travelers, the majority of whom drive cars modified in the 2020s to use Garrett carburetors.\n\n"}
{"id": "18027908", "url": "https://en.wikipedia.org/wiki?curid=18027908", "title": "We Feed the World", "text": "We Feed the World\n\nWe Feed the World is a 2005 documentary in which Austrian filmmaker Erwin Wagenhofer traces the origins of the food we eat and views modern industrial production of food and factory farming in a critical light. His journey takes him to France, Spain, Romania, Switzerland, Brazil and back to Austria.\n\nThe film features interviews with several people, including one with sociologist and politician Jean Ziegler.\n\nThe film was the most successful Austrian documentary ever. In German-speaking countries it was seen by about 600,000 cinemagoers.\n\n"}
{"id": "7922286", "url": "https://en.wikipedia.org/wiki?curid=7922286", "title": "William Merriam Burton", "text": "William Merriam Burton\n\nWilliam Merriam Burton (November 17, 1865 – December 29, 1954) was an American chemist who developed the first thermal cracking process for crude oil.\n\nBurton was born in Cleveland, Ohio. In 1886, he received a Bachelor of Science degree at Western Reserve University. He earned a PhD at Johns Hopkins University in 1889.\n\nBurton initially worked for the Standard Oil refinery at Whiting, Indiana. He became president of Standard Oil from 1918 to 1927, when he retired.\n\nThe process of thermal cracking invented by Burton, which became on January 7, 1913, doubled the yield of gasoline that can be extracted from crude oil.\n\nThe first thermal cracking method, the Shukhov cracking process, was invented by Russian engineer Vladimir Shukhov (1853-1939), in the Russian empire, Patent No. 12926, November 27, 1891.\nBurton died in Miami, Florida.\n\n\n"}
{"id": "34240", "url": "https://en.wikipedia.org/wiki?curid=34240", "title": "Ytterbium", "text": "Ytterbium\n\nYtterbium is a chemical element with symbol Yb and atomic number 70. It is the fourteenth and penultimate element in the lanthanide series, which is the basis of the relative stability of its -6 oxidation state. However, like the other lanthanides, its most common oxidation state is +3, as in its oxide, halides, and other compounds. In aqueous solution, like compounds of other late lanthanides, soluble ytterbium compounds form complexes with nine water molecules. Because of its closed-shell electron configuration, its density and melting and boiling points differ significantly from those of most other lanthanides.\n\nIn 1878, the Swiss chemist Jean Charles Galissard de Marignac separated from the rare earth \"erbia\" another independent component, which he called \"ytterbia\", for Ytterby, the village in Sweden near where he found the new component of erbium. He suspected that ytterbia was a compound of a new element that he called \"ytterbium\" (in total, four elements were named after the village, the others being yttrium, terbium and erbium). In 1907, the new earth \"lutecia\" was separated from ytterbia, from which the element \"lutecium\" (now lutetium) was extracted by Georges Urbain, Carl Auer von Welsbach, and Charles James. After some discussion, Marignac's name \"ytterbium\" was retained. A relatively pure sample of the metal was not obtained until 1953. At present, ytterbium is mainly used as a dopant of stainless steel or active laser media, and less often as a gamma ray source.\n\nNatural ytterbium is a mixture of seven stable isotopes, which altogether are present at concentrations of 3 parts per million. This element is mined in China, the United States, Brazil, and India in form of the minerals monazite, euxenite, and xenotime. The ytterbium concentration is low because it is found only among many other rare earth elements; moreover, it is among the least abundant. Once extracted and prepared, ytterbium is somewhat hazardous as an eye and skin irritant. The metal is a fire and explosion hazard.\n\nYtterbium is a soft, malleable and ductile chemical element that displays a bright silvery luster when pure. It is a rare earth element, and it is readily dissolved by the strong mineral acids. It reacts slowly with cold water and it oxidizes slowly in air.\n\nYtterbium has three allotropes labeled by the Greek letters alpha, beta and gamma; their transformation temperatures are −13 °C and 795 °C, although the exact transformation temperature depends on the pressure and stress. The beta allotrope (6.966 g/cm) exists at room temperature, and it has a face-centered cubic crystal structure. The high-temperature gamma allotrope (6.57 g/cm) has a body-centered cubic crystalline structure. The alpha allotrope (6.903 g/cm) has a hexagonal crystalline structure and is stable at low temperatures. The beta allotrope has a metallic electrical conductivity at normal atmospheric pressure, but it becomes a semiconductor when exposed to a pressure of about 16,000 atmospheres (1.6 GPa). Its electrical resistivity increases ten times upon compression to 39,000 atmospheres (3.9 GPa), but then drops to about 10% of its room-temperature resistivity at about 40,000 atm (4.0 GPa).\n\nIn contrast with the other rare-earth metals, which usually have antiferromagnetic and/or ferromagnetic properties at low temperatures, ytterbium is paramagnetic at temperatures above 1.0 kelvin. However, the alpha allotrope is diamagnetic. With a melting point of 824 °C and a boiling point of 1196 °C, ytterbium has the smallest liquid range of all the metals.\n\nContrary to most other lanthanides, which have a close-packed hexagonal lattice, ytterbium crystallizes in the face-centered cubic system. Ytterbium has a density of 6.973 g/cm, which is significantly lower than those of the neighboring lanthanides, thulium (9.32 g/cm) and lutetium (9.841 g/cm). Its melting and boiling points are also significantly lower than those of thulium and lutetium. This is due to the closed-shell electron configuration of ytterbium ([Xe] 4f 6s), which causes only the two 6s electrons to be available for metallic bonding (in contrast to the other lanthanides where three electrons are available) and increases ytterbium's metallic radius.\n\nYtterbium metal tarnishes slowly in air. Finely dispersed ytterbium readily oxidizes in air and under oxygen. Mixtures of powdered ytterbium with polytetrafluoroethylene or hexachloroethane burn with a luminous emerald-green flame. Ytterbium reacts with hydrogen to form various non-stoichiometric hydrides. Ytterbium dissolves slowly in water, but quickly in acids, liberating hydrogen gas.\n\nYtterbium is quite electropositive, and it reacts slowly with cold water and quite quickly with hot water to form ytterbium(III) hydroxide:\n\nYtterbium reacts with all the halogens:\n\nThe ytterbium(III) ion absorbs light in the near infrared range of wavelengths, but not in visible light, so ytterbia, YbO, is white in color and the salts of ytterbium are also colorless. Ytterbium dissolves readily in dilute sulfuric acid to form solutions that contain the colorless Yb(III) ions, which exist as nonahydrate complexes:\n\nAlthough usually trivalent, ytterbium readily forms divalent compounds. This behavior is unusual for lanthanides, which almost exclusively form compounds with an oxidation state of +3. The +2 state has a valence electron configuration of 4\"f\" because the fully filled \"f\"-shell gives more stability. The yellow-green ytterbium(II) ion is a very strong reducing agent and decomposes water, releasing hydrogen gas, and thus only the colorless ytterbium(III) ion occurs in aqueous solution. Samarium and thulium also behave this way in the +2 state, but europium(II) is stable in aqueous solution. Ytterbium metal behaves similarly to europium metal and the alkaline earth metals, dissolving in ammonia to form blue electride salts.\n\nNatural ytterbium is composed of seven stable isotopes: Yb, Yb, Yb, Yb, Yb, Yb, and Yb, with Yb being the most common, at 31.8% of the natural abundance). 27 radioisotopes have been observed, with the most stable ones being Yb with a half-life of 32.0 days, Yb with a half-life of 4.18 days, and Yb with a half-life of 56.7 hours. All of its remaining radioactive isotopes have half-lives that are less than two hours and most of these have half-lives are less than 20 minutes. Ytterbium also has 12 meta states, with the most stable being Yb (\"t\" 46 seconds).\n\nThe isotopes of ytterbium range in atomic weight from 147.9674 atomic mass unit (u) for Yb to 180.9562 u for Yb. The primary decay mode of ytterbium isotopes lighter than the most abundant stable isotope, Yb, is electron capture, and the primary decay mode for those heavier than Yb is beta decay. The primary decay products of ytterbium isotopes lighter than Yb are thulium isotopes, and the primary decay products of ytterbium isotopes with heavier than Yb are lutetium isotopes.\n\nYtterbium is found with other rare earth elements in several rare minerals. It is most often recovered commercially from monazite sand (0.03% ytterbium). The element is also found in euxenite and xenotime. The main mining areas are China, the United States, Brazil, India, Sri Lanka, and Australia. Reserves of ytterbium are estimated as one million tonnes. Ytterbium is normally difficult to separate from other rare earths, but ion-exchange and solvent extraction techniques developed in the mid- to late 20th century have simplified separation. Compounds of ytterbium are rare and have not yet been well characterized. The abundance of ytterbium in the Earth's crust is about 3 mg/kg.\n\nAs an even-numbered lanthanide, in accordance with the Oddo-Harkins rule, ytterbium is significantly more abundant than its immediate neighbors, thulium and lutetium, which occur in the same concentrate at levels of about 0.5% each. The world production of ytterbium is only about 50 tonnes per year, reflecting that it has few commercial applications. Microscopic traces of ytterbium are used as a dopant in the , a solid-state laser in which ytterbium is the element that undergoes stimulated emission of electromagnetic radiation.\n\nYtterbium is often the most common substitute in yttrium minerals. In very few known cases/occurrences ytterbium prevails over yttrium, as, e.g., in xenotime-(Yb). A report of native ytterbium from the Moon's regolith is known.\n\nIt is relatively difficult to separate ytterbium from other lanthanides due to its similar properties. As a result, the process is somewhat long. First, minerals such as monazite or xenotime are dissolved into various acids, such as sulfuric acid. Ytterbium can then be separated from other lanthanides by ion exchange, as can other lanthanides. The solution is then applied to a resin, which different lanthanides bind in different matters. This is then dissolved using complexing agents, and due to the different types of bonding exhibited by the different lanthanides, it is possible to isolate the compounds.\n\nYtterbium is separated from other rare earths either by ion exchange or by reduction with sodium amalgam. In the latter method, a buffered acidic solution of trivalent rare earths is treated with molten sodium-mercury alloy, which reduces and dissolves Yb. The alloy is treated with hydrochloric acid. The metal is extracted from the solution as oxalate and converted to oxide by heating. The oxide is reduced to metal by heating with lanthanum, aluminium, cerium or zirconium in high vacuum. The metal is purified by sublimation and collected over a condensed plate.\n\nThe chemical behavior of ytterbium is similar to that of the rest of the lanthanides. Most ytterbium compounds are found in the +3 oxidation state, and its salts in this oxidation state are nearly colorless. Like europium, samarium, and thulium, the trihalides of ytterbium can be reduced to the dihalides by hydrogen, zinc dust, or by the addition of metallic ytterbium. The +2 oxidation state occurs only in solid compounds and reacts in some ways similarly to the alkaline earth metal compounds; for example, ytterbium(II) oxide (YbO) shows the same structure as calcium oxide (CaO).\n\nYtterbium forms both dihalides and trihalides with the halogens fluorine, chlorine, bromine, and iodine. The dihalides are susceptible to oxidation to the trihalides at room temperature and disproportionate to the trihalides and metallic ytterbium at high temperature:\n\nSome ytterbium halides are used as reagents in organic synthesis. For example, ytterbium(III) chloride (YbCl) is a Lewis acid and can be used as a catalyst in the Aldol and Diels–Alder reactions. Ytterbium(II) iodide (YbI) may be used, like samarium(II) iodide, as a reducing agent for coupling reactions. Ytterbium(III) fluoride (YbF) is used as an inert and non-toxic tooth filling as it continuously releases fluoride ions, which are good for dental health, and is also a good X-ray contrast agent.\n\nYtterbium reacts with oxygen to form ytterbium(III) oxide (YbO), which crystallizes in the \"rare-earth C-type sesquioxide\" structure which is related to the fluorite structure with one quarter of the anions removed, leading to ytterbium atoms in two different six coordinate (non-octahedral) environments. Ytterbium(III) oxide can be reduced to ytterbium(II) oxide (YbO) with elemental ytterbium, which crystallizes in the same structure as sodium chloride.\n\nYtterbium was discovered by the Swiss chemist Jean Charles Galissard de Marignac in the year 1878. While examining samples of gadolinite, Marignac found a new component in the earth then known as erbia, and he named it ytterbia, for Ytterby, the Swedish village near where he found the new component of erbium. Marignac suspected that ytterbia was a compound of a new element that he called \"ytterbium\".\n\nIn 1907, the French chemist Georges Urbain separated Marignac's ytterbia into two components: neoytterbia and lutecia. Neoytterbia later became known as the element ytterbium, and lutecia became known as the element lutetium. The Austrian chemist Carl Auer von Welsbach independently isolated these elements from ytterbia at about the same time, but he called them aldebaranium and cassiopeium; the American chemist Charles James also independently isolated these elements at about the same time. Urbain and Welsbach accused each other of publishing results based on the other party. The Commission on Atomic Mass, consisting of Frank Wigglesworth Clarke, Wilhelm Ostwald, and Georges Urbain, which was then responsible for the attribution of new element names, settled the dispute in 1909 by granting priority to Urbain and adopting his names as official ones, based on the fact that the separation of lutetium from Marignac's ytterbium was first described by Urbain. After Urbain's names were recognized, neoytterbium was reverted to ytterbium.\n\nThe chemical and physical properties of ytterbium could not be determined with any precision until 1953, when the first nearly pure ytterbium metal was produced by using ion-exchange processes. The price of ytterbium was relatively stable between 1953 and 1998 at about US $1,000/kg.\n\nThe Yb isotope (with a half-life of 32 days), which is created along with the short-lived Yb isotope (half-life 4.2 days) by neutron activation during the irradiation of ytterbium in nuclear reactors, has been used as a radiation source in portable X-ray machines. Like X-rays, the gamma rays emitted by the source pass through soft tissues of the body, but are blocked by bones and other dense materials. Thus, small Yb samples (which emit gamma rays) act like tiny X-ray machines useful for radiography of small objects. Experiments show that radiographs taken with a Yb source are roughly equivalent to those taken with X-rays having energies between 250 and 350 keV. Yb is also used in nuclear medicine.\n\nYtterbium clocks hold the record for stability with ticks stable to within less than two parts in 1 quintillion (). The clocks developed at the National Institute of Standards and Technology (NIST) rely on about 10,000 rare-earth atoms cooled to 10 microkelvin (10 millionths of a degree above absolute zero) and trapped in an optical lattice—a series of pancake-shaped wells made of laser light. Another laser that \"ticks\" 518 trillion times per second provokes a transition between two energy levels in the atoms. The large number of atoms is key to the clocks' high stability.\n\nVisible light waves oscillate faster than microwaves, and therefore optical clocks can be more precise than caesium atomic clocks. The Physikalisch-Technische Bundesanstalt is working on several such optical clocks. The model with one single ytterbium ion caught in an ion trap is highly accurate. The optical clock based on it is exact to 17 digits after the decimal point.\nA pair of experimental atomic clocks based on ytterbium atoms at the National Institute of Standards and Technology has set a record for stability. NIST physicists reported in the August 22, 2013 issue of Science Express that the ytterbium clocks' ticks are stable to within less than two parts in 1 quintillion (1 followed by 18 zeros), roughly 10 times better than the previous best published results for other atomic clocks. The clocks would be accurate within a second for a period comparable to the age of the universe.\n\nYtterbium can also be used as a dopant to help improve the grain refinement, strength, and other mechanical properties of stainless steel. Some ytterbium alloys have rarely been used in dentistry.\n\nThe ytterbium +3 ion is used as a doping material in active laser media, specifically in solid state lasers and double clad fiber lasers. Ytterbium lasers are highly efficient, have long lifetimes and can generate short pulses; ytterbium can also easily be incorporated into the material used to make the laser. Ytterbium lasers commonly radiate in the 1.06–1.12 µm band being optically pumped at wavelength 900 nm–1 µm, dependently on the host and application. The small quantum defect makes ytterbium a prospective dopant for efficient lasers and power scaling.\n\nThe kinetic of excitations in ytterbium-doped materials is simple and can be described within the concept of effective cross-sections; for most ytterbium-doped laser materials (as for many other optically pumped gain media), the McCumber relation holds, although the application to the ytterbium-doped composite materials was under discussion.\n\nUsually, low concentrations of ytterbium are used. At high concentrations, the ytterbium-doped materials show photodarkening\n(glass fibers) or even a switch to broadband emission (crystals and ceramics) instead of efficient laser action. This effect may be related with not only overheating, but also with conditions of charge compensation at high concentrations of ytterbium ions.\n\nMuch progress has been made in the power scaling lasers and amplifiers produced with ytterbium (Yb) doped optical fibers. Power levels have increased from the 1 kW regimes due to the advancements in components as well as the Yb-doped fibers. Fabrication of Low NA, Large Mode Area fibers enable achievement of near perfect beam qualities (M2<1.1) at power levels of 1.5 kW to greater than 2 kW at ~1064 nm in a broadband configuration. Ytterbium-doped LMA fibers also have the advantages of a larger mode field diameter, which negates the impacts of nonlinear effects such as stimulated Brillouin scattering and stimulated Raman scattering, which limit the achievement of higher power levels, and provide a distinct advantage over single mode ytterbium-doped fibers.\n\nIn order to achieve even higher power levels in ytterbium-based fiber systems. all factors of the fiber must be considered. These can be achieved only via optimization of all the ytterbium fiber parameters, ranging from the core background losses to the geometrical properties, in order to reduce the splice losses within the cavity. Power scaling also requires optimization of matching passive fibers within the optical cavity. The optimization of the ytterbium-doped glass itself through host glass modification of various dopants also plays a large part in reducing the background loss of the glass, improvements in slope efficiency of the fiber, and improved photodarkening performance, all of which contribute to increased power levels in 1 µm systems.\n\nYtterbium metal increases its electrical resistivity when subjected to high stresses. This property is used in stress gauges to monitor ground deformations from earthquakes and explosions.\n\nCurrently, ytterbium is being investigated as a possible replacement for magnesium in high density pyrotechnic payloads for kinematic infrared decoy flares. As ytterbium(III) oxide has a significantly higher emissivity in the infrared range than magnesium oxide, a higher radiant intensity is obtained with ytterbium-based payloads in comparison to those commonly based on magnesium/Teflon/Viton (MTV).\n\nAlthough ytterbium is fairly stable chemically, it is stored in airtight containers and in an inert atmosphere such as a nitrogen-filled dry box to protect it from air and moisture. All compounds of ytterbium are treated as highly toxic, although studies appear to indicate that the danger is minimal. However, ytterbium compounds cause irritation to human skin and eyes, and some might be teratogenic. Metallic ytterbium dust can spontaneously combust, and the resulting fumes are hazardous. Ytterbium fires cannot be extinguished using water, and only dry chemical class D fire extinguishers can extinguish the fires.\n\n\n"}
{"id": "9065728", "url": "https://en.wikipedia.org/wiki?curid=9065728", "title": "Yttrium lithium fluoride", "text": "Yttrium lithium fluoride\n\nYttrium lithium fluoride (LiYF, sometimes abbreviated YLF) is a birefringent crystal, typically doped with neodymium and used as a \ngain medium in solid-state lasers.\n\n"}
