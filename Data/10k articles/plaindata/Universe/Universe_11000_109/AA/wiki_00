{"id": "48943512", "url": "https://en.wikipedia.org/wiki?curid=48943512", "title": "2010s oil glut", "text": "2010s oil glut\n\nThe 2010s oil glut is a considerable surplus of crude oil that started in 2014–2015 and accelerated in 2016, with multiple causes. They include general oversupply as US and Canadian shale oil production reached critical volumes, geopolitical rivalries amongst oil-producing nations, falling demand across commodities markets due to the deceleration of the Chinese economy, and possible restraint of long-term demand as Environmental policy promotes fuel efficiency and steers an increasing share of energy consumption away from fossil fuels.\n\nThe world price of oil was above US$125 per barrel in 2012, and remained relatively strong above $100 until September 2014, after which it entered a sharp downward spiral, falling below $30 by January 2016. OPEC production was poised to rise further with the lifting of international sanctions against Iran, at a time when markets already appeared to be oversupplied by at least 2 million barrels per day.\n\nIn December 2015, \"The Telegraph\" quoted a major oil broker stating: \"The world is floating in oil. The numbers we are facing now are dreadful\" – and \"Forbes\" magazine stated: \"The ongoing oil price slump has more or less morphed into a complete rout, with profound long-term implications for the industry as a whole.\"\n\nAs 2016 continued, the price gradually rose back into the $40s, with the world waiting to see if and when and how the market would return to balance.\n\nIn October 2018, Brent prices had recovered to their pre-2015 levels, peaking at $86.29 a barrel on October 3. However, prices began to tumble yet again following the assassination of \"The Washington Post\" journalist Jamal Khashoggi.\n\nOn April 6, 2014, writing in a Saudi Arabian journal, World Pensions Forum economist Nicolas J. Firzli warned that the escalating oversupply situation could have durably negative economic consequences for all Gulf Cooperation Council member states: \n\nCombined U.S. and Canadian oil production nearly doubled from 2008 levels, due to substantial improvements in shale \"fracking\" technology in response to record oil prices. The steady rise in additional output, mostly from North Dakota, West Texas, Oklahoma and Alberta, eventually led to a plunge in U.S. oil import requirements and a record high volume of worldwide oil inventories in storage.\n\nThe 2015–16 Chinese stock market turbulence slowed the growth of the economy in China, restraining its demand for oil and other industrial commodities.\n\nIn spite of longstanding geopolitical rivalries – notably the GCC bloc versus Iran and Venezuela – emerging markets oil producers within and outside OPEC maintained at least some output discipline until the fall of 2014, when Saudi Arabia advocated higher OPEC production and lower price levels to erode the profitability of high-cost shale oil production.\n\nIt has been suggested that the Saudi Arabian-Iranian Proxy War was a powerful influence in the Saudi decision to launch the price war. \n\nSome geoeconomics experts have argued that the Saudi – Qatari rivalry has shattered the semblance of unity that may have existed amongst fossil fuel producers: \n\nIn the quarters leading up to the 21st UN Climate Change Conference in Paris, US and European policy makers, pension trustees and academic thought-leaders became active devising new ways of fostering private capital stewardship and “greener” investment: persuading and incentivizing institutional asset owners to embrace renewable energy and a low-carbon investment ethos, more propitious for long-term growth.\n\nSpeaking at the 5th annual World Pensions Forum held in Paris on the sidelines of the UN Conference, Earth Institute Director Jeffrey Sachs argued that institutional investors would eventually have to divest from carbon-reliant oil industry firms if they could not react to political and regulatory efforts to halt climate change: \"Every energy company in a pension fund's portfolio needs to be scrutinized from purely a financial view about its future, 'Why is this [a company] we would want to hold over a five- to 20-year period?'... If we continue to hold major energy companies that don’t have an answer to a basic financial test, we are just gambling. We have to take a fiduciary responsibility – these are not good bets.\"\n\nPresident Obama insisted on America’s essential role in that regard: “We’ve led by example […] from Alaska to the Gulf Coast to the Great Plains [...] we’ve seen the longest streak of private job creation in our history. We’ve driven our economic output to all-time highs while driving our carbon pollution down to its lowest level in nearly two decades. And then, with our historic joint announcement with China last year, we showed it was possible to bridge the old divide between developed and developing nations that had stymied global progress for so long […] That was the foundation for success in Paris.”\n\nUnder Hugo Chávez and his Bolivarian government, PDVSA resources were used to fund social programmes, with Chávez treating it like a \"piggybank\". His social policies resulted in overspending that caused shortages in Venezuela and allowed the inflation rate to grow to one of the highest rates in the world.\n\nAccording to Cannon, the state income from oil revenue grew \"from 51% of total income in 2000 to 56% 2006\"; oil exports increased \"from 77% in 1997 ... to 89% in 2006\"; and his administration's dependence on petroleum sales was \"one of the chief problems facing the Chávez government\". By 2008, exports of everything but oil \"collapsed\". and in 2012, the World Bank explained that Venezuela's economy is \"extremely vulnerable\" to changes in oil prices since in 2012 \"96% of the country's exports and nearly half of its fiscal revenue\" relied on oil production. When oil prices dropped in 2015, this worsened the crisis Venezuela was experiencing from the government's mismanagement.\n\nVenezuela has not yet, as of 2018, recovered from the oil production loss. Combined with OPEC/NOPEC production cuts, higher than average demand growth, US shale pipeline bottlenecks, and Iranian sanctions, the oil glut came to an end.\n\nImmediately after the death of Hugo Chavez, Castro sought a new benefactor as the oil that was shipped from Venezuela to Cuba began to slow. With Cuba needing new support, relations between the United States and Cuba began to be reestablished in 2014 during United States–Cuban Thaw. \n\nHowever in 2016, Cuba still relied on Venezuela's oil and economic assistance. With Cuba's economy slowing as a result of Venezuela's own crisis, many Cubans feared that their nation would soon return to having similar experiences to that of the Special Period, which occurred following the dissolution of the Soviet Union, which Cuba heavily relied on.\n\n"}
{"id": "15031355", "url": "https://en.wikipedia.org/wiki?curid=15031355", "title": "ALOX12B", "text": "ALOX12B\n\nArachidonate 12-lipoxygenase, 12R type, also known as ALOX12B, 12\"R\"-LOX, and arachiconate lipoygenase 3, is a lipoxygenase-type enzyme composed of 701 amino acids and encoded by the \"ALOX12B\" gene. The gene is located on chromosome 17 at position 13.1 where it forms a cluster with two other lipoxygenases, ALOXE3 and ALOX15B. Among the human lipoxygenases, ALOX12B is most closely (54% identity) related in amino acid sequence to ALOXE3\n\nALOX12B oxygenates arachidonic acid by adding molecular oxygen (O) in the form of a hydroperoxyl (HO) residue to its 12th carbon thereby forming 12(\"R\")-hydroperoxy-5\"Z\",8\"Z\",10\"E\",14\"Z\"-icosatetraenoic acid (also termed 12(\"R\")-HpETE or 12\"R\"-HpETE). When formed in cells, 12\"R\"-HpETE may be quickly reduced to its hydroxyl analog (OH), 12(\"R\")-hydroxy-5\"'Z\",8\"Z\",10\"E\",14\"Z\"-eicosatetraenoic acid (also termed 12(\"R\")-HETE or 12\"R\"-HETE), by ubiquitous peroxidase-type enzymes. These sequential metabolic reactions are:\narachidonic acid + O formula_1 12\"R\"-HpETE → 12\"R\"-HETE\n12\"R\"-HETE stimulates animal and human neutrophil chemotaxis and other responses in vitro and is able to elicit inflammatory responses when injected into the skin of an animal model However, the production of 12\"R\"-HETE for this or other purposes may not be primary function of ALOX12B.\n\nALOX12B is also capable of metabolizing free linoleic acid to 9(\"R\")-hydroperoxy-10(E),12(Z)-octadecadienoic acid (9\"R\"-HpODE) which is also rapidly converted to its hydroxyl derivative, 9-Hydroxyoctadecadienoic acid (9\"R\"-HODE).\nLinoleic acid + O formula_1 9\"R\"-HpODE → 9\"R\"-HODE\nThe \"S\" stereoisomer of 9\"R\"-HODE, 9\"S\"-HODE, has a range of biological activities related to oxidative stress and pain perception (see 9-Hydroxyoctadecadienoic acid. It is known or likely that 9\"R\"-HODE possesses at least some of these activities. For example, 9\"R\"-HODE, similar to 9\"S\"-HODE, mediates the perception of acute and chronic pain induced by heat, UV light, and inflammation in the skin of rodents (see 9-Hydroxyoctadecadienoic acid#9-HODEs as mediators of pain perception). However, production of these LA metabolites does not appear to be the primary function of ALOX12B; ALOX12B's primary function appears to be to metabolize linoleic acid that is not free but rather esterified to certain \n\nALOX12B targets Linoleic acid (LA). LA is the most abundant fatty acid in the skin epidermis, being present mainly esterified to the omega-hydroxyl residue of amide-linked omega-hydroxylated very long chain fatty acids (VLCFAs) in a unique class of ceramides termed esterified omega-hydroxyacyl-sphingosine (EOS). EOS is an intermediate component in a proposed multi-step metabolic pathway which delivers VLCFAs to the cornified lipid envelop in the skin's Stratum corneum; the presence of these wax-like, hydrophobic VLCFAs is needed to maintain the skin's integrity and functionality as a water barrier (see Lung microbiome#Role of the epithelial barrier). ALOX12B metabolizes the LA in EOS to its 9-hydroperoxy derivative; ALOXE3 then converts this derivative to three products: a) 9\"R\",10\"R\"-trans-epoxide,13\"R\"-hydroxy-10\"E\"-octadecenoic acid, b) 9-keto-10\"E\",12\"Z\"-octadecadienoic acid, and c) 9\"R\",10\"R\"-trans-epoxy-13-keto-11\"E\"-octadecenoic acid. These ALOX12B-oxidized products signal for the hydrolysis (i.e. removal) of the oxidized products from EOS; this allows the multi-step metabolic pathway to proceed in delivering the VLCFAs to the cornified lipid envelop in the skin's Stratum corneum.\n\nALOX12B protein has been detected in humans that in the same tissues the express ALOXE3 and ALOX15B viz., upper layers of the human skin and tongue and in tonsils. mRNA for it has been detected in additional tissues such as the lung, testis, adrenal gland, ovary, prostate, and skin with lower abundance levels detected in salivary and thyroid glands, pancreas, brain, and plasma blood leukocytes.\n\nDeletions of \"Alox12b\" or \"AloxE2\" genes in mice cause a congenital scaly skin disease which is characterized by a greatly reduced skin water barrier function and is similar in other ways to the autosomal recessive nonbullous Congenital ichthyosiform erythroderma (ARCI) disease of humans. Mutations in many of the genes that encode proteins, including ALOX12B and ALOXE3, which conduct the steps that bring and then bind VLCFA to the stratums corneum are associated with ARCI. ARCI refers to nonsyndromic (i.e. not associated with other signs or symptoms) congenital Ichthyosis including Harlequin-type ichthyosis, Lamellar ichthyosis, and Congenital ichthyosiform erythroderma. ARCI has an incidence of about 1/200,000 in European and North American populations; 40 different mutations in \"ALOX12B\" and 13 different mutations in \"ALOXE3\" genes account for a total of about 10% of ARCI case; these mutations uniformly cause a total loss of ALOX12B or ALOXE3 function (see mutations).\n\nIn psoriasis and other proliferative skin diseases such as the Erythrodermas underlying lung cancer, cutaneous T cell lymphoma, and drug reactions, and in Discoid lupus, Seborrheic dermatitis, Subacute Cutaneous lupus erythematosus, and Pemphigus foliaceus, cutaneous levels of ALOX12B mRNA and 12\"R\"-HETE are greatly increased. It is not clear if these increases contribute to the disease by, for example, 12\"R\"-HETE induction of inflammation, or are primarily a consequence of skin proliferation.\n\nThe expression of Alox12b and Aloxe3 mRNA in mice parallels, and is proposed to be instrumental for, skin development in mice embryogenesis; the human orthologs of these genes, i.e. ALOX12B and ALOXE3, may have a similar role in humans.\n\nSevere dietary deficiency of polyunsaturated omega 6 fatty acids leads to the essential fatty acid deficiency syndrome that is characterized by scaly skin and excessive water loss; in humans and animal models the syndrome is fully reversed by dietary omega 6 fatty acids, particularly linoleic acid. It is proposed that this deficiency disease resembles and has a similar basis to Congenital ichthyosiform erythrodema; that is, it is at least in part due to a deficiency of linoleic acid and thereby in the EOS-based delivery of VLCFA to the stratum corneum.\n"}
{"id": "9020962", "url": "https://en.wikipedia.org/wiki?curid=9020962", "title": "Adowlie", "text": "Adowlie\n\nAn adowlie (also \"adholee\", \"adholy\", \"adowly\") is an obsolete unit of dry volume and mass formerly used in western India a standard measurement for grain and salt.\n\n\nAfter metrification in the mid-20th century, the unit became obsolete.\n\n"}
{"id": "10714336", "url": "https://en.wikipedia.org/wiki?curid=10714336", "title": "April 2007 nor'easter", "text": "April 2007 nor'easter\n\nThe April or Spring Nor'easter of 2007 was a nor'easter that affected mainly the eastern parts of North America during its four-day course, from April 14 to April 17, 2007. The combined effects of high winds, heavy rainfall, and high tides led to flooding, storm damages, power outages, and evacuations, and disrupted traffic and commerce. In the north, heavy wet snow caused the loss of power for several thousands of homes in Ontario and Quebec. The storm caused at least 18 fatalities.\n\nThe storm that would become the April 2007 Nor'easter started out in the Southwestern United States, as an upper-level disturbance in the jet stream, on April 13. It brought high wind and fire danger to California, Nevada, New Mexico and Arizona. The storm then moved out into the southern Plains States, bringing heavy snow to Colorado, Oklahoma, and Texas. Heavy rain and severe thunderstorms, with hail, wind, and tornadoes, affected parts of Texas, Louisiana, Mississippi, Alabama, Georgia, Florida, and the Carolinas. The storm then moved across the Mid-Atlantic States, and into the Atlantic Ocean, following the East Coast. The storm then rapidly intensified into a major nor'easter, with the warm waters of the Gulf Stream. The storm stalled just offshore from New York City and continued to strengthen. The lowest barometric pressure recorded was 968 millibars (28.58 in Hg), equivalent to that of a moderate category 2 hurricane.\n\nThe National Weather Service reported 7.57 inches (192 mm) of rain in Central Park by midnight of April 15, the second heaviest rainfall in 24 hours on record, and indicated that this storm caused the worst flooding since Hurricane Floyd in 1999.\nMajor airports in the New York area resumed flights on April 16, after having had to cancel over 500 flights before. Local rail and transit lines reported delays and cancellations affecting the MTA, NJ Transit, LIRR, and Metro-North Railroad. Power failures affected several thousand people.\n\nOn April 16, 2007, the storm caused sustained winds of nearly 100 mph (87 knots) on, and near New Hampshire's Mount Washington, with gusts topping out at 156 mph (136 knots).\nWhile areas closer to the shore received heavy rainfall, higher regions inland received unseasonal snow. Several towns suffered from flooding including Mamaroneck in New York, and Bound Brook and Manville in New Jersey, while coastal towns had to deal with damage from high tides. Most major highways in Westchester County, New York were closed on April 15 and April 16 due to extreme flooding. In Quebec, several regions including the Laurentides and Charlevoix regions received in excess of 6 inches (15 cm) of snow with areas exceeding well above 1 foot (30 cm) of snow. In the city of Ottawa, 17 cm (5 in) of heavy wet snow fell in just a few hours causing power lines and trees to fall down causing scattered blackouts in several parts of the city. Similar damage was reported in the higher elevations north of Montreal and Ottawa.\n\nIn total, more than 175,000 homes in Canada suffered a power outage, including 160,000 Hydro-Québec customers mainly in areas from Gatineau towards Quebec City including Montreal, Lanaudière and the Laurentians with an additional 15,000 Hydro One and Hydro Ottawa customers. In the US over a quarter million homes lost power with New York and Pennsylvania being the hardest hit states due to the strong winds.\n\nGovernor Eliot Spitzer of New York activated 3,200 members of the National Guard on alert. Richard Codey, acting governor of New Jersey, declared a state of emergency.\nThe storm was blamed for several fatalities, including one person in a tornado in South Carolina, two people in storm-related traffic accidents in New York and Connecticut, two people in West Virginia, three people in New Jersey, and prior to its arrival in the East, five deaths in Kansas and Texas. In Quebec, an accident between a van and a tractor trailer killed five occupants of the van in a highway north of Montreal. Numerous other accidents were reported by OPP and the Sûreté du Québec during the storm.\n\nThe Boston Marathon took place in what many considered to be the worst weather in its 110-year history. Race officials held serious talks about whether or not to cancel the race. The men's race had the slowest winning time in thirty years (1977). In the women's race, \"[t]he rainy and windy conditions led to the slowest winning time since 1985\".\n\nHigh winds during the storm prevented emergency medical services from using helicopters for evacuation of the injured at the Virginia Tech massacre.\n\nThe Internal Revenue Service delayed by two days the deadline for tax filing for victims of the nor'easter.\n\nWhile filing for federal disaster relief, acting governor Codey of New Jersey indicated that the storm caused $180 million in property damage in New Jersey, making it the second-worst rain storm in its history, after Hurricane Floyd.\n\n\n"}
{"id": "2033875", "url": "https://en.wikipedia.org/wiki?curid=2033875", "title": "Artificial leather", "text": "Artificial leather\n\nArtificial leather is a material intended to substitute for leather in fields such as upholstery, clothing, footwear and fabrics and other uses where a leather-like finish is desired but the actual material is cost-prohibitive or unsuitable. Artificial leather is marketed under many names, including \"leatherette\", \"faux leather\", \"vegan leather\", \"PU leather\" and \"pleather\".\n\nMany different methods for the manufacture of imitation leathers have been developed.\n\nOne of the earliest was \"Presstoff\". Invented in 19th century Germany, it is made of specially layered and treated paper pulp. It gained its widest use in Germany during the Second World War in place of leather, which under wartime conditions was rationed. \"Presstoff\" could be used in almost every application normally filled by leather, excepting items like footwear that were repeatedly subjected to flex wear or moisture. Under these conditions \"Presstoff\" tends to delaminate and lose cohesion.\n\n\"Poromerics\" are made from a plastic coating (usually a polyurethane) on a fibrous base layer (typically a polyester). The term poromeric was coined by DuPont as a derivative of the terms \"porous\" and \"polymeric\". The first poromeric material was DuPont's Corfam, introduced in 1963 at the Chicago Shoe Show. Corfam was the centerpiece of the DuPont pavilion at the 1964 New York World's Fair in New York City. After spending millions of dollars marketing the product to shoe manufacturers, DuPont withdrew Corfam from the market in 1971 and sold the rights to a company in Poland.\n\n\"Leatherette\" is also made by covering a fabric base with a plastic. The fabric can be made of natural or synthetic fiber which is then covered with a soft PVC layer. Leatherette is used in bookbinding and was common on the casings of 20th century cameras.\n\nCork leather is a natural-fiber alternative made from the bark of cork oak trees that has been compressed, similar to \"Presstoff\".\n\nA fermentation method of making collagen, the main chemical in real leather, is under development.\n\nArtificial leathers are often used in clothing fabrics, furniture upholstery, and automotive uses. One disadvantage of plastic-coated artificial leather is that it is not porous and does not allow air to pass through; thus, sweat can accumulate if it is used for clothing, car seat coverings, etc. One of its primary advantages, especially in cars, is that it requires little maintenance in comparison to leather, and does not crack or fade easily.\n\n\n"}
{"id": "3240922", "url": "https://en.wikipedia.org/wiki?curid=3240922", "title": "Axial engine", "text": "Axial engine\n\nAxial engines (sometimes known as barrel or Z-crank engines) are a type of reciprocating engine with pistons arranged around an output shaft with their axes parallel to the shaft. Barrel refers to the cylindrical shape of the cylinder group (result of the pistons being spaced evenly around the central crankshaft and aligned parallel to the crankshaft axis) whilst the Z-crank alludes to the shape of the crankshaft.\n\nThe key advantage of the axial design is that the cylinders are arranged in parallel around the output/crank shaft in contrast to radial and inline engines, both types having cylinders at right angles to the shaft. As a result, it is a very compact, cylindrical engine, allowing variation in compression ratio of the engine while running. In a swashplate engine the piston rods stay parallel with the shaft, and piston side-forces that cause excessive wear can be eliminated almost completely. The small-end bearing of a traditional connecting rod, one of the most problematic bearings in a traditional engine, is eliminated.\n\nAn alternate design, the Rand cam engine, replaces the plate with one or more sinousoidal cam surfaces. Vanes mounted parallel to a shaft mounted inside a cylindrical 'barrel' that are free to sliding up and down ride the sinuous cam, the segments formed by rotor, stator walls and vanes constituting combustion chambers. In effect these spaces serving the same purpose as the cylinders of an axial engine, and the sinuous cam surface acts as the face of the pistons. In other respect this form follows the normal cycles of internal combustion but with burning gas directly imparting a force on the cam surface, translated into a rotational force by timing one or more detonations. This design eliminates the multiple reciprocal pistons, ball joints and swash plate of a conventional 'barrel' engine but crucially depends on effective sealing provided by sliding and rotating surfaces.\n\nIn either form the axial or 'barrel' engine can be derived as a cam engine or swashplate or wobble plate engine.\n\nWhile axial engines are challenging to make practicable at typical engine operating speeds some cam engines have been tested that offer extremely compact size (approximating to a six-inch (150mm) cube) yet producing approximately forty horsepower at c 7000 rpm, useful for light aerial applications. The attraction of lightweight and mechanically simple (far fewer major moving parts, in the form of a rotor plus twelve axial vanes forming twenty-four combustion chambers) engines, even with a finite working life, have obvious application for small unmanned aircraft. (Such a design having allegedly been tested at NAVAIR PSEF in 2003.)\n\nIn 1911 the Macomber Rotary Engine Company of Los Angeles marketed one of the first axial internal-combustion engines, manufactured by the Avis Engine Company of Allston, Massachusetts. A four-stroke, air-cooled unit, it had seven cylinders and a variable compression ratio, altered by changing the wobble-plate angle and hence the length of piston stroke. It was called a \"rotary engine\", because the entire engine rotated apart from the end casings.\n\nIgnition was supplied by a Bosch magneto directly driven from the cam gears. The high voltage current was then taken to a fixed electrode on the front bearing case, from which the sparks would jump to the spark plugs in the cylinder heads as they passed within 1/16 inch (1.5 mm) from it. According to Macomber's literature, it was \"guaranteed not to overheat\".\n\nThe engine was claimed to be able to run at 150 to 1,500 rpm. At the normal speed of 1,000 rpm, it reportedly developed 50 hp. It weighed and it was long by in diameter.\n\nPioneer aviator Charles Francis Walsh flew an aircraft powered by a Macomber engine in May 1911, the \"Walsh Silver Dart\".\n\nIn 1913 Statax-Motor of Zürich, Switzerland introduced a swashplate engine design. Only a single prototype was produced, which is currently held in the Science Museum, London. In 1914 the company moved to London to become the Statax Engine Company and planned on introducing a series of rotary engines; a 3-cylinder of 10 hp, a 5-cylinder of 40 hp, a 7-cylinder of 80 hp, and a 10-cylinder of 100 hp.\n\nIt appears only the 40 hp design was ever produced, which was installed in a Caudron G.II for the British 1914 Aerial Derby but was withdrawn before the flight. Hansen introduced an all-aluminum version of this design in 1922, but it is not clear if they produced it in any quantity. Much improved versions were introduced by Statax's German division in 1929, producing 42 hp in a new sleeve valve version known as the \"29B\". Greenwood and Raymond of San Francisco acquired the patent rights for the US, Canada, and Japan, and planned a 5-cylinder of 100 hp and a 9-cylinder of 350 hp.\n\nIn 1917 Anthony Michell obtained patents for his swashplate engine design. Its unique feature was the means of transferring the load from the pistons to the swashplate, achieved using tilting slipper pads sliding on a film of oil. Another innovation by Michell was his mathematical analysis of the mechanical design, including the mass and motion of the components, so that his engines were in perfect dynamic balance at all speeds.\n\nIn 1920 Michell established the Crankless Engines Company in Fitzroy (Australia), and produced working prototypes of pumps, compressors, car engines and aero engines, all based on the same basic design.\n\nEngine designer Phil Irving worked for the Crankless Engine Company before his time at HRD.\n\nA number of companies obtained a manufacturing licence for Michell’s design. The most successful of these was the British company Waller and Son, who produced gas boosters.\n\nThe largest Michell crankless engine was the XB-4070, a diesel aircraft engine built for the United States Navy. Consisting of 18 pistons, it was rated at 2000 horsepower and weighed 2150 pounds.\n\nExperimental barrel engines for aircraft use were built and tested by American John O. Almen of Seattle, Washington in the early 1920s, and by the mid-1920s the water-cooled \"Almen A-4\" (18 cylinders, two groups of nine each horizontally-opposed) had passed its United States Army Air Corps acceptance tests. However, it never entered production, reportedly due to limited funds and the Air Corps' growing emphasis on air-cooled radial engines. The A-4 had much smaller frontal area than water-cooled engines of comparable power output, and thereby offered better streamlining possibilities. It was rated at 425 horsepower (317 kW), and weighed only 749 pounds (340 kg), thus giving a power/weight ratio of better than 1:2, a considerable design achievement at the time.\n\nHeraclio Alfaro was a Spanish aviator who was knighted at the age of 18 by King Alfonso XIII of Spain for designing, building, and flying Spain's first airplane. He developed a barrel engine for aircraft use which was later produced by the Indian Motocycle Manufacturing Company as the \"Alfaro\". It was a perfect example of the \"put in everything\" design, as it included a sleeve valve system based on a rotating cylinder head, a design that never entered production on any engine. It was later developed further for use in the Doman helicopter by Stephen duPont, son of the president of the Indian Motorcycle Company, who had been one of Alfaro's students at Massachusetts Institute of Technology.\n\nThe Bristol Axial Engine of the mid-1930s was designed by Charles Benjamin Redrup for the Bristol Tramways and Carriage Company; it was a 7-litre, 9-cylinder, wobble-plate type engine. It was originally conceived as a power unit for buses, possibly because its compact format would allow it to be installed beneath the vehicle's floor. The engine had a single rotary valve to control induction and exhaust. Several variants were used in Bristol buses during the late 1930s, the engine going through several versions from RR1 to RR4, which had a power output of 145 hp at 2900 rpm. Development was halted in 1936 following a change of management at the Bristol company.\n\nPerhaps the most refined of the designs was the British Wooler wobble-plate engine of 1947. This 6-cylinder engine was designed by John Wooler, better known as a motorcycle engine designer, for aircraft use. It was similar to the Bristol axial engine but had two wobble-plates, driven by 12 opposed pistons in 6 cylinders. The engine is often incorrectly referred to as a swashplate engine. A single example is preserved in the Aeroplane Gallery of the Science Museum, London.\n\nSome small barrel engines were produced by the H.L.F. Trebert Engine Works of Rochester, New York for marine usage.\n\nThe Dyna-Cam engine originally came from a design by the Blazer brothers, two American engineers in the brass era automotive industry who worked for Studebaker in 1916. They sold the rights to Karl Herrmann, Studebaker's head of engineering, who developed the concept over many years, eventually taking out US patent 2237989 in 1941. It has 6 double-ended pistons working in 6 cylinders, and its 12 combustion chambers are fired every revolution of the drive shaft. The pistons drive a sine-shaped cam, as opposed to a swashplate or wobble-plate, hence its name.\n\nIn 1961, at the age of 80, Herrmann sold the rights to one of his employees, Edward Palmer, who set up the Dyna-Cam Engine Corp. along with son Dennis. Edward's son Dennis and daughter Pat then helped get the engine installed in a Piper Arrow airplane. The engine was flown for about 700 hours from 1987 through 1991. Their longest-life engine ran for nearly 4000 hours before overhaul. Dyna-Cam opened a research and development facility about 1993 and won many various awards from NASA, the United States Navy, the United States Marine Corps, California Energy Commission, Air Quality Management District, and Los Angeles Regional Technology Alliance for different variations of the same Dyna-Cam engine. About 40 prototype engines were built by the Herrmann Group and another 25 built by the Dyna-Cam Group since they acquired the engine and opened their shop. A new patent was granted to Dennis Palmer and Edward Palmer, first in 1985 and then several more around 2000 to Dennis Palmer. In 2003 the assets of the Dyna-Cam Engine Corporation were acquired by Aero-Marine Corporation, who changed their name to Axial Vector Engine Corporation. Axial Vector then totally re-designed the cam engine. Axial Vector's new engine, like many of the others on this list, suffers from the \"put in everything\" problem, including piezoelectric valves and ignition, ceramic cylinder liners with no piston rings, and a variety of other advanced features. It has little similarity to the original Herrmann and Dyna-Cam engines, since the Dyna-Cam engine used conventional valves, piston rings, accessories, had no unproven ceramic materials and actually flew in an aircraft and also powered a \"Eliminator\" ski boat for over four years.\n\nUnited Kingdom company FairDiesel Limited is designing two-stroke Diesel opposed piston barrel engines that use non-sinusoidal cams, for industrial applications and aviation use. Their engine designs range from a 2-cylinder, 80 mm bore to 32-cylinder, 160 mm bore.\n\nNew Zealand company Duke Engines started in 1993 has created several different engines and installed one in a car in 1999. The engine runs a 5-cylinder, 3 litre, 4-stroke internal combustion engine platform with its unique axial arrangement, which is in its third generation. Due to a valveless design, Duke engine loses less energy between the power strokes. Current prototypes of Duke's engines claim to match characteristics of conventional internal combustion engines but with fewer parts and 30% lighter. This goes in the direction of developing a more efficient engine. It is being stated that this design for engines could be ideal for motorcycles in future. During development the Duke has been tested at MAHLE Powertrain in the United Kingdom and in the United States; test results are available with it also having multi-fuel capabilities.\n\nThe Cylindrical Energy Module is a sine-wave swashplate engine that can also be used as a standalone pump, powered by an external source. The rotating swashplate rotor assembly is moved back and forth with the help of piston drive pins, which follow a stationary sinusoidal cam track that encircles the rotor assembly.\n\n\n\n\n"}
{"id": "3057518", "url": "https://en.wikipedia.org/wiki?curid=3057518", "title": "Backward-wave oscillator", "text": "Backward-wave oscillator\n\nA backward wave oscillator (BWO), also called carcinotron (a trade name for tubes manufactured by CSF, now Thales) or backward wave tube, is a vacuum tube that is used to generate microwaves up to the terahertz range. It belongs to the traveling-wave tube family. It is an oscillator with a wide electronic tuning range.\n\nAn electron gun generates an electron beam that is interacting with a slow-wave structure. It sustains the oscillations by propagating a traveling wave backwards against the beam. The generated electromagnetic wave power has its group velocity directed oppositely to the direction of motion of the electrons. The output power is coupled out near the electron gun.\n\nIt has two main subtypes, the M-type (M-BWO), the most powerful, and the O-type (O-BWO). The output power of the O-type is typically in the range of 1 mW at 1000 GHz to 50 mW at 200 GHz. Carcinotrons are used as powerful and stable microwave sources. Due to the good quality wavefront they produce (see below), they find use as illuminators in terahertz imaging.\n\nThe backward wave oscillators were demonstrated in 1951, M-type by Bernard Epsztein\n\nand O-type by Rudolf Kompfner. The M-type BWO is a voltage-controlled non-resonant extrapolation of magnetron interaction. Both types are tunable over a wide range of frequencies by varying the accelerating voltage. They can be swept through the band fast enough to be appearing to radiate over all the band at once, which makes them suitable for effective radar jamming, quickly tuning into the radar frequency. Carcinotrons allowed airborne radar jammers to be highly effective. However, frequency-agile radars can hop frequencies fast enough to force the jammer to use barrage jamming, diluting its output power over a wide band and significantly impairing its efficiency.\n\nCarcinotrons are used in research, civilian and military applications. For example, the Czechoslovak air defense detection systems Kopac passive sensor and Ramona passive sensor employed carcinotrons in their receiver systems.\n\nAll travelling-wave tubes operate in the same general fashion, and differ primarily in details of their construction.\n\nThe concept is dependent on a steady stream of electrons from an electron gun that travel down the center of the tube (see [Basic Concept Figure] on the left). Surrounding the electron beam is some sort of radio frequency source signal; in the case of the traditional klystron this is a resonant cavity fed with an external signal, whereas in more modern devices there are a series of these cavities or a helical metal wire fed with the same signal.\n\nAs the electrons travel down the tube, they will interact with the RF signal (see [Basic Concept Figure] on the left). The electrons will be attracted to areas with maximum positive bias and repelled from negative areas. This causes the electrons to \"bunch up\" as they are repelled or attracted along the length of the tube, a process known as \"velocity modulation\". This process makes the electron beam take on the same general structure as the original signal; the density of the electrons in the beam matches the relative amplitude of the RF signal in the induction system. The result is a signal in the electron beam that is an amplified version of the original RF signal.\n\nAs the electrons are moving, they will induce a magnetic field in any nearby conductor as illustrated in the Basic Concept Figure. This allows the now-amplified signal to be extracted. In systems like the magnetron or klystron, this is accomplished with another resonant cavity. In the helical designs, this process occurs along the entire length of the tube, reinforcing the original signal in the helical conductor. The \"problem\" with traditional designs is that they have relatively narrow bandwidths; designs based on resonators will work with signals within 10% or 20% of their design, as this is physically built into the resonator design, while the helix designs have a much wider bandwidth, perhaps 100% on either side of the design peak.\n\nThe BWO is built in a fashion similar to the helical TWT. However, instead of the RF signal propagating in the same (or similar) direction as the electron beam, the original signal travels at right angles to the beam. This is normally accomplished by drilling a hole through a rectangular waveguide and shooting the beam through the hole. The waveguide then goes through two right angle turns, forming a C-shape and crossing the beam again. This basic pattern is repeated along the length of the tube so the waveguide passes across the beam several times, forming a series of S-shapes.\n\nThe original RF signal enters from what would be the far end of the TWT, where the energy would be extracted. The effect of the signal on the passing beam causes the same velocity modulation effect, but because of the direction of the RF signal and specifics of the waveguide, this modulation travels backward along the beam, instead of forward. This propagation, the \"slow-wave\", reaches the next hole in the folded waveguide just as the same phase of the RF signal does. This causes amplification just like the traditional TWT.\n\nThe difference in the two systems is that in the TWT the speed of propagation in the helix has to be similar to that of the electrons in the beam. This is not the case in the BWO. The waveguide places strict limits on the bandwidth of the signal and sets its propagation speed as a basic function of its construction, but the speed of the signal induced into the electron beam is relative to the speed of the electrons. That means the frequency of the output signal can be changed by changing the speed of the electrons, which is easily accomplished by changing the voltage of the electron gun.\n\nThe device was originally given the name \"carcinotron\" because it was like cancer to existing radar systems. By simply changing the supply voltage, the device could produce any required frequency across a band that was much larger than any existing microwave amplifier could match - the cavity magnetron and klystron worked at a single frequency defined by the physical dimensions of their resonators, and could be tuned only within a small range of that design.\n\nPreviously, jamming a radar was a complex and time-consuming operation. Operators had to listen for potential frequencies being used, set up one of a bank of amplifiers on that frequency, and then begin broadcasting. When the radar station realized what was happening, they would change their frequencies and the process would begin again.\n\nIn contrast, the carcinotron could sweep through all the possible frequencies so rapidly that it appeared to be a constant signal on all of the frequencies at once. Typical designs could generate hundreds or low thousands of watts, so at any one frequency, there might be a few watts of power. However, at long range the amount of energy from the original radar that reaches the aircraft is only a few watts anyway, so the carcinotron can overpower them.\n\nThe system was so powerful that it was found that a carcinotron operating on an aircraft would begin to be effective even before it rose above the radar horizon. As it swept through the frequencies it would broadcast on the radar's own pulse frequency at what were effectively random times, filling the display with random dots any time the antenna was pointed near it, perhaps 3 degrees on either side of the target. There were so many dots that the display simply filled with white noise in that area. As it approached the station, the signal would also begin to appear in the antennas sidelobes, creating further areas that were blanked out by noise. At close range, on the order of , the entire radar display would be completely filled with noise, rendering it useless.\n\nThe concept was so powerful as a jammer that there were serious concerns that ground-based radars were obsolete. Airborne radars had the advantage that they could approach the aircraft carrying the jammer, and, eventually, the huge output from their transmitter would \"burn through\" the jamming. However, interceptors of the era relied on ground direction to get into range. This represented an enormous threat to air defense operations.\n\nFor ground radars, the threat was eventually solved in two ways. The first was that radars were upgraded to operate on many different frequencies and switch among them randomly from pulse to pulse, a concept now known as frequency agility. Some of these frequencies were never used in peacetime, and highly secret, with the hope that they would not be known to the jammer in wartime. The carcinotron could still sweep through the entire band, but then it would be broadcasting on the same frequency as the radar only at random times, reducing its effectiveness. The other solution was to add passive receivers that triangulated on the carcinotron broadcasts, allowing the ground stations to produce accurate tracking information on the location of the jammer and allowing them to be attacked.\n\nThe needed slow-wave structures must support a radio frequency (RF) electric field with a longitudinal component; the structures are periodic in the direction of the beam and behave like microwave filters with passbands and stopbands. Due to the periodicity of the geometry, the fields are identical from cell to cell except for a constant phase shift Φ.\nThis phase shift, a purely real number in a passband of a lossless structure, varies with frequency.\nAccording to Floquet's theorem (see Floquet theory), the RF electric field E(z,t) can be described at an angular frequency ω, by a sum of an infinity of \"spatial or space harmonics\" E\n\nwhere the wave number or propagation constant k of each harmonic is expressed as\n\nz being the direction of propagation, p the pitch of the circuit and n an integer.\n\nTwo examples of slow-wave circuit characteristics are shown, in the ω-k or Brillouin diagram:\n\n\nA periodic structure can support both forward and backward space harmonics, which are not modes of the field, and cannot exist independently, even if a beam can be coupled to only one of them.\n\nAs the magnitude of the space harmonics decreases rapidly when the value of n is large, the interaction can be significant only with the fundamental or the first space harmonic.\n\nThe M-type carcinotron, or M-type backward wave oscillator, uses crossed static electric field E and magnetic field B, similar to the magnetron, for focussing an electron sheet beam drifting perpendicularly to E and B, along a slow-wave circuit, with a velocity E/B. Strong interaction occurs when the phase velocity of one space harmonic of the wave is equal to the electron velocity. Both E and E components of the RF field are involved in the interaction (E parallel to the static E field). Electrons which are in a decelerating E electric field of the slow-wave, lose the potential energy they have in the static electric field E and reach the circuit. The sole electrode is more negative than the cathode, in order to avoid collecting those electrons having gained energy while interacting with the slow-wave space harmonic.\n\nThe O-type carcinotron, or O-type backward wave oscillator, uses an electron beam longitudinally focused by a magnetic field, and a slow-wave circuit interacting with the beam. A collector collects the beam at the end of the tube.\n\nThe BWO is a voltage tunable oscillator, whose voltage tuning rate is directly related to the propagation characteristics of the circuit. The oscillation starts at a frequency where the wave propagating on the circuit is synchronous with the slow space charge wave of the beam. Inherently the BWO is more sensitive than other oscillators to external fluctuations. Nevertheless, its ability to be phase- or frequency-locked has been demonstrated, leading to successful operation as a heterodyne local oscillator.\n\nThe frequency–voltage sensitivity, is given by the relation\n\nThe oscillation frequency is also sensitive to the beam current (called \"frequency pushing\"). The current fluctuations at low frequencies are mainly due to the anode voltage supply, and the sensitivity to the anode voltage is given by\n\nThis sensitivity as compared to the cathode voltage sensitivity, is reduced by the ratio ω/ω, where ω is the angular plasma frequency; this ratio is of the order of a few times 10.\n\nMeasurements on submillimeter-wave BWO's (de Graauw et al., 1978) have shown that a signal-to-noise ratio of 120 dB per MHz could be expected in this wavelength range. In heterodyne detection using a BWO as a local oscillator, this figure corresponds to a noise temperature added by the oscillator of only 1000–3000 K.\n\n\n"}
{"id": "14685983", "url": "https://en.wikipedia.org/wiki?curid=14685983", "title": "Bavaria Solarpark", "text": "Bavaria Solarpark\n\nThe Bavaria Solarpark is a group of three photovoltaic power stations in different locations in Germany. \nIts total capacity amounts to 10 megawatts (MW) and consists of the following distinct solar farms south of Neumarkt in der Oberpfalz, in Bavaria:\n\nThe Bavaria Solarpark was constructed and is operated by the American company SunPower. \nIt consists of 57,600 solar panels (model \"PowerLight NT-5AE3D\" by Sharp) mounted on SunPower's solar trackers and covers a total area of 40 hectares (99 acres). \nInaugurated on 30 June 2005, the solar farm was grid-connected six months later in December 2005. \nFor a few months, the Solarpark Mühlhausen was the world's largest photovoltaic power station.\n\n"}
{"id": "203175", "url": "https://en.wikipedia.org/wiki?curid=203175", "title": "Benoît Paul Émile Clapeyron", "text": "Benoît Paul Émile Clapeyron\n\nBenoît Paul Émile Clapeyron (; 26 February 1799 – 28 January 1864) was a French engineer and physicist, one of the founders of thermodynamics.\n\nBorn in Paris, Clapeyron studied at the École polytechnique, graduating in 1818. He also studied at École des mines. In 1820 he and Gabriel Lamé went to Saint Petersburg to teach and work at the school of public works there. He returned to Paris only after the Revolution of July 1830, supervising the construction of the first railway line connecting Paris to Versailles and Saint-Germain.\nThe half brothers Stéphane Mony and Eugène Flachat collaborated in this project, which was financed by Adolphe d'Eichthal, Rothschild, Auguste Thurneyssen, Sanson Davillier and the Péreire brothers (Émile and Isaac).\nClapeyron took his steam engine designs to England in 1836 to find a manufacturer and engaged Sharp, Roberts and Co..\n\nFrom 1844 to 1859 Clapyron was a professor at Ecole des Ponts et Chaussees.\n\nClapeyron married Mélanie Bazaine, daughter of Pierre-Dominique Bazaine (mathematician and ingénieur des ponts), and older sister of Pierre-Dominique (Adolphe) Bazaine (railway engineer) and Francois Achille Bazaine (Marshal of France).\n\nIn 1834, he made his first contribution to the creation of modern thermodynamics by publishing a report entitled \"Mémoire sur la puissance motrice de la chaleur\" (\"Memoir on the Motive Power of Heat\"), in which he developed the work of the physicist Nicolas Léonard Sadi Carnot, deceased two years before. Though Carnot had developed a compelling analysis of a generalised heat engine, he had employed the clumsy and already unfashionable caloric theory.\n\nClapeyron, in his memoire, presented Carnot's work in a more accessible and analytic graphical form, showing the Carnot cycle as a closed curve on an indicator diagram, a chart of pressure against volume (named in his honor Clapeyron's graph). Clapeyron's analysis of Carnot was more broadly disseminated in 1843 when Johann Poggendorff translated it into German.\n\nIn 1842 Clapeyron published his findings on the \"optimal position for the piston at which the various valves should be opened or closed.\" In 1843, Clapeyron further developed the idea of a reversible process, already suggested by Carnot and made a definitive statement of \"Carnot's principle\", what is now known as the second law of thermodynamics.\n\nThese foundations enabled him to make substantive extensions of Clausius' work, including the formula, now known as the Clausius–Clapeyron relation, which characterises the phase transition between two phases of matter. He further considered questions of phase transitions in what later became known as Stefan problems.\n\nClapeyron also worked on the characterisation of perfect gases, the equilibrium of homogeneous solids, and calculations of the statics of continuous beams, notably the theorem of three moments (Clapeyron's theorem).\n\n\n\n"}
{"id": "44656810", "url": "https://en.wikipedia.org/wiki?curid=44656810", "title": "Bicyclobutane", "text": "Bicyclobutane\n\nBicyclobutane is an organic compound with the formula CH. It is a bicyclic molecule consisting of two \"cis\"-fused cyclopropane rings, and is a colorless and easily condensed gas. Bicyclobutane is noted for being one of the most strained compounds that is isolatable on a large scale – its strain energy is estimated at 63.9 kcal mol. It is a nonplanar molecule, with a dihedral angle between the two cyclopropane rings of 123°.\n\nThe first reported bicyclobutane was the carboxyethyl derivative, CHCOEt, which was prepared by dehydrohalogenation the corresponding bromocyclobutanecarboxylate ester with sodium hydride. The parent hydrocarbon was prepared from 1-bromo-3-chlorocyclobutane by conversion of the bromocyclobutanecarboxylate ester, followed by intramolecular Wurtz coupling using molten sodium. The intermediate 1-bromo-3-chlorocyclobutane can also be prepared via a modified Hunsdiecker reaction from 3-chlorocyclobutanecarboxylic acid using mercuric oxide and bromine:\n\nA synthetic approach to bicyclobutane derivatives involves ring closure of a suitably substituted 2-bromo-1-(chloromethyl)cyclopropane with magnesium in THF.\n\nStereochemical evidence indicates that bicyclobutane undergoes thermolysis to form 1,3-butadiene with an activation energy of 41 kcal mol via a concerted pericyclic mechanism (cycloreversion, [σ2s+σ2a]).\n\n"}
{"id": "37890717", "url": "https://en.wikipedia.org/wiki?curid=37890717", "title": "Brine rejection", "text": "Brine rejection\n\nBrine rejection is a process that occurs when salty water freezes. The salts do not fit in the crystal structure of water ice, so the salt is expelled.\n\nSince the oceans are salty, this process is important in nature. Salt rejected by the forming sea ice drains into the surrounding seawater, creating saltier, denser brine. The denser brine sinks, influencing ocean circulation.\n\nAs water reaches the temperature where it begins to crystallize and form ice, salt ions are rejected from the lattices within the ice and either forced out into the surrounding water, or trapped among the ice crystals in pockets called brine cells. Generally, sea ice has a salinity ranging from 0 psu at the surface to 4 psu at the base. The faster that this freezing process occurs, the more brine cells are left in the ice. Once the ice reaches a critical thickness, roughly 15 cm, the concentration of salt ions in the liquid around the ice begins to increase, as leftover brine is rejected from the cells. This increase is associated with the appearance of strong convective plumes, which flow from channels and within the ice and carry a significant salt flux. The brine that drains from the newly formed ice is replaced by a weak flow of relatively fresh water, from the liquid region below it. The new water partially freezes within the pores of the ice, increasing the solidity of the ice.\n\nAs sea ice ages and thickens, the initial salinity of the ice decreases due to the rejection of brine over time [Fig. 2]. While the sea ice ages, desalinization occurs to such a degree that some multiyear ice has a salinity of less than 1 PSU. This occurs in three different ways:\n\nBrine rejection occurs in the sea ice packs around at the north and south poles of the earth [Fig. 3]. The Arctic Ocean has historically ranged from roughly 14-16 million square kilometers in late winter to roughly 7 million square kilometers each September. The annual increase of ice plays a major role in the movement of ocean circulation and deep water formation. The density of the water below the newly formed ice increases due to the brine rejection. Saltier water can also become colder without freezing.\n\nThe dense waters that form in the Arctic are called North Atlantic Deep Waters (NADW), while the Antarctic Bottom Water (AABW) forms in the southern hemisphere. These two areas of brine rejection play an important role in the thermohaline circulation of all of earth's oceans.\n\nAs sea ice freezes, it rejects increasingly salty water, which drains through narrow brine channels that thread through the ice. The brine flowing through the brine channels and out of the bottom of the ice is very cold and salty, so it sinks in the warmer, fresher seawater under the ice, forming a plume. The plume is colder than the freezing point of sea water under the ice, so the seawater can freeze where it touches the plume. Ice freezing around the edges of the plume gradually builds a hollow icicle-like tube, called a brinicle. These frozen stalactite-like forms are fragile during early stages, but if brine drainage ceases, they may freeze solid. In calm waters, brinicles can reach the sea floor, freezing it fairly abruptly.\n\nThe deep ocean basins are stably stratified, so mixing of surface waters with the deep ocean waters occurs only very slowly. The dissolved CO of the surface waters of the ocean is roughly in equilibrium with the partial pressure of CO in the atmosphere. As atmospheric CO levels are rising, the oceans are absorbing some CO from the atmosphere. When surface waters sink, they carry considerable amounts of CO into the deep oceans, away from the atmosphere. Because these waters are able to contain a large amount of CO, they have helped slow the rise in atmospheric CO concentrations, thus slowing some aspects of climate change.\n\nClimate change could have different effects on ice melt and brine rejection. Previous studies have suggested that as ice cover thins, it will become a weaker insulator, resulting in larger ice production during the autumn and winter. The consequent increase in winter brine rejection will drive ocean ventilation, and strengthen the inflow of warm Atlantic waters. Studies of the last glacial maximum (LGM) have indicated that a drastic reduction in the production of sea ice and thus reduction of brine rejection, would result in the weakening of the stratification in the global deep oceans and in CO release into the shallow oceans and the atmosphere, triggering global deglaciation.\n\nLife in sea ice is energetically demanding, and sets limits at any hierarchical organizational, and organismic level, ranging from molecules to everything that an organism does. Despite this fact, the brine-containing interstices and pockets found in sea ice host a variety of organisms, including bacteria, autotrophic and heterotrophic protists, microalgae, and metazoa.\n\n"}
{"id": "78634", "url": "https://en.wikipedia.org/wiki?curid=78634", "title": "Carambola", "text": "Carambola\n\nCarambola, or star fruit, is the fruit of \"Averrhoa carambola\", a species of tree native to Indonesia, the Philippines, and throughout Malesia. The fruit is commonly consumed throughout Southeast Asia, the South Pacific, Micronesia, and parts of East Asia. The tree is cultivated throughout non-indigenous tropical areas.\n\nThe fruit has distinctive ridges running down its sides (usually five but can sometimes vary); when cut in cross-section, it resembles a star, hence its name. The entire fruit is edible and is usually eaten out of hand. They may also be used in cooking and can be made into relishes, preserves, and juice drinks.\n\nThe original range of \"Averrhoa carambola\" is believed to be from Sri Lanka or Indonesia, but has been cultivated in the Indian Subcontinent and Southeast Asia for hundreds of years. They remain a local favorite in those areas but have also recently gained popularity in parts of East Asia and Queensland, Australia; as well as in the Pacific Islands, particularly Tahiti, New Caledonia, Papua New Guinea, Hawaii, and Guam. They are cultivated commercially in India, Southeast Asia, southern China, Taiwan, and the southern United States. They are also grown in Central America, the Southwestern United States and Florida, and parts of Africa. In other areas they are usually grown as ornamentals, rather than for consumption.\n\nThe fruit is about in length and is an oval shape. It usually has five prominent longitudinal ridges, but in rare instances it can have as few as four or as many as eight. In cross section, it resembles a star. The skin is thin, smooth, and waxy and turns a light to dark yellow when ripe. The flesh is translucent and light yellow to yellow in color. Each fruit can have 10 to 12 flat light brown seeds about in width and enclosed in gelatinous aril. Once removed from the fruit, they lose viability within a few days.\n\nLike the closely related bilimbi, there are two main types of carambola: the small sour (or tart) type and the larger sweet type. The sour varieties have a higher oxalic acid content than the sweet type. A number of cultivars have been developed in recent years. The most common cultivars grown commercially include the sweet types \"Arkin\" (Florida), \"Dah Pon\" (Taiwan), \"Ma fueng\" (Thailand), \"Maha\" (Malaysia), and \"Demak\" (Indonesia) and the sour types \"Golden Star\", \"Newcomb\", \"Star King\", and \"Thayer\" (all from Florida). Some of the sour varieties like \"Golden Star\" can become sweet if allowed to ripen.\n\nThe entire fruit is edible, including the slightly waxy skin. The flesh is crunchy, firm, and extremely juicy. It does not contain fibers and has a texture similar in consistency to that of grapes. Carambolas are best consumed shortly after they ripen, when they are yellow with a light shade of green or just after all traces of green have disappeared. They will also have brown ridges at the edges and feel firm. Fruits picked while still slightly green will turn yellow in storage at room temperature, but will not increase in sugar content. Overripe carambola will be yellow with brown spots and can become blander in taste and soggier in consistency.\n\nRipe sweet type carambolas are sweet without being overwhelming as they rarely have more than 4% sugar content. They have a tart, sour undertone, and an oxalic acid odor. The taste is difficult to match, but it has been compared to a mix of apple, pear, grape, and citrus family fruits. Unripe star fruits are firmer and sour, and taste like green apples.\n\nRipe carambolas may also be used in cooking. In Southeast Asia, they are usually stewed in cloves and sugar, sometimes with apples. In China, they are cooked with fish. In Australia, they may be cooked as a vegetable, pickled, or made into jams. In Jamaica they are sometimes dried.\n\nUnripe and sour type carambolas can be mixed with other chopped spices to make relishes in Australia. In the Philippines, unripe carambolas are eaten dipped in rock salt. In Thailand, they are cooked together with shrimp.\n\nThe juice from carambolas is also used in iced drinks, particularly the juice of the sour varieties. In the Philippines they can be used as seasoning. In India, the juice is bottled for drinking.\n\nRaw carambola fruit is 91% water, 7% carbohydrates, 1% protein, and has negligible fat (table). In a 100 gram reference amount, raw fruit supplies 31 calories and is mainly devoid of significant micronutrient content, except for vitamin C at 41% of the Daily Value.\n\nCarambolas contain caramboxin and oxalic acid. Both substances are harmful to individuals suffering from kidney failure, kidney stones, or those under kidney dialysis treatment. Consumption by those with kidney failure can produce hiccups, vomiting, nausea, mental confusion, and sometimes death. Caramboxin is a neurotoxin which is structurally similar to phenylalanine, and is a glutamatergic agonist.\n\nLike the grapefruit, carambola is considered to be a potent inhibitor of seven cytochrome P450 isoforms. These enzymes are significant in the first-pass elimination of many medications, and, thus, the consumption of carambola or its juice in combination with certain prescription medications can significantly increase their effective dosage within the body.\n\nThe carambola is a tropical and subtropical fruit which can be grown at elevations up to . It prefers full sun exposure, but requires enough humidity and annual rainfall of at least . It does not have a soil type preference, but requires good drainage.\n\nCarambola trees are planted at least from each other and typically are fertilized three times a year. The tree grows rapidly and typically produces fruit at four or five years of age. The large amount of rain during spring actually reduces the amount of fruit, but, in ideal conditions, carambola can produce from of fruit a year. The carambola tree flowers throughout the year, with main fruiting seasons from April to June and October to December in Malaysia, for example, but fruiting also occurs at other times in some other locales, such as South Florida.\n\nGrowth and leaf responses of container-grown `Arkin' carambola (\"Averrhoa carambola\" L.) trees to long-term exposure of 25%, 50%, or 100% sunlight showed that shading increased rachis length and leaflet area, decreased leaflet thickness, and produced more horizontal branch orientation. \n\nMajor pests are fruit flies, fruit moths, ants, and birds. Crops are also susceptible to frost.\n\nTop producers of carambola in the world market include Australia, Guyana, India, Israel, Malaysia, the Philippines, Taiwan, and the United States. Malaysia is a global leader in star fruit production by volume and ships the product widely to Asia and Europe. Due to concerns over pests and pathogens, however, whole star fruits cannot yet be imported to the US from Malaysia under current United States Department of Agriculture regulations. In the United States, carambolas are grown in tropical and semitropical areas, including Texas, South Carolina, Louisiana, California, Virginia, Florida and Hawaii.\n\nIn the United States, commercial cultivation and broad consumer acceptance of the fruit only dates to the 1970s, attributable to Morris Arkin, a backyard horticulturalist, in Coral Gables, Florida. The 'Arkin' variety represented 98% of the acreage in South Florida in the early 21st century.\n\nThe trees are also grown as ornamentals for their abundant brightly colored and unusually shaped fruits, as well as for their attractive dark green leaves and their lavender to pink flowers. \n\nLike the bilimbi, the juice of the more acidic sour types can be used to clean rusty or tarnished metal (especially brass) as well as bleach rust stains from cloth. They may also be used as a mordant in dyeing.\n\nThe Portuguese word \"carambola\", first known use 1598, was taken from Marathi \"karambal\" derived from Sanskrit \"karmaphala\".\n\nThe carambola is known under different names in different countries. It should not be confused with the closely related bilimbi, with which it shares some common names. It is also called \"star fruit\" in English (including Jamaican English and Philippine English) and literally translates into \"Stjernefrugt\" in Danish, \"Sternfrucht\" in German, and \"Stjärnfrukt\" in Swedish. In Spanish, it is known as \"carambola\", \"carambolo\", \"tamarindo chino\", \"tamarindo culí\", \"balimbín\" (Philippine Spanish, from Tagalog \"balimbing\", plural is \"balimbines\"), and \"fruta china\" (Ecuador). In maritime Southeast Asia, it is known as \"belimbing\" in Indonesian and Malay and \"balimbíng\" or \"saranate\" in Tagalog.\n"}
{"id": "27296809", "url": "https://en.wikipedia.org/wiki?curid=27296809", "title": "Ceroplastic acid", "text": "Ceroplastic acid\n\nCeroplastic acid (or pentatriacontanoic acid) is a saturated aliphatic carboxylic acid.\n\nLike many other carboxylic acids, ceroplastic acid can react with UV curable moiety alcohols to form reactive esters, such as 2-allyloxyethanol.\n\n\n"}
{"id": "5489107", "url": "https://en.wikipedia.org/wiki?curid=5489107", "title": "Coated paper", "text": "Coated paper\n\nCoated paper is paper which has been coated by a mixture of materials or a polymer to impart certain qualities to the paper, including weight, surface gloss, smoothness or reduced ink absorbency. Various materials, including Kaolinite, calcium carbonate, Bentonite, and talc can be used to coat paper for high quality printing used in packaging industry and in magazines. The chalk or china clay is bound to the paper with synthetic viscosifiers, such as styrene-butadiene latexes and natural organic binders such as starch. The coating formulation may also contain chemical additives as dispersants, resins, or polyethylene to give water resistance and wet strength to the paper, or to protect against ultraviolet radiation.\n\n\"Machine-finished coated paper\" (MFC) has a basis weight of 48–80 g/m. They have good surface properties, high print gloss and adequate sheet stiffness. MFC papers are made of 60–85% groundwood or TMP and 15–40% chemical pulp with a total pigment content of 20–30%. The paper can be soft nip calendered or supercalendered. These are often used in paperbacks.\n\n\"Coated fine paper\" or \"woodfree coated paper\" (WFC) are primarily produced for offset printing:\n\n\nOther types of paper coatings include polyethylene or polyolefin extrusion coating, silicone, and wax coating to make paper cups and photographic paper. Biopolymer coatings are available as more sustainable alternatives to common petrochemical coatings like LDPE (see plastic-coated paper) or mylar.\n\nPrinted papers commonly have a top coat of a protective polymer to seal the print, provide scuff resistance, and sometimes gloss. Some coatings are processed by UV curing for stability.\n\nA release liner is a paper (or film) sheet used to prevent a sticky surface from adhering. It is coated on one or both sides with a release agent.\n\nHeat printed papers such as receipts are often coated with estrogenic and carcinogenic poisons, such as BPA. It is possible to check whether a piece of paper is coated, as it will quickly turn deep black when the ember of a cigarette is applied. (see thermal paper)\n\n\n"}
{"id": "695757", "url": "https://en.wikipedia.org/wiki?curid=695757", "title": "Crosswind", "text": "Crosswind\n\nA crosswind is any wind that has a perpendicular component to the line or direction of travel. This affects the aerodynamics of many forms of transport. Moving non-parallel to the wind's direction creates a crosswind component on the object and thus increasing the apparent wind on the object; such use of cross wind travel is used to advantage by sailing craft, kiteboarding craft, power kiting, etc. On the other side, crosswind moves the path of vehicles sideways and can be a hazard.\n\nWhen winds are not parallel to or directly with/against the line of travel, the wind is said to have a crosswind \"component\"; that is, the force can be separated into two vector components: \n\nA vehicle behaves as though it is directly experiencing a lateral effect of the magnitude of the crosswind component only. The crosswind component is computed by multiplying the wind speed by the sine of the angle between the wind and the direction of travel while the headwind component is computed in the same manner, using cosine instead of sine. For example, a 10 knot wind coming at 45 degrees from either side will have a crosswind component of 10 knots × sin(45°) and a head/tailwind component of 10 knots × cos(45°), both equals to 7.07 knots.\n\nTo determine the crosswind component in aviation, aviators frequently refer to a nomograph chart on which the wind speed and angle are plotted, and the crosswind component is read from a reference line. Direction of travel relative to the wind may be left or right, up or down, or oblique.\n\nIn aviation, a crosswind is the component of wind that is blowing across the runway, making landings and take-offs more difficult than if the wind were blowing straight down the runway. If a crosswind is strong enough, it can damage an aircraft's undercarriage upon landing. Crosswinds, sometimes abbreviated as X/WIND, are reported in knots, abbreviated kt, and often use the plural form in expressions such as \"with 40kt crosswinds\". Smaller aircraft are often not limited by their ability to land in a crosswind, but may see their ability to taxi safely reduced.\n\nCrosswinds can also cause difficulty with ground vehicles traveling on wet or slippery roads (snow, ice, standing water, etc.), especially when gusting conditions affect vehicles that have a large side area such as vans, SUVs, and tractor-trailers. This can be dangerous for motorists because of the possible lift force created, causing the vehicle to lose traction or change direction of travel. The safest way for motorists to deal with crosswinds is by reducing their speed to reduce the effect of the lift force and to steer into the direction of the crosswind.\n\nCyclists are also significantly affected by crosswinds. Saving energy by avoiding riding in wind is a major part of the tactics of road bicycle racing, and this particularly applies in crosswinds. In crosswinds, groups of cyclists form 'echelons', rotating from the windward and leeward side. Riders who fail to form part of an echelon will have to work much harder, and can be dropped by the group that they are with. Crosswinds are common on races near the coast, and are often a feature of the Belgian classic one-day races, or flat stages of the Tour de France.\n\n\n"}
{"id": "26128168", "url": "https://en.wikipedia.org/wiki?curid=26128168", "title": "Dharni (unit)", "text": "Dharni (unit)\n\nThe dharni () is an obsolete unit of mass, formerly used in Nepal, of about  seer. It was divided into 2 bisauli (बिसौलि), 4 boṛi (बोड़ि), or 12 pāu (पाउ). The United Nations Statistical Office gave an approximate equivalence of 2.3325 kilograms (5.142 pounds avoirdupois) in 1966.\n\n"}
{"id": "34267284", "url": "https://en.wikipedia.org/wiki?curid=34267284", "title": "Dorobanțu Wind Farm", "text": "Dorobanțu Wind Farm\n\nThe Dorobanţu Wind Farm is wind farm located in Constanţa County, Romania. It has 15 Vestas-V90 wind turbines with a nominal output of around 3 MW each. It delivers up to 45 MW of power, enough to power over 30,000 homes, which required a capital investment of approximately €90 million.\n"}
{"id": "42884381", "url": "https://en.wikipedia.org/wiki?curid=42884381", "title": "Equilibrant force", "text": "Equilibrant force\n\nAn equilibrant force is a force which brings a body into mechanical equilibrium. According to Newton's second law, a body has zero acceleration when the vector sum of all the forces acting upon it is zero. Therefore, an equilibrant force is equal in magnitude and opposite in direction to the resultant of all the other forces acting on a body. The term has been attested since the late 19th century.\n\nSuppose that two known forces are pushing an object and an unknown equilibrant force is acting to maintain that object in a fixed position. One force points to the west and has a magnitude of 10 N, and the other points to the south and has a magnitude of 8.0 N. By the Pythagorean theorem, the resultant of these two forces has a magnitude of approximately 12.8 N, which is also the magnitude of the equilibrant force. The angle of the equilibrant force can be found by trigonometry to be approximately 51 degrees north of east.\n\n"}
{"id": "29050978", "url": "https://en.wikipedia.org/wiki?curid=29050978", "title": "Falklands Conservation", "text": "Falklands Conservation\n\nFalklands Conservation (FC) is a charitable organisation formed to protect the wildlife and the natural environment of the Falkland Islands in the South Atlantic Ocean. It intends to conserve and undertake scientific research in the biosphere of the Falkland Islands and publish the results of the research to inform the public in the field of nature conservation. They also intend to preserve the Falkland Islands heritage and carry out other charitable activities.\n\nThe origins of FC go back to 1979 when a group of naturalists, including Peter Scott, established a UK registered charity, the Falkland Islands Foundation (FIF), to protect the wildlife of the Falklands and its historic shipwrecks. In 1980 another body, the Falkland Islands Trust (FIT) was formed in the Islands. In 1982, following the Falklands War, FIF became a membership-based organisation. With the merger of FIT and FIF in 1991, it was formally launched on 1 August by David Attenborough as Falklands Conservation.\n\nFalklands Conservation has a partnership with BirdLife International, representing the Falkland Islands, and a member of the International Union for Conservation of Nature.\n\n\n"}
{"id": "1295452", "url": "https://en.wikipedia.org/wiki?curid=1295452", "title": "Gas-generator cycle", "text": "Gas-generator cycle\n\nThe gas-generator cycle is a power cycle of a bipropellant rocket engine. Some of the propellant is burned in a gas generator and the resulting hot gas is used to power the engine's pumps. The gas is then exhausted. Because something is \"thrown away\" this type of engine is also known as open cycle.\n\nThere are several advantages to the gas-generator cycle over its counterpart, the staged combustion cycle. The gas generator turbine does not need to deal with the counter pressure of injecting the exhaust into the combustion chamber. This simplifies plumbing and turbine design, and results in a less expensive and lighter engine.\n\nThe main disadvantage is lost efficiency due to discarded propellant. Gas-generator cycles tend to have lower specific impulse than staged combustion cycles. However there are forms of the gas-generator cycle that recycle the exhaust into the nozzle of the rocket engine. This is seen in the F-1 rocket engine used on the Saturn V booster stage. \n\nAs in most cryogenic rocket engines, some of the fuel in a gas-generator cycle may be used to cool the nozzle and combustion chamber (regenerative cooling).\n\nThe ultimate performance of a rocket engine is primarily limited by the ability of the construction materials to withstand the extreme temperatures of rocket combustion processes, as a higher temperature directly increases the local speed of sound that limits exhaust velocity.\nGas-generator combustion engines include the following:\n\nRocket launch systems that use gas-generator combustion engines:\n\n\n"}
{"id": "13017", "url": "https://en.wikipedia.org/wiki?curid=13017", "title": "Gilbert N. Lewis", "text": "Gilbert N. Lewis\n\nGilbert Newton Lewis (October 25 (or 23), 1875 – March 23, 1946) was an American physical chemist known for the discovery of the covalent bond and his concept of electron pairs; his Lewis dot structures and other contributions to valence bond theory have shaped modern theories of chemical bonding. Lewis successfully contributed to thermodynamics, photochemistry, and isotope separation, and is also known for his concept of acids and bases.\n\nG. N. Lewis was born in 1875 in Weymouth, Massachusetts. After receiving his PhD in chemistry from Harvard University and studying abroad in Germany and the Philippines, Lewis moved to California to teach chemistry at the University of California, Berkeley. Several years later, he became the Dean of the college of Chemistry at Berkeley, where he spent the rest of his life. As a professor, he incorporated thermodynamic principles into the chemistry curriculum and reformed chemical thermodynamics in a mathematically rigorous manner accessible to ordinary chemists. He began measuring the free energy values related to several chemical processes, both organic and inorganic.\n\nIn 1916, he also proposed his theory of bonding and added information about electrons in the periodic table of the chemical elements. In 1933, he started his research on isotope separation. Lewis worked with hydrogen and managed to purify a sample of heavy water. He then came up with his theory of acids and bases, and did work in photochemistry during the last years of his life. In 1926, Lewis coined the term \"photon\" for the smallest unit of radiant energy. He was a brother in Alpha Chi Sigma, the professional chemistry fraternity.\n\nThough he was nominated 41 times, G. N. Lewis never won the Nobel Prize in Chemistry. On March 23, 1946, Lewis was found dead in his Berkeley laboratory where he had been working with hydrogen cyanide; many postulated that the cause of his death was suicide. After Lewis' death, his children followed their father's career in chemistry.\n\nLewis was born in 1875 and raised in Weymouth, Massachusetts, where there exists a street named for him, G.N. Lewis Way, off Summer Street. Additionally, the wing of the new Weymouth High School Chemistry department has been named in his honor. Lewis received his primary education at home from his parents, Frank Wesley Lewis, a lawyer of independent character, and Mary Burr White Lewis. He read at age three and was intellectually precocious. In 1884 his family moved to Lincoln, Nebraska, and in 1889 he received his first formal education at the university preparatory school.\n\nIn 1893, after two years at the University of Nebraska, Lewis transferred to Harvard University, where he obtained his B.S. in 1896. After a year of teaching at Phillips Academy in Andover, Lewis returned to Harvard to study with the physical chemist T. W. Richards and obtained his Ph.D. in 1899 with a dissertation on electrochemical potentials. After a year of teaching at Harvard, Lewis took a traveling fellowship to Germany, the center of physical chemistry, and studied with Walther Nernst at Göttingen and with Wilhelm Ostwald at Leipzig. While working in Nernst's lab, Nernst and Lewis apparently developed a lifelong enmity. In the following years, Lewis started to criticize and denounce his former teacher on many occasions, calling Nernst's work on his heat theorem \"a regrettable episode in the history of chemistry\". A friend of Nernst's, , was a member of the Nobel Chemistry Committee. There is evidence that he used the Nobel nominating and reporting procedures to block a Nobel Prize for Lewis in thermodynamics by nominating Lewis for the prize three times, and then using his position as a committee member to write negative reports.\n\nAfter his stay in Nernst's lab, Lewis returned to Harvard in 1901 as an instructor for three more years. He was appointed instructor in thermodynamics and electrochemistry. In 1904 Lewis was granted a leave of absence and became Superintendent of Weights and Measures for the Bureau of Science in Manila, Philippines. The next year he returned to Cambridge, Massachusetts when the Massachusetts Institute of Technology (MIT) appointed him to a faculty position, in which he had a chance to join a group of outstanding physical chemists under the direction of Arthur Amos Noyes. He became an assistant professor in 1907, associate professor in 1908, and full professor in 1911. He left MIT in 1912 to become a professor of physical chemistry and dean of the College of Chemistry at the University of California, Berkeley. Lewis Hall at Berkeley, built in 1948, is named in his honor.\n\nMost of Lewis’ lasting interests originated during his Harvard years. The most important was thermodynamics, a subject in which Richards was very active at that time. Although most of the important thermodynamic relations were known by 1895, they were seen as isolated equations, and had not yet been rationalized as a logical system, from which, given one relation, the rest could be derived. Moreover, these relations were inexact, applying only to ideal chemical systems. These were two outstanding problems of theoretical thermodynamics. In two long and ambitious theoretical papers in 1900 and 1901, Lewis tried to provide a solution. Lewis introduced the thermodynamic concept of activity and coined the term \"fugacity\". His new idea of fugacity, or \"escaping tendency\", was a function with the dimensions of pressure which expressed the tendency of a substance to pass from one chemical phase to another. Lewis believed that fugacity was the fundamental principle from which a system of real thermodynamic relations could be derived. This hope was not realized, though fugacity did find a lasting place in the description of real gases.\n\nLewis’ early papers also reveal an unusually advanced awareness of J. W. Gibbs’s and P. Duhem’s ideas of free energy and thermodynamic potential. These ideas were well known to physicists and mathematicians, but not to most practical chemists, who regarded them as abstruse and inapplicable to chemical systems. Most chemists relied on the familiar thermodynamics of heat (enthalpy) of Berthelot, Ostwald, and Van’t Hoff, and the calorimetric school. Heat of reaction is not, of course, a measure of the tendency of chemical changes to occur, and Lewis realized that only free energy and entropy could provide an exact chemical thermodynamics. He derived free energy from fugacity; he tried, without success, to obtain an exact expression for the entropy function, which in 1901 had not been defined at low temperatures. Richards too tried and failed, and not until Nernst succeeded in 1907 was it possible to calculate entropies unambiguously. Although Lewis’ fugacity-based system did not last, his early interest in free energy and entropy proved most fruitful, and much of his career was devoted to making these useful concepts accessible to practical chemists.\n\nAt Harvard, Lewis also wrote a theoretical paper on the thermodynamics of blackbody radiation in which he postulated that light has a pressure. He later revealed that he had been discouraged from pursuing this idea by his older, more conservative colleagues, who were unaware that Wilhelm Wien and others were successfully pursuing the same line of thought. Lewis’ paper remained unpublished; but his interest in radiation and quantum theory, and (later) in relativity, sprang from this early, aborted effort. From the start of his career, Lewis regarded himself as both chemist and physicist.\n\nAbout 1902 Lewis started to use unpublished drawings of cubical atoms in his lecture notes, in which the corners of the cube represented possible electron positions. Lewis later cited these notes in his classic 1916 paper on chemical bonding, as being the first expression of his ideas.\n\nA third major interest that originated during Lewis’ Harvard years was his valence theory. In 1902, while trying to explain the laws of valence to his students, Lewis conceived the idea that atoms were built up of a concentric series of cubes with electrons at each corner. This “cubic atom” explained the cycle of eight elements in the periodic table and was in accord with the widely accepted belief that chemical bonds were formed by transfer of electrons to give each atom a complete set of eight. This electrochemical theory of valence found its most elaborate expression in the work of Richard Abegg in 1904, but Lewis’ version of this theory was the only one to be embodied in a concrete atomic model. Again Lewis’ theory did not interest his Harvard mentors, who, like most American chemists of that time, had no taste for such speculation. Lewis did not publish his theory of the cubic atom, but in 1916 it became an important part of his theory of the shared electron pair bond.\n\nIn 1916, he published his classic paper on chemical bonding \"The Atom and the Molecule\" in which he formulated the idea of what would become known as the covalent bond, consisting of a shared pair of electrons, and he defined the term odd molecule (the modern term is free radical) when an electron is not shared. He included what became known as Lewis dot structures as well as the cubical atom model. These ideas on chemical bonding were expanded upon by Irving Langmuir and became the inspiration for the studies on the nature of the chemical bond by Linus Pauling.\n\nIn 1908 he published the first of several papers on relativity, in which he derived the mass-energy relationship in a different way from Albert Einstein's derivation.\nIn 1909, he and Richard C. Tolman combined his methods with special relativity.\nIn 1912 Lewis and Edwin Bidwell Wilson presented a major work in mathematical physics that not only applied synthetic geometry to the study of spacetime, but also noted the identity of a spacetime squeeze mapping and a Lorentz transformation.\nIn 1913, he was elected to the National Academy of Sciences. He resigned in 1934, refusing to state the cause for his resignation; it has been speculated that it was due to a dispute over the internal politics of that institution or to the failure of those he had nominated to be elected. His decision to resign may have been sparked by resentment over the award of the 1934 Nobel Prize for chemistry to his student, Harold Urey, for the discovery of deuterium, a prize Lewis almost certainly felt he should have shared for his work on purification and characterization of heavy water.\n\nIn 1923, he formulated the electron-pair theory of acid–base reactions. In this theory of acids and bases, a \"Lewis acid\" is an \"electron-pair acceptor\" and a \"Lewis base\" is an \"electron-pair donor\". This year he also published a monograph on his theories of the chemical bond.\n\nBased on work by J. Willard Gibbs, it was known that chemical reactions proceeded to an equilibrium determined by the free energy of the substances taking part. Lewis spent 25 years determining free energies of various substances. In 1923 he and Merle Randall published the results of this study, which helped formalize modern chemical thermodynamics.\n\nLewis was the first to produce a pure sample of deuterium oxide (heavy water) in 1933 and the first to study survival and growth of life forms in heavy water. By accelerating deuterons (deuterium nuclei) in Ernest O. Lawrence's cyclotron, he was able to study many of the properties of atomic nuclei. During the 1930s, he was mentor to Glenn T. Seaborg, who was retained for post-doctoral work as Lewis' personal research assistant. Seaborg went on to win the 1951 Nobel Prize in Chemistry and have the element seaborgium named in his honor while he was still alive.\n\nIn 1921, Lewis was the first to propose an empirical equation describing the failure of strong electrolytes to obey the law of mass action, a problem that had perplexed physical chemists for twenty years. His empirical equations for what he called ionic strength were later confirmed to be in accord with the Debye–Hückel equation for strong electrolytes, published in 1923.\n\nIn 1924, by studying the magnetic properties of solutions of oxygen in liquid nitrogen, he found that O molecules were formed. This was the first evidence for tetratomic oxygen.\n\nIn 1926, he coined the term \"photon\" for the smallest unit of radiant energy (light). Actually, the outcome of his letter to \"Nature\" was not what he had intended. In the letter, he proposed a photon being a structural element, not energy. He insisted on the need for a new variable, \"the number of photons\". Although his theory differed from the quantum theory of light introduced by Albert Einstein in 1905, his name was adopted for what Einstein had called a light quantum (Lichtquant in German).\n\nOver the course of his career, Lewis published on many other subjects besides those mentioned in this entry, ranging from the nature of light quanta to the economics of price stabilization. \nIn the last years of his life, Lewis and graduate student Michael Kasha, his last research associate, established that phosphorescence of organic molecules involves emission of light from one electron in an excited triplet state (a state in which two electrons have their spin vectors oriented in the \"same\" direction, but in different orbitals) and measured the paramagnetism of this triplet state.\n\nIn 1946, a graduate student found Lewis's lifeless body under a laboratory workbench at Berkeley. Lewis had been working on an experiment with liquid hydrogen cyanide, and deadly fumes from a broken line had leaked into the laboratory. The coroner ruled that the cause of death was coronary artery disease, because of a lack of any signs of cyanosis, but some believe that it may have been a suicide. Berkeley Emeritus Professor William Jolly, who reported the various views on Lewis's death in his 1987 history of UC Berkeley’s College of Chemistry, \"From Retorts to Lasers\", wrote that a higher-up in the department believed that Lewis had committed suicide.\n\nIf Lewis's death was indeed a suicide, a possible explanation was depression brought on by a lunch with Irving Langmuir. Langmuir and Lewis had a long rivalry, dating back to Langmuir's extensions of Lewis's theory of the chemical bond. Langmuir had been awarded the 1932 Nobel Prize in chemistry for his work on surface chemistry, while Lewis had not received the Prize despite having been nominated 41 times. On the day of Lewis's death, Langmuir and Lewis had met for lunch at Berkeley, a meeting that Michael Kasha recalled only years later. Associates reported that Lewis came back from lunch in a dark mood, played a morose game of bridge with some colleagues, then went back to work in his lab. An hour later, he was found dead. Langmuir's papers at the Library of Congress confirm that he had been on the Berkeley campus that day to receive an honorary degree.\n\nOn June 21, 1912, he married Mary Hinckley Sheldon, daughter of a Harvard professor of Romance languages. They had two sons, both of whom became chemistry professors, and a daughter.\n\n\n\n"}
{"id": "44933050", "url": "https://en.wikipedia.org/wiki?curid=44933050", "title": "Gouina Hydroelectric Plant", "text": "Gouina Hydroelectric Plant\n\nThe Gouina Hydroelectric Plant is a run-of-the-river-type hydroelectric installation currently being constructed on Gouina Falls along the Senegal River in Mali. It is located about southeast of Diamou in the Kayes Region. It is the fourth project of the Senegal River Basin Development Authority and its ground-breaking ceremony on 17 December 2013 was attended by the heads of state of each member country. Mauritanian President Mohamed Ould Abdel Aziz laid the foundation stone. Preliminary construction had been suspended due to the 2012 Malian coup d'état and subsequent Northern Mali conflict. The plant is expected to be complete in 2017 and will provide power to Mauritania, Mali and Senegal. The plant will cost US$329 million and the of transmission lines will cost US$65 million. The project is receiving 85 percent of its funding from the Exim Bank of China along with US$1 million from the EU-Africa Infrastructure Trust Fund and US$1.4 million from the International Development Association and European Investment Bank.\n\nIt will have an installed capacity of and will use the outflows of the Manantali Dam upstream to regulation water flow into the plant. A long weir just above the water fall will direct water into a channel which will feed the power house just downstream of the falls. The power house will contain three 46.6 MW Kaplan turbine-generators. The difference in elevation the water fall and weir provide will afford a hydraulic head (water drop) of .\n\n"}
{"id": "2886032", "url": "https://en.wikipedia.org/wiki?curid=2886032", "title": "ISIS neutron source", "text": "ISIS neutron source\n\nISIS Neutron and Muon Source is a pulsed neutron and muon source. It is situated at the Rutherford Appleton Laboratory of the Science and Technology Facilities Council, on the Harwell Science and Innovation Campus in Oxfordshire, United Kingdom. It uses the techniques of muon spectroscopy and neutron scattering to probe the structure and dynamics of condensed matter on a microscopic scale ranging from the subatomic to the macromolecular.\n\nHundreds of experiments are performed every year the facility by researchers from around the world, in diverse science areas such as physics, chemistry, materials engineering, earth sciences, biology and archaeology. \n\nNeutrons are uncharged constituents of atoms and penetrate materials well, deflecting only from the nuclei of atoms. The statistical accumulation of deflected neutrons at different positions beyond the sample can be used to find the structure of a material, and the loss or gain of energy by neutrons can reveal the dynamic behaviour of parts of a sample, for example diffusive processes in solids. At ISIS the neutrons are created by accelerating 'bunches' of protons in a synchrotron, then colliding these with a heavy tungsten metal target, under a constant cooling load to dissipate the heat from the 160 kW proton beam. The impacts cause neutrons to spall off the tungsten atoms, and the neutrons are channelled through guides, or beamlines, to around 20 instruments, each individually optimised for the study of different types of interactions between the neutron beam and matter. The target station and most of the instruments are set in a large hall. Neutrons are a dangerous form of radiation, so the target and beamlines are heavily shielded with concrete.\n\nISIS Neutron and Muon Source produces muons by colliding a fraction of the proton beam with a graphite target, producing pions which decay rapidly into muons, delivered in a spin-polarised beam to sample stations.\n\nISIS Neutron and Muon Source is administered and operated by the Science and Technology Facilities Council (previously CCLRC). The Science and Technology Facilities council, or STFC, is part of UK Research and Innovation. Experimental time is open to academic users from funding countries and is applied for through a twice-yearly 'call for proposals'. Research allocation, or 'beam-time', is allotted to applicants via a peer-review process. Users and their parent institutions do not pay for the running costs of the facility, which are as much as £11,000 per instrument per day. Their transport and living costs used to be refunded whilst carrying out the experiment, but aren't anymore. Most users stay in Ridgeway House, a hotel near the site, or at Cosener's House, an STFC-run conference centre in Abingdon. Over 600 experiments by 1600 users are completed every year.\n\nA large number of support staff operate the facility, aid users, and carry out research, the control room is staffed 24 hours a day, every day of the year. Instrument scientists oversee the running of each instrument and liaise with users, and other divisions provide sample environment, data analysis and computing expertise, maintain the accelerator, and run education programmes. ISIS is also one of the few neutron facilities to have a significant detector group that researches and develops new techniques for collecting data.\n\nAmong the important and pioneering work carried out was the discovery of the structure of high-temperature superconductors and the solid phase of buckminster-fullerene. Other recent developments can be found here.\n\nConstruction for a second target station (TS2) started in 2003, and the first neutrons were delivered to the target on December 14, 2007. TS2 uses low-energy neutrons to study soft condensed matter, biological systems, advanced composites and nanomaterials.\n\nThe International Muon Ionization Cooling Experiment (MICE) runs parasitically off the ISIS proton beam.\n\nThe instruments currently at ISIS Neutron and Muon Source are:\n\n\n\n\nThe source was approved in 1977 for the RAL site on the Harwell campus and recycled components from earlier UK science programmes including the accelerator hall which had previously been occupied by the Nimrod accelerator. The first beam was produced in 1984, and the facility was formally opened by the then Prime Minister Margaret Thatcher in October 1985.\n\nThe name ISIS is not an acronym: it refers to the Ancient Egyptian goddess and the local name for the River Thames. The name was selected for the official opening of the facility in 1985, prior to this it was known as the SNS, or Spallation Neutron Source. The name was considered appropriate as Isis was a goddess who could restore life to the dead, and ISIS made use of equipment previously constructed for the Nimrod and NINA accelerators.\n\nThe second target station was given funding in 2003 by Lord Sainsbury, then science minister, and was completed in 2009, on time and budget, with the opening of 7 instruments. In April 2010, the Science Minister, David Willetts gave a £21 million investment to build 4 new instruments, which are now all in their commissioning phase or fully scheduled instruments. \n\nISIS Neutron and Muon Source was originally expected to have an operational life of 20 years (1985 to 2005), but its continued success led to a process of refurbishment and further investment, intended to advance the facility and extend the life of ISIS through to 2030. \n\nThe final episode of series 1 of the Sparticle Mystery was filmed on site.\n\n"}
{"id": "32721281", "url": "https://en.wikipedia.org/wiki?curid=32721281", "title": "International Gas Union", "text": "International Gas Union\n\nThe International Gas Union (IGU) is a global association aimed at promoting the technical and economic progress of the gas industry. It was founded in 1931. It is registered in Vevey, Switzerland, and its secretariat is located in Barcelona, Spain.\n\nThe International Gas Union (IGU) is a worldwide non-profit organisation aimed at promoting the political, technical and economic progress of the gas industry. The Union has more than 140 members worldwide on all continents, representing approximately 97% of the world gas market. The members of the IGU are national associations and corporations within the gas industry worldwide. The IGU organises the World Gas Conference (WGC) every three years, with the forthcoming WGC taking place in Washington D.C, United States, in June 2018. IGU also organises the global events LNG X and IGRC X every three years, the next being LNG 2019 in Shanghai and IGRC 2020 in Iran respectively. The IGU's working organisation covers all aspects of the gas industry from exploration and production, storage, LNG, distribution and natural gas utilisation in all market segments.\n\nDavid Carroll from GTI acts as President of the IGU for 2015–2018 on behalf of the USA Presidency. Luis Bertran is Secretary General of IGU.\n\nIGU's working organisation covers all aspects of the gas industry, including exploration and production, storage, LNG, distribution and natural gas utilisation in all market segments. IGU promotes technical and economic progress of the gas industry emphasising environmental performance worldwide.\n\n"}
{"id": "21787339", "url": "https://en.wikipedia.org/wiki?curid=21787339", "title": "Kirei board", "text": "Kirei board\n\nKirei(TM) Board is an engineered panel product of the company Kirei USA, constructed from the left-over, post-harvest stalks of the sorghum plant.\n\nIts manufacture is more involved than that of particle board, as the stalks are first woven tightly and then heat-pressed with an adhesive. Designed to be strong, lightweight, environmentally friendly and sustainable, Kirei Board is intended for wall coverings, cabinetry, furniture, flooring and other decorative and finished products.\n\nThe word \"kirei\" (JP 奇麗/きれい;IPA [ki 'rei]) is a Japanese adjective possessing a range of meanings: \"beautiful\", \"clean\", \"pure\" and \"truthful\". \n\nIn some applications, Kirei Board is used as a decorative material in its own right (featuring its visually distinctive grain), while in others it is used as a building wood in cabinets and shelving, then covered with a veneer of another wood. As it is a soft material, Kirei board is not indicated for flooring in high traffic areas. It is not rated as an outdoor structural panel material.\n\n"}
{"id": "1786225", "url": "https://en.wikipedia.org/wiki?curid=1786225", "title": "Leonhard Seppala", "text": "Leonhard Seppala\n\nLeonhard \"Sepp\" Seppala (September 14, 1877 – January 28, 1967) was a Norwegian Sled dog musher who played a pivotal role in the 1925 serum run to Nome and participated in the 1932 Winter Olympics. Seppala introduced the work dogs used by Native Siberians at the time to the American public; the breed came to be known as the Siberian Husky in the English-speaking world. The Leonhard Seppala Humanitarian Award, which honors excellence in sled dog care is named in honour of him.\n\nLeonhard Seppala was born in the village of Skibotn in Storfjord municipality, Troms county, Norway. He was the oldest-born child of father Isak Isaksson Seppälä (of Swedish descent) and Anne Henriksdatter (of Finnish Norwegian Kven descent). His father's family name is of Finnish origin. When Seppala was two years old, his family moved within Troms county to nearby Skjervøy municipality on the island of Skjervøya. While in Skjervøy, his father worked as a blacksmith and fisherman, building up a relatively large estate. Seppala initially followed in his father's footsteps as both a blacksmith and a fisherman. However, he emigrated to Alaska during the Nome gold rush of the 1900s. His friend Jafet Lindeberg had returned from Alaska and convinced Seppala to come to work for his mining company in Nome.\n\nDuring his first winter in Alaska, Seppala became a dogsled driver for Lindeberg's company. He enjoyed the task from his first run, which he recalled clearly for the rest of his life. He expressed pleasure in the rhythmic patter of the dogs' feet and the feeling of the sled gliding along the snow. While most drivers considered a long run, Seppala travelled between and most days. This also meant he worked as long as 12 hours a day. He kept his dogs in form during the summer by having them pull a cart on wheels instead of a sled. It was unusual at that time to keep sled dogs working when the snow thawed, or to spend as much time with them as he did.\n\nIn 1913, Seppala inherited his first team of sled dogs by chance. Lindeberg, his friend and supervisor at Pioneer Mining Company, had brought the puppies from Siberia as a gift for the explorer Roald Amundsen, whom he hoped would use them for his upcoming expedition to the North Pole. Seppala was assigned to train the dogs. \"I literally fell in love with them from the start\", he recalled; \"I could hardly wait for sledding snow to start their training\". When Amundsen cancelled his trip a few weeks after the puppies arrived in Nome, Lindeberg gave them to Seppala.\n\nSeppala made the decision to compete in his first race, the 1914 All Alaska Sweepstakes, at the last minute. Neither he nor his canine freight leader, Suggen, knew the dangerous trail, and when a blizzard suddenly descended on the area during the race, Seppala realized his young dogs had lost the trail and they were all at great risk of death due to the nearby drop-off to the Bering Sea. Indeed, when the whiteout conditions suddenly lifted, Seppala found that he and his team were at the bottom of a hill, racing towards the cliffs along the sea. Fortunately, he succeeded in stopping 20 feet from the drop-off, saving all their lives. However, many of his dogs' paw pads had been shredded and claws broken by the ice-encrusted snow as they clawed their way back to the top of the hill. A few also suffered frostbite. Seppala felt he had abused the dogs' loyalty by putting them in danger of death and injury, and withdrew from the race in shame. He would nurse them back to health for most the remainder of that year; they would not be ready to train again until Autumn.\n\nSeppala's racing career took off the following year, at the 1915 All Alaska Sweepstakes. After a close competition between him and experienced musher Scotty Allan, Seppala defeated him on the fourth day of the race and finished two hours ahead of Allan to win the Sweepstakes. He went on to win the race the following two years, as well, at which point the All Alaska Sweepstakes was suspended until 1983.\n\nA diphtheria outbreak struck Seppala's town of Nome, Alaska in the winter of 1925. Previously unexposed children as well as adults were at risk of dying from the infection. Seppala's only child—an eight-year-old daughter named Sigrid—was also at risk. The only treatment available in 1925 was diphtheria antitoxin serum. However, the town's supply was not only insufficient but also of presumably low efficacy, being past its expiry date. The only practical way to deliver more serum to Nome in the middle of the coldest winter in 20 years was by dog sled. A relay of respected mushers was organized to expedite the delivery, and Seppala (with lead dog Togo) was chosen for the most forbidding part of the trail. \nThe serum would be taken by train to Nenana, and from there relay teams would set out from Nome and Nenana, and meet in the middle at Nulato. The whole trail was 674 miles from Nenana to Nome, and Seppala was initially selected to cover the more than 400 miles from Nome to Nulato and back.\nSeppala's section of trail featured a dangerous shortcut across Norton Sound, which could save the serum a day of travel. It was decided that he was the most qualified of the relay mushers to attempt this shortcut. The ice on Norton Sound was in constant motion due to currents from the sea and the incessant wind. It could range from rough hills of smashed-together ice, to slippery \"glare ice\" polished by the wind, where it was difficult for the dogs to get a foothold to pull the sled. Small cracks in the ice could also suddenly widen, and driver and team could be plunged into the freezing water. If the wind blew from the east, it could reach speeds as high as , flipping over sleds, pushing dogs off-course, and causing a windchill as low as . A sustained east wind could also push the ice out to sea, and a team suddenly caught on a drifting floe could find itself stranded on open water. Seppala had been forced to take the shortcut across the Sound several times in his career; a less-experienced musher was likelier to lose not only his life and the lives of his dogs, but also the urgently needed serum. Seppala would cross the sound each way in the race to deliver the serum.\n\nSeppala set out from Nome on January 28—several days before he was due to meet the relay coming to Nulato. He crossed Norton Sound without incident. Meanwhile, the number of diphtheria cases in Nome continued to climb. To hasten delivery of the serum, additional mushers were added to the relay. However, it was too late to inform Seppala that he would be meeting the relay closer to Nome than had originally been planned. After 3 days and , he came in sight of another relay musher, Henry Ivanoff—but did not realise it. Seppala saw the musher stopped on the trail and having trouble with his dogs, but did not intend to stop and be delayed. Ivanoff had to run after Seppala as he raced past, shouting, \"The serum! The serum! I have it here!\".\n\nWhen the serum passed to Seppala, night was falling and a powerful low-pressure system was moving towards the trail from the Gulf of Alaska. Seppala had to decide whether to risk Norton Sound in high winds in the dark, when he could not see or hear potential warning signs from the ice. However, as going around the ice meant slowing the delivery of the serum by a full day, he chose to go across. While he raced to the roadhouse at Isaac's Point on the opposite shore, gale-force winds dropped the windchill to an estimated . When he arrived there at 8 pm, his dogs were exhausted; they had run that day, much of it against the wind and in brutal cold. However, they could only afford a short rest, and set out again at 2 am.\n\nThe next day, the gale had intensified into a severe blizzard, with blinding snow and winds of at least . Seppala continued the trail across Norton Sound. This meant avoiding rocky cliffs along the shore, but it also exposed him and his team to the dangers of the Sound. Conditions on the ice were perilous, with sudden soft spots in the ice underfoot, or outright open water only a few feet away. Only a few hours after they had crossed it, the ice had broken up altogether and drifted out to sea.\n\nWith Norton Sound behind them, Seppala and his team now faced the final challenge of the trail—climbing an ridge formation that led to the summit of Little McKinley. The trail here was exposed and the steep grade grueling for the dogs, who were sleep-deprived and had already raced over the previous 4.5 days. However, at 3 P.M. that day, Seppala and his team arrived at Golovin and handed over the serum to the next musher. The serum was now only from Nome. Indeed, it arrived there the next day, Monday, February 2, at approximately 5:30 A.M., and was thawed and ready for use by 11 am.\n\nThis emergency delivery, also known as the \"Great Race of Mercy\", is commemorated annually with the Iditarod Trail Sled Dog Race.\n\nAfter the Serum Run, Seppala and some 40 of his dogs toured the \"lower 48\" with an Eskimo handler. His tour ended in January 1927 with the dogsled race at Poland Spring, Maine, where he accepted the challenge to race against Arthur Walden, founder of the New England Sled Dog Club and owner of the famous lead dog \"Chinook.\" Despite a series of time-consuming mishaps on the trail, Seppala won the race against the bigger, slower dogs driven by Walden and his followers. The enthusiasm for sled dog racing in New England together with the Serum Run publicity and the victory over Walden made it possible for Seppala and partner Elizabeth Ricker to establish a Siberian kennel at Poland Spring, Maine. This was the start of the spread of the Siberian Husky breed in the United States and Canada.\nIn 1928, Seppala moved his permanent home to near Fairbanks, Alaska. In 1931 the Seppala–Ricker partnership ended. Sled dog racing was a demonstration event at the Lake Placid Winter Olympic Games in 1932, where Seppala earned a silver in the event. In 1946, he and his wife Constance moved to Seattle, Washington. In 1961 Seppala revisited Fairbanks and other places in Alaska at the invitation of American journalist Lowell Thomas, enjoying a warm reception from the Alaskan people. He and his wife lived in Seattle until his death at the age of eighty nine. His wife Constance died a few years later aged eighty-five, survived by their daughter, Sigrid Hanks. Both are buried in Nome, Alaska.\n\nA street in Nome named Seppala Drive connects the town to its airport. Leonhard Seppolasvei in Tromsø was also named for him. Alaska Airlines has established the Leonhard Seppala Humanitarian Award. In June 1999, a memorial was erected to him in Skibotn.\n\n"}
{"id": "286001", "url": "https://en.wikipedia.org/wiki?curid=286001", "title": "Lipolysis", "text": "Lipolysis\n\nLipolysis is the breakdown of lipids and involves hydrolysis of triglycerides into glycerol and free fatty acids. Predominantly occurring in adipose tissue, lipolysis is used to mobilize stored energy during fasting or exercise. Lipolysis is directly induced in adipocytes by glucagon, epinephrine, norepinephrine, growth hormone, atrial natriuretic peptide, brain natriuretic peptide, and cortisol.\n\nIn adipose tissue, intracellular triglycerides are stored in cytoplasmic lipid droplets. When lipases are phosphorylated, they access lipid droplets and through multiple steps of hydrolysis, breakdown triglycerides into fatty acids and glycerol. Each step of hydrolysis leads to the removal of one fatty acid. The first step and the rate-limiting step of lipolysis is carried out by adipose triglyceride lipase (ATGL). This enzyme catalyzes the hydrolysis of triacylglycerol to diacylglycerol. Subsequently, hormone-sensitive lipase (HSL) catalyzes the hydrolysis of diacylglycerol to monoacylglycerol and monoacylglycerol lipase (MGL) catalyzes the hydrolysis of monoacylglycerol to glycerol. Perilipin 1A is a key protein regulator of lipolysis in adipose tissue. This lipid droplet-associated protein, when deactivated, will prevent the interaction of lipases with triglycerides in the lipid droplet and grasp the ATGL co-activator, comparative gene identification 58 (CGI-58) (a.k.a. ABHD5). When perilipin 1A is phosphorylated by PKA, it releases CGI-58 and it expedites the docking of phosphorylated lipases to the lipid droplet. CGI-58 can be further phosphorylated by PKA to assist in its dispersal to the cytoplasm. In the cytoplasm, CGI-58 can co-activate ATGL. ATGL activity is also impacted by the negative regulator of lipolysis, G0/G1 switch gene 2 (G0S2). When expressed, G0S2 acts as a competitive inhibitor in the binding of CGI-58. Fat-specific protein 27 (FSP-27) (a.k.a. CIDEC) is also a negative regulator of lipolysis. FSP-27 expression is negatively correlated with ATGL mRNA levels.\n\nLipolysis can be regulated through cAMP's binding and activation of protein kinase A (PKA). PKA can phosphorylate lipases, perilipin 1A, and CGI-58 to increase the rate of lipolysis. Catecholamines bind to 7TM receptors (G protein-coupled receptors) on the adipocyte cell membrane, which activate adenylate cyclase. This results in increased production of cAMP, which activates PKA and leads to an increased rate of lipolysis. Insulin counter-regulates this increase in lipolysis when it binds to insulin receptors on the adipocyte cell membrane. Insulin receptors activate insulin-like receptor substrates. These substrates activate phosphoinositide 3-kinases (PI-3K) which then phosphorylate protein kinase B (PKB) (a.k.a. Akt). PKB subsequently phosphorylates phosphodiesterase 3B (PD3B) which converts cAMP, produced by adenylate cyclase, into 5'AMP. Due to the reduced levels of cAMP, insulin decreases the rate of lipolysis. Insulin has additional actions in the mediobasal hypothalamus. It has been shown to suppress lipolysis due to lower sympathetic nervous outflow to white adipose tissue. The regulation of this process involves interactions between insulin receptors and gangliosides present in the neuronal cell membrane.\n\nTriglycerides are transported through the blood to appropriate tissues (adipose, muscle, etc.) by lipoproteins such as Very-Low-Density-Lipoproteins (VLDL). Triglycerides present on the VLDL undergo lipolysis by the cellular lipases of target tissues, which yields glycerol and free fatty acids. Free fatty acids released into the blood are then available for cellular uptake. Free fatty acids not immediately taken up by cells may bind to albumin for transport to surrounding tissues that require energy. Serum albumin is the major carrier of free fatty acids in the blood. The glycerol also enters the bloodstream and is absorbed by the liver or kidney where it is converted to glycerol 3-phosphate by the enzyme glycerol kinase. Hepatic glycerol 3-phosphate is converted mostly into dihydroxyacetonephosphate (DHAP) and then glyceraldehyde 3-phosphate (GA3P) to rejoin the glycolysis and gluconeogenesis pathway.\n\nWhile lipolysis is triglyceride hydrolysis (the process by which triglycerides are broken down), esterification is the process by which triglycerides are formed. Esterification and lipolysis are, in essence, reversals of one another.\n\nCurrently there are four main non-invasive body contouring techniques growing in aesthetic medicine for reducing localized subcutaneous adipose tissue: low-level laser therapy (LLLT), cryolipolysis, radio frequency (RF) and high-intensity focused ultrasound (HIFU).\n"}
{"id": "623465", "url": "https://en.wikipedia.org/wiki?curid=623465", "title": "Metallate", "text": "Metallate\n\nMetallate is the name given to any complex anion containing a metal ligated to several atoms or small groups. The spelling metalate is almost as common.\n\nTypically, the metal will be one of the transition elements and the ligand will be oxygen or another chalcogenide or a cyanide group (though others are known). The chalcogenide metallates are known as oxometallates, thiometallates, selenometallates and tellurometallates; the cyanide metallates are known as cyanometallates.\n\nOxometallates include permanganate (), chromate () and vanadate ( or ).\n\nThiometallates include tetrathiovanadate (), tetrathiomolybdate (), tetrathiotungstate () and similar ions.\n\nCyanometallates include ferricyanide and ferrocyanide.\n\nMetallate is also used as a verb by bioinorganic chemistry to describe the act of adding metal atoms or ions to a site (synthetic ligand or protein).\n"}
{"id": "15685558", "url": "https://en.wikipedia.org/wiki?curid=15685558", "title": "Mithqal", "text": "Mithqal\n\nMithqāl () is a unit of mass equal to 4.5 grams which is mostly used for measuring precious metals, such as gold, and other commodities, like saffron.\n\nThe name was also applied as an alternative term for the gold dinar, a coin that was used throughout much of the Islamic world from the 8th century onward and survived in parts of Africa until the 19th century. The name of Mozambique's currency since 1980, the \"metical\", is derived from \"mithqāl\".\n\nThe word \"mithqāl\" (; “weight, unit of weight”) comes from the Arabic \"thaqala\" (), meaning “to weigh”. Other variants of the unit in English include \"miskal\" (from Persian or Urdu ; \"misqāl\"), \"mithkal, mitkal\" and \"mitqal.\"\n\nIn India, the measurement is known as \"mithqaal\". It contains 4 mashas and 3½ raties (rata'ii; مثقال).\n\nIt is equivalent to 4.25 grams when measuring gold, or 4.5 grams when measuring commodities. It may be more or less than this.\n\nThe mithqāl in another more modern calculation is as follows:\nNakhud is a Baha'i unit of mass used by Baha'u'llah. The mithqāl had originally consisted of 24 nakhuds, but in the Bayán, the collective works of the Babist founder the Báb, this was reduced to 19.\n\n"}
{"id": "887234", "url": "https://en.wikipedia.org/wiki?curid=887234", "title": "Mupirocin", "text": "Mupirocin\n\nMupirocin, sold under the brand name Bactroban among others, is a topical antibiotic useful against superficial skin infections such as impetigo or folliculitis. It may also be used to get rid of methicillin-resistant \"S. aureus\" (MRSA) when present in the nose without symptoms. Due to concerns of developing resistance, use for greater than ten days is not recommended. It is used as a cream or ointment applied to the skin.\nCommon side effects include itchiness and rash at the site of application, headache, and nausea. Long term use may result in increased growth of fungi. Use during pregnancy and breastfeeding appears to be safe. Mupirocin is in the carboxylic acid class of medications. It works by blocking a bacteria's ability to make protein, which usually results in bacterial death.\nMupirocin was initially isolated in 1971 from \"Pseudomonas fluorescens\". It is on the World Health Organization's List of Essential Medicines, the most effective and safe medicines needed in a health system. The wholesale cost in the developing world is about US$2.10 for a 15 g tube. In the United States, a course of treatment costs $25 to $50.\n\nMupirocin is used as a topical treatment for bacterial skin infections, for example, furuncle, impetigo, open wounds, etc. It is also useful in the treatment of superficial methicillin-resistant \"Staphylococcus aureus\" (MRSA) infections. Mupirocin is inactive for most anaerobic bacteria, mycobacteria, mycoplasma, chlamydia, yeast and fungi.\n\nShortly after the clinical use of mupirocin began, strains of \"Staphylococcus aureus\" that were resistant to mupirocin emerged, with nares clearance rates of less than 30% success. Two distinct populations of mupirocin-resistant \"S. aureus\" were isolated. One strain possessed low-level resistance, MuL, (MIC = 8–256 mg/L) and another possessed high-level resistance, MuH, (MIC > 256 mg/L). Resistance in the MuL strains is probably due to mutations in the organism's wild-type isoleucyl-tRNA synthetase. In \"E. coli\" IleRS, a single amino acid mutation was shown to alter mupirocin resistance. MuH is linked to the acquisition of a separate Ile synthetase gene, MupA. Mupirocin is not a viable antibiotic against MuH strains. Other antibiotic agents, such as azelaic acid, nitrofurazone, silver sulfadiazine, and ramoplanin have been shown to be effective against MuH strains.\n\nMost strains of \"Propionibacterium acnes\", a causative agent in the skin disease acne vulgaris, are naturally resistant to mupirocin.\n\nThe mechanism of action of mupirocin differs from other clinical antibiotics, rendering cross-resistance to other antibiotics unlikely. However, the MupA gene may co-transfer with other antibacterial resistance genes. This has been observed already with resistance genes for triclosan, tetracycline, and trimethoprim. It may also result in overgrowth of non-susceptible organisms.\n\nMupirocin reversibly binds to the isoleucyl t-RNA synthetase in \"Staphylococcus aureus\" and \"Streptococcus\", resulting in inhibition of protein synthesis. DNA and cell wall formation are also negatively impacted to a lesser degree. The inhibition of RNA synthesis was shown to be a protective mechanism in response to a lack of one amino acid, isoleucine. \"In vivo\" studies in \"E. coli\" demonstrated that pseudomonic acid inhibits isoleucine t-RNA synthetase. This mechanism of action is shared with furanomycin, an analog of isoleucine.\n\nMupirocin is a mixture of several pseudomonic acids, with pseudomonic acid A (PA-A) constituting greater than 90% of the mixture. Also present in mupirocin are pseudomonic acid B with an additional hydroxyl group at C8, pseudomonic acid C with a double bond between C10 and C11, instead of the epoxide of PA-A, and pseudomonic acid D with a double bond at C4` and C5` in the 9-hydroxy-nonanoic acid portion of mupirocin.\n\nThe 74 kb mupirocin gene cluster contains six multi-domain enzymes and twenty-six other peptides (Table 1). Four large multi-domain type I polyketide synthase (PKS) proteins are encoded, as well as several single function enzymes with sequence similarity to type II PKSs. Therefore, it is believed that mupirocin is constructed by a mixed type I and type II PKS system. The mupirocin cluster exhibits an atypical acyltransferase (AT) organization, in that there are only two AT domains, and both are found on the same protein, MmpC. These AT domains are the only domains present on MmpC, while the other three type I PKS proteins contain no AT domains. The mupirocin pathway also contains several tandem acyl carrier protein doublets or triplets. This may be an adaptation to increase the throughput rate or to bind multiple substrates simultaneously.\n\nPseudomonic acid A is the product of an esterification between the 17C polyketide monic acid and the 9C fatty acid 9-hydroxy-nonanoic acid. The possibility that the entire molecule is assembled as a single polyketide with a Baeyer-Villiger oxidation inserting an oxygen into the carbon backbone has been ruled out because C1 of monic acid and C9' of 9-hydroxy-nonanoic acid are both derived from C1 of acetate.\n\nBiosynthesis of the 17C monic acid unit begins on MmpD (Figure 1). One of the AT domains from MmpC may transfer an activated acetyl group from acetyl-Coenzyme A (CoA) to the first ACP domain. The chain is extended by malonyl-CoA, followed by a SAM-dependent methylation at C12 (see Figure 2 for PA-A numbering) and reduction of the B-keto group to an alcohol. The dehydration (DH) domain in module 1 is predicted to be non-functional due to a mutation in the conserved active site region. Module 2 adds another two carbons by the malonyl-CoA extender unit, followed by ketoreduction (KR) and dehydration. Module three adds a malonyl-CoA extender unit, followed by SAM-dependent methylation at C8, ketoreduction, and dehydration. Module 4 extends the molecule with a malonyl-CoA unit followed by ketoreduction.\n\nAssembly of monic acid is continued by the transfer of the 12C product of MmpD to MmpA. Two more rounds of extension with malonyl-CoA units are achieved by module 5 and 6. Module 5 also contains a KR domain.\n\nThe keto group at C3 is replaced with a methyl group in a multi-step reaction (Figure 3). MupG begins by decarboxylating a malonyl-ACP. The alpha carbon of the resulting acetyl-ACP is linked to C3 of the polyketide chain by MupH. This intermediate is dehydrated and decarboxylated by MupJ and MupK, respectively.\n\nThe formation of the pyran ring requires many enzyme-mediated steps (Figure 4). The double bond between C8 and C9 is proposed to migrate to between C8 and C16. Gene knockout experiments of mupO, mupU, mupV, and macpE have eliminated PA-A production. PA-B production is not removed by these knockouts, demonstrating that PA-B is not created by hydroxylating PA-A. A knockout of mupW eliminated the pyran ring, identifying MupW as being involved in ring formation. It is not known whether this occurs before or after the esterification of monic acid to 9-hydroxy-nonanoic acid.\n\nThe epoxide of PA-A at C10-11 is believed to be inserted after pyran formation by a cytochrome P450 such as MupO. A gene knockout of mupO abolished PA-A production but PA-B, which also conatins the C10-C11 epoxide, remained. This indicates that MupO is either not involved or is not essential for this epoxidation step.\n\nThe nine-carbon fatty acid 9-hydroxy-nonanoic acid (9-HN) is derived as a separate compound and later esterified to monic acid to form pseudomonic acid. C labeled acetate feeding has shown that C1-C6 are constructed with acetate in the canonical fashion of fatty acid synthesis. C7' shows only C1 labeling of acetate, while C8' and C9' show a reversed pattern of 13C labeled acetate. It is speculated that C7-C9 arises from a 3-hydroxypropionate starter unit, which is extended three times with malonyl-CoA and fully reduced to yield 9-HN. It has also been suggested that 9-HN is initiated by 3-hydroxy-3-methylglutaric acid (HMG). This latter theory was not supported by feeding of [3-C] or [3,6-C]-HMG.\n\nIt is proposed that MmpB to catalyzes the synthesis of 9-HN (Figure 5). MmpB contains a KS, KR, DH, 3 ACPs, and a thioesterase (TE) domain. It does not contain an enoyl reductase (ER) domain, which would be required for the complete reduction to the nine-carbon fatty acid. MupE is a single-domain protein that shows sequence similarity to known ER domains and may complete the reaction. It also remains possible that 9-hydroxy-nonanoic acid is derived partially or entirely from outside of the mupirocin cluster.\n\n"}
{"id": "38624912", "url": "https://en.wikipedia.org/wiki?curid=38624912", "title": "NHS Sustainable Development Unit", "text": "NHS Sustainable Development Unit\n\n\"The Sustainable Development Unit (SDU)\" is a British Government Agency with the purpose of embedding the principals of sustainable development, social value and the wider determinants of health across the health and social care system in England. The SDU develops tools, policy and research to help people and organisations to promote sustainable development and adapt to climate change. The SDU is co-funded by NHS England and Public Health England and hosted within NHS England. It has a cross-system advisory group that includes the Department of Health, DEFRA and most of the major national health agencies. The SDU calculated the first comprehensive carbon footprint for the NHS in 2008 and in 2012 for the whole health and social care system. The NHS was the first major organisation to measure its Scope 3 Supply Chain Carbon Emissions in 2008 and recognise that it was impossible to achieve its carbon reduction targets without also working to achieve a significant reduction in supply chain emissions. \n\nSDU staff are based in Cambridge.\n\nNHS SDU was set up in 2008 by the CEO of the NHS Sir David Nicholson.\n\nIt was formed by the NHS in England under the auspices of the Office of the Strategic Health Authorities (OSHA) and originally hosted by NHS East of England and its Chief Executive, Sir Neil McKay.\n\nIn 2013 the SDU dropped 'NHS' from its title when it took on responsibility for the whole health and care sector. It was at this point that it became co-funded by and accountable to both NHS England and Public Health England, and began reporting to a cross-system advisory board with representation from the Department of Health, DEFRA, DH Arm's Length Bodies (including NHS Improvement, NICE, Care Quality Commission, NHS Digital, Health Education England) and key national agencies such as NHS Property Services, The Kings Fund, The NHS Confederation and NHS Providers.\n\n"}
{"id": "4749163", "url": "https://en.wikipedia.org/wiki?curid=4749163", "title": "Neon-sign transformer", "text": "Neon-sign transformer\n\nA neon-sign transformer (NST) is a transformer made for the purpose of powering a neon sign. They convert line voltage from the 120-347 V up to high voltages, in the range of 2 to 15 kV. These transformers supply between 18-30 mA; 60 mA on special order( ) A more general designation would be \"luminous tube transformer\", because many other gases are used in luminous gas discharge tubes. \n\nOlder NSTs are simply iron-cored transformers, usually embedded in asphalt to reduce noise. The core either has a magnetic shunt, or a gap in the iron core, both of which serve to current-limit the output, allowing them to run indefinitely in short-circuit conditions. They can also run indefinitely with no load. Iron cored varieties are quite heavy, for example a 15 kV, 60 mA device may weigh up to 20 kg (44 lb).\nSome newer manufactured iron cored NSTs include a large capacitor in parallel with the output for PFC (power factor correction). This serves to correct the shift in the phase of voltage and current caused by the large inductance of the transformer.\n\nSince the 1990s, manufacturers have been producing switch mode power supplies to power neon signs. These generate the same voltage and current ranges as iron cored transformers, but in much smaller, lighter designs at high frequency (not the common 50–60 Hz). They are gradually replacing iron cored transformers in neon signs.\n\nAll NSTs are designed to produce a high voltage starting pulse to a tube, then limit the current through the tube when it has started. This is opposite of most line transformers, which will produce full voltage to a load even if overloaded, unless the resistance of windings is too great to allow the excess current or until a winding burns out.\n\nBesides the obvious purpose of powering neon signs, iron cored NST's are often used by hobbyists for:\n\n"}
{"id": "31204224", "url": "https://en.wikipedia.org/wiki?curid=31204224", "title": "Nuclear energy in Iran", "text": "Nuclear energy in Iran\n\nNuclear energy in Iran may refer to:\n\n\n"}
{"id": "176752", "url": "https://en.wikipedia.org/wiki?curid=176752", "title": "Overhead line", "text": "Overhead line\n\nAn overhead line or overhead wire is used to transmit electrical energy to trams, trolleybuses or trains. It is known variously as:\n\nIn this article, the generic term \"overhead line\" is used, as used by the International Union of Railways.\n\nAn overhead line is designed on the principle of one or more overhead wires (or rails, particularly in tunnels) situated over rail tracks, raised to a high electrical potential by connection to feeder stations at regular intervals. The feeder stations are usually fed from a high-voltage electrical grid.\n\nElectric trains that collect their current from overhead lines use a device such as a pantograph, bow collector or trolley pole. It presses against the underside of the lowest overhead wire, the contact wire. Current collectors are electrically conductive and allow current to flow through to the train or tram and back to the feeder station through the steel wheels on one or both running rails. Non-electric locomotives (such as diesels) may pass along these tracks without affecting the overhead line, although there may be difficulties with overhead clearance. Alternative electrical power transmission schemes for trains include third rail, ground-level power supply, batteries and electromagnetic induction.\n\nTo achieve good high-speed current collection, it is necessary to keep the contact wire geometry within defined limits. This is usually achieved by supporting the contact wire from a second wire known as the \"\" (in the US & Canada) or \"catenary\" (in the UK). This wire approximates the natural path of a wire strung between two points, a catenary curve, thus the use of \"catenary\" to describe this wire or sometimes the whole system. This wire is attached to the contact wire at regular intervals by vertical wires known as \"droppers\" or \"drop wires\". It is supported regularly at structures, by a pulley, link or clamp. The whole system is then subjected to mechanical tension.\n\nAs the contact wire makes contact with the pantograph, the carbon insert on top of the pantograph is worn down. The \"straight\" wire between the supports cause the contact wire to cross over the whole surface of the pantograph as the train travels around the curve, causing uniform wear and avoiding any notches. On straight track, the contact wire is zigzagged slightly to the left and right of the centre at each successive support so that the pantograph wears evenly. The movement of the contact wire across the head of the pantograph is called the \"sweep\".\n\nThe zigzagging of the overhead line is not required for trolley poles.\n\nDepot areas tend to have only a single wire and are known as \"simple equipment\" or \"trolley wire\". When overhead line systems were first conceived, good current collection was possible only at low speeds, using a single wire. To enable higher speeds, two additional types of equipment were developed:\n\n\nDropper wires traditionally provide physical support of the contact wire without joining the catenary and contact wires electrically. Modern systems use current-carrying droppers, which eliminate the need for separate wires.\n\nThe present transmission system originated about 100 years ago. A simpler system was proposed in the 1970s by the Pirelli Construction Company, consisting of a single wire embedded at each support for of its length in a clipped, extruded aluminum beam with the wire contact face exposed. A somewhat higher tension than used before clipping the beam yielded a deflected profile for the wire that could be easily handled at by a pneumatic servo pantograph with only 3 g acceleration.\n\nFor tramways, a contact wire without a messenger wire is used.\n\nAn electrical circuit requires at least two conductors. Trams and railways use the overhead line as one side of the circuit and the steel rails as the other side of the circuit. For a trolleybus, no rails are available for the return current, as the vehicles use rubber tyres on the road surface. Trolleybuses use a second parallel overhead line for the return, and two trolley-poles, one contacting each overhead wire. The circuit is completed by using both wires. Parallel overhead wires are also used on the rare railways with three-phase AC railway electrification.\n\nCatenary wires are kept in mechanical tension because the pantograph causes mechanical oscillations in the wire and the wave must travel faster than the train to avoid producing standing waves that would cause wire breakage. Tensioning the line makes waves travel faster.\n\nFor medium and high speeds, the wires are generally tensioned by weights or occasionally by hydraulic tensioners. Either method is known as \"auto-tensioning\" (AT) or \"constant tension\" and ensures that the tension is virtually independent of temperature. Tensions are typically between per wire. Where weights are used, they slide up and down on a rod or tube attached to the mast, to prevent them from swaying.\n\nFor low speeds and in tunnels where temperatures are constant, fixed termination (FT) equipment may be used, with the wires terminated directly on structures at each end of the overhead line. The tension is generally about . This type of equipment sags on hot days and is taut on cold days.\n\nWith AT the continuous length of overhead line is limited due to the change in the position of the weights with temperature as the overhead line expands and contracts. This movement is proportional to the tension length, the distance between anchors. Tension length has a maximum. For most 25 kV OHL equipment in the UK, the maximum tension length is 1970m.\n\nAn additional issue with AT equipment is that, if balance weights are attached to both ends, the whole tension length is free to move along the track. To avoid this a midpoint anchor (MPA), close to the centre of the tension length, restricts movement of the messenger/catenary wire by anchoring it; the contact wire and its suspension hangers can move only within the constraints of the MPA. MPAs are sometimes fixed to low bridges, otherwise anchored to vertical catenary poles or portal catenary supports. A tension length can be seen as a fixed centre point, with the two half tension lengths expanding and contracting with temperature.\n\nMost systems include a brake to stop the wires from unravelling completely if a wire breaks or tension is lost. German systems usually use a single large tensioning pulley (basically a ratchet mechanism) with a toothed rim, mounted on an arm hinged to the mast. Normally the downward pull of the weights and the reactive upward pull of the tensioned wires lifts the pulley so its teeth are well clear of a stop on the mast. The pulley can turn freely while the weights move up or down as the wires contract or expand. If tension is lost the pulley falls back toward the mast, and one of its teeth jams against the stop. This stops further rotation, limits the damage, and keeps the undamaged part of the wire intact until it can be repaired. Other systems use various braking mechanisms, usually with multiple smaller pulleys in a block and tackle arrangement.\n\nLines are divided into sections to limit the scope of an outage and to allow maintenance.\n\nTo allow maintenance to the overhead line without having to turn off the entire system, the line is broken into electrically separated portions known as \"sections\". Sections often correspond with tension lengths. The transition from section to section is known as a \"section break\" and is set up so that the vehicle's pantograph is in continuous contact with the wire.\n\nFor bow collectors and pantographs, this is done by having two contact wires run side by side over the length of 3 or 4 wire supports. A new one drops down and the old one rises up, allowing the pantograph to smoothly transfer from one to the other. The two wires do not touch (although the bow collector/pantograph is briefly in contact with both wires). In normal service, the two sections are electrically connected (to different substations if at or near the halfway mark between them) but this can be interrupted for servicing.\n\nOn overhead wires designed for trolley poles this is done by having a neutral section between the wires, requiring an insulator. The driver of the tram or trolleybus must turn off the power when the trolley pole passes through, to prevent arc damage to the insulator.\n\nPantograph-equipped locomotives must not run through a section break when one side is de-energized. The locomotive would become trapped, but as it passes the section break the pantograph briefly shorts the two catenary lines. If the opposite line is de-energized, this voltage transient may trip supply breakers. If the line is under maintenance, injury may occur as the catenary is suddenly energized. Even if the catenary is properly grounded, the arc generated across the pantograph can damage the pantograph, the catenary insulator or both.\n\nSometimes on a larger electrified railway, tramway or trolleybus system it is necessary to power different areas of track from different power grids, without guaranteeing synchronisation of the phases. (Sometimes the sections are powered with different voltages or frequencies.) The grids may be synchronised on a normal basis, but events may interrupt synchronisation. This is not a problem for DC systems, but for AC systems it is highly undesirable to connect unsynchronised grids. A simple section break is insufficient to guard against this as the pantograph briefly connects both sections.\n\nA neutral section or phase break is used, consisting of two section breaks back-to-back with a short section of line that belongs to neither grid. If the two grids are synchronised this stretch of line is energised (by either supply) and trains run through it normally. If the two supplies are not synchronised it is disconnected from the supplies, leaving it electrically dead, isolating the two grids.\n\nIn countries such as France, South Africa and the United Kingdom, a pair of permanent magnets beside the rails at either side of the neutral section operate a bogie-mounted transducer on the train which causes a large electrical circuit-breaker to open and close when the locomotive or the pantograph vehicle of a multiple unit passes over them. In the United Kingdom AWS equipment is used, but with reversed polarity. Lineside signs on the approach to the neutral section warn the driver to shut off traction power and coast through the dead section.\n\nOn the Pennsylvania Railroad, phase breaks were indicated by a position light signal face with all eight radial positions with lenses and no center light. When the phase break was active (the catenary sections out of phase), all lights were lit. The position light signal aspect was originally devised by the Pennsylvania Railroad and was continued by Amtrak and adopted by Metro North. Metal signs were hung from the catenary supports with the letters \"PB\" created by a pattern of drilled holes.\n\nA special category of phase break was developed in America, primarily by the Pennsylvania Railroad. Since its traction power network was centrally supplied and only segmented by abnormal conditions, normal phase breaks were generally not active. Phase breaks that were always activated were known as \"Dead Sections\": they were often used to separate power systems (for example, the Hell's Gate Bridge boundary between Amtrak and Metro North's electrifications) that would never be in-phase. Since a dead section is always dead, no special signal aspect was developed to warn drivers of its presence, and a metal sign with \"DS\" in drilled-hole letters was hung from the catenary supports.\n\nOccasionally gaps may be present in the overhead lines, when switching from one voltage to another or to provide clearance for ships at moveable bridges, as a cheaper alternative for moveable overhead power rails. Electric trains coast across the gaps. To prevent arcing, power must be switched off before reaching the gap. Usually the pantograph must be lowered too.\n\nGiven limited clearance such as in tunnels, the overhead wire may be replaced by a rigid overhead rail. An early example was in the tunnels of the Baltimore Belt Line, where a ∏ section bar (fabricated from three strips of iron and mounted on wood) was used, with the brass contact running inside the groove. When the overhead line was raised in the Simplon Tunnel to accommodate taller rolling stock, a rail was used. A rigid overhead rail may also be used in places where tensioning the wires is impractical, for example on moveable bridges.\nIn a movable bridge that uses a rigid overhead rail, there is a need to transition from the catenary wire system into an overhead conductor rail at the bridge portal (the last post before the movable bridge). For example, the power supply can be done through a catenary wire system near a swing bridge. The catenary wire typically comprises messenger wire (also called catenary wire) and a contact wire where it meets the pantograph. The messenger wire is terminated at the portal, while the contact wire runs into the overhead conductor rail profile at the transition end section before it is terminated at the portal. There is a gap between the overhead conductor rail at the transition end section and the overhead conductor rail that runs across the entire span of the swing bridge. The gap is required for the swing bridge to be opened and closed. To connect the conductor rails together when the bridge is closed, there is another conductor rail section called \"rotary overlap\" that is equipped with a motor. When the bridge is fully closed, the motor of the rotary overlap is operated to turn it from a tilted position into the horizontal position, connecting the conductor rails at the transition end section and the bridge together to supply power.\n\nShort overhead conductor rails are installed at tram stops as for the Combino Supra.\n\nTrams draw their power from a single overhead wire at about 500 to 750 V. Trolleybuses draw from two overhead wires at a similar voltage, and at least one of the trolleybus wires must be insulated from tram wires. This is usually done by the trolleybus wires running continuously through the crossing, with the tram conductors a few centimetres lower. Close to the junction on each side, the tram wire turns into a solid bar running parallel to the trolleybus wires for about half a metre. Another bar similarly angled at its ends is hung between the trolleybus wires, electrically connected above to the tram wire. The tram's pantograph bridges the gap between the different conductors, providing it with a continuous pickup.\n\nWhere the tram wire crosses, the trolleybus wires are protected by an inverted trough of insulating material extending below.\n\nUntil 1946, a level crossing in Stockholm, Sweden connected the railway south of Stockholm Central Station and a tramway. The tramway operated on 600-700 V DC and the railway on 15 kV AC. In the Swiss village of Oberentfelden, the WSB tramway operating at 750 V DC crosses the SBB line at 15 kV AC; there used to be a similar crossing between the WSB and the SBB at Suhr, this has been replaced by an underpass in 2010. Some crossings between tramway/light rail and railways are extant in Germany. In Zürich, Switzerland, VBZ trolleybus line 32 has a level crossing with the 1,200 V DC Uetliberg railway line; at many places, trolleybus lines cross the tramway. In some cities, trolleybuses and trams shared a positive (feed) wire. In such cases, a normal trolleybus frog can be used.\n\nAlternatively, section breaks can be sited at the crossing point, so that the crossing is electrically dead.\n\nMany cities had trams and trolleybuses using trolley poles. They used insulated crossovers, which required tram drivers to put the controller into neutral and coast through. Trolleybus drivers had to either lift off the accelerator or switch to auxiliary power.\n\nIn Melbourne, Victoria, tram drivers put the controller into neutral and coast through section insulators, indicated by insulator markings between the rails.\n\nMelbourne has three level crossings between electrified suburban railways and tram lines. They have mechanical switching arrangements (changeover switch) to switch the 1,500V DC overhead of the railway and the 650V DC of the trams, called a Tram Square. Proposals have been advanced to grade separate these crossings or divert the tram routes.\n\nAthens has two crossings of tram and trolleybus wires, at Vas. Amalias Avenue and Vas. Olgas Avenue, and at Ardittou Street and Athanasiou Diakou Street. They use the above-mentioned solution.\n\nIn Rome, at the crossing between viale Regina Margherita and via Nomentana, tram and trolleybus lines cross: tram on viale Regina Margherita and trolleybus on via Nomentana. The crossing is orthogonal, therefore the typical arrangement was not available.\n\nIn Milan, most tram lines cross its circular trolleybus line once or twice. Trolleybus and tram wires run parallel in streets such as viale Stelvio and viale Tibaldi.\n\nSome railways used two or three overhead lines, usually to carry three-phase current. This is used only on the Gornergrat Railway and Jungfrau Railway in Switzerland, the Petit train de la Rhune in France, and the Corcovado Rack Railway in Brazil. Until 1976, it was widely used in Italy. On these railways, the two conductors are used for two different phases of the three-phase AC, while the rail was used for the third phase. The neutral was not used.\n\nSome three-phase AC railways used three overhead wires. These were an experimental railway line of Siemens in Berlin-Lichtenberg in 1898 (length 1.8 kilometres), the military railway between Marienfelde and Zossen between 1901 and 1904 (length 23.4 kilometres) and an 800-metre-long section of a coal railway near Cologne between 1940 and 1949.\n\nOn DC systems, bipolar overhead lines were sometimes used to avoid galvanic corrosion of metallic parts near the railway, such as on the Chemin de fer de la Mure.\n\nAll systems with multiple overhead lines have high risk of short circuits at switches and therefore tend to be impractical in use, especially when high voltages are used or when trains run through the points at high speed.\n\nThe Sihltal Zürich Uetliberg Bahn has two lines with different electrification. To be able to use different electric systems on shared tracks, the Sihltal line has its overhead wire right above the train, whilst the Uetliberg line has its overhead wire off to one side.\n\nA catenary is a system of overhead wires used to supply electricity to a locomotive, tram (streetcar), or light rail vehicle that is equipped with a pantograph.\n\nUnlike simple overhead wires, in which the uninsulated wire is attached by clamps to closely spaced crosswires supported by poles, catenary systems use at least two wires. The catenary or messenger wire is hung at a specific tension between line structures, and a second wire is held in tension by the messenger wire, attached to it at frequent intervals by clamps and connecting wires known as \"droppers\". The second wire is straight and level, parallel to the rail track, suspended over it as the roadway of a suspension bridge is over water.\n\nCatenary systems are suited to high-speed operations whereas simple wire systems, which are less expensive to build and maintain, are common on light rail or tram (streetcar) lines, especially on city streets. Such vehicles can be fitted with either a pantograph or trolley pole.\n\nThe Northeast Corridor in the United States has catenary over the between Boston, Massachusetts and Washington, D.C. for Amtrak's inter-city trains. Commuter rail agencies including MARC, SEPTA, NJ Transit, and Metro-North Railroad utilize the catenary to provide local service.\n\nIn Cleveland, Ohio the interurban/light rail lines and the heavy rail line use the same overhead wires, due to a city ordinance intended to limit air pollution from the large number of steam trains that passed through Cleveland between the east coast and Chicago. Trains switched from steam to electric locomotives at the Collinwood rail yards about east of Downtown and at Linndale on the west side. When Cleveland constructed its rapid transit (heavy rail) line between the airport, downtown, and beyond, it employed a similar catenary, using electrification equipment left over after railroads switched from steam to diesel. Light and heavy rail share trackage for about along the Cleveland Hopkins International Airport Red (heavy rail) line, Blue and Green interurban/light rail lines between Cleveland Union Terminal and just past East 55th Street station, where the lines separate.\n\nPart of the Boston, Massachusetts Blue Line through the northeast suburbs uses overhead lines, as does the Green Line.\n\nThe height of overhead wiring can create hazards at level crossings, where it may be struck by road vehicles. Warning signs are placed on the approaches, advising drivers of the maximum safe height.\n\nThe wiring in most countries is too low to allow double stack container trains. The Channel Tunnel has an extended height overhead line to accommodate double-height car and truck transporters. India is proposing a network of freight-only lines that would be electrified with extra height wiring and pantographs.\n\nOverhead line equipment can be adversely affected by strong winds causing wires to swing. Power storms can knock the power out with lightning strikes on systems with overhead wires, stopping trains following a power surge.\n\nDuring cold or frosty weather, ice may coat overhead lines. This can result in poor electrical contact between the collector and the overhead line, resulting in electrical arcing and power surges.\n\nOverhead line equipment may require reconstruction of bridges to provide safe electrical clearance.\n\nOverhead equipment, like most electrified systems, requires a greater capital expenditure when building the system than an equivalent non-electric system. While a conventional rail line requires only the grade, ballast, ties and rails, an overhead system also requires a complex system of support structures, lines, insulators, power-control systems and power lines, all of which require maintenance. This makes non-electrical systems more attractive in the short term, although electrical systems can pay for themselves eventually. Also, the added construction and maintenance cost-per-mile makes overhead systems less attractive on long-distance railways, such as those found in North America, where the distances between cities are typically far greater than in Europe. Such long lines require enormous investment in overhead line equipment, and major difficulties confront energizing long portions of overhead wire on a permanent basis, especially in areas where energy demand already outstrips supply.\n\nMany people consider overhead lines to be \"visual pollution\", due to the many support structures and complicated system of wires and cables that fill the air. Such considerations have driven the move towards replacing overhead power and communications lines with buried cables.\n\nThe first tram with overhead lines was presented by Werner von Siemens at the International Electric Exposition in Paris 1881: the installation was removed after that event. In October 1883, the first permanent tram service with overhead lines was on the Mödling and Hinterbrühl Tram in Austria. The trams had bipolar overhead lines, consisting of two U-pipes, in which the pantographs hung and ran like shuttles. In April to June 1882, Siemens had tested a similar system on his Electromote, an early percursor of the trolleybus.\n\nMuch simpler and more functional was an overhead wire in combination with a pantograph borne by the vehicle and pressed at the line from below. This system, for rail traffic with a unipolar line, was invented by Frank J. Sprague in 1888. From 1889 it was used at the Richmond Union Passenger Railway in Richmond, Virginia, pioneering electric traction.\n\n\n"}
{"id": "40119077", "url": "https://en.wikipedia.org/wiki?curid=40119077", "title": "Pan-African Media Alliance on Climate Change", "text": "Pan-African Media Alliance on Climate Change\n\nThe Pan-African Media Alliance for Climate Change (PAMACC) is one of Africa’s associations of environment journalists. It was created in June 2013 at a workshop the PanAfrican Climate Justice Alliance organised for African journalists.\n\nPAMACC's aim is to support journalists to improve their reporting on climate change and has set up a website to share their stories.\n\nIt has regional coordinators, who will encourage journalists to set up national bodies in each country. The coordinator for Southern Africa, Sellina Nkowani from Malawi, has also stated that she wants the alliance to encourage more women journalists to report on climate change.\nThe other regional coordinators are Elias Ngalame from Cameroon (for Central Africa), Atayi Babs Opaluwah from Nigeria (for West Africa) and Kizito Makoye from Tanzania (for East Africa). Isaiah Esipisu from Kenya will act as the continent-wide coordinator.\n"}
{"id": "12687042", "url": "https://en.wikipedia.org/wiki?curid=12687042", "title": "Petroleum Institute", "text": "Petroleum Institute\n\nThe Petroleum Institute (\"PI\") is an engineering university located in Sas Al Nakhl, Abu Dhabi, United Arab Emirates, offers a variety of engineering degrees and is funded by a consortium of national and international oil companies. The goal of the institution is to provide the local oil and gas industry with engineers.\n\nPI was established in 2000 by an Emiri decree under the direction of His Highness Sheikh Khalifa bin Zayed Al Nahyan, ruler of Abu Dhabi and the president of the United Arab Emirates. It is financed and governed by a consortium of five major oil companies: ADNOC, Royal Dutch Shell, BP, Total S.A., and Japan Oil Development Company, a wholly owned subsidiary of INPEX. The university is partnered with the following universities: Colorado School of Mines, Johannes Kepler Universitat Linz, University of Maryland, University of Minnesota, The University of Texas at Austin, Rice University and China University of Petroleum.\n\nCurrently, it is merging with Khalifa University and Masdar Institute of Science and Technology.\n\nThe PI admitted its first students in the Fall of 2001. At present, over 800 undergraduate male students and 100 female students are studying in one of the following five engineering disciplines:\nMechanical Engineering, \nPetroleum Engineering, \nElectrical Engineering, \nChemical Engineering, \nPetroleum Geosciences,\nPolymer science and \nmaterial science.\n\nThe Petroleum Institute also offers graduate level programs.\n\n"}
{"id": "33876683", "url": "https://en.wikipedia.org/wiki?curid=33876683", "title": "Piezoluminescence", "text": "Piezoluminescence\n\nPiezoluminescence is a form of luminescence created by pressure upon certain solids. This phenomenon is characterized by recombination processes involving electrons, holes and impurity ion centres. Some piezoelectric crystals give off a certain amount of piezoluminescence when under pressure. Irradiated salts, such as NaCl, KCl, KBr and polycrystalline chips of LiF (TLD-100), have been found to exhibit piezoluminescent properties. It has also been discovered that ferroelectric polymers exhibit piezoluminescence upon the application of stress.\n\nIn the folk-literature surrounding psychedelic production, DMT, 5-MeO-DMT, and LSD have been reported to exhibit piezoluminescence. As specifically noted in the book \"Acid Dreams\", it is stated that Augustus Owsley Stansley III, one of the most prolific producers of LSD in the 1960s, observed piezoluminescence in the compound's purest form, which observation is confirmed by Alexander Shulgin: \"A totally pure salt, when dry and when shaken in the dark, will emit small flashes of white light.\"\n"}
{"id": "41568", "url": "https://en.wikipedia.org/wiki?curid=41568", "title": "Power factor", "text": "Power factor\n\nIn electrical engineering, the power factor of an AC electrical power system is defined as the ratio of the \"real power\" absorbed by the load to the \"apparent power\" flowing in the circuit, and is a dimensionless number in the closed interval of −1 to 1. A power factor of less than one indicates the voltage and current are not in phase, reducing the instantaneous product of the two. Real power is the instantaneous product of voltage and current and represents the capacity of the electricity for performing work. Apparent power is the product of average current and voltage. Due to energy stored in the load and returned to the source, or due to a non-linear load that distorts the wave shape of the current drawn from the source, the apparent power may be greater than the real power. A negative power factor occurs when the device (which is normally the load) generates power, which then flows back towards the source.\n\nIn an electric power system, a load with a low power factor draws more current than a load with a high power factor for the same amount of useful power transferred. The higher currents increase the energy lost in the distribution system, and require larger wires and other equipment. Because of the costs of larger equipment and wasted energy, electrical utilities will usually charge a higher cost to industrial or commercial customers where there is a low power factor.\n\nPower-factor correction increases the power factor of a load, improving efficiency for the distribution system to which it is attached. Linear loads with low power factor (such as induction motors) can be corrected with a passive network of capacitors or inductors. Non-linear loads, such as rectifiers, distort the current drawn from the system. In such cases, active or passive power factor correction may be used to counteract the distortion and raise the power factor. The devices for correction of the power factor may be at a central substation, spread out over a distribution system, or built into power-consuming equipment.\n\nIn a purely resistive AC circuit, voltage and current waveforms are in step (or in phase), changing polarity at the same instant in each cycle. All the power entering the load is consumed (or dissipated).\n\nWhere reactive loads are present, such as with capacitors or inductors, energy storage in the loads results in a phase difference between the current and voltage waveforms. During each cycle of the AC voltage, extra energy, in addition to any energy consumed in the load, is temporarily stored in the load in electric or magnetic fields, and then returned to the power grid a fraction of the period later.\n\nBecause high voltage alternating current distribution systems are essentially quasi-linear circuit systems subject to continuous daily variation, there is a continuous \"ebb and flow\" of nonproductive power. Non productive power increases the current in the line, potentially to the point of failure.\n\nThus, a circuit with a low power factor will use higher currents to transfer a given quantity of real power than a circuit with a high power factor. A linear load does not change the shape of the waveform of the current, but may change the relative timing (phase) between voltage and current.\n\nElectrical circuits containing dominantly resistive loads (incandescent lamps, heating elements) have a power factor of almost 1.0, but circuits containing inductive or capacitive loads (electric motors, solenoid valves, transformers, fluorescent lamp ballasts, and others) can have a power factor well below 1.\n\nAC power flow has two components:\n\nThese are combined to the complex power (formula_3) expressed volt-amperes (VA). The magnitude of the complex power is the apparent power (formula_4), also expressed in volt-amperes (VA).\n\nThe VA and var are non-SI units mathematically identical to the watt, but are used in engineering practice instead of the watt to state what quantity is being expressed. The SI explicitly disallows using units for this purpose or as the only source of information about a physical quantity as used.\n\nThe power factor is defined as the ratio of real power to apparent power. As power is transferred along a transmission line, it does not consist purely of real power that can do work once transferred to the load, but rather consists of a combination of real and reactive power, called apparent power. The power factor describes the amount of real power transmitted along a transmission line relative to the total apparent power flowing in the line.\n\nOne can relate the various components of AC power by using the power triangle in vector space. Real power extends horizontally in the î direction as it represents a purely real component of AC power. Reactive power extends in the direction of ĵ as it represents a purely imaginary component of AC power. Complex power (and its magnitude, Apparent power) represents a combination of both real and reactive power, and therefore can be calculated by using the vector sum of these two components. We can conclude that the mathematical relationship between these components is:\n\nAs the power factor (i.e. cos \"θ\") increases, the ratio of real power to apparent power (which = cos \"θ\"), increases and approaches unity (1), while the angle \"θ\" decreases and the reactive power decreases. [As cos \"θ\" → 1, its maximum possible value, \"θ\" → 0 and so Q → 0, as the load becomes less reactive and more purely resistive].\n\nAs the power factor decreases, the ratio of real power to apparent power also decreases, as the angle θ increases and reactive power increases.\n\nThere is also a difference between a lagging and leading power factor. The terms refer to whether the phase of the current is leading or lagging the phase of the voltage. A lagging power factor signifies that the load is inductive, as the load will “consume” reactive power, and therefore the reactive component formula_2 is positive as reactive power travels through the circuit and is “consumed” by the inductive load. A leading power factor signifies that the load is capacitive, as the load “supplies” reactive power, and therefore the reactive component formula_2 is negative as reactive power is being supplied to the circuit.\n\nIf θ is the phase angle between the current and voltage, then the power factor is equal to the cosine of the angle, formula_8:\n\nSince the units are consistent, the power factor is by definition a dimensionless number between −1 and 1. When power factor is equal to 0, the energy flow is entirely reactive and stored energy in the load returns to the source on each cycle. When the power factor is 1, all the energy supplied by the source is consumed by the load. Power factors are usually stated as \"leading\" or \"lagging\" to show the sign of the phase angle. Capacitive loads are leading (current leads voltage), and inductive loads are lagging (current lags voltage).\n\nIf a purely resistive load is connected to a power supply, current and voltage will change polarity in step, the power factor will be 1, and the electrical energy flows in a single direction across the network in each cycle. Inductive loads such as induction motors (any type of wound coil) consume reactive power with current waveform lagging the voltage. Capacitive loads such as capacitor banks or buried cable generate reactive power with current phase leading the voltage. Both types of loads will absorb energy during part of the AC cycle, which is stored in the device's magnetic or electric field, only to return this energy back to the source during the rest of the cycle.\n\nFor example, to get 1 kW of real power, if the power factor is unity, 1 kVA of apparent power needs to be transferred (1 kW ÷ 1 = 1 kVA). At low values of power factor, more apparent power needs to be transferred to get the same real power. To get 1 kW of real power at 0.2 power factor, 5 kVA of apparent power needs to be transferred (1 kW ÷ 0.2 = 5 kVA). This apparent power must be produced and transmitted to the load, and is subject to the losses in the production and transmission processes.\n\nElectrical loads consuming alternating current power consume both real power and reactive power. The vector sum of real and reactive power is the apparent power. The presence of reactive power causes the real power to be less than the apparent power, and so, the electric load has a power factor of less than 1.\n\nA negative power factor (0 to −1) can result from returning power to the source, such as in the case of a building fitted with solar panels when surplus power is fed back into the supply.\n\nA high power factor is generally desirable in a power delivery system to reduce losses and improve voltage regulation at the load. Compensating elements near an electrical load will reduce the apparent power demand on the supply system. Power factor correction may be applied by an electric power transmission utility to improve the stability and efficiency of the network. Individual electrical customers who are charged by their utility for low power factor may install correction equipment to increase their power factor so as to reduce costs.\n\nPower factor correction brings the power factor of an AC power circuit closer to 1 by supplying or absorbing reactive power, adding capacitors or inductors that act to cancel the inductive or capacitive effects of the load, respectively. In the case of offsetting the inductive effect of motor loads, capacitors can be locally connected. These capacitors help to generate reactive power to meet the demand of the inductive loads. This will keep that reactive power from having to flow all the way from the utility generator to the load. In the electricity industry, inductors are said to consume reactive power and capacitors are said to supply it, even though reactive power is just energy moving back and forth on each AC cycle.\n\nThe reactive elements in power factor correction devices can create voltage fluctuations and harmonic noise when switched on or off. They will supply or sink reactive power regardless of whether there is a corresponding load operating nearby, increasing the system's no-load losses. In the worst case, reactive elements can interact with the system and with each other to create resonant conditions, resulting in system instability and severe overvoltage fluctuations. As such, reactive elements cannot simply be applied without engineering analysis.\n\nAn automatic power factor correction unit consists of a number of capacitors that are switched by means of contactors. These contactors are controlled by a regulator that measures power factor in an electrical network. Depending on the load and power factor of the network, the power factor controller will switch the necessary blocks of capacitors in steps to make sure the power factor stays above a selected value.\n\nInstead of using a set of switched capacitors, an unloaded synchronous motor can supply reactive power. The reactive power drawn by the synchronous motor is a function of its field excitation. This is referred to as a synchronous condenser. It is started and connected to the electrical network. It operates at a leading power factor and puts vars onto the network as required to support a system's voltage or to maintain the system power factor at a specified level.\n\nThe synchronous condenser's installation and operation are identical to large electric motors. Its principal advantage is the ease with which the amount of correction can be adjusted; it behaves like a variable capacitor. Unlike capacitors, the amount of reactive power supplied is proportional to voltage, not the square of voltage; this improves voltage stability on large networks. Synchronous condensers are often used in connection with high-voltage direct-current transmission projects or in large industrial plants such as steel mills.\n\nFor power factor correction of high-voltage power systems or large, fluctuating industrial loads, power electronic devices such as the Static VAR compensator or STATCOM are increasingly used. These systems are able to compensate sudden changes of power factor much more rapidly than contactor-switched capacitor banks, and being solid-state require less maintenance than synchronous condensers.\n\nExamples of non-linear loads on a power system are rectifiers (such as used in a power supply), and arc discharge devices such as fluorescent lamps, electric welding machines, or arc furnaces. Because current in these systems is interrupted by a switching action, the current contains frequency components that are multiples of the power system frequency. Distortion power factor is a measure of how much the harmonic distortion of a load current decreases the average power transferred to the load.\nIn linear circuits having only sinusoidal currents and voltages of one frequency, the power factor arises only from the difference in phase between the current and voltage. This is \"displacement power factor\".\n\nNon-linear loads change the shape of the current waveform from a sine wave to some other form. Non-linear loads create harmonic currents in addition to the original (fundamental frequency) AC current. This is of importance in practical power systems that contain non-linear loads such as rectifiers, some forms of electric lighting, electric arc furnaces, welding equipment, switched-mode power supplies, variable speed drives and other devices. Filters consisting of linear capacitors and inductors can prevent harmonic currents from entering the supplying system.\n\nTo measure the real power or reactive power, a wattmeter designed to work properly with non-sinusoidal currents must be used.\n\nThe \"distortion power factor\" is the distortion component associated with the harmonic voltages and currents present in the system.\nformula_11 is the total harmonic distortion of the load current. \n\nformula_13 is the fundamental component of the current and formula_14 is the total current – both are root mean square-values (distortion power factor can also be used to describe individual order harmonics, using the corresponding current in place of total current). This definition with respect to total harmonic distortion assumes that the voltage stays undistorted (sinusoidal, without harmonics). This simplification is often a good approximation for stiff voltage sources (not being affected by changes in load downstream in the distribution network). Total harmonic distortion of typical generators from current distortion in the network is on the order of 1–2%, which can have larger scale implications but can be ignored in common practice.\n\nThe result when multiplied with the displacement power factor (DPF) is the overall, true power factor or just power factor (PF):\n\nIn practice, the local effects of distortion current on devices in a three-phase distribution network rely on the magnitude of certain order harmonics rather than the total harmonic distortion.\n\nFor example, the triplen, or zero-sequence, harmonics (3rd, 9th, 15th, etc.) have the property of being in-phase when compared line-to-line. In a delta-wye transformer, these harmonics can result in circulating currents in the delta windings and result in greater resistive heating. In a wye-configuration of a transformer, triplen harmonics will not create these currents, but they will result in a non-zero current in the neutral wire. This could overload the neutral wire in some cases\nand create error in kilowatt-hour metering systems and billing revenue. The presence of current harmonics in a transformer also result in larger eddy currents in the magnetic core of the transformer. Eddy current losses generally increase as the square of the frequency, lowering the transformer's efficiency, dissipating additional heat, and reducing its service life.\n\nNegative-sequence harmonics (5th, 11th, 17th, etc.) combine 120 degrees out of phase, similarly to the fundamental harmonic but in a reversed sequence. In generators and motors, these currents produce magnetic fields which oppose the rotation of the shaft and sometimes result in damaging mechanical vibrations.\n\nA particularly important class of non-linear loads is the millions of personal computers that typically incorporate switched-mode power supplies (SMPS) with rated output power ranging from a few watts to more than 1 kW. Historically, these very-low-cost power supplies incorporated a simple full-wave rectifier that conducted only when the mains instantaneous voltage exceeded the voltage on the input capacitors. This leads to very high ratios of peak-to-average input current, which also lead to a low distortion power factor and potentially serious phase and neutral loading concerns.\n\nA typical switched-mode power supply first converts the AC mains to a DC bus by means of a bridge rectifier or a similar circuit. The output voltage is then derived from this DC bus. The problem with this is that the rectifier is a non-linear device, so the input current is highly non-linear. That means that the input current has energy at harmonics of the frequency of the voltage.\n\nThis presents a particular problem for the power companies, because they cannot compensate for the harmonic current by adding simple capacitors or inductors, as they could for the reactive power drawn by a linear load. Many jurisdictions are beginning to legally require power factor correction for all power supplies above a certain power level.\n\nRegulatory agencies such as the EU have set harmonic limits as a method of improving power factor. Declining component cost has hastened implementation of two different methods. To comply with current EU standard EN61000-3-2, all switched-mode power supplies with output power more than 75 W must include passive power factor correction, at least. 80 Plus power supply certification requires a power factor of 0.9 or more.\n\nThe simplest way to control the harmonic current is to use a filter that passes current only at line frequency (50 or 60 Hz). The filter consists of capacitors or inductors, and makes a non-linear device look more like a linear load. An example of passive PFC is a valley-fill circuit.\n\nA disadvantage of passive PFC is that it requires larger inductors or capacitors than an equivalent power active PFC circuit. Also, in practice, passive PFC is often less effective at improving the power factor.\n\nActive PFC is the use of power electronics to change the waveform of current drawn by a load to improve the power factor. Some types of the active PFC are buck, boost, buck-boost and synchronous condenser. Active power factor correction can be single-stage or multi-stage.\n\nIn the case of a switched-mode power supply, a boost converter is inserted between the bridge rectifier and the main input capacitors. The boost converter attempts to maintain a constant DC bus voltage on its output while drawing a current that is always in phase with and at the same frequency as the line voltage. Another switched-mode converter inside the power supply produces the desired output voltage from the DC bus. This approach requires additional semiconductor switches and control electronics, but permits cheaper and smaller passive components. It is frequently used in practice.\n\nFor a three-phase SMPS, the Vienna rectifier configuration may be used to substantially improve the power factor.\n\nSMPSs with passive PFC can achieve power factor of about 0.7–0.75, SMPSs with active PFC, up to 0.99 power factor, while a SMPS without any power factor correction have a power factor of only about 0.55–0.65.\n\nDue to their very wide input voltage range, many power supplies with active PFC can automatically adjust to operate on AC power from about 100 V (Japan) to 240 V (Europe). That feature is particularly welcome in power supplies for laptops.\n\nDynamic power factor correction (DPFC), sometimes referred to as \"real-time power factor correction,\" is used for electrical stabilization in cases of rapid load changes (e.g. at large manufacturing sites). DPFC is useful when standard power factor correction would cause over or under correction. DPFC uses semiconductor switches, typically thyristors, to quickly connect and disconnect capacitors or inductors to improve power factor.\n\nPower factors below 1.0 require a utility to generate more than the minimum volt-amperes necessary to supply the real power (watts). This increases generation and transmission costs. For example, if the load power factor were as low as 0.7, the apparent power would be 1.4 times the real power used by the load. Line current in the circuit would also be 1.4 times the current required at 1.0 power factor, so the losses in the circuit would be doubled (since they are proportional to the square of the current). Alternatively, all components of the system such as generators, conductors, transformers, and switchgear would be increased in size (and cost) to carry the extra current. When the power factor is close to unity, for the same KVA rating of the transformer more load can be connected. \n\nUtilities typically charge additional costs to commercial customers who have a power factor below some limit, which is typically 0.9 to 0.95. Engineers are often interested in the power factor of a load as one of the factors that affect the efficiency of power transmission.\n\nWith the rising cost of energy and concerns over the efficient delivery of power, active PFC has become more common in consumer electronics. Current Energy Star guidelines for computers call for a power factor of ≥ 0.9 at 100% of rated output in the PC's power supply. According to a white paper authored by Intel and the U.S. Environmental Protection Agency, PCs with internal power supplies will require the use of active power factor correction to meet the ENERGY STAR 5.0 Program Requirements for Computers.\n\nIn Europe, EN 61000-3-2 requires power factor correction be incorporated into consumer products.\n\nSmall customers, such as households, are not usually charged for reactive power and so power factor metering equipment for such customers will not be installed.\n\nThe power factor in a single-phase circuit (or balanced three-phase circuit) can be measured with the wattmeter-ammeter-voltmeter method, where the power in watts is divided by the product of measured voltage and current. The power factor of a balanced polyphase circuit is the same as that of any phase. The power factor of an unbalanced poly phase circuit is not uniquely defined.\n\nA direct reading power factor meter can be made with a moving coil meter of the electrodynamic type, carrying two perpendicular coils on the moving part of the instrument. The field of the instrument is energized by the circuit current flow. The two moving coils, A and B, are connected in parallel with the circuit load. One coil, A, will be connected through a resistor and the second coil, B, through an inductor, so that the current in coil B is delayed with respect to current in A. At unity power factor, the current in A is in phase with the circuit current, and coil A provides maximum torque, driving the instrument pointer toward the 1.0 mark on the scale. At zero power factor, the current in coil B is in phase with circuit current, and coil B provides torque to drive the pointer towards 0. At intermediate values of power factor, the torques provided by the two coils add and the pointer takes up intermediate positions.\n\nAnother electromechanical instrument is the polarized-vane type. In this instrument a stationary field coil produces a rotating magnetic field, just like a polyphase motor. The field coils are connected either directly to polyphase voltage sources or to a phase-shifting reactor if a single-phase application. A second stationary field coil, perpendicular to the voltage coils, carries a current proportional to current in one phase of the circuit. The moving system of the instrument consists of two vanes that are magnetized by the current coil. In operation the moving vanes take up a physical angle equivalent to the electrical angle between the voltage source and the current source. This type of instrument can be made to register for currents in both directions, giving a four-quadrant display of power factor or phase angle.\n\nDigital instruments exist that directly measure the time lag between voltage and current waveforms. Low-cost instruments of this type measure the peak of the waveforms. More sophisticated versions measure the peak of the fundamental harmonic only, thus giving a more accurate reading for phase angle on distorted waveforms. Calculating power factor from voltage and current phases is only accurate if both waveforms are sinusoidal.\n\nPower Quality Analyzers, often referred to as Power Analyzers, make a digital recording of the voltage and current waveform (typically either one phase or three phase) and accurately calculate true power (watts), apparent power (VA) power factor, AC voltage, AC current, DC voltage, DC current, frequency, IEC61000-3-2/3-12 Harmonic measurement, IEC61000-3-3/3-11 flicker measurement, individual phase voltages in delta applications where there is no neutral line, total harmonic distortion, phase and amplitude of individual voltage or current harmonics, etc.\n\nEnglish-language power engineering students are advised to remember:\n\"ELI the ICE man\" or \"ELI on ICE\" – the voltage E leads the current I in an inductor L; the current leads the voltage in a capacitor C.\n\nAnother common mnemonic is CIVIL – in a capacitor (C) the current (I) leads voltage (V), voltage (V) leads current (I) in an inductor (L).\n\n"}
{"id": "9175375", "url": "https://en.wikipedia.org/wiki?curid=9175375", "title": "Prandtl–Meyer function", "text": "Prandtl–Meyer function\n\nIn aerodynamics, the Prandtl–Meyer function describes the angle through which a flow can turn isentropically for the given initial and final Mach number. It is the maximum angle through which a sonic (\"M\" = 1) flow can be turned around a convex corner. For an ideal gas, it is expressed as follows,\n\nwhere formula_2 is the Prandtl–Meyer function, formula_3 is the Mach number of the flow and formula_4 is the ratio of the specific heat capacities.\n\nBy convention, the constant of integration is selected such that formula_5\n\nAs Mach number varies from 1 to formula_6, formula_2 takes values from 0 to formula_8, where\n\nwhere, formula_10 is the absolute value of the angle through which the flow turns, formula_3 is the flow Mach number and the suffixes \"1\" and \"2\" denote the initial and final conditions respectively.\n\n"}
{"id": "2970774", "url": "https://en.wikipedia.org/wiki?curid=2970774", "title": "Radiant flux", "text": "Radiant flux\n\nIn radiometry, radiant flux or radiant power is the radiant energy emitted, reflected, transmitted or received, per unit time, and spectral flux or spectral power is the radiant flux per unit frequency or wavelength, depending on whether the spectrum is taken as a function of frequency or of wavelength. The SI unit of radiant flux is the watt (W), that is the joule per second () in SI base units, while that of spectral flux in frequency is the watt per hertz () and that of spectral flux in wavelength is the watt per metre ()—commonly the watt per nanometre ().\n\nRadiant flux, denoted Φ (\"e\" for \"energetic\", to avoid confusion with photometric quantities), is defined as\nwhere\n\nSpectral flux in frequency, denoted Φ, is defined as\nwhere \"ν\" is the frequency.\n\nSpectral flux in wavelength, denoted Φ, is defined as\nwhere \"λ\" is the wavelength.\n\nOne can show that the radiant flux of a \"surface\" is the flux of the Poynting vector through this surface, hence the name \"radiant flux\":\nwhere\nBut the time-average of the norm of the Poynting vector is used instead, because in radiometry it is the only quantity that radiation detectors are able to measure:\nwhere < • > is the time-average.\n\n"}
{"id": "14554303", "url": "https://en.wikipedia.org/wiki?curid=14554303", "title": "Rozenburg refinery", "text": "Rozenburg refinery\n\nThe Rozenburg refinery is an oil refinery owned by Kuwait Petroleum Europort BV which is a subsidiary of Kuwait Petroleum International, KPI. It is sometimes referred to as the Europort refinery. The refinery capacity is 4 mtpa (80 kbpd) of crude oil. And has a Nelson complexity index of approximately 6.\n\n\n"}
{"id": "34254733", "url": "https://en.wikipedia.org/wiki?curid=34254733", "title": "Sardasht Dam", "text": "Sardasht Dam\n\nThe Sardasht Dam is an embankment dam currently under construction on the Little Zab southeast of Sardasht in the Iranian province of West Azerbaijan. Reconnaissance studies for the dam were completed in 1999 by Moshanir Consulting Engineers Company. When complete, it will be a tall and long rock-fill earth core dam. It will support a hydroelectric power station with an installed capacity of 150 MW and expected annual generation of 482 GWh. The construction contract for the dam was awarded in 2009. Official construction on the dam began in 2011. The river diversion tunnels were complete in November 2012 in a ceremony attended by Iran's Ministry of Energy Majid Namjoo. The dam began to impound its reservoir on 22 June 2017.\n\n"}
{"id": "13460762", "url": "https://en.wikipedia.org/wiki?curid=13460762", "title": "Southern Waste Management Partnership", "text": "Southern Waste Management Partnership\n\nBefore disbanding in 2015, the Southern Waste Management Partnership (SWaMP) coordinated the disposal and handling of municipal waste, including recycling, in the South of Northern Ireland. The local authorities that were covered by SWaMP (before local government in Northern Ireland was re-organised in 2015) included:\n\n\nThe key task of the SWaMP was to establish a 20-year waste disposal contract for the region's future waste management requirements.\n\n"}
{"id": "48206901", "url": "https://en.wikipedia.org/wiki?curid=48206901", "title": "Stellification", "text": "Stellification\n\nStellification is a theoretical process by which a brown dwarf star or Jovian-class planet is turned into a star, or by which the luminosity of dim stars is greatly magnified.\n\nThe fusion reaction of stars is strongly dependent upon temperature. For proton-proton reactions such as found in Earth's sun, the reaction rate scales with the fourth power of temperature (T). For other reactions such as the CNO cycle, the proportionality can be as high as T. Thus, increasing the temperature of the star even a small amount (for example by using reflective solar sails), would create a large increase in power output, resulting in a much higher equilibrium temperature, and therefore luminosity, of the star.\n\nBrown dwarf stars and gas-giant planets do not achieve sustained fusion, as they contain insufficient mass to gravitationally compress the reactants to the degree required to initiate a reaction. If the density of the star or planet could be increased, fusion could be initiated. One such method is to \"seed\" the body with a black hole. Although the black hole would initially start swallowing the body, the huge output of radiation caused by this would resist the flow of further material. The rate of infall is bound by the Eddington limit, which shows that the luminosity of the resultant star (in Watts) would be equal to approximately six times its mass (in kilograms).\n\nIt has been suggested that a black hole could be moved into position by placing an asteroid in orbit around the black hole, and using a mass driver to direct a stream of matter into it. This could be used to move the black hole either via simple conservation of momentum, or by harnassing the power generated as a result. Zubrin (1999) suggests that a luminosity 1/10,000th that of our own sun would be required to create Earth-like temperatures on planets in close orbit to a brown dwarf, thus requiring a black hole with a mass of 6.1 × 1021 kg (about 8% the mass of Earth's moon).\n\nIt is well established that Jovian-class planets consist mostly of hydrogen and helium. It is theorised that concentrations of hydrogen and helium isotopes at certain depths of a gas-giant planet may be sufficient to support a fusion chain reaction, if sufficient energy can be delivered to ignite the reaction. Scientists have proposed that a nuclear warhead, heavily shielded and able to withstand pressures of up to may be able to reach a depth of in the atmosphere of Jupiter, potentially deep enough to reach high concentrations of isotopes and ignite a fusion reaction.\n\n"}
{"id": "35567885", "url": "https://en.wikipedia.org/wiki?curid=35567885", "title": "Steven J. Davis", "text": "Steven J. Davis\n\nSteven J. Davis is an earth system scientist at the University of California, Irvine's Department of Earth System Science and holds a joint appointment in the Department of Civil and Environmental Engineering\n\nFrom 2001-2004, Davis worked as a corporate lawyer at Gray, Cary, Ware & Freidenrich, LLC in Palo Alto, California advising venture-backed start-ups in Silicon Valley (now part of DLA Piper). He received his Ph.D in Geological and Environmental Sciences in 2008 from Stanford University. He then worked as a post-doctoral researcher with Ken Caldeira at the Carnegie Institution for Science's Department of Global Ecology from 2008 to 2012.\n\nDavis received his undergraduate education at the University of Florida in Gainesville, Florida, his Juris Doctor at the University of Virginia School of Law, and his doctorate from Stanford University.\n\nDavis researches embedded emissions of carbon dioxide and air pollution in international trade, energy systems, carbon lock-in, the quantities and causes of greenhouse gas emissions, and the interactions of agriculture and the global carbon cycle.\n\nIn 2015, Davis and his co-authors were awarded the Cozzarelli Prize by the Proceedings of the National Academy of Sciences for a paper they published on the role of China's international trade and air pollution in the United States . In 2018, Davis received the James B. Macelwane Medal of the American Geophysical Union (AGU) for his contributions in developing a science that links global climate change and society, and was simultaneously elected AGU Fellow.\n\n\nDavis co-founded two organizations related to climate change, the Climate Conservancy, a group working to assess and label consumer goods with their carbon footprints, and Near Zero, a non-profit that \"...provides credible, impartial, and actionable assessment with the goal of cutting greenhouse gas emissions to near zero\".\n\nDavis is on the editorial board of Environmental Research Letters.\n"}
{"id": "1721161", "url": "https://en.wikipedia.org/wiki?curid=1721161", "title": "Submarine power cable", "text": "Submarine power cable\n\nA submarine power cable is a transmission cable for carrying electric power below the surface of the water. These are called \"submarine\" because they usually carry electric power beneath salt water (arms of the ocean, seas, straits, etc.) but it is also possible to use submarine power cables beneath fresh water (large lakes and rivers). Examples of the latter exist that connect the mainland with large islands in the St. Lawrence River.\n\nThe purpose of submarine power cables is the transport of electric current at high voltage. The electric core is a concentric assembly of inner conductor, electric insulation and protective layers. Modern three-core cables (e.g. for the connection of offshore wind turbines) often carry optical fibers for data transmission or temperature measurement, in addition to the electrical conductors.\n\nThe conductor is made from copper or aluminum wires, the latter material having a small but increasing market share. Conductor sizes ≤ 1200 are most common, but sizes ≥ 2400 mm have been made occasionally. For voltages ≥ 12 kV the conductors are round. The conductor can be stranded from individual round wires, or can be a single solid wire. In some designs, profiled wires (keystone wires) are laid up to form a round conductor with very small interstices between the wires.\n\nThree different types of electric insulation around the conductor are mainly used today.\nCross-linked polyethylene (XLPE) is used up to 420 kV system voltage. It is produced by extrusion in insulation thickness of up to about 30 mm. 36 kV class cables have only 5.5 – 8 mm insulation thickness. Certain formulations of XLPE insulation can also be used for DC.\nLow-pressure oil-filled cables have an insulation lapped from paper strips. The entire cable core is impregnated with a low-viscosity insulation fluid (mineral oil or synthetic). A central oil channel in the conductor facilitates oil flow when the cable gets warm. Rarely used in submarine cables due to oil pollution risk at cable damage. Is used up to 525 kV.\nMass-impregnated cables have also a paper-lapped insulation but the impregnation compound is highly viscous and does not exit when the cable is damaged. MI insulation can be used for massive HVDC cables up to 525 kV.\nCables ≥ 52 kV are equipped with an extruded lead sheath to prevent water intrusion. No other materials have been accepted so far. The lead alloy is extruded onto the insulation in long lengths (over 50 km is possible). \nIn this stage the product is called cable core. In single-core cables the core is surrounded by a concentric armoring. In three-core cables, three cable cores are laid-up in a spiral configuration before the armoring is applied.\nThe armoring consists most often of steel wires, soaked in bitumen for corrosion protection. Since the alternating magnetic field in ac cables causes losses in the armoring those cables are sometimes equipped with non-magnetic metallic materials (stainless steel, copper, brass).\n\nMost electrical power transmission systems use alternating current (AC), because transformers can easily change voltages as needed. Direct-current transmission requires a converter at each end of a direct current line to interface to an alternating current grid. A system using submarine power cables may be less costly overall if using high-voltage direct current transmission, especially on a long link where the capacitance of the cable would require too much additional charging current. The inner and outer conductors of a cable form the plates of a capacitor, and if the cable is long (on the order of tens of kilometres), the current that flows through this capacitance may be significant compared to the load current. This would require larger, therefore more costly, conductors for a given quantity of usable power to be transmitted.\n\nAlternating-current (AC) submarine cable systems for transmitting lower amounts of three-phase electric power can be constructed with three-core cables in which all three insulated conductors are placed into a single underwater cable. Most offshore-to-shore wind-farm cables are constructed this way.\n\nFor larger amounts of transmitted power, the AC systems are composed of three separate single-core underwater cables, each containing just one insulated conductor and carrying one phase of the three phase electric current. A fourth identical cable is often added in parallel with the other three, simply as a spare in case one of the three primary cables is damaged and needs to be replaced. This damage can happen, for example, from a ship's anchor carelessly dropped onto it. The fourth cable can substitute for any one of the other three, given the proper electrical switching system.\n\n\n\n\n[[Category:Submarine power cables| ]]\n[[Category:Electric power transmission]]\n\n[[de:Seekabel#Gleichstromkabel]]"}
{"id": "48507824", "url": "https://en.wikipedia.org/wiki?curid=48507824", "title": "Transactive energy", "text": "Transactive energy\n\nTransactive energy refers to the economic and control techniques used to manage the flow or exchange of energy within an existing electric power system in regards to economic and market based standard values of energy. It is a concept that is used in an effort to improve the efficiency and reliability of the power system, pointing towards a more intelligent and interactive future for the energy industry.\n\nTransactive energy promotes a network environment for distributed energy nodes as opposed to the traditional hierarchical grid structure. The network structure allows for communication such that all levels of energy generation and consumption are able to interact with one another, a concept that is also known as interoperability. In transactive energy, interoperability refers to the ability of involved systems to connect and exchange energy information while maintaining workflow and utility constraints. The network is exponentially more complex than traditional control of generating sources because the demand side of the grid offers millions points of control in contrast with an average 10 to 20 power plant points of control on the supply side.\n\nThe Pacific Northwest Demonstration Project is a 5-year U.S. Department of Energy (DOE) funded research and development project created for the purpose of exploring transactive energy concepts at the regional scale that was completed in June 2015. The project participants included 11 utilities, two universities, and multiple technology companies to span five Pacific Northwest states: Washington, Oregon, Idaho, Montana, and Wyoming.\n\nThe project evaluated 55 different technologies that could help reduce energy use and power bills, including smart meters, advanced energy storage, and voltage controls. It also tested and determined the potential benefits of transactive controls within a regional power grid. Transactive control is a technology developed by the Pacific Northwest National Laboratory (PNNL) that entails \"automatic, electronic transactions between energy providers and users about whether or not to sell or buy power.\" In order to test this, transactive signals were used that would exchange information about predicted price and availability of power in real-time. This information was updated every 5 minutes. When peak power demand was predicted, the transactive control was designed to reduce power use. The project confirmed that transactive control technology works and can help improve energy efficiency and reliability, as well as reduce energy cost and encourage renewable energy usage.\n\nPublic involvement was determined as a key parameter for smart grid deployment. Participants of the project emphasized the importance of customer engagement when new technologies are being implemented.\n\nThe results of the project defined the next steps for implementing and improving transactive energy technologies. Several of the project participants have decided to continue smart grid programs on their own, even though the demonstration project is now complete, and new projects have also arisen from the results of the demonstration.\n\nThe GridAgents™ Platform was created by David A. Cohen at Infotility, was initially funded by the U.S. Department of Energy, and implemented early Transactive Energy concepts to manage SmartGrid implementation such as the advanced demonstrations of the Infotility GridAgents software platform being deployed within Con Edison of New York's distribution system. The GridAgents software and applications were installed within Con Edison's 3G: System of the Future (3G/SOF) development efforts. The GridAgents™ software was used for large-scale integration of distributed energy and renewable energy resources into distribution systems with specific applications on SmartGrid based energy networks including Microgrid management, intelligent load control and smart charging applications, including photovoltaics & storage, load control, and future assets such as plug-in hybrid cars. The GridAgents™ Platform is an advanced software foundation (based on multi-agent system technology) designed to work in concert with Service-oriented Architectures and was designed natively to have resource agents and broker agents to allow scalable aggregation of energy-based grid assets enabling them to buy and sell services as well as implement machine learning and AI methods at the system edge. The architecture enables the deployment of communities and eco-systems of intelligent grid resources which can adapt to changing run-time conditions collaborate to solve complex problems, and flexibly accommodate new behavior through the use of plug-in architectures.\n\nThe gridSMART® Demonstration Project was implemented by AEP Ohio from 2009 to 2013. The project tested various new technologies for smart grid implementation on a local level including smart meters, distribution automation, volt-var optimization, consumer programs, plug-in electric vehicles, and smart appliances. AEP utilized Grid Command, a tool that was developed in partnership with Battelle in order to model much of the gridSMART circuit layout. The next steps for the next phase of gridSMART were identified to be upgrading current technologies in order to better manage supply, reduce costs, and minimize the number of customers affected by outages. This has been proposed through the installation of smart meter technology, distribution automation circuit reconfiguration (DACR), and volt var optimization (VVO).\n\nTesting included SMART Shift, a time-of-day rate plan that helps customers save money by load shifting and SMART Cooling, an air-conditioning technology that helps reduce peak demand in the summer. During the project, eView was developed to assist customers in monitoring their electric use and costs as well as estimating current month usage to measure against their energy budget. eView< is an in-home device that communicates with the smart meter through wireless technology and informs the consumer of the price of electricity and how much was being used.\n\nThe project helped AEP Ohio in determining what methods and solutions would best help the company move forward in the growing industry. It was emphasized that customer experience and feedback is a very valuable and effective method of learning how to deliver electricity efficiently to customers.\n\nThe NIST Transactive Energy (TE) Challenge was designed to bring together researchers, companies, utilities and other grid stakeholders in order to explore the modeling and simulation platforms of TE, and the techniques that may be used to apply TE to real grid problems. This challenge is intended to encourage and promote the development of modeling and simulation tools for transactive energy, as well as the development of a transactive energy community in which organizations and individuals can work together to share data and knowledge in order to cooperatively advance transactive energy. It will demonstrate various transactive energy approaches and how it may improve the reliability and efficiency of the electric grid.\n\nVarious teams were formed to explore different pathways for TE: \nThe NIST TE Challenge is expected to be completed in September 2016.\n\nThe goal of the Energy Flexibility Platform and Interface (EF-Pi)\nSmart Grid services from the customer appliances.\n\nThis opens up the markets and gives the customer freedom of choice in Smart Grid services.\nThe End user should be able to combine it with all the connected appliances he already\nowns in his house, without losing control and ownership.\n\nThe EF-Pi is an open-source software platform that runs on low-power hardware located at a\nconvenient place in the building. The EF-Pi communicates directly with smart appliances\ninside the building. The EF-Pi has an easy to use interface, which the end user can use to\nconfigure and control his own appliances and get insight in how his appliances are\nfunctioning.\n\nThe core of the EF-Pi is the Energy Flexibility Interface (EFI). The EFI is a generic interface\nwhich appliance manufacturers can use to describe energy flexibility, and which Smart Grid\nservice developers can use to describe how they want to use this flexibility. The EFI\neffectively provides a common language for both sides, facilitating interoperability between\nall Smart Grid services and smart appliances.\n\nThere are no current global standards to facilitate transactive energy. In the United States, the IEEE has a working group called P825 — Meshing Smart Grid Interoperability Standards to Enable Transactive Energy Networks to develop transactive energy guidelines.\n"}
{"id": "43611202", "url": "https://en.wikipedia.org/wiki?curid=43611202", "title": "Virunga (film)", "text": "Virunga (film)\n\nVirunga is a 2014 British documentary film directed by Orlando von Einsiedel. It focuses on the conservation work of park rangers within the Congo's Virunga National Park during the rise of the violent M23 Rebellion in 2012 and investigates the activity of the British oil company Soco International within the UNESCO World Heritage site. Soco International ended up officially exploring oil opportunities in Virunga in April 2014. The film premiered at the Tribeca Film Festival on 17 April 2014. After airing on Netflix, it was nominated for an Academy Award for Best Documentary Feature.\n\nThe documentary tells the story of four characters fighting to protect Virunga National Park in the Democratic Republic of Congo, home to the world's last mountain gorillas, from war, poaching, and the threat of oil exploration. Following gorilla caregiver André Bauma, central sector warden Rodrigue Mugaruka Katembo, chief warden Emmanuel de Merode, and the French investigative journalist Mélanie Gouby, the film focuses on the natural beauty and biodiversity of Virunga, as well as the complex political and economic issues surrounding oil exploration and armed conflict in the region.\n\nProduction began in 2012, when von Einsiedel traveled to Virunga National Park in the Democratic Republic of Congo with the intention of documenting the positive progress which had been made by the park authorities in encouraging tourism and development in the region. However, within three weeks of arriving in Virunga, conflict began with the M23 rebellion in April 2012, shifting the focus of the film to cover the emerging conflict.\n\nVon Einsiedel collaborated with park officials and French journalist Mélanie Gouby to investigate the role of the British oil company Soco International, which had been undertaking activities in the area. Undercover filming appeared to show Soco representatives offering bribes to park rangers.\n\nSoco International has strongly denied the allegations made in the documentary.\n\n\"Virunga\" had its world premiere at the Tribeca Film Festival in New York City on 17 April 2014. The premiere of the film came just two days after Virunga National Park's chief warden, Emmanuel de Merode, was shot by unidentified gunmen on the road from the city of Goma to the park's headquarters in Rumangabo. De Merode survived the attack, and with his encouragement, the premiere of \"Virunga\" went ahead.\n\nThe film has been screened at multiple film festivals around the world, including Canadian festivals Hot Docs and DOXA; Docville in Leuven, Belgium; in the U.S. at Little Rock Film Festival in Arkansas, Mountainfilm in Colorado, AFI Docs in Washington DC, Traverse City Film Festival in Michigan. The UK premiere of the film was at the Edinburgh International Film Festival on 24 June 2014.\n\nOn 28 July 2014, it was announced that Netflix had picked up exclusive rights to the film. The documentary was released onto the service on 7 November the same year.\n\n\"Virunga\" has received universal acclaim by critics. Film review aggregator Rotten Tomatoes reports that 100% of critics gave the film a positive, based on 18 reviews with an average score of 9/10. Metacritic, another review aggregator, assigned the film a weighted average score of 95 out of 100 based on 5 reviews from mainstream critics, indicating \"universal acclaim\". It is currently one of the site's highest-rated films.\n\nJeannette Catsoulis, writing for \"The New York Times\", called the film \"extraordinary\", whilst Los Angeles Times film critic Sheri Linden described Virunga as an \"urgent investigative report and unforgettable drama... a work of heart-wrenching tenderness and heart-stopping suspense\". Ronnie Scheib wrote for Variety that \"Virunga\" was an \"extraordinary documentary\" with \"enough action, pathos, suspense, venal villains, stalwart heroes and endangered gorillas for a dozen fiction films\". Tom Roston wrote for PBS's POV blog that \"Virunga is the best documentary I've seen this year.\"\n\n\"Virunga\" has won several awards including the Peabody Award; the Feature Documentary Award at DOXA Documentary Festival in Vancouver, Canada; the Award of International Emerging Filmmaker at Hot Docs in Toronto; the Golden Rock Documentary Award at Little Rock Film Festival, the Cinema for Peace International Green Film Award, and the Action and Change Together (ACT Now) Award at the Crested Butte Film Festival. It won two gongs at the One World Media Awards at BAFTA - Best Documentary, and the Corruption Reporting award. The film was also nominated for Best Documentary Feature at Tribeca Film Festival. The film was nominated for the Academy Award for Best Documentary Feature for the 87th Academy Awards.\n\nThe allegations brought against Soco International by the documentary, and supported by local NGOs and civil society organizations working in and around Virunga National park, put increased pressure on the company to put an end to its exploration for oil within the protected World Heritage Site.\n\nOn 11 June 2014, Soco International and the WWF announced a joint statement in which the oil company committed \"not to undertake or commission any exploratory or other drilling within Virunga National Park unless UNESCO and the DRC government agree that such activities are not incompatible with its World Heritage status\". This was widely cited as a victory for WWF, which had long been campaigning for Soco to leave the region, and credit was also given to the filmmakers. However, strong concerns about the credibility of this agreement were raised by the filmmakers, alongside other NGOs such as Global Witnessㄝ Human Rights Watch, and local civil society organisations.\n\nWorld Wildlife Fund executives now acknowledge that the battle over Virunga is hardly over. Soco has yet to relinquish its operating permits or commit to an unconditional withdrawal. \"They're leaving the door open,\" said Zach Abraham, director of the World Wildlife Fund's global campaigns.\n\nOn 13 March 2015, BBC reported that the Democratic Republic of Congo says it could potentially redraw the boundaries of Virunga National Park, to allow for oil exploration.\n\nOn 4 November 2015, Soco said it no longer held a stake in the exploration licence for the DRC national park.\n\n"}
{"id": "54554143", "url": "https://en.wikipedia.org/wiki?curid=54554143", "title": "Voltage symmetrization system", "text": "Voltage symmetrization system\n\nVoltage Symmetrization System (VSS) was developed especially for electric power networks with great phase voltage unbalance. It is a three-phase system which enables earth-fault current and charging current elimination. Until now, the operation with earth-fault current compensation has been very complicated for the networks with a great voltage unbalance. In the past, it was necessary to operate such networks with an isolated neutral, solid grounding or with a significantly untuned arc-suppression coil. VSS can also substitute for an arc-suppression coil or it can complement it parallelly. Moreover, it contributes to a better function of earth-fault protections for high-resistance earth faults by equalizing phase-to-earth capacity unbalance. The VSS was patented.\nThe three-phase VSS has several basic functions:\n\nIf VSS is used for phase voltage unbalance elimination only, it is possible to operate it with a relatively low power. The VSS can effectively eliminate and control the phase voltage unbalance, which is caused by phase-to-ground capacity unbalance. The system enables to automatically maintain phase-to-ground capacity unbalance of the network in the range of assigned parameters, and thus provides the network with symmetrical phase voltage in the assigned range as well. In particular cases, the VSS can be used for artificial increase in phase voltage unbalance of the network, mostly for a short-time period during earth-fault localization in the network.\n\nIf VSS is used for line’s charging current elimination, it is not necessary to install shunt reactor. In single-phase cable lines, the VSS can eliminate the whole charging current because the operational capacity comprises phase-to-ground capacity only. In three-phase and overhead lines, the operational capacity comprises not only of phase-to-ground capacity but also of phase-to-phase capacity. VSS eliminates only charging currents of phase-to-ground capacity in these networks, which stands for 60% to 70% of line’s operational capacity value (VSS is not designated for elimination of phase-to-phase capacity charging current in the lines). VSS contributes to a significant reduction of charging reactive power flow.<br>\n\nThe advantage of the system is that it does not require a neutral to eliminate the earth-fault current. Therefore, the VSS has been implemented especially in networks which were operated with isolated neutral. During an earth fault in the network, the system eliminates earth-fault current. The function is in principle similar to an arc-suppression coil. In comparison to the arc-suppression coil, VSS eliminates earth-fault current more efficiently, even in unbalanced or extremely unbalanced networks.\n\nVSS enables to control the magnitude of residual active earth-fault current. This can be applied to improve the function of directional ground protection or to decrease great active earth-fault current in the network. Such quality is used in networks with great active earth-fault current value, for instance in large cable lines.\n\nAs VSS does not require neutral for its connection, it can be connected anywhere in the network. At the same time, it is possible to adjust its power to a certain part of the network only, which can be use in local distribution networks or in long feeders, in feeders with high value of charging current, or in feeders with high capacity unbalance. The part of the network in which VSS is installed does not increase the value of phase-to-earth capacity current in power supply network. Furthermore, the value of network’s charging current decreases by the set system power. The VSS is very beneficial in networks with island operation. In island operation, the source is not loaded by charging current of the line and the separated island area is operated with resonant grounding. Such qualities cannot be reached with a classic arc-suppression coil.\n\nVSS systems have been installed in industrial 6 kV networks as well as in standard distribution networks up to 35 kV. Operational experience proved that VSS can eliminate even extremely high cross parameters unbalance in the network without negatively influencing the function of earth fault protection. Thanks to elimination of phase capacity unbalance, high reliability of directional ground protection can be reached even with high-resistance earth-faults. The possibility of operating the network with accurately compensated phase-to-earth capacity currents contributes to higher safety of the network operation.\n\nEuropean Patent EP 1817829\n"}
{"id": "25772493", "url": "https://en.wikipedia.org/wiki?curid=25772493", "title": "Yangjiang Nuclear Power Station", "text": "Yangjiang Nuclear Power Station\n\nThe Yangjiang Nuclear Power Station (YNPS; ) is a nuclear power plant in Guangdong province, China. The site is Dongping Town, Yangjiang City in western Guangdong Province. \nThe station will have six 1,000 megawatt (MW) CPR-1000 pressurized water reactors (PWRs).\nThe plant began commercial operation in March 2014, and as of 2018 is the largest nuclear power station in China.\n\nThe CPR-1000 is a PWR design developed by China from the Areva-designed PWRs at the Daya Bay Nuclear Power Plant. \nYangjiang marks a step in the development of China's domestic nuclear industry. \nShu Guogang, GM of China Guangdong Nuclear Power Project said, \"We built 55 percent of Ling Ao Phase 2, 70 percent of Hongyanhe, 80 percent of Ningde and 90 percent of Yangjiang Station.\"\n\nThe site in Yangjiang was selected for nuclear development in 1988. The project was approved in 2004. \nThe plant was originally to be one of the first in China to host Generation III reactors — specifically AP1000 reactors. In 2007 however, plans were revised from the AP1000 design to EPR design. Later in 2007 these plans were again revised, with the EPR designs to be realized at Taishan, and the established CPR-1000 reactor design (as already used at Daya Bay) selected for Yangjiang.\nGround was broken for the plant in February 2008; the first concrete for the first unit was poured on 16 December 2008.\nConstruction of the fourth unit was to begin in March 2011, but was delayed by China's safety review in reaction to the nuclear accident in Japan; the first concrete was poured in November 2012.\n\nYangjiang 5 is the first construction of an ACPR-1000 reactor, starting in September 2013. \nThis design is an evolution to the Generation III level of the CPR-1000, and will include a core catcher and double containment as additional safety measures.\n\n"}
