{"id": "14293829", "url": "https://en.wikipedia.org/wiki?curid=14293829", "title": "Anheuser–Busch Coastal Research Center", "text": "Anheuser–Busch Coastal Research Center\n\nThe Anheuser–Busch Coastal Research Center is a biological field station located in Oyster, Virginia that is operated by the University of Virginia. It is a member of the Organization of Biological Field Stations (OBFS). It serves as the research home for the Virginia Coast Reserve Long-Term Ecological Research Program. A new $2.5 million laboratory and housing facility was dedicated in on August 26, 2006.\n\nThe Center is located within the Virginia Coast Reserve, a biosphere reserve operated by the Nature Conservancy.\n\n"}
{"id": "15994230", "url": "https://en.wikipedia.org/wiki?curid=15994230", "title": "Ballistol", "text": "Ballistol\n\nBallistol (meaning 'Ballistic Oil') is a mineral oil-based chemical which advertises that it has many uses. It was originally intended for cleaning, lubricating, and protecting firearms. \n\nThe product originated from Germany before World War I, after the German military requested an 'all-around' oil and cleaner for their rifles and equipment. The German military used it from 1905 to 1945.\n\nThe chemical is a yellowish clear liquid with a consistency expected of a light oil. However, when it comes in contact with water it emulsifies, becoming a thick creamy white substance. It has a sweet and mildly pungent smell similar to black licorice. It is distributed in liquid and aerosol forms. The aerosol uses butane or propane as a propellant.\n\nIt advertises it has no carcinogens. Some other similar chemicals contain petro-chemicals which can pollute the environment if improperly handled, and can damage the 'seasoning' developed on the bore of a black-powder gun.\n\n\n\n\n"}
{"id": "16381737", "url": "https://en.wikipedia.org/wiki?curid=16381737", "title": "Bertha Rogers", "text": "Bertha Rogers\n\nThe Lone Star Producing Co. 1–27 Bertha Rogers hole or well was an oil-exploratory hole drilled in Washita County, Oklahoma in 1974, and was the world's deepest hole until it was surpassed in 1979 by the Kola Superdeep Borehole, dug by the USSR.\n\nIt took Lone Star a little over a year and a half to reach 31,441 feet (9,583 m), a depth of almost six miles. During drilling, the well encountered enormous pressure – almost 25,000 psi (172,369 kPa). No commercial hydrocarbons were found before drilling hit a molten sulfur deposit, which melted the drill bit. The well was plugged back and completed in the Granite Wash from 11,000 to 13,200 feet as a natural gas producer.\n"}
{"id": "3788586", "url": "https://en.wikipedia.org/wiki?curid=3788586", "title": "Bioconversion", "text": "Bioconversion\n\nBioconversion, also known as \"biotransformation\", is the conversion of organic materials, such as plant or animal waste, into usable products or energy sources by biological processes or agents, such as certain microorganisms. One example is the industrial production of cortisone, which one step is the bioconversion of progesterone to 11-alpha-Hydroxyprogesterone by \"Rhizopus nigricans.\" Another example is the bioconversion of glycerol to 1,3-propanediol, which is part of scientific research for many decades.\n\nAnother example of bioconversion is the conversion of organic materials, such as plant or animal waste, into usable products or energy sources by biological processes or agents, such as certain microorganisms, some detritivores or enzymes.\n\nIn the USA, the Bioconversion Science and Technology group performs multidisciplinary R&D for the Department of Energy's (DOE) relevant applications of bioprocessing, especially with biomass. Bioprocessing combines the disciplines of chemical engineering, microbiology and biochemistry. The Group 's primary role is investigation of the use of microorganism, microbial consortia and microbial enzymes in bioenergy research. New cellulosic ethanol conversion processes have enabled the variety and volume of feedstock that can be bioconverted to expand rapidly. Feedstock now includes materials derived from plant or animal waste such as paper, auto-fluff, tires, fabric, construction materials, municipal solid waste (MSW), sludge, sewage, etc.\n\n1 - Enzymatic hydrolysis - a single source of feedstock, switchgrass for example, is mixed with strong enzymes which convert a portion of cellulosic material into sugars which can then be fermented into ethanol. Genencor and Novozymes are two companies that have received United States government Department of Energy funding for research into reducing the cost of cellulase, a key enzyme in the production cellulosic ethanol by this process.\n\n2 - Synthesis gas fermentation - a blend of feedstock, not exceeding 30% water, is gasified in a closed environment into a syngas containing mostly carbon monoxide and hydrogen. The cooled syngas is then converted into usable products through exposure to bacteria or other catalysts. BRI Energy, LLC is a company whose pilot plant in Fayetteville, Arkansas is currently using synthesis gas fermentation to convert a variety of waste into ethanol. After gasification, anaerobic bacteria (\"Clostridium ljungdahlii\") are used to convert the syngas (CO, CO, and H) into ethanol. The heat generated by gasification is also used to co-generate excess electricity.\n\n3 - C.O.R.S. and Grub Composting are sustainable technologies that employ organisms that feed on organic matter to reduce and convert organic waste in to high quality feedstuff and oil rich material for the biodiesel industry.\nOrganizations pioneering this novel approach to waste management are EAWAG, ESR International, Prota Culture and BIOCONVERSION that created the \"e\"-CORS® system to meet large scale organic waste management needs and environmental sustainability in both urban and livestock farming reality. This type of engineered system introduces a substantial innovation represented by the automatic modulation of the treatment, able to adapt conditions of the system to the biology of the scavenger used, improving their performances and the power of this technology.\n"}
{"id": "42857088", "url": "https://en.wikipedia.org/wiki?curid=42857088", "title": "Chatsworth Nature Preserve Coalition", "text": "Chatsworth Nature Preserve Coalition\n\nThe Chatsworth Nature Preserve Coalition (CNPC) is an advocacy group dedicated to protecting and preserving the Chatsworth Nature Preserve (CNP), located in Chatsworth, California, United States. CNPC actions have included writing letters to local governmental agencies and politicians regarding policy decisions affecting the CNP, and participation in the Earth Day Celebration held annually at the CNP. CNPC members include representatives from local conservation groups and neighboring residents. The group was founded in 2012.\n\n"}
{"id": "4370411", "url": "https://en.wikipedia.org/wiki?curid=4370411", "title": "Crossatron", "text": "Crossatron\n\nIn electronics, a crossatron is a high-power pulsed modulator device, a cold cathode gas-filled tube that combines the best features of thyratrons, vacuum tubes, and power semiconductor switches. This switch is capable of operating with voltages in excess of 100 kilovolts by the use of deuterium gas fill to increase the Paschen breakdown voltage, axial molybdenum cathode corrugations to provide a higher current capability, and a Paschen shield that is formed from molybdenum. The terminal curvature of the Paschen shield and of the adjacent portion of the anode are selected to establish a voltage stress at the curved Paschen shield surface within the approximate range of 90-150 kV/cm in response to a 100 kV differential. The cold cathode gives the crossatron an advantage of achievable lifetime and reliability in comparison to a hydrogen-filled thyratron.\n\nIt features instant start and rugged operation while enduring high temperatures, high radiation, electromagnetic pulse, and repeated overvoltage and overcurrent events. Crossatron switch applications in power conditioning include high-voltage phase-control-rectifier service, high-frequency DC-to-AC inverter modulation, voltage regulation, command charging, and fault protection. Pulsed power applications include high-speed discharging of capacitors and pulse forming networks, repetitive opening of inductive-energy-storage circuits, modulation of square wave pulses in hard-tube modulators, and fault protection.\n"}
{"id": "16085615", "url": "https://en.wikipedia.org/wiki?curid=16085615", "title": "Earth liberation", "text": "Earth liberation\n\n\"Earth liberation\" is an ideology founded by the radical environmental movement and was popularised by the Earth Liberation Front (ELF) as well as the Earth Liberation Army (ELA) in the 1990s.\n\nEarth liberation has no universal understanding as a concept, as it is much considered to be a part of radical environmentalism, or as a militant offshoot, favouring instead radical \"and\" revolutionary environmentalism.\n\nEarth liberationists reject the mainstream environmental movement and include a diversity of individuals with a variety of different ideologies as well as theories. Activists include animal liberationists, anti-capitalists, green anarchists, deep ecologists, eco-feminists and anti-globalisationists. They are notably also within the ELF and ELA, with others from the Animal Liberation Front (ALF) and Class War.\n\nA primary concern is making long lasting, systemic change, by overturning contemporary social and ecological conditions, as part of an \"earth liberation movement\", similar to the aims of the animal liberation movement.\n\nEarth Liberation Movement (ELM): is a free speech, unity, love movement which seeks unification of the human race and the liberation of Earth and its inhabitants from deliberate annihilation.\n"}
{"id": "25757910", "url": "https://en.wikipedia.org/wiki?curid=25757910", "title": "Energy-Efficient Ethernet", "text": "Energy-Efficient Ethernet\n\nEnergy-Efficient Ethernet (EEE) is a set of enhancements to the twisted-pair and backplane Ethernet family of computer networking standards that reduce power consumption during periods of low data activity. The intention is to reduce power consumption by 50% or more, while retaining full compatibility with existing equipment.\n\nThe Institute of Electrical and Electronics Engineers (IEEE), through the IEEE 802.3az task force developed the standard. The first study group had its call for interest in November 2006, and the official standards task force was authorized in May 2007. The IEEE ratified the final standard in September 2010. Some companies introduced technology to reduce the power required for Ethernet before the standard was ratified, using the name Green Ethernet.\n\nSome energy-efficient switch integrated circuits were developed before the IEEE 802.3az Energy-Efficient Ethernet standard was finalized.\n\nIn 2005, all the network interface controllers in the United States (in computers, switches, and routers) used an estimated 5.3 terawatt-hours of electricity. According to a researcher at the Lawrence Berkeley Laboratory, Energy-Efficient Ethernet can potentially save an estimated a year in energy costs in the U.S. Most of the savings would come from homes () and offices (), and the remaining from data centers.\n\nThe power reduction is accomplished in a few ways. In Fast Ethernet, Gigabit Ethernet and 10 Gigabit Ethernet links, constant and significant energy is used by the physical layer as transmitters are active regardless of whether data is being sent. If they could be put into sleep mode when no data is being sent, that energy could be saved. When the controlling software or firmware decides that no data needs to be sent, it can issue a low-power idle (LPI) request to the Ethernet controller physical layer PHY. The PHY will then send LPI symbols for a specified time onto the link, and then disable its transmitter. Refresh signals are sent periodically to maintain link signaling integrity. When there is data to transmit, a normal IDLE signal is sent for a predetermined period of time. The data link is considered to be always operational, as the receive signal circuit remains active even when the transmit path is in sleep mode.\n\nGreen Ethernet technology was a superset of the 802.3az standard. In addition to the link load power savings of Energy-Efficient Ethernet, Green Ethernet works in one of two ways. First, it detects link status, allowing each port on the switch to power down into a standby mode when a connected device, such as a computer, is not active. Second, it detects cable length and adjusts the power used for transmission accordingly. Standard switches provide enough power to send a signal up to . However, this is often unnecessary in the SOHO environment, where of cabling are typical between rooms. Moreover, small data centers can also benefit from this approach since the majority of cabling is confined to a single room with a few meters of cabling among servers and switches. In addition to the pure power saving benefits of Green Ethernet, backing off the transmit power on shorter cable runs reduces alien crosstalk, and improves the overall performance of the cabling system.\n\nGreen Ethernet also encompasses the use of more efficient circuitry in Ethernet chips, and the use of offload engines on Ethernet interface cards intended for network servers. In April 2008, the term was used for switches, and, in July 2008, used with wireless routers which featured user-selectable off periods for Wi-Fi to further reduce energy consumption.\n\nGreen Ethernet was first employed on home products. However, low port counts mean that significant energy savings are not going to be made using this technology only in the home. Turning off existing devices when they are idle is likely to provide a more immediate saving. Projected power savings of up to 80 percent were estimated using Green Ethernet switches, translating into a longer product life due to reduced heat dissipation.\n\n\n"}
{"id": "11034412", "url": "https://en.wikipedia.org/wiki?curid=11034412", "title": "Ferrallitisation", "text": "Ferrallitisation\n\nFerrallitisation is the process in which rock is changed into a soil consisting of clay (kaolinite) and sesquioxides, in the form of hydrated oxides of iron and aluminium. In humid tropical areas, with consistently high temperatures and rainfall for all or most of the year, chemical weathering rapidly breaks down the rock. This at first produces clays which later also break down to form silica. The silica is removed by leaching and the sesquioxides of iron and aluminium remain, giving the characteristic red colour of many tropical soils. Ferrallitisation is the reverse of podsolisation, where silica remains and the iron and aluminum are removed. In tropical rain forests with rain throughout the year, ferrallitic soils develop. In savanna areas, with altering dry and wet climates, ferruginous soils occur.\n\n"}
{"id": "21763008", "url": "https://en.wikipedia.org/wiki?curid=21763008", "title": "Fiat Phyllis", "text": "Fiat Phyllis\n\nThe Fiat Phyllis is a prototype fuel cell-type hydrogen vehicle introduced in the International Motor Show Bologna (Italy) held til 14 December 2008. Car's engineers are the Ecole Polytechnic of Turin (Politecnico di Torino) and the Fiat Research Center (Centro Ricerche Fiat) who concocted (Piedmont Region supports this study).\n\nThe car is planned to be released in 2010, the first clients being the employees of the Turin International Airport: a dozen vehicles will be delivered for a test in real conditions.\n\n\n\n"}
{"id": "28299082", "url": "https://en.wikipedia.org/wiki?curid=28299082", "title": "Frank R. Bowerman Landfill", "text": "Frank R. Bowerman Landfill\n\nThe Frank R. Bowerman Landfill is a landfill in the western Santa Ana Mountains, in Orange County, California. It opened in 1990. The landfill is located between Limestone Canyon Regional Park and State Route 241.\n\nIt is one of the largest landfills in California and the ninth largest in the United States. It contains an estimated 31 million tons of waste.\n\nIt was named after Professor Frank R. Bowerman, former director of environmental engineering programs, at the University of Southern California, and former president of the American Academy of Environmental Engineers and the American Academy for Environmental Protection. Bowerman was also a technical consultant to the environmentally themed science fiction film \"Soylent Green\".\n\nIt is the site for the world’s first commercial landfill gas to liquid natural gas project, the Bowerman Landfill Project, constructed by Prometheus Energy, an LNG fuel company based in Redmond, Washington, and Montauk Energy, a capital investment firm.\n\n"}
{"id": "4431243", "url": "https://en.wikipedia.org/wiki?curid=4431243", "title": "Gas oil separation plant", "text": "Gas oil separation plant\n\nA Gas-oil separation package (GOSP) is a \"package\" used in the upstream oil industry. The package is fitted the well head after the choke valve and before the production manifold, and separates the crude oil from sediments, solids and sand (below using a filter) and gases and condensates to allow the crude to be pumped on the pipeline. \n\nBeware that water need not be separated, causing the need to add chemicals so that the crude and water emulsifies. This process is then reversed at storage by adding demulsifiers that makes the water fall out, and can be tapped from the bottom of the tank. The gas and condensate are pumped on designated pipelines for this, while the sand and sediments require special handling. A gas/oil and water separator is called a 3-stage separator.\n\nAfter storage of the crude this can be sold to refineries, that then produce the fuels, chemicals and energy we consume. \n\nThe raw crude often leaves the well head under very high pressure. Production pressures of greater than is not uncommon. The high pressure is dampened at the choke valve (or \"Blow-out preventer\" - BOP) that is the only equipment on the rig before the GOSP - or \"3-stage Separator\". Modern oil recovery may place a hydro-cyclone to replace the GOSP, allowing the water to be removed immediately and re-injected into the well. The cyclone will vary the rotation according to the water content and can also separate condensate from the gas where separate storage can be provided for the products close to the production well (e.g. on offshore platforms).\n\nIf the crude is pumped after just filtering away sands, the crude, gas and water will separate on the pipeline, forming \"clogs\" and \"clutter\" where the gas and water runs smooth and crude runs slower. This will make the pressure vary, and the pipeline will burst (M.A.A).\n\nProduced crude oil leaving the well may contain large quantities of sulfur (contains e.g. hydrogen sulfide and sulfuric acid) and is then called \"sour\". The sulfur can be very difficult to remove since it may be bound to molecules in the crude and encapsulated in this. The most usual \"crude sweetening packages\" use glycol and/or fatty acids to remove the sulphur content. Crude that contains water is called \"wet\", and the water can then be bound in an emulsion in the crude to allow pumping on a pipeline. The crude must be processed and treated to make it safe, environmentally acceptable, before it can be transported to a refinery for processing.\n\nIt is often appropriate to separate gases and liquids for separate use. This also involves the separation of oily and water liquid phases.\n123\n\nIn old days, the gas was considered waste and was burned on the spot - flared off. By recovering the gas, carbon emissions are reduced at the production site, and create a marketable commodity.\n\n"}
{"id": "13403260", "url": "https://en.wikipedia.org/wiki?curid=13403260", "title": "Generalised metric", "text": "Generalised metric\n\nIn mathematics, the concept of a generalised metric is a generalisation of that of a metric, in which the distance is not a real number but taken from an arbitrary ordered field.\n\nIn general, when we define metric space the distance function is taken to be a real-valued function. The real numbers form an ordered field which is Archimedean and order complete. These metric spaces have some nice properties like: in a metric space compactness, sequential compactness and countable compactness are equivalent etc. These properties may not, however, hold so easily if the distance function is taken in an arbitrary ordered field, instead of in formula_1.\n\nLet formula_2 be an arbitrary ordered field, and formula_3 a nonempty set; a function formula_4 is called a metric on formula_3, iff the following conditions hold:\n\n\nIt is not difficult to verify that the open balls formula_9 form a basis for a suitable topology, the latter called the \"metric topology\" on formula_3, with the metric in formula_11.\n\nIn view of the fact that formula_11 in its order topology is monotonically normal, we would expect formula_3 to be at least regular.\n\nHowever, under axiom of choice, every general metric is monotonically normal, for, given formula_14, where formula_15 is open, there is an open ball formula_16 such that formula_17. Take formula_18. Verify the conditions for Monotone Normality.\n\nThe matter of wonder is that, even without choice, general metrics are monotonically normal.\n\n\"proof\".\n\nCase I: \"F\" is an Archimedean field.\n\nNow, if \"x\" in formula_19 open, we may take formula_20, where formula_21, and the trick is done without choice.\n\nCase II: F is a non-Archimedean field.\n\nFor given formula_14 where \"G\" is open, consider the set\nformula_23.\n\nThe set \"A\"(\"x\", \"G\") is non-empty. For, as \"G\" is open, there is an open ball \"B\"(\"x\", \"k\") within \"G\". Now, as \"F\" is non-Archimdedean, formula_24 is not bounded above, hence there is some formula_25 with formula_26. Putting formula_27, we see that formula_28 is in \"A\"(\"x\", \"G\").\n\nNow define formula_29. We would show that with respect to this mu operator, the space is monotonically normal. Note that formula_30.\n\nIf \"y\" is not in \"G\"(open set containing \"x\") and \"x\" is not in \"H\"(open set containing \"y\"), then we'd show that formula_31 is empty. If not, say \"z\" is in the intersection. Then\n\nSo we are done!\n\n"}
{"id": "43002833", "url": "https://en.wikipedia.org/wiki?curid=43002833", "title": "Glacial erratic boulders of Estonia", "text": "Glacial erratic boulders of Estonia\n\nGlacial erratic boulders of Estonia are large boulders of rock which have been formed and moved into Estonia by glacial action during previous ice ages. Before the takeover of Estonia by the Soviet Union, these large boulders were a symbol of national identity. They are now registered and protected by the Estonian government.\n\nThese boulders are found in especially large quantities in Estonia. Most of the boulders greater than in circumference found in northern Europe are in Estonia. There are 62 known boulders of this size in Estonia. Some particular large ones have been found in the sea, including some near Osmussaar that are diameter. The Osmussaar boulders are believed to have been pushed there by glaciers from the Neugrund meteorite crater.\n\nThe Estonian boulders were important in the development of the idea that northern Europe was once covered in glaciers. The first to suggest that the boulders were moved by glaciers was Russian mineralogist Vasily Severgin who was led to this conclusion by their similarity to Finnish basement rock at the very early date of 1815. However, Severgin was later to retract this theory in favour of the drift theory of Charles Lyell who believed they had been carried by icebergs. However, in the following years of the 19th century the glaciation theory slowly gained the ascendancy.\n"}
{"id": "12581", "url": "https://en.wikipedia.org/wiki?curid=12581", "title": "Glass", "text": "Glass\n\nGlass is a non-crystalline amorphous solid that is often transparent and has widespread practical, technological, and decorative usage in, for example, window panes, tableware, and optoelectronics. The most familiar, and historically the oldest, types of glass are \"silicate glasses\" based on the chemical compound silica (silicon dioxide, or quartz), the primary constituent of sand. The term \"glass\", in popular usage, is often used to refer only to this type of material, which is familiar from use as window glass and in glass bottles. Of the many silica-based glasses that exist, ordinary glazing and container glass is formed from a specific type called soda-lime glass, composed of approximately 75% silicon dioxide (SiO), sodium oxide (NaO) from sodium carbonate (NaCO), calcium oxide (CaO), also called lime, and several minor additives.\n\nMany applications of silicate glasses derive from their optical transparency, giving rise to their primary use as window panes. Glass will transmit, reflect and refract light; these qualities can be enhanced by cutting and polishing to make optical lenses, prisms, fine glassware, and optical fibers for high speed data transmission by light. Glass can be coloured by adding metallic salts, and can also be painted and printed with vitreous enamels. These qualities have led to the extensive use of glass in the manufacture of art objects and in particular, stained glass windows. Although brittle, silicate glass is extremely durable, and many examples of glass fragments exist from early glass-making cultures. Because glass can be formed or moulded into any shape, it has been traditionally used for vessels: bowls, vases, bottles, jars and drinking glasses. In its most solid forms it has also been used for paperweights, marbles, and beads. When extruded as glass fiber and matted as glass wool in a way to trap air, it becomes a thermal insulating material, and when these glass fibers are embedded into an organic polymer plastic, they are a key structural reinforcement part of the composite material fiberglass. Some objects historically were so commonly made of silicate glass that they are simply called by the name of the material, such as drinking glasses and eyeglasses.\n\nScientifically, the term \"glass\" is often defined in a broader sense, encompassing every solid that possesses a non-crystalline (that is, amorphous) structure at the atomic scale and that exhibits a glass transition when heated towards the liquid state. Porcelains and many polymer thermoplastics familiar from everyday use are glasses. These sorts of glasses can be made of quite different kinds of materials than silica: metallic alloys, ionic melts, aqueous solutions, molecular liquids, and polymers. For many applications, like glass bottles or eyewear, polymer glasses (acrylic glass, polycarbonate or polyethylene terephthalate) are a lighter alternative than traditional glass.\n\nSilicon dioxide (SiO) is a common fundamental constituent of glass. In nature, vitrification of quartz occurs when lightning strikes sand, forming hollow, branching rootlike structures called fulgurites.\n\nFused quartz is a glass made from chemically-pure silica. It has excellent resistance to thermal shock, being able to survive immersion in water while red hot. However, its high melting temperature (1723 °C) and viscosity make it difficult to work with. Normally, other substances are added to simplify processing. One is sodium carbonate (NaCO, \"soda\"), which lowers the glass-transition temperature. The soda makes the glass water-soluble, which is usually undesirable, so lime (CaO, calcium oxide, generally obtained from limestone), some magnesium oxide (MgO) and aluminium oxide (AlO) are added to provide for a better chemical durability. The resulting glass contains about 70 to 74% silica by weight and is called a soda-lime glass. Soda-lime glasses account for about 90% of manufactured glass.\n\nMost common glass contains other ingredients to change its properties. Lead glass or flint glass is more \"brilliant\" because the increased refractive index causes noticeably more specular reflection and increased optical dispersion. Adding barium also increases the refractive index. Thorium oxide gives glass a high refractive index and low dispersion and was formerly used in producing high-quality lenses, but due to its radioactivity has been replaced by lanthanum oxide in modern eyeglasses. Iron can be incorporated into glass to absorb infrared radiation, for example in heat-absorbing filters for movie projectors, while cerium(IV) oxide can be used for glass that absorbs ultraviolet wavelengths.\n\nThe following is a list of the more common types of silicate glasses and their ingredients, properties, and applications:\n\nAnother common glass ingredient is crushed alkali glass or 'cullet' ready for recycled glass. The recycled glass saves on raw materials and energy. Impurities in the cullet can lead to product and equipment failure. Fining agents such as sodium sulfate, sodium chloride, or antimony oxide may be added to reduce the number of air bubbles in the glass mixture. Glass batch calculation is the method by which the correct raw material mixture is determined to achieve the desired glass composition.\n\nGlass is in widespread use largely due to the production of glass compositions that are transparent to visible light. In contrast, polycrystalline materials do not generally transmit visible light. The individual crystallites may be transparent, but their facets (grain boundaries) reflect or scatter light resulting in diffuse reflection. Glass does not contain the internal subdivisions associated with grain boundaries in polycrystals and hence does not scatter light in the same manner as a polycrystalline material. The surface of a glass is often smooth since during glass formation the molecules of the supercooled liquid are not forced to dispose in rigid crystal geometries and can follow surface tension, which imposes a microscopically smooth surface. These properties, which give glass its clearness, can be retained even if glass is partially light-absorbing, i.e., colored.\n\nGlass has the ability to refract, reflect, and transmit light following geometrical optics, without scattering it (due to the absence of grain boundaries). It is used in the manufacture of lenses and windows. Common glass has a refraction index around 1.5. This may be modified by adding low-density materials such as boron, which lowers the index of refraction (see crown glass), or increased (to as much as 1.8) with high-density materials such as (classically) lead oxide (see flint glass and lead glass), or in modern uses, less toxic oxides of zirconium, titanium, or barium. These high-index glasses (inaccurately known as \"crystal\" when used in glass vessels) cause more chromatic dispersion of light, and are prized for their diamond-like optical properties.\n\nAccording to Fresnel equations, the reflectivity of a sheet of glass is about 4% per surface (at normal incidence in air), and the transmissivity of one element (two surfaces) is about 90%. Glass with high germanium oxide content also finds application in optoelectronics—e.g., for light-transmitting optical fibers.\nIn the process of manufacture, silicate glass can be poured, formed, extruded and molded into forms ranging from flat sheets to highly intricate shapes. The finished product is brittle and will fracture, unless laminated or specially treated, but is extremely durable under most conditions. It erodes very slowly and can mostly withstand the action of water. It is mostly resistant to chemical attack, does not react with foods, and is an ideal material for the manufacture of containers for foodstuffs and most chemicals. Glass is also a fairly inert substance.\n\nAlthough glass is generally corrosion-resistant and more corrosion resistant than other materials, it still can be corroded. The materials that make up a particular glass composition have an effect on how quickly the glass corrodes. A glass containing a high proportion of alkalis or alkali earths is less corrosion-resistant than other kinds of glasses.\n\nGlass flakes have applications as anti-corrosive coating.\n\nGlass typically has a tensile strength of , however theoretically it can have a strength of due to glass's strong chemical bonds. Several factors such as imperfections like scratches and bubbles and the glass's chemical composition impact the tensile strength of glass. Several processes such as toughening can increase the strength of glass.\n\nFollowing the glass batch preparation and mixing, the raw materials are transported to the furnace. Soda-lime glass for mass production is melted in gas fired units. Smaller scale furnaces for specialty glasses include electric melters, pot furnaces, and day tanks.\nAfter melting, homogenization and refining (removal of bubbles), the glass is . Flat glass for windows and similar applications is formed by the float glass process, developed between 1953 and 1957 by Sir Alastair Pilkington and Kenneth Bickerstaff of the UK's Pilkington Brothers, who created a continuous ribbon of glass using a molten tin bath on which the molten glass flows unhindered under the influence of gravity. The top surface of the glass is subjected to nitrogen under pressure to obtain a polished finish.\nContainer glass for common bottles and jars is formed by blowing and pressing methods. This glass is often slightly modified chemically (with more alumina and calcium oxide) for greater water resistance. Further glass forming techniques are summarized in the table .\n\nOnce the desired form is obtained, glass is usually annealed for the removal of stresses and to increase the glass's hardness and durability.\nSurface treatments, coatings or lamination may follow to improve the chemical durability (glass container coatings, glass container internal treatment), strength (toughened glass, bulletproof glass, windshields), or optical properties (insulated glazing, anti-reflective coating).\nColor in glass may be obtained by addition of electrically charged ions (or color centers) that are homogeneously distributed, and by precipitation of finely dispersed particles (such as in photochromic glasses).\nOrdinary soda-lime glass appears colorless to the naked eye when it is thin, although iron(II) oxide (FeO) impurities of up to 0.1 wt% produce a green tint, which can be viewed in thick pieces or with the aid of scientific instruments. Further FeO and chromium(III) oxide (CrO) additions may be used for the production of green bottles. Sulfur, together with carbon and iron salts, is used to form iron polysulfides and produce amber glass ranging from yellowish to almost black. A glass melt can also acquire an amber color from a reducing combustion atmosphere. Manganese dioxide can be added in small amounts to remove the green tint given by iron(II) oxide. Art glass and studio glass pieces are colored using closely guarded recipes that involve specific combinations of metal oxides, melting temperatures and \"cook\" times. Most colored glass used in the art market is manufactured in volume by vendors who serve this market, although there are some glassmakers with the ability to make their own color from raw materials.\n\nNaturally occurring glass, especially the volcanic glass obsidian, was used by many Stone Age societies across the globe for the production of sharp cutting tools and, due to its limited source areas, was extensively traded. But in general, archaeological evidence suggests that the first true glass was made in coastal north Syria, Mesopotamia or ancient Egypt. The earliest known glass objects, of the mid third millennium BCE, were beads, perhaps initially created as accidental by-products of metal-working (slags) or during the production of faience, a pre-glass vitreous material made by a process similar to glazing.\n\nGlass remained a luxury material, and the disasters that overtook Late Bronze Age civilizations seem to have brought glass-making to a halt. Indigenous development of glass technology in South Asia may have begun in 1730 BCE. In ancient China, though, glassmaking seems to have a late start, compared to ceramics and metal work. The term \"glass\" developed in the late Roman Empire. It was in the Roman glassmaking center at Trier, now in modern Germany, that the late-Latin term \"glesum\" originated, probably from a Germanic word for a transparent, lustrous substance. Glass objects have been recovered across the Roman Empire in domestic, funerary, and industrial contexts. Examples of Roman glass have been found outside of the former Roman Empire in China, the Baltics, the Middle East and India.\n\nGlass was used extensively during the Middle Ages. Anglo-Saxon glass has been found across England during archaeological excavations of both settlement and cemetery sites. Glass in the Anglo-Saxon period was used in the manufacture of a range of objects including vessels, windows, beads, and was also used in jewelry. From the 10th-century onwards, glass was employed in stained glass windows of churches and cathedrals, with famous examples at Chartres Cathedral and the Basilica of Saint Denis. By the 14th-century, architects were designing buildings with walls of stained glass such as Sainte-Chapelle, Paris, (1203–1248) and the East end of Gloucester Cathedral. Stained glass had a major revival with Gothic Revival architecture in the 19th century. With the Renaissance, and a change in architectural style, the use of large stained glass windows became less prevalent. The use of domestic stained glass increased until most substantial houses had glass windows. These were initially small panes leaded together, but with the changes in technology, glass could be manufactured relatively cheaply in increasingly larger sheets. This led to larger window panes, and, in the 20th-century, to much larger windows in ordinary domestic and commercial buildings.\n\nIn the 20th century, new types of glass such as laminated glass, reinforced glass and glass bricks increased the use of glass as a building material and resulted in new applications of glass. Multi-story buildings are frequently constructed with curtain walls made almost entirely of glass. Similarly, laminated glass has been widely applied to vehicles for windscreens. Optical glass for spectacles has been used since the Middle Ages. The production of lenses has become increasingly proficient, aiding astronomers as well as having other application in medicine and science. Glass is also employed as the aperture cover in many solar energy collectors.\n\nFrom the 19th century, there was a revival in many ancient glass-making techniques including cameo glass, achieved for the first time since the Roman Empire and initially mostly used for pieces in a neo-classical style. The Art Nouveau movement made great use of glass, with René Lalique, Émile Gallé, and Daum of Nancy producing colored vases and similar pieces, often in cameo glass, and also using luster techniques. Louis Comfort Tiffany in America specialized in stained glass, both secular and religious, and his famous lamps. The early 20th-century saw the large-scale factory production of glass art by firms such as Waterford and Lalique. From about 1960 onwards, there have been an increasing number of small studios hand-producing glass artworks, and glass artists began to class themselves as in effect sculptors working in glass, and their works as part fine arts.\n\nIn the 21st century, scientists observe the properties of ancient stained glass windows, in which suspended nanoparticles prevent UV light from causing chemical reactions that change image colors, are developing photographic techniques that use similar stained glass to capture true color images of Mars for the 2019 ESA Mars Rover mission.\n\nNew chemical glass compositions or new treatment techniques can be initially investigated in small-scale laboratory experiments. The raw materials for laboratory-scale glass melts are often different from those used in mass production because the cost factor has a low priority. In the laboratory mostly pure chemicals are used. Care must be taken that the raw materials have not reacted with moisture or other chemicals in the environment (such as alkali or alkaline earth metal oxides and hydroxides, or boron oxide), or that the impurities are quantified (loss on ignition). Evaporation losses during glass melting should be considered during the selection of the raw materials, e.g., sodium selenite may be preferred over easily evaporating SeO. Also, more readily reacting raw materials may be preferred over relatively inert ones, such as Al(OH) over AlO. Usually, the melts are carried out in platinum crucibles to reduce contamination from the crucible material. Glass homogeneity is achieved by homogenizing the raw materials mixture (glass batch), by stirring the melt, and by crushing and re-melting the first melt. The obtained glass is usually annealed to prevent breakage during processing.\n\nTo make glass from materials with poor glass forming tendencies, novel techniques are used to increase cooling rate, or reduce crystal nucleation triggers. Examples of these techniques include aerodynamic levitation (cooling the melt whilst it floats on a gas stream), splat quenching (pressing the melt between two metal anvils) and roller quenching (pouring the melt through rollers).\n\nFiberglass (also called glass-reinforced-plastic) is a composite material made up of glass fibers (also called fiberglass or glass friller) embedded in a plastic resin. It is made by melting glass and stretching the glass into fibers. These fibers are woven together into a cloth and left to set in a plastic resin.\n\nFiberglass filaments are made through a pultrusion process in which the raw materials (sand, limestone, kaolin clay, fluorspar, colemanite, dolomite and other minerals) are melted in a large furnace into a liquid which is extruded through very small orifices (5–25 micrometres in diameter if the glass is E-glass and 9 micrometers if the glass is S-glass).\n\nFiberglass has the properties of being lightweight and corrosion resistant. Fiberglass is also a good insulator, allowing it to be used to insulate buildings. Most fiberglasses are not alkali resistant. Fiberglass also has the property of becoming stronger as the glass ages.\n\nSome types of glass that do not include silica as a major constituent may have physico-chemical properties useful for their application in fiber optics and other specialized technical applications. These include fluoride glass, aluminate and aluminosilicate glass, phosphate glass, borate glass, and chalcogenide glass.\n\nThere are three classes of components for oxide glass: network formers, intermediates, and modifiers. The network formers (silicon, boron, germanium) form a highly cross-linked network of chemical bonds. The intermediates (titanium, aluminium, zirconium, beryllium, magnesium, zinc) can act as both network formers and modifiers, according to the glass composition. The modifiers (calcium, lead, lithium, sodium, potassium) alter the network structure; they are usually present as ions, compensated by nearby non-bridging oxygen atoms, bound by one covalent bond to the glass network and holding one negative charge to compensate for the positive ion nearby. Some elements can play multiple roles; e.g. lead can act both as a network former (Pb replacing Si), or as a modifier.\n\nThe presence of non-bridging oxygens lowers the relative number of strong bonds in the material and disrupts the network, decreasing the viscosity of the melt and lowering the melting temperature.\n\nThe alkali metal ions are small and mobile; their presence in glass allows a degree of electrical conductivity, especially in molten state or at high temperature. Their mobility decreases the chemical resistance of the glass, allowing leaching by water and facilitating corrosion. Alkaline earth ions, with their two positive charges and requirement for two non-bridging oxygen ions to compensate for their charge, are much less mobile themselves and also hinder diffusion of other ions, especially the alkalis. The most common commercial glass types contain both alkali and alkaline earth ions (usually sodium and calcium), for easier processing and satisfying corrosion resistance. Corrosion resistance of glass can be increased by dealkalization, removal of the alkali ions from the glass surface by reaction with sulfur or fluorine compounds. Presence of alkaline metal ions has also detrimental effect to the loss tangent of the glass, and to its electrical resistance; glass manufactured for electronics (sealing, vacuum tubes, lamps ...) have to take this in account.\n\nAddition of lead(II) oxide lowers melting point, lowers viscosity of the melt, and increases refractive index. Lead oxide also facilitates solubility of other metal oxides and is used in colored glass. The viscosity decrease of lead glass melt is very significant (roughly 100 times in comparison with soda glass); this allows easier removal of bubbles and working at lower temperatures, hence its frequent use as an additive in vitreous enamels and glass solders. The high ionic radius of the Pb ion renders it highly immobile in the matrix and hinders the movement of other ions; lead glasses therefore have high electrical resistance, about two orders of magnitude higher than soda-lime glass (10 vs 10 Ω⋅cm, DC at 250 °C). For more details, see lead glass.\n\nAddition of fluorine lowers the dielectric constant of glass. Fluorine is highly electronegative and attracts the electrons in the lattice, lowering the polarizability of the material. Such silicon dioxide-fluoride is used in manufacture of integrated circuits as an insulator. High levels of fluorine doping lead to formation of volatile SiFO and such glass is then thermally unstable. Stable layers were achieved with dielectric constant down to about 3.5–3.7.\n\nIn the past, small batches of amorphous metals with high surface area configurations (ribbons, wires, films, etc.) have been produced through the implementation of extremely rapid rates of cooling. This was initially termed \"splat cooling\" by doctoral student W. Klement at Caltech, who showed that cooling rates on the order of millions of degrees per second is sufficient to impede the formation of crystals, and the metallic atoms become \"locked into\" a glassy state. Amorphous metal wires have been produced by sputtering molten metal onto a spinning metal disk. More recently a number of alloys have been produced in layers with thickness exceeding 1 millimeter. These are known as bulk metallic glasses (BMG). Liquidmetal Technologies sell a number of zirconium-based BMGs. Batches of amorphous steel have also been produced that demonstrate mechanical properties far exceeding those found in conventional steel alloys.\n\nIn 2004, NIST researchers presented evidence that an isotropic non-crystalline metallic phase (dubbed \"q-glass\") could be grown from the melt. This phase is the first phase, or \"primary phase\", to form in the Al-Fe-Si system during rapid cooling. Experimental evidence indicates that this phase forms by a \"first-order transition\". Transmission electron microscopy (TEM) images show that the q-glass nucleates from the melt as discrete particles, which grow spherically with a uniform growth rate in all directions. The diffraction pattern shows it to be an isotropic glassy phase. Yet there is a nucleation barrier, which implies an interfacial discontinuity (or internal surface) between the glass and the melt.\n\nElectrolytes or molten salts are mixtures of different ions. In a mixture of three or more ionic species of dissimilar size and shape, crystallization can be so difficult that the liquid can easily be supercooled into a glass.\nThe best-studied example is CaK(NO). Glass electrolytes in the form of Ba-doped Li-glass and Ba-doped Na-glass have been proposed as solutions to problems identified with organic liquid electrolytes used in modern lithium-ion battery cells.\n\nSome aqueous solutions can be supercooled into a glassy state, for instance LiCl:\"R\"HO (a solution of lithium chloride salt and water molecules) in the composition range 4<\"R\"<8. An aqueous solution containing sugar has a glassy state and can be used as a surfactant.\n\nA \"molecular liquid\" is composed of molecules that do not form a covalent network but interact only through weak van der Waals forces or through transient hydrogen bonds.\nMany molecular liquids can be supercooled into a glass; some are excellent glass formers that normally do not crystallize.\n\nAn example of this is sugar glass.\n\nUnder extremes of pressure and temperature solids may exhibit large structural and physical changes that can lead to polyamorphic phase transitions. In 2006 Italian scientists created an amorphous phase of carbon dioxide using extreme pressure. The substance was named amorphous carbonia(a-CO) and exhibits an atomic structure resembling that of silica.\n\nImportant polymer glasses include amorphous and glassy pharmaceutical compounds. These are useful because the solubility of the compound is greatly increased when it is amorphous compared to the same crystalline composition. Many emerging pharmaceuticals are practically insoluble in their crystalline forms.\n\nConcentrated colloidal suspensions may exhibit a distinct glass transition as function of particle concentration or density.\n\nIn cell biology, there is recent evidence suggesting that the cytoplasm behaves like a colloidal glass approaching the liquid-glass transition. During periods of low metabolic activity, as in dormancy, the cytoplasm vitrifies and prohibits the movement to larger cytoplasmic particles while allowing the diffusion of smaller ones throughout the cell.\n\nGlass-ceramic materials share many properties with both non-crystalline glass and crystalline ceramics. They are formed as a glass, and then partially crystallized by heat treatment. For example, the microstructure of whiteware ceramics frequently contains both amorphous and crystalline phases. Crystalline grains are often embedded within a non-crystalline intergranular phase of grain boundaries. When applied to whiteware ceramics, vitreous means the material has an extremely low permeability to liquids, often but not always water, when determined by a specified test regime.\n\nThe term mainly refers to a mix of lithium and aluminosilicates that yields an array of materials with interesting thermomechanical properties. The most commercially important of these have the distinction of being impervious to thermal shock. Thus, glass-ceramics have become extremely useful for countertop cooking. The negative thermal expansion coefficient (CTE) of the crystalline ceramic phase can be balanced with the positive CTE of the glassy phase. At a certain point (~70% crystalline) the glass-ceramic has a net CTE near zero. This type of glass-ceramic exhibits excellent mechanical properties and can sustain repeated and quick temperature changes up to 1000 °C.\n\nAs in other amorphous solids, the atomic structure of a glass lacks the long-range periodicity observed in crystalline solids. Due to chemical bonding characteristics, glasses do possess a high degree of short-range order with respect to local atomic polyhedra.\n\nIn physics, the standard definition of a glass (or vitreous solid) is a solid formed by rapid melt quenching, although the term glass is often used to describe any amorphous solid that exhibits a glass transition temperature T. For melt quenching, if the cooling is sufficiently rapid (relative to the characteristic crystallization time) then crystallization is prevented and instead the disordered atomic configuration of the supercooled liquid is frozen into the solid state at T. The tendency for a material to form a glass while quenched is called glass-forming ability. This ability can be predicted by the rigidity theory. Generally, a glass exists in a structurally metastable state with respect to its crystalline form, although in certain circumstances, for example in atactic polymers, there is no crystalline analogue of the amorphous phase.\n\nGlass is sometimes considered to be a liquid due to its lack of a first-order phase transition\nwhere certain thermodynamic variables such as volume, entropy and enthalpy are discontinuous through the glass transition range. The glass transition may be described as analogous to a second-order phase transition where the intensive thermodynamic variables such as the thermal expansivity and heat capacity are discontinuous. Nonetheless, the equilibrium theory of phase transformations does not entirely hold for glass, and hence the glass transition cannot be classed as one of the classical equilibrium phase transformations in solids.\n\nGlass is an amorphous solid. It exhibits an atomic structure close to that observed in the supercooled liquid phase but displays all the mechanical properties of a solid. The notion that glass flows to an appreciable extent over extended periods of time is not supported by empirical research or theoretical analysis (see viscosity of amorphous materials). Laboratory measurements of room temperature glass flow do show a motion consistent with a material viscosity on the order of 10–10 Pa s.\n\nAlthough the atomic structure of glass shares characteristics of the structure in a supercooled liquid, glass tends to behave as a solid below its glass transition temperature. A supercooled liquid behaves as a liquid, but it is below the freezing point of the material, and in some cases will crystallize almost instantly if a crystal is added as a core. The change in heat capacity at a glass transition and a melting transition of comparable materials are typically of the same order of magnitude, indicating that the change in active degrees of freedom is comparable as well. Both in a glass and in a crystal it is mostly only the vibrational degrees of freedom that remain active, whereas rotational and translational motion is arrested. This helps to explain why both crystalline and non-crystalline solids exhibit rigidity on most experimental time scales.\nThe observation that old windows are sometimes found to be thicker at the bottom than at the top is often offered as supporting evidence for the view that glass flows over a timescale of centuries, the assumption being that the glass has exhibited the liquid property of flowing from one shape to another. This assumption is incorrect, as once solidified, glass stops flowing. The reason for the observation is that in the past, when panes of glass were commonly made by glassblowers, the technique used was to spin molten glass so as to create a round, mostly flat and even plate (the crown glass process, described above). This plate was then cut to fit a window. The pieces were not absolutely flat; the edges of the disk became a different thickness as the glass spun. When installed in a window frame, the glass would be placed with the thicker side down both for the sake of stability and to prevent water accumulating in the lead cames at the bottom of the window. Occasionally, such glass has been found installed with the thicker side at the top, left or right.\n\nMass production of glass window panes in the early twentieth century caused a similar effect. In glass factories, molten glass was poured onto a large cooling table and allowed to spread. The resulting glass is thicker at the location of the pour, located at the center of the large sheet. These sheets were cut into smaller window panes with nonuniform thickness, typically with the location of the pour centered in one of the panes (known as \"bull's-eyes\") for decorative effect. Modern glass intended for windows is produced as float glass and is very uniform in thickness.\n\nSeveral other points can be considered that contradict the \"cathedral glass flow\" theory:\n\n"}
{"id": "24912841", "url": "https://en.wikipedia.org/wiki?curid=24912841", "title": "Granulation", "text": "Granulation\n\nGranulation is the process of forming of grains or granules from a powdery or solid substance, producing a granular material. It is applied in several technological processes in chemical and pharmaceutical industry. Typically, granulation involves agglomeration of fine particles into larger granules, typically of size range between 0.2 and 4.0 mm depending on their subsequent use. Less commonly, it involves shredding or grinding solid material into finer granules or pellets.\n\nThe granulation process combines one or more powder particles and forms a granule that will allow tableting to be within required limits. It is the process of collecting particles together by creating bonds between them. Bonds are formed by compression or by using a binding agent. Granulation is extensively used in the pharmaceutical industry, for manufacturing of tablets and pellets. This way predictable and repeatable process is possible and granules of consistent quality can be produced.\n\nGranulation is carried out for various reasons, one of which is to prevent the segregation of the constituents of powder mix. Segregation is due to differences in the size or density of the components of the mix. Normally, the smaller and/or denser particles tend to concentrate at the base of the container with the larger and/or less dense ones on the top. An ideal granulation will contain all the constituents of the mix in the correct proportion in each granule and segregation of granules will not occur.\n\nMany powders, because of their small size, irregular shape or surface characteristics, are cohesive and do not flow well. Granules produced from such a cohesive system will be larger and more isodiametric (roughly spherical), both factors contributing to improved flow properties.\n\nSome powders are difficult to compact even if a readily compactable adhesive is included in the mix, but granules of the same powders are often more easily compacted. This is associated with the distribution of the adhesive within the granule and is a function of the method employed to produce the granule.\n\nFor example, if one were to make tablets from granulated sugar versus powdered sugar, powdered sugar would be difficult to compress into a tablet and granulated sugar would be easy to compress. Powdered sugar’s small particles have poor flow and compression characteristics. These small particles would have to be compressed very slowly for a long period of time to make a worthwhile tablet. Unless the powdered sugar is granulated, it could not efficiently be made into a tablet that has good tablet characteristics such as uniform content or consistent hardness.\n\nTwo types of granulation technologies are employed: wet granulation and dry granulation.\n\nIn wet granulation, granules are formed by the addition of a granulation liquid onto a powder bed which is under the influence of an impeller (in a high-shear granulator), screws (in a twin screw granulator) or air (in a fluidized bed granulator). The agitation resulting in the system along with the wetting of the components within the formulation results in the aggregation of the primary powder particles to produce wet granules. The granulation liquid (fluid) contains a solvent or carrier material which must be volatile so that it can be removed by drying, and depending on the intended application, be non-toxic. Typical liquids include water, ethanol and isopropanol either alone or in combination. The liquid solution can be either aqueous based or solvent-based. Aqueous solutions have the advantage of being safer to deal with than other solvents.\n\nWater mixed into the powders can form bonds between powder particles that are strong enough to lock them together. However, once the water dries, the powders may fall apart. Therefore, water may not be strong enough to create and hold a bond. In such instances, a liquid solution that includes a binder (pharmaceutical glue) is required. Povidone, which is a polyvinyl pyrrolidone (PVP), is one of the most commonly used pharmaceutical binders. PVP is dissolved in water or solvent and added to the process. When PVP and a solvent/water are mixed with powders, PVP forms a bond with the powders during the process, and the solvent/water evaporates (dries). Once the solvent/water has been dried and the powders have formed a more densely held mass, then the granulation is milled. This process results in the formation of granules.\n\nThe process can be very simple or very complex depending on the characteristics of the powders, the final objective of tablet making, and the equipment that is available. In the traditional wet granulation method the wet mass is forced through a sieve to produce wet granules which are subsequently dried.\n\nWet granulation is traditionally a batch process in the pharmaceutical production, however, the batch type wet granulations are foreseen to be replaced more and more by continuous wet granulation in the pharmaceutical industry in the future. The shift from batch to continuous technologies has been recommended by the Food and Drug Administration. This continuous wet granulation technology can be carried out on a twin-screw extruder into which solid materials and water can be fed at various parts. In the extruder the materials are mixed and granulated due to the intermesh of the screws, especially at the kneading elements.\n\nThe dry granulation process is used to form granules without a liquid solution because the product granulated may be sensitive to moisture and heat. Forming granules without moisture requires compacting and densifying the powders. In this process the primary powder particles are aggregated under high pressure. A swaying granulator or a roll compactor can be used for the dry granulation.\n\nDry granulation can be conducted under two processes; either a large tablet (slug) is produced in a heavy duty tabletting press or the powder is squeezed between two counter-rotating rollers to produce a continuous sheet or ribbon of material.\n\nWhen a tablet press is used for dry granulation, the powders may not possess enough natural flow to feed the product uniformly into the die cavity, resulting in varying density. The roller compactor (granulator-compactor) uses an auger-feed system that will consistently deliver powder uniformly between two pressure rollers. The powders are compacted into a ribbon or small pellets between these rollers and milled through a low-shear mill. When the product is compacted properly, then it can be passed through a mill and final blend before tablet compression.\n\nTypical roller compaction processes consist of the following steps: convey powdered material to the compaction area, normally with a screw feeder, compact powder between two counter-rotating rolls with applied forces, mill resulting compact to desired particle size distribution. Roller compacted particle are typically dense, with sharp-edged profiles.\n\nIn plastic recycling, granulation is the process of shredding plastic objects to be recycled into flakes or pellets, suitable for later reuse in plastics extrusion. In the first stage, plastic objects to be recycled are fed to an electric motor-powered cutting chamber, which continually cuts the material in scissor-like fashion. The material is ground into all the smaller flakes until they became fine enough to fall through a mesh screen. In wet-granulation lines, water is continually sprayed in the cutting chamber to remove the debris and impurities, and acts as a lubricant of the steel blades; in dry-granulation lines, water is not present, but such technology generally produces output of lower quality than the wet technology. While the process is relatively simple, it must be carefully parametrized, as the high temperatures resulting from friction can damage the material and affect its plasticity. Regular maintenance and sharpening of the scissor blades are essential, as well as close monitoring of the process due to potential clogging and jamming.\n\nIn many cases, granulation may be the only step required before the plastics can be reused for manufacturing of new products. In other, the new or recycled plastic material must be remade into pellets. The material is molten and extruded into thin rods, which are then cooled in a water tank and finely chopped into small cyllindric pellets.\n\n\n\n"}
{"id": "18790726", "url": "https://en.wikipedia.org/wiki?curid=18790726", "title": "High-pressure electrolysis", "text": "High-pressure electrolysis\n\nHigh-pressure electrolysis (HPE) is the electrolysis of water by decomposition of water (HO) into oxygen (O) and hydrogen gas (H) due to the passing of an electric current through the water. The difference with a standard proton exchange membrane electrolyzer is the compressed hydrogen output around at 70 °C. By pressurising the hydrogen in the electrolyser the need for an external hydrogen compressor is eliminated, the average energy consumption for internal differential pressure compression is around 3%.\n\nAs the required compression power for water is less than that for hydrogen-gas the water is pumped up to a high-pressure, in the other approach differential pressure is used.\nThere is also an importance for the electrolyser stacks to be able to accept a fluctuating electrical input, such as that found with renewable energy. This then enables the ability to help with grid balancing and energy storage.\n\nUltrahigh-pressure electrolysis is high-pressure electrolysis operating at . At ultra-high pressures the water solubility and cross-permeation across the membrane of H and O is affecting hydrogen purity, modified PEMs are used to reduce cross-permeation in combination with catalytic H/O recombiners to maintain H levels in O and O levels in H at values compatible with hydrogen safety requirements.\n\nThe US DOE believes that high-pressure electrolysis, supported by ongoing research and development, will contribute to the enabling and acceptance of technologies where hydrogen is the energy carrier between renewable energy resources and clean energy consumers.\n\nHigh-pressure electrolysis is being investigated by the DOE for efficient production of hydrogen from water. The target total in 2005 is $4.75 per gge H at an efficiency of 64%. The total goal for the DOE in 2010 is $2.85 per gge H at an efficiency of 75%. As of 2005 the DOE provided a total of $1,563,882 worth of funding for research.\n\nMitsubishi is pursuing such technology with its High-pressure hydrogen energy generator (HHEG) project.\n\nThe Forschungszentrum Jülich, in Jülich Germany is currently researching the cost reduction of components used in high-pressure PEM electrolysis in the EKOLYSER project. The primary goal of this research is to improve performance and gas purity, reduce cost and volume of expensive materials and reach the alternative energy targets set forth by the German government for 2050 in the Energy Concept published in 2010.\n\n\n"}
{"id": "5379719", "url": "https://en.wikipedia.org/wiki?curid=5379719", "title": "Hydrogen production", "text": "Hydrogen production\n\nHydrogen production is the family of industrial methods for generating hydrogen. Currently the dominant technology for direct production is steam reforming from hydrocarbons. Many other methods are known including electrolysis and thermolysis. \n\nIn 2006, the United States was estimated to have a production capacity of 11 million tons of hydrogen. 5 million tons of hydrogen were consumed on-site in oil refining, and in the production of ammonia (Haber process) and methanol (reduction of carbon monoxide). 0.4 million tons were an incidental by-product of the chlor-alkali process. Hydrogen production is an estimated $100 billion industry. According to the U.S. Department of Energy, 53 million metric tons were consumed worldwide in 2004. There are no natural hydrogen deposits, and for this reason the production of hydrogen plays a key role in modern society.\n\nAs of 1999, the majority of hydrogen (∼95%) is produced from fossil fuels by steam reforming or partial oxidation of methane and coal gasification with only a small quantity by other routes such as biomass gasification or electrolysis of water. Around 8GW of electrolysis capacity is installed \nworldwide, accounting for around 4% of global hydrogen production (Decourt et al., 2014). Developing affordable methods for producing hydrogen with less damage to the environment is a goal of the hydrogen economy.\n\nThere are four main sources for the commercial production of hydrogen: natural gas, oil, coal, and electrolysis; which account for 48%, 30%, 18% and 4% of the world’s hydrogen production respectively.\nFossil fuels are the dominant source of industrial hydrogen. Carbon dioxide can be separated from natural gas with a 60-70% efficiency for hydrogen production and from other hydrocarbons to varying degrees of efficiency.Specifically, bulk hydrogen is usually produced by the steam reforming of methane or natural gas. \nThe production of hydrogen from natural gas is the cheapest source of hydrogen currently. This process consists of heating the gas to between 700-1100°C in the presence of steam and a nickel catalyst. The resulting endothermic reaction breaks up the methane molecules and forms carbon monoxide CO and hydrogen H. The carbon monoxide gas can then be passed with steam over iron oxide or other oxides and undergo a water gas shift reaction to obtain further quantities of H. The downside to this process is that its major byproducts are CO, CO and other greenhouse gases. Depending on the quality of the feedstock (natural gas, rich gases, naphtha, etc.), one ton of hydrogen produced will also produce 9 to 12 tons of CO.\n\nFor this process high temperature (700–1100 °C) steam (HO) reacts with methane (CH) in an endothermic reaction to yield syngas.\n\nIn a second stage, additional hydrogen is generated through the lower-temperature, exothermic, water gas shift reaction, performed at about 360 °C:\n\nEssentially, the oxygen (O) atom is stripped from the additional water (steam) to oxidize CO to CO. This oxidation also provides energy to maintain the reaction. Additional heat required to drive the process is generally supplied by burning some portion of the methane.\n\nSteam reforming generates carbon dioxide (CO). Since the production is concentrated in one facility, it is possible to separate the CO and dispose of it without atmospheric release, for example by injecting it in an oil or gas reservoir (see carbon capture), although this is not currently done in most cases. A carbon dioxide injection project has been started by the Norwegian company Statoil in the North Sea, at the Sleipner field.\n\nIntegrated steam reforming / co-generation - It is possible to combine steam reforming and co-generation of steam and power into a single plant. This can deliver benefits for an oil refinery because it is more efficient than separate hydrogen, steam and power plants. Air Products recently built an integrated steam reforming / co-generation plant in Port Arthur, Texas.\n\nHydrogen production from natural gas or other hydrocarbons is achieved by partial oxidation. A fuel-air or fuel-oxygen mixture is partially combusted resulting in a hydrogen rich syngas. Hydrogen and carbon monoxide are obtained via the water-gas shift reaction. Carbon dioxide can be co-fed to lower the hydrogen to carbon monoxide ratio. \n\nThe partial oxidation reaction occurs when a substoichiometric fuel-air mixture or fuel-oxygen is partially combusted in a reformer or partial oxidation reactor. A distinction is made between \"thermal partial oxidation\" (TPOX) and \"catalytic partial oxidation\" (CPOX). The chemical reaction takes the general form:\n\nIdealized examples for heating oil and coal, assuming compositions CH and CH respectively, are as follows:\n\nThe Kværner-process or Kvaerner carbon black & hydrogen process (CB&H) is a plasma reforming method, developed in the 1980s by a Norwegian company of the same name, for the production of hydrogen and carbon black from liquid hydrocarbons (CH). Of the available energy of the feed, approximately 48% is contained in the hydrogen, 40% is contained in activated carbon and 10% in superheated steam. CO is not produced in the process.\n\nA variation of this process is presented in 2009 using plasma arc waste disposal technology for the production of hydrogen, heat and carbon from methane and natural gas in a plasma converter\n\nFor the production of hydrogen from coal, coal gasification is used. The process of coal gasification uses steam and a carefully controlled concentration of gases to break molecular bonds in coal and form a gaseous mix of hydrogen and carbon monoxide.\nThis source of hydrogen is advantageous since its main product is coal-derived gas which can be used for fuel. The gas obtained from coal gasification can later be used to produce electricity more efficiently and allow a better capture of greenhouse gases than the traditional burning of coal.\n\nAnother method for conversion is low temperature and high temperature coal carbonization.\n\nSimilarly to coal, petroleum coke can also be converted in hydrogen rich syngas, via coal gasification. The syngas in this case consists mainly of hydrogen, carbon monoxide and HS, depending on the sulfur content of the coke feed. Gasification is an attractive option for producing hydrogen from almost any carbon source, while providing attractive hydrogen utilization alternatives through process integration.\n\nMany technologies have been explored but it should be noted that as of 2007 \"Thermal, thermochemical, biochemical and photochemical processes have so far not found industrial applications.\" High temperature electrolysis of alkaline solutions has been used for the industrial scale production of hydrogen (see Sable Chemicals) and there are now a number of small scale polymer electrolyte membrane (PEM) electrolysis units available commercially.\n\nElectrolysis consists of using electricity to split water into hydrogen and oxygen. Electrolysis of water is 70-80% efficient (a 20-30% conversion loss) which is a higher efficiency than steam reforming of natural gas which is 60-75% efficient. The (electrical) efficiency of electrolysis is expected to reach 82-86% before 2030, while also maintaining durability as progress in this area continues at a pace. Water electrolysis can operate between 50-80°C, while steam methane reforming requires temperatures between 700-1100°C. The difference between the two methods is the primary energy used; either electricity (for electrolysis) or natural gas (for steam methane reforming). Due to their use of water, a readily available resource, electrolysis and similar water-splitting methods have attracted the interest of the scientific community. With the objective of reducing the cost of hydrogen production, renewable sources of energy have been targeted to allow electrolysis.\nThere are three main types of cells, solid oxide electrolysis cells (SOECs), polymer electrolyte membrane cells (PEM) and alkaline electrolysis cells (AECs). SOECs operate at high temperatures, typically around 800 °C. At these high temperatures a significant amount of the energy required can be provided as thermal energy (heat), and as such is termed High temperature electrolysis. The heat energy can be provided from a number of different sources, including waste industrial heat, nuclear power stations or concentrated solar thermal plants. This has the potential to reduce the overall cost of the hydrogen produced by reducing the amount of electrical energy required for electrolysis. PEM electrolysis cells typically operate below 100 °C and are becoming increasingly available commercially. These cells have the advantage of being comparatively simple and can be designed to accept widely varying voltage inputs which makes them ideal for use with renewable sources of energy such as solar PV. AECs optimally operate at high concentrations electrolyte (KOH or potassium carbonate) and at high temperatures, often near 200 °C.\n\nEfficiency of modern hydrogen generators is measured by \"energy consumed per standard volume of hydrogen\" (MJ/m), assuming standard temperature and pressure of the H. The lower the energy used by a generator, the higher would be its efficiency; a 100%-efficient electrolyser would consume of hydrogen, . Practical electrolysis (using a rotating electrolyser at 15 bar pressure) may consume , and a further if the hydrogen is compressed for use in hydrogen cars.\n\nElectrolyser vendors provide efficiencies based on enthalpy. To assess the claimed efficiency of an electrolyser it is important to establish how it was defined by the vendor (i.e. what enthalpy value, what current density, etc.).\n\nThere are two main technologies available on the market, \"alkaline\" and \"proton exchange membrane\" (PEM) electrolysers.\nTraditionally, alkaline electrolysers are cheaper in terms of investment (they generally use nickel catalysts), but less efficient; PEM electrolysers, conversely, are more expensive (they generally use expensive platinum-group metal catalysts) but are more efficient and can operate at higher current densities, and can therefore be possibly cheaper if the hydrogen production is large enough.\n\nConventional alkaline electrolysis has an efficiency of about 70%, however thyssenkrupp have recently developed an advanced alkaline water electrolyser with an efficiency of 82%. Accounting for the use of the higher heat value (because inefficiency via heat can be redirected back into the system to create the steam required by the catalyst), average working efficiencies for PEM electrolysis are around 80%, or 82% using the most modern alkaline electrolysers. PEM efficiency is expected to increase to approximately 86% before 2030. Theoretical efficiency for PEM electrolysers are predicted up to 94%.\nConsidering the industrial production of hydrogen, and using current best processes for water electrolysis (PEM or alkaline electrolysis) which have an effective electrical efficiency of 70-82%, producing 1 kg of hydrogen (which has a specific energy of 143 MJ/kg or about 40 kWh/kg) requires 50–55 kWh of electricity. At an electricity cost of $0.06/kWh, as set out in the Department of Energy hydrogen production \ntargets for 2015, the hydrogen cost is $3/kg. With the range of natural gas prices from 2016 as shown in the graph (Hydrogen Production Tech Team Roadmap, November 2017) putting the cost of SMR hydrogen at between $1.20 and $1.50, the cost price of hydrogen via electrolysis is still over double 2015 DOE hydrogen target prices. The US DOE target price for hydrogen in 2020 is $2.30/kg, requiring an electricity cost of $0.037/kWh, which is achievable given recent PPA tenders for wind and solar in many regions. This puts the $4/gge H2 dispensed objective well within reach, and close to a slightly elevated natural gas production cost for SMR.\n\nIn many cases, the advantage of electrolysis over SMR hydrogen is that the hydrogen can be produced on-site, meaning that the costly process of delivery via truck or pipeline is avoided.\n\nIn other parts of the world, steam methane reforming is between $1-3/kg on average. This makes production of hydrogen via electrolysis cost competitive in many regions already, as outlined by Nel Hydrogen and others, including an article by the IEA examining the conditions which could lead to a competitive advantage for electrolysis.\n\nIn addition to reducing the voltage required for electrolysis via the increasing of the temperature of the electrolysis cell it is also possible to electrochemically consume the oxygen produced in an electrolyser by introducing a fuel (such as carbon/coal, methanol, ethanol, formic acid, glycerol, etc.) into the oxygen side of the reactor. This reduces the required electrical energy and has the potential to reduce the cost of hydrogen to less than 40~60% with the remaining energy provided in this manner. In addition, carbon/hydrocarbon assisted electrolysis has the potential to offer a less energy intensive, cleaner method of using chemical energy in various sources of carbon, such as low-rank and high sulfur coals, biomass, alcohols and methane (Natural Gas), where pure CO produced can be easily sequestered without the need for separation.\n\nNuclear radiation routinely breaks water bonds, in the Mponeng gold mine, South Africa, researchers found in a naturally high radiation zone a community dominated by a new phylotype of \"Desulfotomaculum\", feeding on primarily radiolytically produced H. Spent nuclear fuel is also being looked at as a potential source of hydrogen.\n\nWater spontaneously dissociates at around 2500 °C, but this thermolysis occurs at temperatures too high for usual process piping and equipment. Catalysts are required to reduce the dissociation temperature.\n\nThermochemical cycles combine solely heat sources (\"thermo\") with \"chemical\" reactions to split water into its hydrogen and oxygen components. The term \"cycle\" is used because aside from water, hydrogen and oxygen, the chemical compounds used in these processes are continuously recycled. If electricity is partially used as an input, the resulting thermochemical cycle is defined as a hybrid one.\n\nThe sulfur-iodine cycle (S-I cycle) is a thermochemical cycle processes which generates hydrogen from water with an efficiency of approximately 50%. The sulfur and iodine used in the process are recovered and reused, and not consumed by the process. The cycle can be performed with any source of very high temperatures, approximately 950 °C, such as by Concentrating solar power systems (CSP) and is regarded as being well suited to the production of hydrogen by high-temperature nuclear reactors, and as such, is being studied in the High Temperature Test Reactor in Japan. There are other hybrid cycles that use both high temperatures and some electricity, such as the Copper–chlorine cycle, it is classified as a hybrid thermochemical cycle because it uses an electrochemical reaction in one of the reaction steps, it operates at 530 °C and has an efficiency of 43 percent.\n\nFerrosilicon is used by the military to quickly produce hydrogen for balloons. The chemical reaction uses sodium hydroxide, ferrosilicon, and water. The generator is small enough to fit a truck and requires only a small amount of electric power, the materials are stable and not combustible, and they do not generate hydrogen until mixed. The method has been in use since World War I. A heavy steel pressure vessel is filled with sodium hydroxide and ferrosilicon, closed, and a controlled amount of water is added; the dissolving of the hydroxide heats the mixture to about 93 °C and starts the reaction; sodium silicate, hydrogen and steam are produced.\n\nBiological hydrogen can be produced in an algae bioreactor. In the late 1990s it was discovered that if the algae are deprived of sulfur it will switch from the production of oxygen, i.e. normal photosynthesis, to the production of hydrogen. It seems that the production is now economically feasible by surpassing the 7–10 percent energy efficiency (the conversion of sunlight into hydrogen) barrier. with a hydrogen production rate of 10-12 ml per liter culture per hour.\n\nThe conversion of solar energy to hydrogen by means of water splitting process is one of the most interesting ways to achieve clean and renewable energy systems. However, if this process is assisted by photocatalysts suspended directly in water instead of using photovoltaic and an electrolytic system the reaction is in just one step, it can be made more efficient.\n\nBiomass and waste streams can in principle be converted into biohydrogen with biomass gasification, steam reforming, or biological conversion like biocatalysed electrolysis or fermentative hydrogen production.\n\nAmong hydrogen production methods such as steam methane reforming, thermal cracking, coal and biomass gasification and pyrolysis, electrolysis, and photolysis, biological ones are more eco-friendly and less energy intensive. In addition, a wide variety of waste and low-value materials such as agricultural biomass as renewable sources can be utilized to produce hydrogen via biochemical pathways. Nevertheless, at present hydrogen is produced mainly from fossil fuels, in particular, natural gas which are non-renewable sources. Hydrogen is not only the cleanest fuel but also widely used in a number of industries, especially fertilizer, petrochemical and food ones. This makes it logical to investigate alternative sources for hydrogen production. The main biochemical technologies to produce hydrogen are dark and photo fermentation processes. In dark fermentation, carbohydrates are converted to hydrogen by fermentative microorganisms including strict anaerobe and facultative anaerobe bacteria. A theoretical maximum of 4 mol H/mol glucose can be produced and, besides hydrogen, sugars are converted to volatile fatty acids (VFAs) and alcohols as by-products during this process. Photo fermentative bacteria are able to generate hydrogen from VFAs. Hence, metabolites formed in dark fermentation can be used as feedstock in photo fermentation to enhance the overall yield of hydrogen.\n\nFermentative hydrogen production is the fermentative conversion of organic substrate to biohydrogen manifested by a diverse group of bacteria using multi enzyme systems involving three steps similar to anaerobic conversion. Dark fermentation reactions do not require light energy, so they are capable of constantly producing hydrogen from organic compounds throughout the day and night. Photofermentation differs from dark fermentation because it only proceeds in the presence of light. For example, photo-fermentation with Rhodobacter sphaeroides SH2C can be employed to convert small molecular fatty acids into hydrogen.\n\nFermentative hydrogen production can be done using direct biophotolysis by green algae, indirect biophotolysis by cyanobacteria, photo-fermentation by anaerobic photosynthetic bacteria and dark fermentation by anaerobic fermentative bacteria. For example, studies on hydrogen production using \"H. salinarium\", an anaerobic photosynthetic bacteria, coupled to a hydrogenase donor like \"E. coli\", are reported in literature.\n\n\"Enterobacter aerogenes\" is an outstanding hydrogen producer. It is an anaerobic facultative and mesophilic bacterium that is able to consume different sugars and in contrast to cultivation of strict anaerobes, no special operation is required to remove all oxygen from the fermenter. \"E. aerogenes\" has a short doubling time and high hydrogen productivity and evolution rate. Furthermore, hydrogen production by this bacterium is not inhibited at high hydrogen partial pressures; however, its yield is lower compared to strict anaerobes like \"Clostridia\". A theoretical maximum of 4 mol H/mol glucose can be produced by strict anaerobic bacteria. Facultative anaerobic bacteria such as \"E. aerogenes\" have a theoretical maximum yield of 2 mol H/mol glucose.\n\nBiohydrogen can be produced in bioreactors that utilize feedstocks, the most common feedstock being waste streams. The process involves bacteria feeding on hydrocarbons and exhaling hydrogen and CO. The CO can be sequestered successfully by several methods, leaving hydrogen gas. In 2006-2007, NanoLogix first demonstrated a prototype hydrogen bioreactor using waste as a feedstock at Welch's grape juice factory in Pennsylvania (U.S.).\n\nDue to the Thauer limit (four H/glucose) for dark fermentation, a non-natural enzymatic pathway was designed that can generate 12 moles of hydrogen per mole of glucose units of polysaccharides and water in 2007. The stoichiometric reaction is:\n\nThe key technology is cell-free synthetic enzymatic pathway biotransformation (SyPaB). A biochemist can understand it as \"glucose oxidation by using water as oxidant\". A chemist can describe it as \"water splitting by energy in carbohydrate\". A thermodynamics scientist can describe it as the first entropy-driving chemical reaction that can produce hydrogen by absorbing waste heat. In 2009, cellulosic materials were first used to generate high-yield hydrogen. Furthermore, the use of carbohydrate as a high-density hydrogen carrier was proposed so to solve the largest obstacle to the hydrogen economy and propose the concept of sugar fuel cell vehicles.\n\nSynthetic biology\n\nBesides dark fermentation, electrohydrogenesis (electrolysis using microbes) is another possibility. Using microbial fuel cells, wastewater or plants can be used to generate power. Biocatalysed electrolysis should not be confused with biological hydrogen production, as the latter only uses algae and with the latter, the algae itself generates the hydrogen instantly, where with biocatalysed electrolysis, this happens after running through the microbial fuel cell and a variety of aquatic plants can be used. These include reed sweetgrass, cordgrass, rice, tomatoes, lupines and algae.\n\nIn 2014 a low-temperature , atmospheric-pressure enzyme-driven process to convert xylose into hydrogen with nearly 100% of the theoretical yield was announced. The process employs 13 enzymes, including a novel polyphosphatexylulokinase (XK).\n\nCurrently there are two practical ways of producing hydrogen in a renewable industrial process. One is to use power to gas, in which electric power is used to produce hydrogen from electrolysis, and the other is to use landfill gas to produce hydrogen in a steam reformer. Hydrogen fuel, when produced by renewable sources of energy like wind or solar power, is a renewable fuel.\nHydrogen is mainly used for the conversion of heavy petroleum fractions into lighter ones via the process of hydrocracking and other processes (dehydrocyclization and the aromatization process). It is also required for cleaning fossil fuels via hydrodesulfurization.\n\nHydrogen is mainly used for the production of ammonia via Haber process. In this case, the hydrogen is produced \"in situ\". Ammonia is the major component of most fertilizers.\n\nEarlier it was common to vent the surplus hydrogen off, nowadays the process systems are balanced with hydrogen pinch to collect hydrogen for further use.\n\nHydrogen may be used in fuel cells for local electricity generation, making it possible for hydrogen to be used as a transportation fuel for an electric vehicle.\n\nHydrogen is also produced as a by-product of industrial chlorine production by electrolysis. Although requiring expensive technologies, hydrogen can be cooled, compressed and purified for use in other processes on site or sold to a customer via pipeline, cylinders or trucks. The discovery and development of less expensive methods of production of bulk hydrogen is relevant to the establishment of a hydrogen economy.\n\n"}
{"id": "31262483", "url": "https://en.wikipedia.org/wiki?curid=31262483", "title": "Induction regulator", "text": "Induction regulator\n\nAn induction regulator is an alternating current electrical machine, somewhat similar to an induction motor, which can provide a continuously variable output voltage. The induction regulator was an early device used to control the voltage of electric networks. Since the 1930s it has been replaced in distribution network applications by the tap transformer. Its usage is now mostly confined to electrical laboratories, electrochemical processes and arc welding. With minor variations, its setup can be used as a phase-shifting power transformer.\n\nA single-phase induction regulator has a (primary) excitation winding, connected to the supply voltage, wound on a magnetic core which can be rotated. The stationary secondary winding is connected in series with the circuit to be regulated. As the excitation winding is rotated through 180 degrees, the voltage induced in the series winding changes from adding to the supply voltage to opposing it. By selection of the ratios of the number of turns on the excitation and series windings, the range of voltage can be adjusted, say, plus or minus 20% of the supply voltage, for example.\n\nThe three phase induction regulator can be regarded as a wound induction motor. The rotor is not allowed to turn freely and it can be mechanically shifted by means of a worm gear. The rest of the regulator's construction follows that of a wound rotor induction motor with a slotted three-phase stator and a wound three-phase rotor. Since the rotor is not allowed to turn more than 180 degrees, mechanically, the rotor leads can be connected by flexible cables to the exterior circuit. If the stator winding is a two-pole winding, moving the rotor through 180 degrees physically will change the phase of the induced voltage by 180 degrees. A four-pole winding only requires 90 degrees of physical movement to produce 180 degrees of phase shift.\n\nSince a torque is produced by the interaction of the magnetic fields, the movable element is held by a mechanism such as a worm gear. The rotor may be rotated by a hand wheel attached to the machine, or an electric motor can be used to remotely or automatically adjust the rotor position.\n\nDepending on the application, the ratio of number of turns on the rotor and the stator can vary.\n\nSince the single phase regulator only changes the flux linking the excitation and series windings, it does not introduce a phase shift between the supply voltage and the load voltage. However, the varying position of the movable element in the three-phase regulator does create a phase shift. This may be a concern if the load circuit may be connected to more than one supply, since circulating currents will flow owing to the phase shift.\n\nIf the rotor terminals are connected to a three-phase electric power network, a rotating magnetic field will be driven into the magnetic core. \nThe resulting flux will produce an emf on the windings of the stator with the particularity that if rotor and stator are physically shifted by an angle α, then the electric phase shifting of both windings is α too. Considering just the fundamental harmonic, and ignoring the shifting, the following equation rules:\n\nWhere ξ is the winding factor, a constant related to the construction of the windings.\n\nIf the stator winding is connected to the primary phase, the total voltage seen from the neutral (N) will be the sum of the voltages at both windings rotor and stator. Translating this to electric phasors, both phasors are connected. However, there is an angular shifting of α between them. Since α can be freely chosen between [0, π], both phasors can be added or subtracted, so all the values in between are attainable. The primary and secondary are not isolated. Also, the ratio of the magnitudes of voltages between rotor and stator is constant; the resultant voltage varies owing to the angular shifting of the series winding induced voltage.\n\nThe output voltage can be continuously regulated within the nominal range. This is a clear benefit against tap transformers where output voltage takes discrete values. Also, the voltage can be easily regulated under working conditions.\n\nIn comparison to tap transformers, induction regulators are expensive, with lower efficiency, high open circuit currents (due to the airgap) and limited in voltage to less than 20kV.\n\nAn induction regulator for power networks is usually designed to have a nominal voltage of 14kV and ±(10-15)% of regulation, but this use has declined. Nowadays, its main uses are in electrical laboratories and arc welding.\n\n\n"}
{"id": "8750985", "url": "https://en.wikipedia.org/wiki?curid=8750985", "title": "Insulantarctica", "text": "Insulantarctica\n\nInsulantarctica is a biogeographic province of the Antarctic Realm according to the classification developed by Miklos Udvardy in 1975. It comprises scattered islands of the Southern Ocean, which show clear affinity to each other. These islands belong to different countries. Some of them constitute UNESCO's protected areas.\n\n\n\n"}
{"id": "45464818", "url": "https://en.wikipedia.org/wiki?curid=45464818", "title": "January 1886 blizzard", "text": "January 1886 blizzard\n\nThe January 1886 blizzard was caused by a strong extratropical cyclone which initially dropped southeast across Texas before strengthening while it moved through the South and East, near the Eastern Seaboard through New England. The cyclone was at its strongest as it moved by New Jersey. This system formed within an active pattern which brought significant snow to the southern Rockies over many days. The system brought high winds and snowfall near and west of its path, resulting in blizzard conditions across portions of the Plains and East. A significant cold spell was ushered in by this system across portions of the southern and eastern United States. A slightly stronger storm on January 3, 1913 would break January low sea level pressure records originally set by this storm.\n\nA cold wave was ongoing across the northern Plains when this low developed, with temperatures close to near the central Canada–US border. A surface low developed north of Texas on January 6, dropping southeast to the lower Texas coast early on January 7. The cyclone tracked inland of the coast through the Deep South later on January 7 and early January 8, before turning northeast through Georgia and emerging offshore the Virginia Capes late on January 8. The system reached its maximum intensity as it moved near New Jersey late on January 8.\n\nThe lowest pressure measured on land was at Philadelphia, Pennsylvania, Providence, Rhode Island, and Blue Hill Observatory in Massachusetts; the cyclone was considered one of the most extreme to pass through New England at the time. The storm's center then moved inland across Long Island by 11 a.m. on January 9, then across Rhode Island by 1 p.m. and Boston by 2 p.m. before turning northward through Maine around 10 p.m. Temperature readings quickly fell 10-15F (6-9 C) after the passage of the cyclone. High-level cirrus clouds led the center of circulation by 24 hours, with snow occurring 12 hours after the cirrus invasion. While weakening, the storm moved through southeast Quebec on January 10. The low pressure area moved at an average forward motion of through the United States.\n\nFort Macon, North Carolina registered winds up to from the southwest on January 8. The high winds led to the wreck of the \"Cressie Wright\", with six of the seven aboard perishing. Norfolk, Virginia reported high winds which caused damage. High water and wave action from the Atlantic greatly damaged the New Jersey Southern Railroad track in Sandy Hook, New Jersey on January 9. The gale in New York City blew away the anemometer cups at the local weather observing site, and led to numerous maritime mishaps. A blizzard was caused by the high winds and heavy snow at New London, Connecticut. A schooner sank with all aboard offshore Charles Island.\n\nThe schooner \"Alan Greene\" beached near Port Judith, Rhode Island, while the \"Mattie D.\" went ashore near Newport, Rhode Island. This storm reopened an inlet at Martha's Vineyard first opened by a storm in 1856. The inlet would reclose between 1902 and 1903. Provincetown, Massachusetts experienced their worst gale in a decade, with winds peaking at an estimated . Blue Hill Observatory, Massachusetts saw winds peak at . Boston, Massachusetts measured winds as high as and heavy snow. Seven lives were lost in Boston Harbor while 40 vessels went ashore in New England. Eastport, Maine experienced a $20,000 (1886 USD) loss during the storm. Snow and sleet fell at that station, and 19 schooners were damanged, with five sinking in the rough seas.\n\nAs the initial low pressure system formed, a cold front swept through the Plains, stirring up snow which had fallen from a previous blizzard. Although a minimal amount of new snow fell, blizzard conditions resumed in Colorado, Kansas, and Nebraska late on January 6 due to increasing winds. Telegraph wires were downed over a large area, with the rails of the Santa Fe, Burlington, Union, and Kansas Pacific railroads recovered with snow, resuspending travel. Cattle across the region were driven southward. Across the Texas Panhandle, at least five died due to exposure on January 6.\n\nOther locations across the Midwest received blizzard condition's along the inverted trough to the north and northwest of the main low pressure area. On January 7 and 8, this system's associated blizzard led to the largest loss of life from a blizzard in Iowa's history, with 20 perishing. Snow drifts across Kentucky shut down travel on the Louisville and Nashville, Chesapeake and Ohio, and Chesapeake and Southwestern railroads.\n\nIn Jasper, Alabama, a mix of rain, sleet, and snow fell on January 8 and 9. Savannah, Georgia, reported a light snowfall for the first time in six years. On January 8, a site within Wilkes County, North Carolina recorded of snowfall. The South Mountains near Staunton, Virginia reported of snow, with measured in Staunton proper. The snow delayed trains and covered area roads. Harrisonburg, Virginia received of snow during a \"raging blizzard\" the night of January 8. Baltimore, Maryland was blanketed by of snow. Pittsburgh, Pennsylvania recorded of snow from this storm, which was their heavy snowfall on record at the time (this snowfall has since fallen to fifth place).\n\nUp to of snow fell in parts of Long Island, with falling in Central Park. As the system moved through New England, snowfall appeared concentrated along the cyclone's path, with areas of over measured across southeast Connecticut and northeast Massachusetts. A separate maximum of over was noted across central Vermont. A 7:1 snow to liquid ratio (or 7 cm/in of snow for every 1 cm/in of rain) was determined from an average of the available snowfall and liquid equivalent measurements made at the time. Montreal experienced blizzard conditions with heavy snow and wind on January 9.\n\nA significant chunk of arctic air from the north filtered down into the South in the wake of this system. Corpus Christi, Texas saw its temperature plunge 64 F° (38 C°) within 12 hours. Palestine, Texas fell to , its coldest temperature in 40 years. Nashville, Tennessee recorded five consecutive days with lows below between January 8 and January 12, with a minimum of on January 11. Chattanooga, Tennessee fell to on January 9, their coldest reading on record at the time. On January 11, the temperature at Jasper, Alabama fell to . Portions of North Carolina saw temperatures fall well below from January 11 through 14, with readings as low at in Wilkes County, North Carolina on January 12.\n"}
{"id": "30230710", "url": "https://en.wikipedia.org/wiki?curid=30230710", "title": "Kari Aagaard", "text": "Kari Aagaard\n\nKari Aagaard is a Norwegian handball player. She played 92 matches for the Norway women's national handball team between 1973 and 1978. She participated at the 1973 and 1975 World Women's Handball Championship.\n"}
{"id": "28351299", "url": "https://en.wikipedia.org/wiki?curid=28351299", "title": "Keith Burton", "text": "Keith Burton\n\nDr William Keith Burton FRSE (12 October 1922 – 30 December 1996) was English electrical engineer and theoretical physicist notable, according to American physical chemist Robert Alberty, for the publication of his 1957 thermodynamics tables, the first-ever free energy tables for biochemical reactions.\n\nsee\n\nHe was born in Manchester and attended Manchester Grammar School prior to studying Electrical Engineering at Manchester College of Technology. After graduating he gained a job with GEC at first Heywood then Wembley\n\nFrom 1945 until 1951 he was employed as a theoretical physicist with ICI Ltd, being seconded to Bristol university for the final 4 years, lecturing under Nevill Francis Mott and Herbert Fröhlich. From 1951 onwards he lectured in Natural Philosophy at Glasgow University.\n\nHe was elected a Fellow of the Royal Society of Edinburgh in 1958.\n\nHe died at home, 11 Montrose Terrace, Milngavie near Glasgow on 30 December 1996.\n"}
{"id": "31653940", "url": "https://en.wikipedia.org/wiki?curid=31653940", "title": "List of district heating systems", "text": "List of district heating systems\n\nLarge district heating systems ranked on annual heat sales (2001 data based on this article, other figures based on companies' data):\n\nN.B.: Table may not be exhaustive.\n\n\n"}
{"id": "664082", "url": "https://en.wikipedia.org/wiki?curid=664082", "title": "List of nuclear weapons", "text": "List of nuclear weapons\n\nThis is a list of nuclear weapons listed according to country of origin, & then by type within the states.\n\nUS nuclear weapons of all types – bombs, warheads, shells, and others – are numbered in the same sequence starting with the Mark 1 and () ending with the W-91 (which was canceled prior to introduction into service). All designs which were formally intended to be weapons at some point received a number designation. Pure test units which were experiments (and not intended to be weapons) are not numbered in this sequence.\n\nEarly weapons were very large and could only be used as free fall bombs. These were known by \"Mark\" designators, like the Mark 4 which was a development of the Fat Man weapon. As weapons became more sophisticated they also became much smaller and lighter, allowing them to be used in many roles. At this time the weapons began to receive designations based on their role; bombs were given the prefix \"B\", while the same warhead used in other roles, like missiles, would normally be prefixed \"W\". For instance, the W-53 warhead was also used as the basis for the B53 nuclear bomb. Such examples share the same sequence number.\n\nIn other cases, when the modifications are more significant, variants are assigned their own number. An example is the B61 nuclear bomb, which was the parent design for the W80, W81, and W84. There are also examples of out-of-sequence numbering and other prefixes used in special occasions.\n\nThis list includes weapons which were developed to the point of being assigned a model number (and in many cases, prototypes were test fired), but which were then canceled prior to introduction into military service. Those models are listed as canceled, along with the year or date of cancellation of their program.\n\n\n\nSee also Enduring Stockpile.\n\nA number of American weapons designs shared common components between several designs. These include publicly identified models listed below.\n\nAt the peak of its arsenal in 1988, Russia possessed around 45,000 nuclear weapons in its stockpile, roughly 13,000 more than the United States arsenal, the second largest in the world, which peaked in 1966.\n\n\n\n\nFrance is said to have an arsenal of 350 nuclear weapons stockpiled as of 2002.\n\n\nChina is believed to possess around 250 nuclear weapons, but has released very little information about the contents of its arsenal.\n\nIndia is believed to possess between 150-200 nuclear weapons (March 2010 estimate).\n\nIsrael is widely believed to possess a substantial arsenal of nuclear weapons and missiles, estimated at 75-130 and 100-200 warheads, but refuses officially to confirm or deny whether it has a nuclear weapon program, leaving the details of any such weapons unclear. Mordechai Vanunu, a former nuclear technician for Israel, confirmed the existence of a nuclear weapons program in 1986.\n\nUnconfirmed rumors have hinted at tactical nuclear artillery shells, light fission bombs and missile warheads, and perhaps thermonuclear missile warheads.\n\nThe BBC News Online website published an article on 28 May 2008, which quotes former U.S. President Jimmy Carter as stating that Israel has at least 150 nuclear weapons. The article continues to state that this is the second confirmation of Israel's nuclear capability by a U.S. spokesman following comments from U.S. Defense Secretary Robert Gates at a Senate hearing and had apparently been confirmed a short time later by Israeli Prime Minister Ehud Olmert.\n\nAs of March 2010, Pakistan is believed to possess between 90-120 nuclear weapons. The specifications of its weapon production are not disclosed to the public.\nThe main series for nuclear transportation is Hatf.\n\nNorth Korea claims to possess nuclear weapons, however, the specifications of its systems are not public. It is estimated to have 6-18 low yield nuclear weapons (August 2012 estimate). On 9 October 2006, North Korea achieved its first nuclear detonation.\n\nOn 25 May 2009, North Korea conducted a second test of nuclear weapons at the same location as the original test. The test weapon was of the same magnitude as the atomic bombs dropped on Japan in the 2nd World War. At the same time of the test, North Korea tested two short range ballistic missiles. The country tested a 7 kt nuclear weapon on 2 February 2013. On 3 September 2017, North Korea conducted an underground thermonuclear test which had an estimated yield of 100kt to 250kt, according to various sources.\n\nSouth Africa built six or seven gun-type weapons. All constructed weapons were verified by International Atomic Energy Agency and other international observers to have been dismantled, along with the complete weapons program, and their highly enriched uranium was reprocessed back into low enriched form unsuitable for weapons.\n\n\n\n"}
{"id": "12117361", "url": "https://en.wikipedia.org/wiki?curid=12117361", "title": "Manas River", "text": "Manas River\n\nThe Manas River (Pron: ˈmʌnəs; in Bhutan Drangme Chhu; in China Niamjang) is a transboundary river in the Himalayan foothills between southern Bhutan and India.\nIt is named after Manasa, the serpent god in Hindu mythology. It is the largest river system of Bhutan, among its four major river systems; the other three are Amo Chu or Torsa river, Wong Chu or Raidak, Mo Chu or Sankosh. It is met by three other major streams before it again debouches into India in western Assam. The total length of the river is , flows through Bhutan for and then through Assam for before it joins the mighty Brahmaputra River at Jogighopa. Another major tributary of the Manas, the Aie river joins it in Assam at Bangpari.\n\nThe river valley has two major reserve forest areas, namely the Royal Manas National Park (, established in 1966) in Bhutan and the contiguous Manas Wildlife Sanctuary ( in 1955 increased to in December 1985) encompassing Project Tiger reserve, an elephant teserve and a biosphere reserve, which constitutes a UNESCO World Heritage Site declared in December 1985.\n\nThe Manas River drains of eastern Bhutan and northeast India. It has three major branches: the Drangme Chhu, Mangde Chhu, and Bumthang Chhu that cover most of eastern Bhutan, with the Tongsa and Bumthang valleys also forming part of its catchment. The area drained in Bhutan territory is 18,300 km and is bound by the geographical coordinates . A part of the main stem of the river rises in the southern Tibet before entering into India at Bumla pass at the northwestern corner of Arunachal Pradesh.\n\nThe river flows through Bhutan in a south-west direction between two ranges of the Lower Himalayas in V-shaped gorges and enters into Assam in India into the south-central foot hills of the Himalayas. The valley opens up in the foot hills; marked by the formation of swamps and marshes in the plains. The upper catchment is snow bound while the middle and lower catchment are thickly forested.\nThe river system as a whole in Bhutan constitutes a length of , the main stem of the river is the Manas or Gongri river, which originates in the West Kameng District of Arunachal Pradesh in India and after flowing in a south westerly direction (the rivers in Bhutan generally flow from northwest to southwest) enters Bhutan near Tashigang. At Tashigong, it is joined by the Kulong Chu, which rises in the northern Himalaya snow ranges of Bhutan. At Tashigong, the bed width of the river is about and river bed elevation is . The river Kulong Chu is formed by two rivers namely, the Tongsa (Mangde) Chu that rises in northern Bhutan near Kula Kangri Peak where the elevation is and the Bumthang River, also called Murchangphy Chu; the combined stream joins the Manas river.\n\nThe Lhobrak, or Kuri Chhu, is the main central tributary of the Manas. It is the only river that rises north of the Great Himalayas and it joins the Manas in southern Bhutan; further downstream of the combined stream joining at the Tongsa Chu. Here, the river bed level is . After flowing in a generally south-westerly direction for about in Bhutan, the Aie river enters In India into the Goalpara district of Assam at the village of Agrong. From here, it follows a meandering course for about and outfalls into the Brahmaputra near Jogigopa. Aie River, which rises in the Black mountains at an altitude of about near the village of Bangpari, is about in length. The total length of the Manas measured along its longest tributary the Kur is , out of which about lies in India.\n\nThe river valley in the foothills is surrounded by small meadows located among thickly deciduous forested foothills with many rivulets, streams and natural drainage channels related to the river system. In the lower reaches of the river, there are many smooth sandy stretches populated with trees.\n\nThe foot hills in Bhutan and India known as Terai and the Duars (A Sanskrit word meaning \"passes\" or \"gates\"), which encompass stretch of the foot hills and which are very fertile. Each Duar (with elevation varying from to near sea level as they join the Brahmaputra river) is named after a stream or hemmed between two streams. Some part of the Duars in Bhutan was under British control. The fertile lands have been developed into tea estates and paddy fields.\n\nThe catchment is wholly mountainous, rising within the space of from an elevation of about near the Indian border to the great Himalayan peaks at over along the main Himalayan range bordering Bhutan and Tibet. The huge elevation range and varied climatic conditions are reflected in the great ecological diversity and rich fauna and flora in the river catchment.\n\nThe climate is extremely varied, ranging from hot and humid subtropical conditions in the south to cold and dry alpine conditions in the north. From May to October, the southwest monsoon brings heavy rainfall—more than to the southern part—and there is a pronounced dry season in winter. Further north, the rainfall is generally scanty, of the order of , recorded during June–August.\n\nThe difference between maximum and minimum river flow in monsoon and the dry months is said to be as much as 20 times. The Manas river, the largest north bank tributary of the Brahmaputra, has a recorded maximum discharge of 7,641 cubic metres and contributes 5.48% of the total flows of the Brahmaputra. Its total length up to its confluence with the Brahmaputra is ( in hills and balance in plains) and raises at an altitude of. It has a total catchment area of out of which 85.9% is in hills and plains.\n\nOut of the large catchment of the river valley, protected or reserved areas have been specifically demarcated, both in Bhutan and India, which are declared national parks or sanctuaries. The two reserved forest and wild life areas cover an area of , which account for about 24% of the total catchment area of of the Manas river valley; brief details are provided.\n\nRoyal Manas National Park in southern Bhutan, considered the national heritage of Bhutan, was declared a wildlife sanctuary in 1996 and subsequently raised to the status of a national park in 1993. Covering an area of , the area is forested to the extent of 92% and is a well-preserved Eastern Himalayan ecosystem. The park is bounded on its north by the Black Mountain National Park and the Manas Tiger Reserve on its South in India. Within Bhutan, biological corridors linked with the park are the Thrumshingla National Park in the North, Phibsoo Wildlife Sanctuary in the West, and Khaling Wildlife Sanctuary in the East. As researched so far, the park has a large diversity of tropical grasslands, temperate moist forests, alpine meadows, and scrublands. The various flora and fauna identified in the park are 45 species of mammals, 366 species of birds and 900 species of vascular plants. World Wildlife Fund (WWF) is actively working on a conservation management plan in association with local wildlife authorities to preserve and protect this national heritage.\nSome of the important faunal species identified are: the royal Bengal tiger, elephant, gaur (\"Bos gaurus\"), four rare species of golden langur (\"Trachypithecus geei\"), pygmy hog (\"Sus salvanius\"), hispid hare (\"Caprolagus hispidus\"), greater one-horned rhinoceros (\"Rhinoceros unicornis\") and wild Asiatic water buffalo (\"Bubalus arnee\"). There are 362 species of birds, out of which four species of hornbills (rufous-necked, wreathed, pied and great Indian) have been recorded. Apart from the Ganges river dolphin (\"Platanista gangetica\") other aquan species identified in the river are the deep bodied mahseer (\"Tor tor\"), golden mahseer (\"Tor putitora\"), and chocolate mahseer or katle (\"Acrossocheilus hexangonolepis\"). Five thousand people live within the park limits in several villages. Tigers, the most revered animal in Bhutan, are estimated to number about 100 and are mostly in this national park and the neighbouring Manas National Park in India\n\nThe world's rarest monkeys, the golden langur, which flourish in dense forests, \"with its long tail with a tassel at the end, are found both in Bhutan and India, in the two reserved forest sanctuaries. These monkeys found in groups have no hair on their black face but have generous golden ruff on their body. They are found in large numbers – 180 in India and 1200 in Bhutan, as per counts made in 1978 and 1980 respectively.\n\nThe Manas Wild Life Sanctuary, located in Assam, is considered as one of \"Asia's finest wild life reserves\" and is a UNESCO World Heritage Site. It is contiguous to the Bhutan's Royal National Park to its north. It now encompasses a bio reserve, a tiger reserve and also an elephant reserve. The park is well forested and also comprises grass lands and marshes. In 1928, the core of the area was designated a sanctuary and in 1978 it was declared a tiger reserve.\n\nManas River and another tributary of Manas, the Hakua, flow through the sanctuary. A thick mantle of alluvium is the dominant soil of the terrain. Under subtropical climatic conditions (with of annual rainfall and temperature varying from a maximum of and a minimum of ), the forest consists of the semi-evergreen forest vegetation with mixed deciduous, littorals and swamps, and interspersed with bamboo and cane. Flooding occurs in large parts of the Bio Reserve.\n\nThe park is managed under several conservation management units such as the Core Zone, the Buffer Zone, and the Economic Zone. The park is known for its rare and endangered endemic wildlife such as the tigers, elephants, Assam roofed turtle, hispid hare, golden and capped langurs and pygmy hog, one-horned rhinoceros, Asiatic buffalo, swamp deer, barking deers, leopards, clouded leopards, marbled cats, sloth bears, hoolock gibbons, wild boar, gharials, crocodiles, and river dolphins. Reptiles consist of pythons, common Indian crocodile, common wolf snake, cat snake and many other species. Birds listed include hornbills, common crane, common redshank, Eurasian woodcock, spotted eagles, black-throated diver, little grebe, different types of herons, black ibis, Eurasian sparrowhawk, spot-bellied eagle-owl and several others. The park is home for 22 endangered mammal species. Fish species identified are: katli, jurraha, chenga, telliah, labeo and mahaseer. Conservation measures have been undertaken to prevent poaching, overfishing, encroachments, and many other related issues. These measures are meant to minimise human interference in the fragile core zone, creation of a data base and carry out research on animal and plant populations for better conservation of the ecosystem.\n\nIn 1980, the park has been central to the Bodoland agitation of Assam as the Bodos dominating the area took refuge in the sanctuary. Bodos have been demanding autonomy or a separate state of their own on grounds that their lands were incorporated into Assam during the British Raj.\n\nOne of the development projects planned in the past on the Manas River envisaged flood control in the Brahmaputra River and augmentation of flows in the Ganga river system by building a dam on the river at the Indo-Bhutan border. The stored water stored behind the reservoir was proposed to be transferred through a long canal system through the foot hills of the Himalayas (skirting Bangladesh) crossing 25 major and minor rivers, out of which the major rivers the Sankosh, Raidak, Amo (Torsa), Kartoya, Teesta, Atrai and Mahananda through North Bengal, and finally out falling into the Kosi River in North Bihar. The project has not proceeded further due to several adverse public opinion and environmental concerns.\n\nThe proposal mooted in the 1970s to build a dam on the river for multipurpose uses of power, irrigation and flood control in Assam involved a long canal from the Manas reservoir to another reservoir on the Sankosh River. As the canal was passing through the Manas Tiger Reserve, the Ministry of Environment and Forests of Government of India objected to the proposal on the grounds of adverse impact that would occur due to the dam on the Hydrology and ecology of the area. This view was also supported by the World Heritage Committee of UNESCO. The late Prime Minister of India, Rajiv Gandhi upheld the objections and decided to discontinue with the project. It is unlikely to be revived.\nThe proposal had been mooted as a joint project of India and Bhutan. The pre-feasibility report prepared for this Manas multipurpose project envisaged power generation of 2800 MW. Another cooperation project on the Mandgde Chu, in central Bhutan, tributary of Manas, envisages power generation of 360/600 MW for which Detailed Project Report (DPR) is under preparation.\n\nThe Manas River has often been the centre of environmental controversies, particularly in the 1980s. Two dams were proposed on the Bhutanese side of the river to provide hydroelectric power and to control the flow of the Brahmaputra on its northern bank and make way for irrigation schemes. However, there were not only local concerns but national and international ones among environmentalists with regard to the proposals who lobbied enough support to ensure that the dam proposals were dropped in 1986. In February 1989, the All Bodo Students Union (ABSU) invaded the park and killed several wardens and guards, permitting the entry of poachers and loggers who posed an immediate threat to the wildlife of the park and its river. The threat of flooding remained as ever in 2010.\n\n"}
{"id": "47506706", "url": "https://en.wikipedia.org/wiki?curid=47506706", "title": "Moglicë Hydro Power Plant", "text": "Moglicë Hydro Power Plant\n\nMoglicë Hydro Power Plant is a large hydroelectricity plant under construction on the river Devoll situated near the village Moglicë, Albania. The project consists of a large power plant with an installed capacity of 172 MW and an average annual production of 475 GWh. It is built by Devoll Hydropower, an Albanian company owned by Norwegian power company Statkraft. The asphalt-core rock-fill dam will be 320 m long, 150 m high and 460 m wide. The dam is anticipated to be the world's highest of its kind upon completion. The reservoir will have a surface area of 7.2 km, and a storage capacity of about 360 million m. The power plant is part of the Devoll Hydropower Project and construction on it began in June 2013. It is expected to be completed in 2018.\n"}
{"id": "21273", "url": "https://en.wikipedia.org/wiki?curid=21273", "title": "Neon", "text": "Neon\n\nNeon is a chemical element with symbol Ne and atomic number 10. It is a noble gas. Neon is a colorless, odorless, inert monatomic gas under standard conditions, with about two-thirds the density of air. It was discovered (along with krypton and xenon) in 1898 as one of the three residual rare inert elements remaining in dry air, after nitrogen, oxygen, argon and carbon dioxide were removed. Neon was the second of these three rare gases to be discovered and was immediately recognized as a new element from its bright red emission spectrum. The name neon is derived from the Greek word, , neuter singular form of (\"neos\"), meaning new. Neon is chemically inert, and no uncharged neon compounds are known. The compounds of neon currently known include ionic molecules, molecules held together by van der Waals forces and clathrates.\n\nDuring cosmic nucleogenesis of the elements, large amounts of neon are built up from the alpha-capture fusion process in stars. Although neon is a very common element in the universe and solar system (it is fifth in cosmic abundance after hydrogen, helium, oxygen and carbon), it is rare on Earth. It composes about 18.2 ppm of air by volume (this is about the same as the molecular or mole fraction) and a smaller fraction in Earth's crust. The reason for neon's relative scarcity on Earth and the inner (terrestrial) planets is that neon is highly volatile and forms no compounds to fix it to solids. As a result, it escaped from the planetesimals under the warmth of the newly ignited Sun in the early Solar System. Even the outer atmosphere of Jupiter is somewhat depleted of neon, although for a different reason. It is also lighter than air, causing it to escape even from Earth's atmosphere.\n\nNeon gives a distinct reddish-orange glow when used in low-voltage neon glow lamps, high-voltage discharge tubes and neon advertising signs. The red emission line from neon also causes the well known red light of helium–neon lasers. Neon is used in some plasma tube and refrigerant applications but has few other commercial uses. It is commercially extracted by the fractional distillation of liquid air. Since air is the only source, it is considerably more expensive than helium.\n\nNeon (Greek (\"néon\"), neuter singular form of , meaning \"new\"), was discovered in 1898 by the British chemists Sir William Ramsay (1852–1916) and Morris W. Travers (1872–1961) in London. Neon was discovered when Ramsay chilled a sample of air until it became a liquid, then warmed the liquid and captured the gases as they boiled off. The gases nitrogen, oxygen, and argon had been identified, but the remaining gases were isolated in roughly their order of abundance, in a six-week period beginning at the end of May 1898. First to be identified was krypton. The next, after krypton had been removed, was a gas which gave a brilliant red light under spectroscopic discharge. This gas, identified in June, was named \"neon\", the Greek analogue of \"novum\" (new), suggested by Ramsay's son. The characteristic brilliant red-orange color emitted by gaseous neon when excited electrically was noted immediately. Travers later wrote: \"the blaze of crimson light from the tube told its own story and was a sight to dwell upon and never forget.\"\n\nA second gas was also reported along with neon, having approximately the same density as argon but with a different spectrum – Ramsay and Travers named it \"metargon\". However, subsequent spectroscopic analysis revealed it to be argon contaminated with carbon monoxide. Finally, the same team discovered xenon by the same process, in September 1898.\n\nNeon's scarcity precluded its prompt application for lighting along the lines of Moore tubes, which used nitrogen and which were commercialized in the early 1900s. After 1902, Georges Claude's company Air Liquide produced industrial quantities of neon as a byproduct of his air-liquefaction business. In December 1910 Claude demonstrated modern neon lighting based on a sealed tube of neon. Claude tried briefly to sell neon tubes for indoor domestic lighting, due to their intensity, but the market failed because homeowners objected to the color. In 1912, Claude's associate began selling neon discharge tubes as eye-catching advertising signs and was instantly more successful. Neon tubes were introduced to the U.S. in 1923 with two large neon signs bought by a Los Angeles Packard car dealership. The glow and arresting red color made neon advertising completely different from the competition. The intense color and vibrancy of neon equated with American society at the time, suggesting a \"century of progress\" and transforming cities into sensational new environments filled with radiating advertisements and \"electro-graphic architecture\".\n\nNeon played a role in the basic understanding of the nature of atoms in 1913, when J. J. Thomson, as part of his exploration into the composition of canal rays, channeled streams of neon ions through a magnetic and an electric field and measured the deflection of the streams with a photographic plate. Thomson observed two separate patches of light on the photographic plate (see image), which suggested two different parabolas of deflection. Thomson eventually concluded that some of the atoms in the neon gas were of higher mass than the rest. Though not understood at the time by Thomson, this was the first discovery of isotopes of stable atoms. Thomson's device was a crude version of the instrument we now term a mass spectrometer.\n\nNeon is the second lightest inert gas. Neon has three stable isotopes: Ne (90.48%), Ne (0.27%) and Ne (9.25%). Ne and Ne are partly primordial and partly nucleogenic (i.e. made by nuclear reactions of other nuclides with neutrons or other particles in the environment) and their variations in natural abundance are well understood. In contrast, Ne (the chief primordial isotope made in stellar nucleosynthesis) is not known to be nucleogenic or radiogenic. The causes of the variation of Ne in the Earth have thus been hotly debated.\n\nThe principal nuclear reactions generating nucleogenic neon isotopes start from Mg and Mg, which produce Ne and Ne respectively, after neutron capture and immediate emission of an alpha particle. The neutrons that produce the reactions are mostly produced by secondary spallation reactions from alpha particles, in turn derived from uranium-series decay chains. The net result yields a trend towards lower Ne/Ne and higher Ne/Ne ratios observed in uranium-rich rocks such as granites. Ne may also be produced in a nucleogenic reaction, when Ne absorbs a neutron from various natural terrestrial neutron sources.\n\nIn addition, isotopic analysis of exposed terrestrial rocks has demonstrated the cosmogenic (cosmic ray) production of Ne. This isotope is generated by spallation reactions on magnesium, sodium, silicon, and aluminium. By analyzing all three isotopes, the cosmogenic component can be resolved from magmatic neon and nucleogenic neon. This suggests that neon will be a useful tool in determining cosmic exposure ages of surface rocks and meteorites.\n\nSimilar to xenon, neon content observed in samples of volcanic gases is enriched in Ne and nucleogenic Ne relative to Ne content. The neon isotopic content of these mantle-derived samples represents a non-atmospheric source of neon. The Ne-enriched components are attributed to exotic primordial rare-gas components in the Earth, possibly representing solar neon. Elevated Ne abundances are found in diamonds, further suggesting a solar-neon reservoir in the Earth.\n\nNeon is the second-lightest noble gas, after helium. It glows reddish-orange in a vacuum discharge tube. Also, neon has the narrowest liquid range of any element: from 24.55 K to 27.05 K (−248.45 °C to −245.95 °C, or −415.21 °F to −410.71 °F). It has over 40 times the refrigerating capacity (per unit volume) of liquid helium and three times that of liquid hydrogen. In most applications it is a less expensive refrigerant than helium.\nNeon plasma has the most intense light discharge at normal voltages and currents of all the noble gases. The average color of this light to the human eye is red-orange due to many lines in this range; it also contains a strong green line, which is hidden, unless the visual components are dispersed by a spectroscope.\n\nTwo quite different kinds of neon lighting are in common use. Neon glow lamps are generally tiny, with most operating between 100 and 250 volts. They have been widely used as power-on indicators and in circuit-testing equipment, but light-emitting diodes (LEDs) now dominate in those applications. These simple neon devices were the forerunners of plasma displays and plasma television screens. Neon signs typically operate at much higher voltages (2–15 kilovolts), and the luminous tubes are commonly meters long. The glass tubing is often formed into shapes and letters for signage, as well as architectural and artistic applications.\n\nStable isotopes of neon are produced in stars. Ne is created in fusing helium and oxygen in the alpha process. This requires temperatures above 100 megakelvins, which are only available in the cores of stars of more than 3 solar masses.\n\nNeon is abundant on a universal scale; it is the fifth most abundant chemical element in the universe by mass, after hydrogen, helium, oxygen, and carbon (see chemical element). Its relative rarity on Earth, like that of helium, is due to its relative lightness, high vapor pressure at very low temperatures, and chemical inertness, all properties which tend to keep it from being trapped in the condensing gas and dust clouds that formed the smaller and warmer solid planets like Earth.\n\nNeon is monatomic, making it lighter than the molecules of diatomic nitrogen and oxygen which form the bulk of Earth's atmosphere; a balloon filled with neon will rise in air, albeit more slowly than a helium balloon.\n\nNeon's abundance in the universe is about 1 part in 750; in the Sun and presumably in the proto-solar system nebula, about 1 part in 600. The Galileo spacecraft atmospheric entry probe found that even in the upper atmosphere of Jupiter, the abundance of neon is reduced (depleted) by about a factor of 10, to a level of 1 part in 6,000 by mass. This may indicate that even the ice-planetesimals which brought neon into Jupiter from the outer solar system, formed in a region which was too warm to retain the neon atmospheric component (abundances of heavier inert gases on Jupiter are several times that found in the Sun).\n\nNeon comprises 1 part in 55,000 in the Earth's atmosphere, or 18.2 ppm by volume (this is about the same as the molecule or mole fraction), or 1 part in 79,000 of air by mass. It comprises a smaller fraction in the crust. It is industrially produced by cryogenic fractional distillation of liquefied air.\n\nOn 17 August 2015, based on studies with the Lunar Atmosphere and Dust Environment Explorer (LADEE) spacecraft, NASA scientists reported the detection of neon in the exosphere of the moon.\n\nNeon is the first p-block noble gas, and the first element with a true octet of electrons. It is inert: as is the case with its lighter analogue, helium, no strongly bound neutral molecules containing neon have been identified. The ions [NeAr], [NeH], and [HeNe] have been observed from optical and mass spectrometric studies. Solid neon clathrate hydrate was produced from water ice and neon gas at pressures 0.35–0.48 GPa and temperatures about −30 °C. Ne atoms are not bonded to water and can freely move through this material. They can be extracted by placing the clathrate into a vacuum chamber for several days, yielding ice XVI, the least dense crystalline form of water.\n\nThe familiar Pauling electronegativity scale relies upon chemical bond energies, but such values have obviously not been measured for inert helium and neon. The Allen electronegativity scale, which relies only upon (measurable) atomic energies, identifies neon as the most electronegative element, closely followed by fluorine and helium.\n\nNeon is often used in signs and produces an unmistakable bright reddish-orange light. Although tube lights with other colors are often called \"neon\", they use different noble gases or varied colors of fluorescent lighting.\n\nNeon is used in vacuum tubes, high-voltage indicators, lightning arresters, wavemeter tubes, television tubes, and helium–neon lasers. Liquefied neon is commercially used as a cryogenic refrigerant in applications not requiring the lower temperature range attainable with more extreme liquid-helium refrigeration.\n\nNeon, as liquid or gas, is relatively expensive – for small quantities, the price of liquid neon can be more than 55 times that of liquid helium. Driving neon's expense is the rarity of neon, which unlike helium, can only be obtained from air.\n\nThe triple point temperature of neon (24.5561 K) is a defining fixed point in the International Temperature Scale of 1990.\n\n"}
{"id": "13900777", "url": "https://en.wikipedia.org/wiki?curid=13900777", "title": "Noah Idechong", "text": "Noah Idechong\n\nNoah Idechong is an environmental activist from Palau, and former chief of Palau's Governmental Division of Marine Resources, where he helped draft the Island's first marine conservation legislation in 1997. Having linked marine science, fisheries, and political reform in Palau, he was awarded the Goldman Environmental Prize in 1995 for his efforts in marine conservation. Idechong helped found the Palau Conservation Society, which spearheaded revivals in traditional conservation, now proven as effective management measures in the Micronesia region. He later served as a member of the House of Delegates of Palau, helping to enact progressive, conservation-oriented public programs, such as the Protected Areas Network, the Micronesia Challenge, and the word's first Shark Sanctuary.\n"}
{"id": "36226173", "url": "https://en.wikipedia.org/wiki?curid=36226173", "title": "Off line regulator", "text": "Off line regulator\n\nAn off line regulator, off-line regulator, or offline regulator is an electronic voltage regulation or current regulation device that is designed to directly accept electric power obtained from an alternating current utility power source.\n\nThis electronics design terminology has no relationship to the use of \"online and offline\" for computers and networking, and no relationship with uninterruptible power supplies that provide power while disconnected from the electrical grid.\n\nAn off line regulator can be a complete integrated circuit with all capabilities necessary to provide clean power to a small portable or handheld device, or it may be used as part of a larger switched mode power supply (SMPS) or DC-DC converter.\n\nAlthough alternating current is commonly described as being \"120 V\" or \"240 V\", this is the root mean square of the actual full peak voltage of the AC sinewave, which is +/- 169 V for 120 V and +/- 338 V for 240 V.\n\nAdditionally, 120 V and 240 V are considered nominal voltages, and actual voltages may be somewhat higher or lower during normal operation. The range of this variability is not well-defined but can be as much as +/- 20 volts (100 V - 140 V and 220 V - 260 V). This pushes the peak line voltage up to +/- 198 V for 120 V and +/- 368 V for 240 V.\n\nOff line regulators must also be tolerant of voltage spikes, surges, brownouts, and other power quality conditions that may affect the electronic device.\n\n[1] http://www.onsemi.com/PowerSolutions/parametrics.do?id=449\n"}
{"id": "21028827", "url": "https://en.wikipedia.org/wiki?curid=21028827", "title": "Oil megaprojects (2020)", "text": "Oil megaprojects (2020)\n\nThis page summarizes projects that propose to bring more than of new liquid fuel capacity to market with the first production of fuel beginning in 2020. This is part of the Wikipedia summary of Oil Megaprojects. \n"}
{"id": "10641961", "url": "https://en.wikipedia.org/wiki?curid=10641961", "title": "Pallas (daughter of Triton)", "text": "Pallas (daughter of Triton)\n\nIn Greek mythology, Pallas (/ˈpæləs/; Ancient Greek: Παλλάς) was the daughter of Triton, son of Poseidon and messenger of the seas.\n\nAfter Athena was born fully armed from Zeus' forehead, Triton acting as a foster parent to the goddess raised her alongside his own daughter, Pallas. The sea god taught both girls the arts of war.\n\nDuring an athletics festival, Pallas and Athena fought with spears in a friendly mock battle, where the victor who be whoever managed to disarm their opponent. At the beginning of the fight, Athena got the upper hand, until Pallas took over. Before she could win, Zeus, who was in attendance, fearing to see his own daughter lose, distracted Pallas with the Aegis, which she had once shown interest in. Pallas, stunned in awe, stood still as Athena, expecting her to dodge, impaled Pallas, accidentally.\n\nOut of sadness and regret, she created the palladium, a statue in the likeness of Pallas and wrapped the aegis, which she had feared, about the breast of it, and set it up beside Zeus and honored it. Later, Athena took on the title Pallas as tribute to her late friend.\n\nThis story about Athena and Pallas inspired a yearly festival in Libya dedicated to the goddess. A passage by Herodotus recounts this custom: \"Next to these Machlyes are the Auseans; these and the Machlyes, separated by the Triton, live on the shores of the Tritonian lake. The Machlyes wear their hair long behind, the Auseans in front.\"\"They celebrate a yearly festival of Athena, where their maidens are separated into two bands and fight each other with stones and sticks, thus (they say) honoring in the way of their ancestors that native goddess whom we call Athena. Maidens who die of their wounds are called false virgins.\"\"Before the girls are set fighting, the whole people choose the fairest maid, and arm her with a Corinthian helmet and Greek panoply, to be then mounted on a chariot and drawn all along the lake shore.\"\"With what armor they equipped their maidens before Greeks came to live near them, I cannot say; but I suppose the armor was Egyptian; for I maintain that the Greeks took their shield and helmet from Egypt.\"\n\nPallas is mentioned in the poem \"The Raven\" by American writer Edgar Allan Poe as well as by Pierre de Ronsard in the Second Book of his 'Sonnets For Helen': Sonnet 12, Line 13: \"Ou bien tu es Pallas, ou bien l'une des Graces.\" (Translation: \"Or perhaps you are Pallas, or one of the Graces.\")\n\n"}
{"id": "43690057", "url": "https://en.wikipedia.org/wiki?curid=43690057", "title": "Petroleum Authority of Uganda", "text": "Petroleum Authority of Uganda\n\nThe Petroleum Authority of Uganda (PAOU), also known as the Uganda National Petroleum Authority, is governmental organisation that regulates the petroleum industry in Uganda, the third-largest economy in the East African Community. Its responsibilities include licensing, regulation, supervision of exploration, harvesting, refining, marketing, and disposal of petroleum products in the country. Although owned by the Ugandan government, it is expected to act independently.\n\nThe law authorising the creation of PAOU is known as the Petroleum (Exploration, Development and Production) Act 2013. It was passed by the Ugandan parliament on 7 December 2012 and was assented to by the president of Uganda on 12 March 2013. The law was gazetted on 15 April 2013. PAOU was established once the president named its board of directors and once Uganda's parliament approved the board members. In August 2017, PAOU advertised 37 mainly managerial positions, in preparation for expected first oil in 2020.\n\nThe headquarters of PAOU is at 34-36 Lugard Avenue, in Entebbe, on the northern shores of Lake Victoria. The coordinates of the headquarters of the authority are:0°03'15.0\"N, 32°28'52.0\"E (Latitude:0.054167; Longitude:32.481111).\n\nThe chairperson of the board is Jane Mulemwa, a Ugandan chemist and academic. Other members of the board are Reuben Kashambuzi, Immaculate Semanda Nakimera, Peter Lominit, Doreen Kabasindi Wandera, Patrick Nakoko, and Kiryowa Kiwanuka. In June 2016, Ernest Rubondo was appointed as executive director and chief executive officer of PAOU. \n\n\n"}
{"id": "498674", "url": "https://en.wikipedia.org/wiki?curid=498674", "title": "Polyacrylamide", "text": "Polyacrylamide\n\nPolyacrylamide (IUPAC poly(2-propenamide) or poly(1-carbamoylethylene), abbreviated as PAM) is a polymer (-CHCHCONH-) formed from acrylamide subunits. It can be synthesized as a simple linear-chain structure or cross-linked, typically using \"N\",\"N\"-methylenebisacrylamide. In the cross-linked form, the possibility of the monomer being present is reduced even further. It is highly water-absorbent, forming a soft gel when hydrated, used in such applications as polyacrylamide gel electrophoresis, and can also be called ghost crystals when cross-linked, and in manufacturing soft contact lenses. In the straight-chain form, it is also used as a thickener and suspending agent. More recently, it has been used as a subdermal filler for aesthetic facial surgery (see Aquamid).\n\nOne of the largest uses for polyacrylamide is to flocculate solids in a liquid. This process applies to water treatment, and processes like paper making and screen printing. Polyacrylamide can be supplied in a powder or liquid form, with the liquid form being subcategorized as solution and emulsion polymer.\nEven though these products are often called 'polyacrylamide', many are actually copolymers of acrylamide and one or more other chemical species, such as an acrylic acid or a salt thereof. The main consequence of this is to give the 'modified' polymer a particular ionic character.\n\nAnother common use of polyacrylamide and its derivatives is in subsurface applications such as Enhanced Oil Recovery. High viscosity aqueous solutions can be generated with low concentrations of polyacrylamide polymers, and these can be injected to improve the economics of conventional waterflooding.\n\nThe linear soil conditioning form was developed in the 1950s by Monsanto Company and was marketed under the trade name Krilium. The soil conditioning technology was presented at a symposium on \"Improvement of Soil Structure\" held in Philadelphia, Pennsylvania on December 29, 1951. The technology was strongly documented and was published in the June 1952 issue of the journal \"Soil Science\", volume 73, June 1952 that was dedicated to polymeric soil conditioners.\nThe original formulation of Krilium was difficult to use because it contained calcium which cross-linked the linear polymer under field conditions. Even with a strong marketing campaign, Krilium was abandoned by Monsanto.\nAfter 34 years, the journal \"Soil Science\" wanted to update the soil conditioning technology and published another dedicated issue on polymeric soil conditioner and especially linear, water-soluble, anionic polyacrylamide in the May 1986 issue, volume 141, issue number 5.\nThe Foreword, written by Arthur Wallace from UCLA and Sheldon D. Nelson from BYU stated in part:\nThe new water-soluble soil conditioners may, if used according to established procedures\nConsequently, these translate into\nThe cross-linked form which retains water is often used for horticultural and agricultural under trade names such as Broadleaf P4, Swell-Gel, and so on.\n\nThe anionic form of linear, water soluble polyacrylamide is frequently used as a soil conditioner on farm land and construction sites for erosion control, in order to protect the water quality of nearby rivers and streams.\n\nThe polymer is also used to make Gro-Beast toys, which expand when placed in water, such as the Test Tube Aliens. Similarly, the absorbent properties of one of its copolymers can be utilized as an additive in body-powder.\n\nThe ionic form of polyacrylamide has found an important role in the potable water treatment industry. Trivalent metal salts, like ferric chloride and aluminum chloride, are bridged by the long polymer chains of polyacrylamide. This results in significant enhancement of the flocculation rate. This allows water treatment plants to greatly improve the removal of total organic content (TOC) from raw water.\n\nPolyacrylamide is also often used in molecular biology applications as a medium for electrophoresis of proteins and nucleic acids in a technique known as PAGE.\n\nIt was also used in the synthesis of the first Boger fluid.\n\nThe primary functions of polyacrylamide soil conditioners are to increase soil tilth, aeration, and porosity and reduce compaction, dustiness and water run-off. Secondary functions are to increase plant vigor, color, appearance, rooting depth and emergence of seeds while decreasing water requirements, diseases, erosion and maintenance expenses. FC 2712 is used for this purpose.\n\nIn dilute aqueous solution, such as is commonly used for Enhanced Oil Recovery applications, polyacrylamide polymers are susceptible to chemical, thermal, and mechanical degradation. Chemical degradation occurs when the labile amide moiety hydrolyzes at elevated temperature or pH, resulting in the evolution of ammonia and a remaining carboxyl group. Thus, the degree of anionicity of the molecule increases. Thermal degradation of the vinyl backbone can occur through several possible radical mechanisms, including the autooxidation of small amounts of iron and reactions between oxygen and residual impurities from polymerization at elevated temperature. Mechanical degradation can also be an issue at the high shear rates experienced in the near-wellbore region.\n\nConcerns have been raised that polyacrylamide used in agriculture may contaminate food with acrylamide, a known neurotoxin and carcinogen. While polyacrylamide itself is relatively non-toxic, it is known that commercially available polyacrylamide contains minute residual amounts of acrylamide remaining from its production, usually less than 0.05% w/w.\n\nAdditionally, there are concerns that polyacrylamide may de-polymerise to form acrylamide. In a study conducted in 2003 at the Central Science Laboratory in Sand Hutton, England, polyacrylamide was treated similarly as food during cooking. It was shown that these conditions do not cause polyacrylamide to de-polymerise significantly.\n\nIn a study conducted in 1997 at Kansas State University, the effect of environmental conditions on polyacrylamide were tested, and it was shown that degradation of polyacrylamide under certain conditions can cause the release of acrylamide. The experimental design of this study as well as its results and their interpretation have been questioned, and a 1999 study by the Nalco Chemical Company did not replicate the results.\n\n"}
{"id": "30471288", "url": "https://en.wikipedia.org/wiki?curid=30471288", "title": "Portoscuso Wind Farm", "text": "Portoscuso Wind Farm\n\nThe Portoscuso Wind Farm is a proposed wind power project in Portoscuso, Sardinia, Italy. It will have 39 individual wind turbines with a nominal output of around 2.3 MW each which will deliver up to 90 MW of power, enough to power over 70,000 homes, with a capital investment required of approximately €100 million. The wind farm will have an electricity production of 185 GWh per year that will save the emission of 130,000 tonnes of carbon dioxide.\n"}
{"id": "33085532", "url": "https://en.wikipedia.org/wiki?curid=33085532", "title": "Pyridinium p-toluenesulfonate", "text": "Pyridinium p-toluenesulfonate\n\nPyridinium \"p\"-toluenesulfonate (PPTS) is a salt of pyridine and \"p\"-toluenesulfonic acid. It is a colourless solid.\n\nIn organic synthesis, PPTS is used as a weakly acidic catalyst, providing an organic soluble source of pyridinium (CHNH) ions. For example, PPTS is used to deprotect silyl ethers or tetrahydropyranyl ethers when a substrate is unstable to stronger acid catalysts. It is also a commonly used catalyst for the preparation of acetals and ketals from aldehydes and ketones.\n"}
{"id": "105658", "url": "https://en.wikipedia.org/wiki?curid=105658", "title": "Pyrophosphate", "text": "Pyrophosphate\n\nIn chemistry, a pyrophosphate is a phosphorus oxyanion. Compounds such as salts and esters are also called pyrophosphates. The group is also called diphosphate or dipolyphosphate, although this should not be confused with phosphates. As a food additive, diphosphates are known as E450. A number of hydrogen pyrophosphates also exist, such as NaHPO, as well as the normal pyrophosphates.\n\nPyrophosphates were originally prepared by heating phosphates (\"pyro\" from the Greek, meaning \"fire\"). They generally exhibit the highest solubilities among the phosphates; moreover, they are good complexing agents for metal ions (such as calcium and many transition metals) and have many uses in industrial chemistry. Pyrophosphate is the first member of an entire series of polyphosphates.\n\nThe term pyrophosphate is also the name of esters formed by the condensation of a phosphorylated biological compound with inorganic phosphate, as for dimethylallyl pyrophosphate. This bond is also referred to as a high-energy phosphate bond.\n\nThe synthesis of tetraethyl pyrophosphate was first described in 1854 by Philippe de Clermont at a meeting of the French Academy of Sciences.\n\nPyrophosphates are very important in biochemistry. The anion is abbreviated PP and is formed by the hydrolysis of ATP into AMP in cells.\n\nFor example, when a nucleotide is incorporated into a growing DNA or RNA strand by a polymerase, pyrophosphate (PP) is released. Pyrophosphorolysis is the reverse of the polymerization reaction in which pyrophosphate reacts with the 3′-nucleosidemonophosphate (NMP or dNMP), which is removed from the oligonucleotide to release the corresponding triphosphate (dNTP from DNA, or NTP from RNA).\n\nThe pyrophosphate anion has the structure , and is an acid anhydride of phosphate. It is unstable in aqueous solution and hydrolyzes into inorganic phosphate:\nor in biologists' shorthand notation:\n\nIn the absence of enzymic catalysis, hydrolysis reactions of simple polyphosphates such as pyrophosphate, linear triphosphate, ADP, and ATP normally proceed extremely slowly in all but highly acidic media.\n\nThis hydrolysis to inorganic phosphate effectively renders the cleavage of ATP to AMP and PP irreversible, and biochemical reactions coupled to this hydrolysis are irreversible as well.\n\nPP occurs in synovial fluid, blood plasma, and urine at levels sufficient to block calcification and may be a natural inhibitor of hydroxyapatite formation in extracellular fluid (ECF). Cells may channel intracellular PP into ECF. ANK is a nonenzymatic plasma-membrane PP channel that supports extracellular PP levels. Defective function of the membrane PP channel ANK is associated with low extracellular PP and elevated intracellular PP. Ectonucleotide pyrophosphatase/phosphodiesterase (ENPP) may function to raise extracellular PP.\n\nFrom the standpoint of high energy phosphate accounting, the hydrolysis of ATP to AMP and PP requires two high-energy phosphates, as to reconstitute AMP into ATP requires two phosphorylation reactions.\n\nThe plasma concentration of inorganic pyrophosphate has a reference range of 0.58–3.78 µM (95% prediction interval).\n\nIsopentenyl pyrophosphate converts to geranyl pyrophosphate the precursor to tens of thousand of terpenes and terpenoids. \nVarious diphosphates are used as emulsifiers, stabilisers, acidity regulators, raising agents, sequestrants, and water retention agents in food processing. They are classified in the E number scheme under E450:\n\nIn particular, various formulations of diphosphates are used to stabilize whipped cream.\n\n"}
{"id": "187584", "url": "https://en.wikipedia.org/wiki?curid=187584", "title": "Pyrotechnics", "text": "Pyrotechnics\n\nPyrotechnics is the science of using materials capable of undergoing self-contained and self-sustained exothermic chemical reactions for the production of heat, light, gas, smoke and/or sound. Its etymology stems from the Greek words \"pyro\" (\"fire\") and \"tekhnikos\" (\"made by art\"). Pyrotechnics include not only the manufacture of fireworks but items such as safety matches, oxygen candles, explosive bolts and fasteners, components of the automotive airbag and gas pressure blasting in mining, quarrying and demolition.\n\nIndividuals responsible for the safe storage, handling, and functioning of pyrotechnic devices are referred to as pyrotechnicians.\n\nExplosions, flashes, smoke, flames, fireworks or other pyrotechnic driven effects used in the entertainment industry are referred to as theatrical special effects, special effects, or proximate pyrotechnics. Proximate refers to the pyrotechnic device's location relative to an audience. In the majority of jurisdictions, special training and licensing must be obtained from local authorities to legally prepare and use proximate pyrotechnics.\n\nMany musical groups use pyrotechnics to enhance their live shows. Pink Floyd were innovators of pyrotechnic use in concerts. For instance, at the climax of their song \"Careful with That Axe, Eugene\", a blast of smoke was set off at the back of the stage. Bands such as The Who, KISS and Queen soon followed with use of pyrotechnics in their shows. Michael Jackson attempted using pyrotechnics in a 1984 Pepsi advertisement, where a stray spark caused a small fire in his hair. German industrial metal band Rammstein are renowned for their incorporation of a large variety of pyrotechnics into performances, which range from flaming costumes to face-mounted flamethrowers. Nightwish, Lordi and Green Day are also known for their vivid pyrotechnics in concert. Many professional wrestlers have also used pyrotechnics as part of their entrances to the ring.\n\nModern pyrotechnics are, in general, divided into categories based upon the type of effect produced or manufacturing method. The most common categories are:\n\nA basic theatrical effect, designed to create a jet or fountain of sparks, is referred to as a gerb. A gerb consists of a sufficiently strong and non-flammable container to hold the pyrotechnic compound. Typical pyrotechnic formulations consist either of flammable materials such as nitrocellulose and/or black powder or a mixture of a fuel and oxidizer blended in situ. A plug placed at one end of the container with a small orifice, called a choke, constricts the expulsion of the ignited pyrotechnic compound, increasing the size and aggressiveness of the jet.\nVarious ingredients may be added to pyrotechnic devices to provide colour, smoke, noise or sparks. Special additives and construction methods are used to modify the character of the effect produced, either to enhance or subdue the effect; for example, sandwiching layers of pyrotechnic compounds containing potassium perchlorate, sodium salicylate or sodium benzoate with layers that do not creates a fountain of sparks with an undulating whistle.\n\nIn general, such pyrotechnic devices are initiated by a remotely controlled electrical signal that causes an electric match, or e-match, to produce ignition. The remote control may be manual, via a switch console, or computer controlled according to a pre-programmed sequence and/or a sequence that tracks the live performance via stage cues.\n\nDisplay pyrotechnics, also known as commercial fireworks, are pyrotechnic devices intended for use outdoors, where the audience can be further away, and smoke and fallout is less of a concern. Generally the effects, though often similar to proximate pyrotechnics, are of a larger size and more vigorous in nature. It will typically take an entire day to set up a professional fireworks display. The size of these fireworks can range from 50 mm (2\") to over 600 mm (24\") diameter depending on the type of effect and available distance from the audience. In most jurisdictions, special fireworks training and licensing must be obtained from local authorities to legally prepare and use display pyrotechnics.\n\nConsumer pyrotechnics are devices readily available for purchase to the general public with little or no special licensing or training. These items are considered \"relatively\" low hazard devices but, like all pyrotechnics, can still be hazardous and should be stored, handled and used appropriately. Some of the most common examples of consumer pyrotechnics encountered include recreational fireworks (including whistling and sparking types), model rocket motors, highway and marine distress flares, sparklers and caps for toy guns. Pyrotechnics are also indirectly involved in other consumer products such as powder actuated nail guns, ammunition for firearms, and modern fireplaces. Some types, including bird scarers, shell crackers, whistle crackers and flares, may be designed to be fired from a 12-gauge pistol or rifle.\n\nPyrotechnics are dangerous and must be handled and used properly. Recently, several high-profile incidents involving pyrotechnics have re-enforced the need to respect these explosives at all times. Proximate pyrotechnics is an area of expertise that requires additional training beyond that of other professional pyrotechnics areas and the use of devices specifically manufactured for indoor, close proximity use.\n\nA common low-budget pyrotechnic flash pot is built using modified screw-in electric fuses in a common light fixture. The fuses are intentionally blown, acting as ignitors for a pyrotechnic material.\n\nHomemade devices may fail to include safety features and can provide numerous hazards, including:\n\n\nCommercial flash pots include safety features such as warning pilot lamps, preignition grounding, and safing circuits. They also use isolated and low-voltage power sources, and have keyed power connections to help prevent accidental ignition.\n\nPyrotechnics can be dangerous substances that must always be treated with the utmost respect and with the proper training. Due to the hazardous nature of these materials, precautions must always be taken to ensure the safety of all individuals in the vicinity of pyrotechnics. Despite all precautions, accidents and errors occur from time to time, which may result in property damage, injury and in severe cases loss of life. These incidents may be the result of poorly manufactured product, unexpected or unforeseen events, or in many cases, the result of operator error.\n\nSome of the more widely publicized incidents involving pyrotechnics in recent history include:\n\nAt Olympic Stadium in Montreal during the Guns N' Roses/Metallica Stadium Tour on August 8, 1992, Metallica frontman James Hetfield was the victim of a severe pyrotechnics accident during the song \"Fade to Black\", in which a pyrotechnic charge exploded. Hetfield's guitar protected him from the full force of the blast; however, the fire engulfed his left side, burning his hand, arm, eyebrows, face and hair. He suffered second and third-degree burns, but was back on stage 17 days later.\n\nIn 2003, improper use of pyrotechnics caused a fire in a Rhode Island nightclub called The Station. The Station nightclub fire was started when fireworks used by the band Great White accidentally ignited flammable soundproofing foam. The pyrotechnics in question were not appropriate. The foam caused combustion to spread rapidly and the resulting fire led to 100 deaths, apparently because their quick escape was blocked by ineffective exit doors. While the type of foam used and the lack of a sprinkler system were important factors in the fire, the Great White fire could likely have been prevented had those involved paid attention to standard safety practices around the use of pyrotechnics.\n\nA similar pyrotechnic-induced fire in 2004 destroyed the Republica Cromagnon nightclub in Buenos Aires, Argentina, killing 194 people.\n\nIn May 2000, a small fire led to two massive explosions at the SE Fireworks Depot in Enschede, the Netherlands, leaving 23 people dead, 947 people injured, and an estimated 2,000 homes damaged or destroyed.\n\nIn March 2008, A Pyrotechnic lead cable snapped during the end of the show pyrotechnics, causing some minor burns and injuries to the crowd at WWE Wrestlemania 24 leading to a big investigation.\n\nIn February 2010, a pyrotechnic flame engulfed WWE wrestler Mark Calaway. He was seen with multiple burns throughout the show.\n\nIn July 2012, a WWE RAW pyrotechnic rehearsal caused a small fire on the led wall before doors opened. Crews put it out before it could get any bigger. No one was injured but the doors were opened thirty minutes late.\n\nOn January 27, 2013, at the \"Kiss\" nightclub in Santa Maria, Brazil, an accident due to the use of pyrotechnics by the performing live show band caused a fire which resulted in the deaths of at least 236 people, while dozens suffered serious injuries from the fire and smoke inhalation.\n\nIn January 2015 a fireworks factory in Granada, Colombia exploded injuring one person.\n\nOn June 13, 2015 Michael Clifford of 5 Seconds of Summer suffered face, hair, and shoulder injuries from a pyrotechnics accident on the Rock Out With Your Socks Out tour at the SSE Wembley Arena, London. Calum Hood suffered a minor burn on his arm in the incident.\n\nOn October 30, 2015, at the Colectiv nightclub in Bucharest, Romania, pyrotechnics used by the band Goodbye to Gravity accidentally ignited soundproofing foam on a pillow. The fire quickly spread onto the ceiling and the rest of the club. This led to the death of 64 people and injured approximately 200 others. Four members of Goodbye to Gravity lost their lives, and only their soloist survived. It remains the worst night club fire in Romania's history.\n\n\n\n"}
{"id": "36964590", "url": "https://en.wikipedia.org/wiki?curid=36964590", "title": "Resistograph", "text": "Resistograph\n\nResistograph is an electronic high-resolution needle drill resistance measurement device, developed by Frank Rinn (since 1986 in Heidelberg, Germany).\n\nA thin, long needle is driven into the wood. The electric power consumption of the drilling device is measured, recorded and printed. Resistograph devices are different from other resistance drills because they provide a high linear correlation between the measured values and the density of the penetrated wood. They are used for inspecting trees and timber in order to find internal defects, and to determine wood density and growth rates.\n\n\n"}
{"id": "20600093", "url": "https://en.wikipedia.org/wiki?curid=20600093", "title": "Sandwich theory", "text": "Sandwich theory\n\nSandwich theory describes the behaviour of a beam, plate, or shell which consists of three layers—two facesheets and one core. The most commonly used sandwich theory is linear and is an extension of first order beam theory. Linear sandwich theory is of importance for the design and analysis of sandwich panels, which are of use in building construction, vehicle construction, airplane construction and refrigeration engineering.\n\nSome advantages of sandwich construction are:\n\nThe behavior of a beam with sandwich cross-section under a load differs from a beam with a constant elastic cross section. If the radius of curvature during bending is large compared to the thickness of the sandwich beam and the strains in the component materials are small, the deformation of a sandwich composite beam can be separated into two parts\n\nSandwich beam, plate, and shell theories usually assume that the reference stress state is one of zero stress. However, during curing, differences of temperature between the face-sheets persist because of the thermal separation by the core material. These temperature differences, coupled with different linear expansions of the face-sheets, can lead to a bending of the sandwich beam in the direction of the warmer face-sheet. If the bending is constrained during the manufacturing process, residual stresses can develop in the components of a sandwich composite. The superposition of a reference stress state on the solutions provided by sandwich theory is possible when the problem is linear. However, when large elastic deformations and rotations are expected, the initial stress state has to be incorporated directly into the sandwich theory.\n\nIn the engineering theory of sandwich beams, the axial strain is assumed to vary linearly over the cross-section of the beam as in Euler-Bernoulli theory, i.e.,\nTherefore, the axial stress in the sandwich beam is given by\nwhere formula_3 is the Young's modulus which is a function of the location along the thickness of the beam. The bending moment in the beam is then given by\nThe quantity formula_5 is called the flexural stiffness of the sandwich beam. The shear force formula_6 is defined as\n\nUsing these relations, we can show that the stresses in a sandwich beam with a core of thickness formula_8 and modulus formula_9 and two facesheets each of thickness formula_10 and modulus formula_11, are given by\n\nFor a sandwich beam with identical facesheets and unit width, the value of formula_5 is \nIf formula_15, then formula_5 can be approximated as\nand the stresses in the sandwich beam can be approximated as\nIf, in addition, formula_19, then\nand the approximate stresses in the beam are\nIf we assume that the facesheets are thin enough that the stresses may be assumed to be constant through the thickness, we have the approximation\n\nHence the problem can be split into two parts, one involving only core shear and the other involving only bending stresses in the facesheets.\n\nThe main assumptions of linear sandwich theories of beams with thin facesheets are:\nHowever, the xz shear-stresses in the core are not neglected.\n\nThe constitutive relations for two-dimensional orthotropic linear elastic materials are\nThe assumptions of sandwich theory lead to the simplified relations\nand\n\nThe equilibrium equations in two dimensions are\nThe assumptions for a sandwich beam and the equilibrium equation imply that\nTherefore, for homogeneous facesheets and core, the strains also have the form\n\nLet the sandwich beam be subjected to a bending moment formula_29 and a shear force formula_30. Let the total deflection of the beam due to these loads be formula_31. The adjacent figure shows that, for small displacements, the total deflection of the mid-surface of the beam can be expressed as the sum of two deflections, a pure bending deflection formula_32 and a pure shear deflection formula_33, i.e.,\n\nFrom the geometry of the deformation we observe that the engineering shear strain (formula_35) in the core is related the effective shear strain in the composite by the relation\nNote the shear strain in the core is larger than the effective shear strain in the composite and that small deformations (formula_37) are assumed in deriving the above relation. The effective shear strain in the beam is related to the shear displacement by the relation\n\nThe facesheets are assumed to deform in accordance with the assumptions of Euler-Bernoulli beam theory. The total deflection of the facesheets is assumed to be the superposition of the deflections due to bending and that due to core shear. The formula_39-direction displacements of the facesheets due to bending are given by\nThe displacement of the top facesheet due to shear in the core is\nand that of the bottom facesheet is\nThe normal strains in the two facesheets are given by\nTherefore,\n\nThe shear stress in the core is given by\nor,\n\nThe normal stresses in the facesheets are given by\nHence,\n\nThe resultant normal force in a face sheet is defined as\nand the resultant moments are defined as\nwhere\nUsing the expressions for the normal stress in the two facesheets gives\n\nIn the core, the resultant moment is\nThe total bending moment in the beam is\nor,\n\nThe shear force formula_6 in the core is defined as\n\nwhere formula_58 is a shear correction coefficient. The shear force in the facesheets can be computed from the bending moments using the relation\nor,\n\nFor thin facesheets, the shear force in the facesheets is usually ignored.\n\nThe bending stiffness of the sandwich beam is given by\nFrom the expression for the total bending moment in the beam, we have\nFor small shear deformations, the above expression can be written as\nTherefore, the bending stiffness of the sandwich beam (with formula_19) is given by\n\nand that of the facesheets is\n\nThe shear stiffness of the beam is given by\nTherefore, the shear stiffness of the beam, which is equal to the shear stiffness of the core, is\n\nA relation can be obtained between the bending and shear deflections by using the continuity of tractions between the core and the facesheets. If we equate the tractions directly we get\nAt both the facesheet-core interfaces formula_70 but at the top of the core formula_71 and at the bottom of the core formula_72. Therefore, traction continuity at formula_73 leads to\nThe above relation is rarely used because of the presence of second derivatives of the shear deflection. Instead it is assumed that \nwhich implies that\n\nUsing the above definitions, the governing balance equations for the bending moment and shear force are\nWe can alternatively express the above as two equations that can be solved for formula_31 and formula_33 as\nUsing the approximations\nwhere formula_82 is the intensity of the applied load on the beam, we have\n\nSeveral techniques may be used to solve this system of two coupled ordinary differential equations given the applied load and the applied bending moment and displacement boundary conditions.\n\nAssuming that each partial cross section fulfills Bernoulli's hypothesis, the balance of forces and moments on the deformed sandwich beam element can be used to deduce the bending equation for the sandwich beam.\nThe stress resultants and the corresponding deformations of the beam and of the cross section can be seen in Figure 1. The following relationships can be derived using the theory of linear elasticity:\nwhere\n\nSuperposition of the equations for the facesheets and the core leads to the following equations for the total shear force formula_30 and the total bending moment formula_29:\nWe can alternatively express the above as two equations that can be solved for formula_31 and formula_35, i.e.,\n\nThe bending behavior and stresses in a continuous sandwich beam can be computed by solving the two governing differential equations.\n\nFor simple geometries such as double span beams under uniformly distributed loads, the governing equations can be solved by using appropriate boundary conditions and using the superposition principle. Such results are listed in the standard DIN EN 14509:2006(Table E10.1). Energy methods may also be used to compute solutions directly.\n\nThe differential equation of sandwich continuous beams can be solved by the use of numerical methods such as finite differences and finite elements. For finite differences Berner recommends a two-stage approach. After solving the differential equation for the normal forces in the cover sheets for a single span beam under a given load, the energy method can be used to expand the approach for the calculation of multi-span beams. Sandwich continuous beam with flexible cover sheets can also be laid on top of each other when using this technique. However, the cross-section of the beam has to be constant across the spans.\n\nA more specialized approach recommended by Schwarze involves solving for the homogeneous part of the governing equation exactly and for the particular part approximately. Recall that the governing equation for a sandwich beam is\nIf we define\nwe get\nSchwarze uses the general solution for the homogeneous part of the above equation and a polynomial approximation for the particular solution for sections of a sandwich beam. Interfaces between sections are tied together by matching boundary conditions. This approach has been used in the open source code swe2.\n\nResults predicted by linear sandwich theory correlate well with the experimentally determined results. The theory is used as a basis for the structural report which is needed for the construction of large industrial and commercial buildings which are clad with sandwich panels . Its use is explicitly demanded for approvals and in the relevant engineering standards.\n\nMohammed Rahif Hakmi and others conducted researches into numerical, experimental behavior of materials and fire and blast behavior of Composite material. He published multiple research articles:\n\n\nHakmi developed a design method, which had been recommended by the CIB Working Commission W056 Sandwich Panels, ECCS/CIB Joint Committee and has been used in the European recommendations for the design of sandwich panels (CIB, 2000).\n\n\n\n"}
{"id": "1584112", "url": "https://en.wikipedia.org/wiki?curid=1584112", "title": "Sawdust", "text": "Sawdust\n\nSawdust or wood dust is a by-product or waste product of woodworking operations such as sawing, milling, planing, routing, drilling and sanding. It is composed of fine particles of wood. These operations can be performed by woodworking machinery, portable power tools or by use of hand tools. Wood dust is also the byproduct of certain animals, birds and insects which live in wood, such as the woodpecker and carpenter ant. In some manufacturing industries it can be a significant fire hazard and source of occupational dust exposure. \n\nSawdust is the main component of particleboard. Wood dust is a form of particulate matter, or particulates. Research on wood dust health hazards comes within the field of occupational health science, and study of wood dust control comes within the field of indoor air quality engineering.\n\nTwo waste products, dust and chips, form at the working surface during woodworking operations such as sawing, milling and sanding. These operations both shatter lignified wood cells and break out whole cells and groups of cells. Shattering of wood cells creates dust, while breaking out of whole groups of wood cells creates chips. The more cell-shattering that occurs, the finer the dust particles that are produced. For example, sawing and milling are mixed cell shattering and chip forming processes, whereas sanding is almost exclusively cell shattering.\n\nA major use of sawdust is for particleboard; coarse sawdust may be used for wood pulp. Sawdust has a variety of other practical uses, including serving as a mulch, as an alternative to clay cat litter, or as a fuel. Until the advent of refrigeration, it was often used in icehouses to keep ice frozen during the summer. It has been used in artistic displays, and as scatter in miniature railroad and other models. It is also sometimes used to soak up liquid spills, allowing the spill to be easily collected or swept aside. As such, it was formerly common on barroom floors. It is used to make Cutler's resin. Mixed with water and frozen, it forms pykrete, a slow-melting, much stronger form of ice.\n\nSawdust is used in the manufacture of charcoal briquettes. The claim for invention of the first commercial charcoal briquettes goes to Henry Ford who created them from the wood scraps and sawdust produced by his automobile factory.\n\nCellulose, fibre starch that is indigestible to humans, and a filler in some low calorie foods, can be and is made from sawdust, as well as from other plant sources. While there is no documentation for the persistent rumor, based upon Upton Sinclair's novel \"The Jungle\", that sawdust was used as a filler in sausage, cellulose derived from sawdust was and is used for sausage casings. Sawdust-derived cellulose has also been used as a filler in bread. \n\nWhen cereals were scarce, sawdust was sometimes an ingredient in Kommissbrot. Auschwitz concentration camp survivor, Dr. Miklós Nyiszli, reports in \"Auschwitz: A Doctor's Eyewitness Account\" that the subaltern medical staff, who served Dr. Josef Mengele, subsisted on \"bread made from wild chestnuts sprinkled with sawdust.\"\n\nAirborne sawdust and sawdust accumulations present a number of health and safety hazards. Wood dust becomes a potential health problem when, for example, the wood particles, from processes such as sanding, become airborne and are inhaled. Wood dust is a known human carcinogen. Certain woods and their dust contain toxins that can produce severe allergic reactions.\n\nBreathing airborne wood dust may cause allergic respiratory symptoms, mucosal and non-allergic respiratory symptoms, and cancer. In the USA, lists of carcinogenic factors are published by the American Conference of Governmental Industrial Hygienists (ACGIH), the Occupational Safety and Health Administration (OSHA), and the National Institute for Occupational Safety and Health (NIOSH). All these organisations recognize wood dust as carcinogenic in relation to the nasal cavities and paranasal sinuses.\n\nPeople can be exposed to wood dust in the workplace by breathing it in, skin contact, or eye contact. The Occupational Safety and Health Administration (OSHA) has set the legal limit (permissible exposure limit) for wood dust exposure in the workplace as 15 mg/m total exposure and 5 mg/m respiratory exposure over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 1 mg/m over an 8-hour workday.\n\nAs with all airborne particulates, wood dust particle sizes are classified with regard to effect on the human respiratory system. For this classification, the unit for measurement of particle sizes is the micrometre or micron (μm), where 1 micrometre = 1 micron. Particles below 50 μm are not normally visible to the naked human eye. Particles of concern for human respiratory health are those <100 μm (where the symbol < means ‘less than’).\n\nZhang (2004) has defined the size of indoor particulates according to respiratory fraction:\nParticles which precipitate in the vicinity of the mouth and eyes, and get into the organism, are defined as the inhalable fraction, that is total dust. Smaller fractions, penetrating into the non-cartilage respiratory tract, are defined as respirable dust. Dust emitted in the wood industry is characterized by the dimensional disintegration of particles up to 5 μm, and that\nis why they precipitate mostly in the nasal cavity, increasing the risk of cancer of the upper respiratory tract.\n\nThe parameter most commonly used to characterize exposures to wood dust in air is total wood dust concentration, in mass per unit volume. In countries that use the metric system, this is usually measured in mg/m³ (milligram per cubic metre)\n\nA study to estimate occupational exposure to inhalable wood dust by country, industry, the level of exposure and type of wood dust in 25 member states of the European Union (EU-25) found that in 2000–2003, about 3.6 million workers (2.0% of the employed EU-25 population) were occupationally exposed to inhalable wood dust. The highest exposure levels were estimated to occur in the construction sector and furniture industry.\n\nWood dust is known to be a human carcinogen, based on sufficient evidence of carcinogenicity from studies in humans. It has been demonstrated through human epidemiologic studies that exposure to wood dust increases the occurrence of cancer of the nose (nasal cavities and paranasal sinuses). An association of wood dust exposure and cancers of the nose has been observed in numerous case reports, cohort studies, and case control studies specifically addressing nasal cancer.\n\nWater-borne bacteria digest organic material in leachate, but use up much of the available oxygen. This high \"biological oxygen demand\" can suffocate fish and other organisms. There is an equally detrimental effect on beneficial bacteria, so it is not at all advisable to use sawdust within home aquariums, as was once done by hobbyists seeking to save some expense on activated charcoal.\n\nSawdust is flammable and accumulations provide a ready source of fuel. Airborne sawdust can be ignited by sparks or even heat accumulation and result in explosions.\n\nAt sawmills, unless reprocessed into particleboard, burned in a sawdust burner or used to make heat for other milling operations, sawdust may collect in piles and add harmful leachates into local water systems, creating an environmental hazard. This has placed small sawyers and environmental agencies in a deadlock.\n\nQuestions about the science behind the determination of sawdust being an environmental hazard remain for sawmill operators (though this is mainly with finer particles), who compare wood residuals to dead trees in a forest. Technical advisors have reviewed some of the environmental studies, but say most lack standardized methodology or evidence of a direct impact on wildlife. They don’t take into account large drainage areas, so the amount of material that is getting into the water from the site in relation to the total drainage area is minuscule.\n\nOther scientists have a different view, saying the \"dilution is the solution to pollution\" argument is no longer accepted in environmental science. The decomposition of a tree in a forest is similar to the impact of sawdust, but the difference is of scale. Sawmills may be storing thousands of cubic metres of wood residues in one place, so the issue becomes one of concentration.\n\nBut of larger concern are substances such as lignins and fatty acids that protect trees from predators while they are alive, but can leach into water and poison wildlife. Those types of things remain in the tree and, as the tree decays, they slowly are broken down. But when sawyers are processing a large volume of wood and large concentrations of these materials permeate into the runoff, the toxicity they cause is harmful to a broad range of organisms.\n\nTo lower the concentration of airborne dust concentrations during woodworking, dust extraction systems are used. These can be divided into two types. The first are local exhaust ventilation systems, the second are room ventilation systems. Use of personal respirators, a form of personal protective equipment, can also isolate workers from dust.\n\nThese rely on air pulled with a suction force through piping systems from the point of dust formation to a waste disposal unit. LEV systems consist of four elements: dust hoods at the point of dust formation, ventilation ducts, an air cleaning device (waste separator or dust collector) and an air moving device (a fan, otherwise known as an impeller). The air, containing dust and chips from the woodworking operation, is sucked by an impeller. The impeller is usually built into, or placed close to, the waste disposal unit, or dust collector.\n\nGuidelines of performance for woodworking LEV systems exist, and these tie into occupational air quality regulations that exist in many countries. The LEV guidelines often referred to are those set by the ACIAH.\n\nLow-volume/high-velocity capture systems are a specialised type of LEV that use an extractor hood designed as an integral part of the tool or positioned very close to the operating point of the cutting tool. The hood is designed to provide high capture velocities, often greater than 50 m/s (10,000 fpm) at the contaminant release point. This high velocity is accompanied by airflows often less than 0.02m3/s (50 cfm) resulting from the small face area of the hood that is used. These systems have come into favour for portable power tools, although adoption of the technology is not widespread. Festool is one manufacturer of portable power tools using LVHV ventilation integrated into the tool design.\n\nIf suitably designed, general ventilation can also be used as a control of airborne dust. General ventilation can often help reduce skin and clothing contamination, and dust deposition on surfaces.\n\nWithin industry, many countries have air quality regulations. This is to help ensure that wood dust is extracted to a level that ensures specified maximum allowable residual airborne dust concentrations and worker exposure levels.\n\n\n\n"}
{"id": "2410184", "url": "https://en.wikipedia.org/wiki?curid=2410184", "title": "Tau neutrino", "text": "Tau neutrino\n\nThe tau neutrino or tauon neutrino is a subatomic elementary particle which has the symbol and no net electric charge. Together with the tau, it forms the third generation of leptons, hence the name tau neutrino. Its existence was immediately implied after the tau particle was detected in a series of experiments between 1974 and 1977 by Martin Lewis Perl with his colleagues at the SLAC–LBL group. The discovery of the tau neutrino was announced in July 2000 by the DONUT collaboration (Direct Observation of the Nu Tau).\n\nThe tau neutrino is last of the leptons, and is the second most recent particle of the Standard Model to be discovered. The DONUT experiment from Fermilab was built during the 1990s to specifically detect the tau neutrino. These efforts came to fruition in July 2000, when the DONUT collaboration reported its detection.\n\n"}
{"id": "30043", "url": "https://en.wikipedia.org/wiki?curid=30043", "title": "Tellurium", "text": "Tellurium\n\nTellurium is a chemical element with symbol Te and atomic number 52. It is a brittle, mildly toxic, rare, silver-white metalloid. Tellurium is chemically related to selenium and sulfur. It is occasionally found in native form as elemental crystals. Tellurium is far more common in the universe as a whole than on Earth. Its extreme rarity in the Earth's crust, comparable to that of platinum, is due partly to its high atomic number, but also to its formation of a volatile hydride which caused it to be lost to space as a gas during the hot nebular formation of the planet.\n\nTellurium-bearing compounds were first discovered in 1782 in a gold mine in Zlatna, Romania by Austrian mineralogist Franz-Joseph Müller von Reichenstein, although it was Martin Heinrich Klaproth who named the new element in 1798 after the Latin word for \"earth\", \"tellus\". Gold telluride minerals are the most notable natural gold compounds. However, they are not a commercially significant source of tellurium itself, which is normally extracted as a by-product of copper and lead production.\n\nCommercially, the primary use of tellurium is copper and steel alloys, where it improves machinability. Applications in CdTe solar panels and semiconductors also consume a considerable portion of tellurium production.\n\nTellurium has no biological function, although fungi can use it in place of sulfur and selenium in amino acids such as tellurocysteine and telluromethionine. In humans, tellurium is partly metabolized into dimethyl telluride, (CH)Te, a gas with a garlic-like odor exhaled in the breath of victims of tellurium exposure or poisoning.\n\nTellurium has two allotropes, crystalline and amorphous. When crystalline, tellurium is silvery-white with a metallic luster. It is a brittle and easily pulverized metalloid. Amorphous tellurium is a black-brown powder prepared by precipitating it from a solution of tellurous acid or telluric acid (Te(OH)). Tellurium is a semiconductor that shows a greater electrical conductivity in certain directions depending on atomic alignment; the conductivity increases slightly when exposed to light (photoconductivity). When molten, tellurium is corrosive to copper, iron, and stainless steel. Of the chalcogens, tellurium has the highest melting and boiling points, at and , respectively.\n\nTellurium adopts a polymeric structure consisting of zig-zag chains of Te atoms. This gray material resists oxidation by air and is not volatile.\n\nNaturally occurring tellurium has eight isotopes. Six of those isotopes, Te, Te, Te, Te, Te, and Te, are stable. The other two, Te and Te, have been found to be slightly radioactive, with extremely long half-lives, including 2.2 × 10 years for Te. This is the longest known half-life among all radionuclides and is about 160 trillion (10) times the age of the known universe. Stable isotopes comprise only 33.2% of naturally occurring tellurium.\n\nA further 30 artificial radioisotopes of tellurium are known, with atomic masses ranging from 105 to 142 and with half-lives of 19 days or less. Also, 17 nuclear isomers are known, with half-lives up to 154 days. Tellurium (Te to Te ) are among the lightest elements known to undergo alpha decay.\n\nThe atomic mass of tellurium (127.60 g·mol) exceeds that of iodine (126.90 g·mol), the next element in the periodic table.\n\nWith an abundance in the Earth's crust comparable to that of platinum (about 1 µg/kg), tellurium is one of the rarest stable solid elements. In comparison, even the rarest of the stable lanthanides have crustal abundances of 500 µg/kg (see Abundance of the chemical elements).\n\nThis rarity of tellurium in the Earth's crust is not a reflection of its cosmic abundance. Tellurium is more abundant than rubidium in the cosmos, though rubidium is 10,000 times more abundant in the Earth's crust. The rarity of tellurium on Earth is thought to be caused by conditions during the formation of the Earth, when the stable form of certain elements, in the absence of oxygen and water, was controlled by the reductive power of free hydrogen. Under this scenario, certain elements that form volatile hydrides, such as tellurium, were severely depleted through evaporation of these hydrides. Tellurium and selenium are the heavy elements most depleted by this process.\n\nTellurium is sometimes found in its native (i.e., elemental) form, but is more often found as the tellurides of gold such as calaverite and krennerite (two different polymorphs of AuTe), petzite, AgAuTe, and sylvanite, AgAuTe. The city of Telluride, Colorado, was named in hope of a strike of gold telluride (which never materialized, though gold metal ore was found). Gold itself is usually found uncombined, but when found as a chemical compound, it is most often combined with tellurium.\n\nAlthough tellurium is found with gold more often than in uncombined form, it is found even more often combined as tellurides of more common metals (e.g. melonite, NiTe). Natural tellurite and tellurate minerals also occur, formed by oxidation of tellurides near the Earth's surface. In contrast to selenium, tellurium does not usually replace sulfur in minerals because of the great difference in ion radii. Thus, many common sulfide minerals contain substantial quantities of selenium and only traces of tellurium.\n\nIn the gold rush of 1893, miners in Kalgoorlie discarded a pyritic material as they searched for pure gold, and it was used to fill in potholes and build sidewalks. In 1896, that tailing was discovered to be calaverite, a telluride of gold, and it sparked a second gold rush that included mining the streets.\n\nTellurium (Latin \"tellus\" meaning \"earth\") was discovered in the 18th century in a gold ore from the mines in Zlatna, near today's city of Alba Iulia, Romania. This ore was known as \"Faczebajer weißes blättriges Golderz\" (white leafy gold ore from Faczebaja, German name of Facebánya, now Fața Băii in Alba County) or \"antimonalischer Goldkies\" (antimonic gold pyrite), and according to Anton von Rupprecht, was \"Spießglaskönig\" (\"argent molybdique\"), containing native antimony. In 1782 Franz-Joseph Müller von Reichenstein, who was then serving as the Austrian chief inspector of mines in Transylvania, concluded that the ore did not contain antimony but was bismuth sulfide. The following year, he reported that this was erroneous and that the ore contained mostly gold and an unknown metal very similar to antimony. After a thorough investigation that lasted three years and included more than fifty tests, Müller determined the specific gravity of the mineral and noted that when heated, the new metal gives off a white smoke with a radish-like odor; that it imparts a red color to sulfuric acid; and that when this solution is diluted with water, it has a black precipitate. Nevertheless, he was not able to identify this metal and gave it the names \"aurum paradoxium\" (paradoxical gold) and \"metallum problematicum\" (problem metal), because it did not exhibit the properties predicted for antimony.\n\nIn 1789, a Hungarian scientist, Pál Kitaibel, discovered the element independently in an ore from Deutsch-Pilsen that had been regarded as argentiferous molybdenite, but later he gave the credit to Müller. In 1798, it was named by Martin Heinrich Klaproth, who had earlier isolated it from the mineral calaverite.\n\nThe 1960s brought an increase in thermoelectric applications for tellurium (as bismuth telluride), and in free-machining steel alloys, which became the dominant use.\n\nThe principal source of tellurium is from anode sludges from the electrolytic refining of blister copper. It is a component of dusts from blast furnace refining of lead. Treatment of 1000 tons of copper ore typically yields one kilogram (2.2 pounds) of tellurium. \n\nThe anode sludges contain the selenides and tellurides of the noble metals in compounds with the formula MSe or MTe (M = Cu, Ag, Au). At temperatures of 500 °C the anode sludges are roasted with sodium carbonate under air. The metal ions are reduced to the metals, while the telluride is converted to sodium tellurite.\n\nTellurites can be leached from the mixture with water and are normally present as hydrotellurites HTeO in solution. Selenites are also formed during this process, but they can be separated by adding sulfuric acid. The hydrotellurites are converted into the insoluble tellurium dioxide while the selenites stay in solution.\n\nThe metal is produced from the oxide (reduced) either by electrolysis or by reacting the tellurium dioxide with sulfur dioxide in sulfuric acid.\n\nCommercial-grade tellurium is usually marketed as 200-mesh powder but is also available as slabs, ingots, sticks, or lumps. The year-end price for tellurium in 2000 was US$14 per pound. In recent years, the tellurium price was driven up by increased demand and limited supply, reaching as high as US$100 per pound in 2006. Despite the expectation that improved production methods will double production, the United States Department of Energy (DoE) anticipates a supply shortfall of tellurium by 2025.\n\nTellurium is produced mainly in the United States, Peru, Japan and Canada. The British Geological Survey gives the following production numbers for 2009: United States 50 t, Peru 7 t, Japan 40 t and Canada 16 t.\n\nTellurium belongs to the chalcogen (group 16) family of elements on the periodic table, which also includes oxygen, sulfur, selenium and polonium: Tellurium and selenium compounds are similar. Tellurium exhibits the oxidation states −2, +2, +4 and +6, with +4 being most common.\n\nReduction of Te metal produces the tellurides and polytellurides, Te. The −2 oxidation state is exhibited in binary compounds with many metals, such as zinc telluride, , produced by heating tellurium with zinc. Decomposition of with hydrochloric acid yields hydrogen telluride (), a highly unstable analogue of the other chalcogen hydrides, , and :\n\nThe +2 oxidation state is exhibited by the dihalides, , and . The dihalides have not been obtained in pure form, although they are known decomposition products of the tetrahalides in organic solvents, and the derived tetrahalotellurates are well-characterized:\n\nwhere X is Cl, Br, or I. These anions are square planar in geometry. Polynuclear anionic species also exist, such as the dark brown , and the black .\n\nFluorine forms two halides with tellurium: the mixed-valence and . In the +6 oxidation state, the structural group occurs in a number of compounds such as , , , and . The square antiprismatic anion is also attested. The other halogens do not form halides with tellurium in the +6 oxidation state, but only tetrahalides (, and ) in the +4 state, and other lower halides (, , , and two forms of ). In the +4 oxidation state, halotellurate anions are known, such as and . Halotellurium cations are also attested, including , found in .\n\n\nTellurium monoxide was first reported in 1883 as a black amorphous solid formed by the heat decomposition of in vacuum, disproportionating into tellurium dioxide, and elemental tellurium upon heating. Since then, however, existence in the solid phase is doubted and in dispute, although it is known as a vapor fragment; the black solid may be merely an equimolar mixture of elemental tellurium and tellurium dioxide.\n\nTellurium dioxide is formed by heating tellurium in air, where it burns with a blue flame. Tellurium trioxide, β-, is obtained by thermal decomposition of . The other two forms of trioxide reported in the literature, the α- and γ- forms, were found not to be true oxides of tellurium in the +6 oxidation state, but a mixture of , and . Tellurium also exhibits mixed-valence oxides, and .\n\nThe tellurium oxides and hydrated oxides form a series of acids, including tellurous acid (), orthotelluric acid () and metatelluric acid (). The two forms of telluric acid form \"tellurate\" salts containing the TeO and TeO anions, respectively. Tellurous acid forms \"tellurite\" salts containing the anion TeO. Other tellurium cations include , which consists of two fused tellurium rings and the polymeric .\n\nWhen tellurium is treated with concentrated sulfuric acid, the result is a red solution of the Zintl ion, . The oxidation of tellurium by in liquid produces the same square planar cation, in addition to the trigonal prismatic, yellow-orange :\n\nOther tellurium Zintl cations include the polymeric and the blue-black , consisting of two fused 5-membered tellurium rings. The latter cation is formed by the reaction of tellurium with tungsten hexachloride:\n\nInterchalcogen cations also exist, such as (distorted cubic geometry) and . These are formed by oxidizing mixtures of tellurium and selenium with or .\n\n\nTellurium does not readily form analogues of alcohols and thiols, with the functional group –TeH, that are called tellurols. The –TeH functional group is also attributed using the prefix \"tellanyl-\". Like HTe, these species are unstable with respect to loss of hydrogen. Telluraethers (R–Te–R) are more stable, as are telluroxides.\n\nThe largest consumer of tellurium is metallurgy in iron, stainless steel, copper, and lead alloys. The addition to steel and copper produces an alloy more machinable than otherwise. It is alloyed into cast iron for promoting chill for spectroscopy, where the presence of electrically conductive free graphite tends to interfere with spark emission testing results. In lead, tellurium improves strength and durability, and decreases the corrosive action of sulfuric acid.\n\nTellurium is used in cadmium telluride (CdTe) solar panels. National Renewable Energy Laboratory lab tests of tellurium demonstrated some of the greatest efficiencies for solar cell electric power generators. Massive commercial production of CdTe solar panels by First Solar in recent years has significantly increased tellurium demand. Replacing some of the cadmium in CdTe by zinc, producing (Cd,Zn)Te, produces a solid-state X-ray detector, providing an alternative to single-use film badges.\n\nInfrared sensitive semiconductor material is formed by alloying tellurium with cadmium and mercury to form mercury cadmium telluride.\n\nOrganotellurium compounds such as dimethyl telluride, diethyl telluride, diisopropyl telluride, diallyl telluride and methyl allyl telluride are precursors for synthesizing metalorganic vapor phase epitaxy growth of II-VI compound semiconductors. Diisopropyl telluride (DIPTe) is the preferred precursor for low-temperature growth of CdHgTe by MOVPE. The greatest purity metalorganics of both selenium and tellurium are used in these processes. The compounds for semiconductor industry and are prepared by adduct purification.\n\nTellurium, as tellurium suboxide, is used in the media layer of rewritable optical discs, including ReWritable Compact Discs (CD-RW), ReWritable Digital Video Discs (DVD-RW), and ReWritable Blu-ray Discs.\n\nTellurium dioxide is used to create acousto-optic modulators (AOTFs and AOBSs) for confocal microscropy.\n\nTellurium is used in the new phase change memory chips developed by Intel. Bismuth telluride (BiTe) and lead telluride are working elements of thermoelectric devices. Lead telluride is used in far-infrared detectors.\n\n\nTellurium has no known biological function, although fungi can incorporate it in place of sulfur and selenium into amino acids such as telluro-cysteine and telluro-methionine. Organisms have shown a highly variable tolerance to tellurium compounds. Many bacteria, such as \"Pseudomonas aeruginosa\", take up tellurite and reduce it to elemental tellurium, which accumulates and causes a characteristic and often dramatic darkening of cells. In yeast, this reduction is mediated by the sulfate assimilation pathway. Tellurium accumulation seems to account for a major part of the toxicity effects. Many organisms also metabolize tellurium partly to form dimethyl telluride, although dimethyl ditelluride is also formed by some species. Dimethyl telluride has been observed in hot springs at very low concentrations.\n\nTellurium and tellurium compounds are considered to be mildly toxic and need to be handled with care, although acute poisoning is rare. Tellurium poisoning is particularly difficult to treat as many chelation agents used in the treatment of metal poisoning will increase the toxicity of tellurium. Tellurium is not reported to be carcinogenic.\n\nHumans exposed to as little as 0.01 mg/m or less in air exude a foul garlic-like odor known as \"tellurium breath\".\nThis is caused by the body converting tellurium from any oxidation state to dimethyl telluride, (CH)Te. This is a volatile compound with a pungent garlic-like smell. Even though the metabolic pathways of tellurium are not known, it is generally assumed that they resemble those of the more extensively studied selenium because the final methylated metabolic products of the two elements are similar.\n\nPeople can be exposed to tellurium in the workplace by inhalation, ingestion, skin contact, and eye contact. The Occupational Safety and Health Administration (OSHA) limits (permissible exposure limit) tellurium exposure in the workplace to 0.1 mg/m over an eight-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set the recommended exposure limit (REL) at 0.1 mg/m over an eight-hour workday. In concentrations of 25 mg/m, tellurium is immediately dangerous to life and health.\n\n\n"}
{"id": "223492", "url": "https://en.wikipedia.org/wiki?curid=223492", "title": "Thomas Davenport (inventor)", "text": "Thomas Davenport (inventor)\n\nThomas Davenport (9 July 1802 – 6 July 1851) was a Vermont blacksmith who constructed the first American DC electric motor in 1834.\n\nDavenport was born in Williamstown, Vermont. He lived in Forest Dale, a village near the town of Brandon.\n\nAs early as 1834, he developed a battery-powered electric motor. He used it to operate a small model car on a short section of track, paving the way for the later electrification of streetcars.\n\nDavenport's 1833 visit to the Penfield and Taft iron works at Crown Point, New York, where an electromagnet was operating, based on the design of Joseph Henry, was an impetus for his electromagnetic undertakings. Davenport bought an electromagnet from the Crown Point factory and took it apart to see how it worked. Then he forged a better iron core and redid the wiring, using silk from his wife's wedding gown.\n\nWith his wife Emily and colleague Orange Smalley, Davenport received the first American patent on an electric machine in 1837, U. S. Patent No. 132. He used his electric motor in 1840 to print \"The Electro-Magnetic and Mechanics Intelligencer\" - the first newspaper printed using electricity.\n\nIn 1849, Charles Grafton Page, the Washington scientist and inventor, commenced a project to build an electromagnetically powered locomotive, with substantial funds appropriated by the US Senate. Davenport challenged the expenditure of public funds, arguing for the motors he had already invented. In 1851, Page's full sized electromagnetically operated locomotive was put to a calamity-laden test on the rail line between Washington and Baltimore.\n\n"}
{"id": "23454306", "url": "https://en.wikipedia.org/wiki?curid=23454306", "title": "Tropical horticulture", "text": "Tropical horticulture\n\nTropical horticulture is a branch of horticulture that studies and cultivates plants in the tropics, i.e., the equatorial regions of the world. The field is sometimes known by the portmanteau \"TropHort\".\n\nTropical horticulture includes plants such as perennial woody plants (arboriculture), ornamentals (floriculture), vegetables (olericulture), and fruits (pomology) including grapes (viticulture). The origin of many of these crops is not in the tropics but in temperate zones. Their adoption to tropical climatic conditions is an objective of breeding. Many important crops, however, are indigenous to the tropics. The latter embrace perennial crops such as oil palm, vegetables including okra, field crops such as rice and sugarcane, and particularly fruits including pineapple, banana, papaya, and mango.\n\nSince the tropics represent 36 percent of the earth's surface and 20 percent of its land surface, the potential of tropical horticulture is huge. In contrast to temperate regions, environmental conditions in the tropics are defined less by seasonal temperature fluctuations and more by seasonality of precipitation. Thus the climate in the greater part of the tropics is characterized by distinct wet and dry seasons, although such variation is reduced in locations closer to the equator (±5° latitude). Temperature conditions in the tropics are affected by elevation, in which contrasting warmer and colder climate areas in the tropics can be differentiated, and highland areas in the tropics can consequently be more favourable for production of temperate plant species than are lowland areas.\n\nBoth vascular and non-vascular plants grow in tropical environments. Plants indigenous to the tropics are usually cold sensitive and adapted to receiving high levels of solar radiation. They are sensitive to small variations in photoperiod (\"short day\" plants), and can be adapted to extended drought, high precipitation and/or distinct wet and dry seasons. High night temperatures are a major hindrance to adopting temperate crops (e.g., tomatoes) to the tropical lowlands. Furthermore, such conditions promote high respiration rates of plants, resulting in comparably lower net photosynthesis rates.\n\n"}
{"id": "28362686", "url": "https://en.wikipedia.org/wiki?curid=28362686", "title": "White Earthquake", "text": "White Earthquake\n\nThe White Earthquake (Spanish: \"Terremoto Blanco\") was the name put on the cold wave and heavy snowfall that occurred through southern Chile in August 1995. \n\n7,176 people were left isolated as result of the heavy snowfall and 3 persons died because of it.\nOn August 2, 1995, a cold front entered southern Chile. While at the beginning the cold front seemed normal, it was soon joined by a second cold front that caused temperatures to drop drastically, below in some areas, and generated large snowfalls. The snow made road transit difficult and isolated hundreds of communities in the Andes and the Patagonian plains. \n\nThe fodder stored for animals began to run out, and sheep and cows were trapped in metres of snow. The government of Eduardo Frei declared an emergency state in 24 communes initially but extended it later to encompass more than 30 communes.\n\nBetween 10,000 and 12,000 families suffered the severe effects of the White Earthquake which also buried several houses in snow. Livestock farmers suffered the worst economic effects, since they lost most of their livestock due to starving and freezing. It has been estimated that in Magallanes Region two of every ten sheep died, while in parts of Tierra del Fuego up to eight out of ten sheep died. \n\nThe emerging aquaculture industry of southern Chile also suffered huge losses with the loss of 20,000 trout and 50,000 salmon. Wildlife such as the South Andean Deer also registered high death rates. \n\nOnly by the end of August could the government clean the principal roads of snow and provide effective help to the affected zones, specially in the form of fodder.\n\n"}
{"id": "223992", "url": "https://en.wikipedia.org/wiki?curid=223992", "title": "Wind shear", "text": "Wind shear\n\nWind shear (or windshear), sometimes referred to as wind gradient, is a difference in wind speed and/or direction over a relatively short distance in the atmosphere. Atmospheric wind shear is normally described as either vertical or horizontal wind shear. Vertical wind shear is a change in wind speed or direction with change in altitude. Horizontal wind shear is a change in wind speed with change in lateral position for a given altitude.\n\nWind shear is a microscale meteorological phenomenon occurring over a very small distance, but it can be associated with mesoscale or synoptic scale weather features such as squall lines and cold fronts. It is commonly observed near microbursts and downbursts caused by thunderstorms, fronts, areas of locally higher low-level winds referred to as low level jets, near mountains, radiation inversions that occur due to clear skies and calm winds, buildings, wind turbines, and sailboats. Wind shear has significant effects on control of an aircraft, and it has been a sole or contributing cause of many aircraft accidents.\n\nWind shear is sometimes experienced by pedestrians at ground level when walking across a plaza towards a tower block and suddenly encountering a strong wind stream that is flowing around the base of the tower.\n\nSound movement through the atmosphere is affected by wind shear, which can bend the wave front, causing sounds to be heard where they normally would not, or vice versa. Strong vertical wind shear within the troposphere also inhibits tropical cyclone development, but helps to organize individual thunderstorms into longer life cycles which can then produce severe weather. The thermal wind concept explains how differences in wind speed at different heights are dependent on horizontal temperature differences, and explains the existence of the jet stream.\nWind shear refers to the variation of wind over either horizontal or vertical distances. Airplane pilots generally regard significant wind shear to be a horizontal change in airspeed of for light aircraft, and near for airliners at flight altitude. Vertical speed changes greater than also qualify as significant wind shear for aircraft. Low level wind shear can affect aircraft airspeed during take off and landing in disastrous ways, and airliner pilots are trained to avoid all microburst wind shear (headwind loss in excess of ). The rationale for this additional caution includes:\nWind shear is also a key factor in the creation of severe thunderstorms. The additional hazard of turbulence is often associated with wind shear.\n\nWeather situations where shear is observed include:\n\nWeather fronts are boundaries between two masses of air of different densities, or different temperature and moisture properties, which normally are convergence zones in the wind field and are the principal cause of significant weather. Within surface weather analyses, they are depicted using various colored lines and symbols. The air masses usually differ in temperature and may also differ in humidity. Wind shear in the horizontal occurs near these boundaries.\nCold fronts feature narrow bands of thunderstorms and severe weather, and may be preceded by squall lines and dry lines. Cold fronts are sharper surface boundaries with more significant horizontal wind shear than warm fronts. When a front becomes stationary, it can degenerate into a line which separates regions of differing wind speed, known as a shear line, though the wind direction across the front normally remains constant. In the tropics, tropical waves move from east to west across the Atlantic and eastern Pacific basins. Directional and speed shear can occur across the axis of stronger tropical waves, as northerly winds precede the wave axis and southeast winds are seen behind the wave axis. Horizontal wind shear can also occur along local land breeze and sea breeze boundaries.\n\nThe magnitude of winds offshore are nearly double the wind speed observed onshore. This is attributed to the differences in friction between land masses and offshore waters. Sometimes, there are even directional differences, particularly if local sea breezes change the wind on shore during daylight hours.\n\nThermal wind is a meteorological term not referring to an actual wind, but a \"difference\" in the geostrophic wind between two pressure levels and , with ; in essence, wind shear. It is only present in an atmosphere with horizontal changes in temperature (or in an ocean with horizontal gradients of density), i.e. baroclinicity. In a barotropic atmosphere, where temperature is uniform, the geostrophic wind is independent of height. The name stems from the fact that this wind flows around areas of low (and high) temperature in the same manner as the geostrophic wind flows around areas of low (and high) pressure.\n\nThe \"thermal wind equation\" is\n\nwhere the are geopotential height fields with , is the Coriolis parameter, and is the upward-pointing unit vector in the vertical direction. The thermal wind equation does not determine the wind in the tropics. Since is small or zero, such as near the equator, the equation reduces to stating that is small.\n\nThis equation basically describes the existence of the jet stream, a westerly current of air with maximum wind speeds close to the tropopause which is (even though other factors are also important) the result of the temperature contrast between equator and pole.\n\nTropical cyclones are, in essence, heat engines that are fueled by the temperature gradient between the warm tropical ocean surface and the colder upper atmosphere. Tropical cyclone development requires relatively low values of vertical wind shear so that their warm core can remain above their surface circulation center, thereby promoting intensification. Vertical wind shear tears up the \"machinery\" of the heat engine causing it to break down. Strongly sheared tropical cyclones weaken as the upper circulation is blown away from the low level center.\n\nThe vertical wind shear in a tropical cyclone's environment is very important. When the wind shear is weak, the storms that are part of the cyclone grow vertically, and the latent heat from condensation is released into the air directly above the storm, aiding in development. When there is stronger wind shear, this means that the storms become more slanted and the latent heat release is dispersed over a much larger area.\n\nSevere thunderstorms, which can spawn tornadoes and hailstorms, require wind shear to organize the storm in such a way as to maintain the thunderstorm for a longer period of time. This occurs as the storm's inflow becomes separated from its rain-cooled outflow. An increasing nocturnal, or overnight, low level jet can increase the severe weather potential by increasing the vertical wind shear through the troposphere. Thunderstorms in an atmosphere with virtually no vertical wind shear weaken as soon as they send out an outflow boundary in all directions, which then quickly cuts off its inflow of relatively warm, moist air and kills the thunderstorm.\n\nThe atmospheric effect of surface friction with winds aloft force surface winds to slow and back counterclockwise near the surface of the Earth blowing inward across isobars (lines of equal pressure), when compared to the winds in frictionless flow well above the Earth's surface. This layer where friction slows and changes the wind is known as the planetary boundary layer, sometimes the Ekman layer, and it is thickest during the day and thinnest at night. Daytime heating thickens the boundary layer as winds at the surface become increasingly mixed with winds aloft due to insolation, or solar heating. Radiative cooling overnight further enhances wind decoupling between the winds at the surface and the winds above the boundary layer by calming the surface wind which increases wind shear. These wind changes force wind shear between the boundary layer and the wind aloft, and is most emphasized at night.\n\nIn gliding, wind gradients just above the surface affect the takeoff and landing phases of flight of a glider.\nWind gradient can have a noticeable effect on ground launches, also known as winch launches or wire launches. If the wind gradient is significant or sudden,\nor both, and the pilot maintains the same pitch attitude, the indicated airspeed will increase, possibly exceeding\nthe maximum ground launch tow speed. The pilot must adjust the airspeed to deal with the effect of the\ngradient.\n\nWhen landing, wind shear is also a hazard, particularly when the winds are strong. As the glider descends through the wind gradient on final approach to landing, airspeed decreases while sink rate increases, and there is insufficient time to accelerate prior to ground contact. The pilot must anticipate the wind gradient and use a higher approach speed to compensate for it.\n\nWind shear is also a hazard for aircraft making steep turns near the ground. It is a particular problem for gliders which have a relatively long wingspan, which exposes them to a greater wind speed difference for a given bank angle. The different airspeed experienced by each wing tip can result in an aerodynamic stall on one wing, causing a loss of control accident.\n\nWind shear or wind gradients are a threat to parachutists, particularly to BASE jumping and wingsuit flying. Skydivers have been pushed off of their course by sudden shifts in wind direction and speed, and have collided with bridges, cliffsides, trees, other skydivers, the ground, and other obstacles. Skydivers routinely make adjustments to the position of their open canopies to compensate for changes in direction while making landings to prevent accidents such as canopy collisions and canopy inversion.\n\nSoaring related to wind shear, also called dynamic soaring, is a technique used by soaring birds like albatrosses, who can maintain flight without wing flapping. If the wind shear is of sufficient magnitude, a bird can climb into the wind gradient, trading ground speed for height, while maintaining airspeed. By then turning downwind, and diving through the wind gradient, they can also gain energy. It has also been used by glider pilots on rare occasions.\n\nWind shear can also create wave. This occurs when an atmospheric inversion separates two layers with a marked difference in wind direction. If the wind encounters distortions in the inversion layer caused by thermals coming up from below, it will create significant shear waves that can be used for soaring.\n\nStrong outflow from thunderstorms causes rapid changes in the three-dimensional wind velocity just above ground level. Initially, this outflow causes a headwind that increases airspeed, which normally causes a pilot to reduce engine power if they are unaware of the wind shear. As the aircraft passes into the region of the downdraft, the localized headwind diminishes, reducing the aircraft's airspeed and increasing its sink rate. Then, when the aircraft passes through the other side of the downdraft, the headwind becomes a tailwind, reducing lift generated by the wings, and leaving the aircraft in a low-power, low-speed descent. This can lead to an accident if the aircraft is too low to effect a recovery before ground contact. \n\nAs the result of the accidents in the 1970s and 1980s, most notably following the 1985 crash of Delta Air Lines Flight 191, in 1988 the U.S. Federal Aviation Administration mandated that all commercial aircraft have on-board wind shear detection systems by 1993. Between 1964 and 1985, wind shear directly caused or contributed to 26 major civil transport aircraft accidents in the U.S. that led to 620 deaths and 200 injuries. Since 1995, the number of major civil aircraft accidents caused by wind shear has dropped to approximately one every ten years, due to the mandated on-board detection as well as the addition of Doppler weather radar units on the ground (NEXRAD). The installation of high-resolution Terminal Doppler Weather Radar stations at many U.S. airports that are commonly affected by wind shear has further aided the ability of pilots and ground controllers to avoid wind shear conditions.\n\nWind shear affects sailboats in motion by presenting a different wind speed and direction at different heights along the mast. The effect of low level wind shear can be factored into the selection of sail twist in the sail design, but this can be difficult to predict since wind shear may vary widely in different weather conditions. Sailors may also adjust the trim of the sail to account for low level wind shear, for example using a boom vang.\n\nWind shear can have a pronounced effect upon sound propagation in the lower atmosphere, where waves can be \"bent\" by refraction phenomenon. The audibility of sounds from distant sources, such as thunder or gunshots, is very dependent on the amount of shear. The result of these differing sound levels is key in noise pollution considerations, for example from roadway noise and aircraft noise, and must be considered in the design of noise barriers. This phenomenon was first applied to the field of noise pollution study in the 1960s, contributing to the design of urban highways as well as noise barriers.\n\nThe speed of sound varies with temperature. Since temperature and sound velocity normally decrease with increasing altitude, sound is refracted upward, away from listeners on the ground, creating an acoustic shadow at some distance from the source. In the 1862, during the American Civil War Battle of Iuka, an acoustic shadow, believed to have been enhanced by a northeast wind, kept two divisions of Union soldiers out of the battle, because they could not hear the sounds of battle only six miles downwind.\n\nWind engineering is a field of engineering devoted to the analysis of wind effects on the natural and built environment. It includes strong winds which may cause discomfort as well as extreme winds such as tornadoes, hurricanes and storms which may cause widespread destruction. Wind engineering draws upon meteorology, aerodynamics and a number of specialist engineering disciplines. The tools used include climate models, atmospheric boundary layer wind tunnels and numerical models. It involves, among other topics, how wind impacting buildings must be accounted for in engineering.\n\nWind turbines are affected by wind shear. Vertical wind-speed profiles result in different wind speeds at the blades nearest to the ground level compared to those at the top of blade travel, and this in turn affects the turbine operation. This low level wind shear can create a large bending moment in the shaft of a two bladed turbine when the blades are vertical. The reduced wind shear over water means shorter and less expensive wind turbine towers can be used in shallow seas.\n\n\n"}
