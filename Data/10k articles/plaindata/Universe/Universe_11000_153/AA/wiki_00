{"id": "2703", "url": "https://en.wikipedia.org/wiki?curid=2703", "title": "Aberration of light", "text": "Aberration of light\n\nThe aberration of light (also referred to as astronomical aberration, stellar aberration, or velocity aberration) is an astronomical phenomenon which produces an apparent motion of celestial objects about their true positions, dependent on the velocity of the observer. Aberration causes objects to appear to be displaced towards the direction of motion of the observer compared to when the observer is stationary. The change in angle is typically very small — of the order of \"v/c\" where \"c\" is the speed of light and \"v\" the velocity of the observer. In the case of \"stellar\" or \"annual\" aberration, the apparent position of a star to an observer on Earth varies periodically over the course of a year as the Earth's velocity changes as it revolves around the Sun, by a maximum angle of approximately 20 arcseconds in right ascension or declination.\n\nThe term \"aberration\" has historically been used to refer to a number of related phenomena concerning the propagation of light in moving bodies. \nAberration should not be confused with \"parallax\". The latter is a change in the apparent position of a relatively nearby object, as measured by a moving observer, relative to more distant objects that define a reference frame. The amount of parallax depends on the \"distance\" of the object from the observer, whereas aberration does not. Aberration is also related to light-time correction and relativistic beaming, although it is often considered separately from these effects.\n\nAberration is historically significant because of its role in the development of the theories of light, electromagnetism and, ultimately, the theory of special relativity. It was first observed in the late 1600s by astronomers searching for stellar parallax in order to confirm the heliocentric model of the Solar System. However, it was not understood at the time to be a different phenomenon.\nIn 1727, James Bradley provided a classical explanation for it in terms of the finite speed of light relative to the motion of the Earth in its orbit around the Sun, \nwhich he used to make one of the earliest measurements of the speed of light. However, Bradley's theory was incompatible with 19th century theories of light, and aberration became a major motivation for the aether drag theories of Augustin Fresnel (in 1818) and G. G. Stokes (in 1845), and for Hendrik Lorentz's aether theory of electromagnetism in 1892. The aberration of light, together with Lorentz's elaboration of Maxwell's electrodynamics, the moving magnet and conductor problem, the negative aether drift experiments, as well as the Fizeau experiment, led Albert Einstein to develop the theory of special relativity in 1905, which presents a general form of the equation for aberration in terms of such theory.\n\nAberration may be explained as the difference in angle of a beam of light in different inertial frames of reference. A common analogy is to consider the apparent direction of falling rain. If rain is falling vertically in the frame of reference of a person standing still, then to a person moving forwards the rain will appear to arrive at an angle, requiring the moving observer to tilt their umbrella forwards. The faster the observer moves, the more tilt is needed.\n\nThe net effect is that light rays striking the moving observer from the sides in a stationary frame will come angled from ahead in the moving observer's frame. This effect is sometimes called the \"searchlight\" or \"headlight\" effect.\n\nIn the case of annual aberration of starlight, the direction of incoming starlight as seen in the Earth's moving frame is tilted relative to the angle observed in the Sun's frame. Since the direction of motion of the Earth changes during its orbit, the direction of this tilting changes during the course of the year, and causes the apparent position of the star to differ from its true position as measured in the inertial frame of the Sun.\n\nWhile classical reasoning gives intuition for aberration, it leads to a number of physical paradoxes observable even at the classical level (see history). The theory of special relativity is required to correctly account for aberration. The relativistic explanation is very similar to the classical one however, and in both theories aberration may be understood as a case of addition of velocities.\n\nIn the Sun's frame, consider a beam of light with velocity equal to the speed of light c, with x and y velocity components formula_1 and formula_2, at an angle formula_3. If the Earth is moving at velocity formula_4 in the x direction relative to the Sun, then by velocity addition the x component of the beam's velocity in the Earth's frame of reference is formula_5, and the y velocity is unchanged, formula_6. Thus the angle of the light in the Earth's frame in terms of the angle in the Sun's frame is\n\nIn the case of formula_8, this result reduces to formula_9, which in the limit formula_10 may be approximated by formula_11. \nThe reasoning in the relativistic case is the same except that the relativistic velocity addition formulas must be used, which can be derived from Lorentz transformations between different frames of reference. These formulae are\n\nwhere formula_14, giving the components of the light beam in the Earth's frame in terms of the components in the Sun's frame. The angle of the beam in the Earth's frame is thus \n\nIn the case of formula_8, this result reduces to formula_17, and in the limit formula_10 this may be approximated by formula_11. This relativistic derivation keeps the speed of light formula_20 constant in all frames of reference, unlike the classical derivation above.\n\nAberration is related to two other phenomena, light-time correction, which is due to the motion of an observed object during the time taken by its light to reach an observer, and relativistic beaming, which is an angling of the light emitted by a moving light source. It can be considered equivalent to them but in a different inertial frame of reference. In aberration, the observer is considered to be moving relative to a (for the sake of simplicity) stationary light source, while in light-time correction and relativistic beaming the light source is considered to be moving relative to a stationary observer.\n\nConsider the case of an observer and a light source moving relative to each other at constant velocity, with a light beam moving from the source to the observer. At the moment of emission, the beam in the observer's rest frame is tilted compared to the one in the source's rest frame, as understood through relativistic beaming. During the time it takes the light beam to reach the observer the light source moves in the observer's frame, and the 'true position' of the light source is displaced relative to the apparent position the observer sees, as explained by light-time correction. Finally, the beam in the observer's frame at the moment of observation is tilted compared to the beam in source's frame, which can be understood as an aberrational effect. Thus, a person in the light source's frame would describe the apparent tilting of the beam in terms of aberration, while a person in the observer's frame would describe it as a light-time effect.\n\nThe relationship between these phenomena is only valid if the observer and source's frames are inertial frames. In practice, because the Earth is not an inertial rest frame but experiences centripetal acceleration towards the Sun, many aberrational effects such as annual aberration on Earth cannot be considered light-time corrections. However, if the time between emission and detection of the light is short compared to the orbital period of the Earth, the Earth may be approximated as an inertial frame and aberrational effects are equivalent to light-time corrections.\n\nThere are a number of types of aberration, caused by the differing components of the Earth's and observed object's motion:\n\n\nAnnual aberration is caused by the motion of an observer on Earth as the planet revolves around the Sun. Due to orbital eccentricity, the orbital velocity formula_4 of Earth (in the Sun's rest frame) varies periodically during the year as the planet traverses its elliptic orbit and consequently the aberration also varies periodically, typically causing stars to appear to move in small ellipses.\n\nApproximating Earth's orbit as circular, the maximum displacement of a star due to annual aberration is known as the \"constant of aberration\", conventionally represented by formula_22. It may be calculated using the relation formula_23 substituting the Earth's average speed in the Sun's frame for formula_4 and the speed of light formula_25. Its accepted value is 20.49552  arcseconds (at J2000).\n\nAssuming a circular orbit, annual aberration causes stars exactly on the ecliptic (the plane of Earth's orbit) to appear to move back and forth along a straight line, varying by formula_22 on either side of their position in the Sun's frame. A star that is precisely at one of the ecliptic poles (at 90° from the ecliptic plane) will appear to move in a circle of radius formula_22 about its true position, and stars at intermediate ecliptic latitudes will appear to move along a small ellipse.\n\nFor illustration, consider a star at the northern ecliptic pole viewed by an observer at a point on the Arctic Circle. Such an observer will see the star transit at the zenith, once every day (strictly speaking sidereal day). At the time of the March equinox, Earth's orbit carries the observer in a southwards direction, and the star's apparent declination is therefore displaced to the south by an angle of formula_22. On the September equinox, the star's position is displaced to the north by an equal and opposite amount. On either solstice, the displacement in declination is 0. Conversely, the amount of displacement in right ascension is 0 on either equinox and at maximum on either solstice.\nIn actuality, Earth's orbit is slightly elliptic rather than circular, and its speed varies somewhat over the course of its orbit, which means the description above is only approximate. Aberration is more accurately calculated using Earth's instantaneous velocity relative to the barycenter of the Solar System.\n\nNote that the displacement due to aberration is orthogonal to any displacement due to parallax. If parallax were detectable, the maximum displacement to the south would occur in December, and the maximum displacement to the north in June. It is this apparently anomalous motion that so mystified early astronomers.\n\nA special case of annual aberration is the nearly constant deflection of the Sun from its position in the Sun's rest frame by formula_22 towards the \"west\" (as viewed from Earth), opposite to the apparent motion of the Sun along the ecliptic (which is from west to east, as seen from Earth). The deflection thus makes the Sun appear to be behind (or retarded) from its rest-frame position on the ecliptic by a position or angle formula_22.\n\nThis deflection may equivalently be described as a light-time effect due to motion of the Earth during the 8.3 minutes that it takes light to travel from the Sun to Earth. This is possible since the transit time of sunlight is short relative to the orbital period of the Earth, so the Earth's frame may be approximated as inertial. In the Earth's frame, the Sun moves by a distance formula_31 in the time it takes light to reach Earth, formula_32 for the orbit of radius formula_33. This gives an angular correction formula_34 which can be solved to give formula_35, the same as the aberrational correction.\n\nPlanetary aberration is the combination of the aberration of light (due to Earth's velocity) and light-time correction (due to the object's motion and distance), as calculated in the rest frame of the Solar System. Both are determined at the instant when the moving object's light reaches the moving observer on Earth. It is so called because it is usually applied to planets and other objects in the Solar System whose motion and distance are accurately known.\n\nDiurnal aberration is caused by the velocity of the observer on the surface of the rotating Earth. It is therefore dependent not only on the time of the observation, but also the latitude and longitude of the observer. Its effect is much smaller than that of annual aberration, and is only 0.32 arcseconds in the case of an observer at the Equator, where the rotational velocity is greatest.\n\nThe Sun and Solar System are revolving around the center of the Galaxy. Aberration due to this motion is known as secular aberration and affects the apparent positions of distant stars and extragalactic objects. However, since the galactic year is about 230 million years, the aberration varies very slowly and this change is extremely difficult to observe. Therefore, secular aberration is usually ignored when considering the positions of stars. In other words, star maps show the observed apparent positions of the stars, not their calculated true positions after accounting for secular aberration.\n\nFor stars significantly less than 230 million light years away, the Solar System may be approximated as an inertial frame and so the effect of secular aberration is equivalent to a light-time correction. This includes stars in the Milky Way, since the Milky Way is about 100,000 light years in diameter. For these stars the true position of the star is then easily computed from its proper motion and its distance.\n\nSecular aberration is typically a small number of arcminutes, for example the stationary star Groombridge 1830 is displaced by approximately 3 arcminutes, due to secular aberration. This is roughly 8 times the effect of annual aberration, as one would expect since the velocity of the Solar System relative to the center of the Galaxy is about 8 times the velocity of the Earth relative to the Sun.\n\nThe discovery of the aberration of light was totally unexpected, and it was only by considerable perseverance and perspicacity that Bradley was able to explain it in 1727. It originated from attempts to discover whether stars possessed appreciable parallaxes.\n\nThe Copernican heliocentric theory of the Solar System had received confirmation by the observations of Galileo and Tycho Brahe and the mathematical investigations of Kepler and Newton. As early as 1573, Thomas Digges had suggested that parallactic shifting of the stars should occur according to the heliocentric model, and consequently if stellar parallax could be observed it would help confirm this theory. Many observers claimed to have determined such parallaxes, but Tycho Brahe and Giovanni Battista Riccioli concluded that they existed only in the minds of the observers, and were due to instrumental and personal errors. However, in 1680 Jean Picard, in his \"Voyage d’Uranibourg,\" stated, as a result of ten years' observations, that Polaris, the Pole Star, exhibited variations in its position amounting to 40″ annually. Some astronomers endeavoured to explain this by parallax, but these attempts failed because the motion differed from that which parallax would produce. John Flamsteed, from measurements made in 1689 and succeeding years with his mural quadrant, similarly concluded that the declination of Polaris was 40″ less in July than in September. Robert Hooke, in 1674, published his observations of γ Draconis, a star of magnitude 2 which passes practically overhead at the latitude of London (hence its observations are largely free from the complex corrections due to atmospheric refraction), and concluded that this star was 23″ more northerly in July than in October.\n\nConsequently, when Bradley and Samuel Molyneux entered this sphere of research in 1725, there was still considerable uncertainty as to whether stellar parallaxes had been observed or not, and it was with the intention of definitely answering this question that they erected a large telescope at Molyneux's house at Kew. They decided to reinvestigate the motion of γ Draconis with a telescope constructed by George Graham (1675–1751), a celebrated instrument-maker. This was fixed to a vertical chimney stack in such manner as to permit a small oscillation of the eyepiece, the amount of which (i.e. the deviation from the vertical) was regulated and measured by the introduction of a screw and a plumb line.\n\nThe instrument was set up in November 1725, and observations on γ Draconis were made starting in December. The star was observed to move 40″ southwards between September and March, and then reversed its course from March to September. At the same time, 35 Camelopardalis, a star with a right ascension nearly exactly opposite to that of γ Draconis, was 19\" more northerly at the beginning of March than in September. These results were completely unexpected and inexplicable by existing theories.\n\nBradley and Molyneux discussed several hypotheses in the hope of finding the solution. Since the apparent motion was evidently caused neither by parallax nor observational errors, Bradley first hypothesized that it could be due to oscillations in the orientation of the Earth's axis relative to the celestial sphere – a phenomenon known as nutation. 35 Camelopardalis was seen to possess an apparent motion which could be consistent with nutation, but since its declination varied only one half as much as that of γ Draconis, it was obvious that nutation did not supply the answer (however, Bradley later went on to discover that the Earth does indeed nutate). He also investigated the possibility that the motion was due to an irregular distribution of the Earth's atmosphere, thus involving abnormal variations in the refractive index, but again obtained negative results.\n\nOn August 19, 1727, Bradley embarked upon a further series of observations using a telescope of his own erected at the Rectory, Wanstead. This instrument had the advantage of a larger field of view and he was able to obtain precise positions of a large number of stars over the course of about twenty years. During his first two years at Wanstead, he established the existence of the phenomenon of aberration beyond all doubt, and this also enabled him to formulate a set of rules that would allow the calculation of the effect on any given star at a specified date.\n\nBradley eventually developed his explanation of aberration in about September 1728 and this theory was presented to the Royal Society in mid January the following year. One well-known story was that he saw the change of direction of a wind vane on a boat on the Thames, caused not by an alteration of the wind itself, but by a change of course of the boat relative to the wind direction.\nHowever, there is no record of this incident in Bradley's own account of the discovery, and it may therefore be apocryphal.\n\nThe following table shows the magnitude of deviation from true declination for γ Draconis and the direction, on the planes of the solstitial colure and ecliptic prime meridian, of the tangent of the velocity of the Earth in its orbit for each of the four months where the extremes are found, as well as expected deviation from true ecliptic longitude if Bradley had measured its deviation from right ascension:\n\nBradley proposed that the aberration of light not only affected declination, but right ascension as well, so that a star in the pole of the ecliptic would describe a little ellipse with a diameter of about 40\", but for simplicity, he assumed it to be a circle. Since he only observed the deviation in declination, and not in right ascension, his calculations for the maximum deviation of a star in the pole of the ecliptic are for its declination only, which will coincide with the diameter of the little circle described by such star. For eight different stars, his calculations are as follows:\n\nBased on these calculations, Bradley was able to estimate the constant of aberration at 20.2\", which is equal to 0.00009793 radians, and with this was able to estimate the speed of light at per second. By projecting the little circle for a star in the pole of the ecliptic, he could simplify the calculation of the relationship between the speed of light and the speed of the Earth's annual motion in its orbit as follows:\n\nThus, the speed of light to the speed of the Earth's annual motion in its orbit is \"10210 to one, from whence it would follow, that light moves, or is propagated as far as from the Sun to the Earth in 8' 12\".\"\n\nThe discovery and elucidation of aberration is now regarded as a classic case of the application of scientific method, in which observations are made to test a theory, but unexpected results are sometimes obtained that in turn lead to new discoveries. It is also worth noting that part of the original motivation of the search for stellar parallax was to test the Copernican theory that the Earth revolves around the Sun, but of course the existence of aberration also establishes the truth of that theory.\n\nThe phenomenon of aberration became a driving force for many physical theories during the 200 years between its observation and the conclusive explanation by Albert Einstein.\n\nThe first classical explanation was provided in 1729, by James Bradley as described above, who attributed it to the finite speed of light and the motion of Earth in its orbit around the Sun. However, this explanation proved inaccurate once the wave nature of light was better understood, and correcting it became a major goal of the 19th century theories of luminiferous aether. Augustin-Jean Fresnel proposed a correction due to the motion of a medium (the aether) through which light propagated, known as \"partial aether drag\". He proposed that objects partially drag the aether along with them as they move, and this became the accepted explanation for aberration for some time. George Stokes proposed a similar theory, explaining that aberration occurs due to the flow of aether induced by the motion of the Earth. Accumulated evidence against these explanations, combined with new understanding of the electromagnetic nature of light, led Hendrik Lorentz to develop an electron theory which featured an immobile aether, and he explained that objects contract in length as they move through the aether. Motivated by these previous theories, Albert Einstein then developed the theory of special relativity in 1905, which provides the modern account of aberration.\n\nBradley conceived of an explanation in terms of a corpuscular theory of light in which light is made of particles unaffected by gravity. His classical explanation appeals to the motion of the earth relative to a beam of light-particles moving at a finite velocity, and is developed in the Sun's frame of reference, unlike the classical derivation given above.\n\nConsider the case where a distant star is motionless relative to the Sun, and the star is extremely far away, so that parallax may be ignored. In the rest frame of the Sun, this means light from the star travels in parallel paths to the Earth observer, and arrives at the same angle regardless of where the Earth is in its orbit. Suppose the star is observed on Earth with a telescope, idealized as a narrow tube. The light enters the tube from the star at angle formula_37 and travels at speed formula_25 taking a time formula_39 to reach the bottom of the tube, where it is detected. Suppose observations are made from Earth, which is moving with a speed formula_4. During the transit of the light, the tube moves a distance formula_41. Consequently, for the particles of light to reach the bottom of the tube, the tube must be inclined at an angle formula_42 different from formula_37, resulting in an \"apparent\" position of the star at angle formula_42. As the Earth proceeds in its orbit it changes direction, so formula_42 changes with the time of year the observation is made. The apparent angle and true angle are related using trigonometry as:\n\nIn the case of formula_8, this gives formula_9. While this is different from the more accurate relativistic result described above, in the limit of small angle and low velocity they are approximately the same, within the error of the measurements of Bradley's day. These results allowed Bradley to make one of the earliest measurements of the speed of light.\n\nIn the early nineteenth century the wave theory of light was being rediscovered, and in 1804 Thomas Young adapted Bradley's explanation for corpuscular light to wavelike light traveling through a medium known as the luminiferous aether. His reasoning was the same as Bradley's, but it required that this medium be immobile in the Sun's reference frame and must pass through the earth unaffected, otherwise the medium (and therefore the light) would move along with the earth and no aberration would be observed. \nHowever, it soon became clear Young's theory could not account for aberration when materials with a non-vacuum index of refraction were present. An important example is of a telescope filled with water. The velocity of the light in such a telescope will be slower than in vacuum, and is given by formula_49 rather than formula_25 where formula_51 is the index of refraction of the water. Thus, by Bradley and Young's reasoning the aberration angle is given by\n\nwhich predicts a medium-dependent angle of aberration. When refraction at the telescope's objective is taken into account this result deviates even more from the vacuum result. In 1810 François Arago performed a similar experiment and found that the aberration was unaffected by the medium in the telescope, providing solid evidence against Young's theory. This experiment was subsequently verified by many others in the following decades, most accurately by Airy in 1871, with the same result.\n\nIn 1818, Augustin Fresnel developed a modified explanation to account for the water telescope and for other aberration phenomena. He explained that the aether is generally at rest in the Sun's frame of reference, but objects partially drag the aether along with them as they move. That is, the aether in an object of index of refraction formula_51 moving at velocity formula_4 is partially dragged with a velocity formula_55 bringing the light along with it. This factor is known as \"Fresnel's dragging coefficient\". This dragging effect, along with refraction at the telescope's objective, compensates for the slower speed of light in the water telescope in Bradley's explanation. With this modification Fresnel obtained Bradley's vacuum result even for non-vacuum telescopes, and was also able to predict many other phenomena related to the propagation of light in moving bodies. Fresnel's dragging coefficient became the dominant explanation of aberration for the next decades.\n\nHowever, the fact that light is polarized (discovered by Fresnel himself) led scientists such as Cauchy and Green to believe that the aether was a totally immobile elastic solid as opposed to Fresnel's fluid aether. There was thus renewed need for an explanation of aberration consistent both with Fresnel's predictions (and Arago's observations) as well as polarization.\n\nIn 1845, Stokes proposed a 'putty-like' aether which acts as a liquid on large scales but as a solid on small scales, thus supporting both the transverse vibrations required for polarized light and the aether flow required to explain aberration. Making only the assumptions that the fluid is irrotational and that the boundary conditions of the flow are such that the aether has zero velocity far from the Earth, but moves at the Earth's velocity at its surface and within it, he was able to completely account for aberration.\nThe velocity of the aether outside of the Earth would decrease as a function of distance from the Earth so light rays from stars would be progressively dragged as they approached the surface of the Earth. The Earth's motion would be unaffected by the aether due to D'Alembert's paradox.\n\nBoth Fresnel and Stokes' theories were popular. However, the question of aberration was put aside during much of the second half of the 19th century as focus of inquiry turned to the electromagnetic properties of aether.\n\nIn the 1880s once electromagnetism was better understood, interest turned again to the problem of aberration. By this time flaws were known to both Fresnel's and Stokes' theories. Fresnel's theory required that the relative velocity of aether and matter to be different for light of different colors, and it was shown that the boundary conditions Stokes had assumed in his theory were inconsistent with his assumption of irrotational flow. At the same time, the modern theories of electromagnetic aether could not account for aberration at all. Many scientists such as Maxwell, Heaviside and Hertz unsuccessfully attempted to solve these problems by incorporating either Fresnel or Stokes' theories into Maxwell's new electromagnetic laws.\n\nHendrik Lorentz spent considerable effort along these lines. After working on this problem for a decade, the issues with Stokes' theory caused him to abandon it and to follow Fresnel's suggestion of a (mostly) stationary aether (1892, 1895). However, in Lorentz's model the aether was \"completely\" immobile, like the electromagnetic aethers of Cauchy, Green and Maxwell and unlike Fresnel's aether. He obtained Fresnel's dragging coefficient from modifications of Maxwell's electromagnetic theory, including a modification of the time coordinates in moving frames (\"local time\"). In order to explain the Michelson–Morley experiment (1887), which apparently contradicted both Fresnel's and Lorentz's immobile aether theories, and apparently confirmed Stokes' complete aether drag, Lorentz theorized (1892) that objects undergo \"length contraction\" by a factor of formula_56 in the direction of their motion through the aether. In this way, aberration (and all related optical phenomena) can be accounted for in the context of an immobile aether. Lorentz' theory became the basis for much research in the next decade, and beyond. Its predictions for aberration are identical to those of the relativistic theory.\n\nLorentz' theory matched experiment well, but it was complicated and made many unsubstantiated physical assumptions about the microscopic nature of electromagnetic media. In his 1905 theory of special relativity, Albert Einstein reinterpreted the results of Lorentz' theory in a much simpler and more natural conceptual framework which disposed of the idea of an aether. His derivation is given above, and is now the accepted explanation. Robert S. Shankland reported some conversations with Einstein, in which Einstein emphasized the importance of aberration:\n\nOther important motivations for Einstein's development of relativity were the moving magnet and conductor problem and (indirectly) the negative aether drift experiments, already mentioned by him in the introduction of his first relativity paper. Einstein wrote in a note in 1952:\n\nWhile Einstein's result is the same as Bradley's original equation except for an extra factor of formula_57, it should be emphasized that Bradley's result does not merely give the classical limit of the relativistic case, in the sense that it gives incorrect predictions even at low relative velocities. Bradley's explanation cannot account for situations such as the water telescope, nor for many other optical effects (such as interference) that might occur within the telescope. This is because in the Earth's frame it predicts that the direction of propagation of the light beam in the telescope is not normal to the wavefronts of the beam, in contradiction with Maxwell's theory of electromagnetism. It also does not preserve the speed of light c between frames. However, Bradley did correctly infer that the effect was due to relative velocities.\n\n\n\n"}
{"id": "3097635", "url": "https://en.wikipedia.org/wiki?curid=3097635", "title": "Amphotropism", "text": "Amphotropism\n\nAmphotropism or amphotropic indicates that a pathogen like a virus or a bacterium has a wide host range and can infect more than one species or cell culture line.\n\n"}
{"id": "17139421", "url": "https://en.wikipedia.org/wiki?curid=17139421", "title": "Bargeboard (aerodynamics)", "text": "Bargeboard (aerodynamics)\n\nBargeboards are pieces of bodywork on open-wheel racing cars, serving a purely aerodynamic (as opposed to structural) function. They are curved vertical planes situated longitudinally, between the front wheels and the sidepods, held away from the chassis at the front on struts or other connectors, and connecting to the sidepods or extensions of the floor at the rear. In general, they are significantly taller at the front than at the rear making them trapezoidal in profile, and they curve outward in plan view, being closer to the centerline of the car at the front, and curving out towards the rear.\n\nAerodynamically, bargeboards act primarily as flow conditioners, smoothing and redirecting the turbulent or \"dirty\" air in the wake of the front wing, the front suspension links, and the rotating front wheels.\n\nPart of their function is to direct the turbulent wakes of the front wing and spinning front wheels away from downstream aerodynamic surfaces. In this capacity, they separate different components of this mixture of flows, and direct them either outside and around the sidepods, or direct flow towards the radiator inlets. As such, the configuration of bargeboards can have significance with respect to cooling. They also help direct flow behind the front wheels, reducing the wake drag created by the wheel profile.\n\nAnother important function of bargeboards is to act as vortex generators, redirecting and energizing airflow. The upper, downward sloping edge can shed a large vortex downstream around the sidepods, where it can interact favorably with flip-ups, or aid in sealing the low pressure underbody flow from the ambient stream. The bottom edge of the bargeboard can also shed vortices that energize the airflow to the underbody, which can help delay flow separation and allow the use of more aggressive diffuser profiles. In recent years, these two edges have acquired relatively large serrations or protruding tabs to enhance this aspect of their performance.\n\nSmaller, similar structures are often situated further forward, between the front wheels and the monocoque. These are typically called \"turning vanes\" rather than bargeboards, and are used in addition to, or sometimes in place of full bargeboards, depending on the aerodynamic approach of the car.\n\nInitially relatively simple in their design, bargeboards and turning vanes became progressively more complex through the 2000s, integrating with the floor, mirror supports, suspension mounts, and other structures in more elaborate ways.\n\nSee HVAC turning vanes for turning vanes used inside of ductwork.\n\n\n"}
{"id": "41220345", "url": "https://en.wikipedia.org/wiki?curid=41220345", "title": "Bash valve", "text": "Bash valve\n\nA bash valve is a valve within a piston engine, used to control the admission of the working fluid. They are directly actuated valves, operated by contact between the piston and the valve tip.\n\nBash valves have the advantage of great simplicity, for manufacture and operation. Their disadvantages are that their opening and closing times are relatively crudely controlled, compared to other types of valve gear.\n\nThe valve is usually constructed as a circular poppet valve with a conical seat, inserted into the cylinder from the outside. A protrusion on the inside is hit by the piston as it approaches top dead centre, forcing the valve open.\n\nBash valves are usually held closed by the pressure of fluid in the reservoir behind them. There may be a light spring to assist closing when the reservoir is empty. For this reason they are used as inlet valves, not exhaust. An exhaust bash valve would have the cylinder pressure and the piston actuation both acting to open it, with nothing to close it.\n\nBash valves are not widely used in steam engines, although they are known. Most examples were applied to some form of uniflow steam engine; unlike the more common slide and piston-valved engines with their bidirectional-flow ports, uniflow engines use inlet ports at the cylinder ends and an exhaust near the centre.\n\nAlthough the opening time of a bash valve is fixed, imprecisely controlled and always occurs near top dead centre, this is not a major drawback for a steam engine. A more important requirement is the ability to accurately control the closing time of the valve, and for its duration to be adjustable in order to 'drive' the engine, according to varying load. Some designs of uniflow engine have used a combined mechanical and electromagnetic valve to do this. The valve is opened mechanically, then held by an electromagnet. This requires less electrical power to merely hold the valve than to open it. A patent for such an engine was granted to Sturtevant in 1968.\nThe same idea has recently been revived as the main feature of an Advanced Uniflow Steam Engine. In this engine, a second valve is used for exhaust purposes in the later part of the cycle too, although this one is bashed shut, rather than opened.\n\nBash valves have also been used for the \"ad hoc\" conversion of commonplace petrol small engines, such as lawnmowers, into hobbyist steam engines. The original petrol engine sparkplug mounting hole is used as the location for a new piston-actuated bash valve, together with the original exhaust valve. Performance and efficiency are not a need of such projects.\n\nOne successful application for bash valves has been to pneumatic motors. Owing to the characteristics of compressed air pneumatic power, their simplicity is valuable and their inefficiencies with other fluids are less important.\n\nCompressed air is supplied cold to the motor. Energy is represented solely by the pressure of the air not, unlike steam, by the combination of pressure and temperature. Efficient operation of a steam engine relies upon expansion of the steam during the piston stroke, which relies upon accurate valve timing and an early closure of the valve. During the expansion phase of the steam it does not expand in a simple isothermal fashion, but does so adiabatically, much of the energy having been supplied as heat rather than pressure.\n\nThe compressed air motor is thermodynamically simpler. It uses simple isothermal expansion. This means that expansion is less important, valve timings are thus longer and less crucial and so a simple valve may be adequate.\n\nTo provide long opening times, the bash valve normally incorporates some form of tappet mechanism. Rather than a valve that is held open by the piston directly, the valve becomes double-acting and is opened by the piston's impact at one end of the stroke and closed by a further impact at the other end of the stroke. The tappet and valve are commonly separate, allowing the valve to remain in a well-defined fully open position throughout the stroke, however the tappet is bounced around by the piston.\n\nWhere a reciprocating action is produced, such as for a rock drill, the valve may be actuated either by inertia of the frame or by the movement of the working piston. As the piston hammers back and forth, it impacts a small tappet, which in turn moves the air valve and so reverses the flow of air to the piston. One form of this, the arc tappet valve, was an important feature of the Ingersoll rock drill, the first successful compressed air rock drill for use in mining and tunneling. This used a valve that rotated in a slight arc, rather than sliding. The valve was double-acting, controlling the air supply for both the power and the return stroke. The innovation that made this valve so reliable, thus successful, was a separate tappet that was actuated by the piston in passing at the middle of the stroke, rather than being hammered by a jarring direct impact of the piston. The mid-stroke actuation also opened the valve passageways earlier, before top dead centre, allowing in air that provided a cushioning effect. This further improved the action of the drill, giving a powerful stroke on the working piston and drill rod, but with less damaging hammering to the frame of the drill.\n\nBash valves are not used in such engines. The cylinder peak pressures of an Otto cycle engine are too high for such a valve to remain on its seat.\n\nBash valves are also used in a 'single-shot' situation, where a valve is opened once and then remains open until the contents of a pressure vessel are released. Such valves are used in some fire extinguishers and pre-charged air rifles.\n\nThese valves are arranged so that once lifted off their seat, pressure underneath the valve becomes sufficient to keep the valve raised and so it remains open until the pressure reservoir is empty. The valve is then closed by a light spring.\n\n"}
{"id": "1413411", "url": "https://en.wikipedia.org/wiki?curid=1413411", "title": "Biofouling", "text": "Biofouling\n\nBiofouling or biological fouling is the accumulation of microorganisms, plants, algae, or animals on wetted surfaces. Such accumulation is referred to as epibiosis when the host surface is another organism and the relationship is not parasitic.\n\nAntifouling is the ability of specifically designed materials and coatings to remove or prevent biofouling by any number of organisms on wetted surfaces. Since biofouling can occur almost anywhere water is present, biofouling poses risks to a wide variety of objects such as medical devices and membranes, as well as to entire industries, such as paper manufacturing, food processing, underwater construction, and desalination plants.\n\nSpecifically, the buildup of biofouling on marine vessels poses a significant problem. In some instances, the hull structure and propulsion systems can be damaged. The accumulation of biofoulers on hulls can increase both the hydrodynamic volume of a vessel and the hydrodynamic friction, leading to increased drag of up to 60%. The drag increase has been seen to decrease speeds by up to 10%, which can require up to a 40% increase in fuel to compensate. With fuel typically comprising up to half of marine transport costs, antifouling methods are estimated to save the shipping industry around $60 billion per year. Increased fuel use due to biofouling contributes to adverse environmental effects and is predicted to increase emissions of carbon dioxide and sulfur dioxide between 38 and 72% by 2020.\n\nA variety of antifouling methods have historically been implemented to combat biofouling. Recently, scientists have begun researching antifouling methods inspired by living organisms. This type of design imitation is known as biomimicry.\n\nThe variety among biofouling organisms is highly diverse, and extends far beyond the attachment of barnacles and seaweeds. According to some estimates, over 1,700 species comprising over 4,000 organisms are responsible for biofouling. Biofouling is divided into microfouling — biofilm formation and bacterial adhesion — and macrofouling — attachment of larger organisms. Due to the distinct chemistry and biology that determine what prevents them from settling, organisms are also classified as hard- or soft-fouling types. Calcareous (hard) fouling organisms include barnacles, encrusting bryozoans, mollusks, polychaete and other tube worms, and zebra mussels. Examples of non-calcareous (soft) fouling organisms are seaweed, hydroids, algae and biofilm \"slime\". Together, these organisms form a fouling community.\n\nMarine fouling is typically described as following four stages of ecosystem development. The chemistry of biofilm formation describes the initial steps prior to colonization. Within the first minute the van der Waals interaction causes the submerged surface to be covered with a conditioning film of organic polymers. In the next 24 hours, this layer allows the process of bacterial adhesion to occur, with both diatoms and bacteria (e.g. vibrio alginolyticus, pseudomonas putrefaciens) attaching, initiating the formation of a biofilm. By the end of the first week, the rich nutrients and ease of attachment into the biofilm allow secondary colonizers of spores of macroalgae (e.g. enteromorpha intestinalis, ulothrix) and protozoans (e.g. vorticella, Zoothamnium sp.) to attach themselves. Within 2 to 3 weeks, the tertiary colonizers- the macrofoulers- have attached. These include tunicates, mollusks and sessile Cnidarians.\n\nGovernments and industry spend more than US$5.7 billion annually to prevent and control marine biofouling.\nBiofouling occurs everywhere but is most significant economically to the shipping industries, since fouling on a ship's hull significantly increases drag, reducing the overall hydrodynamic performance of the vessel, and increases the fuel consumption.\n\nBiofouling is also found in almost all circumstances where water-based liquids are in contact with other materials. Industrially important impacts are on the maintenance of mariculture, membrane systems (\"e.g.\", membrane bioreactors and reverse osmosis spiral wound membranes) and cooling water cycles of large industrial equipment and power stations. Biofouling can occur in oil pipelines carrying oils with entrained water, especially those carrying used oils, cutting oils, oils rendered water-soluble through emulsification, and hydraulic oils.\n\nOther mechanisms impacted by biofouling include microelectrochemical drug delivery devices, papermaking and pulp industry machines, underwater instruments, fire protection system piping, and sprinkler system nozzles. In groundwater wells, biofouling buildup can limit recovery flow rates, as is the case in the exterior and interior of ocean-laying pipes where fouling is often removed with a tube cleaning process. Besides interfering with mechanisms, biofouling also occurs on the surfaces of living marine organisms, when it is known as epibiosis.\n\nMedical devices often include fan-cooled heat sinks, to cool their electronic components. While these systems sometimes include HEPA filters to collect microbes, some pathogens do pass through these filters, collect inside the device and are eventually blown out and infect other patients. Devices used in operating rooms rarely include fans, so as to minimize the chance of transmission. Also, medical equipment, high-end computers, swimming pools, drinking-water systems and other products that utilize liquid lines run the risk of biofouling as biological growth occurs inside them.\n\nHistorically, the focus of attention has been the severe impact due to biofouling on the speed of marine vessels. In some instances the hull structure and propulsion systems can become damaged. Over time, the accumulation of biofoulers on hulls increases both the hydrodynamic volume of a vessel and the frictional effects leading to increased drag of up to 60% The additional drag can decrease speeds up to 10%, which can require up to a 40% increase in fuel to compensate. With fuel typically comprising up to half of marine transport costs, biofouling methods are estimated to cost the shipping industry around $1 billion per year. Increased fuel use due to biofouling contributes to adverse environmental effects and is predicted to increase emissions of carbon dioxide and sulfur dioxide between 38 and 72 percent by 2020.\n\nBiofouling also impacts aquaculture, increasing production and management costs, while decreasing product value. Fouling communities may compete with shellfish directly for food resources, impede the procurement of food and oxygen by reducing water flow around shellfish, or interfere with the operational opening of their valves. Consequently, stock affected by biofouling can experience reduced growth, condition and survival, with subsequent negative impacts on farm productivity. Although many methods of removal exist, they often impact the cultured species, sometimes more so than the fouling organisms themselves.\n\nShipping companies have historically relied on scheduled biofouler removal to keep such accretions to a manageable level. However, the rate of accretion can vary widely between vessels and operating conditions, so predicting acceptable intervals between cleanings is difficult.\n\nLED manufacturers have developed a range of UVC (250-280 nm) equipment that can detect biofouling buildup, and can even prevent it.\n\nFouling detection relies on the biomass' property of fluorescence. All microorganisms contain natural intracellular fluorophores, which radiate in the UV range when excited. At UV-range wavelengths, such fluorescence arises from three aromatic amino acids - tyrosine, phenylalanine, and tryptophan. The easiest to detect is tryptophan, which radiates at 350 nm when irradiated at 280 nm.\n\nAnti-fouling is the process of preventing accumulations from forming. In industrial processes, bio-dispersants can be used to control biofouling. In less controlled environments, organisms are killed or repelled with coatings using biocides, thermal treatments, or pulses of energy. Nontoxic mechanical strategies that prevent organisms from attaching include choosing a material or coating with a slippery surface, creating an ultra-low fouling surface with the use of zwitterions, or creating nanoscale surface topologies similar to the skin of sharks and dolphins, which only offer poor anchor points.\n\nBiocides are chemical substances that deter the microorganisms responsible for biofouling. The chemical substances are incorporated into an anti-fouling surface coating, typically through physical adsorption or through chemical modification of the surface. Biofouling occurs on surfaces after formation of a biofilm. The biofilm creates a surface onto which successively larger microorganisms can attach. In marine environments this buildup usually concludes with barnacle attachment. The biocides often target the microorganisms that create the initial biofilm, typically bacteria. Once dead, they are unable to spread and can detach. Other biocides are toxic to larger organisms in biofouling, such as the fungi and algae. The most commonly used biocide, and anti-fouling agent, is the tributyltin moiety (TBT). It is toxic to both microorganisms and larger aquatic organisms. Biocides are also added to pool water, drinking water, and liquid lines for cooling electronics to control biological growth.\n\nThe prevalence of TBT and other tin-based anti-fouling coatings on marine vessels was a major environmental problem. TBT has been shown to harm many marine organisms, specifically oysters and mollusks. Extremely low concentrations of tributyltin moiety (TBT) causes defective shell growth in the oyster \"Crassostrea gigas\" (at a concentration of 20 ng/l) and development of male characteristics in female genitalia in the dog whelk \"Nucella lapillus\" (where gonocharacteristic change is initiated at 1 ng/l).\n\nThe international maritime community has phased out the use of organtin-based coatings. This phase-out of toxic biocides in marine coatings posed a severe problem for the shipping industry; it presents a major challenge for the producers of coatings to develop alternative technologies. Safer methods of biofouling control are actively researched. Copper compounds have successfully been used in paints, heat sinks inside of medical electronics, and continue to be used as metal sheeting (for example Muntz metal, which was specifically made for this purpose), though there is still debate as to the safety of copper.\n\nNon-toxic anti-sticking coatings prevent attachment of microorganisms thus negating the use of biocides. These coatings are usually based on organic polymers, which allow researchers to add additional functions, such as antimicrobial activity.\n\nThere are two classes of non-toxic anti-fouling coatings. The most common class relies on low friction and low surface energies. Low surface energies result in hydrophobic surfaces. These coatings create a smooth surface, which can prevent attachment of larger microorganisms. For example, fluoropolymers and silicone coatings are commonly used. These coatings are ecologically inert but have problems with mechanical strength and long-term stability. Specifically, after days biofilms (slime) can coat the surfaces, which buries the chemical activity and allows microorganisms to attach. The current standard for these coatings is polydimethylsiloxane, or PDMS, which consists of a non-polar backbone made of repeating units of silicon and oxygen atoms. The non-polarity of PDMS allows for biomolecules to readily adsorb to its surface in order to lower interfacial energy. However, PDMS also has a low modulus of elasticity that allows for the release of fouling organisms at speeds of greater than 20 knots. The dependence of effectiveness on vessel speed prevents use of PDMS on slow-moving ships or those that spend significant amounts of time in port.\n\nThe second class of non-toxic anti-fouling coatings are hydrophilic coatings. They rely on high amounts of hydration in order to increase the energetic penalty of removing water for proteins and microorganisms to attach. The most common examples of these coatings are based on highly hydrated zwitterions, such as glycine betaine and sulfobetaine. These coatings are also low-friction, but are considered by some to be superior to hydrophobic surfaces because they prevent bacteria attachment, preventing biofilm formation. These coatings are not yet commercially available and are being designed as part of a larger effort by the Office of Naval Research to develop environmentally safe biomimetic ship coatings.\n\nOne of the more common methods of antifouling comes from growing polymer chains from a surface, often by poly(ethylene glycol) or PEG. However, challenges exist in creating a functionalized surface to which PEG chains may be grown, especially in aqueous environments. Researchers have been able to study the methods by which the common blue mussel \"Mytilus edulis\"' is able to adhere to solid surfaces in marine environments using mussel adhesive proteins, or MAPs. MAPs typically comprise several proteins, of which the most common repeating sequence is Ala-Lys-Pro-Ser-Tyr-trans-2,3-cis-3,4-dihydroxyproline (DHP) -Hyp-Thr-3,4-dihydroxyphenylalanine (DOPA) -Lys. The inclusion of the hydroxylated DHP and DOPA amino acids is thought to contribute to the adhesive nature of the MAPs. Recent studies have looked into using a short chain of DOPA residues as an adhesive end-group for antifouling PEG polymers, which show promise in adsorbing onto certain metal surfaces. Increasing the number of DOPA residues to three greatly improves the total amount of adsorbed DOPA-PEG polymers and exhibits antifouling properties exceeding most other 'grafting-to' polymeric functionalization methods.\n\nThe antifouling characteristics of PEG are well documented, but the service life of such coatings is debated due to the hydrolysis of PEG chains in air, as well as by the low concentrations of transition metal ions present in seawater. Using DOPA residues as attachment points, new polymers similar in structure to the polypeptide backbone of proteins are being investigated, such as peptidomimetic polymer (PMP1). PMP1 uses a repeat unit of N-substituted glycine instead of ethylene glycol to impart antifouling properties. The N-substituted glycine is structurally similar to ethylene glycol and is hydrophilic, so easily dissolves in water. In controlled studies, PMP1-coated titanium surfaces were seen to be resistant to biofouling over a period of 180 days, even with continued addition and exposure to microfouling organisms.\n\nPulsed laser irradiation is commonly used against diatoms. Plasma pulse technology is effective against zebra mussels and works by stunning or killing the organisms with microsecond duration energizing of the water with high voltage electricity.\n\nThere are several companies that offer alternatives to paint-based antifouling, using ultrasonic transducers mounted in or around the hull of small to medium-sized boats. Research has shown these systems can help reduce fouling, by initiating bursts of ultrasonic waves through the hull medium to the surrounding water, killing or denaturing the algae and other micro-organisms that form the beginning of the fouling sequence. The systems cannot work on wooden-hulled boats, or boats with a soft-cored composite material, such as wood or foam. The systems have been loosely based on technology proven to control algae blooms.\n\nSimilarly, another method shown to be effective against algae buildups bounced brief high-energy acoustic pulses down pipes.\n\nThe medical industry utilizes a variety of energy methods to address bioburden issues associated with biofouling. Autoclaving typically involves heating a medical device to 121 °C (249 °F) for 15–20 minutes. Ultrasonic cleaning, UV light, and chemical wipe-down or emersion can also be used for different types of devices.\n\nRegimens to periodically use heat to treat exchanger equipment and pipes have been successfully used to remove mussels from power plant cooling systems using water at 105 °F (40 °C) for 30 minutes.\n\nMedical devices used in operating rooms, ICUs, isolation rooms, biological analysis labs, and other high contamination risk areas have negative pressure (constant exhaust) in the rooms, maintain strict cleaning protocols, require equipment with no fans, and often drape equipment in protective plastic.\n\nAs of 2016, researchers have shown that deep-ultraviolet UVC irradiation, a noncontact, nonchemical solution that can be used across a range of instruments. Radiation in the UVC range prevents biofilm formation by deactivating the DNA in bacteria, viruses, and other microbes. Preventing biofilm formation prevents larger organisms from attaching themselves to the instrument and eventually rendering it inoperable. (Hari Venugopalan, \"Photonic Frontiers: LEDs - UVC LEDs reduce marine biofouling\", Laser Focus World (July 2016) pp. 28-31 )\n\nBiofouling, especially of ships, has been a problem for as long as humanity has been sailing the oceans. The earliest written mention of fouling was by Plutarch who recorded this explanation of its impact on ship speed: \"when weeds, ooze, and filth stick upon its sides, the stroke of the ship is more obtuse and weak; and the water, coming upon this clammy matter, doth not so easily part from it; and this is the reason why they usually calk their ships.\"\n\nThe use of pitch and copper plating as anti-fouling techniques were attributed to ancient seafaring nations, such as the Phoenicians and Carthaginians (1500- 300BC). Wax, tar and asphaltum have been used since early times. An Aramaic record dating from 412 B.C. tells of a ship's bottom being coated with a mixture of arsenic, oil and sulphur. In \"Deipnosophistae\", Athenaeus described the anti-fouling efforts taken in the construction of the great ship of Hieron of Syracuse (died 467 BC).\n\nBefore the 18th century, various anti-fouling techniques were used, with three main substances employed: \"White stuff\", a mixture of train oil (Whale oil), rosin and sulfur; \"Black stuff\", a mixture of tar and pitch; and \"Brown stuff\", which was simply sulfur added to Black stuff. In many of these cases, the purpose of these treatments is ambiguous. There is dispute whether many of these treatments were actual anti-fouling techniques, or whether, when they were used in conjunction with lead and wood sheathing, they were simply intended to combat wood-boring shipworms.\nIn 1708, Charles Perry suggested copper sheathing explicitly as an anti-fouling device but the first experiments were not made until 1761 with the sheathing of HMS Alarm, after which the bottoms and sides of several ships' keels and false keels were sheathed with copper plates.\n\nThe copper performed well in protecting the hull from invasion by worm, and in preventing the growth of weed, for when in contact with water, the copper produced a poisonous film, composed mainly of oxychloride, that deterred these marine creatures. Furthermore, as this film was slightly soluble, it gradually washed away, leaving no way for marine life to attach itself to the ship.\nFrom about 1770, the Royal Navy set about coppering the bottoms of the entire fleet and continued to the end of the use of wooden ships. The process was so successful that the term \"copper-bottomed\" came to mean something that was highly dependable or risk free.\n\nWith the rise of iron hulls in the 19th century, copper sheathing could no longer be used due to its galvanic corrosive interaction with iron. Anti-fouling paints were tried, and in 1860, the first practical paint to gain widespread use was introduced in Liverpool and was referred to as \"McIness\" hot plastic paint. These treatments had a short service life, were expensive, and relatively ineffective by modern standards.\n\nThe inventor of an antifouling paint was Captain (Schiffskapitan) Ferdinand Gravert, born 1847 in Glückstadt, Germany (then Denmark). He sold his formula in 1913 at Taltal, Chile.\n\nBy the mid-twentieth century, copper oxide-based paints could keep a ship out of drydock for as much as 18 months, or as little as 12 in tropical waters. The shorter service life was due to rapid leeching of the toxicant, and chemical conversion into less toxic salts, which accumulated as a crust that would inhibit further leaching of active cuprous oxide from the layer under the crust.\n\nThe 1960s brought a breakthrough, with self-polishing paints that used seawater's ability to hydrolize the paint's copolymer bond and release the stored toxin at a slow, controlled rate. These paints employed organotin chemistry (\"tin-based\") biotoxins such as tributyltin oxide (TBT) and were effective for up to four years. The discovery that these biotoxins have severe impact on mariculture, with biological effects to marine life at a concentration of 1 nanogram per liter, led to their worldwide ban by the International Maritime Organization in October 2001. TBT in particular has been described as the most toxic pollutant ever deliberately released in the ocean.\n\nAs an alternative to organotin toxins, there has been renewed interest in copper as the active agent in ablative or self polishing paints, with reported service lives up to 5 years. Modern adhesives permit application of copper alloys to steel hulls without creating galvanic corrosion. However, copper alone is not impervious to diatom and algae fouling. Some studies indicate that copper may also present an unacceptable environmental impact.\n\nModern empirical study of biofouling began in the early 19th century with Davy's experiments linking the effectiveness of copper to its solute rate. Insights into the stages of formation grew in the 1930s when the microbiologist Claude ZoBell defined the sequence of events initiating the fouling of submerged surfaces. He showed that the attachment of organisms must be preceded by the adsorption of organic compounds now referred to as extracellular polymeric substances.\n\nOne trend of research is the study of the relationship between wettability and anti-fouling effectiveness. Another trend is the study of living organisms as the inspiration for new functional materials. An example of biomimetic antifouling research was conducted at the University of Florida into how marine animals such as dolphins and sharks are able to effectively deter biofouling on their skin. Researchers examined the nanoscale structure of sharks and designed an anti-fouling surface known commercially as Sharklet. Studies show that the nanoscale topologies function not only due to the reduction of sites for macrofoulers to attach, but also due to the same thermodynamic barrier that any surface with low wettability presents.\n\nMaterials research into superior antifouling surfaces for fluidized bed reactors suggest that low wettability plastics such as Polyvinyl chloride (\"PVC\"), high-density polyethylene and polymethylmethacrylate (\"plexiglas\") demonstrate a high correlation between their resistance to bacterial adhesion and their hydrophobicity.\n\nA study of the biotoxins used by organisms has revealed several effective compounds, some of which are more powerful than synthetic compounds. Bufalin, a bufotoxin, was found to be over 100 times as potent as TBT, and over 6,000 times more effective in anti-settlement activity against barnacles.\n\n"}
{"id": "39058868", "url": "https://en.wikipedia.org/wiki?curid=39058868", "title": "Bioresorbable metal", "text": "Bioresorbable metal\n\nBioresorbable (also called biodegradable or bioabsorbable) metals are metals or their alloys that degrade safely within the body. The primary metals in this category are magnesium-based and iron-based alloys, although recently zinc has also been investigated. Currently, the primary uses of bioresorbable metals are as stents for blood vessels (for example bioresorbable stents) and other internal ducts.\n\nAlthough bioabsorbable polymers and other materials have come into widespread use in recent years, degradable metals have not yet had the same success in the medical industry.\n\nThe driving force behind the development of bioresorbable metals is primarily due to their ability to provide metal-like mechanical properties while degrading safely in the body. This is especially relevant in orthopaedic applications, where although many surgeries only require implants to provide temporary support (allowing the surrounding tissue to heal), the majority of current bio-metals are permanent (e.g. stainless steel, titanium). Degradation of the implant means that intervention or secondary surgery will not be necessary to remove the material at the end of its functional life, providing significant savings in both cost and time for the patient and health care system. In addition, the corrosion products of current bio-metals (which will still corrode in the body to some degree) can generally not be considered biocompatible.\n\nThere are a number of applications for biodegradable metals, including cardiovascular implants (i.e. stents) and orthopedics. It is in this latter category where these materials offer the greatest potential. Bioresorbable metals are able to withstand loads that would destroy any currently available polymers, and offer much greater plasticity than bioceramics, which are brittle and prone to fracture. A well-designed implant could provide the exact mechanical support needed for different areas (through alloying and metal working), and load would be transferred to the surrounding tissue over time, letting it heal and reducing the effects of stress shielding. A summary of the primary benefits and drawbacks of magnesium biomaterials has been provided by Kirkland. This is a significant problem as the majority of tests performed in the research community are a mix of other standards from both the biomedical and the engineering (e.g. corrosion) communities, often making comparison between results difficult.\n\nEven though all elements in a bioresorbable metal may themselves be considered biocompatible, the morphology and elemental makeup (or combination of elements) of the degradation products may cause adverse reactions in the body. In addition, the rapid evolution of hydrogen gas that is concomitant with Mg-alloy degradation may cause addition problems in vivo. It is therefore crucial to intricately understand the corrosion of each implant and the products that are release, in light of their toxicity and the likelihood of inflammation. The majority of studies in the literature have focused on elements that are known to be biocompatible or abundant in the body, such as calcium and zinc.\n\nAlthough all metals will degrade and eventually disappear inside the body through the processes of corrosion and wear, true bioresorbable metals must have an appreciable degradation rate to allow the implant to be absorbed in a practical amount of time in reference to their application. Also, any degradation product would have to be safely metabolized or excreted by the body to avoid toxicity and inflammation.\n\nPerhaps the most widely investigated material in this category, magnesium was originally investigated as a potential biomaterial in 1878 when it was used by physician Edward C. Huse in wire form as a ligature to stop bleeding. Development continued into the 1920s, after which Mg-based biomaterials fell out of general investigation due to their poor performance (likely due to impurities in the alloys drastically increasing corrosion). It was not until the late 1990s that interest started to pick up again, spurred by the availability of ultra-high purity Mg, which significantly increases its longevity inside the body.\n\nCurrently, most research on Mg is focused on reducing and controlling the rate of degradation, with many alloys corroding too rapidly (in vitro) for any practical application.\n\nThe majority of iron-based alloy research has been focused on cardiovascular applications, such as stents. However this area receives much less interest in the research community than Mg-based alloys.\n\nTo date little work has been published on the use of a primarily zinc-based biomaterial, with corrosion rates found to be very low and zinc within a tolerable toxicity range \n\nAlthough strictly speaking a side-category, a related, relatively new area of interest has been the investigation of bioabsorbable metallic glass, with a group at UNSW currently investigating these novel materials.\n"}
{"id": "14251545", "url": "https://en.wikipedia.org/wiki?curid=14251545", "title": "Coenzyme-B sulfoethylthiotransferase", "text": "Coenzyme-B sulfoethylthiotransferase\n\nIn enzymology, coenzyme-B sulfoethylthiotransferase, also known as methyl-coenzyme M reductase (MCR) or most systematically as 2-(methylthio)ethanesulfonate:N-(7-thioheptanoyl)-3-O-phosphothreonine S-(2-sulfoethyl)thiotransferase is an enzyme that catalyzes the final step in the formation of methane. It does so by combining the hydrogen donor coenzyme B and the methyl donor coenzyme M. Via this enzyme, most of the natural gas on earth was produced. Ruminants (e.g. cows) produce methane because their rumens contain methanogenic prokaryotes (Archaea) that encode and express the set of genes of this enzymatic complex.\n\nThe enzyme has two active sites, each occupied by the nickel-containing F cofactor.\n\nThus, the two substrates of this enzyme are 2-(methylthio)ethanesulfonate and N-(7-mercaptoheptanoyl)threonine 3-O-phosphate, whereas its two products are CoM-S-S-CoB and methane. 3-Nitrooxypropanol inhibits the enzyme.\n\nIn some species, the enzyme reacts in reverse (a process called \"reverse methanogenesis\"), catalysing the anaerobic oxidation of methane, therefore removing it from the environment. Such organisms are methanotrophs.\n\nThis enzyme belongs to the family of transferases, specifically those transferring alkylthio groups.\n\nThis enzyme participates in folate biosynthesis.\n\nCoenzyme-B sulfoethylthiotransferase is a multiprotein complex made up of a pair of identical halves. Each half is made up of three subunits: α, β and γ, also called McrA, McrB and McrG, respectively.\n\n"}
{"id": "183999", "url": "https://en.wikipedia.org/wiki?curid=183999", "title": "Cold dark matter", "text": "Cold dark matter\n\nIn cosmology and physics, cold dark matter (CDM) is a hypothetical form of dark matter whose particles moved slowly compared to the speed of light (the \"cold\" in CDM) since the universe was approximately one year old (a time when the cosmic particle horizon contained the mass of one typical galaxy); and interact very weakly with ordinary matter and electromagnetic radiation (the \"dark\" in CDM). It is believed that approximately 84.54% of matter in the universe is dark matter, with only a small fraction being the ordinary baryonic matter that composes stars, planets, and living organisms.\n\nThe theory of cold dark matter was originally published in 1982 by three independent groups of cosmologists; James Peebles, J. Richard Bond, Alex Szalay, and Michael Turner; and George Blumenthal, H. Pagels, and Joel Primack.\nA review article in 1984 by Blumenthal, Sandra Moore Faber, Primack, and Martin Rees developed the details of the theory.\n\nIn the cold dark matter theory, structure grows hierarchically, with small objects collapsing under their self-gravity first and merging in a continuous hierarchy to form larger and more massive objects. In the hot dark matter paradigm, popular in the early 1980s, structure does not form hierarchically (\"bottom-up\"), but rather forms by fragmentation (\"top-down\"), with the largest superclusters forming first in flat pancake-like sheets and subsequently fragmenting into smaller pieces like our galaxy the Milky Way. Predictions of the cold dark matter paradigm are in general agreement with observations of cosmological large scale structure.\n\nSince the late 1980s or 1990s, most cosmologists favor the cold dark matter theory (specifically the modern Lambda-CDM model) as a description of how the universe went from a smooth initial state at early times (as shown by the cosmic microwave background radiation) to the lumpy distribution of galaxies and their clusters we see today—the large-scale structure of the universe. The theory sees the role that dwarf galaxies played as crucial, as they are thought to be natural building blocks that form larger structures, created by small-scale density fluctuations in the early universe.\n\nDark matter is detected through its gravitational interactions with ordinary matter and radiation. As such, it is very difficult to determine what the constituents of cold dark matter are. The candidates fall roughly into three categories:\n\n\n\n\nSeveral discrepancies between the predictions of the particle cold dark matter paradigm and observations of galaxies and their clustering have arisen:\n\n\nSome of these problems have proposed solutions, but it remains unclear whether they can be solved without abandoning the CDM paradigm.\n\n"}
{"id": "1950960", "url": "https://en.wikipedia.org/wiki?curid=1950960", "title": "Crowbar (circuit)", "text": "Crowbar (circuit)\n\nA crowbar circuit is an electrical circuit used for preventing an overvoltage condition of a power supply unit from damaging the circuits attached to the power supply. It operates by putting a short circuit or low resistance path across the voltage output (V), quite like were one to drop a crowbar across the output terminals of the power supply. Crowbar circuits are frequently implemented using a thyristor, TRIAC, trisil or thyratron as the shorting device. Once triggered, they depend on the current-limiting circuitry of the power supply or, if that fails, the blowing of the line fuse or tripping the circuit breaker.\n\nAn example crowbar circuit is shown to the right. This particular circuit uses an LM431 adjustable zener regulator to control the gate of the TRIAC. The resistor divider of R and R provide the reference voltage for the LM431. The divider is set so that during normal operating conditions, the voltage across R is slightly lower than V of the LM431. Since this voltage is below the minimum reference voltage of the LM431, it remains off and very little current is conducted through the LM431. If the cathode resistor is sized accordingly, very little voltage will be dropped across it and the TRIAC gate terminal will be essentially at the same potential as MT1, keeping the TRIAC off. If the supply voltage increases, the voltage across R will exceed V and the LM431 cathode will begin to draw current. The voltage at the gate terminal will be pulled down, exceeding the gate trigger voltage of the TRIAC and latching it on.\n\nA crowbar circuit is distinct from a clamp in pulling, once triggered, the voltage below the trigger level, usually close to ground. A clamp prevents the voltage from exceeding a preset level. Thus, a crowbar will not automatically return to normal operation when the overvoltage condition is removed; power must be removed entirely to stop its conduction.\n\nAn active crowbar is a crowbar that can remove the short circuit when the transient is over thus allowing the device to resume normal operation. Active crowbars use a transistor, gate turn off (GTO) thyristor or forced commutated thyristor instead of a thyristor to short the circuit. Active crowbars are commonly used to protect the frequency converter in the rotor circuit of doubly fed generators against high voltage and current transients caused by the voltage dips in the power network. Thus the generator can ride through the fault and quickly continue the operation even during the voltage dip.\n\nThe advantage of a crowbar over a clamp is that the low holding voltage of the crowbar lets it carry higher fault current without dissipating much power (which could otherwise cause overheating). Also, a crowbar is more likely than a clamp to deactivate a device (by blowing a fuse or tripping a breaker), bringing attention to the faulty equipment.\n\nThe term is also used as a verb to describe the act of short-circuiting the output of a power supply, or the malfunction of a CMOS circuit -- the PMOS half of a pair lingering in a near on-state when only its corresponding NMOS is supposed to be on (or the NMOS when the PMOS is supposed to be on) -- resulting in a near short-circuit current between supply rails.\n\nHigh voltage crowbars are used for HV tube (Klystron and IOT) protection.\n\nMany bench top power supplies have a crowbar circuit to protect the connected equipment.\n\nMicrowave ovens often use a microswitch that acts as a crowbar circuit in the door latch assembly. This will absolutely prevent the magnetron from being energized with the door open. Activation will blow the main fuse and ruin the microswitch. \n\n\n"}
{"id": "42934", "url": "https://en.wikipedia.org/wiki?curid=42934", "title": "Cryostasis (clathrate hydrates)", "text": "Cryostasis (clathrate hydrates)\n\nThe term cryostasis was introduced to name the reversible preservation technology for live biological objects which is based on using clathrate-forming gaseous substances under increased hydrostatic pressure and hypothermic temperatures.\n\nLiving tissues cooled below the freezing point of water are damaged by the dehydration of the cells as ice is formed between the cells. The mechanism of freezing damage in living biological tissues has been elucidated by Renfret.\n\nThe vapor pressure of the ice is lower than the vapor pressure of the solute water in the surrounding cells and as heat is removed at the freezing point of the solutions, the ice crystals grow between the cells, extracting water from them. As the ice crystals grow, the volume of the cells shrinks, and the cells are crushed between the ice crystals. Additionally, as the cells shrink, the solutes inside the cells are concentrated in the remaining water, increasing the intracellular ionic strength and interfering with the organization of the proteins and other organized intercellular structures. Eventually, the solute concentration inside the cells reaches the eutectic and freezes. The final state of frozen tissues is pure ice in the former extracellular spaces, and inside the cell membranes a mixture of concentrated cellular components in ice and bound water. In general, this process is not reversible to the point of restoring the tissues to life.\n\nCryostasis utilizes using clathrate-forming gases that penetrate and saturate the biological tissues causing clathrate hydrates formation (under specific pressure-temperature conditions) inside the cells and in the extracellular matrix. Clathrate hydrates are a class of solids in which gas molecules occupy \"cages\" made up of hydrogen-bonded water molecules. These \"cages\" are unstable when empty, collapsing into conventional ice crystal structure, but they are stabilised by the inclusion of the gas molecule within them. Most low molecular weight gases (including CH, HS, Ar, Kr, and Xe) will form a hydrate under some pressure-temperature conditions.\nClathrates formation will prevent the biological tissues from dehydration which will cause irreversible inactivation of intracellular enzymes.\n\n"}
{"id": "5340351", "url": "https://en.wikipedia.org/wiki?curid=5340351", "title": "Crystal engineering", "text": "Crystal engineering\n\nCrystal engineering is the design and synthesis of molecular solid state structures with desired properties, based on an understanding and use of intermolecular interactions. The two main strategies currently in use for crystal engineering are based on hydrogen bonding and coordination bonding. These may be understood with key concepts such as the supramolecular synthon and the secondary building unit.\n\nThe term ‘crystal engineering’ was first used in 1971 by Gerhard Schmidt in connection with photodimerization reactions in crystalline cinnamic acids. Since this initial use, the meaning of the term has broadened considerably to include many aspects of solid state supramolecular chemistry. A useful modern definition is that provided by Gautam Desiraju, who in 1988 defined crystal engineering as \"the understanding of intermolecular interactions in the context of crystal packing and the utilization of such understanding in the design of new solids with desired physical and chemical properties.\" Since many of the bulk properties of molecular materials are dictated by the manner in which the molecules are ordered in the solid state, it is clear that an ability to control this ordering would afford control over these properties.\n\nCrystal engineering relies on noncovalent bonding to achieve the organization of molecules and ions in the solid state. Much of the initial work on purely organic systems focused on the use of hydrogen bonds, though with the recent extension to inorganic systems, the coordination bond has also emerged as a powerful tool. In addition to this, especially through studies during the last decade, the use of halogen bonds has proved beneficial in providing additional control in crystal design. Other intermolecular forces such as π…π and Au…Au interactions have all been exploited in crystal engineering studies, and ionic interactions can also be important. However, the two most common strategies in crystal engineering still employ only hydrogen bonds and coordination bonds.\n\nMolecular self-assembly is at the heart of crystal engineering, and it typically involves an interaction between complementary hydrogen bonding faces or a metal and a ligand. By analogy with the retrosynthetic approach to organic synthesis, Desiraju coined the term \"supramolecular synthon\" to describe building blocks that are common to many structures and hence can be used to order specific groups in the solid state. The carboxylic acid dimer represents a simple supramolecular synthon, though in practice this is only observed in approximately 30% of crystal structures in which it is theoretically possible. The Cambridge Structural Database (CSD) provides an excellent tool for assessing the efficiency of particular synthons. The supramolecular synthon approach has been successfully applied in the synthesis of one dimensional tapes, two dimensional sheets and three dimensional structures. The CSD today contains atomic positional parameters for nearly 800 000 crystal structures, and this forms the basis for heuristic or synthon based or \"experimental\" crystal engineering.\n\nA major development in the field of crystal engineering in the last decade is related to the development of design strategies for bi-component and higher multi-component crystals (also known as cocrystals). The design of cocrystals is a difficult task as it involves recognition between different molecules which might be completely different in shape and size. Therefore, the more the number of components in a crystal, the more challenging it is to synthesize. Initially, the synthesis of cocrystals was centered on the design of binary ones. This is most often achieved with strong heteromolecular interactions. Ternary ones were designed mainly by interaction insulation, interaction hierarchy or by shape-size mimicry. However, it has been shown recently that it is possible to synthesize up to five-component crystals by choosing a suitable retrosynthetic strategy. The main relevance of multi-component crystals, apart from the synthetic challenge, arises from the advantage of tuning a particular property by changing the components. The main development in this front is focused upon designing pharmaceutical cocrystals. Pharmaceutical cocrystals are generally composed of one API (Active Pharmaceutical Ingredient) with other molecular substances that are considered safe according to the guidelines provided by WHO (World Health Organization). It has been shown that various properties (such as solubility, bioavailability, permeability) of an API can be modulated through the formation of pharmaceutical cocrystals.\n\nThe study and formation of 2D architectures (i.e., molecularly thick architectures) has rapidly emerged as a branch of engineering with molecules. The formation (often referred as molecular self-assembly depending on its deposition process) of such architectures lies in the use of solid interfaces to create adsorbed monolayers. Such monolayers may feature spatial crystallinity in an investigated time window, and thus the terminology of 2D crystal engineering is well suited. However the dynamic and wide range of monolayer morphologies ranging from amorphous to network structures have made of the term (2D) supramolecular engineering a more accurate term. Specifically, supramolecular engineering refers to \"(The) design (of) molecular units in such way that a predictable structure is obtained\" or as \"the design, synthesis and self-assembly of well defined molecular modules into tailor-made supramolecular architectures\".\n\nThe field of 2D crystal engineering has advanced over the years especially through the advent of scanning probe microscopic techniques which enable one to visualize networks with sub-molecular precision. Understanding the mechanism of these two dimensional assemblies may provide insights to the bottom up fabrication processes at the interfaces. The many aspects of developments in this field include the understanding of interactions, studies on polymorphism, design of nanoporous networks. Engineering the size and symmetry of the cavities and performing host guest chemistry inside the pores under nano confinement remains an attractive interest of this field. More recently, multicomponent networks are also studied that are formed by the application of crystal engineering principles. Although, there is a very high influence of the underlying substrate on the formation of the two dimensional assemblies, at least in a few cases, a relation was found between 2D assemblies and the bulk crystal structures.\n\nPolymorphism is the phenomenon wherein the same chemical compound exists in different crystal forms. In the initial days of crystal engineering, polymorphism was not properly understood and incompletely studied. Today, it is one of the most exciting branches of the subject partly because polymorphic forms of drugs may be entitled to independent patent protection if they show new and improved properties over the known crystal forms. With the growing importance of generic drugs, the importance of crystal engineering to the pharmaceutical industry is expected to grow exponentially.\n\nPolymorphism arises due to the competition between kinetic and thermodynamic factors during crystallization. While long-range strong intermolecular interactions dictate the formation of kinetic crystals, the close packing of molecules generally drives the thermodynamic outcome. Understanding this dichotomy between the kinetics and thermodynamics constitutes the focus of research related to the polymorphism.\n\nIn organic molecules, three types of polymorphism are mainly observed. Packing polymorphism arises when molecules pack in different ways to give different structures. Conformational polymorphism, on the other hand is mostly seen in flexible molecules where molecules have multiple conformational possibilities within a small energy window. As a result, multiple crystal structures can be obtained with the same molecule but in different conformations. The rarest form of polymorphism arises from the differences in the primary synthon and this type of polymorphism is called as synthon polymorphism. With the growth in research in the cocrystals in recent times, it is observed that cocrystals are also prone to polymorphism.\n\nCrystal structure prediction (CSP) is a computational approach to generate energetically feasible crystal structures (with corresponding space group and positional parameters) from a given molecular structure. The CSP exercise is considered most challenging as “experimental” crystal structures are very often kinetic structures and therefore are very difficult to predict. In this regard, many protocols have been proposed and are tested through several blind tests organized by CCDC since 2002. A major advance in the CSP happened in 2007 while a hybrid method based on tailor made force fields and density functional theory (DFT) was introduced. In the first step, this method employs tailor made force fields to decide upon the ranking of the structures followed by a dispersion corrected DFT method to calculate the lattice energies precisely.\n\nApart from the ability of predicting crystal structures, CSP also gives computed energy landscapes of crystal structures where many structures lie within a narrow energy window. This kind of computed landscapes lend insights into the study on polymorphism, design of new structures and also help to design crystallization experiments.\n\nThe design of crystal structures with desired properties is the ultimate goal of crystal engineering. While major advances towards this have been made in coordination polymers, the application of successful strategies to design properties in purely organic solids is still limited. Most of the initial studies related to the crystal engineering of organic molecular solids were concentrated upon understanding the intermolecular interaction and identification of supramolecular synthons. In recent years, considerable attention has been given to the design the organic solids with specific properties. The early attempts to apply of crystal engineering principles to design the functional solids were focused upon the synthesis of non-linear optical materials, especially those with second harmonic generation (SHG) properties.\n\nAnother major development in this area is related to the control of photodimerization reactions through templates by application of crystlal engineering strategies. The importance of topochemical control in solid-state reactions was first realized by Schmidt. However, with the advent of cocrystals, multiple strategies have been discovered over the years to get photodimerization products from the molecules which are otherwise photo inactive with precise stereochemistry. A more recent development in property design is linked with the mechanical properties in relation with the crystal structures. Various mechanical properties such as bending, shearing or brittleness have been explained on the basisy of crystal packing. It has also been shown that solids of desired mechanical properties can be designed through the precise control of packing features. More recently, nanoindentation techniques are used in quantification of some of these properties in terms of hardness and elasticity.\n\nApart from these properties, significant efforts have been made to control the photophysical properties as well. Despite all the developments made so far, this area of property design, especially in the purely organic solids, is still at an evolving stage and needs more directed approaches to understand the connectivity between the crystal structures and their respective properties.\n\nCrystallization mechanism forms the core of all the problems in crystal engineering. A proper understanding of crystallization mechanisms will enable us to sort among the various possibilities generated by say CSP. Moreover, it will help a crystal engineer to design a solid with particular function. It is therefore of immense importance to understand the progression from solution to the nucleus to the crystal structures, in other terms, how the entropy dominated situation in solution converts to an enthalpy driven one in the crystals through the nucleation step. As the nucleus is difficult to identify, the approaches made towards this end can be generally divided into two categories. The first type centers on the studies in solution through various spectroscopic techniques to understand the structure of the assembly in the solution and these studies give an idea about the initial stages of crystallization. The second type of approach take into account all the relevant crystal structures of a given compound and this constitutes a \"structural landscape\". Each structure in a structural landscape is considered as an individual data point and while taken together it provides a holistic viewpoint towards the crystallization mechanism. A special situation happens when multiple components are observed in an asymmetric unit (Z′>1). These structures result from the interrupted crystallization and therefore shed lights on the intermediate stages of crystallization.\n\nCrystal engineering is a rapidly expanding discipline as revealed by the recent appearance of several international scientific journals in which the topic plays a major role. These include \"CrystEngComm\" from the Royal Society of Chemistry and \"Crystal Growth & Design\" from the American Chemical Society. The new open access journals IUCrJ from the International Union of Crystallography and \"Crystals\" from MDPI have crystal engineering as one of their main sections, reflecting the importance of this subject in modern structural chemistry.\n\n\n"}
{"id": "3330825", "url": "https://en.wikipedia.org/wiki?curid=3330825", "title": "Debye–Waller factor", "text": "Debye–Waller factor\n\nThe Debye–Waller factor (DWF), named after Peter Debye and Ivar Waller, is used in condensed matter physics to describe the attenuation of x-ray scattering or coherent neutron scattering caused by thermal motion. It has also been called the B factor or the temperature factor. Often, \"Debye-Waller factor\" is used as a generic term that comprises the Lamb-Mössbauer factor of incoherent neutron scattering and Mössbauer spectroscopy.\n\nThe DWF depends on the scattering vector q. For a given q, DWF(q) gives the fraction of elastic scattering; 1 - DWF(q) correspondingly gives the fraction of inelastic scattering. (Strictly speaking, this probability interpretation is not true in general.) In diffraction studies, only the elastic scattering is useful; in crystals, it gives rise to distinct Bragg reflection peaks. Inelastic scattering events are undesirable as they cause a diffuse background — unless the energies of scattered particles are analysed, in which case they carry valuable information (for instance in inelastic neutron scattering or electron energy loss spectroscopy).\n\nThe basic expression for the DWF is given by\n\nwhere u is the displacement of a scattering center,\nand formula_2 denotes either thermal or time averaging.\nAssuming harmonicity of the scattering centers in the material under study, the Boltzmann distribution implies that formula_3 is normally distributed with zero mean. Then, using for example the expression of the corresponding characteristic function, the DWF takes the form\n\nNote that although the above reasoning is classical, the same holds in quantum mechanics.\n\nAssuming also isotropy of the harmonic potential, one may write\n\nwhere \"q\", \"u\" are the magnitudes (or absolute values) of the vectors q, u respectively, and formula_6 is the mean squared displacement. In crystallographic publications, values of formula_7 are often given where formula_8. Note that if the incident wave has wavelength formula_9, and it is elastically scattered by an angle of formula_10, then\n\nIn the context of protein structures, the term B-factor is used. The B-factor is defined as\n\nIt is measured in units of Å.\nThe B-factors can be taken as indicating the relative vibrational motion of different parts of the structure. Atoms with low B-factors belong to a part of the structure that is well ordered. Atoms with large B-factors generally belong to part of the structure that is very flexible. Each ATOM record (PDB file format) of a crystal structure deposited with the Protein Data Bank contains a B-factor for that atom.\n"}
{"id": "14017274", "url": "https://en.wikipedia.org/wiki?curid=14017274", "title": "Ecotoxicity", "text": "Ecotoxicity\n\nEcotoxicity, the subject of study of the field of ecotoxicology (a portmanteau of ecology and toxicology), refers to the potential for biological, chemical or physical stressors to affect ecosystems. Such stressors might occur in the natural environment at densities, concentrations or levels high enough to disrupt the natural biochemistry, physiology, behavior and interactions of the living organisms that comprise the ecosystem.\n\nEcotoxicology has been defined as, \"the branch of toxicology concerned with the study of toxic effects, caused by natural or synthetic pollutants, to the constituents of ecosystems, animal (including human), vegetable and microbial, in an integral context\".\n\n\nIn Canada, there is no law requiring manufactures to state the health and environmental hazards associated with their cleaning products. Many people buy such products to support a clean and healthy home, often unaware of the products ability to harm both their own health and the surrounding environment. \"Canadians spend more than $275 million on household cleaning products in a year\" Chemicals from these cleaners enter our bodies through air passageways and absorption through the skin and when these cleaning products are washed down the drain they negatively affect aquatic ecosystems. There are also no regulations in place stating that the ingredients be listed on labels of cleaning products leading the users to be ultimately unaware of the chemicals they expose themselves and their surrounding environments to.\n\nThe organic compound 2-Butoxyethanol, commonly found in glass cleaners, laundry stain removers, windshield wiper fluid, oven cleaners, and rust removers has been proven to cause reproductive problems in laboratory experiments.\n\nThe compound ammonia is found in many window cleaners, drain cleaners, bathroom cleaners, oven cleaners, car polish, and all-purpose cleaners. Its vapour is particularly harmful to people with asthma and it may also cause kidney and liver damage. When ammonia is mixed with products containing chlorine bleach, highly poisonous chloramine gas is formed. This poisonous gas often forms when cleaners are mixed in the home forming strong irritants.\n\nFragrance chemicals are found in most cleaning products, perfumes, and personal care products. More than 3000 chemicals are used in these fragrance mixtures. The synthetic musks used in detergents accumulate in the environment and are harmful to aquatic organisms. Certain musks are possible endocrine disruptors that interfere with hormone functioning. Phthalates are a common ingredient in these fragrance mixtures found in laundry detergents and fabric softeners. These phthalates are suspected endocrine disrupters that affect reproduction rates including reduced sperm count in males. Certain glass cleaners and floor polishes contain dibutyl phthalate (DBP). The European Union classifies DBP as very toxic to aquatic organisms, posing a huge danger as these cleaners, especially the floor polishes are often rinsed down the drain and into aquatic environments.\n\nPhosphates are found in many dishwasher detergents, laundry detergents, and bathroom cleaners. They act as a fertilizer in water and in high concentrations can promote algae blooms and increase weed growth. When water containing phosphates are washed into water areas they carry with them fertilizers, nutrients, and wastes from the land. Phytoplankton and algae flourish at the surface due to increased phosphates. Dead phytoplankton and other organisms sink to the bottom giving rise to large numbers of decomposers due to increased food supply (dead organisms, phytoplankton). Due to the increased number of decomposers that use more oxygen, fish and shrimp at the lower layers of the ocean become oxygen-starved and hypoxic zones become apparent.\n\nQuats are anti-microbial agents that are found in bathroom cleaners, fabric softeners, and degreasers. They are a class of irritants and sensitizers that negatively affect people who suffer from asthma. Chemicals in this class are persistent in aquatic environments, and toxic to the organisms that live in these environments. Many researchers are concerned that their widespread use in everyday household disinfectants and cosmetics are contributing to antibiotic resistant bacteria, thus limiting microbial infection treatment options.\n\nTrisodium nitrilotriacetate is found in bathroom cleaners and possibly some laundry detergents although more actively used in industrial formulations. Small amounts add up in the environment and add to an overall toxic issue. In aquatic ecosystems these chemicals cause heavy metals in sediment to redisolve and many of these metals are toxic to fish and other wildlife.\n\nPhthalates and BPA date back to the 1920s and 1930s. Phthalates have been applied as polyvinyl chloride (PVC) additives since 1926, but were also used for health care purposes as insect repellents and cercaricides. BPA is present in most aquatic environments, entering water systems through landfills and sewage treatment plant runoff allowing for bioaccumulation in aquatic organisms. These endocrine disrupters are a large group of chemicals that enter into the aquatic environment through the manufacturing of various industrial and consumer products, agriculture and food/drug processing, waste water treatment plants and human wastes. Phthalate esters are common additives that soften and make PVC more flexible. It is used in many everyday items such as medical devices, packaging for fragrances and cosmetics, ropes and varnishes, in plastic used to wrap food, and shower curtains. These Phthalate esters have been found in areas of water, air, sediment, and in gulfs and rivers around the world, Giam et al. as cited by. Phalates and BPA affect reproduction in animal groups such as Molluscs, crustaceans, amphibians and fish. Most of these plasticizers affect hormone systems, and some phthalates have even larger pathways of disruption. Phthalates and BPA have been proven to affect development and reproduction in a variety of species. Disturbances include changes in the number of offspring produced and reduced hatching success. In amphibians for example, phthalates and BPA disrupt thyroid functioning which in turn impacts larval development. Molluscs, crustaceans and amphibians appear to be more responsive than fish, with most effects being induced in low concentration ranges with the exception of disrupted spermatogenesis in fish in the low range. \nA Phthalate referred to as diethyl phthalate (DEP) enters the aquatic environment through industries that manufacture cosmetics, plastics and many commercial products that pose hazards to aquatic organisms and human health. Through exposing an adult male common Carp (Cyprinus carpio) to LC50 doses it was evident that a bioaccumulation of DEP in testis, liver, brain, gills and muscle tissue was present. Fish exposed to 20 ppm of DEP became drowsy and discolored during the onset of the fourth week. \nSources of DEP contamination and accumulation in humans include cosmetic products and dietary meat of fish, Persky et al. This DEP acts as a cosmetic ingredient and vehicle for fragrances, both which come in contact with the skin. \nMany countries around the world including India practice sewage fed fisheries where waste waters are used for the purpose of culturing fish. Endocrine disruption and a presence of phthalate residue is highly likely to be observable in these sewage fed fish. This is the case as waste water from various industries and garbage containing DEP are released into these waters. Through a DEP treatment with Cyprinus carpio, liver size was observed to increase and testis size decreased. In fish, muscle ALT and AST activities decreased as it was effected by DEP treatment. Like many toxic chemicals DEP has been known to affect metabolic enzyme profiles and activities of phosphatases and transaminases, Ghorpade et al. as cited by. A decrease in immunity of M. rosenbergii after exposure to DEP was also noted. \nGiven that biological effect concentrations for plasticizers used in the laboratory coincide with concentrations present in the environment, it seems that some wildlife species must be negatively impacted.\n\nPersonal care products can reach the environment through drainage from waste water treatment plants and digested sludge. Recently, the anti-dandruff and antimycotic, Climbazole, was detected in waste water treatment drainages. Climbazole is readily used in cosmetics, and is an ingredient in anti-dandruff shampoos. Shampoos contain formulations of up to 2.0% which is the equivalent of approximately 15g/L. Climbazole is classified as very toxic to aquatic organisms. It affects the growth of green algae Pseudokirchneriella subcapitata at very low concentrations. Zebrafish experienced lethal effects after exposure to climbazole in laboratory testing. Effects included thickening of fertilized eggs, lack of somite formation, lack of detachment of the tail bud from the yolk sac, and lack of a heartbeat were evaluated after 48 hours. Along with Danio rerio, Lemna minor, Navicula pelliculosa, Pseudokirchneriella subcapitata, and Daphnia magna were tested and all were found to be negatively affected by climbazole in a concentration-dependant manner with highest toxicity observed in L. minor. Effects included stunted colony growth and darkening in color. Effects of climbazole on oats and turnip included retarded, stunted growth of the leaves and shoot as well as turning darker in color. The aquatic ecotoxicity of climbazole can be classified as very toxic to Lemna and algae, toxic to fish and harmful to Daphnia.\n\nPesticides often pose serious problems as they kill not only targeted organisms but also non-targeted organisms in the process. They are released into the natural environment intentionally by people who are often unaware that the chemicals will travel further than anticipated, Hatakeyama et al. as cited in. Thus the pesticides largely affect the natural communities in which they are used. They negatively effect multiple levels ranging from molecules to tisues to organs to individuals to populations and onto communities. In the natural environment, a combination of pesticide exposure and natural stressors such as fluctuating temperature, food shortage or decreased oxygen availability are worse than when presented alone. Pesticides can affect the feeding rates of zoo-plankton. In the presence of pesticides zoo-plankton display lower feeding rates which result in reduced growth and reproduction. Swimming may also be affected by pesticides which poses a life-threatening issue for zoo-plankton as they swim to obtain food and nutrients and avoid predators. Such changes may alter predator-prey relationships. A spinning behavior became apparent in Daphnia induced by carbaryl which increased the probability of the Daphnia being eaten by other fish, Dodson et al. as cited by. The toxicant pentachlorophenol increases swimming speed in the rotifer Brachionus calyciflorus, in turn increasing the encounter rate of the prey with their predators, Preston et al. as cited by.\n\nOne of the major environmental impacts of oil exploration on the environment is the contamination of aquatic ecosystems from oil spills and oil seepages from pits. Oftentimes, as is the case in the Ecuadorian Amazon, oil is used to control dust on roadways, causing the precipitation runoff from these roads to also be contaminated. Direct human health hazards occur since many people, including children walk barefoot on these oiled roads putting them in direct contact with the crude oil. Other hazards to humans include seepages into ponds that provide drinking water for the population. \nDuring the exploration for oil, mud that has been drilled is deposited into pits. These production pits are often not lined risking the possibility for contaminants to leak into the surrounding environment. Environmental concerns are primarily focused on a group of polycyclic aromatic hydrocarbons (PAHs). “PAHs accumulate on particles and sediments, which tend to protect them from biodegrading processes\", Green and Trett as cited in. During the exploration for oil, mud that has been drilled is deposited into pits. These production pits are often not lined risking the possibility for contaminants to leak into the surrounding environment. \nSamples were collected from four sites (13 stations) in the Ecuadorian Amazon where crude oil was the main pollutant. The water collected from Site B, a drinking water pond located 100m from an in use pit, had the highest total petroleum hydrocarbon (THP) concentration. Sediments were found to be acutely phototoxic. This area which has poorly developed infrastructure is one where residents collect water for drinking, cooking and bathing from the rivers and ponds nearby. “A recent study observed excess cancer rates in a village in this region” Sebastian et. Al, as cited in. Not only were excess cancer rates apparent but many people in this area that were consuming the water for drinking purposes became ill.\nIn Wernersson’s study, toxicity of water and sediment samples were studied on Daphnia magna (a crustacean zoo-plankton species) and Hyalella azteca (an amphipod). These samples were collected from four sites where crude oil was the main source of pollution. 1-4 day old organisms of both species were used in the tests. Immobility of D. magna was recorded after 24 hours of exposure indoors. They were then moved outdoors where they were exposed to sunlight. After 1–2 hours the samples were removed from the sunlight and it was found that D. Magna often recovered within an hour after UV exposure.\nHyalella azteca was cultured in the same medium as was used for the D. Magna species. To minimize stress shade was provided. 16 hours of light and 8 hours of darkness were provided. Lethality was recorded after 96 hours of exposure.\n\n\n"}
{"id": "45260885", "url": "https://en.wikipedia.org/wiki?curid=45260885", "title": "Energy consulting", "text": "Energy consulting\n\nEnergy consulting is a sub-discipline of environmental consulting that focuses on optimizing a business' energy usage, as well as the sources from which the actual energy is derived. Energy consulting is often centered on reducing operational costs, though this is not always the main goal. With the increasing importance of corporate social responsibility (CSR) in the minds of the general public, enterprise level businesses may contract an energy consultant to include more environmentally friendly energy sources into their energy mix.\n\nWith the recent volatility in the energy industry, energy consulting is quickly becoming a main component of business operations for enterprises worldwide.\n\nAs a result of the volatility in the modern energy industry, many enterprise level businesses are turning to financial analysts, specifically through the use of derivatives and hedging to minimize exposure to the unstable energy industry.\n\n"}
{"id": "35294926", "url": "https://en.wikipedia.org/wiki?curid=35294926", "title": "Gabardan Solar Park", "text": "Gabardan Solar Park\n\nThe Gabardan Solar Park is a 67.5 megawatt (MW) photovoltaic power station in France. It has about 872,300 thin-film PV panels made by First Solar, and incorporates a 2 MW pilot plant using 11,100 solar trackers.\n\n"}
{"id": "54956865", "url": "https://en.wikipedia.org/wiki?curid=54956865", "title": "Gjika Amplification", "text": "Gjika Amplification\n\nGjika Amplification was founded in Boston, Massachusetts in the early 1980s, by Robert (Bob) Gjika. Gjika amplifiers are original designs, and are completely hand-wired.\n\nRobert Gjika was born in Winthrop, Massachusetts, in 1960. At a young age began building his own guitars and amplifiers. His father was an electrician, giving him an early start in electronics. By 1979, he had established himself in the local Boston area as a luthier, repairing and building guitars and amplifiers for E.U. Wurlitzer Music and Sound. After a year+ working there, he opened up his own shop in Boston, where he was soon in-demand for all things related to guitars and amplifiers. Some of the musicians and bands he did work for included popular Boston bands at the time, guitarists such as Reeves Gabrels, guitar wizard Joe Stump, and other guitarists from the Berklee College of Music scene.\n\nGjika moved his business from Boston to Memphis, Tennessee in 1987, where he met the late Shawn Lane, and introduced Shawn to his guitars and single-ended amp designs. In 1989 he relocated to Los Angeles, California, to focus on producing the \"Gold\" amps, with a new business partner, Bill Walker, under the brand name \"Gjika-Walker\". In 1990, after the business partnership was dissolved, Gjika left Los Angeles to go to Austin, Texas, where the music scene was flourishing, and at that time, transitioned from his Gold Amps to his 3-Channel KT90 amplifier. In 1996 he again relocated, to Nashville, Tennessee. During these years he continued his amp designing and building, on a smaller scale, until in 2000 he went back to California, where he currently resides and builds his amplifiers in Escondido.\n\nIn Memphis, Gjika's Quality Guitars, on Union Avenue, was featured in a local music publication, in which Tim O'Shea said of Gjika's skills: \"His technical expertise is most impressive, especially his \"diagnosis to repair\" knowledge of another rock necessity, the electric PA and amp... Gjika also builds and sells his custom, \"Hot Box\" amplifiers, perhaps the finest available anywhere.\" Of Bob's custom guitar designs: \"Gjika builds three or four of his custom 'Wild Thing' guitars each year, at a cool five grand apiece. The prototype guitars are truly wonders to behold at any stage of construction.\"\n\nFocusing on his own playing, as well as designing and building guitars and amps, Gjika acquired a reputation for being reclusive, resulting in many believing that he had retired from amp building. It was noted by Dave Hunter in his reverb.com article, \"Dave's Corner: Guide to Boutique Amps\", in which he wrote \"This enigmatic amp maker and all-round tech extraordinaire goes to few (if any) lengths to promote his own work.\" As to the quality of the guitar tone produced by Bob's original design amps, he assessed that \"Characteristics of Gjika amps include their thick, rich, harmonically saturated tone and fast playing response.\" Further, \"Bob Gjika has worked with Eric Johnson, but might be best known for the scorching single-ended EL34-based amp that the late and largely underappreciated fusion virtuoso Shawn Lane used on his \"Powers of Ten\"...a massive beast with four EL34s (or eight in its stereo configuration) and possibly the largest transformers I’ve seen on a guitar amp, giving it earth-shaking body and tremendous gain.\"\n\nIn an interview by \"Guitar Player\" magazine, Shawn Lane referred to his use of his Gjika Gold amp on his album \"Powers of Ten\" in glowing terms: \"It's class-A power, all-tube, and just one of the most amazing amplifiers I've ever heard. It has a harmonic richness that really responds to a certain kind of touch.\"\n\n\"Premier Guitar\" covered the Premier Builder's Guild's NAMM booth featuring Gjika Amps, with highly positive reception.\n\n"}
{"id": "100296", "url": "https://en.wikipedia.org/wiki?curid=100296", "title": "Great White Fleet", "text": "Great White Fleet\n\nThe Great White Fleet was the popular nickname for the powerful United States Navy battle fleet that completed a journey around the globe from 16 December 1907, to 22 February 1909, by order of United States President Theodore Roosevelt. Its mission was to make friendly courtesy visits to numerous countries, while displaying new U.S. naval power to the world.\n\nIt consisted of 16 battleships divided into two squadrons, along with various escorts. Roosevelt sought to demonstrate growing American military power and blue-water navy capability. Hoping to enforce treaties and protect overseas holdings, the United States Congress appropriated funds to build American naval power. Beginning in the 1880s with just 90 small ships, over one-third of them wooden and therefore obsolete, the navy quickly grew to include new modern steel fighting vessels. The hulls of these ships were painted a stark white, giving the armada the nickname \"Great White Fleet\".\n\nIn the twilight of his administration, United States President Theodore Roosevelt dispatched 16 U.S. Navy battleships of the Atlantic Fleet on a worldwide voyage of circumnavigation from 16 December 1907 to 22 February 1909. The hulls were painted white, the Navy's peacetime color scheme, decorated with gilded scrollwork with a red, white, and blue banner on their bows. These ships would later come to be known as the Great White Fleet.\n\nThe purpose of the fleet deployment was multifaceted. Ostensibly, it served as a showpiece of American goodwill, as the fleet visited numerous countries and harbors. In this, the voyage was not unprecedented. Naval courtesy calls, many times in conjunction with the birthdays of various monarchs and other foreign celebrations, had become common in the 19th century. Port calls showcased pomp, ceremony, and militarism during a period of rising pre-war nationalism. In 1891, a large French fleet visited Kronstadt, Russia, in conjunction with negotiations between the two nations. Although France and Russia had been hostile to each other for at least three decades prior, the significance of the call was not lost on Russia, and Tsar Nicholas II signed a treaty of alliance with France in 1894. As navies grew larger, naval pageants grew longer, more elaborate, and more frequent. The United States began participating in these events in 1902 when Roosevelt invited Kaiser Wilhelm II of Germany to send a squadron for a courtesy call to New York City. Invitations for U.S. Navy ships to participate in fleet celebrations in the United Kingdom, France, and Germany followed.\n\nAdditionally, the voyage of the Great White Fleet demonstrated both at home and on the world stage that the U.S. had become a major sea power in the years after its triumph in the Spanish–American War, with possessions that included Guam, the Philippines, and Puerto Rico. It was not the first flexing of U.S. naval muscle since that war, however; during the Algeciras Conference in 1906, which was convened to settle a diplomatic crisis between France and Germany over the fate of Morocco, Roosevelt had ordered eight battleships to maintain a presence in the Mediterranean Sea. Since Japan had arisen as a major sea power with the 1905 annihilation of the Russian fleet at Tsushima, the deployment of the Great White Fleet was therefore intended, at least in part, to send a message to Tokyo that the American fleet could be deployed anywhere, even from its Atlantic ports, and would be able to defend American interests in the Philippines and the Pacific.\n\nThat gesture capitalized on diplomatic trouble that had resulted from anti-Japanese riots in San Francisco. Those problems had been resolved by the Gentlemen's Agreement of 1907 and the fleet visit was a friendly gesture to Japan. The Japanese welcomed it. Roosevelt saw the deployment as one that would encourage patriotism, and give the impression that he would teach Japan \"a lesson in polite behavior\", as historian Robert A. Hart phrased it. After the fleet had crossed the Pacific, Japanese statesmen realized that the balance of power in the East had changed since the Root–Takahira Agreement that defined relevant spheres of interest of the United States and Japan.\n\nThe voyage also provided an opportunity to improve the sea- and battle-worthiness of the fleet. While earlier capital ship classes such as the , and were designed primarily for coastal defense, later classes such as the and incorporated lessons learned from the Spanish–American War and were conceived as ships with \"the highest practicable speed and the greatest radius of action\", in the words of the appropriation bills approved by the United States Congress for their construction. They were intended as modern warships capable of long-range operations. Nevertheless, the experience gained in the recent war with Spain had been limited.\n\nRoosevelt's stated intent was to give the navy practice in navigation, communication, coal consumption and fleet maneuvering; navy professionals maintained, however, that such matters could be served better in home waters. In light of what had happened to the Russian Baltic Fleet, they were concerned about sending their own fleet on a long deployment, especially since part of the intent was to impress a modern, battle-tested navy that had not known defeat. The fleet was untested in making such a voyage, and Tsushima had proven that extended deployments had no place in practical strategy. The Japanese Navy was close to coaling and repair facilities; while American ships could coal in the Philippines, docking facilities were far from optimal. An extended stop on the West Coast of the United States during the voyage for overhaul and refurbishment in dry dock would be a necessity. Planning for the voyage, however, showed a dearth of adequate facilities there, as well. The main sea channel of the Mare Island Navy Yard near San Francisco was too shallow for battleships, which left only the Puget Sound Navy Yard in Bremerton, Washington, for refit and repair. The Hunter's Point civilian yard in San Francisco could accommodate capital ships, but had been closed due to lack of use and was slated for demolition. President Roosevelt ordered that Hunter's Point be reopened, facilities be brought up to date, and the fleet to report there.\n\nAlso, the question of adequate resources for coaling existed. This was not an issue when the Atlantic Fleet cruised the Atlantic or Caribbean, as fuel supplies were readily available. However, the United States did not enjoy a worldwide network of coaling stations like that of Great Britain, nor did it have an adequate supply of auxiliary vessels for resupply. During the Spanish–American War, this lack had forced Admiral George Dewey to buy a collier-load of British coal in Hong Kong before the Battle of Manila Bay to ensure his squadron would not run out of steam at sea. The need had been even more pressing for the Russian Baltic Fleet during its long deployment during the Russo-Japanese War, not just for the distance it was to steam, but also because, as a belligerent nation in wartime, most neutral ports were closed to it due to international law. While the lack of support vessels was pointed out and a vigorous program of building such ships suggested by Rear Admiral George W. Melville, who had served as chief of the Bureau of Equipment, his words were not heeded adequately until World War II.\n\nFederal regulations that restricted supply vessels for Navy ships to those flying the United States flag, complicated by the lack of an adequate United States Merchant Marine, proved another obstacle. Roosevelt initially offered to award Navy supply contracts to American skippers whose bids exceeded those of foreign captains by less than 50 percent. Many carriers declined this offer because they could not obtain enough cargo to cover the cost of the return trip. Two months before the fleet sailed, Roosevelt ordered the Navy Department to contract 38 ships to supply the fleet with the 125,000 tons of coal it would need to steam from Hampton Roads, Virginia, to San Francisco. Only eight of these were American-registered; most of the other 30 were of British registry. This development was potentially awkward, since part of the mission was to impress Japan with the perception of overwhelming American naval power. Britain had become a military ally of Japan in 1905 with the Anglo-Japanese Alliance, which obliged it to aid Japan should a foreign power declare war against it. Technically, the list of potential combatants included the United States. The British government decided to play both sides of the political fence with the intent of moderating any Japanese-American friction that might arise.\n\nAs the Panama Canal was not yet complete, the fleet had to pass through the Straits of Magellan. The scope of such an operation was unprecedented in U.S. history, as ships had to sail from all points of the compass to rendezvous points and proceed according to a carefully orchestrated, well-conceived plan. It involved almost the entire operational capability of the U.S. Navy. Unlike the formidable obstacles that had faced the Russian fleet on its voyage from the Baltic to the Pacific, which eventually led to its destruction by the Japanese in 1905, the U.S. effort benefited from a peaceful environment which aided the coordination of ship movements. \n\nIn port after port, citizens in the thousands turned out to see and greet the fleet. In 1908, the Great White Fleet visited Monterey, California, from 1–4 May. The nearby Hotel Del Monte in Del Monte, California, hosted a grand ball for the officers of the fleet. \n\nIn Australia, the arrival of the Great White Fleet on 20 August 1908 was used to encourage support for the forming of Australia's own navy. When the fleet sailed into Yokohama, the Japanese went to extraordinary lengths to show that their country desired peace with the U.S.; thousands of Japanese schoolchildren waved American flags to greet navy officials as they came ashore. In Sicily, the sailors helped in recovery operations after the 1908 Messina earthquake.\n\nIn February 1909, Roosevelt was in Hampton Roads, Virginia, to witness the triumphant return of the fleet from its long voyage, and what he saw as a fitting finish for his administration. To the officers and men of the fleet, Roosevelt said, \"Other nations may do what you have done, but they'll have to follow you.\" This parting act of grand strategy by Roosevelt greatly expanded foreign respect for the United States, as well as its role in the international arena.\n\nThe fourteen-month-long voyage was a grand pageant of American naval power. The squadrons were manned by 14,000 sailors. They covered some and made twenty port calls on six continents. The fleet was impressive, especially as a demonstration of American industrial prowess (all eighteen ships had been constructed since the Spanish–American War), but already the battleships represented the suddenly outdated 'pre-dreadnought' type of capital ship, as the first battleships of the revolutionary had just entered service, and the U.S. Navy's first dreadnought, , was already fitting out. The two oldest ships in the fleet, and , were already obsolete and unfit for battle; two others, and , had to be detached at San Francisco because of mechanical troubles and were replaced by the and the . (After repairs, \"Alabama\" and \"Maine\" completed their \"own, more direct, circumnavigation of the globe\" via Honolulu, Guam, Manila, Singapore, Colombo, Suez, Naples, Gibraltar, the Azores, and finally back to the United States, arriving on 20 October 1908, four months before the remainder of the fleet, which had taken a more circuitous route.)\n\nThe battleships were accompanied during the first leg of their voyage by a \"Torpedo Flotilla\" of six early destroyers, as well as by several auxiliary ships. The destroyers and their tender did not actually steam in company with the battleships, but followed their own itinerary from Hampton Roads, Virginia to San Francisco, California. Also of note is that the armored cruiser preceded the Fleet itinerary for its first and second legs by about a month, perhaps making arrangements to later receive the Fleet.\n\nWith as flagship under the command of Rear Admiral Robley D. Evans, the fleet sailed from Hampton Roads on 16 December 1907 for Trinidad, British West Indies, thence to Rio de Janeiro, Brazil; Punta Arenas, Chile; Callao, Peru; Magdalena Bay, Mexico, and up the West Coast, arriving at San Francisco, 6 May 1908.\n\nAt San Francisco, Rear Admiral Charles S. Sperry assumed command of the fleet, owing to the poor health of Admiral Evans. Also at San Francisco, the squadrons were slightly rearranged, bringing the newest and best ships in the fleet up to the First Squadron. was detached and later became the supply ship of the Pacific Fleet. At this time also, , under Captain Reginald F. Nicholson, and , under Captain Frank E. Beatty, were substituted for and . In San Francisco, was brought forward into First Squadron, First Division and took her place as flagship, Second Squadron.\n\nLeaving that port on 7 July 1908 the U.S. Atlantic Fleet visited Honolulu; Auckland, New Zealand; Sydney, Melbourne, and Albany, Australia; Manila, Philippines; Yokohama, Japan; and Colombo, Ceylon; then arriving at Suez, Egypt, on 3 January 1909.\n\nWhile the fleet was in Egypt, word was received of an earthquake in Sicily, thus affording an opportunity for the United States to show its friendship to Italy by offering aid to the sufferers. \"Connecticut\", , , and were dispatched to Messina, Italy, at once. The crew of \"Illinois\" recovered the bodies of the American consul, Arthur S. Cheney, and his wife, entombed in the ruins.\n\n, the fleet's station ship at Constantinople, and , a refrigerator ship fitted out in New York, were hurried to Messina, relieving \"Connecticut\" and \"Illinois\", so that they could continue on the cruise.\n\nLeaving Messina on 9 January 1909, the fleet stopped at Naples, Italy, thence to Gibraltar, arriving at Hampton Roads on 22 February 1909. There, President Roosevelt reviewed the fleet as it passed into the roadstead.\n\nFrom Hampton Roads to San Francisco, .\n\nThe Fleet, First Squadron and First Division, were commanded by Rear Admiral Robley D. Evans.\nFirst Division consisted of four ships of the 1906 :\n, the fleet's flagship,\nCaptain Hugo Osterhaus;\nCaptain Charles E. Vreeland;\nCaptain William P. Potter; and\nCaptain Richard Wainwright.\n\nSecond Division was commanded by Rear Admiral William H. Emory.\nSecond Division consisted of four ships of the 1904 :\n, the division flagship,\nCaptain Henry McCrea;\nCaptain William H. H. Southerland;\nCaptain Joseph B. Murdock; and\nCaptain Seaton Schroeder.\n\nSecond Squadron and Third Division were commanded by Rear Admiral Charles M. Thomas.\nThird Division consisted of one \"Connecticut\"-class ship and the three ships of the 1902 :\n, the squadron flagship,\nCaptain John Hubbard;\nCaptain Giles B. Harber;\nCaptain Greenlief A. Merriam; and\nCaptain Charles W. Bartlett.\n\nFourth Division was commanded by Rear Admiral Charles S. Sperry.\nFourth Division consisted of two ships of the 1901 and the two 1900 ships:\n, the division flagship,\nCaptain Ten Eyck De Witt Veeder;\nCaptain John M. Bowyer,\nCaptain Hamilton Hutchins; and\nCaptain Walter C. Cowles.\n\nThe fleet auxiliaries consisted of\nLieutenant Commander John B. Patton;\nCommander William S. Hogg;\nCommander Valentine S. Nelson;\nLieutenant Walter R. Gherardi; and\n\nThe \"Torpedo Flotilla\" of destroyers consisted of\nLieutenant Alfred G. Howe;\nLieutenant Julius F. Hellweg;\nLieutenant Frank McCommon;\nLieutenant Charles S. Kerrick;\nLieutenant Ernest Friedrick;\nLieutenant Hutch I. Cone; and\nCommander Albert W. Grant.\n\nThe second leg of the voyage was from San Francisco to Puget Sound and back. On 23 May 1908 the 16-battleships of the Great White Fleet steamed into the Puget Sound where they separated to visit six Washington state ports: Bellingham, Bremerton, Port Angeles, Port Townsend, Seattle and Tacoma. The fleet arrived in Seattle on 23 May and departed 27 May 1908.\n\nThe Fleet, First Squadron, and First Division were commanded by Rear Admiral Charles S. Sperry.\nFirst Division consisted of\n, the Fleet's flagship,\nCaptain Hugo Osterhaus;\nCaptain Charles E. Vreeland;\nCaptain John Hubbard;\nand ,\nCaptain William P. Potter.\n\nSecond Division was commanded by Rear Admiral Richard Wainwright.\nSecond Division consisted of\n, the Division flagship,\nCaptain Edward F. Qualtrough;\nCaptain Reginald F. Nicholson, replacing her sister \"Virginia\";\nCaptain William H.H. Southerland; and\nCaptain Joseph B. Murdock.\n\nSecond Squadron and Third Division were commanded by Rear Admiral William H. Emory.\nThird Division consisted of\n, the Squadron's flagship,\nCaptain Kossuth Niles;\nCaptain Alexander Sharp;\nCaptain Robert M. Doyle; and\nCaptain Thomas B. Howard.\n\nFourth Division was commanded by Rear Admiral Seaton Schroeder.\nFourth Division consisted of\n, the Division flagship,\nCaptain Frank E. Beatty, which replaced her sister \"Alabama\";\nCaptain John M. Bowyer;\nCaptain Hamilton Hutchins; and\nCaptain Walter C. Cowles.\n\nThe Fleet Auxiliaries were\nLieutenant Commander John B. Patton;\nLieutenant Commander Charles B. McVay;\nCommander William S. Hogg;\nSurgeon Charles F. Stokes; and\nCommander Valentine S. Nelson.\n\nFrom San Francisco to Manila, .\nThe Fleet, First Squadron, and First Division were commanded by Rear Admiral Charles S. Sperry.\nFirst Division consisted of\n, the Fleet's flagship,\nCaptain Hugo Osterhaus;\nCaptain Charles E. Vreeland;\nCaptain John Hubbard; and\nCaptain William P. Potter.\n\nSecond Division consisted of\n, the Division flagship,\nCaptain Edward F. Qualtrough;\nCaptain Reginald F. Nicholson;\nCaptain William H.H. Southerland; and\nCaptain Joseph B. Murdock.\n\nThe Second Squadron and Third Division were commanded by Rear Admiral William H. Emory.\nThird Division consisted of\n, the Squadron flagship,\nCaptain Kossuth Niles;\nCaptain Alexander Sharp;\nCaptain Robert M. Doyle; and\nCaptain Thomas B. Howard.\n\nFourth Division was commanded by Rear Admiral Seaton Schroeder.\nFourth Division consisted of\n, the Division flagship,\nCaptain Frank E. Beatty;\nCaptain John M. Bowyer;\nCaptain Hamilton Hutchins; and\nCaptain Walter C. Cowles.\n\nThe Fleet Auxiliaries were\nLieutenant Commander John B. Patton;\nLieutenant Commander Charles B. McVay;\nCommander William S. Hogg;\nSurgeon Charles F. Stokes; and\nCommander Valentine S. Nelson.\n\nThe final leg ran from Manila to Hampton Roads, .\n\nThe cruise of the Great White Fleet provided practical experience for US naval personnel in sea duty and ship handling. It also showed the viability of US warships for long-range operations as no major mechanical mishaps occurred. However, while the cruise uncovered design flaws, it did not test the abilities to engage in battle fleet action. In fact, the success of the deployment might have helped obscure design deficiencies that were not addressed until World War I. These included excessive draft, low armor belts, large turret openings and exposed ammunition hoists.\n\nWhile the capital ships of the Great White Fleet were already obsolescent in light of the \"big gun\" revolution ushered in by the construction of , their behavior at sea furnished valuable information that affected future construction. For instance, in terms of seaworthiness, all the capital ships in the fleet proved wet in all but the calmest seas, which led to the flared bows of subsequent U.S. battleships, increased freeboard forward and such spray-reducing measures as the elimination of billboards for anchors and gun sponsons. Increased freeboard was needed; this and related considerations demanded increases in beam and overall size. Between the s, the last American capital ships completed before data from the cruise became available, and the , the first designed after this data was received, displacement (and, as a result, cost) per ship increased by one third.\n\nDeficiencies in seaworthiness in turn reduced the battle-worthiness of the fleet. Turret heights for main armament proved too low and needed to be raised. Secondary armament was useless at speed and especially in tradewind conditions (with the wind moving over the sea at or greater) and needed to be moved much higher in the hull. Improved placement began with the \"Wyoming\"-class battleships and was further refined in the . Casemates for the bow 3-inch guns in the newer pre-dreadnoughts were untenable due to wetness and were removed. Another discovery was that, even when fully loaded, the bottom of the battleships' side armor was visible—and the ships thus vulnerable to shells that might hit beneath it to reach their machinery and magazines—in smooth to moderate seas. The profile of crests and troughs in some ships contributed to this problem. Admiral Evans concluded that the standard width of belt armor was inadequate.\n\nOne other necessity the cruise outlined was the need for tactical homogeneity. Before the cruise, critics such as then-Captain William Sims (to whom President Roosevelt listened) had argued that American warship design had remained too conservative and precluded the level of efficiency needed for the fleet to function as an effective unit. The cruise proved the charge true. This would eventually lead to the building of standard type battleships in the U.S. Navy. When President Roosevelt convened the 1908 Newport Conference of the Naval War College, he placed responsibility for U.S. battleship design on the General Board of the United States Navy. This gave line officers and planners direct input and control over warship design, a pattern which has persisted to the present day.\n\nExperience gained by the cruise led to improvements in formation steaming, coal economy and morale. Gunnery exercises doubled the fleet's accuracy. However, the mission also underlined the fleet's dependence on foreign colliers and the need for coaling stations and auxiliary ships for coaling and resupply.\n\n\nNotes\nCitations\n\n"}
{"id": "1530406", "url": "https://en.wikipedia.org/wiki?curid=1530406", "title": "Gulf War oil spill", "text": "Gulf War oil spill\n\nThe Gulf War oil spill was one of the largest oil spills in history, resulting from the Gulf War in 1991. The apparent strategic goal was to foil a potential landing by US Marines. It also made commandeering oil reserves dangerous for US forces as visibility and movement were inhibited. \nThe immediate reports from Baghdad said that American air strikes had caused a discharge of oil from two tankers. Coalition forces determined the main source of oil to be the Sea Island terminal in Kuwait. On January 26, three US F-117 fighter-bombers destroyed pipelines to prevent further spillage into the Persian Gulf. Several other sources of oil were found to be active: tankers and a damaged Kuwaiti oil refinery near Mina Al Ahmadi, tankers near Bubiyan Island, and Iraq's Mina Al Bakr terminal.\n\nEarly estimates on the volume spilled ranged around .. These numbers were however significantly adjusted downward by later, more detailed studies, both by government researchers (between and ), and by private researchers (between and ).\n\nThe slick reached a maximum size of by and was 5 inches (13 cm) thick in some areas. Despite the uncertainty surrounding the size of the spill, figures place it several times the size (by volume) of the Exxon Valdez oil spill.\n\nThe \"New York Times\" reported that a 1993 study sponsored by UNESCO, Bahrain, Iran, Iraq, Kuwait, Oman, Qatar, Saudi Arabia, the United Arab Emirates and the United States found the spill did \"little long-term damage\": About half the oil evaporated, were recovered and to washed ashore, mainly in Saudi Arabia.\n\nMore recent scientific studies have tended to disagree with this assessment. Marshlands and mud tidal flats continued to contain large quantities of oil, over nine years later, and full recovery is likely to take decades.\n\nDr. Jacqueline Michel, US geochemist (2010 interview – transcript of radio broadcast): The long term effects were very significant. There was no shoreline cleanup, essentially, over the 800 kilometers that the oil – - in Saudi Arabia. And so when we went back in to do quantitative survey in 2002 and 2003, there was a million cubic meters of oil sediment remained then 12 years after the spill... [T]he oil penetrated much more deeply into the intertidal sediment than normal because those sediments there have a lot of crab burrows, and the oil penetrated deep, sometimes 30, 40 centimeters, you know a couple of feet, into the mud of these tidal flats. There’s no way to get it out now. So it has had long term impact.\n\nDr. Hans-Jörg Barth, German geographer (2001 research report): The study demonstrated that, in contrary to previously published reports e.g. already 1993 by UNEP, several coastal areas even in 2001 still show significant oil impact and in some places no recovery at all. The salt marshes which occur at almost 50% of the coastline show the heaviest impact compared to the other ecosystem types after 10 years. Completely recovered are the rocky shores and mangroves. Sand beaches are on the best way to complete recovery. The main reason for the delayed recovery of the salt marshes is the absence of physical energy (wave action) and the mostly anaerobic milieu of the oiled substrates. The latter is mostly caused by cyanobacteria which forms impermeable mats. In other cases tar crusts are responsible. The availability of oxygen is the most important criteria for oil degradation. Where oil degrades it was obvious that benthic intertidal fauna such as crabs re-colonise the destroyed habitats long before the halophytes. The most important paths of regeneration are the tidal channels and the adjacent areas. Full recovery of the salt marshes will certainly need some centuries.\n\nThe \"Financial Times\", in reference to the April 2010 Deepwater Horizon oil spill in the Gulf of Mexico, cited the 1993 optimistic assessment of the Gulf War oil spill as evidence that \"Initial warnings of catastrophic environmental damage from oil spills can turn out to be overdone\".\n\n"}
{"id": "23120729", "url": "https://en.wikipedia.org/wiki?curid=23120729", "title": "Hueypoxtla", "text": "Hueypoxtla\n\nHueypoxtla or Villa de San Bartolomé Hueypoxtla is a town and municipal seat from Hueypoxtla Municipality in Mexico State, in Mexico. In 2010, this village had a total population of 3,989.\n\nOn December 4, 2013, cobalt-60 from a truck theft two days before 2 km away was recovered there, as well as the heavy truck itself; the decommissioned cobalt therapy machine had been \"en route\" to proper disposal. Federal police and military units established an armed cordon approximately 50 meters around the radiation source in the empty lot where it had been abandoned. Six people showing signs of possible radiation exposure from the orphan source were later detained. It is not known whether the thieves wanted the truck (which included a crane), the cobalt-60, or both.\n"}
{"id": "470323", "url": "https://en.wikipedia.org/wiki?curid=470323", "title": "Iron(II) oxide", "text": "Iron(II) oxide\n\nIron(II) oxide or ferrous oxide is the inorganic compound with the formula FeO. Its mineral form is known as wüstite. One of several iron oxides, it is a black-colored powder that is sometimes confused with rust, the latter of which consists of hydrated iron(III) oxide (ferric oxide). Iron(II) oxide also refers to a family of related non-stoichiometric compounds, which are typically iron deficient with compositions ranging from FeO to FeO.\n\nFeO can be prepared by the thermal decomposition of iron(II) oxalate. \nThe procedure is conducted under an inert atmosphere to avoid the formation of ferric oxide. A similar procedure can also be used for the synthesis of manganous oxide and stannous oxide.\n\nStoichiometric FeO can be prepared by heating FeO with metallic iron at 770 °C and 36 kbar.\n\nFeO is thermodynamically unstable below 575 °C, tending to disproportionate to metal and FeO:\n\nIron(II) oxide adopts the cubic, rock salt structure, where iron atoms are octahedrally coordinated by oxygen atoms and the oxygen atoms octahedrally coordinated by iron atoms. The non-stoichiometry occurs because of the ease of oxidation of Fe to Fe effectively replacing a small portion of Fe with two thirds their number of Fe, which take up tetrahedral positions in the close packed oxide lattice.\n\nBelow 200 K there is a minor change to the structure which changes the symmetry to rhombohedral and samples become antiferromagnetic.\n\nIron(II) oxide makes up approximately 9% of the Earth's mantle. Within the mantle, it may be electrically conductive, which is a possible explanation for perturbations in Earth's rotation not accounted for by accepted models of the mantle's properties.\n\nIron dissolved in groundwater is in the reduced iron II form. If this groundwater comes in contact with oxygen at the surface, e.g. in natural springs, iron II is oxidised to iron III and forms insoluble hydroxides in water.\n\nIron(II) oxide is used as a pigment. It is FDA-approved for use in cosmetics and it is used in some tattoo inks. It can also be used as a phosphate remover from home aquaria.\n\n"}
{"id": "21850851", "url": "https://en.wikipedia.org/wiki?curid=21850851", "title": "Kai Frobel", "text": "Kai Frobel\n\nKai Frobel is a German environmental ecologist. When he was young he lived very near the German part of the Iron Curtain, on the west side, near Coburg. He realized that the public-excluded security zone along the Iron Curtain had become a de facto wildlife reserve. When the Iron Curtain fell in 1989, he was instrumental in getting these wildlife reserves preserved as the European Green Belt. He runs the Ribbon of Life project which seeks to preserve these areas. He is associated with the BUND (Bund für Umwelt und Naturschutz Deutschland).\n\n"}
{"id": "10842601", "url": "https://en.wikipedia.org/wiki?curid=10842601", "title": "Las Cruces Biological Station", "text": "Las Cruces Biological Station\n\nThe Las Cruces Biological Station / Wilson Botanical Garden is located in the southern Puntarenas province of Costa Rica, and is the newest of the three research stations operated by the Organization for Tropical Studies (OTS). Las Cruces includes a biological research station, tourist facilities, and the botanical gardens started by Robert and Catherine Wilson, and bequeathed to OTS.\nLas Cruces is located in a mountainous region at an elevation of approximately 1,000 meters above sea level (m.a.s.l.). Surrounded mostly by pastures, coffee plantations and other agricultural areas, Las Cruces includes a relatively small 270 Ha forest fragment that ranges from 900–1300 m.a.s.l. As such, much of the research conducted here focuses on agroecology or studies of forest fragmentation.\n\n"}
{"id": "40493181", "url": "https://en.wikipedia.org/wiki?curid=40493181", "title": "List of Blue Flag Beaches of Wales", "text": "List of Blue Flag Beaches of Wales\n\nBelow is a list of Blue Flag beaches and marinas in Wales, sorted by regulatory body. As of July 2013, 38 locations had been designated by the internationally recognised Foundation for Environmental Education based in Copenhagen, Denmark. The foundation awards, or removes, beaches and marinas from their list each July.\n\nThe 2013 figure is down from 41 beaches in 2011 and 43 beaches in 2012. The reduced number was due to stricter water quality criteria being used, coupled with the fact that 2012 was a wet year in the country meaning that there were more naturally occurring bacteria present. In comparison, the larger countries of England and Scotland had 55 and 3 beaches listed and no marinas. The Welsh organisation Keep Wales Tidy helps to keep the beaches in good condition as well as running two separate sets of awards - the Green Coast Award and the Seaside Award.\n\nThe vast majority of the beaches and marinas are located on the Wales Coast Path.\n<br>\n<br>\n<br>\n<br>\n<br>\n<br>\n<br>\n<br>\n\n"}
{"id": "5373894", "url": "https://en.wikipedia.org/wiki?curid=5373894", "title": "List of mountain lists", "text": "List of mountain lists\n\nPerhaps the first of what would become many notable mountain lists around the world was Sir Hugh Munro’s catalogue of the Munros, the peaks above 3,000’ in Scotland). Once defined the list became a popular target for what became known as peak bagging, where the adventurous attempted to summit all of the peaks on the list. \n\nOver time the peaks on such lists grew more challenging. An example is the Seven Summits, defined as the highest peaks on each of the seven continents.\n\nSome notable lists of mountains are shown below.\n\n\n\n\nThe hills of Britain and Ireland are classified into a large number of lists for 'peak-bagging' purposes. Among the better-known lists are the following:\n\n\n\n\n\n\n\nThe standard list for the major peaks of the Andes is the list of 6000 m peaks as first compiled by John Biggar in 1996 and listed in his Andes guidebook. This list currently stands at 102 peaks, with no known completers.\n\n\n\n\n\"See also List of ribus.\"\n\nPopular peak-bagging challenges in Australia include the State 8: the highest peak in each of the six states and two territories (excluding Australia's external territories).\n\nThe Abels are a group of peaks in Tasmania over 1100 metres above sea level and separated from other mountains by a drop of at least 150 metres on all sides. Named after Abel Tasman, the first European to sight Tasmania.\n\n"}
{"id": "46827481", "url": "https://en.wikipedia.org/wiki?curid=46827481", "title": "Marine Well Containment Company", "text": "Marine Well Containment Company\n\nMarine Well Containment Company (MWCC) is an independent company with headquarters in Houston, Texas. The company provides well containment equipment and technology in the deepwater U.S. Gulf of Mexico and \"is committed to advancing its capabilities and technologies to keep pace with its members' evolving needs\". MWCC's mission is to be continuously ready to respond to a deepwater well control incident in the U.S. Gulf of Mexico.\n\nIn response to the Deepwater Horizon oil spill of 2010, ExxonMobil, Chevron Corporation, ConocoPhillips and Shell Oil committed to providing a new containment response capability. These founding companies of MWCC recognized the need to be better prepared in the event of a deepwater well control incident. As a result of unprecedented industry collaboration, MWCC introduced the Interim Containment System in February 2011, improving previous response capabilities and helping the industry get back to work in the U.S. Gulf of Mexico. In January 2015, MWCC accepted its Expanded Containment System forming the MWCC Containment System which builds upon the past system capabilities.\n\nThe Containment System is available for use in the deepwater U.S. Gulf of Mexico in depths from 500 feet to 10,000 feet, temperatures up to and pressures up to 15k psi. The system can cap or cap and flow an incident well and has the capacity to process up to 100,000 barrels of liquid per day and up to 200 million cubic feet of gas per day. Additionally, it is able to store up to 700,000 barrels of liquid in each of its two Modular Capture Vessels. The liquid is then brought onshore for further processing via shuttle tankers.\n\nMWCC maintains components of the Containment System in a ready state at two shore base locations along the U.S. Gulf Coast and conducts drills and training sessions on a regular basis to ensure personnel and equipment are ready to respond. TechnipFMC and Kiewitt Offshore Services (KOS) are MWCC's two main contractors. TechnipFMC manages and operates the subsea containment system at SURF shore base in Theodore, Alabama. KOS operates the MCV shore base at Ingleside, Texas. The Containment System is designed to be flexible, adaptable and ready to be mobilized upon incident notification.\n\nMWCC currently has 10 member companies: Anadarko Petroleum, Apache Corporation, BHP Billiton, BP, Chevron Corporation, ConocoPhillips, ExxonMobil, Hess Corporation, Shell Oil, and Statoil. Member companies are committed to deepwater drilling in the U.S. Gulf of Mexico, and a majority of wells drilled in the deepwater U.S. Gulf of Mexico are operated by these members. Membership remains open to all oil and gas operators in the U.S. Gulf of Mexico.\n\n\n"}
{"id": "19589623", "url": "https://en.wikipedia.org/wiki?curid=19589623", "title": "Minipermeameter", "text": "Minipermeameter\n\nIn petroleum engineering, a minipermeameter is a gas-based device for measuring permeability in porous rocks.\n\nMinipermeametry has been used in the oil industry since the late 1960s (Eijpe and Weber, 1971) without becoming in any way a standard experimental method in core analysis or reservoir characterisation. The laboratory minipermeametry can make important contributions both as an improved methodology within experimental petrophysics and as a source of data invaluable in routine reservoir characterisation (C. HALVORSEN AND A. HURST, 1990)\n\nThe values obtained from the minipermeameter should possibly be calibrated by a Klinkenberg correction\n\nhttps://web.archive.org/web/20110728002022/http://www.scaweb.org/assets/papers/1990_papers/1-SCA1990-27EURO.pdf\n"}
{"id": "17312943", "url": "https://en.wikipedia.org/wiki?curid=17312943", "title": "Non-trophic networks", "text": "Non-trophic networks\n\nAny action or influence that species have on each other is considered a biological interaction. These interactions between species can be considered in several ways. One such way is to depict interactions in the form of a network, which identifies the members and the patterns that connect them. Species interactions are considered primarily in terms of trophic interactions, which depict which species feed on others.\n\nCurrently, ecological networks that integrate non-trophic interactions are being built. The type of interactions they can contain can be classified into six categories: mutualism, commensalism, neutralism, amensalism, antagonism, and competition.\n\nObserving and estimating the fitness costs and benefits of species interactions can be very problematic. The way interactions are interpreted can profoundly affect the ensuing conclusions.\n\nCharacterization of interactions can be made according to various measures, or any combination of them.\n\nPrevalence identifies the proportion of the population affected by a given interaction, and thus quantifies whether it is relatively rare or common. Generally, only common interactions are considered.\nWhether the interaction is beneficial or harmful to the species involved determines the sign of the interaction, and what type of interaction it is classified as. To establish whether they are harmful or beneficial, careful observational and/or experimental studies can be conducted, in an attempt to establish the cost/benefit balance experienced by the members.\nThe sign of an interaction does not capture the impact on fitness of that interaction. One example of this is of antagonism, in which predators may have a much stronger impact on their prey species (death), than parasites (reduction in fitness). Similarly, positive interactions can produce anything from a negligible change in fitness to a life or death impact.\nThe relationship in space and time is not currently considered within a network structure, though it has been observed by naturalists for centuries. It would be highly informative to include geographical proximity, duration, and seasonal patterns of interactions into network analysis.\n\nIn the same way that a trophic cascade can occur, it is expected that 'interaction cascades' take place. Thus, it should be possible to construct 'effect' networks which parallel in many ways the energy or matter networks common in the literature. By assessing the network topology and constructing models, we might better understand how interacting species affect each other and how these effects spread through the network. In certain instances, it has been shown that indirect trophic effects tend to dominate direct ones (Patten, 1995)—perhaps this pattern will also emerge in non-trophic interactions.\n\nBy analyzing network structures, one can determine keystone species that are of particular importance.\nA different class of keystone species is what are termed 'ecosystem engineers'. Certain organisms alter the environment so drastically that it affects many interactions that take place within a habitat. This term is used for organisms that \"directly or indirectly modulate availability of resources (other than themselves) to other species, by causing physical state changes in biotic or abiotic materials\". Beavers are an example of such engineers. Other examples include earthworms, trees, coral reefs, and planktonic organisms. Such 'network engineers' can be seen as \"interaction modifiers\", meaning that a change in their population density affects the interactions between two or more other species.\n\nCertain interactions may be particularly problematic to understand. These may include\n\n\n"}
{"id": "24914493", "url": "https://en.wikipedia.org/wiki?curid=24914493", "title": "Photoluminescence excitation", "text": "Photoluminescence excitation\n\nPhotoluminescence excitation (abbreviated PLE) is a specific type of photoluminescence and concerns the interaction between electromagnetic radiation and matter. It is used in spectroscopic measurements where the frequency of the excitation light is varied, and the luminescence is monitored at the typical emission frequency of the material being studied. Peaks in the PLE spectra often represent absorption lines of the material. PLE spectroscopy is a useful method to investigate the electronic level structure of materials with low absorption due to the superior signal-to-noise ratio of the method compared to absorption measurements.\n\nIn a quantum-mechanical description of matter, the electrons confined to a material (such as those in individual atoms, molecules or crystals) are limited to a discrete set of energy values. The ground state of such a material system is such that the most energetic electron has its minimal energy. In photoluminescence, energy is transferred from light incident on the material and absorbed to electrons. The light is absorbed in minimal \"quanta\" or \"packets\" of energy of the electromagnetic radiation called photons. The amount of energy carried by a photon is proportional to its frequency. The electron is then in an excited state of higher energy. Such states are not stable and with time the material system will return to its ground state and the electron will lose its energy. Luminescence is the process whereby light is emitted when the electron drops to a lower energy level.\n\nOften when a photon is absorbed, the system is excited in the corresponding excited state, then it relaxes in an intermediate lower energy state, with a \"non-radiative relaxation\" (a relaxation that doesn't involve the emission of a photon, but e.g. involves the emission of vibrational energy) and then there is the emission of a photon with a lower energy than the absorbed one, because of the relaxation from the intermediate, lower energy state to the \"ground state\". Usually the strongest luminescence of the material is from the lower levels to the ground state. This process is called fluorescence. For instance, in semiconductors, most of the light emitted is at the frequency corresponding to the bandgap energy, i.e. from the bottom of the conduction band to the top of the valence band. In such systems, more light absorbed by the material, results in more electrons decaying non-radiatively to the lower states, and more luminescence in the emission wavelength.\n\n"}
{"id": "1588980", "url": "https://en.wikipedia.org/wiki?curid=1588980", "title": "Public Utility Regulatory Policies Act", "text": "Public Utility Regulatory Policies Act\n\nThe Public Utility Regulatory Policies Act (PURPA, ) is a United States Act passed as part of the National Energy Act. It was meant to promote energy conservation (reduce demand) and promote greater use of domestic energy and renewable energy (increase supply). The law was created in response to the 1973 energy crisis, and one year in advance of a second energy crisis.\n\nUpon entering the White House, President Jimmy Carter made energy policy a top priority. The law started the energy industry on the road to restructuring.\n\nThe Public Utility Regulatory Policies Act of 1978 (PURPA) encouraged \n\nEnergy companies were classified as natural monopolies, and for this reason, most were established with vertically integrated structures (that is, they undertook all the functions of generating, transmitting, and distributing electricity to the customer). Utilities became protected as regulated monopolies because it was thought that a company could produce power more efficiently and economically as one company than as several.\n\nPURPA started the industry on the road to restructuring and is one of the first laws that began the deregulation of energy companies. The provision which enabled non-utility generators (\"NUGs\") to produce power for use by customers attached to a utility's grid broke the previous monopoly in the generation function.\n\nUtilities offered customers a \"rate structure\" that decreased the cost per kWh price of electricity with increasing usage, with subsequent increments costing less per unit. PURPA eliminated promotional rate structures except when they could be justified by the cost structure of utility companies.\n\nOne provision of PURPA is the requirement for increased use of energy cogeneration. The law forced electric utilities to buy power from other more efficient producers, such as cogeneration plants, if that cost was less than the utility's own \"avoided cost\" rate to the consumer; the avoided cost rate was the additional costs that the electric utility would incur if it generated the required power itself, or if available, could purchase its demand requirements from another source. At the time generally, where demand was growing, this \"avoided cost\" was considered to be the construction and fossil fuel costs incurred in the operation of another thermal power plant.\n\nAs an effect, the number of cogeneration plants, which produce electric power and steam, increased. These plants are encouraged by the law, on the basis that they harness thermal energy (in the form of usable steam) that would be otherwise wasted if electricity alone was produced. PURPA also became the basic legislation that enabled renewable energy providers to gain a toehold in the market, particularly in California, where state authorities were more aggressive in their interpretation of the statute. The portion of the act dealing with cogeneration and small power production appears in US code in Title 16 – Conservation, Chapter 12 – Federal Regulation and Development of Power, Subchapter II – Regulation of Electric Utility Companies Engaged in Interstate Commerce, Sec 824a-3 – Cogeneration and Small Power Production.\n\nThis led to the establishment of a new class of generating facilities, which would receive special rate and regulatory treatment. Generating facilities in this group are known as qualifying facilities (QFs), and fall into two categories: qualifying small power production facilities and qualifying cogeneration facilities.\n\nPURPA provided favorable terms to companies that produced electricity from renewable (non-fossil-fuel) resources.\n\nAlthough a Federal law, PURPA's implementation was left to the individual states, because needs varied; a variety of regulatory regimes developed in states where renewable power resources were needed, available for development, or the generated power could be transmitted. Little was done in many states where such resources were unavailable, where the demand growth was slower or previously accommodated in planning.\n\nPURPA is becoming less important, as many of the contracts made under it during the 1980s are expiring. Another reason for PURPA's reduced significance is that electric deregulation and open access to electricity transportation by utilities has created a vast market for the purchase of energy and State regulatory agencies have therefore stopped forcing utilities to give contracts to developers of non-utility power projects. However, it is still an important piece of legislation promoting renewable energy because it exempts the developers of such projects from numerous State and Federal regulatory regimes.\n\nThis free market approach presented investment opportunity and government encouragement for more development of environment-friendly, renewable energy projects and technologies; the law created a market in which non-utility Independent Power Producers developed, and some energy market players failed.\n\nCritics of PURPA cited that power producers signed multi-year cost of electricity contracts at a time when energy prices were high. When oil prices went down, utilities had to honor the rates of those contracts, leading to high power prices.\n\nPURPA was the only existing federal law that requires competition in the utility industry and the only law that encourages renewables, if it is cost competitive with conventional polluting resources.\n\nIn February 2005, Senator Jim Jeffords from Vermont introduced an amendment to PURPA calling for a Renewable portfolio standard.\n\nPURPA was amended in 2005 by the Energy Policy Act of 2005 by sections 1251 through 1254. There is pending legislation in the US Senate that would amend PURPA to require FERC to develop standards for interconnection of distributed generation facilities, and that would require \"electric utilities\" meeting the PURPA size requirement (retail sales of more than 500 million kw hrs) to implement those standards.\n\nOne proposed law that would amend PURPA is the Hydropower Regulatory Efficiency Act of 2013 (H.R. 267). The bill was introduced into the United States House of Representatives of the 113th United States Congress on January 15, 2013, and it passed the House on February 13, 2013 by a vote of 422-0. If enacted, the bill would change some of the regulations in the United States surrounding hydropower by making it easier for smaller hydropower stations to be created. According to the bill's proponents, current regulations are unwieldy and represent a significant hurdle to creating more hydropower plants. H.R. 267 would alter those regulations to make it easier for smaller plants to get approval quickly. Section 3 of H.R. 267 amends the Public Utility Regulatory Policies Act of 1978 (PURPA) to increase from 5,000 to 10,000 kilowatts the size of small hydroelectric power projects which the Federal Energy Regulatory Commission (FERC) may exempt from its license requirements.\n\nSee related energy policy contained in 42 USC Chapter 134 – Energy Policy.\n\nIn October 2018, the National Association of Regulatory Utility Commissioners (NARUC) made suggestions in a report that FERC should modernize PURPA for the energy sector. NARUC's paper \"proposes that FERC exempt from PURPA’s mandatory purchase obligation those utilities which are subject to state competitive solicitation requirements and other best practices that ensure all technologies access to the market.\"\n\n\n"}
{"id": "2908738", "url": "https://en.wikipedia.org/wiki?curid=2908738", "title": "Pulsed energy weapon", "text": "Pulsed energy weapon\n\nA pulsed energy weapon is any weapon that:\nThese weapons often use large capacitors to build up a charge which is released when the weapon is fired. Large high-end systems sometimes use compulsators which store energy using rotational inertia. Weapons which do not use projectiles, such as the stun gun and laser gun, do not need any ammunition other than a power source.\n\n\n\n\nDirected energy weapon\n"}
{"id": "2631978", "url": "https://en.wikipedia.org/wiki?curid=2631978", "title": "Qantas Flight 1", "text": "Qantas Flight 1\n\nQantas Flight 1 (QF1, QFA1) was a Qantas passenger flight between Sydney and London that was involved in a runway overrun accident at Don Mueang International Airport in Bangkok on 23 September 1999 as it was landing for a stopover.\n\nQantas flights travel between London and Australia on a route known as the \"Kangaroo Route\". The Kangaroo Route traditionally refers to air routes flown between Australia and the United Kingdom, via the Eastern Hemisphere.\n\nThis flight was operated by a Boeing 747-438 S/N 24806, delivered new to Qantas in August 1990 and registered VH-OJH. It departed Sydney earlier that day at 16:45 local time, and after more than eight hours' flying was approaching Don Mueang International Airport at 22:45 local time.\n\nDuring the approach to Bangkok the weather conditions deteriorated significantly, from 5 statute mile visibility half an hour before landing to nearly one half statute mile visibility at the time of landing. The flight crew observed a storm cloud over the airport and ground reports were that it was raining heavily. However, these conditions are common at Bangkok. Seven minutes prior to Flight 1's landing a Thai Airways Airbus A330 landed normally, but three minutes before Flight 1's landing another Qantas Boeing 747 (QF15, a Sydney-Rome via Bangkok service), conducted a go-around due to poor visibility during final approach. The crew of Qantas Flight 1, however, were unaware of this.\n\nThe first officer was flying the aircraft during final approach. The aircraft's altitude and airspeed were high, but were within company limits. The rain was now heavy enough that the runway lights were visible only intermittently after each windscreen wiper stroke. Just before touchdown the captain, concerned about the long touchdown point (over 3000 feet past the runway threshold) and unable to see the end of the runway, ordered the first officer to perform a \"go-around\" and the first officer advanced the throttles but did not engage the takeoff/Go-around switch (TO/GA). At this point visibility improved markedly and the landing gear contacted the runway, although the aircraft continued to accelerate. The captain then decided to cancel the go-around by retarding the thrust levers, even though he was not flying the aircraft. This caused confusion as he did not announce his actions to the first officer who still had formal control. When over-riding the first officer's actions, the captain inadvertently left one engine at TO/GA power and as a result cancelled the preselected auto-brake settings.\n\nThe landing continued, but manual braking did not commence until the aircraft was over 5,200 feet down the runway. The aircraft then began to aquaplane and skid its way down the runway, departing substantially from the runway centreline. Company standard operating procedures mandated that idle reverse thrust should be used for landings and that flaps should be set at 25 degrees, not the maximum of 30 degrees. The combination of flaps 25, no auto-braking, no reverse thrust, a high and fast approach, a late touch down, poor Cockpit Resource Management, and the standing water on the runway led to a runway overshoot.\n\nThe aircraft gradually decelerated, ran off the end of the runway over a stretch of boggy grassland, colliding with a ground radio antenna as it did so, and came to rest with its nose resting on the perimeter road. The ground on the other side of the road forms part of a golf course.\n\nThere were no significant passenger injuries during an orderly evacuation of the aircraft carried out some 20 minutes after the rough landing. Thirty-eight passengers reported minor injuries. \n\nThe collision with the antenna caused the nose and right wing landing gear to collapse, the nose landing gear being forced back into the fuselage. The aircraft slid along in a nose-down, right wing low attitude, causing some further damage to the nose and damage to the two right engines and their mountings. The intrusion of the nose landing gear also caused the failure of the cabin intercom and public address system.\n\nThe damage was such that the aircraft was initially a write-off, but to preserve its reputation Qantas had it repaired at a cost of $100 million. By returning the aircraft to service, Qantas was able to retain its record of having no hull-loss accidents since the advent of the Jet Age.\n\n\n"}
{"id": "997410", "url": "https://en.wikipedia.org/wiki?curid=997410", "title": "Ram Pickup", "text": "Ram Pickup\n\nThe Ram pickup (formerly the Dodge Ram pickup) is a full-size pickup truck manufactured by FCA US LLC (formerly Chrysler Group LLC) and marketed as of 2011 onwards under the Ram Trucks brand. The current fifth-generation Ram debuted at the 2018 North American International Auto Show in Detroit, Michigan.\n\nPreviously, Ram was part of the Dodge line of light trucks. The name \"Ram\" was first used in 1932–1954 Dodge Trucks, then returned on the redesigned 1981 Ram and Power Ram, following the retiring and rebadging of the Dodge D Series pickup trucks as well as B-series vans.\n\nRam trucks have been named \"Motor Trend\" magazine's Truck of the Year five times; the second-generation Ram won the award in 1994, the third-generation Ram \"Heavy Duty\" won the award in 2003, the fourth-generation Ram Heavy Duty won in 2010 and the fourth-generation Ram 1500 won in 2013 and 2014. \n\nThe first-generation Ram trucks and vans introduced in 1981 featured a Ram hood ornament first used on Dodge vehicles from 1932 to 1954. Not all of the first-generation trucks have this ornament and is most commonly seen on four-wheel-drive models. Dodge kept the previous generation's model designations: \"D\" or \"Ram\" indicated two-wheel drive while \"W\" or \"Power Ram\" indicated four-wheel drive. Just like Ford, Dodge used 150 to indicate a half-ton truck, 250 for a three-quarter-ton truck, and 350 for a one-ton truck. The truck models were offered in standard cab, \"Club\" extended cab, and crew cab configurations. They also were offered along with and bed lengths and \"Utiline\" and \"Sweptline\" styled boxes along with standard boxes. Externally, the first-generation Rams were facelifted versions of the previous generation Dodge D-Series pickups introduced in 1972. The new model introduced larger wraparound tail lamps, dual rectangular headlamps, and squared-off body lines. Engine choices were pared down to the 225 Slant-6 and 318 and 360 V8s. The interior was updated and included a new bench seat and a completely new dashboard and instrument cluster with an optional three-pod design - a speedometer in the center, with the two side pods containing an ammeter on the top left, a temperature gauge bottom left, a fuel gauge on the top right and an oil pressure gauge bottom right. Models without the full gauge package had only indicator lights in the place of the temperature and oil pressure gauges. Among the options offered on the Ram were front bumper guards, a sliding rear cab window, air-conditioning, cruise control, tilt steering column, power door locks and windows, AM/FM stereo with cassette tape player, styled road wheels, aluminum turbine-style mag wheels, special paint and stripe packages, two-tone paint, and a plow package for four-wheel-drive models (referred to as the Sno Commander).\n\nThe \"Club Cab\" was dropped from the lineup after 1982, but Dodge kept the tooling and re-introduced nearly a decade later in the 1991 models. The four-door crew cab and Utiline beds were dropped after the 1985 model year, to make room on the assembly line for the upcoming 1987 Dodge Dakota, and were never reintroduced in this generation.\n\nBasic Ram 100 models were reintroduced for 1984, replacing the previous \"Miser\" trim level available on the Ram 150. A \"Ram-Trac\" shift-on-the-fly transfer case was added for the 1985's Power Rams, and both the crew cab and Utiline flared bed were dropped for 1986. In 1988 the Slant-6 engine was replaced by a fuel-injected V6 engine. The engine also received electronic fuel injection in 1988. Because of a new computer controlled fuel injection, ignition and ABS system, more vehicle information needed to be displayed through any warning or notification lights; so inside the cab where a small compartment was once located on the dash, a new \"message center\" with four small rectangular light spots, contained the check engine light and other tell-tales including one for the parking brake and the ABS if the truck was so equipped. The message center later included \"Wait to Start\" and \"Water in Fuel\" lights on diesel models. Diagnostic fault codes were stored in the computer's memory, and cycling the ignition key three times would allow the computer to flash the trouble codes through the check-engine light for diagnosis of some problems. Rear ABS became standard equipment in 1989.\n\nThe Ram 100 model designation was dropped and these models folded back into the 150 range for 1990, due to the introduction and sales success of the Dodge Dakota pickup. Additionally, the instrument cluster was slightly revised; the ammeter was replaced by a voltmeter while maintaining the 3-pod arrangement of the speedometer and gauges. Also in 1990, Dodge reintroduced the Club Cab, equipped with fold-out jump seats for the 1991-1993 models. Entry was made through the passenger or driver's doors, as there were no rear doors for this configuration.\n\nThese trucks, though popular with fleets, sold poorly compared to the Ford F-Series and the General Motors C/K Trucks, with just under 100,000 units sold most years of their production. Part of this was due to the dated cab and chassis design which had been in production since 1972, there was no powerful diesel option until 1989, and there was no big-block gas V8 option. Additionally, the interior had been given few updates since 1981.\n\nFor 1989, the 5.9 L V8 received throttle-body fuel injection for a 20 hp (15 kW) gain. Additionally, Dodge introduced a new overdrive automatic transmission for reduced fuel consumption. This light-duty transmission was designated the A500, and was offered with the 3.9 L V6 and 5.2 L V8. An \"O/D Off\" pushbutton switch to lock out the overdrive 4th gear was added to the message center. The A727 automatic saw continued use for some 5.2 L engines, all 5.9 L engines, and heavy-duty applications.\n\nThe grille was redesigned for 1991 but kept the large rectangular headlamps and crossbar appearance. The engines were substantially upgraded for 1992 (3.9 L and 5.2 L) 1993 and 1994 (5.9 L) with multi-port fuel injection, new manifolds, and higher-compression cylinder heads for noticeably higher output. These newly-revised engines were marketed under the \"Magnum\" name. A heavy-duty automatic transmission with overdrive called the A518 was offered with the 5.2 L and 5.9 L engines. As part of Chrysler's overhaul of corporate transmission nomenclature, the A500 and A518 were redesignated 42RH and 46RH, respectively, in 1992. The initial \"4\" signified a 4-speed transmission, the second digit identified the transmission's relative torque capacity, the letter \"R\" in the third position denoted a rear-wheel-drive transmission, and the final letter \"H\" signified hydraulic shift control. The 3-speed automatic remained available; the A727 was redesignated 36RH, and the A904, A998, and A999 became the 30RH, 31RH, and 32RH, respectively.\n\nA Cummins B Series engine was also added to the engine lineup in 1989. For the first time, Dodge saw sales increase. The Cummins was coupled with a heavier-duty version of the A727 automatic or a 5-speed manual transmission and is available on 250 and 350 pickups and pickup-based chassis-cab trucks. This diesel engine option was different from the optional diesels in Ford and GM trucks. The Cummins features direct injection, whereas the Ford and GM diesels feature indirect injection; this also means that the Cummins does not have to rely on glowplugs. The Cummins is a straight-six engine, whereas the GM and Ford diesel engines are V8 engines. Additionally, the Cummins is turbocharged, while the 6.2 L GM/DDC and 7.3 IDI Ford/IH are naturally aspirated.\n\nPre-1994 Cummins-engined Dodge pickups are often termed a \"1st Gen Cummins\".\n\nThis was not the first engine to appear in Dodge pickup trucks as a diesel option. The 1978 and 1979 D-Series models were available with a Mitsubishi naturally-aspirated diesel, but it was seldom ordered.\n\nIn the middle of 1991, Dodge started using 350 frames in Ram 250 diesels, club cabs and crew cabs. http://1stgen.org/viewtopic.php?t=5143\n\n\n\nThe second-generation Ram began development in 1986. The original concept, dubbed the \"Louisville Slugger\" by Chrysler's Advanced Packaging Studio, was to be a modular platform that would accommodate a full-size truck and full-size van, which would have provided a roomy cab and cargo bed. The modular design was scrapped in 1987 and was replaced with a more conventional truck design when the design was moved to the AMC design studio. The more conventional design, dubbed \"Phoenix\", was originally scheduled for a 1991 production; when Bob Lutz showed it to the new styling designers, chief designer Phillip E. Payne told him, \"It looks like nothing more than a rehash of everybody else's truck.\" At that, Lutz told him he had six months to come up with something better. The exterior styling of the truck that was eventually released was the result of design concepts by Payne during 1988-1990. A review by the Dodge pick-up truck studio designers felt that modern pick-ups looked \"too flat and sedan-like\", while the early 50's Studebaker pick-up and the semi-trailer trucks had just the right \"macho\" look to them. The design featured a big-rig-looking front end and a large grille that was nothing like the current Ford or Chevy/GMC pickups in design. The Dodge Ram Pick-up was selected as \"Truck of the Year\" for 1994.\n\nThe redesigned 1994 Ram was a sales success, with sales rocketing from 95,542 units in 1993 to 232,092 in 1994, 410,000 in 1995, and 411,000 by 1996. That year, it was prominently featured as the hero vehicle in the film \"Twister\". Sales of this generation peaked at just over 400,000 in 1999 before declining against the redesigned Ford and GM trucks. By 2001, Ram sales figures were below those of Ford and Chevy trucks.\n\nEngine offerings continued over from the first-generation Ram and were the 3.9 L V6, 5.2 L V8, 5.9 L V8, and 5.9 L I6 Cummins turbo diesel. Added to the line up was a new 488 cubic inch 8.0L V10 engine designed as an alternative for those who wanted superior pulling power but did not want a diesel. The new V10 and Cummins turbo diesel could only be had in the 2500 and higher designation models. Models were now the 1500 half-ton, 2500 three-quarter-ton, and 3500 dual-rear-wheel one-ton in both 2- and 4-wheel drive. 1500 Rams offered both 6.5- and 8-foot (2 and 2.4 m, respectively) boxes. 2500 Rams offered boxes with club or quad Cabs.\n\nDodge offered the 2500 series in two different gross-vehicle-weight ratings for the first few years, but this was later dropped. The purpose of the difference between the light-duty and heavy-duty 2500 trucks was for the heavy-duty 2500 to take the place of the discontinued one-ton single-rear-wheel trucks. Rear axles for the light-duty 2500 trucks were semi-floating, while the heavy-duty 2500 rear axles were full-floating.\n\nOn the inside, special attention was paid to in-cab storage features, with a large glovebox, a center armrest storage area, and extra storage space behind the seat. The dash and gauge cluster were a far cry from the previous model Ram and were far more modern as well. A redesign of the dashboard and instrument cluster was introduced in 1998 along with the introduction of the quad cab, and rounded black plastic side-view mirrors replaced the previous rectangular design.\n\nIn 1998, Dodge introduced the \"Quad Cab\", which uses smaller, \"suicide\" doors directly behind the main doors. This was offered as an option on the \"Club Cab\" for this model year. Other changes for 1998 included rounded mirrors replacing the classic square ones, a revised interior, dual airbags, a chime replacing the buzzer for seat belts/door ajar/headlights/ and a digital odometer. The OBD II System was also standard, with a computer port near the driver's-side footwell and a code-checking system via the new digital odometer readout.\n\nIn late 1998 Dodge introduced a revised front end for the 1999 model year Sport models with a restyled bumper, quad-beam clear-lens headlamps, and body-color grille. A 6-speed manual transmission was made optional for diesel variants in late 2000 for the 2001 model year.\nA small percentage of the diesel engines for 1999 and 2000 model years were subject to problems within the water jackets and fuel injectors. The most problematic was the \"53\" stamped engine block which had a defect that would cause fracturing in the structure of the block itself.\nThe 2000 models became optional with heated leather seats. The braking system was upgraded to dual-piston calipers in the front. An Offroad Edition was offered as a package with a 2-inch lift accomplished with stiffer front springs and rear lift blocks, unique 17x8 wheels, 275/70/17 all terrain tires, 4.10 gears, trussed Dana 44 in the front, limited slip differential, and skid plates. The Offroad Edition models are also distinguishable with an additional decal on the tailgate under the 4x4 decal that says \"Offroad.\"\n\nAlthough Dodge introduced a new Ram 1500 for 2002, the old second-generation style Ram was carried over for the 2002 model year heavy-duty 2500 and 3500 trucks. The new third-generation Ram would not appear in the 2500/3500 variants until 2003. Part of this delay was due to the then new 5.7 L Hemi engine not being ready for production.\n\nTransmissions for the Ram\nNV3500 was offered in 1500 Rams and light-duty 2500 Rams.\nNV4500 was standard in 2500 and 3500 trucks; the NV4500HD for V10 and diesel models (\"except the uncommon, light-duty 2500 rams\").\nA NV5600 was offered in 1999 and 2000 Rams and was the only transmission offered behind the High Output diesel in 2001 and 2002.\n\nThere was a total of five transfer cases available for the four-wheel-drive Ram. All are part-time and have a low range of 2.72:1. The 1500 featured a NP231 and NP231HD. The NP241 was standard on V8 2500 Rams. The 2500 and 3500 V10 and diesel featured a NP241DLD from 1993 to 1997. In 1997 the NP241DHD became an option for 2500 Rams and was standard on 3500 Rams from 1998 to 2002.\n\nThe Dodge Ram features a wide variety of axles. For the front axle of 4x4 Rams, a Dana 44 was used on all 1500 Rams and the early (light-duty) 2500 Rams. However, most of the 2500 and all 3500 Rams use Dana 60 front axles. The 1500 Rams and some early light duty 2500 Rams used a 9.25 Chrysler (Spicer) axle in the rear. A Dana 60 rear axle was used on heavy duty 2500 V8 Rams. A Dana 70 rear axle was used in 2500 Rams with a V10 or a Diesel/Automatic transmission combination. A Dana 80 rear axle was used on 2500 Rams with a manual transmission and V10/diesel engine combination. Every 3500 Ram was made with a Dana 80. The front drive axles in these Rams were unique in the fact they did not have locking hubs, but featured a center axle disconnect. The 2002 2500 and 3500 Rams saw the eventual phase out of the Center axle disconnect, in favor of front axles that were permanently locked in. Dodge continued to include front axles like this for their 2500, 3500, 4500, and 5500 trucks until 2013 models.\n\nA natural-gas-powered engine debuted for 1995 but was not popular and was only used in fleet vehicles on a very limited production run. The Cummins B Series engine was switched from the 12-valve to the 24-valve (ISB) version in the middle of the 1998 model-year Dodge Rams due to emissions regulations. The ISB featured a new computer-controlled injection pump, 24-valve head design, and an electronic injection pump.\n\n\nIn development from 1996 (styling by Cliff Wilkins finalized in 1998), the third-generation Ram was unveiled on February 7, 2001 at the 2001 Chicago Auto Show, and debuted for 2002 model year on 1500 models and 2003 on 2500 and 3500 models. This was a major update including an all new frame, suspension, powertrains, interiors, and sheet metal. The crew cab models for this generation were actually Quad Cab trucks that had conventional-opening rear doors. The four-wheel-drive light trucks (1500 series) lost their live axles in favor of an independent front suspension, but the 2500 and 3500 series retained the live axles for maximum longevity and durability. This body style drew heavily from the previous generation.\n\nThe redesigned trucks bolstered sales, with 400,000 sold during 2001-2002 and nearly 450,000 sold during 2002-2003, a new high point for the Ram name. At the same time, both Ford and GM trucks were increasing in sales from a 2001 peak over 850,000 to the 900,000 range. But with 400,543 Rams sold that year, the Ram's sales could not keep up with the eleventh-generation F-150 in 2004.\n\nThe Dodge Ram was updated for the 2006 model year. One notable addition was the \"Mega Cab\", featuring a cargo box and of extra cab space, allowing seating for six with rear recliners, a full screen mapping in-dash navigation system became an option, and the headlamps were redesigned to a more modern design.\n\nFor 2006, the steering wheel design was changed to one from the Dodge Dakota and Dodge Durango. Bluetooth U Connect was now available as an option, and a front facelift was given to all Ram models. SIRIUS Satellite Radio was available, as well was a rear seat DVD entertainment system with wireless headphones. The SRT model, with the 8.3 L V10 engine from the Dodge Viper SRT/10, was discontinued after the 2006 model year.\n\nFor 2007, Dodge changed the tail lights.\n\nIn 2007, a 3500 Chassis Cab model was introduced with industry-standard rear frame width and wiring to accommodate outfitters. In addition to the , a Cummins diesel rated at and was also available. Automatic transmissions used were the 545RFE with the the AS68RC with the . The G56 transmission was the only manual transmission offered.\n\nFor 2008, Dodge introduced two more Chassis Cab models, the 4500 and 5500. These were Class-4 and Class-5 trucks with a gross weight of and , respectively. Both trucks came equipped with the same version of the Cummins diesel as the 3500 chassis-cab model. Sterling, who worked with Dodge in development, had their own version, called the Sterling Bullet with a unique grille. Sterling is a division of Freightliner LLC which, like Dodge, was owned by the former DaimlerChrysler. Sterling Trucks was licensed to sell Dodge Ram 4500 series trucks as the Sterling Bullet. When the Sterling brand was phased out by Chrysler Corporation, the Bullet was discontinued.\n\nModels built after January 1, 2007 offered a new 6.7 L Cummins turbo diesel introduced as an option in 2500/3500 models replacing the 5.9 L. It produced and . Unlike the 5.9 L which was backed by the 4-speed 48RE Transmission, the 6.7 L was equipped with the new 6-speed 68RFE transmission.\n\n2005 was the last year for the first version of the Hemi V8. 2006 half-ton models offered the Multi-Displacement System Hemi V8 engine that also became available in Chrysler and Dodge sedans. This engine featured the same performance but had a cylinder-deactivating feature enabled under light loads to increase fuel economy by 3 MPG city and 4 MPG hwy. This new Hemi still delivered and .\n\nFor the 2003 model year, AAM axles replaced the Dana Corp axles. In the front all 2500 and 3500 trucks were 9.25-inch with 33 spline axles. The rear options for the 2500 and 3500 were the AAM (often referred to as \"corporate\") 10.5\" and 11.5\". Rear axle shafts are 30 spline. The rear 11.5\" has a gear ratio \"carrier split\" at 3.73 and numerically higher, but the General Motors AAM axles used a different carrier spacing preventing installation of a Chrysler carrier into some GM axles, but the GM carrier can be installed in the Chrysler axle if a ring gear spacer is installed. Strength is similar to their earlier Dana 70 and 80 counterparts. Direct comparisons are difficult as the axles are made with completely different metallurgy.\n\n\n\n\nThe truck won the 2003 and 2004 NASCAR Craftsman Truck Series Manufacturers' Championship. It also won the San Felipe 250 in 2008 and 2009.\n\nThe fourth generation Dodge Ram was introduced at the 2008 North American International Auto Show in Detroit. This latest generation was sold as the 2009 Dodge Ram 1500 starting in Fall 2008. The 2500, 3500, 4500 and 5500 models were later added to the lineup. In 2010, the Ram Trucks brand was separated from Dodge.\n\nChrysler LLC attempted to keep the Ram competitive in the market through various developments for the 2009 model, including a new four-door cab style offering, new suspension, a new hemi engine option, and the Rambox, a new storage system that allows secure storage inside the truck's bed walls. Later models have the Rambox system tied in with the remote keyless system\n\nSince 2011, Ram trucks are marketed as having \"class-exclusive\" manual transmissions. This is because the competitors stopped making manual transmissions available in the domestic market. Chevrolet Silverado trucks discontinued the option of a manual transmission after model year 2006. Ford Super Duty trucks discontinued the option of a manual transmission after model year 2010. This applies to Class 2, Class 3, Class 4 and Class 5 trucks. 2011 and 2012 models make and of torque. Horsepower remained the same for 2013 models, torque however, was increased to of torque. Engine output remained the same for 2014 models.\n\nThe Mega Cab option was deleted, at least on the 1500 model, replaced by a true four-door crew cab. Other cab options are regular cab and quad cab. The mega cab option remains on the 2500/3500 models. Like the previous generation, the mega cab uses the same wheelbase and overall length as the crew cab/long bed configuration, but instead of using a long bed, the rear cab is extended. Legroom remains unchanged, but the rear seats are able to recline.\n\nA coil spring five-link rear suspension replaces the leaf-springs for the 1500.\n\nThe 2009 model's towing capacity was originally rated at for 2WD Ram 1500 with regular cab, long-bed, 5.7 L Hemi engine, 3.92 differential and 17-inch wheels, but the rating increased to without changing the setup. For the 2010 model, payload was increased by to for the regular cab 2WD model with the 3.7 litre V6 engine.\n\nTowing capacity for the regular cab Ram 1500 with 3.21 differential is rated at with 17-inch wheels and with 19-inch wheels. Crew Cab and Quad Cab models are rated at and respectively.\n\nGross Combined Weight Ratings is for all Ram 1500s with 3.21 axles; max for 2WD Ram 1500 with long bed, 5.7 L hemi engine, and a 3.92 differential.\n\nChassis Cab versions were made for the Ram 3500, 4500, and 5500 models. The 3500 Heavy Duty model was unveiled at the 2009 Chicago Auto Show.\n\nEngine choices include 5.7 L Hemi V8 rated at at 5,600 rpm and at 4,000 rpm for Ram 3500, 6.7 l Cummins turbo diesel rated at at 3,013 rpm and at 1,500 rpm for Ram 3500 (optional), 4500, 5500. Late-model 2011 diesel trucks were up-rated to of torque.\n\nTransmission choices included a standard 6-speed manual or optional 6-speed Aisin automatic. Both transmissions support the Power Takeoff option.\n\nThe 3500 model has regular or crew cab, single-rear-wheel or dual-rear-wheel. Four cab-axle lengths (60, 84, 108 and 120 inches) for 4500/5500 or two cab-axle lengths (60 and 84 inches) for 3500, and three trim levels (ST, SLT and Laramie).\n\nThe Ram 3500 has three axle ratios (3.42, 3.73 and 4.10) and 17-inch wheels. Ram 4500/5500 has three axle ratios (4.10, 4.44 and 4.88) and 19.5-inch wheels. The 4500/5500 Rams rear axle is a Dana S 110. The front axle on 4x4 models is manufactured by Magna, while 2-wheel-drive models just have a solid, non-drive axle.\n\nThe Ram concept truck \"Long Hauler\" is based mostly on existing Ram truck parts. The powertrain, frame and wheels are all components of the Ram 5500 Chassis Cab. It is a \"Mega Cab\" optional on the lighter Rams. The GCWR for this truck is 37,500 lb and the weight of the truck is 9,300 lb.\n\nThe Ram comes standard with four-wheel anti-lock disc brakes, side curtain airbags, and an electronic stability program or ESP. In Insurance Institute for Highway Safety (IIHS) crash tests, the 2009 Ram received a \"Good\" overall score in the frontal crash test, and a \"Marginal\" score in the side impact test, as well as the roof strength test.\n\nIt received a 5-star frontal crash test rating from the NHTSA, however under later testing methods it received an overall 3-star rating. In the side-pole test it was given a single star rating, because of excessive forces to the thoracic region. While the Ram 1500 features standard side-curtain airbags it does not include side torso airbags. The vehicle was redesigned and retested, and received a 5-star rating.\n\nThe 1500 gets a minor restyling. It features new front fascia, optional projector beam halogen headlamps with LED turn signals/parking lamps, wheels and interior, where the \"DODGE\" name was removed from the dashboard and replaced with \"RAM\".\n\nAll models offered for 2012 continue to be available for 2013 with the addition of a new trim, the Laramie, Longhorn, Limited. (Tradesman, Express, SLT, Big Horn, Lone Star, Sport, R/T, Laramie, and Laramie Longhorn). The Rambox cargo management system continues to be available on most models for 2013. For 2013, the base ST model becomes the Tradesman model.\n\n2013 models have revised engine and transmission options. The 3.7 L V6 is discontinued, and the 4.7 L V8 equipped with the 6-speed 65RFE Automatic takes its place as the new base engine, still producing and . New to the lineup is Chrysler's corporate 3.6 L Pentastar V6, coupled to the new ZF 8-speed Torqueflite8 Automatic. It achieves best in class fuel efficiency and makes and . The Pentastar/ZF 8-speed is optional. Due to a new electric power steering system, the 5.7 L HEMI V8 no longer has a power steering pump, and gains 5 horsepower, now making and . It is still available with the 65RFE 6-speed Automatic, with the new 8-speed Torqueflite8 Automatic optional.\n\nAir suspension is optional for 2013 models, offering four ride height settings. Electronic stability control becomes standard on 2500 and 3500 models. For 2013, the Ram is also the first North American pickup truck to offer keyless ignition.\n\nNew infotainment systems are available for the RAM 1500 for 2013:\n\nThe base U Connect 3.0 (RA1) radio includes an AM/FM radio, Radio Data System (RDS), a monochrome LCD display screen, a 3.5-millimeter auxiliary audio input jack, and a single USB port for charging purposes only.\n\nThe \"up-level\" U Connect 5.0BT (RA2) radio adds a full-color, 5.0-inch color touch-screen display, voice control for phone, a microSD card slot, Bluetooth for phone and A2DP wireless stereo audio streaming capabilities, and vehicle customization options on-screen.\n\nThe \"top-of-the-line\" radios are the U Connect ACCESS 8.4A (RA3) and U Connect ACCESS 8.4AN (RA4). These radios add a full-color, 8.4-inch color touch-screen display, full voice control, SiriusXM satellite radio, the U Connect ACCESS System, featuring roadside assistance and 9-1-1 emergency call buttons on the rear-view mirror, mobile app compatibility, remote USB ports for integration of compatible devices, enhanced in-vehicle customization options, automatic notification of airbag deployment, concierge services, remote control via an app installed on a compatible smartphone, and the ability to add GPS navigation from Garmin for vehicles not equipped with the option from the factory (it is activated by a dealership for a fee, however, does not include SiriusXM Travel Link or 3D mapping). The U Connect ACCESS 8.4AN (RA4) adds GPS navigation from Garmin, HD Radio SiriusXM Travel Link, 3D mapping, and enhanced mobile app compatibility. The system also includes a built-in 3G internet router, allowing for wireless connection to the internet while the vehicle is parked with a monthly service subscription. The system can also be updated to add additional features that will be available in the future via a USB stick inserted into one of the remote USB ports. Remote steering wheel-mounted controls are also included with this system.\n\nOptions such as a CD Player (mounted in the center console of the vehicle), SiriusXM satellite radio and a Rearview Backup Camera can be added to any radio.\n\nThe 2013 models move up to an 18-inch standard wheel, with 18-inch forged Alcoa wheels as an option. 3500 models offer a High Output package for the diesel. The rear differential ring gear is increased from 11.5 inches, to 11.8 inches on H.O. trucks. The 11.8 axle cover doubles as a heat sink.\n\nRam, once again, put the Center axle disconnect in 3500 4x4 models. 2500 Ram trucks did not have a CAD system until 2014 models. Ram claims this technology to improve fuel efficiency by 1 MPG, although EPA tests do not include class 2 and class 3 trucks in the MPG/fuel efficiency tests. The 4x4 saw additional changes with the transfer case now being made by BorgWarner instead of New Venture Gear.\n\nA new model offered for the 2013 Ram is the HFE (High Fuel Efficiency). Based on the SLT model, the HFE offers 18/25 MPG out of its Pentastar V6 engine and eight-speed Torque-Flite transmission, which has nothing in common with the original Chrysler built TorqueFlite. It is available only in the six-foot bed, two-door, regular cab model with two-wheel-drive only. Standard features include a 220 amp alternator and an 800 amp battery to assist with the Start-stop system. 3.21:1 axle gearing is also standard.\n\nThe option of air suspension became available on the rear of 2014 model year 2500 and 3500 pick up trucks. 1500 models added the option of a Eco-Diesel V6. 2500 models now feature coil spring suspension, instead of leaf spring. The cast iron, flex fuel, 4.7 V8 Chrysler PowerTech engine was discontinued, ending Corsair engine production, leaving the Ram 1500 with two engine choices. The aluminum, flex fuel 3.6 V6 Pentastar is now the base engine, along with the 8-speed Torque-Flite transmission. The 3.6 L Pentastar V6 was available on Laramie and Longhorn trims. The Ram heavy-duty series of trucks (2500, 3500, 4500, and 5500) received a new interior design and revised exterior styling from the Ram 1500, which was restyled in 2013.\n\nFor 2016, the all-new Ram 1500 Rebel debuted, offering off-road suspension with a higher ride height, larger tires, and a unique interior to add to the Ram 1500 Big Horn and Lone Star trim levels. It features a unique blacked-out grille with powder coated front bumper and exclusive aluminum hood, skid plates, and tow hooks. It is available with either a and 3.6 L Pentastar VVT FlexFuel-Capable V6 engine or a and 5.7 L HEMI V8 engine. Four-wheel drive is available with either powertrain, but the two-wheel drive model is V8-only. The sole body configuration offered is a crew cab with a short pickup bed.\n\nSLT models of all Ram trucks now receive the U Connect 5.0BT (RA2) touch-screen radio as standard equipment, adding steering wheel-mounted remote phone controls, Bluetooth hands-free calling and wireless audio streaming, SiriusXM Satellite Radio, a USB port and 3.5 mm auxiliary input jack, Radio Data System (RDS), voice control, in-radio vehicle customization, and a full-color five-inch LCD touch-screen display. This was previously a $495.00 option on the Ram SLT.\n\nFor 2017, the Ram 1500 Laramie Longhorn and Limited trims received the new RAM front grille and \"RAM\" emblem across the rear tailgate that debuted on the 2016 RAM 1500 Rebel.\n\nIn mid-2017, the RAM 1500's 3.0 L EcoDiesel V6 turbodiesel engine, produced by VM Motori, was dropped in mid-2017 in response to emissions cheating allegations. The engine will be re-introduced in early 2018, as re-certification by the EPA has been obtained.\n\nThe 1500 SLT model loses the U Connect ACCESS 8.4 infotainment system (RA3) option, leaving the U Connect 5.0BT (RA2) radio as the only radio option. However. all Ram heavy-duty (2500, 3500, 4500, and 5500) SLT models will still offer the U Connect ACCESS 8.4 infotainment system (RA3) as an option.\n\nThe Rebel trim level of the Ram 1500 gains the previously-optional U Connect ACCESS 8.4A infotainment system as standard equipment, as do the Big Horn and Lone Star trim levels. A Rebel Black Edition Package is available that adds black accents to the Rebel.\n\nThe Laramie trim level of the Ram 1500, 2500, 3500, 4500, and 5500 gets a standard nine-speaker, 506-watt Alpine 7.1-channel surround-sound premium audio system as standard equipment.\n\nThree new special edition trim levels debut for 2017 for the Ram 1500 model:\n\n\nThere is also a new Off-Road Package available for all Ram 2500 models (aside from the Power Wagon trim level, which includes the package as standard equipment), and includes unique pickup bedside graphics, off-road suspension, front tow hooks, and the protection package.\n\nThe mid-level SLT trim level of the RAM 1500 is no longer available to retail customers, as it is now a lesser-equipped model reserved exclusively for fleet customers. However, the SLT trim level still remains for the Ram 2500, 3500, 4500, and 5500 models, and are still available to retail customers. However, retail customers wanting SLT features such as chrome front and rear bumpers and front grille, seventeen-inch aluminum-alloy wheels, power windows and door locks with keyless entry, the U Connect 5.0BT (RA2) touch-screen radio, cloth seating surfaces, SiriusXM Satellite Radio, a rear-view backup camera system, and floor carpeting, can still opt for the Chrome Appearance and Popular Equipment Packages on the Tradesman model.\n\nFor customers wanting a less expensive Bluetooth option for the Ram truck, a new U Connect 3.0BT (RA2) radio is available for an additional $200.00, which includes an A/M-F/M radio and SiriusXM Satellite Radio tuner, Radio Data System (RDS), U Connect Bluetooth hands-free phone system with audio streaming and voice command, steering wheel-mounted remote voice command controls, a USB port and 3.5 mm auxiliary audio input jack, and a three-inch (3.0\") monochrome LCD display screen. This radio option is available for the Tradesman trim level of all Ram trucks, and the Express trim level of the Ram 1500, and is included as part of a Popular Equipment Package on the latter model. Previously, customers wanting Bluetooth on a Ram Tradesman or Ram 1500 Express would have to upgrade to the U Connect 5.0BT (RA2) touch-screen radio, which would add the same features as described above, as well as onscreen vehicle customization via the radio, and a full five-inch (5.0\") color touch-screen display, and cost an additional $495.00. In order to do this, the customer would also have to select the Popular Equipment Package on both models.\n\nThe RAM 2500 Power Wagon returned with a 6.4 L HEMI gasoline V8 engine, a unique graphics package (delete option), black front grille, door handles, larger tires and black aluminum-alloy wheels, and the Off-Road Package. The Power Wagon Package is also available for the Ram 2500 Tradesman trim level, and also adds the 6.4L HEMI gasoline V8 engine.\n\nChanges to the Ram for 2018 included the addition of HD Radio to all U Connect 8.4 infotainment systems, as well as a new 4G LTE wireless hot-spot provided by AT&T Wireless, an AT&T Wireless 4G LTE in-vehicle modem (both the AT&T Wireless 4G LTE modem and 4G LTE mobile hot-spot replace the Sprint 3G CDMA modem and 3G CDMA mobile hot-spot offered on previous Ram models), Apple CarPlay and Android Auto smartphone integration for the U Connect 8.4 infotainment systems, and the SiriusXM Guardian service replacing the U Connect ACCESS service offered on previous RAM models for the U Connect 8.4 infotainment systems. In addition, all U Connect 8.4 infotainment systems get a new User Interface design for easier navigation of menus and applications.\n\nThe VM Motori-produced 3.0 L EcoDiesel V6 turbodiesel engine returned in early 2018 for the RAM 1500, as it has been re-certified by the EPA after allegations of emissions cheating in 2017.\n\nMost upper trim levels of the RAM 1500 (Sport, Rebel, Laramie Longhorn, and Limited) get the new RAM front grille introduced for 2016, as well as a large 'RAM' emblem on the rear tailgate. Lower to mid trim levels of the RAM 1500 (Tradesman, Express, Big Horn, Lone Star, and Laramie) retain the standard RAM \"Cross-Hair\" front grille.\n\nThe Night Edition and Lone Star Silver Edition trim levels were continued.\n\nFor 2018, there are two new special editions:\n\nFor 2019, the current fourth-generation Ram 1500 will be sold alongside its successor, the fifth-generation Ram 1500. Virtually identical to 2018 models, 2019 trucks will be given a new name, Ram 1500 Classic, to distinguish them from their all-new fifth-generation successors. The Ram 1500 Classic will offer a 2-Door Regular Cab model, whereas the fifth-generation Ram 1500 will not. In addition, Quad and Crew Cab models are also expected to be offered.\n\nIn addition, the current-generation Ram 2500, Ram 3500, and Ram 3500/4500/5500 Chassis Cab models will continue for 2019, and will be virtually unchanged from 2018 models. A redesign of these trucks is due for the 2020 model year.\n\nThe Ram 1500, Ram Mega Cab, Ram 2500/3500, DX Chassis Cab (Mexico Market), Ram 4500/5500 are assembled at FCA's Saltillo Assembly Plant in Coahuila, Mexico; the Ram 1500 (DT) is assembled at Sterling Heights Assembly (SHAP) in Sterling Heights, Michigan. The Ram 1500 (DS) was previously assembled at Warren Truck Assembly in Warren, Michigan.\n\nA two-mode hybrid version was planned to begin production in 2010, but Chrysler decided to cancel it in favor of a smaller test fleet of 140 plug-in hybrid (PHEV) Rams developed with support from a million grant from the U.S. Department of Energy financed through the American Recovery and Reinvestment Act of 2009. The Chrysler PHEV pickup project has an estimated total cost of million. The RAM 1500 pickup PHEV was introduced at the January 2011 Washington Auto Show. The vehicle is part of a three-year demonstration program intended to field test and evaluate battery performance across a wide range of drive cycles and temperature ambients, and also to evaluate customer acceptance. The PHEV demonstrator pick-up trucks are assembled at the Warren Truck Assembly plant in Michigan and the plug-in hybrid conversion takes place at the Chrysler Technology Center in Auburn Hills, Michigan. The demonstrators are being allocated for field testing among local and state governments, utility companies, and a U.S. Army base. Chrysler has no plans for a production version.\n\nThe Ram 1500 PHEV demonstrator has a 345 hp 5.7L Hemi V8 gasoline engine mated to a two-mode hybrid transmission and a 12.9 kWh 355V lithium-ion battery from Electrovaya. This setup allows an all-electric range of more than , but as a blended plug-in hybrid, the RAM PHEV does not run exclusively all-electric during EV mode. The fully charged plug-in starts off with charge depletion with limited regeneration at the high end of the state of charge (SoC). That ramps up to a full regenerative capability somewhere in the 70 to 95% range and depletes down to about 20%. Once depleted, it comes into a narrow charge-sustaining range. The plug-in pick-up meets AT-PZEV emissions standards, making it the only full-size V8 pickup with an AT-PZEV rating. Its fuel economy in charge-depleting mode is more than in city driving. The Ram 1500 PHEV is capable of towing up to .\n\nThe first 20 Ram PHEV demonstrators were delivered in May 2011 to Clark County, Nevada and Yuma, Arizona, with 10 units each city. Other cities that received the demonstration PHEVs are San Francisco and Sacramento, California, Albany, New York, and Charlotte, North Carolina. In September 2011, another 10 units were delivered to the Massachusetts Bay Transportation Authority (MBTA). Another 5 units will be delivered to Central Hudson Gas & Electric and National Grid. DTE Energy in Detroit is scheduled to receive 10 Ram PHEVs.\n\n\nIn September 2012, Chrysler temporarily suspended the demonstration program. All 109 Dodge Ram 1500 Plug-in Hybrids and 23 Chrysler Town & Country plug-in hybrids deployed by the program were recalled due to damage sustained by three separate pickup trucks when their 12.9 kWh battery packs overheated. The carmaker plans to upgrade the battery packs with cells that use a different lithium-ion chemistry before the vehicles go back into service. Chrysler explained that no one was injured from any of the incidents, and the vehicles were not occupied at the time, nor any of the minivans were involved in any incident, but they were withdrawn as a precaution. The demonstration is a program jointly funded by Chrysler and the U.S. Department of Energy that includes the first-ever factory-produced vehicles capable of reverse power flow. The experimental system would allow fleet operators to use their plug-in hybrids to supply electricity for a building during a power outage, reduce power usage when electric rates are high or even sell electricity back to their utility company. The company reported that the demonstration fleet had collectively accumulated 1.3 million miles (2.1 million km) before the vehicles were recalled. Chrysler also reported that the plug-in pickups delivered peak average fuel economy of , while the plug-in hybrid minivans delivered .\n\nCharlie Miller, security researcher for Twitter and former NSA hacker, and Chris Valasek, director of vehicle security research at the consultancy IOActive, showed how a Jeep Cherokee can be remote controlled. FCA recalled 1.4 Mio cars of various brand with the Uconnect entertainment system on board.\n\nThe fifth-generation Ram made its debut at the 2018 North American International Auto Show in Detroit, Michigan on January 15, 2018.\n\nThe 2019 Ram 1500 will reach Ram Truck dealers in the U.S. in the first quarter of 2018 as an early 2019 model year vehicle. The starting manufacturer's suggested retail price will be $31,695 plus $1,645 destination fee.\n\nThe 2019 Ram 1500 pickup truck will offer just seven trim levels, compared to the previous 11 different trim levels.\n\nAs of 2016, Ram trucks are available in the United States, Canada, Mexico, Brazil, Colombia, Argentina, Chile, Paraguay, Europe, and several Middle Eastern countries. The Ram trucks officially sold in many Latin American countries are outfitted with UNECE WP.29 lighting equipments, including the taillights with separate amber turn signal indicators.\n\nNo right-hand-drive version has been built for the United Kingdom, South Africa, and other countries where the rule of road is on the left. In 2016, FCA commenced selling the Ram 2500 and Ram 3500 in Australia. These are converted to right hand drive in Australia before sale. Pricing commences from $139,500AUD (approx $100,000USD).\n\nRam trucks are provided for the European market by the Swiss company AutoGlobal Trade in cooperation with FCA. Germany and Scandinavian countries are some of the largest European markets for the Ram. A thriving cottage industry in Australia, New Zealand, and the United Kingdom has imported and converted Ram trucks to right-hand-drive and to meet the local regulations, being more common in Australia since LHD cars less than 30 years old (or 15 years old if registered in Western Australia) cannot be legally driven on Australian public roads unless they are granted a diplomatic or a research and development exception to the rule. In the United Kingdom there is no such restriction, so a stock LHD Ram (or any other LHD vehicle) is not required to undergo the costly and time-consuming process of an RHD conversion in order to be compliant with EU regulations.\n\nFollowing the collapse of Nissan Titan sales in 2008, there were talks that the next generation of Nissan's full-sized pickup would be outsourced to Chrysler as a version of the Ram 1500. Nissan had been planning to phase out Titan production in its Canton, Mississippi factory in 2010 with the new Nissan-only design for a cab, body and interior riding on the Dodge Ram chassis assembled in Chrysler's truck assembly lines in Saltillo, Mexico. However, the deal to build Nissan Titan pickups off the full-sized Dodge Ram pickup starting in 2011 was delayed with the changes at Chrysler and Fiat. Nissan eventually decided to keep the Titan, and went on to release the second generation Titan as a 2016 model called the Titan XD.\n\n\n"}
{"id": "4531072", "url": "https://en.wikipedia.org/wiki?curid=4531072", "title": "Reactimeter", "text": "Reactimeter\n\nA reactimeter is a diagnostic device used in nuclear power plants (and other nuclear applications) for measuring the reactivity (in inhours) of fissile materials as they approach criticality.\n"}
{"id": "2413266", "url": "https://en.wikipedia.org/wiki?curid=2413266", "title": "Sculptured stones", "text": "Sculptured stones\n\nSculptured stones is a name applied to commemorative monuments of early Christian date found in various parts of the British Isles and Scandinavia.\n\nThey are usually rough-hewn slabs or boulders, and in a few cases well-shaped crosses, bearing lettered and symbolic inscriptions of a rude sort and ornamental designs resembling those found on Celtic scriptures of the Gospels. Their lettered inscriptions are in Latin, Ogam or Scandinavian and Anglican runes, while some are uninscribed. Sculptured stones are usually found near ancient ecclesiastical sites, and their date is approximately fixed according to the character of the ornamentation. Some of these stones date as late as the 11th century. The Pictish stones from Scotland are particularly remarkable for their elaborate decoration and for certain symbolic characters to which as yet no satisfactory interpretation has been found.\n\n"}
{"id": "5042942", "url": "https://en.wikipedia.org/wiki?curid=5042942", "title": "Sea silk", "text": "Sea silk\n\nSea silk is an extremely fine, rare, and valuable fabric that is made from the long silky filaments or byssus secreted by a gland in the foot of pen shells (in particular \"Pinna nobilis\"). The byssus is used by the clam to attach itself to the sea bed.\n\nSea silk was produced in the Mediterranean region from the large marine bivalve mollusc \"Pinna nobilis\" until early in the 20th century. The shell, which is sometimes almost a metre long, adheres itself to rocks with a tuft of very strong thin fibres, pointed end down, in the intertidal zone. These byssi or filaments (which can be up to 6 cm long) are spun and, when treated with lemon juice, turn a golden colour, which never fades. \n\nThe cloth produced from these filaments can be woven even finer than silk, and is extremely light and warm; it was said that a pair of women's gloves made from the fabric could fit into half a walnut shell and a pair of stockings in a snuffbox. The cloth attracts clothes moths, the larvae of which will eat it.\n\nIn addition, \"Pinna nobilis\" is also sometimes gathered for its flesh (as food) and occasionally has pearls of fair quality.\n\nThe Greek text of the (196 BCE) Rosetta Stone records that Ptolemy V reduced taxes on priests, including one paid in \"byssus\" cloth. This is thought to be fine linen cloth, not sea silk. In Ancient Egyptian burial customs, \"byssus\" was used to wrap mummies; this was also linen and not sea silk.\n\nThe sophist author Alciphron first records \"sea wool\" in his (c. 2nd century CE) \"Galenus to Cryton\" letter.\n\nThe early Christian Tertullian (c. 160–220 CE) mentions it when justifying his wearing a pallium instead of a toga.\nNor was it enough to comb and to sew the materials for a tunic. It was necessary also to fish for one's dress; for fleeces are obtained from the sea where shells of extraordinary size are furnished with tufts of mossy hair.\n\nSea silk has been suggested as an interpretation of the nature of the golden fleece that was sought by Jason and the Argonauts but scholars refute this hypothesis.\n\nSeveral sources mention \"lana pinna\" \"pinna wool\". Emperor Diocletian's (301 CE) \"Edict on Maximum Prices\" lists it as a valuable textile.\n\nThe Byzantine historian Procopius's (c. 550 CE) \"Persian War\", \"stated that the five hereditary satraps (governors) of Armenia who received their insignia from the Roman Emperor were given chlamys (or cloaks) made from \"lana pinna\". Apparently only the ruling classes were allowed to wear these chlamys.\"\n\nThe Arabic name for \"sea silk\" is \"ṣūf al-baḥr\" \"sea wool\". The 9th-century Persian geographer Estakhri notes that a sea-wool robe cost more than 1000 gold pieces and records its mythic source.\n\nAt a certain period of the year an animal is seen running out of the sea and rubbing itself against certain stones of the littoral, whereupon it deposes a kind of wool of silken hue and golden colour. This wool is very rare and highly esteemed, and nothing of it is allowed to waste.\n\nTwo 13th-century authors, Ibn al-Baitar and Zakariya al-Qazwini, repeat this inaccurate \"sea wool\" story.\n\nBeginning in the Eastern Han dynasty (25–220 CE), Chinese histories document importing sea silk. Chinese language names include \"cloth from the west of the sea\" and \"mermaid silk\".\n\nThe (3rd century CE) \"Weilüe\" \"Brief Account of the Wei\", which was an unofficial history of the Cao Wei empire (220-265 CE), records \"haixi\" 海西 \"West of the Sea\" cloth made from \"shuiyang\" 水羊 \"water sheep\".\n\nThey have fine brocaded cloth that is said to be made from the down of \"water-sheep\". It is called \"Haixi\" (\"Egyptian\") cloth. This country produces the six domestic animals [traditionally: horses, cattle, sheep, chickens, dogs and pigs], which are all said to come from the water. It is said that they not only use sheep's wool, but also bark from trees, or the silk from wild silkworms, to make brocade, mats, pile rugs, woven cloth and curtains, all of them of good quality, and with brighter colours than those made in the countries of Haidong (\"East of the Sea\").\n\nThe (c. 5th century CE) \"Hou Hanshu\" \"Book of the Eastern Han\" expresses doubt about \"water sheep\" in the \"Products of Daqin (the Roman Empire)\" section. \"They also have a fine cloth which some people say is made from the down of 'water sheep,' but which is made, in fact, from the cocoons of wild silkworms\". The historian Fan Ye (398–445 CE), author of the \"Hou Hanshu\", notes this section's information comes from the report that General Ban Yong 班勇 (son of General Ban Chao 班超, 32-102 CE) presented to the Emperor in 125. Both Bans administered the Western Regions on the Silk Road.\n\nThe (945 CE) \"Tang shu\" \"Book of Tang\" mentioned \"Haixi\" cloth from Folin 佛菻 \"Syria\", which Emil Bretschneider first identified as sea silk from Greece. \"There is also a stuff woven from the hair of sea-sheep, and called \"hai si pu\" (stuff from the western sea)\". He notes, \"This is, perhaps, the \"Byssus\", a clothstuff woven up to the present time by the Mediterranean coast, especially in Southern Italy, from the thread-like excrescences of several sea-shells, (especially \"Pinna nobilis\").\"\n\nThe (early 6th century CE) \"Shuyiji\" 遹異記 \"Records of Strange Things\" mentions silk woven by \"Jiaoren\" 蛟人 \"jiao\"-dragon people\", which Edward H. Schafer identifies as sea silk.\nIn the midst of the South Sea are the houses of the \"kău\" people who dwell in the water like fish, but have not given up weaving at the loom. Their eyes have the power to weep, but what they bring forth is pearls. \nThis aquatic type of raw silk was called \"jiaoxiao\" 蛟綃, i.e. \"mermaid silk\" or \"jiaonujuan \"蛟女絹, \"mermaid women's silk\".\n\nThe earliest usage of the English name \"sea silk\" remains uncertain, but the \"Oxford English Dictionary\" defines \"sea-silkworm\" as \"a bivalve mollusc of the genus \"Pinna\".\"\n\nAlexander Serov's 1863 opera \"Judith\" includes an aria \"I shall don my robe of byssus\" (Я оденусь в виссон).\n\nIn Jules Verne's 1870 novel \"20,000 Leagues Under the Sea\", the crew of the Nautilus wear clothes made of byssus (alternately translated as \"seashell tissue\" or \"fan-mussel fabric\").\n\n\"Pinna nobilis\" has become threatened with extinction, partly due to overfishing, the decline in seagrass fields, and pollution. As it has declined so dramatically, the once small but vibrant sea silk industry has almost disappeared, and the art is now preserved only by a few women on the island of Sant'Antioco near Sardinia. Chiara Vigo claimed on various media to be the sole person living today to master the art of working with byssus and the local people \nhelped her to open the Sea Silk Museum in Sant'Antioco which was closed following a building code dispute with the local government. The 'Project Sea-Silk' from the Natural History Museum of Basel is collecting extensive data and studies on the subject, and informs the public that a couple of other women still produce and work today with byssus in Sant'Antioco in Sardinia, such as the sisters Assuntina e Giuseppina Pes which contradicts the claims of Chiara Vigo who is credited as having \"invented with an extraordinary imagination her own story of sea-silk and [spinning] it tirelessly and to the delight of all media on and on\". In 2013, Efisia Murroni, a 100 year old sea-silk master weaver nicknamed \"la signora del bisso\" (born in 1913) died and her work is now shown in the Museo Etnografico di Sant'Antioco, with other artefacts being already on display in various museums throughout Europe.\n\n\n"}
{"id": "12093668", "url": "https://en.wikipedia.org/wiki?curid=12093668", "title": "Siemens Energy Sector", "text": "Siemens Energy Sector\n\nThe Siemens Energy Sector, founded on January 1, 2008, is one of the four sectors of Siemens. The company generates and delivers power from numerous sources including the extraction, conversion and transport of oil and natural gas in addition to renewable and alternative energy sources.\n\nAs of October 1, 2014, the sector level has been eliminated, including the Siemens Energy Sector.\n\nThe Siemens Energy Sector consists of the following divisions:\n\n\nSiemens has acquired the following companies:\n\nPartners:\n\nIn financial year 2013 the Energy Sector earned approximately €26.6 billion in revenue and had 83,500 employees.\n\n"}
{"id": "10501599", "url": "https://en.wikipedia.org/wiki?curid=10501599", "title": "Société Nationale d'électricité du Burkina Faso", "text": "Société Nationale d'électricité du Burkina Faso\n\nSociété Nationale d'électricité du Burkina Faso (SONABEL) is the national electricity company of Burkina Faso. The company represents Burkina Faso in the West African Power Pool.\n\n"}
{"id": "8249499", "url": "https://en.wikipedia.org/wiki?curid=8249499", "title": "Steam carousel", "text": "Steam carousel\n\nThe Steam carousel is:\n\nSpecific examples of the steam carousel are:\n"}
{"id": "30309", "url": "https://en.wikipedia.org/wiki?curid=30309", "title": "Thallium", "text": "Thallium\n\nThallium is a chemical element with symbol Tl and atomic number 81. It is a gray post-transition metal that is not found free in nature. When isolated, thallium resembles tin, but discolors when exposed to air. Chemists William Crookes and Claude-Auguste Lamy discovered thallium independently in 1861, in residues of sulfuric acid production. Both used the newly developed method of flame spectroscopy, in which thallium produces a notable green spectral line. Thallium, from Greek , , meaning \"a green shoot or twig\", was named by Crookes. It was isolated by both Lamy and Crookes in 1862; Lamy by electrolysis, and Crookes by precipitation and melting of the resultant powder. Crookes exhibited it as a powder precipitated by zinc at the International exhibition, which opened on 1 May that year.\n\nThallium tends to oxidize to the +3 and +1 oxidation states as ionic salts. The +3 state resembles that of the other elements in group 13 (boron, aluminium, gallium, indium). However, the +1 state, which is far more prominent in thallium than the elements above it, recalls the chemistry of alkali metals, and thallium(I) ions are found geologically mostly in potassium-based ores, and (when ingested) are handled in many ways like potassium ions (K) by ion pumps in living cells.\n\nCommercially, thallium is produced not from potassium ores, but as a byproduct from refining of heavy-metal sulfide ores. Approximately 60–70% of thallium production is used in the electronics industry, and the remainder is used in the pharmaceutical industry and in glass manufacturing. It is also used in infrared detectors. The radioisotope thallium-201 (as the soluble chloride TlCl) is used in small, nontoxic amounts as an agent in a nuclear medicine scan, during one type of nuclear cardiac stress test.\n\nSoluble thallium salts (many of which are nearly tasteless) are toxic, and they were historically used in rat poisons and insecticides. Use of these compounds has been restricted or banned in many countries, because of their nonselective toxicity. Thallium poisoning usually results in hair loss, although this characteristic symptom does not always surface. Because of its historic popularity as a murder weapon, thallium has gained notoriety as \"the poisoner's poison\" and \"inheritance powder\" (alongside arsenic).\n\nA thallium atom has 81 electrons, arranged in the electron configuration [Xe]4f5d6s6p; of these, the three outermost electrons in the sixth shell are valence electrons. Due to the inert pair effect, the 6s electron pair is relativistically stabilised and it is more difficult to get them involved in chemical bonding than for the heavier elements. Thus, very few electrons are available for metallic bonding, similar to the neighboring elements mercury and lead, and hence thallium, like its congeners, is a soft, highly electrically conducting metal with a low melting point of 304 °C.\n\nA number of standard electrode potentials, depending on the reaction under study, are reported for thallium, reflecting the greatly decreased stability of the +3 oxidation state:\nThallium is the first element in group 13 where the reduction of the +3 oxidation state to the +1 oxidation state is spontaneous under standard conditions. Since bond energies decrease down the group, by thallium, the energy released in forming two additional bonds and attaining the +3 state is not always enough to outweigh the energy needed to involve the 6s-electrons. Accordingly, thallium(I) oxide and hydroxide are more basic and thallium(III) oxide and hydroxide are more acidic, showing that thallium conforms to the general rule of elements being more electropositive in their lower oxidation states.\n\nThallium is malleable and sectile enough to be cut with a knife at room temperature. It has a metallic luster that, when exposed to air, quickly tarnishes to a bluish-gray tinge, resembling lead. It may be preserved by immersion in oil. A heavy layer of oxide builds up on thallium if left in air. In the presence of water, thallium hydroxide is formed. Sulfuric and nitric acid dissolve thallium rapidly to make the sulfate and nitrate salts, while hydrochloric acid forms an insoluble thallium(I) chloride layer.\n\nThallium has 25 isotopes which have atomic masses that range from 184 to 210. Tl and Tl are the only stable isotopes and make up nearly all of natural thallium. Tl is the most stable radioisotope, with a half-life of 3.78 years. It is made by the neutron activation of stable thallium in a nuclear reactor. The most useful radioisotope, Tl (half-life 73 hours), decays by electron capture, emitting X-rays (~70–80 keV), and photons of 135 and 167 keV in 10% total abundance; therefore, it has good imaging characteristics without excessive patient radiation dose. It is the most popular isotope used for thallium nuclear cardiac stress tests.\n\nThallium(III) compounds resemble the corresponding aluminium(III) compounds. They are moderately strong oxidizing agents and are usually unstable, as illustrated by the positive reduction potential for the Tl/Tl couple. Some mixed-valence compounds are also known, such as TlO and TlCl, which contain both thallium(I) and thallium(III). Thallium(III) oxide, TlO, is a black solid which decomposes above 800 °C, forming the thallium(I) oxide and oxygen.\n\nThe simplest possible thallium compound, thallane (TlH), is too unstable to exist in bulk, both due to the instability of the +3 oxidation state as well as poor overlap of the valence 6s and 6p orbitals of thallium with the 1s orbital of hydrogen. The trihalides are more stable, although they are chemically distinct from those of the lighter group 13 elements and are still the least stable in the whole group. For instance, thallium(III) fluoride, TlF, has the β-BiF structure rather than that of the lighter group 13 trifluorides, and does not form the complex anion in aqueous solution. The trichloride and tribromide disproportionate just above room temperature to give the monohalides, and thallium triiodide contains the linear triiodide anion () and is actually a thallium(I) compound. Thallium(III) sesquichalcogenides do not exist.\n\nThe thallium(I) halides are stable. In keeping with the large size of the Tl cation, the chloride and bromide have the caesium chloride structure, while the fluoride and iodide have distorted sodium chloride structures. Like the analogous silver compounds, TlCl, TlBr, and TlI are photosensitive. The stability of thallium(I) compounds demonstrates its differences from the rest of the group: a stable oxide, hydroxide, and carbonate are known, as are many chalcogenides.\n\nThe double salt has been shown to have hydroxyl-centred triangles of thallium, , as a recurring motif throughout its solid structure.\n\nOrganothallium compounds tend to be thermally unstable, in concordance with the trend of decreasing thermal stability down group 13. The chemical reactivity of the Tl–C bond is also the lowest in the group, especially for ionic compounds of the type RTlX. Thallium forms the stable [Tl(CH)] ion in aqueous solution; like the isoelectronic Hg(CH) and [Pb(CH)], it is linear. Trimethylthallium and triethylthallium are, like the corresponding gallium and indium compounds, flammable liquids with low melting points. Like indium, thallium cyclopentadienyl compounds contain thallium(I), in contrast to gallium(III).\n\nThallium (Greek , , meaning \"a green shoot or twig\") was discovered by flame spectroscopy in March 1861. The name comes from thallium's bright green spectral emission lines.\n\nAfter the publication of the improved method of flame spectroscopy by Robert Bunsen and Gustav Kirchhoff and the discovery of caesium and rubidium in the years 1859 to 1860, flame spectroscopy became an approved method to determine the composition of minerals and chemical products. William Crookes and Claude-Auguste Lamy both started to use the new method. William Crookes used it to make spectroscopic determinations for tellurium on selenium compounds deposited in the lead chamber of a sulfuric acid production plant near Tilkerode in the Harz mountains. He had obtained the samples for his research on selenium cyanide from August Hofmann years earlier. By 1862, Crookes was able to isolate small quantities of the new element and determine the properties of a few compounds. Claude-Auguste Lamy used a spectrometer that was similar to Crookes' to determine the composition of a selenium-containing substance which was deposited during the production of sulfuric acid from pyrite. He also noticed the new green line in the spectra and concluded that a new element was present. Lamy had received this material from the sulfuric acid plant of his friend Fréd Kuhlmann and this by-product was available in large quantities. Lamy started to isolate the new element from that source. The fact that Lamy was able to work ample quantities of thallium enabled him to determine the properties of several compounds and in addition he prepared a small ingot of metallic thallium which he prepared by remelting thallium he had obtained by electrolysis of thallium salts.\n\nAs both scientists discovered thallium independently and a large part of the work, especially the isolation of the metallic thallium was done by Lamy, Crookes tried to secure his own priority on the work. Lamy was awarded a medal at the International Exhibition in London 1862: \"For the discovery of a new and abundant source of thallium\" and after heavy protest Crookes also received a medal: \"thallium, for the discovery of the new element.\" The controversy between both scientists continued through 1862 and 1863. Most of the discussion ended after Crookes was elected Fellow of the Royal Society in June 1863.\nThe dominant use of thallium was the use as poison for rodents. After several accidents the use as poison was banned in the United States by Presidential Executive Order 11643 in February 1972. In subsequent years several other countries also banned its use.\n\nAlthough thallium is a modestly abundant element in the Earth's crust, with a concentration estimated to be about 0.7 mg/kg, mostly in association with potassium-based minerals in clays, soils, and granites, thallium is not generally economically recoverable from these sources. The major source of thallium for practical purposes is the trace amount that is found in copper, lead, zinc, and other heavy-metal-sulfide ores.\nThallium is found in the minerals crookesite TlCuSe, hutchinsonite TlPbAsS, and lorándite TlAsS. Thallium also occurs as a trace element in iron pyrite, and thallium is extracted as a by-product of roasting this mineral for the production of sulfuric acid.\n\nThallium can also be obtained from the smelting of lead and zinc ores. Manganese nodules found on the ocean floor contain some thallium, but the collection of these nodules has been prohibitively expensive. There is also the potential for damaging the oceanic environment. In addition, several other thallium minerals, containing 16% to 60% thallium, occur in nature as complexes of sulfides or selenides that primarily contain antimony, arsenic, copper, lead, and/or silver. These minerals are rare, and they have had no commercial importance as sources of thallium. The Allchar deposit in southern Macedonia was the only area where thallium was actively mined. This deposit still contains an estimated 500 tonnes of thallium, and it is a source for several rare thallium minerals, for example lorándite.\n\nThe United States Geological Survey (USGS) estimates that the annual worldwide production of thallium is about 10 metric tonnes as a by-product from the smelting of copper, zinc, and lead ores. Thallium is either extracted from the dusts from the smelter flues or from residues such as slag that are collected at the end of the smelting process. The raw materials used for thallium production contain large amounts of other materials and therefore a purification is the first step. The thallium is leached either by the use of a base or sulfuric acid from the material. The thallium is precipitated several times from the solution to remove impurities. At the end it is converted to thallium sulfate and the thallium is extracted by electrolysis on platinum or stainless steel plates. The production of thallium decreased by about 33% in the period from 1995 to 2009 – from about 15 metric tonnes to about 10 tonnes. Since there are several small deposits or ores with relatively high thallium content, it would be possible to increase the production if a new application, such as a hypothetical thallium-containing high-temperature superconductor, becomes practical for widespread use outside of the laboratory.\n\nThe odorless and tasteless thallium sulfate was once widely used as rat poison and ant killer. Since 1972 this use has been prohibited in the United States due to safety concerns. Many other countries followed this example in subsequent years. Thallium salts were used in the treatment of ringworm, other skin infections and to reduce the night sweating of tuberculosis patients. This use has been limited due to their narrow therapeutic index, and the development of improved medicines for these conditions.\n\nThallium(I) bromide and thallium(I) iodide crystals have been used as infrared optical materials, because they are harder than other common infrared optics, and because they have transmission at significantly longer wavelengths. The trade name KRS-5 refers to this material. Thallium(I) oxide has been used to manufacture glasses that have a high index of refraction. Combined with sulfur or selenium and arsenic, thallium has been used in the production of high-density glasses that have low melting points in the range of 125 and 150 °C. These glasses have room temperature properties that are similar to ordinary glasses and are durable, insoluble in water and have unique refractive indices.\n\nThallium(I) sulfide's electrical conductivity changes with exposure to infrared light therefore making this compound useful in photoresistors. Thallium selenide has been used in a bolometer for infrared detection. Doping selenium semiconductors with thallium improves their performance, thus it is used in trace amounts in selenium rectifiers. Another application of thallium doping is the sodium iodide crystals in gamma radiation detection devices. In these, the sodium iodide crystals are doped with a small amount of thallium to improve their efficiency as scintillation generators. Some of the electrodes in dissolved oxygen analyzers contain thallium.\n\nResearch activity with thallium is ongoing to develop high-temperature superconducting materials for such applications as magnetic resonance imaging, storage of magnetic energy, magnetic propulsion, and electric power generation and transmission. The research in applications started after the discovery of the first thallium barium calcium copper oxide superconductor in 1988. Thallium cuprate superconductors have been discovered that have transition temperatures above 120 K. Some mercury-doped thallium-cuprate superconductors have transition temperatures above 130 K at ambient pressure, nearly as high as the world-record-holding mercury cuprates.\n\nBefore the widespread application of technetium-99m in nuclear medicine, the radioactive isotope thallium-201, with a half-life of 73 hours, was the main substance for nuclear cardiography. The nuclide is still used for stress tests for risk stratification in patients with coronary artery disease (CAD). This isotope of thallium can be generated using a transportable generator, which is similar to the technetium-99m generator. The generator contains lead-201 (half-life 9.33 hours), which decays by electron capture to the thallium-201. The lead-201 can be produced in a cyclotron by the bombardment of thallium with protons or deuterons by the (p,3n) and (d,4n) reactions.\n\nA thallium stress test is a form of scintigraphy, where the amount of thallium in tissues correlates with tissue blood supply. Viable cardiac cells have normal Na/K ion-exchange pumps. The Tl cation binds the K pumps and is transported into the cells. Exercise or dipyridamole induces widening (vasodilation) of arteries in the body. This produces coronary steal by areas where arteries are maximally dilated. Areas of infarct or ischemic tissue will remain \"cold\". Pre- and post-stress thallium may indicate areas that will benefit from myocardial revascularization. Redistribution indicates the existence of coronary steal and the presence of ischemic coronary artery disease.\n\nA mercury–thallium alloy, which forms a eutectic at 8.5% thallium, is reported to freeze at −60 °C, some 20 °C below the freezing point of mercury. This alloy is used in thermometers and low-temperature switches. In organic synthesis, thallium(III) salts, as thallium trinitrate or triacetate, are useful reagents for performing different transformations in aromatics, ketones and olefins, among others. Thallium is a constituent of the alloy in the anode plates of magnesium seawater batteries. Soluble thallium salts are added to gold plating baths to increase the speed of plating and to reduce grain size within the gold layer.\n\nA saturated solution of equal parts of thallium(I) formate (Tl(CHO)) and thallium(I) malonate (Tl(CHO)) in water is known as Clerici solution. It is a mobile, odorless liquid which changes from yellowish to colourless upon reducing the concentration of the thallium salts. With a density of 4.25 g/cm at 20 °C, Clerici solution is one of the heaviest aqueous solutions known. It was used in the 20th century for measuring the density of minerals by the flotation method, but its use has discontinued due to the high toxicity and corrosiveness of the solution.\nThallium iodide is frequently used as an additive in metal-halide lamps, often together with one or two halides of other metals. It allows optimization of the lamp temperature and color rendering, and shifts the spectral output to the green region, which is useful for underwater lighting.\n\nThallium and its compounds are extremely toxic, and should be handled with care. There are numerous recorded cases of fatal thallium poisoning. The Occupational Safety and Health Administration (OSHA) has set the legal limit (permissible exposure limit) for thallium exposure in the workplace as 0.1 mg/m skin exposure over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) also set a recommended exposure limit (REL) of 0.1 mg/m skin exposure over an 8-hour workday. At levels of 15 mg/m, thallium is immediately dangerous to life and health.\n\nContact with skin is dangerous, and adequate ventilation should be provided when melting this metal. Thallium(I) compounds have a high aqueous solubility and are readily absorbed through the skin. Exposure by inhalation should not exceed 0.1 mg/m in an 8-hour time-weighted average (40-hour work week). Thallium will readily absorb through the skin, and care should be taken to avoid this route of exposure, as cutaneous absorption can exceed the absorbed dose received by inhalation at the permissible exposure limit (PEL). Thallium is a suspected human carcinogen. For a long time thallium compounds were readily available as rat poison. This fact and that it is water-soluble and nearly tasteless led to frequent intoxication caused by accident or criminal intent.\n\nOne of the main methods of removing thallium (both radioactive and normal) from humans is to use Prussian blue, a material which absorbs thallium. Up to 20 grams per day of Prussian blue is fed by mouth to the person, and it passes through their digestive system and comes out in the stool. Hemodialysis and hemoperfusion are also used to remove thallium from the blood serum. At later stage of the treatment additional potassium is used to mobilize thallium from the tissue.\n\nAccording to the United States Environmental Protection Agency (EPA), man-made sources of thallium pollution include gaseous emission of cement factories, coal-burning power plants, and metal sewers. The main source of elevated thallium concentrations in water is the leaching of thallium from ore processing operations.\n\n"}
{"id": "21487040", "url": "https://en.wikipedia.org/wiki?curid=21487040", "title": "Uranyl carbonate", "text": "Uranyl carbonate\n\nUranyl carbonate, UO(CO), is a carbonate of uranium that forms the backbone of several uranyl mineral species such as andersonite, mckelveyite-(Y) and wyartite and most importantly rutherfordine. It is also found in both the mineral and organic fractions of coal and its fly ash and is the main component of uranium in mine tailing seepage water.\n\nUranium like other actinides readily forms a dioxide uranyl core (UO). In the environment, this uranyl core readily complexes with carbonate to form charged complexes. Although uranium forms insoluble solids or adsorbs to mineral surfaces at alkaline pH it is these soluble carbonate complexes that increase its solubility, availability, and mobility with low affinities to soil. Uranium(VI) generally forms a pH-dependent suite of uranyl carbonate and various hydrated complexes in ground water solutions.\n\n\nA common method for concentrating uranium from a solution uses solutions of uranyl carbonates, which are passed through a resin bed where the complex ions are transferred to the resin by ion exchange with a negative ion like chloride. After build-up of the uranium complex on the resin, the uranium is eluted with a salt solution and the uranium is precipitated in another process.\nUranyl carbonate complexes form a large class of mineral species. Several have been described in literature. These include:\n\n\n"}
{"id": "24093465", "url": "https://en.wikipedia.org/wiki?curid=24093465", "title": "Valea Nucarilor Wind Farm", "text": "Valea Nucarilor Wind Farm\n\nThe Valea Nucarilor Wind Farm is a wind power project in Valea Nucarilor, Tulcea County, Romania. It has 80 individual wind turbines with a nominal output of around 0.75 MW which will deliver up to 70 MW of power, enough to power over 36,000 homes, with a capital investment required of approximately US$75 million.\n"}
{"id": "3384048", "url": "https://en.wikipedia.org/wiki?curid=3384048", "title": "WASH-740", "text": "WASH-740\n\nWASH-740 was a report published by the U.S. Atomic Energy Commission (USAEC) in 1957. This report, called \"Theoretical Possibilities and Consequences of Major Accidents in Large Nuclear Power Plants\" (also known as \"The Brookhaven Report\"), estimated maximum possible damage from a meltdown with no containment building at a large nuclear reactor.\n\nThe conclusions of this study estimated the possible effects of a \"maximum credible accident\" for nuclear reactors then envisioned as being 3400 deaths, 43,000 injuries and property damage of $7 billion ($57bn adjusted for inflation in 2012 since 1957). The estimate of probability was one in a hundred thousand to one in a billion per reactor-year. When WASH-740 was revised in 1964-65 to account for the larger reactors then being designed, the new figures indicated that there could be as many as 45,000 deaths, 100,000 injuries, and $17 billion in property damage ($125bn adjusted for inflation since 1964).\n\nHowever, the assumptions underlying the results were unrealistic (including the worst meteorological conditions, no containment building, and that half the reactor core is released into the atmosphere as micrometre-sized pellets without any examination of how this might occur). These were due to conservatism (estimating the maximum possible damage) and the need to use atomic bomb fallout data, which had been collected from tests (computers in 1955 being greatly insufficient to do the calculations).\n\nAs knowledge, models and computers improved the conclusions of this report were replaced by those of first WASH-1400 (1975, The Rasmussen Report), then CRAC-II (1982), and most recently NUREG-1150 (1991). Now all of these studies are considered obsolete (see the disclaimer to NUREG-1150), and are being replaced by the State-of-the-Art Reactor Consequence Analyses (SOARCA) study.\n\n\n"}
{"id": "10495617", "url": "https://en.wikipedia.org/wiki?curid=10495617", "title": "Wood-decay fungus", "text": "Wood-decay fungus\n\nA wood-decay fungus is any species of fungus that digests moist wood, causing it to rot. Some species of wood-decay fungi attack dead wood, such as brown rot, and some, such as \"Armillaria\" (honey fungus), are parasitic and colonize living trees. Excessive moisture above the fibre saturation point in wood is required for fungal colonization and proliferation. Fungi that not only grow on wood but permeate its fibrous structure and actually cause decay, are called lignicolous fungi. In nature, this process causes the breakdown of complex molecules and leads to return nutrients to the soil. Various lignicolous fungi consume wood in various ways; for example, some attack the carbohydrates in wood and some others decay lignin. The rate of decay of wooden materials in various climates can be estimated by empirical models. \n\nWood-decay fungi can be classified according to the type of decay that they cause. The best-known types are brown rot, soft rot, and white rot. Each produce different enzymes, can degrade different plant materials, and can colonise different environmental niches. The residual products of decomposition from fungal action have variable pH, solubility and redox potentials. Over time this residue will become incorporated in the soil and sediment, so can have a noticeable effect on the environment of that area.\n\nBrown-rot fungi break down hemicellulose and cellulose that form the wood structure. Cellulose is broken down by hydrogen peroxide (HO) that is produced during the breakdown of hemicellulose. Because hydrogen peroxide is a small molecule, it can diffuse rapidly through the wood, leading to a decay that is not confined to the direct surroundings of the fungal hyphae. As a result of this type of decay, the wood shrinks, shows a brown discoloration, and cracks into roughly cubical pieces, a phenomenon termed cubical fracture. The fungi of certain types remove cellulose compounds from wood and hence the wood becomes brown colour.\n\nBrown rot in a dry, crumbly condition is sometimes incorrectly referred to as \"dry rot\" in general. The term \"brown rot\" replaced the general use of the term \"dry rot\", as wood must be damp to decay, although it may become dry later. Dry rot is a generic name for certain species of brown-rot fungi.\n\nBrown-rot fungi of particular economic importance include \"Serpula lacrymans\" (true dry rot), \"Fibroporia vaillantii\" (mine fungus), and \"Coniophora puteana\" (cellar fungus), which may attack timber in buildings. Other brown-rot fungi include the sulfur shelf, \"Phaeolus schweinitzii\", and \"Fomitopsis pinicola\".\n\nBrown-rot fungal decay is characterised by extensive demethylation of lignins whereas white-rot tends to produce low yields of molecules with demethylated functional groups.\n\nThere are very few brown rot fungi in tropical climates or in southern temperate zones. Most brown rot fungi have a geographical range north of the Tropic of Cancer (23.5° latitude), and most of these are found north of the 35° latitude, corresponding to a roughly boreal distribution. Those brown rot fungi between latitudes 23.5° and 35° are typically found at high elevations in pine forest regions, or in coniferous forest regions such as the Rocky Mountains or the Himalayas.\n\nSoft-rot fungi secrete cellulase from their hyphae, an enzyme that breaks down cellulose in the wood. This leads to the formation of microscopic cavities inside the wood, and sometimes to a discoloration and cracking pattern similar to brown rot. Soft-rot fungi need fixed nitrogen in order to synthesize enzymes, which they obtain either from the wood or from the environment. Examples of soft-rot-causing fungi are \"Chaetomium\", \"Ceratocystis\", and \"Kretzschmaria deusta\".\n\nSoft-rot fungi are able to colonise conditions that are too hot, cold or wet for brown or white-rot to inhabit. They can also decompose woods with high levels of compounds that are resistant to biological attack. Bark in woody plants contains a high concentration of tannin, which is difficult for fungi to decompose, and suberin which may act as a microbial barrier. The bark acts as form of protection for the more vulnerable interior of the plant.\nSoft-rot fungi do not tend to be able to decompose matter as effectively as white-rot fungi: they are less aggressive decomposers.\n\nWhite-rot fungi break down the lignin in wood, leaving the lighter-colored cellulose behind; some of them break down both lignin and cellulose. As a result, the wood changes texture, becoming moist, soft, spongy, or stringy; its colour becomes white or yellow. Because white-rot fungi are able to produce enzymes, such as laccase, needed to break down lignin and other complex organic molecules, they have been investigated for use in mycoremediation applications.\n\nThere are many different enzymes that are involved in the decay of wood by white-rot fungi, some of which directly oxidize lignin. The relative abundance of phenylpropane alkyl side chains of lignin characteristically decreases when decayed by white-rot fungi. It has been reported that the oyster mushroom (\"Pleurotus ostreatus\") preferentially decays lignin instead of polysaccharides. This is different from some other white-rot fungi, e.g., \"Phanerochaete chrysosporium\", which shows no selectivity to lignocellulose.\n\nHoney mushroom (\"Armillaria spp.\") is a white-rot fungus notorious for attacking living trees. \"Pleurotus ostreatus\" and other oyster mushrooms are commonly cultivated white-rot fungi, but \"P. ostreatus\" is not parasitic and will not grow on a living tree, unless it is already dying from other causes. Other white-rot fungi include the turkey tail, artist's conk, and tinder fungus.\n\nWhite-rot fungi are grown all over the world as a source of food – for example the shiitake mushroom, which in 2003 comprised approximately 25% of total mushroom production.\n\n\n"}
{"id": "52089813", "url": "https://en.wikipedia.org/wiki?curid=52089813", "title": "Yttre Stengrund offshore wind farm", "text": "Yttre Stengrund offshore wind farm\n\nYttre Stengrund was an offshore wind farm in Sweden, operated by Vattenfall. The wind farm was commissioned in 2001, using five 2 MW NEG Micon turbines. \nAll turbines in Yttre Stengrund were decommissioned by November 2015, becoming the first offshore wind farm to be decommissioned in the world.\n"}
{"id": "57816009", "url": "https://en.wikipedia.org/wiki?curid=57816009", "title": "ZKZM-500", "text": "ZKZM-500\n\nThe ZKZM-500 is a laser gun developed in China since 2018 by the Xian Institute of Optics and Precision Mechanics at the Chinese Academy of Sciences in Shaanxi.\n\nIt can reportedly destroy a target 800 meters away by charring skin and human tissue. The frequency of the laser makes it invisible to the human eye and it does not produce noise. It is powered by a rechargeable lithium battery and can shoot more than 1,000 shots per charge.\n\nIt is now ready for mass production and the first units are likely to be given to anti-terrorism squads in the People's Armed Police.\n\nThere is no specific international regulation for the development or use of this type of laser weapon. The Protocol on Blinding Laser Weapons, signed by more than 100 nations, focuses on previous generations of weapons and prohibits the use of weapons that can cause blindness. It is classified as a non-lethal weapon on a Chinese government website.\n\nSkeptics have noted that the ZKZM-500 would require a battery weighing several hundred pounds in order to achieve the rifle's described capabilities.\n\nThe manufacturer of the ZKZM-500 has since released a video demonstration of the weapon's effects against cardboard, pork, and a rubber tire.\n\n"}
