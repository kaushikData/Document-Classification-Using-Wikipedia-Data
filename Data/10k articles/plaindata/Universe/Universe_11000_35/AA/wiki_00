{"id": "44358555", "url": "https://en.wikipedia.org/wiki?curid=44358555", "title": "15-Hydroxyeicosatetraenoic acid", "text": "15-Hydroxyeicosatetraenoic acid\n\n15-Hydroxyeicosatetraenoic acid (also termed 15-HETE, 15(\"S\")-HETE, and 15\"S\"-HETE) is an eicosanoid, i.e. a metabolite of arachidonic acid. Various cell types metabolize arachidonic acid to 15(\"S\")-hydroperoxyeicosatetraenoic acid (15(\"S\")-HpETE). This initial hydroperoxide product is extremely short-lived in cells: if not otherwise metabolized, it is rapidly reduced to 15\"(S)\"-HETE. Both of these metabolites, depending on the cell type which forms them, can be further metabolized to 15-oxo-eicosatetraenoic acid (15-oxo-ETE), 5\"S\",15\"S\"-dihydroxy-eicosatetraenoic acid (5(\"S\"),15(\"S\")-diHETE), 5-oxo-15(\"S\")-hydroxyeicosatetraenoic acid (5-oxo-15(\"S\")-HETE, a subset of specialized pro-resolving mediators viz., the lipoxins, a class of pro-inflammatory mediators, the eoxins, and other products that have less well-defined activities and functions. Thus, 15(\"S\")-HETE and 15(\"S\")-HpETE, in addition to having intrinsic biological activities, are key precursors to numerous biologically active derivatives.\n\nSome cell types (e.g. platelets) metabolize arachidonic acid to the stereoisomer of 15(\"S\")-HpETE, 15(\"R\")-HpETE. Both stereoisomers may also be formed as result of the metabolism of arachidonic acid by cellular microsomes or as a result of arachidonic acid auto-oxidation. Similar to 15(\"S\"\")-HpETEs, 15(\"R\")-HpETE may be rapidly reduced to 15(\"R\")-HETE. These \"R,S\" stereoisomers differ only in having their hydroxy residue in opposite orientations. While the two \"R\" stereoisomers are sometimes referred to as 15-HpETE and 15-HETE, proper usage should identify them as \"R\" stereoisomers. 15(\"R\")-HpETE and 15(\"R\")-HETE lack some of the activity attributed to their \"S\" stereoisomers but can be further to metabolized to bioactive products viz., the 15(\"R\") class of lipoxins (also termed epi-lipoxins).\n\n15(\"S\")-HETE, 15(\"S\")-HpETE, and many of their derivative metabolites are thought to have physiologically important functions. They appear to act as hormone-like autocrine and paracrine signalling agents that are involved in regulating inflammatory and perhaps other responses. Clinically, drugs that are stable analogs, and therefor mimic the anti-inflammatory actions of the lipoxins and drugs that block the production or actions of the pro-inflammatory eoxins may prove useful for treating acute and chronic inflammatory disorders.\n\n15(\"S\")-HETE is unambiguously designated by a shortened version of its IUPAC name viz., 15(\"S\")-hydroxy-5\"Z\",8\"Z\",11\"Z\",13\"E\"-eicosatetraenoic acid. In this terminology \"S\" refers to the absolute configuration of the chirality of the hydroxy functional group at carbon position 15. Its 15(\"R\") enantiomer is designated 15(\"R\")-hydroxy-5\"Z\",8\"Z\",11\"Z\",13\"E\"-eicosatetraenoic acid. \"Z\" and \"E\" give the cis–trans isomerism about each double bond moiety at carbon positions 5, 8, 11, and 13 with Z indicating cis and E indicating trans isomerism. Both stereoisomers are produced from their corresponding \"S\" and \"R\" 15-HpETE stereoisomers, i.e. 15(\"S\")-hydroperoxy-5\"Z\",8\"Z\",11\"Z\",13\"E\"-eicosatetraenoic acid (15(S)-HpETE) and (15\"R\")-hydroperoxy-5\"Z\",8\"Z\",11\"Z\",13\"E\"-eicosatetraenoic acid (15(R)-HpETE).\n\nHuman cells release arachidonic acid (i.e. 5\"Z\",8\"Z\",11\"Z\",14\"Z\"-eicosatetraenoic acid) from its storage site in phospholipids by reactions that involve phospholipase C and/or lipase enzymes. This release is stimulated or enhanced by cell stimulation. The freed arachidonic acid is then converted to 15-hydroperoxy/hydroxy products by one or more of the following five pathways.\n\n15-Lipoxygenase-1: Cells metabolize arachidonic acid with 15-lipoxygenase-1 (i.e., 15-LO-1, ALOX15) to form 15(\"S\")-HpETE as a major product and 12(\"S\")-hydroperoxy-5\"Z\",8\"Z\",10\"E\",15\"Z\"-eicosatetraenoic acid (12(\"S\")-HpETE) and 14(\"S\"),15(\"S\")-\"trans\"-oxido-5\"Z\",8\"Z\",11\"Z\"-14,15-leukotriene A4 as minor products; 15(\"S\")-HpETE and 12(\"S\")-HpETE are rapidly converted to 15(\"S\")-HETE and 12(\"S\")-hydroxy-5\"Z\",8\"Z\",10\"E\",15\"Z\"-eicosatetraenoic acid (12(\"S\")-hydroxyeicosatetraenoic acid), (i.e. 12(\"S\")-HETE), respectively, or further metabolized through other enzyme pathways; 14(\"S\"),15(\"S\")-\"trans\"-oxido-5\"Z\",8\"Z\",11\"Z\"-14,15-leukotriene A is metabolized by 15-LO-1 to various isomers of 8,15(\"S\")-dihydroxy-5\"S\",8\"S\",\"11Z\",13\"S\"-eicosatetraenoic acids, e.g. 8,15(S)-LTB's.\n\n15-Lipooxygenase-2: Cells also used 15-lipoxygenase 2 (i.e. 15-LOX-2 or ALOX15B) to make 15(\"S\")-HpETE and 15(\"S\")-HETE. However this enzyme has a preference for metabolizing linoleic acid rather than arachidonic acid. It therefore forms linoleic acid metabolites (e.g. 13-hydoxyperoxy/hydroxy-octadecadienoic and 9-hydroperoxy/hydroxyl-octadecadienoic acids) in greater amounts than 15(\"S\")-HpETE and 15(\"S\")-HETE. 15-LOX-2 also differs from 15-LOX-1 in that it does not make 12(\"S\")-HpETE or the leukotriene A isomer cited above.\n\nCycloxygenase: Cells can use prostaglandin-endoperoxide synthase 1 (i.e. cyclooxygenenase-1 or COX-1) and Prostaglandin-endoperoxide synthase 2 (COX-2) to metabolize arachidonic acid primarily to prostaglandins but also to small amounts of 11(\"R\")-HETE and a racemic mixture of 15-HETEs composed of ~22% 15(\"R\")-HETE and ~78% 15(\"S\")-HETE. When pretreated with aspirin, however, COX-1 is inactive while COX-2 attacks arachidonic acid to produce almost exclusively 15(\"R\")-HETE along with its presumed precursor 15(\"R\")-HpETE.\n\nMicrosome metabolism: Human and rat microsomal cytochrome P450s, e.g. CYP2C19, metabolize arachidonic acid to a racemic mixture of 15-HETEs, i.e., 15(\"R\",\"S\")-HETEs, >90% of which is the 15(\"R\") stereoisomer.\n\nAutoxidation: The spontaneous and non-enzymatically-induced autoxidation of arachidonic acid yields 15(\"R\",\"S\")-hydroperoxy-5\"Z\",8\"Z\",11\"Z\",13\"E\"-eicosatetraenoic acids. This non-enzymatic reaction is promoted in cells undergoing oxidative stress. Cells forming this racemic mixture of 15-hydroperoxy products may convert then to 15(\"R,S\")-HETEs and other products. However, the uncontrolled overproduction of the 15-hydroperoxy products may react with other elements to produce cell injury.\n\nThe newly formed products formed by the pathways cited in the previous section are bioactive but may also flow into down-stream pathways to form other metabolites with a different sets of bioactivity. The initially formed 15(\"S\")-HpETE may be further metabolized by its parent cell or pass it to nearby cell by a process termed transcellular metabolism.\n\n15(\"S\")-HpETE may be: \n\n15(\"S\")-HETE may be:\n\n15(\"R\")-HpETE may be:\n\n15(\"R\")-HETE may be:\n\nMost studies have analyzed the action of 15(\"S\")-HETE but not that of its less stable precursor 15(\"S\")-HpETE. Since this precursor is rapidly converted to 15(\"S\")-HETE in cells, it is likely that the two metabolites share similar activities. In many studies, however, is not clear that these activities reflect their intrinsic action or reflect their conversion to the metabolites sited above.\n\n15(\"S\")-HpETE and 15(\"S\")-HETE bind to and activate the G protein-coupled receptor, Leukotriene B4 receptor 2, i.e. BLT2. This receptor activation may mediate, at least in part, certain cell-stimulating activities of the two metabolites. BLT2 may be responsible in part or whole for mediating the growth-promoting and anti-apoptosis (i.e. anti-cell death) activities of 15(S)-HETE in cultured human breast cancer cells;<ref name=\"PLoS One. 2013 May 2;8(5):e63076. doi: 10.1371/journal.pone.0063076\" ></ref> human cancer colon cells, human hepatocellular HepG2 and SMMC7721 cancer cells; mouse 3T3 cells (a fibroblast cell line); rat PA adventitia fibroblasts; Baby hamster kidney cells; and diverse types of vascular endothelial cells. These growth-stimulating effects could contribute to the progression of the cited cancer types in animal models or even humans and the excess fibrosis that causes the narrowing of pulmonary arteries in hypoxia-induced pulmonary hypertension or narrowing of portal arteries in the portal hypertension accompanying liver cirrosis. 15(\"S\")-HETE may also act through BLT2 to stimulate an immediate contractile response in rat pulmonary arteries and its angiogenic effect on human umbilical and dermal vascular endothelial cells.\n\n15(\"S\")-HpETE and 15(\"S\")-HETE also directly bind with and activate peroxisome proliferator-activated receptor gamma. This activation may contribute to the ability of 15(S)-HETE to inhibit the growth of cultured human prostate cancer PC-3, LNCaP, and DU145 cell lines and non-malignant human prostate cells; lung adenocarcinoma A549 cells; human colorectal cancer cells; corneal epithelial cells; and Jurkat T-cell leukemia cells. The decline in the level of 15(\"S\")-HpETE-forming enzymes and consequential fall in cellular 15-HETE production that occurs in human prostate cancer cells may be one mechanism by which this and perhaps other human cancer cells (e.g. those of the colon, rectum, and lung) avoid the apoptosis-inducing actions of 15(\"S\")-HpETE and/or 15(\"S\")-HETE and thereby proliferate and spread. In this scenario, 15(S)-HETE and one of its formaing enzymes, particularly 15-LOX-2, appear to act as tumor suppressors.\n\nSome of the inhibitory effects of 15(\"S\")-HpETE and 15(\"S\")-HETE, particularly when induced by high concentrations (e.g. >1-10 micromolar), may be due to a less specific mechanism: 15(\"S\")-HpETE and to a lesser extent 15(\"S\")-HETE induce the generation of Reactive oxygen species. These species trigger cells to activate their death programs, i.e. apoptosis, and/or are openly toxic to the cells. 15(\"S\")-HpETE and 15(S)-HETE inhibit angiogenesis and the growth of cultured human chronic myelogenous leukemia K-562 cells by a mechanism that is associated with the production of reactive oxygen species.\n\nSeveral bifuctional electrophilic breakdown products of 15(\"S\")-HpETE, e.g. 4-hydroxy-2(\"E\")-nonenal, 4-hydroperoxy-2(\"E\")-nonenal, 4-oxo-2(\"E\")-nonenal, and \"cis\"-4,5-epoxy-2(\"E\")-decanal, are mutagens in mammalian cells and thereby may contripute to the development and/or progression of human cancers.\n\nSimilar to 15(\"S\")-HpETE and 15(\"S\")-HETE and with similar potency, 15(\"R\")-HETE binds with and activates peroxisome proliferator-activated receptor gamma. The precursor of 15(\"R\")-HETE, 15(\"R\")-HpETE may, similar to 15(\"S\")-HpETE, break down to the mutagenic products 4-hydroxy-2(\"E\")-nonenal, 4-hydroperoxy-2(\"E\")-nonenal, 4-oxo-2(\"E\")-nonenal, and \"cis\"-4,5-epoxy-2(\"E\")-decanal and therefore be involved in cancer development and/or progression.\n\nIn cultured human monocytes of the THP1 cell line, 15-oxo-ETE inactivates IKKβ (also known as IKK2) thereby blocking this cell's NF-κB-mediated pro-inflammatory responses (e.g.. Lipopolysaccharide-induced production of TNFα, Interleukin 6, and IL1B) while concurrently activating anti-oxidant responses upregulated through the anti-oxidant response element (ARE) by forcing cytosolic KEAP1 to release NFE2L2 which then moves to the nucleus, binds ARE, and induces production of, e.g. hemoxygenase-1, NADPH-quinone oxidoreductase, and possibly glutamate-cysteine ligase modifier. By these actions, 15-oxo-ETE may dampen inflammatory and/or Oxidative stress responses. In a cell-free system, 15-oxo-ETE is a moderately potent (IC=1 μM) inhibitor of 12-lipoxygenase but not other human lipoxygenases. This effect could also have anti-inflammatory and anti-oxidative effects by blocking the formation of 12-HETE and Hepoxilins. 15-Oxo-ETE is an example of an α,β unsaturated ketone Electrophile. These ketones are highly reactive with nucleophiles, adducting to, for example, the cysteines in transcription and transcription-related regulatory factors and enzymes to form their alkylated and thereby often inactivated products. It is presumed that the preceding activities of 15-oxo-ETE reflect its adduction to the indicated elements. 15-Oxo-ETE, at 2-10 μM, also inhibits the proliferation of cultured Human umbilical vein endothelial cells and LoVo human colorectal cancer cells and at the extremely high concentration of 100 μM inhibits the proliferation of cultured MBA-MD-231 and MCF7 breast cancer cells as well as SKOV3 ovarian cancer cells. They may use a similar \"protein-adduction\" mechanism; if so the target protein(s) for these effects have not been defined or even suggested. This 15-oxo-ETE action may prove to inhibit the remodeling of blood vessels and reduce the growth of the cited cell types and cancers. At sub-micromolar concentrations, 15-oxo-ETE has weak Chemotaxis activity for human monocytes and could serve to recruit this White blood cell into inflammatory responses.\n\n5-Oxo-15(S)-hydroxy-ETE is properly a member of the 5-HETE family of agonists which binds to the Oxoeicosanoid receptor 1, a G protein-coupled receptor, to activate its various target cells. As such, it is a potent stimulator of leukocytes, particularly eosinophils, as well as other OXE1-bearing cells including MDA-MB-231, MCF7, and SKOV3 cancer cells (see 5-Hydroxyicosatetraenoic acid and 5-oxo-eicosatetraenoic acid). \nIt also binds with and activates PPARγ and thereby can stimulate or inhibit cells independently of OXE1.\n\nLXA4, LXB4, AT-LXA4, and AT-LXB4 are specialized proresolving mediators, i.e. they potently inhibit the progression and contribute to the resolution of diverse inflammatory and allergic reactions (see specialized proresolving mediators#lipoxins and Lipoxins).\n\nEoxin A4, Eoxin C4, Eoxin D4, and Eoxin E4 and analogs of leukotriene A4, C4, leukotriene D4, and E4. Formation of the leukotrienes is initiated by 5-lipoxygenase metabolism of arachidonic acid to form a 5,6-epoxide viz, leukotriene A4; the latter metabolite is then converted to C4, D4, and E4 in succession. Formation of the eoxins is initiated by a 15-lipoxyenase-mediated metabolism of arachiconic acid to a 14,15-epoxide, eoxin A4 followed by its serial conversion to epoxins C4, D4, and E4 using the same pathways and enzymes that metabolize leukotriene A4 to its down-stream products. Preliminary studies have found that the eoxins have pro-inflammatory actions, suggest that they are involved in severe asthma, aspirin-induced asthma attacks, and perhaps other allergic reactions. The production of eoxins by Reed-Sternburg cells has also led to suggestion that they are involve in the lymphoma of Hodgkins disease. Drugs blocking the 15-lipoxygenases may be useful for inhibiting inflammation by reducing the production of the eoxins.\n\n\n"}
{"id": "47972269", "url": "https://en.wikipedia.org/wiki?curid=47972269", "title": "4,4′-Diamino-2,2′-stilbenedisulfonic acid", "text": "4,4′-Diamino-2,2′-stilbenedisulfonic acid\n\n4,4′-Diamino-2,2′-stilbenedisulfonic acid is the organic compound with the formula (HNCHSOH)CH. It is a white, water-soluble solid. Structurally, it is a derivative of trans-stilbene, containing amino and sulfonic acid functional groups on each of the two phenyl rings. \n\nThe compound is a popular optical brightener for use in laundry detergents. \n\nIt is produced by reduction of 4,4′-dinitro-2,2′-stilbenedisulfonic acid with iron powder.\n"}
{"id": "13610801", "url": "https://en.wikipedia.org/wiki?curid=13610801", "title": "All in This Tea", "text": "All in This Tea\n\nAll in This Tea is a 2007 documentary film co-directed by Les Blank and Gina Leibrecht, about Chinese tea. It follows the American tea connoisseur David Lee Hoffman as he travels to remote tea-growing areas of China. Hoffman attempts to interest Chinese tea growers and distributors in fair trade issues, and explores the importance of terroir and organic growing methods in both the quality and future sustainability of the Chinese tea market.\n\nThe film premiered at the San Francisco International Film Festival in 2007.\n\nIt was filmed with a hand-held camera on digital video and is 70 minutes in length.\n\nOn Rotten Tomatoes, the film has a rating of 83%, based on 12 reviews, with an average rating of 7/10.\n\n\n"}
{"id": "232102", "url": "https://en.wikipedia.org/wiki?curid=232102", "title": "Angle of attack", "text": "Angle of attack\n\nIn fluid dynamics, angle of attack (AOA, or formula_1) is the angle between a reference line on a body (often the chord line of an airfoil) and the vector representing the relative motion between the body and the fluid through which it is moving. Angle of attack is the angle between the body's reference line and the oncoming flow. This article focuses on the most common application, the angle of attack of a wing or airfoil moving through air.\n\nIn aerodynamics, angle of attack specifies the angle between the chord line of the wing of a fixed-wing aircraft and the vector representing the relative motion between the aircraft and the atmosphere. Since a wing can have twist, a chord line of the whole wing may not be definable, so an alternate reference line is simply defined. Often, the chord line of the root of the wing is chosen as the reference line. Another choice is to use a horizontal line on the fuselage as the reference line (and also as the longitudinal axis). Some authors do not use an arbitrary chord line but use the zero lift axis where, by definition, zero angle of attack corresponds to zero coefficient of lift.\n\nSome British authors have used the term angle of incidence instead of angle of attack. However, this can lead to confusion with the term \"riggers' angle of incidence\" meaning the angle between the chord of an airfoil and some fixed datum in the airplane.\n\nThe lift coefficient of a fixed-wing aircraft varies with angle of attack. Increasing angle of attack is associated with increasing lift coefficient up to the maximum lift coefficient, after which lift coefficient decreases.\n\nAs the angle of attack of a fixed-wing aircraft increases, separation of the airflow from the upper surface of the wing becomes more pronounced, leading to a reduction in the rate of increase of the lift coefficient. The figure shows a typical curve for a cambered straight wing. A symmetrical wing has zero lift at 0 degrees angle of attack. The lift curve is also influenced by the wing shape, including its airfoil section and wing planform. A swept wing has a lower, flatter curve with a higher critical angle.\n\nThe critical angle of attack is the angle of attack which produces maximum lift coefficient. This is also called the \"stall angle of attack\". Below the critical angle of attack, as the angle of attack increases, the lift coefficient increases. Conversely, above the critical angle of attack, as angle of attack increases, the air begins to flow less smoothly over the upper surface of the airfoil and begins to separate from the upper surface. On most airfoil shapes, as the angle of attack increases, the upper surface separation point of the flow moves from the trailing edge towards the leading edge. At the critical angle of attack, upper surface flow is more separated and the airfoil or wing is producing its maximum lift coefficient. As angle of attack increases further, the upper surface flow becomes more fully separated and the lift coefficient reduces further.\n\nAbove this critical angle of attack, the aircraft is said to be in a stall. A fixed-wing aircraft by definition is stalled at or above the critical angle of attack rather than at or below a particular airspeed. The airspeed at which the aircraft stalls varies with the weight of the aircraft, the load factor, the center of gravity of the aircraft and other factors. However the aircraft always stalls at the same critical angle of attack. The critical or stalling angle of attack is typically around 15° - 20° for many airfoils.\n\nSome aircraft are equipped with a built-in flight computer that automatically prevents the aircraft from increasing the angle of attack any further when a maximum angle of attack is reached, regardless of pilot input. This is called the 'angle of attack limiter' or 'alpha limiter'. Modern airliners that have fly-by-wire technology avoid the critical angle of attack by means of software in the computer systems that govern the flight control surfaces.\n\nIn takeoff and landing operations from short runways, such as Naval Aircraft Carrier operations and STOL back country flying, aircraft may be equipped with angle of attack or Lift Reserve Indicators. These indicators measure the angle of attack (AOA) or the Potential of Wing Lift (POWL, or Lift Reserve) directly and help the pilot fly close to the stalling point with greater precision. STOL operations require the aircraft to be able to operate close to the critical angle of attack during landings and at the best angle of climb during takeoffs. Angle of attack indicators are used by pilots for maximum performance during these maneuvers since airspeed information is only indirectly related to stall behaviour.\n\nSome military aircraft are able to achieve controlled flight at very high angles of attack, but at the cost of massive induced drag. This provides the aircraft with great agility. A famous military example is sometimes thought to be Pugachev's Cobra. Although the aircraft experiences high angles of attack throughout the maneuver, the aircraft is not capable of either aerodynamic directional control or maintaining level flight until the maneuver ends. The Cobra is an example of supermaneuvering as the aircraft's wings are well beyond the critical angle of attack for most of the maneuver.\n\nAdditional aerodynamic surfaces known as \"high-lift devices\" including leading edge wing root extensions allow fighter aircraft much greater flyable 'true' alpha, up to over 45°, compared to about 20° for aircraft without these devices. This can be helpful at high altitudes where even slight maneuvering may require high angles of attack due to the low density of air in the upper atmosphere as well as at low speed at low altitude where the margin between level flight AoA and stall AoA is reduced. The high AoA capability of the aircraft provides a buffer for the pilot that makes stalling the airplane (which occurs when critical AoA is exceeded) more difficult. However, military aircraft usually do not obtain such high alpha in combat, as it robs the aircraft of speed very quickly due to induced drag, and in extreme cases, increased frontal area and parasitic drag. Not only do such maneuvers slow the aircraft down, but they cause significant structural stress at high speed. Modern flight control systems tend to limit a fighter's angle of attack to well below its maximum aerodynamic limit.\n\nIn sailing, the physical principles involved are the same as for aircraft. A sail's angle of attack is the angle between the sail's chord line and the direction of the relative wind.\n\nA boat's angle of attack is the angle between the boat's course and the wind direction. See points of sail.\n\n\n"}
{"id": "1208403", "url": "https://en.wikipedia.org/wiki?curid=1208403", "title": "Australian Atomic Energy Commission", "text": "Australian Atomic Energy Commission\n\nThe Australian Atomic Energy Commission (AAEC) was a statutory body of the Australian government.\n\nIt was established in 1952, replacing the Atomic Energy Policy Committee. In 1981 parts of the Commission were split off to become part of CSIRO, the remainder continuing until 1987, when it was replaced by the Australian Nuclear Science and Technology Organisation (ANSTO). The Commission head office was in Coogee, and its main facilities were at the Atomic Energy Research Establishment at Lucas Heights, established in 1958.\n\nHighlights of the Commission's history included:\n\n\nOther significant facilities constructed by the Commission at Lucas Heights included a 3MeV Van de Graaff particle accelerator, installed in 1964 to provide proton beams and now upgraded to become ANTARES, a smaller 1.3MeV betatron, and radioisotope production and remote handling facilities associated with HIFAR reactor.\n\nSignificant research work included:\n\n"}
{"id": "1374117", "url": "https://en.wikipedia.org/wiki?curid=1374117", "title": "Avery Weigh-Tronix", "text": "Avery Weigh-Tronix\n\nAvery Weigh-Tronix is a company specialising in weighing machines. Its headquarters stands on the site of the Soho Foundry overlooking Black Patch Park in Smethwick, England, with a United States-based manufacturing and retail manufacturing plant.\n\nAvery Weigh-Tronix (AWTX UK) is the UK’s largest manufacturer of scales and weighing equipment. AWTX is a company specialising in the service, maintenance, calibration and sales of weighing equipment. AWTX also offer nationwide cover for scale and IT-based product installation, repair and maintenance through a network of service centres and technicians.\n\nIt was formed in June 2000 when United States weighing company Weigh-Tronix acquired the Avery Berkel group of businesses, bringing together the four brands of W & T Avery, Berkel, Salter and Weigh-Tronix. Avery Weigh-Tronix is one of the world’s largest manufacturers of weighing equipment.\n\nOn the Soho Foundry site the company runs one of the very few Museums of Weighing in the world; admission by appointment only.\n\nThe Illinois Tool Works or ITW (NYSE: ITW) is a Fortune 200 company that produces engineered fasteners and components, equipment and consumable systems, and specialty products. It was founded in 1912 by Byron L. Smith.\n\nThe consolidation of the group was finally completed in September 2003, when the change of name to Avery Weigh-Tronix created a single corporate entity to service today’s global markets.\n\nAvery Weigh-Tronix also sells products under the brand name Salter Brecknell: scales are sold through a dealer-distributor network and direct to some retailers, including mechanical and consumer products.\n\n"}
{"id": "19927923", "url": "https://en.wikipedia.org/wiki?curid=19927923", "title": "Cemesto", "text": "Cemesto\n\nCemesto is a sturdy, light-weight, waterproof and fire-resistant composite building material made from a core of sugar cane fiber insulating board surfaced on both sides with asbestos and cement. Its name is a portmanteau word combining \"cem\" from \"cement\" and \"esto\" from \"asbestos.\" A type of prefabricated home using this material came to be called \"cemestos\".\n\nCemesto was introduced by the Celotex Company in 1937. It was manufactured in the form of boards and panels that were wide, about thick, and to long. Each by panel weighs just . Cemesto was used primarily for interior and exterior walls.\n\nThe John B. Pierce Foundation and Celotex collaborated to develop a prefabrication system for building low-cost housing using cemesto panels, in which single cemesto panels were slid horizontally into light wooden frames to create walls. A prototype cemesto house was displayed at the 1939 World's Fair in New York City. The Pierce system was first used in 1941 for building employee housing at the Glenn L. Martin Aircraft Company near Baltimore, Maryland. For this development, named Aero Acres, the architecture firm of Skidmore, Owings & Merrill designed gable-roofed Cape Cod houses with dimensions of by , featuring large commercial-style windows in their principal rooms. In 1941 a total of 600 homes were built at Aero Acres using this design.\n\nDuring World War II, when other building materials were in short supply, cemesto was used extensively in the United States. Cemesto was used to build temporary office buildings in Washington, D.C. Skidmore, Owings, and Merrill adapted the Pierce system and used cemesto panels for the designs of some 2,500 pre-fabricated homes, known by the nickname \"cemestos,\" erected in Oak Ridge, Tennessee, to house Manhattan Project workers and their families. In 1942 the U.S. Farm Security Administration built 400 cemesto homes in Maryland at a site alongside Aero Acres.\n\nDuring the 1940s, the manufacturer of cemesto touted it as a material that would in the future make it possible to mass-produce housing at a low cost. One use of the material during the post-war era was in the late 1940s in Circle Pines, Minnesota, where cemesto panels were used in building the first homes in what was envisioned to be a cooperative housing community for people of color. The use of cemesto in Circle Pines came to be regarded as substandard construction, as the builders failed to adequately seal the joints between cemesto panels.\n\nSeveral prominent architects embraced cemesto as a modern material and used it in their designs. For the Bousquet-Wightman House in Houston, Texas, built in 1941, architect Donald Barthelme used cemesto panels for exterior sheathing. In 1949 Edward Durell Stone called for cemesto panels in the design of a home to be built in Armonk, New York. That same year, Charles Eames designed his Eames House, Case Study House #8, to use brightly painted and unfinished Cemesto panels in a prefabricated steel frame. Frank Lloyd Wright designed the Raymond Carlson House in Phoenix, Arizona, built in 1950, to use a structural system of wood posts and cemesto boards. In the Arthur Pieper House in Paradise Valley, Arizona, built in 1952 from concrete block, Wright used cemesto for the ceilings.\n\nIn addition to houses and office buildings, cemesto was used to build gasoline stations and factories.\n"}
{"id": "53152064", "url": "https://en.wikipedia.org/wiki?curid=53152064", "title": "Chrysler minivans (RT)", "text": "Chrysler minivans (RT)\n\nThe RT-platform Chrysler minivans are a series of passenger minivans marketed by Chrysler starting in model year 2008, the fifth in six generations of Chrysler minivans. Depending on the market, these vans were known as the Dodge Grand Caravan, Chrysler Town & Country, Chrysler Grand Voyager, Lancia Voyager and the Volkswagen Routan, a modified version sold by Volkswagen in North America. Only long wheelbase models were offered with the Dodge Journey replacing the short wheelbase model. While most versions were discontinued in 2016 with the launch of the Chrysler Pacifica minivan, the Grand Caravan remains in production as a value-priced legacy model in North America.\n\nChrysler debuted the 2008 model year minivans at the 2007 Detroit Auto Show — eliminating the short-wheelbase model. With discontinuation of the short-wheelbase minvans, Dodge offered the Journey on nearly an identical wheelbase and as a crossover rather than a minivan. Although the SWB model, which had accounted for half of all sales in Canada, cost approximately $2,000 less and offered a four-cylinder engine option with improved fuel economy, Chrysler executives stated the SWB Caravan was discontinued to accommodate new features offered in the Grand Caravan, consistent with the demands of the majority of the minivan market.\n\nThe RT minivans were a completely new design, sharing few components with the previous model. They abandoned the rounded styling introduced on the NS vans, instead using a more boxy look styled by Ralph Gilles. New features included standard electronic stability control, the MyGIG entertainment system (a stereo with built in hard drive for recording, storing, and playing music), second and third row video screens, powered second row windows, standard side curtain airbags, and dashboard-mounted transmission controls. Much like its competitors, the Toyota Sienna and Honda Odyssey, the Town & Country now featured power windows on the sliding doors and moved the gear shift from the steering column to the center console, in a higher position. Another new feature of this generation was an available rear overhead console which featured LED map lights as well as halo ambient lighting. A new DVD system was also available, which featured dual screens for the rear passengers. SIRIUS Backseat TV was also offered, which featured three channels of children's programming. The \"Stow 'n Go\" seats, carried over from the previous generation, were standard, with a new seating system marketed as \"Swivel 'n Go\" optional on higher end models. In this seating system, two full size second row seats swivel to face the third row. A detachable table can be placed between the second and third row seats. The Swivel 'n Go seating system includes the 3rd row seating from the Stow 'n Go system, but not the second row, which must be manually removed from the vehicle. This model was exported as the Dodge Grand Caravan and Chrysler Town & Country to the Philippines, Chrysler Town & Country to Central and South America, and Chrysler Grand Voyager to all other markets.\n\nThe Volkswagen Routan is a seven-seat rebadged variant of the RT minivans with revised styling, content features and suspension tuning. Manufactured alongside the Chrysler and Dodge minivans at Windsor Assembly and marketed in the United States, Canada, and Mexico, the Routan debuted at the 2008 Chicago Auto Show and went on sale in the United States in September 2008. The Routan's minivan variants include the Dodge Caravan, Ram C/V, Chrysler Town & Country, and Chrysler Grand Voyager (export)—that by 2009 have ranked as the 13th bestselling automotive nameplate worldwide, with over 12 million sold.\n\nThe Routan marked the start of Volkswagen's business strategy to offer additional vehicles specially developed for the U.S. market. The introduction of the 2008 model year minivan resulted from a partnership that began in 2005 between Volkswagen and DaimlerChrysler. Prior to the agreement, Volkswagen had no minivan model for the United States or Canadian markets. The Routan is sold only in North America (U.S., Canada, Mexico).\n\nThe automaker's intent with outsourcing production the Routan to Chrysler was to avoid the significant expense of developing its own family-sized minivan. VW announced in an early 2008 projection that the company intended for the Routan and other models to help achieve significant expansion of U.S. sales. The Routan was Volkswagen's first van offered in North America since discontinuation of the Volkswagen Eurovan in 2003, and is not related to the European-market Volkswagen Touran.\n\nThe Routan features a rebranded version of Chrysler's hard drive-based audio and navigation system—marketed by Chrysler as the MyGig system and by Volkswagen as the Joybox, but has neither Chrysler's Stow'n Go nor Swivel'n Go seating systems. Instead, the second row seats in the Routan feature the Easy Out Roller Seat system, but can be modified using Chrysler or Dodge parts to have Stow'n Go or have Swivel'n Go seats installed. Routans as of 2010 offer optional Wi-Fi access, which was also offered in Dodge and Chrysler versions as UConnect Web.\n\nVW of America had projected for the Routan to gain at least five percent of the U.S. minivan market, or 45,000 units of the 700,000 minivans sold currently. In January 2009, VW of America asked Chrysler Canada to stop production of the Routan for the month of February after 29,000 Routans had been shipped to US dealerships. By July 2009, 11,677 units had been sold. In 2012, Volkswagen halted production of the Routan at Chrysler's Windsor, Ontario, plant, despite having a production contract that ran through 2014. In January 2013, Volkswagen announced there would be no 2013 retail model, but held open the possibility that development may resume with a potential 2014 model. The 2013 Routan was reserved for fleet purchasers, and 2,500 were produced by Chrysler during the calendar year.\n\nAutomotive industry analysts were not surprised by VW's decision to drop the Routan because buyers had no reason for selecting the Routan over the similar Dodge Grand Caravan or the Chrysler Town & Country, and the Routan's base price of nearly $28,000 was far more than the basic $21,000 Grand Caravan, while the Routan's list of equipment was less than included on the upscale Town & Country.\n\nThe Chrysler minivans underwent a mid-cycle refresh for the 2011 model year, which included major changes in both styling and functionality. Changes included restyled exterior and interior with all-new wing logo, standard SafetyTec (including Blind Spot Monitoring and Rear Cross Path Detection), improvement to the Stow n Go seating and storage system, a one-touch fold down feature for easier access to the third row, (The Swivel n' Go seats were dropped) a new super center console and technology, a dual DVD system that can play different media at the same time, SIRIUS Backseat TV which offers three channels of childrens programming, FLO TV featuring 20 channels of live programming, Pentastar V6 (283 hp) engine replacing previous 3.8-liter and 3.3-liter V-6 engines, six-speed automatic transmission, a new fuel economizer mode, a new instrument panel and instrument cluster, new Chrysler Brand steering wheel with integrated controls that allow the driver to operate the radio, cruise control, hands-free phone and other vehicle functions while keeping their hands on the wheel; upgraded cloth and leather seating materials; new soft touch door trim, new heating and cooling control system. The suspension was heavily re-tuned, with both Dodge and Chrysler minivans gaining a larger front sway bar and new rear sway bar, increased rear roll center height, adjusted spring rates, a new steering gear, a revised front static camber setting, and lowered ride height. This dramatically improved handling in both the Chrysler and Dodge. Other changes included extra sound insulation, acoustic glass, new seats, softer-touch surfaces, new LED ambient lighting and center console, and halogen projector headlamps with LED accents. The Chrysler models were adjusted so that instead of competing against equivalent Dodge trim levels, they were above Dodge in trim and features.\n\nAll three of the former engine choices were replaced by the new Pentastar 3.6-liter V6 with six-speed automatic transmission, now the sole powertrain choice for all models. Interior trim was restyled on both vans, in addition to major exterior revisions highlighted by the new \"double-crosshair\" grille on the Grand Caravan and a new chrome grille for the Town & Country.\n\nChrysler Voyagers sold in continental Europe was rebranded as the Lancia Voyager. The Chrysler-branded variant continued to be sold in the United Kingdom, Ireland, Russia, Australia, New Zealand, South Korea, Singapore, and China, as Lancia does not have sales operations in those markets.\n\nThe Ram Cargo Tradesman, or Ram C/V Tradesman, debuted for the 2012 model year, replacing the Dodge Grand Caravan C/V. It is based on the Dodge Grand Caravan, but with solid metal instead of rear windows and a flat load space with of interior storage, and a . cargo payload plus a towing capability of up to . The Ram C/V is offered with a 3.6-liter Pentastar V-6 engine and 6-speed automatic transmission. The C/V Tradesman was discontinued after the 2015 model year in favor of the ProMaster City.\n\nIn the U.S. the National Highway Traffic Safety Administration's (NHTSA) New Car Assessment Program crash testing, the 2010 Dodge Grand Caravan achieved a five star (top safety) rating in several categories.\n\nThe 2.4 L \"EDZ\" I4 was dropped, with the 3.3 L \"EGA\" V6 standard. The 3.8 L \"EGH\" V6 engine was optional on mid level models, with a 4.0L SOHC V6 engine optional as the top level engine. A 2.8 L \"RA428\" I4 diesel was available on export models, while the 3.3L wasn't offered in Europe. The 3.3L got a 4 speed Ultradrive automatic transmission, while all the other engines came with a new 6 speed 62TE automatic transmission. In 2011, the 3.3L, 3.8L and 4.0L engines were all dropped, replaced with the \"Pentastar\" 3.6 L V6, which came with the 62TE transmission.\n\nGrand Caravan:\nAutomotive news reported that, from January to October in 2010, Dodge sold about a third of its 2010 Grand Caravans to rental fleets. The number of returned ex-rental 2010 Grand Caravan to the market jumped fourfold between July to October, depressing prices of used 2009 and 2010 Dodge minivans by as much as 20%.\n\nTown & Country:\n\nFor 2018, the Dodge Grand Caravan is only available in three trim levels: SE, SE Plus, and SXT. The SXT Plus and GT trim levels have been discontinued for 2018, according to Dodge's Online Build-and-Price Tool. All trim levels get new standard equipment, some of which was previously optional for those trim levels.\n\nThe base SE, starting at $25,995 includes standard equipment such as: front and second-row rear power windows and rear third-row quarter vent windows, keyless entry, manually-sliding side rear doors, seventeen-inch black-painted steel wheels with plastic wheel covers, the U Connect 430 radio (A/M-F/M radio, SiriusXM Satellite Radio, single-disc CD/MP3/DVD/DVD Audio player, USB input, 3.5-millimeter (3.5mm) auxiliary audio input jack, 6.5-inch color touch screen display, and Bluetooth with streaming audio functionality and voice control), a multifunction steering wheel with controls for cruise control, voice command, and audio system, a six-speaker audio system, premium cloth upholstery, manually-adjustable front bucket seats, \"Stow-'n'-Go\" System, front and rear tri-zone HVAC controls, second and third-row fold-in-floor bench seats, and front and rear side SRS airbags.\n\nThe mid-range SE Plus, starting at $28,760, adds features such as seventeen-inch aluminum-alloy wheels, second-row rear captain's chairs, security system, power-sliding second-row rear doors, leather-wrapped steering wheel, power rear tailgate, and exterior color-keyed accents to the base SE trim. This trim level takes the place of both the mid-range SXT and SXT Plus trims.\n\nThe top-of-the-line SXT, starting at $31,495, adds features such as luxury leather-trimmed seating surfaces, heated front bucket seats, electronic climate control system, and a power-folding third-row rear bench seat to the SE Plus trim. This trim level takes the place of the previous top-of-the-line GT trim.\n\nAll trim levels offer an optional U Connect 430N radio with GPS navigation, SiriusXM Travel Link Service and a rear seat, single-screen DVD entertainment system with wireless headphones (n/a with the U Connect 430N radio).\n\nSE Plus and SXT trim levels also offer a Blacktop Package, which adds a blacked-out front upper grille, darkened front head and rear tail lamps, black-finished seventeen-inch aluminum-alloy wheels, and an all-black interior color scheme.\n\nOptions that have been deleted for the 2018 model year include the nine-speaker premium amplified surround-sound audio system, the dual-screen rear DVD entertainment system, and leather-and-suede seating surfaces.\n\nProduction began in November 2017 of 2018 model year Dodge Grand Caravans due to the newly-revised 2018 Federal Motor Vehicle Safety Standards, which now requires that all new vehicles sold in the United States include standard side-impact airbags, which the Grand Caravan only offered as an extra-cost option on some trim levels. Dodge did not start production of the 2018 Grand Caravan until November 2017 because of this. \n\nWith the all-new 2017 Chrysler Pacifica introduced to replace the Chrysler Town & Country, the Dodge Grand Caravan was due to be discontinued after the 2016 model year. However, it was later announced that the Grand Caravan would be sold alongside the Chrysler Pacifica unchanged from the 2016 model, with a simplified model lineup. \n\nAt the 2018 North American International Auto Show in Detroit, Michigan, Fiat CEO Sergio Marchionne said there may be a successor to the current Dodge Grand Caravan, which would be based on the Chrysler Pacifica, saying that the Grand Caravan successor would be \"a Caravan-like vehicle\" . \n\nMarchionne is quoted as saying, \"The replacement of the Caravan will be a Caravan-like vehicle. I need another minivan. It's going to be in line with the Pacifica architecture\".\n"}
{"id": "17542964", "url": "https://en.wikipedia.org/wiki?curid=17542964", "title": "Cobbled classics", "text": "Cobbled classics\n\nThe Cobbled classics are four cycling classics held in March and April. Cobblestones, like mountainous terrain, are important elements in courses of cycling. Many classic cycle races in northwestern Europe contain cobbled sections. The two Monuments of this race type are the Tour of Flanders and Paris–Roubaix, with over 20 cobbled sectors.\n\nThe first race with cobbled sections is Omloop Het Nieuwsblad, which traditionally opens the Belgian classics season, followed the next day by Kuurne–Brussels–Kuurne. Starting late March, the Flemish Cycling Week (\"Vlaamse Wielerweek\") kicks off the most important period for cobbled cycling classics. Currently it features the Dwars door Vlaanderen on Wednesday, the E3 Harelbeke on Friday, and Gent–Wevelgem on Sunday. During the following week, the stage-race Driedaagse van De Panne keeps the riders busy, concluding with the Monument Tour of Flanders on Sunday. The Scheldeprijs on the following Wednesday prepares the riders for the historical Paris–Roubaix (another Monument), which ends the cobbled classics.\n\nAmong the cobbled cycling races, the three most historical are usually held on consecutive Sundays in March and April: Gent–Wevelgem, Tour of Flanders and Paris–Roubaix. Gent–Wevelgem has lost a lot of its historical status due to the relative easiness of the route. The E3 Harelbeke is considered to be harder and thus better preparation for the Ronde and Roubaix. In 2012, both races received equal status on the UCI World Tour. In 2017, Omloop Het Nieuwsblad (the opening event of the Belgian cycling season, as well as the first race of the year in Northwestern Europe) and Dwars door Vlaanderen became World Tour races.\n\nIn 2012 Belgian rider Tom Boonen managed to win all four races in the same season, as the first and only rider to do so.\n\n"}
{"id": "25693060", "url": "https://en.wikipedia.org/wiki?curid=25693060", "title": "Donghai Bridge Wind Farm", "text": "Donghai Bridge Wind Farm\n\nThe Donghai Bridge Wind Farm is a 102 MW offshore wind farm close to the Donghai Bridge, Shanghai and is capable of powering 200,000 households. It started producing and transmitting power to the mainland grid on July 6, 2010. It is the first commercial offshore wind farm in China.\n\n\n"}
{"id": "14659170", "url": "https://en.wikipedia.org/wiki?curid=14659170", "title": "Eddie Kolb", "text": "Eddie Kolb\n\nEdward William \"Eddie\" Kolb (July 20, 1880 – October 1, 1949) was an American Major League Baseball pitcher from Cincinnati, Ohio, who pitched one game for the Cleveland Spiders. The Spiders that season were a horrible team, compiling a historically low win/loss record of 20-134. To finish off the season, the team ended with a 35 game road trip, losing 40 of their last 41 games. It was on the last game of the schedule, on October 15 (the second game of a doubleheader), that the team allowed a clerk at a local tobacco shop to pitch a game in exchange for a box of cigars. In that game, he pitched a complete game, giving up 18 hits, and 19 runs, 9 of which were earned. He did, however, did get one hit in four at bats.\n\nAfter the Cleveland Spiders folded after the season, and Eddie's short career came to end, he continued his enthusiastic involvement in baseball, playing and managing in several semi-professional leagues, spending his winters in Florida. At one point, he attempted to purchase the Montreal team of the New England League in , which turned out to be unsuccessful. Having spent more than 15 years in baseball, he settled in Calgary, Alberta, Canada, and ran a successful restaurant. He ran his restaurant for 22 years until he became heavily involved in the development of an oil field in Turner Valley. It was this involvement that eventually saw him named as the first secretary of the Alberta Petroleum Association, which became the Western Canada Petroleum Association. Eddie died at the age of 69 in Calgary, and was cremated.\n"}
{"id": "43424526", "url": "https://en.wikipedia.org/wiki?curid=43424526", "title": "Energy Policy (journal)", "text": "Energy Policy (journal)\n\nEnergy Policy is a monthly peer-reviewed academic journal covering research on energy policy and energy supply. It is published by Elsevier. According to the \"Journal Citation Reports\", the journal has a 2013 impact factor of 2.696.\n"}
{"id": "31998854", "url": "https://en.wikipedia.org/wiki?curid=31998854", "title": "Energy in the Netherlands", "text": "Energy in the Netherlands\n\nEnergy in the Netherlands describes energy and electricity production, consumption and import in the Netherlands. Electricity sector in the Netherlands is the main article of electricity in the Netherlands.\n\nTo reduce its greenhouse emissions, the government of the Netherlands is subsidizing a transition away from natural gas for all homes in the country by 2050. In Amsterdam, no new residential gas accounts are allowed as of July 1, 2018, and all homes in the city are expected to be converted by 2040. Electric stoves are expected to replace gas stoves.\n\nDistrict heating is expected to replace natural gas for the heating of buildings. The Amsterdam area is already supplied to some degree with heat from waste incineration. New sources are expected to include geothermal energy, surface waters, and data centers.\n\n\n"}
{"id": "5663769", "url": "https://en.wikipedia.org/wiki?curid=5663769", "title": "Enormous Toroidal Plasma Device", "text": "Enormous Toroidal Plasma Device\n\nThe Enormous Toroidal Plasma Device (ETPD) is an experimental physics device housed at the Basic Plasma Science Facility at UCLA. It previously operated as the Electric Tokamak (ET) between 1999 and 2006 and was noted for being the world's largest tokamak before being decommissioned due to the lack of support and funding. The machine was renamed to ETPD in 2009. At present, the machine is undergoing upgrades to be re-purposed into a general laboratory for experimental plasma physics research.\n\nThe Electric Tokamak (ET) was the last of a series of small tokamak machines built in 1998 under the direction of principal investigator and designer, Robert Taylor, a UCLA professor. The machine was designed to be a low field (0.25 T) magnetic confinement fusion device with a large aspect ratio. It is composed of 16 vacuum chambers made of 1-inch thick steel, with a major radius of 5 meters and a minor radius of 1 meter. The ET was the largest tokamak ever built at its time, with a vacuum vessel slightly bigger than that of the Joint European Torus.\n\nThe first plasma was achieved in January 1999. The ET is capable of producing a plasma current of 45 kiloamperes and can produce a core electron plasma temperature of 300 eV.\n\nFour sets of independent coils are necessary for OH (ohmic heating) current drive, vertical equilibrium field, plasma elongation and plasma shaping (D or reverse-D). The OH system provides 10 V·s using a 10 kA power supply. Up to 0.1 T of vertical field can be applied for horizontal control and this is more than sufficient for all plasma configurations, including high beta. An additional set of coils provide a small horizontal field to correct for error field and to stabilize the plasma vertically. All the coils are located outside the vessel and are constructed out of aluminium.\n\nA Rogowski probe outside the vessel and sets of Hall probes inside the vessel are used to monitor plasma current, position and shaping and are used in the control feedback loop. The poloidal system was designed using an in-house equilibrium code as well as a variety of other codes in order to cross-check computations and to assess the stability of the resulting plasma.\n\nLike most tokamaks, the machine uses a combination of RF heating and neutral beam injection to drive and shape the plasma.\n\nIn 2006, the ET had run out of funding and was decommissioned following the retirement of Taylor. Factors leading to loss of funding are attributed to the lack of extensive plasma diagnostics, its large size, and its place in the politics of fusion. When it was operating, the ET was funded mostly by the Department of Energy (DOE).\n\nIn 2009, the Electric Tokamak (ET) was renamed to the Enormous Toroidal Plasma Device (ETPD) and was re-purposed for basic plasma research. A lanthanium hexaboride (LaB) plasma source was developed for the ETPD (similar to the one used in the LArge Plasma Device), and is capable of producing a long column of magnetized plasma (~100 m) that winds itself multiple times along the toroidal axis of the machine. The plasma column was shown to be current-free and terminates on the neutral gas within the chamber without touching the machine walls.\n\nThe typical operational parameters of the ETPD are:\n\n\nThe ETPD is currently in the process of being upgraded (i.e. larger sources, better diagnostic capabilities) to support a wide range of plasma physics experiments.\n\n\n"}
{"id": "17904352", "url": "https://en.wikipedia.org/wiki?curid=17904352", "title": "Eolica Pantelimon Wind Farm", "text": "Eolica Pantelimon Wind Farm\n\nThe Eolica Pantelimon Wind Farm is a proposed wind power project in Pantelimon, Constanţa County, Romania. It will consist of two individual wind farms connected together. It will have 33 individual wind turbines with a nominal output of around 2 MW which will deliver up to 66 MW of power, enough to power over 42,000 homes, with a capital investment required of approximately US$80 million.\n"}
{"id": "57736318", "url": "https://en.wikipedia.org/wiki?curid=57736318", "title": "Force-sensing capacitor", "text": "Force-sensing capacitor\n\nA force-sensing capacitor is a material whose capacitance changes when a force, pressure or mechanical stress is applied. They are also known as \"force-sensitive capacitors\". They can provide improved sensitivity and repeatability compared to force-sensitive resistors but traditionally required more complicated electronics.\n\nTypical force sensitive capacitors are examples of parallel plate capacitors. For small deflections, there is a linear relationship between applied force and change in capacitance, which can be shown as follows:\n\nThe capacitance, formula_1, equals formula_2, where formula_3 is permeability, formula_4 is the area of the sensor and formula_5 is the distance between parallel plates. If the material is linearly elastic (so follows Hooks Law), then the displacement, due to an applied force formula_6, is formula_7, where formula_8 is the spring constant. Combining these equations gives the capacitance after an applied force as:\n\nThis can be rearranged to:\n\nAssuming that formula_12, which is true for small deformations where formula_13, we can simplify this to:\n\nIt follows that:\n\nWe can express the change in capacitance formula_18 as:\n\nSingleTact makes force-sensitive capacitors using moulded silicon between two layers of polymide to construct a 0.35mm thick sensor, with force ranges from 1N to 450N. The 8mm SingleTact has a nominal capacitance of 75pF, which increases by 2.2pF when the rated force is applied.\n\nForce-sensing capacitors can be used to create low-profile force-sensitive buttons. They have been used in medical imaging to map pressures in the esophagusand to image breast and prostrate cancer.\n"}
{"id": "810534", "url": "https://en.wikipedia.org/wiki?curid=810534", "title": "Fragile matter", "text": "Fragile matter\n\nIn materials science, fragile matter is a granular material that is jammed solid. Everyday examples include beans getting stuck in a hopper in a whole food shop, or milk powder getting jammed in an upside-down bottle. The term was coined by physicist Michael Cates, who asserts that such circumstances warrant a new class of materials. The jamming thus described can be unjammed by mechanical means, such as tapping or shaking the container, or poking it with a stick.\n\nCates proposed that such jammed systems differ from ordinary solids in that if the direction of the applied stress changes, the jam will break up. Sometimes the change of direction required is very small.\n\nPerhaps the simplest example is a pile of sand, which is solid in the sense that the pile sustains its shape despite the force of gravity. Slight tilting or vibration is enough to enable the grains to shift, collapsing the pile.\n\nNot all jammed systems are fragile, i.e. foam. Shaving foam is jammed because the bubbles are tightly packed together under the isotropic stress imposed by atmospheric pressure. If it were a fragile solid, it would respond plastically to shear stress, however small. But because bubbles deform, foam actually responds elastically provided that the stress is below a threshold value. Fragile matter is also not to be confused with cases in which the particles have adhered to one another (\"caking\"). \n"}
{"id": "14035997", "url": "https://en.wikipedia.org/wiki?curid=14035997", "title": "Fredrikstad Energi", "text": "Fredrikstad Energi\n\nFredrikstad Energi, branded as FEAS is a Norwegian power company that operates in Fredrikstad and Hvaler. The company is owned by the Municipality of Fredrikstad (51%) and Fortum (49%).\n\nThe company has bought a number of privatized power grid operators and power distribution companies in Norway, including Energi 1, Røyken Energiverk and Askøy Energi, as well as the now integrated Hvaler kommunale elverk. FEAS also is the largest owner of the Norwegian football premiership team Fredrikstad F.K. with 24% ownership.\n"}
{"id": "37928477", "url": "https://en.wikipedia.org/wiki?curid=37928477", "title": "Future Trees Trust", "text": "Future Trees Trust\n\nThe Future Trees Trust is a charity, which was formed in 2008, which aims to improve and increase the stock of hardwood trees in Britain and Ireland.\n\nThe British and Irish Hardwoods Improvement Programme (BIHIP) was established in 1991 with the aim of improving and increasing the stock of hardwood trees in Britain and Ireland. It was renamed the Future Trees Trust in 2008. The Future Trees Trust arose out of an informal network of organisations and researchers established in 1991: the British and Irish Hardwoods Improvement Programme (BIHIP).\n\nThe Future Trees Trust is a registered charity in England and Wales, and Ireland. It is supported by a network of organisations, and has six species groups that lead research on: ash, birch, cherry, oak, sycamore, and walnut.\n\n"}
{"id": "26343059", "url": "https://en.wikipedia.org/wiki?curid=26343059", "title": "GE Wind Energy", "text": "GE Wind Energy\n\nGE Wind Energy is a branch of GE Renewable Energy a subsidiary of General Electric. The company manufactures and sells wind turbines to the international market. In 2016, GE was the second largest wind turbine manufacturer in the world.\n\nThe entity was created as developer (not manufacturer) Zond in 1980 by James G.P. Dehlsen, who also formed Clipper Windpower in 2001. Enron acquired Zond and the German manufacturer \"Tacke\" in 1997.\n\nIn 2002 GE acquired the wind power assets of Enron during its bankruptcy proceedings while gas turbine sales slumped. Enron Wind was the only surviving US manufacturer of large wind turbines at the time, and GE increased engineering and supplies for the Wind Division and doubled the annual sales to $1.2B in 2003. It acquired ScanWind in 2009.\n\nIn 2011, GE acquired Wind Tower Systems LLC, a manufacturer of space frame wind turbine towers.\n\nAs of 2016, GE has a nacelle production capacity of 4.8 GW, some of which is in Florida.\n\nAfter the acquisition of Alstom's energy generating assets (2015) GE's wind portfolio was expanded to include the 6MW 'Haliade' offshore turbine (one of the most powerful turbines on Earth) from Alstom Wind. This became GE Wind (offshore). The same year, GE added Blade Dynamics Ltd., a designer and manufacturer of modular wind turbine blades with principal facilities on the Isle of Wight and in Southampton.\n\nGE acquired LM Wind Power from Doughty Hanson & Co for an enterprise value of €1.5 billion in April 2017.\n\nThe GE platform began development with the creation of the 1.5MW series of wind turbines which was developed with the cooperation of the United States Department of Energy. It consisted of three fibreglass blades attached to a horizontal axis hub. The hub is connected to the main shaft which turns a multi-stage system of gears. The gears increase the rotational rate and send the kinetic energy obtained from the wind to a doubly fed electric machine, where it is converted into electrical energy. The angle of the blades and the direction which the turbine faces are controlled by an active, all electric pitch and yaw system. The generator and gearbox are contained in the nacelle which is further insulated to minimize noise emissions.\n\nSeveral optional features support its presence in electrical grids, including voltage regulation, low voltage ride through, and the delivery of reactive power during grid disturbances or periods of low wind.\n\nTo further wind power research, a unit was commissioned at the National Wind Technology Center in late 2009. Its 10,000 installations in the US at the time constituted 50% of the national commercial wind energy fleet, influencing the NREL's decision to install a model at the Center.\n\nThe next evolution, the 2.5XL used a permanent magnet generator, and its entire output was converted to AC at mains frequency.The platform was then moved back to the use of a doubly fed generator and rotor converter similar to the 1.5 series.\n\nAs of 2017 GE had over 35,000 turbines installed across the globe [https://www.youtube.com/watch?v=Gvj7tdUUwf4&feature=youtu.be <nowiki>[1]</nowiki>].\n\nThe offshore GE 3.6 SL model was installed in 2003 at the Arklow Bank Wind Park.\n\nIn the early 2010s GE cancelled its development of a 4.1MW offshore wind turbine - the sole built example, owned by Goteborg Energi was erected in Goteborg in 2011.\n\nFollowing the purchase of the Ecotècnia by Alstom and the acquisition of Alstom by GE, production started in 2016 at the St. Nazaire factory for the 6MW Haliade offshore turbine featuring a permanent magnet design. This unit started an extended test period in Spring 2016 at Østerild Wind Turbine Test Field.\n\nThe 845MW Shepherds Flat Wind Farm in Oregon is the first windpark in the United States to utilize this model as its primary wind turbine.\n\nThe Fantanele-Cogealac wind farm in Romania, constructed in 2008, uses 240 GE 2.5xl wind turbines capable of generating 600MW, powering a million Romanian households each year.\n\nThe offshore GE 3.6 SL model was installed at the Arklow Bank Wind Park.\n\nThrough the provision of 179 GE 3.6-137 turbines to Markbyden in Sweden, GE will create the largest single onshore wind installation in Europe. Norsk Hydro will purchase the power via a 19-year fixed volume corporate PPA; understood to be the largest corporate wind energy PPA in the world.\n\nIn April 2018 General Electric announced that it will begin testing the world’s largest wind turbine – the Haliade-X – at its facilities in Blyth, England. General Electric’s renewable energy department signed a five-year contract with the British government-funded Offshore Renewable Energy (ORE) Catapult to begin trials of the 12-megawatt turbine.\n\n\n"}
{"id": "12582", "url": "https://en.wikipedia.org/wiki?curid=12582", "title": "Gel electrophoresis", "text": "Gel electrophoresis\n\nGel electrophoresis is a method for separation and analysis of macromolecules (DNA, RNA and proteins) and their fragments, based on their size and charge. It is used in clinical chemistry to separate proteins by charge or size (IEF agarose, essentially size independent) and in biochemistry and molecular biology to separate a mixed population of DNA and RNA fragments by length, to estimate the size of DNA and RNA fragments or to separate proteins by charge.\n\nNucleic acid molecules are separated by applying an electric field to move the negatively charged molecules through a matrix of agarose or other substances. Shorter molecules move faster and migrate farther than longer ones because shorter molecules migrate more easily through the pores of the gel. This phenomenon is called sieving. Proteins are separated by charge in agarose because the pores of the gel are too large to sieve proteins. Gel electrophoresis can also be used for separation of nanoparticles.\n\nGel electrophoresis uses a gel as an anticonvective medium or sieving medium during electrophoresis, the movement of a charged particle in an electrical field. Gels suppress the thermal convection caused by application of the electric field, and can also act as a sieving medium, retarding the passage of molecules; gels can also simply serve to maintain the finished separation, so that a post electrophoresis stain can be applied. DNA Gel electrophoresis is usually performed for analytical purposes, often after amplification of DNA via polymerase chain reaction (PCR), but may be used as a preparative technique prior to use of other methods such as mass spectrometry, RFLP, PCR, cloning, DNA sequencing, or Southern blotting for further characterization.\n\nIn simple terms, electrophoresis is a process which enables the sorting of molecules based on size. Using an electric field, molecules (such as DNA) can be made to move through a gel made of agarose or polyacrylamide. The electric field consists of a negative charge at one end which pushes the molecules through the gel, and a positive charge at the other end that pulls the molecules through the gel. The molecules being sorted are dispensed into a well in the gel material. The gel is placed in an electrophoresis chamber, which is then connected to a power source. When the electric current is applied, the larger molecules move more slowly through the gel while the smaller molecules move faster. The different sized molecules form distinct bands on the gel.\n\nThe term \"gel\" in this instance refers to the matrix used to contain, then separate the target molecules. In most cases, the gel is a crosslinked polymer whose composition and porosity is chosen based on the specific weight and composition of the target to be analyzed. When separating proteins or small nucleic acids (DNA, RNA, or oligonucleotides) the gel is usually composed of different concentrations of acrylamide and a cross-linker, producing different sized mesh networks of polyacrylamide. When separating larger nucleic acids (greater than a few hundred bases), the preferred matrix is purified agarose. In both cases, the gel forms a solid, yet porous matrix. Acrylamide, in contrast to polyacrylamide, is a neurotoxin and must be handled using appropriate safety precautions to avoid poisoning. Agarose is composed of long unbranched chains of uncharged carbohydrate without cross links resulting in a gel with large pores allowing for the separation of macromolecules and macromolecular complexes.\n\nElectrophoresis refers to the electromotive force (EMF) that is used to move the molecules through the gel matrix. By placing the molecules in wells in the gel and applying an electric field, the molecules will move through the matrix at different rates, determined largely by their mass when the charge-to-mass ratio (Z) of all species is uniform. However, when charges are not all uniform then, the electrical field generated by the electrophoresis procedure will affect the species that have different charges and therefore will attract the species according to their charges being the opposite. Species that are positively charged will migrate towards the cathode which is negatively charged (because this is an electrolytic rather than galvanic cell). If the species are negatively charged they will migrate towards the positively charged anode.\n\nIf several samples have been loaded into adjacent wells in the gel, they will run parallel in individual lanes. Depending on the number of different molecules, each lane shows separation of the components from the original mixture as one or more distinct bands, one band per component. Incomplete separation of the components can lead to overlapping bands, or to indistinguishable smears representing multiple unresolved components. Bands in different lanes that end up at the same distance from the top contain molecules that passed through the gel with the same speed, which usually means they are approximately the same size. There are molecular weight size markers available that contain a mixture of molecules of known sizes. If such a marker was run on one lane in the gel parallel to the unknown samples, the bands observed can be compared to those of the unknown in order to determine their size. The distance a band travels is approximately inversely proportional to the logarithm of the size of the molecule.\n\nThere are limits to electrophoretic techniques. Since passing current through a gel causes heating, gels may melt during electrophoresis. Electrophoresis is performed in buffer solutions to reduce pH changes due to the electric field, which is important because the charge of DNA and RNA depends on pH, but running for too long can exhaust the buffering capacity of the solution. There are also limitations in determining the molecular weight by SDS-PAGE, especially if you are trying to find the MW of an unknown protein. There are certain biological variables that are difficult or impossible to minimize and can affect the electrophoretic migration. Such factors include protein structure, post-translational modifications, and amino acid composition. For example, tropomyosin is an acidic protein that migrates abnormally on SDS-PAGE gels. This is because the acidic residues are repelled by the negatively charged SDS, leading to an inaccurate mass-to-charge ratio and migration. Further, different preparations of genetic material may not migrate consistently with each other, for morphological or other reasons.\n\nThe types of gel most typically used are agarose and polyacrylamide gels. Each type of gel is well-suited to different types and sizes of analyte. Polyacrylamide gels are usually used for proteins, and have very high resolving power for small fragments of DNA (5-500 bp). Agarose gels on the other hand have lower resolving power for DNA but have greater range of separation, and are therefore used for DNA fragments of usually 50-20,000 bp in size, but resolution of over 6 Mb is possible with pulsed field gel electrophoresis (PFGE). Polyacrylamide gels are run in a vertical configuration while agarose gels are typically run horizontally in a submarine mode. They also differ in their casting methodology, as agarose sets thermally, while polyacrylamide forms in a chemical polymerization reaction.\n\nAgarose gels are made from the natural polysaccharide polymers extracted from seaweed.\nAgarose gels are easily cast and handled compared to other matrices, because the gel setting is a physical rather than chemical change. Samples are also easily recovered. After the experiment is finished, the resulting gel can be stored in a plastic bag in a refrigerator.\n\nAgarose gels do not have a uniform pore size, but are optimal for electrophoresis of proteins that are larger than 200 kDa. Agarose gel electrophoresis can also be used for the separation of DNA fragments ranging from 50 base pair to several megabases (millions of bases), the largest of which require specialized apparatus. The distance between DNA bands of different lengths is influenced by the percent agarose in the gel, with higher percentages requiring longer run times, sometimes days. Instead high percentage agarose gels should be run with a pulsed field electrophoresis (PFE), or field inversion electrophoresis.\n\n\"Most agarose gels are made with between 0.7% (good separation or resolution of large 5–10kb DNA fragments) and 2% (good resolution for small 0.2–1kb fragments) agarose dissolved in electrophoresis buffer. Up to 3% can be used for separating very tiny fragments but a vertical polyacrylamide gel is more appropriate in this case. Low percentage gels are very weak and may break when you try to lift them. High percentage gels are often brittle and do not set evenly. 1% gels are common for many applications.\"\n\nPolyacrylamide gel electrophoresis (PAGE) is used for separating proteins ranging in size from 5 to 2,000 kDa due to the uniform pore size provided by the polyacrylamide gel. Pore size is controlled by modulating the concentrations of acrylamide and bis-acrylamide powder used in creating a gel. Care must be used when creating this type of gel, as acrylamide is a potent neurotoxin in its liquid and powdered forms.\n\nTraditional DNA sequencing techniques such as Maxam-Gilbert or Sanger methods used polyacrylamide gels to separate DNA fragments differing by a single base-pair in length so the sequence could be read. Most modern DNA separation methods now use agarose gels, except for particularly small DNA fragments. It is currently most often used in the field of immunology and protein analysis, often used to separate different proteins or isoforms of the same protein into separate bands. These can be transferred onto a nitrocellulose or PVDF membrane to be probed with antibodies and corresponding markers, such as in a western blot.\n\nTypically resolving gels are made in 6%, 8%, 10%, 12% or 15%. Stacking gel (5%) is poured on top of the resolving gel and a gel comb (which forms the wells and defines the lanes where proteins, sample buffer and ladders will be placed) is inserted. The percentage chosen depends on the size of the protein that one wishes to identify or probe in the sample. The smaller the known weight, the higher the percentage that should be used. Changes on the buffer system of the gel can help to further resolve proteins of very small sizes.\n\nPartially hydrolysed potato starch makes for another non-toxic medium for protein electrophoresis. The gels are slightly more opaque than acrylamide or agarose. Non-denatured proteins can be separated according to charge and size. They are visualised using Napthal Black or Amido Black staining. Typical starch gel concentrations are 5% to 10%.\n\nDenaturing gels are run under conditions that disrupt the natural structure of the analyte, causing it to unfold into a linear chain. Thus, the mobility of each macromolecule depends only on its linear length and its mass-to-charge ratio. Thus, the secondary, tertiary, and quaternary levels of biomolecular structure are disrupted, leaving only the primary structure to be analyzed.\n\nNucleic acids are often denatured by including urea in the buffer, while proteins are denatured using sodium dodecyl sulfate, usually as part of the SDS-PAGE process. For full denaturation of proteins, it is also necessary to reduce the covalent disulfide bonds that stabilize their tertiary and quaternary structure, a method called reducing PAGE. Reducing conditions are usually maintained by the addition of beta-mercaptoethanol or dithiothreitol. For general analysis of protein samples, reducing PAGE is the most common form of protein electrophoresis.\n\nDenaturing conditions are necessary for proper estimation of molecular weight of RNA. RNA is able to form more intramolecular interactions than DNA which may result in change of its electrophoretic mobility. Urea, DMSO and glyoxal are the most often used denaturing agents to disrupt RNA structure. Originally, highly toxic methylmercury hydroxide was often used in denaturing RNA electrophoresis, but it may be method of choice for some samples.\n\nDenaturing gel electrophoresis is used in the DNA and RNA banding pattern-based methods temperature gradient gel electrophoresis (TGGE) and denaturing gradient gel electrophoresis (DGGE).\n\nNative gels are run in non-denaturing conditions, so that the analyte's natural structure is maintained. This allows the physical size of the folded or assembled complex to affect the mobility, allowing for analysis of all four levels of the biomolecular structure. For biological samples, detergents are used only to the extent that they are necessary to lyse lipid membranes in the cell. Complexes remain—for the most part—associated and folded as they would be in the cell. One downside, however, is that complexes may not separate cleanly or predictably, as it is difficult to predict how the molecule's shape and size will affect its mobility. Addressing and solving this problem is a major aim of quantitative native PAGE.\n\nUnlike denaturing methods, native gel electrophoresis does not use a charged denaturing agent. The molecules being separated (usually proteins or nucleic acids) therefore differ not only in molecular mass and intrinsic charge, but also the cross-sectional area, and thus experience different electrophoretic forces dependent on the shape of the overall structure. For proteins, since they remain in the native state they may be visualised not only by general protein staining reagents but also by specific enzyme-linked staining.\n\nA specific experiment example of an application of native gel electrophoresis is to check for enzymatic activity to verify the presence of the enzyme in the sample during protein purification. For example, for the protein alkaline phosphatase, the staining solution is a mixture of 4-chloro-2-2methylbenzenediazonium salt with 3-phospho-2-naphthoic acid-2’-4’-dimethyl aniline in Tris buffer. This stain is commercially sold as kit for staining gels. If the protein is present, the mechanism of the reaction takes place in the following order: it starts with the de-phosphorylation of 3-phospho-2-naphthoic acid-2’-4’-dimethyl aniline by alkaline phosphatase (water is needed for the reaction). The phosphate group is released and replaced by an alcohol group from water. The electrophile 4- chloro-2-2 methylbenzenediazonium (Fast Red TR Diazonium salt) displaces the alcohol group forming the final product Red Azo dye. As its name implies, this is the final visible-red product of the reaction. In undergraduate academic experimentation of protein purification, the gel is usually ran next to commercial purified samples in order to visualize the results and make confusions of whether or not purification was successful.\n\nNative gel electrophoresis is typically used in proteomics and metallomics. However, native PAGE is also used to scan genes (DNA) for unknown mutations as in Single-strand conformation polymorphism.\n\nBuffers in gel electrophoresis are used to provide ions that carry a current and to maintain the pH at a relatively constant value.\nThese buffers have plenty of ions in them, which is necessary for the passage of electricity through them. Something like distilled water or benzene contains few ions, which is not ideal for the use in electrophoresis. There are a number of buffers used for electrophoresis. The most common being, for nucleic acids Tris/Acetate/EDTA (TAE), Tris/Borate/EDTA (TBE). Many other buffers have been proposed, e.g. lithium borate, which is almost never used, based on Pubmed citations (LB), iso electric histidine, pK matched goods buffers, etc.; in most cases the purported rationale is lower current (less heat) matched ion mobilities, which leads to longer buffer life. Borate is problematic; Borate can polymerize, or interact with cis diols such as those found in RNA. TAE has the lowest buffering capacity but provides the best resolution for larger DNA. This means a lower voltage and more time, but a better product. LB is relatively new and is ineffective in resolving fragments larger than 5 kbp; However, with its low conductivity, a much higher voltage could be used (up to 35 V/cm), which means a shorter analysis time for routine electrophoresis. As low as one base pair size difference could be resolved in 3% agarose gel with an extremely low conductivity medium (1 mM Lithium borate).\n\nMost SDS-PAGE protein separations are performed using a \"discontinuous\" (or DISC) buffer system that significantly enhances the sharpness of the bands within the gel. During electrophoresis in a discontinuous gel system, an ion gradient is formed in the early stage of electrophoresis that causes all of the proteins to focus into a single sharp band in a process called isotachophoresis. Separation of the proteins by size is achieved in the lower, \"resolving\" region of the gel. The resolving gel typically has a much smaller pore size, which leads to a sieving effect that now determines the electrophoretic mobility of the proteins.\n\nAfter the electrophoresis is complete, the molecules in the gel can be stained to make them visible. DNA may be visualized using ethidium bromide which, when intercalated into DNA, fluoresce under ultraviolet light, while protein may be visualised using silver stain or Coomassie Brilliant Blue dye. Other methods may also be used to visualize the separation of the mixture's components on the gel. If the molecules to be separated contain radioactivity, for example in a DNA sequencing gel, an autoradiogram can be recorded of the gel. Photographs can be taken of gels, often using a Gel Doc system.\n\nAfter separation, an additional separation method may then be used, such as isoelectric focusing or SDS-PAGE. The gel will then be physically cut, and the protein complexes extracted from each portion separately. Each extract may then be analysed, such as by peptide mass fingerprinting or de novo peptide sequencing after in-gel digestion. This can provide a great deal of information about the identities of the proteins in a complex.\n\n\nGel electrophoresis is used in forensics, molecular biology, genetics, microbiology and biochemistry. The results can be analyzed quantitatively by visualizing the gel with UV light and a gel imaging device. The image is recorded with a computer operated camera, and the intensity of the band or spot of interest is measured and compared against standard or markers loaded on the same gel. The measurement and analysis are mostly done with specialized software.\n\nDepending on the type of analysis being performed, other techniques are often implemented in conjunction with the results of gel electrophoresis, providing a wide range of field-specific applications.\n\nIn the case of nucleic acids, the direction of migration, from negative to positive electrodes, is due to the naturally occurring negative charge carried by their sugar-phosphate backbone.\n\nDouble-stranded DNA fragments naturally behave as long rods, so their migration through the gel is relative to their size or, for cyclic fragments, their radius of gyration. Circular DNA such as plasmids, however, may show multiple bands, the speed of migration may depend on whether it is relaxed or supercoiled. Single-stranded DNA or RNA tend to fold up into molecules with complex shapes and migrate through the gel in a complicated manner based on their tertiary structure. Therefore, agents that disrupt the hydrogen bonds, such as sodium hydroxide or formamide, are used to denature the nucleic acids and cause them to behave as long rods again.\n\nGel electrophoresis of large DNA or RNA is usually done by agarose gel electrophoresis. See the \"Chain termination method\" page for an example of a polyacrylamide DNA sequencing gel. Characterization through ligand interaction of nucleic acids or fragments may be performed by mobility shift affinity electrophoresis.\n\nElectrophoresis of RNA samples can be used to check for genomic DNA contamination and also for RNA degradation. RNA from eukaryotic organisms shows distinct bands of 28s and 18s rRNA, the 28s band being approximately twice as intense as the 18s band. Degraded RNA has less sharply defined bands, has a smeared appearance, and intensity ratio is less than 2:1.\n\nProteins, unlike nucleic acids, can have varying charges and complex shapes, therefore they may not migrate into the polyacrylamide gel at similar rates, or at all, when placing a negative to positive EMF on the sample. Proteins therefore, are usually denatured in the presence of a detergent such as sodium dodecyl sulfate (SDS) that coats the proteins with a negative charge. Generally, the amount of SDS bound is relative to the size of the protein (usually 1.4g SDS per gram of protein), so that the resulting denatured proteins have an overall negative charge, and all the proteins have a similar charge-to-mass ratio. Since denatured proteins act like long rods instead of having a complex tertiary shape, the rate at which the resulting SDS coated proteins migrate in the gel is relative only to its size and not its charge or shape.\n\nProteins are usually analyzed by sodium dodecyl sulfate polyacrylamide gel electrophoresis (SDS-PAGE), by native gel electrophoresis, by preparative gel electrophoresis (QPNC-PAGE), or by 2-D electrophoresis.\n\nCharacterization through ligand interaction may be performed by electroblotting or by affinity electrophoresis in agarose or by capillary electrophoresis as for estimation of binding constants and determination of structural features like glycan content through lectin binding.\n\n\nA 1959 book on electrophoresis by Milan Bier cites references from the 1800s. However, Oliver Smithies made significant contributions. Bier states: \"The method of Smithies ... is finding wide application because of its unique separatory power.\" Taken in context, Bier clearly implies that Smithies' method is an improvement.\n\n"}
{"id": "49891925", "url": "https://en.wikipedia.org/wiki?curid=49891925", "title": "Green Illusions", "text": "Green Illusions\n\nGreen Illusions: The Dirty Secrets of Clean Energy and the Future of Environmentalism (), by Ozzie Zehner, was published in 2012 by the University of Nebraska Press. It discusses various approaches to \"clean energy\", and why they do not provide the desired benefits. In successive chapters, it discusses solar cells, wind power, biofuels, nuclear power, hydrogen power, coal power, hydropower, alternative energy, green investment, population control, consumption, architecture, carbon taxes, environmental education.\n\nThe author writes: \"We don’t have an energy crisis. We have a consumption crisis.\"\n\nWriting in the \"Huffington Post\", Tom Zeller Jr. calls the author a provocateur. He cites Chris Meehan, who called his view of photovoltaics \"alarmist\" and \"misleading\", and he cites Nick Chambers, who called his view of electric vehicles \"ridiculous\". However, Zeller writes that Zehner cites a \"2010 lifecycle analysis\" by the National Academy of Sciences as a basis for evaluating the \"aggregate environmental damage\" from an electric car.\n\nWriting for \"The Tyee\", Justin Ritchie points to a fundamental question: \"in a world of limited decisions, is it really smart to subsidize marginally effective mitigation strategies of our car culture, suburbia and overpopulation without addressing the root causes?\"\n\nAll copies of the book sold in the United States were self-censored due to food libel laws that enable the food industry to sue journalists and authors who criticize their products.\n\n\n"}
{"id": "3811299", "url": "https://en.wikipedia.org/wiki?curid=3811299", "title": "HT-7", "text": "HT-7\n\nHT-7, or Hefei Tokamak-7, is an experimental superconducting tokamak nuclear fusion reactor built in Hefei, China, to investigate the process of developing fusion power. The HT-7 was developed with the assistance of Russia, and was based on the earlier T-7 tokamak reactor. The reactor was built by the Hefei-based Institute of Plasma Physics under the direction of the Chinese Academy of Sciences.\n\nThe HT-7 construction was completed in May 1994, with final tests accomplished by December of the same year allowing experiments to proceed.\n\nThe HT-7 has been superseded by the Experimental Advanced Superconducting Tokamak (EAST) built in Hefei by the Institute of Plasma Physics as an experimental reactor before ITER is completed.\n\n\n"}
{"id": "49163826", "url": "https://en.wikipedia.org/wiki?curid=49163826", "title": "Haywardville, Massachusetts", "text": "Haywardville, Massachusetts\n\nIn the early 1800s, Nathaniel Hayward bought remodeled shoe mills in Stoneham, Massachusetts from Elisha Converse, founder of the largest rubber shoe manufacturer in the world, the Boston Rubber Shoe Company in Malden, Massachusetts. Straddling Spot Pond Brook, the village was the site of early industrial development which later blossomed into larger factories. The factory grew to be an industrial community that has come to be known as Haywardville. It is here where Hayward and Charles Goodyear invented slickers (canvas and rubber coats) and the process of vulcanization. The factory produced a variety of rubber products including boots, pails and spittoons. There were numerous large factory buildings here during this period, a community of living quarters, some shops - or at least places to barter for goods.\n\nIndustry in Haywardville began to decline as larger, more successful businesses overshadowed those in the village. It is believed that at least four mills were using the tiny Spot Pond Brook at one time. Another possible source of decline was the rapid development of both steam and electric power. The fatal blow was the taking of Haywardville’s water rights by the communities of Medford, Malden and Melrose. By this time, the mostly deserted Haywardville was bought by the Commonwealth of Massachusetts in 1894 as part of the effort to create a park system around Boston. It helped form the Middlesex Fells Reservation.\n\nAll of the buildings from what used to be known as Haywardville were relocated to Ravine Terrace and Brook Street, both in Stoneham. The remaining lands were restored by Charles Eliot, noted protege of Frederick Law Olmsted, prominent urban landscaper. This site was instrumental in the creation of the larger Metropolitan Park System, the first in the United States.\n\nToday, only traces of Haywardville remain in the Fells. The site is accessible by trails in the Virginia Wood, part of the Spot Pond Archeological District. Trail maps are available at the nearby Botume House.\n"}
{"id": "275216", "url": "https://en.wikipedia.org/wiki?curid=275216", "title": "Hemodynamics", "text": "Hemodynamics\n\nHemodynamics or hæmodynamics is the dynamics of blood flow. The circulatory system is controlled by homeostatic mechanisms, such as hydraulic circuits are controlled by control systems. Hemodynamic response continuously monitors and adjusts to conditions in the body and its environment. Thus hemodynamics explains the physical laws that govern the flow of blood in the blood vessels.\n\nBlood flow ensures the transportation of nutrients, hormones, metabolic wastes, O and CO throughout the body to maintain cell-level metabolism, the regulation of the pH, osmotic pressure and temperature of the whole body, and the protection from microbial and mechanical harms.\n\nBlood is a non-Newtonian fluid, best studied using rheology rather than hydrodynamics. Blood vessels are not rigid tubes, so classic hydrodynamics and fluids mechanics based on the use of classical viscometers are not capable of explaining hemodynamics.\n\nThe study of the blood flow is called hemodynamics. The study of the properties of the blood flow is called hemorheology.\n\nBlood is a complex liquid. Blood is composed of plasma and formed elements. The plasma contains 91.5% water, 7% proteins and 1.5% other solutes. The formed elements are platelets, white blood cells and red blood cells, the presence of these formed elements and their interaction with plasma molecules are the main reasons why blood differs so much from ideal Newtonian fluids.\n\nNormal blood plasma behaves like a Newtonian fluid at physiological rates of shear. Typical values for the viscosity of normal human plasma at 37 °C is 1.4 mN·s/m. The viscosity of normal plasma varies with temperature in the same way as does that of its solvent water; a 5 °C increase of temperature in the physiological range reduces plasma viscosity by about 10%.\n\nThe osmotic pressure of solution is determined by the number of particles present and by the temperature. For example, a 1 molar solution of a substance contains molecules per liter of that substance and at 0 °C it has an osmotic pressure of . The osmotic pressure of the plasma affects the mechanics of the circulation in several ways. An alteration of the osmotic pressure difference across the membrane of a blood cell causes a shift of water and a change of cell volume. The changes in shape and flexibility affect the mechanical properties of whole blood. A change in plasma osmotic pressure alters the hematocrit, that is, the volume concentration of red cells in the whole blood by redistributing water between the intravascular and extravascular spaces. This in turn affects the mechanics of the whole blood.\n\nThe red blood cell is highly flexible and biconcave in shape. Its membrane has a Young's modulus in the region of 106 Pa. Deformation in red blood cells is induced by shear stress. When a suspension is sheared, the red blood cells deform and spin because of the velocity gradient, with the rate of deformation and spin depending on the shear-rate and the concentration.\nThis can influence the mechanics of the circulation and may complicate the measurement of blood viscosity. It is true that in a steady state flow of a viscous fluid through a rigid spherical body immersed in the fluid, where we assume the inertia is negligible in such a flow, it is believed that the downward gravitational force of the particle is balanced by the viscous drag force. From this force balance the speed of fall can be shown to be given by Stokes' law \nWhere \"a\" is the particle radius, \"ρ\", \"ρ\" are the respectively particle and fluid density \"μ\" is the fluid viscosity, \"g\" is the gravitational acceleration. From the above equation we can see that the sedimentation velocity of the particle depends on the square of the radius. If the particle is released from rest in the fluid, its sedimentation velocity \"U\" increases until it attains the steady value called the terminal velocity (U), as shown above.\n\nHemodilution is the dilution of the concentration of red blood cells and plasma constituents by partially substituting the blood with colloids or crystalloids. It is a strategy to avoid exposure of patients to the potential hazards of homologous blood transfusions.\n\nHemodilution can be normovolemic, which implies the dilution of normal blood constituents by the use of expanders. During acute normovolemic hemodilution, (ANH) blood subsequently lost during surgery contains proportionally fewer red blood cells per millimetre, thus minimizing intraoperative loss of the whole blood. Therefore, blood lost by the patient during surgery is not actually lost by the patient, for this volume is purified and redirected into the patient.\n\nOn the other hand, hypervolemic hemodilution (HVH) uses acute preoperative volume expansion without any blood removal. In choosing a fluid, however, it must be assured that when mixed, the remaining blood behaves in the microcirculation as in the original blood fluid, retaining all its properties of viscosity.\n\nIn presenting what volume of ANH should be applied one study suggests a mathematical model of ANH which calculates the maximum possible RCM savings using ANH, given the patients weight \"H\" and \"H\". (See below for a glossary of the terms used.)\n\nTo maintain the normovolemia, the withdrawal of autologous blood must be simultaneously replaced by a suitable hemodilute. Ideally, this is achieved by isovolemia exchange transfusion of a plasma substitute with a colloid osmotic pressure (OP). A colloid is a fluid containing particles that are large enough to exert an oncotic pressure across the micro-vascular membrane.\nWhen debating the use of colloid or crystalloid, it is imperative to think about all the components of the starling equation:\nTo identify the minimum safe hematocrit desirable for a given patient the following equation is useful:\nwhere EBV is the estimated blood volume; 70 mL/kg was used in this model and \"H\" (initial hematocrit) is the patient’s initial hematocrit.\nFrom the equation above it is clear that the volume of blood removed during the ANH to the \"H\" is the same as the \"BL\".\nHow much blood is to be removed is usually based on the weight, not the volume. The number of units that need to be removed to hemodilute to the maximum safe hematocrit (ANH) can be found by \nThis is based on the assumption that each unit removed by hemodilution has a volume of 450 mL (the actual volume of a unit will vary somewhat since completion of collection ais dependent on weight and not volume).\nThe model assumes that the hemodilute value is equal to the \"H\" prior to surgery, therefore, the re-transfusion of blood obtained by hemodilution must begin when SBL begins.\nThe RCM available for retransfusion after ANH (RCMm) can be calculated from the patient's \"H\" and the final hematocrit after hemodilution(\"H\")\nThe maximum SBL that is possible when ANH is used without falling below Hm(BLH) is found by assuming that all the blood removed during ANH is returned to the patient at a rate sufficient to maintain the hematocrit at the minimum safe level\nIf ANH is used as long as SBL does not exceed \"BL\" there will not be any need for blood transfusion. We can conclude from the foregoing that \"H\" should therefore not exceed \"s\".\nThe difference between the \"BL\" and the \"BL\" therefore is the incremental surgical blood loss (\"BL\") possible when using ANH.\nWhen expressed in terms of the RCM \nWhere \"RCM\" is the red cell mass that would have to be administered using homologous blood to maintain the \"H\" if ANH is not used and blood loss equals BLH.\n\nThe model used assumes ANH used for a 70 kg patient with an estimated blood volume of 70 ml/kg (4900 ml). A range of \"H\" and \"H\" was evaluated to understand conditions where hemodilution is necessary to benefit the patient.\n\nThe result of the model calculations are presented in a table given in the appendix for a range of \"H\" from 0.30 to 0.50 with ANH performed to minimum hematocrits from 0.30 to 0.15. Given a \"H\" of 0.40, if the \"H\" is assumed to be 0.25.then from the equation above the RCM count is still high and ANH is not necessary, if BL does not exceed 2303 ml, since the hemotocrit will not fall below H, although five units of blood must be removed during hemodilution. Under these conditions, to achieve the maximum benefit from the technique if ANH is used, no homologous blood will be required to maintain the \"H\" if blood loss does not exceed 2940 ml. In such a case ANH can save a maximum of 1.1 packed red blood cell unit equivalent, and homologous blood transfusion is necessary to maintain \"H\", even if ANH is used.\nThis model can be used to identify when ANH may be used for a given patient and the degree of ANH necessary to maximize that benefit.\n\nFor example, if \"H\" is 0.30 or less it is not possible to save a red cell mass equivalent to two units of homologous PRBC even if the patient is hemodiluted to an \"H\" of 0.15. That is because from the RCM equation the patient RCM falls short from the equation giving above.\nIf \"H\" is 0.40 one must remove at least 7.5 units of blood during ANH, resulting in an \"H\" of 0.20 to save two units equivalence. Clearly, the greater the \"H\" and the greater the number of units removed during hemodilution, the more effective ANH is for preventing homologous blood transfusion. The model here is designed to allow doctors to determine where ANH may be beneficial for a patient based on their knowledge of the \"H\", the potential for SBL, and an estimate of the \"H\". Though the model used a 70 kg patient, the result can be applied to any patient. To apply these result to any body weight, any of the values BLs, BLH and ANHH or PRBC given in the table need to be multiplied by the factor we will call T\nBasically, the model considered above is designed to predict the maximum RCM that can save ANH.\n\nIn summary, the efficacy of ANH has been described mathematically by means of measurements of surgical blood loss and blood volume flow measurement. This form of analysis permits accurate estimation of the potential efficiency of the techniques and shows the application of measurement in the medical field.\n\nThe heart is the driver of the circulatory system, pumping blood through rhythmic contraction and relaxation. The rate of blood flow out of the heart (often expressed in L/min) is known as the cardiac output (CO).\n\nBlood being pumped out of the heart first enters the aorta, the largest artery of the body. It then proceeds to divide into smaller and smaller arteries, then into arterioles, and eventually capillaries, where oxygen transfer occurs. The capillaries connect to venules, and the blood then travels back through the network of veins to the right heart. The micro-circulation — the arterioles, capillaries, and venules —constitutes most of the area of the vascular system and is the site of the transfer of O, glucose, and enzyme substrates into the cells. The venous system returns the de-oxygenated blood to the right heart where it is pumped into the lungs to become oxygenated and CO and other gaseous wastes exchanged and expelled during breathing. Blood then returns to the left side of the heart where it begins the process again.\n\nIn a normal circulatory system, the volume of blood returning to the heart each minute is approximately equal to the volume that is pumped out each minute (the cardiac output). Because of this, the velocity of blood flow across each level of the circulatory system is primarily determined by the total cross-sectional area of that level. This is mathematically expressed by the following equation:\n\nwhere \n\nBlood flow is also affected by the smoothness of the vessels, resulting in either turbulent (chaotic) or laminar (smooth) flow. Smoothness is reduced by the buildup of fatty deposits on the arterial walls.\n\nThe Reynold’s number (denoted NR or Re) is a relationship that helps determine the behavior of a fluid in a tube, in this case blood in the vessel.\n\nThe equation for this dimensionless relationship is written as:\n\nThe Reynold’s number is directly proportional to the velocity and diameter of the tube. Note that NR is directly proportional to the mean velocity as well as the diameter. A Reynold’s number of less than 2300 is laminar fluid flow, which is characterized by constant flow motion, whereas a value of over 4000, is represented as turbulent flow. Due to its smaller radius and lowest velocity compared to other vessels, the Reynold’s number at the capillaries is very low, resulting in laminar instead of turbulent flow.\n\nOften expressed in cm/s. This value is inversely related to the total cross-sectional area of the blood vessel and also differs per cross-section, because in normal condition the blood flow has laminar characteristics. For this reason, the blood flow velocity is the fastest in the middle of the vessel and slowest at the vessel wall. In most cases, the mean velocity is used. There are many ways to measure blood flow velocity, like videocapillary microscoping with frame-to-frame analysis, or laser Doppler anemometry.\nBlood velocities in arteries are higher during systole than during diastole. One parameter to quantify this difference is the \"pulsatility index\" (PI), which is equal to the difference between the peak systolic velocity and the minimum diastolic velocity divided by the mean velocity during the cardiac cycle. This value decreases with distance from the heart.\n\nResistance is also related to vessel radius, vessel length, and blood viscosity.\n\nIn a first approach based on fluids, as indicated by the Hagen–Poiseuille equation. The equation is as follows:\n\nIn a second approach, more realistic of the vascular resistance and coming from experimental observations on blood flows, according to Thurston, there is a plasma release-cell layering at the walls surrounding a plugged flow. It is a fluid layer in which at a distance δ, viscosity η is a function of δ written as η(δ), and these surrounding layers do not meet at the vessel centre in real blood flow. Instead, there is the plugged flow which is hyperviscous because holding high concentration of RBCs. Thurston assembled this layer to the flow resistance to describe blood flow by means of a viscosity η(δ) and thickness δ from the wall layer.\n\nThe blood resistance law appears as R adapted to blood flow profile :\n\nwhere\n\n\nBlood resistance varies depending on blood viscosity and its plugged flow (or sheath flow since they are complementary across the vessel section) size as well, and on the size of the vessels.\nAssuming steady, laminar flow in the vessel, the blood vessels behavior is similar to that of a pipe. For instance if p1 and p2 are pressures are at the ends of the tube, the pressure drop/gradient is:\n\nThe larger arteries, including all large enough to see without magnification, are conduits with low vascular resistance (assuming no advanced atherosclerotic changes) with high flow rates that generate only small drops in pressure. The smaller arteries and arterioles have higher resistance, and confer the main blood pressure drop across major arteries to capillaries in the circulatory system.\n\nIn the arterioles blood pressure is lower than in the major arteries. This is due to bifurcations, which cause a drop in pressure. The more bifurcations, the higher the total cross-sectional area, therefore the pressure across the surface drops. This is why the arterioles have the highest pressure-drop. The pressure drop of the arterioles is the product of flow rate and resistance: ∆P=Q xresistance. The high resistance observed in the arterioles, which factor largely in the ∆\"P\" is a result of a smaller radius of about 30 µm. The smaller the radius of a tube, the larger the resistance to fluid flow.\n\nImmediately following the arterioles are the capillaries. Following the logic observed in the arterioles, we expect the blood pressure to be lower in the capillaries compared to the arterioles. Since pressure is a function of force per unit area, (\"P\" = \"F\"/\"A\"), the larger the surface area, the lesser the pressure when an external force acts on it. Though the radii of the capillaries are very small, the network of capillaries have the largest surface area in the vascular network. They are known to have the largest surface area (485 mm^2) in the human vascular network. The larger the total cross-sectional area, the lower the mean velocity as well as the pressure.\n\nSubstances called vasoconstrictors can reduce the size of blood vessels, thereby increasing blood pressure. Vasodilators (such as nitroglycerin) increase the size of blood vessels, thereby decreasing arterial pressure.\n\nIf the blood viscosity increases (gets thicker), the result is an increase in arterial pressure. Certain medical conditions can change the viscosity of the blood. For instance, anemia (low red blood cell concentration), reduces viscosity, whereas increased red blood cell concentration increases viscosity. It had been thought that aspirin and related \"blood thinner\" drugs decreased the viscosity of blood, but instead studies found that they act by reducing the tendency of the blood to clot.\n\nRegardless of site, blood pressure is related to the wall tension of the vessel according to the Young–Laplace equation (assuming that the thickness of the vessel wall is very small as compared to the diameter of the lumen):\nwhere\nFor the thin-walled assumption to be valid the vessel must have a wall thickness of no more than about one-tenth (often cited as one twentieth) of its radius.\n\nThe cylinder stress, in turn, is the average force exerted circumferentially (perpendicular both to the axis and to the radius of the object) in the cylinder wall, and can be described as:\nwhere:\n\nWhen force is applied to a material it starts to deform or move. As the force needed to deform a material (e.g. to make a fluid flow) increases with the size of the surface of the material A., the magnitude of this force F is proportional to the area A of the portion of the surface. Therefore, the quantity (F/A) that is the force per unit area is called the stress. The shear stress at the wall that is associated with blood flow through an artery depends on the artery size and geometry and can range between 0.5 and 4 Pa. \nUnder normal conditions, to avoid atherogenesis, thrombosis, smooth muscle proliferation and endothelial apoptosis, shear stress maintains its magnitude and direction within an acceptable range. In some cases occurring due to blood hammer, shear stress reaches larger values. While the direction of the stress may also change by the reverse flow, depending on the hemodynamic conditions. Therefore, this situation can lead to atherosclerosis disease.\n\nVeins are described as the \"capacitance vessels\" of the body because over 70% of the blood volume resides in the venous system. Veins are more compliant than arteries and expand to accommodate changing volume.\n\nThe blood pressure in the circulation is principally due to the pumping action of the heart. The pumping action of the heart generates pulsatile blood flow, which is conducted into the arteries, across the micro-circulation and eventually, back via the venous system to the heart. During each heartbeat, systemic arterial blood pressure varies between a maximum (systolic) and a minimum (diastolic) pressure. In physiology, these are often simplified into one value, the mean arterial pressure (MAP), which is calculated as follows:\n\nwhere:\n\nDifferences in mean blood pressure are responsible for blood flow from one location to another in the circulation. The rate of mean blood flow depends on both blood pressure and the resistance to flow presented by the blood vessels. Mean blood pressure decreases as the circulating blood moves away from the heart through arteries and capillaries due to viscous losses of energy. Mean blood pressure drops over the whole circulation, although most of the fall occurs along the small arteries and arterioles. Gravity affects blood pressure via hydrostatic forces (e.g., during standing), and valves in veins, breathing, and pumping from contraction of skeletal muscles also influence blood pressure in veins.\n\nThe relationship between pressure, flow, and resistance is expressed in the following equation:\n\nWhen applied to the circulatory system, we get:\n\nwhere\n\nA simplified form of this equation assumes right atrial pressure is approximately 0:\n\nThe ideal blood pressure in the brachial artery, where standard blood pressure cuffs measure pressure, is <120/80 mmHg. Other major arteries have similar levels of blood pressure recordings indicating very low disparities among major arteries. In the innominate artery, the average reading is 110/70 mmHg, the right subclavian artery averages 120/80 and the abdominal aorta is 110/70 mmHg. The relatively uniform pressure in the arteries indicate that these blood vessels act as a pressure reservoir for fluids that are transported within them.\n\nPressure drops gradually as blood flows from the major arteries, through the arterioles, the capillaries until blood is pushed up back into the heart via the venules, the veins through the vena cava with the help of the muscles. At any given pressure drop, the flow rate is determined by the resistance to the blood flow. In the arteries, with the absence of diseases, there is very little or no resistance to blood. The vessel diameter is the most principal determinant to control resistance. Compared to other smaller vessels in the body, the artery has a much bigger diameter (4  mm), therefore the resistance is low.\n\nThe \"arm–leg (blood pressure) gradient\" is the difference between the blood pressure measured in the arms and that measured in the legs. It is normally less than 10 mm Hg, but may be increased in e.g. coarctation of the aorta.\n\nHemodynamic monitoring is the observation of hemodynamic parameters over time, such as blood pressure and heart rate. Blood pressure can be monitored either invasively through an inserted blood pressure transducer assembly (providing continuous monitoring), or noninvasively by repeatedly measuring the blood pressure with an inflatable blood pressure cuff.\n\n\nThe word \"hemodynamics\" () uses combining forms of \"hemo-\" and \"dynamics\", thus \"the dynamics of blood\". The vowel of the \"hemo-\" syllable is variously written according to the ae/e variation.\n\n\n"}
{"id": "55851493", "url": "https://en.wikipedia.org/wiki?curid=55851493", "title": "IEC 61851", "text": "IEC 61851\n\nIEC 61851 is an international standard for electric vehicle conductive charging systems, parts of which are currently still under development. IEC 61851 is one of the International Electrotechnical Commission's group of standards for electric road vehicles and electric industrial trucks, and is the responsibility of IEC Technical Committee 69 (TC69).\n\nIEC 61851 consists of the following parts, detailed in separate IEC 61851 standard documents:\n\n\n"}
{"id": "56566719", "url": "https://en.wikipedia.org/wiki?curid=56566719", "title": "Kids Saving the Rainforest", "text": "Kids Saving the Rainforest\n\nKids Saving the Rainforest (KSTR) is a Costa Rica-based non-governmental non-profit 501 C3 organization founded in 1999 to plant trees in depleted areas of the country, and to rescue, rehabilitate and, when possible, release the animals who live in these forests. Since its inception, Kids Saving the Rainforest has planted or is in the process of planting nearly 100,000 trees and rescued and rehabilitated 3,000 wild animals, two-thirds of which have been released back into the wild.\n\nKSTR's mission is: \"to protect the diverse wildlife of Costa Rica’s Pacific Coast by rehabilitating wildlife, conducting original scientific research, training volunteers, and promoting conservation.\"\n\nKids Saving the Rainforest was founded in February 1999 by two nine-year old girls, Janine Licare and Aislin Livingstone, who were living in the jungle of Manuel Antonio, Costa Rica. The girls made paper-mache bottles and painted rocks and sold them by the side of the road to raise money for saplings that would be planted in the nearby forest.\n\nSince then, KSTR has become a fully functional environmental organization with a board of directors. Licare's mother, Jennifer Rice is president. The staff includes a number of full-time people plus volunteers.\n\nKSTR is located outside Manual Antonio, Costa Rica, but has projects in 18 countries.\n\nKSTR rescues and rehabs animals that are brought to their facility. In addition, the ones that can not be released back into the wild are given sanctuary for life on the property. The goal is to release as many animals as possible. With an average of 200 rescues a year, over 75 species have been rescued at their center. This includes, among others, two- and three-toed sloths, squirrel monkeys, kinkajous, coatis, marmosets, tamarins, orange-chinned parakeets.\n\nKSTR started a Wildlife Bridge Program in 2000, originally to protect the endangered titi monkeys (squirrel monkey). They have since put up 130 bridges and 14 different animal species have been using them. Meanwhile, the titi monkey population has doubled. Whenever an arboreal animal is electrocuted by live wires, hit by a car, or attacked by dogs, KSTR reviews the necessity of a bridge and places it for them to cross high off the ground. \n\nSince its inception, KSTR has planted over 7,000 trees, is in the process of planting another 80,000 on donated property elsewhere in Costa Rica, and reforests in other countries as well. \n\nIn order to run its facility and care for the animals, KSTR relies on volunteers who come in from around the world to help with all aspects of its programs. The volunteers clean, repair, and build cages, work on trails, prepare food for the animals, observe behavior, and more. Many volunteers are housed where they work at KSTR, but some opt for a day of volunteer work offered by area tour companies.\n\nA key element of KSTR is education about biodiversity conservation. Visitors who take the tours of the facility are given an educational talk about the rainforest and its inhabitants.\n\nKSTR operates entirely on donations. \n"}
{"id": "2449741", "url": "https://en.wikipedia.org/wiki?curid=2449741", "title": "Lightning strike", "text": "Lightning strike\n\nA lightning strike or lightning bolt is an electric discharge between the atmosphere and an Earth-bound object. They mostly originate in a cumulonimbus cloud and terminate on the ground, called cloud to ground (CG) lightning. A less common type of strike, called ground to cloud (GC), is upward propagating lightning initiated from a tall grounded object and reaches into the clouds. About 25% of all lightning events worldwide are strikes between the atmosphere and earth-bound objects. The bulk of lightning events are intra-cloud (IC) or cloud to cloud (CC), where discharges only occur high in the atmosphere.\n\nA single lightning event is a \"flash\", which is a complex, multi-stage process, some parts of which are not fully understood. Most cloud to ground flashes only \"strike\" one physical location, referred to as a \"termination\". The primary conducting channel, the bright coursing light that may be seen and is called a \"strike\", is only about 30 feet in diameter, but because of its extreme brilliance, it often looks much larger to the human eye and in photographs. Lightning discharges are typically miles long, but certain types of horizontal discharges can be upwards of tens of miles in length. The entire flash lasts only a fraction of a second. Most of the early formative and propagation stages are much dimmer and not visible to the human eye.\n\nLightning strikes can injure humans in several different ways:\n\nLightning strikes can produce severe injuries, and have a mortality rate of between 10% and 30%, with up to 80% of survivors sustaining long-term injuries. These severe injuries are not usually caused by thermal burns, since the current is too brief to greatly heat up tissues; instead, nerves and muscles may be directly damaged by the high voltage producing holes in their cell membranes, a process called electroporation.\n\nIn a direct strike, the electrical currents in the flash channel pass directly through the victim. The relatively high voltage drop around poorer electrical conductors (such as a human being), causes the surrounding air to ionize and break down, and the external flashover diverts most of the main discharge current so that it passes \"around\" the body, reducing injury.\n\nMetallic objects in contact with the skin may \"concentrate\" the lightning's energy, given it is a better natural conductor and the preferred pathway, resulting in more serious injuries, such as burns from molten or evaporating metal. At least two cases have been reported where a strike victim wearing an iPod suffered more serious injuries as a result.\n\nHowever, during a flash, the current flowing through the channel and around the body will generate large electromagnetic fields and EMPs, which may induce electrical transients (surges) within the nervous system or pacemaker of the heart, upsetting normal operations. This effect might explain cases where cardiac arrest or seizures followed a lightning strike that produced no external injuries. It may also point to the victim not being directly struck at all, but just being very close to the strike termination.\n\nAnother effect of lightning on bystanders is to their hearing. The resulting shock wave of thunder can damage the ears. Also, electrical interference to telephones or headphones may result in damaging acoustic noise.\n\nOne estimate is that 24,000 people are killed by lightning strikes around the world each year and about 240,000 are injured.\nAnother estimate is that the annual global death toll is 6,000.\nAccording to the NOAA, over the last 20 years, the United States averaged 51 annual lightning strike fatalities, placing it in the second position, just behind floods for deadly weather. In the US, between 9% and 10% of those struck die, for an average of 40 to 50 deaths per year (28 in 2008).\n\nIn Kisii in western Kenya, some 30 people die each year from lightning strikes. Kisii's high rate of lightning fatalities occurs because of the frequency of thunderstorms and because many of the area's structures have metal roofs.\n\nThese statistics do not reflect the difference between direct strikes, where the victim was part of the lightning pathway, indirect effects of being close to the termination point, like ground currents, and resultant, where the casualty arose from subsequent events, such as fires or explosions. Even the most knowledgeable first responders may not recognize a lightning related injury, let alone particulars, which a medical examiner, police investigator or on the rare occasion a trained lightning expert may have difficulty identifying to record accurately. This ignores the reality that lightning, as the first event, may assume responsibility for the overall and resulting accident.\n\nDirect strike casualties could be much higher than reported numbers.\n\nTrees are frequent conductors of lightning to the ground. Since sap is a relatively poor conductor, its electrical resistance causes it to be heated explosively into steam, which blows off the bark outside the lightning's path. In following seasons trees overgrow the damaged area and may cover it completely, leaving only a vertical scar. If the damage is severe, the tree may not be able to recover, and decay sets in, eventually killing the tree.\n\nIn sparsely populated areas such as the Russian Far East and Siberia, lightning strikes are one of the major causes of forest fires. The smoke and mist expelled by a very large forest fire can cause electric charges, starting additional fires many kilometers downwind.\n\nWhen water in fractured rock is rapidly heated by a lightning strike, the resulting steam explosion can cause rock disintegration and shift boulders. It may be a significant factor in erosion of tropical and subtropical mountains that have never been glaciated. Evidence of lightning strikes includes erratic magnetic fields.\n\nTelephones, modems, computers and other electronic devices can be damaged by lightning, as harmful overcurrent can reach them through the phone jack, Ethernet cable, or electricity outlet. Close strikes can also generate electromagnetic pulses (EMPs) – especially during \"positive\" lightning discharges.\n\nLightning currents have a very fast rise time, on the order of 40 kA per microsecond. Hence, conductors of such currents exhibit marked skin effect, causing most of the currents to flow through the outer surface of the conductor.\n\nIn addition to electrical wiring damage, the other types of possible damage to consider include structural, fire, and property damage.\n\nThe field of lightning protection systems is an enormous industry worldwide due to the impacts lightning can have on the constructs and activities of humankind. Lightning, as varied in properties measured across orders of magnitude as it is, can cause direct effects or have secondary impacts; lead to the complete destruction of a facility or process or simply cause the failure of a remote electronic sensor; it can result in outdoor activities being halted for safety concerns to employees as a thunderstorm nears an area and until it has sufficiently passed; it can ignite volatile commodities stored in large quantities or interfere with the normal operation of a piece of equipment at critical periods of time.\n\nMost lightning protection devices and systems protect physical structures on the earth, aircraft in flight being the notable exception. While some attention has been paid to attempting to control lightning in the atmosphere, all attempts proved extremely limited in success. Chaff and silver iodide crystal concepts were devised to deal directly with the cloud cells and were dispensed directly into the clouds from an overflying aircraft. The chaff was devised to deal with the electrical manifestations of the storm from within, while the silver iodide salting technique was devised to deal with the mechanical forces of the storm.\n\nHundreds of devices, including lightning rods and charge transfer systems, are used to mitigate lightning damage and influence the path of a lightning flash.\n\nA lightning rod (or lightning protector) is a metal strip or rod connected to earth through conductors and a grounding system, used to provide a preferred pathway to ground if lightning terminates on a structure. The class of these products are often called a \"finial\" or \"air terminal\". A lightning rod or \"Franklin rod\" in honor of its famous inventor, Benjamin Franklin, is simply a metal rod, and without being connected to the lightning protection system, as was sometimes the case in the old days, will provide no added protection to a structure. Other names include \"lightning conductor\", \"arrester\", and \"discharger\"; however, over the years these names have been incorporated into other products or industries with a stake in lightning protection. Lightning arrester, for example, often refers to fused links that explode when a strike occurs to a high voltage overhead power line to protect the more expensive transformers down the line by opening the circuit. In reality, it was an early form of a heavy duty surge protection device (SPD). Modern arresters, constructed with metal oxides, are capable of safely shunting abnormally high voltage surges to ground while preventing normal system voltages from being shorted to ground.\n\nThe exact location of a lightning strike or when it will occur is still impossible to predict. However, products and systems have been designed of varying complexities to alert people as the probability of a strike increases above a set level determined by a risk assessment for the location's conditions and circumstances. One significant improvement has been in the area of detection of flashes through both ground and satellite-based observation devices. The strikes and atmospheric flashes are not predicted, however the level of detail recorded by these technologies has vastly improved in the past 20 years.\n\nAlthough commonly associated with thunderstorms at close range, lightning strikes can occur on a day that seems devoid of clouds. This occurrence is known as \"A Bolt From the Blue\"; lightning can strike up to 10 miles from a cloud.\n\nLightning interferes with AM (amplitude modulation) radio signals much more than FM (frequency modulation) signals, providing an easy way to gauge local lightning strike intensity. To do so, one should tune a standard AM medium wave receiver to a frequency with no transmitting stations, and listen for crackles amongst the static. Stronger or nearby lightning strikes will also cause cracking if the receiver is tuned to a station. As lower frequencies propagate further along the ground than higher ones, the lower medium wave (MW) band frequencies (in the 500–600 kHz range) can detect lightning strikes at longer distances; if the longwave band (153–279 kHz) is available, using it can increase this range even further.\n\nLightning detection systems have been developed and may be deployed in locations where lightning strikes present special risks, such as public parks. Such systems are designed to detect the conditions which are believed to favor lightning strikes and provide a warning to those in the vicinity to allow them to take appropriate cover.\n\nThe U.S. National Lightning Safety Institute advises American citizens to have a plan for their safety when a thunderstorm occurs and to commence it as soon as the first lightning is seen or thunder heard. This is important as lightning can strike without rain actually falling. If thunder can be heard at all, then there is a risk of lightning. The safest place is inside a building or a vehicle. Risk remains for up to 30 minutes after the last observed lightning or thunder.\n\nThe National Lightning Safety Institute recommends using the F-B (flash to boom) method to gauge distance to a lightning strike. The flash of a lightning strike and resulting thunder occur at roughly the same time. But light travels 300,000 kilometers in a second, almost a million times the speed of sound. Sound travels at the slower speed of 344 m/s, so the flash of lightning is seen before thunder is heard. A method to determine the distance between lightning strike and viewer, involves counting the seconds between the lightning flash and thunder. Then, dividing by three to determine the distance in kilometers, or by five for miles. Immediate precautions against lightning should be taken if the F-B time is 25 seconds or less, that is, if the lightning is closer than .\n\nReports differ regarding what to do if caught outside during a storm. One study shows that prostration is safer than lying down flat when there are no other alternatives. A contrasting report suggested that it did not matter whether a person was standing up, squatting, or lying down when outside during a thunderstorm, because lightning can travel along the ground; this report suggested it was safest to be inside a solid structure or vehicle. In the United States, the average annual death toll from lightning is 51 deaths per year, although there were only 23 deaths in 2013, which was a record low; the riskiest activities include fishing, boating, camping, and golf. A person injured by lightning does not carry an electrical charge, and can be safely handled to apply first aid before emergency services arrive. Lightning can affect the brainstem, which controls breathing.\n\nSeveral studies conducted in South Asia and Africa suggest that the dangers of lightning are not taken sufficiently seriously there. A research team from the University of Colombo found that even in neighborhoods which had experienced deaths from lightning, no precautions were taken against future storms. An expert forum convened in 2007 to address how to raise awareness of lightning and improve lightning protection standards, and expressed concern that many countries had no official standards for the installation of lightning rods.\n\nAll events associated or suspected of causing damage are called \"lightning incidents\" due to four important factors.\nAs such it is often inconclusive, albeit highly probably a lightning flash was involved, hence categorizing it as a \"lightning incident\" covers all bases.\n\n\n\n\n\n"}
{"id": "9974564", "url": "https://en.wikipedia.org/wiki?curid=9974564", "title": "Linjeflyg Flight 277", "text": "Linjeflyg Flight 277\n\nLinjeflyg Flight 277 was a controlled flight into terrain by a Convair 440-75 Metropolitan on 20 November 1964 at 21:14 in Ängelholm, Skåne, Sweden. The Linjeflyg pilots, misled by a non-conventional military runway light configuration, descended too early and on a faulty course during approach to Ängelholm–Helsingborg Airport. The crash killed 31 of 43 people on board, making it the deadliest aviation accident in Sweden.\n\nThe flight was en route from Stockholm to Ängelholm, but bad weather caused it to skip stopovers at Hultsfred and Halmstad. There was less than visibility and a low cloud base at Ängelholm, so air traffic control lit its approach lighting system. As the civilian sector at Swedish Air Force base F 10 Ängelholm, the airport had a military configuration and did not follow normal civilian configuration. This caused a pilot error in the navigation and the aircraft landed before the runway threshold. The aircraft inverted while sliding after impact. Despite the death toll, three people walked uninjured from the crash.\n\nThe investigation commission found no fault of the pilot or air traffic control, instead focusing on short-cuts being taken by the Swedish Air Force and the Civil Aviation Administration (CAA) to not configure military airports in line with civilian regulations. The finding caused a surge of funding which subsequently caused military airports to change their instrument landing system and approach lighting system to meet civilian requirements.\n\nThe accident aircraft was a Convair CV-340-62, which had been converted to a CV-440 Metropolitan. It had production date 23 June 1954 and was delivered to Real Transportes Aéreos of Brazil on 17 November 1955, where it was registered as PP-YRC. It became the property of Varig with the merger of the two airlines in August 1961. This made the aircraft superfluous and it was subsequently sold to Linjeflyg on 13 December 1961 through the holding company Airtaco. The aircraft was registered in Sweden on 14 March 1962 as SE-CCK. It was subsequently sent to Oslo for conversion to CV-440 Metroliner. Ownership was transferred from Airtaco to its owner, \"Dagens Nyheter\", in 1962. They sold it to Aerotransport on 1 October 1964.\n\nFlight 267 was a scheduled, domestic service which was scheduled to fly from Stockholm Bromma Airport to Ängelholm–Helsingborg Airport, with intermediate stops at Hultsfred Airport and Halmstad Airport. Due to poor weather it was decided that the aircraft would not land at Hultsfred and the flight code was changed to FL 267V to reflect this. The aircraft had a flight crew of four, of which one of the two flight attendants was under training. Thirty-nine passengers boarded the aircraft in Stockholm, including one child and three members of Parliament. It departed Bromma at 19:46. It cruised at an altitude of 3,600 meters (12,000 ft). While en-route increasing fog was observed at Halmstad and the meteorologist at Ängelholm recommended that the aircraft bypass Halmstad and fly directly to Ängelholm.\nÄngelholm–Helsingborg Airport was primarily a military air station, denoted F 10 Ängelholm. Because of this its instrument landing system was configured significantly different from most civilian airports. The runway's two radio beacons, LJ and J, were located at unusual distances from normal. LJ was situated from the runway threshold instead of the normal and J was located at instead of the normal . Also, the approach lighting system's strobeacon was located from and starboard of the runway's center line, but this light was not indicated on the instrument, approach and landing chart. Therefore, any aircraft which would pass beacon and aimed for the approach light would be aligned to land right of the runway.\n\nÄngelholm was experiencing rain and visibility between . The cloud base was only . The crew considered diverting to another suitable location, such as Malmö Bulltofta Airport and Copenhagen Airport, or even returning to Stockholm, but upon reaching Halmstad the pilots chose to make a direct approach to Ängelholm runway 14 using visual flight rules. Given the weather conditions this was a highly unusual landing plan: normal procedure would be to turn northwest and carry out an instrument approach instead.\n\nÄngelholm tower contacted the flight crew at 20:57 and issued the latest weather report, which was for a slight clearing. At 21:08 the pilots confirmed that they were at an altitude of 600 meters (2,000 ft) and that they were aiming for LJ. The tower informed the pilots that they had lit a strobeacon, which was (incorrectly) said to be located on the runway center line, from the threshold (it was actually 110m off center, to the right). At 21:13:10 the tower commenced directing the flight of the aircraft, asking it to fly to the left, as the air traffic controller could see it was off course. The last transmission from the aircraft was made at 21:13:47.\n\nThe aircraft was off course and at too low an altitude. This was discovered by the pilots prior to impact and they attempted to pull the aircraft up, but hit a field with its starboard wing tip and landing gear. It succeeded at ascending slightly, but continued to fly nearly at ground level. Eighty meters (260 ft) later it collided with the overhead lines of the West Coast Railway Line, knocking off two concrete posts. It continued for another before hitting ground, at which point it inverted. It slid for another before coming to a halt, from a house. Fire broke out in some of the wreck parts, although not in the main section of fuselage.\n\nThirty-one people on board were killed, including both the pilots. Three of the survivors were not injured, including the flight attendants. Some of the survivors were able to free themselves and each other and walk out of the fuselage. Most were hanging in their seat belts, wedged in by the wreckage. The fire departments in Ängelholm and Vejbystrand arrived at the scene eleven minutes after the crash and started freeing the survivors. The airport's rescue service was on the scene seventeen minutes after the accident. Nine people were seriously injured and were brought to the field hospital at the air base. The survivors were generally seated in the aft of the cabin. One of the passengers had brought their cat along in a cage on the flight, and it survived the accident.\n\nAn \"ad hoc\" investigation was appointed and later that night a Douglas DC-3 of officials and experts from the airline and the CAA were dispatched to Ängelholm. The commission carried out test flights to Ängelholm and concluded that it was fully possible for the pilot to mistake the strobeacon for the centerline lights, though the finding was not conclusive as the two types of lighting are still somewhat distinctive. Linjeflyg pilots reported that they had previously made the same mistake, but that the issue had otherwise always been detected and they had corrected their course, landing safely. A quarter of Linjeflyg's pilots were not aware of the strobeacon.\n\nDuring the investigation there was a large media coverage of a dispute between the Swedish Airline Pilots Association (SPF) and the Swedish Air Traffic Controllers Association (SFTF). The former accused the F 10 air traffic control for not following correct civil aviation procedures. SFTF responded by an official letter to the government questioning why the pilots were represented in the commission, as this could pose a potential conflict of interest. Both parties were criticized for speculating about the cause of the accident before the conclusion of the commission.\n\nThe commission stated that the probable cause of the accident was that the crew had carried out a premature descent. This was caused by the crew misunderstanding the lighting arrangement at the airport due to lack of proper information about its configuration. The report did not criticize the pilots for choosing to land with visual flight rules, nor for carrying out the landing in the encountered weather conditions. The commission found that no individuals were at fault in the incident and that it had been caused by a series of system errors. It placed responsibility with both the CAA and the Air Force for the inadequate configuration of the lighting system and for not properly following civilian regulations at military airports.\n\nAfter the accident Linjeflyg resumed service to Ängelholm with Douglas DC-3 aircraft. The airline changed its procedures regarding landing at Ängelholm, making more strict policies in regard to minimum visibility. Both radio beacons were moved to the conventional civilian locations. Flight 277 was the seventh loss of a Convair CV-240 family aircraft. At the time it was deadliest and now remains the fifth-deadliest accident of the type. The accident remains the deadliest aviation accident in Sweden. Linjeflyg would suffer one other fatal accident, Flight 618 in 1977, although it was a wet lease operated by Skyline.\n\nThe power balance between the CAA and the Swedish Air Force prior to the accident was skewed whereby the latter permitted civilian flights to their air bases on the condition that they did not intervene in military operations and procedures. The commission’s findings caused an increased focus on safety by the Civil Aviation Administration and a shift in attitude. Specifically, the military air bases with joint traffic were reconfigured to meet international civil standards in their instrument landing and lighting systems. This was made possible because the government, in the wake of the accident, increased funding to the CAA to improve the systems.\n"}
{"id": "3251560", "url": "https://en.wikipedia.org/wiki?curid=3251560", "title": "Mercedes-Benz O405", "text": "Mercedes-Benz O405\n\nThe Mercedes-Benz O405 was a highly successful single-decker bus manufactured by Mercedes-Benz from the mid-1980s to the early 2000s as either an integral bus or a bus chassis and was the last VöV SL-II standard bus in production. It was the replacement for the Mercedes-Benz O305 and was widely used in Europe, the United Kingdom, Australia and Singapore.\n\nThe step-entrance version was known as the O405. A 3-axle articulated version was also built known as O405G. There were two generations of O405, designated as O405 MkI and O405 MkII. Most of them have their boxy roof dome (slightly arched) with a double-curvature windscreen, separately mounted destination indicator and pantograph system windshield wipers that was used on some buses such as the Dennis Dart, Leyland Lynx, MAN NL262 and the MAN SL202.\n\nThe O405 MkI was marketed between mid-1980s and early 1990s. It featured a Mercedes Benz OM447h naturally aspirated engine with outputs of either 157kW (210hp) or 184 kW (250 hp). Optionally available was a naturally aspirated compressed natural gas (CNG) engine model M447hG with 150 kW (205 hp) output. The gearbox coupled to the engine was usually the Mercedes-Benz W3E110/2.2R or Mercedes-Benz W3E112/2.2R (the former being able to handle the more powerful 184 kW engine), although there have been other gearboxes such as the ZF 5HP 500 or Allison B300R coupled to the engine.\n\nThe O405 MkII was marketed from the early 1990s to the late 1990s and into the early 2000s in some parts of the world. It featured a Mercedes-Benz OM447hA turbocharged engine with an output of 184 kW (250 hp), although some examples feature either a naturally aspirated engine (OM447h-II), a turbocharged engine (OM447hA) or a turbocharged, intercooled engine (OM447hLA). The gearbox coupled to the engine was either the ZF 4HP 500 or 5HP 500, or the Voith D864.3.\n\nFrom 1994, this chassis was available with the M447hG Euro II 175 kW (238 hp) naturally aspirated compressed natural gas (CNG) engine.\n\nThe low-floor version of the regular O405 was known as the O405N (or O405GN for the 3-axle articulated version), later a further developed version was also built, it was known as the O405N²/O405N2 (or O405GN²/O405GN2 for the 3-axle articulated version). The O405(G)N do not have steps at the entrances and exits, but the seats are mounted on \"platforms\". The GN2 type addressed this problem. These buses are usually fitted with ZF transmissions, but some are fitted with Voith examples. It has a boxy roof dome (slightly arched) similar to the MAN NL202 and the MAN NL262 with a double-curvature windscreen with a separately mounted destination display just that it has a full low floor layout with seats mounted on platforms.\n\nThe low-entry version of the O405 was called the O405NH chassis was produced by EvoBus for the Australian market. Much of its popularity with government and private operators alike can be attributed to the popularity of the Mercedes-Benz O405 MkII chassis that it replaced.\n\nThe chassis was derived from a combination of the rear modules of an O405 MkII chassis and the front modules of an O405N²/O405GN² chassis. Because of the difference in height between the front and rear modules of the chassis, there are one or more steps leading up from behind the centre door position to a standard O405 floorline. Because the chassis has a horizontally-mounted engine, there's no room for a rear door. This \"low-entry\" concept has become very popular in Europe - many integral products using this concept have been released such as the Mercedes-Benz Citaro LE.\n\nThe suburban version was called O407 (or O407G for the 3-axle articulated version), It had a single door at the front and a pair in the middle, with all of the seats facing towards the front of the bus. It featured a Mercedes Benz OM447h naturally aspirated engine with output 177kW (237hp) and 6-speed manual Mercedes-Benz gearbox.\n\n\n\nA total of 365 O405s were built for operators in the United Kingdom, with Travel West Midlands purchasing 204. A sole O405Gs was bodied by Alexander for Grampian Regional Transport.\n\nA total of 204 O405Ns were built for operators in the United Kingdom, with Travel West Midlands purchasing 193. A total of 15 O405GNs were built for operators in the United Kingdom, with Travel West Midlands purchasing 11.\n\nDiscovery Bay Transit Services of Hong Kong acquired four O405 single-decker buses between 1995 and 1998, two were fitted with locally-built bodywork by Asia Auto Body Works and the other two had Hispano Carrocera bodywork.\n\nSydney Buses took delivery of 247 Pressed Metal Corporation bodied O405s between 1987 and October 1990. The last were retired in early 2016. In January 1997 it received two Ansair bodied O405Ns. It also inherited four Custom Coaches bodied O405s when it purchased North & Western Bus Lines. Between October 1999 and December 2002 it received 300 Custom Coaches bodied CNG O405NHs with the O405NH discontinued after this order.\n\nThe 300 Custom Coaches 0405NH ordered by State Transit were delivered from Germany as a fully constructed frame and panels were fitted by Custom Coaches, so that as many of these buses could be on the road in time for the Sydney Olympic Games. They can seat 45 passengers and are 12.5 metres long. They have M447hG Euro 2 engines producing 175 kW.\n\nTransperth took delivery of 48 natural gas and 349 diesel O405NHs. Other purchasers of O405s included Busways, Quince's Scenicruisers and Westbus.\n\nSingapore Bus Services ordered 700 Mercedes-Benz O405 to replace its first batch of Mercedes-Benz OF1413s between 1990 and 1992. They were bodied by Alexander PS and Duple Metsec (OAC (original air-con) and NAC (non-aircon)). The non air-con buses were later converted to air-con buses in 1998-99.\n\nAll were withdrawn from 1 March 2008 to 3 June 2011 with all replaced by Scania K230UB. A few O405s were shipped to Thailand and were rebodied due to the bodywork being no longer designed whereas the rest of them were scrapped and only one unit was converted by the National Library Board.\n\nTrans-Island Bus Services bought Mercedes-Benz O405 with Hispano Carrocera (OAC (original air-con) and CAC (converted air-con) and Volgren CR221 bodies to replace the ageing Nissan U31RCN. All Mercedes-Benz O405 with Hispano Carrocera bodywork had been retired by 25 September 2016. The Volgren-bodied O405s on the other hand are used as permanent training buses. It was fully retired by late February 2017. TIBS had bought for the first time, various batch of bendy buses (Hispano MK1 using the box body, Hispano MK2 using parts from Mercedes-Benz O530 Citaro, Hispano Habit and Volgren) bodies between 1996 and 2004. They will be retiring by 1 January 2019. \n\n\nProduction of the Mercedes-Benz O405 ended in 2002 when the last bendy buses were delivered to Trans-Island Bus Services. The Mercedes-Benz O405 / O405N series was superseded by the Citaro, and the O405NH was superseded by the OC500LE.\n\n<br>\n<br>\n<br>\n"}
{"id": "21888868", "url": "https://en.wikipedia.org/wiki?curid=21888868", "title": "Mercury polycations", "text": "Mercury polycations\n\nMercury polycations are polyatomic cations that contain only mercury atoms. The best known example is the ion, found in mercury(I) (mercurous) compounds. The existence of the metal-metal bond in Hg(I) compounds was established using X-ray studies in 1927 and Raman spectroscopy in 1934 making it one of the earliest, if not the first, covalent metal-metal bonds to be characterised.\n\nOther mercury polycations are the linear and ions, and the triangular ion and a number of chain and layer polycations.\n\nThe best known polycation of mercury is , in which mercury has a formal oxidation state of +1. The ion was perhaps the first metal-metal bonded species confirmed. The presence of the ion in solution was shown by Ogg in 1898. In 1900, Baker showed the presence of HgCl dimers in the vapour phase. The presence of units in the solid state was first determined in 1926 using X-ray diffraction. The presence of the metal-metal bond in solution was confirmed using Raman spectroscopy in 1934.\n\nMinerals that are known that contain the cation include eglestonite.\n\nCompounds containing the linear (mercury()) and (mercury()) cations have been synthesised. These ions are only known in the solid state in compounds such as and . The Hg–Hg bond length is 255 pm in , and 255–262 pm in . The bonding involves 2-centre-2-electron bonds formed by 6s orbitals.\n\nThe triangular cation was confirmed in a reinvestigation of the mineral terlinguaite in 1989 and subsequently synthesised in a number of compounds. The bonding has been described in terms of a three-center two-electron bond where overlap of the 6s orbitals on the mercury atoms gives (in D symmetry) a bonding \"a\" orbital.\n\nThe golden yellow compound ), named \"alchemists' gold\" by its discoverers, contains perpendicular chains of Hg atoms.\n\nThe \"metallic\" compounds and contain hexagonal layers of mercury atoms separated by layers of anions. They are both superconductors below 7 K.\n"}
{"id": "3096890", "url": "https://en.wikipedia.org/wiki?curid=3096890", "title": "Minor actinide", "text": "Minor actinide\n\nThe minor actinides are the actinide elements in used nuclear fuel other than uranium and plutonium, which are termed the major actinides. The minor actinides include neptunium (element 93), americium (element 95), curium (element 96), berkelium (element 97), californium (element 98), einsteinium (element 99), and fermium (element 100). The most important isotopes in spent nuclear fuel are neptunium-237, americium-241, americium-243, curium-242 through -248, and californium-249 through -252.\n\nPlutonium and the minor actinides will be responsible for the bulk of the radiotoxicity and heat generation of used nuclear fuel in the medium term (300 to 20,000 years in the future).\n\nThe plutonium from a power reactor tends to have a greater amount of Pu-241 than the plutonium generated by the lower burnup operations designed to create weapons-grade plutonium. Because the reactor-grade plutonium contains so much Pu-241 the presence of americium-241 makes the plutonium less suitable for making a nuclear weapon. The ingrowth of americium in plutonium is one of the methods for identifying the origin of an unknown sample of plutonium and the time since it was last separated chemically from the americium.\n\nAmericium is commonly used in industry as both an alpha particle and as a low photon energy gamma radiation source. For instance it is used in many smoke detectors. Americium can be formed by neutron capture of Pu-239 and Pu-240 forming Pu-241 which then beta decays to Am-241. In general, as the energy of the neutrons increases, the ratio of the fission cross section to the neutron capture cross section changes in favour of fission. Hence if MOX is used in a thermal reactor such as a boiling water reactor (BWR) or pressurized water reactor (PWR) then more americium can be expected in the used fuel than that from a fast neutron reactor.\n\nSome of the minor actinides have been found in fallout from bomb tests. See Actinides in the environment for details.\n"}
{"id": "11068261", "url": "https://en.wikipedia.org/wiki?curid=11068261", "title": "Operating reserve", "text": "Operating reserve\n\nIn electricity networks, the operating reserve is the generating capacity available to the system operator within a short interval of time to meet demand in case a generator goes down or there is another disruption to the supply. Most power systems are designed so that, under normal conditions, the operating reserve is always at least the capacity of the largest generator plus a fraction of the peak load.\n\nThe operating reserve is made up of the spinning reserve as well as the non-spinning or supplemental reserve:\n\n\nGenerators that intend to provide either spinning and non-spinning reserve should be able to reach their promised capacity within roughly ten minutes. Most power system guidelines require a significant fraction of their operating reserve to come from spinning reserve. This is because the spinning reserve is slightly more reliable (it doesn't suffer from start-up issues) and can respond immediately whereas with non-spinning reserve generators there is a delay as the generator starts-up offline.\n\nIn addition, there are two other kinds of reserve power that are often discussed in combination with the operating reserve: the frequency-response reserve and the replacement reserve.\n\n\nOperating reserve is a crucial concept for ensuring that the day-ahead planning of generators' schedule can withstand the uncertainty due to unforeseen variations in the load profile or equipment (generators, transformers, transmission links) faults.\n\nThe California System Operator has an operating reserve at 6% of the metered load. Included in that is a spinning reserve at 3% of the metered load.\n"}
{"id": "43814081", "url": "https://en.wikipedia.org/wiki?curid=43814081", "title": "Pellet boiler", "text": "Pellet boiler\n\nPellet boilers are used in central heating systems for heat requirements (heating load) from 3.9 kW (Kilowatt) to 1 MW (Megawatt) or more. Pellet central heating systems are not only used in single family homes, but also in larger residential, commercial, or institutional applications. Pellet burner systems run most efficiently at full load and can usually be regulated down to 30% of full load. Since the warm up phase of pellet boilers usually takes longer than for oil or gas firing systems, short burning phases have negative effects on the fuel efficiency. In order to improve energy efficiency and reduce harmful emissions, pellet boilers are usually combined with buffer systems, insulated water tanks for example.\n\nSimilar to wood chip fuel heating systems also pellet fuel is delivered periodically and automatically from the pellet storage (for central heating systems) or the day tank (for pellet stove) according to need in the combustion chamber. With the heat generated from the heating circuit water is heated in the boiler of the pellet in pellet central heating systems. The heat distribution is the same as in other systems, which use water for heat distribution. Unlike for oil or gas heating systems, pellet heating systems demand the integration of a hot water tank in the heating system in order to reduce heat losses.\n\nThe furnace is automatically supplied with combustible material. The control technology of the system regulates the fuel input gradually in order to match the required heat output. Depending on the specific system, the supplied wood pellets are automatically ignited either with hot air blowers, or it uses a permanent ember bed in the combustion chamber.\n\nWood pellet heating systems work with different techniques of charging and combustion: Today there are specifically developed loading techniques for pellet combustion like drop chute firing, underfeed firing, side-fed firing or the use of a roller grate system. The method of charging and combustion of the pellet fuel is divided into 5 technologies.\n\n\nFor more efficiency and less pollution in the air, modern pellet heating systems control combustion either via a temperature or flame space sensor in combination with an infinitely variable input of combustion air via a suck-blow fan or a lambda probe. The hot flue gases are led into the chimney via a heat exchanger with manual or automatic cleaning of the reheating surfaces.\n\nPellet systems are available in different power ranges from about 3.9 Kilowatt single ovens between about 4 and 20 kW. Most systems available today have a power control over the fuel and combustion air supply, so that they can be operated at full load and at part load. Currently pellet boilers achieve a combustion efficiency of about 85-95% at full load (nominal thermal output) in thermal power operation.\n\nWith few exceptions, the efficiency decreases when the pellets boiler operates at partial load. The technical heating efficiencies described may vary greatly from the actual plant efficiencies, the reason the plant concept plays a major role. The use of a sufficiently large buffer storage is useful.\n\nThe wood pellets are stored in bulk in a tank or storage area and supplied to the burner by means of a conveyor system. The storage area must be dry, since the pellets react hygroscopic (Hygrocopy) on clammy walls or high humidity during storage with crumbling.\n\nCompared to oil, wood pellets require about three times the storage volume, though with less technical effort for the space, as unlike pellets, heating oils are water-polluting substances. For storage, the pellets can be accommodated in a single storage space. The floor should be in the shape of a funnel usually made out of wood. At the end of the funnel is the inlet for the screw conveyor or the extraction pipe. More outlets in the storage room ensure unobstructed operation even in case of problems of one of the extraction points. Alternatives to a storage room are prefabricated tanks made of fabric or sheet steel. Buried underground tanks or free-standing silos can be used, if sufficient space is available in the building. In areas with high humidity it is important to use must tight tank systems to ensure the quality of pellet fuel.\n\nTo convey the pellets from the storage facility to the boiler room different systems are in use: Blower or screw systems can be used. The choice depends primarily on the distance from the storage to the boiler room. For distances greater than two meters flexible multi-stage screw conveyors are usually necessary. Blower systems can be used flexibly and feed distances up to 20 m. The discharge from the storage room or container is usually also supported by an inclined tank bottom or a hopper outlet.\n\n"}
{"id": "40466295", "url": "https://en.wikipedia.org/wiki?curid=40466295", "title": "Plasma diffusion", "text": "Plasma diffusion\n\nThe plasma diffusion across the magnetic field is an important topic in magnetic confinement of fusion plasma. It is especially concerning how the plasma transport is reduced by the strength of the external magnetic field. The classical diffusion gives the 1/B scaling, while the Bohm diffusion, borne out of experimental observations from the early confinement machines, was conjectured to follow the 1/B scaling. The Hsu diffusion predicts the 1/B scaling, that is presumably the best confinement scenario in magnetized plasma.\n\n"}
{"id": "14057225", "url": "https://en.wikipedia.org/wiki?curid=14057225", "title": "Plasma pencil", "text": "Plasma pencil\n\nThe plasma pencil is a dielectric tube where two disk-shaped electrodes of about the same diameter as the tube are inserted, and are separated by a small gap. Each of the two electrodes is made of a thin copper ring attached to the surface of a centrally perforated dielectric disk. The plasma is ignited when nanoseconds-wide high voltage pulses at kHz repetition rate are applied between the two electrodes and a gas mixture (such as helium and oxygen) is flown through the holes of the electrodes. When a plasma is ignited in the gap between the electrodes, a plasma plume reaching lengths up to 12 cm is launched through the aperture of the outer electrode and into the surrounding room air. The cold plasma plume emitted by the plasma pencil can be used to kill bacteria without harming skin tissue. \n\nApplications of the plasma pencil are in wound healing, killing of oral bacteria, and in controlled surface modification of heat-sensitive materials. The plasma pencil was invented by Mounir Laroussi, a plasma science professor at Old Dominion University, Norfolk, VA, USA.\n\n\n"}
{"id": "2214999", "url": "https://en.wikipedia.org/wiki?curid=2214999", "title": "Plasma scaling", "text": "Plasma scaling\n\nThe parameters of plasmas, including their spatial and temporal extent, vary by many orders of magnitude. Nevertheless, there are significant similarities in the behaviors of apparently disparate plasmas. Understanding the scaling of plasma behavior is of more than theoretical value. It allows the results of laboratory experiments to be applied to larger natural or artificial plasmas of interest. The situation is similar to testing aircraft or studying natural turbulent flow in wind tunnels with smaller-scale models.\n\nSimilarity transformations (also called similarity laws) help us work out how plasma properties change in order to retain the same characteristics. A necessary first step is to express the laws governing the system in a nondimensional form. The choice of nondimensional parameters is never unique, and it is usually only possible to achieve by choosing to ignore certain aspects of the system.\n\nOne dimensionless parameter characterizing a plasma is the ratio of ion to electron mass. Since this number is large, at least 1836, it is commonly taken to be infinite in theoretical analyses, that is, either the electrons are assumed to be massless or the ions are assumed to be infinitely massive. In numerical studies the opposite problem often appears. The computation time would be intractably large if a realistic mass ratio were used, so an artificially small but still rather large value, for example 100, is substituted. To analyze some phenomena, such as lower hybrid oscillations, it is essential to use the proper value.\n\nOne commonly used similarity transformation was derived for gas discharges by James Dillon Cobine (1941), Alfred Hans von Engel and Max Steenbeck (1934). They can be summarised as follows:\n\nThis scaling applies best to plasmas with a relatively low degree of ionization. In such plasmas, the ionization energy of the neutral atoms is an important parameter and establishes an absolute \"energy\" scale, which explains many of the scalings in the table:\n\nWhile these similarity transformations capture some basic properties of plasmas, not all plasma phenomena scale in this way. Consider, for example, the degree of ionization, which is dimensionless and thus would ideally remain unchanged when the system is scaled. The number of charged particles per unit volume is proportional to the current density, which scales as \"x\", whereas the number of neutral particles per unit volume scales as \"x\" in this transformation, so the degree of ionization does not remain unchanged but scales as \"x\".\n\n"}
{"id": "24573046", "url": "https://en.wikipedia.org/wiki?curid=24573046", "title": "Popa Mountain National Park", "text": "Popa Mountain National Park\n\nPopa Mountain National Park () is a national park of Burma. It is located in Kyaukpadaung Township in Mandalay Division. It occupies an area of and was established in 1989. It surrounds Mount Popa.\n\n"}
{"id": "3136380", "url": "https://en.wikipedia.org/wiki?curid=3136380", "title": "Scary sharp", "text": "Scary sharp\n\nScary sharp is a method of sharpening woodworking tools with sandpaper instead of conventional methods of oilstone or waterstone sharpening. The sandpaper referred to here can be any abrasive impregnated sheet used in the various industries to smooth surfaces and examples include glass paper, silicon carbide, emery cloth, etc. The sandpaper is affixed to another hard, flat substrate to create the sharpening surface. Sheet-glass is commonly used, but a machinist's granite surfacing block, marble baking slabs, plywood, medium-density fibreboard (MDF) or even jointer out-feed tables will produce satisfactory results. The method of fixation is usually a matter of the user's preference, and can include plain water (by means of surface tension), sprayed-on adhesive, or by simply using adhesive-backed abrasive paper. This is exactly the same method as that used by materials scientists in preparing polished samples for metallography.\n\nThe basics of the method involves holding the cutting area of the chisel or plane iron flat to the sandpaper and gently moving back and forth in either side to side or back to front motions, as one would with a sharpening stone. The blade is taken through a series of increasingly finer grades of sandpaper.\nThe most notable advantage of the scary sharp system is speed and cost. Anyone with access to sandpaper and a reasonably flat surface can sharpen any cutting tool, often with good results. Unlike traditional stones, no maintenance is required for the scary-sharp set-up, since the sharpening surface does not form a hollow with repeated, uneven usage. A hollowed surface is undesirable in most sharpening situations, an example being the flat surface required in the back (as opposed to the beveled surface) of most woodworking plane irons. Compared to stones, sandpaper is lower in cost as well. A fresh sheet of sandpaper often cuts significantly faster than a bench sharpening stone. For this reason, coarse-grit papers can be used to dress damaged cutting edge quickly, removing chips along the edge and preparing it for the next stage/grit of edge refinement.\n\nThe blade is assumed to be well shaped, with a clean accurate bevel. The back of the chisel is also flattened and cleaned up using the same grades of sandpaper to ensure a clean sharp edge where the back and bevel meet. Nicks, deep scratches, or excessive wear may call for shaping the profile on a grinder before beginning this method.\n\n\n"}
{"id": "27709", "url": "https://en.wikipedia.org/wiki?curid=27709", "title": "Semiconductor", "text": "Semiconductor\n\nA semiconductor material has an electrical conductivity value falling between that of a metal, like copper, gold, etc. and an insulator, such as glass. Their resistance decreases as their temperature increases, which is behaviour opposite to that of a metal. Their conducting properties may be altered in useful ways by the deliberate, controlled introduction of impurities (\"doping\") into the crystal structure. Where two differently-doped regions exist in the same crystal, a semiconductor junction is created. The behavior of charge carriers which include electrons, ions and electron holes at these junctions is the basis of diodes, transistors and all modern electronics. Some examples of semiconductors are silicon, germanium, and gallium arsenide. After silicon, gallium arsenide is the second most common semiconductor used in laser diodes, solar cells, microwave frequency integrated circuits, and others. Silicon is a critical element for fabricating most electronic circuits.\n\nSemiconductor devices can display a range of useful properties such as passing current more easily in one direction than the other, showing variable resistance, and sensitivity to light or heat. Because the electrical properties of a semiconductor material can be modified by doping, or by the application of electrical fields or light, devices made from semiconductors can be used for amplification, switching, and energy conversion.\n\nThe conductivity of silicon is increased by adding a small amount of pentavalent (antimony, phosphorus, or arsenic) or trivalent (boron, gallium, indium) atoms (part in 10). This process is known as doping and resulting semiconductors are known as doped or extrinsic semiconductors. Apart from doping, the conductivity of a semiconductor can equally be improved by increasing its temperature. This is contrary to the behaviour of a metal in which conductivity decreases with increase in temperature.\n\nThe modern understanding of the properties of a semiconductor relies on quantum physics to explain the movement of charge carriers in a crystal lattice. Doping greatly increases the number of charge carriers within the crystal. When a doped semiconductor contains mostly free holes it is called \"p-type\", and when it contains mostly free electrons it is known as \"n-type\". The semiconductor materials used in electronic devices are doped under precise conditions to control the concentration and regions of p- and n-type dopants. A single semiconductor crystal can have many p- and n-type regions; the p–n junctions between these regions are responsible for the useful electronic behavior.\n\nAlthough some pure elements and many compounds display semiconductor properties, silicon, germanium, and compounds of gallium are the most widely used in electronic devices. Elements near the so-called \"metalloid staircase\", where the metalloids are located on the periodic table, are usually used as semiconductors.\n\nSome of the properties of semiconductor materials were observed throughout the mid 19th and first decades of the 20th century. The first practical application of semiconductors in electronics was the 1904 development of the cat's-whisker detector, a primitive semiconductor diode used in early radio receivers. Developments in quantum physics in turn allowed the development of the transistor in 1947 and the integrated circuit in 1958.\n\nSemiconductors with high thermal conductivity can be used for heat dissipation and improving thermal management of electronics.\n\n\nA large number of elements and compounds have semiconducting properties, including:\n\nMost common semiconducting materials are crystalline solids, but amorphous and liquid semiconductors are also known. These include hydrogenated amorphous silicon and mixtures of arsenic, selenium and tellurium in a variety of proportions. These compounds share with better known semiconductors the properties of intermediate conductivity and a rapid variation of conductivity with temperature, as well as occasional negative resistance. Such disordered materials lack the rigid crystalline structure of conventional semiconductors such as silicon. They are generally used in thin film structures, which do not require material of higher electronic quality, being relatively insensitive to impurities and radiation damage.\n\nAlmost all of today's electronic technology involves the use of semiconductors, with the most important aspect being the integrated circuit (IC), which are found in laptops, scanners, cell-phones, etc. Semiconductors for ICs are mass-produced. To create an ideal semiconducting material, chemical purity is paramount. Any small imperfection can have a drastic effect on how the semiconducting material behaves due to the scale at which the materials are used.\n\nA high degree of crystalline perfection is also required, since faults in crystal structure (such as dislocations, twins, and stacking faults) interfere with the semiconducting properties of the material. Crystalline faults are a major cause of defective semiconductor devices. The larger the crystal, the more difficult it is to achieve the necessary perfection. Current mass production processes use crystal ingots between in diameter which are grown as cylinders and sliced into wafers.\n\nThere is a combination of processes that is used to prepare semiconducting materials for ICs. One process is called thermal oxidation, which forms silicon dioxide on the surface of the silicon. This is used as a gate insulator and field oxide. Other processes are called photomasks and photolithography. This process is what creates the patterns on the circuity in the integrated circuit. Ultraviolet light is used along with a photoresist layer to create a chemical change that generates the patterns for the circuit.\n\nEtching is the next process that is required. The part of the silicon that was not covered by the photoresist layer from the previous step can now be etched. The main process typically used today is called plasma etching. Plasma etching usually involves an etch gas pumped in a low-pressure chamber to create plasma. A common etch gas is chlorofluorocarbon, or more commonly known Freon. A high radio-frequency voltage between the cathode and anode is what creates the plasma in the chamber. The silicon wafer is located on the cathode, which causes it to be hit by the positively charged ions that are released from the plasma. The end result is silicon that is etched anisotropically.\n\nThe last process is called diffusion. This is the process that gives the semiconducting material its desired semiconducting properties. It is also known as doping. The process introduces an impure atom to the system, which creates the p-n junction. In order to get the impure atoms embedded in the silicon wafer, the wafer is first put in a 1,100 degree Celsius chamber. The atoms are injected in and eventually diffuse with the silicon. After the process is completed and the silicon has reached room temperature, the doping process is done and the semiconducting material is ready to be used in an integrated circuit.\n\nSemiconductors are defined by their unique electric conductive behavior, somewhere between that of a conductor and an insulator.\nThe differences between these materials can be understood in terms of the quantum states for electrons, each of which may contain zero or one electron (by the Pauli exclusion principle). These states are associated with the electronic band structure of the material.\nElectrical conductivity arises due to the presence of electrons in states that are delocalized (extending through the material), however in order to transport electrons a state must be \"partially filled\", containing an electron only part of the time. If the state is always occupied with an electron, then it is inert, blocking the passage of other electrons via that state.\nThe energies of these quantum states are critical, since a state is partially filled only if its energy is near the Fermi level (see Fermi–Dirac statistics).\n\nHigh conductivity in a material comes from it having many partially filled states and much state delocalization.\nMetals are good electrical conductors and have many partially filled states with energies near their Fermi level.\nInsulators, by contrast, have few partially filled states, their Fermi levels sit within band gaps with few energy states to occupy.\nImportantly, an insulator can be made to conduct by increasing its temperature: heating provides energy to promote some electrons across the band gap, inducing partially filled states in both the band of states beneath the band gap (valence band) and the band of states above the band gap (conduction band).\nAn (intrinsic) semiconductor has a band gap that is smaller than that of an insulator and at room temperature significant numbers of electrons can be excited to cross the band gap.\n\nA pure semiconductor, however, is not very useful, as it is neither a very good insulator nor a very good conductor.\nHowever, one important feature of semiconductors (and some insulators, known as \"semi-insulators\") is that their conductivity can be increased and controlled by doping with impurities and gating with electric fields. Doping and gating move either the conduction or valence band much closer to the Fermi level, and greatly increase the number of partially filled states.\n\nSome wider-band gap semiconductor materials are sometimes referred to as semi-insulators. When undoped, these have electrical conductivity nearer to that of electrical insulators, however they can be doped (making them as useful as semiconductors). Semi-insulators find niche applications in micro-electronics, such as substrates for HEMT. An example of a common semi-insulator is gallium arsenide. Some materials, such as titanium dioxide, can even be used as insulating materials for some applications, while being treated as wide-gap semiconductors for other applications.\n\nThe partial filling of the states at the bottom of the conduction band can be understood as adding electrons to that band.\nThe electrons do not stay indefinitely (due to the natural thermal recombination) but they can move around for some time.\nThe actual concentration of electrons is typically very dilute, and so (unlike in metals) it is possible to think of the electrons in the conduction band of a semiconductor as a sort of classical ideal gas, where the electrons fly around freely without being subject to the Pauli exclusion principle. In most semiconductors the conduction bands have a parabolic dispersion relation, and so these electrons respond to forces (electric field, magnetic field, etc.) much like they would in a vacuum, though with a different effective mass.\nBecause the electrons behave like an ideal gas, one may also think about conduction in very simplistic terms such as the Drude model, and introduce concepts such as electron mobility.\n\nFor partial filling at the top of the valence band, it is helpful to introduce the concept of an electron hole.\nAlthough the electrons in the valence band are always moving around, a completely full valence band is inert, not conducting any current.\nIf an electron is taken out of the valence band, then the trajectory that the electron would normally have taken is now missing its charge.\nFor the purposes of electric current, this combination of the full valence band, minus the electron, can be converted into a picture of a completely empty band containing a positively charged particle that moves in the same way as the electron.\nCombined with the \"negative\" effective mass of the electrons at the top of the valence band, we arrive at a picture of a positively charged particle that responds to electric and magnetic fields just as a normal positively charged particle would do in vacuum, again with some positive effective mass.\nThis particle is called a hole, and the collection of holes in the valence band can again be understood in simple classical terms (as with the electrons in the conduction band).\n\nWhen ionizing radiation strikes a semiconductor, it may excite an electron out of its energy level and consequently leave a hole. This process is known as \"electron–hole pair generation\". Electron-hole pairs are constantly generated from thermal energy as well, in the absence of any external energy source.\n\nElectron-hole pairs are also apt to recombine. Conservation of energy demands that these recombination events, in which an electron loses an amount of energy larger than the band gap, be accompanied by the emission of thermal energy (in the form of phonons) or radiation (in the form of photons).\n\nIn some states, the generation and recombination of electron–hole pairs are in equipoise. The number of electron-hole pairs in the steady state at a given temperature is determined by quantum statistical mechanics. The precise quantum mechanical mechanisms of generation and recombination are governed by conservation of energy and conservation of momentum.\n\nAs the probability that electrons and holes meet together is proportional to the product of their numbers, the product is in steady state nearly constant at a given temperature, providing that there is no significant electric field (which might \"flush\" carriers of both types, or move them from neighbour regions containing more of them to meet together) or externally driven pair generation. The product is a function of the temperature, as the probability of getting enough thermal energy to produce a pair increases with temperature, being approximately exp(−\"E\"/\"kT\"), where \"k\" is Boltzmann's constant, \"T\" is absolute temperature and \"E\" is band gap.\n\nThe probability of meeting is increased by carrier traps—impurities or dislocations which can trap an electron or hole and hold it until a pair is completed. Such carrier traps are sometimes purposely added to reduce the time needed to reach the steady state.\n\nThe conductivity of semiconductors may easily be modified by introducing impurities into their crystal lattice. The process of adding controlled impurities to a semiconductor is known as \"doping\". The amount of impurity, or dopant, added to an \"intrinsic\" (pure) semiconductor varies its level of conductivity. Doped semiconductors are referred to as \"extrinsic\". By adding impurity to the pure semiconductors, the electrical conductivity may be varied by factors of thousands or millions.\n\nA 1 cm specimen of a metal or semiconductor has of the order of 10 atoms. In a metal, every atom donates at least one free electron for conduction, thus 1 cm of metal contains on the order of 10 free electrons, whereas a 1 cm sample of pure germanium at 20 °C contains about atoms, but only free electrons and holes. The addition of 0.001% of arsenic (an impurity) donates an extra 10 free electrons in the same volume and the electrical conductivity is increased by a factor of 10,000.\n\nThe materials chosen as suitable dopants depend on the atomic properties of both the dopant and the material to be doped. In general, dopants that produce the desired controlled changes are classified as either electron acceptors or donors. Semiconductors doped with \"donor\" impurities are called \"n-type\", while those doped with \"acceptor\" impurities are known as \"p-type\". The n and p type designations indicate which charge carrier acts as the material's majority carrier. The opposite carrier is called the minority carrier, which exists due to thermal excitation at a much lower concentration compared to the majority carrier.\n\nFor example, the pure semiconductor silicon has four valence electrons which bond each silicon atom to its neighbors. In silicon, the most common dopants are \"group III\" and \"group V\" elements. Group III elements all contain three valence electrons, causing them to function as acceptors when used to dope silicon. When an acceptor atom replaces a silicon atom in the crystal, a vacant state (an electron \"hole\") is created, which can move around the lattice and functions as a charge carrier. Group V elements have five valence electrons, which allows them to act as a donor; substitution of these atoms for silicon creates an extra free electron. Therefore, a silicon crystal doped with boron creates a p-type semiconductor whereas one doped with phosphorus results in an n-type material.\n\nDuring manufacture, dopants can be diffused into the semiconductor body by contact with gaseous compounds of the desired element, or ion implantation can be used to accurately position the doped regions.\n\nSome materials, when rapidly cooled to a glassy amorphous state, have semiconducting properties. These include B, Si, Ge, Se, Te and there are multiple theories to explain them.\n\nThe history of the understanding of semiconductors begins with experiments on the electrical properties of materials. The properties of negative temperature coefficient of resistance, rectification, and light-sensitivity were observed starting in the early 19th century.\n\nThomas Johann Seebeck was the first to notice an effect due to semiconductors, in 1821. In 1833, Michael Faraday reported that the resistance of specimens of silver sulfide decreases when they are heated. This is contrary to the behavior of metallic substances such as copper. In 1839, Alexandre Edmond Becquerel reported observation of a voltage between a solid and a liquid electrolyte when struck by light, the photovoltaic effect. In 1873 Willoughby Smith observed that selenium resistors exhibit decreasing resistance when light falls on them. In 1874 Karl Ferdinand Braun observed conduction and rectification in metallic sulfides, although this effect had been discovered much earlier by Peter Munck af Rosenschold () writing for the Annalen der Physik und Chemie in 1835, and Arthur Schuster found that a copper oxide layer on wires has rectification properties that ceases when the wires are cleaned. William Grylls Adams and Richard Evans Day observed the photovoltaic effect in selenium in 1876.\n\nA unified explanation of these phenomena required a theory of solid-state physics which developed greatly in the first half of the 20th Century. In 1878 Edwin Herbert Hall demonstrated the deflection of flowing charge carriers by an applied magnetic field, the Hall effect. The discovery of the electron by J.J. Thomson in 1897 prompted theories of electron-based conduction in solids. Karl Baedeker, by observing a Hall effect with the reverse sign to that in metals, theorized that copper iodide had positive charge carriers. Johan Koenigsberger classified solid materials as metals, insulators and \"variable conductors\" in 1914 although his student Josef Weiss already introduced the term \"Halbleiter\" (semiconductor in modern meaning) in PhD thesis in 1910. Felix Bloch published a theory of the movement of electrons through atomic lattices in 1928. In 1930, B. Gudden stated that conductivity in semiconductors was due to minor concentrations of impurities. By 1931, the band theory of conduction had been established by Alan Herries Wilson and the concept of band gaps had been developed. Walter H. Schottky and Nevill Francis Mott developed models of the potential barrier and of the characteristics of a metal-semiconductor junction. By 1938, Boris Davydov had developed a theory of the copper-oxide rectifier, identifying the effect of the p–n junction and the importance of minority carriers and surface states.\n\nAgreement between theoretical predictions (based on developing quantum mechanics) and experimental results was sometimes poor. This was later explained by John Bardeen as due to the extreme \"structure sensitive\" behavior of semiconductors, whose properties change dramatically based on tiny amounts of impurities. Commercially pure materials of the 1920s containing varying proportions of trace contaminants produced differing experimental results. This spurred the development of improved material refining techniques, culminating in modern semiconductor refineries producing materials with parts-per-trillion purity.\n\nDevices using semiconductors were at first constructed based on empirical knowledge, before semiconductor theory provided a guide to construction of more capable and reliable devices.\n\nAlexander Graham Bell used the light-sensitive property of selenium to transmit sound over a beam of light in 1880. A working solar cell, of low efficiency, was constructed by Charles Fritts in 1883 using a metal plate coated with selenium and a thin layer of gold; the device became commercially useful in photographic light meters in the 1930s. Point-contact microwave detector rectifiers made of lead sulfide were used by Jagadish Chandra Bose in 1904; the cat's-whisker detector using natural galena or other materials became a common device in the development of radio. However, it was somewhat unpredictable in operation and required manual adjustment for best performance. In 1906 H.J. Round observed light emission when electric current passed through silicon carbide crystals, the principle behind the light-emitting diode. Oleg Losev observed similar light emission in 1922 but at the time the effect had no practical use. Power rectifiers, using copper oxide and selenium, were developed in the 1920s and became commercially important as an alternative to vacuum tube rectifiers.\n\nIn the years preceding World War II, infra-red detection and communications devices prompted research into lead-sulfide and lead-selenide materials. These devices were used for detecting ships and aircraft, for infrared rangefinders, and for voice communication systems. The point-contact crystal detector became vital for microwave radio systems, since available vacuum tube devices could not serve as detectors above about 4000 MHz; advanced radar systems relied on the fast response of crystal detectors. Considerable research and development of silicon materials occurred during the war to develop detectors of consistent quality.\n\nDetector and power rectifiers could not amplify a signal. Many efforts were made to develop a solid-state amplifier and were successful in developing a device called the point contact transistor which could amplify 20db or more. In 1922 Oleg Losev developed two-terminal, negative resistance amplifiers for radio, and he perished in the Siege of Leningrad after successful completion. In 1926 Julius Edgar Lilienfeld patented a device resembling a modern field-effect transistor, but it was not practical. R. Hilsch and R. W. Pohl in 1938 demonstrated a solid-state amplifier using a structure resembling the control grid of a vacuum tube; although the device displayed power gain, it had a cut-off frequency of one cycle per second, too low for any practical applications, but an effective application of the available theory. At Bell Labs, William Shockley and A. Holden started investigating solid-state amplifiers in 1938. The first p–n junction in silicon was observed by Russell Ohl about 1941, when a specimen was found to be light-sensitive, with a sharp boundary between p-type impurity at one end and n-type at the other. A slice cut from the specimen at the p–n boundary developed a voltage when exposed to light.\n\nIn France, during the war, Herbert Mataré had observed amplification between adjacent point contacts on a germanium base. After the war, Mataré's group announced their \"Transistron\" amplifier only shortly after Bell Labs announced the \"transistor\".\n\n\n\n"}
{"id": "28648730", "url": "https://en.wikipedia.org/wiki?curid=28648730", "title": "Skellefteå Kraft", "text": "Skellefteå Kraft\n\nSkellefteå Kraft is a municipality-owned power company in Sweden. The company was established in 1908. It operates in Skellefteå, Lycksele, Storuman and Sundsvall in Sweden, and in Jakobstad in Finland. The company owns a number of hydroelectric power stations as also stakes in the Forsmark Nuclear Power Plant and Alholmens Kraft Power Station. In cooperation with Fortum, the company develops the Blaiken wind farm.\n\nThe company also operates several regional and local power distribution networks and owns a wood pellets production plant. Together with the technology company Outotec it has established a bio energy power plants technology company GreenExergy AB.\n"}
{"id": "172987", "url": "https://en.wikipedia.org/wiki?curid=172987", "title": "Solar mass", "text": "Solar mass\n\nThe solar mass () is a standard unit of mass in astronomy, equal to approximately . It is used to indicate the masses of other stars, as well as clusters, nebulae, and galaxies. It is equal to the mass of the Sun (denoted by the solar symbol ⊙︎). This equates to about two nonillion (two quintillion in the long scale) kilograms:\n\nThe above mass is about times the mass of Earth (), or times the mass of Jupiter ().\n\nBecause Earth follows an elliptical orbit around the Sun, the solar mass can be computed from the equation for the orbital period of a small body orbiting a central mass. Based upon the length of the year, the distance from Earth to the Sun (an astronomical unit or AU), and the gravitational constant (), the mass of the Sun is given by:\n\nThe value of \"G\" is difficult to measure and is only known with limited accuracy in SI units (\"see\" Cavendish experiment). The value of \"G\" times the mass of an object, called the standard gravitational parameter, is known for the Sun and several planets to much higher accuracy than \"G\" alone. As a result, the solar mass is used as the standard mass in the astronomical system of units.\n\nThe value of the gravitational constant was first derived from measurements that were made by Henry Cavendish in 1798 with a torsion balance. The value he obtained differs by only 1% from the modern value. The diurnal parallax of the Sun was accurately measured during the transits of Venus in 1761 and 1769, yielding a value of (9 arcseconds, compared to the present 1976 value of ). From the value of the diurnal parallax, one can determine the distance to the Sun from the geometry of Earth.\n\nThe first person to estimate the mass of the Sun was Isaac Newton. In his work \"Principia\" (1687), he estimated that the ratio of the mass of Earth to the Sun was about 1/28 700. Later he determined that his value was based upon a faulty value for the solar parallax, which he had used to estimate the distance to the Sun (1 AU). He corrected his estimated ratio to 1/169 282 in the third edition of the \"Principia\". The current value for the solar parallax is smaller still, yielding an estimated mass ratio of 1/332 946.\n\nAs a unit of measurement, the solar mass came into use before the AU and the gravitational constant were precisely measured. This is because the relative mass of another planet in the Solar System or the combined mass of two binary stars can be calculated in units of Solar mass directly from the orbital radius and orbital period of the planet or stars using Kepler's third law, provided that orbital radius is measured in astronomical units and orbital period is measured in years.\n\nThe mass of the Sun has been decreasing since the time it formed. This occurs through two processes in nearly equal amounts. First, in the Sun's core, hydrogen is converted into helium through nuclear fusion, in particular the p–p chain, and this reaction converts some mass into energy in the form of gamma ray photons. Most of this energy eventually radiates away from the Sun. Second, high-energy protons and electrons in the atmosphere of the Sun are ejected directly into outer space as the solar wind and coronal mass ejections.\n\nThe original mass of the Sun at the time it reached the main sequence remains uncertain. The early Sun had much higher mass-loss rates than at present, and it may have lost anywhere from 1–7% of its natal mass over the course of its main-sequence lifetime. The Sun gains a very small amount of mass through the impact of asteroids and comets. However, as the Sun already contains 99.86% of the Solar System's total mass, these impacts cannot offset the mass lost by radiation and ejection.\n\nOne solar mass, , can be converted to related units:\n\nIt is also frequently useful in general relativity to express mass in units of length or time.\n\n\nThe solar mass parameter (\"G\"·), as listed by the IAU Division I Working Group, has the following estimates:\n"}
{"id": "477661", "url": "https://en.wikipedia.org/wiki?curid=477661", "title": "Sublimation (phase transition)", "text": "Sublimation (phase transition)\n\nSublimation is the transition of a substance directly from the solid to the gas phase, without passing through the intermediate liquid phase. Sublimation is an endothermic process that occurs at temperatures and pressures below a substance's triple point in its phase diagram, which corresponds to the lowest pressure at which the substance can exist as a liquid. The reverse process of sublimation is deposition or desublimation, in which a substance passes directly from a gas to a solid phase. Sublimation has also been used as a generic term to describe a solid-to-gas transition (sublimation) followed by a gas-to-solid transition (deposition). While a transition from liquid to gas is described as evaporation if it occurs below the boiling point of the liquid, and as boiling if it occurs at the boiling point, there is no such distinction within the solid-to-gas transition, which is always described as sublimation. \n\nAt normal pressures, most chemical compounds and elements possess three different states at different temperatures. In these cases, the transition from the solid to the gaseous state requires an intermediate liquid state. The pressure referred to is the \"partial pressure\" of the substance, not the \"total\" (e.g. atmospheric) pressure of the entire system. So, all solids that possess an appreciable vapour pressure at a certain temperature usually can sublime in air (e.g. water ice just below 0 °C). For some substances, such as carbon and arsenic, sublimation is much easier than evaporation from the melt, because the pressure of their triple point is very high, and it is difficult to obtain them as liquids.\n\nThe term \"sublimation\" refers to a physical change of state and is not used to describe the transformation of a solid to a gas in a chemical reaction. For example, the dissociation on heating of solid ammonium chloride into hydrogen chloride and ammonia is \"not\" sublimation but a chemical reaction. Similarly the combustion of candles, containing paraffin wax, to carbon dioxide and water vapor is \"not\" sublimation but a chemical reaction with oxygen.\n\nSublimation is caused by the absorption of heat which provides enough energy for some molecules to overcome the attractive forces of their neighbors and escape into the vapor phase. Since the process requires additional energy, it is an endothermic change. The enthalpy of sublimation (also called heat of sublimation) can be calculated by adding the enthalpy of fusion and the enthalpy of vaporization.\n\nSolid carbon dioxide (dry ice) sublimes everywhere along the line below the triple point (e.g., at the temperature of −78.5 °C (194.65 K, ) at atmospheric pressure, whereas its melting into liquid CO can occur only along the line at pressures and temperatures above the triple point (i.e., 5.2 atm, −56.4 °C).\n\nSnow and ice sublime, although more slowly, at temperatures below the freezing/melting point temperature line at 0 °C for most pressures; see line below triple point. In freeze-drying, the material to be dehydrated is frozen and its water is allowed to sublime under reduced pressure or vacuum. The loss of snow from a snowfield during a cold spell is often caused by sunshine acting directly on the upper layers of the snow. Ablation is a process that includes sublimation and erosive wear of glacier ice.\n\nNaphthalene, an organic compound commonly found in pesticide such as mothball, sublimes easily because it is made of non-polar molecules that are held together only by van der Waals intermolecular forces. Naphthalene is a solid that sublimes at standard atmospheric temperature with the sublimation point at around 80°C or 176°F. At low temperature, its vapour pressure is high enough, 1mmHg at 53°C, to make the solid form of naphthalene evaporate into gas. On cool surfaces, the naphthalene vapours will solidify to form needle-like crystals.\n\nIodine produces fumes on gentle heating. It is possible to obtain liquid iodine at atmospheric pressure by controlling the temperature at just above the melting point of iodine. In forensic science, iodine vapor can reveal latent fingerprints on paper.\nArsenic can also sublime at high temperatures.\n\nSublimation is a technique used by chemists to purify compounds. A solid is typically placed in a sublimation apparatus and heated under vacuum. Under this reduced pressure, the solid volatilizes and condenses as a purified compound on a cooled surface (cold finger), leaving a non-volatile residue of impurities behind. Once heating ceases and the vacuum is removed, the purified compound may be collected from the cooling surface.\nFor even higher purification efficiencies a temperature gradient is applied, which also allows for the separation of different fractions. Typical setups use an evacuated glass tube that is gradually heated in a controlled manner. The material flow is from the hot end, where the initial material is placed, to the cold end that is connected to a pump stand. By controlling temperatures along the length of the tube the operator can control the zones of recondensation, with very volatile compounds being pumped out of the system completely (or caught by a separate cold trap), moderately volatile compounds recondensating along the tube according to their different volatilities, and non-volatile compounds remaining in the hot end.\nVacuum sublimation of this type is also the method of choice for purification of organic compounds for the use in the organic electronics industry, where very high purities (often > 99.99%) are needed to satisfy the standards for consumer electronics and other applications.\n\nIn ancient alchemy, a protoscience that contributed to the development of modern chemistry and medicine, alchemists developed a structure of basic laboratory techniques, theory, terminology, and experimental methods. \"Sublimation\" was used to refer to the process in which a substance is heated to a vapor, then immediately collects as sediment on the upper portion and neck of the heating medium (typically a retort or alembic), but can also be used to describe other similar non-laboratory transitions. It is mentioned by alchemical authors such as Basil Valentine and George Ripley, and in the Rosarium philosophorum, as a process necessary for the completion of the magnum opus. Here, the word \"sublimation\" is used to describe an exchange of \"bodies\" and \"spirits\" similar to laboratory phase transition between solids and gases. Valentine, in his \"Triumphal Chariot of Antimony\" (published 1678) makes a comparison to spagyrics in which a vegetable sublimation can be used to separate the spirits in wine and beer. Ripley uses language more indicative of the mystical implications of sublimation, indicating that the process has a double aspect in the spiritualization of the body and the corporalizing of the spirit. He writes:\n\n<poem>\nAnd Sublimations we make for three causes,\nThe first cause is to make the body spiritual.\nThe second is that the spirit may be corporeal,\nAnd become fixed with it and consubstantial.\nThe third cause is that from its filthy original.\nIt may be cleansed, and its saltiness sulphurious\nMay be diminished in it, which is infectious.\n</poem>\n\nThe enthalpy of sublimation has commonly been predicted using the equipartition theorem. If the lattice energy is assumed to be approximately half the packing energy, then the following thermodynamic corrections can be applied to predict the enthalpy of sublimation. Assuming a 1 molar ideal gas gives a correction for the thermodynamic environment (pressure and volume) in which pV = RT, hence a correction of 1RT. Additional corrections for the vibrations, rotations and translation then need to be applied. From the equipartition theorem gaseous rotation and translation contribute 1.5RT each to the final state, therefore a +3RT correction. Crystalline vibrations and rotations contribute 3RT each to the initial state, hence −6RT. Summing the RT corrections ; −6RT + 3RT + RT = −2RT. This leads to the following approximate sublimation enthalpy. A similar approximation can be found for the entropy term if rigid bodies are assumed.\nformula_1\n\n"}
{"id": "57574752", "url": "https://en.wikipedia.org/wiki?curid=57574752", "title": "The Hitching Stone", "text": "The Hitching Stone\n\nThe Hitching Stone is a gritstone erratic block on Keighley Moor, North Yorkshire, near Earl Crag and the village of Cowling. It is very close to the border between North Yorkshire and West Yorkshire and the border between Yorkshire and Lancashire.. \n\nIt is said to be the largest boulder in Yorkshire at 29 feet long, 25 feet wide and 21 feet high. It is also said to weigh a lot more than 1000 tonnes.\n\nThe Hitching Stone is 5 miles from the town of Keighley and is at an elevation of 1200 feet.\n\nThe Hitching Stone and all the other erratic boulders on Keighley Moor were put in place thousands to possibly millions of years ago during the Pleistocene Epoch. The Hitching Stone most likely originally came from Earl Crag during this time. As a result of the fact that The Hitching Stone lies at the borders of historic counties, ancient councils and parliaments met at the stone and markets, fairs, and other gatherings were also held at the stone, with the last fair being held in 1870.\n\n\n"}
{"id": "39502019", "url": "https://en.wikipedia.org/wiki?curid=39502019", "title": "Tidewater (marketing)", "text": "Tidewater (marketing)\n\nTidewater is a term used by industries and governments to refer to access to ocean ports with international marine services for import and export of commodities. For export, the commodities can be shipped via trucks, trains and/or pipelines to a port, thereby opening the door to more lucrative prices on global markets. Getting to such a port is particularly important for landlocked jurisdictions seeking to expand and diversify markets for natural resources.\n\nAn example of the use of the term \"tidewater\" can be seen in the debate over exports of oil produced by the Athabasca oil sands. Upon separation from the sand, this bituminous oil, marketed as Western Canadian Select, is forced to sell at the price established for landlocked oil (see: West Texas Intermediate benchmark). If this oil were able to reach \"tidewater\" for export using oil tankers, it would presumably command a higher price (see: Brent Crude benchmark). Several pipelines have been proposed to bring the oil to \"tidewater\", including the Keystone XL project (via the Gulf of Mexico), the Enbridge Northern Gateway Pipelines project (via the British Columbia coast), the Mackenzie Valley Pipeline project (via the Beaufort Sea), or the Energy East pipeline project (via the Bay of Fundy). The Government of Alberta and the Government of Canada claimed a loss of $C4 – $C30 billion (CAD) in taxes and non-renewable natural resource royalties in 2013. In comparison, Maya crude oil,(Moore et al. 2011:2). a similar product to Western Canadian Select, but located close to tidewater, is reaching peak prices. In the United States, opponents of pipeline projects have expressed concern that pipeline construction and expansion would simply facilitate getting Alberta oil sands products to an American port for export to China and other countries via the Gulf of Mexico and that the resulting expansion in production in Alberta and consumption of fossil fuels worldwide would have a detrimental contribution to greenhouse gases. Canaport is a receiving terminal for crude oil and LNG. There is a proposal to expand it to allow the export of inland oil delivered via pipeline.\n\n\n"}
{"id": "43846071", "url": "https://en.wikipedia.org/wiki?curid=43846071", "title": "Voltage-sensitive relay", "text": "Voltage-sensitive relay\n\nA voltage-sensitive relay (VSR) is a relay used for automotive, truck and marine applications.\n\nA VSR senses the input voltage generated and automatically connects or disconnects the appliance or circuit at pre-set voltages for a variety of applications. It is typically used to prevent over-discharging of a car battery and is very common in a dual-battery system.\n\nA VSR is usually classified into 12 V – 24 V range with varying electric current. A single-sense relay only detects voltage at one pole while a dual-sense relay detects voltage at both poles.\n\nAn ignition-protected voltage-sensitive relay can withstand repetitive engine cranking transients without significant effect.\n\nVSRs are inexpensive and offer little or no voltage drop. However, the relay may chatter (repeatedly open and close at very short intervals) if the voltage is not stable.\n"}
{"id": "26564321", "url": "https://en.wikipedia.org/wiki?curid=26564321", "title": "WIMP Argon Programme", "text": "WIMP Argon Programme\n\nThe WIMP Argon Programme (WARP) is an experiment at Laboratori Nazionali del Gran Sasso, Italy, for the research of cold dark matter. It aims to detect nuclear recoils in liquid argon induced by weakly interacting massive particles (WIMP) through scintillation light; the apparatus can also detect ionization so to exclude interactions of photons and electrons.\n\nP.Benetti, E.Calligarich, M.Cambiaghi, C.Montanari(+), A.Rappoldi, G.L.Raselli, M.Roncadelli, M.Rossella, C.Vignoli\n\nM. Antonello, O.Palamara, L.Pandola, C.Rubbia(*), E.Segreto, A.Szelc\n\nR.Acciarri, M. Antonello, N. Canci, F.Cavanna, F.Di Pompeo(++), L.Grandi(++)\n\nF.Carbonara, A.Cocco, G.Fiorillo, G.Mangano\n\nF.Calaprice, C.Galbiati, B.Loer, R.Saldanha\n\nB.Baibussinov, S.Centro, M.B.Ceolin, G.Meng, F.Pietropaolo,S.Ventura\n\n"}
{"id": "47415993", "url": "https://en.wikipedia.org/wiki?curid=47415993", "title": "Water gel explosive", "text": "Water gel explosive\n\nA water-gel explosive is a fuel sensitized explosive mixture consisting of an aqueous ammonium nitrate solution that acts as the oxidizer. Water gels that are cap-insensitive are referred to under United States safety regulations as blasting agents. Water gel explosives have a jelly-like consistency and come in sausage-like packing stapled shut on both sides.\n\nWater-gel explosives have almost completely displaced dynamite, becoming the most-used civil blasting agents.\n\nWater gels usually have many different ingredients. They contain a gelatinizing agent, also known as a thickener, that modifies their consistency, ranging from easily pourable gels to hard solids. Polyvinyl alcohol, guar gum, dextran gums, and urea-formaldehyde resins are the typical gelling agents. Guar, specifically, is a gelling agent used for the aqueous portion of the water gel explosives. The primary component of water gels is monomethylamine nitrate. Monomethylamine nitrate is made from monomethyl amine (MMA) and nitric acid. Water gel explosives are also made of ammonium nitrate, calcium nitrate, aluminum, ethylene glycol and TNT. The proportions of these components vary depending on the desired explosiveness of the water gel.\n\nWater gel explosives are produced by combining nitroparaffins, usually nitromethane, with an aqueous salt solution and a gelling agent. These nitroparaffins typically make up most of the water gel explosive. Different types of gelling agents are used to create the water gel explosive. One agent is insoluble in water, but able to gel with nitromethane. The gel used for nitromethane is cyanoethylether, a derivative of galactomannan gum. Other agents are water-soluble and are used for the aqueous salt solution. As referenced in the first paragraph, water-soluble gums and gel modifiers like guar can be used for the gelling of aqueous solutions. When the salt solution and nitroparaffin are gelled, the entire mixture is combined and mixed together until the desired consistency is achieved. One characteristic that allows the explosive to work so well is the insoluble nature of the nitroparaffin. The effectiveness of the water gels is dependent on the dissemination of salts in the salt solution. The particles need to be very small and fine so that they can be dispersed well throughout the solution. Some salts that are commonly used include: ammonium nitrate, sodium nitrate, sodium perchlorate and potassium chlorate. The sensitivity of the explosive must be increased in order to improve the initiation of the detonation of the explosive. There are different techniques for increasing the sensitivity. Aluminum or other powdered metals can help increase the sensitivity of the water gel, but increasing the sensitivity also means that the explosives are more combustible. Powdered metals have not proven to be completely effective in increasing the sensitivity of the explosive because they do not uniformly mix through the solution. They also lose sensitivity as storage time increases. Liquid non-self-explosive sensitizers like nitrobenzene and liquid nitrotoluene have not worked well either because they are difficult to hold in suspension. Liquid aliphatic mononitrates have been found to work very effectively as sensitizers when they are well mixed in the water gel.\n\nWater gel explosives tend to be less toxic and are less hazardous than dynamite to manufacture, transport, and store. Water gels are also less expensive than conventional explosives. Because they are relatively safe and easy to use, they are often used in the mining industry. There are many different types of water gel explosives for use in different situations. One type, a small diameter slurry explosive, can be used specifically for blasting in coal undercut, midcut, and depillaring areas. They are preferable to nitroglycerin-based explosives like dynamite because they produce less noxious fumes. Detagel, which is very high in strength, is a specific example of a small diameter water gel explosive that is used for mining activities.\n\nWater gel explosives are frequently used as cartridge explosives because they are much easier to load into large casings. With water gel explosives, the slurry material can simply be poured into the casing. Traditional explosives are cast into the casing. This process is laborious and the charge may begin to shrink, creating multiple voids. A final advantage of slurry is that it can be stored in non-explosive component form and sensitized into field-manufactured explosive as it is needed. The explosive may be sensitized by the addition of gas, metal powder, or another explosive such as TNT, RDX, HMX, or PETN. The water in water gel explosives is converted into a reactant by the addition of large amounts of aluminum.\n"}
{"id": "30207675", "url": "https://en.wikipedia.org/wiki?curid=30207675", "title": "World Petroleum Council", "text": "World Petroleum Council\n\nThe World Petroleum Council (WPC) is an oil and gas industry forum and international organization representing the petroleum sector worldwide \n\nWPC has been called \"the world’s premier oil & gas forum since 1933.\" It is widely recognised to \"include the most prestigious national oil and gas companies and agencies of the world\"\n\nThe premier conference that is organized by the World Petroleum Council is called the World Petroleum Congress. Starting in 1933, the congress was held every four years until 1991, with a 14-year hiatus in between 1937 and 1951 because of World War II. After 1991, it was held every three years until the year 2000. There was a move to have it hosted every two years after the 2000 edition, with Rio de Janeiro hosting one in 2002, but the cycle returned to every three years after that. In order to host a congress, there is a bidding process by interested cities for one in a particular year.\n\nThis award is named for Institute of Petroleum President Thomas Dewhurst (1881-1973), who organized the first World Petroleum Congress in 1933, and is awarded to individuals who have shown \"excellence in the petroleum industry.\"\n\n"}
{"id": "46995540", "url": "https://en.wikipedia.org/wiki?curid=46995540", "title": "Younicos", "text": "Younicos\n\nYounicos is a German-American technology company that develops and sells energy storage systems and control software. The company integrates battery technologies, power electronics and control software to create systems that respond to the energy management requirements of power networks of all sizes, including micro-grids.\n\nYounicos has two primary facilities: Younicos AG in Berlin, Germany, and Younicos Inc. in Austin, Texas. It employs a total of about 130 people as of 2017.\n\nThe company was founded in Berlin, Germany, in 2005 under the name Solon Laboratories by executives of German solar manufacturer Solon. In 2009, Solon Laboratories merged with I-Sol Ventures GmbH and the company was renamed Younicos AG.\n\nIn 2009, Younicos began operating a one-megawatt/6-megawatt-hour sodium-sulfur battery testing facility at its Berlin headquarters, which was the first of its size in Europe.\n\nIn 2012, the company added a 200-kilowatt lithium-ion battery array and integrated both batteries with a total of 1.2 MW into the German frequency regulation market.\n\nIn 2014, Younicos commissioned the largest commercial, and first stand-alone, battery plant in Europe for WEMAG, a German utility company.\n\nIn 2015, the company introduced its first hardware product – the Y.Cube. It combines lithium-ion battery blocks with a Younicos-engineered power conversion system (PCS) in a custom enclosure, controlled by Younicos energy storage software. The firm also raised $50 million in growth capital from a group that includes First Solar, Inc. and Grupo ECOS.\n\nAs of end-2016, Younicos had contracted for or installed a total of 200 MW of energy storage systems, with 75 MW booked in 2016. Compared to 2015, that represented a 400% increase in annual bookings.\n\nIn July 2017 Younicos was acquired by Aggreko and is now a wholly owned subsidiary of the Scottish company.\n\nY.Cube\n\nY.Cube is a ready-to-install storage system with all components inside a single enclosure. This off the-shelf solution comprises batteries, inverter, HVAC and auxiliary components, tested and pre-assembled by Younicos.\n\nY.Station\n\nY.Station is designed specifically to meet large-scale energy storage requirements. The pre-designed, pre-engineered solution allows for efficient project planning and faster deployment.\n\nY.Q\n\nThe Younicos Y.Q operating system is an integrated control platform and the intelligent core of every Younicos energy storage solution. \n\nCentrica (UK)\n\nYounicos has been selected by Centrica to design and deliver one of the world’s largest battery-based energy storage systems. To be completed by winter 2018, the 49 megawatt (MW) lithium-ion system will respond to fluctuations in electrical demand in less than a second, to maintain electric power frequency and stability. \n\nWEMAG II (Germany)\n\nYounicos, in partnership with German utility WEMAG, built a 5 MW/5 MWh battery power plant in the Schwerin district of Lankow, Germany. In late 2016, WEMAG decided to enlarge their battery park to double its power output from 5 MW to 10 MW, with the energy capacity increasing from 5 MWh to 15 MWh. \n\nNotrees (Texas)\n\nU.S. utility Duke Energy is using Younicos controls technology to manage operations for a 36 MW energy storage system at the site of Duke’s 153 MW wind farm in Notrees, Texas. The Notrees Battery Energy Storage System (BESS), funded in part by a U.S. Department of Energy Smart Grid award, was constructed by Duke Energy in 2012, and is the largest wind-integrated storage resource in North America. In 2016-17, the Notrees BESS is being repowered with more advanced lithium-ion batteries.\n\nGraciosa (Azores)\n\nYounicos Y.Q software platform is at its full grid-forming potential on the island of Graciosa in the Azores, where it provides real-time power management and energy dispatch for the entire island’s grid. The system allows the island to be powered entirely by wind and solar, with the existing diesel generators used only as backup power during prolonged periods of unfavorable weather.\n\nIn April 2014, Younicos acquired the assets of the former Xtreme Power, a Kyle, Texas-based battery systems developer, after it had filed for Chapter 11 bankruptcy protection in January of that year.\n\nXtreme Power was founded in 2004 in Austin, Texas. In 2009, it installed its first megawatt-scale storage system in Maui, Hawaii. Working with Duke Energy, in 2012 Xtreme Power commissioned the largest wind-integrated battery storage system (36 MW / 24 MWh) in the U.S. in Notrees, Texas.\n\nEventually the company would deploy more than 60 megawatts of storage systems. After acquiring its assets, Younicos combined operations into the parent company and renamed the U.S. operation Younicos Inc.\n\n"}
