{"id": "542531", "url": "https://en.wikipedia.org/wiki?curid=542531", "title": "Arachidic acid", "text": "Arachidic acid\n\nArachidic acid, also known as eicosanoic acid, is a saturated fatty acid with a 20-carbon chain. It is a minor constituent of cupuaçu butter (7%), perilla oil (0–1%), peanut oil (1.1–1.7%), corn oil (3%), and cocoa butter (1%). It also constitutes 7.08% of the fats from the fruit of the durian species \"Durio graveolens\".\n\nIts name derives from the Latin \"arachis\"—peanut. It can be formed by the hydrogenation of arachidonic acid.\n\nReduction of arachidic acid yields arachidyl alcohol.\n\nArachidic acid is used for the production of detergents, photographic materials and lubricants.\n"}
{"id": "11054533", "url": "https://en.wikipedia.org/wiki?curid=11054533", "title": "Archard equation", "text": "Archard equation\n\nThe Archard wear equation is a simple model used to describe sliding wear and is based on the theory of asperity contact. The Archard equation was developed much later than (sometimes also known as energy dissipative hypothesis), though both came to the same physical conclusions, that the volume of the removed debris due to wear is proportional to the work done by friction forces. Theodor Reye's model became popular in Europe and it is still taught in university courses of applied mechanics. Until recently, Reye's theory of 1860 has, however, been totally ignored in English and American literature where subsequent works by Ragnar Holm and John Frederick Archard are usually cited. In 1960, and Mikhail Alekseevich Babichev published a similar model as well. In modern literature, the relation is therefore also known as Reye–Archard–Khrushchov wear law.\n\nwhere:\n\nNote that formula_2 is proportional to the work done by the friction forces as described by Reye's hypothesis.\n\nThe equation can be derived by first examining the behavior of a single asperity.\nThe local load formula_3, supported by an asperity, assumed to have a circular cross-section with a radius formula_4, is:\n\nwhere \"P\" is the yield pressure for the asperity, assumed to be deforming plastically. \"P\" will be close to the indentation hardness, \"H\", of the asperity.\n\nIf the volume of wear debris, formula_6, for a particular asperity is a hemisphere sheared off from the asperity, it follows that:\n\nThis fragment is formed by the material having slid a distance 2\"a\"\n\nHence, formula_8, the wear volume of material produced from this asperity per unit distance moved is:\n\nHowever, not all asperities will have had material removed when sliding distance 2\"a\". Therefore, the total wear debris produced per unit distance moved, formula_11 will be lower than the ratio of \"W\" to \"3H\". This is accounted for by the addition of a dimensionless constant \"K\", which also incorporates the factor 3 above. These operations produce the Archard equation as given above. Archard interpreted \"K\" factor as a probability of forming wear debris from asperity encounters. Typically for 'mild' wear, \"K\" ≈ 10, whereas for 'severe' wear, \"K\" ≈ 10. Recently, it has been shown that there exists a critical length scale that controls the wear debris formation at the asperity level. This length scale defines a critical junction size, where bigger junctions produce debris, while smaller ones deform plastically.\n\n\n"}
{"id": "692112", "url": "https://en.wikipedia.org/wiki?curid=692112", "title": "Aspirator (pump)", "text": "Aspirator (pump)\n\nAn aspirator is a type of ejector-jet pump, which produces vacuum by means of the Venturi effect.\n\nIn an aspirator, fluid (liquid or gaseous) flows through a tube that first narrows and then expands in cross-sectional area. When the tube narrows, the fluid pressure decreases. In this narrow area the fluid velocity must increase to conserve mass continuity. Where the tube narrows, a vacuum is drawn because of the Venturi effect.\n\nThe cheap and simple water aspirator is the most common type of aspirator. It is used in chemistry and biology laboratories and consists of a tee fitting attached to a tap and has a hose barb at one side. The flow of water passes through the straight portion of the tee, which has a restriction at the intersection, where the hose barb is attached. The vacuum hose should be connected to this barb. In the past, water aspirators were common for low-strength vacuums in chemistry benchwork. However, they are water-intensive, and depending on what the vacuum is being used for (i.e. solvent removal), they can violate environmental protection laws such as the RCRA by mixing potentially hazardous chemicals into the water stream, then flushing them down a drain that often leads directly to the municipal sewer.\n\nIf a liquid is used as the working fluid, the strength of the vacuum produced is limited by the vapor pressure of the liquid (for water, or 32 mbar at ). If a gas is used, however, this restriction does not exist. The industrial steam ejector (also called the \"steam jet ejector\", \"steam aspirator\", or \"steam jet aspirator\") uses steam as a working fluid.\n\nIn order to avoid using too much steam, a single steam-ejector stage is generally not used to generate vacuum below approximately 10 kPa (75 mmHg). To generate higher vacuum, multiple stages are used; in a two-stage steam ejector, for example, the second stage provides vacuum for the waste steam output by the first stage. Condensers may be used between stages to reduce the load on the later stages. Steam ejectors with two, three, four, five and six stages may be used to produce vacuums down to 2.5 kPa, 300 Pa, 40 Pa, 4 Pa, and 0.4 Pa, respectively.\n\nThe air ejector or Venturi pump is similar to the steam ejector but uses high-pressure air as the working fluid. They are called Vacuum Ejectors and they generate vacuum by changing the pressure of the compressed air. Multistage air ejectors can be used, but since air cannot easily be condensed at room temperature, an air ejector is usually limited to two stages.\n\n"}
{"id": "8035260", "url": "https://en.wikipedia.org/wiki?curid=8035260", "title": "Astrocaryum murumuru", "text": "Astrocaryum murumuru\n\nAstrocaryum murumuru (Portuguese common name\" murumuru\") is a palm native to Amazon Rainforest vegetation in Brazil, which bears edible fruits. \"Murumuru\" butter, extracted from the seeds of the plant, may be used as a moisturizer.\n\nThe \"murumuru\" palm trees grow in Brazil and around the Amazon and are one of the dominant trees in this region. It has a thick trunk and a shuttlecock-shaped, bushy crown. The nutritious, edible fruits are an important local food source and materials made from the tree, fruit, and seed kernels are commercially significant to the region. Hammocks are made from the tree’s fibres. \"Murumuru\" butter is highly emollient and moisturizing. It is also film-forming and naturally glossy. These qualities make it very protective. It contains vitamins and has a high content of beneficial oleic acid. The oil from the seeds is traditionally used to soften and protect hair. \"Murumuru\" butter is the white to yellowish fat obtained from the seeds of the \"murumuru\" palm.\n\nThe Murumuru palm (Astrocaryum murumuru) is abundant in the Brazilian Amazon, extending to the borders of Bolivia and Peru. It prefers to grow in periodically flooded areas, especially on islands and in lowlands along the rivers throughout the Amazon River estuary and its tributaries, in dense or semi-open forests.\nIt is also frequently found in the lowlands of Marajo Island. The stem, leaves and fruit stalks are covered with hard, black spines that can reach over 20 cm in length, complicating fruit harvesting. When the fruit is ripe, the inflorescence (cluster of flowers) drops to the ground. The fruit contains a yellow flesh often consumed by rodents as food, which leave the seeds clean. The seed’s hard shell is only separable from the kernel when dry. In general, 100 kg of dry seeds (12%– 15% water) yields 27 kg to 29 kg of kernels, which must be further dried to prevent deterioration during storage. These kernels then yield 40% to 42% oil. A single Murumuru palm produces about 11 kg of dry seeds. Hydraulic extraction can produce 35% oil relative to the dry weight of the kernel, which is equivalent to about 3.8 liters of oil per Murumuru palm. The kernels must be ground with grinding discs before hydraulic extraction. A kilogram of fruit pulp contains approximately 50 seeds. Seed germination is moderate and growth in the field is slow. .\n\nMurumuru butter contains lauric, myristic, and oleic acids. The fruit contains a white butter that is odorless and tasteless and has the advantage of not becoming rancid easily. The quality of Murumuru butter is similar to the seed fat of the Tucumã palm and coconut palm, but it has the advantage of providing greater consistency because of its melting point (33 C), which is superior to that of the Tucumã palm (30 °C) and coconut palm (22.7 °C). The quality of Murumuru butter makes it possible to mix it with other vegetable butters that have a lower melting point. It can also be used to partially substitute cocoa butter in chocolate, providing a firmer consistency in environments where the temperature is higher.\nMurumuru butter has the great advantage of having a low acidity value (4% to 5%), especially when made from fresh seeds, which reduces the cost of refinement.\nMurumuru butter was highly valued in Europe and the United States during the 1940s and 1950s, when it served as an ingredient in vegetable creams and soaps. Today, soaps containing Murumuru butter together with Ucuuba butter leave a protection layer on the skin similar to silicone, only it does not clog the pores.\nThe combination of these two butters is a treatment for dry and tired skins.. In hair products, Murumuru butter helps to maintain curls, while nourishing and strengthening hair roots. Murumuru butter is used in small amounts in shampoos (0.5% to 1%) and formulas for conditioners, creams, soaps, lipsticks and deodorants (0.5% to 8%).\n\n\n"}
{"id": "77060", "url": "https://en.wikipedia.org/wiki?curid=77060", "title": "Biological hazard", "text": "Biological hazard\n\nBiological hazards, also known as biohazards, refer to biological substances that pose a threat to the health of living organisms, primarily that of humans. This can include samples of a microorganism, virus or toxin (from a biological source) that can affect human health. It can also include substances harmful to other animals.\nThe term and its associated symbol are generally used as a warning, so that those potentially exposed to the substances will know to take precautions. The biohazard symbol was developed in 1966 by Charles Baldwin, an environmental-health engineer working for the Dow Chemical Company on the containment products.\n\nIt is used in the labeling of biological materials that carry a significant health risk, including viral samples and used hypodermic needles.\n\nIn Unicode, the biohazard symbol is U+2623 (☣).\n\nBio hazardous agents are classified for transportation by UN number:\n\nThe United States Centers for Disease Control and Prevention (CDC) categorizes various diseases in levels of biohazard, Level 1 being minimum risk and Level 4 being extreme risk. Laboratories and other facilities are categorized as BSL (Biosafety Level) 1–4 or as \"P1\" through \"P4\" for short (Pathogen or Protection Level).\n\n\nThe biohazard symbol was developed by the Dow Chemical Company in 1966 for their containment products. According to Charles Baldwin, an environmental-health engineer who contributed to its development: \"We wanted something that was memorable but meaningless, so we could educate people as to what it means.\" In an article he wrote for \"Science\" in 1967, the symbol was presented as the new standard for all biological hazards (\"biohazards\"). The article explained that over 40 symbols were drawn up by Dow artists, and all of the symbols investigated had to meet a number of criteria:\nThe chosen symbol scored the best on nationwide testing for memorability.\n\nThe design was first specified in 39 FR 23680 but was dropped in the succeeding amendment. However, various US states adopted the specification for their state code.\n\nThere are four circles within the symbol, signifying the chain of infection.\n\n\n\n"}
{"id": "8591354", "url": "https://en.wikipedia.org/wiki?curid=8591354", "title": "Bukhara–Tashkent–Bishkek–Almaty pipeline", "text": "Bukhara–Tashkent–Bishkek–Almaty pipeline\n\nThe Bukhara–Tashkent–Bishkek–Almaty pipeline is a Uzbekistan's main natural gas export pipeline.\n\nConstruction of the pipeline started in 1967. In 1968, the pipeline reached to Tashkent, in 1970 to Bishkek (then Frunze) and in 1971 to Almaty.\n\nThe diameter of the pipeline is and the annual capacity of the pipeline is almost 22 billion cubic meter (bcm) of natural gas. The Bukhara–Tashkent–Bishkek–Almaty pipeline is the main source of gas supply for Kyrgyzstan and southern part of Kazakhstan. It is possible that the pipeline will be connected with the planned Central Asia-China gas pipeline.\n\nThe Kazakhstan section of pipeline is operated by KazTransGas, a wholly owned subsidiary of KazMunayGas. The Kyrgyzstan section of pipe line is operated by KyrKazGas, the joint venture of KazTransGas and Kyrgyzgas.\n\nThe technical condition of the pipeline is alarming, particularly in Kyrgyzstan. Therefore, there are plans for construction of a second trunk gas pipeline and renovation of the existing pipeline.\n\n"}
{"id": "47242273", "url": "https://en.wikipedia.org/wiki?curid=47242273", "title": "Canebrake", "text": "Canebrake\n\nA canebrake or canebreak is a thicket of any of a variety of \"Arundinaria\" grasses: \"A. gigantea\", \"A. tecta\" and \"A. appalachiana\". As a bamboo, these giant grasses grow in thickets up to 8 m tall. \"A. gigantea\" is generally found in stream valleys and ravines throughout the southeastern US. \"A. tecta\" is a smaller stature species found on the Atlantic and Gulf Coastal Plains. Finally, \"A. appalachiana\" is found in more upland areas at the southern end of the Appalachian mountains.\n\nCanebrakes were formerly widespread in the Southern United States but have been widely replaced by agriculture. Concomitant with this destruction have been challenges to the survival of the Florida panther (\"Puma concolor\" subsp. \"coryi\") and Bachman's warbler (\"Vermivora bachmanii\"), the latter of which is critically endangered and may in fact be extinct. Other species considered canebrake specialists include several butterfly species, and Swainson's warbler (\"Limnothlypis swainsonii\"). Swainson's warbler has recently been found to use pine plantations (widespread across the Southeastern United States) of a particular age, as they may provide the structural features and prey base that the species seeks.\n\nCanes can reproduce asexually and rapidly, an adaptation that allows them to persist quietly in the shade of a forest for years and rapidly take advantage of disturbance which disrupts the overstory, such as blowdowns, floods or hurricanes. When released in this way, canes can quickly reoccupy these gaps.\n"}
{"id": "2186009", "url": "https://en.wikipedia.org/wiki?curid=2186009", "title": "Centre National d'Appui au Développement et à la Participation populaire", "text": "Centre National d'Appui au Développement et à la Participation populaire\n\nCENADEP (Centre National d’Appui au Développement et à la Participation populaire) is an environmental federation of the \nDemocratic Republic of the Congo. CENADEP was founded in 2000.\nThe spokesman is Joseph Bobia. CENADEP's main aim is to protect the Congolese rainforest.\n\n"}
{"id": "8783360", "url": "https://en.wikipedia.org/wiki?curid=8783360", "title": "Chevrolet Volt", "text": "Chevrolet Volt\n\nThe Chevrolet Volt is a plug-in hybrid car manufactured by General Motors, also marketed in rebadged variants as the Holden Volt in Australia and New Zealand, Buick Velite 5 in China, and with a different fascia as the Vauxhall Ampera in the United Kingdom and as the Opel Ampera in the remainder of Europe. In November 2018, GM announced it would cease Volt production in March 2019.\n\nSales of the 2011 Volt began in the United States in mid-December 2010, followed by various European countries and other international markets in 2011. Global combined Volt/Ampera-family sales totaled about 177,000 units by the end of October 2018. The U.S. is the leading market, with 148,556 Volts delivered through October 2018, followed by Canada with 16,653 Volts sold through September 2018. Just over 10,000 Opel/Vauxhall Ampera cars had been sold in Europe , with the Netherlands leading the European region. , the Volt/Ampera family of vehicles is the world's all-time best-selling plug-in hybrid vehicle, and the Volt is also the U.S. all-time top-selling plug-in electric car.\n\nThe Volt operates as a pure battery electric vehicle until its battery capacity drops to a predetermined threshold from full charge. From there, its internal combustion engine powers an electric generator to extend the vehicle's range as needed. When the engine is running it may be periodically mechanically linked (by a clutch) to a planetary gear set, and hence the output drive axle, to improve energy efficiency. The Volt's regenerative braking also contributes to the on-board electricity generation. Under the United States Environmental Protection Agency (EPA) cycle, the 2013/15 model year Volt all-electric range is , with a combined electric mode/gasoline-only rating of equivalent (MPG-equivalent).\n\nThe second-generation Volt's improved battery system and drivetrain increased the all-electric range to , its EPA-rated fuel economy in charge-sustaining mode to , and the combined city/highway fuel economy in all-electric mode to 106 MPG-e, up from 98 MPG-e. Deliveries to retail customers in the U.S. and Canada began in October 2015 as a 2016 model year.\n\nThe Volt has won several awards, including the 2009 Green Car Vision Award, 2011 Green Car of the Year, 2011 North American Car of the Year, 2011 World Green Car, 2012 European Car of the Year, and 2016 Green Car of the Year. Controversies regarding the Volt include the extent to which the U.S. federal government may have participated in the Volt’s development, which continued through General Motors' 2009 government-led bankruptcy, and concerns about the battery-pack fire risk following a crash test that the National Highway Traffic Safety Administration (NHTSA) performed on a Volt in 2011. At the completion of its investigation, NHTSA concluded that no discernible defect trend exists.\n\nThe Society of Automotive Engineers' (SAE) definition of a hybrid vehicle states that the vehicle shall have \"two or more energy storage systems both of which must provide propulsion power, either together or independently.\" General Motors has avoided the use of the term \"hybrid\" when describing its Voltec designs, even after the carmaker revealed that in some cases the combustion engine provided some assist at high speeds or to improve performance. Instead General Motors describes the Volt as an electric vehicle equipped with a \"range extending\" gasoline-powered internal combustion engine (ICE) as a genset and therefore dubbed the Volt an \"Extended Range Electric Vehicle\" or E-REV. In a January 2011 interview, the Chevy Volt's Global Chief Engineer, Pamela Fletcher, referred to the Volt as \"an electric car with extended range.\"\n\nAccording to the Society of Automotive Engineers (SAE) definitions, the Volt is a plug-in hybrid vehicle, due to the combination of an internal combustion engine and two electric motors, along with a battery that can accept off-board energy. The Volt operates as a purely electric vehicle for the first in charge-depleting mode. When the battery capacity drops below a pre-established threshold from full charge, the vehicle enters charge-sustaining mode, and the Volt's control system selects the most optimally efficient drive mode to improve performance and boost high-speed efficiency.\n\nThe Chevrolet Volt concept car debuted at the January 2007 North American International Auto Show, becoming the first-ever series plug-in hybrid concept car shown by a major car manufacturer. The Volt concept vehicle had four doors with a rear liftgate and seating for four passengers. This was a significant change in design when compared to the General Motors EV1 of the 1990s, which only seated two to reduce weight and to make the necessary room for the lead-acid battery pack. The top speed was also increased on the Volt, from the electronically limited to . The battery pack size was reduced, from about in volume in the EV1, to in the Volt.\n\nGeneral Motors' then-Vice-Chairman Robert Lutz said the two-seater sports car being developed by Tesla, the Tesla Roadster (2008), and the rapid advancement of lithium-ion battery technology inspired him to push the carmaker to develop the Volt after the 2006 Detroit Auto Show, overcoming internal opposition. Lutz's initial idea was to develop an all-electric car, but Jon Lauckner, General Motors Vice President for Global Vehicle Development, convinced him that to avoid an expensive battery, range anxiety concerns, and lack of public charging infrastructure, they could use a smaller battery pack with a small gasoline engine driving a generator acting as a backup to extend the range, but without a mechanical connection between the gasoline engine and the drive wheels, so it would be a pure electrically driven vehicle without many of the limitations General Motors learned from the EV1 experience.\nMost of the Volt initial design parameters defined for the development of the concept car, then referred as the \"iCar\" in homage to the iPod, were kept throughout the process up to the final production version. A key design parameter was a target of for the all-electric range, selected to keep the battery size small and lower costs, and mainly because research showed that in the U.S. 78 percent of daily commuters travel 40 miles or less. This target range lets drivers make most travel electrically driven, with the assumption that charging takes place at home overnight. This requirement translated to using a lithium-ion battery pack with an energy storage capacity of 16 considering that the battery would be used until the state of charge (SOC) of the battery reached 30%. This limit to the SOC was necessary in order to maintain operational performance under a wide range of environments, and to minimize the battery degradation to allow at least a ten-year life span. The initial target range for the gasoline engine/generator was set between and the vehicle had to be family size for four or five passengers.\n\nAnother key design decision was to develop the concept car based on a new family of common powertrain components for electric propulsion, which initially was called the E-Flex Systems, “E” stands for electric drive and “Flex” for the different sources of electricity, but later was renamed Voltec drive system. The E-Flex or Voltec powertrain is an attempt to standardize many components of possible future electrically propelled vehicles, and to allow multiple interchangeable electricity-generating systems. The E-Flex powertrain has the potential to adapt the vehicles to pure battery electric, to fuel cell-powered or to several other sources of energy to create electricity on board, such as engine-generator sets (genset) fueled by gasoline, diesel, biodiesel, ethanol fuel (E100), or flex-fuel (E85). Regenerative braking would also contribute to the on-board electricity generation. In October 2006 the E-flex powertrain was selected for the new propulsion architecture and the name Volt was chosen by General Motors.\n\nThe Volt concept car became the first application of the E-Flex (Voltec) drive system with a combination of an electric motor, the same used in the Chevrolet Equinox Fuel Cell, a lithium-ion battery pack with 136 kW of peak power, and a genset consisting of a small 1.0 L, 3-cylinder turbocharged flex-fuel capable engine linked to a generator. General Motors called this genset an electric vehicle (EV) range extender. The vehicle was propelled by an electric motor with a peak output of delivering 236 lb ft (320 Nm) of motoring torque. The concept car featured several advanced materials from GE Automotive Plastics that helped GM reduce the vehicle weight by up to 50 percent.\n\nThe Volt concept featured a fuel capacity providing the vehicle a total driving range of around , which considered a gasoline fuel efficiency of about and a all-electric range. According to General Motors estimates, a daily drive of , combined with an overnight recharge to support the first 40 all-electric miles, would yield an effective gasoline fuel economy of . General Motors also emphasized that the Volt would further reduce dependence on imported oil if E85 ethanol was used instead of gasoline to power the on-board generator engine. Robert Lutz added that if the driver used E85, \"\"the fuel economy figure became 525 miles per\" (equivalent) \"petroleum gallon\"\", as only 15% of gasoline is used in this blend. General Motors also noted that actual production of the Volt depended on further battery development, because the required rechargeable batteries needed to make the Volt a viable vehicle did not exist in the market and had yet to be developed. The concept car was actually powered by two 12-volt conventional car batteries, just enough power to allow the vehicle to move at low speeds in the stand.\n\nThe production design model officially unveiled on September 16, 2008, as part of General Motors centennial celebration at the Wintergarden headquarters in Detroit. The production model differed greatly in design from the original concept car. The carmaker cited necessary aerodynamic changes needed to reduce the concept car's high drag coefficient of down to a more efficient , though still somewhat higher than the Toyota Prius . Another reason was the use of General Motors' new global compact vehicle platform Delta II to keep costs reasonable, and shared with the 2010 model year Chevrolet Cruze. Another significant difference from the concept car is the seating, as the production Volt seats four rather than five passengers. This change was due to the higher-than-usual central tunnel that runs from the front console to the rear seat that houses the car's T-shaped battery pack.\n\nAfter the concept was put into the pipeline for production, General Motors began looking for a partner to develop the Volt's lithium-ion battery pack. The carmaker evaluated about twenty-five different battery cell chemistries and constructions from around two dozen lithium-ion battery makers around the world. Due to their more promising cell technologies, two companies were selected in June 2007, Compact Power (CPI), which uses a lithium manganese oxide (LiMnO) cell made by its parent company, LG Chemical; and Continental Automotive Systems, which uses lithium iron phosphate based cylindrical cells made by A123Systems. By the end of October 2007 CPI (LG Chem) delivered their finished battery pack prototypes, and A123 delivered theirs by January 2008. General Motors testing process was conducted at the laboratory the carmaker had created for the GM EV1 program. The battery packs included monitoring systems designed to keep the batteries cool and operating at optimum capacity despite a wide range of ambient temperatures. To ensure the battery pack would last ten years and expected for the battery warranty, the Volt team decided to use only half of the 16 capacity to reduce the rate of capacity degradation, limiting the state of charge (SOC) up to 80% of capacity and never depleting the battery below 30%. General Motors also was expecting the battery could withstand 5,000 full discharges without losing more than 10% of its charge capacity. According to GM, , no batteries have been changed due to degradation.\nIn April 2008 General Motors started extensive battery testing. In two years the carmaker put the battery packs to the equivalent of 150,000 real-world miles (240,000 km) and ten years of use. The durability of the battery pack was tested for a broad range of extreme ambient conditions including a shaker table to simulate potholes and a thermal chamber, to simulate temperatures varying from , typical of the Southwest deserts, to typical of the Alaska tundra. In April 2008 the lithium-ion battery pack was placed in Chevrolet Malibus fitted with the Volt powertrain to be used as test mules for further real-world testing. In October 2008 General Motors chose CPI (LG Chemical) to provide the battery systems for the first production version of the Volt. In July 2008 General Motors confirmed that a non-turbocharged, 1.4 L 4-cylinder engine would be used as the range extender, and that the intention was to build it in Flint, Michigan. In April 2009, General Motors let journalists test the Volt powertrain without the range-extending generator in the body of Chevrolet Cruze sedans that GM used as test mules at the GM Technical Center in Warren, Michigan.\n\nThe first pre-production test car based on the final Volt design was built in June 2009, in Warren, Michigan, and by October 2009, 80 Volts had been built and were tested under various conditions. On March 31, 2010, the first factory-built Volt was produced at the Detroit Hamtramck Assembly Plant to test the production line and for quality control purposes, both of the tooling and the pre-production vehicles produced before regular production began.\n\nTony Posawatz was the Volt Vehicle Line Director from 2006 to 2012, and he was known as employee #1 and led the team from concept to production.\nGeneral Motors held a ceremony at its Detroit Hamtramck Assembly Plant on November 30, 2010, to introduce the first Chevrolet Volt off the assembly line. The first Volt built for retail sale was earmarked for display at General Motors' Heritage Center museum in Sterling Heights, Michigan. The second unit was offered at a public auction, with an opening bid of and it was won by Rick Hendrick who paid . The proceeds went to fund math and sciences education in Detroit through the Detroit Public Schools Foundation. Deliveries to retail customers in the United States began in mid December 2010. Volt deliveries began in Canada in September 2011. The first deliveries of the Chevrolet Volt in Europe took place in November 2011. The European version of the Volt, the Opel Ampera, was released to retail customers in Europe in February 2012. Deliveries of the right-hand drive Vauxhall Ampera in the UK began in May 2012. The Holden Volt was released in Australia in December 2012.\n\nThe 2011 Chevrolet Volt has a / 45 A·h ( usable) lithium-ion battery pack that can be charged by plugging the car into a 120-240 VAC residential electrical outlet using the provided SAE J1772-compliant charging cord. No external charging station is required. The Volt is propelled by an electric motor with a peak output of delivering of torque. Capacity of the battery pack was increased to ( usable) for 2013 models, which increased the all-electric range from . Other specifications remained the same. The battery pack capacity was increased to for 2015 models. This incremental upgrade is likely to reflect in an improvement in range over previous model years, but , the 2015 Volt has not been re-certified with the EPA.\n\nWhile driving, after the Volt battery has dropped to a predetermined threshold from full charge, a small naturally aspirated 1.4 L 4-cylinder gasoline fueled internal combustion engine (Opel's Family 0) with approximately , powers a 55 kW generator to extend the Volt's range. The vehicle also has a regenerative braking system. The electrical power from the generator is sent primarily to the electric motor, with the excess going to the batteries, depending on the state of charge (SOC) of the battery pack and the power demanded at the wheels.\n\nPrior to the 2016 model year, the Volt required premium gasoline of (R+M)/2 octane rating of 91 or higher because the higher octane permitted the 10.5:1 compression ratio engine to use more ignition timing advance to maximize fuel efficiency by 5 to 10% compared to regular gasoline. For users who drive mostly in electric mode, and to avoid maintenance problems caused by storing the same gasoline in the tank for months, the 2011 Volt has a sealed and pressurized fuel tank to avoid evaporation. As a result, the fuel filler must be depressurized before opening the tank. Also, the engine management system monitors the time since the engine last ran, and prompts the driver to run past the all-electric range before recharging to consume some gasoline. If the driver does not run on gasoline, the system automatically runs the maintenance mode, which starts the engine to consume some of the aging fuel and circulate fluids within the engine. A configuration with an E85 flex-fuel capable engine is under development and was expected to be available in 2013.\nThe Voltec drivetrain has three power converting elements:\n\nThese units are connected via a planetary gear and three electrically controlled hydraulic clutches to provide power output for propulsion in any of four programmed operating modes:\n\n\nThe drivetrain permits the Volt to operate as a pure battery electric vehicle until its battery capacity has been depleted to a defined level, at which time it commences to operate as a series hybrid design where the gasoline engine drives the generator, which keeps the battery at minimum level charge and provides power to the electric motors. The full charge of the battery is replenished only by loading it on the electrical grid. While in this series mode at higher speeds and loads, (typically above at light to moderate loads) the gasoline engine can engage mechanically to the output from the transmission and assist both electric motors in driving the wheels, in which case the Volt operates as a power-split or series-parallel hybrid. After its all-electric range has been depleted, at speeds between , the Volt is programmed to select the most efficient drive mode, which improves performance and boosts high-speed efficiency by 10 to 15 percent.\n\nWhile operating modes are switched automatically the Volt allows the driver to choose from three drive modes: normal, sport and mountain. The mountain mode, which is expected to be required only under unusual power demand conditions, increases minimum battery state of charge (SOC) to around 45%, thus maintaining performance on steep and long grades. The driver hears more engine noise due to the higher rate of power generation required to maintain this mode. The sport mode causes the engine to rev higher, and the response to the throttle pedal is quicker. The Ampera has an additional option, the \"City Mode\" or \"battery hold\", allowing the driver to save the energy currently stored in the battery for use when traveling in urban areas or restricted zones. The 2013 model year Volt includes a \"Hold\" option to provide the same choice.\n\nThe 2011 Volt's lithium-ion battery (Li-ion) battery pack weighs and \"consists of 288 individual cells arranged into nine modules. Plastic frames hold pairs of lithium-ion cells that sandwich an aluminum cooling fin. The design and construction of that aluminum plate was critical to ensuring an even temperature distribution with no hot or cool spots across the flat, rectangular cell. The battery pack has its own cooling circuit that is similar to, but independent from, the engine cooling system.\"\n\nFor the 2011/2012 model years, the battery pack stores of energy but it is controlled or buffered via the energy management system to use only of this capacity to maximize the life of the pack. For this reason the battery pack never fully charges or depletes, as the software only allows the battery to operate within a state of charge (SOC) window of 65%, after which the engine kicks in and maintains the charge near the lower level. The minimum SOC varies depending on operating conditions. When the car needs more power, such as in mountain mode, the lower limit of the SOC rises to 45% to ensure enough power is available. The battery capacity was increased to for the 2013 model year, the SOC window was increased to use of the total battery energy, and the buffer to ensure battery life is not reduced. These changes increases the Volt's all-electric range, but charging takes slightly longer. GM achieved the improved battery performance and durability through minor changes to the material composition of the battery cell chemistry.\n\nDespite containing near identical energy (+/- ), the Volt's battery pack is over 70% lighter than the EV1's original , AC Delco lead-acid battery pack, mainly because the Volt uses higher specific energy Li-ion batteries. Li-ion batteries are expected to become less expensive as economies of scale take effect.\n\nBecause batteries are sensitive to temperature changes, the Volt has a thermal management system to monitor and maintain the battery cell temperature for optimum performance and durability. The Volt's battery pack provides reliable operation, when plugged in, at cell temperatures as low as and as high as . The Volt features a battery pack that can be both warmed or cooled. In cold weather, the car electrically heats the battery coolant during charging or operation to provide full power capability. In hot weather, the car can use its air conditioner to cool the battery coolant to prevent over-temperature damage.\n\nGeneral Motors guarantees the Volt's battery for eight years or , and cover all 161 battery components. GM estimates that Volt batteries will degrade by 10 to 30% after 8 years or 100,000 miles. GM has applied for a patent that may allow technicians to quickly and cheaply recover some of the performance of degraded battery packs. The Volt’s battery management system runs more than 500 diagnostics at 10 times per second, allowing it to keep track of the Volt’s battery pack in real-time, 85% of which ensure the battery pack is operating safely and 15% monitor battery performance and life.\n\nThe Volt uses a plug specification published in 2009, SAE J1772-2009, that is considered a standard for electric cars in North America. Depending on in-car settings a full charge takes from approximately 10 hours (12A setting) to as much as 14 hours (8A setting) from a standard North American 120 V, 15 A outlet and about 4 hours from a 240 V AC source and suitable 240 V EVSE. The Volt comes with a charging cord suitable for the standard household power outlet in its country of sale. If plugged in, recharging can be controlled remotely through a smartphone application.\n\nTo save energy, the Volt sometimes heats the seats instead of blowing heated air through HVAC system, as heating the vehicle's cabin draws significant power, and can even exceed what is needed to move the vehicle on occasions. A power-saving stereo system uses amplifiers that switch on and off rapidly to save power. It uses 50 percent less energy. The system is also lighter because the use of high grade neodymium magnets.\n\nThe Volt has a top speed of . According to Edmunds.com road tests, the Volt's 0 to 60 mph acceleration time is 9.2 seconds running on electric-only mode, and 9.0 seconds with the gasoline engine assisting propulsion. Motor Trend reports the Volt's quarter mile (402 m) time is 16.9 sec @ , while Edmunds reports a quarter mile (402 m) time of 16.8 sec @ in electric-only operation, and 16.6 sec @ with the gasoline engine assisting. Motor Trend reports a braking distance of and Edmunds.com of .\n\nAccording to General Motors the Volt's all-electric range with fully charged batteries varies from depending on terrain, driving technique, and temperature. \n\nThe 2013 model year Volt increased its EPA's rated all-electric range to . The 2014 and 2015 Volt have the same EPA ratings.\n\nThe Opel Ampera official all-electric range under the EU-approved UN ECE R101 standard for plug-in hybrids is . \n\n\nThe U.S. Environmental Protection Agency (EPA) officially rated the 2011 model year Volt's combined city/highway fuel economy in all-electric mode at 93 miles per gallon gasoline equivalent (MPG-e) (2.5 L gasoline equivalent/100 km; 112 mpg-imp gasoline equivalent) and 94 MPG-e for the 2012 model year. \n\nFor the 2012 model year, EPA revised the Volt's fuel economy ratings, increasing the combined city/highway rating in all-electric mode from 93 MPG-e to 94 MPG-e, and the highway rating was increased from 90 MPG-e to 93 MPG-e. As a result of its improved battery pack, the 2013 model year EPA rating climbed to a combined city/highway fuel economy of 98 miles per gallon gasoline equivalent (2.4 L gasoline equivalent/100 km; 118 mpg-imp gasoline equivalent).\n\nThe Opel Ampera official equivalent fuel consumption under the EU-approved UN ECE R101 standard for plug-in hybrids is (83.3 km/L). However, a leading Opel engineer prefers saying 169 Wh/km (16.9 kWh/100 km) while battery-powered, and then 20 km/L (5 L/100 km) petrol-powered. The ECE R101 standard weights charge-depleting mode as 76% and gasoline-only driving as 24%.\n\nAccording to \"Consumer Reports\" in December 2011, the Chevrolet Volt fuel cost in electric mode was 3.8¢/mile. \n\nAccording to Edmunds.com, the price premium paid for the Volt in 2012, after discounting the U.S. federal tax credit, takes a long time for consumers to recover in fuel savings, often longer than the normal ownership time period. \n\nThe Volt received a five-star overall crash safety rating from the National Highway Traffic Safety Administration (NHTSA), the highest-possible score. This rating was obtained with NHTSA's New Car Assessment Program for 2011 model year vehicles.\n\nIn August 2010, General Motors began a training program for first responders when performing rescue duties involving the Chevrolet Volt. .\n\nDue to significant noise reduction typical of vehicles traveling in all-electric mode at low speeds, the Volt is fitted with a manually activated electronic warning sound system called \"Pedestrian-Friendly Alert System\" for use when the car is operating at low speeds to alert pedestrians to the car's presence.\n\nThe second generation Chevrolet Volt was officially unveiled at the January 2015 North American International Auto Show. Retail deliveries began in the United States and Canada in October 2015 as a 2016 model year, with 1,324 units delivered in the U.S. that month. Availability in the American market was limited to California and the other 10 states that follow California’s zero emission vehicle regulations. GM scheduled the second generation as a 2017 model year to be released in the 39 remaining states by early 2016. Manufacturing of the 2017 MY Volt began in February 2016, and the first units arrived at dealerships at the end of February 2016. The 2017 model complies with stricter Tier 3 emissions requirements and was available nationwide.\n\nThe second generation Volt has an upgraded powertrain with a 1.5-liter engine that uses regular gasoline; the 18.4 kWh battery pack has new chemistry that stores 20% more electrical energy and uses fewer cells, 192 compared with 288 on the 2014 Volt; it uses a new power controller that is integrated with the motor housing; the electric motors weigh less and use smaller amounts of rare earth metals. GM engineers explained that the second generation Volt was developed using extensive input from Volt owners.\n\nThese improvements allow the 2016 Volt to deliver better EPA ratings than the first generation model. The all-electric range was officially rated at , up from attained by the 2015 Volt. The gains in efficiency allow the second generation Volt to improve its combined fuel economy in gasoline-only (charge-sustaining) mode to , up from for the previous model. The official second generation Volt's rating for combined city/highway fuel economy in all-electric mode is 106 miles per gallon gasoline equivalent (MPG-e; 2.2Le/100Km), up from 98 MPG-e (2.4Le/Km) for the 2015 first generation model. The combined gasoline-electricity fuel economy rating of the 2016 model year Volt is equivalent, 82 MPG-e (2.9Le/Km) in city driving and 72 MPG-e (3.3Le/Km) in highway. Both the all-electric range and fuel economy ratings are the same for the 2017 model year Volt.\n\nIn April 2013, CEO Daniel Akerson announced that GM expects the second generation Volt to be priced on the order of to lower than the 2013 model year with the same features. The 2016 Volt pricing started at before any government incentives, plus for destination. The starting price was lower than the 2015 Volt. In California, order books for the second generation Volt were opened on May 28, 2015.\n\nIn July 2014, Opel announced that due to the slowdown in sales, they would discontinue the Ampera after the launch of the second generation Volt—and that between 2014 and 2018, Opel planned to introduce a successor electric vehicle in Europe. General Motors announced in February 2016 that the all-electric Opel Ampera-e hatchback would go into production in 2017. This is the European version of the Chevrolet Bolt EV.\n\nIn April 2015, General Motors confirmed that it would not build the second generation Volt in right-hand-drive configuration. Due to low sales, only 246 units had been sold in Australia by mid-April 2015, and they discontinued the Holden Volt once they sold the remaining stock.\nNotes:\n\nAssembly of the Volt was assigned to Detroit/Hamtramck Assembly plant following the conclusion of the 2007 UAW-GM contract talks. For initial production the gasoline engine is being imported from the Opel engine plant in Aspern, Austria. In November 2010, General Motors began investing million at its engine operations plant in Flint, Michigan to support increased production of the Ecotec 1.4 L engine that is used in the Chevrolet Cruze, the upcoming 2012 Chevrolet Sonic, and the variant used in the Chevrolet Volt. The Flint plant was expected to start production of 400 engines a day in early 2011, ramp up daily production to 800 engines in late 2011, and to increase its capacity to 1,200 a day by late 2012. In May 2011, General Motors decided to invest an additional at the Flint plant to further increase 1.4 L engine production capacity.\n\nIn 2010, General Motors planned an initial production for calendar year 2011 of 10,000 Volts and 45,000 units for 2012, up from the 30,000 units initially announced. In May 2011, the carmaker again raised its production targets, as Volt and Ampera production capacity was increased to 16,000 units in 2011, including 3,500 units for exports and 2,500 demonstration units destined to U.S. dealerships, and the rest for U.S. sales. However, in November 2011 GM's sales chief announced that they would not meet its sales goal of 10,000 vehicles in 2011.\n\nOut of the 2012 production, General Motors expected to produce 10,000 Amperas for sale in Europe, 6,000 destined for Opel and 4,000 for Vauxhall in the UK. In addition, 2,000 Volts were available for the region. By early 2012 GM abandoned its sales target to deliver 45,000 Volts in the U.S and instead announced that production in 2012 would depend on demand. By March 2012 the Volt plant has a global production capacity of 60,000 vehicles per year.\n\nThe Volt's battery cells are produced by LG Chem in South Korea and then shipped to the US, where the battery packs are assembled at a purpose-built facility in Brownstown Charter Township, Michigan owned and operated by General Motors. Compact Power, the North American subsidiary of LG Chem, is building a battery plant in Holland, Michigan to manufacture the advanced battery cells for the Volt and other carmakers, with capacity to produce enough cells for 50,000 to 200,000 battery packs per year. The million Holland plant was funded by 50% U.S. Department of Energy matching stimulus funds and is planned to open by mid-2012.\n\nThe 2011 Chevrolet Volt was officially launched on November 30, 2010 at a ceremony at the Hamtramck plant, where the first production unit for retail sale came off the assembly line. The first retail vehicle was delivered to a customer in Denville, New Jersey on December 15, 2010. GM reported it had built 12,400 Volts in total through December 2011. This includes dealers' demo vehicles in North America and Amperas in dealerships in Europe, crash test vehicles and other unavailable Volts owned by GM.\n\nGM halted production for about one month at the Detroit/Hamtramck Assembly plant by mid June 2011 to complete some upgrades, including the installation of new tooling, equipment and overhead conveyor systems throughout the facility. These upgrades allowed GM to triple the rate of Volt production and prepared the plant for 2012 Volt and Ampera production. After the plant retooling, the production rate reached 150 units per day four days a week by August 2011. The Volt plant was also down during January 2012 in preparation for building the California lower-emission version. A four-week shutdown due to slow sales took place between March and April 2012. GM said it had around 3,600 Volts in inventory and needed to reduce dealer inventories as production is expected to meet market demand. GM also extended the traditional two-week summer vacation by an extra week at the Hamtramck plant. GM closed its Detroit-Hamtramck plant from September 17 until October 15, 2012, affecting roughly 1,500 workers on downtime while the plant was retooled to assemble the all-new 2014 Chevrolet Impala alongside the 2013 Volt. This was the second time in 2012 that GM has halted Volt production.\n\nProduction of the 2013 model year Volt began in July 2012 and customer deliveries began during the same month. In October 2012, GM announced they would build the Cadillac ELR extended-range luxury coupe at the Detroit-Hamtramck Assembly plant, together with the Chevrolet Volt, Opel Ampera, and Holden Volt. The addition of the ELR to the plant represents an additional million investment, bringing the total product investment to million since December 2009. The first 2014 ELRs rolled off the production line in late May 2013. These were pre-production units destined for testing purposes before production for retail customers began at the end of 2013. Deliveries of the 2014 model year Volt began in August 2013. Volt sales in the U.S. reached the 50,000 unit milestone in October 2013, out of more than 60,000 vehicles of the Volt/Ampera family sold worldwide. Production of the 2015 model year Volt ended in mid-May 2015, while manufacturing of pre-production units of the second generation began in March 2015. In July 2016, Volt sales in the American market passed the 100,000 unit milestone, out of about 117,000 Volt/Ampera family vehicles sold worldwide through June 2016. In November 2018, GM announced it would cease Volt production in March 2019.\n\nSales of the 2011 Chevrolet Volt began in selected markets due to limited initial production, as General Motors' original target for 2011 was 10,000 units. The first cars were delivered in Washington D.C., the New York City metropolitan region, California, and Austin, Texas. By May 2011 the Volt had been launched also in Connecticut, Maryland, Michigan, New Jersey, and Virginia. Deliveries in Delaware, Florida, Georgia, Hawaii, North Carolina, Oregon, Pennsylvania, South Carolina, and Washington began in the third quarter of 2011. In June 2011, Chevrolet dealers nationwide began taking orders for the 2012 Volt, and deliveries in all 50 states began in November 2011.\n\nThe suggested retail price (MSRP) for the 2011 Chevrolet Volt in the U.S. started at . That price excluded destination freight charge, tax, title, license, dealer fees, optional equipment—and savings due to factory incentives, tax deductions, or other subsidies for qualifying buyers. The MSRP for the 2012 Volt starts at including a destination freight charge and excludes tax, title and license fees, or other available government subsidies. The base price is less than the 2011 model year, and General Motors explained that this price reduction was possible because of a \"wider range of options and configurations that come with the expansion of Volt production for sale nationally.\" The price dropped to including destination charges for the 2014 model year.\n\nDue to the capacity of the Volt's battery pack it qualifies for the maximum federal tax credit as specified in the Emergency Economic Stabilization Act of 2008. The federal tax credit phases out over a one-year period after the manufacturer has sold at least 200,000 vehicles in the U.S. Several states also have additional incentives or rebates available for plug-in electric vehicles for qualifying buyers. The 2011 Volt price including all available regular production and premium options is , including destination charges and before tax credits or any subsidies. For the 2012 model year the price of the Volt with all available options is before tax credits or any subsidies available. , General Motors combined sales of plug-in electric vehicles in the U.S. totaled almost 197,000 units and are expected to pass 200,000 early in 2019. Thereafter, the applicable tax credit reduces gradually until it is completely phased out beginning on January 1, 2020.\n\nThe price for the home charging units is plus installation costs. The Voltec is a home-charging unit built by SPX for Volt owners. It is a 240-volt (Level II) charger, and, according to General Motors, can replenish the Volt's batteries in about four hours. Consumer Reports has advised buyers to budget up to , as many older homes may need a substantial electrical upgrade because the U.S. National Electrical Code requires that the charger have its own dedicated 220-volt, 30-amp circuit. Early buyers can benefit from the federal tax credit available for charging equipment.\n\nThe 2011 Volt was not submitted for application to the California Air Resources Board's (CARB) Clean Vehicle Rebate Project rebate and therefore was not required to meet the 10-year battery warranty requirement for enhanced advanced technology partial zero-emissions vehicles (enhAT-PZEV). The Volt team explained that for the launch General Motors decided to go with a common national package that includes an 8-year battery warranty. For this reason owners of the 2011 Volt did not qualify for California's rebates and free access to use carpool lanes even when traveling solo. A third package, scheduled for 2013, is under development with an E85 flex-fuel engine. General Motors engineering team commented that \"introducing two or three packages of an entirely new technology set and platform at the same time wasn't an option.\"\n\nIn February 2012 General Motors began deliveries of a low emission version destined for California that features a new low emissions package that allows the 2012 Chevrolet Volt to qualify as an enhanced, advanced technology –partial zero emissions vehicle (enhAT-PZEV) and have access to California’s high-occupancy vehicle lanes (HOV). The new standard California version of the Volt features a modified engine and exhaust components. The catalytic converter was modified to add a secondary air-injection pump that \"\"streams ambient air into the exhaust stream to increase its ability to remove pollutants\".\" Owners of a 2012 Volt with the low emissions package are eligible to apply for one of 40,000 available HOV lane stickers issued to vehicles that qualify as a California AT-PZEV. The permits are handed out on a first-apply, first-served basis. Additionally, the new low emissions package makes the 2012 Volt eligible for owners to receive up to in state rebates through the state’s Clean Vehicle Rebate Project (CVRP). This incentive is in addition to the federal government’s tax credit. Only the 2012 Volts manufactured after February 6, 2012, are fitted with the low emission package and sold as standard models in California. Other states where the Volt has solo driving access to HOV lanes are Florida, Georgia, New York and Virginia.\n\n\nSince sales began in December 2010, a total of 148,556 Volts have been sold in the country through October 2018. December 2016 is the best monthly sales volume on record ever, with 3,691 units delivered. Sales in 2016 set a calendar year record of 24,739 sales. The previous annual record was 2012 with 23,461 units sold. The Volt ranked as the all-time top selling plug-in electric car in the United States until February 2015, when it was surpassed by the all-electric Nissan Leaf in March 2015. Cumulative Volt sales passed Leaf sales in March 2016, and became once again the best selling plug-in car in the U.S. ever. In July 2016, Volt sales in the American market passed the 100,000 unit milestone. , the Volt continued to rank as the all-time plug-in electric car in the U.S.\n\n\nIn May 2011 Kelley Blue Book (KBB) projected the 2011 Chevrolet Volt resale value at just over after 36 months, the length of a typical lease, which represents 42% of the car's suggested retail price (MSRP). KBB explains that even though the residual value seems low, the projection considered that the first 200,000 Volts sold qualified for a federal tax credit, which effectively reduces the MSRP to , making the represent 51% of its original value after the tax credit. In comparison, KBB notes, the 2011 Toyota Prius has a projected residual of 46% after 36 months. KBB's estimate assumed a gasoline price around per gallon in 2014. For 2012, Kelley Blue Book expected the Volt to retain 42% of its original value after 3 years and 27% after 5 years. Based on these figures, in November 2011 KBB awarded the Volt with the 2012 Best Resale Value Awards in the plug-in electric car category. KBB explains that the residual value for the Volt is lower than the market 35.5% average due to the federal tax credit, which lowers the transaction price and pushes down the residual value.\n\nConsumer Reports' analysis show that many Chevrolets lose about half of their purchase price after three years of ownership, and if the Volt depreciates the same, seems a reasonable estimate. However, Consumer Reports have noted that fuel-efficient hybrids and diesel models often depreciate far less than most vehicles, which might increase the Volt's resale value after three years above the estimate. Additionally, if gasoline prices continue to rise or if the tax credits expire, the demand for used Chevrolet Volts could quickly increase, raising their market value. On the other hand, if the next-generation Volt’s battery has twice the capacity and cost less, as General Motors has claimed, the first generation Volts would be obsolete when the new ones come out in 2015. Considering these assumptions, Consumer Reports said, \"At this point we believe it’s still unclear how the Volt will fare.\"\n\nGeneral Motors is sponsoring the Pecan Street demonstration project at the Mueller neighborhood in Austin, Texas. The project objective is to learn the charging patterns of plug-in electric car owners, and to study how a residential fleet of electric vehicles might strain the electric grid if all owners try to charge them at the same, which is what the preliminary monitoring found when the plug-in cars return home in the evening. , the community has nearly 60 Chevrolet Volt owners alone thanks to GM's commitment to match the federal government's rebate incentive, which halves the purchase price of the Volt.\n\nChevrolet began taking orders in May 2011 and deliveries began in September 2011 in major cities only. During 2012 the Volt was the best selling plug-in car in Canada, outselling all other PEVs combined. Despite a 24% reduction from 2012 sales, the Volt continued as the top selling PEV in the Canadian market in 2013, and again in 2014. , the Volt continued to rank as the all-time top selling plug-in electric car in Canada. Since September 2011, a total of 16,653 new Volts have been delivered in Canada through the end of October 2018. The monthly sales record was set in May 2018 with 731 deliveries. Sales in 2016 set a calendar year record of 3,469 units delivered.\n\nThe suggested retail price (MSRP) for the 2012 Chevrolet Volt started at ( in June 2011). This excludes any charges, fees, and optional equipment—and is before any available subsidies or incentives for qualifying buyers. The Canadian market offers the Volt in one standard trim level with two option packages: \"Premium Trim Package\" and \"Rear Camera and Park Assist Package\". Some provinces are offering Government incentives including Ontario, Quebec (both at ) and British Columbia has announced their new \"LiveSmart BC\" program in which the Chevrolet Volt qualifies for a incentive/rebate as well as towards charging equipment.\n\nThe European version of the Volt, the Opel Ampera (known as the Vauxhall Ampera in the United Kingdom), was unveiled at the Geneva Auto Show in March 2009 and also was exhibited at the 2009 Frankfurt Auto Show. Opel developed the battery control modules for the Ampera at the Opel Alternative Propulsion Center Europe in Mainz-Kastel, Germany. The production version of the Ampera was unveiled at the 2011 Geneva Motor Show. The Ampera is assembled at the Detroit/Hamtramck Assembly plant, Michigan.\nThe main differences between the Volt and the Ampera are in their styling. The Ampera has a distinctive front and rear fascia, with a large cut-out in the rear bumper. The Opel Ampera features more stylized alloy wheels as standard, and the side skirts are body-colored rather than black. In the inside there are only minor differences and both versions share the same exact powertrain and battery pack. A key operational difference was that the Ampera has four drive modes, one more than the 2011/12 model year Volt. The additional option is \"City Mode\", which adapts battery management to the needs of commuter travel. City mode or \"battery hold\" engages the range-extender immediately, allowing to save the energy currently stored in the battery, and when switched off, the range-extender stops and the Ampera is then able to use the energy saved in the battery for pure electric driving, for example for traveling urban areas or restricted zones, such as the European low emission zones or to allow the Ampera to qualify for an exemption of the London congestion charge The 2013 model year Volt included the \"Hold Drive\" button to allow drivers to conserve battery-pack energy for use at a particular time of their choice.\n\nGeneral Motors production target for 2012 was to manufacture 10,000 Amperas for sale in Europe, 6,000 destined for Opel and 4,000 for Vauxhall in the UK, plus an additional 2,000 Volts were to be made available for the region. The carmaker targeted the Ampera for business fleet market and local government agencies, where Opel has a strong customer base, while the Volt is aimed at retail customers. According to Opel, by June 2011 around 5,000 customers across Europe had reserved an Ampera, with fleet or business customers representing 60% of reservations, and a total of 7,000 orders were received by March 2012, with Benelux, Germany and the United Kingdom as the top markets in terms of orders.\n\nThe first deliveries of the Chevrolet Volt in Europe took place on November 30, 2011, to the U.S. Embassy in France. Distribution of the Opel Ampera to dealerships began in December 2011, but deliveries to customers were delayed until February 2012 because Opel decided to wait until the NHTSA completed its investigation of the Volt's battery fire risk after a crash. Since May 2012 the Vauxhall Ampera is available through the Zipcar carsharing club in London, Bristol, Cambridge and Oxford.\n\nThe Opel/Vauxhall Ampera was Europe's top selling plug-in electric car in 2012 with 5,268 units and captured a 21.5% market share of the region's plug-in electric passenger car segment. , the Ampera held a market share of almost 10% of European registration of plug-in electric cars since 2011. The market share in the Netherlands was 40% and 10% in Germany. Ampera sales fell 40% in 2013 to 3,184 cars, and within the plug-in hybrid segment, the Ampera was surpassed in 2013 by the Mitsubishi Outlander P-HEV (8,197), Volvo V60 plug-in (7,437), and the Prius plug-in (4,314). In 2013 the Ampera ranked eighth among Europe's top selling plug-in electric vehicles, and its market share fell to about 5%. During the first five months of 2014, only 332 units had been sold, down 67% from the same period in 2013. In July 2014, Opel announced that due to the slowdown in sales, they would discontinue the Ampera after the second generation Volt launch—and that between 2014 and 2018, they plan to introduce a successor electric vehicle in Europe. Ampera sales totaled 939 units in 2014, and only 215 units during the first nine months of 2015.\n\n, Opel/Vauxhall Ampera sales totaled just over 10,000 units since 2011, with the Netherlands as the leading market with 5,031 Amperas registered, followed by Germany with 1,542 units, and the UK with 1,250 units registered by the end of June 2015. The Netherlands is also the top selling Volt market in Europe with 1,062 units registered through December 2014, out of about 1,750 Volts sold through 2014.\n\nIn February 2011 Opel announced they would offer the Ampera throughout the Eurozone for a uniform (, including VAT. Prices by country still varied due to different trim levels in each market. The Chevrolet Volt also has a uniform price that starts at ( in May 2012) including VAT. The Opel Ampera is eligible to several subsidies and tax breaks available for plug-in electric vehicles in several European countries.\n\nIn the UK, the Vauxhall Ampera starts at ( in May 2012) before discounting the Plug-in Car Grant The Chevrolet Volt was also be available in the UK at ( in May 2012) before the government grant. All Volts in the UK came standard with leather interior.\n\nGeneral Motors unveiled the Chevrolet Volt in Shanghai under its Chinese name of 沃蓝达 (Wo Lan Da) in September 2010. The first Volts, out of the 10-vehicle demonstration fleet, arrived in China by late December 2011. The demonstration program is taking place in Beijing, Tianjin and Shanghai.\n\nThe Volt went on sale in China by late 2011 with pricing starting at (around ) and sales are limited to eight Chinese cities: Beijing, Shanghai, Hangzhou, Suzhou, Wuxi, Guangzhou, Shenzhen and Foshan. GM explained that 13 dealerships were selected in the eight cities, and they were chosen because these \"cities have more elites who are inclined to try new technologies and lead the fashion tide.\"\n\nHowever, according to General Motors, in a move illegal under WTO rules the Chinese government refused the allow Chevrolet Volt owners access to up to in government subsidies available for plug-in vehicles unless GM had agreed to transfer intellectual property to a joint venture with a Chinese automaker for at least one of the Volt’s three core technologies: electric motors, complex electronic controls, and power storage devices, whether batteries or a fuel cell. General Motors negotiated with the Chinese government to let the Volt qualify for the subsidies without the technology transfer, but , the subsidies were available only for electric cars made by Chinese automakers. As a result of the high import duties, General Motors reported in August 2012 that sales are minimal, those of a very low-volume car. According to LMC Automotive, a total of 18 Volts have been sold in China through June 2012.\n\nIn March 2012, General Motors announced that an agreement was signed with the China Automotive Technology and Research Center (CATARC) to manage the Volt demonstration fleet in Beijing and to gather feedback from the fleet usage for one year. The demonstration Volts were scheduled to be delivered in April 2012.\n\nThe Buick Velite 5 was introduced at the 2017 Shanghai Auto Show, a rebadged second generation Chevrolet Volt tailored for the Chinese market. The Velite 5 will be manufactured in China.\n\n\nDeliveries of the Holden Volt in the Australian market began in December 2012, and the first Volt was delivered to the U.S. Ambassador in Canberra. Pricing starts at (around US$62,598). In November 2011 the first Holden Volt arrived in Australia for a series of evaluation tests. Holden stated that the Volt underwent numerous modifications to better suit it to Australian roads, though the test vehicles were still left-hand drive.\n\nThe Holden Volt was available through 49 select Holden dealerships throughout metropolitan and rural Australia, with 18 in Victoria, 11 in New South Wales, 9 in Queensland, 7 in Western Australia and 4 in South Australia. A total of 80 Holden Volts were sold during 2012, and 101 units in 2013. A total of 246 had been sold in the country by mid April 2015, with the stock of the first generation almost empty. General Motors announced that it would not build the second generation Volt in right-hand-drive configuration, so the Volt was discontinued in Australia when the remaining stock sold out. fewer than 250 were sold in total .\n\nGeneral Motors do Brasil announced that it will import from five to ten Volts to Brazil during the first semester of 2011 as part of a demonstration and also to lobby the federal government to enact financial incentives for green cars. If successful, General Motors would adapt the Volt to operate on ethanol fuel, as most new Brazilian cars are flex-fuel.\n\nIn December 2010, General Motors announced plans to introduce the Volt in limited numbers into Japan in 2011 for technology and market test purposes. Exports for retail sales will depend on the results of this trial.\n\nThe second generation Volt was released for retail customers in December 2015. Pricing starts at 638,000 pesos (~), and it is available in Mexico City, Monterrey, Guadalajara, Querétaro, and Puebla.\n\nThe Holden Volt will be released in New Zealand through three dealerships, with one in Auckland, Christchurch and Wellington. Deliveries are scheduled to begin by late 2012 and pricing starts at (around ).\n\nCombined global Volt/Ampera sales passed the 100,000 unit milestone in October 2015. The Volt/Ampera family was the world's best selling plug-in electric car in 2012 with 31,400 units sold. The Opel/Vauxhall Ampera was Europe's top selling plug-in electric car in 2012 with 5,268 units, representing a market share of 21.5% of the region's plug-in electric passenger car segment. However, during 2013 Ampera sales fell 40%, and the declining trend continued during 2014 and 2015.\n\n, global Volt/Ampera family sales totaled about 177,000 units since its inception in December 2010, including over 10,000 Opel/Vauxhall Amperas sold in Europe up to December 2015. , the Volt family of vehicles ranks as the world's all-time top-selling plug-in hybrid, and it is also the third best selling plug-in electric car in history after the Nissan Leaf (375,000) and the Tesla Model S (253,000).\n\n, Chevrolet Volt sales are led by the United States with 148,556 units delivered, followed by Canada with 16,653 units through September 2018, and the Netherlands with 1,062 Volts registered through December 2015. Out of the 9,989 Opel/Vauxhall Amperas sold in Europe through December 2015, 5,031 were registered in the Netherlands, 1,542 in Germany, and 1,279 in the UK by the end of September 2015, together representing 78% of Ampera sales.\n\nThe following tables present retail sales of the Volt and Ampera variants through December 2015 for the top selling national markets by year since deliveries began in December 2010. Demonstration vehicles allocated to dealerships are not included in retail sales reports while they are used for test drives.\n\n\nThe Cadillac Converj is a plug-in hybrid concept car first unveiled at the 2009 North American International Auto Show. It incorporated the propulsion system from the Chevrolet Volt, including the Voltec powertrain. In August 2011, General Motors announced it would produce the Converj as the Cadillac ELR. The first 2014 ELRs rolled off the production line in late May 2013. These were pre-production units destined for testing purposes and production for retail customers started at the end of 2013. The ELR was released to retail customers in the U.S. in December 2013.\n\nAt the 2010 Auto China show General Motors unveiled the Chevrolet Volt MPV5 Concept. The Volt MPV5 is a plug-in crossover hybrid and has a top speed of and an electric range of . The MPV5 integrates design elements from the Volt, with a body style very similar to the Chevrolet Orlando and four inches larger than its predecessor, Chevrolet HHR.\n\nThe Opel Monza Concept is a four-seat coupe plug-in hybrid concept car with gullwing door unveiled at the 2013 Frankfurt Motor Show. The concept shares the same basic plug-in hybrid setup as the Chevrolet Volt and Opel Ampera, but using a turbocharged 1 L 3-cylinder natural gas-powered engine as its range extender instead of General Motors’ current 1.4 L gasoline engine. According to Opel, this concept is the role-model for the next generation of Opel cars, and because of its modular chassis design, future cars based on it would be able to accommodate gasoline, diesel or electric power.\n\nIn 2008, General Motors was concerned about how the United States Environmental Protection Agency (EPA) would test the Volt to determine its official fuel economy rating. The controversy centered on whether, by including a gasoline engine, the Volt should be classified as a hybrid rather than an electric car as claimed by General Motors. If tested with the same EPA tests used by other hybrids, the Volt's EPA fuel economy rating would be around due to the current EPA test for hybrids disallowing vehicles from boosting their mpg rating using stored battery power. General Motors stated that the Volt is an entirely new type of vehicle that the EPA's current fuel economy tests are not suited to rate, and that a new test should be devised for this emerging class of hybrid-electrics. General Motors also advocated for a more simplified mpg calculation method to take into account the range of a plug-in hybrid while running solely on electricity. Because the Volt can travel on batteries alone, GM argued that most drivers with a daily commute of less than that distance would drive only in electric mode, so long as they recharged their vehicle at work or at home overnight.\n\nThe EPA official rating issued in November 2010 included separate fuel economy ratings for all-electric mode and gasoline-only mode, with an overall combined city/highway gasoline-electricity fuel economy rating of equivalent (MPG-e). To address the variability of the fuel economy outcome depending on miles driven between charges, EPA also included in the Volt's fuel economy label a table showing fuel economy and electricity consumed for five different scenarios driven between a full charge, and a never-charge scenario. According to this table the Volt's fuel economy goes up to equivalent (MPG-e) if driven between full charges. Also, in recognition of the multiple operating modes that a plug-in hybrid can be built with (all-electric, blended, and gasoline-only), for the new fuel economy and environment label that will be mandatory in the U.S. beginning in model year 2013, EPA and the National Highway Traffic Safety Administration (NHTSA) issued two separate fuel economy labels for plug-in hybrids. One label is for extended-range electric vehicles, like the Chevy Volt, with two modes: all-electric and gasoline-only; and a second label for blended mode that includes a combination of all-electric, gasoline and electric operation, and gasoline only, like a conventional hybrid vehicle.\n\nIn August 2009, General Motors released its estimated city fuel economy rating for the Volt of of gasoline plus /100 mi (560 kJ/km) of electricity using the EPA's proposed method for evaluating plug-in hybrids. The U.S. Environmental Protection Agency (EPA) issued a statement clarifying that the \"EPA has not tested a Chevy Volt and therefore cannot confirm the fuel economy values claimed by GM.\" In July 2010, GM explained that their estimate was based on a formula that had not been officially approved and that they had been awaiting the EPA's decision on how the equivalent fuel economy of plug-in hybrids would be estimated.\n\nThe official EPA rating was issued in November 2010 and became the agency's first fuel economy label for a plug-in hybrids. The EPA rated the 2011 Volt combined fuel economy at 93 miles per gallon gasoline equivalent (MPG-e) in all-electric mode, and in gasoline-only mode, for an overall combined fuel economy rating of equivalent (MPG-e). The label also shows the combined city-highway fuel economy in all-electric mode expressed in traditional energy consumption units, rating the Volt at 36 per .\n\nIn 2009, the Presidential Task Force on the Auto Industry said that \"GM is at least one generation behind Toyota on advanced, “green” powertrain development. In an attempt to leapfrog Toyota, GM has devoted significant resources to the Chevy Volt\" and that \"while the Chevy Volt holds promise, it is currently projected to be much more expensive than its gasoline-fueled peers and will likely need substantial reductions in manufacturing cost in order to become commercially viable.\"\nA 2009 Carnegie Mellon University study found that a PHEV-40 will be less cost effective than a HEV or a PHEV-7 in all of the scenarios considered, due to the cost and weight of the battery. Jon Lauckner, a Vice President at General Motors, responded that the study did not consider the inconvenience of a electric range and that the study's cost estimate of per for the Volt's battery pack was \"many hundreds of dollars per kilowatt hour higher\" than what it costs to make today.\"\n\nIn early 2010, it was reported that General Motors would lose money on the Volt for at least the first couple of generations, but it hoped the car would create a green image that could rival the Prius.\n\nAfter the Volt's sales price was announced in July 2010, there was concern expressed of the launch price of the Volt and its affordability and resulting popularity, especially when the federal subsidies of were taken into account in the development of the car.\n\nGeneral Motors CEO Edward Whitacre Jr. rejected as \"ridiculous\" criticism that the Volt's price is too expensive. He said that \"I think it's a very fair price. It's the only car that will go coast to coast on electricity without plugging it in, and nobody else can come close.\" Despite the federal government being the major GM shareholder due to the 2009 government-led bankruptcy of the automaker, during a press briefing at the White House a Treasury official clarified that the federal government did not have any input on the pricing of the 2011 Chevrolet Volt.\n\nThere have also been complaints regarding price markups due to the initial limited availability in 2010 of between to above the recommended price, and at least in one case a mark up in California. Even though the carmaker cannot dictate vehicle pricing to its dealers, GM said that it had requested its dealers to keep prices in line with the company’s suggested retail price.\n\nIn May 2011 the National Legal and Policy Center announced that some Chevrolet dealers were selling Volts to other dealers and claiming the federal tax credit for themselves. Then the dealers who bought the Volts sell them as used cars with low mileage to private buyers, who no longer qualify for the credit. General Motors acknowledged that 10 dealer-to-dealer Volt sales had taken place among Chevrolet dealers, but the carmaker said they do not encourage such practice.\n\nIn September 2012, Reuters published an opinion/editorial article where it claimed that General Motors, nearly two years after the introduction of the car, was losing on each Volt it built. The article concluded that the Volt is \"over-engineered and over-priced\" and that its technological complexity has put off many prospective buyers, due to fears the car may be unreliable. GM executives replied that Reuters' estimates were significantly flawed as they also allocated the vehicle's research and development program costs only against the number of Volts sold in the United States (), instead of spreading the total costs over the entire lifetime of the model, as well as including those units sold in Europe and other countries. GM explained that the investments will pay off once the innovative technologies of the Volt are applied across multiple current and future products.\n\nIn June 2011 a Volt that had been subjected by the National Highway Traffic Safety Administration (NHTSA) to a side pole impact crash test followed by a post-impact rollover, caught fire three weeks later in the test center parking lot, burning nearby vehicles. The battery was found to be the source of the fire. After the fire, both Chevrolet and the NHTSA independently replicated the crash test and a subsequent vehicle rotation procedure to test for any fluid leakage, but in their first attempt they could not reproduce the conditions under which the battery pack ignited. The NHTSA said it had \"concluded that the crash test damaged the Volt’s lithium-ion battery and that the damage led to a vehicle fire that took several weeks to develop.\" In further testing of the Volt's batteries carried out by NHTSA in November 2011, two of the three tests resulted in thermal events. One battery pack was rotated 180 degrees within hours after it was impacted and began to smoke and emit sparks after rotation. In the other case, the battery pack that was crashed-tested one week earlier and that had been monitored since the test caught fire. The NHTSA then took an uncommon step on November 25, 2011 and opened a formal safety defect investigation \"without any data from real-world incidents\" to examine the potential risks involved from intrusion damage to the battery pack in the Chevrolet Volt. After the initial Volt fire, the NHTSA examined the Nissan Leaf and other plug-in electric vehicles and said its testing \"has not raised safety concerns about vehicles other than the Chevy Volt.\"\n\nAs a result of this investigation, GM announced that it would offer any new GM car in exchange to any Volt owner who has concerns while the federal investigation was taking place. In December 2011, the company said that if necessary they were prepared to recall all the vehicles and repair them upon determination of the cause of the fires, and also announced they would buy back the car if the owner was too afraid of the potential for a fire. GM's CEO also said that it may be necessary to redesign or make changes to the battery pack depending on the recommendations from federal officials. As of December 1, 33 Volt owners in the U.S. and 3 in Canada had requested a loaner car. As of December 5, General Motors reported that a couple dozen Volt owners had requested the carmaker to buy back their cars, and the company had already agreed to repurchase about a dozen. Before the carmaker agrees to buy back each vehicle, other options are explored as GM primarily wants to provide loaner cars, but \"if the only way we can make them happy is to repurchase it, then we will,\" a GM spokesman said. General Motors explained that the buy back price includes the Volt purchase price, plus taxes and fees, less a usage fee based on how many miles the car has been run. As of January 5, 2012, GM reported that around 250 Volt owners had requested either a loaner vehicle or a potential buyback.\n\nThe NHTSA also said it was working with all automakers to develop postcrash procedures to keep occupants of electric vehicles and emergency personnel who respond to crash scenes safe. Additionally, NHTSA advised to be aware that fires may occur a considerable amount of time after a crash. General Motors said the first fire would have been avoided if GM's protocols for deactivating the battery after the crash had been followed. These protocols had been used by GM since July 2011 but were not shared with the NHTSA until November 2011. In another statement the carmaker stated that they \"are working with other vehicle manufacturers, first responders, tow truck operators, and salvage associations with the goal of implementing industrywide protocols.\"\nCustomer deliveries of the Opel Ampera in Europe were delayed until the NHTSA completed its investigation of the Volt's battery fire risk to make sure the vehicle is safe. However, deliveries of the first Chevrolet Volts in Europe began in France in November 2011. Deliveries of the Vauxhall Ampera in the UK continued as scheduled for May 2012. Opel Ampera deliveries began in February 2012.\n\nOn January 5, 2012, General Motors announced that it would offer a customer-satisfaction program to provide modifications to the Chevrolet Volt to reduce the chance that the battery pack could catch fire days or weeks after a severe accident. The carmaker described the modifications as voluntary enhancements and stated that neither the car nor the battery was being recalled. General Motors determined the June fire was the result of a minor intrusion from a portion of the vehicle into a side section of the battery pack. This intrusion resulted in a small coolant leak inside the battery of approximately . When the vehicle was put through a slow roll, where it was rotated at 90-degree increments, holding in each position for about five minutes, an additional of coolant leaked. With the vehicle in the 180 degrees position (upside down), the coolant came in contact with the printed circuit board electronics at the top of the battery pack and later crystallized. Three weeks later this condition, in combination with a charged battery, led to a short circuit that resulted in the post-crash fire.\n\nGeneral Motors explained the modifications will enhance the vehicle structure that surround the battery and the battery coolant system to improve battery protection after a severe crash. The safety enhancements consist of strengthening an existing portion of the Volt’s vehicle safety structure to further protect the battery pack in a severe side collision; add a sensor in the reservoir of the battery coolant system to monitor coolant levels; and add a tamper-resistant bracket to the top of the battery coolant reservoir to help prevent potential coolant overfill. The additional side safety structural pieces have a total weight of , and their function is to spread the load of a severe side impact away from the battery pack, reducing the possibility of intrusion into the pack.\n\nDuring December 2011, GM conducted four crash tests of Volts with the reinforced steel and upgraded cooling system, resulting in no intrusion to the battery and no coolant leakage. On December 22, 2011, the NHTSA also subjected a modified Volt to the same test that led to the original fire, with no signs of the damage that is believed to have been the cause. The NHTSA said \"the preliminary results of the crash test indicate the remedy proposed by General Motors today should address the issue of battery intrusion\" though its investigation remained open. General Motors declined to say how much the modifications would cost.\n\nAll 12,400 Chevrolet Volts produced until December 2011, including all Amperas in stock at European dealerships, were scheduled to receive the safety enhancements. Since production was halted during the holidays, the enhancements were in place when production resumed in early 2012. Sales continued, and dealers modified the Volts they had in stock. General Motors sent a letter to Volt owners indicating that they could schedule the service appointment to protect their batteries beginning in the last week of March 2012. General Motors also decided to replace the 120-volt charging cords in most of the nearly 10,000 Volts sold since late 2010. The new cords were enhanced to add durability, and some of the chargers built after February 5 have the new cords.\n\nOn January 20, 2012, the National Highway Traffic Safety Administration closed the Volt's safety defect investigation related to post-crash fire risk. The agency concluded that \"no discernible defect trend exists\" and also found that the modifications recently developed by General Motors are sufficient to reduce the potential for battery intrusion resulting from side impacts. The NHTSA also said that \"based on the available data, NHTSA does not believe that Chevy Volts or other electric vehicles pose a greater risk of fire than gasoline-powered vehicles.\" The agency also announced it has developed interim guidance to increase awareness and identify appropriate safety measures regarding electric vehicles for the emergency response community, law enforcement officers, tow truck operators, storage facilities and consumers.\n\n\nThe chairman of the Subcommittee on Regulatory Affairs, Stimulus Oversight and Government Spending, U.S. Representative Jim Jordan held hearings on January 25, 2012, to investigate why the NHTSA opened a formal investigation only five months after the first postcrash battery fire occurred in June. The subcommittee of the House Committee on Oversight and Government Reform wanted to determine if government officials, including from NHTSA, purposely held back information on the Volt fire for political reasons. Both Daniel Akerson, General Motors CEO, and David L. Strickland, NHTSA administrator, denied any wrongdoing.\n\nThe Volt has received awards from multiple organizations:\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "20764699", "url": "https://en.wikipedia.org/wiki?curid=20764699", "title": "Chu 13", "text": "Chu 13\n\nCHU 13 medium is a culture medium used in microbiology for the growth of certain algal species, first published by S.P. Chu in 1942. It is used as growth medium for the biofuel candidate alga \"Botryococcus braunii\".\n\nCHU 13 includes essential minerals and trace elements that are required by algae for growth, but does not include a carbon source and so is only appropriate for growth of phototrophs. It can be prepared as either a liquid medium or as an agar medium.\n\nThe remaining volume is pure, de-ionized water.\n\nBecause it is difficult to weigh out some of the trace minerals, it is advisable to create a mixture of all components at a large concentration, such as a thousand times these measures, and then mix with the appropriate amount of (pure, de-ionized) water. Correct pH to 7.5, and then autoclave.\n"}
{"id": "30031679", "url": "https://en.wikipedia.org/wiki?curid=30031679", "title": "Cocrystal", "text": "Cocrystal\n\nCocrystals are \"solids that are crystalline single phase materials composed of two or more different molecular or ionic compounds generally in a stoichiometric ratio which are neither solvates nor simple salts.\" A broader definition is that cocrystals \"consist of two or more components that form a unique crystalline structure having unique properties.\" Several subclassifications of cocrystals exist.\n\nCocrystals can encompass many types of compounds, including hydrates, solvates and clathrates, which represent the basic principle of host-guest chemistry. Hundreds of examples of cocrystallization are the reported annually.\n\nThe first reported cocrystal, quinhydrone, was studied by Friedrich Wöhler in 1844. Quinhydrone is a cocrystal of quinone and hydroquinone (known archaically as quinol). He found that this material was made up of a 1:1 molar combination of the components. Quinhydrone was analyzed by numerous groups over the next decade and several related cocrystals were made from halogenated quinones.\n\nMany cocrystals discovered in the late 1800s and early 1900s were reported in \"Organische Molekulverbindungen\", published by Paul Pfeiffer in 1922. This book separated the cocrystals into two categories; those made of inorganic:organic components, and those made only of organic components. The inorganic:organic cocrystals include organic molecules cocrystallized with alkali and alkaline earth salts, mineral acids, and halogens as in the case of the halogenated quinones. A majority of the organic:organic cocrystals contained aromatic compounds, with a significant fraction containing di- or trinitro aromatic compounds. The existence of several cocrystals containing eucalyptol, a compound which has no aromatic groups, was an important finding which taught scientists that pi stacking is not necessary for the formation of cocrystals.\n\nCocrystals continued to be discovered throughout the 1900s. Some were discovered by chance and others by screening techniques. Knowledge of the intermolecular interactions and their effects on crystal packing allowed for the engineering of cocrystals with desired physical and chemical properties. In the last decade there has been an enhanced interest in cocrystal research, primarily due to applications in the pharmaceutical industry.\n\nCocrystals represent about 0.5% of the crystal structures archived in the Cambridge Structural Database (CSD). However, the study of cocrystals has a long history spanning more than 160 years. They have found use in a number of industries, including pharmaceutical, textile, paper, chemical processing, photographic, propellant, and electronic.\n\nThe meaning of the term \"cocrystal\" is subject of disagreement. One definition states that a cocrystal is a crystalline structure composed of at least two components, where the components may be atoms, ions or molecules. This definition is sometimes extended to specify that the components be solid in their pure forms at ambient conditions. However, it has been argued that this separation based on ambient phase is arbitrary. A more inclusive definition is that cocrystals \"consist of two or more components that form a unique crystalline structure having unique properties.\" Due to variation in the use of the term, structures such as solvates and clathrates may or may not be considered cocrystals in a given situation. The difference between a crystalline salt and a cocrystal lies merely in the transfer of a proton. The transfer of protons from one component to another in a crystal is dependent on the environment. For this reason, crystalline salts and cocrystals may be thought of as two ends of a proton transfer spectrum, where the salt has completed the proton transfer at one end and an absence of proton transfer exists for cocrystals at the other end.\n\nThe components interact via non-covalent interactions such as hydrogen bonding, ionic interactions, van der Waals interactions and Π-interactions. The intermolecular interactions and resulting crystal structures can generate physical and chemical properties that differ from the properties of the individual components. Such properties include melting point, solubility, chemical stability, and mechanical properties. Some cocrystals have been observed to exist as polymorphs, which may display different physical properties depending on the form of the crystal.\n\nPhase diagrams determined from the \"contact method\" of thermal microscopy is valuable in the detection of cocrystals. The construction of these phase diagrams is made possible due to the change in melting point upon cocrystallization. Two crystalline substances are deposited on either side of a microscope slide and are sequentially melted and resolidified. This process creates thin films of each substance with a contact zone in the middle. A melting point phase diagram may be constructed by slow heating of the slide under a microscope and observation of the melting points of the various portions of the slide. For a simple binary phase diagram, if one eutectic point is observed then the substances do not form a cocrystal. If two eutectic points are observed, then the composition between these two points corresponds to the cocrystal.\n\nThere are many synthetic strategies that are available to prepare cocrystals. However, it may be difficult to prepare single cocrystals for X-ray diffraction, as it has been known to take up to 6 months to prepare these materials.\n\nCocrystals are typically generated through slow evaporation of solutions of the two components. This approach has been successful with molecules of complimentary hydrogen bonding properties, in which case cocrystallization is likely to be thermodynamically favored.\n\nMany other methods exist in order to produce cocrystals. Crystallizing with a molar excess of one cocrystal former may produce a cocrystal by a decrease in solubility of that one component. Another method to synthesize cocrystals is to conduct the crystallization in a slurry. As with any crystallization, solvent considerations are important. Changing the solvent will change the intermolecular interactions and possibly lead to cocrystal formation. Also, by changing the solvent, phase considerations may be utilized. The role of a solvent in nucleation of cocrystals remains poorly understood but critical in order to obtain a cocrystal from solution.\n\nCooling molten mixture of cocrystal formers often affords cocrystals. Seeding can be useful. Another approach that exploits phase change is sublimation which often forms hydrates.\n\nGrinding, both neat and liquid-assisted, is employed to produce cocrystal, e.g., using a mortar and pestle, using a ball mill, or using a vibratory mill. In liquid-assisted grinding, or kneading, a small or substoichiometric amount of liquid (solvent) is added to the grinding mixture. This method was developed in order to increase the rate of cocrystal formation, but has advantages over neat grinding such as increased yield, ability to control polymorph production, better product crystallinity, and applies to a significantly larger scope of cocrystal formers. and nucleation through seeding.\n\nSupercritical fluids (SCFs) serve as a medium for growing cocrystals. Crystal growth is achieved due to unique properties of SCFs by using different supercritical fluid properties: supercritical CO2 solvent power, anti-solvent effect and its atomization enhancement.\n\nUsing intermediate phases to synthesize solid-state compounds is also employed. The use of a hydrate or an amorphous phase as an intermediate during synthesis in a solid-state route has proven successful in forming a cocrystal. Also, the use of a metastable polymorphic form of one cocrystal former can be employed. In this method, the metastable form acts as an unstable intermediate on the nucleation pathway to a cocrystal. As always, a clear connection between pairwise components of the cocrystal is needed in addition to the thermodynamic requirements in order to form these compounds.\n\nImportantly, the phase that is obtained is independent of the synthetic methodology used. It may seem facile to synthesize these materials, but on the contrary the synthesis is far from routine.\n\nCocrystals may be characterized in a wide variety of ways. Powder X-Ray diffraction proves to be the most commonly used method in order to characterize cocrystals. It is easily seen that a unique compound is formed and if it could possibly be a cocrystal or not owing to each compound having its own distinct powder diffractogram. Single-crystal X-ray diffraction may prove difficult on some cocrystals, especially those formed through grinding, as this method more often than not provides powders. However, these forms may be formed often through other methodologies in order to afford single crystals.\n\nAside from common spectroscopic methods such as FT-IR and Raman spectroscopy, solid state NMR spectroscopy allows differentiation of chiral and racemic cocrystals of similar structure.\n\nOther physical methods of characterization may be employed. Thermogravimetric analysis (TGA) and differential scanning calorimetry (DSC) are two commonly used methods in order to determine melting points, phase transitions, and enthalpic factors which can be compared to each individual cocrystal former.\n\nCocrystal engineering is relevant to production of energetic materials, pharmaceuticals, and other compounds. Of these, the most widely studied and used application is in drug development and more specifically, the formation, design, and implementation of active pharmaceutical ingredients, or API’s. Changing the structure and composition of the API can greatly influence the bioavailability of a drug. The engineering of cocrystals takes advantage of the specific properties of each component to make the most favorable conditions for solubility that could ultimately enhance the bioavailability of the drug. The principal idea is to develop superior physico-chemical properties of the API while holding the properties of the drug molecule itself constant. Cocrystal structures have also become a staple for drug discovery. Structure-based virtual screening methods, such as docking, makes use of cocrystal structures of known proteins or receptors to elucidate new ligand-receptor binding conformations.\n\nCocrystal engineering has become of such great importance in the field of pharmaceuticals that a particular subdivision of multicomponent cocrystals has been given the term pharmaceutical cocrystals to refer to a solid cocrystal former component and a molecular or ionic API (active pharmaceutical ingredient). However, other classifications also exist when one or more of the components are not in solid form under ambient conditions. For example, if one component is a liquid under ambient conditions, the cocrystal might actually be deemed a cocrystal solvate as discussed previously. The physical states of the individual components under ambient conditions is the only source of division among these classifications. The classification naming scheme of the cocrystals might seem to be of little importance to the cocrystal itself, but in the categorization lies significant information regarding the physical properties, such as solubility and melting point, and the stability of API’s.\n\nThe objective for pharmaceutical cocrystals is to have properties that differ from that expected of the pure API’s without making and/or breaking covalent bonds.\nAmong the earliest pharmaceutical cocrystals reported are of sulfonamides. The area of pharmaceutical cocrystals has thus increased on the basis of interactions between API’s and cocrystal formers. Most commonly, API’s have hydrogen-bonding capability at their exterior which makes them more susceptible to polymorphism, especially in the case of cocrystal solvates which can be known to have different polymorphic forms. Such a case is in the drug sulfathiazole, a common oral and topical antimicrobial,which has over a hundred different solvates. It is thus important in the field of pharmaceuticals to screen for every polymorphic form of a cocrystal before it is considered as a realistic improvement to the existing API. Pharmaceutical cocrystal formation can also be driven by multiple functional groups on the API, which introduces the possibility of binary, ternary, and higher ordered cocrystal forms. Nevertheless, the cocrystal former is used to optimize the properties of the API but can also be used solely in the isolation and/or purification of the API, such as a separating enantiomers from each other, as well and removed preceding the production of the drug.\n\nIt is with reasoning that the physical properties of pharmaceutical cocrystals could then ultimately change with varying amounts and concentrations of the individual components. One of the most important properties to change with varying the concentrations of the components is solubility. It has been shown that if the stability of the components is less than the cocrystal formed between them, then the solubility of the cocrystal will be lower than the pure combination of the individual constituents. If the solubility of the cocrystal is lower, this means that there exists a driving force for the cocrystallization to occur. Even more important for pharmaceutical applications is the ability to alter the stability to hydration and bioavailability of the API with cocrystal formation, which has huge implications on drug development. The cocrystal can increase or decrease such properties as melting point and stability to relative humidity compared to the pure API and therefore, must be studied on a case to case basis for their utilization in improving a pharmaceutical on the market.\n\nA screening procedure has been developed to help determine the formation of cocrystals from two components and the ability to improve the properties of the pure API. First, the solubilities of the individual compounds are determined. Secondly, the cocrystallization of the two components is evaluated. Finally, phase diagram screening and powder X-ray diffraction (PXRD) are further investigated to optimize conditions for cocrystallization of the components. This procedure is still done to discover cocrystals of pharmaceutical interest including simple APIs, such as carbamazepine (CBZ), a common treatment for epilepsy, trigeminal neuralgia, and bipolar disorder. CBZ has only one primary functional group involved in hydrogen bonding, which simplifies the possibilities of cocrystal formation that can greatly improve its low dissolution bioavailability.\n\nAnother example of an API being studied would be that of Piracetam, or (2-oxo-1-pyrrolidinyl)acetamide, which is used to stimulate the central nervous system and thus, enhance learning and memory. Four polymorphs of Piracetam exist that involve hydrogen bonding of the carbonyl and primary amide. It is these same hydrogen bonding functional groups that interact with and enhance the cocrystallization of Piracetam with gentisic acid, a non-steroidal anti-inflammatory drug (NSAID), and with p-hydroxybenzoic acid, an isomer of the aspirin precursor salicylic acid. No matter what the API is that is being researched, it is quite evident of the wide applicability and possibility for constant improvement in the realm of drug development, thus making it clear that the driving force of cocrystallization continues to consist of attempting to improve on the physical properties in which the existing cocrystals are lacking.\n\nOn August 16, 2016, the US food and drug administration (FDA) published a draft guidance Regulatory Classification of Pharmaceutical Co-Crystals. In this guide, the FDA suggests treating co-crystals as polymorphs, as long as proof is presented to rule out the existence of ionic bonds.\n\nTwo explosives HMX and CL-20 cocrystallized in a ratio 1:2 to form a hybrid explosive. This explosive had the same low sensitivity of HMX and nearly the same explosive power of CL-20. Physically mixing explosives creates a mixture that has the same sensitivity as the most sensitive component, which cocrystallisation overcomes.\n\n"}
{"id": "14001145", "url": "https://en.wikipedia.org/wiki?curid=14001145", "title": "Die shrink", "text": "Die shrink\n\nThe term die shrink (sometimes optical shrink or process shrink) refers to a simple semiconductor scaling of semiconductor devices, mainly transistors. The act of shrinking a die is to create a somewhat identical circuit using a more advanced fabrication process, usually involving an advance of lithographic node. This reduces overall costs for a chip company, as the absence of major architectural changes to the processor lowers research and development costs, while at the same time allowing more processor dies to be manufactured on the same piece of silicon wafer, resulting in less cost per product sold.\n\nDie shrinks are the key to improving price/performance at semiconductor companies such as Intel, AMD (including the former ATI), NVIDIA, and Samsung. Examples in the 2000s include the codenamed Cedar Mill Pentium 4 processors (from 90 nm CMOS to 65 nm CMOS) and Penryn Core 2 processors (from 65 nm CMOS to 45 nm CMOS), the codenamed Brisbane Athlon 64 X2 processors (from 90 nm SOI to 65 nm SOI), and various generations of GPUs from both ATI and NVIDIA. In January 2010, Intel released Clarkdale Core i5 and Core i7 processors fabricated with a 32 nm process, down from a previous 45 nm process used in older iterations of the Nehalem processor microarchitecture. Intel, in particular, formerly focused on leveraging die shrinks to improve product performance at a regular cadence through its Tick-Tock model. In this business model, every new microarchitecture (tick) is followed by a die shrink (tock) to improve performance with the same microarchitecture.\n\nDie shrinks are beneficial to end-users as shrinking a die reduces the current used by each transistor switching on or off in semiconductor devices while maintaining the same clock frequency of a chip, making a product with less power consumption (and thus less heat production), increased clock rate headroom, and lower prices. Since the cost to fabricate a 200-mm or 300-mm silicon wafer is proportional to the number of fabrication steps, and not proportional to the number of chips on the wafer, die shrinks cram more chips onto each wafer, resulting in lowered manufacturing costs per chip.\n\nIn CPU fabrications, a die shrink always involves an advance to a lithographic node as defined by ITRS (see list at right). For GPU and SoC manufacturing, the die shrink often involves shrinking the die on a node not defined by the ITRS, for instance the 150 nm, 110 nm, 80 nm, 55 nm, 40 nm and more currently 14 nm nodes, sometimes referred to as \"half-nodes\". This is a stopgap between two ITRS-defined lithographic nodes (thus called a \"half-node shrink\") before further shrink to the lower ITRS-defined nodes occurs, which helps save further R&D cost. The choice to perform die shrinks to either full-nodes or half-nodes rests with the foundry and not the integrated circuit designer.\n\n\n"}
{"id": "6038229", "url": "https://en.wikipedia.org/wiki?curid=6038229", "title": "Docosatetraenoic acid", "text": "Docosatetraenoic acid\n\nDocosatetraenoic acid designates any straight chain 22:4 fatty acid. (\"See\" essential fatty acid for nomenclature.)\n\nOne isomer is of particular interest:\n\n\n"}
{"id": "55852028", "url": "https://en.wikipedia.org/wiki?curid=55852028", "title": "Dresselhaus effect", "text": "Dresselhaus effect\n\nThe Dresselhaus effect is a phenomenon in solid-state physics in which spin–orbit interaction causes energy bands to split. It is usually present in crystal systems lacking inversion symmetry. The effect is named after Gene Dresselhaus, husband of Mildred Dresselhaus, who discovered this splitting in 1955.\n\nSpin-orbit interaction is a relativistic coupling between the electric field produced by an ion-core and the resulting dipole moment arising from the relative motion of the electron, and its intrinsic magnetic dipole proportional to the electron spin. In an atom, the coupling weakly splits an orbital energy state into two states: one state with the spin aligned to the orbital field and one anti-aligned. In a solid crystalline material, the motion of the conduction electrons in the lattice can be altered by a complementary effect due to the coupling between the potential of the lattice and the electron spin. If the crystalline material is not centro-symmetric, the asymmetry in the potential can favour one spin orientation over the opposite and split the energy bands into spin aligned and anti-aligned subbands.\n\nThe Rashba spin–orbit coupling has a similar energy band splitting, but the asymmetry comes either from the bulk asymmetry of uniaxial crystals (e.g. of wurtzite type) or the spatial inhomogeneity of an interface or surface. Dresselhaus and Rashba effects are often of similar strength in the band splitting of GaAs nanostructures.\n\nMaterials with zincblende structure are non-centrosymmetric (i.e., they lack inversion symmetry). This bulk induced asymmetry (BIA) forces the perturbative Hamiltonian to contain only odd powers of the linear momentum. The bulk Dresselhaus Hamiltonian or BIA term is usually written in this form:\n\nwhere formula_2, formula_3 and formula_4 are the Pauli matrices related to the spin formula_5 of the electrons as formula_6 (here formula_7 is the reduced Planck constant), and formula_8, formula_9 and formula_10 are the components of the momentum in the crystallographic directions [100], [010] and [001], respectively. \n\nWhen treating 2D nanostructures where the width direction formula_11 or [001] is finite, the Dresselhaus Hamiltonian can be separated into a linear and a cubic term. The linear Dresselhaus Hamiltonian formula_12 is usually written as\nwhere formula_14 is a coupling constant.\n\nThe cubic Dresselhaus term formula_15 is written as\nwhere formula_17 is the width of the material.\n\nThe Hamiltonian is generally derived using a combination of the k·p perturbation theory alongside the Kane model.\n\n"}
{"id": "12252726", "url": "https://en.wikipedia.org/wiki?curid=12252726", "title": "Environmental Centre ARCTUROS", "text": "Environmental Centre ARCTUROS\n\nEnvironmental Centre ARCTUROS () commonly known as Arcturos, is a Greek ecological organization which focuses its efforts on saving the brown bear and its habitats.\n\nBrown bears once ranged all across Europe, but human encroachment on their forest habitats have made them an endangered species. Through the efforts of Arcturos, the size of the Greek brown bear population appears to have doubled in recent years. The organization also undertakes the rescue of bears kept captive in inhumane conditions—such as the \"dancing bears\", which are taken as cubs to be trained following the killing of their mother, as well as orphan bears and those improperly kept in zoos. It comprises a veterinary centre located in the village of Aetos, where animals are nursed back to health and a mountain sanctuary in the nearby Verno mountains close to the village of Nymfaio, an enclosed section of forest where the bears are transferred to be cared for and studied until they are able to be released back into the wild. The sanctuary also serves as an educational and study field for scientists and the public.\n\n"}
{"id": "23037879", "url": "https://en.wikipedia.org/wiki?curid=23037879", "title": "Fibrin scaffold", "text": "Fibrin scaffold\n\nA fibrin scaffold is a network of protein that holds together and supports a variety of living tissues. It is produced naturally by the body after injury, but also can be engineered as a tissue substitute to speed healing. The scaffold consists of naturally occurring biomaterials composed of a cross-linked fibrin network and has a broad use in biomedical applications.\n\nFibrin consists of the blood proteins fibrinogen and thrombin which participate in blood clotting. Fibrin glue or fibrin sealant is also referred to as a fibrin based scaffold and used to control surgical bleeding, speed wound healing, seal off hollow body organs or cover holes made by standard sutures, and provide slow-release delivery of medications like antibiotics to tissues exposed.\n\nFibrin scaffold use is helpful in repairing injuries to the urinary tract, liver lung, spleen, kidney, and heart. In biomedical research, fibrin scaffolds have been used to fill bone cavities, repair neurons, heart valves, vascular grafts and the surface of the eye.\n\nThe complexity of biological systems requires customized care to sustain their function. When they are no longer able to perform their purpose, interference of new cells and biological cues is provided by a scaffold material. Fibrin scaffold has many aspects like being biocompatible, biodegradable and easily processable. Furthermore, it has an autologous nature and it can be manipulated in various size and shape. Inherent role in wound healing is helpful in surgical applications. Many factors can be bound to fibrin scaffold and those can be released in a cell-controlled manner. Its stiffness can be managed by changing the concentration according to needs of surrounding or encapsulated cells. Additional mechanical properties can be obtained by combining fibrin with other suitable scaffolds. Each biomedical application has its own characteristic requirement for different kinds of tissues and recent studies with fibrin scaffold are promising towards faster recovery, less complications and long-lasting solutions.\n\nFibrin scaffold is an important element in tissue engineering approaches as a scaffold material. It is advantageous opposed to synthetic polymers and collagen gels when cost, inflammation, immune response, toxicity and cell adhesion are concerned. When there is a trauma in body, cells at site start the cascade of blood clotting and fibrin is the first scaffold formed normally. To achieve in clinical use of a scaffold, fast and entire incorporation into host tissue is very essential. Regeneration of the tissue and the degradation of the scaffold should be balanced in terms of rate, surface area and interaction so that ideal templating can be achieved. Fibrin satisfies many requirements of scaffold functions. Biomaterials made up of fibrin can attach many biological surfaces with high adhesion. Its biocompatibility comes from being not toxic, allergenic or inflammatory. By the help of fibrinolysis inhibitors or fiber cross-linkers, biodegradation can be managed. Fibrin can be provided from individuals to be treated many times so that gels from autologous fibrin have no undesired immunogenic reactions in addition to be reproducible. Inherently, structure and biochemistry of fibrin has an important role in wound healing. Although there are limitations due to diffusion, exceptional cellular growth and tissue development can be achieved. According to the application, fibrin scaffold characteristics can be adjustable by manipulating concentrations of components. Long-lasting durable fibrin hydrogels are enviable in many applications.\n\nPolymerization time of fibrinogen and thrombin is affected primarily by concentration of thrombin and temperature, while fibrinogen concentration has a minor effect. Fibrin gel characterization by scanning electron microscopy reveals that thick fibers make up a dense structure at lower fibrinogen concentrations (5 mg/ml) and thinner fibers and looser gel can be obtained as fibrinogen concentration (20 mg/ml) increases whereas increase in thrombin concentration (from 0.5 U/ml to 5 U/ml) has no such significant result although the fibers steadily get thinner.\n\nFibrin gels can be enriched by addition of other extracellular matrix (ECM) components such as fibronectin, vitronectin, laminin and collagen. These can be linked covalently to fibrin scaffold by reactions catalyzed by transglutaminase. Laminin originated substrate amino acid sequences for transglutaminase can be IKVAV, YIGSR or RNIAEIIKDI. Collagen originated sequence is DGEA and many other ECM protein originated RGD sequence can be given as other examples. Heparin binding sequences KβAFAKLAARLYRKA, RβAFARLAARLYRRA, KHKGRDVILKKDVR, YKKIIKKL are from antithrombin III, modified antithrombin III, neural cell adhesion molecule and platelet factor 4, respectively. Heparin-binding growth factors can be attached to heparin binding domains via heparin. As a result, a reservoir can be provided instead of passive diffusion by liberation of growth factors in extended time. Acidic and basic fibroblast growth factor, neurotrophin 3, transforming growth factor beta 1, transforming growth factor beta 2, nerve growth factor, brain derived neurotrophic factor can be given as examples for such growth factors.\n\nFor some tissues like cartilage, highly dense polymeric scaffolds such as polyethylene glycol (PEG) are essential due to mechanical stress and that can be achieved by combining them with natural biodegradable cell-adhesive scaffolds since cells can not attach to synthetic polymers and take proper signals for normal cell function. Various scaffold combinations with PEG-based hydrogels are studied to assess the chondrogenic response to dynamic strain stimulation in a recent study. PEG-Proteoglycan, PEG-Fibrinogen, PEG-Albumin conjugates and only PEG including hydrogels are used to evaluate the mechanical effect on bovine chondrocytes by using a pneumatic reactor system. The most substantial increase in stiffness is observed in PEG-Fibrinogen conjugated hydrogel after 28 days of mechanical stimulation.\n\nIn orthopedics, methods with minimum invasion are desired and improving injectable systems is a leading aim. Bone cavities can be filled by polymerizing materials when injected and adaptation to the shape of the cavity can be provided. Shorter surgical operation time, minimum large muscle retaraction harm, smaller scar size, less pain after operation and consequently faster recovery can be obtained by using such systems. In a study to evaluate if injectable fibrin scaffold is helpful for transplantation of bone marrow stromal cell (BMSC) when central nervous system (CNS) tissue is damaged, Yasuda et al. found that BMSC has extended survival, migration and differentiation after transplantation to rat cortical lesion although there is complete degradation of fibrin matrix after four weeks. Another study to assess if fibrin glue enriched with platelet is better than just platelet rich plasma (PRP) on bone formation was conducted. Each combined with bone marrow mesenchymal stem cells and bone morphogenetic protein 2 (BMP-2) are injected into the subcutaneous space. Results shows that fibrin glue enriched with platelet has better osteogenic properties when compared to PRP. To initiate and speed up tissue repair and regeneration, platelet-rich fibrin gels are ideal since they have a high concentration of platelet releasing growth factors and bioactive proteins. Addition of fibrin glue to calcium phosphate granules has promising results leading to faster bone repair by inducing mineralization and possible effects of fibrin on angiogenesis, cell attachment and proliferation.\n\nValvular heart disease is a major cause of death globally. Both mechanical valves and fixed biological xenograft or homografts used clinically have many drawbacks. One study focused on fibrin-based heart valves to assess structure and mechanical durability on sheep revealed promising potential for patient originated valve replacements. From autologous arterial-derived cells and fibrin scaffold, tissue engineered heart valves are formed, then mechanically conditioned and transplanted into the pulmonary trunk of the same animals. The preliminary result are potentially hopeful towards autologous heart valve production.\n\nIn atherosclerosis, a severe disease in modern society, coronary blood vessels occlude. These vessels have to be freed and held open i.e. by stents. Unfortunately after certain time these vessels close again and have to be bypassed to allow for upkeep of circulation. Usually autologous vessels from the patient or synthetic polymer grafts are used for this purpose. Both options have disadvantages. Firstly there are only few autologous vessels available in a human body that might be of low quality, considering the health status of the patient. The synthetic polymer based grafts on the other hand often have insufficient haemocompatibility and thus rapidly occlude - a problem that is especially prone in small calibre grafts. In this context the fibrin-gel-based tissue engineering of autologous vessel substitutes is a very promising approach to overcome the current problems. Cells and fibrin are isolated by a low invasive procedure from the patient and shaped in individual moulds to meet the required dimensions. Additional pre-cultivation in a specialized bioreactor is inevitable to ensure appropriate properties of the graft.\n\nBullous keratopathy that is characterized by corneal stromal edema related to cell loss and endothelial decompensation as well as subepithelial fibrosis and corneal vascularization in further cases, results vision problems due to loss of corneal transparency. Fibrin glue is used as a sutureless method onto the corneal surface to fix amniotic membrane that is cryopreserved. Complete re-epithelialization on the ocular surface with no symptom is achieved in 3 weeks. Results show that fibrin glue fixation is easy, reliable and efficient with the corneal surface.\n\nBecause fibrin fulfills the mechanical aspects of neuronal growth without initiation of glial proliferation, it can be potentially used in neuronal wound healing even with no need of growth factors or such constituents. Neurons and astrocytes, two major cell type of central nervous system, can show various responses to differences in matrix stiffness. Neuronal development of precursor cells is maintained by gels with low elastic modulus. When stiffness of the matrix is more than that of a normal brain, extension of spinal cord and cortical brain neurons is inhibited since neurite extension and branch forming take place on soft materials (<1000Pa). In a study, fibrins from different species are used to compare the effects on neurite growth of mouse spinal cord neurons. Among salmon, bovine and human fibrin in addition to Matrigel, salmon fibrin promotes the neurite growth best and it is more proteolysis resistant than mammalian fibrins. Because down to 0 °C, salmon fibrinogen can clot whereas polymerization of human fibrinogen occurs slowly below 37 °C, this can be taken as an advantage in surgical settings that are cooler. Therefore, for treatment of central nervous system damages, salmon fibrin can be a useful biomaterial.\n\nFor sciatic nerve regeneration, fibrin scaffold is used with glial derived neurotrophic factor (GDNF) in a recent study. Survival of both sensory and motor neurons is promoted by glial-derived neurotrophic factor and its delivery to peripheral nervous system improves regeneration after an injury. GDNF and nerve growth factor (NGF) is sequestered in the gel via a bi-domain peptide. This peptide is composed of heparin binding domain and transglutaminase substrate domain which can be cross-linked into the fibrin matrix by polymerization via transglutaminase activity of factor XIIIa. Many neurotrophic factors can bind to heparin through its sulfated domains. This is the affinity-based delivery system in which growth factors are released by cell-based degradation control. After a 13 mm rat sciatic nerve defect is made, the fibrin matrix delivery system is applied to the gap as a nerve guiding channel. Results show that such a delivery system is efficient to enhance maturity and promote organized architecture of nerve regenerating in presence of GDNF, in addition to expressing the promising treatment variations for peripheral nerve injuries.\n\nThe use of fibrin hydrogel in gene delivery (transfection) is studied to address essential factors controlling the delivery process such as fibrinogen and pDNA concentration in addition to significance of cell-mediated fibrin degradation for pursuing the potential of cell-transfection microarray engineering or in vivo gene transfer. Gene transfer is more successful in-gel than on-gel probably because of proximity of lipoplexes and target cells. Less cytotoxicity is observed due to less use of transfection agents like lipofectamine and steady degradation of fibrin. Consequently, each cell type requires optimization of fibrinogen and pDNA concentrations for higher transfection yields and studies towards high-throughput transfection microarray experiments are promising.\n"}
{"id": "11034", "url": "https://en.wikipedia.org/wiki?curid=11034", "title": "Fluid dynamics", "text": "Fluid dynamics\n\nIn physics and engineering, fluid dynamics is a subdiscipline of fluid mechanics that describes the flow of fluids—liquids and gases. It has several subdisciplines, including aerodynamics (the study of air and other gases in motion) and hydrodynamics (the study of liquids in motion). Fluid dynamics has a wide range of applications, including calculating forces and moments on aircraft, determining the mass flow rate of petroleum through pipelines, predicting weather patterns, understanding nebulae in interstellar space and modelling fission weapon detonation,\n\nFluid dynamics offers a systematic structure—which underlies these practical disciplines—that embraces empirical and semi-empirical laws derived from flow measurement and used to solve practical problems. The solution to a fluid dynamics problem typically involves the calculation of various properties of the fluid, such as flow velocity, pressure, density, and temperature, as functions of space and time.\n\nBefore the twentieth century, \"hydrodynamics\" was synonymous with fluid dynamics. This is still reflected in names of some fluid dynamics topics, like magnetohydrodynamics and hydrodynamic stability, both of which can also be applied to gases.\n\nThe foundational axioms of fluid dynamics are the conservation laws, specifically, conservation of mass, conservation of linear momentum (also known as Newton's Second Law of Motion), and conservation of energy (also known as First Law of Thermodynamics). These are based on classical mechanics and are modified in quantum mechanics and general relativity. They are expressed using the Reynolds transport theorem.\n\nIn addition to the above, fluids are assumed to obey the . Fluids are composed of molecules that collide with one another and solid objects. However, the continuum assumption assumes that fluids are continuous, rather than discrete. Consequently, it is assumed that properties such as density, pressure, temperature, and flow velocity are well-defined at infinitesimally small points in space and vary continuously from one point to another. The fact that the fluid is made up of discrete molecules is ignored.\n\nFor fluids that are sufficiently dense to be a continuum, do not contain ionized species, and have flow velocities small in relation to the speed of light, the momentum equations for Newtonian fluids are the Navier–Stokes equations—which is a non-linear set of differential equations that describes the flow of a fluid whose stress depends linearly on flow velocity gradients and pressure. The unsimplified equations do not have a general closed-form solution, so they are primarily of use in Computational Fluid Dynamics. The equations can be simplified in a number of ways, all of which make them easier to solve. Some of the simplifications allow some simple fluid dynamics problems to be solved in closed form.\n\nIn addition to the mass, momentum, and energy conservation equations, a thermodynamic equation of state that gives the pressure as a function of other thermodynamic variables is required to completely describe the problem. An example of this would be the perfect gas equation of state:\n\nwhere \"p\" is pressure, ρ is density, \"T\" the absolute temperature, while \"R\" is the gas constant and \"M\" is molar mass for a particular gas.\n\nThree conservation laws are used to solve fluid dynamics problems, and may be written in integral or differential form. The conservation laws may be applied to a region of the flow called a \"control volume\". A control volume is a discrete volume in space through which fluid is assumed to flow. The integral formulations of the conservation laws are used to describe the change of mass, momentum, or energy within the control volume. Differential formulations of the conservation laws apply Stokes' theorem to yield an expression which may be interpreted as the integral form of the law applied to an infinitesimally small volume (at a point) within the flow.\n\n\n\n\nAll fluids are compressible to some extent; that is, changes in pressure or temperature cause changes in density. However, in many situations the changes in pressure and temperature are sufficiently small that the changes in density are negligible. In this case the flow can be modelled as an incompressible flow. Otherwise the more general compressible flow equations must be used.\n\nMathematically, incompressibility is expressed by saying that the density ρ of a fluid parcel does not change as it moves in the flow field, i.e.,\nwhere \"D\"/\"Dt\" is the material derivative, which is the sum of local and convective derivatives. This additional constraint simplifies the governing equations, especially in the case when the fluid has a uniform density.\n\nFor flow of gases, to determine whether to use compressible or incompressible fluid dynamics, the Mach number of the flow is evaluated. As a rough guide, compressible effects can be ignored at Mach numbers below approximately 0.3. For liquids, whether the incompressible assumption is valid depends on the fluid properties (specifically the critical pressure and temperature of the fluid) and the flow conditions (how close to the critical pressure the actual flow pressure becomes). Acoustic problems always require allowing compressibility, since sound waves are compression waves involving changes in pressure and density of the medium through which they propagate.\n\nAll fluids are viscous, meaning that they exert some resistance to deformation: neighbouring parcels of fluid moving at different velocities exert viscous forces on each other. The velocity gradient is referred to as a strain rate; it has dimensions formula_14. Isaac Newton showed that for many familiar fluids such as water and air, the stress due to these viscous forces is linearly related to the strain rate. Such fluids are called Newtonian fluids. The coefficient of proportionality is called the fluid's viscosity; for Newtonian fluids, it is a fluid property that is independent of the strain rate.\n\nNon-Newtonian fluids have a more complicated, non-linear stress-strain behaviour. The sub-discipline of rheology describes the stress-strain behaviours of such fluids, which include emulsions and slurries, some viscoelastic materials such as blood and some polymers, and \"sticky liquids\" such as latex, honey and lubricants.\n\nThe dynamic of fluid parcels is described with the help of Newton's second law. An accelerating parcel of fluid is subject to inertial effects.\n\nThe Reynolds number is a dimensionless quantity which characterises the magnitude of inertial effects compared to the magnitude of viscous effects. A low Reynolds number (\"Re\"«1) indicates that viscous forces are very strong compared to inertial forces. In such cases, inertial forces are sometimes neglected; this flow regime is called Stokes or creeping flow.\n\nIn contrast, high Reynolds numbers (\"Re\"»1) indicate that the inertial effects have more effect on the velocity field than the viscous (friction) effects. In high Reynolds number flows, the flow is often modeled as an inviscid flow, an approximation in which viscosity is completely neglected. Eliminating viscosity allows the Navier–Stokes equations to be simplified into the Euler equations. The integration of the Euler equations along a streamline in an inviscid flow yields Bernoulli's equation. When, in addition to being inviscid, the flow is irrotational everywhere, Bernoulli's equation can completely describe the flow everywhere. Such flows are called potential flows, because the velocity field may be expressed as the gradient of a potential energy expression.\n\nThis idea can work fairly well when the Reynolds number is high. However, problems such as those involving solid boundaries may require that the viscosity be included. Viscosity cannot be neglected near solid boundaries because the no-slip condition generates a thin region of large strain rate, the boundary layer, in which viscosity effects dominate and which thus generates vorticity. Therefore, to calculate net forces on bodies (such as wings), viscous flow equations must be used: inviscid flow theory fails to predict drag forces, a limitation known as the d'Alembert's paradox.\n\nA commonly used model, especially in computational fluid dynamics, is to use two flow models: the Euler equations away from the body, and boundary layer equations in a region close to the body. The two solutions can then be matched with each other, using the method of matched asymptotic expansions.\n\nA flow that is not a function of time is called steady flow. Steady-state flow refers to the condition where the fluid properties at a point in the system do not change over time. Time dependent flow is known as unsteady (also called transient). Whether a particular flow is steady or unsteady, can depend on the chosen frame of reference. For instance, laminar flow over a sphere is steady in the frame of reference that is stationary with respect to the sphere. In a frame of reference that is stationary with respect to a background flow, the flow is unsteady.\n\nTurbulent flows are unsteady by definition. A turbulent flow can, however, be statistically stationary. According to Pope:\n\nThis roughly means that all statistical properties are constant in time. Often, the mean field is the object of interest, and this is constant too in a statistically stationary flow.\n\nSteady flows are often more tractable than otherwise similar unsteady flows. The governing equations of a steady problem have one dimension fewer (time) than the governing equations of the same problem without taking advantage of the steadiness of the flow field.\n\nTurbulence is flow characterized by recirculation, eddies, and apparent randomness. Flow in which turbulence is not exhibited is called laminar. The presence of eddies or recirculation alone does not necessarily indicate turbulent flow—these phenomena may be present in laminar flow as well. Mathematically, turbulent flow is often represented via a Reynolds decomposition, in which the flow is broken down into the sum of an average component and a perturbation component.\n\nIt is believed that turbulent flows can be described well through the use of the Navier–Stokes equations. Direct numerical simulation (DNS), based on the Navier–Stokes equations, makes it possible to simulate turbulent flows at moderate Reynolds numbers. Restrictions depend on the power of the computer used and the efficiency of the solution algorithm. The results of DNS have been found to agree well with experimental data for some flows.\n\nMost flows of interest have Reynolds numbers much too high for DNS to be a viable option, given the state of computational power for the next few decades. Any flight vehicle large enough to carry a human (L > 3 m), moving faster than 20 m/s (72 km/h) is well beyond the limit of DNS simulation (Re = 4 million). Transport aircraft wings (such as on an Airbus A300 or Boeing 747) have Reynolds numbers of 40 million (based on the wing chord dimension). Solving these real-life flow problems requires turbulence models for the foreseeable future. Reynolds-averaged Navier–Stokes equations (RANS) combined with turbulence modelling provides a model of the effects of the turbulent flow. Such a modelling mainly provides the additional momentum transfer by the Reynolds stresses, although the turbulence also enhances the heat and mass transfer. Another promising methodology is large eddy simulation (LES), especially in the guise of detached eddy simulation (DES)—which is a combination of RANS turbulence modelling and large eddy simulation.\n\nWhile many flows (e.g. flow of water through a pipe) occur at low Mach numbers, many flows of practical interest in aerodynamics or in turbomachines occur at high fractions of M=1 (transonic flows) or in excess of it (supersonic or even hypersonic flows). New phenomena occur at these regimes such as instabilities in transonic flow, shock waves for supersonic flow, or non-equilibrium chemical behaviour due to ionization in hypersonic flows. In practice, each of those flow regimes is treated separately.\n\nReactive flows are flows that are chemically reactive, which finds its applications in many areas such as combustion(IC engine), propulsion devices (Rockets, jet engines etc.), detonations, fire and safety hazards, astrophysics etc. In addition to conservation of mass, momentum and energy, conservation of individual species (for example, mass fraction of methane in methane combustion) need to be derived, where the production/depletion rate of any species are obtained by simultaneously solving the equations of chemical kinetics.\n\nMagnetohydrodynamics is the multi-disciplinary study of the flow of electrically conducting fluids in electromagnetic fields. Examples of such fluids include plasmas, liquid metals, and salt water. The fluid flow equations are solved simultaneously with Maxwell's equations of electromagnetism.\n\nRelativistic fluid dynamics studies the macroscopic and microscopic fluid motion at large velocities comparable to the velocity of light. This branch of fluid dynamics accounts the relativistic effects both from the special theory of relativity and the general theory of relativity. The governing equations are derived in Riemannian geometry for Minkowski spacetime.\n\nThere are a large number of other possible approximations to fluid dynamic problems. Some of the more commonly used are listed below.\n\nThe concept of pressure is central to the study of both fluid statics and fluid dynamics. A pressure can be identified for every point in a body of fluid, regardless of whether the fluid is in motion or not. Pressure can be measured using an aneroid, Bourdon tube, mercury column, or various other methods.\n\nSome of the terminology that is necessary in the study of fluid dynamics is not found in other similar areas of study. In particular, some of the terminology used in fluid dynamics is not used in fluid statics.\n\nThe concepts of total pressure and dynamic pressure arise from Bernoulli's equation and are significant in the study of all fluid flows. (These two pressures are not pressures in the usual sense—they cannot be measured using an aneroid, Bourdon tube or mercury column.) To avoid potential ambiguity when referring to pressure in fluid dynamics, many authors use the term static pressure to distinguish it from total pressure and dynamic pressure. Static pressure is identical to pressure and can be identified for every point in a fluid flow field.\n\nA point in a fluid flow where the flow has come to rest (i.e. speed is equal to zero adjacent to some solid body immersed in the fluid flow) is of special significance. It is of such importance that it is given a special name—a stagnation point. The static pressure at the stagnation point is of special significance and is given its own name—stagnation pressure. In incompressible flows, the stagnation pressure at a stagnation point is equal to the total pressure throughout the flow field.\n\nIn a compressible fluid, it is convenient to define the total conditions (also called stagnation conditions) for all thermodynamic state properties (e.g. total temperature, total enthalpy, total speed of sound). These total flow conditions are a function of the fluid velocity and have different values in frames of reference with different motion.\n\nTo avoid potential ambiguity when referring to the properties of the fluid associated with the state of the fluid rather than its motion, the prefix \"static\" is commonly used (e.g. static temperature, static enthalpy). Where there is no prefix, the fluid property is the static condition (i.e. \"density\" and \"static density\" mean the same thing). The static conditions are independent of the frame of reference.\n\nBecause the total flow conditions are defined by isentropically bringing the fluid to rest, there is no need to distinguish between total entropy and static entropy as they are always equal by definition. As such, entropy is most commonly referred to as simply \"entropy\".\n\n\n"}
{"id": "35599555", "url": "https://en.wikipedia.org/wiki?curid=35599555", "title": "Ford Seattle-ite XXI", "text": "Ford Seattle-ite XXI\n\nThe Ford Seattle-ite XXI was a 3/8 scale concept car designed by Alex Tremulis and displayed on 20 April 1962 on the Ford stand at the Seattle World's Fair. \n\nThe car contained novel ideas that have since become reality: interchangeable fuel cell power units; interchangeable bodies; interactive computer navigation, mapping, and auto information systems; and four driving and steering wheels. \n\nThe concept of some form of compact nuclear propulsion device was included as a possible power source on the assumption that radiation issues could be overcome without the need for massive shielding.\n\nThe car had six wheels, with four steerable ones at the front and two fixed ones at the rear – similar to the fictional six-wheel 1965 FAB1 and the real Tyrrell P34 racing car of the mid-1970s. The designers considered the six-wheel concept would enhance tracking, traction, and braking. It had an interchangeable front-powered section that enabled the car to be turned into either an economical city runabout or, when needed, a powerful transcontinental cruiser. All control mechanisms were through flexible couplings. Steering was by way of a fingertip-controlled dial.\n\n\n\n"}
{"id": "32007087", "url": "https://en.wikipedia.org/wiki?curid=32007087", "title": "Friends of Baxter Creek", "text": "Friends of Baxter Creek\n\nFriends of Baxter Creek is a community organization in El Cerrito, California.\n\nThe group was created by those passionate about restoring Baxter Creek in El Cerrito and Richmond, California. The group has led to the restoration of various stretches of the creek. The FoGC has also made it that creek restoration is a part of any development effort along the path of this small river. The friends created along with the city of El Cerrito \"Baxter Creek Gateway Park\" along the Ohlone Greenway and the Richmond Greenway.\n"}
{"id": "3122365", "url": "https://en.wikipedia.org/wiki?curid=3122365", "title": "Gas turbine modular helium reactor", "text": "Gas turbine modular helium reactor\n\nThe Gas Turbine Modular Helium Reactor (GT-MHR) is a nuclear fission power reactor design that was under development by a group of Russian enterprises (OKBM Afrikantov, Kurchatov Institute, VNIINM and others), an American group headed by General Atomics, French Framatome and Japanese Fuji Electric. It is a helium cooled, graphite moderated reactor and uses TRISO fuel compacts in a prismatic core design.\n\nThe core consists of a graphite cylinder with a radius of 4 m and a height of 10 m which includes 1 m axial reflectors at top and bottom. The cylinder allocates three or four concentric rings, each of 36 hexagonal blocks with an interstitial gap of 0.2 cm. Each hexagonal block contains 108 helium coolant channels and 216 fuel pins. Each fuel pin contains a random lattice of TRISO particles dispersed into a graphite matrix. The reactor exhibits a thermal spectrum with a peak located at about 0.2 eV. The TRISO fuel concept allows the reactor to be inherently safe. The reactor and containment structure are located below grade and in contact with the ground, which serves as a passive safety measure to conduct heat away from the reactor in the event of a coolant failure.\n\nThe Gas Turbine Modular Helium Reactor utilizes the Brayton cycle turbine arrangement, which gives it an efficiency of up to 48% – higher than any other reactor, as of 1995. Commercial light water reactors (LWRs) generally use the Rankine cycle, which is what coal-fired power plants use. Commercial LWRs average 32% efficiency, again as of 1995.\n\nIn 2010 General Atomics conceptualized a new reactor that utilizes the power conversion features of the GT-MHR, the Energy Multiplier Module (EM2). The EM2 uses fast neutrons and is a gas-cooled fast reactor, enabling it to reduce nuclear waste considerably by transmutation.\n\n"}
{"id": "31915408", "url": "https://en.wikipedia.org/wiki?curid=31915408", "title": "Gobius tropicus", "text": "Gobius tropicus\n\nGobius tropicus is a species of fish currently classified in the family Gobiidae. It is native the Atlantic waters around Ascension Island. The actual taxonomic position of this species is uncertain and it is suspected that it is not even a goby.\n"}
{"id": "9639128", "url": "https://en.wikipedia.org/wiki?curid=9639128", "title": "Golden gimmick", "text": "Golden gimmick\n\nThe Golden Gimmick refers to a foreign tax credit deal enacted in November 1950 by the US Government under president Harry Truman between King Ibn Saud of Saudi Arabia and the Arabian-American Oil Company (ARAMCO), a consortium comprising Standard Oil of California (Chevron), Standard Oil of New Jersey (Exxon), Standard Oil of New York (Mobil) and Texaco. King Ibn Saud was being influenced by Juan Pablo Pérez Alfonso of Venezuela who cut a similar 50/50 deal with New Jersey Standard Oil and Royal Dutch Shell. This 50/50 deal accorded the American oil companies a tax break equivalent to 50% of their profits on oil sales, with the other 50% to be diverted to King Ibn Saud via the US Treasury. The King agreed to this 50/50 splitting of Aramco's oil profits instead of nationalizing Aramco's oil facilities on Saudi soil. Venezuela eventually led the effort in forming OPEC and Saudi Arabia gained full control of Aramco by 1980.\n\n"}
{"id": "1711059", "url": "https://en.wikipedia.org/wiki?curid=1711059", "title": "HVDC Troll", "text": "HVDC Troll\n\nThe HVDC Troll is a bipolar high-voltage direct current (HVDC) electric power transmission line for the supply of the gas compressor station on the offshore construction work Troll A platform. It consists of dual set of a long bipolar submarine cable designed for ±60 kV between the inverter at the Troll A platform and the static rectifier station at Kollsnes in Norway. The HVDC Troll has a maximum transmission rate of \n\nIn 2013 work started on a new rectifier station at Kollsnes for supplying the two new compressors being installed on Troll A.\n\n"}
{"id": "2667358", "url": "https://en.wikipedia.org/wiki?curid=2667358", "title": "Horse artillery", "text": "Horse artillery\n\nHorse artillery was a type of light, fast-moving, and fast-firing artillery which provided highly mobile fire support, especially to cavalry units. Horse artillery units existed in armies in Europe, the Americas, and some Asian countries, from the 17th to the early 20th century. A precursor of modern self-propelled artillery, it consisted of light cannons or howitzers attached to light but sturdy two-wheeled carriages called caissons or limbers, with the individual crewmen riding on horses. This was in contrast to the rest of the field artillery, in which the pieces were heavier and the crew marched on foot, or in some cases rode on the guns or caissons.\n\nOnce in position, horse artillery crews were trained to quickly dismount, deploy or unlimber their guns (detach them from their caissons), then rapidly fire grapeshot, shells or round shot at the enemy. They could then just as rapidly limber-up (reattach the guns to the caissons), remount, and be ready to move to a new position, similar to the shoot-and-scoot tactics of their modern counterparts.\n\nHorse artillery was highly versatile and often supported friendly cavalry units by disrupting enemy infantry formations such as infantry squares with rapid concentrated fire. This would leave the enemy infantry vulnerable to cavalry charges. Their mobility also enabled them to outmaneuver enemy foot artillery units, and to act as a rearguard (in concert with friendly cavalry) to cover the retreat of slower units. A full battery could have a combined front of riders over 50 men strong. If the horse artillery was mistaken for cavalry, the enemy might receive an unpleasant surprise when the towed batteries wheeled around, unlimbered, loaded, sighted and opened fire. Highly proficient batteries could do so in less than a minute.\n\nEssentially a hybrid of cavalry and artillery, irregular horse artillery units were first used by Sweden in the 17th century during the Thirty Years' War by Lennart Torstenson. Torstenson was the artillery expert of Gustavus Adolphus, and used them to provide cavalry with the fire support it needed to deal with massed infantry formations without sacrificing their speed and mobility. Gustavus Adolphus had previously tried intermixing infantry units with cavalry, and this was somewhat successful since the cavalry at that time did not charge the enemy at full gallop.\n\nOthers tried to combine firepower with mobility by using novel cavalry tactics such as the caracole, but these slowed the cavalry down and proved largely ineffective. The best solutions involved creating hybrid units of mounted infantry, most notably dragoons. Although they proved highly useful and versatile troops, whether they fired mounted or dismounted, they still had to slow down or stop at least temporarily, thereby losing their main advantages as cavalry.\n\nIn the early 18th century the Russian army began equipping cavalry formations with small units of light horse artillery equipped with 2-pound cannons, and portable 3-pound mortars which were transported on horseback (the weights refer to the size of the projectiles, not the artillery pieces.) Though not decisive by themselves, these units inflicted losses on Prussian troops and influenced Frederick the Great to form the first regular horse artillery unit in 1759.\n\nFrederick understood that the greatest threat to massed infantry was concentrated artillery fire. He realized that even small and relatively light guns could severely disrupt or destroy infantry units if they could be brought in close enough and fire often enough. But since even light foot artillery travelled at the speed of a marching soldier, the solution was to make every artilleryman a part-time horseman. Through relentless drill and discipline Frederick emphasized mobility and speed in all phases of their operations. The unit consisted of a battery of six 6-pound cannons with 48 men, including 3 officers. The battery was wiped out and reformed twice in that same year at the Battle of Kunersdorf and the Battle of Maxen. Despite the setbacks, the new arm had proved so successful that it was quickly reorganized and by the start of the French Revolutionary Wars in 1792 consisted of three companies of 605 men, with batteries consisting of eight 6-pound guns and one 7-pound mortar each.\n\nFrench artilleryman, engineer and general Jean-Baptiste de Gribeauval had served with the military mission to Prussia, as well as fighting against Frederick in the Seven Years' War. After that war he made numerous technical improvements to French cannons which made them lighter, faster and much easier to aim. These improvements proved a great advantage to horse artillery as well. Later, the British army officer Henry Shrapnel invented a deadly new type of ammunition that was put to effective use by horse artillery units.\n\nThe popularity of the new type of unit caught on quickly with other armies. Austria organized a limited amount of \"cavalry artillery\" in 1778 where most of the gun crew rode specially designed, padded gun carriages called \"Wursts\" (\"sausages\"), rather than on separate horses, into battle. Hanover formed its first cavalry batteries in 1786 and the Hanoverian general Victor von Trew performed several trials in 1791 which proved the great speed and efficiency by which an all-mounted crew could operate. At this time the Denmark had also formed mounted artillery units and by 1792 Sweden had formed its first regular riding batteries, followed by Great Britain in 1793, Russia in 1794 and Portugal in 1796.\n\nDuring the Napoleonic Wars, horse artillery would be used extensively and effectively in every major battle and campaign.\nThe largest and probably most efficient horse artillery of any nation was that of the French revolutionary army which was first formed in 1792. The French units were especially well-trained and disciplined since the newly formed arm had proved very popular and could draw on a considerable number of recruits. By 1795 it had grown to eight regiments of six six-gun batteries each, making it the largest horse artillery force ever assembled.\n\nHorse artillery units generally used lighter pieces (6-pounders), pulled by six horses. 9-pounders were pulled by eight horses, and heavier artillery pieces (12-pounders) needed a team of twelve horses. With the individual riding horses required for officers, surgeons and other support staff, as well as those pulling the artillery guns and supply wagons, an artillery battery of six guns could require 160 to 200 horses. Horse artillery usually came under the command of cavalry divisions, but in some battles, such as Waterloo, horse artillery was used as a rapid response force, repulsing attacks and assisting the infantry. Agility was important; the ideal artillery horse was around 15-16 hands high (150–160 cm, 60 to 64 inches), strongly built, but able to move quickly.\nIn the Mexican–American War, the U.S. Army horse artillery, or \"flying artillery\" played a decisive role in several key battles. In the American Civil War, various elements of the horse artillery of the Army of the Potomac were at times grouped together in the U.S. Horse Artillery Brigade. In the U.S., units of horse artillery were generally referred to officially as \"light artillery\".\n\nDuring the 19th and early 20th century, European-style horse artillery was used in South American countries such as Chile and Peru, quite prominently during the War of the Pacific.\n\nAs technology advanced and the firepower of infantry and foot artillery increased, the role of cavalry, and thus the horse artillery, began to decline. It continued to be used and improved into the early 20th century, seeing action during and in between both world wars. In World War I, Russia and some other countries equipped the artillery batteries of their cavalry divisions with the same field gun used by other units. France and the United Kingdom, however, used specialist horse guns (the Canon de 75 modèle 1912 Schneider and the Ordnance QF 13 pounder, respectively.)\n\nSubsequently, the cavalry and horse artillery units rearmed with tanks and self-propelled artillery. As with the cavalry, though, certain artillery units, for instance the Royal Horse Artillery, retain their old designations. Horse artillery was last used in a few units in World War II, including the Wehrmacht's cavalry divisions on the Eastern Front, the Italian \"fast divisions\"(i.e. in the Isbuscenskij charge), and the Imperial Japanese Army in Malaya. A form of riding artillery using heavy machine guns called tachankas were used by the Poles and Russians in World War I, the Russian Civil War, and the German Invasion of Poland. In the United Kingdom, the King's Troop, Royal Horse Artillery retains six traditional teams of six horses each and 13-pounder guns for ceremonial duties to this day.\n\n\n\n\n"}
{"id": "20749014", "url": "https://en.wikipedia.org/wiki?curid=20749014", "title": "Hydrogen internal combustion engine vehicle", "text": "Hydrogen internal combustion engine vehicle\n\nA hydrogen internal combustion engine vehicle (HICEV) is a type of hydrogen vehicle using an internal combustion engine. Hydrogen internal combustion engine vehicles are different from hydrogen fuel cell vehicles (which use electrochemical conversion of hydrogen rather than combustion); the hydrogen internal combustion engine is simply a modified version of the traditional gasoline-powered internal combustion engine.\n\nFrancois Isaac de Rivaz designed in 1806 the De Rivaz engine, the first internal combustion engine, which ran on a hydrogen/oxygen mixture. Étienne Lenoir produced the Hippomobile in 1863. Paul Dieges patented in 1970 a modification to internal combustion engines which allowed a gasoline-powered engine to run on hydrogen.\n\nTokyo City University have been developing hydrogen internal combustion engine from 1970. Recently developed hydrogen fueled Bus and Truck.\n\nMazda has developed Wankel engines that burn hydrogen. The advantage of using ICE (internal combustion engine) such as wankel and piston engines is that the cost of retooling for production is much lower. Existing-technology ICE can still be used to solve those problems where fuel cells are not a viable solution as yet, for example in cold-weather applications.\n\nBetween 2005 - 2007, BMW tested a luxury car named the BMW Hydrogen 7, powered by a hydrogen ICE, which achieved 301 km/h (187 mph) in tests. At least two of these concepts have been manufactured.\n\nHICE forklift trucks have been demonstrated based on converted diesel internal combustion engines with direct injection.\n\nAlset GmbH developed a hybrid hydrogen systems that allows vehicle to use petrol and hydrogen fuels individually or at the same time with an internal combustion engine. This technology was used with Aston Martin Rapide S during the 24 Hours Nürburgring race. The Rapide S was the first vehicle to finish the race with hydrogen technology.\n\nThe combustion of hydrogen with oxygen produces water as its only product:\nIn contrast, combustion of high temperature combustion fuels, such as kerosene, gasoline, or natural gas, with air can produce oxides of nitrogen, known as NO. Tuning a hydrogen engine in 1976 to produce the greatest amount of emissions possible resulted in emissions comparable with consumer operated gasoline engines from 1976. \n\nThe differences between a hydrogen ICE and a traditional gasoline engine include hardened valves and valve seats, stronger connecting rods, non-platinum tipped spark plugs, a higher voltage ignition coil, fuel injectors designed for a gas instead of a liquid, larger crankshaft damper, stronger head gasket material, modified (for supercharger) intake manifold, positive pressure supercharger, and a high temperature engine oil. All modifications would amount to about one point five times (1.5) the current cost of a gasoline engine. These hydrogen engines burn fuel in the same manner that gasoline engines do.\n\nThe theoretical maximum power output from a hydrogen engine depends on the air/fuel ratio and fuel injection method used. The stoichiometric air/fuel ratio for hydrogen is 34:1. At this air/fuel ratio, hydrogen will displace 29% of the combustion chamber leaving only 71% for the air. As a result, the energy content of this mixture will be less than it would be if the fuel were gasoline. Since both the carbureted and port injection methods mix the fuel and air prior to it entering the combustion chamber, these systems limit the maximum theoretical power obtainable to approximately 85% of that of gasoline engines. For direct injection systems, which mix the fuel with the air after the intake valve has closed (and thus the combustion chamber has 100% air), the maximum output of the engine can be approximately 15% higher than that for gasoline engines.\n\nTherefore, depending on how the fuel is metered, the maximum output for a hydrogen engine can be either 15% higher or 15% less than that of gasoline if a stoichiometric air/fuel ratio is used. However, at a stoichiometric air/fuel ratio, the combustion temperature is very high and as a result it will form a large amount of nitrogen oxides (NOx), which is a criteria pollutant. Since one of the reasons for using hydrogen is low exhaust emissions, hydrogen engines are not normally designed to run at a stoichiometric air/fuel ratio.\n\nTypically hydrogen engines are designed to use about twice\nas much air as theoretically required for complete combustion.\nAt this air/fuel ratio, the formation of NOx is reduced\nto near zero. Unfortunately, this also reduces the power output\nto about half that of a similarly sized gasoline engine. To\nmake up for the power loss, hydrogen engines are usually\nlarger than gasoline engines, and/or are equipped with turbochargers\nor superchargers. \n\n\n"}
{"id": "19082332", "url": "https://en.wikipedia.org/wiki?curid=19082332", "title": "Licensed to Kill?", "text": "Licensed to Kill?\n\nLicensed to Kill? The Nuclear Regulatory Commission and the Shoreham Power Plant, a 1998 book by Joan Aron, presents the first detailed case study of how an activist public and elected officials of New York state opposed the Shoreham Nuclear Power Plant on Long Island. The book explains that nuclear power faltered when \"public concerns about health, safety, and the environment superseded other interests about national security or energy supplies\".\n\nAron argues that the Shoreham closure resulted from the collapse of public trust for the Nuclear Regulatory Commission and the entire nuclear industry. For Aron, the unwillingness of the Long Island Lighting Company (LILCO) management to consider true public interest in the debate resulted in \"the loss of the goodwill of its customers\". Also, the willingness of LILCO to press on with plans for Shoreham despite changes in the economics of nuclear power and market demand \"reflected a basic failure of foresight\". \n\n"}
{"id": "10916658", "url": "https://en.wikipedia.org/wiki?curid=10916658", "title": "Linear transformer driver", "text": "Linear transformer driver\n\nA linear transformer driver (LTD) is an annular parallel connection of switches and capacitors designed to deliver rapid high power pulses. The LTD was designed at the Institute of High Current Electronics (IHCE) in Tomsk, Russia. The LTD is capable of producing high current pulses, up to 1 mega amps (10 ampere), with a risetime of less than 100 ns. This is an improvement over Marx generator based pulsed power devices which require pulse compression to achieve such fast risetimes. It is being considered as a driver for z-pinch based inertial confinement fusion.\n\nSandia National Laboratory is currently investigating a z-pinch as a possible ignition source for inertial confinement fusion. On its \"Z machine\", Sandia can achieve dense, high temperature plasmas by firing fast, 100-nanosecond current pulses exceeding 20 million amps through hundreds of tungsten wires with diameters on the order of tens of micrometres. The LTD is currently being investigated as a driver for the next generation of high power accelerators.\n\nSandia's roadmap includes another future Z machine version called ZN (Z Neutron) to test higher yields in fusion power and automation systems. ZN is planned to give between 20 and 30 MJ of hydrogen fusion power with a shot per hour thanks to Russian Linear Transformer Driver (LTD) replacing the current Marx generators. After 8 to 10 years of operation, ZN would become a transmutation pilot plant capable of a fusion shot every 100 seconds.\n\nThe next step planned would be the Z-IFE (Z-inertial fusion energy) test facility, the first true z-pinch driven prototype fusion power plant. It is suggested it would integrate Sandia's latest designs using LTDs. Sandia labs recently proposed a conceptual 1 petawatt (10 watts) LTD Z-pinch power plant, where the electric discharge would reach 70 million amperes.\n\n"}
{"id": "2331922", "url": "https://en.wikipedia.org/wiki?curid=2331922", "title": "Luminous paint", "text": "Luminous paint\n\nLuminous paint or luminescent paint is paint that exhibits luminescence. In other words, it gives off visible light through fluorescence, phosphorescence, or radioluminescence. There are three types of luminous paints..\n\nFluorescent paints offer a wide range of pigments and chroma which also 'glow' when exposed to the long-wave \"ultraviolet\" frequencies (UV). These UV frequencies are found in sunlight and some artificial lights, but they—and their glowing-paint applications—are popularly known as black light and 'black-light effects', respectively.\n\nIn fluorescence the visible light component—sometimes known as \"white light\"—tends to be reflected and perceived normally, as colour; while the UV component of light is modified, 'stepped down' energetically into longer wavelengths, producing additional visible light frequencies, which are then emitted alongside the reflected white light. Human eyes perceive these changes as the unusual 'glow' of \"fluorescence\".\n\nThe fluorescent type of luminescence is significantly different from the natural bioluminescence of bacteria, insects and fish such as the case of the firefly, etc. Bio-luminescence involves no reflection at all, but living generation of light (via the chemistry of Luciferin).\n\nThere are both visible and invisible fluorescent paints. The visible appear under white light to be any bright color, turning peculiarly brilliant under black lights. Invisible fluorescent paints appear transparent or pale under daytime lighting, but will glow under UV light in a limited range of colors. Since these can seem to 'disappear', they can be used to create a variety of clever effects.\n\nBoth types of fluorescent painting benefit when used within a contrasting ambiance of clean, matte-black backgrounds and borders. Such a \"black out\" effect will minimize other awareness, so cultivating the peculiar luminescence of UV fluorescence. Both types of paints have extensive application where artistic lighting effects are desired, particularly in \"black box\" entertainments and environments such as theaters, bars, shrines, etc. Out-of-doors, however, UV wavelengths are rapidly scattered in space (waves are known to bounce off surfaces, outdoors the bounce is imperceptible) or absorbed by complex natural surfaces, dulling the effect. (Humans only see reflected waves of photons, which are light; black matte is ultimate absorption at infinite angles thus enhancing any escaped wave of light. Thus, dim becomes bright.) Furthermore, the complex pigments will degrade quickly in sunlight.\n\nPhosphorescent paint is commonly called \"glow-in-the-dark\" paint. It is made from phosphors such as silver-activated zinc sulfide or doped strontium aluminate, and typically glows a pale green to greenish-blue color. The mechanism for producing light is similar to that of fluorescent paint, but the emission of visible light persists long after it has been exposed to light. Phosphorescent paints have a sustained glow which lasts for up to 12 hours after exposure to light, fading over time.\n\nThis type of paint has been used to mark escape paths in aircraft and for decorative use such as \"stars\" applied to walls and ceilings. It is an alternative to radioluminescent paint. Kenner's \"Lightning Bug Glo-Juice\" was a popular non-toxic paint product in 1968, marketed at children, alongside other glow-in-the-dark toys and novelties. Phosphorescent paint is typically used as body paint, on children's walls and outdoors.\n\nWhen applied as a paint or a more sophisticated coating (e.g. a thermal barrier coating), phosphorescence can be used for temperature detection or degradation measurements known as phosphor thermometry.\n\nRadioluminescent paint is a self-luminous paint that consists of a small amount of a radioactive isotope (radionuclide) mixed with a radioluminescent phosphor chemical. The radioisotope continually decays, emitting radiation particles which strike molecules of the phosphor, exciting them to emit visible light. The isotopes selected are typically strong emitters of beta radiation, preferred since this radiation will not penetrate an enclosure. Radioluminescent paints will glow without exposure to light until the radioactive isotope has decayed (or the phosphor degrades), which may be many years.\n\nBecause of safety concerns and tighter regulation, consumer products such as clocks and watches now increasingly use phosphorescent rather than radioluminescent substances. Radioluminescent paint may still be preferred in specialist applications, such as diving watches.\n\nRadioluminescent paint was invented in 1908 by Sabin Arnold von Sochocky and originally incorporated radium-226. Radium paint was widely used for 40 years on the faces of watches, compasses, and aircraft instruments, so they could be read in the dark. Radium is a radiological hazard, emitting gamma rays that can penetrate a glass watch dial and into human tissue. During the 1920s and 1930s, the harmful effects of this paint became increasingly clear. A notorious case involved the \"Radium Girls\", a group of women who painted watchfaces and later suffered adverse health effects from ingestion. In 1928, Dr von Sochocky himself died of aplastic anemia as a result of radiation exposure. Radium was banned from this use decades ago by international law, but the thousands of legacy radium dials still owned by the public can be a dangerous source of radioactive contamination.\n\nRadium paint used zinc sulfide phosphor, usually trace metal doped with an activator, such as copper (for green light), silver (blue-green), and more rarely copper-magnesium (for yellow-orange light). The phosphor degrades relatively fast and the dials lose luminosity in several years to a few decades; clocks and other devices available from antique shops and other sources therefore are not luminous any more. However, due to the long 1600 year half-life of the Ra-226 isotope they are still radioactive and can be identified with a Geiger counter.\n\nThe dials can be renovated by application of a very thin layer of fresh phosphor, without the radium content (with the original material still acting as the energy source); the phosphor layer has to be thin due to the light self-absorption in the material.\n\nIn the second half of the 20th century, radium was progressively replaced with promethium-147. Promethium is only a relatively low-energy beta-emitter, which, unlike alpha emitters, does not degrade the phosphor lattice and the luminosity of the material does not degrade so fast. Promethium-based paints are significantly safer than radium; the half-life of Pm however, is only 2.62 years, it is therefore not too suitable for long-life applications.\n\nPromethium-based paint was used to illuminate Apollo Lunar Module electrical switch tips and painted on control panels of the Lunar Roving Vehicle.\n\nThe latest generation of the radioluminescent materials is based on tritium, a radioactive isotope of hydrogen with half-life of 12.32 years that emits very low-energy beta radiation. The devices are similar to a fluorescent tube in construction, as they consist of a hermetically sealed (usually borosilicate-glass) tube, coated inside with a phosphor, and filled with tritium. They are known under many names – e.g. gaseous tritium light source (GTLS), traser, betalight.\n\nTritium light sources are most often seen as \"permanent\" illumination for the hands of wristwatches intended for diving, nighttime, or tactical use. They are additionally used in glowing novelty keychains, in self-illuminated exit signs, and formerly in fishing lures. They are favored by the military for applications where a power source may not be available, such as for instrument dials in aircraft, compasses, lights for map reading, and sights for weapons.\n\nTritium lights are also found in some old rotary dial telephones, though due to their age they no longer produce a useful amount of light.\n\n"}
{"id": "41701568", "url": "https://en.wikipedia.org/wiki?curid=41701568", "title": "MV Treasure oil spill", "text": "MV Treasure oil spill\n\nThe MV \"Treasure\" oil spill occurred on 23 June 2000, when the ship sank six miles off the coast of South Africa while transporting iron ore from China to Brazil. The ship was carrying an estimated 1,300 tons of fuel oil, some of which spilled into the ocean, threatening the African penguin populations living on nearby islands. Cleanup efforts began promptly after the incident with particular attention being paid to salvaging the African penguin communities.\n\nMV \"Treasure\" was a Panamanian-registered cargo ship. The 17-year-old ship was transporting a load of of iron ore from China to Brazil at the time of the incident. The \"Weekend Argus\" newspaper quoted unnamed sources as saying the ship was owned by Universal Pearls, which it claimed to be the same Chinese shipping company that owned (which sank off Cape Town's coast in 1994 and caused extensive environmental damage).\n\n\"Treasure\" sank on 23 June 2000. She went down off the coast of South Africa, between Robben Island and Dassen Island after developing a hole in her hull. However, the hole was not the immediate cause of the sinking. Authorities wanted to tow the ship into the South African harbor for repair, but she was too large for the maneuver and was ordered farther off-shore in an attempt to reduce environmental damage from oil pollution. The ship sank while under tow in rough seas when the tow ropes ripped loose. The ship then drifted eastward and subsequently sank. The ship's crew were airlifted to safety.\n\n\"Treasure\" was estimated to have been carrying 1,300 tons of bunker oil of which 400 tons, approximately 2,680 barrels, spilled into the sea off the coast. The pear-shaped slick, about in area, was spotted around noon by Kuswag VII, the Department of Environmental Affairs' oil pollution patrol aircraft. \nThe oil spilled was the ship's own fuel oil, which was of the heaviest and most viscous commercial fuel that can be obtained from petroleum. Bunker oil, also known as fuel oil, is what remains after the lighter fractions (gasoline, kerosene, diesel, etc.) are removed by distillation. The heaviest materials in crude petroleum are not distilled, as their boiling points are too high to be conveniently recovered. As a result, bunker oil is usually very dark in color, more dense, and a significantly more serious contaminant than less-dense oils.\n\nAside from causing the temporary closing of South Africa's ports and threats to species of gannets, cormorants, and seals, \"Treasure\" bunker oil spill was dubbed South Africa's worst environmental disaster, as it seriously threatened its population of African penguins. The spill mainly affected African penguin colonies inhabiting South Africa's Robben and Dassen Islands, which support the largest and third largest colonies of African penguins in the world. The worldwide population of African penguins is numbered at less than 180,000, and is declining. About 150,000 African penguins live off South Africa's coast, 19,000 of which live on Robben Island. The Robben Island nature reserve, home to about 14,000 endangered adult African penguins and 6,000 chicks, was hit badly during their breeding season by the oil spill. Over 20,000 penguins were oiled and approximately 2,000 died.\n\nSouth African Maritime Safety Authority (SAMSA) spokesman Pim Zandee reported that divers confirmed the ship had suffered structural damage when sinking and that oil globules were rising from cracks in the hull. It was also reported that engine room vents, which leaked a steady stream of oil, were closed off, drastically reducing the amount of oil polluting the surface. The dive team continued to seal oil leakages from the wreck. Three days after the sinking, the dive team reported that very little oil was leaking out of the ship.\n\nDifferent types of methods were used in the cleanup of the oil spill, two of which included workers loading kelp covered in oil into trucks and vacuuming up pools of oil with specially designed vacuums. In addition, booms were used to keep the oil from entering Cape Town Harbor.\n\nSouth African company Bio-Matrix was contracted to help clean up the oil slick that was polluting the penguins' habitats. The company used a Canadian product, also called Bio-Matrix, made of sphagnum moss properties, which are notable for their natural ability to soak up oil. Bio-matrix works by encapsulating oil without absorbing water. Bio-Matrix is also effective in helping break down and digest oil.\n\nThe African penguin rescue effort was one of the largest bird rescue missions undertaken thanks to its many volunteers and teams of professionals. The rescue effort consisted of washing and rehabilitating already-oiled birds and capturing non-oiled birds as a preemptive measure. Within ten days of the \"Treasure\" spill, 20,251 oiled African penguins were admitted into the rehabilitation center in Cape Town, and 90% of the oiled birds were rehabilitated and released. Another 19,500 non-oiled penguins were relocated successfully.\n\nThe rehabilitation effort was greatly funded by the International Fund for Animal Welfare, which worked together with the local rehabilitation center, the Southern African Foundation for the Conservation of Coastal Birds (SANCCOB), and the International Bird Rescue Center (IBRRC), whose oiled wildlife team took action the same day the cargo ship sank. The 12-week rehabilitation process, which cared for over 20,000 birds, required over 130 international team members supervising over 45,000 volunteers, 400 tons of fish to feed the penguins, 7,000 tons of beach sand used in bird pens, and 302 containers of detergent to wash the oil off the penguins' feathers.\n\n\"Treasure\" is now a dive spot. The ship's large size and the facts that its hull is only down, and its main deck is within of the surface, coupled with its location in the Bloubergstrand area near Cape Town, South Africa, have contributed to its popularity. The approximate position of the wreck is .\n\n"}
{"id": "20891110", "url": "https://en.wikipedia.org/wiki?curid=20891110", "title": "Middelgrunden", "text": "Middelgrunden\n\nMiddelgrunden is an offshore wind farm in the Øresund 3.5 km\noutside Copenhagen, Denmark. When it was built in 2000, it was the world's largest offshore farm, with 20 turbines (2 MW Bonus each) and a capacity of 40 MW. The farm delivers about 4% of the power for Copenhagen.\n\nIn 1996, the project was initiated by the \"Copenhagen Environment and Energy Office (CEEO)\" after Middelgrunden had been listed as a potential site in the Danish Action Plan for Offshore Wind. Together with the CEEO a group of local people formed the \"Middelgrunden Wind Turbine Cooperative\" and established a cooperation with Copenhagen Energy, the local electric utility. The proposed location was initially opposed by the Danish Society for Nature Conservation, but this decision was later changed.\n\nConcrete gravity base foundations were chosen as the cheapest option.\n\nThis project is an example for community wind energy. It is 50% owned by the 10,000 investors in the \"Middelgrunden Wind Turbine Cooperative\", and 50% by the municipal utility company. Being clearly visible from the capital of Denmark it states the importance of wind power in Denmark.\n\n\n"}
{"id": "38851393", "url": "https://en.wikipedia.org/wiki?curid=38851393", "title": "Parshall flume", "text": "Parshall flume\n\nThe Parshall flume is an open channel flow metering device that was developed to measure the flow of surface waters and irrigation flows. The Parshall flume is a fixed hydraulic structure. It is used to measure volumetric flow rate in industrial discharges, municipal sewer lines, and influent/effluent flows in wastewater treatment plants. The Parshall flume accelerates flow through a contraction of both the parallel sidewalls and a drop in the floor at the flume throat. Under free-flow conditions the depth of water at specified location upstream of the flume throat can be converted to a rate of flow. Some states specify the use of Parshall flumes, by law, for certain situations (commonly water rights).\n\nThe design of the Parshall flume is standardized under ASTM D1941, ISO 9826:1992, and JIS B7553-1993. The flumes are not patented and the discharge tables are not copyright protected.\n\nA total of 22 standard sizes of Parshall flumes have been developed, covering flow ranges from 0.005 cfs [0.1416 l/s] to 3,280 cfs [92,890 l/s].\n\nSubmergence transitions for Parshall flumes range from 50% (1”-3” sizes) to 80% (10’-50’ sizes), beyond which point level measurements must be taken at both the primary and secondary points of measurement and a submergence correction must be applied to the flow equations. It is important to note that the secondary point of measurement (Hb) for a Parshall flume is located in the throat, measuring Hb can be difficult as flow in the throat of the flume is turbulent and prone to fluctuations in the water level. 90% is viewed as the upper limit for which corrections for submerged flow are practical.\n\nUnder laboratory conditions Parshall flumes can be expected to exhibit accuracies to within +/-2%, although field conditions make accuracies better than 5% doubtful.\n\nBeginning in 1915, Dr. Ralph Parshall of the U.S. Soil Conservation Service altered the subcritical Venturi flume to include a drop in elevation through the throat of the flume. This created a transition from subcritical flow conditions to supercritical flow conditions through the throat of the flume.\n\nModifications to the Venturi flume that Parshall made include: \n\nin 1930, the improved flume was named the Parshall Measuring Flume by the Irrigation Committee of the American Society of Civil Engineers (ASCE) in recognition of Parshall's accomplishments. Parshall was additionally honored as a Life Member of the ASCE.\n\nThe Parshall Flume acts essentially as a constriction, a downward step, and then an expansion: the upstream section is uniformly converging and flat, the throat is a short parallel section that slopes downward, and the downstream section is uniformly diverging and slopes upward to an ending elevation that is less than the upstream starting elevation. The width of the throat determines the flume size; 22 standardized sizes have been developed, ranging from 1 in. to 50 ft. (0.005 ft3/s to 3,280 ft3/s).\n\nThere are two conditions of flow that can occur in a Parshall Flume: free flow and submerged flow. When free flow conditions exist, the user only needs to collect one head measurement (Ha, the primary point of measurement) to determine the discharge. For submerged flow a secondary head measurement (Hb) is required to determine the flume is submerged and the degree of submergence.\n\nThe primary point of measurement (Ha) is located in the inlet of the flume, two-thirds of the length of the converging section from the flume crest. The secondary point of measurement (Hb) is located in the throat of the flume.\n\nA hydraulic jump occurs downstream of the flume for free flow conditions. As the flume becomes submerged, the hydraulic jump diminishes and ultimately disappears as the downstream conditions increasingly restrict the flow out of the flume.\nThe free-flow discharge can be summarized as\n\nWhere\n\nA Parshall Flume relies on the conservation of energy principle. The sum of the kinetic and potential energy at a given point must be equal to the energy at any other point along the stream. The total energy or “head” must be equal. \nUsing the equations, we will solve for Q.\n\nformula_2 \nformula_3\n\nformula_4 \n\nWhere E is the energy at H, E at the flume crest, and E at H respectively.\nSince E is located at the flume crest where there is a steep drop, critical flow conditions occur.\n\nformula_5 \n\nRearranging and substituting in the above equations, we get\n\nformula_6\n\nOr\n\nformula_7\n\nSince we know that Q = v⋅y⋅b and v = at critical depth, we can use these relationships to solve for the discharge.\n\nformula_8\n\nBroken further down, we realize that\n\nformula_9 \n\nAnd\n\nformula_10\n\nSince this is measured upstream, where flow is sub-critical, it can be stated that y ≫ v/2g\n\nTherefore, for a rough approximation we can say\n\nformula_11\n\nThis equation simplifies to:\n\n\nThese final two equations are very similar to the Q = CH equations that are used for Parshall Flumes. In fact when looking at the flume tables, n has a value equal to or slightly greater than 1.5, while the value of C is larger than (3.088 b) but still in a rough estimation. It should be noted that the derived equations above will always underestimate actual flow since both the derived C and n values are lower than their respective chart values.\n\nFor the Parshall Flume equation used to calculate the flow rate, both empirical values C and n are known constants (with various values for each Parshall Flume size) leaving Ha (depth upstream) as the only variable needing to be measured. Likewise, in the energy conservation equation, y (or the depth of flow) is needed.\n\nFree Flow – when there is no “back water” to restrict flow through a flume. Only the upstream depth needs to be measured to calculate the flow rate. A free flow also induces a hydraulic jump downstream of the flume.\n\nSubmerged Flow – when the water surface downstream of the flume is high enough to restrict flow through a flume, submerged flume conditions exist. A backwater buildup effect occurs in a submerged flume. For a flow calculation a depth measurement both upstream and downstream is needed.\n\nAlthough commonly thought of as occurring at higher flow rates, It should be noted that submerged flow can exist at any flow level as it is a function of downstream conditions. In natural stream applications, submerged flow is frequently the result of vegetative growth on the downstream channel banks, sedimentation, or subsidence of the flume.\n\nIllustrated above is a unitless E – Y diagram and how Energy and depth of flow changes throughout a Parshall Flume. The two blue lines represent the q values, q for the flow before the constriction, and q representing the value at the constriction (q = Q/b= ft2/s, or flow over width in a rectangular channel). When a constriction (decrease in width) happens Between E and E, the q value changed (and becomes the new critical depth), while the energy remains the same. Then the flume experiences a downward step which results in a gain in energy. This gain in energy is equal to the size of the step (or Δz). From this the principles of conservation of energy are used to develop a set of calculations to predict the flow rate.\n\nFor free flow, the equation to determine the flow rate is simply Q = CH \nwhere:\n\n(See Figure 1 above)\n\nParshall flume discharge table for free flow conditions:\n\nTable 1\nFor submerged flow, a depth of flow needs to be taken upstream (H) and downstream (H). See locations of H and H in Figure 1.\n\nTable 2\nIf H/H is greater or equal to S then it is a submerged flow. If there is submerged flow, adjustments need to be made in order for the Parshall Flume to work properly.\n\nWhere:\n\n\nTable 3\nParshall Flume Free Flow Example Problem:\n\nUsing the Parshall Flume free flow equation, determine the discharge of a 72-inch flume with a depth, Ha of 3 feet.\n\nFrom Table 1: Throat Width = 72 in = 6 ft, C = 24, and n = 1.59.\n\nSo if there is a depth of 3 feet, the flow rate is ≈ 140 ft/s\n\nApproximate the discharge using the derived discharge equation shown above (Equation 5). This equation was derived using the principles of specific energy and is only to serve as an estimate for the actual discharge of the Parshall Flume. Again, it should be noted that equations 5 and 6 will always underestimate the actual flow since both the derived C and n values are lower than their respective empirically derived chart values.\n\n\nParshall flume submerged flow example problem:\n\nUsing the Parshall Flume flow equations and Tables 1-3, determine the flow type (free flow or submerged flow) and discharge for a 36-inch flume with an upstream depth, Ha of 1.5 ft and a downstream depth, H of 1.4 ft. For reference of locations H and H, refer to Figure 1.\n\nFrom Table 2, the Parshall Flume submergence transition (St) for a 36-inch = 3 feet flume is 0.7. Since H/H is greater than or equal to 0.7, it is a submerged flow.\n\nQ = Q – Q\n\nQ = CH\n\nFrom Table 1: Throat Width = 36 in = 3 ft, C = 12, and n = 1.57.\n\nQ = 12(1.5 ft)1.57 = 22.68 ft3/s\n\nQ = M (0.000132 Ha2.123 e9.284 S)\nWhere S = H/H = 1.4 ft/1.5 ft = 0.93\nFrom Table 3, M = 2.4 for a flume size of 3 ft\nQ = 2.4(0.000132(1.5 ft)2.123e9.284(0.93)) = 4.21 ft3/s\n\nQ = 22.68 ft3/s – 4.21 ft3/s = 18.5 ft3/s\n\nA wide variety of materials are used to make Parshall flumes, including:\n\n\nSmaller Parshall flumes tend to be fabricated from fiberglass and galvanized steel (depending upon the application), while larger Parshall flumes tend to fabricated from fiberglass (sizes up to 144\") or concrete (160\"-600\").\n\nBy the 1960s several different companies began to commercially offer Parshall flumes. These manufacturers have typically produce flumes from one type of material only (typically fiberglass/grp or steel), although currently a few, such as Openchannelflow, offer Parshall flumes in a variety of materials.\n\nDr. Parshall's initial focus was for the use of his namesake flume to measure flows in irrigation channels and other surface waters.\n\nOver time, however, the Parshall flume has proven to be applicable to a wide variety of open channel flows including:\n\n\n\nTwo variations of the Parshall flume have been developed over time: the Montana flume and the Short Section (USGS / Portable) Parshall flume.\n\nThe Montana flume omits the throat and discharge sections of the Parshall. By omitting these sections, the flume is shortened by more than half, while retaining the free-flow characteristics of the same-size Parshall. With the deletion of the throat and discharge section, the Montana flume has little resistance to submersion and, like the H flume, should be used where free-spilling discharge is present under all flow conditions. The Montana flume is described in US Bureau of Reclamation's Water Measurement Manual and two technical standards MT199127AG and MT199128AG by Montana State University (note that Montana State University has currently withdrawn both standards for updating/review).\n\nThe short-section Parshall (sometimes referred to as a USGS or Portable Parshall) omits the discharge section of the flume. Originally designed by Troxell and Taylor in 1931 and published under \"Venturi Flume\" as a memorandum from the office of the Ground Water Branch, USGS, the design was again brought to the attention of potential users in Taylors' paper \"Portable Venturi Flume for Measuring Small Flows in 1954. This modification - supplied by the USGS Hydrologic Instrumentation Facility - is available in two sizes: the original 3\" and the recently added 6\".\n\nKilpatrick notes that the discharge for this modification of the Parshall flume is slightly greater than for a standard Parshall flume of the same size. This has been attributed to potential manufacturing tolerance variations rather than the actual operation of the flume itself and users are cautioned to verify the flume's dimensions before proceeding with data collection. As with any Parshall flume, flumes varying from the standard dimensions flumes should be individual rated.\n\nWhen used for stream gauging, aluminum is the typical material of construction - primarily due to its light weight.\n\n\n"}
{"id": "16509656", "url": "https://en.wikipedia.org/wiki?curid=16509656", "title": "Plant efficiency", "text": "Plant efficiency\n\nThe efficiency of a plant is the percentage of the total energy content of a power plant's fuel that is converted into electricity. The remaining energy is usually lost to the environment as heat unless it is used for district heating.\n\nThis is complicated by the fact that there are two different ways to measure the fuel energy input—LCV = Lower Calorific Value, which is the same as NCV = Net Calorific Value, or, HCV = Higher Calorific Value which is the same as GCV, Gross Calorific Value.\n\nDepending on which convention is used, a differences of 10% in the apparent efficiency of a gas fired plant can arise, so it is very important to know which convention, HCV or LCV (NCV or GCV) is being used.\n\nElectric Turbo Compounding (ETC) is a technology solution to the challenge of improving energy efficiency for the stationary power generation industry.\n\nFossil fuel based power generation is predicted to continue for decades, especially in developing economies. This is against the global need to reduce carbon emissions, of which, a high percentage is produced by the power sector worldwide.\n\nETC works by making gas and diesel-powered gensets (Electric Generators) work more effectively and cleaner, by recovering waste energy from the exhaust to improve power density and fuel efficiency.\n\n\n"}
{"id": "1779482", "url": "https://en.wikipedia.org/wiki?curid=1779482", "title": "Pneumatic motor", "text": "Pneumatic motor\n\nA pneumatic motor (air motor) or compressed air engine is a type of motor which does mechanical work by expanding compressed air. Pneumatic motors generally convert the compressed air energy to mechanical work through either linear or rotary motion. Linear motion can come from either a diaphragm or piston actuator, while rotary motion is supplied by either a vane type air motor, piston air motor, air turbine or gear type motor.\n\nPneumatic motors have existed in many forms over the past two centuries, ranging in size from hand-held motors to engines of up to several hundred horsepower. Some types rely on pistons and cylinders; others on slotted rotors with vanes (vane motors) and others use turbines. Many compressed air engines improve their performance by heating the incoming air or the engine itself. Pneumatic motors have found widespread success in the hand-held tool industry, but are also used stationary in a wide range of industrial applications. Continual attempts are being made to expand their use to the transportation industry. However, pneumatic motors must overcome inefficiencies before being seen as a viable option in the transportation industry.\n\nIn order to achieve linear motion from compressed air, a system of pistons is most commonly used. The compressed air is fed into an air-tight chamber that houses the shaft of the piston. Also inside this chamber a spring is coiled around the shaft of the piston in order to hold the chamber completely open when air is not being pumped into the chamber. As air is fed into the chamber the force on the piston shaft begins to overcome the force being exerted on the spring. As more air is fed into the chamber, the pressure increases and the piston begins to move down the chamber. When it reaches its maximum length the air pressure is released from the chamber and the spring completes the cycle by closing off the chamber to return to its original position. \nPiston motors are the most commonly used in hydraulic systems. Essentially, piston motors are the same as hydraulic motors except they are used to convert hydraulic energy into mechanical energy.\nPiston motors are often used in series of two, three, four, five, or six cylinders that are enclosed in a housing. This allows for more power to be delivered by the pistons because several motors are in sync with each other at certain times of their cycle.\n\nA type of pneumatic motor, known as a rotary vane motor, uses air to produce rotational motion to a shaft. The rotating element is a slotted rotor which is mounted on a drive shaft. Each slot of the rotor is fitted with a freely sliding rectangular vane. The vanes are extended to the housing walls using springs, cam action, or air pressure, depending on the motor design. Air is pumped through the motor input which pushes on the vanes creating the rotational motion of the central shaft. Rotation speeds can vary between 100 and 25,000 rpm depending on several factors which include the amount of air pressure at the motor inlet and the diameter of the housing.\nOne application for vane-type air motors is to start large industrial diesel or natural gas engines. Stored energy in the form of compressed air, nitrogen or natural gas enters the sealed motor chamber and exerts pressure against the vanes of a rotor. This causes the rotor to turn at high speed. Because the engine flywheel requires a great deal of torque to start the engine, reduction gears are used. Reduction gears create high torque levels with the lower amounts of energy input. These reduction gears allow for sufficient torque to be generated by the engine flywheel while it is engaged by the pinion gear of the air motor or air starter.\n\nA widespread application of pneumatic motors is in hand-held tools, impact wrenches, pulse tools, screwdrivers, nut runners, drills, grinders, sanders and so on. Pneumatic motors are also used stationary in a wide range of industrial applications. Though overall energy efficiency of pneumatics tools is low and they require access to a compressed-air source, there are several advantages over electric tools. They offer greater power density (a smaller pneumatic motor can provide the same amount of power as a larger electric motor), do not require an auxiliary speed controller (adding to its compactness), generate less heat, and can be used in more volatile atmospheres as they do not require electric power and do not create sparks. They can be loaded to stop with full torque without damages.\n\nHistorically, many individuals have tried to apply pneumatic motors to the transportation industry. Guy Negre, CEO and founder of Zero Pollution Motors, has pioneered this field since the late 1980s. Recently Engineair has also developed a rotary motor for use in automobiles. Engineair places the motor immediately beside the wheel of the vehicle and uses no intermediate parts to transmit motion which means almost all of the motor's energy is used to rotate the wheel.\n\nThe pneumatic motor was first applied to the field of transportation in the mid-19th century. Though little is known about the first recorded compressed-air vehicle, it is said that the Frenchmen Andraud and Tessie of Motay ran a car powered by a pneumatic motor on a test track in Chaillot, France, on July 9, 1840. Although the car test was reported to have been successful, the pair didn’t explore further expansion of the design.\nThe first successful application of the pneumatic motor in transportation was the Mekarski system air engine used in locomotives. Mekarski’s innovative engine overcame cooling that accompanies air expansion by heating air in a small boiler prior to use. The Tramway de Nantes, located in Nantes, France, was noted for being the first to use Mekarski engines to power their fleet of locomotives. The tramway began operation on December 13, 1879, and continues to operate today, although the pneumatic trams were replaced in 1917 by more efficient and modern electrical trams.\nAmerican Charles Hodges also found success with pneumatic motors in the locomotive industry. In 1911 he designed a pneumatic locomotive and sold the patent to the H.K. Porter Company in Pittsburgh for use in coal mines. Because pneumatic motors do not use combustion they were a much safer option in the coal industry.\n\nMany companies claim to be developing Compressed air cars, but none are actually available for purchase or even independent testing.\n\nImpact wrenches, pulse tools, nutrunners, screwdrivers, drills, grinders, die grinders, sanders, dental drills and other pneumatic tools use a variety of air motors. These include vane type motors, turbines and piston motors.\n\nMost successful early forms of self-propelled torpedoes used high pressure compressed air, although this was superseded by internal or external combustion engines, steam engines, or electric motors.\n\nCompressed air engines were used in trams and shunters, and eventually found a successful niche in mining locomotives, although in the end they were replaced by electric trains, underground. Over the years designs increased in complexity, resulting in a triple expansion engine with air-to-air reheaters between each stage. For more information see Fireless locomotive and Mekarski system. \n\nTransport category airplanes, such as commercial airliners, use compressed air starters to start the main engines. The air is supplied by the load compressor of the aircraft's auxiliary power unit, or by ground equipment.\n\nWater rockets use compressed air to power their water jet and generate thrust, they are used as toys.\n\nAir Hogs, a toy brand, also uses compressed air to power piston engines in toy airplanes (and some other toy vehicles).\n\nThere is currently some interest in developing air cars. Several engines have been proposed for these, although none have demonstrated the performance and long life needed for personal transport.\n\nThe Energine Corporation was a South Korean company that claimed to deliver fully assembled cars running on a hybrid compressed air and electric engine. The compressed-air engine is used to activate an alternator, which extends the autonomous operating capacity of the car. The CEO was arrested for fraudulently promoting air motors with false claims.\n\nEngineAir, an Australian company, is making a rotary engine powered by compressed air, called The Di Pietro motor.\nThe Di Pietro motor concept is based on a rotary piston. Different from existing rotary engines, the Di Pietro motor uses a simple cylindrical rotary piston (shaft driver) which rolls, with little friction, inside the cylindrical stator.\n\nIt can be used in boat, cars, burden carriers and other vehicles. Only 1 psi (≈ 6,8 kPa) of pressure is needed to overcome the friction. The engine was also featured on the ABC's New Inventors programme in Australia on 24 March 2004.\n\nK'Airmobiles vehicles were intended to be commercialized from a project developed in France in 2006-2007 by a small group of researchers. However, the project has not been able to gather the necessary funds.\nPeople should note that, meantime, the team has recognized the physical impossibility to use on-board stored compressed air due to its poor energy capacity and the thermal losses resulting from the expansion of the gas. \nThese days, using the patent pending 'K'Air Generator', converted to work as a compressed-gas motor, the project should be launched in 2010, thanks to a North American group of investors, but for the purpose of developing first a green energy power system.\n\nIn the original Nègre air engine, one piston compresses air from the atmosphere to mix with the stored compressed air (which will cool drastically as it expands). This mixture drives the second piston, providing the actual engine power. MDI's engine works with constant torque, and the only way to change the torque to the wheels is to use a pulley transmission of constant variation, losing some efficiency. When vehicle is stopped, MDI's engine had to be on and working, losing energy. In 2001-2004 MDI switched to a design similar to that described in Regusci's patents (see below), which date back to 1990.\n\nIt has been reported in 2008 that Indian car manufacturer Tata was looking at an MDI compressed air engine as an option on its low priced Nano automobiles. Tata announced in 2009 that the compressed air car was proving difficult to develop due to its low range and problems with low engine temperatures.\n\nThe Pneumatic Quasiturbine engine is a compressed air pistonless rotary engine using a rhomboidal-shaped rotor whose sides are hinged at the vertices.\n\nThe Quasiturbine has demonstrated as a pneumatic engine using stored compressed air \n\nIt can also take advantage of the energy amplification possible from using available external heat, such as solar energy.\n\nThe Quasiturbine rotates from pressure as low as 0.1 atm (1.47psi).\n\nSince the Quasiturbine is a pure expansion engine, while the Wankel and most other rotary engines are not, it is well-suited as a compressed fluid engine, air engine or air motor.\n\nArmando Regusci's version of the air engine couples the transmission system directly to the wheel, and has variable torque from zero to the maximum, enhancing efficiency. Regusci's patents date from 1990.\n\nPsycho-Active is developing a multi-fuel/air-hybrid chassis which is intended to serve as the foundation for a line of automobiles. Claimed performance is 50 hp/litre. The compressed air motor they use is called the DBRE or Ducted Blade Rotary Engine.\n\nMilton M. Conger in 1881 patented and supposedly built a motor that ran off compressed air or steam that using a flexible tubing which will form a wedge-shaped or inclined wall or abutment in the rear of the tangential bearing of the wheel, and propel it with greater or less speed according to the pressure of the propelling medium.\n\n"}
{"id": "12564734", "url": "https://en.wikipedia.org/wiki?curid=12564734", "title": "Protected areas of Russia", "text": "Protected areas of Russia\n\nProtected areas of Russia, (official Russian title: , literally \"Specially Protected Natural Areas\"), is governed by the corresponding 1995 law of the Russian Federation. \n\nThe law establishes the following categories of protected areas:\n\nOther areas that are protected in Russia include:\n\n"}
{"id": "2930612", "url": "https://en.wikipedia.org/wiki?curid=2930612", "title": "Roberval balance", "text": "Roberval balance\n\nThe Roberval balance is a weighing scale presented to the French Academy of Sciences by the French mathematician Gilles Personne de Roberval in 1669.\n\nIn this scale, two identical horizontal beams are attached, one directly above the other, to a vertical column, which is attached to a stable base. On each side, both horizontal beams are attached to a vertical beam. The six attachment points are pivots. Two horizontal plates, suitable for placing objects to be weighed, are fixed to the top of the two vertical beams. An arrow on the lower horizontal beam (and perpendicular to it) and a mark on the vertical column may be added to aid in leveling the scale.\nThe object to be weighed is placed on one plate, and calibrated masses are added to and subtracted from the other plate until level is reached. The mass of the object is equal to the mass of the calibrated masses regardless of where on the plates items are placed. Since the vertical beams are always vertical, and the weighing platforms always horizontal, the potential energy lost by a weight as its platform goes down a certain distance will always be the same, so it makes no difference where the weight is placed. For maximum accuracy, Roberval balances require that their top fulcrum be placed on the line between the left and right pivot so that tipping will not result in the net transfer of weight to either the left or right side of the scale: a fulcrum placed \"below\" the idea pivot point will tend to cause a net shift in the direction of any downward-moving vertical column (in a kind of positive feedback loop); likewise, a fulcrum placed \"above\" this point will tend to level out the arms of the balance rather than respond to small changes in weight (in a negative feedback loop).\n\nThe vertical column supporting a plate with an offset weight must be in axial compression and flexure. Here the axial compression is carried by the bearing at the top beam in most balance scales, the lower beam just being supported horizontally at midpoint by the body of the scales by a simple peg-in-slot arrangement, so it effectively hangs beneath the top beam and stops the platforms from rotating. The flexural force in the column (a.k.a. bending moment) is taken by a pair of equal and opposite forces in the horizontal beams. So if the offset weight is towards the outside of the platform, further from the centre of the scales, the top beam will be in axial tension and the bottom beam will be in axial compression. These tensions and compressions are carried by horizontal reactions from the central supports, the other side of the scales is not affected at all, nor is the balance of the scales. \n\nCertain presumptions are made in a theoretical Roberval balance. In order for such a balance to appear level in its natural state and be able to balance theoretical masses, the following must be true:\n\n\nThe Roberval balance is arguably less accurate and more difficult to manufacture than a beam balance with suspended plates. The beam balance, however, has the significant disadvantage of requiring suspensory strings, chains, or rods. For over three hundred years the Roberval balance has instead been popular for applications requiring convenience and only moderate accuracy, notably in retail trade.\n\nWell known manufacturers of Roberval balances include W & T Avery Ltd. and George Salter & Co. Ltd. in the United Kingdom and in France. Henry Troemner, who designed scales for the United States Department of Treasury, was the first American to use the design.\n\n\n"}
{"id": "2157145", "url": "https://en.wikipedia.org/wiki?curid=2157145", "title": "Rollin film", "text": "Rollin film\n\nA Rollin film, named after Bernard V. Rollin, is a 30 nm-thick liquid film of helium in the helium II state. It exhibits a \"creeping\" effect in response to surfaces extending past the film's level (wave propagation). Helium II can escape from any non-closed container via creeping toward and eventually evaporating from capillaries of 10 to 10 meters or greater.\n\nRollin films are involved in the \"fountain effect\" where superfluid helium leaks out of a container in a fountain-like manner. They have high thermal conductivity. \n\nThe ability of superfluid liquids to cross obstacles that lie at a higher level is often referred to as the Onnes effect, named after Heike Kamerlingh Onnes. The Onnes effect is enabled by the capillary forces dominating gravity and viscous forces.\n\nWaves propagating across a Rollin film are governed by the same equation as gravity waves in shallow water, but rather than gravity, the restoring force is the van der Waals force. The film suffers a change in chemical potential when the thickness varies. These waves are known as third sound.\n\nThe thickness of the film can be calculated by the energy balance. Consider a small fluid volume element formula_1 which is located at a height formula_2 from the free surface. The potential energy due to the gravitational force acting on the fluid element is formula_3, where formula_4 is the total density and formula_5 is the gravitational acceleration. The quantum kinetic energy per particle is formula_6, where formula_7 is the thickness of the film and formula_8 is the mass of the particle. Therefore, the net kinetic energy is given by formula_9, where formula_10 is the fraction of atoms which are Bose–Einstein condensate. Minimizing the total energy with respect to the thickness provides us the value of the thickness,\n\n\n\n"}
{"id": "25171720", "url": "https://en.wikipedia.org/wiki?curid=25171720", "title": "Safety fuse", "text": "Safety fuse\n\nThe safety fuse is a type of fuse invented and patented by English inventor William Bickford in 1831. Originally it consisted of a \"tube\" of gunpowder surrounded by a waterproofed varnished jute \"rope\". It replaced earlier and less reliable methods of igniting gunpowder blasting charges which had caused many injuries and deaths in the mining industry. The safety fuse burns at a rate of typically about 30 seconds per foot (1 second per cm).\n\nDocumented evidence suggests that the earliest fuses were first used by the Chinese between the 10th and 12th centuries. After the Chinese had invented gunpowder, they began adapting its explosive properties for use in military technology. By 1044 they were using gunpowder in simple grenades, bombs, and flamethrowers. Gunpowder did not reach Europe until the early 13th century, carried over from China by Middle Eastern traders and merchants along the old Silk Road.\n\nFor three centuries gunpowder was primarily used for military warfare. It was not until 1574 that gunpowder was first introduced to the mining industry, and it took until 1617 before it was first used in a large-scale mining operation—at Thillot in France.\n\nOne of the problems miners faced when introducing gunpowder into their operations was that it was relatively easy to ignite when exposed to sparks, intense heat, or flames. The method used by miners to blast away rock involved drilling several holes across a rock face which would be filled with charges of gunpowder. In order to confine the gases produced on ignition, the gunpowder was confined within each shot hole by inserting a pointed rod known as a \"needle\" in the gunpowder-charged hole and then packing in soft clay and tamping it down to form a plug. The \"needle\" was then removed and replaced by a fuse. To prevent sparking, a copper needle and a non-metallic ramming rod, typically made from hickory, were used.\n\nIf a spark was created the results could be disastrous to the work force, and this was a common occurrence. Miners and mine owners were aware of the dangers of the use of gunpowder in mining, as is evident in their instructions for handling the material. A mill in England preparing the material wrote in its instructions, \"Whosoever is at Labour within or without the powder magazines should execute his commission in such a respectful and revered silence as is seemly in such a place where (unless the Almighty in his Grace keeps a protective hand over the Labour) the least lack of care may not alone cause the loss of life of all present, but may even in a moment transform this place as well as its surroundings into a heap of stone.\"\n\nThe other major problem concerned the intentional ignition of the gunpowder charges. To provide some protection from the blast and the fumes, a nominated miner ignited the far end of the fuse which was intended to burn at a known rate. The miners, therefore, knowing the length of the fuse, could estimate the delay between ignition of the fuse and the ignition of the main charges. However, early fuses, known as filled \"quills\", had a tendency to either burn irregularly, \"flash off\", or break—either by separation or by \"pinching\" in the shot hole due to the tamping process. They could also be damaged allowing moisture in, which could cause them to smoulder instead of burn and introduce a long delay. If the main charge failed to ignite, this was known as a misfire or \"hang fire\", and the miners would need to wait before returning to the work face to set new fuses. Increasingly, miners in Cornwall in the late 18th and early 19th centuries were becoming badly injured as a result of suspecting that there had been a misfire and returning to the work face just as a smouldering damp quill ignited the gunpowder charges.\n\nIn 1831 English merchant William Bickford moved to the heart of the Cornish mining district near Camborne; where at Tuckingmill he developed the first practical and reliable means for igniting gunpowder when mining, the \"Safety Fuze\". After earlier attempts at developing a safer way had failed, Bickford had an insight while visiting his friend who was a ropemaker. While observing his friend winding cord together to generate a rope, Bickford believed he could adapt the same method towards developing a fuse. This was done with the help of his son-in-law George Smith and a working miner named Thomas Davey.\n\nBickford invented a machine which would thread and weave two layers of jute yarn (a shiny vegetable fibre), spun in opposite directions, over a small \"tube\" of gunpowder, the whole of which would then be \"varnished\" with tar to waterproof the product. The outcome was the development of a fuse which when lit \"the fire only travels along it slowly, rate of burning ... being about 30 seconds per foot.\" Bickford had developed a fuse which would burn for a known length of time, depending on the length of the fuse.\n\nBickford obtained a British Patent for his device (No. 6159 \"Safety Fuze for Igniting Gunpowder used in Blasting Rocks, Etc\") on 6 September 1831. It was originally called \"The Patent Safety Rod\" but its name was later changed to the \"Safety Fuse\". It was supplied as a \"rope\" of about diameter; and was sold at the time for about same price as its predecessor, quills, at three pence per fathom (6 ft, 1.8 m). Bickford also set up a partnership with Thomas Davey, who gained twenty five percent of the profits for the first fourteen years.\n\nGiven the unreliability of fuses and means of detonation prior to Bickford’s fuse, this new technology changed the safety and conditions of mining. Due to poor record keeping or lack thereof, it is relatively difficult to determine the exact number of mining accidents and related statistics prior to the invention of the safety fuse. However \"this fuse soon replaced the less reliable fuses which were made of straws or quills filled with black powder, thus greatly reducing the hazard of accidental explosions in mining or construction.\" Word of the reliability of Bickford's safety fuse spread, and was soon in large demand across world markets.\n\nBickford’s fuse not only dramatically improved the safety conditions of mines around the world, but also contributed to the development of dynamite. Alfred Nobel created dynamite in 1867, by moulding nitroglycerine and a mud-like compound found near his laboratories called kieselguhr into individual cylinders. At the end of each cylinder Nobel inserted a blast cap which could be ignited in one of two ways. First, by inserting a safety fuse into the blast cap and igniting the fuse, it will set the blast cap off and produce enough energy to detonate the dynamite. Second, it is possible to detonate dynamite by inserting a blast cap into the end of the cylinder and then by attaching an electrical wire into the blast cap and producing a current which will travel from the source to the blast cap will also set off the dynamite.\n\n\n"}
{"id": "40344", "url": "https://en.wikipedia.org/wiki?curid=40344", "title": "Semiconductor device", "text": "Semiconductor device\n\nSemiconductor devices are electronic components that exploit the electronic properties of semiconductor materials, principally silicon, germanium, and gallium arsenide, as well as organic semiconductors. Semiconductor devices have replaced thermionic devices (vacuum tubes) in most applications. They use electronic conduction in the solid state as opposed to the gaseous state or thermionic emission in a high vacuum.\n\nSemiconductor devices are manufactured both as single discrete devices and as \"integrated circuits\" (ICs), which consist of a number – from a few (as low as two) to billions – of devices manufactured and interconnected on a single semiconductor substrate, or wafer.\n\nSemiconductor materials are useful because their behavior can be easily manipulated by the addition of impurities, known as doping. Semiconductor conductivity can be controlled by the introduction of an electric or magnetic field, by exposure to light or heat, or by the mechanical deformation of a doped monocrystalline grid; thus, semiconductors can make excellent sensors. Current conduction in a semiconductor occurs via mobile or \"free\" \"electrons\" and \"holes\", collectively known as \"charge carriers\". Doping a semiconductor such as silicon with a small proportion of an atomic impurity, such as phosphorus or boron, greatly increases the number of free electrons or holes within the semiconductor. When a doped semiconductor contains excess holes it is called \"p-type\", and when it contains excess free electrons it is known as \"n-type\", where \"p\" (positive for holes) or \"n\" (negative for electrons) is the sign of the charge of the majority mobile charge carriers. The semiconductor material used in devices is doped under highly controlled conditions in a fabrication facility, or \"fab\", to control precisely the location and concentration of p- and n-type dopants. The junctions which form where n-type and p-type semiconductors join together are called p–n junctions.\n\nSemiconductor devices made per year have been growing by 9.1% on average since 1978, and shipments in 2018 are predicted for the first time to exceed 1 trillion, meaning that well over 7 trillion has been made to date, in just in the decade prior.\n\nA semiconductor diode is a device typically made from a single p–n junction. At the junction of a p-type and an n-type semiconductor there forms a depletion region where current conduction is inhibited by the lack of mobile charge carriers. When the device is \"forward biased\" (connected with the p-side at higher electric potential than the n-side), this depletion region is diminished, allowing for significant conduction, while only very small current can be achieved when the diode is \"reverse biased\" and thus the depletion region expanded.\n\nExposing a semiconductor to light can generate electron–hole pairs, which increases the number of free carriers and thereby the conductivity. Diodes optimized to take advantage of this phenomenon are known as \"photodiodes\".\nCompound semiconductor diodes can also be used to generate light, as in light-emitting diodes and laser diodes.\n\nBipolar junction transistors are formed from two p–n junctions, in either n–p–n or p–n–p configuration. The middle, or \"base\", region between the junctions is typically very narrow. The other regions, and their associated terminals, are known as the \"emitter\" and the \"collector\". A small current injected through the junction between the base and the emitter changes the properties of the base-collector junction so that it can conduct current even though it is reverse biased. This creates a much larger current between the collector and emitter, controlled by the base-emitter current.\n\nAnother type of transistor, the field-effect transistor, operates on the principle that semiconductor conductivity can be increased or decreased by the presence of an electric field. An electric field can increase the number of free electrons and holes in a semiconductor, thereby changing its conductivity. The field may be applied by a reverse-biased p–n junction, forming a \"junction field-effect transistor\" (JFET) or by an electrode insulated from the bulk material by an oxide layer, forming a \"metal–oxide–semiconductor field-effect transistor\" (MOSFET).\n\nThe MOSFET, a solid-state device, is the most used semiconductor device today. The \"gate\" electrode is charged to produce an electric field that controls the conductivity of a \"channel\" between two terminals, called the \"source\" and \"drain\". Depending on the type of carrier in the channel, the device may be an \"n-channel\" (for electrons) or a \"p-channel\" (for holes) MOSFET. Although the MOSFET is named in part for its \"metal\" gate, in modern devices polysilicon is typically used instead.\n\nBy far, silicon (Si) is the most widely used material in semiconductor devices. Its combination of low raw material cost, relatively simple processing, and a useful temperature range makes it currently the best compromise among the various competing materials. Silicon used in semiconductor device manufacturing is currently fabricated into boules that are large enough in diameter to allow the production of 300 mm (12 in.) wafers.\n\nGermanium (Ge) was a widely used early semiconductor material but its thermal sensitivity makes it less useful than silicon. Today, germanium is often alloyed with silicon for use in very-high-speed SiGe devices; IBM is a major producer of such devices.\n\nGallium arsenide (GaAs) is also widely used in high-speed devices but so far, it has been difficult to form large-diameter boules of this material, limiting the wafer diameter to sizes significantly smaller than silicon wafers thus making mass production of GaAs devices significantly more expensive than silicon.\n\nOther less common materials are also in use or under investigation.\n\nSilicon carbide (SiC) has found some application as the raw material for blue light-emitting diodes (LEDs) and is being investigated for use in semiconductor devices that could withstand very high operating temperatures and environments with the presence of significant levels of ionizing radiation. IMPATT diodes have also been fabricated from SiC.\n\nVarious indium compounds (indium arsenide, indium antimonide, and indium phosphide) are also being used in LEDs and solid state laser diodes. Selenium sulfide is being studied in the manufacture of photovoltaic solar cells.\n\nThe most common use for organic semiconductors is organic light-emitting diodes.\n\n\"Two-terminal devices:\"\n\n\"Three-terminal devices:\"\n\n\"Four-terminal devices:\"\n\nAll transistor types can be used as the building blocks of logic gates, which are fundamental in the design of digital circuits. In digital circuits like microprocessors, transistors act as on-off switches; in the MOSFET, for instance, the voltage applied to the gate determines whether the switch is on or off.\n\nTransistors used for analog circuits do not act as on-off switches; rather, they respond to a continuous range of inputs with a continuous range of outputs. Common analog circuits include amplifiers and oscillators.\n\nCircuits that interface or translate between digital circuits and analog circuits are known as mixed-signal circuits.\n\nPower semiconductor devices are discrete devices or integrated circuits intended for high current or high voltage applications. Power integrated circuits combine IC technology with power semiconductor technology, these are sometimes referred to as \"smart\" power devices. Several companies specialize in manufacturing power semiconductors.\n\nThe type designators of semiconductor devices are often manufacturer specific. Nevertheless, there have been attempts at creating standards for type codes, and a subset of devices follow those. For discrete devices, for example, there are three standards: JEDEC JESD370B in United States, Pro Electron in Europe and Japanese Industrial Standards (JIS) in Japan.\n\nSemiconductors had been used in the electronics field for some time before the invention of the transistor. Around the turn of the 20th century they were quite common as detectors in radios, used in a device called a \"cat's whisker\" developed by Jagadish Chandra Bose and others. These detectors were somewhat troublesome, however, requiring the operator to move a small tungsten filament (the whisker) around the surface of a galena (lead sulfide) or carborundum (silicon carbide) crystal until it suddenly started working. Then, over a period of a few hours or days, the cat's whisker would slowly stop working and the process would have to be repeated. At the time their operation was completely mysterious. After the introduction of the more reliable and amplified vacuum tube based radios, the cat's whisker systems quickly disappeared. The \"cat's whisker\" is a primitive example of a special type of diode still popular today, called a Schottky diode.\n\nAnother early type of semiconductor device is the metal rectifier in which the semiconductor is copper oxide or selenium. Westinghouse Electric (1886) was a major manufacturer of these rectifiers.\n\nDuring World War II, radar research quickly pushed radar receivers to operate at ever higher frequencies and the traditional tube based radio receivers no longer worked well. The introduction of the cavity magnetron from Britain to the United States in 1940 during the Tizard Mission resulted in a pressing need for a practical high-frequency amplifier. \n\nOn a whim, Russell Ohl of Bell Laboratories decided to try a cat's whisker. By this point they had not been in use for a number of years, and no one at the labs had one. After hunting one down at a used radio store in Manhattan, he found that it worked much better than tube-based systems.\n\nOhl investigated why the cat's whisker functioned so well. He spent most of 1939 trying to grow more pure versions of the crystals. He soon found that with higher quality crystals their finicky behaviour went away, but so did their ability to operate as a radio detector. One day he found one of his purest crystals nevertheless worked well, and it had a clearly visible crack near the middle. However as he moved about the room trying to test it, the detector would mysteriously work, and then stop again. After some study he found that the behaviour was controlled by the light in the room – more light caused more conductance in the crystal. He invited several other people to see this crystal, and Walter Brattain immediately realized there was some sort of junction at the crack.\n\nFurther research cleared up the remaining mystery. The crystal had cracked because either side contained very slightly different amounts of the impurities Ohl could not remove – about 0.2%. One side of the crystal had impurities that added extra electrons (the carriers of electric current) and made it a \"conductor\". The other had impurities that wanted to bind to these electrons, making it (what he called) an \"insulator\". Because the two parts of the crystal were in contact with each other, the electrons could be pushed out of the conductive side which had extra electrons (soon to be known as the \"emitter\") and replaced by new ones being provided (from a battery, for instance) where they would flow into the insulating portion and be collected by the whisker filament (named the \"collector\"). However, when the voltage was reversed the electrons being pushed into the collector would quickly fill up the \"holes\" (the electron-needy impurities), and conduction would stop almost instantly. This junction of the two crystals (or parts of one crystal) created a solid-state diode, and the concept soon became known as semiconduction. The mechanism of action when the diode is off has to do with the separation of charge carriers around the junction. This is called a \"depletion region\".\n\nArmed with the knowledge of how these new diodes worked, a vigorous effort began to learn how to build them on demand. Teams at Purdue University, Bell Labs, MIT, and the University of Chicago all joined forces to build better crystals. Within a year germanium production had been perfected to the point where military-grade diodes were being used in most radar sets.\n\nAfter the war, William Shockley decided to attempt the building of a triode-like semiconductor device. He secured funding and lab space, and went to work on the problem with Brattain and John Bardeen.\n\nThe key to the development of the transistor was the further understanding of the process of the electron mobility in a semiconductor. It was realized that if there were some way to control the flow of the electrons from the emitter to the collector of this newly discovered diode, an amplifier could be built. For instance, if contacts are placed on both sides of a single type of crystal, current will not flow between them through the crystal. However if a third contact could then \"inject\" electrons or holes into the material, current would flow.\n\nActually doing this appeared to be very difficult. If the crystal were of any reasonable size, the number of electrons (or holes) required to be injected would have to be very large, making it less than useful as an amplifier because it would require a large injection current to start with. That said, the whole idea of the crystal diode was that the crystal itself could provide the electrons over a very small distance, the depletion region. The key appeared to be to place the input and output contacts very close together on the surface of the crystal on either side of this region.\n\nBrattain started working on building such a device, and tantalizing hints of amplification continued to appear as the team worked on the problem. Sometimes the system would work but then stop working unexpectedly. In one instance a non-working system started working when placed in water. Ohl and Brattain eventually developed a new branch of quantum mechanics, which became known as surface physics, to account for the behaviour. The electrons in any one piece of the crystal would migrate about due to nearby charges. Electrons in the emitters, or the \"holes\" in the collectors, would cluster at the surface of the crystal where they could find their opposite charge \"floating around\" in the air (or water). Yet they could be pushed away from the surface with the application of a small amount of charge from any other location on the crystal. Instead of needing a large supply of injected electrons, a very small number in the right place on the crystal would accomplish the same thing.\n\nTheir understanding solved the problem of needing a very small control area to some degree. Instead of needing two separate semiconductors connected by a common, but tiny, region, a single larger surface would serve. The electron-emitting and collecting leads would both be placed very close together on the top, with the control lead placed on the base of the crystal. When current flowed through this \"base\" lead, the electrons or holes would be pushed out, across the block of semiconductor, and collect on the far surface. As long as the emitter and collector were very close together, this should allow enough electrons or holes between them to allow conduction to start.\n\nThe Bell team made many attempts to build such a system with various tools, but generally failed. Setups where the contacts were close enough were invariably as fragile as the original cat's whisker detectors had been, and would work briefly, if at all. Eventually they had a practical breakthrough. A piece of gold foil was glued to the edge of a plastic wedge, and then the foil was sliced with a razor at the tip of the triangle. The result was two very closely spaced contacts of gold. When the wedge was pushed down onto the surface of a crystal and voltage applied to the other side (on the base of the crystal), current started to flow from one contact to the other as the base voltage pushed the electrons away from the base towards the other side near the contacts. The point-contact transistor had been invented.\n\nWhile the device was constructed a week earlier, Brattain's notes describe the first demonstration to higher-ups at Bell Labs on the afternoon of 23 December 1947, often given as the birthdate of the transistor. What is now known as the \"p–n–p point-contact germanium transistor\" operated as a speech amplifier with a power gain of 18 in that trial. John Bardeen, Walter Houser Brattain, and William Bradford Shockley were awarded the 1956 Nobel Prize in physics for their work.\n\nBell Telephone Laboratories needed a generic name for their new invention: \"Semiconductor Triode\", \"Solid Triode\", \"Surface States Triode\", \"Crystal Triode\" and \"Iotatron\" were all considered, but \"transistor\", coined by John R. Pierce, won an internal ballot. The rationale for the name is described in the following extract from the company's Technical Memoranda (May 28, 1948) [26] calling for votes:\n\nTransistor. This is an abbreviated combination of the words \"transconductance\" or \"transfer\", and \"varistor\". The device logically belongs in the varistor family, and has the transconductance or transfer impedance of a device having gain, so that this combination is descriptive.\n\nShockley was upset about the device being credited to Brattain and Bardeen, who he felt had built it \"behind his back\" to take the glory. Matters became worse when Bell Labs lawyers found that some of Shockley's own writings on the transistor were close enough to those of an earlier 1925 patent by Julius Edgar Lilienfeld that they thought it best that his name be left off the patent application.\n\nShockley was incensed, and decided to demonstrate who was the real brains of the operation. A few months later he invented an entirely new, considerably more robust, type of transistor with a layer or 'sandwich' structure. This structure went on to be used for the vast majority of all transistors into the 1960s, and evolved into the bipolar junction transistor.\n\nWith the fragility problems solved, a remaining problem was purity. Making germanium of the required purity was proving to be a serious problem, and limited the yield of transistors that actually worked from a given batch of material. Germanium's sensitivity to temperature also limited its usefulness. Scientists theorized that silicon would be easier to fabricate, but few investigated this possibility. Gordon K. Teal was the first to develop a working silicon transistor, and his company, the nascent Texas Instruments, profited from its technological edge. From the late 1960s most transistors were silicon-based. Within a few years transistor-based products, most notably easily portable radios, were appearing on the market.\n\nThe static induction transistor, the first high frequency transistor, was invented by Japanese engineers Jun-ichi Nishizawa and Y. Watanabe in 1950. It was the fastest transistor through to the 1980s.\n\nA major improvement in manufacturing yield came when a chemist advised the companies fabricating semiconductors to use distilled rather than tap water: calcium ions present in tap water were the cause of the poor yields. \"Zone melting\", a technique using a band of molten material moving through the crystal, further increased crystal purity.\n\n"}
{"id": "6870280", "url": "https://en.wikipedia.org/wiki?curid=6870280", "title": "Solar Cup", "text": "Solar Cup\n\nThe Solar Cup is an eco-boating competition in Temecula, California. Paid for by the Metropolitan Water District of Southern California and taking place on Lake Skinner, the competition revolves around high school teams constructing solar powered electric boats. Boats are built from identical kits of marine-caliber wood provided by the water district, and are generally 16 feet long, and weigh about 250 pounds. This competition is inspired by the Solar Splash competition at the university level.\n\nSolar Cup begins with sign up in December, and includes a boat-building event and several workshops focusing on technical aspects such as drive trains, electrical systems, solar power collection and steering systems. Teams are also required to meet deadlines for submitting illustrated reports on these systems. In 2007 the competition was split between \"veteran\" (returning) and \"rookie\" (new) teams. The competition is a scored event, divided up into several categories: Technical Reports, Workshops, Visual Displays, Qualifying Times, Endurance Distance, and Sprint Times. Combined, all of these categories add up for a 1000 maximum possible points. Overall, 41 teams competed in 2007, and 36 teams competed in 2010.\n"}
{"id": "2667760", "url": "https://en.wikipedia.org/wiki?curid=2667760", "title": "Stockbridge damper", "text": "Stockbridge damper\n\nA Stockbridge damper is a tuned mass damper used to suppress wind-induced vibrations on slender structures such as overhead power lines and long cantilevered signs. The dumbbell-shaped device consists of two masses at the ends of a short length of cable or flexible rod, which is clamped at its middle to the main cable. The damper is designed to dissipate the energy of oscillations in the main cable to an acceptable level. Its distinctive shape gives it the nickname \"dog-bone damper\".\n\nWind can generate three major modes of oscillation in suspended cables:\n\nThe Stockbridge damper targets oscillations due to aeolian vibration; it is less effective outside this amplitude and frequency range. Aeolian vibration occurs in the vertical plane and is caused by alternating shedding of vortices on the leeward side of the cable. A steady but moderate wind can induce a standing wave pattern on the line consisting of several wavelengths per span. Aeolian vibration causes damaging stress fatigue to the cable and represents the principal cause of failure of conductor strands. The ends of a power line span, where it is clamped to the transmission towers, are at most risk. The effect becomes more pronounced with increased cable tension, as its natural self-damping is reduced.\n\nThe Stockbridge damper was invented in the 1920s by George H. Stockbridge, who was an engineer for Southern California Edison. Stockbridge obtained US patent 1675391 on 3 July 1928 for a \"vibration damper\". His patent described three means of damping vibrations on lines: a sack of metal punchings tied to the line; a short length of cable clamped parallel to the main cable; and a short (30 in, 75 cm) cable with a concrete mass fixed at each end. This last device developed into the widely used Stockbridge damper.\n\nVibrations in the main cable were passed down through the clamp and into the shorter damper, or \"messenger\", cable. This would flex and cause the symmetrically-placed concrete blocks at its ends to oscillate. Careful choice of the mass of the blocks, and the stiffness and length of the damper cable would match the mechanical impedance of the damper to that of the line, and greatly attenuate oscillation of the main cable. Since Stockbridge dampers were economical, effective and easy to install, they became used routinely on overhead lines. Live-line working using hot stick tools meant it was possible to retrofit dampers to lines while energised.\n\nModern designs use metal bell-shaped weights rather than Stockbridge's concrete blocks. The bell is hollow and the damper cable is fixed internally to the distal end, which permits relative motion between the cable and damping weights. To provide for greater freedom of motion, the weights may be partially slotted in the vertical plane, allowing the cable to travel outside the confines of the bell. In some installations, the weights are unequal, allowing damping over a greater frequency range. More complex designs use weights with asymmetric mass distribution, which enables the damper to oscillate in several different frequency modes.\n\nThe most vulnerable section of the cable is where it is clamped to the end of an insulator string, so dampers are typically installed at the nearest anti-nodes (points of maximum displacement) either side of the clamp. There are thus normally two dampers per span, though more can be installed if necessary on longer spans.\n\nOverhead transmission lines form a catenary for which vibration is predominately in the vertical plane. When more than one plane of vibration is anticipated, Stockbridge dampers may be mounted at right angles to each other. This is common when the cable runs in a vertical or off-horizontal plane, for example in cable-stayed bridges or radio mast guy-wires.\n\n\n"}
{"id": "267484", "url": "https://en.wikipedia.org/wiki?curid=267484", "title": "Synthetic geometry", "text": "Synthetic geometry\n\nSynthetic geometry (sometimes referred to as axiomatic or even pure geometry) is the study of geometry without the use of coordinates or formulas. It relies on the axiomatic method and the tools directly related to them, that is, compass and straightedge, to draw conclusions and solve problems. \n\nOnly after the introduction of coordinate methods was there a reason to introduce the term \"synthetic geometry\" to distinguish this approach to geometry from other approaches.\nOther approaches to geometry are embodied in analytic and algebraic geometries, where one would use analysis and algebraic techniques to obtain geometric results.\n\nAccording to Felix Klein,\n\nSynthetic geometry is that which studies figures as such, without recourse to formulas, whereas analytic geometry consistently makes use of such formulas as can be written down after the adoption of an appropriate system of coordinates.\nGeometry, as presented by Euclid in the \"Elements\", is the quintessential example of the use of the synthetic method. It was the favoured method of Isaac Newton for the solution of geometric problems. \nSynthetic methods were most prominent during the 19th century when geometers rejected coordinate methods in establishing the foundations of projective geometry and non-Euclidean geometries. For example the geometer Jakob Steiner (1796 – 1863) hated analytic geometry, and always gave preference to synthetic methods.\n\nThe process of logical synthesis begins with some arbitrary but definite starting point. This starting point is the introduction of primitive notions or primitives and axioms about these primitives:\n\nFrom a given set of axioms, synthesis proceeds as a carefully constructed logical argument. When a significant result is proved rigorously, it becomes a theorem.\n\nThere is no fixed axiom set for geometry, as more than one consistent set can be chosen. Each such set may lead to a different geometry, while there are also examples of different sets giving the same geometry. With this plethora of possibilities, it is no longer appropriate to speak of \"geometry\" in the singular.\n\nHistorically, Euclid's parallel postulate has turned out to be independent of the other axioms. Simply discarding it gives absolute geometry, while negating it yields hyperbolic geometry. Other consistent axiom sets can yield other geometries, such as projective, elliptic, spherical or affine geometry.\n\nAxioms of continuity and \"betweeness\" are also optional, for example, discrete geometries may be created by discarding or modifying them.\n\nFollowing the Erlangen program of Klein, the nature of any given geometry can be seen as the connection between symmetry and the content of the propositions, rather than the style of development.\n\nEuclid's original treatment remained unchallenged for over two thousand years, until the simultaneous discoveries of the non-Euclidean geometries by Gauss, Bolyai, Lobachevsky and Riemann in the 19th century led mathematicians to question Euclid's underlying assumptions.\n\nOne of the early French analysts summarized synthetic geometry this way:\n\nThe heyday of synthetic geometry can be considered to have been the 19th century, when analytic methods based on coordinates and calculus were ignored by some geometers such as Jakob Steiner, in favor of a purely synthetic development of projective geometry. For example, the treatment of the projective plane starting from axioms of incidence is actually a broader theory (with more models) than is found by starting with a vector space of dimension three. Projective geometry has in fact the simplest and most elegant synthetic expression of any geometry.\n\nIn his Erlangen program, Felix Klein played down the tension between synthetic and analytic methods:\n\nThe close axiomatic study of Euclidean geometry led to the construction of the Lambert quadrilateral and the Saccheri quadrilateral. These structures introduced the field of non-Euclidean geometry where Euclid's parallel axiom is denied. Gauss, Bolyai and Lobachevski independently constructed hyperbolic geometry, where parallel lines have an angle of parallelism that depends on their separation. This study became widely accessible through the Poincaré disc model where motions are given by Möbius transformations. Similarly, Riemann, a student of Gauss's, constructed Riemannian geometry, of which elliptic geometry is a particular case.\n\nAnother example concerns inversive geometry as advanced by Ludwig Immanuel Magnus, which can be considered synthetic in spirit. The closely related operation of reciprocation expresses analysis of the plane.\n\nKarl von Staudt showed that algebraic axioms, such as commutativity and associativity of addition and multiplication, were in fact consequences of incidence of lines in geometric configurations. David Hilbert showed that the Desargues configuration played a special role. Further work was done by Ruth Moufang and her students. The concepts have been one of the motivators of incidence geometry.\n\nWhen parallel lines are taken as primary, synthesis produces affine geometry. Though Euclidean geometry is both an affine and metric geometry, in general affine spaces may be missing a metric. The extra flexibility thus afforded makes affine geometry appropriate for the study of spacetime, as discussed in the history of affine geometry.\n\nIn 1955 Herbert Busemann and Paul J. Kelley sounded a nostalgic note for synthetic geometry:\n\nFor example, college studies now include linear algebra, topology, and graph theory where the subject is developed from first principles, and propositions are deduced by elementary proofs.\n\nToday's student of geometry has axioms other than Euclid's available: see Hilbert's axioms and Tarski's axioms.\n\nErnst Kötter published a (German) report in 1901 on \"The development of synthetic geometry from Monge to Staudt (1847)\";\n\nSynthetic proofs of geometric theorems make use of auxiliary constructs (such as helping lines) and concepts such as equality of sides or angles and similarity and congruence of triangles. Examples of such proofs can be found in the articles Butterfly theorem, Angle bisector theorem, Apollonius' theorem, British flag theorem, Ceva's theorem, Equal incircles theorem, Geometric mean theorem, Heron's formula, Isosceles triangle theorem, Law of cosines, and others that are linked to .\n\nIn conjunction with computational geometry, a computational synthetic geometry has been founded, having close connection, for example, with matroid theory. Synthetic differential geometry is an application of topos theory to the foundations of differentiable manifold theory.\n\n\n"}
{"id": "18877738", "url": "https://en.wikipedia.org/wiki?curid=18877738", "title": "Tire-derived fuel", "text": "Tire-derived fuel\n\nTire-derived fuel (TDF) is composed of shredded scrap tires. Tires may be mixed with coal or other fuels, such as wood or chemical wastes, to be burned in concrete kilns, power plants, or paper mills. An EPA test program concluded that, with the exception of zinc emissions, potential emissions from TDF are not expected to be very much different from other conventional fossil fuels, as long as combustion occurs in a well-designed, well-operated\nand well-maintained combustion device.\n\nHistorically, there has not been any volume use for scrap tires other than burning that has been able to keep up with the volume of waste generated yearly. Tires produce the same energy as petroleum and approximately 25% more energy than coal. Burning tires is lower on the hierarchy of reducing waste than recycling, but it is better than placing the tire waste in a landfill or dump, where there is a possibility for uncontrolled tire fires or the harboring of disease vectors such as mosquitoes. \nTire Derived Fuel is an interim solution to the scrap tire waste problem. Advances in tire recycling technology might one day provide a solution other than burning by reusing tire derived material in high volume applications.\n\nTire derived fuel is usually consumed in the form of shredded or chipped material with most of the metal wire from the tire's steel belts removed. The analytical properties of this refined material are published in TDF Produced From Scrap Tires with 96+% Wire Removed\n\nTires are typically composed of about 1 to 1.5% Zinc oxide, which is a well known component used in the manufacture of tires and is also toxic to aquatic and plant life. The chlorine content in tires is due primarily to the chlorinated butyl rubber liner that slows the leak rate of air. The Rubber Manufacturers Association (RMA) is a very good source for compositional data and other information on tires.\nThe use of TDF for heat production is controversial due to the possibility for toxin production. Reportedly, polychlorinated dibenzodioxins and furans are produced during the combustion process and there is supportive evidence to suggest that this is true under some incineration conditions. Other toxins such as NOx, SOx and heavy metals are also produced, though whether these levels of toxins are higher or lower than conventional coal and oil fired incinerators is not clear.\n\nOn one hand, some argue that it is better to use the energy stored in a tire than to put it in a landfill, in line with the waste hierarchy. On the other, it is difficult to justify introducing toxins into the atmosphere, and much energy can be saved by recycling the tires so that new ones do not need to be remanufactured from raw materials.\n\nWhile environmental controversy surrounding use of this fuel is wide and varied, the greatest supported evidence of toxicity comes from the presence of dioxins and furans in the flue gases. Zinc has also been found to dissolve into storm water, from shredded rubber, at acutely toxic levels for aquatic life and plants.\n\nA study of dioxin and furan content of stack gasses at a variety of cement mills, paper mills, boilers, and power plants conducted in the 1990s shows a wide and inconsistent variation in dioxin and furan output when fueled partially by TDF as compared to the same facilities powered by only coal. Some facilities added as little as 4% TDF and experienced as much as a 4,140% increase in dioxin and furan emsissions. Other facilities added as much as 30% TDF and experienced dioxin and furan emissions increases of only as much as 58%. Still other facilities used as much as 8% TDF and experienced a decrease of as much as 83% of dioxin and furan emissions. One facility conducted four tests with two tests resulting in decreased emissions and two resulting in increased emissions. Another facility also conducted four tests and had widely varying increases in emissions.\n\nA 2004 study of Tire rubber use in energy generation,<ref name=\"Soil, Water, and Air Environmental Impact from Tire Rubber/Coal Fluidized-Bed Cocombustion\">http://viewer.zoho.com/docs/sa7Pp </ref> deeply studies the environmental impact on soil, water, and air from combustion of waste rubber (TDF).\n\nAlvarez research shows that huge polyaromatic emissions are generated from combustion of tire rubber, at a minimum, 2 orders of magnitude higher than coal alone.\n\nThe study concludes with, \"atmospheric contamination dramatically increases when tire rubber is used as the fuel. Other different combustion variables compared to the ones used for coal combustion should be used to avoid atmospheric contamination by toxic, mutagenic, and carcinogenic pollutants, as well as hot- gas cleaning systems and COx capture systems.\"\n"}
{"id": "55582706", "url": "https://en.wikipedia.org/wiki?curid=55582706", "title": "Vandellòs I Nuclear Accident", "text": "Vandellòs I Nuclear Accident\n\nVandellòs I Nuclear Accident was a fire that caused an interruption of the refrigeration system in the nuclear reactor of Vandellòs, Catalonia (Spain) on 19 October 1989.\n\nAt the end of the Franco regime, France sold Spain a UNGG reactor. This already obsolete energy technology was installed because as a by-product it provided plutonium that could be used to manufacture atomic bombs. Vandellòs I Nuclear Power Plant was inaugurated in 1972 when there were only two operational nuclear power plants in Spain: Garoña and Zorita. Seventeen years after opening, the rudimentary technology produced a mechanical failure in the gas turbine which caused a fire. The cabling of the plant was not fireproof and the control computer and the refrigeration system would also receive damage.\n\nThe core meltdown was avoided due to the intervention of the Corps of Firefighters of Catalonia and the plant technicians, which were able to extinguish the fire and turn off the nuclear reactor manually. It continues to be one of the biggest nuclear accidents in Western Europe, classified as a serious incident according to the International Nuclear Event Scale. As a consequence, the plant would be decommissioned, due to the damage in the systems, and new safety protocols were introduced, because the old ones proved to be inefficient.\n\n\n"}
{"id": "32813", "url": "https://en.wikipedia.org/wiki?curid=32813", "title": "Vibranium", "text": "Vibranium\n\nVibranium () is a fictional metal appearing in American comic books published by Marvel Comics. This fictional metal is noted for its uncanny ability to leverage thermodynamics in absorbing, storing, and releasing kinetic energy in a controlled manner. Vibranium is associated with Black Panther, as his suit is made from vibranium and is found in his native homeland of Wakanda. Antarctic Vibranium or Anti-Metal is created by artificial means, in contrast to natural, or Wakandan, vibranium. Vibranium is also commonly known as one of the materials used to construct Captain America's shield.\n\nVibranium first appeared in \"Daredevil\" #13 (February 1966), which was written by Stan Lee and illustrated by John Romita. Here, vibranium was seen to be an unusual metallic element with decidedly strange properties. Since that point in Marvel continuity, it has been established that there are a few variations of this element which can be found in isolated regions all around the world. The variation first introduced in \"Daredevil\" #13 eventually became known as Anti-Metal. This variation's unique attribute is that it can cut through any known metal. In the Marvel Universe, Anti-Metal can traditionally be found only in Antarctica. Later in \"Fantastic Four\" #53 (August 1966), by Stan Lee and Jack Kirby, a new variation of vibranium was introduced in the isolated nation of Wakanda. This variation had the unique attribute of being able to absorb sound. This is the variation which is most often identified in continuity as simply \"vibranium\".\n\nIn the Marvel Universe, vibranium was first deposited on Earth by a meteorite 10,000 years ago. In the comics, the first documented discovery of vibranium was during a human expedition to Antarctica. This particular isotope of vibranium was called \"Anti-Metal\" due to its property of dissolving other metals.\n\nIn the comics, a different variety of vibranium found in Wakanda absorbs sound waves and other vibrations, including kinetic energy. Absorbing sound waves, vibrations, and kinetic energy makes this metal stronger. It was discovered by the Wakandan ancestors. To protect this resource, they concealed their country from the outside world. T'Chaka funded his country's education by occasionally selling off minuscule quantities of the metal. As a result, Wakanda is one of the world's most technologically advanced nations.\nIn the comics, during the early 1940s, a small amount of Wakandan vibranium came into the possession of the scientist Myron MacLain. He tried to combine vibranium with iron to form a new tank armor, but was unable to fuse the elements. One morning, he found that the two materials had bonded on their own in an unknown manner. The ultra-resilient alloy was used to create Captain America's shield. MacLain worked for decades (without success) to duplicate the accident. However, during an experiment in the 1960s, he developed the virtually indestructible metal adamantium.\nIn the comics, when T'Challa became king of Wakanda, he strove to end his country's isolation from the rest of the world. Making the existence of vibranium known to the outside world around the mid-1980s, he sold small quantities of it to foreigners who, he believed, would not use it to harmful ends. T'Challa used the profits to enrich and modernize his nation.\nIn the comics, over the years, many have tried to obtain or affect the mound of vibranium at Wakanda, but for the most part Wakanda has kept it safe, and become quite powerful in the process.\n\nIn the comics, during their Secret Invasion of Earth, the Skrulls assumed the identity of S.H.I.E.L.D. agents and enslaved natives of the Savage Land to mine Anti-Metal. They also invaded Wakanda. The Wakandans successfully repelled the attack.\nIn the comics, when Wakanda was politically taken over by the xenophobic Desturi, they granted Doctor Doom access to the country's vibranium vaults. Fearing Doom would use it to amplify his mystical energies, T'Challa activated a failsafe he had developed that rendered all processed vibranium inert.\n\nIn the comics, the rumors about its alien origin had later been proven true, as while most of earth's vibranium had been all but liquidated, certain planetary systems carry an ample supply of the element deep within certain extraterrestrial biospheres, as was the case with a refugee planet that the Spartax Empire tried reclaiming during Captain Marvel's space travels. In the wake of the continuity-wide spanning reboot of the Marvel multiverse called \"Secret Wars: Battleworld\", vibranium's abundance in Wakanda and beyond has re-flourished to sizable quantities, mutant criminal Vanisher was making off with and selling Wakandan vibranium in the black market in New York City.\n\nIn the Marvel Comics Universe, vibranium is a rare metallic substance of extraterrestrial origin. It exists in three forms:\n\nWakandan Vibranium is the most common variety, and is often referred to simply as \"vibranium\". It is a rare substance native only to the fictional small African nation of Wakanda.\n\nThe Wakandan isotope possesses the ability to absorb all vibrations in the vicinity as well as kinetic energy directed at it. The energy absorbed is stored within the bonds between the molecules that make up the substance. As a result, kinetic energy is dissipated within the bonds instead. There are limits to the capacity of the energy that can be stored, and although the exact limitations are not yet known, there have been a few examples. One such instance was when the oil conglomerate Roxxon discovered that a small island in the South Atlantic had a foundation composed of vibranium. Due to this, Roxxon found it necessary to destroy the island and so blew it up with bombs. Unable to absorb the force of the explosions, the vibranium was destroyed, but it did succeed in entirely absorbing the sound made by the explosion, preventing damage to the surrounding area.\n\nThis variety of vibranium is a powerful mutagen. Vibranium exposure led to the mutation of many Wakandan natives. Its radiation has also permeated much of Wakanda's flora and fauna, including the Heart-Shaped Herb eaten by members of the Black Panther Tribe and the flesh of the White Gorilla eaten by the members of the White Gorilla Tribe. Both give superhuman abilities to whoever eats them.\n\nIt is also believed to dramatically enhance mystical energies.\n\nBetter known as Anti-Metal, this isotope is native to the Savage Land. The variation produces vibrations of a specific wavelength that break down the molecular bonds in other metals, causing them to liquefy. It was first discovered by the famous explorer named Robert Plunder; the father of Kevin and Parnival Plunder during his initial jaunt in the primordial environment untouched to time. \n\nHis more villainous son, who went on to become The Plunderer, would seek to find his father who dubbed the Plunder Stone and all recorded knowledge of the family relic only accessible through his inherited medallion fashioned from the stone, to pillage and terrorize the world by liquidating all armaments used against him. Wakandan vibranium is able to become an artificial and unstable form of the Anti-Metal variety of vibranium through certain particle bombardments on it. If huge quantities of Anti-Metal are gathered together, the vibrations increase exponentially. One such case occurred with the most stable reactionary transformation through a Cyclotron using gigawatt baryon beam bombardment. \n\nMuch like natural Wakandan vibranium, Antarctic vibranium also has the mutating effect of over exposure to its unique energy signature. Causing one such individual who adorned an Anti-Metal full body suit for protection against Moon Knight and his arsenal to emit the same metal melting radiation he had intended to weaponize for his own purposes.\n\nThere are at least two forms of man made Vibranium created outside of Wakanda through various means. The first variant is called NuForm which featured in the Vibranium Vendetta event of Marvel Comics imprint, created by the Roxxon Corporation for unknown reasons, It was an alchemic blend made through the combination of organic and mineral elements, the properties of this Vibranium brand mimicked natural Vibranium but had the tendency to degrade into Antarctic Vibranium unless tempered through use of Microwave bombardment, and even then that was only a temporary solution. \n\nThe second is a particularly dangerous artificial brand created at Horizon Labs by Professor Sajani Jaffrey called Reverbium. \n\nUnlike standard Vibranium this faux material rapidly amplifies and projects sound and vibratory energy in pulse waves which would only strengthen over time before violently detonating. Max Modell, head scientist of Horizon at the time, ordered its immediate dissolution given how dangerous it was but Sajani held on to some of it without her team's knowledge. Reverbium would turn up again in the hands of A.I.M Scientists under the influence of Klaw, who'd used it in a new scheme against Black Panther and the nation of Wakanda; the faux vibranium also had the effect of enhancing his sonic powers to unknown degrees.\n\nWhen a small sub-molecular imperfection was introduced into Captain America's shield, each impact over the years spread to neighboring molecules. It grew until the molecular bonds of the shield were completely broken down, shattering the shield. The shattering effect continued to spread to other vibranium, unconnected to the shield. This created a vibranium \"cancer\", a shock wave propagating throughout the world. It violently detonated any vibranium it found, from mineral deposits to components of ships or equipment. The shock wave was traveling to the \"Great Vibranium Mound\" in Wakanda, where the resulting explosion could destroy the world. With the unwitting aid of the villain Klaw, Captain America was able to stop the cancer and restore his shield.\n\nVibranium appears frequently in the Marvel Universe.\n\n\n\n\n\nIn the computer game \"\" the nanite artificial intelligence known as \"the Fold\" attempts to harvest vibranium in Wakanda in order to construct communication towers around the world, spreading its control signal globally. While the heroes of the game manage to thwart the invasion, they are too late to prevent the construction of enough towers to make the Fold a worldwide threat.\n\nIn 2016, Hyperloop Transportation Technologies developed a real-world smart composite material named Vibranium. The lightweight carbon fiber material for the Hyperloop pods is reported to provide the passengers double protection against damage to the exterior. The company says that its Vibranium is eight times lighter than aluminum and 10 times stronger than steel alternatives. The smart material can transmit critical information regarding temperature, stability, integrity and more, wirelessly and virtually instantly.\n\n\n"}
{"id": "7269782", "url": "https://en.wikipedia.org/wiki?curid=7269782", "title": "Waste disposal authority", "text": "Waste disposal authority\n\nWaste disposal authorities (WDA) were established in the United Kingdom following the Environmental Protection Act 1990. WDAs are in charge of the use of funds from Council Tax to facilitate the disposal of municipal waste. WDAs must manage waste which is collected by local councils. In the case of unitary authorities waste disposal authorities are the same as the waste collection authority. WDAs are responsible for developing and implementing plans to deal with municipal waste.\n\n"}
{"id": "30468790", "url": "https://en.wikipedia.org/wiki?curid=30468790", "title": "Wentworth Wooden Puzzles", "text": "Wentworth Wooden Puzzles\n\nThe Wentworth Wooden Jigsaw Company (also known as Wentworth Wooden Puzzles) manufactures jigsaw puzzles with 'whimsical' shaped pieces reflecting the theme of the image portrayed on the puzzle. It was founded in 1994 by Kevin Wentworth Preston and is based in the village of Pinkney near Malmesbury, Wiltshire, an area of England known as the Cotswolds.\n\nThe venture was established on an existing dairy farm which was forced to diversify into other sources of income when milk production became uneconomical to sustain. Some of the old buildings were converted into industrial use, and the farm became an industrial estate housing many other traders, as well as the new puzzle enterprise.\n\nTraditionally jigsaws are manufactured using a thin flexible cutting blade driven by a motor known as a bandsaw. This method of cutting thin wood requires a degree of manual dexterity and patience to avoid spoiling the work. An alternative solution to this labour-intensive method of cutting intricate shapes in wood was required using modern technology solutions. The advent of the commercial medium-power Laser device has enabled many industries to use this tool to cut many different types of material speedily. The puzzle-manufacturing process uses a laser-cutting method invented and perfected by the founder, Kevin Wentworth Presto, in 1994.\n\nWentworth production can now focus on the quality of manufacture and design innovation that this new tooling provides. The high-speed production technique allows the small company to supply in excess of 150,000 puzzles a year to destinations in over 35 countries throughout the world.\n\nThe design team produces each cutout style individually, most of the designs are unique \"whimsy\" jigsaw shapes. Whimsies are specially shaped pieces cut into puzzles \"on a whim\" by Victorian-era hand cutters, an era when jigsaw puzzles became a popular pastime. Wentworth retained this older style of manufacture, and is one of the remaining companies still producing puzzles using these Victorian techniques.\n\nThe ‘Whimsy’ laser-cut wooden puzzles feature unique, individual, “whimsical” cut-out shapes that reflect the theme of the image used on the face of the puzzle. All puzzles are supplied in a cotton draw-string bag within a lidded box. These wooden puzzles are cut from 3mm thick wooden boards (as opposed to softer cardboard) to ensure they will survive the rigours of use for a very long time. Puzzles are supplied to the customer with the option of an image of the puzzle's subject matter printed on the box. With no reference image there is the added difficulty of assembling the pieces into the correct pattern, and the element of surprise concerning the subject matter when the puzzle's image is reassembled.\n\n\nAll traditional puzzles include the unique whimsy pieces. Common sizes included are 100, 250, 500, 1,000 and 1,500 pieces.\n\nThe ability to use a photograph or image design is a feature that Wentworth's puzzles make available in all the various sizes. Text may be added at the image creation process to include such messages as 'Happy Birthday' and 'Happy Anniversary' etc.\n\nThe Tessellation puzzles range use jigsaw pieces in which all the pieces are almost all identical in pattern. Some utilise pieces shaped like animals, such as deer. Other subjects include repetitive plant shapes such as ivy and holly cuts.\n\nPuzzles shapes and styles are designed to suit all ages and ability, including images specially suited for children, which are traditionally constructed with larger, more manageable pieces with simpler pattern and shape design. The company was awarded recognition for its production in this section of the market in 2008.\n\n\n"}
{"id": "8737154", "url": "https://en.wikipedia.org/wiki?curid=8737154", "title": "Witkar", "text": "Witkar\n\nThe Witkar (Dutch, literally \"white car\" or \"white cart\") was one of the first technology-based carsharing projects in the world. It is the invention of Dutch social inventor and politician Luud Schimmelpennink, an industrial designer and at the time Amsterdam city councillor. The project was opened by minister Irene Vorrink on 21 March 1974.\n\nThe first actual Witkar project took place in Amsterdam between 1974 and 1986. While it provided daily service for more than 4,000 registered users over those years, the project never got beyond the limited demonstration phase due to a lack of support by government.\n\nThe system was initially conceived in 1969 by Schimmelpennink in order to reduce traffic in central Amsterdam, but it failed to win the support of the City Council and was spun off to be developed by a specially formed co-operative society. The co-operative managed to raise loans of US$ 250,000 for the first phase though 1974. This included the design and construction of the vehicles, the purchase of a PDP-11 mini-computer for the central control system, development of the control software, and construction of the first 35 cars and five stations. A further 10 stations were scheduled for operation by the end of 1976, by which time the fleet was to have been extended to 100 vehicles. The ultimate target was 150 stations and 1,000 vehicles, but as a result of lack of government support this never happened.\n\nThe core of the Witkar concept is that of people sharing small environmentally-acceptable vehicles specially designed for urban use. It was seen by Schimmelpennink and his team as a way to wean people away from conventional car ownership and use, on the grounds that such vehicles have no place in densely settled cities.\n\nThese were specially designed electric vehicles. They had two seats, and offered little luxury. They were very easy to recognize. The vehicles were located around the city in pods. In Amsterdam 35 of these cars were available to hire from five stations in the city centre. The original system was designed for fully automatic control, including direct debit of hirers' accounts at the Amsterdam Savings Bank.\n\nThe idea was that any person could take a Witkar, and leave it at the Witkar station closest to their destination. The initiators thought that the flow of traffic to and from these stations would allow for an even spread of Witkarren (the Dutch plural of Witkar). The system actually operated. The problems were several: the vehicles needed long recharging times, so many were needed to have some on the road all the time. The biggest problem was that the flow of traffic was generally in one direction. As a result of this, some stations were always full, and others always empty. To correct this by moving Witkarren would have caused a lot of extra traffic.\n\nAs of this date (January 2007), Schimmelpennink and his associates are planning a new Witkar program for Amsterdam, where he has recently (March 2006) been re-elected to the Municipal Council.\n\n"}
