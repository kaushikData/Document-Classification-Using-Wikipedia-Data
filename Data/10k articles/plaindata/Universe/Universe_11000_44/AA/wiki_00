{"id": "33062705", "url": "https://en.wikipedia.org/wiki?curid=33062705", "title": "2011 Nairobi pipeline fire", "text": "2011 Nairobi pipeline fire\n\nThe 2011 Nairobi pipeline fire was caused by an explosion secondary to a fuel spill in the Kenyan capital Nairobi on 12 September 2011. Approximately 100 people were killed in the fire and at least 116 others were hospitalized with varying degrees of burns. The incident was not the first such pipeline accident in Kenya, with the Molo fire of 2009 resulting in at least 133 fatalities and hundreds more injured.\n\nA fuel tank, located in the industrial Lunga Lunga area of Nairobi and part of a pipeline system operated by the state owned Kenya Pipeline Company (KPC), had sprung a leak. People in the adjacent densely populated shanty town of Sinai had started to collect leaking fuel when at about 10 a.m. a massive explosion occurred at the scene. Fire spread to the Sinai area.\n\nThe cause of the explosion has not yet been determined but some reports indicate that the fire might have started from a discarded cigarette or when the wind changed, bringing embers from nearby garbage fires.\n\nEnergy Minister Kiraitu Murungi is reported as saying that the disaster began when a pipeline valve failed under pressure allowing the oil to leak into the sewer. Selest Kilinda, the managing director of KPC, is reported to have said the spill occurred from two pipelines, and that engineers had already depressurised the Sinai pipeline but not in time to prevent fuel leaking into the sewer.\n\nEarly police estimates have the number of fatalities to be above one hundred; in addition, at least 116 other people were hospitalized with burn injuries. The exact death toll remains uncertain due to some bodies being badly charred or lost in the murky waters of a nearby river.\nKenya's Red Cross Disaster Risk Reduction Officer said that the Red Cross would counsel the victims and also would attempt to reconcile the casualty figures with those reported missing. He also reported that most bodies taken to the mortuary were burnt beyond recognition and would require DNA tests to confirm their identities.\nIn November 2011, The Kenya Pipeline Company funded the delivery to the Ministry of Public Health and Sanitation a computer and software system to facilitate forensic DNA identification of victims.\nThe system, called M-FISys (pronounced like \"emphasis,\" an acronym for the Mass-Fatality Identification System), was developed to identify victims of the World Trade Center Disaster of September 11, 2001.\n\nCity hospitals were hard pressed by the surge of the need for care provisions, food and a strained medicare staff complement.\nThe Kenyatta National Hospital has only 22 burn unit beds and considers any more than 60 casualties as a 'disaster', requiring them to put disaster plans into action. At least 112 people were admitted with burns, many critical or severe. The long-term treatment required for burns patients means that extra tents have been erected for blood donations.\nThe nearer Mater Hospital admitted three casualties with less than 30% burns into the normal ward and one other casualty with 80–90% burns into the intensive care unit.\n\nNeither the managing director of the KPC, which operates the pipeline, nor the energy minister Kiraitu Murungi have given any indication of accepting responsibility. Kiraitu Murungi initially said that the KPC would compensate the victims, but later the KPC stated it would not do so as it was \"not responsible\".\n\nIn 2008 the KPC had issued an eviction order to nearby residents, but they refused to leave. In response to protests by students, an inter-ministerial committee was tasked with gathering names to arrange relocation when funds became available. KPC sent representatives to inform the residents of the danger and to make sure holes were not dug.\n\nPrime minister Raila Odinga and vice-president Kalonzo Musyoka have visited the scene and various hospitals to console injured victims and to condole bereaved families. President Mwai Kibaki visited the main Kenyatta National Hospital to empathize with the injured.\nThe secretary-general of the United Nations, Ban Ki Moon, expressed sorrow and sympathy for the victims, wishing a full and speedy recovery to the survivors, while the United States ambassador to Kenya, Scott Gration, lauded the rescue workers and the personal heroism of the locals.\nAmnesty International-Kenya said that the failure to relocate people puts the majority of the blame on government officials.\n\nThe National Environment Management Authority (NEMA) said it will act against the KPC for failing to enforce EMCA 1999—and suggests that if the required spill containment measures were in place at the facility the oil would not have run off into the drains. NEMA dismissed KPC claims that they had acted sufficiently, saying they had not received the Environmental audit that is obligatory under the 2003 Environmental Impact Assessment and Environmental Audit Regulations.\nThe slum has been in that place for approximately 20 years despite the requirement for KPC to keep those areas clear of settlement.\nNEMA said it would also require KPC to deal with the pollution in the environment, particularly regarding the flora and fauna along the Ngong River into which the storm drain flows.\n\nIn 2009 journalist John Ngirachu wrote for the local newspaper \"Daily Nation\" and reported that the slums in Sinai being located so near to the pipeline were a disaster waiting to happen. The permanent secretary to the Ministry of Energy, Patrick Nyoike, had asked the KPC to refurbish the pipelines but it was reported that the Ministry of Finance declined.\n"}
{"id": "7752786", "url": "https://en.wikipedia.org/wiki?curid=7752786", "title": "APNPP", "text": "APNPP\n\nThe APNPP, an acronym of \"l’association des pays non producteurs de pétrole\" (in English: the \"Pan-African Non-Petroleum Producers Association\"), is an association of 15 African nations that signed a treaty in July, 2006.\n\nTheir stated aim is to work together to promote biofuel production and reduce the effects of high oil prices.\n\nThe APNPP, which was first proposed by Abdoulaye Wade, is being led by the Ministry of Energy and Mines of Senegal. At the moment, the acting head is Mr Madické Niang. \n\n\n"}
{"id": "39022925", "url": "https://en.wikipedia.org/wiki?curid=39022925", "title": "Aluminium powder", "text": "Aluminium powder\n\nAluminum powder is powdered aluminum. This was originally produced by mechanical means using a stamp mill to create flakes. Subsequently, a process of spraying molten aluminum to create a powder of droplets was developed by E. J. Hall in the 1920s. The resulting powder might then be processed further in a ball mill to flatten it into flakes for use as a coating or pigment. Aluminum powder is non-toxic and is not harmful unless injected directly in a major blood vessel such as the aorta. Aluminum powder, if breathed in, is not particularly harmful and will only cause minor irritation. The melting point of aluminum powder is 660 °C.\n\nDepending on the usage, the powder is either coated or uncoated.\n\n\n"}
{"id": "57832784", "url": "https://en.wikipedia.org/wiki?curid=57832784", "title": "Amplus Energy Solutions", "text": "Amplus Energy Solutions\n\nAmplus Energy Solutions is an Indian company that provides solar energy solutions to industrial and commercial consumers. The company works in the area of rooftop solar power plants. Amplus Energy Solutions function on the BOOT/OPEX/Resco business model making it its unique selling proposition. The company falls under the OPEX project development category. \n\nThe Indian government is keen towards fostering a sustainable growth for the nation. In a move towards this objective, the government has set a goal to reach its target of 100 GW of solar energy capacity by 2022. Players like Tata Power Solar systems, EMMVEE, Kotak Urja Pvt. Ltd. are some of the other contributors towards the goal. \n\nThe company was incorporated in 2013 by Mr. Guru Inder Mohan Singh and Mr. Sanjeev Aggarwal. It commissioned its first rooftop project for Raisoni Group of Institutions in Maharashtra. Since then the company has entered into various private and public agreements for their solar solutions.\n\nBelow are some of the awards that Amplus Energy Solutions has been conferred with:\n\n\n\nIn November 2016, Amplus Energy Solutions became the first Indian company to win the bid to install 14.5 MW of solar rooftop plants by offering the cheapest solar power rates to the users, at a record low price in three states - Uttarakhand, Himachal Pradesh, and Puducherry. \n\nAmplus Energy Solutions’ into an agreement with the Indian Army to enlighten the Gurez Valley in Jammu & Kashmir with their Solar Lamps.\n\nIn February 2018, during the Uttar Pradesh Investors Summit, the company entered into an agreement with the Uttar Pradesh government to set up a facility with 500 MW solar power capacity.\n\n"}
{"id": "634180", "url": "https://en.wikipedia.org/wiki?curid=634180", "title": "Avenue (archaeology)", "text": "Avenue (archaeology)\n\nBritish Archaeologists refine the general archaeological use of avenue to denote a long, parallel-sided strip of land, measuring up to about 30m in width, open at either end and with edges marked by stone or timber alignments and/or a low earth bank and ditch. The term is used for such features all over the British Isles but they are concentrated in the centre and south of England.\n\nMost are either short and straight (Type I, less than 800m long), or long and curving (Type II, up to 2.5 km). It has been noted that they often link stone circles with rivers. They are a common element to Bronze Age ritual landscapes.\n\nAvenues are identified through their earthworks or using aerial archaeology as their parallel side features can be seen stretching over some distance. In most examples, it is the association of the avenue with other contemporary monuments that provides diagnosis. Avenues differ from cursus monuments, in that the latter also have earthworks at their terminal ends and have no upright stone or timber alignments.\n\nAvenues are thought to have been ceremonial or processional paths and to be of early Bronze Age date. They seem to have been used to indicate the intended route of approach to a particular monument.\n\nExamples include the Stonehenge Avenue, the Beckhampton Avenue at Avebury, West Kennet Avenue and that at Thornborough.\n\n"}
{"id": "32508994", "url": "https://en.wikipedia.org/wiki?curid=32508994", "title": "Bheema lift irrigation project", "text": "Bheema lift irrigation project\n\nThe Bheema lift irrigation project or Rajiv Bheema Project is a lift irrigation canal project located in Mahbubnagar district in Telangana, India. Water is lifted at Panchdev Pahad from the back waters of Jurala Dam. Under the same project, another lift canal originates separately from the Ramanpahad balancing reservoir ( ) under Jurala left bank to irrigate the uplands.\n\nThe Bheema Lift irrigation project would irrigate over , would benefit 180 villages that fall in Makthal, Atmakur, Wanaparthy, and Kollapur Taluks of Mahabubnagar District utilising 20 TMC ()\nof water.\n\nThe lifts are proposed at two points.\nLift I is from the Krishna river from the point near Panchadevpad (V) in Atmakur taluk to irrigate an extent of . \nLift II is from existing Ookachettivagu Project pondage near Ramanpad (V) in Wanaparthy Taluk to irrigate an extent of .\n\nBenefitted Mandals:- The Mandals that would be benefited under the project are Magnur, Makthal, Narva, Atmakur, Chinnachintakunta, Wanaparthy, Peddamandadi, Pebbair, Pangal, Veepanagandla, Kollapur, Kothakota and Devarakadra of Mahabubnagar district.\n\n"}
{"id": "20903424", "url": "https://en.wikipedia.org/wiki?curid=20903424", "title": "Breathing", "text": "Breathing\n\nBreathing (or respiration, or ventilation) is the process of moving air into and out of the lungs to facilitate gas exchange with the internal environment, mostly by bringing in oxygen and flushing out carbon dioxide.\n\nAll aerobic creatures need oxygen for cellular respiration, which uses the oxygen to break down foods for energy and produces carbon dioxide as a waste product. Breathing, or \"external respiration\", brings air into the lungs where gas exchange takes place in the alveoli through diffusion. The body's circulatory system transports these gases to and from the cells, where \"cellular respiration\" takes place.\n\nThe breathing of all vertebrates with lungs consists of repetitive cycles of inhalation and exhalation through a highly branched system of tubes or airways which lead from the nose to the alveoli. The number of respiratory cycles per minute is the breathing or respiratory rate, and is one of the four primary vital signs of life. Under normal conditions the breathing depth and rate is automatically, and unconsciously, controlled by several homeostatic mechanisms which keep the partial pressures of carbon dioxide and oxygen in the arterial blood constant. Keeping the partial pressure of carbon dioxide in the arterial blood unchanged under a wide variety of physiological circumstances, contributes significantly to tight control of the pH of the extracellular fluids (ECF). Over-breathing (hyperventilation) and under-breathing (hypoventilation), which decrease and increase the arterial partial pressure of carbon dioxide respectively, cause a rise in the pH of ECF in the first case, and a lowering of the pH in the second. Both cause distressing symptoms.\n\nBreathing has other important functions. It provides a mechanism for speech, laughter and similar expressions of the emotions. It is also used for reflexes such as yawning, coughing and sneezing. Animals that cannot thermoregulate by perspiration, because they lack sufficient sweat glands, may lose heat by evaporation through panting.\n\nThe lungs are not capable of inflating themselves, and will expand only when there is an increase in the volume of the thoracic cavity. In humans, as in the other mammals, this is achieved primarily through the contraction of the diaphragm, but also by the contraction of the intercostal muscles which pull the rib cage upwards and outwards as shown in the diagrams on the left. During forceful inhalation (Figure on the right) the accessory muscles of inhalation, which connect the ribs and sternum to the cervical vertebrae and base of the skull, in many cases through an intermediary attachment to the clavicles, exaggerate the pump handle and bucket handle movements (see illustrations on the left), bringing about a greater change in the volume of the chest cavity. During exhalation (breathing out), at rest, all the muscles of inhalation relax, returning the chest and abdomen to a position called the “resting position”, which is determined by their anatomical elasticity. At this point the lungs contain the functional residual capacity of air, which, in the adult human, has a volume of about 2.5–3.0 liters.\n\nDuring heavy breathing (hyperpnea) as, for instance, during exercise, exhalation is brought about by relaxation of all the muscles of inhalation, (in the same way as at rest), but, in addition, the abdominal muscles, instead of being passive, now contract strongly causing the rib cage to be pulled downwards (front and sides). This not only decreases the size of the rib cage, but also pushes the abdominal organs upwards against the diaphragm which consequently bulges deeply into the thorax. The end-exhalatory lung volume is now less air than the resting \"functional residual capacity\". However, in a normal mammal, the lungs cannot be emptied completely. In an adult human there is always still at least one liter of residual air left in the lungs after maximum exhalation.\n\nDiaphragmatic breathing causes the abdomen to rhythmically bulge out and fall back. It is, therefore, often referred to as \"abdominal breathing\". These terms are often used interchangeably because they describe the same action.\n\nWhen the accessory muscles of inhalation are activated, especially during labored breathing, the clavicles are pulled upwards, as explained above. This external manifestation of the use of the accessory muscles of inhalation is sometimes referred to as clavicular breathing, seen especially during asthma attacks and in people with chronic obstructive pulmonary disease.\n\nUsually air is breathed in and out through the nose. The nasal cavities (between the nostrils and the pharynx) are quite narrow, firstly by being divided in two by the nasal septum, and secondly by lateral walls that have several longitudinal folds, or shelves, called\nnasal conchae, thus exposing a large area of nasal mucous membrane to the air as it is inhaled (and exhaled). This causes the inhaled air to take up moisture from the wet mucus, and warmth from the underlying blood vessels, so that the air is very nearly saturated with water vapor and is at almost body temperature by the time it reaches the larynx. Part of this moisture and heat is recaptured as the exhaled air moves out over the partially dried-out, cooled mucus in the nasal passages, during breathing out. The sticky mucus also traps much of the particulate matter that is breathed in, preventing it from reaching the lungs.\n\nThe anatomy of a typical mammalian respiratory system, below the structures normally listed among the \"upper airways\" (the nasal cavities, the pharynx, and larynx), is often described as a respiratory tree or tracheobronchial tree (figure on the left). Larger airways give rise to branches that are slightly narrower, but more numerous than the \"trunk\" airway that gives rise to the branches. The human respiratory tree may consist of, on average, 23 such branchings into progressively smaller airways, while the respiratory tree of the mouse has up to 13 such branchings. Proximal divisions (those closest to the top of the tree, such as the trachea and bronchi) function mainly to transmit air to the lower airways. Later divisions such as the respiratory bronchioles, alveolar ducts and alveoli are specialized for gas exchange.\n\nThe trachea and the first portions of the main bronchi are outside the lungs. The rest of the \"tree\" branches within the lungs, and ultimately extends to every part of the lungs.\n\nThe alveoli are the blind-ended terminals of the \"tree\", meaning that any air that enters them has to exit via the same route it used to enter the alveoli. A system such as this creates dead space, a volume of air that fills the airways (the dead space) at the end of inhalation, and is breathed out, unchanged, during the next exhalation, never having reached the alveoli. Similarly, the dead space is filled with alveolar air at the end of exhalation, and is the first air to breathed back into the alveoli, before any fresh air reaches the alveoli during inhalation. The dead space volume of a typical adult human is about 150 ml.\n\nThe primary purpose of breathing is to bring atmospheric air (in small doses) into the alveoli where gas exchange with the gases in the blood takes place. The equilibration of the partial pressures of the gases in the alveolar blood and the alveolar air occurs by diffusion. At the end of each exhalation the adult human lungs still contain 2,500–3,000 mL of air, their functional residual capacity or FRC. With each breath (inhalation) only as little as about 350 mL of warm, moistened atmospheric is added, and well mixed, with the FRC. Consequently, the gas composition of the FRC changes very little during the breathing cycle. Since the pulmonary capillary blood equilibrates with this virtually unchanging mixture of air in the lungs (which has a substantially different composition from that of the ambient air), the partial pressures of the arterial blood gases also do not change with each breath. The tissues are therefore not exposed to swings in oxygen and carbon dioxide tensions in the blood during the breathing cycle, and the peripheral and central chemoreceptors do not need to \"choose\" the point in the breathing cycle at which the blood gases need to be measured, and responded to. Thus the homeostatic control of the breathing rate simply depends on the partial pressures of oxygen and carbon dioxide in the arterial blood. This then also maintains the constancy of the pH of the blood.\n\nThe rate and depth of breathing is automatically controlled by the respiratory centers that receive information from the peripheral and central chemoreceptors. These chemoreceptors continuously monitor the partial pressures of carbon dioxide and oxygen in the arterial blood. The sensors are, firstly, the central chemoreceptors on the surface of the medulla oblongata of the brain stem which are particularly sensitive to pH as well as the partial pressure of carbon dioxide in the blood and cerebrospinal fluid. The second group of sensors measure the partial pressure of oxygen in the arterial blood. Together the latter are known as the peripheral chemoreceptors which are situated in the aortic and carotid bodies. Information from all of these chemoreceptors is conveyed to the respiratory centers in the pons and medulla oblongata, which responds to deviations in the partial pressures of carbon dioxide and oxygen in the arterial blood from normal by adjusting the rate and depth of breathing, in such a way as to restore partial pressure of carbon dioxide back to 5.3 kPa (40 mm Hg), the pH to 7.4 and, to a lesser extent, the partial pressure of oxygen to 13 kPa (100 mm Hg). For instance, exercise increases the production of carbon dioxide by the active muscles. This carbon dioxide diffuses into the venous blood, and ultimately raises the partial pressure of carbon dioxide in the arterial blood. This is immediately sensed by the carbon dioxide chemoreceptors on the brain stem. The respiratory centers respond to this information by causing the rate and depth of breathing to increase to such an extent that the partial pressures of carbon dioxide and oxygen in the arterial blood return almost immediately to the same levels as at rest. The respiratory centers communicate with the muscles of breathing via motor nerves, of which the phrenic nerves, which innervate the diaphragm, are probably the most important.\n\nAutomatic breathing can be overridden to a limited extent by simple choice, or to facilitate swimming, speech, singing or other vocal training. It is impossible to suppress the urge to breathe to the point of hypoxia but training can increase the ability to breath-hold; for example, in February 2016, a Spanish, professional freediver broke the world record for holding the breath under water at just over 24 minutes.\n\nOther automatic breathing control reflexes also exist. Submersion, particularly of the face, in cold water, triggers a response called the diving reflex. This firstly has the result of shutting down the airways against the influx of water. The metabolic rate slows right down. This is coupled with intense vasoconstriction of the arteries to the limbs and abdominal viscera. This reserves the oxygen that is in blood and lungs at the beginning of the dive almost exclusively for the heart and the brain. The diving reflex is an often-used response in animals that routinely need to dive, such as penguins, seals and whales. It is also more effective in very young infants and children than in adults.\n\nInhaled air is by volume 78.08% nitrogen, 20.95% oxygen and small amounts include argon, carbon dioxide, neon, helium, and hydrogen.\n\nThe gas exhaled is 4% to 5% by volume of carbon dioxide, about a 100 fold increase over the inhaled amount. The volume of oxygen is reduced by a small amount, 4% to 5%, compared to the oxygen inhaled. The typical composition is:\n\n\nIn addition to air, underwater divers practicing technical diving may breathe oxygen-rich, oxygen-depleted or helium-rich breathing gas mixtures. Oxygen and analgesic gases are sometimes given to patients under medical care. The atmosphere in space suits is pure oxygen. However, this is kept at around 20% of Earthbound atmospheric pressure to regulate the rate of inspiration.\n\nAtmospheric pressure decreases with the height above sea level (altitude) and since the alveoli are open to the outside air through the open airways, the pressure in the lungs also decreases at the same rate with altitude. At altitude, a pressure differential is still required to drive air into and out of the lungs as it is at sea level. The mechanism for breathing at altitude is essentially identical to breathing at sea level but with the following differences:\n\nThe atmospheric pressure decreases exponentially with altitude, roughly halving with every rise in altitude. The composition of atmospheric air is, however, almost constant below 80 km, as a result of the continuous mixing effect of the weather. The concentration of oxygen in the air (mmols O per liter of air) therefore decreases at the same rate as the atmospheric pressure. At sea level, where the ambient pressure is about 100 kPa, oxygen contributes 21% of the atmosphere and the partial pressure of oxygen () is 21 kPa (i.e. 21% of 100 kPa). At the summit of Mount Everest, , where the total atmospheric pressure is 33.7 kPa, oxygen still contributes 21% of the atmosphere but its partial pressure is only 7.1 kPa (i.e. 21% of 33.7 kPa = 7.1 kPa). Therefore, a greater volume of air must be inhaled at altitude than at sea level in order to breath in the same amount of oxygen in a given period.\n\nDuring inhalation, air is warmed and saturated with water vapor as it passes through the nose and pharynx before it enters the alveoli. The \"saturated\" vapor pressure of water is dependent only on temperature; at a body core temperature of 37 °C it is 6.3 kPa (47.0 mmHg), regardless of any other influences, including altitude. Consequently, at sea level, the \"tracheal\" air (immediately before the inhaled air enters the alveoli) consists of: water vapor ( = 6.3 kPa), nitrogen ( = 74.0 kPa), oxygen ( = 19.7 kPa) and trace amounts of carbon dioxide and other gases, a total of 100 kPa. In dry air, the at sea level is 21.0 kPa, compared to a of 19.7 kPa in the tracheal air (21% of [100 – 6.3] = 19.7 kPa). At the summit of Mount Everest tracheal air has a total pressure of 33.7 kPa, of which 6.3 kPa is water vapor, reducing the in the tracheal air to 5.8 kPa (21% of [33.7 – 6.3] = 5.8 kPa), beyond what is accounted for by a reduction of atmospheric pressure alone (7.1 kPa).\n\nThe pressure gradient forcing air into the lungs during inhalation is also reduced by altitude. Doubling the volume of the lungs halves the pressure in the lungs at any altitude. Halving the sea level air pressure (100 kPa) results in a pressure gradient of 50 kPa but doing the same at 5500 m, where the atmospheric pressure is 50 kPa, a doubling of the volume of the lungs results in a pressure gradient of only 25 kPa. In practice, because we breathe in a gentle, cyclical manner that generates pressure gradients of only 2–3 kPa, this has little effect on the actual rate of inflow into the lungs and is easily compensated for by breathing slightly deeper. The lower viscosity of air at altitude allows air to flow more easily and this also helps compensate for any loss of pressure gradient.\n\nAll of the above effects of low atmospheric pressure on breathing are normally accommodated by increasing the respiratory minute volume (the volume of air breathed in – \"or\" out – per minute), and the mechanism for doing this is automatic. The exact increase required is determined by the respiratory gases homeostatic mechanism, which regulates the arterial and . This homeostatic mechanism prioritizes the regulation of the arterial over that of oxygen at sea level. That is to say, at sea level the arterial is maintained at very close to 5.3 kPa (or 40 mmHg) under a wide range of circumstances, at the expense of the arterial , which is allowed to vary within a very wide range of values, before eliciting a corrective ventilatory response. However, when the atmospheric pressure (and therefore the atmospheric ) falls to below 75% of its value at sea level, oxygen homeostasis is given priority over carbon dioxide homeostasis. This switch-over occurs at an elevation of about . If this switch occurs relatively abruptly, the hyperventilation at high altitude will cause a severe fall in the arterial with a consequent rise in the pH of the arterial plasma leading to respiratory alkalosis. This is one contributor to high altitude sickness. On the other hand, if the switch to oxygen homeostasis is incomplete, then hypoxia may complicate the clinical picture with potentially fatal results.\n\nPressure increases with the depth of water at the rate of about one atmosphere – slightly more than 100 kPa, or one bar, for every 10 metres. Air breathed underwater by divers is at the ambient pressure of the surrounding water and this has a complex range of physiological and biochemical implications. If not properly managed, breathing compressed gasses underwater may lead to several diving disorders which include pulmonary barotrauma, decompression sickness, nitrogen narcosis, and oxygen toxicity. The effects of breathing gasses under pressure are further complicated by the use of one or more special gas mixtures.\n\nAir is provided by a diving regulator, which reduces the high pressure in a diving cylinder to the ambient pressure. The breathing performance of regulators is a factor when choosing a suitable regulator for the type of diving to be undertaken. It is desirable that breathing from a regulator requires low effort even when supplying large amounts of air. It is also recommended that it supplies air smoothly without any sudden changes in resistance while inhaling or exhaling. In the graph, right, note the initial spike in pressure on exhaling to open the exhaust valve and that the initial drop in pressure on inhaling is soon overcome as the Venturi effect designed into the regulator to allow an easy draw of air. Many regulators have an adjustment to change the ease of inhaling so that breathing is effortless.\n\nAbnormal breathing patterns include Kussmaul breathing, Biot's respiration and Cheyne–Stokes respiration.\n\nOther breathing disorders include shortness of breath (dyspnea), stridor, apnea, sleep apnea (most commonly obstructive sleep apnea), mouth breathing, and snoring. Many conditions are associated with obstructed airways. Hypopnea refers to overly shallow breathing; hyperpnea refers to fast and deep breathing brought on by a demand for more oxygen, as for example by exercise. The terms hypoventilation and hyperventilation also refer to shallow breathing and fast and deep breathing respectively, but under inappropriate circumstances or disease. However, this distinction (between, for instance, hyperpnea and hyperventilation) is not always adhered to, so that these terms are frequently used interchangeably.\n\nA range of breath tests can be used to diagnose diseases such as dietary intolerances.\nA rhinomanometer uses acoustic technology to examine the air flow through the nasal passages.\n\nThe word \"spirit\" comes from the Latin \"spiritus\", meaning breath. Historically, breath has often been considered in terms of the concept of life force. The Hebrew Bible refers to God breathing the breath of life into clay to make Adam a living soul (nephesh). It also refers to the breath as returning to God when a mortal dies. The terms spirit, prana, the Polynesian mana, the Hebrew ruach and the psyche in psychology are related to the concept of breath.\n\nIn T'ai chi, aerobic exercise is combined with specific breathing exercises to strengthen the diaphragm muscles, improve posture and make better use of the body's Qi, (energy). Different forms of meditation, and yoga advocate various breathing methods. A form of Buddhist meditation called anapanasati meaning mindfulness of breath was first introduced by Buddha. Breathing disciplines are incorporated into meditation, certain forms of yoga such as pranayama, and the Buteyko method as a treatment for asthma and other conditions.\n\nIn music, some wind instrument players use a technique called circular breathing. Singers also rely on breath control.\n\nCommon cultural expressions related to breathing include: \"to catch my breath\", \"took my breath away\", \"inspiration\", \"to expire\", \"get my breath back\".\n\nCertain breathing patterns have a tendency to occur with certain moods. Due to this relationship, practitioners of various disciplines consider that they can encourage the occurrence of a particular mood by adopting the breathing pattern that it most commonly occurs in conjunction with. For instance, and perhaps the most common recommendation, is that deeper breathing which utilises the diaphragm and abdomen more can encourage a more relaxed and confident mood. Practitioners of different disciplines often interpret the importance of breathing regulation and its perceived influence on mood in different ways. Buddhists may consider that it helps precipitate a sense of inner-peace, holistic healers that it encourages an overall state of health and business advisers that it provides relief from work based stress.\n\nDuring physical exercise a deeper breathing pattern is adopted to facilitate greater oxygen absorption. An additional reason for the adoption of a deeper breathing pattern is to strengthen the body’s core. During the process of deep breathing, the thoracic diaphragm adopts a lower position in the core and this helps to generate intra-abdominal pressure which strengthens the lumbar spine. Typically, this allows for more powerful physical movements to be performed. As such, it is frequently recommended when lifting heavy weights to take a deep breath or adopt a deeper breathing pattern.\n\n\n"}
{"id": "18713734", "url": "https://en.wikipedia.org/wiki?curid=18713734", "title": "Bucharest West Power Station", "text": "Bucharest West Power Station\n\nThe Bucharest West Power Station is a large thermal power plant located in Bucharest having 5 generation groups, 4 of 40 MW each commissioned in 1955, and one group of 190 MW commissioned in 2007 having a total electricity generation capacity of 310 MW.\n\n"}
{"id": "351661", "url": "https://en.wikipedia.org/wiki?curid=351661", "title": "Carbon neutrality", "text": "Carbon neutrality\n\nCarbon neutrality, or having a net zero carbon footprint, refers to achieving net zero carbon emissions by balancing a measured amount of carbon released with an equivalent amount sequestered or offset, or buying enough carbon credits to make up the difference. It is used in the context of carbon dioxide releasing processes associated with transportation, energy production, and industrial processes such as production of carbon neutral fuel.\n\nThe carbon neutrality concept may be extended to include other greenhouse gases (GHG) measured in terms of their carbon dioxide equivalence (e) —the impact a GHG has on the atmosphere expressed in the equivalent amount of CO. The term \"climate neutral\" reflects the broader inclusiveness of other greenhouse gases in climate change, even if CO is the most abundant, encompassing other greenhouse gases regulated by the Kyoto Protocol, namely: methane (CH), nitrous oxide (NO), hydrofluorocarbons (HFC), perfluorocarbons (PFC), and sulphur hexafluoride (SF). Both terms are used interchangeably throughout this article.\n\nThe best practice for organizations and individuals seeking carbon neutral status entails reducing and/or avoiding carbon emissions first so that only unavoidable emissions are offset. Carbon neutral status is commonly achieved in two ways:\n\nThe concept may be extended to include other greenhouse gases measured in terms of their carbon dioxide equivalence. The phrase was the \"New Oxford American Dictionary\"'s Word of the Year for 2006.\n\nCarbon neutrality is usually achieved by combining the following steps (although these may vary depending whether the strategy is implemented by individuals, companies, organizations, cities, regions, or countries):\n\nIn the case of individuals, decision-making is likely to be straightforward, but for more complex set-ups, it usually requires political leadership at the highest level and wide popular agreement that the effort is worth making.\n\nCounting and analyzing the emissions that need to be eliminated, and the options for doing so, is the most crucial step in the cycle as it enables setting the priorities for action – from the products purchased to energy use and transport – and to start monitoring progress. This can be achieved through a GHG inventory that aims at answering questions such as:\nFor individuals, carbon calculators simplify compiling an inventory. Typically they measure electricity consumption in kWh, the amount and type of fuel used to heat water and warm the house, and how many kilometres an individual drives, flies and rides in different vehicles. Individuals may also set various limits of the system they are concerned with, e.g. personal GHG emissions, household emissions, or the company they work for.\nThere are plenty of carbon calculators available online, which vary significantly in their usefulness and the parameters they measure. Some, for example, factor in only cars, aircraft and household energy use. Others cover household waste or leisure interests as well.\nIn some circumstances, actually going beyond carbon neutral (usually after a certain length of time taken to reach carbon breakeven) is an objective.\n\nIn starting to work towards climate neutrality, businesses and local administrations can make use of an environmental (or sustainability) management system or EMS established by the international standard ISO 14001 (developed by the International Organization for Standardization). Another EMS framework is EMAS, the European Eco Management and Audit Scheme, used by numerous companies throughout the EU. Many local authorities apply the management system to certain sectors of their administration or certify their whole operations.\n\nOne of the strongest arguments for reducing GHG emissions is that it will often save money. Energy prices across the world are rising, making it harder to afford to travel, heat and light homes and factories, and keep a modern economy ticking over. So it is both common sense and sensible for the climate to use energy as sparingly as possible.\nExamples of possible actions to reduce GHG emissions are:\n\nThe use of Carbon offsets aims to neutralize a certain volume of GHG emissions by funding projects which should cause an equivalent reduction of GHG emissions somewhere else, such as tree planting. Under the premise “First reduce what you can, then offset the remainder”, offsetting can be done by supporting a responsible carbon project, or by buying carbon offsets or carbon credits.\n\nCarbon offsetting is also a tool for severals local authorities in the world.\n\nin 2015, the UNFCCC, following the mandate of the CDM Executive board, launched a dedicated website where organizations, companies, but also private person are able to offset their footprint (https://offset.climateneutralnow.org/) with the aim of facilitating everyone's participation in the process of promoting sustainability.\n\nOffsetting is sometimes seen as a charged and contentious issue. For example, James Hansen describes offsets as \"modern day indulgences, sold to an increasingly carbon-conscious public to absolve their climate sins.\"\n\nThis phase includes evaluation of the results and compilation of a list of suggested improvements, with results documented and reported, so that experience gained of what does (and does not) work is shared with those who can put it to good use.\nFinally, with all that completed, the cycle starts all over again, only this time incorporating the lessons learned. Science and technology move on, regulations become tighter, the standards people demand go up. So the second cycle will go further than the first, and the process will continue, each successive phase building on and improving on what went before.\n\nBeing carbon neutral is increasingly seen as good corporate or state social responsibility and a growing list of corporations and states are announcing dates for when they intend to become fully neutral.\nEvents such as the G8 Summit and organizations like the World Bank are also using offset schemes to become carbon neutral. Artists like The Rolling Stones and Pink Floyd have made albums or tours carbon neutral.\n\nTo be considered carbon neutral, an organization must reduce its carbon footprint to zero. Determining what to include in the carbon footprint depends upon the organization and the standards they are following.\n\nGenerally, direct emissions sources must be reduced and offset completely, while indirect emissions from purchased electricity can be reduced with renewable energy purchases.\n\nDirect emissions include all pollution from manufacturing, company owned vehicles and reimbursed travel, livestock and any other source that is directly controlled by the owner. Indirect emissions include all emissions that result from the use or purchase of a product. For instance, the direct emissions of an airline are all the jet fuel that is burned, while the indirect emissions include manufacture and disposal of airplanes, all the electricity used to operate the airline's office, and the daily emissions from employee travel to and from work. In another example, the power company has a direct emission of greenhouse gas, while the office that purchases it considers it an indirect emission.\n\nCarbon neutral fuels are those that neither contribute to nor reduce the amount of carbon into the atmosphere. Before an agency can certify an organization or individual as carbon neutral, it is important to specify whether indirect emissions are included in the Carbon Footprint calculation. Most Voluntary Carbon neutral certifiers such as Standard Carbon in the US, require both direct and indirect sources to be reduced and offset. As an example, for an organization to be certified carbon neutral by Standard Carbon, it must offset all direct and indirect emissions from travel by 1 lb COe per passenger mile, and all non-electricity direct emissions 100%. Indirect electrical purchases must be equalized either with offsets, or renewable energy purchase. This standard differs slightly from the widely used World Resource Institute and may be easier to calculate and apply.\n\nMuch of the confusion in carbon neutral standards can be attributed to the number of voluntary carbon standards which are available. For organizations looking at which carbon offsets to purchase, knowing which standards are robust, credible and permanent is vital in choosing the right carbon offsets and projects to get involved in. Some of the main standards in the voluntary market include; The Verified Carbon Standard, The Gold Standard, The American Carbon Registry, The Climate Action Reserve, and Plan Vivo. In addition companies can purchase Certified Emission Reductions (CERs) which result from mitigated carbon emissions from UNFCCC approved projects for voluntary purposes. \nThe concept of shared resources also reduces the volume of carbon a particular organization has to offset, with all upstream and downstream emissions the responsibility of other organizations or individuals. If all organizations and individuals were involved then this would not result in any double accounting.\n\nRegarding terminology in UK and Ireland, in December 2011 the Advertising Standards Authority (in an ASA decision which was upheld by its Independent Reviewer, Sir Hayden Phillips) controversially ruled that no manufactured product can be marketed as \"zero carbon\", because carbon was inevitably emitted during its manufacture. This decision was made in relation to a solar panel system whose embodied carbon was repaid during 1.2 years of use and it appears to mean that no buildings or manufactured products can legitimately be described as zero carbon in its jurisdiction.\n\nBeing carbon neutral is increasingly seen as good corporate or state social responsibility and a growing list of corporations, cities and states are announcing dates for when they intend to become fully neutral.\n\nThe original Climate Neutral Network was an Oregon-based non-profit organization founded by Sue Hall and incorporated in 1999 to persuade companies that being climate neutral was potentially cost saving as well as environmentally sustainable. It developed both the Climate Neutral Certification and Climate Cool brand name with key stakeholders such as the United States Environmental Protection Agency, The Nature Conservancy, the Rocky Mountain Institute, Conservation International, and the World Resources Institute and succeeded in enrolling the 2002 Winter Olympics to compensate for its associated greenhouse gas emissions. The non-profit's web site as of March 2011, lists the organization as closing its doors and plans to continue the Climate Cool upon transfer to a new non-profit organization, unknown at this time. The for-profit consulting firm Climate Neutral Business Network listed the same Sue Hall as CEO and many of the same companies who were participants in the original Climate Neutral Network as consulting clients.\n\nFew companies have actually attained Climate Neutral Certification, applying to a rigorous review process and establishing that they have achieved absolute net zero or better impact on the world's climate. Shaklee Corporation became the first Climate Neutral certified company in April 2000. The company employs a variety of investments, and offset activities, including tree-planting, use of solar energy, methane capture in abandoned mines and its manufacturing processes. Climate Neutral Business Network states that it certified Dave Matthews Band's concert tour as Climate Neutral. The Christian Science Monitor criticized the use of NativeEnergy. a for-profit company that sells offsets credits to businesses and celebrities like Dave Matthews.\n\nSalt Spring Coffee has become carbon neutral by lowering emissions through reducing long-range trucking and using bio-diesel fuel in delivery trucks, upgrading to energy efficient equipment and purchasing carbon offsets. The company claims to the first carbon neutral coffee sold in Canada. Salt Spring Coffee was recognized by the David Suzuki Foundation in their 2010 report \"Doing Business in a New Climate\".\n\nSome corporate examples of self-proclaimed carbon neutral and climate neutral initiatives include Dell,\nGoogle, HSBC, ING Group, PepsiCo, Sky,\nTesco, Toronto-Dominion Bank, Asos and Bank of Montreal.\n\nUnder the leadership of Secretary-General Ban Ki-moon, the United Nations pledged to work towards climate neutrality in December 2007. The United Nations Environment Programme (UNEP) announced it was becoming climate neutral in 2008 and established a Climate Neutral Network to promote the idea in February 2008.\n\nEvents such as the G8 Summit and organizations like the World Bank are also using offset schemes to become carbon neutral. Artists like The Rolling Stones and Pink Floyd have made albums or tours carbon neutral, while Live Earth says that its seven concerts held on 7 July 2007 were the largest carbon neutral public event in history.\n\nThe Vancouver 2010 Olympic and Paralympic Winter Games were the first carbon neutral Games in history.\n\nBuildings are the largest single contributor to the production of greenhouse gases. The American Institute of Architects 2030 Commitment is a voluntary program for AIA member firms and other entities in the built environment that asks these organizations to pledge to design all their buildings to be carbon neutral by 2030.\n\nIn 2010, architectural firm HOK worked with energy and daylighting consultant The Weidt Group to design a net zero carbon emissions Class A office building prototype in St. Louis, Missouri, U.S.\n\nOne country has achieved carbon neutrality:\n\nSeveral places have pledged carbon neutrality, including:\n\n\nIn June 2011, the Canadian Province of British Columbia announced they had officially become the first provincial/state jurisdiction in North America to achieve carbon neutrality in public sector operations: every school, hospital, university, Crown corporation, and government office measured, reported and purchased carbon offsets on all their 2010 Greenhouse Gas emissions as required under legislation. Local Governments across B.C. are also beginning to declare Carbon Neutrality. The province intends to accelerate the deployment of natural gas vehicles. Under the LiveSmart BC initiative, natural gas furnaces and water heaters receive cash back thereby promoting the burning of fossil fuel in the province. The province states that an important part of \nnew natural gas production will come from the Horn River basin where about 500 million tonnes of CO2 will be released into the atmosphere.\n\nCosta Rica aims to be fully carbon neutral by 2021. In 2004, 46.7% of Costa Rica's primary energy came from renewable sources, while 94% of its electricity was generated from hydroelectric power, wind farms and geothermal energy in 2006. A 3.5% tax on gasoline in the country is used for payments to compensate landowners for growing trees and protecting forests and its government is making further plans for reducing emissions from transport, farming and industry.\n\nSamsø island in Denmark is the largest carbon-neutral settlement on the planet, with a population of 4200, based on wind-generated electricity and biomass-based district heating. They currently generate extra wind power and export the electricity to compensate for petro-fueled vehicles. There are future hopes of using electric or biofuel vehicles.\n\nIceland is also moving towards climate neutrality. Over 99% of electricity production and almost 80% of total energy production comes from hydropower and geothermal. No other nation uses such a high proportion of renewable energy resources.\nIn February 2008, Costa Rica, Iceland, New Zealand and Norway were the first four countries to join the Climate Neutral Network, an initiative led by the United Nations Environment Programme (UNEP) to catalyze global action towards low carbon economies and societies.\n\nThe ex-president of the Maldives has pledged to make his country carbon-neutral within a decade by moving to wind and solar energy. The Maldives, a country consisting of very low-lying islands, would be one of the first countries to be submerged due to sea level rise. The Maldives presided over the foundation of the Climate Vulnerable Forum.\n\nAnother nation to pledge carbon neutrality is New Zealand. Its Carbon Neutral Public Sector Initiative aimed to offset the greenhouse gas emissions of an initial group of six governmental agencies by 2012. Unavoidable emissions would be offset, primarily through indigenous forest regeneration projects on conservation land. All 34 public service agencies also needed to have emission reduction plans in place. The Carbon Neutral Public Service programme was discontinued in March 2009.\n\nOn April 19, 2007, Prime Minister Jens Stoltenberg announced to the Labour Party annual congress that Norway's greenhouse gas emissions would be cut by 10 percent more than its Kyoto commitment by 2012, and that the government had agreed to achieve emission cuts of 30% by 2020. He also proposed that Norway should become carbon neutral by 2050, and called upon other rich countries to do likewise. This carbon neutrality would be achieved partly by carbon offsetting, a proposal criticised by Greenpeace, who also called on Norway to take responsibility for the 500m tonnes of emissions caused by its exports of oil and gas. World Wildlife Fund Norway also believes that the purchase of carbon offsets is unacceptable, saying 'it is a political stillbirth to believe that China will quietly accept that Norway will buy climate quotas abroad'. The Norwegian environmental activist Bellona Foundation believes that the prime minister was forced to act due to pressure from anti-European Union members of the coalition government, and called the announcement 'visions without content'.\nIn January 2008 the Norwegian government went a step further and declared a goal of being carbon neutral by 2030. But the government has not been specific about any plans to reduce emissions at home; the plan is based on buying carbon offsets from other countries, and very little has actually been done to reduce Norway's emissions, apart from a very successful policy for electric vehicles\n\nIn Spain, in 2014, the island of El Hierro became carbon neutral (for its power production). Also, the city of Logroño Montecorvo in La Rioja will be carbon neutral once completed.\n\nThe islands of Orkney have significant wind and marine energy resources, and renewable energy has recently come into prominence. Although Orkney is connected to the mainland, it generates over 100% of its net power from renewables. This comes mainly from wind turbines situated right across Orkney.\n\nSweden aims to become carbon neutral by 2045. The vision is that net greenhouse gas emissions should be zero. The overall objective is that the increase in global temperature should be limited to two degrees, and that the concentration of greenhouse gases in the atmosphere stabilizes at a maximum of 400 ppm.\n\nJeju Island aims to be carbon neutral by 2030.\n\nIn July 2007, Vatican City announced a plan to become the first carbon-neutral state in the world, following the politics of the Pope to eliminate global warming. The goal would be reached through the donation of the Vatican Climate Forest in Hungary. The forest is to be sized to offset the year's carbon dioxide emissions. However, no trees have actually been planted as of 2008. The company KlimaFa is no longer in existence and hasn't fulfilled its promises. In November 2008, the city state also installed and put into operation 2,400 solar panels on the roof of the Paul VI Centre audience hall.\n\nMany initiatives seek to assist individuals, businesses and states in reducing their carbon footprint or achieving climate neutrality. These include website neutralization projects like COStats and, the similar European initiative CO neutral website as well as the Climate Neutral Network, Caring for Climate, and Together campaign.\n\nAlthough there is currently no international certification scheme for carbon or climate neutrality, some countries have established national certification schemes. Examples include Norwegian Eco-Lighthouse Program and the Australian government's National Carbon Offset Standard (NCOS).\n\nCertifications are also available from the CEB, BSI (PAS 2060) and The CarbonNeutral Company (CarbonNeutral).\n\nClimate Neutral Certification was established and trademarked originally through the Climate Neutral Network, an Oregon-based non-profit organization, not to be confused with the UNEP's Climate Neutral Network. Applications for certification are no longer being accepted according to the non-profit organization's web site, where the organization also states it is closing its doors. The first three companies certified as Climate Neutral were Shaklee Corporation, Interface, Inc., and Saunders Hotels.\nStakeholders in developing and supporting the Climate Neutral Certification are listed as The Nature Conservancy, Conservation International, Rocky Mountain Institute, and the U.S EPA. What is unclear is whether the Climate Neutral certification will be continued by the for-profit consulting firm, Climate Neutral Business Network, or another non-profit organization in the future.\n\nClimate Neutral Network also promoted, trademarked, and licensed the brand Climate Cool, for products certified by the organization's Environmental Review Panel and determined to achieve net zero climate impact, by reducing and offsetting associated emissions. The organization's website promises to transfer the Climate Cool branding to another non-profit organization, upon closing the current organization.\n\nIn Australia, the government-endorsed carbon neutral certification is the National Carbon Offset Standard (NCOS).\n\n"}
{"id": "42152637", "url": "https://en.wikipedia.org/wiki?curid=42152637", "title": "Colombo Port Power Station", "text": "Colombo Port Power Station\n\nThe Colombo Port Power Station (also sometimes referred to as the Colombo Port Power Barge) is a , permanently moored at the Colombo Harbour, in the Western Province of Sri Lanka. After the plant's 15-year license expired in 2015, the Ceylon Electricity Board purchased the powership in a controversial deal. It was previously owned by Colombo Power Private Limited, a 50-50 joint venture by Mitsui Engineering & Shipbuilding and Kawasho Corporation.\n\nThe powership consists of four units, totalling the plant capacity to . Although the plant is estimated to generate per annum, the actual average generation is , above initial estimates. The barge was built by Sasebo Heavy Industries, with funding from the Japan Bank for International Cooperation.\n\n"}
{"id": "1481873", "url": "https://en.wikipedia.org/wiki?curid=1481873", "title": "Copper(II) chloride", "text": "Copper(II) chloride\n\nCopper(II) chloride is the chemical compound with the chemical formula CuCl. This is a light brown solid, which slowly absorbs moisture to form a blue-green dihydrate. The copper(II) chlorides are some of the most common copper(II) compounds, after copper sulfate.\n\nBoth the anhydrous and the dihydrate forms occur naturally as the very rare minerals tolbachite and eriochalcite, respectively.\n\nAnhydrous CuCl adopts a distorted cadmium iodide structure. In this motif, the copper centers are octahedral. Most copper(II) compounds exhibit distortions from idealized octahedral geometry due to the Jahn-Teller effect, which in this case describes the localization of one d-electron into a molecular orbital that is strongly antibonding with respect to a pair of chloride ligands. In CuCl·2HO, the copper again adopts a highly distorted octahedral geometry, the Cu(II) centers being surrounded by two water ligands and four chloride ligands, which bridge asymmetrically to other Cu centers.\n\nCopper(II) chloride is paramagnetic. Of historical interest, CuCl·2HO was used in the first electron paramagnetic resonance measurements by Yevgeny Zavoisky in 1944.\n\nAqueous solution prepared from copper(II) chloride contain a range of copper(II) complexes depending on concentration, temperature, and the presence of additional chloride ions. These species include blue color of [Cu(HO)] and yellow or red color of the halide complexes of the formula [CuCl].\n\nCopper(II) hydroxide precipitates upon treating copper(II) chloride solutions with base:\n\nPartial hydrolysis gives copper oxychloride, CuCl(OH), a popular fungicide.\n\nCuCl is a mild oxidant. It decomposes to CuCl and Cl at 1000 °C:\nCuCl reacts with several metals to produce copper metal or copper(I) chloride with oxidation of the other metal. To convert copper(II) chloride to copper(I) derivatives, it can be convenient to reduce an aqueous solution with sulfur dioxide as the reductant:\n\nCuCl reacts with HCl or other chloride sources to form complex ions: the red CuCl (it is a dimer in reality, CuCl, a couple of tetrahedrons that share an edge), and the green or yellow CuCl.\n\nSome of these complexes can be crystallized from aqueous solution, and they adopt a wide variety of structures.\n\nCopper(II) chloride also forms a variety of coordination complexes with ligands such as pyridine and triphenylphosphine oxide:\nHowever \"soft\" ligands such as phosphines (e.g., triphenylphosphine), iodide, and cyanide as well as some tertiary amines induce reduction to give copper(I) complexes.\n\nCopper(II) chloride is prepared commercially by the action of chlorination of copper:\nCopper metal itself cannot be oxidised by hydrochloric acid, but copper-containing bases such as the hydroxide, oxide, or copper(II) carbonate can be reacted with hydrochloric acid.\n\nOnce prepared, a solution of CuCl may be purified by crystallization. A standard method takes the solution mixed in hot dilute hydrochloric acid, and causes the crystals to form by cooling in a CaCl-ice bath.\n\nThere are indirect and rarely used means of using copper ions in solution to form copper(II) chloride. Electrolysis of aqueous sodium chloride with copper electrodes produces (among other things) a blue-green foam that can be collected and converted to the hydrate. While this is not usually done due to the emission of toxic chlorine gas, and the prevalence of the more general chloralkali process, the electrolysis will convert the copper metal to copper ions in solution forming the compound. Indeed, any solution of copper ions can be mixed with hydrochloric acid and made into copper chloride by removing any other ions.\n\nCopper(II) chloride occurs naturally as the very rare anhydrous mineral tolbachite and the dihydrate eriochalcite. Both are found near fumaroles. More common are mixed oxyhydroxide-chlorides like atacamite Cu(OH)Cl, arising among Cu ore beds oxidation zones in arid climate (also known from some altered slags).\n\nA major industrial application for copper(II) chloride is as a co-catalyst with palladium(II) chloride in the Wacker process. In this process, ethene (ethylene) is converted to ethanal (acetaldehyde) using water and air. During the reaction, PdCl is reduced to Pd, and the CuCl serves to re-oxidize this back to PdCl. Air can then oxidize the resultant CuCl back to CuCl, completing the cycle.\n\n\nThe overall process is:\n\nCopper(II) chloride is used as a catalyst in a variety of processes that produce chlorine by oxychlorination. The Deacon process takes place at about 400 to 450 °C in the presence of a copper chloride:\n\nCopper(II) chloride catalyzes the chlorination in the production of vinyl chloride and dichloroethane.\n\nCopper(II) chloride is used in the Copper–chlorine cycle in which it splits steam into a copper oxygen compound and hydrogen chloride, and is later recovered in the cycle from the electrolysis of copper(I) chloride. Copper is a metal\n\nCopper(II) chloride has some highly specialized applications in the synthesis of organic compounds. It effects chlorination of aromatic hydrocarbons- this is often performed in the presence of aluminium oxide. It is able to chlorinate the alpha position of carbonyl compounds:\n\nThis reaction is performed in a polar solvent such as dimethylformamide (DMF), often in the presence of lithium chloride, which accelerates the reaction.\n\nCuCl, in the presence of oxygen, can also oxidize phenols. The major product can be directed to give either a quinone or a coupled product from oxidative dimerization. The latter process provides a high-yield route to 1,1-binaphthol:\nSuch compounds are intermediates in the synthesis of BINAP and its derivatives\n\nCopper(II) chloride dihydrate promotes the hydrolysis of acetonides, i.e., for deprotection to regenerate diols or aminoalcohols, as in this example (where TBDPS = \"tert\"-butyldiphenylsilyl):\n\nCuCl also catalyses the free radical addition of sulfonyl chlorides to alkenes; the alpha-chlorosulfone may then undergo elimination with base to give a vinyl sulfone product.\n\nCopper(II) chloride is also used in pyrotechnics as a blue/green coloring agent. In a flame test, copper chlorides, like all copper compounds, emit green-blue.\n\nIn humidity indicator cards (HICs), cobalt-free brown to azure (copper(II) chloride base) HICs can be found on the market. In 1998, the European Community (EC) classified items containing cobalt(II) chloride of 0.01 to 1% w/w as T (Toxic), with the corresponding R phrase of R49 (may cause cancer if inhaled). As a consequence, new cobalt-free humidity indicator cards have been developed that contain copper.\n\nCopper(II) chloride can be toxic. Only concentrations below 5 ppm are allowed in drinking water by the US Environmental Protection Agency.\n\n\n"}
{"id": "17593223", "url": "https://en.wikipedia.org/wiki?curid=17593223", "title": "Dag Roger Rinde", "text": "Dag Roger Rinde\n\nDag Roger Rinde (born 1961) is a Norwegian businessperson.\n\nHe is the managing director of Statoil Norway and a member of the board at the Norwegian Petroleum Institute.\n"}
{"id": "312308", "url": "https://en.wikipedia.org/wiki?curid=312308", "title": "Dirac sea", "text": "Dirac sea\n\nThe Dirac sea is a theoretical model of the vacuum as an infinite sea of particles with negative energy. It was first postulated by the British physicist Paul Dirac in 1930 to explain the anomalous negative-energy quantum states predicted by the Dirac equation for relativistic electrons. The positron, the antimatter counterpart of the electron, was originally conceived of as a hole in the Dirac sea, well before its experimental discovery in 1932.\n\nUpon solving the free Dirac equation,\n\none finds\n\nwhere\n\nfor plane wave solutions with -momentum . This is a direct consequence of the relativistic energy-momentum relation\n\nupon which the Dirac equation is built. The quantity is a constant column vector and is a normalization constant. The quantity is called the \"time evolution factor\", and its interpretation in similar roles in, for example, the plane wave solutions of the Schrödinger equation, is the energy of the wave (particle). This interpretation is not immediately available here since it may acquire negative values. A similar situation prevails for the Klein–Gordon equation. In that case, the \"absolute value\" of can be interpreted as the energy of the wave since in the canonical formalism, waves with negative actually have \"positive\" energy . But this is not the case with the Dirac equation. The energy in the canonical formalism associated with negative is .\n\nIn hole theory, the solutions with negative time evolution factors are reinterpreted as representing the positron, discovered by Carl Anderson. The interpretation of this result requires a Dirac sea, showing that the Dirac equation is not merely a combination of special relativity and quantum mechanics, but it also implies that the number of particles cannot be conserved.\n\nThe origins of the Dirac sea lie in the energy spectrum of the Dirac equation, an extension of the Schrödinger equation that is consistent with special relativity, that Dirac had formulated in 1928. Although the equation was extremely successful in describing electron dynamics, it possesses a rather peculiar feature: for each quantum state possessing a positive energy \"E\", there is a corresponding state with energy \"-E\". This is not a big difficulty when an isolated electron is considered, because its energy is conserved and negative-energy electrons may be left out. However, difficulties arise when effects of the electromagnetic field are considered, because a positive-energy electron would be able to shed energy by continuously emitting photons, a process that could continue without limit as the electron descends into lower and lower energy states. Real electrons clearly do not behave in this way.\n\nDirac's solution to this was to turn to the Pauli exclusion principle. Electrons are fermions, and obey the exclusion principle, which means that no two electrons can share a single energy state within an atom. Dirac hypothesized that what we think of as the \"vacuum\" is actually the state in which \"all\" the negative-energy states are filled, and none of the positive-energy states. Therefore, if we want to introduce a single electron we would have to put it in a positive-energy state, as all the negative-energy states are occupied. Furthermore, even if the electron loses energy by emitting photons it would be forbidden from dropping below zero energy.\n\nDirac also pointed out that a situation might exist in which all the negative-energy states are occupied except one. This \"hole\" in the sea of negative-energy electrons would respond to electric fields as though it were a positively charged particle. Initially, Dirac identified this hole as a proton. However, Robert Oppenheimer pointed out that an electron and its hole would be able to annihilate each other, releasing energy on the order of the electron's rest energy in the form of energetic photons; if holes were protons, stable atoms would not exist. Hermann Weyl also noted that a hole should act as though it has the same mass as an electron, whereas the proton is about two thousand times heavier. The issue was finally resolved in 1932 when the positron was discovered by Carl Anderson, with all the physical properties predicted for the Dirac hole.\n\nDespite its success, the idea of the Dirac sea tends not to strike people as very elegant. The existence of the sea implies an infinite positive electric charge filling all of space. In order to make any sense out of this, one must assume that the \"bare vacuum\" must have an infinite negative charge density which is exactly cancelled by the Dirac sea. Since the absolute energy density is unobservable—the cosmological constant aside—the infinite energy density of the vacuum does not represent a problem. Only changes in the energy density are observable. Geoffrey Landis (author of \"Ripples In The Dirac Sea\", a hard science fiction short story) also notes that Pauli exclusion does not definitively mean that a filled Dirac sea cannot accept more electrons, since, as Hilbert elucidated, a sea of infinite extent can accept new particles even if it is filled. This happens when we have a chiral anomaly and a gauge instanton.\n\nThe development of quantum field theory (QFT) in the 1930s made it possible to reformulate the Dirac equation in a way that treats the positron as a \"real\" particle rather than the absence of a particle, and makes the vacuum the state in which no particles exist instead of an infinite sea of particles. This picture is much more convincing, especially since it recaptures all the valid predictions of the Dirac sea, such as electron-positron annihilation. On the other hand, the field formulation does not eliminate all the difficulties raised by the Dirac sea; in particular the problem of the vacuum possessing infinite energy.\n\nThe Dirac sea interpretation and the modern QFT interpretation are related by what may be thought of as a very simple Bogoliubov transformation, an identification between the creation and annihilation operators of two different free field theories. In the modern interpretation, the field operator for a Dirac spinor is a sum of creation operators and annihilation operators, in a schematic notation:\n\nAn operator with negative frequency lowers the energy of any state by an amount proportional to the frequency, while operators with positive frequency raise the energy of any state.\n\nIn the modern interpretation, the positive frequency operators add a positive energy particle, adding to the energy, while the negative frequency operators annihilate a positive energy particle, and lower the energy. For a Fermionic field, the creation operator formula_6 gives zero when the state with momentum k is already filled, while the annihilation operator formula_7 gives zero when the state with momentum k is empty.\n\nBut then it is possible to reinterpret the annihilation operator as a \"creation\" operator for a negative energy particle. It still lowers the energy of the vacuum, but in this point of view it does so by creating a negative energy object. This reinterpretation only affects the philosophy. To reproduce the rules for when annihilation in the vacuum gives zero, the notion of \"empty\" and \"filled\" must be reversed for the negative energy states. Instead of being states with no antiparticle, these are states that are already filled with a negative energy particle.\n\nThe price is that there is a nonuniformity in certain expressions, because replacing annihilation with creation adds a constant to the negative energy particle number. The number operator for a Fermi field is:\n\nwhich means that if one replaces N by 1-N for negative energy states, there is a constant shift in quantities like the energy and the charge density, quantities that count the total number of particles. The infinite constant gives the Dirac sea an infinite energy and charge density. The vacuum charge density should be zero, since the vacuum is Lorentz invariant, but this is artificial to arrange in Dirac's picture. The way it is done is by passing to the modern interpretation.\n\nDirac's idea is more directly applicable to solid state physics, where the valence band in a solid can be regarded as a \"sea\" of electrons. Holes in this sea indeed occur, and are extremely important for understanding the effects of semiconductors, though they are never referred to as \"positrons\". Unlike in particle physics, there is an underlying positive charge — the charge of the ionic lattice — that cancels out the electric charge of the sea.\n\nDirac's original concept of a sea of particles was revived in the theory of causal fermion systems, a recent proposal for a unified physical theory. In this approach, the problems of the infinite vacuum energy and infinite charge density of the Dirac sea disappear because these divergences drop out of the physical equations formulated via the causal action principle. These equations do not require a preexisting space-time, making it possible to realize the concept that space-time and all structures therein arise as a result of the collective interaction of the sea states with each other and with the additional particles and \"holes\" in the sea.\n\n\n\n"}
{"id": "751771", "url": "https://en.wikipedia.org/wiki?curid=751771", "title": "Distribution board", "text": "Distribution board\n\nA distribution board (also known as panelboard, breaker panel, or electric panel) is a component of an electricity supply system that divides an electrical power feed into subsidiary circuits, while providing a protective fuse or circuit breaker for each circuit in a common enclosure. Normally, a main switch, and in recent boards, one or more residual-current devices (RCD) or residual current breakers with overcurrent protection (RCBO), are also incorporated.\n\nIn the United Kingdom, a distribution board designed for domestic installations is known as a consumer unit.\nNorth American distribution boards are typically housed in sheet metal enclosures, with the circuit breakers positioned in two columns operable from the front. Some panelboards are provided with a door covering the breaker switch handles, but all are constructed with a \"dead front\"; that is to say the front of the enclosure (whether it has a door or not) prevents the operator of the circuit breakers from contacting live electrical parts within. Busbars carry the current from incoming line (\"hot\") conductors to the breakers, which are secured to the bus with either a \"bolt-on\" connection (using a threaded screw) or a \"plug-in\" connection using a retaining clip. \"Panelboards\" are more common in commercial and industrial applications and employ bolt-on breakers. Residential and light commercial panels are generally referred to as \"load centers\" and employ plug-in breakers. The neutral conductors are secured to a neutral bus using screw terminals. The branch circuit bonding conductors are secured to a terminal block attached directly to the panelboard enclosure, which is itself grounded.\n\nDuring servicing of the distribution board, when the cover has been removed and the cables are visible, American panelboards commonly have some live parts exposed. In Canadian service entrance panelboards the main switch or circuit breaker is located in a \"service box\", a section of the enclosure separated from the rest of the panelboard, so that when the main switch or breaker is switched off no live parts are exposed when servicing the branch circuits.\n\nBreakers are usually arranged in two columns. In a U.S.-style board, breaker positions are numbered left-to-right, along each row from top to bottom. This numbering system is universal with numerous competitive manufacturers of breaker panels.\n\nEach row is fed from a different phase (\"A\", \"B\", and \"C\" below), to allow 2- or 3-pole common-trip breakers to have one pole on each phase. In North America, it is common to wire large permanently installed equipment line-to-line. This takes two slots in the panel (two-pole) and gives a voltage of 240 V for split-phase electric power, or 208 V for three-phase power.\n\nThe photograph to the right shows the interior of a residential service panelboard manufactured by General Electric. The three service conductors—two 'hot' lines and one neutral—can be seen coming in at the top. The neutral wire is connected to the neutral busbar to the left with all the white wires, and the two hot wires are attached to the main breaker. Below it are the two bus bars carrying the current between the main breaker and the two columns of branch circuit breakers, with each respective circuit's red and black hot wires leading off. Three wires (hot black, neutral white, and bare ground) can be seen exiting the left side of the enclosure running directly to a NEMA 5-15 electrical receptacle with a power cord plugged into it. The incoming bare, stranded ground wire can be seen near the bottom of the neutral bus bar.\n\nThe photograph on the left shows a dual panel configuration: a main panel on the right (with front cover in place) and a subpanel on the left (with cover removed). The subpanel is fed by two large hot wires and a neutral wire running through the angled conduit near the top of the panels. This configuration appears to display three violations of the current U.S. National Electric Code: the main panel does not have a grounding conductor (here it is fed through the subpanel instead), the connection between the main and subpanel lacks a grounding conductor (it must have four wires instead of three), and the subpanel neutral bar is bonded to the ground bar (these should be separate bars after the first service disconnect, which in this case is the main panel).\n\nA common design of fuse box that was featured in homes built from 1940 to 1965 was the 60-amp fuse box that featured four plug fuses (i.e. Edison base) for branch circuits and one or more fuse blocks containing cartridge fuses for purposes such as major appliance circuits. After 1965, the more substantial 100 A panel with three-wire (230 V) service became common; a fuse box could have fuse blocks for the main shut-off and an electric range circuit plus a number of plug fuses (Edison base or Type S) for individual circuits.\n\nThis picture shows the interior of a typical distribution panel in the United Kingdom. The three incoming phase wires connect to the busbars via a main switch in the centre of the panel. On each side of the panel are two busbars, for neutral and earth. The incoming neutral connects to the lower busbar on the right side of the panel, which is in turn connected to the neutral busbar at the top left. The incoming earth wire connects to the lower busbar on the left side of the panel, which is in turn connected to the earth busbar at the top right. The cover has been removed from the lower-right neutral bar; the neutral bar on the left side has its cover in place.\n\nDown the left side of the phase busbars are two two-pole RCBOs and two single-pole breakers, one unused. The two-pole RCBOs in the picture are not connected across two phases, but have supply-side neutral connections exiting behind the phase busbars. Down the right side of the busbars are a single-pole breaker, a two-pole RCBO and a three-pole breaker.\n\nThe illustrated panel includes a great deal of unused space; it is likely that the manufacturer produces 18- and 24-position versions of this panel using the same chassis.\n\nLarger commercial, public, and industrial installations generally use three-phase supplies, with distribution boards which have twin vertical rows of breakers. Larger installations will often use subsidiary distribution boards.\n\nIn both cases, modern boards handling supplies up to around 100 A (CUs) or 200 A (distribution boards) use circuit breakers and RCDs on DIN rail mountings. The main distribution board in an installation will also normally provide a main switch (known as an \"incomer\") which switches the phase and neutral lines for the whole supply. (n.b., an incomer may be referred to, or sold as, an \"isolator\", but this is problematic, as it will not necessarily be used as an isolator in the strict sense.)\n\nFor each phase, power is fed along a busbar. In split-phase panels, separate busbars are fed directly from the incomer, which allows RCDs to be used to protect groups of circuits. Alternatively RCBOs may be used to provide both overcurrent and residual-current protection to single circuits.\n\nOther devices, such as transformers (e.g. for bell circuits) and contactors (relays; e.g. for large motor or heating loads) may also be used.\n\nNew British distribution boards generally have the live parts enclosed to IP2X, even when the cover has been removed for servicing.\n\nIn the United Kingdom, BS 7671 defines a Consumer unit as \" A particular type of distribution board comprising a type tested coordinated assembly for the control and distribution of electrical energy, principally in domestic premises...\" These installations usually have single-phase supplies at 230 V (nominal standard) and historically they were known as fuse boxes as older consumer units used fuses until the advent of mini-circuit breakers (MCBs).\nA normal new domestic CU used as a main panel might have from 6 to 24 ways for devices (some of which might occupy two ways), and will be split into two or more sections (e.g. a non-RCD section for alarms etc., an RCD-protected section for socket outlets, and an RCD-protected section for lighting and other built-in appliances). Secondary CUs used for outbuildings usually have 1 to 4 ways plus an RCD.\n\nRecent (pre-17th edition wiring regulations) CUs would not normally have RCD protected sections for anything other than socket outlets, though some older CUs featured RCD Incomers. Before 1990, RCDs (and split busbars) were not standard in CUs.\nFuse Boxes normally use cartridge or rewirable fuses with no other protective device, and basic 4-ways boxes are very common. Some older boxes are made of brown-black bakelite, sometimes with a wooden base. Although their design is historic, these were standard equipment for new installs as recently as the 1980s, so they are very common. Fuseholders in these boxes may not provide protection from accidental contact with live terminals.\n\nThe popular 4-way fusebox normally had two lighting and two socket circuits, with heavy or sustained loads such as immersion heater and oven on a socket circuit. This arrangement is not recommended practice today, but it is common for existing installations. Larger boxes with more ways will have separate fuses for large loads such as immersion heater, oven and shower.\n\nA small number of pre-1950 fuseboxes are still in service. These should be treated with caution because exposed live parts are common on these boxes. The installations they supply will not meet modern standards for electrical safety. Another characteristic of very old installations is that there may be two fuses for each circuit; one on the live and one on the neutral. In rare instances, old ring circuits may be encountered with no fewer than 4 15 A fuses per ring, one on each of L and N, and this duplicated for each of the two feeds for the ring.\n\nMost of the time, the panels and the breakers inserted inside them must be by the same manufacturer. Each manufacturer has one or more \"systems\", or kinds of breaker panels, that will only accept breakers of that type. In Europe, this is still the case, despite the adoption of a standard DIN rail for mounting and a standard cut-out shape, as the positions of the busbar connections are not standardized.\n\nCertain panels use seemingly interchangeable breakers. However, a given manufacturer will specifically mention exactly which devices may be installed in their equipment. These assemblies have been tested and approved for use by a recognized authority. Replacing or adding equipment which \"just happens to fit\" can result in unexpected or even dangerous conditions. Such installations should not be done without first consulting knowledgeable sources, including manufacturers.\n\nFor reasons of aesthetics and security, domestic circuit breaker panels and consumer units are normally located in out-of-the-way closets, attics, garages, or basements, but sometimes they are also featured as part of the aesthetic elements of a building (as an art installation, for example) or where they can be easily accessible. However, current U.S. building codes prohibit installation of a panel in a bathroom (or similar room), in closets intended for clothing, or where there is insufficient space for an electrician to gain access to the panel. Specific situations, such as an installation outdoors, in a hazardous environment, or in other out-of-the-ordinary locations might require specialized equipment and more stringent installation practices.\n\nDistribution boards may be designated for three phase or single phase and normal power or emergency power, or designated by use such as distribution panels for supplying other panels, lighting panels for lights, power panels for equipment and receptacles and special uses. Panels are located throughout the building in electric closets serving a section of the building.\n\nIn a theatre, a specialty panel known as a \"dimmer rack\" is used to feed stage lighting instruments. A U.S. style dimmer rack has a 208Y/120 volt 3-phase feed. Instead of just circuit breakers, the rack has a solid state electronic dimmer with its own circuit breaker for each stage circuit. This is known as a \"dimmer-per-circuit\" arrangement. The dimmers are equally divided across the three incoming phases. In a 96 dimmer rack, there are 32 dimmers on phase A, 32 dimmers on phase B, and 32 on phase C to spread out the lighting load as equally as possible. In addition to the power feed from the supply transformer in the building, a control cable from the lighting desk carries information to the dimmers in a control protocol such as DMX-512. The information includes lighting level information for each channel, by which it controls which dimmer circuits come up and go out during the lighting changes of the show (light cues), and over what fade time.\n\nDistribution boards may be surface-mounted or flush. The former arrangement provides easier alteration or addition to wiring at a later date, but the latter arrangement might be neater, particularly for a residential application. The other problem with recessing a distribution board into a wall is that if the wall is solid, a lot of brick or block might need to be removed—generally for this reason, recessed boards would only be installed on new-build projects when the required space can be built into the wall.\n\nSometimes it is desired to have a portable breaker panel, for example, for special events. In this case, a breaker panel is mounted to a board, together with various sockets. The American one pictured at the right has a cord with an L21-30 plug to supply power. Power leaves the board through four three-phase circuits: three 15 ampere circuits; and one 20 A circuit. The 15 A circuits each go to a triplex box. The 20 A circuit goes to an L21-20 receptacle, and one leg of it goes to a 20 A duplex receptacle shown at the upper left. The neon night-lights on the upper right triplex box are to show the phase presence.\n\nThe use of a load center in this type of configuration is dangerous and violates UL and NEC rules for their use. When power distribution is required on movie sets, concert stages and theatrical venues it should be provided via products listed \"for portable power distribution.\"\n\n"}
{"id": "1762062", "url": "https://en.wikipedia.org/wiki?curid=1762062", "title": "Energy-Quest", "text": "Energy-Quest\n\nEnergy-Quest (formerly \"The Hydrogen Expedition\") was an organization that sought to educate the public about the risks of an impending energy crisis due to the current worldwide dependence on fossil fuels and the benefits of energy conservation and the use of renewable energy sources. It intended to do so through a series of three record-setting ocean voyages:\n\n\nStarted and staffed by university students, the project was endorsed by several organizations and prominent individuals.\n\nThe Earthrace will be an attempt to break the speed record for circumnavigating the globe in a powerboat while using only renewable fuels. It is being supported by over 100 companies, and is led by New Zealander Pete Bethune. Energy-Quest is providing organizational and fundraising assistance to the Earthrace in the United States.\n\nThe Aurora Expedition will involve an around-the-world voyage in a powerboat which, at 20 ft, will be the smallest vessel of its kind to ever complete such a journey. Due to its limited fuel capacity, the \"Aurora\" will rely on advanced fuel-efficient technologies for propulsion. Energy-Quest thus seeks to incorporate highly clean and efficient diesel engines, solar cells, and wind turbines into this design.\n\n\"Aurora\" will stop at over forty major ports around the world, where she will showcase the cleanest and most efficient energy technologies available at the time, and her crew will promote the widespread use of such technologies. Ultimately, the Aurora Expedition is intended as a stepping stone towards the completion of the organization's final and most important endeavor – the Triton Expedition.\n\nThe Triton Expedition will be the first circumnavigation of the world by a boat powered solely by a hydrogen fuel cell. \"Triton\" will stop at major destinations around the world, where her crew will meet with local political and industrial leaders to promote the importance of alternative energy development. Through \"Triton\"'s voyage, Energy-Quest hopes to generate international media attention, and in this way stimulate the development of a renewable energy-based hydrogen economy.\n\nThe project was started in the summer of 2005 by Joseph F. Sahid, a graduate of the Phillips Exeter Academy and currently a sophomore at the University of Pennsylvania. He soon was joined by other volunteers, and an official team was rapidly assembled. The Energy Quest program has now been discontinued.\n\n\nOrganisations who officially endorse the Hydrogen Expedition:\n\n"}
{"id": "11273929", "url": "https://en.wikipedia.org/wiki?curid=11273929", "title": "Energy service company", "text": "Energy service company\n\nAn energy service company (ESCO) is a commercial or non-profit business providing a broad range of energy solutions including designs and implementation of energy savings projects, retrofitting, energy conservation, energy infrastructure outsourcing, power generation and energy supply, and risk management.\n\nA newer breed of ESCO evolving in the UK now focuses more on innovative financing methods. These include off-balance sheet vehicles which own a range of applicable equipment configured in such a way as to reduce the energy cost of a building. The building occupants, or landlord, then benefit from the energy savings and pay a fee to the ESCO SPV in return. At all times, the saving is guaranteed to exceed the fee. The ESCO starts by performing an analysis of the property, designs an energy efficient solution, installs the required elements, and maintains the system to ensure energy savings during the payback period. The savings in energy costs are often used to pay back the capital investment of the project over a five- to twenty-year period, or reinvested into the building to allow for capital upgrades that may otherwise be unfeasible. If the project does not provide returns on the investment, the ESCO is often responsible to pay the difference.\n\nThe start of the energy services business can be attributed to the energy crisis of the late 1970s, as entrepreneurs developed ways to combat the rise in energy costs. One of the earliest examples was a company in Texas, Time Energy, which introduced a device to automate the switching of lights and other equipment to regulate energy use. The primary reason that the product did not initially sell was because potential users doubted that the savings would actually materialize. To combat this doubt, the company decided to install the device upfront and ask for a percentage of the savings that was accumulated. The result was the basis for the ESCO model. Through this process, the company achieved higher sales and more return since the savings were large.\n\nAs more entrepreneurs saw this market grow, more companies came into creation. The first wave of ESCOs were often small divisions of large energy companies or small, upstart, independent companies. However, after the energy crisis came to an end, the companies had little leverage on potential clients to perform energy-saving projects, given the lower cost of energy. This prevented the growth experienced in the late 1970s from continuing. The industry grew slowly through the 1970s and 1980s, spurred by specialist firms such as Hospital Efficiency Corporation (HEC Inc.), established in 1982 to focus on the energy intensive medical sector. HEC Inc., later renamed Select Energy Services, was acquired in 1990 by Northeast Utilities, and sold in 2006 to Ameresco.\n\nWith the rising cost of energy and the availability of efficiency technologies in lighting, HVAC (heating, ventilation and air conditioning), and building energy management, ESCO projects became much more commonplace. The term ESCO has also become more widely known among potential clients looking to upgrade their building systems that are either outdated and need to be replaced, or for campus and district energy plant upgrades.\n\nWith deregulation in the U.S. energy markets in the 1990s, the energy services business experienced a rapid rise. Utilities, which for decades enjoyed the shelter of monopolies with guaranteed returns on power plant investments, now had to compete to supply power to many of their largest customers. They now looked to energy services as a potential new business line to retain their existing large customers. Also, with the new opportunities on the supply side, many energy services companies (ESCOs) started to expand into the generation market, building district power plants or including cogeneration facilities within efficiency projects. For example, in November 1996 BGA, Inc., formerly a privately held, regional energy performance contracting and consulting company was acquired by TECO Energy, and in 2004 was acquired by Chevron Corporation. In 1998, BGA entered the District Energy Plant business, completing construction on the first 3rd-party owned and operated district cooling plant in Florida.\n\nIn the wake of the Enron collapse in 2001, and the sputtering or reverse of deregulation efforts, many utilities shut down or sold their energy services businesses. There was a significant consolidation among the remaining independent firms. According to the industry group NAESCO, revenues of ESCOs in the U.S. grew by 22% in 2006, reaching $3.6 billion.\n\nAn energy service company (ESCO) is a company that provides comprehensive energy solutions to its customers, including auditing, redesigning and implementing changes to the ways the customer consumes energy, the main goal being improved efficiency. Other possible services provided include energy infrastructure outsourcing, energy supply, financing and risk management. It is this comprehensiveness of services that differentiates an ESCO from a common energy company, whose main business is solely providing energy to its customers. Typically compensation to the ESCO is performance based so that the benefits of improved energy efficiency are shared between the client and the ESCO.\n\nESCOs often use performance contracting, meaning that if the project does not provide returns on the investment, the ESCO is responsible to pay the difference, thus assuring their clients of the energy and cost savings. Therefore, ESCOs are fundamentally different from consulting engineers and equipment contractors: the former are typically paid for their advice, whereas the latter are paid for the equipment, and neither accept any project risk. The risk-free nature of the service the ESCOs provide offers a convincing incentive for their clients to invest.\n\nSome typical characteristic of ESCOs are as follows:\n\nThe energy savings project often begins with the development of ideas that would generate energy savings, and in turn, cost savings. This task is usually the responsibility of the ESCO. The ESCO often approaches a potential client with a proposal of an energy savings project and a performance contract. This ESCO is said to “drive” the project. Once the owner is aware of the possibility of an energy savings project, he or she may choose to place it out for bid, or just stick with the original ESCO. During the initial period of research and investigation, an energy auditor from the ESCO surveys the site and reviews the project's systems to determine areas where cost savings are feasible, usually free of charge to the client. This is the energy audit, and the phase is often referred to as the feasibility study. A hypothesis of the potential project is developed by the client and the auditor, and then the ESCO’s engineering development team expands upon and compiles solutions.\n\nThis next phase is referred to as the engineering and design phase, which further defines the project and can provide more firm cost and savings estimates. The engineers are responsible for creating cost-effective measures to obtain the highest potential of energy savings. These measures can range from highly efficient lighting and heating/air conditioning upgrades, to more productive motors with variable speed drives and centralized energy management systems. There is a wide array of measures that can produce large energy savings.\n\nOnce the project has been developed and a performance contract signed, the construction or implementation phase begins. Following the completion of this phase, the monitoring and maintenance or Measurement and Verification (M & V) phase begins. This phase is the verification of the pre-construction calculations and is used to determine the actual cost savings. This phase is not always included in the performance contract. In fact, there are three options the owner must consider during the performance contract review. These options are, from least to most expensive:\n\nA typical transaction involves the ESCO borrowing cash to purchase equipment or to implement energy-savings for its clients. The client pays the ESCO its regular energy cost (or a large fraction of it), but the energy savings enable the ESCO to pay only a fraction of that to their energy supplier. The difference goes to pay the interest on the loan and to profit. Typically, ESCs are able to implement and finance the efficiency improvements better than their client company could by itself.\n\nOnce the project has been defined, but before much of the engineering work has been completed, it may be necessary to choose an ESCO by putting the project “out to bid”. This is usually the case when the client has developed the project on his or her own or is required to allow others to bid on the work as required by the government. The latter is the case on any state or federally funded project. The typical process includes a Request for qualifications (RFQ) in which the interested ESCO’s submit their corporate resumes, business profiles, experience, and initial plan. Once received, the client creates a “short list” of 3-5 companies. This list is of the companies whose profile for the project best matches with the owners’ ideas in the RFQ. The client then asks for a Request for Proposal (RFP) that is a much more detailed explanation of the project. This document contains all cost savings measures, products, M & V plans, and the performance contract. The client often allows a minimum of six weeks to compile the information before having it submitted. Once submitted, the Proposals are then reviewed by the client, who may conduct interviews with the applicants. The client then selects the ESCO that presents the best possible solution to the energy project, as determined by the client. A good ESCO will help the owner put all the pieces together from start to finish. According to the Energy Services Coalition,\n\n“A qualified ESCO can help you put the pieces together:\n\nAfter installing energy conservation measures (ECMs), ESCOs often determine the energy savings resulting from the project and present the savings results to their customers. A common way to calculate energy savings is to measure the flows of energy associated with the ECM, and then to apply spreadsheet calculations to determine savings. For example, a chiller retrofit would require measurements of chilled water supply and return temperatures and kW. The benefit of this approach is that the ECM is isolated, and that only energy flows associated with the ECM itself are considered.\n\nThis method is described as Option A or Option B in the International Performance Measurement and Verification Protocol (IPMVP). Table 1 presents the different options. Option A requires some measurement and allows for estimations of some parameters. Option B requires measurement of all parameters. In both options, calculations are done (typically in spreadsheets) to determine what energy savings. Option C uses utility bills to determine energy savings.\n\nThere are many situations where Option A or Option B (Metering and Calculating) is the best approach to measuring energy savings, however, some ESCOs insist upon only using Option A or Option B, when clearly Option C would be most appropriate. If the ESCO was a lighting contractor, then Option A should work in all cases. Spot measurements of fixtures before and after, agreed upon hours of operation, and simple calculations can be inserted into a spreadsheet that can calculate savings. The same spreadsheet can be used over and over. However, for ESCOs that offer a variety of different retrofits, it is necessary to be able to employ all options so that the best option can be selected for each individual job. Controls Retrofits, or retrofits to HVAC systems are typically excellent candidates for Option C.\n\nAfter installing the energy conservation measures (ECMs), the savings created from the project must be determined. This process, termed Measurement and Verification (M&V), is frequently performed by the ESCO, but may also be performed by the customer or a third party. The International Performance Measurement and Verification Protocol (IPMVP) is the standard M&V guideline for determining actual savings created by an energy management program. Because savings are the absence of energy use, they cannot be directly measured. IPMVP provides 4 methods for using measurement to reliably determine actual savings. A plan for applying the most appropriate of the 4 general methods to a specific project is typically created and agreed upon by all parties before implementation of the ECMs.\n\nIPMVP Option A – Retrofit Isolation: Key Parameter Measurement\nSavings are determined by field measurement of the key performance parameter(s) which define the energy use of the ECM’s affected system(s). Parameters not selected for field measurement are estimated.\n\nIPMVP Option B – Retrofit Isolation: All Parameter Measurement\nSavings are determined by field measurement of the energy use of the ECM-affected system.\n\nIPMVP Option C – Whole Facility\nSavings are determined by measuring energy use at the whole facility or sub-facility level.\n\nIPMVP Option D – Calibrated Simulation\nSavings are determined through simulation of the energy use of the facility, or of a sub-facility. The simulation model must be calibrated so that it predicts an energy pattern that approximately matches actual metered data.\n\nTable 1 provides suggested IPMVP options for different project characteristics. For each project, an M&V approach which balances the uncertainty in achieved savings and the cost of the M&V plan should be selected. Some plans include only short term verification approaches and others include repeated measurements for an extended period. Because the expense of determining the amount of savings achieved erodes the benefit of the savings themselves, IPMVP suggests not spending more than 10% of the expected savings on M&V. Often M&V approaches are bundled with other monitoring, support, or maintenance services that help achieve or ensure the savings performance. These costs should not be considered M&V expenses and depending on the project and services details, may greatly exceed 10% of the savings.\n\n Once the project is completed the immediate results of energy savings (often between 15 and 35 percent), and the long term maintenance costs can be put towards the capital investment of upgrading the energy system. This is often how ESCOs and performance contracts work. The initial implementation is done, in a sense, free of charge, with the payment coming from the percentage of the energy savings collected by a financing company or the ESCO. The client may also wish to use some capital investment money to lower that percentage during the payback period. The payback period can range from five to twenty years, depending on the negotiated contract. Most state or federally funded projects have a max payback of 15 years. Once the equipment and project have been paid for, the client may be entitled to the full amount of savings to use at their will. It is also common to see large capital improvements financed through energy savings projects. Upgrades to the mechanical/electrical system, new building envelope components, or even restorations and retrofits may be included in the contract even though they have no effect on the amount of energy savings. By utilizing the energy savings, the client may be able to put the funds once used to pay for energy towards the capital improvement that would otherwise be unfeasible with the currently allotted funding.\n\nSince its creation in the 1990s, a single U. S. government program known as \"Super-ESPC\" (ESPC stands for Energy Savings Performance Contracts) has been responsible for $2.9B in ESCO contracts. The program was modified and reauthorized in December 2008, and sixteen firms were awarded Indefinite delivery/indefinite quantity (IDIQ) contracts for up to $5B each, for total potential energy-savings projects worth $80B.\n\nGrouping the sixteen firms provides a convenient illustration of the industry structure and the ways that each firm generates value through projects that use the ESCO model of energy-savings performance contracts. Equipment-affiliated firms use performance contracting as a sales channel for their products. Utility-affiliated firms offer ESCO projects as a value-added service to attract and retain large customers and generally focus only on their utility footprint. Non-utility energy services companies are product neutral, tend to have a larger geographic footprint, and typically offer a wide range of services from energy retrofits to renewable energy development.\n\nEquipment affiliated\n\nUtility affiliated\n\nNon-utility energy services\n\nIn June 2005, the GAO released a report, “Energy Savings: Performance Contracts Offer Benefits, But Vigilance Is Needed To Protect Government Interests.” The Office of the Under Secretary of Defense for Technology, Acquisition, and Logistics agreed with the GAO findings. “While these complicated contracts are structured to ensure that savings will exceed costs,” the DOD noted, “we recognize that our measurement and verification procedures must be improved to confirm estimates with actual data.” Unverified savings, often stipulated rather than proven, do not put more oil in the ground, take CO out of the air or reduce operating budgets\n\nThe GAO ESPC study brings into question whether or not there is sufficient data to prove that the gains delivered by ESCOs are sustainable over time. The study further questions the practice of having ESCOs monitoring and validating the performance of their own projects.\n\nIn fact, most buildings and facilities exhibit the same basic limitations with respect to energy conservation and optimum maintenance. US Federal studies show that major and minor building systems routinely fail to meet performance expectations, and these faults often go unnoticed over time. The functions of a building, the number of tenants, and the configuration of the space change over time in unanticipated manners that adversely affect the systems that control building performance.\n\nSurprisingly, almost all buildings, building complexes, and systems inside buildings still operate in a disconnected, stand-alone manner. Proprietary systems result in buildings that needlesly waste energy. Recent studies have found that roughly 30% of LEED certified buildings perform substantially better than anticipated, while 25% perform substantially worse than anticipated. In general, LEED certified buildings perform 25-30% better than non-LEED certified buildings with regards to energy use. It is ultimately difficult or impossible for customers to construct a single integrated picture that correlates energy usage and maintenance costs to control system performance, space usage, conservation measures, and the behavior of those using the facility space.\n\nA more recent phenomenon is the concept of combining the benefits of performance contracting with the benefits of green buildings, affectionately described as green performance contracting. The reason the concept makes sense is because for green buildings, the costliest pre-requisites to meet are usually the energy efficiency requirements. The LEED rating system requires buildings to be benchmarked using the EPA EnergyStar system. The minimum score to meet the LEED prerequisites is a score of 75 or greater (meaning the building is in the top 75 percentile of benchmarked buildings). Since performance contracting attempts to find all the sources of energy waste, then a building that has gone through the performance contracting process should meet the LEED prerequisite.\n\nGreen performance contracting can be used to achieve sustainability goals in new building design and construction as well as in existing buildings.\nNew Buildings: Higher-efficiency choices are compared to the modeled performance of the as-designed less-efficient building. Applying performance contracting to buildings being designed and built is the perfect cure for pressure to “value engineer” the efficiency and sustainability out of new buildings as they are designed. In new buildings, performance contracting bridges the gap between the first-cost and life-cycle-cost perspectives by using long-term energy savings to pay for the incremental first-cost of high-efficiency measures.\n\nExisting buildings: Green performance contracting provides a mechanism for implementing and financing the building’s efficiency and sustainability upgrades, including improved operations. Achieving sustainable building performance in existing buildings can be done at reasonable costs. If needed, system or building upgrades can be spread out over time and implemented when capital dollars become available.\n\nGreen performance contracting provides comprehensive integrated solutions to a wide variety of building, site and infrastructure improvements, and it allows building owners to pay for these building sustainability improvements, including capital improvements or renewable energy, with funds in the organization’s expense budget.\n\nThe result is a better performing building along with all the public relations and marketing benefits of green buildings.\n\nStudies show that virtually every building suffers from incompletely installed controls systems, excessive chilling and heating capacity, and an inability to obtain the data needed to let senior decision makers understand how a building is really performing. The National Institute of Standards and Technology (NIST) found that an average building lasts only two-thirds of its forecast life before it needs to be replaced or substantially retrofitted. Often the explanation for this cluster of problems is incomplete or improper building commissioning at the beginning of the building’s life cycle. (Building commissioning is the start-up process by which every new building’s systems are initially configured and calibrated to its occupancy loads to get it up and running.)\n\nAccording to NIST, the time needed to do building commissioning right is rarely available, defects and opportunities are overlooked, and system potential goes unrealized. Over time equipment performance and control sequences naturally degrade, and substandard performance or even failures of systems and components go unrecognized. The ultimate result is almost universal waste of various kinds, including substantial energy and maintenance cost.\n\nFew, if any, of these factors are addressable by the Energy Services Companies or through ESPCs because the information needed to define the real problems is not captured. There is a clear need for integrated solutions that offer the kind of accountability and transparency — and plenty of the “actual data” — that is currently lacking in the ESPC process. What is needed in fact is an independent means of continuously monitoring performance so that buildings reach peak performance sooner and maintain peak performance over time (as represented by the yellow field in the figure) despite changes in use, maintenance, energy cost, and user behavior.\n\nUS Federal reporting into OMB Scorecard\n\nA number of firms have started offering ESCO services in Europe. As in the US, some belong to utilities, some belong to manufacturers and others are independent.\n\n\n"}
{"id": "57905257", "url": "https://en.wikipedia.org/wiki?curid=57905257", "title": "Flint Wagon Works", "text": "Flint Wagon Works\n\nFlint Wagon Works of Flint, Michigan, manufactured wagons from the early 1880s. One of the world's most successful horse-drawn vehicle makers they formed with their Flint neighbours a core of the American automobile industry. In 1905 Flint was promoting itself as \"Flint the Vehicle City\".\n\nFlint Wagon Works brought the automotive industry to Flint by buying David Buick's Detroit business and moving it to Flint.\n\nOverburdened with debt and litigation Flint Wagon Works shareholders sold their business to William C. Durant as of October 12, 1911. Durant took the useful parts of the business and began to manufacture Little automobiles. Sales were hindered by poor quality product and their unappealing brandname and Durant put Little into Chevrolet in 1913.\n\nTheir business became the second of Flint's \"Big Three\" wagon builders following William A. Paterson's founded by Paterson in 1869. The third new business was founded in the mid 1880s, William C. Durant's Flint Road Cart Company later renamed Durant-Dort Carriage Company. Their main competitor was the South Bend, Indiana, Studebaker Brothers Manufacturing Company. The following numbers were reported in April 1904:\n\nBelieving buyers of their farm wagons would be interested in buying many more Buick stationary engines Flint Wagon Works bought existing supplier David Buick's business, Buick Motor Company, in September 1903 for its stationary and marine engines and its plans to manufacture automobile motors and transmissions. Flint Wagon Works built a suitable new building on the opposite side of West Kearsley Street. Some cars were built with these engines and sold with the brand name Whiting-Buick (after chairman James H Whiting). The first production Buick cars were built in that building in 1904. \n\nFlint Wagon Works dissolved the old Buick Motor Company and incorporated a wholly new entity, The Buick Motor Company, on January 29, 1904. Its initial capital stock of $37,500 in shares of $10 each was owned by: David Buick, 1,500; James H. Whiting 610 (and 978 as trustee = 1,588); George L. Walker 590; and William S. Ballenger 72. James H Whiting to be manager. Reported as 40 per cent to David Buick and 60 per cent to Flint Wagon Works stockholders. Later Charles Cumings and Charles M. Begole joined the other shareholders.\n\nThe first completed car — a Model B — was begun May 20, 1904, on the road by the beginning of July and delivered to a Dr Herbert Hills of Flint 27 July 1904.\n\nThe Buick shareholders persuaded William C. Durant to take on its management. The capital was increased to $300,000 at the beginning of November 1904 when Durant took control. Begole replaced Whiting as president, Ballenger was secretary, Whiting remained a director and Durant joined them on the board of directors. In September 1905 a further $300,000 of capital was invested in cash and in addition more shares were issued to Durant in payment for $268,000 worth of patentable inventions and other property introduced by him. Durant moved manufacture to the former plant of Imperial Wheel in Jackson, assembly remained in Flint where the bodies were made by Flint Wagon Works.\n\nDurant's friend, Sam McLaughlin of Canada's McLaughlin Motor Car Company, offshoot of another major carriage manufacturer, was one of the early customers for the engines because McLaughlin's own in-house design was not ready when their car production began. The manufacture of complete Buicks moved to Hamilton's Farm — Oak Park, Flint — but the engines did not follow until some years later. Buick engines continued to be made in the same building until 1908 or 1909.\nDemand for Flint Wagon Works' horse-drawn vehicles maintained its steady decline in the face of the rapidly rising automobile trade. Flint Wagon Works let it be known in early 1909 they were developing new automobiles which would be available in 1910. Their Whiting model A and Whiting Model C were displayed at the Detroit Auto Show in January 1910. The Model A was a light four-cylinder two-passenger roadster with an engine rated at 20 hp and the Model C was a five-passenger touring car with a four-cylinder motor rated at 40 hp. Both were of conventional design.\n\nFlint Wagon Works then fell deep into well-publicised and expensive litigation over their use of Selden patent designs in their new engines. At first the Whiting cars were built in Detroit and Flint-made bodies were added to them then the whole production process was moved to Flint . Some 1200 Whiting cars were sold in 1910 but in the fall of 1910 Flint Wagon Works failed to make a loan repayment and their bank took control appointing a special board of directors. The bank's trusteeship ended in February 1911 when more than half a million dollars more capital was given to Flint Wagon Works which paid off the bank loan.\n\nImpressed by Durant's success the Flint Wagon Works directors invited Durant to invest in their own company on the condition that he participate in its management. By this time, 1911, Durant had organised General Motors to hold not only Buick but Cadillac, Olds, Oakland and other successful automobile businesses.\n\nFlint Wagon Works was in severe financial difficulties by the beginning of 1911. Some months following that offer Durant paid them just $10 and bought Flint Wagon Works from its shareholders along with all its assets and liabilities. Wagon manufacture continued.\n\nDurant used those assets of Flint Wagon Works that were still useful to manufacture Little cars. William H. Little's Little Motor Car Company was incorporated on October 19, 1911, by Charles M. Begole, William S. Ballenger Sr, William H. Little (1876-1922), and Durant with a capital of $1,200,000.<br> A.B.C. (Alexander Brownell Cullen) Hardy (1869-1946) was appointed to manage the Little plant.\n\nThe same week Durant's other project, Chevrolet Motor Co of Detroit was incorporated with $100,000 capital stock. Its incorporators were: Louis Chevrolet, William H Little and Durant's business confidant and son-in-law, Dr Edwin R Campbell.\n\nArthur C. Mason, previously manager of the Buick engine plant in conjunction with Charles Byrne, Charles E. Wetherald and Durant set up his Mason Motor Co in 1911 to make Chevrolet engines for Durant's new Chevrolet enterprise and first occupied a Flint Wagon Works building before moving to the old Buick building. See Mason Truck\n\nUltimately all the buildings became part of the complex known as Chevrolet Manufacturing.\n"}
{"id": "39242726", "url": "https://en.wikipedia.org/wiki?curid=39242726", "title": "Froth treatment", "text": "Froth treatment\n\nFroth treatment is an oil refining technique which converts bitumen recovered from oil sands into diluted bitumin, a marketable product, which can be shipped through an oil pipeline to an oil refinery or oil terminal. The process removes heavier hydrocarbon compounds producing dilbit which can flow through a pipeline. The product can then be sold or upgraded further.\n"}
{"id": "18359501", "url": "https://en.wikipedia.org/wiki?curid=18359501", "title": "Gallery forest", "text": "Gallery forest\n\nGallery forests are forests that form as corridors along rivers or wetlands and project into landscapes that are otherwise only sparsely treed such as savannas, grasslands, or deserts.\n\nGallery forests are able to exist where the surrounding landscape does not support forests for a number of reasons. The riparian zones in which they grow offer greater protection from fire which would kill tree seedlings. In addition, the alluvial soils of the gallery habitat are often of higher fertility and have better drainage than the soils of the surrounding landscape with a more reliable water supply at depth. As a result, the boundary between gallery forest and the surrounding woodland or grassland is usually abrupt, with the ecotone being only a few metres wide.\n\nGallery forests have shrunk in extent worldwide as a result of human activities, including domestic livestock's preventing tree seedling establishment and the construction of dams and weirs causing flooding or interfering with natural stream flow. In addition to these disturbances, gallery forests are also threatened by many of the same processes that threaten savannas.\n"}
{"id": "31003015", "url": "https://en.wikipedia.org/wiki?curid=31003015", "title": "Global Steak", "text": "Global Steak\n\nGlobal Steak: Demain nos enfants mangeront des criquets is a 2010 French documentary television film directed by Anthony Orliange.\n\nThe film explores the problem of meat consumption by humans and suggests that the increasing demand of meat in the world could lead to a catastrophe.\n\n\n"}
{"id": "2660107", "url": "https://en.wikipedia.org/wiki?curid=2660107", "title": "Grindstone", "text": "Grindstone\n\nA grindstone is a round sharpening stone used for grinding or sharpening ferrous tools. Grindstones are usually made from sandstone. Grindstone machines usually have pedals for speeding up and slowing down the stone to control the sharpening process. The earliest known representation of a rotary grindstone, operated by a crank handle, is found in the Carolingian manuscript \"Utrecht Psalter\". This pen drawing from about 830 goes back to a late antique original. The \"Luttrell Psalter\", dating to around 1340, describes a grindstone rotated by two cranks, one at each end of its axle. Around 1480, the early medieval rotary grindstone was improved with a treadle and crank mechanism.\n\n\n"}
{"id": "1671224", "url": "https://en.wikipedia.org/wiki?curid=1671224", "title": "HVDC Rihand–Delhi", "text": "HVDC Rihand–Delhi\n\nThe HVDC Rihand–Delhi is a HVDC connection between Rihand and Dadri (near Delhi) in India, put into service in 1990.\nIt connects the 3,000 MW coal-based Rihand Thermal Power Station in Uttar Pradesh to the northern region of India. The project has an long bipolar overhead line. The transmission voltage is 500 kV and the maximum transmission power is 1,500 megawatts. The project was built by ABB.\n\nOn 24 June 1990, during the commissioning of the scheme, a complete quadrivalve of the Rihand converter station was destroyed, and the other two quadrivalves of the same pole badly damaged, by a fire which is believed to have started as a result of a loose connection on a grading capacitor. The fire was so intense that the valve hall was structurally damaged, and the affected converter was out of action for 18 months. Similar incidents on the Itaipu project in 1989 and the Sylmar Converter Station of the Pacific DC Intertie scheme in 1993 led to CIGRÉ publishing guidelines on the design of thyristor valves in order to reduce fire risks.\n\nHVDC Rihand-Dadri crosses north of E. Manjhpati the HVDC Ballia-Bhiwadi. This is the first crossing of two independent HVDC lines in India and one of the few worldwide.\n\n"}
{"id": "603121", "url": "https://en.wikipedia.org/wiki?curid=603121", "title": "Headstone", "text": "Headstone\n\nA headstone, tombstone, or gravestone is a stele or marker, usually stone, that is placed over a grave. They are traditional for burials in the Christian, Jewish and Muslim religions, among others. In most cases they have the deceased's name, date of birth, and date of death inscribed on them, along with a personal message, or prayer, but they may contain pieces of funerary art, especially details in stone relief. In many parts of Europe insetting a photograph of the deceased in a frame is very common.\n\nThe stele (plural stelae), as it is called in an archaeological context, is one of the oldest forms of funerary art. Originally, a tombstone was the stone lid of a stone coffin, or the coffin itself, and a gravestone was the stone slab that was laid over a grave. Now all three terms are also used for markers placed at the head of the grave. Some graves in the 18th century also contained footstones to demarcate the foot end of the grave. This sometimes developed into full kerb sets that marked the whole perimeter of the grave. Footstones were rarely annotated with more than the deceased's initials and year of death, and sometimes a memorial mason and plot reference number. Many cemeteries and churchyards have removed those extra stones to ease grass cutting by machine mower. Note that in some UK cemeteries the principal, and indeed only, marker is placed at the \"foot\" of the grave.\n\nOwing to soil movement and Downhill creep on gentle slopes, older headstones and footstones can often be found tilted at an angle. Over time, this movement can result in the stones being sited several metres away from their original location.\n\nGraves, and any related memorials are a focus for mourning and remembrance. The names of relatives are often added to a gravestone over the years, so that one marker may chronicle the passing of an entire family spread over decades. Since gravestones and a plot in a cemetery or churchyard cost money, they are also a symbol of wealth or prominence in a community. Some gravestones were even commissioned and erected to their own memory by people who were still living, as a testament to their wealth and status. In a Christian context, the very wealthy often erected elaborate memorials within churches rather than having simply external gravestones. Crematoria frequently offer similar alternatives to families who do not have a grave to mark, but who want a focus for their mourning and for remembrance. Carved or cast commemorative plaques inside the crematorium for example may serve this purpose.\n\nA cemetery may follow national codes of practice or independently prescribe the size and use of certain materials, especially in a conservation area. Some may limit the placing of a wooden memorial to six months after burial, after which a more permanent memorial must be placed. Others may require stones of a certain shape or position to facilitate grass-cutting. Headstones of granite, marble and other kinds of stone are usually created, installed, and repaired by monumental masons. Cemeteries require regular inspection and maintenance, as stones may settle, topple and, on rare occasions, fall and injure people; or graves may simply become overgrown and their markers lost or vandalised.\n\nRestoration is a specialized job for a monumental mason. Even overgrowth removal requires care to avoid damaging the carving. For example, ivy should only be cut at the base roots and left to naturally die off, never pulled off forcefully.\nMany materials have been used as markers.\n\n\n\nMarkers sometimes bear inscriptions. The information on the headstone generally includes the name of the deceased and their date of birth and death. Such information can be useful to genealogists and local historians. Larger cemeteries may require a discreet reference code as well to help accurately fix the location for maintenance. The cemetery owner, church, or, as in the UK, national guidelines might encourage the use of 'tasteful' and accurate wording in inscriptions. The placement of inscriptions is traditionally placed on the forward-facing side of the memorial but can also be seen in some cases on the reverse and around the edges of the stone itself. Some families request that an inscription be made on the portion of the memorial that will be underground.\n\nIn addition, some gravestones also bear epitaphs in praise of the deceased or quotations from religious texts, such as \"requiescat in pace\". In a few instances the inscription is in the form of a plea, admonishment, testament of faith, claim to fame or even a curse—William Shakespeare's inscription famously declares\n\n<poem>Good friend, for Jesus' sake forbear,\nTo dig the dust enclosèd here.\nBlest be the man that spares these stones,\nAnd cursed be he that moves my bones.</poem>\n\nOr a warning about mortality, such as this Persian poetry carved on an ancient tombstone in the Tajiki capital of Dushanbe.\n<poem>I heard that mighty Jamshed the King\nCarved on a stone near a spring of water these words:\n\nOr a simpler warning of inevitability of death:\n<poem>Remember me as you pass by,\nAs you are now, so once was I,\nAs I am now, so you will be,\nPrepare for death and follow me.</poem>\n\nHeadstone engravers faced their own \"year 2000 problem\" when still-living people, as many as 500,000 in the United States alone, pre-purchased headstones with pre-carved death years beginning with 19–.\n\nBas-relief carvings of a religious nature or of a profile of the deceased can be seen on some headstones, especially up to the 19th century. Since the invention of photography, a gravestone might include a framed photograph or cameo of the deceased; photographic images or artwork (showing the loved one, or some other image relevant to their life, interests or achievements) are sometimes now engraved onto smooth stone surfaces.\n\nSome headstones use lettering made of white metal fixed into the stone, which is easy to read but can be damaged by ivy or frost. Deep carvings on a hard-wearing stone may weather many centuries exposed in graveyards and still remain legible. Those fixed on the inside of churches, on the walls, or on the floor (often as near the altar as possible) may last much longer: such memorials were often embellished with a monumental brass.\n\nThe choice of language and/or script on gravestones has been studied by sociolinguists as indicators of language choices and language loyalty. For example, by studying cemeteries used by immigrant communities, some languages were found to be carved \"long after the language ceased to be spoken\" in the communities. In other cases, a language used in the inscription may indicate a religious affiliation.\n\nMarker inscriptions have also been used for political purposes, such as the grave marker installed in January 2008 at Cave Hill Cemetery in Louisville, Kentucky by Mathew Prescott, an employee of PETA. The grave marker is located near the grave of KFC founder Harland Sanders and bears the acrostic message \"KFC tortures birds\". The group placed its grave marker to promote its contention that KFC is cruel to chickens.\n\nGravestones may be simple upright slabs with semi-circular, rounded, gabled, pointed-arched, pedimental, square or other shaped tops. During the 18th century, they were often decorated with \"memento mori\" (symbolic reminders of death) such as skulls or winged skulls, winged cherub heads, heavenly crowns, urns or the picks and shovels of the grave digger. Somewhat unusual were more elaborate allegorical figures, such as Old Father Time, or emblems of trade or status, or even some event from the life of the deceased (particularly how they died). Large tomb chests, false sarcophagi as the actual remains were in the earth below, or smaller coped chests were commonly used by the gentry as a means of commemorating a number of members of the same family. In the 19th century, headstone styles became very diverse, ranging from plain to highly decorated, and often using crosses on a base or other shapes differing from the traditional slab. They might be replaced by more elaborately carved markers, such as crosses or angels. Simple curb surrounds, sometimes filled with glass chippings, were popular during the mid-20th century.\n\nIslamic headstones are traditionally more a rectangular upright shaft, often topped with a carved topknot symbolic of a turban; but in Western countries more local styles are often used.\n\nSome form of simple decoration may be employed. Special emblems on tombstones indicate several familiar themes in many faiths. Some examples are:\n\nGreek letters might also be used:\n\n\nOver time a headstone may settle or its fixings weaken. After several instances where unstable stones have fallen in dangerous circumstances, some burial authorities \"topple test\" headstones by firm pressure to check for stability. They may then tape them off or flatten them.\n\nThis procedure has proved controversial in the UK, where an authority's duty of care to protect visitors is complicated because it often does not have any ownership rights over the dangerous marker. Authorities that have knocked over stones during testing or have unilaterally lifted and laid flat any potentially hazardous stones have been criticised, after grieving relatives have discovered that their relative's marker has been moved. Since 2007 Consistory Court and local authority guidance now restricts the force used in a topple test and requires an authority to consult relatives before moving a stone. In addition, before laying a stone flat, it must be recorded for posterity.\n\n\n"}
{"id": "28094230", "url": "https://en.wikipedia.org/wiki?curid=28094230", "title": "Hutan Pinus/Janthoi Nature Reserve", "text": "Hutan Pinus/Janthoi Nature Reserve\n\nThe Hutan Pinus/Janthoi Nature Reserve is a restricted nature reserve located near the city of Kota Jantho in the north west tip of the island of Sumatra in Indonesia. It was established in 1984.\n"}
{"id": "672218", "url": "https://en.wikipedia.org/wiki?curid=672218", "title": "Isospin", "text": "Isospin\n\nIn nuclear physics and particle physics, isospin (I) is a quantum number related to the strong interaction. More specifically, isospin symmetry is a subset of the flavour symmetry seen more broadly in the interactions of baryons and mesons. \n\nThe name of the concept contains the term \"spin\" because its quantum mechanical description is mathematically similar to that of angular momentum (in particular, in the way it couples; for example, a proton-neutron pair can be coupled either in a state of total isospin 1 or in one of 0.). Unlike angular momentum, however, it is a dimensionless quantity, and is not actually any type of spin.\n\nEtymologically, the term was derived from isotopic spin, a confusing term to which nuclear physicists prefer isobaric spin, which is more precise in meaning. Before the concept of quarks were introduced, particles that are affected equally by the strong force but had different charges (e.g. protons and neutrons) were treated as being different states of the same particle, but having isospin values related to the number of charge states.. A close examination of isospin symmetry ultimately led directly to the discovery and understanding of quarks, and of the development of Yang–Mills theory. Isospin symmetry remains an important concept in particle physics.\n\nIn the modern formulation, isospin (\"I\") is defined as a vector quantity in which up and down quarks have a value of \"I\" = , with the 3rd-component (\"I\") being for up quarks, and − for down quarks, while all other quarks have \"I\" = 0. In general, for hadrons, therefore\nwhere \"n\" and \"n\" are the numbers of up and down quarks respectively.\n\nIn any combination of quarks, the 3rd component of the isospin vector (\"I\") could either be aligned between a pair of quarks, or face the opposite direction, giving different possible values for total isospin for any combination of quark flavours. Hadrons with the same quark content but different total isospin can be distinguished experimentally, verifying that flavour is actually a vector quantity, not a scalar (up vs down simply being a projection in the quantum mechanical \"z\"-axis of flavour-space).\n\nFor example, a strange quark can be combined with an up and a down quark to form a baryon, but there are two different ways the isospin values can combine - either adding (due to being flavour-aligned) or cancelling out (due to being in opposite flavour-directions). The isospin 1 state (the Sigma 0) and the isospin 0 state (the Lambda 0) have different experimentally detected masses and half-lives.\n\nIsospin is regarded as a symmetry of the strong interaction under the action of the Lie group SU(2), the two states being the up flavour and down flavour. In quantum mechanics, when a Hamiltonian has a symmetry, that symmetry manifests itself through a set of states that have the same energy (the states are described as being \"degenerate\"). In simple terms, that the energy operator for the strong interaction gives the same result when an up quark and an otherwise identical down quark are swapped around. \n\nLike the case for regular spin, the isospin operator I is vector-valued: it has three components I, I, I which are coordinates in the same 3-dimensional vector space where the 3 representation acts. Note that it has nothing to do with the physical space, except similar mathematical formalism. Isospin is described by two quantum numbers: \"I\", the total isospin, and \"I\", an eigenvalue of the I projection for which flavor states are eigenstates, not an \"arbitrary projection\" as in the case of spin. In other words, each \"I\" state specifies certain flavor state of a multiplet. The third coordinate (\"z\"), to which the \"3\" subscript refers, is chosen due to notational conventions which relate bases in 2 and 3 representation spaces. Namely, for the spin- case, components of I are equal to Pauli matrices divided by 2 and I = \"τ\", where\n\nWhile the forms of these matrices are isomorphic to those of spin, \"these\" Pauli matrices only act within the Hilbert space of isospin, not that of spin, and therefore is common to denote them with τ rather than σ to avoid confusion.\n\nAlthough isospin symmetry is actually very slightly broken, SU(3) symmetry is more badly broken, due to the much higher mass of the strange quark compared to the up and down. The discovery of charm, bottomness and topness could lead to further expansions up to SU(6) flavour symmetry, which would hold if all six quarks were identical. However, the very much larger masses of the charm, bottom and top quarks means that SU(6) flavour symmetry is very badly broken in nature (at least at low energies) and assuming this symmetry leads to qualitatively and quantitatively incorrect predictions. In modern applications, such as lattice QCD, isospin symmetry is often treated as exact while the heavier quarks must be treated separately.\n\nHadron nomenclature is based on isospin.\n\nIsospin was introduced as a concept in 1932, well before the 1960s development of the quark model. The man who introduced it, Werner Heisenberg, did so to explain symmetries of the then newly discovered neutron (symbol n):\n\nThis behavior is not unlike the electron, where there are two possible states based on their spin. Other properties of the particle are conserved in this case. Heisenberg introduced the concept of another conserved quantity that would cause the proton to turn into a neutron and vice versa. In 1937, Eugene Wigner introduced the term \"isospin\" to indicate how the new quantity is similar to spin in behavior, but otherwise unrelated..\n\nProtons and neutrons were then grouped together as nucleons because they both have nearly the same mass and interact in nearly the same way, if the (much weaker) electromagnetic interaction is neglected. In particle physics, the near mass-degeneracy of the neutron and proton points to an approximate symmetry of the Hamiltonian describing the strong interactions. It was thus convenient to treat them as being different states of the same particle. \n\nHeisenberg's particular contribution was to note that the mathematical formulation of this symmetry was in certain respects similar to the mathematical formulation of spin, whence the name \"isospin\" derives. The neutron and the proton are assigned to the doublet (the spin-, 2, or fundamental representation) of SU(2). The pions are assigned to the triplet (the spin-1, 3, or adjoint representation) of SU(2). Though, there is a difference from the theory of spin: the group action does not preserve flavor (specifically, the group action is an exchange of flavour).\n\nSimilar to a spin particle, which has two states, protons and neutrons were said to be of isospin . The proton and neutron were then associated with different isospin projections \"I\" = + and − respectively.\n\nAlthough the neutron does in fact have a slightly higher mass due to isospin breaking (this is now understood to be due to the difference in the masses of the up and down quarks and the effects of the electromagnetic interaction), the appearance of an approximate symmetry is useful even if it doesn't exactly hold; the small symmetry breakings can be described by a perturbation theory, which gives rise to slight differences between the near-degenerate states\n\nWhen constructing a physical theory of nuclear forces, one could simply assume that it does not depend on isospin, although the total isospin should be conserved.\n\nThese considerations would also prove useful in the analysis of meson-nucleon interactions after the discovery of the pions in 1947. The three pions (, , ) could be assigned to an isospin triplet with \"I\" = 1 and \"I\" = +1, 0 or −1. By assuming that isospin was conserved by nuclear interactions, the new mesons were more easily accommodated by nuclear theory.\n\nAs further particles were discovered, they were assigned into isospin multiplets according to the number of different charge states seen: 2 doublets \"I\" =  of K mesons (, ),(, ), a triplet \"I\" = 1 of Sigma baryons (, , ), a singlet \"I\" = 0 Lambda baryon (), a quartet \"I\" =  Delta baryons (, , , ), and so on. \n\nThe power of isospin symmetry and related methods comes from the observation that families of particles with similar masses tend to correspond to the invariant subspaces associated with the irreducible representations of the Lie algebra SU(2). In this context, an invariant subspace is spanned by basis vectors which correspond to particles in a family. Under the action of the Lie algebra SU(2), which generates rotations in isospin space, elements corresponding to definite particle states or superpositions of states can be rotated into each other, but can never leave the space (since the subspace is in fact invariant). This is reflective of the symmetry present. The fact that unitary matrices will commute with the Hamiltonian means that the physical quantities calculated do not change even under unitary transformation. In the case of isospin, this machinery is used to reflect the fact that the mathematics of the strong force behaves the same if a proton and neutron are swapped around (in the modern formulation, the up and down quark).\n\nFor example, the particles known as the Delta baryons—baryons of spin were grouped together because they all have nearly the same mass (approximately ), and interact in nearly the same way.\n\nThey could be treated as the same particle, with the difference in charge being due to the particle being in different states. Isospin was introduced in order to be the variable that defined this difference of state. In an analogue to spin, an isospin projection (denoted \"I\") is associated to each charged state; since there were four Deltas, four projections were needed. Like spin, isospin projections were made to vary in increments of 1. Hence, in order to have four increments of 1, an isospin value of is required (giving the projections \"I\" = , , −, −). Thus, all the Deltas were said to have isospin \"I\" =  and each individual charge had different \"I\" (e.g. the was associated with \"I\" = +). \n\nIn the isospin picture, the four Deltas and the two nucleons were thought to simply be the different states of two particles. The Delta baryons are now understood to be made of a mix of three up and down quarks - uuu ((), uud (), udd (), and ddd (); the difference in charge being difference in the charges of up and down quarks ((+ \"e\" and − \"e\" respectively); yet, they can also be thought of as the excited states of the nucleons.\n\nAttempts have been made to promote isospin from a global to a local symmetry. In 1954, Chen Ning Yang and Robert Mills suggested that the notion of protons and neutrons, which are continuously rotated into each other by isospin, should be allowed to vary from point to point. To describe this, the proton and neutron direction in isospin space must be defined at every point, giving local basis for isospin. A gauge connection would then describe how to transform isospin along a path between two points.\n\nThis Yang–Mills theory describes interacting vector bosons, like the photon of electromagnetism. Unlike the photon, the SU(2) gauge theory would contain self-interacting gauge bosons. The condition of gauge invariance suggests that they have zero mass, just as in electromagnetism.\n\nIgnoring the massless problem, as Yang and Mills did, the theory makes a firm prediction: the vector particle should couple to all particles of a given isospin \"universally\". The coupling to the nucleon would be the same as the coupling to the kaons. The coupling to the pions would be the same as the self-coupling of the vector bosons to themselves.\n\nWhen Yang and Mills proposed the theory, there was no candidate vector boson. J. J. Sakurai in 1960 predicted that there should be a massive vector boson which is coupled to isospin, and predicted that it would show universal couplings. The rho mesons were discovered a short time later, and were quickly identified as Sakurai's vector bosons. The couplings of the rho to the nucleons and to each other were verified to be universal, as best as experiment could measure. The fact that the diagonal isospin current contains part of the electromagnetic current led to the prediction of rho-photon mixing and the concept of vector meson dominance, ideas which led to successful theoretical pictures of GeV-scale photon-nucleus scattering.\n\nThe discovery and subsequent analysis of additional particles, both mesons and baryons, made it clear that the concept of isospin symmetry could be broadened to an even larger symmetry group, now called flavor symmetry. Once the kaons and their property of strangeness became better understood, it started to become clear that these, too, seemed to be a part of an enlarged symmetry that contained isospin as a subgroup. The larger symmetry was named the Eightfold Way by Murray Gell-Mann, and was promptly recognized to correspond to the adjoint representation of SU(3). To better understand the origin of this symmetry, Gell-Mann proposed the existence of up, down and strange quarks which would belong to the fundamental representation of the SU(3) flavor symmetry.\n\nIn the quark model, the isospin projection (\"I\") followed from the up and down quark content of particles; \"uud\" for the proton and \"udd\" for the neutron. Technically, the nucleon doublet states are seen to be linear combinations of products of 3-particle isospin doublet states and spin doublet states. That is, the (spin-up) proton wave function, in terms of quark-flavour eigenstates, is described by\n\nand the (spin-up) neutron by\n\nHere, formula_5 is the up quark flavour eigenstate, and formula_6 is the down quark flavour eigenstate, while formula_7 and formula_8 are the eigenstates of formula_9. Although these superpositions are the technically correct way of denoting a proton and neutron in terms of quark flavour and spin eigenstates, for brevity, they are often simply referred to as \"\"uud\" and \"udd\"\". Note also that the derivation above assumes exact isospin symmetry and is modified by SU(2)-breaking terms.\n\nSimilarly, the isospin symmetry of the pions are given by:\n\nAlthough the discovery of the quarks led to reinterpretation of mesons as a vector bound state of a quark and an antiquark, it is sometimes still useful to think of them as being the gauge bosons of a hidden local symmetry.\n\nIsospin is similar to, but should not be confused with weak isospin. Briefly, weak isospin is the gauge symmetry of the weak interaction which connects quark and lepton doublets of left-handed particles in all generations; for example, up and down quarks, top and bottom quarks, electrons and electron neutrinos. By contrast (strong) isospin connects only up and down quarks, acts on both chiralities (left and right) and is a global (not a gauge) symmetry.\n\n\n\n"}
{"id": "19984680", "url": "https://en.wikipedia.org/wiki?curid=19984680", "title": "Kryevidhi Wind Farm", "text": "Kryevidhi Wind Farm\n\nThe Kryevidhi Wind Farm is an under construction wind power project in Albania. It will have 75 individual wind turbines with a nominal output of around 2 MW which will deliver up to 150 MW of power, enough to power over 100,250 homes, with a capital investment required of approximately US$300 million.\n"}
{"id": "2384029", "url": "https://en.wikipedia.org/wiki?curid=2384029", "title": "Los Cardones National Park", "text": "Los Cardones National Park\n\nThe Los Cardones National Park () is a national park of Argentina, located in the center-west of the province of Salta, within the San Carlos and Cachi Departments, in the Argentine Northwest.\n\nThe park protects an area of the Argentine Monte ecoregion.\nThe park has an area of 650 square kilometres, with hills and ravines at the height levels between 2,700 m and 5,000 m. It gets its name from the prevalence of bush formations of cardon grande cactus. It features fossil remains of extinct animals, as well as dinosaur tracks.\n\nThe protected area was created in 1996, when the National Parks Administration acquired the land from private owners.\n\nMost of the park has an arid climate that is characterized by a large thermal amplitude (large difference between day and night temperatures). The park receives an average rainfall of ; most of it falling between November to March. Snowfall is extremely rare in low-lying areas. Mean temperatures range from in winter to in summer.\n\n"}
{"id": "29548278", "url": "https://en.wikipedia.org/wiki?curid=29548278", "title": "Minimum design metal temperature", "text": "Minimum design metal temperature\n\nMDMT is one of the design conditions for pressure vessels engineering calculations, design and manufacturing according to the ASME Boilers and Pressure Vessels Code. Each pressure vessel that conforms to the ASME code has its own MDMT, and this temperature is stamped on the vessel nameplate. The precise definition can sometimes be a little elaborate, but in simple terms the MDMT is a temperature arbitrarily selected by the user of type of fluid and the temperature range the vessel is going to handle. The so-called \"arbitrary\" MDMT must be lower than or equal to the CET (which is an environmental or \"process\" property, see below) and must be higher than or equal to the (MDMT) (which is a material property).\n\nCritical exposure temperature (CET) is the lowest anticipated temperature to which the vessel will be subjected, taking into consideration lowest operating temperature, operational upsets, autorefrigeration, atmospheric temperature, and any other sources of cooling. In some cases it may be the lowest temperature at which significant stresses will occur and not the lowest possible temperature.\n\n(MDMT) is the lowest temperature permitted according to the metallurgy of the vessel fabrication materials and the thickness of the vessel component, that is, according to the low temperature embrittlement range and the charpy impact test requirements per temperature and thickness, for each one of the vessel's components.\n\n"}
{"id": "37525550", "url": "https://en.wikipedia.org/wiki?curid=37525550", "title": "Moscow–Kashira HVDC transmission system", "text": "Moscow–Kashira HVDC transmission system\n\nThe Moscow–Kashira HVDC transmission system was an early high-voltage direct current (HVDC) connection between the town of Kashira and the city of Moscow in Russia, where the terminal was at . The system was built using mercury-arc valves and other equipment removed from the Elbe Project in Berlin at the end of World War II. Although primarily experimental in nature, the system was the first true static, electronic, high-voltage DC scheme to enter service. Earlier DC transmission schemes had either used electromechanical converters based on the Thury system, such as the Lyon–Moutiers DC transmission scheme or had been at only medium voltage, such as the 12 kV frequency converter scheme at Mechanicville, New York in the United States.\n\nThe scheme had a nominal power rating of 30 MW and was operated at different times as a bipole at ±100 kV or a monopole, with earth return, at 200 kV. For most of the transmission distance of 125 km, the transmission conductor was underground cable but some sections were converted to overhead line.\nThe route of the line was between Kashira and Moscow parallel to the existing 110 kV overhead AC line.\n\nInitial operating experience was gained using three series-connected, single anode mercury arc valves in each converter arm, but by 1959 experience had been gained with operating with either one or two mercury arc valves in series per arm.\n\nThe experience gained with multiple valves in series in each arm was not wholly successful and the little available literature suggests that the reliability of the scheme was poor. This may have been because the valves, unlike those being developed in Sweden by Dr Uno Lamm, lacked the external anode voltage divider networks which were found necessary to obtain reliable operation at high voltage.\n\nNevertheless the scheme provided valuable experience for designing the much larger ±400 kV Volgograd–Donbass project which was completed in 1965 using mercury arc valves of entirely Soviet design.\n\nIn 1969 the first thyristor valve built in the Soviet Union was installed in the converter . The scheme is however today not in use any more, whereby it is unknown when it was shut down. It is unknown, if the cable is still in place and if the converter hall in Kashira is still standing.\n\n"}
{"id": "40945512", "url": "https://en.wikipedia.org/wiki?curid=40945512", "title": "Myriagram", "text": "Myriagram\n\nThe myriagram () is a former French and metric unit of mass equal to 10,000 grams (\"myriad\" being the Greek word for ten thousand). Although never as widely used as the kilogram, the myriagram was employed during the 19th century as a replacement for the earlier American customary system quarter, which was equal to .\n\nIn 1975, the United States, having previously authorized use of the myriagram in 1866, declared the term no longer acceptable.\n\n"}
{"id": "5735522", "url": "https://en.wikipedia.org/wiki?curid=5735522", "title": "Petroleum pricing in Nova Scotia", "text": "Petroleum pricing in Nova Scotia\n\nPetroleum pricing in Nova Scotia is based on the Petroleum Products Pricing Act which governs the wholesale and minimum and maximum price of gasoline and diesel fuels that are authorised in Nova Scotia.\n\nIn the spring of 2004, some consumers and an association representing some retailers (the Retail Gasoline Dealers Association of Nova Scotia) complained about the rising price of gasoline and diesel fuel, and the closure of rural gas stations due to low volumes and low margins. After appearing before an all party committee on petroleum pricing, the Nova Scotia legislature passed Bill 79 \"The Nova Scotia Petroleum Products Pricing Act\". After a year of not proclaiming it law, Rodney MacDonald, the Premier of Nova Scotia announced in May 2006 that petroleum prices will be regulated, beginning on 1 July 2006, two weeks following New Brunswick's announcement of doing the same.\n\nThe price of gasoline and diesel fuel are based on the price on the New York Mercantile Exchange as a benchmark in Canadian funds. This plus a 6 cent per litre wholesale margin, plus a transportation allowance of 0.5 to 2 cents per litre, and a retail margin of 4 to 5.5 cents per litre (or to a maximum of 7.5 cents per litre for full-serve gasoline).\n\nThe transportation of the product is calculated based on six zones in Nova Scotia:\n\n\n\nThere is no notice of change in prices. Every Friday, prices may go up or down or stay the same, depending on the market price over the past 7 days.\n\nScheduled price changes occur every second Thursday starting 13 July 2006.\nThe interrupter is based on a formula that will determine whether the market prices of fuels have changed enough to warrant a price change . This “interrupter” formula will run every second Wednesday beginning 19 July 2006.\nFor the interrupter formula to trigger a price change, a 5-day average NYMEX spot price from the last price change to the current date, will be calculated. If this new average NYMEX spot price changes by ±4 cents per litre over the last NYMEX spot price average, used to set the scheduled price change, the pump price may be adjusted using the new average NYMEX spot price as the baseline. If the interrupter is not triggered on the interruption date, the current pump price will continue until the next price change the following Thursday.\nIn extreme circumstances, precipitated by a weather or perhaps a geopolitical event, a catastrophic interrupter could initiated whereby international product prices change by significant amounts, say ±15 cents per litre. This interrupter would be initiated on the day of the price change or the next day when rack prices in Nova Scotia would change.\n\nMany critics argue that this method of controlling the price of petroleum products is not in the public interest; these critics include the Liberal Party of Nova Scotia, the Nova Scotia Chambers of Commerce, and vocal critics, mainly in the Halifax Regional Municipality, from the public at large. When this was first brought about many including in the New Democratic Party and the Retail Gasoline Dealers Association of Nova Scotia wanted a system similar to Prince Edward Island for lower prices. The ones opposing this scheme argued that the PEI system not only keeps prices higher but it does not allow for competition. The lower gasoline prices are a result of PEI's not charging the Provincial Sales Tax of 10% unlike in Nova Scotia where the Harmonized Sales Tax of 15% is charged. In New Brunswick, the HST is also charged, but they set a maximum price for most of entire province (no zones) without setting a minimum price. As a price comparison, on 1 July 2006, the same day Nova Scotia started its system, the maximum price in New Brunswick was 112.4 cents per litre for regular self serve compared to Nova Scotia's minimum price of 113.3 to 115.2 (depending on the zone) cents per litre.\n\n"}
{"id": "13136417", "url": "https://en.wikipedia.org/wiki?curid=13136417", "title": "Piracema", "text": "Piracema\n\nPiracema (from tupi \"pirá\", fish + \"sema\", exit) is the name given to the period of the year when fish within the Paraguay River drainage basin―which includes the Pantanal region in the Brazilian states of Mato Grosso and Mato Grosso do Sul―reproduce.\n\nThe season lasts from October to March, during which the fish swim upstream to lay their eggs and reproduce. Thus the season is critical for the maintenance of fish populations in the waters of the local rivers and lakes. Both of the Brazilian states prohibit fishing during this period.\n\nMeasures have been taken as a way of preventing impacts from overfishing during the \"piracema\" period.\n\nIn the states of Mato Grosso and Mato Grosso do Sul it is a crime to fish in any location that has been designated by any environmental institution.\n\nThe use of explosives, toxic substances, fishing gear such as spears, harpoons, drag nets, and diving equipment are all prohibited by law, since they can affect the life cycles of the fish population.\n\nSome species of fish are protected and they can only be caught if they are within a certain size range. For example, the golden dourado (\"Salminus brasiliensis\") can only be caught if it is no longer than 55 cm.\n\n"}
{"id": "825748", "url": "https://en.wikipedia.org/wiki?curid=825748", "title": "Propene", "text": "Propene\n\nPropene, also known as propylene or methyl ethylene, is an unsaturated organic compound having the chemical formula <chem>C3H6</chem>. It has one double bond, and is the second simplest member of the alkene class of hydrocarbons. It is a colorless gas with a faint petroleum-like odor\n\nPropene is a byproduct of oil refining and natural gas processing. During oil refining, ethylene, propene, and other compounds are produced as a result of cracking larger hydrocarbons. A major source of propene is naphtha cracking intended to produce ethylene, but it also results from refinery cracking producing other products. Propene can be separated by fractional distillation from hydrocarbon mixtures obtained from cracking and other refining processes; refinery-grade propene is about 50 to 70%.\n\nA shift to lighter steam cracker feedstocks with relatively lower propene yields and reduced motor gasoline demand in certain areas has reduced propene supply.\n\nIn the Phillips Triolefin and the Olefin conversion technology interconverts propylene is interconverted with ethylene and 2-butenes. Rhenium and molybdenum catalysts are used: \n\nThe technology is founded on an olefin metathesis reaction discovered at Phillips Petroleum Company.. Propene yields of about 90 wt% are achieved. \n\nRelated is the Methanol-to-Olefins/Methanol-to-Propene converts synthesis gas (syngas) to methanol, and then converts the methanol to ethylene and/or propene. The process produces water as by-product. Synthesis gas is produced from the reformation of natural gas or by the steam-induced reformation of petroleum products such as naphtha, or by gasification of coal.\n\nPropane dehydrogenation (PDH) converts propane into propene and by-product hydrogen. The propene from propane yield is about 85 m%. Reaction by-products (mainly hydrogen) are usually used as fuel for the propane dehydrogenation reaction. As a result, propene tends to be the only product, unless local demand exists for hydrogen. This route is popular in regions, such as the Middle East, where there is an abundance of propane from oil/gas operations. In this region, the propane output is expected to be capable of supplying not only domestic needs, but also the demand from China, where many PDH projects are scheduled to go on stream. However, as natural gas offerings in the United States are significantly increasing due to the rising exploitation of shale gas, propane prices are decreasing. Chemical companies are already planning to establish PDH plants in the USA to take advantage of the low price raw material, obtained from shale gas. Numerous plants dedicated to propane dehydrogenation are currently under construction around the world. There are already five licensed technologies. The propane dehydrogenation process may be accomplished through different commercial technologies. The main differences between each of them concerns the catalyst employed, design of the reactor and strategies to achieve higher conversion rates.\n\nHigh severity fluid catalytic cracking (FCC) uses traditional FCC technology under severe conditions (higher catalyst-to-oil ratios, higher steam injection rates, higher temperatures, etc.) in order to maximize the amount of propene and other light products. A high severity FCC unit is usually fed with gas oils (paraffins) and residues, and produces about 20–25 m% propene on feedstock together with greater volumes of motor gasoline and distillate byproducts.\n\nSeveral companies have explored biomanufacturing using engineered enzymes. The starting materials for the fermentation could be either sugars or petrochemicals.\n\nPropene production has remained static at around 35 million tonnes (Europe and North America only) from 2000 to 2008, but it has been increasing in East Asia, most notably Singapore and China. Total world production of propene is currently about half that of ethylene.\n\nPropene is the second most important starting product in the petrochemical industry after ethylene. It is the raw material for a wide variety of products. Manufacturers of the plastic polypropylene account for nearly two thirds of all demand. Polypropylene end uses include films, fibers, containers, packaging, and caps and closures. Propene is also used for the production of important chemicals such as propylene oxide, acrylonitrile, cumene, butyraldehyde, and acrylic acid. In the year 2013 about 85 million tonnes of propene were processed worldwide.\n\nPropene and benzene are converted to acetone and phenol via the cumene process. \nPropene is also used to produce isopropanol (propan-2-ol), acrylonitrile, propylene oxide, and epichlorohydrin.\nThe industrial production of acrylic acid involves the catalytic partial oxidation of propene. Propene is also an intermediate in the one-step propane selective oxidation to acrylic acid.\nIn industry and workshops, propene is used as an alternative fuel to acetylene in Oxy-fuel welding and cutting, brazing and heating of metal for the purpose of bending. It has become a standard in BernzOmatic products and others in MAPP substitutes, now that true MAPP gas is no longer available.\n\nPropene resembles other alkenes in that it undergoes addition reactions relatively easily at room temperature. The relative weakness of its double bond explains its tendency to react with substances that can achieve this transformation. Alkene reactions include: 1) polymerization, 2) oxidation, 3) halogenation and hydrohalogenation, 4) alkylation, 5) hydration, 6) oligomerization, and 7) hydroformylation.\n\nPropene undergoes combustion reactions in a similar fashion to other alkenes. In the presence of sufficient or excess oxygen, propene burns to form water and carbon dioxide.\n\nWhen insufficient oxygen is present for complete combustion, incomplete combustion occurs allowing carbon monoxide and/or soot (carbon) to be formed as well.\n\nPropene is a product of combustion from forest fires, cigarette smoke, and motor vehicle and aircraft exhaust. It is an impurity in some heating gases. Observed concentrations have been in the range of 0.1-4.8 parts per billion (ppb) in rural air, 4-10.5 ppb in urban air, and 7-260 ppb in industrial air samples.\n\nIn the United States and some European countries a threshold limit value of 500 parts per million (ppm) was established for occupational (8-hour time-weighted average) exposure. It is considered a volatile organic compound (VOC) and emissions are regulated by many governments, but it is not listed by the U.S. Environmental Protection Agency (EPA) as a hazardous air pollutant under the Clean Air Act. With a relatively short half-life, it is not expected to bioaccumulate.\n\nPropene has low acute toxicity from inhalation. Inhalation of the gas can cause anesthetic effects and at very high concentrations, unconsciousness. However, the asphyxiation limit for humans is about 10 times higher (23%) than the lower flammability level.\n\nSince propene is volatile and flammable, precautions must be taken to avoid fire hazards in the handling of the gas. If propene is loaded to any equipment capable of causing ignition, such equipment should be shut down while loading, unloading, connecting or disconnecting.\nPropene is usually stored as liquid under pressure, although it is also possible to store it safely as gas at ambient temperature in approved containers.\n\nPropene acts as a central nervous system depressant via allosteric agonism of the GABA receptor. Excessive exposure may result in sedation and amnesia, progressing to coma and death in a mechanism equivalent to benzodiazepine overdose. Intentional inhalation may also result in death via asphyxiation (sudden inhalant death).\n\nOn September 30, 2013, NASA announced that the Cassini orbiter spacecraft, part of the Cassini-Huygens mission, had discovered small amounts of naturally occurring propene in the atmosphere of Titan using spectroscopy.\n\n"}
{"id": "35313070", "url": "https://en.wikipedia.org/wiki?curid=35313070", "title": "Richard Handl", "text": "Richard Handl\n\nRichard Handl (born 23 May 1980) is a Swedish man who experimented with building a breeder reactor in his apartment in Ängelholm, Sweden for six months in 2011 with the intention to create a nuclear reaction.\n\nAfter working in a factory for four years, Handl became unemployed and decided to start a collection of the elements on the periodic table. Out of curiosity he began experimenting with his collected elements to see if he could create a nuclear reaction.\n\nHandl's experiment included the acquisition of fissile material from outside the country, a radiator suitable for transmutation, and instruments to measure the reaction, including a Geiger counter. He spent about 5000 to 6,000 Swedish kronor in materials and equipment. Experiments were done with tritium, americium, aluminium, beryllium, thorium, radium, and uranium, most of which he acquired from foreign companies. One step involved cooking americium, radium and beryllium in on a stove in order to more easily mix the ingredients; this resulted in an explosion. He kept a blog called \"Richard's Reactor\" in which he documented the progress of the reactor.\n\nOn 22 July 2011 he was detained by the police after having contacted the Swedish Radiation Safety Authority (SSM) to inquire as to whether his project was legal or not. His apartment was searched, and the radioactive materials as well as his computer were taken by the police. He was released, then convicted in July 2014 on the violation of the radiation safety act and the violation of . A fine of 13,600 kronor was imposed.\n"}
{"id": "29752099", "url": "https://en.wikipedia.org/wiki?curid=29752099", "title": "Sahara Solar Breeder Project", "text": "Sahara Solar Breeder Project\n\nThe Sahara Solar Breeder Project is a joint Japanese-Algerian universities plan to use the abundant solar energy and sand in the Sahara desert to build silicon manufacturing plants, and solar power plants, in a way that their products are used in a \"breeding\" manner to build more and more such plants. The project's declared goal is to provide 50% of the world’s electricity by 2050, using superconductors to deliver the power to distant locations.\n\nThe project was first proposed by Hideomi Koinuma from the Science Council of Japan, at the 2009 G8+5 Academies' Meeting in Rome.\n\n\n \n \n"}
{"id": "28685638", "url": "https://en.wikipedia.org/wiki?curid=28685638", "title": "Sinan Solar Power Plant", "text": "Sinan Solar Power Plant\n\nThe Sinan solar power plant is a 24 MW photovoltaic power station in Sinan, Jeollanam-do, South Korea. , it is the largest photovoltaic installation in Asia. The project was developed by the German company Conergy and it cost US$150 million. It was built by the Dongyang Engineering & Construction Corporation.\n\n"}
{"id": "46324244", "url": "https://en.wikipedia.org/wiki?curid=46324244", "title": "Skeletal changes of organisms transitioning from water to land", "text": "Skeletal changes of organisms transitioning from water to land\n\nInnovations conventionally associated with terrestrially first appeared in aquatic elpistostegalians such as \"Panderichthys rhombolepis\", \"Elpistostege watsoni\", and \"Tiktaalik roseae\". Phylogenetic analyses distribute the features that developed along the tetrapod stem and display a stepwise process of character acquisition, rather than abrupt. The complete transition occurred over a period of 25 million years beginning with the tetrapodomorph diversification in the Middle Devonian (380 myr).\n\nBy the Upper Devonian period, the fin-limb transition as well as other skeletal changes such as gill arch reduction, opercular series loss, mid-line fin loss, and scale reduction were already completed in many aquatic organisms. As aquatic tetrapods began their transition to land, several skeletal changes are thought to have occurred to allow for movement and respiration on land. Some adaptations required to adjust to non-aquatic life include the movement and use of alternating limbs, the use of pelvic appendages as sturdy propulsors, and the use of a solid surface at the organism’s base to generate propulsive force required for walking.\n\nThe Osteolepiformes and Elpistostegalia are two crown groups of rhipidistians with respect to the tetrapods. The development of skull roof and cheekbone patterns in these organisms match those found in the first tetrapods. Palatal and nasal skeletal features like choanae are present in these groups and are also observed in modern amphibians. This indicates that incipient air breathing was developed, as well as modification of the hyoid arch towards stapes development. These characteristics account for why osteichthyans are accepted as the sister group of tetrapods.\n\nThe elpistostegalid fish are considered the most apomorphic of fish in comparison to tetrapods. From well-preserved fossils, it is observed that they share a paltybasic skull with eye ridges, and external nares situated on the margin of the mouth. Development of eye ridges and flatting of the skull are also observed in primitive fossil amphibians and reptiles. The most likely reason for the traits to be adaptive was for their use in aerial vision above the waterline. The traits enabled animals to check area on land for safe spots if being chased by a predator in water, as well as being useful for searching for prey items above the water. The water-based lateral line system was used substantially by these aquatic tetrapods to detect danger from predators. Within the Osteichthyan diversification, there were no changes related to respiration in the transition as can be seen by the nasal region and palatal morphology in elpistostegalid fishes. The primary change from basic ostelepiform ancestors to the first elpistostegalid in the middle Devonian was to the pre-existing roof skulls.\n\nIn \"Elginerpeton pancheni\", a prototetrapod from the late Frasnian, basic tetrapod characteristics in the lower jaw and the cranium are observed. The taxon is believed to fill the gap between elpistostegalid fishes and well-preserved Devonian tetrapods. The \"Elginerpeton\" is considered more derived than the elpistostegalid fishes due to presence of paired fangs on the parasymphysial toothplate, a slender shaped anterior coronoid, and in the loss of the intracranial joint and coronoid fossa. The loss of the intercranial joint was a direct functional necessity to strengthen the broad and long platybasic skull when the animal was out of the water. The tubular lower jaw of the \"Elginerpeton\", compared to the flat-lamina jaw shape of fishes gave it superior cross-sectional force, required when not supported in an aquatic setting – allowing for opening of the mouth outside of water. The adaptation may also be interpreted as a specialization for buccopharyngeal breathing. It is speculated to be the first step towards aerial respiration in the transition from fish to tetrapod.\n\nIn the tetrapod and higher clades from the lower-middle Famennian there are several defining changes on the basis of anatomy of \"Ichthyostega\", \"Tulerpeton\", and \"Acanthostega\". In the cranium, there is a stapes derived from the hyomandibular of fishes; a single bilateral pair of nasal bones, and a fenestra ovalis in the otic capsule of the braincase. The opening of the otic wall of the braincase can be considered a paedomorphic feature for tetrapods and is linked to the stapes functionally. The stapes was thought to be just a structural support between the palate and the stapedial plate of the braincase. In the \"Acanthostega\", it is likely that due to the otic capsule of the brain case being mesial to the stapedial plate, sound was picked up from the palate or the otic notch to allow for rudimentary hearing. It was able to perceive vibrations by opening its mouth by way of the palate. Other factors that caused aquatic tetrapods to spend more time on land caused the development of terrestrial hearing with the development of a tympanum within an otic notch and developed by convergent evolution at least three times.\nThere was also a change in the dermal bones of the skull in the aquatic tetrapods. It involved the enlargement of the jugal, ceasing the contact of the maxilla with the squamosal and the single bilateral pair of nasal bones. The feature allows for a stronger bite as well as increasing the strength of the skull.\n\nFeeding on land is a completely different task than feeding in water. Water is much more dense and viscous compared to air, causing hunting techniques adapted in water to be less successful when applied on land. The main technique used in water is suction feeding and is used by most aquatic vertebrates. This technique does not function in air so animals use methods of overtaking prey with jaws followed by biting down. Transitional forms prior to fully developed terrestrial tetrapods such as \"Acanthostega\", are thought to have captured prey in the water. Large coronoid fangs are present in the fishes \"Eusthenopteron\", \"Panderichthys\", and \"Tiktaalik\", and the early tetrapod, \"Ventasega\". In \"Acanthostega\", which is more derived, the large teeth are absent. In \"Eusthenopetron\" and \"Panderichthys\", an ossified operculum is exhibited unlike in the \"Tiktaalik\", \"Ventastega\", and \"Acanthostega\". These differences as well as reductions of the gill chamber and changes in the nature of the lower jaw are hypothesized to indicate a reduced reliance on suction feeding in early tetrapods in comparison to osteolepiform fish. This morphological data is not enough however to prove that suction feeding was less used as the morphological changes have been found in fish that use the suction feeding mechanism.\n\nCranial sutures are indicators of skull function and morphologies can be linked to specific feeding modes. Transitional feeding changes can be observed by examining cross sectional morphology of a suture in taxa of the fish-tetrapod transition. Comparing positionally comparable sutures in extant fish allows for the creation of a sutural morphospace. The main cause of sutural deformation is caused by strain during feeding activity, most prominent with feeding mechanisms involving sucking a prey into the mouth. There is a tension anteriorly, and compression posteriorly strain patterns are observed in \"Polypterus\", a prey-sucking predator. In terrestrial tetrapod \"Phonerpeton\", there is compression between the frontals and parietals and a complex loading between the post parietals. There is no evidence of tensile strain in any sutures. \"Acanthostega\" fossil records demonstrate that no strain pattern was exhibited that relate to prey capture by means of suction. The load compression is similar to extant tetrapods. It is most likely that the organism captured prey by biting in the water or near the edge of the water. This finding indicates that the terrestrial mode of feeding first emerged in an aquatic environment.\n\nThe cranial endoskeleton of \"T. roseae\" shares derived features with tetrapods. There was a loss of opercular and extrascapular elements, enhancing head mobility in \"T. roseae\" compared to other tetrapodomorph fish. The formation of the neck allowed for locomotion in shallow waters. This environment allows for less motility compared to the three-dimensional space that fish are able to orient themselves in. The body of the organism in these environments would be fixed in the shallow pools with appendages planted on a substrate.\n\nIn the \"Acanthostega\" and \"Ichthyostega\", which are considered to be more derived than other basal aquatic tetrapods, the pectoral girdle is decoupled from the skull. There is also a loss of the dorsal pectoral girdle bones, which permits a large degree of movement for the shoulder. This allowed for a greater degree of movement, and is a necessity for improving aquatic maneuveurs and terrestrial locomotion. This could have been driven by the need to lift the head to aid aerial respiration by using nostrils and choanae.\n\nLimbs in vertebrates are occasionally organized into stylopod (relating to the humerus and femur), zeugopod (relating to the radius and tibia, along with associated structures) and autopod (relating to digits) categories, although anatomically, the evolutionary differences between these groups in early tetrapods tends to be vague.\nThe transition from fins to limbs occurred once an endoskeleton entered the base of the fin, as seen in today's lungfish. This is thought to have originated in the group Sarcopterygians, including osteolipiforms like \"Eusthenopteron\", due to the homology of the tetrapod forelimb and the osteolepiform fin endoskeleton.\n\n\"Acanthostega\" is a partially aquatic tetrapod with developed limbs that shares features common with the earlier tetrapods, \"Panderichthys\" and \"Eusthenopteron\". Like \"Panderichthys\", the humerus of \"Acanthostega\" is flattened dorso-ventrally, the intermedium terminates level with the radius, and the endoskeleton can be divided into stylopodium, zeugopodium and autopodium segments. Similar to \"Eusthenopteron\", the radials do not articulate with the radius on the distal end. \"Acanthostega\" also has a 1:2 ratio of humerus to radius and ulna, a feature seen in all tetrapods higher than \"Acanthostega\" on the phylogeny.\n\nUnlike \"Panderichthys\", \"Acanthostega\" hind limbs are at least the size of its fore limbs, if not larger. This development of larger limbs is required to physically support the organism during emergence from an aquatic setting to land. The humerus and femur of \"Acanthostega\" also contain evidence of greater development of the appendicular muscles compared to more aquatic tetrapods, hinting at the presence of digits.\n\nSimilarly, \"Ossinodus\" has two hindlimbs located bilaterally and proximodistally aymmetrical. Due to the presence of a small femur during juvenile development, this Carboniferous- period tetrapod is thought to be aquatic during juvenile development; only emerging onto land once it reaches adulthood. \"Ossinodus\" also has a broad, flat tibia, akin to \"Acanthostega\", and is thought to be only partially terrestrial.\n\nThe development of the pelvic region was crucial for the adaptation from water to land, yet some features of tetrapod locomotion are thought to have arose before the origin of digited limbs or the transition from water to land. The fossil record of early tetrapods shows evidence of distinct pelvic development occurring in osteolepiforms, further supporting osteolepiform ancestry of terrestrial tetrapods.\n\n\"Acanthostega\" has a large pelvis, with the iliac region articulating with the axial skeleton and a broad ischial plate. It has a sacrum; a fundamental skeletal feature that allows the organism to transfer force produced in its hindlimbs to its axial skeleton, and move in a terrestrial environment. A pubo-ischiadic symphysis is also observed, uniting the two pelvic halves.\n\nIn contrast, \"Protopterus annectens\" (a member of lungfish, thought to be a sister group to tetrapods) has a small, anatomically simpler pelvis, a derived limb endoskeleton and a lack of digits. Yet, it shares the ability to lift itself using a solid surface as a base with its pelvic region with \"Acanthostega\" and is also observed to move with tetrapod-like locomotion in an aquatic environment. This illustrates that a fundamental innovation in tetrapods is also found in a lower, sister taxon, in which members lack a sacrum.\n\n\"Acanthostega\" is the earliest example of a digitized tetrapod. The humerus and femur of \"Acanthostega\" contain evidence of greater development of the appendicular muscles compared to more aquatic tetrapods. \"Acanthostega\" has a total lack of dermal fin rays and displays the presence of two or more spool-shaped bones or cartilages articulating individually in antero-posterial sets on the distal end of its limbs. This feature can now be distinguished as digits instead of the endoskeletal radials seen in earlier tetrapods.\n\n\"Pederpes\", a tetrapod from the Early Carboniferous period, also has hindlimbs containing 5 digits that are rotated to face anteriorly. Unlike previous tetrapods, who have been only partially adapted to land, \"Pederpes\" has the novel ability to bend its limbs and propel itself forwards in a terrestrial setting. This is attributed to the symmetry of the digits and limbs in \"Pederpes\", allowing it to rotate its hindlimbs to an anteriorly facing position and propel itself from the edge of the foot when moving forward. This morphological development of bendable wrists and ankles can distinguish \"Pederpes\" the first true terrestrial tetrapod.\n"}
{"id": "10778792", "url": "https://en.wikipedia.org/wiki?curid=10778792", "title": "South American Energy Council", "text": "South American Energy Council\n\nThe South American Energy Council is a body set up to co-ordinate the regional energy policy of the Union of South American Nations. Its establishment was agreed at the first South American Energy Summit, which took place on April 16–17 2007 on Isla Margarita in the Venezuelan state of Nueva Esparta.\n\n\n<br>\n"}
{"id": "30869", "url": "https://en.wikipedia.org/wiki?curid=30869", "title": "Tesla turbine", "text": "Tesla turbine\n\nThe Tesla turbine is a bladeless centripetal flow turbine patented by Nikola Tesla in 1913. It is referred to as a \"bladeless turbine\". The Tesla turbine is also known as the \"boundary layer turbine\", \"cohesion-type turbine\", and \"Prandtl layer turbine\" (after Ludwig Prandtl) because it uses the \"boundary layer effect\" and not a fluid impinging upon the blades as in a conventional turbine. Bioengineering researchers have referred to it as a multiple disk centrifugal pump. One of Tesla's desires for implementation of this turbine was for geothermal power, which was described in \"Our Future Motive Power\".\n\nThe guiding idea for developing Tesla turbine is the fact that in order to attain the highest economy, the changes in the velocity and direction of movement of fluid should be as gradual as possible. Therefore, the propelling fluid of Tesla turbine moves in natural paths or stream lines of least resistance.\n\nA Tesla turbine consists of a set of smooth disks, with nozzles applying a moving fluid to the edge of the disk. The fluid drags on the disk by means of viscosity and the adhesion of the surface layer of the fluid. As the fluid slows and adds energy to the disks it spirals into the center exhaust. Since the rotor has no projections it is very sturdy.\n\nTesla wrote, \"This turbine is an efficient self-starting prime mover which may be operated as a steam or mixed fluid turbine at will, without changes in construction and is on this account very convenient. Minor departures from the turbine, as may be dictated by the circumstances in each case, will obviously suggest themselves but if it is carried out on these general lines it will be found highly profitable to the owners of the steam plant while permitting the use of their old installation. However, the best economic results in the development of power from steam by the Tesla turbine will be obtained in plants especially adapted for the purpose.\"\n\nThis turbine can also be applied to condensing plants operating with high vacuum. In such a case, owing to the very great expansion ratio, the exhaust mixture will be at a relatively low temperature and suitable for admission to the condenser.\n\nAll the plates and washers are fitted on and keyed to a sleeve threaded at the ends and equipped with nuts and collars for drawing the thick end-plates together or the collars may be simply forced onto it and the ends upset. The sleeve has a hole fitting snugly on the shaft, to which it is fastened as usual.\n\nThis construction permits free expansion and contraction of each plate individually under the varying influence of heat and centrifugal force and possesses a number of other advantages which are of considerable practical importance. A larger active plate area and consequently more power is obtained for a given width, improving efficiency. Warping is virtually eliminated and smaller side clearances may be used, which results in diminished leakage and friction losses. The rotor is better adapted for dynamic balancing and through rubbing friction resists disturbing influences thereby ensuring quieter running. For this reason and also because the discs are not rigidly joined it is protected against damage which might otherwise be caused by vibration or excessive speed.\n\nThe Tesla turbine has the trait of being in an installation normally working with a mixture of steam and products of combustion and in which the exhaust heat is used to provide steam which is supplied to the turbine, providing a valve governing the supply of the steam so that the pressures and temperatures can be adjusted to the optimum working conditions.\n\nAs diagrammed, a Tesla turbine installation is:\n\nAn efficient Tesla turbine requires close spacing of the disks. For example, a steam-powered type must maintain 0.4 millimeter (.016 inch) inter-disk spacing. The disks must be extremely smooth to minimize surface and shear losses. Disks must also be very thin to prevent drag and turbulence at disk edges. Unfortunately, preventing disks from warping and distorting was a major challenge in Tesla's time. It is thought that this inability to prevent the disks distorting contributed to the commercial failure of the turbines, because metallurgical technology at the time was not able to produce disks of sufficient quality and rigidity.\n\nThe device can function as a pump if a similar set of disks and a housing with an involute shape (versus circular for the turbine) are used. In this configuration a motor is attached to the shaft. The fluid enters near the center, is given energy by the disks, then exits at the periphery. The Tesla turbine does not use friction in the conventional sense; precisely, it avoids it, and uses adhesion (the Coandă effect) and viscosity instead. It uses the boundary layer effect on the disc blades.\n\nSmooth rotor disks were originally proposed but these gave poor starting torque. Tesla subsequently discovered that smooth rotor disks with small washers bridging the disks in ~12–24 places around the perimeter of a 10″ disk and a second ring of 6–12 washers at a sub-diameter made for a significant improvement in starting torque without compromising efficiency.\n\nTesla's patents state that the device was intended for the use of fluids as motive agents, as distinguished from the application of the same for the propulsion or compression of fluids (though the device can be used for those purposes as well). As of 2016, the Tesla turbine has not seen widespread commercial use since its invention. The Tesla pump, however, has been commercially available since 1982 and is used to pump fluids that are abrasive, viscous, shear sensitive, contain solids, or are otherwise difficult to handle with other pumps. Tesla himself did not procure a large contract for production. The main drawback in his time, as mentioned, was the poor knowledge of materials characteristics and behaviors at high temperatures. The best metallurgy of the day could not prevent the turbine disks from moving and warping unacceptably during operation.\n\nIn 2003, Scott O’Hearen took a patent on the Radial turbine blade system. This invention utilizes a combination of the concepts of a smooth runner surface for working fluid frictional contact and that of blades projecting axially from plural transverse runner faces.\n\nToday, many amateur experiments in the field have been conducted using Tesla turbines which use compressed air, steam as its power source (the steam being generated with heat from fuel combustion, from a vehicle's turbocharger or from solar radiation). The issue of the warping of the discs has been partially solved using new materials such as carbon fiber.\n\nOne proposed current application for the device is a waste pump, in factories and mills where normal vane-type turbine pumps typically get blocked.\n\nApplications of the Tesla turbine as a multiple-disk centrifugal blood pump have yielded promising results.Biomedical engineering research on such applications has been continued into the 21st century.\n\nIn 2010, was issued to Howard Fuller for a wind turbine based on the Tesla design.\n\nIn Tesla's time, the efficiency of conventional turbines was low because turbines used a direct drive system that severely limited the potential speed of a turbine to whatever it was driving. At the time of introduction, modern ship turbines were massive and included dozens, or even hundreds of stages of turbines, yet produced extremely low efficiency due to their low speed. For example, the turbine on the Titanic weighed over 400 tons, ran at just 165rpm, and used steam at a pressure of only 6 PSI. This limited it to harvesting waste steam from the main power plants, a pair of reciprocating steam engines. The Tesla turbine also had the ability to run on higher temperature gasses than bladed turbines of the time contributed to its greater efficiency. Eventually axial turbines were given gearing to allow them to operate at higher speeds, but efficiency of axial turbines remained very low in comparison to the Tesla Turbine.\n\nAs time went on, competing Axial turbines became dramatically more efficient and powerful, a second stage of reduction gears was introduced in most cutting edge U.S. naval ships of the 1930s. The improvement in steam technology gave the U.S. Navy aircraft carriers a clear advantage in speed over both Allied and enemy aircraft carriers, and so the proven axial steam turbines became the preferred form of propulsion until the 1973 oil embargo took place. The oil crisis drove the majority of new civilian vessels to turn to diesel engines. Axial steam turbines still had not exceeded 50% efficiency by that time, and so civilian ships chose to utilize diesel engines due to their superior efficiency. By this time, the comparably efficient Tesla turbine was over 60 years old.\nTesla's design attempted to sidestep the key drawbacks of the bladed axial turbines, and even the lowest estimates for efficiency still dramatically outperformed the efficiency of axial steam turbines of the day. However, in testing against more modern engines, the Tesla Turbine had expansion efficiencies far below contemporary steam turbines and far below contemporary reciprocating steam engines. It does suffer from other problems such as shear losses and flow restrictions, but this is partially offset by the relatively massive reduction in weight and volume. Some of Tesla turbine's advantages lie in relatively low flow rate applications or when small applications are called for. The disks need to be as thin as possible at the edges in order not to introduce turbulence as the fluid leaves the disks. This translates to needing to increase the number of disks as the flow rate increases. Maximum efficiency comes in this system when the inter-disk spacing approximates the thickness of the boundary layer, and since boundary layer thickness is dependent on viscosity and pressure, the claim that a single design can be used efficiently for a variety of fuels and fluids is incorrect. A Tesla turbine differs from a conventional turbine only in the mechanism used for transferring energy to the shaft. Various analyses demonstrate the flow rate between the disks must be kept relatively low to maintain efficiency. Reportedly, the efficiency of the Tesla turbine drops with increased load. Under light load, the spiral taken by the fluid moving from the intake to the exhaust is a tight spiral, undergoing many rotations. Under load, the number of rotations drops and the spiral becomes progressively shorter. This will increase the shear losses and also reduce the efficiency because the gas is in contact with the discs for less distance.\n\nEfficiency is a function of power output. A moderate load makes for high efficiency. Too heavy a load increases the slip in the turbine and lowers the efficiency; with too light a load, little power is delivered to the output, which also decreases efficiency (to zero at idle). This behaviour is not exclusive to Tesla turbines.\nThe turbine efficiency of the gas Tesla turbine is estimated to be above 60, reaching a maximum of 95 percent . Keep in mind that turbine efficiency is different from the cycle efficiency of the engine using the turbine. Axial turbines which operate today in steam plants or jet engines have efficiencies of about 60–70% (Siemens Turbines Data). This is different from the cycle efficiencies of the plant or engine which are between approximately 25% and 42%, and are limited by any irreversibilities to be below the Carnot cycle efficiency. Tesla claimed that a steam version of his device would achieve around 95 percent efficiency. Actual tests of a Tesla steam turbine at the Westinghouse works showed a steam rate of 38 pounds per horsepower-hour, corresponding to a turbine efficiency in the range of 20%, while contemporary steam turbines could often achieve turbine efficiencies of well over 50%. The thermodynamic efficiency is a measure of how well it performs compared to an isentropic case. It is the ratio of the ideal to the actual work input/output. Turbine efficiency is defined as the ratio of the ideal change in enthalpy to the real enthalpy for the same change in pressure.\n\nIn the 1950s, Warren Rice attempted to re-create Tesla's experiments, but he \"did not\" perform these early tests on a pump built strictly in line with the Tesla's patented design (it, among other things, was not a Tesla multiple staged turbine nor did it possess Tesla's nozzle). Rice's experimental single stage system's working fluid was air. Rice's test turbines, as published in early reports, produced an overall measured efficiency of 36–41% for a \"single stage\". Higher percentages would be expected if designed as originally proposed by Tesla.\n\nIn his final work with the Tesla turbine and published just prior to his retirement, Rice conducted a bulk-parameter analysis of model laminar flow in \"multiple disk\" turbines. A very high claim for rotor efficiency (as opposed to overall device efficiency) for this design was published in 1991 titled \"Tesla Turbomachinery\". This paper states:\n\nModern \"multiple stage\" bladed turbines typically reach 60–70% efficiency, while large steam turbines often show turbine efficiency of over 90% in practice. Volute rotor matched Tesla-type machines of reasonable size with common fluids (steam, gas, and water) would also be expected to show efficiencies in the vicinity of 60–70% and possibly higher.\n\n\n\n\"Tesla\"\n\"Other\"\n\n\n\n\n\n"}
{"id": "44815791", "url": "https://en.wikipedia.org/wiki?curid=44815791", "title": "The Oil Prince", "text": "The Oil Prince\n\nThe Oil Prince () is a 1965 West German-Yugoslav Western film starring Stewart Granger. It was also known as Rampage at Apache Wells. The screenplay is based on a novel by Karl May. The film was shot on location in Yugoslavia.\n\nIt recorded admissions of 409,817 in France, 1,449,558 in Spain, and over 3 million in Germany.\n\n"}
{"id": "24298031", "url": "https://en.wikipedia.org/wiki?curid=24298031", "title": "Tianjin Climate Exchange", "text": "Tianjin Climate Exchange\n\nTianjin Climate Exchange (TCX) is a domestic carbon market cap-and-trade scheme exchange. Jeff Huang is assistant chairman of Tianjin Climate Exchange and vice-president of Chicago Climate Exchange.\n\nIt is China’s first integrated exchange for trading of environmental financial instruments \n\nTCX is a joint venture between Chicago Climate Exchange, the municipal government of Tianjin and the asset management unit of PetroChina, the country’s largest oil and gas producer.\n\nCap-and-trade schemes are programs under which member companies commit to lowering their greenhouse gas emissions by a certain amount in a certain period of time and trade carbon credits generated by this. As China does not have a national cap on emissions, any such scheme would be voluntary, similar to the situation in the US when the Chicago Climate Exchange launched in 2003.\n\nOn September 25 2008, Tianjin Climate Exchange, co-established by CNPC Assets Management Co., Ltd. (holding a 53% stake), Tianjin Property Rights Exchange (北方产权交易市场) (holding a 22% stake), and Chicago Climate Exchange (CCX) (holding a 25% stake), was unveiled in the Tianjin Binhai New Area. \n\nAt the request of the State Council, Tianjin Climate Exchange is established as China's first comprehensive platform for trading carbon credits under the Clean Development Mechanism, and will promote environmental protection and emission reduction by means of market and financial measures. \n\nTianjin Climate Exchange has the following goals: to help enterprises cost-effectively reduce emissions of pollutants, such as sulfur dioxide, chemical oxygen demand, etc.; to help enterprises achieve maximum energy efficiency at minimum cost; to help enterprises manage environmental risks and meet increasing disclosure requirements; and to provide enterprises with integrated international emissions market access and experience. \n\nIn 2006, Tianjin Binhai New Area was designed by the State Council of the PRC as the national experimental zone for comprehensive reforms related to financial innovation, land and administrative management.\n\nChina's Eleventh Five-Year Plan (2006-10) called for cutting energy consumption per unit of GDP up to 20 percent by 2010 while reducing major pollutants, such as sulfur dioxide (SO2) by 10 percent.[3]\n\nTPRE was launched in 1994, under government approval. It is a government agent under the charge of Tianjin SASAC and is the only appointed exchange authorized by Tianjin SASAC for state-owned assets and equities transaction. It is one of three national institutions permitted by SASAC to transact assets and equities of SOEs under control of central government.[2]\n\n\n\n"}
{"id": "30042", "url": "https://en.wikipedia.org/wiki?curid=30042", "title": "Tin", "text": "Tin\n\nTin is a chemical element with the symbol Sn (from ) and atomic number 50. It is a post-transition metal in group 14 of the periodic table of elements. It is obtained chiefly from the mineral cassiterite, which contains stannic oxide, SnO. Tin shows a chemical similarity to both of its neighbors in group 14, germanium and lead, and has two main oxidation states, +2 and the slightly more stable +4. Tin is the 49th most abundant element and has, with 10 stable isotopes, the largest number of stable isotopes in the periodic table, thanks to its magic number of protons. It has two main allotropes: at room temperature, the stable allotrope is β-tin, a silvery-white, malleable metal, but at low temperatures it transforms into the less dense grey α-tin, which has the diamond cubic structure. Metallic tin does not easily oxidize in air.\n\nThe first tin alloy used on a large scale was bronze, made of tin and copper, from as early as 3000 BC. After 600 BC, pure metallic tin was produced. Pewter, which is an alloy of 85–90% tin with the remainder commonly consisting of copper, antimony, and lead, was used for flatware from the Bronze Age until the 20th century. In modern times, tin is used in many alloys, most notably tin/lead soft solders, which are typically 60% or more tin and in the manufacture of transparent, electrically conducting films of indium tin oxide in optoelectronic applications. Another large application for tin is corrosion-resistant tin plating of steel. Because of the low toxicity of inorganic tin, tin-plated steel is widely used for food packaging as tin cans. However, some organotin compounds can be almost as toxic as cyanide.\n\nTin is a soft, malleable, ductile and highly crystalline silvery-white metal. When a bar of tin is bent, a crackling sound known as the \"tin cry\" can be heard from the twinning of the crystals. Tin melts at low temperatures of about , the lowest in group 14. The melting point is further lowered to for 11 nm particles.\nβ-tin (the metallic form, or white tin, BCT structure), which is stable at and above room temperature, is malleable. In contrast, α-tin (nonmetallic form, or gray tin), which is stable below , is brittle. α-tin has a diamond cubic crystal structure, similar to diamond, silicon or germanium. α-tin has no metallic properties at all because its atoms form a covalent structure in which electrons cannot move freely. It is a dull-gray powdery material with no common uses other than a few specialized semiconductor applications. These two allotropes, α-tin and β-tin, are more commonly known as \"gray tin\" and \"white tin\", respectively. Two more allotropes, γ and σ, exist at temperatures above   and pressures above several GPa. In cold conditions, β-tin tends to transform spontaneously into α-tin, a phenomenon known as \"tin pest\". Although the α-β transformation temperature is nominally , impurities (e.g. Al, Zn, etc.) lower the transition temperature well below and, on the addition of antimony or bismuth, the transformation might not occur at all, increasing the durability of the tin.\n\nCommercial grades of tin (99.8%) resist transformation because of the inhibiting effect of the small amounts of bismuth, antimony, lead, and silver present as impurities. Alloying elements such as copper, antimony, bismuth, cadmium, and silver increase its hardness. Tin tends rather easily to form hard, brittle intermetallic phases, which are often undesirable. It does not form wide solid solution ranges in other metals in general, and few elements have appreciable solid solubility in tin. Simple eutectic systems, however, occur with bismuth, gallium, lead, thallium and zinc.\n\nTin becomes a superconductor below 3.72 K and was one of the first superconductors to be studied; the Meissner effect, one of the characteristic features of superconductors, was first discovered in superconducting tin crystals.\n\nTin resists corrosion from water, but can be attacked by acids and alkalis. Tin can be highly polished and is used as a protective coat for other metals. A protective oxide (passivation) layer prevents further oxidation, the same that forms on pewter and other tin alloys. Tin acts as a catalyst when oxygen is in solution and helps to accelerate the chemical reaction.\n\nTin has ten stable isotopes, with atomic masses of 112, 114 through 120, 122 and 124, the greatest number of any element. Of these, the most abundant are Sn (almost a third of all tin), Sn, and Sn, while the least abundant is Sn. The isotopes with even mass numbers have no nuclear spin, while those with odd have a spin of +1/2. Tin, with its three common isotopes Sn, Sn and Sn, is among the easiest elements to detect and analyze by NMR spectroscopy, and its chemical shifts are referenced against .\n\nThis large number of stable isotopes is thought to be a direct result of the atomic number 50, a \"magic number\" in nuclear physics. Tin also occurs in 29 unstable isotopes, encompassing all the remaining atomic masses from 99 to 137. Apart from Sn, with a half-life of 230,000 years, all the radioisotopes have a half-life of less than a year. The radioactive Sn, discovered in 1994, and Sn are one of the few nuclides with a \"doubly magic\" nucleus: despite being unstable, having very lopsided proton–neutron ratios, they represent endpoints beyond which stability drops off rapidly. Another 30 metastable isomers have been characterized for isotopes between 111 and 131, the most stable being Sn with a half-life of 43.9 years.\n\nThe relative differences in the abundances of tin's stable isotopes can be explained by their different modes of formation in stellar nucleosynthesis. Sn through Sn inclusive are formed in the \"s\"-process (slow neutron capture) in most stars and hence they are the most common isotopes, while Sn and Sn are only formed in the \"r\"-process (rapid neutron capture) in supernovae and are less common. (The isotopes Sn through Sn also receive contributions from the \"r\"-process.) Finally, the rarest proton-rich isotopes, Sn, Sn, and Sn, cannot be made in significant amounts in the \"s\"- or \"r\"-processes and are considered among the p-nuclei, whose origins are not well understood yet. Some speculated mechanisms for their formation include proton capture as well as photodisintegration, although Sn might also be partially produced in the \"s\"-process, both directly, and as the daughter of long-lived In.\n\nThe word \"tin\" is shared among Germanic languages and can be traced back to reconstructed Proto-Germanic \"*tin-om\"; cognates include German ', Swedish ' and Dutch '. It is not found in other branches of Indo-European, except by borrowing from Germanic (e.g., Irish ' from English).\n\nThe Latin name ' originally meant an alloy of silver and lead, and came to mean 'tin' in the 4th century—the earlier Latin word for it was ', or \"white lead\". ' apparently came from an earlier ' (meaning the same substance), the origin of the Romance and Celtic terms for \"tin\". The origin of '/' is unknown; it may be pre-Indo-European.\n\nThe ' speculates on the contrary that ' is derived from (the ancestor of) Cornish \"\", and is proof that Cornwall in the first centuries AD was the main source of tin.\n\nTin extraction and use can be dated to the beginnings of the Bronze Age around 3000 BC, when it was observed that copper objects formed of polymetallic ores with different metal contents had different physical properties. The earliest bronze objects had a tin or arsenic content of less than 2% and are therefore believed to be the result of unintentional alloying due to trace metal content in the copper ore. The addition of a second metal to copper increases its hardness, lowers the melting temperature, and improves the casting process by producing a more fluid melt that cools to a denser, less spongy metal. This was an important innovation that allowed for the much more complex shapes cast in closed moulds of the Bronze Age. Arsenical bronze objects appear first in the Near East where arsenic is commonly found in association with copper ore, but the health risks were quickly realized and the quest for sources of the much less hazardous tin ores began early in the Bronze Age. This created the demand for rare tin metal and formed a trade network that linked the distant sources of tin to the markets of Bronze Age cultures.\n\nCassiterite (SnO), the tin oxide form of tin, was most likely the original source of tin in ancient times. Other forms of tin ores are less abundant sulfides such as stannite that require a more involved smelting process. Cassiterite often accumulates in alluvial channels as placer deposits because it is harder, heavier, and more chemically resistant than the accompanying granite. Cassiterite is usually black or generally dark in color, and these deposits can be easily seen in river banks. Alluvial (placer) deposits could be easily collected and separated by methods similar to gold panning.\n\nIn the great majority of its compounds, tin has the oxidation state II or IV.\n\nHalide compounds are known for both oxidation states. For Sn(IV), all four halides are well known: SnF, SnCl, SnBr, and SnI. The three heavier members are volatile molecular compounds, whereas the tetrafluoride is polymeric. All four halides are known for Sn(II) also: SnF, SnCl, SnBr, and SnI. All are polymeric solids. Of these eight compounds, only the iodides are colored.\n\nTin(II) chloride (also known as stannous chloride) is the most important tin halide in a commercial sense. Illustrating the routes to such compounds, chlorine reacts with tin metal to give SnCl whereas the reaction of hydrochloric acid and tin produces SnCl and hydrogen gas. Alternatively SnCl and Sn combine to stannous chloride by a process called comproportionation:\n\nTin can form many oxides, sulfides, and other chalcogenide derivatives. The dioxide SnO (cassiterite) forms when tin is heated in the presence of air. SnO is amphoteric, which means that it dissolves in both acidic and basic solutions. Stannates with the structure [Sn(OH)], like K[Sn(OH)], are also known, though the free stannic acid H[Sn(OH)] is unknown.\n\nSulfides of tin exist in both the +2 and +4 oxidation states: tin(II) sulfide and tin(IV) sulfide (mosaic gold).\nStannane (SnH), with tin in the +4 oxidation state, is unstable. Organotin hydrides are however well known, e.g. tributyltin hydride (Sn(CH)H). These compound release transient tributyl tin radicals, which are rare examples of compounds of tin(III).\n\nOrganotin compounds, sometimes called stannanes, are chemical compounds with tin–carbon bonds. Of the compounds of tin, the organic derivatives are the most useful commercially. Some organotin compounds are highly toxic and have been used as biocides. The first organotin compound to be reported was diethyltin diiodide ((CH)SnI), reported by Edward Frankland in 1849.\n\nMost organotin compounds are colorless liquids or solids that are stable to air and water. They adopt tetrahedral geometry. Tetraalkyl- and tetraaryltin compounds can be prepared using Grignard reagents:\nThe mixed halide-alkyls, which are more common and more important commercially than the tetraorgano derivatives, are prepared by redistribution reactions:\n\nDivalent organotin compounds are uncommon, although more common than related divalent organogermanium and organosilicon compounds. The greater stabilization enjoyed by Sn(II) is attributed to the \"inert pair effect\". Organotin(II) compounds include both stannylenes (formula: RSn, as seen for singlet carbenes) and distannylenes (RSn), which are roughly equivalent to alkenes. Both classes exhibit unusual reactions.\n\nTin is generated via the long \"s\"-process in low-to-medium mass stars (with masses of 0.6 to 10 times that of Sun), and finally by beta decay of the heavy isotopes of indium.\n\nTin is the 49th most abundant element in Earth's crust, representing 2 ppm compared with 75 ppm for zinc, 50 ppm for copper, and 14 ppm for lead.\n\nTin does not occur as the native element but must be extracted from various ores. Cassiterite (SnO) is the only commercially important source of tin, although small quantities of tin are recovered from complex sulfides such as stannite, cylindrite, franckeite, canfieldite, and teallite. Minerals with tin are almost always associated with granite rock, usually at a level of 1% tin oxide content.\n\nBecause of the higher specific gravity of tin dioxide, about 80% of mined tin is from secondary deposits found downstream from the primary lodes. Tin is often recovered from granules washed downstream in the past and deposited in valleys or the sea. The most economical ways of mining tin are by dredging, hydraulicking, or open pits. Most of the world's tin is produced from placer deposits, which can contain as little as 0.015% tin.\n\nAbout 253,000 tonnes of tin have been mined in 2011, mostly in China (110,000 t), Indonesia (51,000 t), Peru (34,600 t), Bolivia (20,700 t) and Brazil (12,000 t). Estimates of tin production have historically varied with the dynamics of economic feasibility and the development of mining technologies, but it is estimated that, at current consumption rates and technologies, the Earth will run out of mine-able tin in 40 years. Lester Brown has suggested tin could run out within 20 years based on an extremely conservative extrapolation of 2% growth per year.\nSecondary, or scrap, tin is also an important source of the metal. Recovery of tin through secondary production, or recycling of scrap tin, is increasing rapidly. Whereas the United States has neither mined since 1993 nor smelted tin since 1989, it was the largest secondary producer, recycling nearly 14,000 tonnes in 2006.\n\nNew deposits are reported in southern Mongolia, and in 2009, new deposits of tin were discovered in Colombia by the Seminole Group Colombia CI, SAS.\n\nTin is produced by carbothermic reduction of the oxide ore with carbon or coke. Both reverberatory furnace and electric furnace can be used.\n\nThe ten largest companies produced most of the world's tin in 2007.\n\nMost of the world's tin is traded on the London Metal Exchange (LME), from 8 countries, under 17 brands.\n\nAn International Tin Council was established in 1947 to control the price of tin, until it collapsed in 1985. In 1984, an \"Association of Tin Producing Countries\" was created, with Australia, Bolivia, Indonesia, Malaysia, Nigeria, Thailand, and Zaire as members.\n\nTin is unique among other mineral commodities because of the complex agreements between producer countries and consumer countries dating back to 1921. The earlier agreements tended to be somewhat informal and sporadic and led to the \"First International Tin Agreement\" in 1956, the first of a continuously numbered series that effectively collapsed in 1985. Through this series of agreements, the International Tin Council (ITC) had a considerable effect on tin prices. The ITC supported the price of tin during periods of low prices by buying tin for its buffer stockpile and was able to restrain the price during periods of high prices by selling tin from the stockpile. This was an anti-free-market approach, designed to assure a sufficient flow of tin to consumer countries and a profit for producer countries. However, the buffer stockpile was not sufficiently large, and during most of those 29 years tin prices rose, sometimes sharply, especially from 1973 through 1980 when rampant inflation plagued many world economies.\n\nDuring the late 1970s and early 1980s, the U.S. Government tin stockpile was in an aggressive selling mode, partly to take advantage of the historically high tin prices. The sharp recession of 1981–82 proved to be quite harsh on the tin industry. Tin consumption declined dramatically. The ITC was able to avoid truly steep declines through accelerated buying for its buffer stockpile; this activity required the ITC to borrow extensively from banks and metal trading firms to augment its resources. The ITC continued to borrow until late 1985 when it reached its credit limit. Immediately, a major \"tin crisis\" followed — tin was delisted from trading on the London Metal Exchange for about three years, the ITC dissolved soon afterward, and the price of tin, now in a free-market environment, plummeted sharply to $4 per pound and remained at that level through the 1990s. The price increased again by 2010 with a rebound in consumption following the 2008–09 world economic crisis, accompanying restocking and continued growth in consumption by the world's developing economies.\n\nLondon Metal Exchange (LME) is the principal trading site for tin. Other tin contract markets are Kuala Lumpur Tin Market (KLTM) and Indonesia Tin Exchange (INATIN).\n\nThe price per kg over years:\n\nIn 2006, about half of all tin produced was used in solder. The rest was divided between tin plating, tin chemicals, brass and bronze alloys, and niche uses.\n\nTin has long been used in alloys with lead as solder, in amounts 5 to 70% w/w. Tin with lead forms a eutectic mixture at the weight proportion of 61.9% tin and 38.1% lead (the atomic proportion: 73.9% tin and 26.1% lead), with melting temperature of 183 °C (361.4 °F) . Such solders are primarily used for joining pipes or electric circuits. Since the European Union Waste Electrical and Electronic Equipment Directive (WEEE Directive) and Restriction of Hazardous Substances Directive came into effect on 1 July 2006, the lead content in such alloys has decreased. Replacing lead has many problems, including a higher melting point, and the formation of tin whiskers causing electrical problems. Tin pest can occur in lead-free solders, leading to loss of the soldered joint. Replacement alloys are rapidly being found, although problems of joint integrity remain.\n\nTin bonds readily to iron and is used for coating lead, zinc and steel to prevent corrosion. Tin-plated steel containers are widely used for food preservation, and this forms a large part of the market for metallic tin. A tinplate canister for preserving food was first manufactured in London in 1812. Speakers of British English call them \"tins\", while speakers of American English call them \"cans\" or \"tin cans\". One derivation of such use is the slang term \"tinnie\" or \"tinny\", meaning \"can of beer\" in Australia. The tin whistle is so called because it was first mass-produced in tin-plated steel. Copper cooking vessels such as saucepans and frying pans are frequently lined with a thin plating of tin, since the combination of acid foods with copper can be toxic.\n\nTin in combination with other elements forms a wide variety of useful alloys. Tin is most commonly alloyed with copper. Pewter is 85–99% tin; bearing metal has a high percentage of tin as well. Bronze is mostly copper (12% tin), while addition of phosphorus gives phosphor bronze. Bell metal is also a copper–tin alloy, containing 22% tin. Tin has sometimes been used in coinage; for example, it once formed a single-digit percentage (usually five percent or less) of American and Canadian pennies. Because copper is often the major metal in such coins, sometimes including zinc, these could be called bronze and/or brass alloys.\n\nThe niobium–tin compound NbSn is commercially used in coils of superconducting magnets for its high critical temperature (18 K) and critical magnetic field (25 T). A superconducting magnet weighing as little as two kilograms is capable of the magnetic field of a conventional electromagnet weighing tons.\n\nA small percentage of tin is added to zirconium alloys for the cladding of nuclear fuel.\n\nMost metal pipes in a pipe organ are of a tin/lead alloy, with 50/50 being the most common composition. The proportion of tin in the pipe defines the pipe's tone, since tin has a desirable tonal resonance. When a tin/lead alloy cools, the lead cools slightly faster and produces a mottled or spotted effect. This metal alloy is referred to as spotted metal. Major advantages of using tin for pipes include its appearance, its workability, and resistance to corrosion.\n\nThe oxides of indium and tin are electrically conductive and transparent, and are used to make transparent electrically conducting films with applications in Optoelectronics devices such as liquid crystal displays.\n\nPunched tin-plated steel, also called pierced tin, is an artisan technique originating in central Europe for creating housewares that are both functional and decorative. Decorative piercing designs exist in a wide variety, based on local tradition and the artisan's personal creations. Punched tin lanterns are the most common application of this artisan technique. The light of a candle shining through the pierced design creates a decorative light pattern in the room where it sits. Lanterns and other punched tin articles were created in the New World from the earliest European settlement. A well-known example is the Revere lantern, named after Paul Revere.\n\nBefore the modern era, in some areas of the Alps, a goat or sheep's horn would be sharpened and a tin panel would be punched out using the alphabet and numbers from one to nine. This learning tool was known appropriately as \"the horn\". Modern reproductions are decorated with such motifs as hearts and tulips.\n\nIn America, pie safes and food safes were in use in the days before refrigeration. These were wooden cupboards of various styles and sizes – either floor standing or hanging cupboards meant to discourage vermin and insects and to keep dust from perishable foodstuffs. These cabinets had tinplate inserts in the doors and sometimes in the sides, punched out by the homeowner, cabinetmaker or a tinsmith in varying designs to allow for air circulation while excluding flies. Modern reproductions of these articles remain popular in North America.\n\nWindow glass is most often made by floating molten glass on molten tin (float glass), resulting in a flat and flawless surface. This is also called the \"Pilkington process\".\n\nTin is also used as a negative electrode in advanced Li-ion batteries. Its application is somewhat limited by the fact that some tin surfaces catalyze decomposition of carbonate-based electrolytes used in Li-ion batteries.\n\nTin(II) fluoride is added to some dental care products as stannous fluoride (SnF). Tin(II) fluoride can be mixed with calcium abrasives while the more common sodium fluoride gradually becomes biologically inactive in the presence of calcium compounds. It has also been shown to be more effective than sodium fluoride in controlling gingivitis.\n\nOf all the chemical compounds of tin, the organotin compounds are most heavily used. Worldwide industrial production probably exceeds 50,000 tonnes.\n\nThe major commercial application of organotin compounds is in the stabilization of PVC plastics. In the absence of such stabilizers, PVC would otherwise rapidly degrade under heat, light, and atmospheric oxygen, resulting in discolored, brittle products. Tin scavenges labile chloride ions (Cl), which would otherwise initiate loss of HCl from the plastic material. Typical tin compounds are carboxylic acid derivatives of dibutyltin dichloride, such as the dilaurate.\n\nSome organotin compounds are relatively toxic, with both advantages and problems. They are used for biocidal properties as fungicides, pesticides, algaecides, wood preservatives, and antifouling agents. Tributyltin oxide is used as a wood preservative. Tributyltin was used as additive for ship paint to prevent growth of marine organisms on ships, with use declining after organotin compounds were recognized as persistent organic pollutants with an extremely high toxicity for some marine organisms (the dog whelk, for example). The EU banned the use of organotin compounds in 2003, while concerns over the toxicity of these compounds to marine life and damage to the reproduction and growth of some marine species (some reports describe biological effects to marine life at a concentration of 1 nanogram per liter) have led to a worldwide ban by the International Maritime Organization. Many nations now restrict the use of organotin compounds to vessels greater than long.\n\nSome tin reagents are useful in organic chemistry. In the largest application, stannous chloride is a common reducing agent for the conversion of nitro and oxime groups to amines. The Stille reaction couples organotin compounds with organic halides or pseudohalides.\n\nTin forms several inter-metallic phases with lithium metal, making it a potentially attractive material for battery applications. Large volumetric expansion of tin upon alloying with lithium and instability of the tin-organic electrolyte interface at low electrochemical potentials are the greatest challenges to employment in commercial cells. The problem was partially solved by Sony. Tin inter-metallic compound with cobalt and carbon has been implemented by Sony in its Nexelion cells released in the late 2000s. The composition of the active material is approximately SnCoC. Recent research showed that only some crystalline facets of tetragonal (beta) Sn are responsible for undesirable electrochemical activity.\n\nCases of poisoning from tin metal, its oxides, and its salts are almost unknown. On the other hand, certain organotin compounds are almost as toxic as cyanide.\n\nExposure to tin in the workplace can occur by inhalation, skin contact, and eye contact. The Occupational Safety and Health Administration (OSHA) has set the legal limit (permissible exposure limit) for tin exposure in the workplace as 2 mg/m over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has determined a recommended exposure limit (REL) of 2 mg/m over an 8-hour workday. At levels of 100 mg/m, tin is immediately dangerous to life and health.\n\n\n\n"}
{"id": "42492208", "url": "https://en.wikipedia.org/wiki?curid=42492208", "title": "Ulu Tutong Dam", "text": "Ulu Tutong Dam\n\nThe Ulu Tutong Dam is an embankment dam currently under construction on the Sungai Tutong River in Tutong District, Brunei. The primary purpose of the dam is to increase water supply in Tutong and Brunei-Muara Districts by as much as annually. When complete it will have a normal reservoir volume of and be the largest water supply project in the country. In February 2010 Sinohydro won the bid to construct the US$85.5 million project and construction began soon thereafter. It was originally expected to be complete in February 2014 but the date was pushed to February 2015 due to difficulties at the construction site. The dam is owned by the Public Works Department of Brunei.\n\n"}
{"id": "1845708", "url": "https://en.wikipedia.org/wiki?curid=1845708", "title": "Variable-frequency drive", "text": "Variable-frequency drive\n\nA variable-frequency drive (VFD; also termed \"adjustable-frequency drive\", “variable-voltage/variable-frequency (VVVF) drive”, \"variable speed drive\", \"AC drive\", \"micro drive\" or \"inverter drive\") is a type of adjustable-speed drive used in electro-mechanical drive systems to control AC motor speed and torque by varying motor input frequency and voltage.\n\nVFDs are used in applications ranging from small appliances to large compressors. About 25% of the world's electrical energy is consumed by electric motors in industrial applications, which can be more efficient when using VFDs in centrifugal load service; however, VFDs' global market penetration for all applications is relatively small. \nOver the last four decades, power electronics technology has reduced VFD cost and size and has improved performance through advances in semiconductor switching devices, drive topologies, simulation and control techniques, and control hardware and software.\n\nVFDs are made in a number of different low- and medium-voltage AC-AC and DC-AC topologies.\n\nA variable-frequency drive is a device used in a drive system consisting of the following three main sub-systems: AC motor, main drive controller assembly, and drive/operator interface.\n\nThe AC electric motor used in a VFD system is usually three-phase induction motor. Some types of single-phase motors or synchronous motors can be advantageous in some situations, but generally three-phase induction motors are preferred as the most economical. Motors that are designed for fixed-speed operation are often used. Elevated-voltage stresses imposed on induction motors that are supplied by VFDs require that such motors be designed for definite-purpose inverter-fed duty in accordance with such requirements as Part 31 of NEMA Standard MG-1.\n\nThe VFD controller is a solid-state power electronics conversion system consisting of three distinct sub-systems: a rectifier bridge converter, a direct current (DC) link, and an inverter. Voltage-source inverter (VSI) drives (see 'Generic topologies' sub-section below) are by far the most common type of drives. Most drives are AC-AC drives in that they convert AC line input to AC inverter output. However, in some applications such as common DC bus or solar applications, drives are configured as DC-AC drives. The most basic rectifier converter for the VSI drive is configured as a three-phase, six-pulse, full-wave diode bridge. In a VSI drive, the DC link consists of a capacitor which smooths out the converter's DC output ripple and provides a stiff input to the inverter. This filtered DC voltage is converted to quasi-sinusoidal AC voltage output using the inverter's active switching elements. VSI drives provide higher power factor and lower harmonic distortion than phase-controlled current-source inverter (CSI) and load-commutated inverter (LCI) drives (see 'Generic topologies' sub-section below). The drive controller can also be configured as a phase converter having single-phase converter input and three-phase inverter output.\n\nController advances have exploited dramatic increases in the voltage and current ratings and switching frequency of solid-state power devices over the past six decades. Introduced in 1983, the insulated-gate bipolar transistor (IGBT) has in the past two decades come to dominate VFDs as an inverter switching device.\n\nIn variable-torque applications suited for Volts-per-Hertz (V/Hz) drive control, AC motor characteristics require that the voltage magnitude of the inverter's output to the motor be adjusted to match the required load torque in a linear V/Hz relationship. For example, for 460 V, 60 Hz motors, this linear V/Hz relationship is 460/60 = 7.67 V/Hz. While suitable in wide-ranging applications, V/Hz control is sub-optimal in high-performance applications involving low speed or demanding, dynamic speed regulation, positioning, and reversing load requirements. Some V/Hz control drives can also operate in quadratic V/Hz mode or can even be programmed to suit special multi-point V/Hz paths.\n\nThe two other drive control platforms, vector control and direct torque control (DTC), adjust the motor voltage magnitude, angle from reference, and frequency so as to precisely control the motor's magnetic flux and mechanical torque.\n\nAlthough space vector pulse-width modulation (SVPWM) is becoming increasingly popular, sinusoidal PWM (SPWM) is the most straightforward method used to vary drives' motor voltage (or current) and frequency. With SPWM control (see Fig. 1), quasi-sinusoidal, variable-pulse-width output is constructed from intersections of a saw-toothed carrier signal with a modulating sinusoidal signal which is variable in operating frequency as well as in voltage (or current).\n\nOperation of the motors above rated nameplate speed (base speed) is possible, but is limited to conditions that do not require more power than the nameplate rating of the motor. This is sometimes called \"field weakening\" and, for AC motors, means operating at less than rated V/Hz and above rated nameplate speed. Permanent magnet synchronous motors have quite limited field-weakening speed range due to the constant magnet flux linkage. Wound-rotor synchronous motors and induction motors have much wider speed range. For example, a 100 HP, 460 V, 60 Hz, 1775 RPM (4-pole) induction motor supplied with 460 V, 75 Hz (6.134 V/Hz), would be limited to 60/75 = 80% torque at 125% speed (2218.75 RPM) = 100% power. At higher speeds, the induction motor torque has to be limited further due to the lowering of the breakaway torque of the motor. Thus, rated power can be typically produced only up to 130-150% of the rated nameplate speed. Wound-rotor synchronous motors can be run at even higher speeds. In rolling mill drives, often 200-300% of the base speed is used. The mechanical strength of the rotor limits the maximum speed of the motor.\n\nAn embedded microprocessor governs the overall operation of the VFD controller. Basic programming of the microprocessor is provided as user-inaccessible firmware. User programming of display, variable, and function block parameters is provided to control, protect, and monitor the VFD, motor, and driven equipment.\n\nThe basic drive controller can be configured to selectively include such optional power components and accessories as follows: \n\nThe operator interface provides a means for an operator to start and stop the motor and adjust the operating speed. Additional operator control functions might include reversing, and switching between manual speed adjustment and automatic control from an external process control signal. The operator interface often includes an alphanumeric display or indication lights and meters to provide information about the operation of the drive. An operator interface keypad and display unit is often provided on the front of the VFD controller as shown in the photograph above. The keypad display can often be cable-connected and mounted a short distance from the VFD controller. Most are also provided with input and output (I/O) terminals for connecting push buttons, switches, and other operator interface devices or control signals. A serial communications port is also often available to allow the VFD to be configured, adjusted, monitored, and controlled using a computer.\n\nReferring to the accompanying chart, drive applications can be categorized as single-quadrant, two-quadrant, or four-quadrant; the chart's four quadrants are defined as follows: \n\nMost applications involve single-quadrant loads operating in quadrant I, such as in variable-torque (e.g. centrifugal pumps or fans) and certain constant-torque (e.g. extruders) loads.\n\nCertain applications involve two-quadrant loads operating in quadrant I and II where the speed is positive but the torque changes polarity as in case of a fan decelerating faster than natural mechanical losses. Some sources define two-quadrant drives as loads operating in quadrants I and III where the speed and torque is same (positive or negative) polarity in both directions.\n\nCertain high-performance applications involve four-quadrant loads (Quadrants I to IV) where the speed and torque can be in any direction such as in hoists, elevators, and hilly conveyors. Regeneration can occur only in the drive's DC link bus when inverter voltage is smaller in magnitude than the motor back-EMF and inverter voltage and back-EMF are the same polarity.\n\nIn starting a motor, a VFD initially applies a low frequency and voltage, thus avoiding high inrush current associated with direct-on-line starting. After the start of the VFD, the applied frequency and voltage are increased at a controlled rate or ramped up to accelerate the load. This starting method typically allows a motor to develop 150% of its rated torque while the VFD is drawing less than 50% of its rated current from the mains in the low-speed range. A VFD can be adjusted to produce a steady 150% starting torque from standstill right up to full speed. However, motor cooling deteriorates and can result in overheating as speed decreases such that prolonged low-speed operation with significant torque is not usually possible without separately motorized fan ventilation.\n\nWith a VFD, the stopping sequence is just the opposite as the starting sequence. The frequency and voltage applied to the motor are ramped down at a controlled rate. When the frequency approaches zero, the motor is shut off. A small amount of braking torque is available to help decelerate the load a little faster than it would stop if the motor were simply switched off and allowed to coast. Additional braking torque can be obtained by adding a braking circuit (resistor controlled by a transistor) to dissipate the braking energy. With a four-quadrant rectifier (active front-end), the VFD is able to brake the load by applying a reverse torque and injecting the energy back to the AC line.\n\nMany fixed-speed motor load applications that are supplied direct from AC line power can save energy when they are operated at variable speed by means of VFD. Such energy cost savings are especially pronounced in variable-torque centrifugal fan and pump applications, where the load's torque and power vary with the square and cube, respectively, of the speed. This change gives a large power reduction compared to fixed-speed operation for a relatively small reduction in speed. For example, at 63% speed a motor load consumes only 25% of its full-speed power. This reduction is in accordance with affinity laws that define the relationship between various centrifugal load variables.\n\nIn the United States, an estimated 60-65% of electrical energy is used to supply motors, 75% of which are variable-torque fan, pump, and compressor loads. Eighteen percent of the energy used in the 40 million motors in the U.S. could be saved by efficient energy improvement technologies such as VFDs.\n\nOnly about 3% of the total installed base of AC motors are provided with AC drives. However, it is estimated that drive technology is adopted in as many as 30-40% of all newly installed motors.<ref name=\"Motoring Ahead 1/11\"></ref>\n\nAn energy consumption breakdown of the global population of AC motor installations is as shown in the following table:\n\nAC drives are used to bring about process and quality improvements in industrial and commercial applications' acceleration, flow, monitoring, pressure, speed, temperature, tension, and torque.\n\nFixed-speed loads subject the motor to a high starting torque and to current surges that are up to eight times the full-load current. AC drives instead gradually ramp the motor up to operating speed to lessen mechanical and electrical stress, reducing maintenance and repair costs, and extending the life of the motor and the driven equipment.\n\nVariable-speed drives can also run a motor in specialized patterns to further minimize mechanical and electrical stress. For example, an S-curve pattern can be applied to a conveyor application for smoother deceleration and acceleration control, which reduces the backlash that can occur when a conveyor is accelerating or decelerating.\n\nPerformance factors tending to favor the use of DC drives over AC drives include such requirements as continuous operation at low speed, four-quadrant operation with regeneration, frequent acceleration and deceleration routines, and need for the motor to be protected for a hazardous area. The following table compares AC and DC drives according to certain key parameters: \n^ High-frequency injection\n\nAC drives can be classified according to the following generic topologies: \n\nMost drives use one or more of the following control platforms:\n\nVariable-frequency drives are also categorized by the following load torque and power characteristics:\n\nVFDs are available with voltage and current ratings covering a wide range of single-phase and multi-phase AC motors. Low-voltage (LV) drives are designed to operate at output voltages equal to or less than 690 V. While motor-application LV drives are available in ratings of up to the order of 5 or 6 MW, economic considerations typically favor medium-voltage (MV) drives with much lower power ratings. Different MV drive topologies (see Table 2) are configured in accordance with the voltage/current-combination ratings used in different drive controllers' switching devices such that any given voltage rating is greater than or equal to one to the following standard nominal motor voltage ratings: generally either 2.3/4.16 kV (60 Hz) or 3.3/6.6 kV (50 Hz), with one thyristor manufacturer rated for up to 12 kV switching. In some applications a step-up transformer is placed between a LV drive and a MV motor load. MV drives are typically rated for motor applications greater than between about 375 kW (500 HP) and 750 kW (1000 hp). MV drives have historically required considerably more application design effort than required for LV drive applications. The power rating of MV drives can reach 100 MW, a range of different drive topologies being involved for different rating, performance, power quality, and reliability requirements.\n\nIt is lastly useful to relate VFDs in terms of the following two classifications:\n\nNote of clarification:.\n\nWhile harmonics in the PWM output can easily be filtered by carrier-frequency-related filter inductance to supply near-sinusoidal currents to the motor load, the VFD's diode-bridge rectifier converts AC line voltage to DC voltage output by super-imposing non-linear half-phase current pulses thus creating harmonic current distortion, and hence voltage distortion, of the AC line input. When the VFD loads are relatively small in comparison to the large, stiff power system available from the electric power company, the effects of VFD harmonic distortion of the AC grid can often be within acceptable limits. Furthermore, in low-voltage networks, harmonics caused by single-phase equipment such as computers and TVs are partially cancelled by three-phase diode bridge harmonics because their 5th and 7th harmonics are in counterphase. However, when the proportion of VFD and other non-linear load compared to total load or of non-linear load compared to the stiffness at the AC power supply, or both, is relatively large enough, the load can have a negative impact on the AC power waveform available to other power company customers in the same grid.\n\nWhen the power company's voltage becomes distorted due to harmonics, losses in other loads such as normal fixed-speed AC motors are increased. This condition may lead to overheating and shorter operating life. Also, substation transformers and compensation capacitors are affected negatively. In particular, capacitors can cause resonance conditions that can unacceptably magnify harmonic levels. In order to limit the voltage distortion, owners of VFD load may be required to install filtering equipment to reduce harmonic distortion below acceptable limits. Alternatively, the utility may adopt a solution by installing filtering equipment of its own at substations affected by the large amount of VFD equipment being used. In high-power installations, harmonic distortion can be reduced by supplying multi-pulse rectifier-bridge VFDs from transformers with multiple phase-shifted windings.\n\nIt is also possible to replace the standard diode-bridge rectifier with a bi-directional IGBT switching device bridge mirroring the standard inverter which uses IGBT switching device output to the motor. Such rectifiers are referred to by various designations including active infeed converter (AIC), active rectifier, IGBT supply unit (ISU), active front end (AFE), or four-quadrant operation. With PWM control and a suitable input reactor, an AFE's AC line current waveform can be nearly sinusoidal. AFE inherently regenerates energy in four-quadrant mode from the DC side to the AC grid. Thus, no braking resistor is needed, and the efficiency of the drive is improved if the drive is frequently required to brake the motor.\n\nTwo other harmonics mitigation techniques exploit use of passive or active filters connected to a common bus with at least one VFD branch load on the bus. Passive filters involve the design of one or more low-pass LC filter traps, each trap being tuned as required to a harmonic frequency (5th, 7th, 11th, 13th, . . . kq+/-1, where k=integer, q=pulse number of converter).\n\nIt is very common practice for power companies or their customers to impose harmonic distortion limits based on IEC or IEEE standards. For example, IEEE Standard 519 limits at the customer's connection point call for the maximum individual frequency voltage harmonic to be no more than 3% of the fundamental and call for the voltage total harmonic distortion (THD) to be no more than 5% for a general AC power supply system.\n\nOne drive uses a default switching frequency setting of 4 kHz. Reducing the drive’s switching frequency (the carrier-frequency) reduces the heat generated by the IGBTs.\n\nA carrier frequency of at least ten times the desired output frequency is used to establish the PWM switching intervals. A carrier frequency in the range of 2,000 to 16,000 Hz is common for LV [low voltage, under 600 Volts AC] VFDs. A higher carrier frequency produces a better sine wave approximation but incurs higher switching losses in the IGBT, decreasing the overall power conversion efficiency.\n\nSome drives have a noise smoothing feature that can be turned on to introduce a random variation to the switching frequency. This distributes the acoustic noise over a range of frequencies to lower the peak noise intensity.\n\nThe carrier-frequency pulsed output voltage of a PWM VFD causes rapid rise times in these pulses, the transmission line effects of which must be considered. Since the transmission-line impedance of the cable and motor are different, pulses tend to reflect back from the motor terminals into the cable. The resulting reflections can produce overvoltages equal to twice the DC bus voltage or up to 3.1 times the rated line voltage for long cable runs, putting high stress on the cable and motor windings, and eventual insulation failure. Insulation standards for three-phase motors rated 230 V or less adequately protect against such long-lead overvoltages. On 460 V or 575 V systems and inverters with 3rd-generation 0.1-microsecond-rise-time IGBTs, the maximum recommended cable distance between VFD and motor is about 50 m or 150 feet. For emerging SiC MOSFET powered drives, significant overvoltages have been observed at cable lengths as short as 3 meters. Solutions to overvoltages caused by long lead lengths include minimizing cable length, lowering carrier frequency, installing dV/dt filters, using inverter-duty-rated motors (that are rated 600 V to withstand pulse trains with rise time less than or equal to 0.1 microsecond, of 1,600 V peak magnitude), and installing LCR low-pass sine wave filters. Selection of optimum PWM carrier frequency for AC drives involves balancing noise, heat, motor insulation stress, common-mode voltage-induced motor bearing current damage, smooth motor operation, and other factors. Further harmonics attenuation can be obtained by using an LCR low-pass sine wave filter or dV/dt filter.\n\nCarrier frequencies above 5 kHz are likely to cause bearing damage unless protective measures are taken.\n\nPWM drives are inherently associated with high-frequency common-mode voltages and currents which may cause trouble with motor bearings. When these high-frequency voltages find a path to earth through a bearing, transfer of metal or electrical discharge machining (EDM) sparking occurs between the bearing's ball and the bearing's race. Over time, EDM-based sparking causes erosion in the bearing race that can be seen as a fluting pattern. In large motors, the stray capacitance of the windings provides paths for high-frequency currents that pass through the motor shaft ends, leading to a circulating type of bearing current. Poor grounding of motor stators can lead to shaft-to-ground bearing currents. Small motors with poorly grounded driven equipment are susceptible to high-frequency bearing currents.\n\nPrevention of high-frequency bearing current damage uses three approaches: good cabling and grounding practices, interruption of bearing currents, and filtering or damping of common-mode currents for example through soft magnetic cores, the so-called inductive absorbers. Good cabling and grounding practices can include use of shielded, symmetrical-geometry power cable to supply the motor, installation of shaft grounding brushes, and conductive bearing grease. Bearing currents can be interrupted by installation of insulated bearings and specially designed electrostatic-shielded induction motors. Filtering and damping high-frequency bearing can be done though inserting soft magnetic cores over the three phases giving a high frequency impedance against the common mode or motor bearing currents. Another approach is to use instead of standard 2-level inverter drives, using either 3-level inverter drives or matrix converters.\n\nSince inverter-fed motor cables' high-frequency current spikes can interfere with other cabling in facilities, such inverter-fed motor cables should not only be of shielded, symmetrical-geometry design but should also be routed at least 50 cm away from signal cables.\n\nTorque generated by the drive causes the induction motor to run at synchronous speed less the slip. If the load drives the motor faster than synchronous speed, the motor acts as a generator, converting mechanical power back to electrical power. This power is returned to the drive's DC link element (capacitor or reactor). A DC-link-connected electronic power switch or braking DC chopper controls dissipation of this power as heat in a set of resistors. Cooling fans may be used to prevent resistor overheating.\n\nDynamic braking wastes braking energy by transforming it to heat. By contrast, regenerative drives recover braking energy by injecting this energy into the AC line. The capital cost of regenerative drives is, however, relatively high.\n\nRegenerative AC drives have the capacity to recover the braking energy of a load moving faster than the designated motor speed (an \"overhauling\" load) and return it to the power system.\n\nCycloconverter, Scherbius, matrix, CSI, and LCI drives inherently allow return of energy from the load to the line, while voltage-source inverters require an additional converter to return energy to the supply.\n\nRegeneration is useful in VFDs only where the value of the recovered energy is large compared to the extra cost of a regenerative system, and if the system requires frequent braking and starting. Regenerative VFDs are widely used where speed control of overhauling loads is required.\nSome examples:\n\nBefore solid-state devices became available, variable-frequency drives used rotary machines and the General Electric Company obtained several patents for these in the early 20th century. One example is US patent 949320 of 1910 which states: \"Such a generator finds a useful application in supplying current to induction motors for driving cars, locomotives, or other mechanism which are to be driven at variable speeds\". Another is British patent 7061 of 1911 by Brown, Boveri & Cie.\n\n"}
{"id": "51417988", "url": "https://en.wikipedia.org/wiki?curid=51417988", "title": "Vertically aligned carbon nanotube arrays", "text": "Vertically aligned carbon nanotube arrays\n\nVertically aligned carbon nanotube arrays (VANTAs) are a unique microstructure consisting of carbon nanotubes oriented along their longitudinal axes normal to a substrate surface. These VANTAs effectively preserve and often accentuate the unique anisotropic properties of individual carbon nanotubes and possess a morphology that may be precisely controlled. VANTAs are consequently widely useful in a range of current and potential device applications.\n\nThere are a handful of experimental technologies available to align a single or an array of CNTs along a pre-determined orientation. The techniques rely on different mechanisms and therefore are applicable to different situations. These techniques are categorized into two groups pertaining to when the alignment is achieved: (a) in-situ techniques where alignment is achieved during the CNT growth process and (b) ex-situ techniques where CNTs are originally grown in random orientations and alignment is achieved afterwards such as during the device integration process.\n\nThermal chemical vapor deposition is a common technique to grow aligned arrays of CNTs. In the CVD process, a hot carbonaceous gas decomposes, *leaving carbon to diffuses into or around the catalyst particles*, and then nucleates a graphitic nanotube sidewall at one crystallographic face of the catalyst. The catalyst diameter directly controls the diameter of the nanotubes that are grown. There are two primary growth models for the CVD growth of VANTAs: “tip-growth model” and the “base-growth model.” In the case of the tip-growth model, hydrocarbon decomposes on the top surface of the metal, carbon diffuses down through the metal, and CNT precipitates out across the metal bottom, pushing the whole metal particle off the substrate, and continues to grow until the metal is fully covered with excess carbon and its catalytic activity ceases. In the case of the base-growth model, the initial hydrocarbon decomposition and carbon diffusion take place similar to that in the tip-growth case, but the CNT precipitation emerges out from the metal particle’s apex and forms a hemispherical dome, which then extends up in the form of seamless graphitic cylinder. Subsequent hydrocarbon decomposition takes place on the lower peripheral surface of the metal, and as-dissolved carbon diffuses upward. Most thermal CVD processes grow nanotubes by the root or base growth method. The morphology of both the individual CNTs and the CNT array is dictated by various CVD growth parameters, which may be tuned to yield vertically aligned arrays of CNTs with various structures.\n\nThe catalyst enables the pyrolysis of carbon and subsequent growth of VANTA. Catalysts are typically metals that have high carbon solubility at high temperatures and that exhibit a high carbon diffusion rate, such as iron (Fe), cobalt (Co), and nickel (Ni). Other transition metals such as copper (Cu), gold (Au), silver (Ag), platinum (Pt), and palladium (Pd) are also reported to catalyze CNT growth from various hydrocarbons but have lower carbon solubility and consequently lower growth rates. Solid organometallocenes such as ferrocene, cobaltocene, nickelocene are also common catalysts. It is found that the temperature and time of the thermal and reduction catalyst pre-treatment steps are crucial variables for optimized nanoparticle distribution with different average diameters, depending on the initial film thickness. For CNT growth by CVD, a sputtered thin film of catalyst (e.g. 1 nm of Fe) is applied. During heating, the film de-wets, creating islands of iron that then nucleate nanotubes. As the iron is mobile, islands can merge if left too long at the growth temperature before initiating nanotube growth. Annealing at the growth temperature reduces the site density #/mm2 and increases the diameter of the nanotubes. As the nanotubes grow from the catalyst islands, the crowding effects and van der Waals forces between other CNTs leave them no choice to grow in any direction but vertically to the substrate.\n\nThe height of vertically aligned CNTs varies with catalyst particle spacing as well. Reports have indicated that for vertically aligned arrays of CNT bundles, the CNTs grow longer when there are other CNTs growing near them, indicated by longer CNTs grown on larger catalyst particles or when catalyst particles are spaced close together. Choi et al. reported good morphology and dense distribution of VANTAs grown from Ni nano powders and magnetic fluids mixed in polyvinyl alcohol spin-coated on Si and alumina. Xiong et al. demonstrated that single crystal magnesium oxide (MgO) is a capable substrate for growing VANTAs as long as 2.2 mm when catalyzed with a Fe catalyst.\nIt has also been demonstrated that applying a monolayer of Mo with a Co catalyst suppressed the broadening of the SWNT diameter distribution in the as-grown VANTA, while both the composition and amount of Co and Mo affected the catalytic activity.\n\nThe substrate material, its surface morphology and textural properties greatly affect the resulting VANTA yield. Some examples of commonly used substrates in CVD are quartz, silicon, silicon carbide, silica, alumina, zeolite, CaCO, and magnesium oxide. Most substrates are coated with an underlayer consisting of 10-20 nm of alumina before depositing the catalyst. This regularizes the dewetting of the catalyst into islands of predictable size, and is a diffusion barrier between the substrate and the metal catalyst.\nLi et al. have produced VANTA consisting of Y- shaped carbon nanotubes by the pyrolysis of methane over cobalt- covered magnesium oxide catalyst on branched nanochannel alumina templates.\nQu et al. used a pitch-based carbon fiber as a support for the growth of VANTA using a FePc carbon source. The resulting array propagates radially on the surface of the carbon fiber.\n\nZhong, et al. demonstrated the direct growth of VANTAs on metallic titanium (Ti) coatings with a Fe/Ti/Fe catalyst sputtered on SiO/Si wafers. \nAlvarez et al. reports the ability to spin-coat an alumoxane solution as a catalyst support for VANTA growths via CVD. After a conventional Fe catalyst was evaporated onto the spin-coated support, the resulting VANTA growth yield was similar to conventional AlO powder supports.\n\nThe carbon source for the CVD of VANTAs is most commonly a carbon gas such as methane, ethylene, acetylene, benzene, xylene, or carbon monoxide. Other examples of carbon precursors include cyclohexane, fullerene, methanol, and ethanol. The pyrolysis of these gases into carbon atoms varies based on the decomposition rate at growth temperatures, the carbon content of the gas molecules, and the growth catalyst. Linear hydrocarbons such as methane, ethylene, acetylene, thermally decompose into atomic carbons or linear dimers/trimers of carbon, and generally produce straight and hollow CNTs. On the other hand, cyclic hydrocarbons such as benzene, xylene, cyclohexane, fullerene, produce relatively curved/hunched CNTs with the tube walls often bridged inside. \nAligned arrays of MWNTs have been synthesized through the catalytic decomposition of ferrocene-xylene precursor mixture onto quartz substrates at atmospheric pressure and relatively low temperature (~675 °C).\n\nEres et al. found that the addition of ferrocene into the gas stream by thermal evaporation concurrently with acetylene enhanced carbon nanotube growth rates and extend the VANTA thickness to 3.25 mm. Ferrocene was introduced into the gas stream by thermal evaporation concurrently with the flow of acetylene. \nQu et al. reported a low-pressure CVD process on a SiO/Si wafer that produces a VANTA consisting of CNTs with curly entangled ends. During the pyrolytic growth of the VANTAs, the initially formed nanotube segments from the base growth process grew in random directions and formed a randomly entangled nanotube top layer to which the underlying straight nanotube arrays then emerged.\nZhong et al. studied purely thermal CVD process for SWNT forests without an etchant gas, and demonstrated that acteylene is the main growth precursor, and the conversion of any feedstock to CH is of key importance to SWNT VANTA growth. A reactive etchant, such as water, atomic hydrogen, or hydroxyl radicals, can widen the SWNT forest deposition window but is not required in cold-wall reactors at low pressures.\n\nDasgupta et al. synthesized a free-standing macro-tubular VANTA with a spray pyrolysis of ferrocene-benzene solution in a nitrogen atmosphere, with the optimum condition for the formation of macro-tubular geometry was found to be 950 °C, 50 mg/ml ferrocene in benzene, 1.5 ml/min pumping rate of liquid precursor and 5 lpm of nitrogen gas flow rate.\n\nAt a too low temperature, the catalyst atoms are not mobile enough to aggregate together into particles to nucleate and grow nanotubes and the catalytic decomposition of the carbon precursor may be too slow for the formation of nanotubes. If the temperature is too high, the catalyst becomes too mobile to form particles small enough to nucleate and grow CNTs. A typical range of growth temperatures amenable to the CVD growth of VANTA is 600-1200 °C. The individual CNT structure is impacted by the growth temperature; a low-temperature CVD (600–900 °C) yields MWCNTs, whereas high-temperature (900–1200 °C) reaction favors SWCNT since they have a higher energy of formation. A critical temperature exists for each CVD system where the growth rate plateaus at a maximum value.\n\nThe temperature dependence of the carbon nanotube growth with ferrocene exhibits a steep drop at high substrate temperatures and a loss of vertical alignment at 900 °C.\nZhang et al. conducted VANTA growths on a series of Fe/Mo/vermiculite catalysts and reported that with the increasing growth temperature, the alignment of CNTs intercalated among vermiculites became worse.\n\nA key to high growth yields is a proper introduction of oxidative agents under the gas ambient so that the catalyst particle surfaces remain active for the longest possible period, which is presumably achieved by balancing the competition between amorphous carbon growth and sp graphitic crystal formation on the catalyst particles. Oxidants can not only remove or prevent amorphous carbon growth, but may also etch into graphite layers when used at higher than favorable concentrations. Hata et al. reported millimeter-scale vertically aligned 2.5 mm long SWCNTs using the water assisted ethylene CVD process with Fe/Al or aluminum oxide multilayers on Si wafers. It was proposed that controlled supply of steam into the CVD reactor acted as a weak oxidizer and selectively removed amorphous carbon without damaging the growing CNTs.\n\nSince CNTs are all electrically conductive they have a tendency to align with the electric field lines. Various methods have been developed to apply a strong enough electric field during the CNT growth process to achieve uniform alignment of CNTs based on this principle. The orientation of the aligned CNTs is mainly dependent on the length of CNTs and the electric field besides the thermal randomization and van der Waals forces. This technique has been employed to grow VANTAs by positively biasing the substrate during CVD growth.\n\nAnother modified approach to grow VANTAs is to control the orientation of ferromagnetic catalysts that have one crystallographic magnetic easy axis. The magnetic easy axis tends to be parallel to the magnetic field. As a result, an applied magnetic force can orient these magnetic catalytic nanoparticles, like catalytic iron nanoparticles and FeO nanoparticles. Because only a certain nanocrystalline facet of catalytic nanoparticles is catalytically active and the diffusion rate of carbon atoms on the facet is the highest, the CNTs preferentially grow from the certain facet of the catalytic nanoparticles and the grown CNTs are oriented at a certain angle.\n\nIn plasma enhanced CVD (PECVD) processes, DC electric fields, radio-frequency electric fields, or microwaves produce plasmas to primarily lower the synthesis temperature of CNTs. At the same time, an electric field (DC or AC) is also produced over the substrate surface to direct CNT growth propagation. The DC-PECVD process for vertically aligned CNT arrays includes four basic steps: evacuation, heating, plasma generation, and cooling. A typical procedure is conducted at a pressure of 8 Torr in NH3 and at a growth temperature in the range of 450–600 ◦. As soon as the temperature and pressure are stabilized, a DC bias voltage of 450–650V is applied to the gap between two electrodes to ignite an electrical discharge (plasma) over the sample. The growth time may vary from a couple of minutes to hours depending on the growth rate and desired CNT length. When the end of growth time is reached, the bias voltage is removed immediately to terminate the plasma.\n\nZhong et al. reported a novel point-arc microwave plasma CVD apparatus employed to SWNTs on Si substrates coated with a sandwich-like nano-layer structure of 0.7 nm AlO/0.5 nm Fe/ 5–70 nm AlO by conventional high frequency sputtering. The growth of extremely dense and vertically aligned SWNTs with an almost constant growth rate of 270 mm/h within 40 min at a temperature as low as 600 °C was demonstrated for the first time and the volume density of the as-grown SWNT films is as higher as 66 kg/m.\n\nThe formation of a dense and relatively uniform layer of catalyst nanoparticles is also essential for vertically aligned SWCNT growth vertically aligned SWCNTs using the PECVD method. Amaratunga et al. reported the growth of vertically aligned CNTs using a direct current PECVD technique with a Ni and Co catalyst system. Their results show that the alignment of vertically aligned CNTs depends on the electric field and that the growth rate can be changed depending on the CNT diameter, which reaches a maximum as a function of growth temperature. VANTAs consisting of SWNTs have been grown as long as 0.5 cm. \nZhong et al. reported a novel point-arc microwave plasma CVD apparatus employed to SWNTs on Si substrates coated with a sandwich-like nano-layer structure of 0.7 nm Al2O3/0.5 nm Fe/ 5–70 nm AlO by conventional high frequency sputtering. The growth of extremely dense and vertically aligned SWNTs with an almost constant growth rate of 270 mm/h within 40 min at a temperature as low as 600 °C was demonstrated for the first time and the volume density of the as-grown SWNT films is as higher as 66 kg/m.\n\nFor PECVD processes, the substrate must be chemically stable under the plasma which is rich of H-species. Some weakly bonded oxides such as indium oxide can be quickly reduced in this plasma and is therefore usually not applicable as the substrate or underlayer. The substrate must also be electrically conductive to sustain a continuous DC current flow through its surface where the CNTs grow from. Most metals and semiconductors are very good substrate materials, and insulating substrates can be first coated with a conductive layer to work properly to support PECVD VANTA growth.\n\nCH is typically introduced to trigger the CNT growth during PECVD of VANTAs. The flow rate ratio of NH:CH is usually around 4:1 to minimize the amorphous carbon formation. Behr et al. studied the effect of hydrogen on the catalyst nanoparticles during the PECVD of VANTAs, and demonstrated that at H-to-CH ratios of about 1 iron catalyst nanoparticles are converted to FeC and well-graphitized nanotubes grow from elongated FeC crystals. H-to-CH ratios greater than 5 in the feed gas result in high hydrogen concentrations in the plasma and strongly reducing conditions, which prevents the conversion of Fe to FeC and cause poorly-graphitized nanofibers to grow with thick walls.\n\nOne of the major advantages of using PECVD growth techniques is the low growth temperature. The ionization of the neutral hydrocarbon molecules inside the plasma facilitates the breaking of the C–H bonds and lowers the activation energy of the CNT growth to be about 0.3eV as opposed to the 1.2eV needed for thermal CVD processes.\n\nCNT solutions can form VANTAs through alignment along DC or AC electric field lines. The CNTs are polarized in the suspension by the electric field because of dielectric mismatch between CNTs and the liquid. The polarization moment rotates the CNTs toward the direction of electric field lines, therefore aligning them in a common direction. After being aligned, the CNTs are taken out with the substrates and dried to form functional VANTAs.\n\nRandomly oriented CNTs on a substrate can be stretched to straighten and detangle the film by breaking the substrate and pulling the ends apart. The aligned CNTs are parallel to each other and perpendicular to the crack. The stretching method can macroscopically align the CNTs while not providing deterministic control over individual CNT alignment or position during assembly.\n\nCNTs have high aspect ratios (length divided by diameter) and induce very high local electric field intensities around the tips. Field emission in solids occurs in intense electric fields and is strongly dependent on the work function of the emitting material. In a parallel-plate arrangement, the macroscopic field Emacro between the plates is given by E = V/d, where d is the plate separation and V the applied voltage. If a sharp object is created on a plate, then the local field Elocal at its apex is greater than Emacro and can be related to:\nE=γ×E \nThe parameter γ is called the field-enhancement factor and basically determined by the shape of the object. Typical field-enhancement factors ranging from 30,000 to 50,000 can be obtained from individual CNTs, therefore making VANTAs one of the best electron-emitting materials.\n\nVANTAs offer a unique light absorbing surface due to their extremely low index of refraction and the nanoscale surface roughness of the aligned CNTs. Yang et al. demonstrated that low-density VANTAs exhibit an ultralow diffuse reflectance of 1 × 10-7 with a corresponding integrated total reflectance of 0.045%. Although VANTA black coatings must be directly transferred or grown on substrates, unlike black coatings consisting of random networks of CNTs that may be processed into CNT paints, they are the considered the blackest man-made material on earth.\n\nVANTA blackbody absorbers are thus useful as stray light absorbers to improve the resolution of sensitive spectroscopes, telescopes, microscopes, and optical sensing devices. Several commercial optical black coating products such as Vantablack and adVANTA nanotube optical blacks have been produced from VANTA coatings. VANTA absorbers may also increase the absorption of heat in materials used in concentrated solar power technology, as well as military applications such as thermal camouflage. Visual displays of VANTA absorbers have generated interest by artists as well seeking to benefit from the quenching of shadows from rough surface.\n\nVANTAs can be processed through volatile solutions or twisted to condense into spun CNT yarns or ropes. Jiang et al demonstrated a spinning and twisting method that forms a CNT yarn from a VANTA that gives rise to both a round cross-section and a tensile strength of around 1 GPa. The tensile strengths of CNT yarns spun from ultra-long CNT arrays of 1 mm height can range from 1.35 to 3.3 GPa.\n\nBiomimicry studies directed towards replicating the adhesion of gecko feet on smooth surfaces have reported success utilizing VANTA as a dry adhesive film. Qu et al. was able to demonstrate VANTA films that exhibited macroscopic adhesive forces of ~100 newtons per square centimeter, which is almost 10 times that of a gecko foot. This was achieved by tuning the growth conditions of the VANTA to form curls at the end of the CNTs, which provide stronger interfacial interactions even with a smooth surface.\n\nVANTAs allow the development of novel sensors and/or sensor chips without the need for direct manipulation of individual nanotubes. The aligned nanotube structure further provides a large well-defined surface area and the capacity for modifying the carbon nanotube surface with various transduction materials to effectively enhance the sensitivity and to broaden the scope of analytes to be detected. Wei et al. reported a gas sensor fabricated by partially covering a VANTA with a polymer coating top-down along their tube length by depositing a droplet of polymer solution (e.g., poly(vinyl acetate), PVAc, polyisoprene, PI) onto the nanotube film, inverting the composite film as a free-standing film , and then sputter-coating two strip electrodes of gold across the nanotube arrays that were protruding from the polymer matrix. The flexible VANTA device was demonstrated to successfully sense chemical vapors through monitoring conductivity changes caused by the charge-transfer interaction with gas molecules and/or the inter-tube distance changes induced by polymer swelling via gas absorption. To date, CNTs have shown sensitivities toward gases such as NH, NO, H, CH, CO, SO, HS, and O.\n\nVANTAs act as forests of molecular wires to allow electrical communication between the underlying electrode and a biological entity. The main advantages of VANTAs are the nanosize of the CNT-sensing element and the corresponding small amount of material required for a detectable response. The well-aligned CNT arrays have been employed to work as ribonucleic acid (RNA) sensors, enzymes sensors, DNA sensors, and even protein sensors. Similar VANTAs of MWNTs, grown on platinum substrates, are useful for amperometric electrodes where the oxygenated or functionalized open-ends of nanotubes are used for the immobilization of biological species, while the platinum substrate provides the signal transduction. To increase the selectivity and sensitivity of amperometric biosensors, artificial mediators and permselective coatings are often used in the biosensor fabrication. Artificial mediators are used to shuttle electrons between the enzyme and the electrode to allow operation at low potentials.\nGooding et al. demonstrated that shortened SWNTs can be aligned normal to an electrode by self-assembly and act as molecular wires to allow electrical communication between the underlying electrode and redox proteins covalently attached to the ends of the SWNTs. The high rate of electron transfer through the nanotubes to redox proteins is clearly demonstrated by the similarity in the rate constant for electron transfer to MP-11 regardless of whether SWNTs are present or not.\n\nVANTA interfaces are more thermally conductive than conventional thermal interface materials at the same temperatures because phonons propagate easily along the highly thermally conductive CNTs and thus heat is transported in one direction along the alignment of the CNTs. The distribution and alignment of the thermally conductive CNT fillers are important factors to affect the phonon transport. Huang et al. demonstrated a thermally conductive composite shows an enhancement of 0.65W/m/K with a 0.3wt% loading of VANTA, whereas the enhanced thermal conductivity of a composite with of 0.3 wt% loading of randomly dispersed CNT is below 0.05W/m/K.\n\nVANTAs of SWNTs with perfectly linear geometries are applicable as high-performance p- and n-channel transistors and unipolar and complementary logic gates. The excellent properties of the devices derive directly from a complete absence, to within experimental uncertainties, of any defects in the arrays, as defined by tubes or segments of tubes that are misaligned or have nonlinear shapes. The large number of SWNTs enables excellent device-level performance characteristics and good device-to-device uniformity, even with SWNTs that are electronically heterogeneous. Measurements on p- and n-channel transistors that involve as many as about 2,100 SWNTs reveal device-level mobilities and scaled transconductance approaching about 1,000 cm2 V-1 s-1 and $3,000 S m-1, respectively, and with current outputs of up to about 1 A in devices that use interdigitated electrodes.\n\nThe low κ materials with low relative dielectric constants are employed as the insulating layers in integrated circuits to reduce the coupling capacitance. The relative dielectric constant of electrically insulating layers can be reduced further by introducing cavities into the low-κ materials. If elongated and oriented pores are used, it is possible to reduce significantly the effective κ value without increasing the proportion of the cavity volume in a dielectric. The CNTs in VANTAs have a high aspect ratio and can be used to introduce elongated, oriented pores into a low-κ dielectric to further reduce the effective κ value of the dielectric.\n\nFuel cells are made up of three sandwiched segments: an anode, an electrolyte, and a cathode, in a reaction cell where electricity is produced inside the fuel cells through the reactions between an external fuel and an oxidant in the presence of an electrolyte. The anode hosts a catalyst that oxidizes the fuel, turning the fuel into positively charged ions and negatively charged electrons. This fuel is typically hydrogen, hydrocarbons, and alcohols. The electrolyte blocks the transportation of electrons while conducting ions. The ions traveling through the electrolyte are re-united on the cathode with the electrons passing through a load during a reaction with an oxidant to produce water or carbon dioxide. Ideal anode supports for the deposition of catalytic nanoparticles are porous conductive materials to maximize the electrocatalytic activity. VANTAs are therefore ideal materials due to their intrinsic high conductivity, high surface area, and stability in most fuel cell electrolytes. A typical catalyst deposited on VANTA anodes is platinum, which can be electrodeposited on the individual CNTs of the VANTA. The electrocatalytic activity at the anode is optimal when the Pt particles are uniformly dispersed within the VANTA.\n\nGong et al. reported that VANTAs doped with nitrogen can act as a metal-free electrode with a much better electrocatalytic activity, long-term operation stability, and tolerance to crossover effect than platinum for oxygen reduction in alkaline fuel cells. In air-saturated 0.1 molar potassium hydroxide, a steady-state output potential of –80 millivolts and a current density of 4.1 milliamps per square centimeter at –0.22 volts was observed, as compared to –85 millivolts and 1.1 milliamps per square centimeter at –0.20 volts for a platinum-carbon electrode. The incorporation of electron-accepting nitrogen atoms in the conjugated nanotube carbon plane appears to impart a relatively high positive charge density on adjacent carbon atoms. This effect, coupled with aligning the nitrogen-doped CNTs, provides a four-electron pathway for the oxygen reduction reactions on VANTAs with a superb performance.\n\nLike ordinary capacitors, VANTA supercapacitors and electromechanical actuators typically comprise two electrodes separated by an electronically insulating material, which is ionically conducting in electrochemical devices. The capacitance for an ordinary planar sheet capacitor inversely depends on the inter-electrode separation. In contrast, the capacitance for an electrochemical device depends on the separation between the charge on the electrode and the countercharge in the electrolyte. Because this separation is about a nanometer for CNTs in VANTA electrodes, as compared with the micrometer or larger separations in ordinary dielectric capacitors, very large capacitances result from the high CNT surface area accessible to the electrolyte. These capacitances (typically 15 - 200 F/g, depending on the surface area of the nanotube array) result in large amounts of charge injection when only a few volts are applied.\n\nFutaba et al. reported a technique to form super-capacitors from a VANTA flattened by settling the erect CNTs by wetting them with a liquid. The capacitance of the SWNT solid EDLC was estimated as 20 F g from the discharge curves of cells charged at 2.5V for a two-electrode cell, and corresponds to 80 F g for a three-electrode cell. The energy density (W = CV/2) was estimated to be 69.4 W h kg (from 80 F g) when normalized to the single electrode weight.\n\nUnlike in ultracapacitors where the solvent of the electrolyte is not involved in the charge storage mechanism, the solvent of the electrolyte contributes to the solid–electrolyte interphase in batteries. The Li-ion batteries usually consist of an active carbon anode, a lithium–cobalt oxide cathode, and an organic electrolyte. In order to obtain better electrode performance than networks of random CNTs and CNT composites, VANTAs are used as to provide better electron transport and higher surface area.\n"}
{"id": "29726343", "url": "https://en.wikipedia.org/wiki?curid=29726343", "title": "Waterlife", "text": "Waterlife\n\nWaterlife is a 2009 documentary film and web documentary about the state of the Great Lakes.\n\nKevin McMahon began filming \"Waterlife\" in 2007. The film explores the beauty of the Great Lakes as well as their degradation due to water pollution. The film looks at the water system from its headwaters in Lake Superior to the Gulf of St. Lawrence, accompanied by Josephine Mandamin, an Anishinabe elder from Thunder Bay, who walks along the Great Lakes each spring to protest deteriorating conditions.\n\n\"Waterlife\" is co-produced by Primitive Entertainment and the National Film Board of Canada (NFB). The film received the Special Jury Prize for Canadian Feature at the 2009 Hot Docs Canadian International Documentary Festival. The United Kingdom distributor is Dogwoof Pictures.\n\nThe interactive version of \"Waterlife\" was created by Toronto-based web and design company Jam3 and creative directors Adrian Belina and Pablo Vio for the NFB, incorporating material from the documentary film. The conception and development of the website took approximately four months. \"Waterlife\" explores different aspects of the state of the Great Lakes through 23 individual sections, incorporating text, images and sound. It received the Webby Award for best web documentary (individual episode).\n\n\n"}
{"id": "22417723", "url": "https://en.wikipedia.org/wiki?curid=22417723", "title": "Waterproof paper", "text": "Waterproof paper\n\nWaterproof paper is a type of paper that is good for outdoor, marine, field use and general wet environments. Often designed especially for printing topographic maps. It is normally durable and tear-resistant.\n\nThe paper is created with special coatings (plastic-coated paper) and fibers to allow it to stay together and not change shape or texture when exposed to rain, dampness, or immersed in water.\n\nWaterproof paper is difficult to recycle.\n\n"}
{"id": "4331120", "url": "https://en.wikipedia.org/wiki?curid=4331120", "title": "Winter of 1886–1887", "text": "Winter of 1886–1887\n\nThe winter of 1886–1887 was extremely harsh for much of continental North America, especially the United States. Although it affected other regions in the country, it is most known for its effects on the Western United States and its cattle industry. This winter marked the end of the open range era and led to the entire reorganization of ranching.\n\nThe summer of 1886 had been unusually hot and dry, with numerous prairie fires, and water sources often dried up. In the fall, signs of a harsh winter ahead began to appear. Birds began flying south earlier than usual, beavers were seen collecting more wood than normal for the winter ahead, and some cattle grew thicker and shaggier coats.\n\nThe first snows fell earlier than usual, in November, and were reported as some of the worst in memory. Extreme cold killed humans and animals. Some people got lost near their houses and froze to death very near their front doors. The winter weather even reached the West Coast, with snowfall of 3.7 inches in downtown San Francisco setting an all-time record on February 5, 1887.\n\nThe loss of livestock was not discovered until spring, when many cattle carcasses were spread across the fields and washed down streams. The few remaining cattle were in poor health, emaciated and suffering from frostbite. This resulted in the cattle being sold for much less, in some cases leading to bankruptcy.\n\nFuture president Theodore Roosevelt's cattle ranch near Medora, Dakota Territory was among those hit hard by that winter. In a letter to his friend, Henry Cabot Lodge, Roosevelt remarked \"Well, we have had a perfect smashup all through the cattle country of the northwest. The losses are crippling. For the first time I have been utterly unable to enjoy a visit to my ranch. I shall be glad to get home.\"\n\n\n"}
{"id": "1532591", "url": "https://en.wikipedia.org/wiki?curid=1532591", "title": "Zud", "text": "Zud\n\nA zud or dzud () is a Mongolian term for a severe winter in which large number of livestock die, primarily due to starvation due to being unable to graze, in other cases directly from the cold. There are various kinds of zud, including white zud, which is an extremely snowy winter in which livestock are unable to find nourishing foodstuff through the snow cover and starve.\n\nOne-third of Mongolia's population depends entirely on pastoral farming for its livelihood, and harsh zuds can cause economic crises and food security issues in the country. This natural disaster is unique to Mongolia.<ref name=\"http://www.bbc.com\"> The slow and deadly dzud in Mongolia, \"BBC\", 14 May 2010</ref>\n\nThere are different types of zud:\n\n\nSome traditional methods to protect the livestock from such inclement weather conditions include drying and storing cut grass during the summer months, and collecting sheep and goat dung to build dried flammable blocks called \"Khurjun\" or kizyak. Dried grass can be fed to animals to prevent death from starvation when zud occurs. The \"Khurjun\", or blocks of sheep and goat dung, were stacked to create a wall that protects the animals from the wind chills, and keep them warm enough to withstand the harsh conditions. These blocks can also be burnt as fuel during the winter. These methods are still practiced today in the westernmost parts of Mongolia, and areas formerly part of the Zuun Gar nation.\n\nAlso because of the semi-permanent structure of the winter shelter for their livestock and the cold, most if not all nomads engage in transhumance (seasonal migration). They have winter locations to spend the winter that is in a valley protected by mountains on most sides from the wind, while in the summer they move to more open space.\n\nHuman factors worsen the situation caused by the harsh winters. Under the communist regime, the state regulated the size of the herds to prevent overgrazing. The 1990s saw a deregulation of Mongolia's economy and a simultaneous growth in worldwide demand for cashmere wool, which is made from goat hair. As a result, the number of goats in Mongolia has grown sharply. Unlike sheep, goats tend to damage the grass by nibbling at its roots; their sharp hooves also damage the upper layer of the pasture, which is subsequently swept away by the wind. This leads to desertification.\n\nIt is not uncommon for zuds to kill over 1 million head of livestock in a winter. The 1944 record of almost 7 million head of livestock lost was shattered in the 21st century. Of note, the arctic oscillation in both 1944–45 and in 2010 was pushed much deeper into Central Asia bringing prolonged extreme cold weather. In 1999–2000, 2000–2001 and 2001–2002, Mongolia was hit by three \"zuds\" in a row, in which a combined number of 11 million animals were lost.\n\nIn winter 2009–2010 80% of the country's territory was covered with a snow blanket of 200–600mm (7-24 inches). In the Uvs aimag, extreme cold (night temperature of −48 °C / −54 °F) remained for almost 50 days. 9,000 families lost their entire herds while a further 33,000 suffered 50% loss. The Ministry of Food, Agriculture and Light Industry reported 2,127,393 head of livestock were lost as of February 9, 2010 (188,270 horse, cattle, camel and 1,939,123 goat and sheep). The agriculture ministry predicted that livestock losses might reach 4 million before the end of winter. But by May 2010, the United Nations reported that eight million, or about 17% of the country's entire livestock, had died.\n\nIn the winter 2015–2016, extreme temperatures were again recorded and the previous summer's drought lead to insufficient hay fodder reserves for many herders which is creating another ongoing loss of livestock.\n\nSome herders who lose all of their animals to \"zud\" have to seek a new life in the cities. Mongolia's capital Ulaanbaatar is surrounded by clusters of wooden houses without roads, water or sewage systems. Lacking in education and skills to survive in an urban environment, many displaced herders cannot find work and fall into extreme poverty, alcoholism and crime. Others risk their lives in dangerous illegal mining jobs.<ref name=\"https://newint.org\"> Mongolia’s dzud disaster, \"The New Internationalist\", May 10, 2016</ref>\n\nIn Kazakh, zhut is a term for a famine due to natural disaster.\n\n"}
