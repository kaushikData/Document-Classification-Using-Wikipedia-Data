{"id": "7134810", "url": "https://en.wikipedia.org/wiki?curid=7134810", "title": "AMBO pipeline", "text": "AMBO pipeline\n\nAMBO pipeline was a planned oil pipeline from the Bulgarian Black Sea port of Burgas via the Republic of Macedonia to the Albanian Adriatic port of Vlorë.\n\nThe pipeline was proposed already in 1993. On 27 December 2004, prime-ministers of Albania, the Republic of Macedonia and Bulgaria signed the latest political declaration, followed by the memorandum of understanding between representatives of Albania, the Republic of Macedonia and Bulgaria and Ted Ferguson, the president and CEO of AMBO. On 30 October 2006, Albania and the Republic of Macedonia signed a protocol to determinate the entrance points of the pipeline. The entrance point will be Stebleve village in Albania and Lakaica village in the Republic of Macedonia. A similar protocol between Bulgaria and the Republic of Macedonia was signed later in 2006. \n\nOn 31 January 2007, the Republic of Macedonia, Bulgaria and Albania signed a trilateral convention on the construction of the AMBO pipeline. This document was ratified by the Parliaments of all three countries and governed the construction, operation, and maintenance of the pipelines.\n\nThe aim of the long pipeline was to bypass the Turkish Straits in transportation of Russian and Caspian oil. The pipeline was expected to cost about US$1.5 billion and it would have a capacity of . There would be four pump stations, two in Bulgaria and one each in the Republic of Macedonia and Albania, constructed along the route. A pre-front-end engineering and design study (FEED) was to be prepared by KBR. The pipeline was expected to be operational by 2011.\n\nThe pipeline was to be built and operated by the US-registered Albanian Macedonian Bulgarian Oil Corporation (AMBO). The project was backed by the US government, who financed a feasibility study of pipeline.\n\nOther pipeline projects were the Burgas-Alexandroupoli pipeline from Burgas to the Greek Aegean port Alexandroupoli, and the Pan-European Pipeline from Constanţa in Romania to Trieste in Italy. Compared with Burgas-Alexandroupoli pipeline, the AMBO pipeline would be longer and more expensive, but Vlorë (which is a sheltered, deep-water, all-weather port) could accommodate larger tankers and is more accessible than Alexandroupoli. Also, an oil spill in the Aegean would have a negative influence on Greece's tourism industry.\n\n"}
{"id": "18946671", "url": "https://en.wikipedia.org/wiki?curid=18946671", "title": "Arad Power Station", "text": "Arad Power Station\n\nThe Arad Power Station is a large thermal power plant located in Arad, having 2 generation groups of 50 MW each and one generating unit of 12 MW having a total electricity generation capacity of 112 MW.\n\nThe eastern chimney of the power station's two chimneys is 201 meters tall.\n\n\n"}
{"id": "44226762", "url": "https://en.wikipedia.org/wiki?curid=44226762", "title": "Beachport Conservation Park", "text": "Beachport Conservation Park\n\n\n"}
{"id": "48661943", "url": "https://en.wikipedia.org/wiki?curid=48661943", "title": "Bismuth titanate", "text": "Bismuth titanate\n\nBismuth titanate or bismuth titanium oxide is a solid inorganic compound of bismuth, titanium and oxygen with the chemical formula of BiTiO,\nBi TiO or BiTiO\n\nBismuth titanate ceramics can be produced by heating a mixture of bismuth and titanium oxides. BiTiO forms at 730–850 °C, and melts when the temperature is raised above 875 °C, decomposing in the melt to BiTiO and BiO. Millimeter-sized single crystals of BiTiO can be grown by the Czochralski process, from the molten phase at 880–900 °C.\n\nBismuth titanates exhibit electrooptical effect and photorefractive effect, that is, a reversible change in the refractive index under applied electric field or illumination, respectively. Consequently, they have potential applications in reversible recording media for real-time holography or image processing applications.\n\n"}
{"id": "34688121", "url": "https://en.wikipedia.org/wiki?curid=34688121", "title": "Coastal hazards", "text": "Coastal hazards\n\nCoastal Hazards are physical phenomena that expose a coastal area to risk of property damage, loss of life and environmental degradation. Rapid-onset hazards last over periods of minutes to several days and examples include major cyclones accompanied by high winds, waves and surges or tsunamis created by submarine earthquakes and landslides. Slow-onset hazards develop incrementally over longer time periods and examples include erosion and gradual inundation.\n\nSince early civilisation, coastal areas have been attractive settling grounds for human population as they provided abundant marine resources, fertile agricultural land and possibilities for trade and transport. This has led to high population densities and high levels of development in many coastal areas and this trend is continuing into the 21st century. At present, about 1,2 billion people live in coastal areas globally, and this number is predicted to increase to 1,8–5,2 billion by the 2080s due to a combination of population growth and coastal migration. Along with this increase follows major investments in infrastructure and the build environment.\n\nThe characteristics of coastal environments, however, pose some great challenges to human habitation. Coastlines are highly dynamic natural systems that interact with terrestrial, marine and atmospheric processes and undergo continuous change in response to these processes. Over the years, human society has often failed to recognize the hazards related to these dynamics and this has led to major disasters and societal disruption to various degrees. Even today, coastal development is often taking place with little regard to the hazards present in these environments, although climate change is likely to increase the general hazard levels. Societal activities in coastal areas can also pose a hazard to the natural balance of coastal systems, thereby disrupting e.g. sensitive ecosystems and subsequently human livelihood.\n\nCoastal hazard management has become an increasingly important aspect of coastal planning in order to improve the resilience of society to coastal hazards. Possible management options include hard engineering structures, soft protection measures, various accommodation approaches as well as a managed retreat from the coastline. For addressing coastal hazards, it is also important to have early warning systems and emergency management plans in place to be able to address sudden and potential disastrous hazards i.e. major flooding events. Events as the Hurricane Katrina affecting the southern USA in 2005 and the cyclone Nargis affecting Myanmar in 2008 provides clear examples of the importance of timely coastal hazard management.\n\nThere are many different types of environments along the coasts of the United States with very diverse features that affect, influence, and mold the near-shore processes that are involved. Understanding these ecosystems and environments can further advance the mitigating techniques and policy-making efforts against natural and man-made coastal hazards in these vulnerable areas. The five most common types of coastal zones range from the northern ice-pushing, mountainous coastline of Alaska and Maine, the barrier island coasts facing the Atlantic, the steep, cliff-back headlands along the pacific coast, the marginal-sea type coastline of the Gulf region, and the coral reef coasts bordering Southern Florida and Hawaii.\n\nIce-pushing/mountainous coastline\n\nThese coastal regions along the northernmost part of the nation were affected predominantly by, along with the rest of the Pacific Coast, continuous tectonic activity, forming a very long, irregular, ridged, steep and mostly mountainous coastline. These environments are heavily occupied with permafrost and glaciers, which are the two major conditions affecting Alaska's Coastal Development.\n\nBarrier island coastline\n\nBarrier islands are a land form system that consists of fairly narrow strips of sand running parallel to the mainland and play a significant role in mitigating storm surges and oceans swells as natural storm events occur. The morphology of the various types and sizes of barrier islands depend on the wave energy, tidal range, basement controls, and sea level trends. The islands create multiple unique environments of wetland systems including marshes, estuaries, and lagoons.\n\nSteep, cliff-backing abrasion coastline\n\nThe coastline along the western part of the nation consists of very steep, cliffed rock formations generally with vegetative slopes descending down and a fringing beach below. The various sedimentary, metamorphic, and volcanic rock formations assembled along a tectonically disturbed environment, all with altering resistances running perpendicular, cause the ridged, extensive stretch of uplifted cliffs that form the peninsulas, lagoons, and valleys.\n\nMarginal-sea type coastline\n\nThe southern banks of the United States border the Gulf of Mexico, intersecting numerous rivers, forming many inlets bays, and lagoons along its coast, consisting of vast areas of marsh and wetlands. This region of landform is prone to natural disasters yet highly and continuously developed, with man-made structures attaining to water flow and control.\n\nCoral reef coastline\n\nCoral reefs are located off the shores of the southern Florida and Hawaii consisting of rough and complex natural structures along the bottom of the ocean floor with extremely diverse ecosystems, absorbing up to ninety percent of the energy dissipated from wind-generated waves. This process is a significant buffer for the inner-lying coastlines, naturally protecting and minimizing the impact of storm surge and direct wave damage. Because of the highly diverse ecosystems, these coral reefs not only provide for the shoreline protection, but also deliver an abundant amount of services to fisheries and tourism, increasing its economic value.\n\nNatural VS Human disasters\n\nThe population that lives along or near our coastlines are an extremely vulnerable population. There are numerous issues facing our coastlines and there are two main categories that these hazards can be placed under, Natural disasters and Human disasters. Both of these issues cause great damage to our coastlines and discussion is still ongoing regarding what standards or responses need to be met to help both the individuals who want to continue living along the coastline, while keeping them safe and not eroding more coastline away. Natural disasters are disasters that are out of human control and are usually caused by the weather. Disasters that include but are not limited to; storms, tsunamis, typhoons, flooding, tides, waterspouts, nor'easters, and storm surge. Human disasters occur when humans are the main culprit behind why the disaster happened. Some human disasters are but are not limited to; pollution, trawling, and human development. Natural and human disasters continue to harm the coastlines severely and they need to be researched in order to prepare/stop the hazards if possible.\n\nThe populations that live near or along the coast experience many hazards and it affects millions of people. Around ten million people globally feel the effects of coastal problems yearly and most are due to certain natural hazards like coastal flooding with storm surges and typhoons. A major problem related to coastal regions deals with how the entire global environment is changing and in response, the coastal regions are easily affected.\n\nStorms, Flooding and Erosion\n\nStorms are one of the major hazards that are associated to coastal regions. Storms, flooding, and erosion are closely associated and can happen simultaneously. Tropical storms or Hurricanes especially can devastate coastal regions. For example, Florida during Hurricane Andrew occurred in 1992 that caused extreme damage. It was a category five hurricane that caused $26.5 billion in damages and even 23 individuals lost their lives from the storm. Hurricane Katrina also caused havoc along the coast to show the extreme force a hurricane can do in a certain region. The Chennai Floods of 2015, which affected many people, is an example of flooding due to cyclones. People across the whole state of Tamil Nadu felt its impact and even parts of Andhra Pradesh got affected. There was a loss of Rs.900 crore and 280 people died. Many cyclones like this happen across Asia but the media reports only minor hurricanes which hit the United States.\n\nAlmost all storms with high wind and water cause erosion along the coast. Erosion occurs when but not limited to; along shore currents, tides, sea level rise and fall, and high winds. Larger amounts of erosion cause the coastline to erode away at a faster rate and can leave people homeless and leave less land to develop or keep for environmental reasons. Coastal erosion has been increasing over the past few years and it is still on the rise which makes it a major coastline hazard. In the United States, 45 percent of its coast line is along the Atlantic or Gulf coast and the erosion rate per year along the Gulf coast is at six feet a year. The average rate of erosion along the Atlantic is around two to three feet a year. Even with these findings, erosion rates in specific locations vary because of various environmental factors such as major storms that can cause major erosion upwards to 100 feet or more in only one day.\n\nPollution, Trawling and Human Development\n\nPollution, trawling, and human development are major human disasters that affect coastal regions. There are two main categories related to pollution, point source pollution, and nonpoint source pollution. Point source pollution is when there is an exact location such as a pipeline or a body of water that leads into the rivers and oceans. Known dumping into the ocean is also another point source of pollution. Nonpoint source pollution would pertain more to fertilizer runoff, and industrial waste. Examples of pollution that affect the coastal regions are but are not limited to; fertilizer runoff, oil spills, and dumping of hazardous materials into the oceans. More human acts that hurt the coastline are as follows; waste discharge, fishing, dredging, mining, and drilling. Oil spills are one of the most hazardous dangers towards coastal communities. They are hard to contain, difficult to clean up, and devastate everything. The fish, animals such as birds, the water, and especially the coastline near the spill. The most recent oil spill that had everybody concerned with oil spill was the BP oil spill.\n\nTrawling hurts the normal ecosystems in the water around the coastline. It depletes all ecosystems on the ocean floor such as, flounder, shellfish, marsh etc.. It is simply a giant net that is drug across the ocean floor and destroys and catches anything in its path. Human development is one of the major problems when facing coastal hazards. The overall construction of buildings and houses on the coast line takes away the natural occurrences to handle the fluctuation in water and sea level rise. Building houses in pre-flood areas or high risk areas that are extremely vulnerable to flooding are major concerns towards human development in coastal regions. Having houses and buildings in areas that are known to have powerful storms that will create people to be in risk by living there. Also pertaining to barrier islands, where land is at risk for erosion but they still continue to build there anyway. More and more houses today are being taken by the ocean; look at picture above.\n\nCoastal hazards & climate change\n\nThe predicted climate change is adding an extra risk factor to human settlement in coastal areas. Whereas the natural dynamics that shape our coastlines have been relatively stable and predictable over the last centuries, much more rapid change is now expected in processes as sea level rise, ocean temperature and acidity, tropical storm intensity and precipitation/runoff patterns. The world's coastlines will respond to these changes in different ways and at different pace depending on their bio-geophysical characteristics, but generally society will have to recognize that past coastal trends cannot be directly projected into the future. Instead, it is necessary to consider how different coastal environments will respond to the predicted climate change and take the expected future hazards into account in the coastal planning processes.\n\nNational Flood Insurance Program\n\nThe National Flood Insurance Program or NFIP was instituted in 1968 and offers home owners in qualifying communities an opportunity to rebuild and recover after flooding events following the decision by insurance companies to discontinue providing flood insurance. This decision was made on behalf of the private insurers after continually high and widespread flood losses. The goals of this program are to not only better protect individuals from flood, but to reduce property losses, and reduce the total amount disbursed for flood loses by the government. Only communities which have adopted and implemented mitigation policies that are compliant with or exceed federal regulations. The regulatory policies reduce risk to life and property located within floodplains. The NFIP also comprehensively mapped domestic floodplains increasing public awareness of risk. The majority of structures were constructed after the mapping was completed and risk could be assessed. To reduce the cost to these owners, which constitute roughly 25% of the total policies the rates for insurance are subsidized.\n\nCoastal States Organization\n\nThe Coastal States Organization or COS was established in 1970 to represent 35 U.S. sub-federal governments on issues of coastal policies. CSO lobbies Congress on issues pertaining to Coastal Policy allowing states input on federal policy decisions. Funding, support, water quality, coastal hazards, and coastal zone management are the primary issues COS promotes. The strategic goals of COS are to provide information and assistance to members,evaluate and manage coastal needs, and secure long term funding for member states initiatives.\n\nCoastal Zone Management Act\n\nIn 1972 the Coastal Zone Management Act or CZMA works to streamline the policies which states create to a minimum federal standard for environmental protection. CZMA establishes the national policy for the development and implementation of regulatory programs for coastal land usage, which is supposed to be reflected in state legislation such as CAMA. CZMA also provides minimum building requirements to make the insurance provided through the NFIP less expensive for the government to operate by mitigating losses. Congress found that it was necessary to establish the minimum which programs should provide for. Each coastal state is required to have a program with 7 distinct parts: Identifying land uses,Identifying critical coastal areas, Management measures,Technical assistance, Public participation, Administrative coordination, State coastal zone boundary modification.\n\nThe Coastal Area Management Act\n\nThe Coastal Area Management Act or CAMA is policy that was implemented by the state of North Carolina in 1974 to work in-tandem with the CZMA. It creates a cooperative program between the state and local governments. The State government operates in an advisory capacity and reviews decisions made by local government planners. The goal of this legislation was to create a management system capable of preserving the coastal environment, insure the preservation of land and water resources, balance the use of coastal resources and establish guidelines and standards for conservations, economic development, tourism, transportation, and the protection of common law.\n\nDue to the increasing urbanization along the coastlines, planning and management are essential to protecting the ecosystems and environment from depleting. Coastal management is becoming implemented more because of the movement of people to the shore and the hazards that come with the territory. Some of the hazards include movement of barrier islands, sea level rise, hurricanes, nor'easters, earthquakes, flooding, erosion, pollution and human development along the coast. The Coastal Zone Management Act (CZMA) was created in 1972 because of the continued growth along the coast, this act introduced better management practices such as integrated coastal zone management, adaptive management and the use mitigation strategies when planning. According to the Coastal Zone Management Act, the objectives are to remain balanced to \"preserve, protect, develop, and where possible, to restore or enhance the resources of the nation's coastal zone\".\nThe development of the land can strongly affect the sea, for example the engineering of structures versus non-structures and the effects of erosion along the shore.\n\nIntegrated coastal zone management\n\nIntegrated coastal zone management means the integration of all aspects of the coastal zone; this includes environmentally, socially, culturally politically and economically to meet a sustainable balance all around. Sustainability is the goal to allow development yet protect the environment in which we develop. Coastal zones are fragile and do not do well with change so it is important to acquire sustainable development. The integration from all views will entitle a holistic view for the best implementation and management of that country, region and local scales. The five types of integration include integration among sectors, integration between land and water elements of the coastal zone, integration amount levels of government, integration between nations and integration among disciplines are all essential to meet the needs for implementation.\nManagement practices include\nThese four management practices should be based on a bottom-up approach, meaning the approach starts from a local level which is more intimate to the specific environment of that area. After assessment from the local level, the state and federal input can be implemented. The bottom-up approach is key for protecting the local environments because there is a diversity of environments that have specific needs all over the world.\n\nAdaptive management\n\nAdaptive management is another practice of development adaptation with the environment. Resources are the major factor when managing adaptively to a certain environment to accommodate all the needs of development and ecosystems. Strategies used must be flexible by either passive or active adaptive management include these key features:\nTo achieve adaptive management is testing the assumptions to achieve a desired outcome, such as trial and error, find the best known strategy then monitoring it to adapt to the environment, and learning the outcomes of success and failures of a project.\n\nMitigation\n\nThe purpose of mitigation is not only to minimize the loss of property damage, but minimize environmental damages due to development. To avoid impacts by not taking or limiting actions, to reduce or rectify impacts by rehabilitation or restoring the affected environments or instituting long-term maintenance operations and compensating for impacts by replacing or providing substitute environments for resources\nStructural mitigation is the current solution to eroding beaches and movement of sand is the use of engineered structures along the coast have been short lived and are only an illusion of safety to the public that result in long term damage of the coastline. Structural management deals with the use of the following: groins which are man-made solution to longshore current movements up and down the coast. The use of groins are efficient to some extent yet cause erosion and sand build up further down the beaches. Bulkheads are man-made structures that help protect the homes built along the coast and other bodies of water that actually induce erosion in the long run. Jetties are structures built to protect sand movement into the inlets where boats for fishing and recreation move through.\nThe use of nonstructural mitigation is the practice of using organic and soft structures for solutions to protect against coastal hazards. These include: artificial dunes, which are used to create dunes that have been either developed on or eroded. There needs to be at least two lines of dunes before any development can occur. Beach Nourishment is a major source of nonstructural mitigation to ensure that beaches are present for the communities and for the protection of the coastline. Vegetation is a key factor when protecting from erosion, specifically for to help stabilize dune erosion.\n\n\n"}
{"id": "508504", "url": "https://en.wikipedia.org/wiki?curid=508504", "title": "Compressor", "text": "Compressor\n\nA compressor is a mechanical device that increases the pressure of a gas by reducing its volume. An air compressor is a specific type of gas compressor.\n\nCompressors are similar to pumps: both increase the pressure on a fluid and both can transport the fluid through a pipe. As gases are compressible, the compressor also reduces the volume of a gas. Liquids are relatively incompressible; while some can be compressed, the main action of a pump is to pressurize and transport liquids.\n\nThe main and important types of gas compressors are illustrated and discussed below:\nA positive displacement compressor is a system which compresses the air by the displacement of a mechanical linkage reducing the volume (since the reduction in volume due to a piston in thermodynamics is considered as positive displacement of the piston).\n\nReciprocating compressors use pistons driven by a crankshaft. They can be either stationary or portable, can be single or multi-staged, and can be driven by electric motors or internal combustion engines. Small reciprocating compressors from 5 to 30 horsepower (hp) are commonly seen in automotive applications and are typically for intermittent duty. Larger reciprocating compressors well over are commonly found in large industrial and petroleum applications. Discharge pressures can range from low pressure to very high pressure (>18000 psi or 180 MPa). In certain applications, such as air compression, multi-stage double-acting compressors are said to be the most efficient compressors available, and are typically larger, and more costly than comparable rotary units.\nAnother type of reciprocating compressor, usually employed in automotive cabin air conditioning systems, is the swash plate compressor, which uses pistons moved by a swash plate mounted on a shaft (see \"axial piston pump\").\n\nHousehold, home workshop, and smaller job site compressors are typically reciprocating compressors 1½ hp or less with an attached receiver tank.\n\nAn ionic liquid piston compressor, \"ionic compressor\" or \"ionic liquid piston pump\" is a hydrogen compressor based on an ionic liquid piston instead of a metal piston as in a piston-metal diaphragm compressor.\n\nRotary screw compressors use two meshed rotating positive-displacement helical screws to force the gas into a smaller space. These are usually used for continuous operation in commercial and industrial applications and may be either stationary or portable. Their application can be from to over and from low pressure to moderately high pressure (>).\n\nThe classifications of rotary screw compressors vary based on stages, cooling methods, and drive types among others. Rotary screw compressors are commercially produced in Oil Flooded, Water Flooded and Dry type.\nThe efficiency of rotary compressors depends on the air drier, and the selection of air drier is always 1.5 times volumetric delivery of the compressor.\n\nRotary vane compressors consist of a rotor with a number of blades inserted in radial slots in the rotor. The rotor is mounted offset in a larger housing that is either circular or a more complex shape. As the rotor turns, blades slide in and out of the slots keeping contact with the outer wall of the housing. Thus, a series of increasing and decreasing volumes is created by the rotating blades. Rotary Vane compressors are, with piston compressors one of the oldest of compressor technologies.\n\nWith suitable port connections, the devices may be either a compressor or a vacuum pump. They can be either stationary or portable, can be single or multi-staged, and can be driven by electric motors or internal combustion engines. Dry vane machines are used at relatively low pressures (e.g., ) for bulk material movement while oil-injected machines have the necessary volumetric efficiency to achieve pressures up to about in a single stage. A rotary vane compressor is well suited to electric motor drive and is significantly quieter in operation than the equivalent piston compressor.\n\nRotary vane compressors can have mechanical efficiencies of about 90%.\n\nThe Rolling piston in a rolling piston style compressor plays the part of a partition between the vane and the rotor. Rolling piston forces gas against a stationary vane.\n\nA scroll compressor, also known as scroll pump and scroll vacuum pump, uses two interleaved spiral-like vanes to pump or compress fluids such as liquids and gases. The vane geometry may be involute, archimedean spiral, or hybrid curves. They operate more smoothly, quietly, and reliably than other types of compressors in the lower volume range.\n\nOften, one of the scrolls is fixed, while the other orbits eccentrically without rotating, thereby trapping and pumping or compressing pockets of fluid between the scrolls.\n\nDue to minimum clearance volume between the fixed scroll and the orbiting scroll, these compressors have a very high volumetric efficiency.\n\nThis type of compressor was used as the supercharger on Volkswagen G60 and G40 engines in the early 1990s.\n\nA diaphragm compressor (also known as a membrane compressor) is a variant of the conventional reciprocating compressor. The compression of gas occurs by the movement of a flexible membrane, instead of an intake element. The back and forth movement of the membrane is driven by a rod and a crankshaft mechanism. Only the membrane and the compressor box come in contact with the gas being compressed.\n\nThe degree of flexing and the material constituting the diaphragm affects the maintenance life of the equipment. Generally stiff metal diaphragms may only displace a few cubic centimeters of volume because the metal can not endure large degrees of flexing without cracking, but the stiffness of a metal diaphragm allows it to pump at high pressures. Rubber or silicone diaphragms are capable of enduring deep pumping strokes of very high flexion, but their low strength limits their use to low-pressure applications, and they need to be replaced as plastic embrittlement occurs.\n\nDiaphragm compressors are used for hydrogen and compressed natural gas (CNG) as well as in a number of other applications.\nThe photograph on the right depicts a three-stage diaphragm compressor used to compress hydrogen gas to for use in a prototype compressed hydrogen and compressed natural gas (CNG) fueling station built in downtown Phoenix, Arizona by the Arizona Public Service company (an electric utilities company). Reciprocating compressors were used to compress the natural gas. The reciprocating natural gas compressor was developed by Sertco.\n\nThe prototype alternative fueling station was built in compliance with all of the prevailing safety, environmental and building codes in Phoenix to demonstrate that such fueling stations could be built in urban areas.\n\nDynamic compressors depend upon the inertia and momentum of a fluid.\n\nAlso known as a trompe. A mixture of air and water generated through turbulence is allowed to fall into a subterranean chamber where the air separates from the water. The weight of falling water compresses the air in the top of the chamber. A submerged outlet from the chamber allows water to flow to the surface at a lower height than the intake. An outlet in the roof of the chamber supplies the compressed air to the surface. A facility on this principle was built on the Montreal River at Ragged Shutes near Cobalt, Ontario in 1910 and supplied 5,000 horsepower to nearby mines.\n\nCentrifugal compressors use a rotating disk or impeller in a shaped housing to force the gas to the rim of the impeller, increasing the velocity of the gas. A diffuser (divergent duct) section converts the velocity energy to pressure energy. They are primarily used for continuous, stationary service in industries such as oil refineries, chemical and petrochemical plants and natural gas processing plants. Their application can be from to thousands of horsepower. With multiple staging, they can achieve high output pressures greater than .\n\nMany large snowmaking operations (like ski resorts) use this type of compressor. They are also used in internal combustion engines as superchargers and turbochargers. Centrifugal compressors are used in small gas turbine engines or as the final compression stage of medium-sized gas turbines.\n\nDiagonal or mixed-flow compressors are similar to centrifugal compressors, but have a radial and axial velocity component at the exit from the rotor. The diffuser is often used to turn diagonal flow to an axial rather than radial direction. Comparative to the conventional centrifugal compressor (of the same stage pressure ratio), the value of the speed of the mixed flow compressor is 1.5 times larger.\n\nAxial-flow compressors are dynamic rotating compressors that use arrays of fan-like airfoils to progressively compress a fluid. They are used where high flow rates or a compact design are required.\n\nThe arrays of airfoils are set in rows, usually as pairs: one rotating and one stationary. The rotating airfoils, also known as blades or \"rotors\", accelerate the fluid. The stationary airfoils, also known as \"stators\" or vanes, decelerate and redirect the flow direction of the fluid, preparing it for the rotor blades of the next stage. Axial compressors are almost always multi-staged, with the cross-sectional area of the gas passage diminishing along the compressor to maintain an optimum axial Mach number. Beyond about 5 stages or a 4:1 design pressure ratio a compressor will not function unless fitted with features such as stationary vanes with variable angles (known as variable inlet guide vanes and variable stators), the ability to allow some air to escape part-way along the compressor (known as interstage bleed) and being split into more than one rotating assembly (known as twin spools, for example).\n\nAxial compressors can have high efficiencies; around 90% polytropic at their design conditions. However, they are relatively expensive, requiring a large number of components, tight tolerances and high quality materials. Axial-flow compressors are used in medium to large gas turbine engines, natural gas pumping stations, and some chemical plants.\n\nCompressors used in refrigeration systems are often described as being either hermetic, open, or semi-hermetic, to describe how the compressor and motor drive are situated in relation to the gas or vapor being compressed. The industry name for a hermetic is hermetically sealed compressor, while a semi-hermetic is commonly called a semi-hermetic compressor.\n\nIn hermetic and most semi-hermetic compressors, the compressor and motor driving the compressor are integrated, and operate within the pressurized gas envelope of the system. The motor is designed to operate in, and be cooled by, the refrigerant gas being compressed.\n\nThe difference between the hermetic and semi-hermetic, is that the hermetic uses a one-piece welded steel casing that cannot be opened for repair; if the hermetic fails it is simply replaced with an entire new unit. A semi-hermetic uses a large cast metal shell with gasketed covers that can be opened to replace motor and pump components.\n\nThe primary advantage of a hermetic and semi-hermetic is that there is no route for the gas to leak out of the system. Open compressors rely on shaft seals to retain the internal pressure, and these seals require a lubricant such as oil to retain their sealing properties.\n\nAn open pressurized system such as an automobile air conditioner can be more susceptible to leak its operating gases. Open systems rely on lubricant in the system to splash on pump components and seals. If it is not operated frequently enough, the lubricant on the seals slowly evaporates, and then the seals begin to leak until the system is no longer functional and must be recharged. By comparison, a hermetic system can sit unused for years, and can usually be started up again at any time without requiring maintenance or experiencing any loss of system pressure.\n\nThe disadvantage of hermetic compressors is that the motor drive cannot be repaired or maintained, and the entire compressor must be replaced if a motor fails. A further disadvantage is that burnt-out windings can contaminate whole systems, thereby requiring the system to be entirely pumped down and the gas replaced. Typically, hermetic compressors are used in low-cost factory-assembled consumer goods where the cost of repair is high compared to the value of the device, and it would be more economical to just purchase a new device.\n\nAn advantage of open compressors is that they can be driven by non-electric power sources, such as an internal combustion engine or turbine. However, open compressors that drive refrigeration systems are generally not totally \"maintenance-free\" throughout the life of the system, since some gas leakage will occur over time.\n\nA compressor can be idealized as internally reversible and adiabatic, thus an isentropic steady state device, meaning the change in entropy is 0. By defining the compression cycle as isentropic, an ideal efficiency for the process can be attained, and the ideal compressor performance can be compared to the actual performance of the machine. Isotropic Compression as used in ASME PTC 10 Code refers to a reversible, adiabatic compression process \n\nIsentropic efficiency of Compressors:\n\nComparison of the differential form of the energy balance for each device\n\nLet formula_5 be heat, formula_6 be work, formula_7 be kinetic energy and formula_8 be potential energy.\nActual Compressor:\n\nReversible Compressor:\n\nThe right hand side of each compressor type is equivalent, thus:\n\nre-arranging:\n\nBy substituting the know equation formula_13 into the last equation and dividing both terms by T:\n\nFurthermore, formula_15 and T is [absolute temperature] (formula_16) which produces:\n\nformula_17\n\nor\n\nformula_18\n\nTherefore, work-consuming devices such as pumps and compressors (work is negative) require less work when they operate reversibly.\n\nisentropic process: involves no cooling,\n\npolytropic process: involves some cooling\n\nisothermal process: involves maximum cooling\n\nBy making the following assumptions the required work for the compressor to compress a gas from formula_19 to formula_20 is the following for each process:\n\nAssumptions:\n\nIsentropic (formula_23, where formula_24):\n\nPolytropic (formula_26):\n\nIsothermal (formula_28 or formula_29):\n\nBy comparing the three internally reversible processes compressing an ideal gas from formula_19 to formula_20, the results show that isentropic compression (formula_23) requires the most work in and the isothermal compression(formula_28 or formula_29) requires the least amount of work in. For the polytropic process (formula_26) work in decreases as the exponent, n, decreases, by increasing the heat rejection during the compression process. One common way of cooling the gas during compression is to use cooling jackets around the casing of the compressor.\n\nIdeal Rankine Cycle 1->2 Isentropic compression in a pump\n\nIdeal Carnot Cycle 4->1 Isentropic compression\n\nIdeal Otto Cycle 1->2 Isentropic compression\n\nIdeal Diesel Cycle 1->2 Isentropic compression\n\nIdeal Brayton Cycle 1->2 Isentropic compression in a compressor\n\nIdeal Vapor-compression refrigeration Cycle 1->2 Isentropic compression in a compressor\n\nNOTE: The isentropic assumptions are only applicable with ideal cycles. Real world cycles have inherent losses due to inefficient compressors and turbines. The real world system are not truly isentropic but are rather idealized as isentropic for calculation purposes.\n\nCompression of a gas increases its temperature.\n\nwhere\nor\nand\n\nso\n\nin which \"p\" is pressure, \"V\" is volume, \"n\" takes different values for different compression processes (see below), and 1 & 2 refer to initial and final states.\nwith \"T\" and \"T\" in degrees Rankine or kelvins, \"p\" and \"p\" being absolute pressures and \"k\" = ratio of specific heats (approximately 1.4 for air). The rise in air and temperature ratio means compression does not follow a simple pressure to volume ratio. This is less efficient, but quick. Adiabatic compression or expansion more closely model real life when a compressor has good insulation, a large gas volume, or a short time scale (i.e., a high power level). In practice there will always be a certain amount of heat flow out of the compressed gas. Thus, making a perfect adiabatic compressor would require perfect heat insulation of all parts of the machine. For example, even a bicycle tire pump's metal tube becomes hot as you compress the air to fill a tire.\nThe relation between temperature and compression ratio described above means that the value of n for an adiabatic process is k (the ratio of specific heats).\n\nFor an isothermal process, n is 1, so the value of the work integral for an isothermal process is:\n\nWhen evaluated, the isothermal work is found to be lower than the adiabatic work.\n\nIn the case of centrifugal compressors, commercial designs currently do not exceed a compression ratio of more than 3.5 to 1 in any one stage (for a typical gas). Since compression raises the temperature, the compressed gas is to be cooled between stages making the compression less adiabatic and more isothermal. The inter-stage coolers typically result in some partial condensation that is removed in vapor-liquid separators.\n\nIn the case of small reciprocating compressors, the compressor flywheel may drive a cooling fan that directs ambient air across the intercooler of a two or more stage compressor.\n\nBecause rotary screw compressors can make use of cooling lubricant to reduce the temperature rise from compression, they very often exceed a 9 to 1 compression ratio. For instance, in a typical diving compressor the air is compressed in three stages. If each stage has a compression ratio of 7 to 1, the compressor can output 343 times atmospheric pressure (7 × 7 × 7 = 343 atmospheres). ()\n\nThere are many options for the motor that powers the compressor:\n\nGas compressors are used in various applications where either higher pressures or lower volumes of gas are needed:\n"}
{"id": "50110441", "url": "https://en.wikipedia.org/wiki?curid=50110441", "title": "Cytochrome P450 omega hydroxylase", "text": "Cytochrome P450 omega hydroxylase\n\nCytochrome P450 omega hydroxylases, also termed cytochrome P450 ω-hydroxylases, CYP450 omega hydroxylases, CYP450 ω-hydroxylases, CYP omega hydroxylase, CYP ω-hydroxylases, fatty acid omega hydroxylases, cytochrome P450 monooxygenases, and fatty acid monooxygenases, are a set of cytochrome P450-containing enzymes that catalyze the addition of a hydroxyl residue to a fatty acid Substrate (chemistry). The CYP omega hydroxylases are often referred to as monoxygenases; however, the monooxygenases are CYP450 enzymes that add a hydroxyl group to a wide range of xenobiotic (e.g. drugs, industrial toxins) and naturally occurring endobiotic (e.g. cholesterol) substrates, most of which are not fatty acids. The CYP450 omega hydroxylases are accordingly better viewed as a subset of monooxygenases that have the ability to hydroxylate fatty acids. While once regarded as functioning mainly in the catabolism of dietary fatty acids, the omega oxygenases are now considered critical in the production or break-down of fatty acid-derived mediators which are made by cells and act within their cells of origin as autocrine signaling agents or on nearby cells as paracrine signaling agents to regulate various functions such as blood pressure control and inflammation.\n\nThe omega oxygenases metabolize fatty acids (RH) by adding a hydroxyl (OH) to their terminal (i.e. furthest from the fatty acids' carboxy residue) carbons; in the reaction, the two atoms of molecular oxygen(O[ are reduced to one hydroxyl group and one water (HO molecule) by the concomitant oxidation of NAD(P)H (see monooxygenase).\nRH + O + NADPH + H → ROH + HO + NADP\nCYP450 enzymes belong to a superfamily which in humans is composed of at least 57 CYPs; within this superfamily, members of six CYP4A subfamilies, (which are CYP4A, CYP4B, CYP4F, CYP4V, CYP4X, and CYP4z) possess ω-hydroxylase activity viz., CYP4A, CYP4B, and CYP4F CYP2U1 also possesses ω hydroxylase activity. These CYP ω-hydroxylases can be categorized into several groups based on their substrates and consequential function\n\n"}
{"id": "3426993", "url": "https://en.wikipedia.org/wiki?curid=3426993", "title": "Economic dispatch", "text": "Economic dispatch\n\nEconomic dispatch is the short-term determination of the optimal output of a number of electricity generation facilities, to meet the system load, at the lowest possible cost, subject to transmission and operational constraints. The Economic Dispatch Problem is solved by specialized computer software which should satisfy the operational and system constraints of the available resources and corresponding transmission capabilities. In the US Energy Policy Act of 2005, the term is defined as \"the operation of generation facilities to produce energy at the lowest cost to reliably serve consumers, recognising any operational limits of generation and transmission facilities\".\n\nThe main idea is that, in order to satisfy the load at a minimum total cost, the set of generators with the lowest marginal costs must be used first, with the marginal cost of the final generator needed to meet load setting the system marginal cost. This is the cost of delivering one additional MWh of energy onto the system. The historic methodology for economic dispatch was developed to manage fossil fuel burning power plants, relying on calculations involving the input/output characteristics of power stations.\n\nThe following is based on Kirschen (2010). The economic dispatch problem can be thought of as maximising the economic welfare of a power network whilst meeting system constraints. For a network with buses (nodes), where represents the net power injection at bus , and is the cost function of producing power at bus , the unconstrained problem is formulated as:\n\nConstraints imposed on the model are the need to maintain a power balance, and that the flow on any line must not exceed its capacity. For the power balance, the sum of the net injections at all buses must be equal to the power losses in the branches of the network:\n\nThe power losses depend on the flows in the branches and thus on the net injections as shown in the above equation. However it cannot depend on the injections on all the buses as this would give an over-determined system. Thus one bus is chosen as the Slack bus and is omitted from the variables of the function . The choice of Slack bus is entirely arbitrary, here bus is chosen.\n\nThe second constraint involves capacity constraints on the flow on network lines. For a system with lines this constraint is modeled as:\n\nwhere is the flow on branch , and is the maximum value that this flow is allowed to take. Note that the net injection at the slack bus is not included in this equation for the same reasons as above.\n\nThese equations can now be combined to build the Lagrangian of the optimization problem:\n\nwhere π and μ are the Lagrangian multipliers of the constraints. The conditions for optimality are then:\n\nwhere the last condition is needed to handle the inequality constraint on line capacity.\n\nSolving these equations is computationally difficult as they are nonlinear and implicitly involve the solution of the power flow equations. The analysis can be simplified using a linearised model called a DC power flow.\n\nIn environmental dispatch, additional considerations concerning reduction of pollution further complicate the power dispatch problem. The basic constraints of the economic dispatch problem remain in place but the model is optimized to minimize pollutant emission in addition to minimizing fuel costs and total power loss. Due to the added complexity, a number of algorithms have been employed to optimize this environmental/economic dispatch problem. Notably, a modified bees algorithm implementing chaotic modeling principles was successfully applied not only \"in silico\", but also on a physical model system of generators.. Other methods used to address the economic emission dispatch problem include Particle Swarm Optimization (PSO) and neural networks \n\nAnother notable algorithm combination is used in a real-time emissions tool called Locational Emissions Estimation Methodology (LEEM) that links electric power consumption and the resulting pollutant emissions. The LEEM estimates changes in emissions associated with incremental changes in power demand derived from the locational marginal price (LMP) information from the independent system operators (ISOs) and emissions data from the US Environmental Protection Agency (EPA). LEEM was developed at Wayne State University as part of a project aimed at optimizing water transmission systems in Detroit, MI starting in 2010 and has since found a wider application as a load profile management tool that can help reduce generation costs and emissions.\n\n\n"}
{"id": "279350", "url": "https://en.wikipedia.org/wiki?curid=279350", "title": "Electric vehicle", "text": "Electric vehicle\n\nAn electric vehicle, also called an EV, uses one or more electric motors or traction motors for propulsion. An electric vehicle may be powered through a collector system by electricity from off-vehicle sources, or may be self-contained with a battery, solar panels or an electric generator to convert fuel to electricity. EVs include, but are not limited to, road and rail vehicles, surface and underwater vessels, electric aircraft and electric spacecraft.\n\nEVs first came into existence in the mid-19th century, when electricity was among the preferred methods for motor vehicle propulsion, providing a level of comfort and ease of operation that could not be achieved by the gasoline cars of the time. Modern internal combustion engines have been the dominant propulsion method for motor vehicles for almost 100 years, but electric power has remained commonplace in other vehicle types, such as trains and smaller vehicles of all types.\n\nIn the 21st century, EVs saw a resurgence due to technological developments, and an increased focus on renewable energy. A great deal of demand for electric vehicles developed and a small core of do-it-yourself (DIY) engineers began sharing technical details for doing electric vehicle conversions. Government incentives to increase adoptions were introduced, including in the United States and the European Union.\n\nElectric motive power started in 1827, when Hungarian priest Ányos Jedlik built the first crude but viable electric motor, provided with stator, rotor and commutator, and the year after he used it to power a tiny car. A few years later, in 1835, professor Sibrandus Stratingh of University of Groningen, the Netherlands, built a small scale electric car and a Robert Anderson of Scotland is reported to have made a crude electric carriage sometime between the years of 1832 and 1839. Around the same period, early experimental electrical cars were moving on rails, too. American blacksmith and inventor Thomas Davenport built a toy electric locomotive, powered by a primitive electric motor, in 1835. In 1838, a Scotsman named Robert Davidson built an electric locomotive that attained a speed of four miles per hour (6 km/h). In England a patent was granted in 1840 for the use of rails as conductors of electric current, and similar American patents were issued to Lilley and Colten in 1847.\n\nBetween 1832 and 1839 (the exact year is uncertain), Robert Anderson of Scotland invented the first crude electric carriage, powered by non-rechargeable primary cells.\n\nThe first mass-produced electric vehicles appeared in America in the early 1900s. In 1902, \"Studebaker Automobile Company\" entered the automotive business with electric vehicles, though it also entered the gasoline vehicles market in 1904. However, with the advent of cheap assembly line cars by Ford, electric cars fell to the wayside\n\nDue to the limitations of storage batteries at that time, electric cars did not gain much popularity, however electric trains gained immense popularity due to their economies and fast speeds achievable. By the 20th century, electric rail transport became commonplace. Over time their general-purpose commercial use reduced to specialist roles, as platform trucks, forklift trucks, ambulances, tow tractors and urban delivery vehicles, such as the iconic British milk float; for most of the 20th century, the UK was the world's largest user of electric road vehicles.\n\nElectrified trains were used for coal transport, as the motors did not use precious oxygen in the mines. Switzerland's lack of natural fossil resources forced the rapid electrification of their rail network. One of the earliest rechargeable batteries - the nickel-iron battery - was favored by Edison for use in electric cars.\n\nEVs were among the earliest automobiles, and before the preeminence of light, powerful internal combustion engines, electric automobiles held many vehicle land speed and distance records in the early 1900s. They were produced by Baker Electric, Columbia Electric, Detroit Electric, and others, and at one point in history out-sold gasoline-powered vehicles. In fact, in 1900, 28 percent of the cars on the road in the USA were electric. EVs were so popular that even President Woodrow Wilson and his secret service agents toured Washington, DC, in their Milburn Electrics, which covered per charge.\n\nA number of developments contributed to decline of electric cars. Improved road infrastructure required a greater range than that offered by electric cars, and the discovery of large reserves of petroleum in Texas, Oklahoma, and California led to the wide availability of affordable gasoline/petrol, making internal combustion powered cars cheaper to operate over long distances. Also internal combustion powered cars became ever easier to operate thanks to the invention of the electric starter by Charles Kettering in 1912, which eliminated the need of a hand crank for starting a gasoline engine, and the noise emitted by ICE cars became more bearable thanks to the use of the muffler, which Hiram Percy Maxim had invented in 1897. As roads were improved outside urban areas electric vehicle range could not compete with the ICE. Finally, the initiation of mass production of gasoline-powered vehicles by Henry Ford in 1913 reduced significantly the cost of gasoline cars as compared to electric cars.\n\nIn the 1930s, National City Lines, which was a partnership of General Motors, Firestone, and Standard Oil of California purchased many electric tram networks across the country to dismantle them and replace them with GM buses. The partnership was convicted of conspiring to monopolize the sale of equipment and supplies to their subsidiary companies, but were acquitted of conspiring to monopolize the provision of transportation services.\n\nIn January 1990, General Motors' President introduced its EV concept two-seater, the \"Impact\", at the Los Angeles Auto Show. That September, the California Air Resources Board mandated major-automaker sales of EVs, in phases starting in 1998. From 1996 to 1998 GM produced 1117 EV1s, 800 of which were made available through three-year leases.\n\nChrysler, Ford, GM, Honda, and Toyota also produced limited numbers of EVs for California drivers. In 2003, upon the expiration of GM's EV1 leases, GM discontinued them. The discontinuation has variously been attributed to:\nA movie made on the subject in 2005-2006 was titled \"Who Killed the Electric Car?\" and released theatrically by Sony Pictures Classics in 2006. The film explores the roles of automobile manufacturers, oil industry, the U.S. government, batteries, hydrogen vehicles, and consumers, and each of their roles in limiting the deployment and adoption of this technology.\n\nFord released a number of their Ford Ecostar delivery vans into the market. Honda, Nissan and Toyota also repossessed and crushed most of their EVs, which, like the GM EV1s, had been available only by closed-end lease. After public protests, Toyota sold 200 of its RAV EVs to eager buyers; they later sold at over their original forty-thousand-dollar price. This lesson did not go unlearned; BMW of Canada sold off a number of Mini EVs when their Canadian testing ended.\n\nThe production of the Citroën Berlingo Electrique stopped in September 2005.\n\nDuring the last few decades, environmental impact of the petroleum-based transportation infrastructure, along with the fear of peak oil, has led to renewed interest in an electric transportation infrastructure. EVs differ from fossil fuel-powered vehicles in that the electricity they consume can be generated from a wide range of sources, including fossil fuels, nuclear power, and renewable sources such as tidal power, solar power, and wind power or any combination of those. The carbon footprint and other emissions of electric vehicles varies depending on the fuel and technology used for electricity generation. The electricity may then be stored on board the vehicle using a battery, flywheel, or supercapacitors. Vehicles making use of engines working on the principle of combustion can usually only derive their energy from a single or a few sources, usually non-renewable fossil fuels. A key advantage of hybrid or plug-in electric vehicles is regenerative braking, which recovers kinetic energy, typically lost during friction braking as heat, as electricity restored to the on-board battery.\n\n, there are some 45 series production highway-capable all-electric cars available in various countries. As of early December 2015, the Leaf, with 200,000 units sold worldwide, is the world's top-selling highway-capable all-electric car in history, followed by the Tesla Model S with global deliveries of about 100,000 units. Leaf global sales achieved the 300,000 unit milestone in January 2018.\n\n, more than 500,000 highway-capable all-electric passenger cars and light utility vehicles have been sold worldwide since 2008, out of total global sales of about 850,000 light-duty plug-in electric vehicles. , the United States had the largest fleet of highway-capable plug-in electric vehicles in the world, with about 335,000 highway legal plug-in electric cars sold in the country since 2008, and representing about 40% of the global stock. California is the largest plug-in car regional market in the country, with almost 143,000 units sold between December 2010 and March 2015, representing over 46% of all plug-in cars sold in the U.S. Cumulative global sales of all-electric cars and vans passed the 1 million unit milestone in September 2016.\n\nNorway is the country with the highest market penetration per capita in the world, with four plug-in electric vehicles per 1000 inhabitants in 2013. In March 2014, Norway became the first country where over 1 in every 100 passenger cars on the roads is a plug-in electric. In 2016, 29% of all new car sales in the country were battery-powered or plug-in hybrids. Norway also has the world's largest plug-in electric segment market share of total new car sales, 13.8% in 2014, up from 5.6% in 2013. In June 2016, Andorra became the second country in this list, with a 6% of market share combining electric vehicles and plug-in hybrids due to a strong public policy providing multiple advantages. , there were 58,989 plug-in electric vehicles registered in Norway, consisting of 54,160 all-electric vehicles and 4,829 plug-in hybrids. By the end of 2016, Norway's 100,000th battery-powered car was sold.\n\nBy some estimates electric vehicles sales may constitute almost a third of new-car sales by the end of 2030.\n\nThere are many ways to generate electricity, of varying costs, efficiency and ecological desirability.\n\n(See articles on diesel-electric and gasoline-electric hybrid locomotion for information on EVs using also combustion engines).\n\nIt is also possible to have hybrid EVs that derive electricity from multiple sources. Such as:\n\nAnother form of chemical to electrical conversion is fuel cells, projected for future use.\n\nFor especially large EVs, such as submarines, the chemical energy of the diesel-electric can be replaced by a nuclear reactor. The nuclear reactor usually provides heat, which drives a steam turbine, which drives a generator, which is then fed to the propulsion. \"See Nuclear Power\"\n\nA few experimental vehicles, such as some cars and a handful of aircraft use solar panels for electricity.\n\nThese systems are powered from an external generator plant (nearly always when stationary), and then disconnected before motion occurs, and the electricity is stored in the vehicle until needed.\n\nBatteries, electric double-layer capacitors and flywheel energy storage are forms of rechargeable on-board electrical storage. By avoiding an intermediate mechanical step, the energy conversion efficiency can be improved over the hybrids already discussed, by avoiding unnecessary energy conversions. Furthermore, electro-chemical batteries conversions are easy to reverse, allowing electrical energy to be stored in chemical form.\n\nMost electric vehicles use lithium-ion batteries (Li-Ions or LIBs). Lithium ion batteries have higher energy density, longer life span and higher power density than most other practical batteries. Complicating factors include safety, durability, thermal breakdown and cost. Li-ion batteries should be used within safe temperature and voltage ranges in order to operate safely and efficiently.\n\nIncreasing the battery's lifespan decreases effective costs. One technique is to operate a subset of the battery cells at a time and switching these subsets.\n\nIn the past, Nickel Metal Hydride batteries were used among EV cars such as those made by General Motors. These battery types are considered out-dated due to their tendencies to self discharge in the heat. Also the batteries' patent was held by Chevron which created a problem for their widespread development. These detractors coupled with their high cost has led to Lithium-ion (Li-Ion) batteries leading as the predominant battery for EVs.\n\nLithium-ion batteries' price is constantly decreasing, thus, making electric vehicles more affordable and attractive on the market.\n\nThe power of a vehicle's electric motor, as in other vehicles, is measured in kilowatts (kW). 100 kW is roughly equal to 134 horsepower, but electric motors can deliver their maximum torque over a wide RPM range. This means that the performance of a vehicle with a 100 kW electric motor exceeds that of a vehicle with a 100 kW internal combustion engine, which can only deliver its maximum torque within a limited range of engine speed.\n\nEnergy is lost during the process of converting the electrical energy to mechanical energy. Approximately 90% of the energy from the battery is converted to mechanical energy, the losses being in the motor and drivetrain.\n\nUsually, direct current (DC) electricity is fed into a DC/AC inverter where it is converted to alternating current (AC) electricity and this AC electricity is connected to a 3-phase AC motor.\n\nFor electric trains, forklift trucks, and some electric cars, DC motors are often used. In some cases, universal motors are used, and then AC or DC may be employed. In recent production vehicles, various motor types have been implemented, for instance: Induction motors within Tesla Motor vehicles and permanent magnet machines in the Nissan Leaf and Chevrolet Bolt.\n\nIt is generally possible to equip any kind of vehicle with an electric powertrain.\n\nA plug-in electric vehicle (PEV) is any motor vehicle that can be recharged from any external source of electricity, such as wall sockets, and the electricity stored in the rechargeable battery packs drives or contributes to drive the wheels. PEV is a subcategory of electric vehicles that includes all-electric or battery electric vehicles (BEVs), plug-in hybrid vehicles, (PHEVs), and electric vehicle conversions of hybrid electric vehicles and conventional internal combustion engine vehicles.\n\nCumulative global sales of highway-capable light-duty pure electric vehicles passed one million units in total, globally, in September 2016. Cumulative global sales of plug-in cars and utility vans totaled over 2 million by the end of 2016, of which 38% were sold in 2016, and the 3 million milestone was achieved in November 2017.\n\n, the world's top selling plug-in electric cars is the Nissan Leaf, with global sales of more than 300,000 units. , it was followed by the all-electric Tesla Model S with about 129,400 units sold worldwide, the Chevrolet Volt plug-in hybrid, which together with its sibling the Opel/Vauxhall Ampera has combined global sales of about 117,300 units, the Mitsubishi Outlander P-HEV with about 107,400 units, and the Prius Plug-in Hybrid with over 75,400 units.\n\nA hybrid electric vehicle combines a conventional (usually fossil fuel-powered) powertrain with some form of electric propulsion. , over 11 million hybrid electric vehicles have been sold worldwide since their inception in 1997. Japan is the market leader with more than 5 million hybrids sold, followed by the United States with cumulative sales of over 4 million units since 1999, and Europe with about 1.5 million hybrids delivered since 2000. Japan has the world's highest hybrid market penetration. By 2013 the hybrid market share accounted for more than 30% of new standard passenger car sold, and about 20% new passenger vehicle sales including kei cars. Norway ranks second with a hybrid market share of 6.9% of new car sales in 2014, followed by the Netherlands with 3.7%\n\nGlobal hybrid sales are by Toyota Motor Company with more than 9 million Lexus and Toyota hybrids sold , followed by Honda Motor Co., Ltd. with cumulative global sales of more than 1.35 million hybrids , Ford Motor Corporation with over 424,000 hybrids sold in the United States through June 2015, and the Hyundai Group with cumulative global sales of 200,000 hybrids , including both Hyundai Motor Company and Kia Motors hybrid models. , worldwide hybrid sales are led by the Toyota Prius liftback, with cumulative sales of over 3.7 million units. The Prius nameplate has sold more than 5.7 million hybrids up to April 2016.\n\nEVs are on the road in many functions, including electric cars, electric trolleybuses, electric buses, battery electric buses, electric trucks, electric bicycles, electric motorcycles and scooters, personal transporters, neighborhood electric vehicles, golf carts, milk floats, and forklifts. Off-road vehicles include electrified all-terrain vehicles and tractors.\n\nThe fixed nature of a rail line makes it relatively easy to power EVs through permanent overhead lines or electrified third rails, eliminating the need for heavy onboard batteries. Electric locomotives, electric trams/streetcars/trolleys, electric light rail systems, and electric rapid transit are all in common use today, especially in Europe and Asia.\n\nSince electric trains do not need to carry a heavy internal combustion engine or large batteries, they can have very good power-to-weight ratios. This allows high speed trains such as France's double-deck TGVs to operate at speeds of 320 km/h (200 mph) or higher, and electric locomotives to have a much higher power output than diesel locomotives. In addition, they have higher short-term surge power for fast acceleration, and using regenerative brakes can put braking power back into the electrical grid rather than wasting it.\n\nMaglev trains are also nearly always EVs.\n\nManned and unmanned vehicles have been used to explore the Moon and other planets in the solar system. On the last three missions of the Apollo program in 1971 and 1972, astronauts drove silver-oxide battery-powered Lunar Roving Vehicles distances up to on the lunar surface. Unmanned, solar-powered rovers have explored the Moon and Mars.\n\nSince the beginning of the dawn of the time of aviation, electric power for aircraft has received a great deal of experimentation. Currently flying electric aircraft include manned and unmanned aerial vehicles.\n\nElectric boats were popular around the turn of the 20th century. Interest in quiet and potentially renewable marine transportation has steadily increased since the late 20th century, as solar cells have given motorboats the infinite range of sailboats. Electric motors can and have also been used in sailboats instead of traditional diesel engines. Electric ferries operate routinely. Submarines use batteries (charged by diesel or gasoline engines at the surface), nuclear power, fuel cells or Stirling engines to run electric motor-driven propellers.\n\nElectric power has a long history of use in spacecraft. The power sources used for spacecraft are batteries, solar panels and nuclear power. Current methods of propelling a spacecraft with electricity include the arcjet rocket, the electrostatic ion thruster, the Hall effect thruster, and Field Emission Electric Propulsion. A number of other methods have been proposed, with varying levels of feasibility.\n\nMost large electric transport systems are powered by stationary sources of electricity that are directly connected to the vehicles through wires. Electric traction allows the use of regenerative braking, in which the motors are used as brakes and become generators that transform the motion of, usually, a train into electrical power that is then fed back into the lines. This system is particularly advantageous in mountainous operations, as descending vehicles can produce a large portion of the power required for those ascending. This regenerative system is only viable if the system is large enough to utilise the power generated by descending vehicles.\n\nIn the systems above, motion is provided by a rotary electric motor. However, it is possible to \"unroll\" the motor to drive directly against a special matched track. These linear motors are used in maglev trains which float above the rails supported by magnetic levitation. This allows for almost no rolling resistance of the vehicle and no mechanical wear and tear of the train or track. In addition to the high-performance control systems needed, switching and curving of the tracks becomes difficult with linear motors, which to date has restricted their operations to high-speed point to point services.\n\nThe type of battery, the type of traction motor and the motor controller design vary according to the size, power and proposed application, which can be as small as a motorized shopping cart or wheelchair, through pedelecs, electric motorcycles and scooters, neighborhood electric vehicles, industrial fork-lift trucks and including many hybrid vehicles.\n\nAlthough EVs have few direct emissions, all rely on energy created through electricity generation, and will usually emit pollution and generate waste, unless it is generated by renewable source power plants. Since EVs use whatever electricity is delivered by their electrical utility/grid operator, EVs can be made more or less efficient, polluting and expensive to run, by modifying the electrical generating stations. This would be done by an electrical utility under a government energy policy, in a timescale negotiated between utilities and government.\n\nFossil fuel vehicle efficiency and pollution standards take years to filter through a nation's fleet of vehicles. New efficiency and pollution standards rely on the purchase of new vehicles, often as the current vehicles already on the road reach their end-of-life. Only a few nations set a retirement age for old vehicles, such as Japan or Singapore, forcing periodic upgrading of all vehicles already on the road.\n\nEVs will take advantage of whatever environmental gains happen when a renewable energy generation station comes online, a fossil-fuel power station is decommissioned or upgraded. Conversely, if government policy or economic conditions shifts generators back to use more polluting fossil fuels and internal combustion engine vehicles (ICEVs), or more inefficient sources, the reverse can happen. Even in such a situation, electrical vehicles are still more efficient than a comparable amount of fossil fuel vehicles. In areas with a deregulated electrical energy market, an electrical vehicle owner can choose whether to run his electrical vehicle off conventional electrical energy sources, or strictly from renewable electrical energy sources (presumably at an additional cost), pushing other consumers onto conventional sources, and switch at any time between the two.\n\nBecause of the different methods of charging possible, the emissions produced have been quantified in different ways. Plug-in all-electric and hybrid vehicles also have different consumption characteristics.\n\nElectromagnetic radiation from high performance electrical motors has been claimed to be associated with some human ailments, but such claims are largely unsubstantiated except for extremely high exposures. Electric motors can be shielded within a metallic Faraday cage, but this reduces efficiency by adding weight to the vehicle, while it is not conclusive that all electromagnetic radiation can be contained.\n\nIf a large proportion of private vehicles were to convert to grid electricity it would increase the demand for generation and transmission, and consequent emissions. However, overall energy consumption and emissions would diminish because of the higher efficiency of EVs over the entire cycle. In the USA it has been estimated there is already nearly sufficient existing power plant and transmission infrastructure, assuming that most charging would occur overnight, using the most efficient off-peak base load sources.\n\nIn the UK however, things are different. While National Grid's high-voltage electricity transmission system can currently manage the demand of 1 million electric cars, Steve Holliday (CEO National Grid PLC) said, “penetration up and above that becomes a real issue. Local distribution networks in cities like London may struggle to balance their grids if drivers choose to all plug in their cars at the same time.\"\n\nEVs typically charge from conventional power outlets or dedicated charging stations, a process that typically takes hours, but can be done overnight and often gives a charge that is sufficient for normal everyday usage.\n\nHowever, with the widespread implementation of electric vehicle networks within large cities in the UK and Europe, EV users can plug in their cars whilst at work and leave them to charge throughout the day, extending the possible range of commutes and eliminating range anxiety.\n\nA recharging system that avoids the need for a cable is Curb Connect, patented in 2012 by Dr Gordon Dower. In this system, electrical contacts are fitted into curbs, such as angle parking spaces on city streets. When a suitably authorized vehicle is parked so that its front end overhangs the curb, the curb contacts become energized and charging occurs.\n\nAnother proposed solution for daily recharging is a standardized inductive charging system such as Evatran's Plugless Power. Benefits are the convenience of parking over the charge station and minimized cabling and connection infrastructure. Qualcomm is trialling such a system in London in early 2012.\n\nYet another proposed solution for the typically less frequent, long distance travel is \"rapid charging\", such as the Aerovironment PosiCharge line (up to 250 kW) and the Norvik MinitCharge line (up to 300 kW). Ecotality is a manufacturer of Charging Stations and has partnered with Nissan on several installations. Battery replacement is also proposed as an alternative, although no OEMs including Nissan/Renault have any production vehicle plans. Swapping requires standardization across platforms, models and manufacturers. Swapping also requires many times more battery packs to be in the system.\n\nAccording to Department of Energy research conducted at Pacific Northwest National Laboratory, 84% of existing vehicles could be switched over to plug-in hybrids without requiring any new grid infrastructure. In terms of transportation, the net result would be a 27% total reduction in emissions of the greenhouse gases carbon dioxide, methane, and nitrous oxide, a 31% total reduction in nitrogen oxides, a slight reduction in nitrous oxide emissions, an increase in particulate matter emissions, the same sulfur dioxide emissions, and the near elimination of carbon monoxide and volatile organic compound emissions (a 98% decrease in carbon monoxide and a 93% decrease in volatile organic compounds). The emissions would be displaced away from street level, where they have \"high human-health implications.\"\n\nInstead of recharging EVs from electric socket, batteries could be mechanically replaced at special stations in a couple of minutes (battery swapping).\n\nBatteries with greatest energy density such as metal-air fuel cells usually cannot be recharged in purely electric way. Instead, some kind of metallurgical process is needed, such as aluminum smelting and similar.\n\nSilicon-air, aluminum-air and other metal-air fuel cells look promising candidates for swap batteries.\nAny source of energy, renewable or non-renewable, could be used to remake used metal-air fuel cells with\nrelatively high efficiency. Investment in infrastructure will be needed. The cost of such batteries could be\nan issue, although they could be made with replaceable anodes and electrolyte.\n\nInstead of replacing batteries, it is possible to replace the entire chassis (including the batteries, electric motor and wheels) of an electric Modular vehicle.\n\nSuch a system was patented in 2000 by Dr Gordon Dower and three road-licensed prototypes have been built by the Ridek Corporation in Point Roberts, Washington. Dower proposed that an individual might own only the body (or perhaps a few different style bodies) for their vehicle, and would lease the chassis from a pool, thereby reducing the depreciation costs associated with vehicle ownership.\n\nConventional electric double-layer capacitors are being worked to achieve the energy density of lithium ion batteries, offering almost unlimited lifespans and no environmental issues. High-K electric double-layer capacitors, such as EEStor's EESU, could improve lithium ion energy density several times over if they can be produced. Lithium-sulphur batteries offer . Sodium-ion batteries promise with only minimal expansion/contraction during charge/discharge and a very high surface area. Researchers from one of the Ukrainian state universities claim that they have manufactured samples of pseudocapacitor based on Li-ion intercalation process with specific energy, which seem to be at least two times improvement in comparison to typical Li-ion batteries.\n\nThe United Nations in Geneva (UNECE) has adopted the first international regulation (Regulation 100) on safety of both fully electric and hybrid electric cars, with the intent of ensuring that cars with a high voltage electric power train, such as hybrid and fully EVs, are as safe as combustion-powered cars. The EU and Japan have already indicated that they intend to incorporate the new UNECE Regulation in their respective rules on technical standards for vehicles\n\nThere is a growing concern about the safety of EVs, given the demonstrated tendency of the Lithium-ion battery, most promising for EV use because of its high energy density, to overheat, possibly leading to fire or explosion, especially when damaged in a crash. The U.S. National Highway Traffic Safety Administration opened a defect investigation of the Chevy Volt on November 25, 2011 amid concerns over the risk of battery fires in a crash. At that time, automotive consulting firm CNW Marketing Research reported a decline in consumer interest in the Volt, citing the fires as having made an impact on consumer perception. Consumer response impelled GM to make safety enhancements to the battery system in December, and the NHTSA closed its investigation on January 20, 2012, finding the matter satisfactorily resolved with \"no discernible defect trend\" remaining. The agency also announced it has developed interim guidance to increase awareness and identify appropriate safety measures regarding electric vehicles for the emergency response community, law enforcement officers, tow truck operators, storage facilities and consumers.\n\nEVs release no tail pipe air pollutants at the place where they are operated. They also typically generate less noise pollution than an internal combustion engine vehicle, whether at rest or in motion. The energy that electric and hybrid cars consume is usually generated by means that have environmental impacts. Nevertheless, adaptation of EVs would have a significant net environmental benefit, except in a few countries that continue to rely on older coal fired power plants for the bulk of their electricity generation throughout the life of the car.\n\nThere are special kind of electric vehicles named SAFA TEMPO in Nepal that help lower the pollution created by vehicles. These vehicles are powered by electricity - usually charged batteries - rather than oil or gas and currently heavily promoted by the government to facilitate environmental and vehicle management issues. Electric motors don't require oxygen, unlike internal combustion engines; this is useful for submarines and for space rovers.\n\nA study by Cambridge Econometrics shows the potential air pollution benefits of EVs. According to one of the scenarios in the study, Europe would be on track to reduce CO2 emissions from cars by 88% by 2050. The associated technology improvements would cut toxic nitrogen oxides (NOx) from cars from around 1.3 million tonnes per year to around 70,000 tonnes per year.\n\nElectric motors are mechanically very simple and often achieve 90% energy conversion efficiency over the full range of speeds and power output and can be precisely controlled. They can also be combined with regenerative braking systems that have the ability to convert movement energy back into stored electricity. This can be used to reduce the wear on brake systems (and consequent brake pad dust) and reduce the total energy requirement of a trip. Regenerative braking is especially effective for start-and-stop city use.\n\nThey can be finely controlled and provide high torque from rest, unlike internal combustion engines, and do not need multiple gears to match power curves. This removes the need for gearboxes and torque converters.\n\nEVs provide quiet and smooth operation and consequently have less noise and vibration than internal combustion engines. While this is a desirable attribute, it has also evoked concern that the absence of the usual sounds of an approaching vehicle poses a danger to blind, elderly and very young pedestrians. To mitigate this situation, automakers and individual companies are developing systems that produce warning sounds when EVs are moving slowly, up to a speed when normal motion and rotation (road, suspension, electric motor, etc.) noises become audible.\n\nElectricity can be produced from a variety of sources, therefore it gives the greatest degree of energy resilience.\n\nEV 'tank-to-wheels' efficiency is about a factor of 3 higher than internal combustion engine vehicles. Energy is not consumed while the vehicle is stationary, unlike internal combustion engines which consume fuel while idling. However, looking at the well-to-wheel efficiency of EVs, their total emissions, while still lower, are closer to an efficient gasoline or diesel in most countries where electricity generation relies on fossil fuels.\n\nWell-to-wheel efficiency of an EV has less to do with the vehicle itself and more to do with the method of electricity production. A particular EV would instantly become twice as efficient if electricity production were switched from fossil fuel to a wind or tidal primary source of energy. Thus, when \"well-to-wheels\" is cited, one should keep in mind that the discussion is no longer about the vehicle, but rather about the entire energy supply infrastructure - in the case of fossil fuels this should also include energy spent on exploration, mining, refining, and distribution.\n\nThe lifecycle analysis of EVs shows that even when powered by the most carbon intensive electricity in Europe, they emit less greenhouse gases than a conventional diesel vehicle.\n\nThe cost of operating an EV varies wildly depending on location. In some parts of the world, an EV costs less to drive than a comparable gas-powered vehicle, as long as the higher initial purchase price is not factored in. In the US, in states which have a tiered electricity rate schedule, \"fuel\" for EVs today costs owners significantly more than fuel for a comparable gas-powered vehicle. A 2011 study done by Purdue University found that in California most users already reach the third pricing tier for electricity each month, and adding an EV could push them into the fourth or fifth (highest, most expensive) tier, meaning that they will be paying in excess of $0.45 per kWh for electricity to recharge their vehicle. At this price, which is higher than the average electricity price in the US, it is dramatically more expensive to drive a pure-EV than it is to drive a traditional pure-gas powered vehicle. \"The objective of a tiered pricing system is to discourage consumption. It's meant to get you to think about turning off your lights and conserving electricity. In California, the unintended consequence is that plug-in hybrid cars won't be economical under this system,\" said Tyner (the author), whose findings were published in the online version of the journal Energy Policy.\n\nSince EVs can be plugged into the electric grid when not in use, there is a potential for battery-powered vehicles to even cut the demand for electricity by feeding electricity \"into\" the grid from their batteries during peak use periods (such as midafternoon air conditioning use) while doing most of their charging at night, when there is unused generating capacity. This vehicle-to-grid (V2G) connection has the potential to reduce the need for new power plants, as long as vehicle owners do not mind reducing the life of their batteries, by being drained by the power company during peak demand. It is also proved that an electric vehicle parking lot was able to well play the role of an agent that provides demand response.\n\nFurthermore, our current electricity infrastructure may need to cope with increasing shares of variable-output power sources such as wind and solar PV. This variability could be addressed by adjusting the speed at which EV batteries are charged, or possibly even discharged.\n\nSome concepts see battery exchanges and battery charging stations, much like gas/petrol stations today. Clearly these will require enormous storage and charging potentials, which could be manipulated to vary the rate of charging, and to output power during shortage periods, much as diesel generators are used for short periods to stabilize some national grids.\n\nElectric vehicles may have shorter range compared to Internal Combustion Engines, however, the price per mile of electric vehicles is falling. Most owners opt to charge their vehicles primarily at their houses while not in use due to their typically slower charging times, and added convenience.\n\nIn cold climates, considerable energy is needed to heat the interior of a vehicle and to defrost the windows. With internal combustion engines, this heat already exists as waste combustion heat diverted from the engine cooling circuit. This process offsets the greenhouse gases' external costs. If this is done with battery EVs, the interior heating requires extra energy from the vehicles' batteries. Although some heat could be harvested from the motor or motors and battery, their greater efficiency means there is not as much waste heat available as from a combustion engine.\n\nHowever, for vehicles which are connected to the grid, battery EVs can be preheated, or cooled, with little or no need for battery energy, especially for short trips.\n\nNewer designs are focused on using super-insulated cabins which can heat the vehicle using the body heat of the passengers. This is not enough, however, in colder climates as a driver delivers only about 100 W of heating power. A heat pump system, capable of cooling the cabin during summer and heating it during winter, seems to be the most practical and promising way of solving the thermal management of the EV. Ricardo Arboix introduced (2008) a new concept based on the principle of combining the thermal-management of the EV-battery with the thermal-management of the cabin using a heat pump system. This is done by adding a third heat-exchanger, thermally connected with the battery-core, to the traditional heat pump/air conditioning system used in previous EV-models like the GM EV1 and Toyota RAV4 EV. The concept has proven to bring several benefits, such as prolonging the life-span of the battery as well as improving the performance and overall energy-efficiency of the EV.\n\nShifts from private to public transport (train, trolleybus, personal rapid transit or tram) have the potential for large gains in efficiency in terms of an individual's distance traveled per kWH.\n\nResearch shows people do prefer trams, because they are quieter and more comfortable and perceived as having higher status. Therefore, it may be possible to cut liquid fossil fuel consumption in cities through the use of electric trams. Trams may be the most energy-efficient form of public transportation, with rubber wheeled vehicles using 2/3 more energy than the equivalent tram, and run on electricity rather than fossil fuels.\n\nIn terms of net present value, they are also the cheapest—Blackpool trams are still running after 100-years, but combustion buses only last about 15-years.\n\nIn May 2017, India was the first to announce plans to sell only electric vehicles by 2030. Prime Minister Narendra Modi's government kickstarted the ambitious plan by floating a tender to purchase 10,000 electric vehicles, hailed as \"the world’s single-largest EV procurement initiative.\" Along with fulfilling the urgent need to keep air pollution in check, the Indian government aims at reducing the petroleum import bill and running cost of vehicles. With nearly a third of all cars sold in 2017 of all new cars either fully electric or a hybrid, Norway is the world leader in the adoption of electric cars and pushes to sell only electric or hybrid cars by 2030. The other nations followed the lead, with France and UK announcing the plan to ban the sale of gas and diesel cars by 2040. Austria, China, Denmark, Germany, Ireland, Japan, the Netherlands, Portugal, Korea and Spain have also set official targets for electric car sales.\n\nMany governments offer incentives to promote the use of electric vehicles, with the goals of reducing air pollution and oil consumption. Some incentives intend to increase purchases of electric vehicles by offsetting the purchase price with a grant. Other incentives include lower tax rates or exemption from certain taxes, and investment in charging infrastructure.\n\nIn some states, car companies have partnered with local private utilities in order to provide large incentives on select electric vehicles. For example, in the state of Florida, Nissan and NextEra Energy, a local energy company, are working together to offer $10,000 incentives on the all-electric 2017 Nissan Leaf. In addition, the government offers electric vehicle incentives up to $7,500 to people who meet the qualifications outlined by the Federal Electric Vehicles Tax Credit. A standard 2017 Nissan Leaf costs around $30,000. As a result, Florida residents could purchase a new Leaf for less than half of the market value price.\nSan Diego's local private utility, San Diego Gas and Electric (SDG&E), offers its customers an electric vehicle incentive of $10,000 for a 2017 BMW i3.\n\nSonoma Clean Power, the public utility that serves both Sonoma and Mendocino, offers its customers EV incentives up to $2,000 on a Volkswagen e-Golf. In addition, Volkswagen offers an incentive of $7,000 towards the purchase of an e-Golf. On top of these local incentives, and the federal tax credit, California residents can receive state incentives up to $2,500 in the form of state rebates. Therefore, Sonoma Clean Power customers can potentially save up to $19,000 on an e-Wolf.\n\nIn March 2018, NPR reported that demand for electricity in the U.S. had begun to decline. The Tennessee Valley Authority projected a 13 percent drop in demand among the seven states it serves, which is \"the first persistent decline in the federally owned agency's 85-year history.\" To combat this, companies in the utility sector launched programs to get more involved in the electric car market. For example, utility companies began to invest in electric vehicle charging infrastructure and to team up with automobile manufacturers to offer rebates to people who purchase electric vehicles.\n\nFerdinand Dudenhoeffer, head of the Centre of Automotive Research at the Gelsenkirchen University of Applied Sciences in Germany, said that \"by 2025, all passenger cars sold in Europe will be electric or hybrid electric\".\n\nFirst, advances in lithium ion batteries, in large part driven by the consumer electronics industry, allow full-sized, highway-capable EVs to be propelled as far on a single charge as conventional cars go on a single tank of gasoline. Lithium batteries have been made safe, can be recharged in minutes instead of hours (see recharging time), and now last longer than the typical vehicle (see lifespan). The production cost of these lighter, higher-capacity lithium batteries is gradually decreasing as the technology matures and production volumes increase (see price history).\n\nToyota Motors Corporation is trying to replace the current lithium ion battery with solid-state battery technology by 2020. The solid-state battery replaces the liquid electrolyte with a solid electrolyte.\n\nRechargeable lithium-air batteries potentially offer increased range over other types and are a current topic of research.\n\nAnother improvement is to decouple the electric motor from the battery through electronic control, employing supercapacitors to buffer large but short power demands and regenerative braking energy. The development of new cell types combined with intelligent cell management improved both weak points mentioned above. The cell management involves not only monitoring the health of the cells but also a redundant cell configuration (one more cell than needed). With sophisticated switched wiring it is possible to condition one cell while the rest are on duty.\n\nSmall electric trucks have been used for decades for specific and/or limited uses.\n\nLarger electric trucks have been made in the 2010s, such as prototypes of electric Renault Midlum tested in real conditions and trucks by E-Force One and Emoss.\n\nRenault and Volvo will launch their first mass-produced electric trucks in early 2019.\n\nAnnounced in 2017, the Tesla Semi, is expected to hit production lines in 2019.\n\nParticularly in Europe, fuel-cell electric trains are gaining in popularity to replace Diesel-electric units. In Germany, several Länder have ordered Alstom Coradia iLINT trainsets, in service since 2018, with France also planning to order trainsets. The United Kingdom, the Netherlands, Denmark, Norway, Italy, Canada and Mexico are equally interested. In France, the SNCF plans to replace all its remaining Diesel-electric trains with hydrogen trains by 2035. In the United Kingdom, Alstom announced in 2018 their plan to retrofit British Rail Class 321 trainsets with fuel cells.\n\n\n"}
{"id": "55406508", "url": "https://en.wikipedia.org/wiki?curid=55406508", "title": "Embedded cluster", "text": "Embedded cluster\n\nEmbedded stellar clusters, or simply embedded clusters (EC), are open clusters that are still surrounded by their progenitor molecular cloud.\nThey are often areas of active star formation, giving rise to stellar objects that have similar ages and compositions.\nBecause of the dense material that surrounds the stars, they appear obscured in visible light but can be observed using other sections of the electromagnetic spectrum, such as the near-infrared and X-rays that can see through the cloud material. In our Galaxy, embedded clusters can mostly be found within the Galactic disk or near the Galactic center where most of the star-formation activity is happening. \n\nThe sizes of stellar objects born in embedded clusters are distributed according to initial mass function, with many low-mass stars formed for every high-mass star. Nevertheless, the high-mass stars of temperature class O and B, which are significantly hotter and more luminous than the low-mass stars, have a disproportionate effect on their interstellar environment by ionizing the gas surrounding them creating H II regions. Many ultra-compact H II regions, the precursors to massive protostars, are associated with embedded clusters.\n\nOver time, radiation pressure from the stellar objects will disperse the molecular cloud and give rise to the better known open cluster.\n\nSeveral famous embedded clusters include the Trapezium cluster in the Orion Nebula, L1688 in the Rho Ophiuchi cloud complex, NGC 2244 in the Rosette Nebula, the cluster in the Trifid Nebula, NGC 6611 in the Eagle Nebula, and Trumpler 14, 15, and 16 in the Carina Nebula\n"}
{"id": "5970169", "url": "https://en.wikipedia.org/wiki?curid=5970169", "title": "Fallout Protection", "text": "Fallout Protection\n\nFallout Protection: What To Know And Do About Nuclear Attack was an official United States federal government booklet released in December 1961 by the United States Department of Defense and the Office of Civil Defense.\nThe first page of the book is a note from then-U.S. Secretary of Defense Robert McNamara explaining that the booklet is the result of the first task he was given when he assumed responsibility for the Federal Civil Defense Program in August 1961. The task, assigned by President John F. Kennedy, was to \"give the American people the facts they need to know about the dangers of a thermonuclear attack and what they can do to protect themselves.\"\n\nMuch more straightforward about the dangers of atomic weapons than \"Survival Under Atomic Attack\", and published in the era of the hydrogen bomb and the ICBM, the book first explains general information and hazards of nuclear weapons, fallout and radiation. Second, it covers community fallout shelters, improvised fallout shelters and supplies that one ought to have ready in case of a nuclear attack. There is a brief section outlining \"emergency housekeeping\" which covers water and food conservation and first aid for the time spent in the shelter. Last, the booklet mentions recovery and how to clean up the fallout.\n\n\n"}
{"id": "33028270", "url": "https://en.wikipedia.org/wiki?curid=33028270", "title": "Flapper valve", "text": "Flapper valve\n\nA flapper valve is a type of valve used in flush toilets. It creates the watertight seal in the toilet tank that holds the water until flushing. When one pushes the handle, the flapper raises, allowing the water to flush out the excreted items, the flapper then goes back down and recreates the watertight seal.\n"}
{"id": "895409", "url": "https://en.wikipedia.org/wiki?curid=895409", "title": "Ford F-Series", "text": "Ford F-Series\n\nThe Ford F-Series is a series of light-duty trucks and medium-duty trucks (Class 2-7) that have been marketed and manufactured by Ford Motor Company since 1948. While most variants of the F-Series trucks are full-size pickup trucks, the F-Series also includes chassis cab trucks and commercial vehicles. The Ford F-Series has been the best-selling vehicle in the United States since 1986 and the best-selling pickup since 1977. It is also the best selling vehicle in Canada. As of the 2018 model year, the F-Series generates $ billion in annual revenue for Ford, making the F-Series brand more valuable than Coca-Cola and Nike.\n\nIn 1999, to bridge the gap between the pickup line and the medium-duty trucks, the F-250 and F-350 became the Ford Super Duty vehicles; considered an expansion of the F-Series, the Super Duty trucks are built on a distinct chassis with heavier-duty components. As of the 2017 model year, the F-Series includes the F-150, the Super Duty (F-250 through F-550), and F-650 and F-750 Super Duty medium-duty commercial trucks. The most popular version of the F-Series is the F-150, now in its thirteenth generation.\n\nThrough the use of rebadging, Ford has marketed the F-Series through all three Ford divisions in North America. From 1946 to 1968, Mercury sold the Mercury M-Series in Canada; during the 2000s, Lincoln sold the Lincoln Blackwood, replaced by the Lincoln Mark LT.\n\nThe first-generation F-Series pickup (known as the Ford Bonus-Built) was introduced in 1948 as a replacement for the previous car-based pickup line introduced in 1942. The F-Series was sold in eight different weight ratings, with pickup, panel truck, cab-over engine (COE), conventional truck, and school bus chassis body styles.\n\nFor the 1953 model year, Ford introduced a second generation of the F-Series trucks. Increased dimensions, improved engines, and an updated chassis were features of the second generation. In another change, the model nomenclature of the F-Series was expanded to three numbers; this remains in use in the present day. The half-ton F-1 became the F-100 (partially influenced by the North American F-100 Super Sabre); the F-2 and F-3 were combined into the ¾-ton F-250 while the F-4 became the one-ton F-350. Conventional F-Series trucks were F-500 to F-900; COE chassis were renamed C-Series trucks.\n\nWhile large body parts are the same from 1953-1956 F-100 & F-250s (namely the hood, fenders, and all bed components), in 1956 the cab underwent a major revision. Centered around a wraparound windshield, the cab was given new doors, a redesigned dashboard, and an (optional) panoramic rear window). In line with Ford cars, the 1956 F-Series offered seat belts as an option. \n\nIntroduced in 1957, the third generation F-series was a significant modernization and redesign. Front fenders became integrated into the body, and the new Styleside bed continued the smooth lines to the rear of the pickup.\n\nThe cab-over F-Series was discontinued, having been replaced by the tilt-cab C-Series.\n\nIn 1959, Ford began in-house production of four-wheel-drive pickups.\n\nFord introduced a dramatically new style of pickup in 1961 with the fourth generation F-Series. Longer and lower than its predecessors, these trucks had increased dimensions and new engine and gearbox choices. Additionally, the 1961–1963 models were constructed as a unibody design with the cab and bed integrated. This proved unpopular and Ford reverted to the traditional separate cab/bed design in 1964.\n\nIn 1965, the F-Series was given a significant mid-cycle redesign. A completely new platform, including the \"Twin I-Beam\" front suspension, was introduced that would be used until 1996 on the F-150 and until 2016 on the F-250/350 4x2. Additionally that year, the Ranger name made its first appearance on a Ford pickup; previously a base model of the Edsel, it was now used to denote a high-level styling package for F-Series pickups.\n\nIntroduced in 1967, the fifth generation F-series pickup was built on the same platform as the 1965 revision of the fourth generation. Dimensions and greenhouse glass were increased, engine options expanded, and plusher trim levels became available during the fifth generation's production run.\n\nSuspension components from all 1969 F-Series models are completely interchangeable.\n\nPerhaps the most rare of this series is the Heavy Duty Special, often confused with the Camper Special. However, the Heavy Duty Special is a 1/2 T with upgraded heavy duty parts such as Firestone overload rear leaf mounts, larger brake drums and axles. \n\nA variant of the fifth generation F-series was produced until 1992 in Brazil for the South American market. \n\nThe sixth generation F-series was introduced in 1973. This version of the F-series continued to be built on the 1965 fourth generation's revised platform, but with significant modernization and refinements. Front disc brakes, increased cabin dimensions, gas tank relocated outside the cab and under the bed, significantly improved heating and air conditioning, full double wall bed construction, increased use of galvanized steel, Super-cab was introduced in the sixth generation pickup.\n\nThe FE engine series was discontinued in 1976 after a nearly 20-year run, replaced by the more modern 351 series (Modified) and 400 series engines.\n\nIn 1975, the F-150 was introduced in between the F-100 and the F-250 in order to avoid certain emission control restrictions. For 1978, square headlights replaced the previous models' round ones on higher trim package models, such as Lariat and Ranger, and in 1979 became standard equipment. Also for 1978, the Ford Bronco was redesigned into a variant of the F-series pickup. 1979 was the last year that the 460 big block engine was available in a half ton truck.\n\nThe 1980 F-Series was redesigned with an all-new chassis and larger body; this was the first ground-up redesign since 1965. The exterior styling of the truck was redone to improve aerodynamics and fuel economy. Medium-duty F-Series (F600-F900) were also redesigned; although they shared the cabin of the smaller pickup trucks, the largest version of F-Series now wore a bonnet with separate front wings (like the L-Series). Medium duty Ford F-Series would carry the 1980–1986 interior design until 2000 (though pickups were restyled again in 1987 and 1992), with very subtle changes such as window glass and electronics.\n\nIn a move towards fuel efficiency, Ford dropped the M-Series engines (the 5.8 Liter \"351M\" and 6.6 Liter 400 cu in V8s) in 1981, replacing them with the 4.2 Liter 255 C.I. and 5.8 Liter 351 C.I. Windsor V8 engines from the Panther platform. The 255 V8 was simply a 5.0L, 302 V8 with a smaller bore, built specifically for better fuel economy, but was dropped for the 1982 model year due to being underpowered and having limited demand. For 1982 and 1983, the 3.8L, 232 C.I. Essex V6 was the base engine but was quickly dropped for the 1984 model year. In 1983, Ford added Diesel power to the F-Series through a partnership with International Harvester (later Navistar). The 6.9L, 420 C.I. IDI V8 produced similar power output as the gasoline 351 Windsor V8, with the fuel economy of the 4.9L, 300 I6. From this point on (1983-present), the heavier duty f-series trucks (F-350 and above) were usually equipped with the Diesel engines as standard horsepower. 1985 was the first year of electronic fuel injection on the 5.0L V8, all other engines following suit in 1988. There was a new \"high output\" version of the 5.8L Windsor beginning in 1984.\n\nA noticeable change was made to the F-Series in 1982 as the Ford \"Blue Oval\" was added to the center of the grill, also the Ranger and Custom trims were no longer available. The Ranger name had been shifted onto the all-new compact pickup developed as a replacement for the Courier. The new trim levels were a no-badge base model (essentially the new Custom), XL, a very rare XLS, and XLT Lariat.\n\n1983 marked the final year of the F-100, making the F-150 the lightest pickup available on the market. F-100s and F-150s were virtually identical with the exception of smaller brakes. F-100s over a certain GVWR and/or without power brakes did use the F-150 axles. Also, the 1980–1983 F-100 was never offered with four-wheel-drive.\n\n1986 marked the final year that the F-150 was available with a 3-speed manual gearbox that shifted via a steering column lever (3-on-the-tree). Incidentally, this was the second-last vehicle in the United States that offered this set up. 1986 was also the last year the Explorer package was available.\n\nThis is the first generation of trucks to incorporate amenities such as power mirrors, power windows, and power door locks.\n\nThe 1987 F-Series carried over the same body style from the seventh generation, yet sported a new rounded front clip that improved aerodynamics, as well as the softening of body lines around the rear of the bed and fender arches around the wheel wells. The interior was also completely redesigned in 1987. The transmissions available on SuperCab models were a four-speed or five-speed manual; regular cab models were also available with automatic transmission. The Custom trim made a comeback for the eighth generation. In 1988, the 4.9 Liter inline-six, 5.8 Liter V8, and 7.5 Liter V8 engines gained electronic fuel injection. International Navistar also increased the displacement of their 6.9 Liter V8 in the same year, resulting in the new 7.3 Liter unit. This was also the first year of a five-speed manual overdrive transmission, which included the Mazda \"M5OD\" in the F-150s and the heavy-duty ZF5 in the F-250s and F-350s. Four-speed manuals were discontinued as standard equipment after 1987, but were available as a customer-ordered option until 1989. In 1989, the C6 three-speed automatic was replaced as the base automatic transmission with the \"E4OD\", a four-speed electronically controlled automatic overdrive unit, though the C6 was still available as an option, mostly in F-250s and F-350s, until 1997. Heavy-Duty models included F-250s and F-350s (along with F-Super Dutys) that were classified as incomplete vehicles only that were produced with no bed, but appeared as tow trucks, box trucks (notably U-Haul), flatbed trucks, dump trucks and other models. Although the F-250s, F-250 HDs (heavy duty), and F-350s were built as a Chassis cab models only from 1987 to 1997, owners can convert the models to pickup trucks. However General Pacheco, Argentina and São Bernardo do Campo, Brazil assemblies were the only places F-250, F-250 HD, and F-350 were built and sold as pickup trucks models. Despite being produced only in General Pacheco, Argentina and São Bernardo do Campo, Brazil assemblies (located in South America) F-250s, F-250 HDs, and F-350s were shipped from South America to the United States as imports from 1987 to 1997.\n\nFor the 1992 model year, the body-shell of the 1980 F-Series was given another major upgrade. To further improve its aerodynamics, the forward bodywork was given a slightly lower hoodline, rounding the front fenders and grille; in addition, the changes matched the F-Series with the design of the newly introduced Explorer and redesigned E-Series and Ranger. Along with the exterior updates, the interior received a complete redesign.\n\nDormant since 1987, the FlareSide bed made its return as an option for 1992. To increase its appeal for younger buyers, the bed bodywork was redesigned, borrowing the sides of the F-350 dual-rear wheel bed (fitted to the narrower F-150 single-wheel chassis). To commemorate the 75th anniversary of the first Ford factory-produced truck (the 1917 Ford Model TT), Ford offered a 75th anniversary package on its 1992 F-series, consisting of a stripe package, an argent colored step bumper, and special 75th anniversary logos. In 1993, the 240 hp SVT Lightning was introduced as a specially-tuned performance truck; over 11,000 were built from 1993 to 1995.\n\nIn a trim shift, the XLT Lariat was dropped and combined with the XLT; the XL took over for the Custom trim after 1993 (marking the last usage of the Custom nameplate by Ford). In 1995, the Eddie Bauer trim made its return.\n\nIn 1995, the medium-duty Ford F-Series (F-600 through F-800 and all Ford B-Series) were given their first update since 1980. All versions (except severe-service) received a hood redesign which enlarged the grille and moved the turn signal indicators to the outside of the front headlights.\n\nDuring the second half of 1997, the F-250 HD (heavy duty) was in the same series as the F-350. The body style stayed the same until the end of 1997. The only change of the F-250 HD and F-350 was that they no longer has classic style tailgates, with the stainless steel cap on them which read FORD. Instead the tailgate had the F-250 Heavy Duty/F-350 nameplate labeled on its left side, while Ford's blue oval was labeled on The right side (similarly styled like the 1997 Ford F-150 around this same time). The F-250 HD also had some minor changes in trim location, and options available. The 1997 F-250 HD with the 7.3 Powerstroke Diesel is also the rarest and most desirable of ninth generation Fords.\n\nFollowing the introduction of the tenth-generation F-150, the F-250 and F-350 continued into production into the 1998 model year, becoming part of the Ford F-Series Super Duty line as they were replaced in 1999.\n\nFor 1997, Ford Motor Company made a major change to its F-Series family of trucks as the F-Series pickup line was essentially split in two. During the 1970s, 1980s, and 1990s, buyers of pickup trucks had increasingly purchased the vehicles for the purposes of personal use over work use. To further increase its growing market share, Ford sought to develop vehicles for both types of buyers. In its ultimate decision, the company decided to make the F-150 as a contemporary vehicle for personal use, while the F-250 and F-350 would be designed more conservatively for work-based customers.\n\nIntroduced in early 1996 as a 1997 model, the F-150 was a completely new vehicle in comparison from its predecessor. Similar to the original Ford Taurus of 1986, the 1997 F-150 was developed with a radically streamlined body. Dispensing with the traditional Twin I-Beam for a fully independent front suspension, the all-new chassis only shared the transmissions with the previous generation. In a major change, the long-running 4.9L inline-6 was replaced by a 4.2 L Essex V6 engine as standard. The 4.6 L Modular V8 shared with the Panther Car was optional (with a bigger 5.4 L Triton V8 version added in mid-1997). Originally developed for use in the F-Series, the Modular/Triton V8 was the first overhead-camshaft engine ever to be installed in a full-size pickup truck.\n\nTo improve rear-seat access for SuperCab models, a rear-hinged (curb-side) door was added to all versions. Following its popularity, the SuperCab gained a fourth door for 1999. In 2001, the F-150 became the first pickup truck in its size segment to become available with four full-size doors. Sharing the length of a standard-bed SuperCab, the F-150 SuperCrew was produced with a slightly shortened bed.\n\nDuring the second half of the 1997 model year, Ford introduced a heavier GVWR version (8800 GVW), bearing the F-250 name. It was distinguished by an uncommon and unique seven-lug bolt pattern for the wheels. At the same time, the F-250 HD (heavy duty) was in the same series as the F-350 (same square body style as the last generation F-150; it was still built in South America only). Due the Super Duty trucks never reaching Ford dealerships as for sale until the beginning of 1999, Ford produced a lighter duty F-250, using the same body the F-150 had at the time as a temporarily model (1997 to 1999). By February 1999, the f-150-based F-250 was discontinued and the Super Duty trucks finally marketed for sale.\n\nIn 1999, the SVT Lightning made its return, with output expanded to 360 hp; over 28,000 were produced from 1999 to 2003. For 2002, Lincoln-Mercury gained its first full-size pickup truck since 1968 with the introduction of the Lincoln Blackwood, the first Lincoln pickup. Sharing the front bodywork of the Lincoln Navigator SUV and the same cab body work as the Ford F-150, the Blackwood was designed with a model-exclusive bed and was sold only in black. Due to very poor sales, the Blackwood was discontinued after 2002.\n\nIn 1999, the F-250 and F-350 pickups were introduced as the 1999 Ford F-Series Super Duty model line. While remaining part of the F-Series, the Super Duty trucks use a different platform architecture, powertrain, and design language, primarily as they are intended for heavy-duty work use. Designed in a joint venture with Navistar International, the medium-duty F-650/F-750 Super Duty were introduced in 2000.\n\nFor the 2004 model year, the F-150 was redesigned on an all-new platform. Externally, the eleventh-generation model was different from its predecessor, with sharper-edged styling; a major change was the adoption of the stepped driver's window from the Super Duty trucks. Regardless of cab type, all F-150s were given four doors, with the rear doors on the regular cab providing access to behind-the-seat storage. Ford also introduced the Triton engines in the variants of the F-150. Due to declining sales, the Flareside beds were dropped; this marked the first time in 56 years of F-Series production that Flareside beds were not available.\n\nFrom 2005 to 2008, Lincoln-Mercury dealers sold this version of the F-150 as the Lincoln Mark LT, replacing the Blackwood.\n\nIn late 2007 for the 2008 model year, the Super Duty trucks were given an all-new platform. While using the same bed and cabin as before, these are distinguished from their predecessors by an all-new interior and a much larger grille and head lamps. Previously available only as a chassis-cab model, the F-450 now was available as a pickup directly from Ford.\nFrom 2004 to 2006 offered a different bed style as well.\n\nThe twelfth generation F-150 was introduced for the 2009 model year as an update of the Ford full-size truck platform. Similar to its predecessor, these trucks are distinguished by their Super Duty-style grilles and head lamps; standard cab models again have two-doors instead of four. The FlareSide bed was continued until 2010, dropped along with the manual gearbox; outside of Mexico, the Lincoln Mark LT was replaced by the F-150 Platinum. A new model for 2010 included the SVT Raptor, a dedicated off-road pickup.\n\nAs part of a major focus on fuel economy, the entire engine lineup for the F-150 was updated for the 2011 model year. Along with two new V8 engines, the F-150 gained a new 3.7 Liter base V6 engine, and a powerful twin-turbocharged 3.5 Liter V6, dubbed EcoBoost by Ford. An automatic transmission is the only option. Other modifications include the addition of a Nexteer Automotive Electric Power Steering (EPS) system on most models.\n\nA recent study conducted by iSeeCars.com and published on the Ford Motor Company website listed Ford F-250 Super Duty as the top longest-lasting vehicle and Expedition, Explorer and F-150 in the top 20 longest-lasting vehicle.\n\nThe thirteenth-generation Ford F-Series was introduced for the 2015 model year. Largely previewed by the Ford Atlas concept vehicle at the 2013 Detroit Auto Show, the new design marked several extensive changes to the F-Series design. In the interest of fuel economy, Ford designers reduced curb weight of the F-150 by nearly 750 pounds, without physically altering its exterior footprint. To allow for such a massive weight reduction, nearly every body panel was switched from steel to aluminum (with the exception of the firewall); the frame itself remains high-strength steel. To prove the durability of the aluminum-intensive design, during the development of the thirteenth-generation F-Series, Ford entered camouflaged prototypes into the Baja 1000 endurance race (where the vehicles finished). The 2015 F-150 was the first pickup truck with adaptive cruise control, which uses radar sensors on the front of the vehicle to maintain a set following distance between it and the vehicle ahead of it, decreasing speed if necessary.\n\nThe 3.7L V6 was dropped, replaced by a 3.5L V6 as the standard engine, with a 2.7L EcoBoost V6 added alongside the 3.5L EcoBoost V6. While the 6.2L V8 was withdrawn, the 5.0L V8 continued as an option, with a 6-speed automatic as the sole transmission. \n\nFor the 2018 model year, the Ford F-150 underwent a mid-cycle redesign, being revealed at the 2017 New York International Auto Show. Following the introduction of the 2017 Super Duty model line, the F-Series (F-150 through F-550 and Ford Raptor) are again manufactured using a common cab (for the first time since 1996). For 2018, the F-150 shifted from the long-running 3-bar design used on Ford trucks to the \"center bar\" designs that debuted on the 2017 Super Duty model line. The powertrain underwent several revisions, as the 3.5L V6 was replaced by a 3.3L V6 mated to a 6 speed transmission. The EcoBoost V6 engines and 5.0L V8 engines were fitted with a 10-speed automatic (from the Raptor) and stop-start capability (previously only from the 2.7L EcoBoost). In 2018, a PowerStroke diesel engine was fitted to the F-150 for the first time, as Ford introduced a 250 hp 3.0L twin-turbocharged V6 (from the \"Lion\" lineup of engines shared by PSA Peugeot Citroën and Jaguar Land Rover).\n\nSafety and driver assistance features improved and added for the 2018 model year include Pre-Collision Assist with Pedestrian Detection and Adaptive Cruise Control with Stop and Go.\n\nThe crew-cab version of the 2018 F-150 reclaimed an IIHS Top Safety Pick rating. \n\nFord F-150's advertisement called \"Big Dog\" takes subtle shots at the redesigned Chevy Silverado and Ram 1500.\n\nThroughout its production, variants of the Ford F-Series has been produced to attract buyers. While these variants primarily consist of trim packages, others are high-performance versions while other variants were designed with various means of improving functionality.\n\nFor 1961 into part of the 1963 model year, the Ford F-Series was offered with a third body configuration, integrating the Styleside bed with the cab. With the pickup bed stampings welded directly to the cab before both assemblies were mounted to the frame, the design simplified the assembly and paint process (the configuration was similar to that of the Ford Ranchero). Following a poor market reception, the unibody pickup bed design was withdrawn during the 1963 model year.\n\nFrom 1961 to 1979, Ford offered several \"Special\" option packages for the F-Series, typically designed for owners with specific uses for their vehicles. For 1961, the \"Camper Special\" option package was introduced; designed for owners of slide-in truck campers, the option package featured pre-wiring for the camper, heavy-duty transmission and engine cooling, and a larger alternator. For 1968, Ford introduced the \"Contractor's Special\", and \"Farm and Ranch Special\", which featured toolboxes and heavier-duty suspension. The \"Explorer Special\" was introduced as a lower-priced variant of the Ranger trim. The \"Trailer Special\" was offered as well with trailer brake, heavy-duty radiator, transmission cooler, and tow hitch.\n\nIn 1980, the Special option packages were withdrawn as part of the F-Series redesign, while a number of features continued as stand-alone options; the Explorer continued as a variant of the Ranger trim through the 1986 model year.\n\nSold from 1991 to 1992 on the Ford F-150 XLT Lariat, the Nite special edition was an monochromatic option package, featuring black paint and trim with a multicolor accent stripe. For 1991, it was exclusive to the regular-cab F-150; for 1992, it was available on all bodystyles of the F-150 and introduced on the Ford Bronco.\n\nThe Nite edition was available with two-wheel drive or four-wheel drive with either the 5.0L or 5.8L V8; it also included a sport suspension and alloy wheels on 235/75R15 white-letter tires.\n\nFor 1994, Ford introduced the Eddie Bauer trim level for the F-150. In a fashion similar to the same trim packages on the Aerostar, Bronco, and Explorer/Bronco II, it consisted of outdoors-themed interior trim with two-tone exterior paint.\n\nIntroduced as a 1993 model, the Ford SVT Lightning is a high-performance version of the F-150 that was produced by the Ford Special Vehicle Team (SVT). Intended as a competitor for the Chevrolet 454SS, the SVT Lightning was derived from the F-150; to improve its handling, extensive modifications were made to the front and rear suspension and frame. Powered by a 240 hp version of the 5.8L V8, the Lightning used a heavy-duty 4-speed automatic transmission from the F-350 (normally paired with the 7.5L V8 or 7.3L diesel V8). While slower in acceleration than the GMC Syclone, the Lightning retained nearly all of the towing and payload capacity of a standard Ford F-150. Produced from 1993 to 1995, the first-generation SVT Lightning was withdrawn as Ford readied the 1997 Ford F-150 for sale. \n\nAfter a three-year hiatus, Ford released a second generation of the SVT Lightning for the 1999 model year. In line with its 1993-1995 predecessor, the second-generation Lightning was based on the F-150 with a number of suspension modifications; in a design change, all examples were produced with a FlareSide bed. In place of a model-specific engine, the second-generation was powered by a supercharged version of the 5.4L V8 from the F-150, producing 360 hp (increased to 380 hp in 2001). As before, the higher-output engine was paired with a heavier-duty transmission from the F-350 pickup. \n\nFor the 2004 redesign of the Ford F-150, the SVT Lightning was not included, leaving 2004 as the final year for the model line. While of an entirely different design focus from the SVT Lightning, the SVT/Ford Raptor is the succeeding generation of high-performance Ford F-Series pickup trucks.\n\nFrom 2000 to 2011, the Harley-Davidson Edition was an option package available on the F-150. Primarily an appearance package featuring monochromatic black trim, from 2002 to 2003, the edition included a slightly detuned version of the supercharged 5.4L V8 engine from the SVT Lightning. In 2003, a 100th Anniversary Edition was produced for F-150 SuperCrew trucks. For 2004, the Harley-Davidson option package became available for F-250/F-350 Super Duty trucks. After 2008, the option package adopted many of the options featured from the Platinum trim level, featuring leather seating produced from materials reserved for Harley-Davidson biker jackets. \n\nFor 2012, the Harley-Davidson Edition was replaced by the Limited trim level, retaining a monochromatic exterior appearance (shifting past motorcycle-themed trim). \n\nFor 2010, Ford introduced its second high-performance truck, the SVT Raptor. In contrast to the enhanced on-road performance of the SVT Lightning, the SVT Raptor is a focused towards off-road use, in line with that of a Baja 1000 racing truck. While a road-legal vehicle, many design modifications of the Raptor were made to improve its off-road capability, with the vehicle featuring a model-exclusive suspension with long-travel springs and shocks. The Raptor shares only its cab with a standard F-150; to accommodate its larger tires, the Raptor is fitted with wider front fenders, hood, and pickup bed. Initially produced as a SuperCab, a Raptor SuperCrew was introduced late in the 2010 model year. For the first time on a Ford vehicle in North America since 1983, the word \"Ford\" was spelled across the grille of the SVT Raptor in place of the Ford Blue Oval badge.\n\nFor 2010, the SVT Raptor was powered by a 310 hp 5.4L V8; a 411 hp 6.2L V8 (from the F-150 Platinum and Super Duty trucks) became optional, replacing the 5.4L V8 for 2011. A six-speed automatic is the sole transmission paired with both engines. \n\nAfter a two-year hiatus, the second-generation Ford Raptor (the SVT prefix was removed) was introduced for the 2017 model year. Derived from the thirteenth-generation F-Series, the Ford Raptor shifted to an aluminum body. Again produced as a high-performance off-road vehicle, the Raptor is produced in SuperCab and SuperCrew configurations, with long-travel suspension specific to the vehicle. As a design theme, the second-generation Raptor does not carry a Ford Blue Oval grille badge, instead spelling out \"Ford\" across the grille. \n\nTo improve fuel economy and reduce weight, the 6.2L V8 was replaced by a 450 hp 3.5L twin-turbocharged V6, paired with a 10-speed automatic transmission. \n\nIntroduced for 2009, Platinum is a luxury-oriented trim of the Ford F-150. Effectively replacing the Lincoln Mark LT in the United States and Canada (though its production continued through 2014 in Mexico), the Platinum adopted many of the luxury features and content from the Mark LT with more subdued exterior styling (the Platinum was fitted with an eggcrate grille similar to early models of the Ford Expedition). \n\nIn 2013, Ford began use of the Platinum trim for Super Duty trucks, from the F-250 to the F-450 pickup trucks. Until 2016, the Platinum trim was an add-on package to a Super Duty that was ordered as a Lariat. 2017 saw the Platinum become a separate trim level. \n\nFor the 2014 model year, Ford introduced the Tremor model of the F-150. The Tremor was released as a high-performance sport truck for street truck enthusiasts. The regular-cab Tremor is based on the style of the FX Appearance Package with the 3.5 Liter EcoBoost engine and a 4.10 rear axle. The interior uses a console-mounted shifter, custom bucket seats and a flow-through center console not found in any other F-150. The Tremor is available in both 4x2 and 4x4. Both options feature an electronic locking rear differential and customized suspension. There were 2,230 Tremors built.\n\nThe truck won the San Felipe 250 eight times between 1999 and 2007.\n\nGreg Biffle won the 2000 NASCAR Craftsman Truck Series.\n\nDrivers such as Roger Norman and Larry Roeseler won the Primm 300 in 2003, 2007 and 2008.\n\nIn 2008, Ford announced its entrance into the Baja 1000 class-eight race for moderately modified, full-size pickups. The driver of record was Steve Oligos, supported by co-drivers Randy Merritt, Greg Foutz, and Bud Brutsman. The vehicle was built with collaboration between the Ford Special Vehicle Team (SVT), Ford Racing, and Foutz Motorsports, Inc. The Ford F-150 SVT Raptor R completed the 2008 41st Tecate SCORE Baja 1000 race in 25.28:10, and ranked third in its class. Tavo Vildosola and Gus Vildosola won the event in 2010.\n\nIn the Best in the Desert race series, an F-150 SVT Raptor R completed the \"Terrible's 250\" race, placing second overall in the class 8000.\n\nIn January 2010, a single Raptor SVT (No. 439), driven by Chilean driver Javier Campillay, competed in the Argentina-Chile Dakar Rally. However, the pickup was unable to finish due to a catch-up crash with another car in the middle of the road during stage seven. In January 2011, two Raptors started in the Argentina-Chile Dakar Rally in Buenos Aires, with Campillay driving the more reliable Raptor (No. 375), and American female driver Sue Mead driving a T2 Raptor (No. 374). Mead crossed the finish line in Buenos Aires and won the \"super production\" class, the first North American class win in Dakar history. Campillay was unable to finish the 12th stage after losing time due to mechanical failure during the 11th stage, which led to his disqualification for failing to reach the race camp by the designated deadline.\n\nThe Ford F-150 has won numerous awards; in 2009 alone, it received:\n\nQuantities of Ford F-Series trucks sold\nSales in 2018 are on track to set new records, with first-half sales at 4.2% more than the first half of the record-setting year 2004.\n\nFor most of its production, the F-Series was sold in a medium-duty conventional truck configuration alongside the traditional pickup trucks. Beginning in 1948 with the 1½ ton F-5 (later F-500), the medium-duty trucks ranged up to the F-8 (F-800). Prior to the 1957 introduction of the Ford C-Series tilt-cab, the medium-duty range was offered as both a conventional and in a COE (cabover) configuration.\n\nFollowing the introduction of the fifth-generation F-Series in 1967, the medium-duty trucks were designed separately from the pickup truck range. Although remaining part of the F-Series range, the medium-duty trucks shared only the cab and interior with the F-Series pickup trucks. Since 1967, the cab design has changed only in 1980 and in 2000. Redesigned on an all-new chassis, the 2017 F-Series medium-duty trucks retain an updated version of the 2000-2016 F-650/F750 cab.\n\nThe medium-duty F-Series served as the donor platform for the B-Series cowled bus chassis produced from 1948 to 1998. Produced primarily for school bus bodies, the B-Series was discontinued as part of the sale of the Ford heavy-truck line to Freightliner in 1996.\n\nAbove its medium-duty truck ranges, the Ford F-Series was used as a heavy-truck chassis during its early production. In 1951, Ford debuted its \"Big Job\" line, denoting the F-8 conventional. In 1958, the \"Super Duty\" and \"Extra Heavy Duty\" replaced the Big Job trucks, marking the debut of the Super Duty V8 engine line. In 1963, the N-Series became the first short-hood conventional built by Ford, replacing the F-900 Super Duty/Extra Heavy Duty. Although based on an all-new chassis and separate bodywork, the cab was sourced from the F-Series.\n\nIn 1970, Ford introduced the L-Series \"Louisville\" line of conventional trucks, moving all heavy truck development away from the F-Series. The L-Series/Aeromax would remain in production through 1998, as Ford exited the heavy-truck segment.\n\nFrom 1948 until 1960, Ford manufactured the F-Series in a panel van configuration, largely replacing it with the Econoline in 1961. In contrast to General Motors, the panel van was never produced in a passenger \"carryall\" variant, such as the Chevrolet/GMC Suburban.\n\nFor much of its production life, the Ford E-Series vans shared a high degree of mechanical commonality with the F-Series; in the late 1970s, some body components were shared. Since the 1990s, this changed somewhat, as Ford updated the F-Series several times since 1992 while the E-Series continued on with very few changes; no powertrains were shared as of 2016.\n\nAs of 2018, outside of the United States, Canada, and Mexico, the Ford F-150 is officially sold in most Caribbean countries (except Trinidad and Tobago, Saint Kitts and Nevis and Cuba), Suriname, Ecuador, Peru, Chile, the Middle East (including Afghanistan), Iceland, Angola, Burkina Faso, Cameroon, Cape Verde, Democratic Republic of Congo, Ethiopia, Gabon, Ghana, Ivory Coast, Liberia, Nigeria, Senegal, Sierra Leone, Madagascar, the Dutch territories of Aruba, Curaçao, Saint Maarten and the British overseas territory of the Cayman Islands. The SVT Raptor is sold in the United States, Canada, Mexico, the Middle East (including Afghanistan), China, Ecuador, Chile and Peru. Both are available in LHD only.\n\nIn Mexico, the F-150 (XLT and higher trim levels) is called the \"Ford Lobo\" while the F-150 SVT Raptor is called the \"Ford Lobo Raptor\". The F-150 XL is called F-150 XL.\n\nThere is a strong grey market presence of Ford F-Series trucks around the world, most notably in Europe, China, and Australia, and usually driven by wealthy car enthusiasts, as the higher end trim models are the most sought-after versions.\n\nIn Bolivia, Ford F-series truck are imported from the United States. F-150 single, super cab and crew cab are available with short and long bed. F-series Heavy Duty like F-250, F-350 are available in Super Cab and Crew cab with long bed, but the F-450 is available only in a chassis version. The F-150 Raptor is available, too.\n\nIn Australia, Ford F-series trucks are imported and converted to right-hand drive by several Australian importers, mostly by the Harrison Motoring Group, which as become the largest importer of F-Series vehicles in the Southern Hemisphere. Harrison F-Trucks have become Australia's number one converter and supplier of the famous Ford F-Series badges. \n\nIn the United Kingdom, most imported Ford F-Series trucks are the F-150 model, and usually the higher-end four door versions.\n\nRight-hand drive versions of the F-Series for the United Kingdom and Australia were manufactured in Brazil.\n\nIn Argentina and Brazil, the petrol engines are often converted to also run with alternative fuels, E-96h (Brazilian-spec ethanol) and compressed natural gas (CNG). Biodiesel also is used in Diesel engines.\n\nThe addition of the 3.0l V6 turbo diesel engine could potentially make the Ford F-150 go global and in both LHD and RHD from the factory.\n\n\n"}
{"id": "14463033", "url": "https://en.wikipedia.org/wiki?curid=14463033", "title": "Free surface", "text": "Free surface\n\nIn physics, a free surface is the surface of a fluid that is subject to zero parallel shear stress,\nsuch as the boundary between two homogeneous fluids,\nfor example liquid water and the air in the Earth's atmosphere. Unlike liquids, gases cannot form a free surface on their own.\nFluidized/liquified solids, including slurries, granular materials, and powders may form a free surface.\n\nA liquid in a gravitational field will form a free surface if unconfined from above.\nUnder mechanical equilibrium this free surface must be perpendicular to the forces acting on the liquid; if not there would be a force along the surface, and the liquid would flow in that direction. Thus, on the surface of the Earth, all free surfaces of liquids are horizontal unless disturbed (except near solids dipping into them, where surface tension distorts the surface in a region called the meniscus).\n\nIn a free liquid that is not affected by outside forces such as a gravitational field, internal attractive forces only play a role (e.g. Van der Waals forces, hydrogen bonds). Its free surface will assume the shape with the least surface area for its volume: a perfect sphere. Such behaviour can be expressed in terms of surface tension. It can be demonstrated experimentally by observing a large globule of oil placed below the surface of a mixture of water and alcohol having the same density so the oil has neutral buoyancy.\n\nIf the free surface of a liquid is disturbed, waves are produced on the surface. These waves are not elastic waves due to any elastic force; they are gravity waves caused by the force of gravity tending to bring the surface of the disturbed liquid back to its horizontal level. Momentum causes the wave to overshoot, thus oscillating and spreading the disturbance to the neighboring portions of the surface. The velocity of the surface waves varies as the square root of the wavelength if the liquid is deep; therefore long waves on the sea go faster than short ones. Very minute waves or ripples are not due to gravity but to capillary action, and have properties different from those of the longer ocean surface waves,\nbecause the surface is increased in area by the ripples and the capillary forces are in this case large compared with the gravitational forces.\nCapillary ripples are damped both by sub-surface viscosity and by surface rheology.\n\nIf a liquid is contained in a cylindrical vessel and is rotating around a vertical axis coinciding with the axis of the cylinder, the free surface will assume a parabolic surface of revolution known as a paraboloid. The free surface at each point is at a right angle to the force acting at it, which is the resultant of the force of gravity and the centrifugal force from the motion of each point in a circle. Since the main mirror in a telescope must be parabolic, this principle is used to create liquid mirror telescopes.\n\nConsider a cylindrical container filled with liquid rotating in the z direction in cylindrical coordinates, the equations of motion are:\n\nformula_1\n\nWhere formula_2 is the density of the fluid, formula_3 is the radius of the cylinder, formula_4 is the angular frequency and formula_5 is the gravitational acceleration. Taking a surface of constant pressure formula_6 the total differential becomes:\n\nformula_7\n\nIntegrating, the equation for the free surface becomes:\n\nformula_8\n\nWhere formula_9 is the distance of the free surface from the bottom of the container along the axis of rotation. If one integrates the volume of the paraboloid formed by the free surface and then solves for the original height, one can find the height of the fluid along the centerline of the cylindrical container:\n\nformula_10\n\nThe equation of the free surface at any distance formula_3 from the center becomes:\n\nformula_12\n\nIf a free liquid is rotating about an axis, the free surface will take the shape of an oblate spheroid: the approximate shape of the Earth due to its equatorial bulge.\n\n\n\n"}
{"id": "26653754", "url": "https://en.wikipedia.org/wiki?curid=26653754", "title": "Futtsu Power Station", "text": "Futtsu Power Station\n\nThe Futtsu Power Station (富津火力発電所) is the second largest gas-fired power station in the world, located in Futtsu, Japan. \nThe power station operates at by utilizing four groups of units, with 14 combined cycle units, 4 advanced combined cycle units, and 3 more advanced combined cycle units (high temperature combined cycle). All four units run on natural gas. The facility is owned by Tepco.\n\n"}
{"id": "29769950", "url": "https://en.wikipedia.org/wiki?curid=29769950", "title": "Gheorgheni Wind Farm", "text": "Gheorgheni Wind Farm\n\nThe Gheorgheni Wind Farm is a proposed wind power project in Gheorgheni, Harghita County, Romania. It will have 38 individual wind turbines with a nominal output of around 2.6 MW each which will deliver up to 100 MW of power, enough to power over 60,000 homes, with a capital investment required of approximately US$150 million.\n"}
{"id": "1424917", "url": "https://en.wikipedia.org/wiki?curid=1424917", "title": "Gridded ion thruster", "text": "Gridded ion thruster\n\nThe gridded ion thruster is a common design for ion thrusters, a highly efficient low-thrust spacecraft propulsion running on electrical power. These designs use high-voltage grid electrodes to accelerate ions with electrostatic forces.\n\nThe ion engine was first demonstrated by German-born NASA scientist Ernst Stuhlinger, and developed in practical form by Harold R. Kaufman at NASA Lewis (now Glenn) Research Center from 1957 to the early 1960s. \n\nThe use of ion propulsion systems were first demonstrated in space by the NASA Lewis \"Space Electric Rocket Test\" (SERT) I and II. These thrusters used mercury as the reaction mass. The first was SERT-1, launched July 20, 1964, which successfully proved that the technology operated as predicted in space. The second test, SERT-II, launched on February 3, 1970, verified the operation of two mercury ion engines for thousands of running hours. Despite the demonstration in the 1960s and 70s, though, they were rarely used before the late 1990s.\n\nNASA Glenn continued to develop electrostatic gridded ion thrusters through the 1980s, developing the NASA Solar Technology Application Readiness (NSTAR) engine, that was used successfully on the Deep Space 1 probe, the first mission to fly an interplanetary trajectory using electric propulsion as the primary propulsion. It is currently flying the Dawn asteroid mission. Hughes Aircraft Company (now L-3 ETI) has developed the XIPS (Xenon Ion Propulsion System) for performing station keeping on its geosynchronous satellites (more than 100 engines flying). NASA is currently working on a 20-50 kW electrostatic ion thruster called HiPEP which will have higher efficiency, specific impulse, and a longer lifetime than NSTAR. Aerojet has recently completed testing of a prototype NEXT ion thruster.\n\nBeginning in the 1970s, radio-frequency ion thrusters were developed at Giessen University and ArianeGroup. RIT-10 engines are flying on the EURECA and ARTEMIS. Qinetiq (UK) has developed the T5 and T6 engines (Kaufman type), used on the GOCE mission (T5) and possibly the BepiColombo mission (T6). From Japan, the µ10, using microwaves, flew on the Hayabusa mission.\n\nPropellant atoms are injected into the discharge chamber and are ionized by electron bombardment, forming a plasma. There are several ways of producing the energetic electrons for the discharge: electrons can be emitted from a hollow cathode and accelerated by potential difference with the anode; the electrons can be accelerated by an oscillating electric field induced by an alternating electromagnet, which results in a self-sustaining discharge and omits any cathode (radio frequency ion thruster); and microwave heating. The positively charged ions diffuse towards the chamber's extraction system (2 or 3 multi-aperture grids). After ions enter the plasma sheath at a grid hole, they are accelerated by the potential difference between the first and second grids (called the screen and accelerator grids, respectively). The ions are guided through the extraction holes by the powerful electric field. The final ion energy is determined by the potential of the plasma, which generally is slightly greater than the screen grids' voltage.\n\nThe negative voltage of the accelerator grid prevents electrons of the beam plasma outside the thruster from streaming back to the discharge plasma. This can fail due to insufficient negative potential in the grid, which is a common ending for ion thrusters' operational life. The expelled ions propel the spacecraft in the opposite direction, according to Newton's 3rd law.\nLower-energy electrons are emitted from a separate cathode, called the neutralizer, into the ion beam to ensure that equal amounts of positive and negative charge are ejected. Neutralizing is needed to prevent the spacecraft from gaining a net negative charge, which would attract ions back toward the spacecraft and cancel the thrust.\n\nThe ion optics are constantly bombarded by a small amount of secondary ions and erode or wear away, thus reducing engine efficiency and life. Ion engines need to be able to run efficiently and continuously for years. Several techniques were used to reduce erosion; most notable was switching to a different propellant. Mercury or caesium atoms were used as propellants during tests in the 1960s and 1970s, but these propellants adhered to, and eroded the grids. Xenon atoms, on the other hand, are far less corrosive, and became the propellant of choice for virtually all ion thruster types. NASA has demonstrated continuous operation of NSTAR engines for over 16,000 hours (1.8 years), and tests are still ongoing for double this lifetime. Electrostatic ion thrusters have also achieved a specific impulse of 30–100 kN·s/kg, better than most other ion thruster types. Electrostatic ion thrusters have accelerated ions to speeds reaching 100 km/s.\n\nIn January 2006, the European Space Agency, together with the Australian National University, announced successful testing of an improved electrostatic ion engine, the Dual-Stage 4-Grid (DS4G), that showed exhaust speeds of 210 km/s, reportedly four times higher than previously achieved, allowing for a specific impulse which is four times higher. Conventional electrostatic ion thrusters possess only two grids, one high voltage and one low voltage, which perform both the ion extraction and acceleration functions. However, when the charge differential between these grids reaches around 5 kV, some of the particles extracted from the chamber collide with the low voltage grid, eroding it and compromising the engine's longevity. This limitation is successfully bypassed when two pairs of grids are used. The first pair operates at high voltage, possessing a voltage differential of around 3 kV between them; this grid pair is responsible for extracting the charged propellant particles from the gas chamber. The second pair, operating at low voltage, provides the electrical field that accelerates the particles outwards, creating thrust. Other advantages to the new engine include a more compact design, allowing it to be scaled up to higher thrusts, and a narrower, less divergent exhaust plume of 3 degrees, which is reportedly five times narrower than previously achieved. This reduces the propellant needed to correct the orientation of the spacecraft due to small uncertainties in the thrust vector direction.\n\nThe largest difference in the many electrostatic ion thrusters is the method of ionizing the propellant atoms - electron bombardment (NSTAR, NEXT, T5, T6), radiofrequency (rf) excitation (RIT 10, RIT 22, µN-RIT), microwave excitation (µ10, µ20). Related to this is the need of a cathode and required effort for the power supplies. Kaufman type engines require at least supplies to the cathode, anode and chamber, whereas the rf and microwave types require an additional rf generator, but no anode and cathode supplies. \n\nIn the extraction grid systems minor differences occur in the grid geometry and the materials used, which may have implications for the grid system lifetime.\n\n\n"}
{"id": "24801112", "url": "https://en.wikipedia.org/wiki?curid=24801112", "title": "Harold C. Anderson", "text": "Harold C. Anderson\n\nHarold C. Anderson was an American accountant and wilderness activist. A prominent member of the Potomac Appalachian Trail Club (PATC) in the Washington D.C. area from its inception, he was also a co-founder of The Wilderness Society.\n\nBecause of his ties to the PATC, Anderson was well acquainted with Benton MacKaye, a forester who was the first to propose the Appalachian Trail. Anderson was especially concerned with the proposed building of skylines along the Appalachian crest, something that he shared with MacKaye, who was at the time butting heads with PATC president Myron Avery on the subject. In August 1934, Anderson wrote to MacKaye of his desire to originate the country's first society dedicated to the protection of wilderness: \"You and Bob Marshall have been preaching that those who love the primitive should get together and give a united expression of their views. That is what I would like to get started.\"\n\n"}
{"id": "9749127", "url": "https://en.wikipedia.org/wiki?curid=9749127", "title": "Heavy fermion material", "text": "Heavy fermion material\n\nIn solid-state physics, heavy fermion materials are a specific type of intermetallic compound, containing elements with 4f or 5f electrons in unfilled electron bands. Electrons are one type of fermion, and when they are found in such materials, they are sometimes referred to as heavy electrons. Heavy fermion materials have a low-temperature specific heat whose linear term is up to 1000 times larger than the value expected from the free-electron theory. The properties of the heavy fermion compounds often derive from the partly filled f-orbitals of rare-earth or actinide ions, which behave like localized magnetic moments. The name \"heavy fermion\" comes from the fact that the fermion behaves as if it has an effective mass greater than its rest mass. In the case of electrons, below a characteristic temperature (typically 10 K), the conduction electrons in these metallic compounds behave as if they had an effective mass up to 1000 times the free-electron mass. This large effective mass is also reflected in a large contribution to the resistivity from electron-electron scattering via the Kadowaki–Woods ratio. Heavy fermion behavior has been found in a broad variety of states including metallic, superconducting, insulating and magnetic states. Characteristic examples are CeCu, CeAl, CeCuSi, YbAl, UBe and UPt.\n\nHeavy fermion behavior was discovered by Andres, Graebner and Ott in 1975, who observed enormous magnitudes of the linear specific heat capacity in CeAl.\n\nWhile investigations on doped superconductors led to the conclusion that the existence of localized magnetic moments and superconductivity in one material was incompatible, the opposite was shown, when in 1979 Steglich \"et al.\" discovered heavy fermion superconductivity in the material CeCuSi.\n\nThe discovery of a quantum critical point and non fermi liquid behavior in the phase diagram of heavy fermion compounds by von Löhneysen \"et al.\" in 1994 led to a new rise of interest in the research of these compounds. Another experimental breakthrough was the demonstration (by the group of Lonzarich) that quantum criticality in heavy fermions can be the reason for unconventional superconductivity.\n\nHeavy fermion materials play an important role in current scientific research, acting as prototypical materials for unconventional superconductivity, non fermi liquid behavior and quantum criticality. The actual interaction between localized magnetic moments and conduction electrons in heavy fermion compounds is still not completely understood and a topic of ongoing investigation.\n\nHeavy fermion materials belong to the group of strongly correlated electron systems.\n\nSeveral members of the group of heavy fermion materials become superconducting below a critical temperature. The superconductivity is unconventional.\n\nAt high temperatures, heavy fermion compounds behave like normal metals and the electrons can be described as a Fermi gas, in which the electrons are assumed to be non-interacting fermions. In this case, the interaction between the \"f\" electrons, which present a local magnetic moment and the conduction electrons, can be neglected.\n\nThe Fermi liquid theory by Landau provides a good model to describe the properties of most heavy fermion materials at low temperatures. In this theory, the electrons are described by quasiparticles, which have the same quantum numbers and charge, but the interaction of the electrons is taken into account by introducing an effective mass, which differs from the actual mass of a free electron.\n\nIn order to obtain the optical properties of heavy fermion systems, these materials have been investigated by optical spectroscopy measurements. In these experiments the sample is irradiated by electromagnetic waves with tuneable wavelength. Measuring the reflected or transmitted light reveals the characteristic energies of the sample.\n\nAbove the characteristic coherence temperature formula_1, heavy fermion materials behave like normal metals; i.e. their optical response is described by the Drude model. Compared to a good metal however, heavy fermion compounds at high temperatures have a high scattering rate because of the large density of local magnetic moments (at least one \"f\" electron per unit cell), which cause (incoherent) Kondo scattering. Due to the high scattering rate, the conductivity for dc and at low frequencies is rather low. A conductivity roll-off (Drude roll-off) occurs at the frequency that corresponds to the relaxation rate.\n\nBelow formula_1, the localized \"f\" electrons hybridize with the conduction electrons. This leads to the enhanced effective mass, and a hybridization gap develops. In contrast to Kondo insulators, the chemical potential of heavy fermion compounds lies within the conduction band. These changes lead to two important features in the optical response of heavy fermions.\n\nThe frequency-dependent conductivity of heavy-fermion materials can be expressed by formula_3, containing the effective mass formula_4 and the renormalized relaxation rate formula_5. Due to the large effective mass, the renormalized relaxation time is also enhanced, leading to a narrow Drude roll-off at very low frequencies compared to normal metals.\nThe lowest such Drude relaxation rate observed in heavy fermions so far, in the low GHz range, was found in UPdAl.\n\nThe gap-like feature in the optical conductivity represents directly the hybridization gap, which opens due to the interaction of localized \"f\" electrons and conduction electrons. Since the conductivity does not vanish completely, the observed gap is actually a pseudogap.\n\nAt low temperature and for normal metals, the specific heat formula_6 consists of the specific heat of the electrons formula_7 which depends linearly on temperature formula_8 and of the specific heat of the crystal lattice vibrations (phonons) formula_9 which depends cubically on temperature\nwith proportionality constants formula_11 and formula_12.\n\nIn the temperature range mentioned above, the electronic contribution is the major part of the specific heat. For the free-electron gas — a simple model system that neglects electron interaction — or metals that could be described by it, the electronic specific heat is given by\nwith Boltzmann's factor formula_14, the electron density formula_15 and the Fermi energy formula_16 (the highest single particle energy of occupied electronic states). The proportionality constant formula_12 is called the Sommerfeld coefficient.\n\nFor electrons with a quadratic dispersion relation (as for the free-electron gas), the Fermi energy ε is inversely proportional to the particle's mass m:\nwhere formula_19 stands for the Fermi wave number that depends on the electron density and is the absolute value of the wave number of the highest occupied electron state. Thus, because the Sommerfeld parameter formula_12 is inversely proportional to formula_16, formula_12 is proportional to the particle's mass and for high values of formula_12, the metal behaves as a free electron gas in which the conduction electrons have a high thermal effective mass.\n\nExperimental results for the specific heat of the heavy fermion compound UBe show a peak at a temperature around 0.75 K that goes down to zero with a high slope if the temperature approaches 0 K. Due to this peak, the formula_12 factor is much higher than for the free-electron gas in this temperature range. In contrast, above 6 K, the specific heat for this heavy fermion compound approaches the value expected from free-electron theory.\n\nThe presence of local moment and delocalized conduction electrons leads to a competition of the Kondo interaction (which favors a non-magnetic ground state) and the RKKY interaction (which generates magneticially ordered states, typically antiferromagnetic for heavy fermions). By suppressing the Néel temperature of a heavy-fermion antiferromagnet down to zero (e.g. by applying pressure or magnetic field or by changing the material composition), a quantum phase transition can be induced. For several heavy-fermion materials it was shown that such a quantum phase transition can generate very pronounced non-Fermi liquid properties at finite temperatures. Such quantum-critical behavior is also studied in great detail in the context of unconventional superconductivity.\n\nExamples of heavy-fermion materials with well-studied quantum-critical properties are CeCuAu, CeIn, CePdSi, YbRhSi, and CeCoIn.\n\n\n"}
{"id": "11686847", "url": "https://en.wikipedia.org/wiki?curid=11686847", "title": "Heiligendamm Process", "text": "Heiligendamm Process\n\nThe Heiligendamm process is an initiative that will institutionalize high-level dialogue between the G8 and the five most important emerging economies, known as the O5 (Outreach 5): China, Mexico, India, Brazil and South Africa. The framework will also seek establishment of a common G8/G5 platform at the OECD.\n\nIn the past, talks between the two groups took place sporadically, but presently the G8 and O5 meet regularly. Innovation, freedom of investment, development in Africa, and technology for reducing emissions were the four main issues this process visited in 2008 and 2009. A progress report was presented at the 2008 G8 Summit in Japan; a final report on the results of the dialogue was put forward in Italy in 2009. German Chancellor Angela Merkel supported this process, stating, \"[W]e cannot get by, or shape globalization in a humane way, without each other\".\n\nOn August 28, 2007, French President Nicolas Sarkozy proposed that China, Mexico, Brazil, South Africa and India should become members of G8: \"The G8 can't meet for two days and the G13 for just two hours... That doesn't seem fitting, given the power of these five emerging countries.\" Nevertheless, as of 2009 formal enlargement of the G8 is not a realistic political option, as G8 states have diverging positions thereon. The US and Japan have been against enlargement, with Great Britain and France actively in favour, and Italy, Germany, and Canada undecided.\n\n"}
{"id": "20191692", "url": "https://en.wikipedia.org/wiki?curid=20191692", "title": "Helium atom", "text": "Helium atom\n\nA helium atom is an atom of the chemical element helium. Helium is composed of two electrons bound by the electromagnetic force to a nucleus containing two protons along with either one or two neutrons, depending on the isotope, held together by the strong force. Unlike for hydrogen, a closed-form solution to the Schrödinger equation for the helium atom has not been found. However, various approximations, such as the Hartree–Fock method, can be used to estimate the ground state energy and wavefunction of the atom.\n\nThe quantum mechanical description of the helium atom is of special interest, because it is the simplest multi-electron system and can be used to understand the concept of quantum entanglement. The Hamiltonian of helium, considered as a three-body system of two electrons and a nucleus and after separating out the centre-of-mass motion, can be written as\n\nwhere formula_2 is the reduced mass of an electron with respect to the nucleus, formula_3 and formula_4 are the electron-nucleus distance vectors and formula_5. The nuclear charge, formula_6 is 2 for helium. In the approximation of an infinitely heavy nucleus, formula_7 we have formula_8 and the mass polarization term formula_9 disappears. In atomic units the Hamiltonian simplifies to\n\nIt is important to note, that it operates not in normal space, but in a 6-dimensional \"configuration space formula_11\". In this approximation (Pauli approximation) the wave function is a second order spinor with 4 components formula_12, where the indices formula_13 describe the spin projection of both electrons (\"z\"-direction up or down) in some coordinate system. It has to obey the usual normalization condition formula_14. This general spinor can be written as 2x2 matrix formula_15 and consequently also as linear combination of any given basis of four orthogonal (in the vector-space of 2x2 matrices) constant matrices formula_16 with scalar function coefficients\nformula_17 as formula_18. A convenient basis consists of one anti-symmetric matrix (with total spin formula_19, corresponding to a singlet state)formula_20 and three symmetric matrices (with total spin formula_21, corresponding to a triplet state) formula_22 formula_23 formula_24It is easy to show, that the singlet state is invariant under all rotations (a scalar entity), while the triplet can be mapped to an ordinary space vector formula_25,\nwith the three componentsformula_26, formula_27 and formula_28.Since all spin interaction terms between the four components of formula_29 in the above (scalar) Hamiltonian are neglected (e.g. an external magnetic field, or relativistic effects, like angular momentum coupling), the four Schrödinger equations can be solved independently.\nThe spin here only comes into play through the Pauli exclusion principle, which for fermions (like electrons) requires antisymmetry under \"simultaneous exchange of spin and coordinates\"\n\nParahelium is then the singlet state formula_31 with a \"symmetric \"function formula_32 \nand orthohelium is the triplet state formula_33 with an \"antisymmetric \"function formula_34. \nIf the electron-electron interaction term is ignored, both spatial functions formula_35 can be written as linear combination of two arbitrary (orthogonal and normalized) \none-electron eigenfunctions formula_36: formula_37 or for the special cases\nof formula_38 (both electrons have identical quantum numbers, parahelium only): formula_39.\nThe total energy (as eigenvalue of formula_40) is then for all cases formula_41 (independent of the symmetry).\n\nThis explains the absence of the formula_42 state (with formula_43) for orthohelium, where \nconsequently formula_44 (with formula_45) is the metastable ground state.\n(A state with the quantum numbers: principal quantum number formula_46, total spin formula_47, angular quantum number formula_48 and total angular momentum \nformula_49 is denoted by formula_50.)\n\nIf the electron-electron interaction term formula_51 is included, the Schrödinger equation is non separable. \nHowever, also if is neglected, all states described above (even with two identical quantum numbers, like formula_52 with formula_53)\ncannot be written as a product of one-electron wave functions:\nformula_54 — the wave function is entangled.\nOne cannot say, particle 1 is in state 1 and the other in state 2, and measurements cannot be made on one particle without affecting the other.\n\nNevertheless, quite good theoretical descriptions of helium can be obtained within the Hartree–Fock and Thomas–Fermi approximations (see below).\n\nThe Hartree–Fock method is used for a variety of atomic systems. However it is just an approximation, and there are more accurate and efficient methods used today to solve atomic systems. The \"many-body problem\" for helium and other few electron systems can be solved quite accurately. For example, the ground state of helium is known to fifteen digits. In Hartree–Fock theory, the electrons are assumed to move in a potential created by the nucleus and the other electrons. The Hamiltonian for helium with two electrons can be written as a sum of the Hamiltonians for each electron:\n\nformula_55\n\nwhere the zero-order unperturbed Hamiltonian is\n\nformula_56\n\nwhile the perturbation term:\n\nformula_57\n\nis the electron-electron interaction. H is just the sum of the two hydrogenic Hamiltonians:\n\nformula_58\n\nwhere\n\nformula_59\n\nE, the energy eigenvalues and formula_60, the corresponding eigenfunctions of the hydrogenic Hamiltonian will denote the normalized energy eigenvalues and the normalized eigenfunctions. So:\n\nformula_61\n\nwhere\n\nformula_62\n\nNeglecting the electron-electron repulsion term, the Schrödinger equation for the spatial part of the two-electron wave function will reduce to the 'zero-order' equation\n\nformula_63\n\nThis equation is separable and the eigenfunctions can be written in the form of single products of hydrogenic wave functions:\n\nformula_64\n\nThe corresponding energies are (in atomic units, hereafter a.u.):\n\nformula_65\n\nNote that the wave function\n\nformula_66\n\nAn exchange of electron labels corresponds to the same energy formula_67. This particular case of degeneracy with respect to exchange of electron labels is called exchange degeneracy. The exact spatial wave functions of two-electron atoms must either be symmetric or antisymmetric with respect to the interchange of the coordinates formula_3 and formula_4 of the two electrons. The proper wave function then must be composed of the symmetric (+) and antisymmetric(-) linear combinations:\n\nformula_70\n\nThis comes from Slater determinants.\n\nThe factor formula_71 normalizes formula_72. In order to get this wave function into a single product of one-particle wave functions, we use the fact that this is in the ground state. So formula_73. So the formula_74 will vanish, in agreement with the original formulation of the Pauli exclusion principle, in which two electrons cannot be in the same state. Therefore, the wave function for helium can be written as\n\nformula_75\n\nWhere formula_76 and formula_77 use the wave functions for the hydrogen Hamiltonian. \\left(\\frac{Z}{a_0}\\right)^{\\frac{3}{2}}\\ e^{-{\\textstyle \\frac{Zr}{a_0}}}\\; </math>. In atomic units, the Bohr radius formula_78 equals 1, and the wavefunction becomes formula_79.}} For helium, Z = 2 from\n\nformula_80\n\nwhere Eformula_81 = −4 a.u. which is approximately −108.8 eV, which corresponds to an ionization potential Vformula_82 = 2 a.u. (≅54.4 eV). The experimental values are Eformula_83 = −2.90 a.u. (≅ −79.0 eV) and Vformula_84 = 0.90 a.u. (≅ 24.6 eV).\n\nThe energy that we obtained is too low because the repulsion term between the electrons was ignored, whose effect is to raise the energy levels. As Z gets bigger, our approach should yield better results, since the electron-electron repulsion term will get smaller.\n\nSo far a very crude independent-particle approximation has been used, in which the electron-electron repulsion term is completely omitted. Splitting the Hamiltonian showed below will improve the results:\n\nformula_85\n\nwhere\n\nformula_86\n\nand\n\nformula_87\n\nV(r) is a central potential which is chosen so that the effect of the perturbation formula_88 is small. The net effect of each electron on the motion of the other one is to screen somewhat the charge of the nucleus, so a simple guess for V(r) is\n\nformula_89\n\nwhere S is a screening constant and the quantity Z is the effective charge. The potential is a Coulomb interaction, so the corresponding individual electron energies are given (in a.u.) by\n\nformula_90\n\nand the corresponding wave function is given by\n\nformula_91\n\nIf Z was 1.70, that would make the expression above for the ground state energy agree with the experimental value E = −2.903 a.u. of the ground state energy of helium. Since Z = 2 in this case, the screening constant is S = 0.30. For the ground state of helium, for the average shielding approximation, the screening effect of each electron on the other one is equivalent to about formula_92 of the electronic charge.\n\nNot long after Schrödinger developed the wave equation, the Thomas–Fermi model was developed. Density functional theory is used to describe the particle density formula_93, and the ground state energy E(N), where N is the number of electrons in the atom. If there are a large number of electrons, the Schrödinger equation runs into problems, because it gets very difficult to solve, even in the atom's ground states. This is where density functional theory comes in. Thomas–Fermi theory gives very good intuition of what is happening in the ground states of atoms and molecules with N electrons.\n\nThe energy functional for an atom with N electrons is given by:\n\nformula_94\n\nWhere\n\nformula_95\n\nThe electron density needs to be greater than or equal to 0, formula_96, and formula_97 is convex.\n\nIn the energy functional, each term holds a certain meaning. The first term describes the minimum quantum-mechanical kinetic energy required to create the electron density formula_98 for an N number of electrons. The next term is the attractive interaction of the electrons with the nuclei through the Coulomb potential formula_99. The final term is the electron-electron repulsion potential energy.\n\nSo the Hamiltonian for a system of many electrons can be written:\n\nformula_100\n\nFor helium, N = 2, so the Hamiltonian is given by:\n\nformula_101\n\nWhere\n\nformula_102\n\nyielding\n\nformula_103\n\nFrom the Hartree–Fock method, it is known that ignoring the electron-electron repulsion term, the energy is 8E = −109 eV.\n\nTo obtain a more accurate energy the variational principle can be applied to the electron-electron potential Vusing the wave function\n\nformula_104:\n\nformula_105\n\nAfter integrating this, the result is:\n\nformula_106\n\nThis is closer to the experimental value, but if a better trial wave function is used, an even more accurate answer could be obtained. An ideal wave function would be one that doesn't ignore the influence of the other electron. In other words, each electron represents a cloud of negative charge which somewhat shields the nucleus so that the other electron actually sees an effective nuclear charge Z that is less than 2. A wave function of this type is given by:\n\nformula_107\n\nTreating Z as a variational parameter to minimize H. The Hamiltonian using the wave function above is given by:\n\nformula_108\n\nAfter calculating the expectation value of formula_109 and V the expectation value of the Hamiltonian becomes:\n\nformula_110\n\nThe minimum value of Z needs to be calculated, so taking a derivative with respect to Z and setting the equation to 0 will give the minimum value of Z:\n\nformula_111\n\nformula_112\n\nThis shows that the other electron somewhat shields the nucleus reducing the effective charge from 2 to 1.69. So we obtain the most accurate result yet:\n\nformula_113\n\nWhere again, E represents the ionization energy of hydrogen.\n\nBy using more complicated/accurate wave functions, the ground state energy of helium has been calculated closer and closer to the experimental value −78.95 eV. The variational approach has been refined to very high accuracy for a comprehensive regime of quantum states by G.W.F. Drake and co-workers as well as J.D. Morgan III, Jonathan Baker and Robert Hill using Hylleraas or Frankowski-Pekeris basis functions. One needs to include relativistic and quantum electrodynamic corrections to get full agreement with experiment to spectroscopic accuracy.\n\nHelium's first ionization energy is −24.587387936(25) eV. This value was derived by experiment. The theoretic value of Helium atom's second ionization energy is −54.41776311(2) eV. The total ground state energy of the helium atom is −79.005151042(40) eV, or −2.90338583(13) a.u.\n\n"}
{"id": "27313300", "url": "https://en.wikipedia.org/wiki?curid=27313300", "title": "Hentriacontylic acid", "text": "Hentriacontylic acid\n\nHentriacontylic acid (also hentriacontanoic acid, henatriacontylic acid, or henatriacontanoic acid) is a carboxylic saturated fatty acid.\n\nHentriacontylic acid can be derived from peat wax and montan wax.\n\nThe olefin triacontene-1 can be reacted to yield linear \"n\"-henatriacontanoic acid.\n\n\n"}
{"id": "3015816", "url": "https://en.wikipedia.org/wiki?curid=3015816", "title": "Hot water bottle", "text": "Hot water bottle\n\nA hot-water bottle is a bottle filled with hot water and sealed with a stopper, used to provide warmth, typically while in bed, but also for the application of heat to a specific part of the body.\n\nContainers for warmth in bed were in use as early as the 16th century. The earliest versions contained hot coals from the dying embers of the fire, and these bed warmers were used to warm the bed before getting into it.\n\nContainers using hot water were soon also used, with the advantage that they could remain in the bed with the sleeper.\n\nPrior to the invention of rubber that could withstand sufficient heat, these early hot-water bottles were made of a variety of materials, such as zinc, copper, brass, glass, earthenware or wood. To prevent burning, the metal hot water flasks were wrapped in a soft cloth bag.\n\n\"India rubber\" hot-water bottles were in use in Britain at least by 1875.\nModern conventional hot-water bottles were patented in 1903 and are manufactured in natural rubber or PVC, to a design patented by the Croatian inventor Eduard Penkala. They are now commonly covered in fabric, sometimes with a novelty design.\nBy the late 20th century, the use of hot-water bottles had markedly declined around most of the world. Not only were homes better heated, but newer items such as electric blankets were competing with hot-water bottles as a source of night-time heat. However the hot-water bottle remains a popular alternative in Australia, Ireland, United Kingdom, developing countries and rural areas. For example, it is widely used in Chile, where it is called a \"guatero\". There has been a recent surge in popularity in Japan where it is seen as an ecologically friendly and thrifty way to keep warm.\n\nSome newer products function like the older bottles, but use a polymer gel or wax in a heat pad. The pads can be heated in a microwave oven, and they are marketed as safer than liquid-filled bottles or electrically-heated devices. Some newer bottles now use a silicone-based material instead of rubber, which resists very hot water better, and does not deteriorate as much as rubber. Although the stopper size in Ireland and the UK has been largely standard for many decades, the newer bottles (notably those from German manufacturer Fashy) use a wider mouth which is easier to fill (and a larger stopper to fit it).\n\nWhile generally used for keeping warm, conventional hot-water bottles can be used to some effect for the local application of heat as a medical treatment, for example for pain relief, but newer items such as purpose-designed heating pads are often used now.\n\nHot-water bottles are meant to contain very hot fluids and also supposed to be in contact with human skin. It is therefore of the utmost importance to ensure, mainly through standards and regulations, that the closing and welding is stable enough to prevent burns, but also to make sure that the bottle’s chemical components are not dangerous for human health. More generally, it is crucial to certify and assure that hot-water bottles, whether manufactured, sold or imported, are safe.\n\nFor instance, the United Kingdom defined British Standards for hot-water bottles to regulate their manufacture and sale as well as to ensure their compliance with all safety standards. The British Standards BS 1970 and BS 1970:2012 (updated version) define, for instance, the bottles’ filling characteristics, safety instructions, allowed materials and components as well as testing methods such as tensile tests for PVC bottles.\n\nMost regulations applied to a country are generally harmonized in order to be applied and applicable in a larger area, such as a trade zone.\n\nThere have been problems with premature failure of rubber hot-water bottles due to faulty manufacture. The rubber may fail strength or fitness tests, or become brittle if manufacturing is not controlled closely. Natural rubber filled with calcium carbonate is the most common material used, but is susceptible to oxidation and polymer degradation at the high temperatures used in shaping the product. Even though the brittle cracks may not be visible externally, the bottle can fracture suddenly after filling with hot water, and can scald the user—sometimes requiring hospitalization for severe burn cases.\n\nBoiling water is not recommended for use in hot-water bottles. This is due to risks of the rubber being degraded from high-temperature water, and the risk of injury in case of breakage.\n\nAlfred, the cantankerous hot-water bottle, is a character from \"Johnson and Friends\", a popular Australian children's television series from the 1990s. This character has gained a cult following in recent years, particularly among those who grew up with the series, due to the odd character choice. Alfred is believed to be the only anthropomorphised hot-water bottle in existence.\n\n"}
{"id": "14445477", "url": "https://en.wikipedia.org/wiki?curid=14445477", "title": "Hydrocarbon economy", "text": "Hydrocarbon economy\n\nHydrocarbon economy is a term referencing the global hydrocarbon industry and its relationship to world markets. Energy used mostly comes from three hydrocarbons: petroleum, coal, and natural gas. Hydrocarbon economy is often used when talking about possible alternatives like the hydrogen economy.\n"}
{"id": "23655649", "url": "https://en.wikipedia.org/wiki?curid=23655649", "title": "Irvine Masson", "text": "Irvine Masson\n\nSir James Irvine Orme Masson FRS FRSE MBE LLD (3 September 1887 – 22 October 1962), generally known as Irvine Masson, was an Australian-born chemist of Scots descent who was Vice-Chancellor of the University of Sheffield from 1938 to 1953. He is usually referred to in documents as J. I. O. Masson.\n\nIrvine Masson was born in Toorak, near Melbourne, the son of Sir David Orme Masson a professor of chemistry at Melbourne University, and his wife, Mary Struthers. He went to Melbourne Grammar School then Melbourne University, achieving a BSc with first class honours in chemistry in 1908. He began medical studies, but reverted to chemistry and in 1910 took up a scholarship in the subject at University College London joining the staff in 1913.\n\nDuring the First World War he did explosives research at the Royal Arsenal, Woolwich, a practical experience very different from his previous academic work, but which had a major influence on his future research.\n\nAfter further time at University College, in 1924 he was made Professor of Chemistry at the University of Durham, also taking on the role of head of the Department of Pure Science. During this time he was lucky to survive one of his experiments which destroyed much of the laboratory. This administrative role led to his appointment as Vice-Chancellor of the University of Sheffield in 1938. However he combined this with running research on explosives during the Second World War.\n\nIn 1939 he was elected a Fellow of the Royal Society of London. He was knighted by King George VI in 1950. In 1953 he was elected a Fellow of the Royal Society of Edinburgh. His proposers were Arthur Holmes, Sir Godfrey Thomson, Sir Sydney A Smith, and John Edwin MacKenzie.\n\nIn 1952 he retired from Sheffield and moved to Edinburgh, where both he and his wife had other family connections.\n\nHe died in Edinburgh on 22 October 1962. He is buried in the Grange Cemetery in southern Edinburgh.\n\nHe was the paternal grandson of David Masson and maternal grandson of John Struthers. He was the nephew of Simon Somerville Laurie. \n\nIn 1913 he married his first cousin, Flora Gulland (d.1960) sister of John Masson Gulland. Their children included David Irvine Masson\n"}
{"id": "11675105", "url": "https://en.wikipedia.org/wiki?curid=11675105", "title": "Keystone Pipeline", "text": "Keystone Pipeline\n\nThe Keystone Pipeline System is an oil pipeline system in Canada and the United States, commissioned in 2010 and now owned solely by TransCanada Corporation. It runs from the Western Canadian Sedimentary Basin in Alberta to refineries in Illinois and Texas, and also to oil tank farms and an oil pipeline distribution center in Cushing, Oklahoma. The pipeline became well-known when a planned fourth phase, Keystone XL, attracted opposition from environmentalists, becoming a symbol of the battle over climate change and fossil fuels. In 2015 Keystone XL was temporarily delayed by then–President Barack Obama. On January 24, 2017, President Donald Trump took action intended to permit the pipeline's completion.\n\nThree phases of the project are in operation:\n\nThe first two phases have the capacity to deliver up to of oil into the Midwest refineries. Phase III has capacity to deliver up to to the Texas refineries. By comparison, production of petroleum in the United States averaged in first-half 2015, with gross exports of through July 2015.\n\nThe proposed Keystone XL (sometimes abbreviated KXL, with XL standing for \"export limited\") Pipeline (Phase IV) would connect the Phase I-pipeline terminals in Hardisty, Alberta, and Steele City, Nebraska by a shorter route and a larger-diameter pipe. It would run through Baker, Montana, where American-produced light crude oil from the Williston Basin (Bakken formation) of Montana and North Dakota would be added to the Keystone's throughput of synthetic crude oil (syncrude) and diluted bitumen (dilbit) from the oil sands of Canada.\n\nThe Keystone Pipeline system consists of the operational Phase I, Phase II, and Phase III, the Gulf Coast Pipeline Project. A fourth, proposed pipeline expansion segment Phase IV, Keystone XL, failed to receive necessary permits from the United States federal government in 2015. Construction of Phase III, from Cushing, Oklahoma, to Nederland, Texas, in the Gulf Coast area, began in August 2012 as an independent economic utility. Phase III was opened on 22 January 2014 The Keystone XL Pipeline Project (Phase IV) revised proposal in 2012 consists of a new pipeline from Hardisty, Alberta, through Montana and South Dakota to Steele City, Nebraska, to \"transport of up to of crude oil from the Western Canadian Sedimentary Basin in Alberta, Canada, and from the Williston Basin (Bakken) region in Montana and North Dakota, primarily to refineries in the Gulf Coast area.\" The Keystone XL pipeline segments were intended to allow American crude oil to enter the XL pipelines at Baker, Montana, on their way to the storage and distribution facilities at Cushing, Oklahoma. Cushing is a major crude oil marketing/refining and pipeline hub.\n\nOperating since 2010, the original Keystone Pipeline System is a pipeline delivering Canadian crude oil to U.S. Midwest markets and Cushing, Oklahoma. In Canada, the first phase of Keystone involved the conversion of approximately of existing natural gas pipeline in Saskatchewan and Manitoba to crude oil pipeline service. It also included approximately of new diameter pipeline, 16 pump stations and the Keystone Hardisty Terminal.\n\nThe U.S. portion of the Keystone Pipeline included of new, diameter pipeline in North Dakota, South Dakota, Nebraska, Kansas, Missouri and Illinois. The pipeline has a minimum ground cover of . It also involved construction of 23 pump stations and delivery facilities at Wood River and Patoka, Illinois. In 2011, the second phase of Keystone included a extension from Steele City, Nebraska, to Cushing, Oklahoma, and 11 new pump stations to increase the capacity of the pipeline from .\n\nAdditional phases (III, completed in 2014, and IV, rejected in 2015) have been in construction or discussion since 2011. If completed, the Keystone XL would have added increasing the total capacity up to .\n\nThe original Keystone Pipeline cost US$5.2 billion with the Keystone XL expansion slated to cost approximately US$7 billion. The Keystone XL had been expected to be completed by 2012–2013, however construction was ultimately overtaken by events.\n\nWhile the project was originally developed as a partnership between TransCanada Corporation and ConocoPhillips, TransCanada is now the sole owner of the Keystone Pipeline System, as TransCanada received regulatory approval on August 12, 2009 to purchase ConocoPhillips' interest.\n\nCertain parties who have agreed to make volume commitments to the Keystone expansion have the option to acquire up to a combined 15% equity ownership. One such company is Valero Energy Corporation.\n\nThis pipeline runs from Hardisty, Alberta, to the junction at Steele City, Nebraska, and on to the Wood River Refinery in Roxana, Illinois, and Patoka Oil Terminal Hub (tank farm) north of Patoka, Illinois. The Canadian section involves approximately of pipeline converted from the Canadian Mainline natural gas pipeline and of new pipeline, pump stations and terminal facilities at Hardisty, Alberta.\n\nThe United States section is long. It runs through Buchanan, Clinton, Caldwell, Montgomery, Lincoln and St. Charles counties in Missouri, and Nemaha, Brown and Doniphan counties in Kansas before entering Madison County, Illinois. Phase 1 went online in June 2010.\n\nThe Keystone-Cushing pipeline phase connected the Keystone pipeline (phase 1) in Steele City, Nebraska, south through Kansas to the oil hub and tank farm in Cushing, Oklahoma, a distance of . It was constructed in 2010 and went online in February 2011.\n\nThe Cushing MarketLink pipeline phase started at Cushing, Oklahoma, where American-produced oil is added to the pipeline, then runs south to a delivery point near terminals in Nederland, Texas, to serve refineries in the Port Arthur, Texas, area. Keystone started pumping oil through this section in January 2014. Oil producers in the U.S. pushed for this phase so the glut of oil can be distributed out of the large oil tank farms and distribution center in Cushing.\n\nThe Houston Lateral pipeline phase is a pipeline to transport crude oil from the pipeline in Liberty County, Texas, to refineries and terminal in the Houston area. This phase was constructed 2013 to 2016 and went online in 2017.\n\nThe proposed Keystone XL pipeline starts from the same area in Alberta, Canada, as the Phase 1 pipeline. The Canadian section would consist of of new pipeline. It would enter the United States at Morgan, Montana, and travel through Baker, Montana, where American-produced oil would be added to the pipeline; then it would travel through South Dakota and Nebraska, where it would join the existing Keystone pipelines at Steele City, Nebraska. This phase has generated the greatest controversy because of its routing over the Sandhills in Nebraska.\n\nOn 6 November 2015, the project of Keystone XL was rejected by the Obama administration after more than six years of review and initial approval by the Obama State Department. Leading up to this, financial commitment towards completion of the pipeline was weakened by a number of technological factors as well. Recent innovations in fracking had increased domestic production of oil and, according to the EIA, reduced annual demand of oil from foreign countries to an all-time low since 1985. Shifts to Gasoline fuel for cargo vehicles, new technologies promoting fuel efficiency, and export restrictions that forced the price of oil to decrease also played a part.\n\nIn the wake of Obama's decision, aides to then President Elect Donald Trump were reportedly looking for the easiest way(s) to countermand it.\n\nOn January 18, 2018, TransCanada announced they had secured commitments to ship 500,000 barrels per day for 20 years. They believe construction could begin in 2019.\n\nOn November 8, 2018, a federal judge in Montana blocked construction of Keystone XL pipeline, due to it failing to comply with federal environmental regulations.\n\nTransCanada Corporation proposed the project on February 9, 2005. In October 2007, the Communications, Energy and Paperworkers Union of Canada asked the Canadian federal government to block regulatory approvals for the pipeline, with union president Dave Coles stating \"the Keystone pipeline will exclusively serve US markets, create a permanent employment for very few Canadians, reduce our energy security, and hinder investment and job creation in the Canadian energy sector.\"\n\nThe National Energy Board of Canada approved the construction of the Canadian section of the pipeline, including converting a portion of TransCanada's Canadian Mainline gas pipeline to crude oil pipeline, on September 21, 2007. On March 17, 2008, the United States Department of State issued a Presidential Permit authorizing the construction, maintenance and operation of facilities at the United States and Canada border.\n\nOn January 22, 2008, ConocoPhillips acquired a 50% stake in the project. On June 17, 2009, TransCanada agreed that they would buy out ConocoPhillips' share in the project and revert to being the sole owner. It took TransCanada more than two years to acquire all the necessary state and federal permits for the pipeline. Construction took another two years. The pipeline, from Hardisty, Alberta, Canada, to Patoka, Illinois, United States, became operational in June 2010.\n\nOn June 3, 2011, PHMSA issued TransCanada a Corrective Action Order (CAO), for Keystone's May 2011 leaks. On April 9, 2016, PHMSA issued a CAO to TransCanada, for a 16,800 gallon leak in Hutchinson County, South Dakota, on April 2. The pipeline restarted at a reduced operating pressure on April 10 after the U.S. regulator approved the companies corrective actions and plan. A 9,700 barrel (407,400 gallons) leak occurred in Marshall County, South Dakota in November 2017. This leak occurred early in the morning on November 16, 2017 near Amherst, South Dakota and was contained shortly after detection 35 miles south of the Ludden pump station.\n\nThe Keystone XL extension was proposed in 2008. The application was filed in September 2008 and the National Energy Board of Canada started hearings in September 2009. On March 11, 2010, the Canadian National Energy Board approved the project. The South Dakota Public Utilities Commission granted a permit to proceed on February 19, 2010.\n\nOn July 21, 2010, the Environmental Protection Agency said the draft environmental impact study for Keystone XL was inadequate and should be revised, indicating that the State Department's original report was \"unduly narrow\" because it did not fully look at oil spill response plans, safety issues and greenhouse gas concerns. The final environmental impact report was released on August 26, 2011. It stated that the pipeline would pose \"no significant impacts\" to most resources if environmental protection measures are followed, but it would present \"significant adverse effects to certain cultural resources\". In September 2011, Cornell ILR Global Labor Institute released the results of the GLI Keystone XL Report, which evaluated the pipeline's impact on employment, the environment, energy independence, the economy, and other critical areas.\n\nOn November 10, 2011, the Department of State postponed a final decision due to necessity \"to seek additional information regarding potential alternative routes around the Sandhills in Nebraska to inform the determination regarding whether issuing a permit for the proposed Keystone XL pipeline is in the national interest\". In its response, TransCanada pointed out fourteen different routes for Keystone XL were being studied, eight that impacted Nebraska. They included one potential alternative route in Nebraska that would have avoided the entire Sandhills region and Ogallala Aquifer and six alternatives that would have reduced pipeline mileage crossing the Sandhills or the aquifer. On November 22, 2011, the Nebraska unicameral legislature passed unanimously two bills with the governor's signature that enacted a compromise agreed upon with the pipeline builder to move the route, and approved up to US$2 million in state funding for an environmental study.\n\nOn November 30, 2011, a group of Republican senators introduced legislation aimed at forcing the Obama administration to make a decision within 60 days. In December 2011, Congress passed a bill giving the Obama Administration a 60-day deadline to make a decision on the application to build the Keystone XL Pipeline.\n\nIn January 2012, Obama rejected the application stating that the deadline for the decision had \"prevented a full assessment of the pipeline's impact\". On September 5, 2012, TransCanada submitted an environmental report on the new route in Nebraska, which the company says is \"based on extensive feedback from Nebraskans, and reflects our shared desire to minimize the disturbance of land and sensitive resources in the state\".\nIn March 2012, Obama endorsed the building of the southern segment (Gulf Coast Extension or Phase III) that begins in Cushing, Oklahoma. The President said in Cushing, Oklahoma, on March 22, \"Today, I'm directing my administration to cut through the red tape, break through the bureaucratic hurdles, and make this project a priority, to go ahead and get it done.\"\n\nOn September 27, 2012, protesters began tree sitting in the path of the Keystone pipeline near Winnsboro, Texas. Eight people stood on tree platforms just ahead of where crews were cutting down trees to make way for the pipeline.\n\nOn October 4, 2012, actress and activist Daryl Hannah and 78-year-old Texas landowner Eleanor Fairchild were arrested for criminal trespassing and other charges after they were accused of standing in front of TransCanada pipeline construction equipment on Fairchild's farm in Winnsboro, a town about 100 miles east of Dallas. Ms. Fairchild had owned the land since 1983 and refused to sign any agreements with TransCanada. Her land was seized by eminent domain.\n\nIn its supplemental environmental impact statement (SEIS) released in March 2013, the U.S. Department of State Bureau of Oceans and International Environmental and Scientific Affairs described changes to the original proposals including the shortening of the pipeline to ; its avoidance of \"crossing the NDEQ-identified Sandhills Region\" and \"reduction of the length of pipeline crossing the Northern High Plains Aquifer system, which includes the Ogallala formation\"; and stated \"there would be no significant impacts to most resources along the proposed Project route.\" In response to a Freedom of Information Act request for route information, the Department of State revealed on June 24, 2013, that \"Neither Cardno ENTRIX nor TransCanada ever submitted GIS information to the Department of State, nor was either corporation required to do so.\" In response to the Department of State's report, which recommended neither acceptance nor rejection, an editor of \"The New York Times\" recommended that Obama should reject the project, which \"even by the State Department's most cautious calculations — can only add to the [climate change] problem.\" On March 21, Mother Jones revealed that key personnel employed by Environment Resources Management (ERM), the consulting firm responsible for generating most of the SEIS, had previously performed contract work for TransCanada corporation. In addition, When the State Department released the original proposal ERM had submitted to secure the SEIS contract, portions of the work histories of key personnel were redacted.\n\nIn April 2013, the EPA challenged the U.S. State Department report's conclusion that the pipeline would not result in greater oil sand production, noting that \"while informative, [it] is not based on an updated energy-economic modeling effort\". Overall, the EPA rated the SEIS with their category \"EO-2\" (EO for \"environmental objections\" and 2 for \"insufficient information\").\n\nIn May 2013 Republicans in the House of Representatives defended the Northern Route Approval Act, which would allow for congressional approval of the pipeline, on the grounds that the pipeline created jobs and energy independence. If enacted the Northern Route Approval Act would waive the requirement for permits for a foreign company and bypass the need for President Obama's approval, and the debate in the Democrat-controlled U.S. Senate, concerned about serious environmental risks, that could result in the rejection of the pipeline.\n\nOn January 22, 2014, the Gulf Coast Extension (phase III) was opened.\n\nOn January 9, 2015, the U.S. House voted 266–153 in favor of the pipeline. On the same day, the Nebraska Supreme Court cleared the way for construction, after Republican Governor Dave Heineman had approved of it in 2013.\n\nA bill approving the construction of the Keystone XL Pipeline was passed by the Senate (62–36) on January 29, 2015, and by the House (270–152) on February 11, 2015. President Obama vetoed the bill on February 24, 2015, arguing that the decision of approval should rest with the Executive Branch. The Senate was unable to override the veto by a two-thirds majority, with a 62-37 vote.\nTransCanada sued Nebraska landowners who refused permission allowing for pipeline easements on their properties, in order to exercise eminent domain over such use. However, on September 29, 2015, it dropped its lawsuits and acceded to the authority of the elected, five-member Nebraska Public Service Commission, which has the state constitutional authority to approve gas and oil pipelines.\n\nOn November 3, 2015, Secretary of State John Kerry issued a determination that the project was not in the public interest. Kerry found that there was a \"perception\" among foreigners that the project would increase greenhouse-gas emissions, and that, whether or not this perception was accurate, the decision would therefore \"undercut the credibility and influence of the United States\" in climate-change-related negotiations. On November 6, 2015, the Obama government rejected the pipeline. Early in his tenure, President Donald Trump signed presidential memoranda to revive both Keystone XL and Dakota Access pipelines. The order would expedite the environmental review that Trump described as an \"incredibly cumbersome, long, horrible permitting process.\" Subsequently, Donald Trump signed a presidential permit to allow TransCanada to build the Keystone XL pipeline on March 24, 2017.\n\nOn November 20, 2017, the Nebraska Public Service Commission approved (3-2) the construction of the pipeline, albeit through an alternative route that is longer, and was deemed to have the least environmental impact compared to two other routes that were considered. This proved to be a major setback for TransCanada since they would have \"years of new review and legal challenges\". TransCanada asked Nebraska to reconsider this decision and is currently working with Pipeline and Hazardous Materials Safety Administration (PHMSA) to determine the structural cause of a leak in South Dakota on November 21.\n\nThe Keystone XL proposal faced criticism from environmentalists and a minority of the members of the United States Congress. In January 2012, President Barack Obama rejected the application amid protests about the pipeline's impact on Nebraska's environmentally sensitive Sandhills region. TransCanada Corporation changed the original proposed route of Keystone XL to minimize \"disturbance of land, water resources and special areas\"; the new route was approved by Nebraska Governor Dave Heineman in January 2013. On April 18, 2014, the Obama administration announced that the review of the controversial Keystone XL oil pipeline has been extended indefinitely, pending the result of a legal challenge to a Nebraska pipeline siting law that could change the route. On January 9, 2015, the Nebraska Supreme Court cleared the way for construction, and on the same day the House voted in favor of the pipeline. On January 29, 2015, the Keystone XL Pipeline was passed by the Senate 62–36. On February 11, 2015, the Keystone XL Pipeline was passed by the House of Representatives with the proposed Senate Amendments 270–152. The Keystone XL Pipeline bill was not officially sent to President Obama, starting the official ten-day count towards the bill becoming law without presidential signature, until February 24, 2015. Republicans delayed delivering the bill over the Presidents Day holiday weekend to ensure Congress would be in session if the president were to veto the bill. On February 24, 2015, the bill was vetoed and returned for congressional action. On March 4, 2015, the Senate held a vote and failed to override President Obama's veto of the bill; the vote was 62 to 37, less than the two-thirds majority required to override a presidential veto. The review by the State Department is ongoing. On June 15, 2015 the House Oversight Committee threatened to subpoena the State Department for the latter's withholding of records relevant to the process since March 2015 and calling the process \"unnecessarily secretive\". Despite some records being posted by consulted agencies such as the EPA, the State Department has not responded to the request. On November 2, 2015, TransCanada asked the Obama administration to suspend its permit application for the Keystone XL.\n\nIn his speech announcing the rejection of the pipeline on November 6, 2015, President Obama lamented the symbolic importance Keystone XL had taken on, stating, \"for years, the Keystone pipeline has occupied what I, frankly, consider an overinflated role in our political discourse. It became a symbol too often used as a campaign cudgel by both parties rather than a serious policy matter. And all of this obscured the fact that this pipeline would neither be a silver bullet for the economy, as was promised by some, nor the express lane to climate disaster proclaimed by others.\" President Obama nonetheless acknowledged the symbolic importance, going on to state, \"frankly, approving this project would have undercut [the United States'] global leadership\" on climate change.\n\nDifferent environmental groups, citizens, and politicians have raised concerns about the potential negative impacts of the Keystone XL project. The main issues are the risk of oil spills along the pipeline, which would traverse highly sensitive terrain, and 17% higher greenhouse gas emissions from the extraction of oil sands compared to extraction of conventional oil.\n\nAs of 2010, there were worries that a pipeline spill could pollute air and critical water supplies and harm migratory birds and other wildlife. Its original route plan crossed the Sandhills, the large wetland ecosystem in Nebraska, and the Ogallala Aquifer, one of the largest reserves of fresh water in the world. The Ogallala Aquifer spans eight states, provides drinking water for two million people, and supports $20 billion in agriculture. Critics say that a major leak could ruin drinking water and devastate the mid-western U.S. economy. After opposition for laying the pipeline in this area, TransCanada agreed to change the route and skip the Sandhills.\n\nUniversity of Nebraska professor Dr. John Stansbury conducted an independent analysis that provides more detail on the potential risks for the Ogallala Aquifer. In his analysis, Dr. Stansbury concludes that safety assessments provided by TransCanada are misleading. According to Dr. Stansbury, \"We can expect no fewer than 2 major spills per state during the 50-year projected lifetime of the pipeline. These spills could release as much as 180 thousand barrels of oil each.\"\n\nOther items of note in Dr. Stansbury's analysis:\n\n\nPipeline industry spokesmen have noted that thousands of miles of existing pipelines carrying crude oil and refined liquid hydrocarbons have crossed over the Ogallala Aquifer for years, in southeast Wyoming, eastern Colorado and New Mexico, western Nebraska, Kansas, Oklahoma, and Texas. The Pioneer crude oil pipeline crosses east-west across Nebraska, and the Pony Express pipeline, which crosses the Ogallala Aquifer in Colorado, Nebraska, and Kansas, was being converted as of 2013 from natural gas to crude oil, under a permit from the Federal Energy Regulatory Commission.\n\nPortions of the pipeline will also cross an active seismic zone that had a 4.3 magnitude earthquake as recently as 2002. Opponents claim that TransCanada applied to the U.S. government to use thinner steel and pump at higher pressures than normal. In October 2011, \"The New York Times\" questioned the impartiality of the environmental analysis of the pipeline done by Cardno Entrix, an environmental contractor based in Houston. The study found that the pipeline would have limited adverse environmental impacts, but was authored by a firm that had \"previously worked on projects with TransCanada and describes the pipeline company as a 'major client' in its marketing materials\". However, the Department of State's Office of the Inspector General conducted an investigation of the potential conflict of interest, and its February 2012 report of that investigation states there was no conflict of interest either in the selection of the contractor or in the preparation of the environmental impact statement.\n\nAccording to \"The New York Times\", legal experts questioned whether the U.S. government was \"flouting the intent\" of the Federal National Environmental Policy Act, which \"[was] meant to ensure an impartial environmental analysis of major projects\". The report prompted 14 senators and congressmen to ask the State Department inspector general on October 26, 2011 \"to investigate whether conflicts of interest tainted the process\" for reviewing environmental impact. In August 2014, a study was published that concluded the pipeline could produce up to 4 times more global warming pollution than the State Department's study indicated. The report blamed the discrepancy on a failure to take account of the increase in consumption due to the drop in the price of oil that would be spurred by the pipeline.\n\nTransCanada CEO Russ Girling has described the Keystone Pipeline as \"routine\", noting that TransCanada has been building similar pipelines in North America for half a century and that there are of similar oil pipelines in the United States today. He also stated that the Keystone Pipeline will include 57 improvements above standard requirements demanded by U.S. regulators so far, making it \"the safest pipeline ever built\". Rep. Ed Whitfield, a member of the House Committee on Energy and Commerce concurred, saying \"this is the most technologically advanced and safest pipeline ever proposed.\" However, while TransCanada had asserted that a set of 57 conditions will ensure Keystone XL's safe operation, Anthony Swift of the Natural Resources Defense Council asserted that all but a few of these conditions simply restate current minimum standards.\n\nTransCanada claims that they will take 100% responsibility for any potential environmental problems. According to their website, \"It's our responsibility – as a good company and under law. If anything happens on the Keystone XL Pipeline, rapid response is key. That's why our Emergency Response plans are approved by state and federal agencies, and why we practice them regularly. We conduct regular emergency exercises, and aerial surveys every two weeks. We're ready to respond with a highly-trained response team standing by.\"\nIn 2016, about 400 barrels were released from the original Keystone pipe network via leaks, which federal investigators said resulted from a \"weld anomaly.\"\n\nOn November 17, 2017, the pipeline leaked what the company claimed was 210,000 gallons of oil onto farmland near Amherst, South Dakota. The oil leak is the largest seen from the Keystone pipeline in the state. The leak lasted for several minutes, with no initial reports of damage to water sources or wildlife. Although the spill did not happen on Sioux property, it was in close enough proximity to potentially contaminate the aquifer used for water. The pipeline was immediately shut down, and TransCanada began using the pipe again 12 days after the leak. For much of late 2017, the Keystone pipeline operated at reduced pressure during remediation efforts. The federal Pipeline and Hazardous Materials Safety Administration said that the failure \"may have been caused by mechanical damage to the pipeline and coating associated with a weight installed on the pipeline in 2008.\" Later, the NTSB found that a metal tracked vehicle had run over the area, damaging the pipeline. In April 2018, a federal investigation found that 408,000 gallons of crude had spilled at the site, almost twice what TransCanada had reported. That number made it the seventh-largest onshore oil spill since 2002.\n\nIn April 2018, \"Reuters\" reviewed documents that showed that Keystone had \"leaked substantially more oil, and more often, in the United States than the company indicated to regulators in risk assessments before operations began in 2010.\"\n\nEnvironmental organizations such as the Natural Resources Defense Council (NRDC) also oppose the project due to its transportation of oil from oil sands. In its March 2010 report, the NRDC stated that \"the Keystone XL Pipeline undermines the U.S. commitment to a clean energy economy\", instead \"delivering dirty fuel at high costs\". On June 23, 2010, 50 Democrats in Congress in their letter to Secretary of State Hillary Clinton warned that \"building this pipeline has the potential to undermine America's clean energy future and international leadership on climate change\", referencing the higher input quantity of fossil fuels necessary to take the tar and turn it into a usable fuel product in comparison to other conventionally derived fossil fuels.\n\nNASA climate scientist James Hansen stated in 2013 that \"moving to tar sands, one of the dirtiest, most carbon-intensive fuels on the planet\" is a step in exactly the wrong direction, \"indicating either that governments don't understand the situation or that they just don't give a damn\". The House Energy and Commerce Committee's chairman at the time, Representative Henry Waxman, had also urged the State Department to block Keystone XL for greenhouse gas emission reasons.\n\nIn December 2010, the No Tar Sands Oil campaign, sponsored by action groups including Corporate Ethics International, NRDC, Sierra Club, 350.org, National Wildlife Federation, Friends of the Earth, Greenpeace, and Rainforest Action Network, was launched.\n\nIn a speech to the Canadian Club in Toronto on September 23, 2011, Joe Oliver, Canada's Minister of Natural Resources, sharply criticized opponents of oil sands development and the pipeline, arguing that:\n\nThe State Department's Final Supplemental Environmental Impact Statement (Final SEIS) estimated that producing and transporting oil to the pipeline's capacity would increase greenhouse-gas emissions compared to alternative sources of oil. However, this estimate would only apply to the pipeline project as a whole if the denial of the pipeline project meant that the oil would stay in the ground. \"However, ... such a change is not likely to occur. [A]pproval or denial of any one crude oil transport project, including the proposed Project, is unlikely to significantly impact the rate of extraction in the oil sands, or the continued demand for heavy crude oil.\" To the extent that the oil would be extracted in any case, the relevant comparison would be to alternative means of transporting it; the Final SEIS considered three alternative scenarios and found that \"total GHG emissions associated with construction and operation (direct and indirect) combined would be higher for each of the three scenarios than for the entire route encompassing the proposed Project.\" The Final SEIS made no estimate of the net effect of the project on greenhouse-gas emissions, considering both the amount of oil that would likely replace other sources (increasing emissions) and the amount of oil for which the pipeline would merely replace alternate means of transportation (decreasing emissions).\n\nIn a February 2, 2015 letter response to the U.S. Department of State's Final Supplemental Environmental Impact Statement (Final SEIS) for the Keystone XL Pipeline Project, U.S. the Environmental Protection Agency (EPA) stated that the pipeline will significantly increase greenhouse gas emissions because it will lead to the expansion of Alberta's carbon intensive oilsands. The letter goes on to add that over the proposed 50-year timeline of the pipeline, this could mean releasing as much as \"1.37 billion more tons of greenhouse gases into the atmosphere\". According to the New York Times, EPA concluded that due to the current relatively cheap cost of oil, companies might be less likely to set up their own developments in the oil sands. It would be too expensive for the companies to ship by rail. However, \"the presence of the pipeline, which offers an inexpensive way to move the oil to market, could increase the likelihood that companies would extract from the oil sands even when prices are low\". In its letter response, the EPA suggested that the State Department should \"revisit\" its prior conclusions in light of the drop in oil prices.\n\nTransCanada Corporation responded with a letter of its own, in which President and CEO Russel K. Girling stated that TransCanada \"rejects the EPA inference that at lower oil prices the [Keystone XL Pipeline] Project will increase the rate of oil sands production growth and accompanying greenhouse gas emissions\". Girling maintained that the EPA's conclusions \"are not supported by the facts outlined in the Final SEIS or actual observations of the marketplace\".\n\nOn May 4, 2012, the U.S. Department of State selected Environmental Resources Management (ERM) to author a Draft Supplemental Environmental Impact Statement, after the Environmental Protection Agency had found previous versions of the study, by contractor Cardno Entrix, to be extremely inadequate. Project opponents panned the study on its release, calling it a \"deeply flawed analysis\". An investigation by the magazine Mother Jones revealed that the State Department had redacted the biographies of the study's authors to hide their previous contract work for TransCanada and other oil companies with an economic interest in the project. Based on an analysis of public documents on the State Department website, one critic asserted that \"Environmental Resources Management was paid an undisclosed amount under contract to TransCanada to write the statement\".\n\nThe pipeline was a top-tier election issue for the November 4, 2014, United States elections for the United States Senate, for U.S. House of Representatives, for governors in states and territories, and for many state and local positions as well. One election-year dilemma facing the Democrats was whether or not Obama should approve the completion of the Keystone XL pipeline. Tom Steyer, and other wealthy environmentalists, were committed to \"make climate change a top-tier issue\" in the elections with opposition to Keystone XL as \"a significant part of that effort.\" In the election, the Republican party gained 13 House seats (gaining their largest majority in the House since 1928) and 9 Senate seats (becoming the majority party therein).\n\nIn February 2011, environmental journalist David Sassoon of \"Inside Climate News\" reported that Koch Industries was poised to be \"big winners\" from the pipeline. In May 2011, Congressmen Waxman and Rush wrote a letter to the Energy and Commerce Committee citing the Reuters story, and urging the Committee to request documents from Koch Industries relating to the Keystone XL pipeline.\n\nLandowners in the path of the pipeline have complained about threats by TransCanada to confiscate private land and lawsuits to allow the \"pipeline on their property even though the controversial project has yet to receive federal approval\". As of October 17, 2011, TransCanada had \"34 eminent domain actions against landowners in Texas\" and \"22 in South Dakota\". Some of those landowners gave testimony for a House Energy and Commerce Committee hearing in May 2011. In his book \"The Pipeline and the Paradigm\", Samuel Avery quotes landowner David Daniel in Texas, who claims that TransCanada illegally seized his land via eminent domain by claiming to be a public utility rather than a private firm.\n\nIn January 2012, Presidential candidate Sen. Bernie Sanders (I-Vt.) and Rep. Steve Cohen (D-Tenn.) requested a new report on the environmental review process.\n\nIn September 2015, Presidential candidate Hillary Clinton publicly expressed her opposition on Keystone XL, citing climate change concerns.\n\nCommentator Bill Mann has linked the Keystone postponement to the Michigan Senate's rejection of Canadian funding for the proposed Gordie Howe International Bridge and to other recent instances of \"U.S. government actions (and inactions) that show little concern about Canadian concerns\". Mann drew attention to a \"Maclean's\" article sub-titled \"we used to be friends\" about U.S./Canada relations after President Obama had \"insulted Canada (yet again)\" over the pipeline.\n\nCanadian Ambassador Doer observes that Obama's \"choice is to have it come down by a pipeline that he approves, or without his approval, it comes down on trains\".\n\nDuring the 2014 Pacific Northwest Economic Region Summit in Whistler, B.C., Canada's US Ambassador Gary Doer stated that there is no proof, be it environmental, economic, safety or scientific, that construction work on Keystone XL should not go ahead. Doer said that all the evidence supports a favourable decision by the US government for the controversial pipeline.\n\nIn contrast, the President of the Rosebud Sioux Nation, Cyril Scott, has stated that the November 14, 2014, vote in favor of the Keystone XL pipeline in the U.S. House of Representatives is an \"act of war\", declaring:\n\nWe are outraged at the lack of intergovernmental cooperation. We are a sovereign nation, and we are not being treated as such. We will close our reservation borders to Keystone XL. Authorizing Keystone XL is an act of war against our people.\nProponents for the Keystone XL pipeline argue that it would allow the U.S. to increase its energy security and reduce its dependence on foreign oil. TransCanada CEO Russ Girling has argued that \"the U.S. needs 10 million barrels a day of imported oil\" and the debate over the proposed pipeline \"is not a debate of oil versus alternative energy. This is a debate about whether you want to get your oil from Canada or Venezuela or Nigeria.\" However, an independent study conducted by the Cornell ILR Global Labor Institute refers to some studies (e.g. a 2011 study by Danielle Droitsch of Pembina Institute) according to which \"a good portion of the oil that will gush down the KXL will probably end up being finally consumed beyond the territorial United States\". It also states that the project will increase the heavy crude oil price in the Midwestern United States by diverting oil sands oil from the Midwest refineries to the Gulf Coast and export markets.\n\nThe US Gulf Coast has a large concentration of refineries designed to process very heavy crude oil. At present, the refineries are dependent on heavy crude from Venezuela, including crude from Venezuela's own massive Orinoco oil sands. The United States is the number one buyer of crude oil exported from Venezuela. The large trade relationship between the US and Venezuela has persisted despite political tensions between the two countries. However, the volume of oil imported into the US from Venezuela dropped in half from 2007 to 2014, as overall Venezuelan exports have dropped, and also as Venezuela seeks to become less dependent on US purchases of its crude oil. The Keystone pipeline is seen as a way to replace imports of heavy oil-sand crude from Venezuela with more reliable Canadian heavy oil.\n\nTransCanada's Girling has also argued that if Canadian oil doesn't reach the Gulf through an environmentally friendly buried pipeline, that the alternative is oil that will be brought in by tanker, a mode of transportation that produces higher greenhouse-gas emissions and that puts the environment at greater risk. Diane Francis has argued that much of the opposition to the oil sands actually comes from foreign countries such as Nigeria, Venezuela, and Saudi Arabia, all of whom supply oil to the United States and who could be affected if the price of oil drops due to the new availability of oil from the pipeline. She cited as an example an effort by Saudi Arabia to stop pro-oil-sands television commercials. TransCanada had said that development of oil sands will expand regardless of whether the crude oil is exported to the United States or alternatively to Asian markets through Enbridge Northern Gateway Pipelines or Kinder Morgan's Trans-Mountain line.\n\nMany Native Americans and Indigenous Canadians are opposed to the Keystone XL project for various reasons, including possible damage to sacred sites, pollution, and water contamination, which could lead to health risks among their communities.\n\nOn September 19, 2011, a number of leaders from Native American bands in the United States and First Nations bands from Canada were arrested for protesting the Keystone XL outside the White House. According to Debra White Plume, a Lakota activist, indigenous peoples \"have thousands of ancient and historical cultural resources that would be destroyed across [their] treaty lands\". TransCanada's Pipeline Permit Application to the South Dakota Public Utilities Commission states project impacts that include potential physical disturbance, demolition or removal of \"prehistoric or historic archaeological sites, districts, buildings, structures, objects, and locations with traditional cultural value to Native Americans and other groups\".\n\nIndigenous communities are also concerned with health risks posed by the extension of the Keystone pipeline. Locally caught fish and untreated surface water would be at risk for contamination through oil sands extraction, and are central to the diets of many indigenous peoples. Earl Hatley, an environmental activist who has worked with Native American tribes has expressed concern about the environmental and public health impact on Native Americans.\n\nTransCanada has developed an Aboriginal Relations policy in order to confront some of these conflicts. In 2004, TransCanada made a major donation to the University of Toronto \"to promote education and research in the health of the Aboriginal population\". Another proposed solution is TransCanada's Aboriginal Human Resource Strategy, which was developed to facilitate aboriginal employment and to provide \"opportunities for Aboriginal businesses to participate in both the construction of new facilities and the ongoing maintenance of existing facilities\".\n\nRuss Girling, president and CEO of TransCanada, touted the positive impact of the project by \"putting 20,000 US workers to work and spending $7 billion stimulating the US economy\". These numbers come from a 2010 report written by The Perryman Group, a financial analysis firm based in Texas that was hired by TransCanada to evaluate Keystone XL. The numbers in the Perryman Group report have been disputed by an independent study conducted by the Cornell ILR Global Labor Institute, which found that while the Keystone XL would result in 2,500 to 4,650 temporary construction jobs, this impact will be reduced by higher oil prices in the Midwest, which will likely reduce national employment. However, it will increase gasoline availability to the Northeast and expand the Gulf refining industry. The State Department estimates that the pipeline would create about 5,000 to 6,000 temporary jobs in the United States during the two-year construction period.\n\nOn January 27, 2012, Greenpeace Executive Director Phil Radford appealed to the U.S. Securities and Exchange Commission to review TransCanada's claims that the Keystone Pipeline would create 20,000 jobs. Stating that the company had \"consistently used public statements and information it knows are false in a concerted effort to secure permitting approval\" of the pipeline, Radford argued that TransCanada had \"misled investors, U.S. and Canadian officials, the media, and the public at large in order to bolster its balance sheets and share price\".\n\nOn July 27, 2013, President Obama stated \"The most realistic estimates are this might create maybe 2,000 jobs during the construction of the pipeline, which might take a year or two, and then after that we're talking about somewhere between 50 and 100 jobs in an economy of 150 million working people.\" The estimate of 2,000 during construction came under heavy attack, while the long-term, permanent job estimates did not receive as much criticism. The Associated Press noted that it was unclear where the president's figure of 2,000 jobs came from. The U.S. State Department's Preliminary Supplemental Environmental Impact Statement, issued in March 2013, estimated 3,900 direct jobs and 42,000 direct and indirect jobs during construction. According to the Final Supplemental Environmental Impact Statement (SEIS), the Pipeline will only create 35 permanent jobs.\n\nThere might be unintended economic consequences to the construction of Keystone XL. As an example, the additional north-south crude oil transport capacity brought by Keystone XL will increase the price the oil sands producers receive for their oil. These higher revenues will have a positive impact on the development of the industry in Alberta. In return, due to the Petrodollar nature of the Canadian currency these same additional revenues will strengthen the Canadian dollar versus the United States dollar. Based on historical trends, this stronger Canadian dollar will result in a reduction of the competitiveness of Canada's manufacturing industry and could lead to the loss of 50,000 to 100,000 jobs in Canada's manufacturing sector. Many of these jobs, such as the ones in the auto industry, would likely find their way south and have a positive impact on manufacturing employment in the U.S.\n\nGlen Perry, a petroleum engineer for Adira Energy, has warned that including the Alberta Clipper pipeline owned by TransCanada's competitor Enbridge, there is an extensive overcapacity of oil pipelines from Canada. After completion of the Keystone XL line, oil pipelines to the U.S. may run nearly half-empty. The expected lack of volume combined with extensive construction cost overruns has prompted several petroleum refining companies to sue TransCanada. Suncor Energy hoped to recoup significant construction-related tolls, though the U.S. Energy Regulatory Commission did not rule in their favor. According to \"The Globe and Mail\",\n\nThe refiners argue that construction overruns have raised the cost of shipping on the Canadian portion of Keystone by 145 per cent while the U.S. portion has run 92 per cent over budget. They accuse TransCanada of misleading them when they signed shipping contracts in the summer of 2007. TransCanada nearly doubled its construction estimates in October 2007, from $2.8-billion (U.S.) to $5.2-billion.\n\nDue to an exemption the state of Kansas gave TransCanada, the local authorities would lose $50 million public revenue from property taxes for a decade.\n\nIn the United States, Democrats are concerned that Keystone XL would not provide petroleum products for domestic use, but simply facilitate getting Alberta oil sands products to American coastal ports on the Gulf of Mexico for export to China and other countries. In January 2015, Senate Republicans blocked a vote on an amendment proposed by Senator Edward J. Markey, D-Mass., which would have banned exports from the Keystone XL pipeline and required that the pipeline be built with steel from the United States.\n\nFrustrated by delays in getting approval for Keystone XL (via the Gulf of Mexico), the Enbridge Northern Gateway Pipelines (via Kitimat, BC) and the expansion of the existing TransMountain line to Vancouver, Alberta has intensified exploration of two northern projects \"to help the province get its oil to tidewater, making it available for export to overseas markets\". Canadian Prime Minister Stephen Harper, spent $9 million by May 2012 and $16.5 million by May 2013 to promote Keystone XL. Until Canadian crude oil accesses international prices like LLS or Maya crude oil by \"getting to tidewater\" (south to the U.S. Gulf ports via Keystone XL for example, west to the BC Pacific coast via the proposed Northern Gateway line to ports at Kitimat, BC or north via the northern hamlet of Tuktoyaktuk, near the Beaufort Sea), the Alberta government (and to some extent, the Canadian government) is losing from $4 – 30 billion in tax and royalty revenues as the primary product of the oil sands, Western Canadian Select (WCS), the bitumen crude oil basket, is discounted so heavily against West Texas Intermediate (WTI) while Maya crude oil, a similar product close to tidewater, is reaching peak prices. Calgary-based Canada West Foundation warned in April 2013, that Alberta is \"running up against a [pipeline capacity] wall around 2016, when we will have barrels of oil we can't move\".\n\nPipeline opponents warn of disruption of farms and ranches during construction, and point to damage to water mains and sewage lines sustained during construction of an Enbridge crude oil pipeline in Michigan. A report by the Cornell University Global Labor Institute noted of the 2010 Enbridge Tar Oil Spill along the Kalamazoo River in Michigan: \"The experience of Kalamazoo residents and businesses provides an insight into some of the ways a community can be affected by a tar sands pipeline spill. Pipeline spills are not just an environmental concern. Pipeline spills can also result in significant economic and employment costs, although the systematic tracking of the social, health, and economic impacts of pipeline spills is not required by law. Leaks and spills from Keystone XL and other tar sands and conventional crude pipelines could put existing jobs at risk..\"\n\nA \"USA Today\" editorial pointed out that the 2013 Lac-Mégantic derailment in Quebec, in which crude oil carried by rail cars exploded and killed 47 people, highlights the safety of pipelines compared to truck or rail transport. The oil in the Lac-Mégantic rail cars came from the Bakken Formation in North Dakota, an area that would be served by the Keystone expansion. Increased oil production in North Dakota has exceeded pipeline capacity since 2010, leading to increasing volumes of crude oil being shipped by truck or rail to refineries. Canadian journalist Diana Furchtgott-Roth commented: \"If this oil shipment had been carried through pipelines, instead of rail, families in Lac-Mégantic would not be grieving for lost loved ones today, and oil would not be polluting Lac Mégantic and the Chaudière River.\" A \"Wall Street Journal\" article in March 2014 points out that the main reason oil producers from the North Dakota Bakken Shale region are using rail and trucks to transport oil is economics not pipeline capacity. The Bakken oil is of a higher quality than the Canadian sand oil and can be sold to east coast refinery at a premium that they would not get sending it to Gulf refineries. The article goes on to state that there is little support remaining among these producers for the Keystone XL.\n\nOn November 6, 2015, President Obama rejected Keystone XL citing the urgency of climate change as a key reason behind his decision.\n\nPublic opinion polls taken by independent national polling organizations toward the beginning of the dispute showed majority support for the proposed pipeline in the US. A September 2013 poll by the Pew Center found 65% favored the project and 30% opposed. The same poll found the pipeline favored by majorities of men (69%), women (61%), Democrats (51%), Republicans (82%), independents (64%), as well as by those in every division of age, education, economic status, and geographic region. The only group identified by the Pew poll with less than majority support for the pipeline was among those Democrats who identified themselves as liberal (41% in favor versus 54% opposed). In contrast, the same polling organization published results of a February 2017 poll showing that support for the pipeline had fallen to only 42% by the beginning of 2017, with 48% of polled respondents opposing the pipeline. The Pew Center reported a 17 percentage point drop in support since 2014, with the majority of the shift due to a sharp decline in support among Democrats and Democrat-leaning independents. At the time of the poll, only 17% of Democrats favored the pipeline. Support among Republicans had also fallen (to 76%) but by no means as steeply as among Democrats.\n\nThe overall results of polls on the Keystone XL pipeline taken by independent national polling organizations prior to 2014 are as follows:\n\nIn 2011, environmental and global warming activist Bill McKibben took the question of the pipeline to NASA scientist James Hansen, who told McKibben the pipeline would be \"game over for the planet\". McKibben and other activists organized opposition, which coalesced in August 2011 with over 1000 nonviolent arrests at the White House, which included environmental leaders such as Phil Radford and celebrities including Daryl Hannah. They promised to continue to challenge President Obama to stand by his 2008 call to \"be the generation that finally frees America from the tyranny of oil\" as he entered the 2012 reelection campaign. A relatively broad coalition came together, including the Republican governor Dave Heineman and senators Ben Nelson and Mike Johanns from Nebraska, and some Democratic funders such as Susie Tompkins Buell.\n\nOn November 6, 2011, several thousand environmentalist supporters, some shouldering a long black inflatable replica of a pipeline, formed a human chain around the White House to convince Barack Obama to block the controversial Keystone XL project. Organizer Bill McKibben said, \"this has become not only the biggest environmental flash point in many, many years, but maybe the issue in recent times in the Obama administration when he's been most directly confronted by people in the street. In this case, people willing, hopeful, almost dying for him to be the Barack Obama of 2008.\"\n\nOn October 4, 2012, actress and activist Daryl Hannah and 78-year-old Texas landowner Eleanor Fairchild were arrested for criminal trespassing and other charges after they were accused of standing in front of pipeline construction equipment on Fairchild's farm in Winnsboro, a town about east of Dallas. Fairchild has owned the land since 1983 and refused to sign any agreements with TransCanada. Her land was seized by eminent domain.\n\nOn October 31, 2012, Green Party presidential candidate Jill Stein was also arrested in Texas for criminal trespass after trying to deliver food and supplies to the Keystone XL protesters.\n\nOn February 17, 2013, approximately 35,000 to 50,000 protestors attended a rally in Washington, D.C. organized by The Sierra Club, 350.org, and The Hip Hop Caucus, in what Bill McKibben described as \"the biggest climate rally by far, by far, by far, in U.S. history\". The event featured Lennox Yearwood; Chief Jacqueline Thomas, immediate past chief of the Saik'uz First Nation; Van Jones; Crystal Lameman, of Beaver Lake Cree Nation; Michael Brune, Sen. Sheldon Whitehouse (D-RI), and others as invited speakers.\nSimultaneous 'solidarity' protests were also organized in several other cities across the United States, Europe, and Canada. Protesters called on President Obama to reject the planned pipeline extension when deciding the fate of the pipeline after Secretary of State John Kerry completes a review of the project.\n\n\"[B]ecause of broader market dynamics and options for crude oil transport in the North American logistics system, the upstream and downstream activities are unlikely to be substantially different whether or not the proposed Project is constructed.\"\n\nOn March 2, 2014, approximately 1000–1200 protesters marched from Georgetown University to the White House to stage a protest against the Keystone Pipeline. 398 arrests were made of people tying themselves to the White House fence with zip-ties and lying on a black tarp in front of the fence. The tarp represented an oil spill, and many protesters dressed in white jumpsuits covered in black ink, symbolizing oil-covered HazMat suits, laid down upon the tarp.\n\nOn November 16, 2011, Enbridge announced it was buying ConocoPhillips' 50% interest in the Seaway pipeline that flowed from the Gulf of Mexico to the Cushing hub. In cooperation with Enterprise Products Partners LP it is reversing the Seaway pipeline so that an oversupply of oil at Cushing can reach the Gulf. This project replaced the earlier proposed alternative Wrangler pipeline project from Cushing to the Gulf Coast. It began reversed operations on May 17, 2012. However, according to industries, the Seaway line alone is not enough for oil transportation to the Gulf Coast.\n\nOn January 19, 2012, TransCanada announced it may shorten the initial path to remove the need for federal approval. TransCanada said that work on that section of the pipeline could start in June 2012 and be on-line by the middle to late 2013.\n\nIn April 2013, it was learned that the government of Alberta was investigating, as an alternative to the pipeline south through the United States, a shorter all-Canadian pipeline north to the Arctic coast, from where the oil would be taken by tanker ships through the Arctic Ocean to markets in Asia and Europe and in August, TransCanada announced a new proposal to create a longer all-Canada pipeline, called Energy East, that would extend as far east as the port city of Saint John, New Brunswick, at the same time providing feedstock to refineries in Montreal, Quebec City and Saint John.\n\nThe Enbridge \"Alberta Clipper\" expansion of the existing cross-border Line 67 pipeline has been continuing since late 2013. When completed it will add 350,000 bpd new capacity to the existing pipeline for cumulative total of 800,000 bpd. In late 2014 Enbridge announced it is awaiting final approval from the US State Department and expects to proceed with the last phase in mid-2015.\n\nIn September 2009, independent refiner CVR sued TransCanada for Keystone Pipeline tolls seeking $250 million damage compensation or release from transportation agreements. CVR alleged that the final tolls for the Canadian segment of the pipeline were 146% higher than initially presented, while the tolls for the U.S. segment were 92% higher. In April 2010, three smaller refineries sued TransCanada to break Keystone transportation contracts, saying the new pipeline has been beset with cost overruns.\n\nIn October 2009, a suit was filed by the Natural Resources Defense Council that challenged the pipeline on the grounds that its permit was based on a deficient environmental impact statement. The suit was thrown out by a federal judge on procedural grounds, ruling that the NRDC lacked the authority to bring it.\n\nIn June 2012, Sierra Club, Inc., Clean Energy Future Oklahoma, and the East Texas Sub Regional Planning Commission filed a joint complaint in the United States District Court for the Western District of Oklahoma seeking injunctive relief and petitioning for a review of the U.S. Army Corps of Engineers' action in issuing Nationwide Permit 12 permits for the Cushing, Oklahoma, to the Gulf Coast portion of the pipeline. The suit alleges that, contrary to the federal Administrative Procedure Act, 5 U.S.C. § 701 et. seq., the Corps' issuance of the permits was arbitrary and capricious and an abuse of discretion.\n\nIn early January 2016, TransCanada announced it would initiate an ISDS claim under NAFTA against the United States, seeking $15 billion in damages and calling the denial of a permit for Keystone XL \"arbitrary and unjustified.\"\n\n\n"}
{"id": "25580539", "url": "https://en.wikipedia.org/wiki?curid=25580539", "title": "Korea Plant Service &amp; Engineering", "text": "Korea Plant Service &amp; Engineering\n\nKorea Plant Service & Engineering or KPS is a South Korean public enterprise established in 1974 to provide electronic power and industrial facilities. \n\n"}
{"id": "20385531", "url": "https://en.wikipedia.org/wiki?curid=20385531", "title": "Lampas", "text": "Lampas\n\nLampas is a type of luxury fabric with a background weft (a \"ground weave\") typically in taffeta with supplementary wefts (the \"pattern wefts\") laid on top and forming a design, sometimes also with a \"brocading weft\". Lampas is typically woven in silk, and often has gold and silver thread enrichment.\n\nLampas weaves were developed around 1000 CE. Beginning late in the 17th century western lampas production began centered in Lyon, France, where an industry of providing for French and other European courts became centered. \n\n"}
{"id": "3855235", "url": "https://en.wikipedia.org/wiki?curid=3855235", "title": "Lithium triborate", "text": "Lithium triborate\n\nLithium triborate (LiBO) or LBO is a non-linear optics crystal. It has a wide transparency range, moderately high nonlinear coupling, high damage threshold and desirable chemical and mechanical properties. This crystal is often used for second harmonic generation (SHG, also known as \"frequency doubling\"), for example of s (1064 nm → 532 nm). LBO can be both critically and non-critically phase-matched. In the latter case the crystal has to be heated or cooled depending on the wavelength.\n\nLithium triborate was discovered and developed by Chen Chuangtian and others of the Fujian Institute of Research on the Structure of Matter, Chinese Academy of Sciences. It has been patented.\n\n\nLithium triborate (LBO) crystals are applicable in various nonlinear optical applications:\n\n"}
{"id": "20731635", "url": "https://en.wikipedia.org/wiki?curid=20731635", "title": "Lyate ion", "text": "Lyate ion\n\nIn chemistry, a lyate ion is the anion derived by the deprotonation of a solvent molecule. For example, a hydroxide ion is formed from the deprotonation of water and methoxide (CHO) is the anion formed by the deprotonation of methanol. See also lyonium ion which is the cation formed by the protonation of a solvent molecule.\n\n"}
{"id": "6803333", "url": "https://en.wikipedia.org/wiki?curid=6803333", "title": "Miltefosine", "text": "Miltefosine\n\nMiltefosine, sold under the trade name Impavido among others, is a medication mainly used to treat leishmaniasis and free-living amoeba infections such as \"Naegleria fowleri\". This includes leishmaniasis of the cutaneous, visceral, and mucosal types. It may be used together with liposomal amphotericin B or paromomycin. It is taken by mouth.\nCommon side effects include vomiting, abdominal pain, fever, headaches, and decreased kidney function. More severe side effects may include Stevens-Johnson syndrome or low blood platelets. Use during pregnancy appears to cause harm to the baby and use during breastfeeding is not recommended. How it works is not entirely clear.\nMiltefosine was first made in the early 1980s and studied as a treatment for cancer. A few years later it was found to be useful for leishmaniasis and was approved for this use in 2002 in India. It is on the World Health Organization's List of Essential Medicines, the most effective and safe medicines needed in a health system. In the developing world a course of treatment costs US$65 to $150. In the developed world treatment may be 10 to 50 times greater.\n\nMiltefosine is primarily used for the treatment of visceral and New World cutaneous leishmaniasis, and is undergoing clinical trials for this use in several countries. This drug is now listed as a core medication for the treatment of leishmaniasis under the WHO Model List of Essential Medicines. Several medical agents have some efficacy against visceral or cutaneous leishmaniasis, however, a 2005 survey concluded that miltefosine is the only effective oral treatment for both forms of leishmaniasis.\n\nIn addition, it has been used successfully in some cases of the very rare, but highly lethal, brain infection by the amoeba, \"Naegleria fowleri\", acquired through water entering the nose during a plunge in contaminated water. It has orphan drug status in the United States for acanthamoeba keratitis and primary amebic meningoencephalitis (PAM).\n\nIt is active against some bacteria and fungi, as well as human trematode \"Schistosoma mansoni\" and the snail that spreads it \"Biomphalaria alexandrina\".\n\nMiltefosine is listed as pregnancy category D by the FDA. This means there is evidence-based adverse reaction data from investigational or marketing experience or studies in humans of harm to the human fetus. Despite this evidence, the potential benefits of miltefosine may warrant use of the drug in pregnant women despite potential risks. A pregnancy test should be done prior to starting treatment. Effective birth control should be used while on miltefosine and 5 months after discontinuation of treatment. Its use during breast feeding is most likely unsafe.\n\nMiltefosine is contraindicated in individuals who have a hypersensitivity to this medication, pregnant women, and people who have the Sjögren-Larsson syndrome. It is embryotoxic and fetotoxic in rats and rabbits, and teratogenic in rats but not in rabbits. It is therefore contraindicated for use during pregnancy, and contraception is required beyond the end of treatment in women of child-bearing age.\n\nCommon side effects from miltefosine treatment are nausea and vomiting, which occur in 60% of people. Other common side effects are dizziness, headache, and daytime sleepiness.\n\nSerious side effects include rash, diarrhea, and arthritis. The side effects are more severe in women and young children. The overall effects are quite mild and easily reversed.\n\nMiltefosine primarily acts on \"Leishmania\" by affecting the species's promastigote and amastigote stages. Miltefosine exerts its activity by interacting with lipids, inhibiting cytochrome c oxidase and causing apoptosis-like cell death. This may affect membrane integrity and mitochondrial function of the parasite.\n\nWhile initially studied as a cancer medication, due to side effects it was never used for this purpose.\n\nPhospholipid group alkylphosphocholine were known since the early 1980s, particularly in terms of their binding affinity with cobra venom. In 1987 the phospholipids were found to be potent toxins on leukemic cell culture. Initial \"in vivo\" investigation on the antineoplastic activity showed positive result, but then only at high dosage and at high toxicity. At the same time in Germany, Hansjörg Eibl, at the Max Planck Institute for Biophysical Chemistry, and Clemens Unger, at the University of Göttingen, demonstrated that the antineoplastic activity of the phospholipid analogue miltefosine (at the time known as hexadecylphosphocholine) was indeed tumour-specific. It was highly effective against methylnitrosourea-induced mammary carcinoma, but less so on transplantable mammary carcinomas and autochthonous benzo(a)pyrene-induced sarcomas, and relatively inactive on Walker 256 carcinosarcoma and autochthonous acetoxymethylmethylnitrosamine-induced colonic tumors of rats. It was subsequently found that miltefosine was structurally unique among lipids having anticancer property in that it lacks the glycerol group, is highly selective on cell types and acts through different mechanism.\n\nIn the same year as the discovery of the anticancer property, miltefosine was reported by S. L. Croft and his team at the London School of Hygiene and Tropical Medicine as having antileishmanial effect as well. The compound was effective against \"Leishmania donovani\" amastigotes in cultured mouse peritoneal macrophages at a dose of 12.8 mg/kg/day in a five-day course. However, priority was given to the development of the compound for cutaneous metastases of breast cancer. In 1992 a new research was reported in which the compound was highly effective in mouse against different life cycle stages of different \"Leishmania\" species, and in fact, more potent than the conventional sodium stibogluconate therapy by a factor of more than 600. Results of the first clinical trial in humans were reported from Indian patients with chronic leishmaniasis with high degree of success and safety. This promising development promulgated a unique public–private partnership collaboration between ASTA Medica (later Zentaris GmbH), the WHO Special Programme for Research and Training in Tropical Diseases, and the Government of India. Eventually, several successful Phase II and III trials led to the approval of miltefosine in 2002 as the first and only oral drug for leishmaniasis.\n\nIn 2013, the US Centers for Disease Control and Prevention recommended miltefosine for the treatment of free-living amebae infections such as granulomatous amoebic encephalitis and primary amoebic meningoencephalitis, two fatal protozoal diseases. Historically, only four survivors have been recorded out of 138 confirmed infections in North America. One American survived the infection in 1978 and one individual from Mexico in 2003. In 2013, two children survived and recovered from primary amoebic meningoencephalitis after treatment with miltefosine. In 2016 after treatment that included miltefosine, another child became the fourth person in the United States to survive \"Naegleria fowleri\" infection.\n\nMiltefosine is commercially available in the United States through Profounda. Previously one could only get it from the CDC for emergency use under an expanded access IND protocol for treatment of free-living amoeba (FLA) infections): primary amoebic meningoencephalitis caused by \"Naegleria fowleri\" and granulomatous amoebic encephalitis caused by \"Balamuthia mandrillaris\", and \"Acanthamoeba\" species. Miltefosine is also produced by Profounda, a private pharmaceutical company.\n\nMiltefosine is being investigated by researchers interested in finding treatments for infections which have become resistant to existing drugs. Animal and \"in vitro\" studies suggest it may have broad anti-protozoal and anti-fungal properties:\n\nMiltefosine targets HIV infected macrophages, which play a role in vivo as long-lived HIV-1 reservoirs. The HIV protein Tat activates pro-survival PI3K/Akt pathway in primary human macrophages. Miltefosine acts by inhibiting the PI3K/Akt pathway, thus removing the infected macrophages from circulation, without affecting healthy cells. It significantly reduces replication of HIV-1 in cocultures of human dendritic cells (DCs) and CD4 T cells, which is due to a rapid secretion of soluble factors and is associated with induction of type-I interferon (IFN) in the human cells.\n\n"}
{"id": "496572", "url": "https://en.wikipedia.org/wiki?curid=496572", "title": "Nuclear propulsion", "text": "Nuclear propulsion\n\nNuclear propulsion includes a wide variety of propulsion methods that fulfill the promise of the Atomic Age by using some form of nuclear reaction as their primary power source. The idea of using nuclear material for propulsion dates back to the beginning of the 20th century. In 1903 it was hypothesised that radioactive material, radium, might be a suitable fuel for engines to propel cars, boats, and planes. H. G. Wells picked up this idea in his 1914 fiction work \"The World Set Free\". \n\nNuclear-powered vessels are mainly military submarines, and aircraft carriers. Russia and America are the only countries that currently have nuclear-powered civilian surface ships, including icebreakers and Aircraft carriers. America currently (as of July 2018) has 11 Aircraft carriers in service, and all are powered by nuclear reactors. They use nuclear reactors as their power plants. For more detailed articles see:\n\n\n\nRussia's Channel One Television news broadcast a picture and details of a nuclear-powered torpedo called \"Status-6\" on about 12 November 2015. The torpedo was stated as having a range of up to 10,000 km, a cruising speed of 100 knots, and operational depth of up to 1000 metres below the surface. The torpedo carried a 100-megaton nuclear warhead.\n\nOne of the suggestions emerging in the summer of 1958 from the first meeting of the scientific advisory group that became JASON was for \"a nuclear-powered torpedo that could roam the seas almost indefinitely\". \n\nResearch into nuclear-powered aircraft was pursued during the Cold War by the United States and the Soviet Union as they would presumably allow a country to keep nuclear bombers in the air for extremely long periods of time, a useful tactic for nuclear deterrence. Neither country created any operational nuclear aircraft. One design problem, never adequately solved, was the need for heavy shielding to protect the crew from radiation sickness. Since the advent of ICBMs in the 1960s the tactical advantage of such aircraft was greatly diminished and respective projects were cancelled. Because the technology was inherently dangerous it was not considered in non-military contexts. Nuclear-powered missiles were also researched and discounted during the same period.\n\nAircraft\nMissiles\n\nMany types of nuclear propulsion have been proposed, and some of them (e.g. NERVA) tested for spacecraft applications.\n\n\n\n\n\n\nAnatolij Perminov, head of the Russian Federal Space Agency, announced that it is going to develop a nuclear-powered spacecraft for deep space travel. Preliminary design was done by 2013, and 9 more years are planned for development (in space assembly). The price is set at 17 billion rubles (600 million dollars).\nThe nuclear propulsion would have mega-watt class, provided necessary funding, Roscosmos Head stated.\n\nThis system would consist of a space nuclear power and a matrix of ion engines. \"...Hot inert gas temperature of 1500 °C from the reactor turns turbines. The turbine turns the generator and compressor, which circulates the working fluid in a closed circuit. The working fluid is cooled in the radiator. The generator produces electricity for the same ion (plasma) engine...\" \n\nAccording to him, the propulsion will be able to support human mission to Mars, with cosmonauts staying on the Red planet for 30 days. This journey to Mars with nuclear propulsion and a steady acceleration would take six weeks, instead of eight months by using chemical propulsion – assuming thrust of 300 times higher than that of chemical propulsion.\n\nThe idea of making cars that used radioactive material, radium, for fuel dates back to at least 1903. Analysis of the concept in 1937 indicated that the driver of such a vehicle might need a 50-ton lead barrier to shield them from radiation.\n\nIn 1941 Dr R M Langer, a Caltech physicist, espoused the idea of a car powered by uranium-235 in the January edition of \"Popular Mechanics\". He was followed by William Bushnell Stout, designer of the Stout Scarab and former Society of Engineers president, on 7 August 1945 in the \"New York Times\". The problem of shielding the reactor continued to render the idea impractical. In December 1945, a John Wilson of London, announced he had created an atomic car. This created considerable interest. The Minister of Fuel and Power along with a large press contingent turned out to view it. The car did not show and Wilson claimed that it had been sabotaged. A later court case found that he was a fraud and there was no nuclear-powered car.\n\nDespite the shielding problem, through the late 1940s and early 1950s debate continued around the possibility of nuclear-powered cars. The development of nuclear-powered submarines and ships, and experiments to develop a nuclear-powered aircraft at that time kept the idea alive. Russian papers in the mid-1950s reported the development of a nuclear-powered car by Professor V P Romadin, but again shielding proved to be a problem. It was claimed that its laboratories had overcome the shielding problem with a new alloy that absorbed the rays.\n\nIn 1958 at the height of the 1950s American automobile culture there were at least four theoretical nuclear-powered concept cars proposed, the American Ford Nucleon and Studebaker Packard Astral, as well as the French Simca Fulgur designed by Robert Opron and the Arbel. Apart from these concept models, none were built and no automotive nuclear power plants ever made. Chrysler engineer C R Lewis had discounted the idea in 1957 because of estimates that an engine would be required by a car. His view was that an efficient means of storing energy was required for nuclear power to be practical. Despite this, Chrysler's stylists in 1958 drew up some possible designs.\n\nIn 1959 it was reported that Goodyear Tire and Rubber Company had developed a new rubber compound that was light and absorbed radiation, obviating the need for heavy shielding. A reporter at the time considered it might make nuclear-powered cars and aircraft a possibility.\n\nFord made another potentially nuclear-powered model in 1962 for the Seattle World's Fair, the Ford Seattle-ite XXI. This also never went beyond the initial concept.\n\nIn 2009, for the hundredth anniversary of General Motors' acquisition of Cadillac, Loren Kulesus created concept art depicting a car powered by thorium.\n\nThe Chrysler TV-8 was an experimental concept tank designed by Chrysler in the 1950s. The tank was intended to be a nuclear-powered medium tank capable of land and amphibious warfare. The design was never mass-produced. The Mars rover Curiosity is powered by a radioisotope thermoelectric generator (RTG), like the successful Viking 1 and Viking 2 Mars landers in 1976.\n\n\n\n"}
{"id": "15213586", "url": "https://en.wikipedia.org/wiki?curid=15213586", "title": "Operational sex ratio", "text": "Operational sex ratio\n\nIn the evolutionary biology of sexual reproduction, operational sex ratio (OSR) is the ratio of sexually competing males that are ready to mate to sexually competing females that are ready to mate, or alternatively the local ratio of fertilizable females to sexually active males at any given time. This differs from physical sex ratio which simply includes all individuals, including those that are sexually inactive or do not compete for mates.\n\nThe theory of OSR hypothesizes that the operational sex ratio affects the mating competition of males and females in a population. This concept is especially useful in the study of sexual selection since it is a measure of how intense sexual competition is in a species, and also in the study of the relationship of sexual selection to sexual dimorphism. The OSR is closely linked to the \"potential rate of reproduction\" of the two sexes; that is, how fast they each could reproduce in ideal circumstances. Usually variation in potential reproductive rates creates bias in the OSR and this in turn will affect the strength of selection. The OSR is said to be biased toward a particular sex when sexually ready members of that sex are more abundant. For example, a male-biased OSR means that there are more sexually competing males than sexually competing females.\n\nThe operational sex ratio is affected by the length of time each sex spends in caring for young or in recovering from mating. For example, if females cease mating activity to care for young, but males do not, then more males would be ready to mate, thus creating a male biased OSR. One aspect of gestation and recovery time would be clutch loss. Clutch loss is when offspring or a group of offspring is lost, due to an accident, predation, etc. This, in turn, effects how long reproductive cycles will be in both males and females. If the males were to invest more time in the care of their offspring, they would be spending less time mating. This pushes the population towards a female biased OSR and vice versa. Whether or not it is the males or females investing more care in their offspring, if they were to lose their offspring for whatever reason, this would then change the OSR to be less biased because the once occupied sex becomes available to mate again.\n\nAs aforementioned, another major factor that influences OSR is potential rate of reproduction (PRR). Any sexual differences in the PRR will also change the OSR, so it is important to look at factors that change PRR as well. These include constraints to environmental factors such as food or nesting sites. For example, if males are required to provide a nutrient high gift before mating (most likely food) then when nutrients available is high, the OSR will be male biased because there is plenty of nutrients available to provide gifts. However, if nutrients is low, less males will be ready to reproduce, causing the population to have a female biased OSR. Another example would be if, in a certain species, males provided care for offspring and a nest. If the availability of nesting sites decreased, we would see the population trend towards a more female biased OSR because only a small number of males actually have a nest while all the females, regardless of a nest or not, are still producing eggs.\n\nA major factor that OSR can predict is the opportunity for sexual selection. As the OSR becomes more biased, the sex that is in excess will tend to undergo more competition for mates and therefore undergo strong sexual selection.\nIntensity of competition is also a factor that can be predicted by OSR. According to sexual selection theory, whichever sex is more abundant is expected to compete more strongly and the sex that is less abundant is expected to be \"choosier\" in who they decide to mate with. It would be expected that when an OSR is more biased to one sex than the other, that one would observe more interaction and competition from the sex that is more available to mate. When the population is more female biased, more female-female competition is observed and the opposite is seen for a male population where a male biased would cause more male-male interaction and competitiveness. Though both sexes may be competing for mates, it is important to remember that the biased OSR predicts which sex is the predominant competitor (the sex that exhibits the most competition). OSR can also predict what will happen to mate guarding in a population. As OSR becomes more biased to one sex, it can be observed that mate-guarding will increase. This is likely due to the fact that rival numbers (number of a certain sex that are also ready to mate) are increased. If a population is male biased then there are a lot more rival males to compete for a mate, meaning that those who have a mate already are more likely to guard the mate that they have.\n\n"}
{"id": "12414930", "url": "https://en.wikipedia.org/wiki?curid=12414930", "title": "Oxy-fuel welding and cutting", "text": "Oxy-fuel welding and cutting\n\nOxy-fuel welding (commonly called oxyacetylene welding, oxy welding, or gas welding in the U.S.) and oxy-fuel cutting are processes that use fuel gases and oxygen to weld and cut metals, respectively. French engineers Edmond Fouché and Charles Picard became the first to develop oxygen-acetylene welding in 1903. Pure oxygen, instead of air, is used to increase the flame temperature to allow localized melting of the workpiece material (e.g. steel) in a room environment. A common propane/air flame burns at about , a propane/oxygen flame burns at about , an oxyhydrogen flame burns at , and an acetylene/oxygen flame burns at about .\n\nDuring the early 20th century, before the development and availability of coated arc welding electrodes in the late 1920s that were capable of making sound welds in steel, oxy-acetylene welding was the only process capable of making welds of exceptionally high quality in virtually all metals in commercial use at the time. This not only included carbon steel but also alloy steels, cast iron, aluminum, and magnesium. In recent decades it has been obsolesced in almost all industrial uses due to various arc welding methods offering greater speed and, in the case of gas tungsten arc welding, the capability of welding very reactive metals such as titanium. Oxy-acetylene welding is still used for metal-based artwork and in smaller home based shops, as well as situations where accessing electricity (e.g., via an extension cord or portable generator) would present difficulties. The oxy-acetylene (and other oxy-fuel gas mixtures) welding torch remains a mainstay heat source for manual brazing and braze welding, as well as metal forming, preparation, and localized heat treating. In addition, oxy-fuel cutting is still widely used, both in heavy industry as well as light industrial and repair. \n\nIn oxy-fuel welding, a welding torch is used to weld metals. Welding metal results when two pieces are heated to a temperature that produces a shared pool of molten metal. The molten pool is generally supplied with additional metal called filler. Filler material depends upon the metals to be welded.\n\nIn oxy-fuel cutting, a torch is used to heat metal to its kindling temperature. A stream of oxygen is then trained on the metal, burning it into a metal oxide that flows out of the kerf as slag.\n\nTorches that do not mix fuel with oxygen (combining, instead, atmospheric air) are not considered oxy-fuel torches and can typically be identified by a single tank (oxy-fuel cutting requires two isolated supplies, fuel and oxygen). Most metals cannot be melted with a single-tank torch. Consequently, single-tank torches are typically suitable for soldering and brazing but not for welding.\n\nOxy-fuel torches are or have been used for:\n\n\nIn short, oxy-fuel equipment is quite versatile, not only because it is preferred for some sorts of iron or steel welding but also because it lends itself to brazing, braze-welding, metal heating (for annealing or tempering, bending or forming), rust or scale removal, the loosening of corroded nuts and bolts, and is a ubiquitous means of cutting ferrous metals.\n\nThe apparatus used in gas welding consists basically of an oxygen source and a fuel gas source (usually contained in cylinders), two pressure regulators and two flexible hoses (one for each cylinder), and a torch. This sort of torch can also be used for soldering and brazing. The cylinders are often carried in a special wheeled trolley.\n\nThere have been examples of oxyhydrogen cutting sets with small (scuba-sized) gas cylinders worn on the user's back in a backpack harness, for rescue work and similar.\n\nThere are also examples of pressurized liquid fuel cutting torches, usually using gasoline. These are used for their increased portability.\n\nThe regulator ensures that pressure of the gas from the tanks matches the required pressure in the hose. The flow rate is then adjusted by the operator using needle valves on the torch. Accurate flow control with a needle valve relies on a constant inlet pressure.\n\nMost regulators have two stages. The first stage is a fixed-pressure regulator, which releases gas from the cylinder at a constant intermediate pressure, despite the pressure in the cylinder falling as the gas in it is consumed. This is similar to the first stage of a scuba-diving regulator. The adjustable second stage of the regulator controls the pressure reduction from the intermediate pressure to the low outlet pressure. The regulator has two pressure gauges, one indicating cylinder pressure, the other indicating hose pressure. The adjustment knob of the regulator is sometimes roughly calibrated for pressure, but an accurate setting requires observation of the gauge.\n\nSome simpler or cheaper oxygen-fuel regulators have only a single stage regulator, or only a single gauge. A single-stage regulator will tend to allow a reduction in outlet pressure as the cylinder is emptied, requiring manual readjustment. For low-volume users, this is an acceptable simplification. Welding regulators, unlike simpler LPG heating regulators, retain their outlet (hose) pressure gauge and do not rely on the calibration of the adjustment knob. The cheaper single-stage regulators may sometimes omit the cylinder contents gauge, or replace the accurate dial gauge with a cheaper and less precise \"rising button\" gauge.\n\nThe hoses are designed for use in welding and cutting metal. A double-hose or twinned design can be used, meaning that the oxygen and fuel hoses are joined together. If separate hoses are used, they should be clipped together at intervals approximately apart, although that is not recommended for cutting applications, because beads of molten metal given off by the process can become lodged between the hoses where they are held together, and burn through, releasing the pressurised gas inside, which in the case of fuel gas usually ignites.\n\nThe hoses are color-coded for visual identification. The color of the hoses varies between countries. In the United States, the oxygen hose is green, and the fuel hose is red. In the UK and other countries, the oxygen hose is blue (black hoses may still be found on old equipment), and the acetylene (fuel) hose is red. If liquefied petroleum gas (LPG) fuel, such as propane, is used, the fuel hose should be orange, indicating that it is compatible with LPG. LPG will damage an incompatible hose, including most acetylene hoses.\n\nThe threaded connectors on the hoses are handed to avoid accidental mis-connection: the thread on the oxygen hose is right-handed (as normal), while the fuel gas hose has a left-handed thread. The left-handed threads also have an identifying groove cut into their nuts.\n\nGas-tight connections between the flexible hoses and rigid fittings are made by using crimped hose clips or ferrules, often referred to as 'O' clips, over barbed spigots. The use of worm-drive hose clips or Jubilee clips is specifically forbidden in the UK and other countries.\n\nAcetylene is not just flammable; in certain conditions it is explosive. Although it has an upper flammability limit in air of 81%, acetylene's explosive decomposition behaviour makes this irrelevant. If a detonation wave enters the acetylene tank, the tank will be blown apart by the decomposition. Ordinary check valves that normally prevent back flow cannot stop a detonation wave because they are not capable of closing before the wave passes around the gate. For that reason a flashback arrestor is needed. It is designed to operate before the detonation wave makes it from the hose side to the supply side.\n\nBetween the regulator and hose, and ideally between hose and torch on both oxygen and fuel lines, a flashback arrestor and/or non-return valve (check valve) should be installed to prevent flame or oxygen-fuel mixture being pushed back into either cylinder and damaging the equipment or causing a cylinder to explode.\n\nEuropean practice is to fit flashback arrestors at the regulator and check valves at the torch. US practice is to fit both at the regulator.\n\nThe flashback arrestor (not to be confused with a check valve) prevents shock waves from downstream coming back up the hoses and entering the cylinder, possibly rupturing it, as there are quantities of fuel/oxygen mixtures inside parts of the equipment (specifically within the mixer and blowpipe/nozzle) that may explode if the equipment is incorrectly shut down, and acetylene decomposes at excessive pressures or temperatures. In case the pressure wave has created a leak downstream of the flashback arrestor, it will remain switched off until someone resets it.\n\nA check valve lets gas flow in one direction only. It is usually a chamber containing a ball that is pressed against one end by a spring. Gas flow one way pushes the ball out of the way, and a lack of flow or a reverse flow allows the spring to push the ball into the inlet, blocking it. Not to be confused with a flashback arrestor, a check valve is not designed to block a shock wave. The shock wave could occur while the ball is so far from the inlet that the wave will get past the ball before it can reach its off position.\n\nThe torch is the tool that the welder holds and manipulates to make the weld. It has a connection and valve for the fuel gas and a connection and valve for the oxygen, a handle for the welder to grasp, and a mixing chamber (set at an angle) where the fuel gas and oxygen mix, with a tip where the flame forms. Two basic types of torches are positive pressure type and low pressure or injector type.\n\nA welding torch head is used to weld metals. It can be identified by having only one or two pipes running to the nozzle, no oxygen-blast trigger, and two valve knobs at the bottom of the handle letting the operator adjust the oxygen and fuel flow respectively.\n\nA cutting torch head is used to cut materials. It is similar to a welding torch, but can be identified by the oxygen blast trigger or lever.\n\nWhen cutting, the metal is first heated by the flame until it is cherry red. Once this temperature is attained, oxygen is supplied to the heated parts by pressing the oxygen-blast trigger. This oxygen reacts with the metal, forming iron oxide and producing heat. It is the heat that continues the cutting process. The cutting torch only heats the metal to start the process; further heat is provided by the burning metal.\n\nThe melting point of the iron oxide is around half that of the metal being cut. As the metal burns, it immediately turns to liquid iron oxide and flows away from the cutting zone. However, some of the iron oxide remains on the workpiece, forming a hard \"slag\" which can be removed by gentle tapping and/or grinding.\n\nA rose bud torch is used to heat metals for bending, straightening, etc. where a large area needs to be heated. It is so-called because the flame at the end looks like a rose bud. A welding torch can also be used to heat small areas such as rusted nuts and bolts.\n\nA typical oxy-fuel torch, called an equal-pressure torch, merely mixes the two gases. In an injector torch, high-pressure oxygen comes out of a small nozzle inside the torch head which drags the fuel gas along with it, using the venturi effect.\n\nOxy-fuel processes may use a variety of fuel gases, the most common being acetylene. Other gases that may be used are propylene, liquified petroleum gas (LPG), propane, natural gas, hydrogen, and MAPP gas. Many brands use different kinds of gases in their mixes.\n\nAcetylene is the primary fuel for oxy-fuel welding and is the fuel of choice for repair work and general cutting and welding. Acetylene gas is shipped in special cylinders designed to keep the gas dissolved. The cylinders are packed with porous materials (e.g. kapok fibre, diatomaceous earth, or (formerly) asbestos), then filled to around 50% capacity with acetone, as acetylene is soluble in acetone. This method is necessary because above 207 kPa (30 lbf/in²) (absolute pressure) acetylene is unstable and may explode.\n\nThere is about 1700 kPa (250 psi) pressure in the tank when full. Acetylene when combined with oxygen burns at 3200 °C to 3500 °C (5800 °F to 6300 °F), highest among commonly used gaseous fuels. As a fuel acetylene's primary disadvantage, in comparison to other fuels, is high cost.\n\nAs acetylene is unstable at a pressure roughly equivalent to 33 feet/10 meters underwater, water submerged cutting and welding is reserved for hydrogen rather than acetylene.\n\nOxy-gasoline, also known as oxy-petrol, torches have been found to perform very well, especially where bottled gas fuel is not available or difficult to transport to the worksite. Tests showed that an oxy-gasoline torch can cut steel plate up to thick at the same rate as oxy-acetylene. In plate thicknesses greater than the cutting rate was better than oxy-acetylene; at it was three times faster.\n\nThe gasoline is fed either from a pressurised tank (whose pressure can be hand-pumped or fed from a gas cylinder). OR from a non pressurised tank with the fuel being drawn into the torch by venturi action by the pressurised oxygen flow. Another low cost approach commonly used by jewelry makers in Asia is using air bubbled through a gasoline container by a foot-operated air pump, and burning the fuel-air mixture in a specialized welding torch.\n\nHydrogen has a clean flame and is good for use on aluminium. It can be used at a higher pressure than acetylene and is therefore useful for underwater welding and cutting. It is a good type of flame to use when heating large amounts of material. The flame temperature is high, about 2,000 °C for hydrogen gas in air at atmospheric pressure, and up to 2800 °C when pre-mixed in a 2:1 ratio with pure oxygen (oxyhydrogen). Hydrogen is not used for welding steels and other ferrous materials, because it causes hydrogen embrittlement.\n\nFor some oxyhydrogen torches the oxygen and hydrogen are produced by electrolysis of water in an apparatus which is connected directly to the torch. Types of this sort of torch:\n\n\n\"Methylacetylene-propadiene\" (MAPP) \"gas\" and \"LPG gas\" are similar fuels, because LPG gas is liquefied petroleum gas mixed with MPS. It has the storage and shipping characteristics of LPG and has a heat value a little less than acetylene. Because it can be shipped in small containers for sale at retail stores, it is used by hobbyists and large industrial companies and shipyards because it does not polymerize at high pressures — above 15 psi or so (as acetylene does) and is therefore much less dangerous than acetylene. Further, more of it can be stored in a single place at one time, as the increased compressibility allows for more gas to be put into a tank. MAPP gas can be used at much higher pressures than acetylene, sometimes up to 40 or 50 psi in high-volume oxy-fuel cutting torches which can cut up to steel. Other welding gases that develop comparable temperatures need special procedures for safe shipping and handling. MPS and MAPP are recommended for cutting applications in particular, rather than welding applications.\n\nOn 31 April 2008 the Petromont Varennes plant closed its methylacetylene/propadiene crackers. As they were the only North American plant making MAPP gas, many substitutes were introduced by the companies who had repackaged the Dow and Varennes product(s) - most of these substitutes are propylene, see below.\n\nPropylene is used in production welding and cutting. It cuts similarly to propane. When propylene is used, the torch rarely needs tip cleaning. There is often a substantial advantage to cutting with an injector torch (see the propane section) rather than an equal-pressure torch when using propylene. Quite a few North American suppliers have begun selling propylene under proprietary trademarks such as FG2 and Fuel-Max.\n\nButane, like propane, is a saturated hydrocarbon. Butane and propane do not react with each other and are regularly mixed. Butane boils at 0.6 °C. Propane is more volatile, with a boiling point of -42 °C. Vaporization is rapid at temperatures above the boiling points. The calorific (heat) values of both are almost equal. Both are thus mixed to attain the vapor pressure that is required by the end user and depending on the ambient conditions. If the ambient temperature is very low, propane is preferred to achieve higher vapor pressure at the given temperature.\n\nPropane does not burn as hot as acetylene in its inner cone, and so it is rarely used for welding. Propane, however, has a very high number of BTUs per cubic foot in its outer cone, and so with the right torch (injector style) can make a faster and cleaner cut than acetylene, and is much more useful for heating and bending than acetylene.\n\nThe maximum neutral flame temperature of propane in oxygen is .\n\nPropane is cheaper than acetylene and easier to transport.\n\nOxygen is not the fuel. It is what chemically combines with the fuel to produce the heat for welding. This is called 'oxidation', but the more specific and more commonly used term in this context is 'combustion'. In the case of hydrogen, the product of combustion is simply water. For the other hydrocarbon fuels, water and carbon dioxide are produced. The heat is released because the molecules of the products of combustion have a lower energy state than the molecules of the fuel and oxygen. In oxy-fuel cutting, oxidation of the metal being cut (typically iron) produces nearly all of the heat required to \"burn\" through the workpiece.\n\nOxygen is usually produced elsewhere by distillation of liquefied air and shipped to the welding site in high-pressure vessels (commonly called \"tanks\" or \"cylinders\") at a pressure of about 21,000 kPa (3,000 lbf/in² = 200 atmospheres). It is also shipped as a liquid in Dewar type vessels (like a large Thermos jar) to places that use large amounts of oxygen. \n\nIt is also possible to separate oxygen from air by passing the air, under pressure, through a zeolite sieve that selectively adsorbs the nitrogen and lets the oxygen (and argon) pass. This gives a purity of oxygen of about 93%. This method works well for brazing, but higher-purity oxygen is necessary to produce a clean, slag-free kerf when cutting.\n\nThe welder can adjust the oxy-acetylene flame to be carbonizing (aka reducing), neutral, or oxidizing. Adjustment is made by adding more or less oxygen to the acetylene flame. The neutral flame is the flame most generally used when welding or cutting. The welder uses the neutral flame as the starting point for all other flame adjustments because it is so easily defined. This flame is attained when welders, as they slowly open the oxygen valve on the torch body, first see only two flame zones. At that point, the acetylene is being completely burned in the welding oxygen and surrounding air. The flame is chemically neutral. The two parts of this flame are the light blue inner cone and the darker blue to colorless outer cone. The inner cone is where the acetylene and the oxygen combine. The tip of this inner cone is the hottest part of the flame. It is approximately and provides enough heat to easily melt steel. In the inner cone the acetylene breaks down and partly burns to hydrogen and carbon monoxide, which in the outer cone combine with more oxygen from the surrounding air and burn.\n\nAn excess of acetylene creates a carbonizing flame. This flame is characterized by three flame zones; the hot inner cone, a white-hot \"acetylene feather\", and the blue-colored outer cone. This is the type of flame observed when oxygen is first added to the burning acetylene. The feather is adjusted and made ever smaller by adding increasing amounts of oxygen to the flame. A welding feather is measured as 2X or 3X, with X being the length of the inner flame cone. The unburned carbon insulates the flame and drops the temperature to approximately . The reducing flame is typically used for hard facing operations or backhand pipe welding techniques. The feather is caused by incomplete combustion of the acetylene to cause an excess of carbon in the flame. Some of this carbon is dissolved by the molten metal to carbonize it. The carbonizing flame will tend to remove the oxygen from iron oxides which may be present, a fact which has caused the flame to be known as a \"reducing flame\".\n\nThe oxidizing flame is the third possible flame adjustment. It occurs when the ratio of oxygen to acetylene required for a neutral flame has been changed to give an excess of oxygen. This flame type is observed when welders add more oxygen to the neutral flame. This flame is hotter than the other two flames because the combustible gases will not have to search so far to find the necessary amount of oxygen, nor heat up as much thermally inert carbon. It is called an oxidizing flame because of its effect on metal. This flame adjustment is generally not preferred. The oxidizing flame creates undesirable oxides to the structural and mechanical detriment of most metals. In an oxidizing flame, the inner cone acquires a purplish tinge, gets pinched and smaller at the tip, and the sound of the flame gets harsh. A slightly oxidizing flame is used in braze-welding and bronze-surfacing while a more strongly oxidizing flame is used in fusion welding certain brasses and bronzes\n\nThe size of the flame can be adjusted to a limited extent by the valves on the torch and by the regulator settings, but in the main it depends on the size of the orifice in the tip. In fact, the tip should be chosen first according to the job at hand, and then the regulators set accordingly.\n\nThe flame is applied to the base metal and held until a small puddle of molten metal is formed. The puddle is moved along the path where the weld bead is desired. Usually, more metal is added to the puddle as it is moved along by dipping metal from a welding rod or filler rod into the molten metal puddle. The metal puddle will travel towards where the metal is the hottest. This is accomplished through torch manipulation by the welder.\n\nThe amount of heat applied to the metal is a function of the welding tip size, the speed of travel, and the welding position. The flame size is determined by the welding tip size. The proper tip size is determined by the metal thickness and the joint design.\n\nWelding gas pressures using oxy-acetylene are set in accordance with the manufacturer's recommendations. The welder will modify the speed of welding travel to maintain a uniform bead width. Uniformity is a quality attribute indicating good workmanship. Trained welders are taught to keep the bead the same size at the beginning of the weld as at the end. If the bead gets too wide, the welder increases the speed of welding travel. If the bead gets too narrow or if the weld puddle is lost, the welder slows down the speed of travel. Welding in the vertical or overhead positions is typically slower than welding in the flat or horizontal positions.\n\nThe welder must add the filler rod to the molten puddle. The welder must also keep the filler metal in the hot outer flame zone when not adding it to the puddle to protect filler metal from oxidation. Do not let the welding flame burn off the filler metal. The metal will not wet into the base metal and will look like a series of cold dots on the base metal. There is very little strength in a cold weld. When the filler metal is properly added to the molten puddle, the resulting weld will be stronger than the original base metal.\n\nWelding lead or 'lead burning' was much more common in the 19th century to make some pipe connections and tanks. Great skill is required but can be quickly learned. In building construction today some lead flashing is welded but soldered copper flashing is much more common in America. In the automotive body collision industry before the 1980s, oxyacetylene gas torch welding was seldom used to weld sheetmetal, since warpage was a byproduct besides the excess heat. Automotive body repair methods at the time were crude and yielded improprieties until MIG welding became the industry standard. Since the 1970s, when high strength steel became the standard for automotive manufacturing, electric welding became the preferred method. After the 1980s, the oxyacetylene torch fell out of use for sheetmetal welding in the industrialized world.\n\nFor cutting, the setup is a little different. A cutting torch has a 60- or 90-degree angled head with orifices placed around a central jet. The outer jets are for preheat flames of oxygen and acetylene. The central jet carries only oxygen for cutting. The use of several preheating flames rather than a single flame makes it possible to change the direction of the cut as desired without changing the position of the nozzle or the angle which the torch makes with the direction of the cut, as well as giving a better preheat balance. Manufacturers have developed custom tips for Mapp, propane, and polypropylene gases to optimize the flames from these alternate fuel gases.\n\nThe flame is not intended to melt the metal, but to bring it to its ignition temperature.\n\nThe torch's trigger blows extra oxygen at higher pressures down the torch's third tube out of the central jet into the workpiece, causing the metal to burn and blowing the resulting molten oxide through to the other side. The ideal kerf is a narrow gap with a sharp edge on either side of the workpiece; overheating the workpiece and thus melting through it causes a rounded edge.\n\nCutting is initiated by heating the edge or leading face (as in cutting shapes such as round rod) of the steel to the ignition temperature (approximately bright cherry red heat) using the pre-heat jets only, then using the separate cutting oxygen valve to release the oxygen from the central jet. The oxygen chemically combines with the iron in the ferrous material to oxidize the iron quickly into molten iron oxide, producing the cut. Initiating a cut in the middle of a workpiece is known as piercing.\n\nIt is worth noting several things at this point:\n\nFor a basic oxy-acetylene rig, the cutting speed in light steel section will usually be nearly twice as fast as a petrol-driven cut-off grinder. The advantages when cutting large sections are obvious: an oxy-fuel torch is light, small and quiet and needs very little effort to use, whereas a cut-off grinder is heavy and noisy and needs considerable operator exertion and may vibrate severely, leading to stiff hands and possible long-term vibration white finger. Oxy-acetylene torches can easily cut through ferrous materials in excess of 200 mm (8 inches). Oxygen lances are used in scrapping operations and cut sections thicker than 200 mm (8 inches). Cut-off grinders are useless for these kinds of application.\n\nRobotic oxy-fuel cutters sometimes use a high-speed divergent nozzle. This uses an oxygen jet that opens slightly along its passage. This allows the compressed oxygen to expand as it leaves, forming a high-velocity jet that spreads less than a parallel-bore nozzle, allowing a cleaner cut. These are not used for cutting by hand since they need very accurate positioning above the work. Their ability to produce almost any shape from large steel plates gives them a secure future in shipbuilding and in many other industries.\n\nOxy-propane torches are usually used for cutting up scrap to save money, as LPG is far cheaper joule for joule than acetylene, although propane does not produce acetylene's very neat cut profile. Propane also finds a place in production, for cutting very large sections.\n\nOxy-acetylene can cut only low- to medium-carbon steels and wrought iron. High-carbon steels are difficult to cut because the melting point of the slag is closer to the melting point of the parent metal, so that the slag from the cutting action does not eject as sparks but rather mixes with the clean melt near the cut. This keeps the oxygen from reaching the clean metal and burning it. In the case of cast iron, graphite between the grains and the shape of the grains themselves interfere with the cutting action of the torch. Stainless steels cannot be cut either because the material does not burn readily.\n\nOxyacetylene welding/cutting is not difficult, but there are a good number of subtle safety points that should be learned such as:\n\n\nProper protection such as welding goggles should be worn at all times, including to protect the eyes against glare and flying sparks. Special safety eyewear must be used—both to protect the welder and to provide a clear view through the yellow-orange flare given off by the incandescing flux. In the 1940s cobalt melters’ glasses were borrowed from steel foundries and were still available until the 1980s. However, the lack of protection from impact, ultra-violet, infrared and blue light caused severe eyestrain and eye damage. Didymium eyewear, developed for glassblowers in the 1960s, was also borrowed—until many complained of eye problems from excessive infrared, blue light, and insufficient shading. Today very good eye protection can be found designed especially for gas-welding aluminum that cuts the sodium orange flare completely and provides the necessary protection from ultraviolet, infrared, blue light and impact, according to ANSI Z87-1989 safety standards for a Special Purpose Lens.\n\nFuel gases that are denser than air (Propane, Propylene, MAPP, Butane, etc...), may collect in low areas if allowed to escape. To avoid an ignition hazard, special care should be taken when using these gases over areas such as basements, sinks, storm drains, etc. In addition, leaking fittings may catch fire during use and pose a risk to personnel as well as property.\n\nWhen using fuel and oxygen tanks they should be fastened securely upright to a wall or a post or a portable cart. An oxygen tank is especially dangerous for the reason that the oxygen is at a pressure of 21 MPa (3000 lbf/in² = 200 atmospheres) when full, and if the tank falls over and its valve strikes something and is knocked off, the tank will effectively become an extremely deadly flying missile propelled by the compressed oxygen, capable of even breaking through a brick wall.\nFor this reason, never move an oxygen tank around without its valve cap screwed in place.\n\nOn an oxyacetylene torch system there will be three types of valves, the tank valve, the regulator valve, and the torch valve. There will be a set of these three valves for each gas. The gas in the tanks or cylinders is at high pressure. Oxygen cylinders are generally filled to approximately 2200 psi. The regulator converts the high pressure gas to a low pressure stream suitable for welding. Acetylene cylinders must be maintained in an upright position to prevent the internal acetone and acetylene from separating in the filler material.\n\nA less obvious hazard of welding is exposure to harmful chemicals. Exposure to certain metals, metal oxides, or carbon monoxide can often lead to severe medical conditions. Damaging chemicals can be produced from the fuel, from the work-piece, or from a protective coating on the work-piece. By increasing ventilation around the welding environment, the welders will have much less exposure to harmful chemicals from any source.\n\nThe most common fuel used in welding is acetylene, which has a two-stage reaction. The primary chemical reaction involves the acetylene disassociating in the presence of oxygen to produce heat, carbon monoxide, and hydrogen gas: CH + O → 2CO + H. A secondary reaction follows where the carbon monoxide and hydrogen combine with more oxygen to produce carbon dioxide and water vapor. When the secondary reaction does not burn all of the reactants from the primary reaction, the welding process can produce large amounts of carbon monoxide, and it often does. Carbon monoxide is also the byproduct of many other incomplete fuel reactions.\n\nAlmost every piece of metal is an alloy of one type or another. Copper, aluminium, and other base metals are occasionally alloyed with beryllium, which is a highly toxic metal. When a metal like this is welded or cut, high concentrations of toxic beryllium fumes are released. Long-term exposure to beryllium may result in shortness of breath, chronic cough, and significant weight loss, accompanied by fatigue and general weakness. Other alloying elements such as arsenic, manganese, silver, and aluminium can cause sickness to those who are exposed.\n\nMore common are the anti-rust coatings on many manufactured metal components. Zinc, cadmium, and fluorides are often used to protect irons and steels from oxidizing. Galvanized metals have a very heavy zinc coating. Exposure to zinc oxide fumes can lead to a sickness named \"metal fume fever\". This condition rarely lasts longer than 24 hours, but severe cases can be fatal. Not unlike common influenza, fevers, chills, nausea, cough, and fatigue are common effects of high zinc oxide exposure.\n\"Flashback\" is the condition of the flame propagating down the hoses of an oxy-fuel welding and cutting system. To prevent such a situation a flashback arrestor is usually employed. The flame burns backwards into the hose, causing a popping or squealing noise. It can cause an explosion in the hose with the potential to injure or kill the operator. Using a lower pressure than recommended can cause a flashback.\n\n\n\n"}
{"id": "51111", "url": "https://en.wikipedia.org/wiki?curid=51111", "title": "Pipeline transport", "text": "Pipeline transport\n\nPipeline transport is the long-distance transportation of a liquid or gas through a system of pipes—a pipeline—typically to a market area for consumption. The latest data from 2014 gives a total of slightly less than of pipeline in 120 countries of the world. The United States had 65%, Russia had 8%, and Canada had 3%, thus 75% of all pipeline were in these three countries.\n\n\"Pipeline and Gas Journal\"'s worldwide survey figures indicate that of pipelines are planned and under construction. Of these, represent projects in the planning and design phase; reflect pipelines in various stages of construction. Liquids and gases are transported in pipelines and any chemically stable substance can be sent through a pipeline. Pipelines exist for the transport of crude and refined petroleum, fuels – such as oil, natural gas and biofuels – and other fluids including sewage, slurry, water, beer, hot water or steam for shorter distances. Pipelines are useful for transporting water for drinking or irrigation over long distances when it needs to move over hills, or where canals or channels are poor choices due to considerations of evaporation, pollution, or environmental impact. \n\nOil pipelines are made from steel or plastic tubes which are usually buried. The oil is moved through the pipelines by pump stations along the pipeline. Natural gas (and similar gaseous fuels) are lightly pressurised into liquids known as Natural Gas Liquids (NGLs). Natural gas pipelines are constructed of carbon steel. Hydrogen pipeline transport is the transportation of hydrogen through a pipe. Pipelines conveying flammable or explosive material, such as natural gas or oil, pose special safety concerns and there have been various accidents. Pipelines can be the target of theft, vandalism, sabotage, or even terrorist attacks. In war, pipelines are often the target of military attacks.\n\nIt is uncertain when the first crude oil pipeline was built. Credit for the development of pipeline transport is disputed, with competing claims for Vladimir Shukhov and the Branobel company in the late 19th century, and the Oil Transport Association, which first constructed a wrought iron pipeline over a track from an oil field in Pennsylvania to a railroad station in Oil Creek, in the 1860s. Pipelines are generally the most economical way to transport large quantities of oil, refined oil products or natural gas over land. For example, in 2014, pipeline transport of crude oil cost about $5 per barrel, while rail transport cost about $10 to $15 per barrel. Trucking has even higher costs due to the additional labor required; employment on completed pipelines represents only \"1% of that of the trucking industry.\"\n\nIn the United States, 70% of crude oil and petroleum products are shipped by pipeline. (23% are by ship, 4% by truck, and 3% by rail) In Canada for natural gas and petroleum products, 97% are shipped by pipeline.\n\nNatural gas (and similar gaseous fuels) are lightly pressurized into liquids knows as Natural Gas Liquids (NGLs). Small NGL processing facilities can be located in oil fields so the butane and propane liquid under light pressure of , can be shipped by rail, truck or pipeline. Propane can be used as a fuel in oil fields to heat various facilities used by the oil drillers or equipment and trucks used in the oil patch. EG: Propane will convert from a gas to a liquid under light pressure, 100 psi, give or take depending on temperature, and is pumped into cars and trucks at less than at retail stations. Pipelines and rail cars use about double that pressure to pump at .\n\nThe distance to ship propane to markets is much shorter, as thousands of natural-gas processing plants are located in or near oil fields. Many Bakken Basin oil companies in North Dakota, Montana, Manitoba and Saskatchewan gas fields separate the NGLs in the field, allowing the drillers to sell propane directly to small wholesalers, eliminating the large refinery control of product and prices for propane or butane.\n\nThe most recent major pipeline to start operating in North America, is a TransCanada natural gas line going north across the Niagara region bridges with Marcellus shale gas from Pennsylvania and others tied in methane or natural gas sources, into the Canadian province of Ontario as of the fall of 2012, supplying 16 percent of all the natural gas used in Ontario.\nThis new US-supplied natural gas displaces the natural gas formerly shipped to Ontario from western Canada in Alberta and Manitoba, thus dropping the government regulated pipeline shipping charges because of the significantly shorter distance from gas source to consumer. To avoid delays and US government regulation, many small, medium and large oil producers in North Dakota have decided to run an oil pipeline north to Canada to meet up with a Canadian oil pipeline shipping oil from west to east. This allows the Bakken Basin and Three Forks oil producers to get higher negotiated prices for their oil because they will not be restricted to just one wholesale market in the US. The distance from the biggest oil patch in North Dakota, in Williston, North Dakota, is only about 85 miles or 137 kilometers to the Canada–US border and Manitoba. Mutual funds and joint ventures are big investors in new oil and gas pipelines. In the fall of 2012, the US began exporting propane to Europe, known as LPG, as wholesale prices there are much higher than in North America. Additionally, a pipeline is currently being constructed from North Dakota to Illinois, commonly known as the Dakota Access Pipeline.\n\nAs more North American pipelines are built, even more exports of LNG, propane, butane, and other natural gas products occur on all three US coasts. To give insight, North Dakota Bakken region's oil production has grown by 600% from 2007 to 2015. North Dakota oil companies are shipping huge amounts of oil by tanker rail car as they can direct the oil to the market that gives the best price, and rail cars can be used to avoid a congested oil pipeline to get the oil to a different pipeline in order to get the oil to market faster or to a different less busy oil refinery. However, pipelines provide a cheaper means to transport by volume.\n\nEnbridge in Canada is applying to reverse an oil pipeline going from east-to-west (Line 9) and expanding it and using it to ship western Canadian bitumen oil eastward. From a presently rated 250,000 barrels equivalent per day pipeline, it will be expanded to between one million to 1.3 million barrels per day. It will bring western oil to refineries in Ontario, Michigan, Ohio, Pennsylvania, Quebec and New York by early 2014. New Brunswick will also refine some of this western Canadian crude and export some crude and refined oil to Europe from its deep water oil ULCC loading port.\n\nAlthough pipelines can be built under the sea, that process is economically and technically demanding, so the majority of oil at sea is transported by tanker ships. Similarly, it is often more economically feasible to transport natural gas in the form of LNG, however the break-even point between LNG and pipelines would depend on the volume of natural gas and the distance it travels.\n\nThe Enbridge Sandpiper pipeline is proposed to transfer valuable oil from Western North Dakota through northwestern Minnesota. The pipeline will be 24-30 inches in diameter. It will carry over 300,000 barrels of oil a day with a volatility of 32.\n\nThe market size for oil and gas pipeline construction experienced tremendous\ngrowth prior to the economic downturn in 2008. After faltering in 2009, demand for pipeline expansion and updating increased the following year as energy production grew. By 2012, almost 32,000 miles of North American pipeline were being planned or under construction.. When pipelines are constrained, additional pipeline product transportation options may include the use of drag reducing agents, or by transporting product via truck or rail.\n\nOil pipelines are made from steel or plastic tubes with inner diameter typically from . Most pipelines are typically buried at a depth of about . To protect pipes from impact, abrasion, and corrosion, a variety of methods are used. These can include wood lagging (wood slats), concrete coating, rockshield, high-density polyethylene, imported sand padding, and padding machines.\n\nCrude oil contains varying amounts of paraffin wax and in colder climates wax buildup may occur within a pipeline. Often these pipelines are inspected and cleaned using pigging, the practice of using devices known as \"pigs\" to perform various maintenance operations on a pipeline. The devices are also known as \"scrapers\" or \"Go-devils\". \"Smart pigs\" (also known as \"intelligent\" or \"intelligence\" pigs) are used to detect anomalies in the pipe such as dents, metal loss caused by corrosion, cracking or other mechanical damage. These devices are launched from pig-launcher stations and travel through the pipeline to be received at any other station down-stream, either cleaning wax deposits and material that may have accumulated inside the line or inspecting and recording the condition of the line.\n\nFor natural gas, pipelines are constructed of carbon steel and vary in size from in diameter, depending on the type of pipeline. The gas is pressurized by compressor stations and is odorless unless mixed with a mercaptan odorant where required by a regulating authority.\n\nHighly toxic ammonia is theoretically the most dangerous substance to be transported through long-distance pipelines. However, incidents on ammonia-transporting lines are uncommon – unlike on industrial ammonia-processing equipment. A major ammonia pipeline is the Ukrainian \"Transammiak\" line connecting the TogliattiAzot facility in Russia to the exporting Black Sea-port of Odessa.\n\nPipelines have been used for transportation of ethanol in Brazil, and there are several ethanol pipeline projects in Brazil and the United States. The main problems related to the transport of ethanol by pipeline are its corrosive nature and tendency to absorb water and impurities in pipelines, which are not problems with oil and natural gas. Insufficient volumes and cost-effectiveness are other considerations limiting construction of ethanol pipelines.\n\nSlurry pipelines are sometimes used to transport coal or ore from mines. The material to be transported is closely mixed with water before being introduced to the pipeline; at the far end, the material must be dried.\nOne example is a slurry pipeline which is planned to transport iron ore from the Minas-Rio mine (producing 26.5 million tonnes per year) to the Port of Açu in Brazil. An existing example is the Savage River Slurry pipeline in Tasmania, Australia, possibly the world's first when it was built in 1967. It includes a bridge span at above the Savage River.\n\nHydrogen pipeline transport is a transportation of hydrogen through a pipe as part of the hydrogen infrastructure. Hydrogen pipeline transport is used to connect the point of hydrogen production or delivery of hydrogen with the point of demand, with transport costs similar to CNG, the technology is proven. Most hydrogen is produced at the place of demand with every 50 to an industrial production facility. The 1938 Rhine-Ruhr hydrogen pipeline is still in operation. , there are of low pressure hydrogen pipelines in the US and in Europe.\n\nTwo millennia ago, the ancient Romans made use of large aqueducts to transport water from higher elevations by building the aqueducts in graduated segments that allowed gravity to push the water along until it reached its destination. Hundreds of these were built throughout Europe and elsewhere, and along with flour mills were considered the lifeline of the Roman Empire. The ancient Chinese also made use of channels and pipe systems for public works. The famous Han Dynasty court eunuch Zhang Rang (d. 189 AD) once ordered the engineer Bi Lan to construct a series of square-pallet chain pumps outside the capital city of Luoyang. These chain pumps serviced the imperial palaces and living quarters of the capital city as the water lifted by the chain pumps was brought in by a stoneware pipe system.\n\nPipelines are useful for transporting water for drinking or irrigation over long distances when it needs to move over hills, or where canals or channels are poor choices due to considerations of evaporation, pollution, or environmental impact.\n\nThe Goldfields Water Supply Scheme in Western Australia using 750 mm (30 inch) pipe and completed in 1903 was the largest water supply scheme of its time.\n\nExamples of significant water pipelines in South Australia are the Morgan-Whyalla pipelne (completed 1944) and Mannum-Adelaide pipeline (completed 1955) pipelines, both part of the larger Snowy Mountains scheme.\n\nThere are two Los Angeles, California aqueducts, the \"Owens Valley aqueduct\" (completed 1913) and the \"Second Los Angeles Aqueduct\" (completed 1970) which also include extensive use of pipelines.\n\nThe Great Manmade River of Libya supplies of water each day to Tripoli, Benghazi, Sirte, and several other cities in Libya. The pipeline is over long, and is connected to wells tapping an aquifer over underground.\n\nDistrict heating or \"teleheating\" systems consist of a network of insulated feed and return pipes which transport heated water, pressurized hot water, or sometimes steam to the customer. While steam is hottest and may be used in industrial processes due to its higher temperature, it is less efficient to produce and transport due to greater heat losses. Heat transfer oils are generally not used for economic and ecological reasons. The typical annual loss of thermal energy through distribution is around 10%, as seen in Norway's district heating network.\n\nDistrict heating pipelines are normally installed underground, with some exceptions. Within the system, heat storage may be installed to even out peak load demands. Heat is transferred into the central heating of the dwellings through heat exchangers at heat substations, without mixing of the fluids in either system.\n\nBars in the Veltins-Arena, a major football ground in Gelsenkirchen, Germany, are interconnected by a long beer pipeline. In Randers city in Denmark, the so-called Thor Beer pipeline was operated. Originally, copper pipes ran directly from the brewery, but when the brewery moved out of the city in the 1990s, Thor Beer replaced it with a giant tank.\n\nA three-kilometer beer pipeline was completed in Bruges, Belgium in September 2016 to reduce truck traffic on the city streets.\n\nThe village of Hallstatt in Austria, which is known for its long history of salt mining, claims to contain \"the oldest industrial pipeline in the world\", dating back to 1595. It was constructed from 13,000 hollowed-out tree trunks to transport brine from Hallstatt to Ebensee.\n\nBetween 1978 and 1994, a 15 km milk pipeline ran between the Dutch island of Ameland and Holwerd on the mainland, of which 8 km beneath the Wadden Sea. Every day, 30.000 litres of milk produced on the island were transported to be processed on the mainland. In 1994, the milk transport was abandoned.\n\nIn places, a pipeline may have to cross water expanses, such as small seas, straits and rivers. In many instances, they lie entirely on the seabed. These pipelines are referred to as \"marine\" pipelines (also, \"submarine\" or \"offshore\" pipelines). They are used primarily to carry oil or gas, but transportation of water is also important. In offshore projects, a distinction is made between a \"flowline\" and a pipeline. The former is an \"intrafield\" pipeline, in the sense that it is used to connect subsea wellheads, manifolds and the platform \"within\" a particular development field. The latter, sometimes referred to as an \"export pipeline\", is used to bring the resource to shore. The construction and maintenance of marine pipelines imply logistical challenges that are different from those onland, mainly because of wave and current dynamics, along with other geohazards.\n\nIn general, pipelines can be classified in three categories depending on purpose:\n\n\nWhen a pipeline is built, the construction project not only covers the civil engineering work to lay the pipeline and build the pump/compressor stations, it also has to cover all the work related to the installation of the field devices that will support remote operation.\n\nThe pipeline is routed along what is known as a \"right of way\". Pipelines are generally developed and built using the following stages:\n\n\nRussia has \"Pipeline Troops\" as part of the Rear Services, who are trained to build and repair pipelines. Russia is the only country to have Pipeline Troops.\n\nField devices are instrumentation, data gathering units and communication systems. The field instrumentation includes flow, pressure, and temperature gauges/transmitters, and other devices to measure the relevant data required. These instruments are installed along the pipeline on some specific locations, such as injection or delivery stations, pump stations (liquid pipelines) or compressor stations (gas pipelines), and block valve stations.\n\nThe information measured by these field instruments is then gathered in local remote terminal units (RTU) that transfer the field data to a central location in real time using communication systems, such as satellite channels, microwave links, or cellular phone connections.\n\nPipelines are controlled and operated remotely, from what is usually known as the \"Main Control Room\". In this center, all the data related to field measurement is consolidated in one central database. The data is received from multiple RTUs along the pipeline. It is common to find RTUs installed at every station along the pipeline.\nThe SCADA system at the Main Control Room receives all the field data and presents it to the pipeline operator through a set of screens or Human Machine Interface, showing the operational conditions of the pipeline. The operator can monitor the hydraulic conditions of the line, as well as send operational commands (open/close valves, turn on/off compressors or pumps, change setpoints, etc.) through the SCADA system to the field.\n\nTo optimize and secure the operation of these assets, some pipeline companies are using what is called \"Advanced Pipeline Applications\", which are software tools installed on top of the SCADA system, that provide extended functionality to perform leak detection, leak location, batch tracking (liquid lines), pig tracking, composition tracking, predictive modeling, look ahead modeling, and operator training.\n\nPipeline networks are composed of several pieces of equipment that operate together to move products from location to location. The main elements of a pipeline system are:\n\n\n\n\n\n\n\nSince oil and gas pipelines are an important asset of the economic development of almost any country, it has been required either by government regulations or internal policies to ensure the safety of the assets, and the population and environment where these pipelines run.\n\nPipeline companies face government regulation, environmental constraints and social situations. Government regulations may define minimum staff to run the operation, operator training requirements, pipeline facilities, technology and applications required to ensure operational safety. For example, in the State of Washington it is mandatory for pipeline operators to be able to detect and locate leaks of 8 percent of maximum flow within fifteen minutes or less. Social factors also affect the operation of pipelines. Product theft is sometimes also a problem for pipeline companies. In this case, the detection levels should be under two percent of maximum flow, with a high expectation for location accuracy.\n\nVarious technologies and strategies have been implemented for monitoring pipelines, from physically walking the lines to satellite surveillance. The most common technology to protect pipelines from occasional leaks is Computational Pipeline Monitoring or CPM. CPM takes information from the field related to pressures, flows, and temperatures to estimate the hydraulic behavior of the product being transported. Once the estimation is completed, the results are compared to other field references to detect the presence of an anomaly or unexpected situation, which may be related to a leak.\n\nThe American Petroleum Institute has published several articles related to the performance of CPM in liquids pipelines. The API Publications are:\n\nWhere a pipeline containing passes under a road or railway, it is usually enclosed in a protective casing. This casing is vented to the atmosphere to prevent the build-up of flammable gases or corrosive substances, and to allow the air inside the casing to be sampled to detect leaks. The \"casing vent\", a pipe protruding from the ground, often doubles as a warning marker called a \"casing vent marker\".\n\nPipelines are generally laid underground because temperature is less variable. Because pipelines are usually metal, this helps to reduce the expansion and shrinkage that can occur with weather changes. However, in some cases it is necessary to cross a valley or a river on a pipeline bridge. Pipelines for centralized heating systems are often laid on the ground or overhead. Pipelines for petroleum running through permafrost areas as Trans-Alaska-Pipeline are often run overhead in order to avoid melting the frozen ground by hot petroleum which would result in sinking the pipeline in the ground.\n\nMaintenance of pipelines includes checking cathodic protection levels for the proper range, surveillance for construction, erosion, or leaks by foot, land vehicle, boat, or air, and running cleaning pigs, when there is anything carried in the pipeline that is corrosive.\n\nUS pipeline maintenance rules are covered in Code of Federal Regulations(CFR) sections, 49 CFR 192 for natural gas pipelines, and 49 CFR 195 for petroleum liquid pipelines.\n\nIn the US, onshore and offshore pipelines used to transport oil and gas are regulated by the Pipeline and Hazardous Materials Safety Administration (PHMSA). Certain offshore pipelines used to produce oil and gas are regulated by the Minerals Management Service (MMS). In Canada, pipelines are regulated by either the provincial regulators or, if they cross provincial boundaries or the Canada–US border, by the National Energy Board (NEB). Government regulations in Canada and the United States require that buried fuel pipelines must be protected from corrosion. Often, the most economical method of corrosion control is by use of pipeline coating in conjunction with cathodic protection and technology to monitor the pipeline. Above ground, cathodic protection is not an option. The coating is the only external protection.\n\nPipelines for major energy resources (petroleum and natural gas) are not merely an element of trade. They connect to issues of geopolitics and international security as well, and the construction, placement, and control of oil and gas pipelines often figure prominently in state interests and actions. A notable example of pipeline politics occurred at the beginning of the year 2009, wherein a dispute between Russia and Ukraine ostensibly over pricing led to a major political crisis. Russian state-owned gas company Gazprom cut off natural gas supplies to Ukraine after talks between it and the Ukrainian government fell through. In addition to cutting off supplies to Ukraine, Russian gas flowing through Ukraine—which included nearly all supplies to Southeastern Europe and some supplies to Central and Western Europe—was cut off, creating a major crisis in several countries heavily dependent on Russian gas as fuel. Russia was accused of using the dispute as leverage in its attempt to keep other powers, and particularly the European Union, from interfering in its \"near abroad\".\n\nOil and gas pipelines also figure prominently in the politics of Central Asia and the Caucasus.\n\nBecause the solvent fraction of dilbit typically comprises volatile aromatics like naptha and benzene, reasonably rapid carrier vaporization can be expected to follow an above-ground spill—ostensibly enabling timely intervention by leaving only a viscous residue that is slow to migrate. Effective protocols to minimize exposure to petrochemical vapours are well-established, and oil spilled from the pipeline would be unlikely to reach the aquifer unless incomplete remediation were followed by the introduction of another carrier (e.g. a series of torrential downpours).\n\nThe introduction of benzene and other volatile organic compounds (collectively BTEX) to the subterranean environment compounds the threat posed by a pipeline leak. Particularly if followed by rain, a pipeline breach would result in BTEX dissolution and equilibration of benzene in water, followed by percolation of the admixture into the aquifer. Benzene can cause many health problems and is carcinogenic with EPA Maximum Contaminant Level (MCL) set at 5 μg/L for potable water. Although it is not well studied, single benzene exposure events have been linked to acute carcinogenesis. Additionally, the exposure of livestock, mainly cattle, to benzene has been shown to cause many health issues, such as neurotoxicity, fetal damage and fatal poisoning.\n\nThe entire surface of an above-ground pipeline can be directly examined for material breach. Pooled petroleum is unambiguous, readily spotted, and indicates the location of required repairs. Because the effectiveness of remote inspection is limited by the cost of monitoring equipment, gaps between sensors, and data that requires interpretation, small leaks in buried pipe can sometimes go undetected\n\nPipeline developers do not always prioritize effective surveillance against leaks. Buried pipes draw fewer complaints. They are insulated from extremes in ambient temperature, they are shielded from ultraviolet rays, and they are less exposed to photodegradation. Buried pipes are isolated from airborne debris, electrical storms, tornadoes, hurricanes, hail, and acid rain. They are protected from nesting birds, rutting mammals, and stray buckshot. Buried pipe is less vulnerable to accident damage (e.g. automobile collisions) and less accessible to vandals, saboteurs, and terrorists.\n\nPrevious work has shown that a 'worst-case exposure scenario' can be limited to a specific set of conditions. Based on the advanced detection methods and pipeline shut-off SOP developed by TransCanada, the risk of a substantive or large release over a short period of time contaminating groundwater with benzene is unlikely. Detection, shutoff, and remediation procedures would limit the dissolution and transport of benzene. Therefore, the exposure of benzene would be limited to leaks that are below the limit of detection and go unnoticed for extended periods of time. Leak detection is monitored through a SCADA system that assesses pressure and volume flow every 5 seconds. A pinhole leak that releases small quantities that cannot be detected by the SCADA system (<1.5% flow) could accumulate into a substantive spill. Detection of pinhole leaks would come from a visual or olfactory inspection, aerial surveying, or mass-balance inconsistencies. It is assumed that pinhole leaks are discovered within the 14-day inspection interval, however snow cover and location (e.g. remote, deep) could delay detection. Benzene typically makes up 0.1 – 1.0% of oil and will have varying degrees of volatility and dissolution based on environmental factors.\n\nEven with pipeline leak volumes within SCADA detection limits, sometimes pipeline leaks are misinterpreted by pipeline operators to be pump malfunctions, or other problems. The Enbridge Line 6B crude oil pipeline failure in Marshall, Michigan on July 25, 2010 was thought by operators in Edmonton to be from column separation of the dilbit in that pipeline. The leak in wetlands along the Kalamazoo River was only confirmed 17 hours after it happened by a local gas company employee in Michigan.\n\nAlthough the Pipeline and Hazardous Materials Safety Administration (PHMSA) has standard baseline incident frequencies to estimate the number of spills, TransCanada altered these assumptions based on improved pipeline design, operation, and safety. Whether these adjustments are justified is debatable as these assumptions resulted in a nearly 10-fold decrease in spill estimates. Given that the pipeline crosses 247 miles of the Ogallala Aquifer, or 14.5% of the entire pipeline length, and the 50-year life of the entire pipeline is expected to have between 11 – 91 spills, approximately 1.6 – 13.2 spills can be expected to occur over the aquifer. An estimate of 13.2 spills over the aquifer, each lasting 14 days, results in 184 days of potential exposure over the 50 year lifetime of the pipeline.\nIn the reduced-scope worst-case exposure scenario, the volume of a pinhole leak at 1.5% of max flow-rate for 14 days has been estimated at 189,000 barrels or 7.9 million gallons of oil. According to PHMSA's incident database, only 0.5% of all spills in the last 10 years were >10,000 barrels.\n\nBenzene is considered a light aromatic hydrocarbon with high solubility and high volatility. It is unclear how temperature and depth would impact the volatility of benzene, so assumptions have been made that benzene in oil (1% weight by volume) would not volatilize before equilibrating with water.\nUsing the octanol-water partition coefficient and a 100-year precipitation event for the area, a worst-case estimate of 75 mg/L of benzene is anticipated to flow toward the aquifer. The actual movement of the plume through groundwater systems is not well described, although one estimate is that up to 4.9 billion gallons of water in the Ogallala Aquifer could become contaminated with benzene at concentrations above the MCL. The Final Environmental Impact Statement from the State Department does not include a quantitative analysis because it assumed that most benzene will volatilize.\n\nOne of the major concerns about dilbit is the difficulty in cleaning it up. Enbridge's Line 6B, a 30-inch crude oil pipeline, ruptured in Marshall, Michigan on July 25, 2010, mentioned above, spilled at least 843,000 gallons of dilbit. After detection of the leak, booms and vacuum trucks were deployed. Heavy rains caused the river to overtop existing dams, and carried dilbit 30 miles downstream before the spill was contained. Remediation work collected over 1.1 million gallons of oil and almost 200,000 cubic yards of oil-contaminated sediment and debris from the Kalamazoo River system. However, oil was still being found in affected waters in October 2012.\n\nFossil fuels can be transported by pipeline, rail, truck, or ship, though natural gas requires compression or liquification to make vehicle transport economical. For transport of crude oil via these four modes, various reports rank pipelines as proportionately causing less human death and property damage than rail and truck, spilling less oil than truck, and having more environmental impact than truck or rail (mostly due to impact on habitat; ship transport impacts marine habitat to a large degree).\n\nPipelines conveying flammable or explosive material, such as natural gas or oil, pose special safety concerns.\n\nPipelines can be the target of vandalism, sabotage, or even terrorist attacks. In war, pipelines are often the target of military attacks, as destruction of pipelines can seriously disrupt enemy logistics.\n\n"}
{"id": "16558602", "url": "https://en.wikipedia.org/wiki?curid=16558602", "title": "Pothead", "text": "Pothead\n\nA pothead is a type of insulated electrical terminal used for transitioning between overhead line and underground high voltage cable or for connecting overhead wiring to equipment like transformers. Its name comes from the process of potting or encapsulation of the conductors inside the terminal's insulating bushing.\n\nPotheads are used where higher voltage power lines go underground, generally voltages of 600 volts or greater. They are used mostly for service drops for commercial and industrial buildings. For lower voltages such as those used for residential service drops, another weather seal called a weatherhead is used.\n\nThe device consists of a molded plastic housing that attaches to the end of an electrical conduit that carries the underground cables up the utility pole to the crossarm. Multiple bushing insulators project from the plastic body, each ending at an electrical terminal. Each overhead wire is connected to a bushing terminal from which the current passes through a rod down the center of the bushing to the interior of the housing, where it is connected to a wire from the conduit. Thus the device allows the overhead conductors to pass into the conduit while serving as a seal to keep out water. The purpose of the bushings, which have corrugations moulded into their surfaces, is to provide enough creepage distance along their surface to prevent leakage current from the high voltage terminal from flowing to the grounded metal conduit.\n\nPothead was once defined in Institute of Electrical and Electronics Engineers (IEEE) Std 48-1962 \"Standards for Potheads.\" This standard was superseded by IEEE Std 48-1975 \"IEEE Standard Test Procedures and Requirements for High-Voltage Alternating-Current Cable Terminations,\" and \"pothead\" was dropped from usage. The current standard is IEEE Std 48-2009.\n\nWhat was once called a pothead is now called a \"Class I High-Voltage Cable Termination,\" which must meet these requirements:\n\n\n"}
{"id": "12309649", "url": "https://en.wikipedia.org/wiki?curid=12309649", "title": "Random close pack", "text": "Random close pack\n\nRandom close packing (RCP) is an empirical parameter used to characterize the maximum volume fraction of solid objects obtained when they are packed randomly. For example, when a solid container is filled with grain, shaking the container will reduce the volume taken up by the objects, thus allowing more grain to be added to the container. In other words, shaking increases the density of packed objects. But shaking cannot increase the density indefinitely, a limit is reached, and if this is reached without obvious packing into a regular crystal lattice, this is the empirical random close-packed density.\n\nExperiments and computer simulations have shown that the most compact way to pack hard perfect spheres randomly gives a maximum volume fraction of about 64%, i.e., approximately 64% of the volume of a container is occupied by the\nspheres. It seems as if because it is not possible to precisely define 'random' in this sense it is not possible to give an exact value. The random close packing value is significantly below the maximum possible close-packing of (equal sized) hard spheres into a regular crystalline arrangements, which is 74.04% -- both the face-centred cubic (fcc) and hexagonal close packed (hcp) crystal lattices have maximum densities equal to this upper limit.\n\nRandom close packing does not have a precise geometric definition. It is defined statistically, and results are empirical. A container is randomly filled with objects, and then the container is shaken or tapped until the objects do not compact any further, at this point the packing state is RCP. The definition of packing fraction can be given as: \"the volume taken by number of particles in a given space of volume\". In other words, packing fraction defines the packing density. It has been shown that the filling fraction increases with the number of taps until the saturation density is reached. Also, the saturation density increases as the tapping amplitude decreases. Thus RCP is the packing fraction given by the limit as the tapping amplitude goes to zero, and the limit as the number of taps goes to infinity.\n\nThe particle volume fraction at RCP depends on the objects being packed. If the objects are polydispersed then the volume fraction depends non-trivially on the size-distribution and can be arbitrarily close to 1. Still for (relatively) monodisperse objects the value for RCP depends on the object shape; for spheres it is 0.64, for M&M's candy it is 0.68.\n\nProducts containing loosely packed items are often labeled with this message: 'Contents May Settle During Shipping'. \nUsually during shipping, the container will be bumped numerous times, which will increase the packing density.\nThe message is added to assure the consumer that the container is full on a mass basis, even though the container appears slightly empty. Systems of packed particles are also used as a basic model of porous media.\n\n\n"}
{"id": "2419994", "url": "https://en.wikipedia.org/wiki?curid=2419994", "title": "Rheometry", "text": "Rheometry\n\nRheometry (from the Greek ῥέος – rheos, \"n\", meaning \"stream\") generically refers to the experimental techniques used to determine the rheological properties of materials, that is the quantitative and qualitative relationships between deformations and stresses and their derivatives.\n\nThe choice of the adequate experimental technique depends on the rheological property which has to be determined. This can be the steady shear viscosity, the linear viscoelastic properties (complex viscosity respectively elastic modulus), the elongational properties, etc.\n\nFor all real materials, the measured property will be a function of the flow conditions during which it is being measured (shear rate, frequency, etc.) even if for some materials this dependence is vanishingly low under given conditions (see Newtonian fluids).\n\nRheometry is a specific concern for smart fluids such as magnetorheological fluids and electrorheological fluids, as it is the primary method to quantify the useful properties of these materials.\n\nThe viscosity of a non-Newtonian fluid is defined by a power law: \n\nwhere \"η\" is the viscosity after shear is applied, \"η\" is the initial viscosity, \"γ\" is the shear rate, and if\n\nIn rheometry, shear forces are applied to non-Newtonian fluids in order to investigate their properties.\n\nDue to the shear thinning properties of blood, computational fluid dynamics (CFD) is used to assess the risk of aneurysms. Using High-Resolution solution strategies, the results when using non-Newtonian rheology were found to be negligible.\n\nA method for testing the behavior of shear thickening fluids is stochastic rotation dynamics-molecular dynamics (SRD-MD). The colloidal particles of a shear thickening fluid are simulated, and shear is applied. These particles create hydroclusters which exert a drag force resisting flow.\n\n"}
{"id": "19232237", "url": "https://en.wikipedia.org/wiki?curid=19232237", "title": "Roller racer", "text": "Roller racer\n\nA Roller Racer, or Flying Turtle as it was originally named by the inventor, is a toy human-powered vehicle for children. It was invented in the 1970s by a retired Boeing engineer as a gift for his grandson, using a tractor seat for the prototype toy.\n\nThe method of propulsion is unique to this scooter. The user moves forward by oscillating the handlebars from side to side. Published studies in Experimental Non-Linear Physics have been conducted worldwide on this product. It is used in amusement parks, schools, day care centers, family fun centers and homes.\n\nHere is a link to the original patent: https://www.google.com.au/patents/US3663038\n"}
{"id": "9527795", "url": "https://en.wikipedia.org/wiki?curid=9527795", "title": "Save Our Selves", "text": "Save Our Selves\n\nSave Our Selves is the name of a group of activists organized to raise awareness of global climate change. They are the organizers of the July 2007 Live Earth concerts.\n\nThe group was founded by Kevin Wall, and includes as major partners former United States Vice President Al Gore, the Alliance for Climate Protection, MSN and Control Room, a concert production company producing Live Earth.\n\n\n"}
{"id": "8508055", "url": "https://en.wikipedia.org/wiki?curid=8508055", "title": "Sears–Haack body", "text": "Sears–Haack body\n\nThe Sears–Haack body is the shape with the lowest theoretical wave drag in supersonic flow, for a given body length and given volume. The mathematical derivation assumes small-disturbance (linearized) supersonic flow, which is governed by the Prandtl–Glauert equation. The derivation and shape were published independently by two separate researchers: Wolfgang Haack in 1941 and later by William Sears in 1947.\n\nThe theory indicates that the wave drag scales as the square of the second derivative of the area distribution, formula_1 (see full expression below), so for low wave drag it is necessary that formula_2 be smooth. Thus, the Sears–Haack body is pointed at each end and grows smoothly to a maximum and then decreases smoothly toward the second point.\n\nThe cross-sectional area of a Sears–Haack body is\n\nits volume is\n\nits radius is\n\nthe derivative (slope) is\n\nthe second derivative is\n\nwhere:\n\nFrom Slender-body theory, it follows that:\n\nalternatively:\n\nThese formulae may be combined to get the following:\n\nwhere:\n\nThe Sears–Haack body shape derivation is correct only in the limit of a slender body.\nThe theory has been generalized to slender but non-axisymmetric shapes by Robert T. Jones in NACA Report 1284. In this extension, the area formula_2 is defined on the Mach cone whose apex is at location formula_16, rather than on the formula_17 plane as assumed by Sears and Haack. Hence, Jones's theory makes it applicable to more complex shapes like entire supersonic aircraft.\n\nA superficially related concept is the Whitcomb area rule, which states that wave drag due to volume in transonic flow depends primarily on the distribution of total cross-sectional area, and for low wave drag this distribution must be smooth. A common misconception is that the Sears–Haack body has the ideal area distribution according to the area rule, but this is not correct. The Prandtl–Glauert equation, which is the starting point in the Sears–Haack body shape derivation, is not valid in transonic flow, which is where the area rule applies.\n\n\n"}
{"id": "2041176", "url": "https://en.wikipedia.org/wiki?curid=2041176", "title": "Simple shear", "text": "Simple shear\n\nSimple shear is a deformation in which parallel planes in a material remain parallel and maintain a constant distance, while translating relative to each other. \n\nIn fluid mechanics, simple shear is a special case of deformation where only one component of velocity vectors has a non-zero value:\n\nAnd the gradient of velocity is constant and perpendicular to the velocity itself:\n\nwhere formula_4 is the shear rate and:\n\nThe displacement gradient tensor Γ for this deformation has only one nonzero term:\n\nSimple shear with the rate formula_7 is the combination of pure shear strain with the rate of formula_7 and rotation with the rate of formula_7:\n\nThe mathematical model representing simple shear is a shear mapping restricted to the physical limits. It is an elementary linear transformation represented by a matrix. The model may represent laminar flow velocity at varying depths of a long channel with constant cross-section. Limited shear deformation is also used in vibration control, for instance base isolation of buildings for limiting earthquake damage.\n\nIn solid mechanics, a simple shear deformation is defined as an isochoric plane deformation in which there are a set of line elements with a given reference orientation that do not change length and orientation during the deformation. This deformation is differentiated from a pure shear by virtue of the presence of a rigid rotation of the material. When rubber deforms under simple shear, its stress-strain behavior is approximately linear. A rod under torsion is a practical example for a body under simple shear.\n\nIf e is the fixed reference orientation in which line elements do not deform during the deformation and e − e is the plane of deformation, then the deformation gradient in simple shear can be expressed as\nWe can also write the deformation gradient as\n\nShear stress, denoted formula_13, is related to shear strain, denoted formula_14, by the following equation:\n\nformula_15\n\nwhere formula_16 is the shear modulus of the material, given by\n\nformula_17\n\nHere formula_18 is Young's modulus and formula_19 is Poisson's ratio. Combining gives\n\nformula_20\n\n"}
{"id": "44057109", "url": "https://en.wikipedia.org/wiki?curid=44057109", "title": "Skarv oil field", "text": "Skarv oil field\n\nSkarv is an oil and gas field in the Norwegian Sea approximately 35 kilometers south-west of the Norne oil field. It is produced from an FPSO. Remaining reserves are though to be 100 million barrels of oil and 40 billion cubic meters of natural gas.\n\nThe field Skarv was discovered in 1998 and the field started producing 31 December 2012.\n"}
{"id": "35555636", "url": "https://en.wikipedia.org/wiki?curid=35555636", "title": "Solarpark Heideblick", "text": "Solarpark Heideblick\n\nThe Solarpark Heideblick is a photovoltaic power station in Heideblick, Germany. It has a capacity of 27.5 megawatts (MW). The solar park was developed and built by Enerparc.\n\nThe PV project is built on a former military training field, using ReneSola modules.\n\n"}
{"id": "27633", "url": "https://en.wikipedia.org/wiki?curid=27633", "title": "Stonehenge", "text": "Stonehenge\n\nStonehenge is a prehistoric monument in Wiltshire, England, west of Amesbury. It consists of a ring of standing stones, with each standing stone around high, wide and weighing around 25 tons. The stones are set within earthworks in the middle of the most dense complex of Neolithic and Bronze Age monuments in England, including several hundred burial mounds.\n\nArchaeologists believe it was constructed from 3000 BC to 2000 BC. The surrounding circular earth bank and ditch, which constitute the earliest phase of the monument, have been dated to about 3100 BC. Radiocarbon dating suggests that the first bluestones were raised between 2400 and 2200 BC, although they may have been at the site as early as 3000 BC.\n\nOne of the most famous landmarks in the United Kingdom, Stonehenge is regarded as a British cultural icon. It has been a legally protected Scheduled Ancient Monument since 1882 when legislation to protect historic monuments was first successfully introduced in Britain. The site and its surroundings were added to UNESCO's list of World Heritage Sites in 1986. Stonehenge is owned by the Crown and managed by English Heritage; the surrounding land is owned by the National Trust.\n\nStonehenge could have been a burial ground from its earliest beginnings. Deposits containing human bone date from as early as 3000 BC, when the ditch and bank were first dug, and continued for at least another five hundred years.\n\nThe \"Oxford English Dictionary\" cites Ælfric's tenth-century glossary, in which \"henge-cliff\" is given the meaning \"precipice\", or stone, thus the \"stanenges\" or \"Stanheng\" \"not far from Salisbury\" recorded by eleventh-century writers are \"supported stones\". William Stukeley in 1740 notes, \"Pendulous rocks are now called henges in Yorkshire...I doubt not, Stonehenge in Saxon signifies the hanging stones.\" Christopher Chippindale's \"Stonehenge Complete\" gives the derivation of the name \"Stonehenge\" as coming from the Old English words \"stān\" meaning \"stone\", and either \"hencg\" meaning \"hinge\" (because the stone lintels hinge on the upright stones) or \"hen(c)en\" meaning \"hang\" or \"gallows\" or \"instrument of torture\" (though elsewhere in his book, Chippindale cites the \"suspended stones\" etymology). Like Stonehenge's trilithons, medieval gallows consisted of two uprights with a lintel joining them, rather than the inverted L-shape more familiar today.\n\nThe \"henge\" portion has given its name to a class of monuments known as henges. Archaeologists define henges as earthworks consisting of a circular banked enclosure with an internal ditch. As often happens in archaeological terminology, this is a holdover from antiquarian use. Because its bank is inside its ditch, Stonehenge is not truly a henge site. \n\nDespite being contemporary with true Neolithic henges and stone circles, Stonehenge is in many ways atypical—for example, at more than tall, its extant trilithons' lintels, held in place with mortise and tenon joints, make it unique.\n\nMike Parker Pearson, leader of the Stonehenge Riverside Project based at Durrington Walls, noted that Stonehenge appears to have been associated with burial from the earliest period of its existence:\n\nStonehenge evolved in several construction phases spanning at least 1500 years. There is evidence of large-scale construction on and around the monument that perhaps extends the landscape's time frame to 6500 years. Dating and understanding the various phases of activity is complicated by disturbance of the natural chalk by periglacial effects and animal burrowing, poor quality early excavation records, and a lack of accurate, scientifically verified dates. The modern phasing most generally agreed to by archaeologists is detailed below. Features mentioned in the text are numbered and shown on the plan, right.\n\nArchaeologists have found four, or possibly five, large Mesolithic postholes (one may have been a natural tree throw), which date to around 8000 BC, beneath the nearby old tourist car-park in use until 2013. These held pine posts around in diameter, which were erected and eventually rotted \"in situ\". Three of the posts (and possibly four) were in an east-west alignment which may have had ritual significance. Another Mesolithic astronomical site in Britain is the Warren Field site in Aberdeenshire, which is considered the world's oldest Lunar calendar, corrected yearly by observing the midwinter solstice. Similar but later sites have been found in Scandinavia. A settlement that may have been contemporaneous with the posts has been found at Blick Mead, a reliable year-round spring from Stonehenge.\n\nSalisbury Plain was then still wooded, but 4,000 years later, during the earlier Neolithic, people built a causewayed enclosure at Robin Hood's Ball and long barrow tombs in the surrounding landscape. In approximately 3500 BC, a Stonehenge Cursus was built north of the site as the first farmers began to clear the trees and develop the area. A number of other previously overlooked stone or wooden structures and burial mounds may date as far back as 4000 BC. Charcoal from the ‘Blick Mead’ camp from Stonehenge (near the Vespasian's Camp site) has been dated to 4000 BC. The University of Buckingham's Humanities Research Institute believes that the community who built Stonehenge lived here over a period of several millennia, making it potentially \"one of the pivotal places in the history of the Stonehenge landscape.\"\n\nThe first monument consisted of a circular bank and ditch enclosure made of Late Cretaceous (Santonian Age) Seaford Chalk, measuring about in diameter, with a large entrance to the north east and a smaller one to the south. It stood in open grassland on a slightly sloping spot. The builders placed the bones of deer and oxen in the bottom of the ditch, as well as some worked flint tools. The bones were considerably older than the antler picks used to dig the ditch, and the people who buried them had looked after them for some time prior to burial. The ditch was continuous but had been dug in sections, like the ditches of the earlier causewayed enclosures in the area. The chalk dug from the ditch was piled up to form the bank. This first stage is dated to around 3100 BC, after which the ditch began to silt up naturally. Within the outer edge of the enclosed area is a circle of 56 pits, each about a metre () in diameter, known as the Aubrey holes after John Aubrey, the seventeenth-century antiquarian who was thought to have first identified them. The pits may have contained standing timbers creating a timber circle, although there is no excavated evidence of them. A recent excavation has suggested that the Aubrey Holes may have originally been used to erect a bluestone circle. If this were the case, it would advance the earliest known stone structure at the monument by some 500 years. A small outer bank beyond the ditch could also date to this period.\n\nIn 2013 a team of archaeologists, led by Mike Parker Pearson, excavated more than 50,000 cremated bones of 63 individuals buried at Stonehenge. These remains had originally been buried individually in the Aubrey holes, exhumed during a previous excavation conducted by William Hawley in 1920, been considered unimportant by him, and subsequently re-interred together in one hole, Aubrey Hole 7, in 1935. Physical and chemical analysis of the remains has shown that the cremated were almost equally men and women, and included some children. As there was evidence of the underlying chalk beneath the graves being crushed by substantial weight, the team concluded that the first bluestones brought from Wales were probably used as grave markers. Radiocarbon dating of the remains has put the date of the site 500 years earlier than previously estimated, to around 3000 BC. A 2018 study of the strontium content of the bones found that many of the individuals buried there around the time of construction had probably come from near the source of the bluestone in Wales and had not extensively lived in the area of Stonehenge before death.\n\nEvidence of the second phase is no longer visible. The number of postholes dating to the early 3rd millennium BC suggest that some form of timber structure was built within the enclosure during this period. Further standing timbers were placed at the northeast entrance, and a parallel alignment of posts ran inwards from the southern entrance. The postholes are smaller than the Aubrey Holes, being only around in diameter, and are much less regularly spaced. The bank was purposely reduced in height and the ditch continued to silt up. At least twenty-five of the Aubrey Holes are known to have contained later, intrusive, cremation burials dating to the two centuries after the monument's inception. It seems that whatever the holes' initial function, it changed to become a funerary one during Phase 2. Thirty further cremations were placed in the enclosure's ditch and at other points within the monument, mostly in the eastern half. Stonehenge is therefore interpreted as functioning as an enclosed cremation cemetery at this time, the earliest known cremation cemetery in the British Isles. Fragments of unburnt human bone have also been found in the ditch-fill. Dating evidence is provided by the late Neolithic grooved ware pottery that has been found in connection with the features from this phase.\n\nArchaeological excavation has indicated that around 2600 BC, the builders abandoned timber in favour of stone and dug two concentric arrays of holes (the Q and R Holes) in the centre of the site. These stone sockets are only partly known (hence on present evidence are sometimes described as forming 'crescents'); however, they could be the remains of a double ring. Again, there is little firm dating evidence for this phase. The holes held up to 80 standing stones (shown blue on the plan), only 43 of which can be traced today. It is generally accepted that the bluestones (some of which are made of dolerite, an igneous rock), were transported by the builders from the Preseli Hills, away in modern-day Pembrokeshire in Wales. Another theory is that they were brought much nearer to the site as glacial erratics by the Irish Sea Glacier although there is no evidence of glacial deposition within southern central England.\n\nThe long distance human transport theory was bolstered in 2011 by the discovery of a megalithic bluestone quarry at Craig Rhos-y-felin, near Crymych in Pembrokeshire, which is the most likely place for some of the stones to have been obtained. Other standing stones may well have been small sarsens (sandstone), used later as lintels. The stones, which weighed about two tons, could have been moved by lifting and carrying them on rows of poles and rectangular frameworks of poles, as recorded in China, Japan and India. It is not known whether the stones were taken directly from their quarries to Salisbury Plain or were the result of the removal of a venerated stone circle from Preseli to Salisbury Plain to \"merge two sacred centres into one, to unify two politically separate regions, or to legitimise the ancestral identity of migrants moving from one region to another\". Each monolith measures around in height, between wide and around thick. What was to become known as the Altar Stone is almost certainly derived from the Senni Beds, perhaps from east of Mynydd Preseli in the Brecon Beacons.\n\nThe north-eastern entrance was widened at this time, with the result that it precisely matched the direction of the midsummer sunrise and midwinter sunset of the period. This phase of the monument was abandoned unfinished, however; the small standing stones were apparently removed and the Q and R holes purposefully backfilled. Even so, the monument appears to have eclipsed the site at Avebury in importance towards the end of this phase.\n\nThe Heelstone, a Tertiary sandstone, may also have been erected outside the north-eastern entrance during this period. It cannot be accurately dated and may have been installed at any time during phase 3. At first it was accompanied by a second stone, which is no longer visible. Two, or possibly three, large portal stones were set up just inside the north-eastern entrance, of which only one, the fallen Slaughter Stone, long, now remains. Other features, loosely dated to phase 3, include the four Station Stones, two of which stood atop mounds. The mounds are known as \"barrows\" although they do not contain burials. Stonehenge Avenue, a parallel pair of ditches and banks leading to the River Avon, was also added. Two ditches similar to Heelstone Ditch circling the Heelstone (which was by then reduced to a single monolith) were later dug around the Station Stones.\n\nDuring the next major phase of activity, 30 enormous Oligocene-Miocene sarsen stones \"(shown grey on the plan)\" were brought to the site. They may have come from a quarry around north of Stonehenge on the Marlborough Downs, or they may have been collected from a \"litter\" of sarsens on the chalk downs, closer to hand. The stones were dressed and fashioned with mortise and tenon joints before 30 were erected as a diameter circle of standing stones, with a ring of 30 lintel stones resting on top. The lintels were fitted to one another using another woodworking method, the tongue and groove joint. Each standing stone was around high, wide and weighed around 25 tons. Each had clearly been worked with the final visual effect in mind; the orthostats widen slightly towards the top in order that their perspective remains constant when viewed from the ground, while the lintel stones curve slightly to continue the circular appearance of the earlier monument.\n\nThe inward-facing surfaces of the stones are smoother and more finely worked than the outer surfaces. The average thickness of the stones is and the average distance between them is . A total of 75 stones would have been needed to complete the circle (60 stones) and the trilithon horseshoe (15 stones). It was thought the ring might have been left incomplete, but an exceptionally dry summer in 2013 revealed patches of parched grass which may correspond to the location of removed sarsens. The lintel stones are each around long, wide and thick. The tops of the lintels are above the ground.\n\nWithin this circle stood five trilithons of dressed sarsen stone arranged in a horseshoe shape across, with its open end facing north east. These huge stones, ten uprights and five lintels, weigh up to 50 tons each. They were linked using complex jointing. They are arranged symmetrically. The smallest pair of trilithons were around tall, the next pair a little higher, and the largest, single trilithon in the south west corner would have been tall. Only one upright from the Great Trilithon still stands, of which is visible and a further is below ground. The images of a 'dagger' and 14 'axeheads' have been carved on one of the sarsens, known as stone 53; further carvings of axeheads have been seen on the outer faces of stones 3, 4, and 5. The carvings are difficult to date, but are morphologically similar to late Bronze Age weapons. Early 21st-century laser scanning of the carvings supports this interpretation. The pair of trilithons in the north east are smallest, measuring around in height; the largest, which is in the south west of the horseshoe, is almost tall.\n\nThis ambitious phase has been radiocarbon dated to between 2600 and 2400 BC, slightly earlier than the Stonehenge Archer, discovered in the outer ditch of the monument in 1978, and the two sets of burials, known as the Amesbury Archer and the Boscombe Bowmen, discovered to the west. Analysis of animal teeth found away at Durrington Walls, thought by Parker Pearson to be the 'builders camp', suggests that, during some period between 2600 and 2400 BC, as many as 4,000 people gathered at the site for the mid-winter and mid-summer festivals; the evidence showed that the animals had been slaughtered around 9 months or 15 months after their spring birth. Strontium isotope analysis of the animal teeth showed that some had been brought from as far afield as the Scottish Highlands for the celebrations. At about the same time, a large timber circle and a second avenue were constructed at Durrington Walls overlooking the River Avon. The timber circle was oriented towards the rising sun on the midwinter solstice, opposing the solar alignments at Stonehenge. The avenue was aligned with the setting sun on the summer solstice and led from the river to the timber circle. Evidence of huge fires on the banks of the Avon between the two avenues also suggests that both circles were linked. They were perhaps used as a procession route on the longest and shortest days of the year. Parker Pearson speculates that the wooden circle at Durrington Walls was the centre of a 'land of the living', whilst the stone circle represented a 'land of the dead', with the Avon serving as a journey between the two.\n\nLater in the Bronze Age, although the exact details of activities during this period are still unclear, the bluestones appear to have been re-erected. They were placed within the outer sarsen circle and may have been trimmed in some way. Like the sarsens, a few have timber-working style cuts in them suggesting that, during this phase, they may have been linked with lintels and were part of a larger structure.\n\nThis phase saw further rearrangement of the bluestones. They were arranged in a circle between the two rings of sarsens and in an oval at the centre of the inner ring. Some archaeologists argue that some of these bluestones were from a second group brought from Wales. All the stones formed well-spaced uprights without any of the linking lintels inferred in Stonehenge 3 III. The Altar Stone may have been moved within the oval at this time and re-erected vertically. Although this would seem the most impressive phase of work, Stonehenge 3 IV was rather shabbily built compared to its immediate predecessors, as the newly re-installed bluestones were not well-founded and began to fall over. However, only minor changes were made after this phase.\n\nSoon afterwards, the north eastern section of the Phase 3 IV bluestone circle was removed, creating a horseshoe-shaped setting (the Bluestone Horseshoe) which mirrored the shape of the central sarsen Trilithons. This phase is contemporary with the Seahenge site in Norfolk.\n\nThe Y and Z Holes are the last known construction at Stonehenge, built about 1600 BC, and the last usage of it was probably during the Iron Age. Roman coins and medieval artefacts have all been found in or around the monument but it is unknown if the monument was in continuous use throughout British prehistory and beyond, or exactly how it would have been used. Notable is the massive Iron Age hillfort Vespasian's Camp built alongside the Avenue near the Avon. A decapitated seventh century Saxon man was excavated from Stonehenge in 1923. The site was known to scholars during the Middle Ages and since then it has been studied and adopted by numerous groups.\n\nStonehenge was produced by a culture that left no written records. Many aspects of Stonehenge, such as how it was built and which purposes it was used for, remain subject to debate. A number of myths surround the stones. The site, specifically the great trilithon, the encompassing horseshoe arrangement of the five central trilithons, the heel stone, and the embanked avenue, are aligned to the sunset of the winter solstice and the opposing sunrise of the summer solstice. A natural landform at the monument's location followed this line, and may have inspired its construction. The excavated remains of culled animal bones suggest that people may have gathered at the site for the winter rather than the summer. Further astronomical associations, and the precise astronomical significance of the site for its people, are a matter of speculation and debate.\n\nThere is little or no direct evidence revealing the construction techniques used by the Stonehenge builders. Over the years, various authors have suggested that supernatural or anachronistic methods were used, usually asserting that the stones were impossible to move otherwise due to their massive size. However, conventional techniques, using Neolithic technology as basic as shear legs, have been demonstrably effective at moving and placing stones of a similar size. How the stones could be transported by a prehistoric people without the aid of the wheel or a pulley system is not known. The most common theory of how prehistoric people moved megaliths has them creating a track of logs on which the large stones were rolled along. Another megalith transport theory involves the use of a type of sleigh running on a track greased with animal fat. Such an experiment with a sleigh carrying a 40-ton slab of stone was successful near Stonehenge in 1995. A team of more than 100 workers managed to push and pull the slab along the journey from Marlborough Downs. Proposed functions for the site include usage as an astronomical observatory or as a religious site.\n\nMore recently two major new theories have been proposed. Professor Geoffrey Wainwright, president of the Society of Antiquaries of London, and Timothy Darvill, of Bournemouth University, have suggested that Stonehenge was a place of healing—the primeval equivalent of Lourdes. They argue that this accounts for the high number of burials in the area and for the evidence of trauma deformity in some of the graves. However, they do concede that the site was probably multifunctional and used for ancestor worship as well. Isotope analysis indicates that some of the buried individuals were from other regions. A teenage boy buried approximately 1550 BC was raised near the Mediterranean Sea; a metal worker from 2300 BC dubbed the \"Amesbury Archer\" grew up near the alpine foothills of Germany; and the \"Boscombe Bowmen\" probably arrived from Wales or Brittany, France.\n\nOn the other hand, Mike Parker Pearson of Sheffield University has suggested that Stonehenge was part of a ritual landscape and was joined to Durrington Walls by their corresponding avenues and the River Avon. He suggests that the area around Durrington Walls Henge was a place of the living, whilst Stonehenge was a domain of the dead. A journey along the Avon to reach Stonehenge was part of a ritual passage from life to death, to celebrate past ancestors and the recently deceased. Both explanations were first mooted in the twelfth century by Geoffrey of Monmouth, who extolled the curative properties of the stones and was also the first to advance the idea that Stonehenge was constructed as a funerary monument. Whatever religious, mystical or spiritual elements were central to Stonehenge, its design includes a celestial observatory function, which might have allowed prediction of eclipse, solstice, equinox and other celestial events important to a contemporary religion.\n\nThere are other hypotheses and theories. According to a team of British researchers led by Mike Parker Pearson of the University of Sheffield, Stonehenge may have been built as a symbol of \"peace and unity\", indicated in part by the fact that at the time of its construction, Britain's Neolithic people were experiencing a period of cultural unification.\n\nResearchers from the Royal College of Art in London have discovered that the monument’s bluestones possess \"unusual acoustic properties\" — when struck they respond with a \"loud clanging noise\". According to Paul Devereux, editor of the journal \"Time and Mind: The Journal of Archaeology, Consciousness and Culture\", this idea could explain why certain bluestones were hauled nearly —a major technical accomplishment at the time. In certain ancient cultures rocks that ring out, known as lithophones, were believed to contain mystic or healing powers, and Stonehenge has a history of association with rituals. The presence of these \"ringing rocks\" seems to support the hypothesis that Stonehenge was a \"place for healing\", as has been pointed out by Bournemouth University archaeologist Timothy Darvill, who consulted with the researchers. The bluestones of Stonehenge were quarried near a town in Wales called Maenclochog, which means \"ringing rock\", where the local bluestones were used as church bells until the 18th century.\n\nThe Heel Stone lies northeast of the sarsen circle, beside the end portion of Stonehenge Avenue. It is a rough stone, above ground, leaning inwards towards the stone circle. It has been known by many names in the past, including \"Friar's Heel\" and \"Sun-stone\". At summer solstice an observer standing within the stone circle, looking northeast through the entrance, would see the Sun rise in the approximate direction of the heel stone, and the sun has often been photographed over it.\n\nA folk tale relates the origin of the Friar's Heel reference.\n\nThe name is not unique; there was a monolith with the same name recorded in the nineteenth century by antiquarian Charles Warne at Long Bredy in Dorset.\n\nIn the twelfth century, Geoffrey of Monmouth included a fanciful story in his \"Historia Regum Britanniae\" (\"History of the Kings of Britain\") that attributed the monument's construction to the wizard Merlin. Geoffrey's story spread widely, appearing in more and less elaborate form in adaptations of his work such as Wace's Norman French \"Roman de Brut\", Layamon's Middle English \"Brut\", and the Welsh \"Brut y Brenhinedd\".\n\nAccording to Geoffrey, the rocks of Stonehenge were healing rocks, called the Giant's dance, which Giants had brought from Africa to Ireland for their healing properties. The fifth-century king Aurelius Ambrosius wished to erect a memorial to 3,000 nobles slain in battle against the Saxons and buried at Salisbury, and, at Merlin's advice, chose Stonehenge. The king sent Merlin, Uther Pendragon (King Arthur's father), and 15,000 knights, to remove it from Ireland, where it had been constructed on Mount Killaraus by the Giants. They slew 7,000 Irish, but as the knights tried to move the rocks with ropes and force, they failed. Then Merlin, using \"gear\" and skill, easily dismantled the stones and sent them over to Britain, where Stonehenge was dedicated. After it had been rebuilt near Amesbury, Geoffrey further narrates how first Ambrosius Aurelianus, then Uther Pendragon, and finally Constantine III, were buried inside the \"Giants' Ring of Stonehenge\".\n\nIn another legend of Saxons and Britons, in 472, the invading king Hengist invited Brythonic warriors to a feast, but treacherously ordered his men to draw their weapons from concealment and fall upon the guests, killing 420 of them. Hengist erected the stone monument—Stonehenge—on the site to show his remorse for the deed.\n\nStonehenge has changed ownership several times since King Henry VIII acquired Amesbury Abbey and its surrounding lands. In 1540 Henry gave the estate to the Earl of Hertford. It subsequently passed to Lord Carleton and then the Marquess of Queensberry. The Antrobus family of Cheshire bought the estate in 1824. During the First World War an aerodrome (Royal Flying Corps \"No. 1 School of Aerial Navigation and Bomb Dropping\") was built on the downs just to the west of the circle and, in the dry valley at Stonehenge Bottom, a main road junction was built, along with several cottages and a cafe. The Antrobus family sold the site after their last heir was killed in the fighting in France. The auction by Knight Frank & Rutley estate agents in Salisbury was held on 21 September 1915 and included \"Lot 15. Stonehenge with about 30 acres, 2 rods, 37 perches [12.44 ha] of adjoining downland.\"\n\nCecil Chubb bought the site for £6,600 and gave it to the nation three years later. Although it has been speculated that he purchased it at the suggestion of—or even as a present for—his wife, in fact he bought it on a whim, as he believed a local man should be the new owner.\n\nIn the late 1920s a nationwide appeal was launched to save Stonehenge from the encroachment of the modern buildings that had begun to rise around it. By 1928 the land around the monument had been purchased with the appeal donations, and given to the National Trust to preserve. The buildings were removed (although the roads were not), and the land returned to agriculture. More recently the land has been part of a grassland reversion scheme, returning the surrounding fields to native chalk grassland.\n\nDuring the twentieth century, Stonehenge began to revive as a place of religious significance, this time by adherents of Neopaganism and New Age beliefs, particularly the Neo-druids. The historian Ronald Hutton would later remark that \"it was a great, and potentially uncomfortable, irony that modern Druids had arrived at Stonehenge just as archaeologists were evicting the ancient Druids from it.\" The first such Neo-druidic group to make use of the megalithic monument was the Ancient Order of Druids, who performed a mass initiation ceremony there in August 1905, in which they admitted 259 new members into their organisation. This assembly was largely ridiculed in the press, who mocked the fact that the Neo-druids were dressed up in costumes consisting of white robes and fake beards.\n\nBetween 1972 and 1984, Stonehenge was the site of the Stonehenge Free Festival. After the Battle of the Beanfield in 1985, this use of the site was stopped for several years and ritual use of Stonehenge is now heavily restricted. Some Druids have arranged an assembling of monuments styled on Stonehenge in other parts of the world as a form of Druidist worship.\n\nWhen Stonehenge was first opened to the public it was possible to walk among and even climb on the stones, but the stones were roped off in 1977 as a result of serious erosion. Visitors are no longer permitted to touch the stones, but are able to walk around the monument from a short distance away. English Heritage does, however, permit access during the summer and winter solstice, and the spring and autumn equinox. Additionally, visitors can make special bookings to access the stones throughout the year.\n\nThe access situation and the proximity of the two roads has drawn widespread criticism, highlighted by a 2006 National Geographic survey. In the survey of conditions at 94 leading World Heritage Sites, 400 conservation and tourism experts ranked Stonehenge 75th in the list of destinations, declaring it to be \"in moderate trouble\".\n\nAs motorised traffic increased, the setting of the monument began to be affected by the proximity of the two roads on either side—the A344 to Shrewton on the north side, and the A303 to Winterbourne Stoke to the south. Plans to upgrade the A303 and close the A344 to restore the vista from the stones have been considered since the monument became a World Heritage Site. However, the controversy surrounding expensive re-routing of the roads has led to the scheme being cancelled on multiple occasions. On 6 December 2007, it was announced that extensive plans to build Stonehenge road tunnel under the landscape and create a permanent visitors' centre had been cancelled.\nOn 13 May 2009, the government gave approval for a £25 million scheme to create a smaller visitors' centre and close the A344, although this was dependent on funding and local authority planning consent. On 20 January 2010 Wiltshire Council granted planning permission for a centre to the west and English Heritage confirmed that funds to build it would be available, supported by a £10m grant from the Heritage Lottery Fund. On 23 June 2013 the A344 was closed to begin the work of removing the section of road and replacing it with grass. The centre, designed by Denton Corker Marshall, opened to the public on 18 December 2013.\n\nThe earlier rituals were complemented by the Stonehenge Free Festival, loosely organised by the Politantric Circle, held between 1972 and 1984, during which time the number of midsummer visitors had risen to around 30,000. However, in 1985 the site was closed to festivalgoers by a High Court injunction. A consequence of the end of the festival in 1985 was the violent confrontation between the police and New Age travellers that became known as the Battle of the Beanfield when police blockaded a convoy of travellers to prevent them from approaching Stonehenge. Beginning in 1985, the year of the Battle, no access was allowed into the stones at Stonehenge for any religious reason. This \"exclusion-zone\" policy continued for almost fifteen years: until just before the arrival of the twenty-first century, visitors were not allowed to go into the stones at times of religious significance, the winter and summer solstices, and the vernal and autumnal equinoxes.\n\nHowever, following a European Court of Human Rights ruling obtained by campaigners such as Arthur Uther Pendragon, the restrictions were lifted. The ruling recognizes that members of any genuine religion have a right to worship in their own church, and Stonehenge is a place of worship to Neo-Druids, Pagans and other \"Earth based' or 'old' religions. The Roundtable meetings include members of the Wiltshire Police force, National Trust, English Heritage, Pagans, Druids, Spiritualists and others.\n\nAt the Summer Solstice 2003, which fell over a weekend, over 30,000 people attended a gathering at and in the stones. The 2004 gathering was smaller (around 21,000 people).\n\nThroughout recorded history, Stonehenge and its surrounding monuments have attracted attention from antiquarians and archaeologists. John Aubrey was one of the first to examine the site with a scientific eye in 1666, and in his plan of the monument, he recorded the pits that now bear his name, the Aubrey holes. William Stukeley continued Aubrey’s work in the early eighteenth century, but took an interest in the surrounding monuments as well, identifying (somewhat incorrectly) the Cursus and the Avenue. He also began the excavation of many of the barrows in the area, and it was his interpretation of the landscape that associated it with the Druids. Stukeley was so fascinated with Druids that he originally named Disc Barrows as Druids' Barrows. The most accurate early plan of Stonehenge was that made by Bath architect John Wood in 1740. His original annotated survey has recently been computer redrawn and published. Importantly Wood’s plan was made before the collapse of the southwest trilithon, which fell in 1797 and was restored in 1958.\n\nWilliam Cunnington was the next to tackle the area in the early nineteenth century. He excavated some 24 barrows before digging in and around the stones and discovered charred wood, animal bones, pottery and urns. He also identified the hole in which the Slaughter Stone once stood. Richard Colt Hoare supported Cunnington's work and excavated some 379 barrows on Salisbury Plain including on some 200 in the area around the Stones, some excavated in conjunction with William Coxe. To alert future diggers to their work they were careful to leave initialled metal tokens in each barrow they opened. Cunnington's finds are displayed at the Wiltshire Museum. In 1877 Charles Darwin dabbled in archaeology at the stones, experimenting with the rate at which remains sink into the earth for his book \"The Formation of Vegetable Mould Through the Action of Worms\".\n\nWilliam Gowland oversaw the first major restoration of the monument in 1901 which involved the straightening and concrete setting of sarsen stone number 56 which was in danger of falling. In straightening the stone he moved it about half a metre from its original position. Gowland also took the opportunity to further excavate the monument in what was the most scientific dig to date, revealing more about the erection of the stones than the previous 100 years of work had done. During the 1920 restoration William Hawley, who had excavated nearby Old Sarum, excavated the base of six stones and the outer ditch. He also located a bottle of port in the Slaughter Stone socket left by Cunnington, helped to rediscover Aubrey's pits inside the bank and located the concentric circular holes outside the Sarsen Circle called the Y and Z Holes.\n\nRichard Atkinson, Stuart Piggott and John F.S. Stone re-excavated much of Hawley's work in the 1940s and 1950s, and discovered the carved axes and daggers on the Sarsen Stones. Atkinson's work was instrumental in furthering the understanding of the three major phases of the monument's construction.\n\nIn 1958 the stones were restored again, when three of the standing sarsens were re-erected and set in concrete bases. The last restoration was carried out in 1963 after stone 23 of the Sarsen Circle fell over. It was again re-erected, and the opportunity was taken to concrete three more stones. Later archaeologists, including Christopher Chippindale of the Museum of Archaeology and Anthropology, University of Cambridge and Brian Edwards of the University of the West of England, campaigned to give the public more knowledge of the various restorations and in 2004 English Heritage included pictures of the work in progress in its book \"Stonehenge: A History in Photographs\".\n\nIn 1966 and 1967, in advance of a new car park being built at the site, the area of land immediately northwest of the stones was excavated by Faith and Lance Vatcher. They discovered the Mesolithic postholes dating from between 7000 and 8000 BC, as well as a length of a palisade ditch – a V-cut ditch into which timber posts had been inserted that remained there until they rotted away. Subsequent aerial archaeology suggests that this ditch runs from the west to the north of Stonehenge, near the avenue.\n\nExcavations were once again carried out in 1978 by Atkinson and John Evans during which they discovered the remains of the Stonehenge Archer in the outer ditch, and in 1979 rescue archaeology was needed alongside the Heel Stone after a cable-laying ditch was mistakenly dug on the roadside, revealing a new stone hole next to the Heel Stone.\n\nIn the early 1980s Julian Richards led the Stonehenge Environs Project, a detailed study of the surrounding landscape. The project was able to successfully date such features as the Lesser Cursus, Coneybury Henge and several other smaller features.\n\nIn 1993 the way that Stonehenge was presented to the public was called 'a national disgrace' by the House of Commons Public Accounts Committee. Part of English Heritage's response to this criticism was to commission research to collate and bring together all the archaeological work conducted at the monument up to this date. This two-year research project resulted in the publication in 1995 of the monograph \"Stonehenge in its landscape\", which was the first publication presenting the complex stratigraphy and the finds recovered from the site. It presented a rephasing of the monument.\n\nMore recent excavations include a series of digs held between 2003 and 2008 known as the Stonehenge Riverside Project, led by Mike Parker Pearson. This project mainly investigated other monuments in the landscape and their relationship to the stones — notably Durrington Walls, where another \"Avenue\" leading to the River Avon was discovered. The point where the Stonehenge Avenue meets the river was also excavated, and revealed a previously unknown circular area which probably housed four further stones, most likely as a marker for the starting point of the avenue. In April 2008 Tim Darvill of the University of Bournemouth and Geoff Wainwright of the Society of Antiquaries, began another dig inside the stone circle to retrieve dateable fragments of the original bluestone pillars. They were able to date the erection of some bluestones to 2300 BC, although this may not reflect the earliest erection of stones at Stonehenge. They also discovered organic material from 7000 BC, which, along with the Mesolithic postholes, adds support for the site having been in use at least 4,000 years before Stonehenge was started. In August and September 2008, as part of the Riverside Project, Julian Richards and Mike Pitts excavated Aubrey Hole 7, removing the cremated remains from several Aubrey Holes that had been excavated by Hawley in the 1920s, and re-interred in 1935. A licence for the removal of human remains at Stonehenge had been granted by the Ministry of Justice in May 2008, in accordance with the \"Statement on burial law and archaeology\" issued in May 2008. One of the conditions of the licence was that the remains should be reinterred within two years and that in the intervening period they should be kept safely, privately and decently.\n\nA new landscape investigation was conducted in April 2009. A shallow mound, rising to about was identified between stones 54 (inner circle) and 10 (outer circle), clearly separated from the natural slope. It has not been dated but speculation that it represents careless backfilling following earlier excavations seems disproved by its representation in eighteenth- and nineteenth-century illustrations. Indeed, there is some evidence that, as an uncommon geological feature, it could have been deliberately incorporated into the monument at the outset. A circular, shallow bank, little more than high, was found between the Y and Z hole circles, with a further bank lying inside the \"Z\" circle. These are interpreted as the spread of spoil from the original Y and Z holes, or more speculatively as hedge banks from vegetation deliberately planted to screen the activities within.\n\nIn July 2010, the Stonehenge Hidden Landscape Project discovered a \"henge-like\" monument less than away from the main site. This new hengiform monument was subsequently revealed to be located \"at the site of Amesbury 50\", a round barrow in the Cursus Barrows group.\n\nOn 26 November 2011, archaeologists from University of Birmingham announced the discovery of evidence of two huge pits positioned within the Stonehenge Cursus pathway, aligned in celestial position towards midsummer sunrise and sunset when viewed from the Heel Stone. The new discovery is part of the Stonehenge Hidden Landscape Project which began in the summer of 2010. The project uses non-invasive geophysical imaging technique to reveal and visually recreate the landscape. According to team leader Vince Gaffney, this discovery may provide a direct link between the rituals and astronomical events to activities within the Cursus at Stonehenge.\n\nOn 18 December 2011, geologists from University of Leicester and the National Museum of Wales announced the discovery of the exact source of some of the rhyolite fragments found in the Stonehenge debitage. These fragments do not seem to match any of the standing stones or bluestone stumps. The researchers have identified the source as a long rock outcrop called Craig Rhos-y-Felin (), near Pont Saeson in north Pembrokeshire, located from Stonehenge.\n\nOn 10 September 2014 the University of Birmingham announced findings including evidence of adjacent stone and wooden structures and burial mounds, overlooked previously, that may date as far back as 4000 BC. An area extending to was studied to a depth of three metres with ground-penetrating radar equipment. As many as seventeen new monuments, revealed nearby, may be Late Neolithic monuments that resemble Stonehenge. The interpretation suggests a complex of numerous related monuments. Also included in the discovery is that the cursus track is terminated by two five-meter wide extremely deep pits, whose purpose is still a mystery.\n\n\n\n\n\n\n\n"}
{"id": "36650277", "url": "https://en.wikipedia.org/wiki?curid=36650277", "title": "Stress resultants", "text": "Stress resultants\n\nStress resultants are simplified representations of the stress state in structural elements such as beams, plates, or shells. The geometry of typical structural elements allows the internal stress state to be simplified because of the existence of a \"thickness'\" direction in which the size of the element is much smaller than in other directions. As a consequence the three traction components that vary from point to point in a cross-section can be replaced with a set of resultant forces and resultant moments. These are the stress resultants (also called \"membrane forces\", \"shear forces\", and \"bending moment\") that may be used to determine the detailed stress state in the structural element. A three-dimensional problem can then be reduced to a one-dimensional problem (for beams) or a two-dimensional problem (for plates and shells).\n\nStress resultants are defined as integrals of stress over the thickness of a structural element. The integrals are weighted by integer powers the thickness coordinate \"z\" (or \"x\"). Stress resultants are so defined to represent the effect of stress as a membrane force \"N\" (zero power in \"z\"), bending moment \"M\" (power 1) on a beam or shell (structure). Stress resultants are necessary to eliminate the \"z\" dependency of the stress from the equations of the theory of plates and shells.\n\nConsider the element shown in the adjacent figure. Assume that the thickness direction is \"x\". If the element has been extracted from a beam, the width and thickness are comparable in size. Let \"x\" be the width direction. Then \"x\" is the length direction.\n\nThe resultant force vector due to the traction in the cross-section (\"A\") perpendicular to the \"x\" axis is\nwhere e, e, e are the unit vectors along \"x\", \"x\", and \"x\", respectively. We define the stress resultants such that\nwhere \"N\" is the \"membrane force\" and \"V\", \"V\" are the shear forces. More explicitly, for a beam of height \"t\" and width \"b\", \nSimilarly the shear force resultants are\n\nThe bending moment vector due to stresses in the cross-section \"A\" perpendicular to the \"x\"-axis is given by\nExpanding this expression we have,\nWe can write the bending moment resultant components as \n\nFor plates and shells, the \"x\" and \"x\" dimensions are much larger than the size in the \"x\" direction. Integration over the area of cross-section would have to include one of the larger dimensions and would lead to a model that is too simple for practical calculations. For this reason the stresses are only integrated through the thickness and the stress resultants are typically expressed in units of force \"per unit length\" (or moment \"per unit length\") instead of the true force and moment as is the case for beams.\n\nFor plates and shells we have to consider two cross-sections. The first is perpendicular to the \"x\" axis and the second is perpendicular to the \"x\" axis. Following the same procedure as for beams, and keeping in mind that the resultants are now per unit length, we have\nWe can write the above as\nwhere the membrane forces are defined as\nand the shear forces are defined as\n\nFor the bending moment resultants, we have\nwhere r = \"x\" e.\nExpanding these expressions we have,\nDefine the bending moment resultants such that\nThen, the bending moment resultants are given by\nThese are the resultants that are often found in the literature but care has to be taken to make sure that the signs are correctly interpreted.\n\n"}
{"id": "37964093", "url": "https://en.wikipedia.org/wiki?curid=37964093", "title": "The Quest: Energy, Security, and the Remaking of the Modern World", "text": "The Quest: Energy, Security, and the Remaking of the Modern World\n\nThe Quest: Energy, Security, and the Remaking of the Modern World is an international bestselling book by energy expert Daniel Yergin. The book was initially published on September 20, 2011 through Penguin Press and is considered to be the follow-up to Yergin’s 1992 Pulitzer Prize winning history of oil, \"\", and describes the development of the current energy system and prospects for the future. Upon its release, the book received praise and criticism both for its breadth of subject as well as for its impartiality. It is often suggested as a “primer” or “guide” to the energy field for the way it combines a narrative across the entire energy spectrum into a single volume.\n\n\"The Quest\" is divided into six sections: \"The New World of Oil\", the first section focuses on developments worldwide in the oil industry following the first Gulf War and the breakup of the Soviet Union. It is noteworthy that \"The Quest\" has two chapters devoted to China, the only country to get such treatment in the book, whereas \"The Prize\" talked very little about China. The second section, \"Securing the Supply\", covers the ways in which concerns over energy security and scarcity have shaped the world’s economy, policies, and planning. This section tells the story of how shale gas was developed. \"The Electric Age\", the third section, follows electricity’s rise, as well as the risks of nuclear proliferation, hurdles faced in ensuring that “the lights stay on” and the current challenges for fuel choice.\n\nThe fourth section, \"Climate and Carbon\", lays out in six chapters the history of climate science from the “discovery” of the atmosphere in the late 18th Century and of the “ice age” in the early 19th century to its current central role in energy policy and debate. \"New Energies\", the fifth section, focuses on the “rebirth of renewables” and the role of energy efficiency. The final section, \"The Road to the Future\" describes the evolution of personal mobility and the growing global demand for automobiles. Yergin covers the topics of 3rd and 4th generation biofuels, hydrogen fuel cells, natural gas vehicles, and what he calls “round two in the race between gasoline and the electric car.” The book concludes with a discussion of the impact of the “globalization of innovation” on future energy supplies.\n\nReception of \"The Quest\" has been generally positive, with \"Asahi Shimbun\" listing it as a \"Book of the Year\". Praise for the book has predominantly centered on Yergin's coverage of the energy system, with \"The New York Times\" referring to the book as \"necessary reading\" and \"The Economist\" calling it a \"comprehensive guide to the world's great energy needs and dilemmas\".\n\nCritiques of \"The Quest\" most often fall into three main categories. Some critics commented on the book's 800 page length, noting that it was not necessarily for the casual reader and that it felt like several different books in one. Others faulted Yergin for his impartiality and not taking a strong position on subjects such as climate change, although \"The Wall Street Journal\" said that the book succumbed to \"the conventional alarmist storyline” on climate. Finally, peak oil advocates take strong issue with Yergin’s handling of the subject in the book, particularly the discussion of the founding of the peak theory and the potential for future oil discoveries.\n\n"}
{"id": "36044905", "url": "https://en.wikipedia.org/wiki?curid=36044905", "title": "Waste management in Russia", "text": "Waste management in Russia\n\nRussia is a big producer of waste as one of the biggest economies in the world.\n\nOver 200 cities in Russia exceed pollution limits, and this is increasing as more vehicles appear on the roads.\n\nFederal law \"On Production and Consumption of Waste and the Development of Basic Regulations\".\nSources: \n\nAttempts to create a comprehensive legislative act which would regulate radioactive waste management in Russia have been made since 1992. In 1995, a draft federal law \"On the State Policy in the Sphere of Radioactive Waste Management\" was developed. However, the Federation Council rejected the specified bill. A conciliation committee was established. After certain amendments have been made to the bill, its name was changed to \"On Radioactive Waste Management\". This wording was adopted by two chambers of Federal Assembly of the Russian Federation and was submitted for signature to the president. However, Russian president dismissed the bill and sent it for revision. As a result, in 2001, the specified bill was excluded from further consideration by the State Duma. In 2008, Rosatom Federal Atomic Energy Agency promulgated the bill \"On Radioactive Waste Management\".\n"}
{"id": "46570796", "url": "https://en.wikipedia.org/wiki?curid=46570796", "title": "Windpark Ellern", "text": "Windpark Ellern\n\nWindpark Ellern is a wind farm facility located in Ellern in the Rhein-Hunsrück-Kreis in Rhineland-Pfalz, Germany. Positioned along the crest of the Soonwald, the wind farm is one of the largest in southwestern Germany, and combined with a nearby wind farm in Kirchberg is among the country's major on-shore wind power areas.\nAccording to the energy company juwi, the eight wind turbine facility is capable of generating up to 46.5MW of energy, enough to power 33,000 residences and other buildings of other purposes, and cut down 84,700 tonnes of carbon dioxide emissions, equivalent to taking off the road 18,140 fossil-fed cars.\n\nConstruction of the facility began in 2012, consisting of five 135 metre tall turbines manufactured by Enercon. Three other turbines were constructed shortly thereafter. While construction activity has provided jobs to the area, the facility has been controversial. Some local residents have formed protest organizations, arguing that the turbines have negatively affected the bucolic nature of the Soonwald and the Hunsrück.\n\n"}
{"id": "49165880", "url": "https://en.wikipedia.org/wiki?curid=49165880", "title": "Wood industry", "text": "Wood industry\n\nThe wood industry or lumber industry is a - usually private - economic sector concerned with forestry, logging, timber trade, and the production of forest products, timber/lumber, primary forest and wood products (e.g. furniture) and secondary products like wood pulp for the pulp and paper industry. Some largest producers are also among the biggest timberland owners.\n\nThe wood industry plays a dominating role in today's wood economy.\n\nIn the narrow sense of the terms, wood, forest, forestry and timber/lumber industry appear to point to different sectors, in the industrialized, internationalized world, there is a tendency toward huge integrated businesses that cover the complete spectrum from silviculture and forestry in private primary or secondary forests or plantations via the logging process up to wood processing and trading and transport (e.g. timber rafting, forest railways, logging roads).\n\nProcessing and products differs especially with regard to the distinction between softwood and hardwood. While softwood primarily goes into the production of wood fuel and pulp and paper, hardwood is used mainly for furniture, floors, etc.. Both types can be of use for building and (residential) construction purposes (e.g. log houses, log cabins, timber framing).\n\nAfter logging, which is now typically done using large harvesters, the trunks of the felled trees are cut in lumber mills.\n\nIn 2012, three of the top timberland owners in the USA by market capitalization were\nIn 2008 the largest lumber and wood producers in the USA were\nAccording to sawmilldatabase, the world top producers of sawn wood in 2007 were:\n\nWorkers within the Forestry and Logging industry sub-sector fall within the Agriculture, Forestry, Fishing, and Hunting (AFFH) industry sector as characterized by the North American Industrial Classification System (NAICS). The National Institute for Occupational Safety and Health (NIOSH) has taken a closer look at the AFFH industry's noise exposures and prevalence of hearing loss. While the overall industry sector had a prevalence of hearing loss lower than the overall prevalence of noise-exposed industries (15% v. 19%), workers within Forestry and Logging exceeded 21%. Thirty-six percent of workers within Forest Nurseries and Gathering of Forest Products, a sub-sector within Forestry and Logging, experienced hearing loss, the most of any AFFH sub-sector. Workers within Forest Nurseries and Gathering of Forest Products are tasked with growing trees for reforestation and gathering products such as rhizomes and barks. Comparatively, non-noise-exposed workers have only a 7% prevalence of hearing loss.\n\nWorker noise exposures in the Forestry and Logging industry have been found to be up to 102 dBA. NIOSH recommends that a worker have an 8 hour time-weighted average of noise exposure of 85 dBA. Excessive noise puts workers at an increased risk of developing hearing loss. If a worker were to develop a hearing loss as a result of occupational noise exposures, it would be classified as occupational hearing loss. Noise exposures within the Forestry and Logging industry can be reduced by enclosing engines and heavy equipment, installing mufflers and silencers, and performing routine maintenance on equipment. Noise exposures can also be reduced through the hierarchy of hazard controls where removal or replacement of noisy equipment serves as the best method of noise reduction.\n\nThe Bureau of Labor Statistics (BLS) has found that fatalities of Forestry and Logging workers have increased from 2013 to 2016, up from 81 to 106 per year. In 2016, there were 3.6 cases of injury and illness per 100 workers within this industry.\n\n\n"}
