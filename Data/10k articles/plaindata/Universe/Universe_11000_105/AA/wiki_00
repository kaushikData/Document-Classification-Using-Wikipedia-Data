{"id": "44601606", "url": "https://en.wikipedia.org/wiki?curid=44601606", "title": "2014 Israeli oil spill", "text": "2014 Israeli oil spill\n\nIn December 2014, a major oil spill occurred in the vicinity of Be'er Ora Israel, with an estimated 3-5 million liters of crude oil leaking from a breached pipeline, contaminating much of the Evrona nature reserve. An Environmental Protection Ministry official stated that the cleanup would likely take years, and that the spill was one of the gravest natural disasters in the country's history.\n\nIn light of this, On December 9, 2014, Ofir Akunis was appointed deputy environmental protection minister. Akunis replaced Amir Peretz, who resigned from his post at the helm of the ministry. The appointment, by Prime Minister Benjamin Netanyahu's office, came a week after the massive crude oil spill in Israel's Arava region. Akunis said that \"it is the deputy minister's intention to prioritize treatment of the ecological disaster in the south and to do all that is necessary to prevent the spread of the crude oil, and to prevent a health and environmental disaster.\" Akunis ordered crews to raise the side walls of the dams that had been built in the Arava to prevent flooding. Thanks to the infrastructure that had already been built in the area, the risk that the oil would reach the Gulf of Eilat significantly dropped. Akunis instructed that the reserve remain closed as long as high values of pollutants were still registered in air quality tests.\n\nIn the end of December 2014, the government approved a NIS 17 million Environmental Protection Ministry plan to rehabilitate. According to Akunis, the NIS 17 million program will serve to treat the soils contaminated by the spill as well as help restore the wildlife populations damaged over the course of the event. As part of the plan, a special team will be appointed to evaluate the environmental impact of various Eilat Ashkelon Pipeline Company activities on both dry land and beaches. The approved plan also involves opening a closed Eilat beach on EAPC-owned property to the city's residents and visitors. In January 2015, air quality tests found that there had been a 90% reduction of pollution in Evrona.\n"}
{"id": "51984967", "url": "https://en.wikipedia.org/wiki?curid=51984967", "title": "Agaric acid", "text": "Agaric acid\n\nAgaric acid, also known as agaricin or 2-hydroxynonadecane-1,2,3-tricarboxylic acid, is an organic tricarboxylic acid (fatty acid) found in fungi, e.g. \"Laricifomes officinalis\". Its molecular formula is CHO.\n\nAgaric acid, as any other fatty acid, has an amphipathic character. It means that it has both polar (hydroxyl groups) and apolar (hydrocarbon chain) sections, and therefore, it is not completely water-soluble. It is a tribasic acid, and therefore, it can donate up to 3 hydrogen ions to other bases in an acid-base reaction. Another example of tribasic acids is phosphoric acid or citric acid.\nIt is an odorless and tasteless acid, and we can also distinguish it by its white color. Its melting point at atmospheric pressure is 140 °C.\n\nAgaric acid is a type of fatty acid that is composed by a long hydrocarbon chain (\"tail\") and three carboxylic acid groups at one end (\"head\"). The hydrocarbon chain has sixteen carbons and thirty four hydrogens.\n\nThis acid has microcrystalline properties, and therefore, forms small crystals that can not be seen through the naked eye, but are only visible with an optical microscope.\n\nAgaric acid is used as an inhibitor of metabolism in several animal experiments. It is shown that this acid prevents the formation of C units from citrate and reduces the availability of citrate for the activation of acetyl-CoA carboxylase.\nMoreover, it has an important role in the metabolism of lipids, because it influences sterol synthesis.\n\nAgaric acid induces the mitochondrial permeability transition by collaborating with adenine nucleotide translocase. It facilitates the efflux of accumulated Ca, disrupts the potential of the membrane and causes mitochondrial lumps. All of these effects bet on membrane fluidity. It's thought that agaric acid activates the opening of membrane pores due to the union of citrate to ADP transporters.\n\nHowever, a later research showed that N-ethylmaleimide inhibits carboxyatractyloside and agaric acid effects. It was found that this amine restricts the pore opening action of agaric acid, but it does not affect the constraint of ADP exchange by agaric acid.\n\nAgaric acid is used in medicine as an anhidrotic agent in order to stop excessive perspiration as it paralyses the nerve terminations in the human body's sweat glands. For example, it helps to avoid tuberculosis patients' frequent night sweats. In addition, when taken in doses from 5 to 15 grams, agaric acid produces vomiting in humans. In the past, agaric acid was used as an irritant, an antidiarrhoeal and a bronchial secretions reducer.\n\nPhysicians use agaric acid, but it also can be used in many other subjects such as veterinary and biochemistry.\nIn lower animals, this substance depresses the nervous, respiratory and circulatory systems. It has been used as a metabolic inhibitor at the cellular and subcellular level in scientific animal experiments. Agaric acid has also been used as an alpha-glycerophosphate dehydrogenase inhibitor in \"Crithidia fasciculata\", which is a species of parasitic protist.\n"}
{"id": "49615976", "url": "https://en.wikipedia.org/wiki?curid=49615976", "title": "Akaogiite", "text": "Akaogiite\n\nAkaogiite is an exceedingly rare mineral, one of natural forms of titanium dioxide. It is the high-pressure polymorph of TiO, polymorphous with anatase, brookite and another high-pressure phase called \"TiO II\". \n\nIt can be found at the Nördlinger Ries Crater, a meteor crater in Germany, where the extreme pressure during the impact allowed its formation.\n"}
{"id": "11686976", "url": "https://en.wikipedia.org/wiki?curid=11686976", "title": "Aluminium-conductor steel-reinforced cable", "text": "Aluminium-conductor steel-reinforced cable\n\nAluminium conductor steel-reinforced cable (ACSR) is a type of high-capacity, high-strength stranded conductor typically used in overhead power lines. The outer strands are high-purity aluminium, chosen for its good conductivity, low weight and low cost. The center strand is steel for additional strength to help support the weight of the conductor. Steel is higher strength than aluminium which allows for increased mechanical tension to be applied on the conductor. Steel also has lower elastic and inelastic deformation (permanent elongation) due to mechanical loading (e.g. wind and ice) as well as a lower coefficient of thermal expansion under current loading. These properties allow ACSR to sag significantly less than all-aluminium conductors. Per the International Electrotechnical Commission (IEC) and The CSA Group (formerly the Canadian Standards Association or CSA) naming convention, ACSR is designated A1/S1A.\n\nThe aluminium alloy and temper used for the outer strands in the United States and Canada is normally 1350-H19 and elsewhere is 1370-H19, each with 99.5+% aluminium content. The temper of the aluminium is defined by the aluminium version's suffix, which in the case of H19 is extra hard.\nTo extend the service life of the steel strands used for the conductor core they are normally galvanized, or coated with another material to prevent corrosion. The diameters of the strands used for both the aluminum and steel strands vary for different ACSR conductors.\n\nACSR cable still depends on the tensile strength of the aluminium; it is only \"reinforced\" by the steel. Because of this, its continuous operating temperature is limited to , the temperature at which aluminium begins to anneal and soften over time. Cable which relies entirely on the steel for strength, and so can be used at temperatures up to , is called aluminium-conductor steel-supported (ACSS).\n\nThe standard steel core used for ACSR is galvanized steel, but zinc, 5% or 10% aluminium alloy and trace mischmetal coated steel (sometimes called by the trade-names Bezinal or Galfan) and aluminium-clad steel (sometimes called by the trade-name Alumoweld) are also available. Higher strength steel may also be used.\n\nIn the United States the most commonly used steel is designated GA2 for galvanized steel (G) with class A zinc coating thickness (A) and regular strength (2). Class C zinc coatings are thicker than class A and provide increased corrosion protection at the expense of reduced tensile strength. A regular strength galvanized steel core with Class C coating thickness would be designated GC2. Higher strength grades of steel are designated high-strength (3), extra-high-strength (4), and ultra-high-strength (5). An ultra-high-strength galvanized steel core with class A coating thickness would be designated GA5. The use of higher strength steel cores increases the tensile strength of the conductor allowing for higher tensions which results in lower sag.\n\nZinc-5% aluminium mischmetal coatings are designated with an \"M\". These coatings provide increased corrosion protection and heat resistance compared to zinc alone. Regular strength Class \"A\" mischmetal thickness weight coated regular strength steel would be designated MA2.\n\nAluminium-clad steel is designated as \"AW\". Aluminium-clad steel offers increased corrosion protection and conductivity at the expense of reduced tensile strength. Aluminium-clad steel is commonly specified for coastal applications.\n\nIEC and CSA use a different naming convention. The most commonly used steel is S1A for S1 regular strength steel with a class A coating. S1 steel has slightly lower tensile strength than the regular strength steel used in the United States. Per the Canadian CSA standards the S2A strength grade is classified as High Strength steel. The equivalent material per the ASTM standards is the GA2 strength grade and called Regular Strength steel. The CSA S3A strength grade is classified as Extra High Strength steel. The equivalent material per the ASTM standards is the GA3 strength grade called High Strength. The present day CSA standards for overhead electrical conductor do not yet officially recognize the ASTM equivalent GA4 or GA5 grades. The present day CSA standards do not yet officially recognize the ASTM \"M\" family of zinc alloy coating material. Canadian utilities are using conductors built with the higher strength steels with the \"M\" zinc alloy coating.\n\nLay of a conductor is determined by four extended fingers; \"right\" or \"left\" direction of the lay is determined depending if it matches finger direction from right hand or left hand respectively. Overhead aluminium (AAC, AAAC, ACAR) and ACSR conductors in the USA are always manufactured with the outer conductor layer with a right-hand lay. Going toward the center each layer has alternating lays. Some conductor types (e.g. copper overhead conductor, OPGW, steel EHS) are different and have left-hand lay on the outer conductor. Some South American countries specify left-hand lay for the outer conductor layer on their ACSR, so those are wound differently than those used in the USA.\n\nACSR conductors are available in numerous specific sizes, with single or multiple center steel wires and generally larger quantities of aluminium strands. Although rarely used, there are some conductors that have more steel strands than aluminum strands. An ACSR conductor can in part be denoted by its stranding, for example, an ACSR conductor with 72 aluminium strands with a core of 7 steel strands will be called 72/7 ACSR conductor. Cables generally range from #6 AWG (\"6/1\" – six outer aluminum conductors and one steel reinforcing conductor) to 2167 kcmil (\"72/7\" – seventy two outer aluminum conductors and seven steel reinforcing conductors).\n\nTo help avoid confusion due to the numerous combinations of stranding of the steel and aluminium strands, code words are used to specify a specific conductor version. In North America bird are used for the code words while animal names are used elsewhere. For instance in North America, Grosbeak is a (636 kcmil) ACSR conductor with 26/7 Aluminium/Steel stranding whereas Egret is the same total aluminium size (, 636 kcmil conductor) but with 30/19 Aluminium/Steel stranding. Although the number of aluminium strands is different between Grosbeak and Egret, differing sizes of the aluminium strands are used to offset the change in the number of strands such that the total amount of aluminium remains the same. Differences in the number of steel strands result in varying weights of the steel portion and also result in different overall conductor diameters. Most utilities standardize on a specific conductor version when various versions of the same amount of aluminum to avoid issues related to different size hardware (such as splices). Due to the numerous different sizes available, utilities often skip over some of the sizes to reduce their inventory. The various stranding versions result in different electrical and mechanical characteristics.\n\nManufacturers of ACSR typically provide ampacity tables for a defined set of assumptions. Individual utilities normally apply different ratings due to using varying assumptions (which may be a result in higher or lower amperage ratings than those the manufacturers provide). Significant variables include wind speed and direction relative to the conductor, sun intensity, emissivity, ambient temperature, and maximum conductor temperature.\n\nIn three phase electrical power distribution, conductors must be designed to have low electrical impedance in order to assure that the power lost in the distribution of power is minimal. Impedance is a combination of two quantities: resistance and reactance. The resistances of ASCR conductors are tabulated for different conductor designs by the manufacturer at DC and AC frequency assuming specific operating temperatures. The reasons that resistance changes with frequency are largely due to the skin effect, the proximity effect, and hysteresis loss. Depending on the geometry of the conductor as differentiated by the conductor name, these phenomena have varying degrees of affecting the overall resistance in the conductor at AC vs DC frequency.\n\nOften not tabulated with ACSR conductors is the electrical reactance of the conductor, which is due largely to the spacing between the other current carrying conductors and the conductor radius. The reactance of the conductor contributes significantly to the overall current that needs to travel through the line, and thus contributes to resistive losses in the line. For more information on transmission line inductance and capacitance, see electric power transmission and overhead power line.\n\nThe skin effect decreases the cross sectional area in which the current travels through the conductor as AC frequency increases. For alternating current, most (63%) of the electric current flows between the surface and the skin depth, δ, which depends on the frequency of the current and the electrical (conductivity) and magnetic properties of the conductor. This decreased area causes the resistance to rise due to the inverse relationship between resistance and conductor cross sectional area. The skin effect benefits the design, as it causes the current to be concentrated towards the low-resistivity aluminum on the outside of the conductor. To illustrate the impact of the skin effect, the American Society for Testing and Materials (ASTM) standard includes the conductivity of the steel core when calculating the DC and AC resistance of the conductor, but the (IEC) and The CSA Group standards do not.\n\nIn a conductor (ACSR and other types) carrying AC current, if currents are flowing through one or more other nearby conductors the distribution of current within each conductor will be constrained to smaller regions. The resulting current crowding is termed as the proximity effect. This crowding gives an increase in the effective AC resistance of the circuit, with the effect at 60 Hertz being greater than at 50 Hertz. Geometry, conductivity, and frequency are factors in determining the amount of proximity effect.\n\nThe proximity effect is result of a changing magnetic field which influences the distribution of an electric current flowing within an electrical conductor due to electromagnetic induction. When an alternating current (AC) flows through an isolated conductor, it creates an associated alternating magnetic field around it. The alternating magnetic field induces eddy currents in adjacent conductors, altering the overall distribution of current flowing through them.\n\nThe result is that the current is concentrated in the areas of the conductor furthest away from nearby conductors carrying current in the same direction.\n\nHysteresis in an ACSR conductor is due to the atomic dipoles in the steel core changing direction due to induction from the 60 or 50 Hertz AC current in the conductor. Hysteresis losses in ACSR are undesirable and can be minimized by using an even number of aluminium layers in the conductor. Due to the cancelling effect of the magnetic field from the opposing lay (right-hand and left-hand) conductors for two aluminium layers there is significantly less hysteresis loss in the steel core than there would be for one or three aluminium layers where the magnetic field does not cancel out.\n\nThe hysteresis effect is negligible on ACSR conductors with even numbers of aluminium layers and so it is not considered in these cases. For ACSR conductors with an odd number of aluminium layers however, a magnetization factor is used to accurately calculate the AC resistance. The correction method for single-layer ACSR is different than that used for three-layer conductors. Due to applying the magnetization factor, a conductor with an odd number of layers has an AC resistance slightly higher than an equivalent conductor with an even number of layers.\n\nDue to higher hysteresis losses in the steel and associated heating of the core, an odd-layer design will have a lower ampacity rating (up to a 10% de-rate) than an equivalent even-layer design.\n\nAll standard ACSR conductors smaller than Partridge ( {266.8 kcmil} 26/7 Aluminium/Steel) have only one layer due to their small diameters so the hysteresis losses cannot be avoided.\n\nACSR is widely used due to its efficient and economical design. Variations of standard (sometimes called traditional or conventional) ACSR are used in some cases due to the special properties they offer which provide sufficient advantage to justify their added expense. Special conductors may be more economic, offer increased reliability, or provide a unique solution to an otherwise difficult, of impossible, design problem.\n\nThe main types of special conductors include \"trapezoidal wire conductor\" (TW) - a conductor having aluminium strands with a trapezoidal shape rather than round) and \"self-damping\" (SD), sometimes called \"self-damping conductor\" (SDC). A similar, higher temperature conductor made from annealed aluminium is called \"aluminium conductor steel supported\" (ACSS) is also available.\n\nTrapezoidal-shaped wire (TW) can be used in lieu of round wire in order to \"fill in the gaps\" and have a 10–15% smaller overall diameter for the same cross-sectional area or a 20–25% larger cross-sectional area for the same overall diameter.\n\nOntario Hydro (Hydro One) introduced trapezoidal-shaped wire ACSR conductor designs in the 1980s to replace existing round-wire ACSR designs (they called them compact conductors; these conductor types are now called ACSR/TW). Ontario Hydro's trapezoidal-shaped wire (TW) designs utilized the same steel core but increased the aluminium content of the conductor to match the overall diameter of the former round-wire designs (they could then use the same hardware fittings for both the round and the TW conductors). Hydro One's designs for their trapezoidal ACSR/TW conductors only use even numbers of aluminium layers (either two layers or four layers). They do not use designs which have odd number of layers (three layers) due to that design incurring higher hysteresis losses in the steel core.\nAlso in the 1980s, Bonneville Power Administration (BPA) introduced TW designs where the size of the steel core was increased to maintain the same Aluminium/Steel ratio.\n\nSelf-damping (ACSR/SD) is a nearly obsolete conductor technology and is rarely used for new installations. It is a concentric-lay stranded, self-damping conductor designed to control Aeolian-type vibration in overhead transmission lines by internal damping. Self-damping conductors consists of a central core of one or more round steel wires surrounded by two layers of trapezoidal shaped aluminium wires. One or more layers of round aluminium wires may be added as required.\n\nSD conductor differs from conventional ACSR in that the aluminium wires in the first two layers are trapezoidal shaped and sized so that each aluminium layer forms a stranded tube which does not collapse onto the layer beneath when under tension, but maintains a small annular gap between layers. The trapezoidal wire layers are separated from each other and from the steel core by the two smaller annular gaps that permit movement between the layers. The round aluminium wire layers are in tight contact with each other and the underlying trapezoidal wire layer.\n\nUnder vibration, the steel core and the aluminium layers vibrate with different frequencies and impact damping results. This impact damping is sufficient to keep any Aeolian vibration to a low level. The use of trapezoidal strands also results in reduced conductor diameter for a given AC resistance per mile.\n\nThe major advantages ACSR/SD are:\n\nThe major disadvantages ACSR/SD are:\n\nAluminium-conductor steel supported (ACSS) conductor visually appears to be similar to standard ACSR but the aluminium strands are fully annealed. Annealing the aluminium strands reduces the composite conductor strength, but after installation, permanent elongation of the aluminium strands results in a much larger percentage of the conductor tension being carried in the steel core than is true for standard ACSR. This in turn yields reduced composite thermal elongation and increased self-damping.\n\nThe major advantages of ACSS are:\n\nThe major disadvantages of ACSS are:\n\nTwisted pair (TP) conductor (sometimes called by the trade-names T-2 or VR) has the two sub-conductors twisted (usually with a left-hand lay) about one another generally with a lay length of approximately three meters (nine feet).\n\nThe conductor cross-section of the TP is a rotating \"figure-8\". The sub-conductors can be any type of standard ACSR conductor but the conductors need to match one another to provide mechanical balance.\n\nThe major advantages of TP conductor are:\n\nThe major disadvantages of TP conductor are:\n\nMany electrical circuits are longer than the length of conductor which can be contained on one reel. As a result, splicing is often necessary to join together conductors to provide the desired length. It is important that the splice not be the weak link. A splice (joint) must have high physical strength along with a high electrical current rating. Within the limitations of the equipment used to install the conductor from the reels, as long of a length of conductor is generally purchased that the reel can accommodate to avoid more splices than are absolutely necessary.\n\nSplices are designed to run cooler than the conductor. The temperature of the splice is kept lower by having a larger cross-sectional area and thus less electrical resistance than the conductor. Heat generated at the splice is also dissipated faster due to the larger diameter of the splice.\n\nFailures of splices are a significant concern as a failure of just one splice can cause an outage that affects a large amount of electrical load.\n\nMost splices are compression-type splices (crimps). These splices are inexpensive and have good strength and conductivity characteristics.\n\nSome splices, called automatics, utilize a jaw-type design that is faster to install (does not require the heavy compression equipment) and are often used during storm restoration when speed of installation is more important than the long term performance of the splice.\n\nCauses for splice failures are numerous. Some of the main failure modes are related to installation issues, such as: insufficient cleaning (wire brushing) of the conductor to eliminate the aluminium oxide layer (which has a high resistance {a poor electrical conductor}), improper application of conducting grease, improper compression force, improper compression locations or number of compressions.\n\nSplice failures can also be due to Aeolian vibration damage as the small vibrations of the conductor over time cause damage (breakage) of the aluminium strands near the ends of the splice.\n\nSpecial splices (two piece splices) are required on SD-type conductors as the gap between the trapezoidal aluminium layer and the steel core prevents the compression force on the splice to the steel core to be adequate. A two-piece design has a splice for the steel core and a longer and larger diameter splice for the aluminium portion. The outer splice must be threaded on first and slid along the conductor and the steel splice compressed first and then the outer splice is slid back over the smaller splice and then compressed. This complicated process can easily result in a poor splice.\n\nWhen ACSR is new, the aluminium has a shiny surface which has a low emissivity for heat radiation and a low absorption of sunlight. As the conductor ages the color becomes dull gray due to the oxidation reaction of the aluminium strands. In high pollution environments, the color may almost turn black after many years of exposure to the elements and chemicals. For aged conductor, the emissivity for heat radiation and the absorption of sunlight increases. Conductor coatings are available that have a high emissivity for high heat radiation and a low absorption of sunlight. These coatings would be applied to new conductor during manufacture. These types of coatings have the ability to potentially increase the current rating of the ACSR conductor. For the same amount of amperage, the temperature of the same conductor will be lower due to the better heat dissipation of the higher emissivity coating.\n"}
{"id": "4122092", "url": "https://en.wikipedia.org/wiki?curid=4122092", "title": "BWOC", "text": "BWOC\n\nBWOC (Bob Wayne's Oil Company) is a petrol company in the United Kingdom. It has over 100 forecourts. It is notable for being one of only two companies licensed to sell leaded petrol (4 star) in the United Kingdom. It has an oil facility in Avonmouth.\n"}
{"id": "31433496", "url": "https://en.wikipedia.org/wiki?curid=31433496", "title": "Berkeley Partners for Parks", "text": "Berkeley Partners for Parks\n\nBerkeley Partners for Parks (BPFP) is a nonprofit organization, made up of volunteers, whose mission is to \"build vibrant, healthy, ecologically sound communities by providing the infrastructure that volunteer groups need in order to improve the beauty and usefulness of public space and recreation in and around Berkeley, California.\" \n\nBPFP encourages volunteerism and community development for parks, community gardens, natural habitat, and open space, and recreation. BPFP helps citizens form new groups, helps those groups find needed financial and volunteer resources, provides voices of experience and help with publicity, and serves as a 501(c)3 nonprofit fiscal sponsor, providing bookkeeping, tax filing, insurance, and the ability to receive tax-free donations and grants. \n\nPartner groups include Friends of Five Creeks, Berkeley Path Wanderers Association, East Bay Green Parks Assn., Every Kid 2 Swim, Aquatic Park EGRET, Friends of Halcyon Commons, Friends of King Park, Los Amigos de Codornices, Schoolhouse Creek Common, Friends of Shorebird Park, and Friends of Westbrae Commons. These groups do such things as restore creeks, build paths and steps, remove invasives, plant natives and drought-tolerant landscaping, raise money for recreation, install public art and interpretive signs, and create new public parks such as Halcyon Commons and Schoolhouse Creek Common.\n\n"}
{"id": "45232552", "url": "https://en.wikipedia.org/wiki?curid=45232552", "title": "Bleach activator", "text": "Bleach activator\n\nBleach activators are compounds that allow a lower washing temperature than would be required otherwise to achieve the full activity of bleaching agents in the wash liquor. Bleaching agents, usually peroxides, are usually sufficiently active only from 60 °C on. With bleach activators, this activity can already be achieved at lower temperatures. Bleach activators react with hydrogen peroxide in aqueous solution to form peroxy acids, they are a component of most laundry detergents. Peroxy acids are more active bleaches than hydrogen peroxide at lower temperatures (<60 °C) but are too unstable to be stored in their active form and hence must be generated in situ. \n\nThe most common bleach activators used commercially are tetraacetylethylenediamine (TAED) and sodium nonanoyloxybenzenesulfonate (NOBS). NOBS is the main activator used in the U.S.A. and Japan, TAED is the main activator used in Europe.\n\nBleach activators are typically made up of two parts: the peroxy acid precursor and the leaving group; and are modified by altering these parts. The peroxy acid precursor affects the bleaching properties of the peroxy acid: determining the activity, selectivity, hydrophobic/hydrophilic balance and oxidation potential. The leaving group influences the solubility, perhydrolysis rate and storage stability of the activator.\n\nBleach activation is also known as perhydrolysis. Persalts are inorganic salts that are used as hydrogen peroxide carriers (examples include sodium percarbonate and sodium perborate). Persalts and bleach activators are included together in powder laundry detergents that contain bleach. In the wash, both compounds dissolve in the water. When dissolved in water, the persalt releases hydrogen peroxide (\"e.g.\" from sodium percarbonate): \n\nIn a basic wash solution, hydrogen peroxide loses a proton and is converted to the perhydroxyl anion:\n\nThe perhydroxyl anion then attacks the activator, forming a peroxy acid: \n\nThe overall reaction of TAED (1) with 2 equivalents of hydrogen peroxide gives diacetylethylenediamine (2) and 2 equivalents of peracetic acid (3):\nOnly the perhydroxyl anion, and not the hydrogen peroxide molecule, reacts with the bleach activator. In aqueous solutions, the hydroxide ion is also present, but owing to the greater nucleophilicity of the perhydroxyl anion, it will react preferentially. Once formed, the peroxy acid can act as a bleach.\n\nThe consumption of bleach activators in 2002 was approximately 105,000 tonnes. Consumption, however, is stagnant or declining due to cost pressures on detergents and the advance of liquid detergent formulations (which contain no bleach and bleach activators). The relatively high cost of conventional bleaching systems restrict their spread in emerging markets, where cold water is used for washing and photobleaching by sunlight is widespread or the use of sodium hypochlorite solution (as in the US). \n\nThere remains considerable potential in Europe for more active bleach activators due to the significant potential energy savings achievable by washing at lower temperatures, but their higher activity must not be accompanied by greater damage to textile dyes and fibers. In addition to stain bleaching in laundry, the disinfecting and deodorizing effects of bleach/activator combinations also play an important role. Therefore, they are also used in dishwashing detergents and denture cleaners.\n\nTypical bleach activators are essentially \"N\"- and \"O\"-acyl compounds that form peroxyacids upon perhydrolysis (meaning hydrolysis by hydrogen peroxide from the bleach, persalts). For example, TAED produces in the wash liquor bleach-active peroxyacetic acid or from DOBA peroxydodecanoic acid. In all cases, the activator is chemically reacted according to the degree of contamination in the laundry and thus \"consumed\".\n\nThe literature describes a variety of active \"N\"-acyl compounds, such as tetraacetyl glycoluril and other acylated saturated nitrogen-containing heterocycles, such as hydantoins, hydrotriazines, diketopiperazines, etc., as well as acylated imides and lactams. A disadvantage of these compounds compared to the standard compound TAED is their usually poorer economic and ecological performance.\n\nIn addition to the acylated phenol derivatives NOBS, LOBS and DOBA (negatively charged in the aqueous medium), further bleach-active O-acyl compounds are described, for example tetraacetylxylose or pentaacetylglucose. DOBA, commonly used in Japan, is characterized by good biodegradability and greater effect on a number of microorganisms compared to TAED. Both work together synergistically. Furthermore, nitriles, such as cyanopyridine and cyanamides, cyanomorpholine and in particular cyanomethyl trialkyl/arylammonium salts are known bleach activators (the latter, the so-called nitrile quats, are present in aqueous solution as cations).\n\nNitrile quats are active in bleaching even at temperatures around 20 °C and act via peroxoimino acids that are formed intermediately from peroxo compounds. These decompose to the corresponding quaternary amides, which react with the help of hydrogen peroxide to the corresponding, readily biodegradable betaines. A disadvantage of nitrile quats is the poor biodegradability of the original substances and their often pronounced hygroscopicity, which, however, can be reduced by suitable counterions.\n\nOther new bleaching systems have been developed, especially for washing at lower temperatures and room temperature and for use in liquid detergent formulations:\n\n"}
{"id": "57709336", "url": "https://en.wikipedia.org/wiki?curid=57709336", "title": "BlueSG", "text": "BlueSG\n\nBlueSG (stylised as blueSG) is a Singapore based company providing electric car sharing services. Announced in September 2017, the company, a subsidiary of the Bolloré group, launched the service in December of the same year, with 30 charging stations and 80 all-electric Bolloré Bluecar for public use on a paid subscription basis.\n\nIn June, 2016, Bolloré signed an agreement with the Land Transport Authority (LTA) and the Economic Development Board to develop an electric car-sharing programme. On September 27, 2017, BlueSG announced with LTA to launch Singapore's first large scale electric car sharing programme, with the service's electric car supplied by Bolloré. The company also opened their Asia-Pacific headquarters for its e-mobility, energy management and system integration business for the region. Bolloré already operates several similar electric car-sharing services such as Autolib', BlueIndy and Bluecity in Paris, Indianapolis and London respectively. \n\nConstruction of the charging stations began at the end of September, 2017 and in December, 2017, the service was officially opened to the public with 30 charging stations and 80 cars located throughout the island. BlueSG plans to expand the service to offer 2,000 charging points in 500 charging locations, with 400 charging points open for public use and 1,000 electric cars deployed by 2020. In January, 2018, within the first 3 weeks of operations, over 3000 members signed up for the service, with 5000 rentals completed.\n\nAnyone aged 21 years and above with a valid Singapore driving license, ASEAN driving license or an International driver’s permit can sign up for the service through the BlueSG mobile application or their official website. Users can choose between 2 rental plans, with\nper minute fees varying from $S0.33 to $S0.50 depending on the rental plan. An available car can only be booked through the mobile application before users can collect their car at the charging station. A parking space also needs to be reserved through the application prior to returning the car to the charging station. \n\nThe table below summaries the subscription types available, the subscription fees and pricing for the per minute rates:\n\nThe service uses the all electric Bolloré Bluecar, which were adapted to suit Singapore's left-hand traffic. It is a three-door hatchback electric car with four seats and has a 30kWh lithium metal polymer (LMP) battery, coupled to a supercapacitor, that provides an electric range of in urban use, and a maximum speed of .\n\n\n"}
{"id": "42747411", "url": "https://en.wikipedia.org/wiki?curid=42747411", "title": "Brushy Fork Coal Impoundment", "text": "Brushy Fork Coal Impoundment\n\nThe Brushy Fork Coal Impoundment, also known as the Brushy Fork Coal Sludge Dam, is a large tailings dam on the Brushy Fork near Marfork in western Raleigh County of West Virginia, United States. It is located northwest of Beckley, the seat of Raleigh County. Brushy Fork flows into Little Marsh Fork, which then enters Marsh Fork, which is a tributary of the Coal River. The purpose of the dam is to store a sludge consisting of tailings and waste from a nearby coal mine. In 1995 Massey Energy received a permit to construct the dam. Over the years additional permits to increase the size and storage volume of the dam have been issued in the midst of local and regional opposition to its structural integrity. Currently at approximately in height, it is the tallest dam in the Western Hemisphere. When complete its designed height will be . Wasted rock from the coal mining process is used as the dam filler. The dam currently withholds about of waste. This capacity will be increased to upon completion.\n"}
{"id": "21686239", "url": "https://en.wikipedia.org/wiki?curid=21686239", "title": "Bundesamt für Strahlenschutz", "text": "Bundesamt für Strahlenschutz\n\nThe Bundesamt für Strahlenschutz (BfS) is the German Federal Office for Radiation Protection. The BfS was established in November 1989; the headquarters is located in Salzgitter, with branch offices in Berlin, Bonn, Freiburg, Gorleben, Oberschleißheim and Rendsburg. It has 708 employees (including 305 scientific) and an annual budget of around 305 million Euro (2009). Since 2009 the BfS is also responsible for the storage site of radioactive waste, Schacht Asse II.\n\nThe BfS is supervised by the Federal Ministry for Environment, Nature Conservation and Nuclear Safety (BMU). \nThe BfS has four sub-departments.\n\n\nThe BfS operates a gamma dose rate measurement network with about 1800 probes, uniformly distributed over Germany. The automatically working systems compare the actual level of radiation with the long term mean and sends an alert to the data centers immediately, if the radiation exceeds the threshold value. This network is a part of the German early warning system, in case of a nuclear accident. Hardware of data logger and probes as well as software are developed in-house by the BfS. On the mountain Schauinsland the BfS operates an international measurement station for gamma dose rate probe calibration and long term tests.\n\n\n"}
{"id": "2169754", "url": "https://en.wikipedia.org/wiki?curid=2169754", "title": "Charge pump", "text": "Charge pump\n\nA charge pump is a kind of DC to DC converter that uses capacitors for energetic charge storage to raise or lower voltage. Charge-pump circuits are capable of high efficiencies, sometimes as high as 90–95%, while being electrically simple circuits.\n\nCharge pumps use some form of switching device to control the connection of a supply voltage across a load through a capacitor. In a two stage cycle, in the first stage a capacitor is connected across the supply, charging it to that same voltage. In the second stage the circuit is reconfigured so that the capacitor is in series with the supply and the load. This doubles the voltage across the load - the sum of the original supply and the capacitor voltages. The pulsing nature of the higher voltage switched output is often smoothed by the use of an output capacitor.\n\nAn external or secondary circuit drives the switching, typically at tens of kilohertz up to several megahertz. The high frequency minimizes the amount of capacitance required, as less charge needs to be stored and dumped in a shorter cycle.\n\nCharge pumps can double voltages, triple voltages, halve voltages, invert voltages, fractionally multiply or scale voltages (such as ×3/2, ×4/3, ×2/3, etc.) and generate arbitrary voltages by quickly alternating between modes, depending on the controller and circuit topology.\n\nThey are commonly used in low-power electronics (such as mobile phones) to raise and lower voltages for different parts of the circuitry - minimizing power consumption by controlling supply voltages carefully.\n\nThe term \"charge pump\" is also commonly used in phase-locked loop (PLL) circuits even though there is no pumping action involved unlike in the circuit discussed above. A PLL charge pump is merely a bipolar switched current source. This means that it can output positive and negative current pulses into the loop filter of the PLL. It cannot produce higher or lower voltages than its power and ground supply levels.\n\n\n\nApplying the equivalent resistor concept to calculating the power losses in the charge pumps \n\nCharge pumps where the voltages across the capacitors follow the binary number system\n\n"}
{"id": "36793290", "url": "https://en.wikipedia.org/wiki?curid=36793290", "title": "Container deposit legislation in Australia", "text": "Container deposit legislation in Australia\n\nContainer deposit legislation (CDL) also known as a Container Deposit Scheme (CDS) is a scheme that was first implemented in South Australia in 1977 and over the decades has spread to the Northern Territory in 2012, New South Wales in 2017 and the Australian Capital Territory in June 2018. Queensland has announced it will pursue a scheme in partnership with NSW for implementation by 1 November 2018. Western Australia has also announced plans for a scheme commencing in early 2020.\n\nAttempts to introduce similar legislation in Tasmania and Victoria have been unsuccessful to date. Victoria did have a similar scheme in place in the 1980's called 'Cash for Cans' but was later rescinded. A Newspoll survey found a majority of people in Australia support a deposit scheme, and a national scheme has also been proposed many times over the year. The recent NSW scheme was strongly opposed by the beverage industry and highlighted their influence.\n\nThe value of deposits and the scope of their application have been influenced by the Australian federal constitution's guarantee of free trade between the states. The defining case in this issue was the attempt to introduce a differential between reusable and recyclable bottle deposits. The issue was taken to the High Court of Australia in the \"Castlemaine Tooheys Ltd v South Australia\" court case., State based schemes need to be exempted from the Commonwealth Mutual Recognition Act which guarantees products can be sold in any jurisdiction without requiring any special labelling. This formed the basis of legal action against the Northern Territories' scheme until an exemption was granted. South Australia, Northern Territory, New South Wales and the ACT is offering 10c to anyone who donates an empty container.\n\nThe ACT government has always been supportive of container deposit legislation, but has never acted alone due to its relatively small size and being enclaved inside the much larger state of NSW. They have always said if NSW adopts a scheme they will follow suit.\nThe ACT Container Deposit Scheme legislation was passed into law on 9 November 2017 and a public consultation was conducted ahead of the scheme's implementation in 2018. The scheme began on 30 June 2018.\n\nThe New South Wales government has indicated it wishes to push ahead with a container deposit scheme as part of a raft of new policies aimed at protecting the environment and doing more to prevent litter and pollution entering the state's rivers, oceans, and parks. The scheme has strong backing from the public, various politicians, NSW councils, and various environmental groups such as Cleanup Australia, Boomerang Alliance, and Total Environment Centre.\n\nIn February 2015 it was revealed that the Baird government has been under a well planned, sustained attack from multiple beverage industry companies and their executives. They have organised the lobbying of various members of parliament in NSW and other jurisdictions (even at the federal level) in order to gain influence over Mike Baird and Minister for the Environment, Rob Stokes, in order to force them to abandon plans for a container deposit scheme in NSW. The beverage industry has even threatened to run scare campaign election advertising. The government said that all of this time and money spent on lobbying has ultimately failed to change their position and the scheme will go ahead and is currently being designed. They have also invited the beverage industry to get on board.\n\nThe NSW government initially announced the start date of a container deposit/refund system being July 2017, however, this was extended to December 2017 following requests from environment groups and industry bodies. \n\nOn 8 May 2016 it was officially announced the NSW will be implementing a full 10c Container Deposit Scheme after looking at many options, including public submissions and industry alternatives. As it stands, drink containers from 150ml in size up to 3L will be covered as long as it has the appropriate NSW labelling. There will be some exclusions - such as wine bottles - as it is mainly targeting drink containers consumed away from home. Draft legislation will be brought forward and an implementation advisory group will be established to meet the 1 December 2017 start date. This will affect the ACT and also QLD which are currently examining options for a state based scheme.\n\nOn 29 July 2017 the NSW EPA announced the scheme coordinator would be a joint venture called Exchange for Change, comprising 5 of Australian beverage companies: Asahi, Carlton & United Breweries, Coca-Cola Amatil, Coopers Brewery and Lion. However, there has been some criticism of this decision given some of these organisations have actively opposed CDL in the past.\n\nOn 1 December 2017, the Container Deposit Scheme, named Return and Earn, officially started, as it pushed to reduce the volume of litter in NSW by 40% by 2020. During the first couple of weeks in the scheme, many people were complaining about it, which including drink prices going up, boxes and bags dumped next to the machines, the allowance of only cans and bottles that were empty, uncrushed, unbroken and had the original label attached, and also, the limited number of collection points around the state. Although there was plenty of problems throughout Return And Earn, the number of containers that were returned increased, as did the amount of collection points. The Environment Minister announced that the scheme has been a success, and they were working hard to fix the minor teething problems. During Christmas and New Years celebrations, the amount grew to over 1 million containers returned a day, and then to over 3 million in towards April March. As of 17 April, there is a growing number of the 549 collection points available out of the planned 800+, and over 240 million containers have been returned throughout the state. A common label has been developed which will start appearing on bottles and cans: \"10c refund at collection depots/points in participating State/Territory of purchase.\"\n\nThe Northern Territory introduced a container deposit scheme similar to South Australia's from 3 January 2012. This was challenged in the Federal Court by Coca-Cola Amatil, Schweppes Australia and Lion Pty Ltd using the Commonwealth Mutual Recognition Act and the scheme ceased on 4 March 2013. Immediately after the Federal Court loss, the NT government personally stepped in to keep the scheme going until a permanent exemption to the Mutual Recognition Act could be secured. On 7 August 2013 the Federal Executive Council (ExCo) ratified the permanent exemption making the NT container deposit scheme completely legal and permanent.\n\nDue to this, all beverage containers sold in Australia now must bear the words \"10c refund at SA/NT collection depots in state/territory of purchase\". With other jurisdictions implementing their own schemes a common label has been developed which will start appearing on bottles and cans: \"10c refund at collection depots/points in participating State/Territory of purchase\"\n\nIn 2013, the Newman government has indicated it does not want to increase the cost of QLD residents' costs of living, seeing the refund scheme as more of a tax rather than a refundable deposit even though 85% of Queenslanders are in favor of a scheme according to a Newspoll.\n\nIn February 2015, the newly elected Palaszczuk government indicated a total overhaul of the state's environmental policies including supporting a national container deposit scheme and also a state based container deposit scheme. It is likely that QLD will follow suit with NSW.\n\nIn May 2015 the QLD government announced its support for a Cash for Containers scheme in Queensland, potentially in 2018 after NSW implements its own scheme. The QLD environment minister says he was inspired by trash that was sent to his office. He has ordered a review and wants to establish an advisory group to help his department with a consultation process with the public and stake holders commencing later this year. He also says he will work with NSW. Eventually the Queensland government on 22 July 2016 announced that State will have a Container Deposit Scheme \"to get drink cans and bottles off our beaches, and out of our parks and public areas\". \nThe Queensland Container Refund Scheme (CRS) was to start from 1 July 2018 but was delayed to 1 November 2018.\n\nIn the days when bottles were washed and re-used, drinks manufacturers paid shopkeepers and \"marine store collectors\" (\"bottle-ohs\") for the return of their (proprietary) containers, both bottles and crates. By arrangement between the manufacturers, the refund to the consumer was standardized (for many decades 2d, later 6d or 5c. for soft drink bottles and ½d for generic beer bottles) and the collectors received a premium for their part in the process. A substantial cost was incurred by the manufacturer in the transportation, sorting, storage, washing and sterilizing of the bottles and their inspection for contamination and damage. With the advent of cheaper single-use bottles great savings were achievable, and their subsequent disposal the responsibility of the consumer.\n\nCDL in South Australia was put in place under the Beverage Container Act 1975 (SA) and came into operation in 1977. Environment Protection Act 1993 (SA) now governs the levying and refund of deposits.\n\nThere is a refund of 10 cents per can or bottle (raised from 5 cents in late 2008). In the 1970s deposits ranged from 20c for a 30 oz bottle and 10c for a 10 oz and 6½ oz bottle. With the introduction of plastic and non re-usable bottles the deposit was reduced to 5c (including aluminium cans). This amount remained unchanged for around thirty years.\n\nAround 600 people are employed in the recovery of bottles in South Australia. Groups such as the Scouts operate container refund depots. While there are professional collectors who collect on an arranged basis from particular venues (e.g. pubs and restaurants), usually operating small trucks for the job, there are also many socially marginalised collectors who forage in spots such as refuse bins for discarded deposit bottles; these collectors often travel by bicycle, sometimes with relatively elaborate and inventive modifications to allow them to carry bulky loads of bottles.\n\nUntil 2008, every beverage container in Australia bore the words \"5c refund at SA collection depots in state of purchase\". This changed to \"10c refund at SA collection depots in state of purchase\" in late 2008. Since the Northern Territory started their own scheme in 2012, this message has changed again. With other jurisdictions implementing their own schemes a common label has been developed which will start appearing on bottles and cans: \"10c refund at collection depots/points in participating State/Territory of purchase\".\n\nIn December 2014 a state based deposit scheme was rejected by the government, citing costs and the need to ship containers to the Australian mainland for processing. Various environmental organisations, including The Greens and many Tasmanian local councils have been pushing for a scheme in Tasmania for many years. Most people are in favour of a scheme according to various studies that have been conducted over the years.\n\nIn 2018 the Tasmanian Government has made a commitment to consider establishing a Container Refund Scheme (CRS) and has engaged a consulting company to produce a report with recommendations. This report is now available and the Tasmanian Government is now reviewing.\n\nThe state of Victoria once had a container deposit scheme in the 1980's called \"Cash for Cans\" but was rescinded in 1989.\nIn 2009 the Victorian Greens introduced a bill for a 10c deposit scheme, which was passed in the upper house but the government quashed the bill in the lower house, allegedly on constitutional grounds, by refusing to allow it to be debated. Despite supporting the Greens' bill when in opposition, when it later became the government the Coalition decided it would not back a bottle refund scheme. Instead, it said it would support a national scheme if one were created.\n\nIn 2012/2013, the Napthine government indicated its strong support for a state based scheme possibly in partnership with NSW. Since there has now been a change of government, the legislation has not been introduced into parliament as of January 2015.\n\nIn February 2015, the then Environment Minister Lisa Neville has publicly said she is not in favour of a container deposit scheme for Victoria. She believes current recycling programs are good enough, even though Clean-up Australia claims beverage related rubbish in Victoria now outnumbers cigarette related rubbish.\nIn July 2017 her successor Lily D'Ambrosio confirmed the state's continuing opposition to a state scheme on the basis that the costs would outweigh the environmental benefits.\n\nIn July 2018, Victorian Environment Minister Lily D'Ambrosio announced a $37m package to tackle recycling in her State. Although she is no longer explicitly saying a CDS for Victoria is off the table, she has stated that she is carefully monitoring the situation in NSW in relation to the newly rolled out scheme.\nThere are private companies that accept used aluminum cans and pay cash for them - usually by weight or by per filled 'metal storage cage'.\n\nIn 2011, opposition Labor and Greens MPs called for the introduction of a container deposit scheme. The Minister for Environment, Bill Marmion, said that WA would wait for a national \"consultation regulatory impact statement\" to be completed at the end of 2011 before taking any action.\n\nIn August 2016, the WA Government announced a State container deposit scheme commencing in 2018. Minister for the Environment Albert Jacob said that efforts to pursue a national scheme had \"fallen by the wayside\" but that Western Australia's policy should be aligned with recent changes in Queensland and New South Wales. After a change of government at the March 2017 election, the new Environment Minister Stephen Dawson said he wanted to make a container deposit scheme a priority and in August 2017. The new WA Government held public consultation and will start a container deposit scheme in January 2020 in a bid to lift the State’s low recycling rates.\n\nThrough the early 20th century, when the cost of producing glass bottles was higher, a natural industry of glass bottle collectors and merchants performed a similar function to the modern CDL. Bottle accumulators, a licensed and unionized workforce commonly known as \"bottle-ohs\" from their street cries, travelled by cart around the streets buying empty bottles from households and businesses. They would then sell the bottles to a bottle yard, which would store and sort the bottles before selling them in bulk to brewers and other bottlers. It was an industry from which a bottle-oh could make a good living; in 1904, they could buy a dozen beer bottles for 6d., sell them to the bottle yard for 9d., who could sell them to brewers for 1s. The commercial reuse of glass bottles and the bottle collecting industry had all but disappeared by the 1950s.\nSoft drink and other beverage bottles were still collected in Queensland and returned for deposits up to the late 1960s.\n\n\n"}
{"id": "30881248", "url": "https://en.wikipedia.org/wiki?curid=30881248", "title": "Curvilinear motion", "text": "Curvilinear motion\n\nThe motion of an object moving in a curved path is called \"curvilinear motion\".\nExample: A stone thrown into the air at an angle.\n\nCurvilinear motion describes the motion of a moving particle that conforms to a known or fixed curve. The study of such motion involves the use of two co-ordinate systems, the first being planar motion and the latter being cylindrical motion.\n\nIn planar motion, the velocity and acceleration components of the particle are always tangential and normal to the fixed curve. The velocity is always tangential to the curve and the acceleration can be broken up into both a tangential and normal component.\n\nWith cylindrical co-ordinates which are described as î and j, the motion is best described in polar form with components that resemble polar vectors. As with planar motion, the velocity is always tangential to the curve, but in this form acceleration consist of different intermediate components that can now run along the radius and its normal vector. This type of co-ordinate system is best used when the motion is restricted to the plane upon which it travels.\n"}
{"id": "49202119", "url": "https://en.wikipedia.org/wiki?curid=49202119", "title": "Diffusion bonding", "text": "Diffusion bonding\n\nDiffusion bonding or diffusion welding is a solid-state welding technique used in metalworking, capable of joining similar and dissimilar metals. It operates on the principle of solid-state diffusion, wherein the atoms of two solid, metallic surfaces intersperse themselves over time. This is typically accomplished at an elevated temperature, approximately 50-70% of the absolute melting temperature of the materials. Diffusion bonding is usually implemented by applying high pressure, in conjunction with necessarily high temperature, to the materials to be welded; the technique is most commonly used to weld \"sandwiches\" of alternating layers of thin metal foil, and metal wires or filaments. Currently, the diffusion bonding method is widely used in the joining of high-strength and refractory metals within the aerospace and nuclear industries.\n\nThe act of diffusion welding is centuries old. This can be found in the form of \"filled gold,\" a technique used to bond gold and copper for use in jewelry and other applications. In order to create filled gold, smiths would begin by hammering out an amount of solid gold into a thin sheet of gold foil. This film was then placed on top of a copper substrate and weighted down. Finally, using a process known as \"hot-pressure welding\" or HPW, the weight/copper/gold-film assembly was placed inside an oven and heated until the gold film was sufficiently bonded to the copper substrate.\n\nModern methods were described by the Soviet scientist N.F. Kazakov in 1953.\n\nDiffusion bonding involves no liquid fusion, and often no filler metal. No weight is added to the total, and the join tends to exhibit both the strength and temperature resistance of the base metal(s). The materials endure no, or very little, plastic deformation. Very little residual stress is introduced, and there is no contamination from the bonding process. It may be performed on a join surface of theoretically any size with no increase in processing time; practically speaking, the surface tends to be limited by the pressure required and physical limitations. It may be performed with similar and dissimilar metals, reactive and refractory metals, or pieces of varying thicknesses.\n\nDiffusion bonding is most often used for jobs either difficult or impossible to weld by other means, due to its relatively high cost. Examples include welding materials normally impossible to join via liquid fusion, such as zirconium and beryllium; materials with very high melting points such as tungsten; alternating layers of different metals which must retain strength at high temperatures; and very thin, honeycombed metal foil structures.\n\nSteady state diffusion is determined by the amount of diffusion flux that passes through the cross-sectional area of the mating surfaces. Fick's first law of diffusion states:\n\nwhere \"J\" is the diffusion flux, \"D\" is a diffusion coefficient, and \"dC\"/\"dx\" is the concentration gradient through the materials in question. The negative sign is a product of the gradient. Another form of Fick's law states:\n\nwhere \"M\" is defined as either the mass or amount of atoms being diffused, \"A\" is the cross-sectional area, and \"t\" is the time required. Equating the two equations and rearranging, we achieve the following result:\n\nAs mass and area are constant for a given joint, time required is largely dependent on the concentration gradient, which changes by only incremental amounts through the joint, and the diffusion coefficient. The diffusion coefficient is determined by the equation:\n\nwhere \"Q\" is the activation energy for diffusion, \"R\" is the universal gas constant, \"T\" is the thermodynamic temperature experienced during the process, and \"D\" is a temperature-independent preexponential factor that depends on the materials being joined. For a given joint, the only term in this equation within control is temperature.\n\nWhen joining two materials of similar crystalline structure, diffusion bonding is performed by clamping the two pieces to be welded with their surfaces abutting each other. Prior to welding, these surfaces must be machined to as smooth a finish as economically viable, and kept as free from chemical contaminants or other detritus as possible. Any intervening material between the two metallic surfaces may prevent adequate diffusion of material. Specific tooling is made for each welding application to mate the welder to the workpieces. Once clamped, pressure and heat are applied to the components, usually for many hours. The surfaces are heated either in a furnace, or via electrical resistance. Pressure can be applied using a hydraulic press at temperature; this method allows for exact measurements of load on the parts. In cases where the parts must have no temperature gradient, differential thermal expansion can be used to apply load. By fixturing parts using a low-expansion metal (i.e. molybdenum) the parts will supply their own load by expanding more than the fixture metal at temperature. Alternative methods for applying pressure include the use of dead weights, differential gas pressure between the two surfaces, and high-pressure autoclaves. Diffusion bonding must be done in a vacuum or inert gas environment when using metals that have strong oxide layers (i.e. copper). Surface treatment including polishing, etching, and cleaning as well as diffusion pressure and temperature are important factors regarding the process of diffusion bounding.\n\nAt the microscopic level, diffusion bonding occurs in three simplified stages:\n\n\nDiffusion bonding is primarily used to create intricate forms for the electronics, aerospace, and nuclear industries. Since this form of bonding takes a considerable amount of time compared to other joining techniques such as explosion welding, parts are made in small quantities, and often fabrication is mostly automated. However, due to different requirements, some of the time interval could be accomplished in few minutes. In an attempt to reduce fastener count, labor costs, and part count, diffusion bonding, in conjunction with superplastic forming, is also used when creating complex sheet metal forms. Multiple sheets are stacked atop one another and bonded in specific sections. The stack is then placed into a mold and gas pressure expands the sheets to fill the mold. This is often done using titanium or aluminum alloys for parts needed in the aerospace industry.\n\nTypical materials that are welded include titanium, beryllium, and zirconium. In many military aircraft diffusion bonding will help to allow for the conservation of expensive strategic materials and the reduction of manufacturing costs. Some aircraft have over 100 diffusion-bonded parts, including; fuselages, outboard and inboard actuator fittings, landing gear trunnions, and nacelle frames.\n\n\n"}
{"id": "44083564", "url": "https://en.wikipedia.org/wiki?curid=44083564", "title": "ELENA reactor", "text": "ELENA reactor\n\nThe ELENA reactor is a compact Russian pressurized water reactor (PWR) of 68-kWe generating capacity currently being developed by the Kurchatov Institute. To develop the reactor, techniques were used derived from the construction and operation of marine and space power plants and the operational experience of the GAMMA reactor. \n\nIt is intended for use in remote areas, towns with a population of\n1500–2000, or individual consumers requiring a highly reliable power supply, such as hospitals.\n\nThe reactor can also be used for water desalination. As of 2014, it is the smallest commercial nuclear reactor being developed.\n\nThe principal of its development are Kurchatov Institute, Krasnaya zvezda, Izhorskiye Zavody, Atomenergoproekt and VNIINT (ВНИИНМ, Высокотехнологический научно-исследовательский институт неорганических материалов имени академика А.А. Бочвара).\n\n"}
{"id": "20842876", "url": "https://en.wikipedia.org/wiki?curid=20842876", "title": "Electrochemical fluorination", "text": "Electrochemical fluorination\n\nElectrochemical fluorination (ECF), or electrofluorination, is a foundational organofluorine chemistry method for the preparation of fluorocarbon-based organofluorine compounds. The general approach represents an application of electrosynthesis. The fluorinated chemical compounds produced by ECF are useful because of their distinctive solvation properties and the relative inertness of carbon–fluorine bonds. Two ECF synthesis routes are commercialized and commonly applied: the Simons process and the Phillips Petroleum process. It is also possible to electrofluorinate in various organic media. Prior to the development of these methods, fluorination with fluorine, a dangerous oxidant, was a dangerous and wasteful process. Also, ECF can be cost-effective but it may also result in low yields.\n\nThe Simons process entails electrolysis of a solution of an organic compound in a solution of hydrogen fluoride. An individual reaction can be described as:\n\nIn the course of a typical synthesis, this reaction occurs once for each C–H bond in the precursor. The cell potential is maintained near 5–6 V. The anode is nickel-plated. Simons discovered the process in the 1930s at Pennsylvania State College (U.S.), under the sponsorship of the 3M Corporation. The results were not published until after World War II because the work was classified due to its relevance to the manufacture of uranium hexafluoride. In 1949 Simons and his coworkers published a long paper in the \"Journal of the Electrochemical Society\". The Simons process is used for the production of perfluorinated amines, ethers, carboxylic acids, and sulfonic acids. For carboxylic and sulfonic acids, the products are the corresponding acyl and sulfonyl fluorides. The method has been adapted to laboratory-scale preparations. Two noteworthy considerations are (i) the hazards associated with hydrogen fluoride (the solvent and fluorine source) and (ii) the requirement for anhydrous conditions.\n\nThis method is similar to the Simons Process but is typically applied to the preparation from volatile hydrocarbons and chlorohydrocarbons. In this process, electrofluorination is conducted at porous graphite anodes in molten potassium fluoride in hydrogen fluoride. The species KHF is relatively low melting, a good electrolyte, and an effective source of fluorine. The technology is sometimes called “CAVE” for Carbon Anode Vapor Phase Electrochemical Fluorination and was widely used at manufacturing sites of the 3M Corporation. The organic compound is fed through a porous anode leading to exchange of fluorine for hydrogen but not chlorine.\n\nECF has also been conducted in organic media, using for example organic salts of fluoride and acetonitrile as the solvent. A typical fluoride source is (CH)N:3HF. In some cases, acetonitrile is omitted, and the solvent and electrolyte are the triethylamine-HF mixture. Representative products of this method are fluorobenzene (from benzene) and 1,2-difluoroalkanes (from alkenes).\n"}
{"id": "4809279", "url": "https://en.wikipedia.org/wiki?curid=4809279", "title": "Energy consumption", "text": "Energy consumption\n\nEnergy consumption is the amount of energy or power used.\n\nIn the body, energy consumption is part of energy homeostasis. It derived from food energy. Energy consumption in the body is a product of the basal metabolic rate and the physical activity level. The physical activity level are defined for a non-pregnant, non-lactating adult as that person's total energy expenditure (TEE) in a 24-hour period, divided by his or her basal metabolic rate (BMR):\n\nTopics related to energy consumption in a demographic sense are:\n\n\n\n"}
{"id": "9696065", "url": "https://en.wikipedia.org/wiki?curid=9696065", "title": "Fender Blues Junior", "text": "Fender Blues Junior\n\nThe Blues Junior is a tube guitar amplifier introduced in 1990 by the Fender company. It is aimed at achieving the warm, tube-driven tone common in many styles of American blues and blues rock dating back to the 1950s, while remaining both portable and affordable. Fender frequently releases limited editions of the Blues Junior. All have the same electronic components and specifications but have cosmetic changes and often a different speaker, at varying prices. The Fender Blues Junior is most similar to the Fender Blues Deluxe, which adds a \"drive\" channel, an effects loop, and uses 6L6GC output tubes for 40 watts of rated output. The Fender Blues Junior was introduced after the Fender Pro Junior, but has entirely different circuitry other than EL84 output tubes rated at 15 watts.\n\n\nLike many Fender amplifiers (particularly in the Hot Rod series), many limited edition versions of the Blues Junior have been manufactured since its introduction in 1995. The original circuit board underwent a major redesign in 2001, when production moved from the US to Mexico. The earlier circuit boards are green in color and are noted for a \"darker,\" more bass-inflected tone. Many green board models have excessive noise in the reverb circuit, as the signal is inserted into the reverb after the master volume. The later circuit boards are cream or tan colored and sound \"brighter\" or more treble-oriented, with the reverb situated before the Master volume.\n\nLacquered Tweed editions, as the name implies, feature a cabinet upholstered with a lacquered tweed fabric. Some of these have been artificially aged from the factory and are known as Lacquered Tweed \"Relic\" editions. The \"Blonde\" and \"Brown Tolex\" Blues Juniors feature traditional Fender Tolex upholstery in their respective colors, while the Sunburst Ash edition features an ash cabinet with a sunburst stain. One edition was issued with a forest green colored cabinet, had a United States flag on the cloth speaker grille, and had United States Air Force markings detailed on the cabinet in yellow.\n\nThe standard Blues Junior is dressed like a traditional black Fender amplifier, with black Tolex upholstery and a silver cloth speaker grille.\n\nAlthough they differ in their external visual aesthetic, all variants of the Blues Junior for a given year are produced with the same electronics, apart from the speaker/driver. Limited editions may use varying driver models, such as the Jensen C12N found in the NOS Lacquered Tweed Blues Junior or the Jensen P12R found in the Relic edition. Fender does not always include these limited edition versions in their updated catalogs (or on their website) either because too few were produced (only a few hundred in some cases) or perhaps because of exclusive retailer agreements.\n\nFender introduced the Blues Junior in 1995 and has revised and updated the amp periodically since then. The Blues Junior history can be divided into two major categories: the early amps with green circuit boards and the later ones with cream-colored boards. The switch to the cream-colored boards reflects the change in where the amps were manufactured, from USA to Mexico. The cream-colored board is laid out entirely differently from the green board. The biggest change is the reworked reverb circuit. The older amps tend to sound darker, while the new ones are brighter with more emphasis on treble tones.\n\nThe table Below details the revision dates and the changes made on those dates.\n\nFender adopted a two-letter dating code in 1990. The code can be found on the Quality Assurance label located inside the cabinet. The label is often located on the bottom next to the reverb tank, sometimes it is placed on the side. The codes can be handwritten, which can occasionally make the letters hard to decipher.\n\nFender discontinued date codes in 2003. There is no reliable way to date 2003–2005 amps other than to ask Fender customer support to look up the date from the serial number. Amps made in 2006 have a small metal \"Fender 60th Anniversary\" button on the back plate. Amps from 2007 are undistinguished.\n\nIn 2010 Fender updated their line of hot rod model amps and released the \"Blues Junior III,\" adding many physical changes to the standard black tolex model. Notable differences include the Fender \"lightning bolt\" speaker, \"sparkle\" circuit modification, rattle-reducing shock absorbers, set-screw \"chicken head\" knobs, \"dog bone\" handle, larger Fender jewel light, black non-reflective control panel, and \"Blues Junior\" subtext on the logo plate.\n\nA set of three matched Groove Tubes 12AX7 preamp tubes and two matched Groove Tubes EL84 power tubes, graded according to Fender's specifications, are used in all factory Blues Juniors. The Blues Junior uses a nonadjustable power tube bias (no way to vary the bias point), but it is fairly \"hot\" and can accommodate most matched pairs on the market. However, because they \"run hot\", a common modification is to add a bias trim to enable adjustments to the idle current when switching tubes.\n\n"}
{"id": "6110300", "url": "https://en.wikipedia.org/wiki?curid=6110300", "title": "Ferroresonance in electricity networks", "text": "Ferroresonance in electricity networks\n\nFerroresonance or nonlinear resonance is a type of resonance in electric circuits which occurs when a circuit containing a nonlinear inductance is fed from a source that has series capacitance, and the circuit is subjected to a disturbance such as opening of a switch. It can cause overvoltages and overcurrents in an electrical power system and can pose a risk to transmission and distribution equipment and to operational personnel.\n\nFerroresonance should not be confused with linear resonance that occurs when inductive and capacitive reactances of a circuit are equal. In linear resonance the current and voltage are linearly related in a manner which is frequency dependent. In the case of ferroresonance it is characterised by a sudden jump of voltage or current from one stable operating state to another one. The relationship between voltage and current is dependent not only on frequency but also on a number of other factors such as the system voltage magnitude, initial magnetic flux condition of transformer iron core, the total loss in the ferroresonant circuit and the point on wave of initial switching.\n\nFerroresonant effects were first described in a 1907 paper by Joseph Bethenod. The term \"ferroresonance\" was apparently coined by French engineer Paul Boucherot in a paper from 1920, where he analysed the phenomenon of two stable fundamental frequency operating points coexisting in a series circuit containing a resistor, nonlinear inductor and a capacitor.\n\nFerroresonance can occur when an unloaded 3-phase system consisting mainly of inductive and capacitive components is interrupted by single phase means. In the electrical distribution field this typically occurs on a medium voltage electrical distribution network of transformers (inductive component) and power cables (capacitive component). If such a network has little or no resistive load connected and one phase of the applied voltage is then interrupted, ferroresonance can occur. If the remaining phases are not quickly interrupted and the phenomenon continues, overvoltage can lead to the breakdown of insulation in the connected components resulting in their failure.\n\nThe phenomenon can be avoided by connecting a minimal resistive load on the transformer secondaries or by interrupting the applied voltage by a 3-phase interrupting device such as a ganged (3 pole) circuit breaker.\n\n"}
{"id": "34937051", "url": "https://en.wikipedia.org/wiki?curid=34937051", "title": "Flooding (nuclear reactor core)", "text": "Flooding (nuclear reactor core)\n\nFlooding refers to a fluid flow phenomenon whereby counter-current two-phase flow is reversed and runs concurrent in the direction of the initial gas/vapor phase flow when filling, or \"flooding\", a nuclear reactor core with coolant. This phenomenon is generally discussed with respect to a loss-of-coolant accident (LOCA). As this phenomenon proceeds, annular flow running counter-current begins as liquid water is inserted into the system. Then if conditions are correct, the frictional force at the gas-liquid interface begins to reverse the flow of the liquid. Finally, the flow of the liquid reverses, running concurrently in a slug (or other) flow regime. The significance of this phenomenon is that, if not properly designed for, it can present issues when trying to fill the core with liquid (the phenomenon works against gravity, forcing liquid out of the core).\n\nIn a boiling water reactor (BWR), the emergency core cooling system (ECCS) injects liquid water into the reactor core from the top. Water vapor produced from boiling will flow in the opposite direction. Given a high enough flow rate of steam, reversal of the ECCS-injected liquid water occurs.\n\nIn a pressurized water reactor (PWR), the ECCS injects liquid into the hot and/or cold leg of the reactor. The cold leg flows through a downcomer on the outside of the core, before flowing up through the core. The core barrel and the reactor vessel wall form a cylindrical shell that is referred to as the downcomer. In the cold leg, boiling in the downcomer creates an upward flow of steam that can reverse the flow of liquid water coming in through the cold leg. The flooding rate in a PWR is pressure dependent.\n\nFlooding as a fluid flow phenomenon should be distinguished from the act of filling the core with coolant (\"flooding of the reactor core\"), as the fluid flow phenomenon occurs during the filling process. \"Flooding of containment\" refers to filling the nuclear reactor containment with liquid (usually water), which is distinctly different from either reactor core flooding or flooding as a fluid flow phenomenon.\n\n"}
{"id": "11190015", "url": "https://en.wikipedia.org/wiki?curid=11190015", "title": "GASUN", "text": "GASUN\n\nThe National Unification Gas Pipeline (\"Gas Unificação\" – GASUN) will be a natural gas pipeline in Brazil. GASUN will connect the Gasbol pipeline with the northern Amazon and the Northeast states (Pará, Tocantins, Maranhão and Piauí) allowing transportation of Bolivian gas into these regions. The project is expected to cost US$2.48 billion.\n\nThe first stage of GASUN will begin in Mimoso, Mato Grosso do Sul, and will join the Gasbol pipeline. From there it will run towards Brasília, passing through Goiânia. Construction of this section started in 2005, and scheduled to be completed in 2007.\n\nThe longest and most costly portion of GASUN will be the long central-north branch, which is to connect Goiás and Maranhão. It will pass through Palmas and Belém, Pará. The entire natural gas pipeline should be complete by 2026.\n"}
{"id": "159046", "url": "https://en.wikipedia.org/wiki?curid=159046", "title": "Geothermal desalination", "text": "Geothermal desalination\n\nGeothermal desalination is a process under development for the production of fresh water using heat energy. Claimed benefits of this method of desalination are that it requires less maintenance than reverse osmosis membranes and that the primary energy input is from geothermal heat, which is a low-environmental-impact source of energy. \n\nCirca 1995, Douglas Firestone from Nevada devised the use of geothermal water directly as a source for desalination. In 1998, several individuals began working with evaporation/condensation air loop water desalination. The experiment was successful and a proof of concept, proving that geothermal waters could be used as process water to produce potable water in 2001. \n\nIn 2005 to 2009 testing was done in a sixth prototype of a device referred to as a delta t device, a closed air loop, atmospheric pressure, evaporation condensation loop geothermally powered desalination device. The device used filtered sea water from Scripps Institution of Oceanography and reduced the salt concentration from 35,000 ppm to 51 ppm. \n\nA total of six prototypes and six modifications demonstrated that, with process water approaching and a chill source about , a full-size device would produce about 600 m³ of water per day. Salt concentration in the wastewater would only be about 10% above the level of the original water, thus, from, say, 35,000 to about 38,000 parts per million, well within the ability of osmoregulators to adjust.\n\n\n\n<br>\n"}
{"id": "48789428", "url": "https://en.wikipedia.org/wiki?curid=48789428", "title": "Gunashli Platform No.10 fire", "text": "Gunashli Platform No.10 fire\n\nOn 4 December 2015, a deadly fire broke out in the northern part of platform No. 10 at the western section of the Gunashli oilfield operated by SOCAR. The fire started, according to SOCAR, when a high-pressure subsea gas pipeline was damaged in a heavy storm. As a result of the fire, the platform, which had been in service since 1984, partially collapsed. Fire spread to several oil and gas wells. Production at all 28 wells (24 oil wells and 4 gas wells) connected to the platform was suspended, pipelines connecting the platform to the shore were closed, and electricity to the platform was cut off. Before the accident, the platform produced 920 tonnes of oil and of gas per day. About 60% of the oil produced by SOCAR was transported through this platform. \n\nAt the time of the accident, 63 workers were on the platform. According to the Ministry of Emergency Situations of Azerbaijan, ten workers are confirmed killed, 20 are missing, 33 were rescued; and nine were hospitalized. According to SOCAR, people went missing when a life boat with 34 people on board fell from the platform into the sea and was damaged after hitting piles of the platform. On 5 December, the Prosecutor General's Office launched a criminal investigation into the possible violation of fire safety rules. President of Azerbaijan Ilham Aliyev has signed an Order declaring 6 December 2015 a day of mourning for those who died in the incident.\n"}
{"id": "442024", "url": "https://en.wikipedia.org/wiki?curid=442024", "title": "Harmonograph", "text": "Harmonograph\n\nA harmonograph is a mechanical apparatus that employs pendulums to create a geometric image. The drawings created typically are Lissajous curves, or related drawings of greater complexity. The devices, which began to appear in the mid-19th century and peaked in popularity in the 1890s, cannot be conclusively attributed to a single person, although Hugh Blackburn, a professor of mathematics at the University of Glasgow, is commonly believed to be the official inventor.\n\nA simple, so-called \"lateral\" harmonograph uses two pendulums to control the movement of a pen relative to a drawing surface. One pendulum moves the pen back and forth along one axis and the other pendulum moves the drawing surface back and forth along a perpendicular axis. By varying the frequency and phase of the pendulums relative to one another, different patterns are created. Even a simple harmonograph as described can create ellipses, spirals, figure eights and other Lissajous figures.\n\nMore complex harmonographs incorporate three or more pendulums or linked pendulums together (for example hanging one pendulum off another), or involve rotary motion in which one or more pendulums is mounted on gimbals to allow movement in any direction.\n\nA particular type of harmonograph, a pintograph, is based on the relative motion of two rotating disks, as illustrated in the links below (as opposed to a pantograph, a mechanical device used to enlarge figures). \n\nA Blackburn pendulum is a device for illustrating simple harmonic motion, it was named after Hugh Blackburn, who described it in 1844. This was first discussed by James Dean in 1815 and analyzed mathematically by Nathaniel Bowditch in the same year. A bob is suspended from a string that in turn hangs from a V-shaped pair of strings, so that the pendulum oscillates simultaneously in two perpendicular directions with different periods. The bob consequently follows a path resembling a Lissajous curve; it belongs to the family of mechanical devices known as harmonographs.\n\nMid-20th century physics textbooks sometimes refer to this type of pendulum as a Double Pendulum.\n\nA harmonograph creates its figures using the movements of damped pendulums. The movement of a damped pendulum is described by the equation\n\nin which formula_2 represents frequency, formula_3 represent phase, formula_4 represent amplitude, formula_5 represents damping and formula_6 represents time. If that pendulum can move about two axes (in a circular or elliptical shape), due to the principle of superposition, the motion of a rod connected to the bottom of the pendulum along one axes will be described by the equation\n\nA typical harmonograph has two pendulums that move in such a fashion, and a pen that is moved by two perpendicular rods connected to these pendulums. Therefore, the path of the harmonograph figure is described by the parametric equations\n\nAn appropriate computer program can translate those equations into a graph that emulates a harmonograph. Applying the first equation a second time to each equation can emulate a moving piece of paper (see the figure below).\n\n"}
{"id": "34141362", "url": "https://en.wikipedia.org/wiki?curid=34141362", "title": "Huang Ming (entrepreneur)", "text": "Huang Ming (entrepreneur)\n\nHuang Ming (; born 1958) is a Chinese solar energy researcher and entrepreneur. He established the solar water heater manufacturing company Himin Solar, which was central in the development of the Solar Valley in the city of Dezhou.\n\nHe was a deputy to the 10th and the 11th National People's Congress. He drafted the Law on Renewable Energy and united other representatives in support of it. As a politician he has played a central role in developing renewable energy in China, including the passing of the Renewable Energy Act in 2005. The Renewable Energy Act was passed in 2005 and took effect in 2006, a substantial achievement that echoed globally. According to Hurun Report's China Rich List 2013, he has an estimated fortune of $330 million USD, and was ranked 945th richest person in China.\n\nHuang Ming was awarded the Right Livelihood Honorary Award in 2011. He owns over 600 patents.\n\n\n"}
{"id": "7005434", "url": "https://en.wikipedia.org/wiki?curid=7005434", "title": "Inkjet paper", "text": "Inkjet paper\n\nInkjet paper is a special fine paper designed for inkjet printers, typically classified by its weight, brightness and smoothness, and sometimes by its opacity.\n\nSome inkjet papers are made from high quality deinked pulp or chemical pulps. Quality inkjet paper requires good dimensional stability, no curling or cockling, and good surface strength. For most purposes surface smoothness is required. Sufficient and even porosity is required to counteract spreading of the ink. For lower quality printing, uncoated copy paper suffices, but higher grades require coating. The traditional coatings are not widely used for inkjet papers. For matte inkjet papers, it is common to use silica as pigment together with polyvinyl alcohol (PVOH). Glossy inkjet papers can be made by multicoating, resin coating, or cast coating on a lamination paper.\n\nA variety of fine art inkjet papers meet the needs of professional photographers and artists. These papers share many characteristics with traditional watercolor, printmaking, and photographic papers. Fine art inkjet papers are designed to meet similar standards for longevity as traditional fine art papers: they should have a neutral pH, be lignin-free, and not include optical brighteners. Fine art inkjet papers differ from traditional fine art papers, in that they include coatings engineered to receive and hold ink. Fine art papers are usually made of rag pulp (100% cotton being the most common) but may also be have an alpha-cellulose base. \nSome descriptions and comparisons of fine art inkjet papers are here, here, here, and here. Some fine art papers are mold made, while others are machine made, and may vary considerably in surface texture. Many fine art papers are available in pre-cut sheets or in rolls.\n\nStandard office paper has traditionally been designed for use with typewriters and copy machines, where the paper usually does not get wet. With these papers, moisture tends to wick through the fibers and away from the point of contact. For inkjet printing, this dulls edges of lines and graphic boundaries, and lessens pigment intensity.\n\nHigh-quality inkjet printing with dark, crisp lines requires that the paper have exactly the right absorbency to accept the ink but prevent sideways spread. Many general-purpose office papers of weights around 21 to 27 lb (80–100 g/m²) have been reformulated to work equally well with both inkjet and laser printers. However, this category of paper is only suitable for printing text, because the ink load is light.\n\nPaper is manufactured by forming pulp fibers into a mat on an open mesh screen (a deckle), and then drying and pressing this mat into paper. Large areas of inkjet color, such as in graphics and photographs, soak the paper fibers with so much moisture that they swell and return to their original shape before pressing, resulting in a wavy buckling of the paper surface.\n\nDouble-sided inkjet printing is usually not possible with inexpensive low-weight copy paper because of bleed-through from one side to the other. These papers are also unsuitable for photographic work because standard office paper is usually not \"white\" enough. This results in a poor color gamut and leads to \"muddy\" colors. For all types of paper, printer driver settings must be adjusted to suit the paper, so that the printer delivers the right amount of ink.\n\nPhoto paper is inkjet paper specifically for printing photographs. It is a bright white due to bleaching or pigments such as titanium dioxide, and has been coated with a highly absorbent material that limits diffusion of the ink. Highly refined clay is a common coating to prevent ink spread.\n\nThe best of these papers, with suitable pigment-based ink systems, can match or exceed the image quality and longevity of photographic gelatin-based silver halide continuous tone printing methods used for color photographs, such as Fuji CrystalArchive (for color prints from negatives) and Cibachrome/Ilfochrome (for color prints from positive transparencies). For printing monochrome photographs, traditional silver-based papers are widely felt to retain some advantage over inkjet prints.\n\nPhoto paper is typically divided into \"glossy\", \"semi-matte\", \"semi-gloss\", \"satin\" or \"silk\", and \"matte\" finishes. Paper thickness varies widely. Lighter weights are not much different from the general-purpose office papers described above, and can be used for all types of printing, though these are the least expensive and lowest-quality photo papers.\n\nPhoto papers for more critical work are thicker and have advanced coatings, sometimes with quick-drying properties. They can normally be printed only on the one specially coated side. A few papers are coated for double-sided printing.\n\nGlossy photo paper, generally the most popular, has a shiny finish that gives photos a vivid look. It is generally smooth to the touch and has some glare. Matte photo paper is less shiny, and produces superior text results. Matte and glossy prints typically feel different to the touch, but when displayed under glass they often look similar. Papers with an imitation canvas texture emulate the look of oil paintings. Photo papers are usually high-brightness, neutral white papers, but off-white papers are available.\n\nAs in offset litho printing and traditional photographic printing, glossy papers give the highest color density (or D), and therefore the widest color gamut. Photo papers vary in their longevity and their color gamut. Ink suppliers often provide color profiles for their ink systems when used with specific papers. Longevity depends on the specific combination of inks and paper. For maximum life, the paper substrate is \"wood-free\" (i.e., wood-based but without lignin), or cotton-based, or a combination of the two. Plastic substrates also exist.\n\nNo official paper industry standard defines glossy, semi-matte, etcetera, though an objective scale is available for the glossiness of papers used in offset litho printing. Hewlett-Packard, Epson and Kodak all use their own terms to describe their paper, such as Everyday (HP), Premium High Gloss and Luster (Epson) and Ultima (Kodak). ECI (www.eci.org) has categorized papers for proofing simulation of litho papers (type1/2 etc.)\n\n\n\n"}
{"id": "32352899", "url": "https://en.wikipedia.org/wiki?curid=32352899", "title": "Ituango Dam", "text": "Ituango Dam\n\nThe Ituango Dam, also referred to as the Pescadero-Ituango Dam, is an embankment dam currently under construction on the Cauca River near Ituango in Antioquia Department, Colombia. The primary purpose of the project is hydroelectric power generation and its power plant will have an installed capacity of when completed. Preliminary construction on the dam began in September 2011 and the power plant was expected to begin operations in late 2018, but will not after heavy rainfall and landslides in April/May 2018 blocked river's diversion tunnel, threatening a breach of the dam. When completed, it will be the largest power station in Colombia.\n\nThe dam's feasibility study was completed in 1983 but the project was shelved in the 1990s due to an economic crisis. The final designs for the project were finished in 2008 and on 8 July 2011, the project management contract was awarded. Preliminary construction (surveying, roads, bridges, diversion tunnels) began in September 2011 and it is expected to be complete in 2013. Main works will begin thereafter and the power plant is expected to being commissioning in 2018. Development of the project is being proposed by EPM Ituango, a consortium of Empresas Publicas de Medellin (EPM) and the Antioquia government. The total cost is expected to be US$2.8 billion.\n\nThe dam will be a tall earth-fill embankment type with a clay core. The volume of the dam will be . Its reservoir will have a capacity of of which will be active (or \"useful\") capacity. The reservoir will be long and cover an area of . To maintain reservoir levels, the dam will have a spillway controlled by four radial gates with a design flow of . The dam's power plant will have a nominal hydraulic head of and contain eight Francis turbine-generators.\n\nThree tunnels were constructed to divert the Cauca River around the construction site. Two were eventually sealed during construction, leaving a third to divert the river. Between 28 April and 7 May 2018, three landslides blocked the tunnel, leading the reservoir to fill. Engineers attempted to open the two closed tunnels with explosives but were unsuccessful. On 10 May, when the reservoir reached the power station intake, engineers began releasing water through the unfinished power house to prevent a breach of the dam. On 12 May one of the previous sealed tunnels naturally reopened, which suddenly increased downstream flows by three times the average. This promoted evacuations downstream, eventually totaling around 25,000.\n\nOn May 16 silt build-up in a portion of the power house, the only means to drain the reservoir, led to water escaping into a transit gallery used by construction vehicles. The water eventually poured onto the downstream face of the dam, eroding the roadway and a portion of the riprap. Subsequently, EPM announced a risk of collapse existed and workers continued to fill the dam to its design height in hopes the spillway could be used to prevent an over-topping of the dam. Heavy rainfall was expected in the Cauca River basin through the end of May.\n\nThe financial loss that has emanated from this is large and the proximate cause is, without a doubt, Natural Perils given the rainfall.\n\nThe dam construction has severe ecological consequences, and there are displaced families, environmentalists, youth groups and concerned locals that oppose the project.\n\n"}
{"id": "27091917", "url": "https://en.wikipedia.org/wiki?curid=27091917", "title": "Jawaharlal Nehru National Solar Mission", "text": "Jawaharlal Nehru National Solar Mission\n\nThe Pandit Jawaharlal Nehru National Solar Mission, also known as the National Solar Mission, is an initiative of the Government of India and State Governments to promote solar power. The mission is one of the several initiatives that are part of the National Action Plan on Climate Change. The program was inaugurated by former Prime Minister Manmohan Singh on 11 January 2010 with a target of 20GW by 2022 which was later increased to 100 GW by the Narendra Modi government in the 2015 Union budget of India. India increased its solar power generation capacity by nearly 5 times from 2,650 MW on 26 May 2014 to 12,288.83 MW on 31 March 2017. The country added 5,525.98 MW in 2016-17, the highest of any year.\n\nThe objective of the National Solar Mission is to establish India as a global leader in solar energy, by creating the policy conditions for its diffusion across the country as quickly as possible. Under the original plan, the Government aimed to achieve a total installed solar capacity of 20 GW by 2022. This was proposed to be achieved in three phase. The first phase comprised the period from 2010 to 2013, the first year of the 12th five-year plan. The second phase extended up to 2017, while the third phase would have been the 13th five-year plan (2017–22). Targets were set as 1.4 GW in the first phase, 11-15 GW by the end of the second phase and 22 GW by the end of the third phase in 2022.\n\nThe Government revised the target from 20 GW to 100 GW on 1 July 2015. To reach 100 GW by 2022, the yearly targets from 2015-16 onwards were also revised upwards. India had an installed solar capacity of 161 MW on 31 March 2010, about 2 and half months after the mission was launched on 11 January. By 31 March 2015, three months before the targets were revised, India had achieved an installed solar capacity of 3,744 MW.\n\nTo meet the scaled up target of 100,000 MW, MNRE has proposed to achieve it through 60 GW of large and medium scale solar projects, and 40 GW through rooftop solar projects.\n\nThe following table records the growth of the utilities installed solar capacity in India for every year of the National Solar Mission. All capacities are as on 31 March of the listed year.\n\nThe first phase of this mission aims to commission 1000 MW of grid-connected solar power projects by 2013. The implementation of this phase is in hands of a subsidiary of National Thermal Power Corporation, the largest power producer in India. The subsidiary, NTPC Vidyut Vyapar Nigam Ltd (NVVN), laid out guidelines for selection of developers for commissioning grid connected solar power projects in India. See JNNSM Phase 1 Guidelines. While NVVN is the public face of this phase, several other departments and ministries will play a significant role in formulating guidelines. NVVN will sign power purchase agreements with the developers. Since NVVN is not a utility, it will sell purchased power to different state utilities via separate agreements.\n\nFor Phase 1 projects, NVVN started with a proposal for 50:50 allocation towards solar PV and solar thermal. The latter is quite ambitious given India has no operational solar thermal projects and less than 10MW of solar PV projects. While growing at a rapid pace lately, solar thermal technologies are still evolving globally. The first batch of projects allotted for Phase 1 included 150MW of Solar PV and 500MW of Solar Thermal. NVVN issued Request for Selection document outlining criteria for selection of projects under the Phase 1. See Solar Thermal RFS, Solar PV RfS\n\nA growing solar PV industry in India is hoping to take off by supplying equipment to power project developers. Well known equipment manufacturers started increasing their presence in India and may give competition to local Indian manufacturers. Due to generally high temperatures in India, crystalline silicon-based products are not the most ideal ones. Thin film technologies like amorphous silicon, CIGS and CdTe could be more suitable for higher temperature situations.\n\nSolar thermal technology providers barely have a foothold in India. A few technology providers like Abengoa have some Indian presence in anticipation of demand from this mission.\n\nNVVN issued Request for Selection notice for allotment of capacity to Independent Power Producers (IPPs). See NVVN Solar PV RfS. 150MWs of Solar PV and 470MW of Solar Thermal were up for allotment under the first batch of Phase 1 projects. Project size per IPP was fixed at 5MW for Solar PV and 100MW for Solar Thermal projects. To avoid allocating entire capacity to a select few corporate, guidelines required no two projects to have the same parent company or common shareholders. In case of over subscription, a reverse bidding process was to be used to select the final IPPs based on lowest tariff they offer. Several hundred IPPs responded to this RfS.\n\nThe approach for reverse bidding and methodology to calculate the discount to be offered was presented by Shri Shakti Alternative Energy Ltd through a webinar on 19 October 2010 on the eve of the reverse bidding by NVVN Download presentation on Reverse Bidding by NVVN - What to Expect. The quantum of discount would depend on project site location (i.e. solar radiation), technology used, simulated energy generation, capital cost and interest cost. Multivariate analysis was carried out using key variables like capital cost, interest and the capacity utilization factor (i.e. CUF which is actual generation of the plant and depends on the location (radiation) and technology used)to calculate the levelized tariff for a target equity IRR based on which the discount to be offered can be determined.\n\nThe final 30 solar PV projects selected had bids between INR 10.95 to INR 12.75. The Solar Thermal projects selected had bids between INR 10.24 to INR 12.24. PPAs were signed with IPPs in early January.\n\nIn December 2014, the Government of India introduced a scheme to establish at least 25 solar parks and Ultra Mega Solar Power Projects to add over 20 GW of installed solar power capacity. The Central Government provides financial support for the construction of these solar projects. As of December 2016, the Central Government has provided in-principle approval to set up 34 solar parks across 21 states. Each power project has a minimum capacity of 500 MW.\n\nGuidelines for the solar mission mandated cells and modules for solar PV projects based on crystalline silicon to be manufactured in India. That accounts to over 60% of total system costs. For solar thermal, guidelines mandated 30% project to have domestic content. A vigorous controversy emerged between power project developers and solar PV equipment manufacturers. The former camp prefers to source modules by accessing highly competitive global market to attain flexible pricing, better quality, predictable delivery and use of latest technologies. The latter camp prefers a controlled/planned environment to force developers to purchase modules from a small, albeit growing, group of module manufacturers in India. Manufacturers want to avoid competition with global players and are lobbying the government to incentivise growth of local industry.\n\nMarket responded to domestic content requirement by choosing to procure thin film modules from well established international players. A significant number of announced project completions are using modules from outside India.\n\nUS Trade Representative has filed a complaint at World Trade Organization challenging India’s domestic content requirements in Phase II of this Mission, citing discrimination against US exports and that industry in US which has invested hugely will be at loss. US insists that such restrictions are prohibited by WTO. India however claims that it is only an attempt to grow local potential and to ensure self sustenance and reduce dependence\n\n\n"}
{"id": "31444778", "url": "https://en.wikipedia.org/wiki?curid=31444778", "title": "Kite experiment", "text": "Kite experiment\n\nThe kite experiment is a scientific experiment in which a kite with a pointed, conductive wire attached to its apex is flown near thunder clouds to collect electricity from the air and conduct it down the wet kite string to the ground. It was proposed and may have been conducted by Benjamin Franklin with the assistance of his son William Franklin. The experiment's purpose was to uncover the unknown facts about the nature of lightning and electricity, and with further experiments on the ground, to demonstrate that lightning and electricity were the result of the same phenomenon.\n\nSpeculations of Jean-Antoine Nollet had led to the issue of the electrical nature of lightning being posed as a prize question at Bordeaux in 1749. In 1750, it was the subject of public discussion in France, with a dissertation of Denis Barberet receiving a prize in Bordeaux; Barberet proposed a cause in line with the triboelectric effect. The same year, Franklin reversed his previous skepticism of electrical lightning's attraction to high points. The physicist Jacques de Romas also wrote a mémoire with similar ideas that year, and later defended them as independent of Franklin's.\n\nIn 1752, Franklin proposed an experiment with conductive rods to attract lightning to a leyden jar, an early form of capacitor. Such an experiment was carried out in May 1752 at Marly-la-Ville in northern France by Thomas-François Dalibard. An attempt to replicate the experiment killed Georg Wilhelm Richmann in Saint Petersburg in August 1753; he was thought to be the victim of ball lightning. Franklin himself is said to have conducted the experiment in June 1752, supposedly on the top of the spire on Christ Church in Philadelphia. However, the spire at Christ Church was not added until 1754.\n\nFranklin's kite experiment was performed in Philadelphia in June 1752, according to the account by Priestly. Franklin described the experiment in the \"Pennsylvania Gazette\" in October 19, 1752, without mentioning that he himself had performed it. This account was read to the Royal Society on December 21 and printed as such in the \"Philosophical Transactions\".\nA more complete account of Franklin's experiment was given by Joseph Priestley in 1767, who presumably learned the details directly from Franklin, who was in London at the time Priestley wrote the book.\nAccording to the 1767 Priestley account, Franklin realized the dangers of using conductive rods and instead used the conductivity of a wet hemp string attached to a kite. This allowed him to stay on the ground while his son assisted him to fly the kite from the shelter of a nearby shed. This enabled Franklin and his son to keep the silk string of the kite dry to insulate them while the hemp string to the kite was allowed to get wet in the rain to provide conductivity. A house key belonging to Benjamin Loxley was attached to the hemp string and connected to a Leyden jar; a silk string was attached to this. \"At this key he charged phials, and from the electric fire thus obtained, he kindled spirits, and performed all other electrical experiments which are usually exhibited by an excited globe or tube.\" The kite was not struck by visible lightning; had it been, Franklin would almost certainly have been killed. However, Franklin did notice that loose threads of the kite string were repelling each other and deduced that the Leyden jar was being charged. He moved his hand near the key and observed an electric spark, proving the electric nature of lightning.\n\nThe Kite Experiment was described in The Pennsylvania Gazette, October 19, 1752 as follows:\n\nThe standard account of Franklin's experiment is disputed by science historian Tom Tucker in 2003. According to Tucker, Franklin never performed the experiment, and the kite as described is incapable of performing its alleged role.\n\n"}
{"id": "27228677", "url": "https://en.wikipedia.org/wiki?curid=27228677", "title": "Liberty Puzzles", "text": "Liberty Puzzles\n\nLiberty Puzzles is a manufacturer of classic style wooden jigsaw puzzles based in Boulder, Colorado. It was founded in 2005 by Christopher Wirth and his business partner Jeffrey Eldridge, after Wirth’s family inherited several hand-cut wooden puzzles from the 1930s. Surprised by the value of hand-cut wooden jigsaw puzzles (which can sell for more than $1,000 each), Wirth decided to start a business using modern cutting technologies, with a goal of producing puzzles in the $100 range. Wirth is the son of former Colorado senator Tim Wirth. \n\nLiberty Puzzles is the largest or second largest wooden jigsaw puzzle manufacturer in America as of 2011. The company offers over 600 different puzzle images, with an emphasis on fine art, vintage prints and Asian art. They continuously expand their range, increasingly with local contemporary artists. \n\nThe puzzles are made in Boulder, Colorado with archival paper and inks adhered to quarter-inch maple plywood cut with computer-controlled laser cutters. The puzzle designs are modeled after the puzzles popular in the early-twentieth century. Jigsaw puzzle historian Bob Armstrong notes: \"the style of cutting and figure pieces emulates the Falls puzzles from the 1930s\". Liberty's puzzles include a relatively large number of whimsy pieces (pieces shaped recognizably, for example, as storks or swans), \"as much as 20 percent of the pieces... [in some puzzles]\". Most Liberty Puzzles include the company's signature whimsy piece in the shape of an eagle.\n\n"}
{"id": "699152", "url": "https://en.wikipedia.org/wiki?curid=699152", "title": "Lighter than air", "text": "Lighter than air\n\nLighter than air refers to materials (usually gases) that are buoyant in air because they have average densities lower than that of air. Dry air has a density of about 1.29 g/L (gram per liter) at standard conditions for temperature and pressure (STP) and an average molecular mass of 28.97 g/mol.\n\nSome of these gases are used as lifting gases in lighter-than-air aircraft, which include free balloons, moored balloons, and airships. Heavier-than-air aircraft include airplanes, gliders and helicopters.\n\nHot air is frequently used in recreational ballooning. Hot air is lighter than air at ambient temperature, to a degree which can be approximated using the ideal gas law.\n\nNeon (density 0.900 g/L at STP, average atomic mass 20.17g/mol) is lighter than air and will lift a balloon. However, it is relatively rare on Earth, expensive, and is among the heavier of the lifting gases.\n\nWater vapor (density 0.804 g/L at STP, average molecular mass 18.015 g/mol) is lighter than air, and has successfully been used as a lifting gas. It is generally impractical due to high boiling point and condensation.\n\nAmmonia (density 0.769 g/L at STP, average molecular mass 17.03 g/mol) has sometimes been used to fill weather balloons. Due to its relatively high boiling point (compared to helium and hydrogen), ammonia could potentially be refrigerated and liquified aboard an airship to reduce lift and add ballast (and returned to a gas to add lift and reduce ballast).\n\nMethane (density 0.716 g/L at STP, average molecular mass 16.04 g/mol) is the chief component of natural gas and is sometimes used as a lift gas when hydrogen and helium are not available. It has the advantage of not leaking through balloon walls as rapidly as the small-moleculed hydrogen and helium. Many lighter-than-air balloons are made of aluminized plastic that limits such leakage; hydrogen and helium leak rapidly through latex balloons.\n\nHydrogen (density 0.090 g/L at STP, average molecular mass 2.016 g/mol) and helium (density 0.179 g/L at STP, average molecular mass 4.003 g/mol) are the most commonly used lift gases. Although helium is twice as heavy as (diatomic) hydrogen, they are both much lighter than air that this difference only results in hydrogen having 8% more buoyancy than helium.\n\nIn a practical dirigible design, the difference is significant, making a 50% difference in the fuel-carrying capacity of the dirigible and hence increasing its range significantly. However, hydrogen is extremely flammable and its use as a lifting gas in dirigibles has decreased since the Hindenburg disaster. Helium is safer as a lifting gas because it is inert and does not undergo combustion. \n\nNitrogen gas (density 1.251 g/L at STP, average atomic mass 28.00 g/mol) is about 3% lighter than air, insufficient for common use as a lifting gas.\n\nIn 2002, aerogel held the Guinness World Record for the least dense (lightest) solid. Aerogel is mostly air because its structure is like that of a highly vacuous sponge. The lightness and low density is due primarily to the large proportion of air within the solid and not the silicon construction materials. Taking advantage of this, SEAgel, in the same family as aerogel but made from agar, can be filled with helium gas to create a solid which floats when placed in an open top container filled with a dense gas.\n\nIn 2012, discovery of aerographite was announced, breaking the record for the least dense material at only 0.2 mg/cm (0.2 kg/m). These solids do not float in air because the hollow spaces in them become filled with air. No lighter-than-air matrix or shell containing a hard vacuum has ever been constructed.\n\n\n"}
{"id": "13162804", "url": "https://en.wikipedia.org/wiki?curid=13162804", "title": "List of countries by natural gas production", "text": "List of countries by natural gas production\n\nThis is a list of countries by natural gas production based on statistics from the International Energy Agency.\n\nThe data in the following table comes from The World Factbook.\n\n\n<br>\n"}
{"id": "960761", "url": "https://en.wikipedia.org/wiki?curid=960761", "title": "List of oil exploration and production companies", "text": "List of oil exploration and production companies\n\nThe following is a list of notable companies in the petroleum industry that are engaged in petroleum exploration and production. The list is in alphabetical order by continent and then by country. This list does not include companies only involved in refining and marketing.\n\n\n\n\n\n\n\n"}
{"id": "40676909", "url": "https://en.wikipedia.org/wiki?curid=40676909", "title": "List of prototype solar-powered cars", "text": "List of prototype solar-powered cars\n\nThis list of prototype solar-powered cars comprises multiperson, relatively practical vehicles powered completely or significantly by solar cells (panels or arrays, mounted on the vehicle) which convert sunlight into electricity to drive electric motors while the vehicle is in motion and have a homologation for public streets.\n\nThe Sunswift V (aka \"eVe\") from the University of New South Wales was built for the 2013 & 2015 World Solar Challenge.\nThe \"Solar Spirit 3\" with 3 seats was built by TAFE South Australia for the 2011 World Solar Challenge.\n\nThe \"e.coTech\" is a normal electric car developed by \"HiTech Electric\" and is sold only to small and medium-sized businesses. In 2017, the solar version of \"e.coTech\" was announced and presented to the public in 2018.\n\nThe \"Schulich Delta\" from the University of Calgary was built for the 2013 World Solar Challenge.\n\nThe handicraft enthusiast \"Chen Shungui\" developed 2 prototypes.\n\nThe solar vehicle was built in 2015 from the \"Hochschule Bochum\" to cross the desert of Tanami in Australia. Range with fully charged accumulator is 50 kilometers. The car have a solar roof with 160 Wp and in a box below the roof (not on the picture) 1943 Wp (!) solar panels for extension during driving breaks.\n\nThe \"PowerCore SunCruiser\" with 3 seats was built in 2013 by \"Hochschule Bochum\".\n\nThe Sion model from Sono Motors was presented to the public in July 2016. Two prototypes were financed by crowdfunding and were available for test drives in August 2017. The price will be EUR 16,000,- plus the battery you can rent or buy (<EUR 4.000, -). Pre-orders are available through the website of Sono Motors. Delivery is scheduled to start in the 3rd quarter of 2019.\n\nThe car has 5 doors, an 80kW engine with a maximum speed of 140 km/h, is rechargeable with AC power (3.7 kW or 22 kW) and direct current (50 kW). The battery will have a realistic range of 250 km.\n\nThe solar cells (1.208 Wp) integrated into the car body charge the battery so that, in good sunshine, up to 30 km of additional range per day are possible.\n\nIt is possible to use the current of the battery in the Sion to operate electrical devices or to charge other electric cars.\n\nThe workshop manual for the Sion including the CAD data of all spare parts for 3D printers or CNC Milling will be public, so that reasonable repair costs are to be expected.\n\nThe owner of the vehicle can offer other people a power supply (power-sharing) or a ride-in (ridesharing) or rent his car (car sharing).\n\nThe 2-seater \"SolarWorld GT\" was built 2011 by \"Hochschule Bochum\" with 2 wheel hub electric engines in the front wheels. After the participation at the 2011 World Solar Challenge, the car and his team did the first full autarky world circumnavigation (10/2011-12/2012). This record was admitted by Guinness World Records.\n\nOn the roof of the car are 3 m² solar cells and in the trunk additional 3 m² which are used during driving breaks. Maximum power creation of all solar cells is 1340Wp. More technical data can be found here:.\n\n\"The \"PowerCore SunCruiser\" with two seats was built by \"Hochschule Bochum\" for the 2015 World Solar Challenge.\n\nMaximum Power created on its 3 m² solar roof is 870 Wp. Car is equipped with 2 wheel hub electric engines and comfort electronics. Top speed is 120 km/h. Range with solar charging only is up to 600 kilometers per day. Range with fully charged accumulator (14,8 kWh) is up to 1100 kilometers.\n\nThe team SolarMobil from the Manipal University built a prototype 2-seater solar passenger car called \"SERVe\" in 2015.\n\nWith an EU - project led by Fiat (May 2010 - April 2013) a complete solar car with 3 seats was developed. This small car have four wheel drive. Range with solar charging only is 20 kilometers per day. Range with fully charged accumulator is 120 kilometers. Homologation and serial production is under investigation (as of September 2013).\n\nDeveloped and built in Emilia Romagna in 2018 by the University of Bologna, the Onda Solare Association and several industrial partners, Emilia 4 is a 4.6 meter long, 1.8 meter wide and 1.2 meter high 4 seats vehicle. It is equipped with a 5 square meter photovoltaic roof, made of 362 silicon cells, with a nominal power of 1200W.\n\nThe \"KAITON II\" from the \"Goko High School\" was built for the 2013 World Solar Challenge.\n\"OWL\" was built for the 2015 World Solar Challenge in the Cruiser class by Kogakuin University.\n\nVenturi Automobiles has built the open 2-seater \"Astrolab\" since 2006. With solar charging only, its range is 18 kilometers per day. Its range with the accumulator fully charged is 110 kilometers. It has a top speed of 120 km/h.\nVenturi Automobiles designed the flanking seating, open-sided 3-seater \"Eclectic\" as a prototype in 2006. Its range with solar charging only is 7 kilometers per day. Fully charged, its accumulator gives a range of 50 kilometers. The car has a top speed 50 km/h. Its price was 'middle of the road' as announced, but serial production never started. There is also a small electric car with the name \"Eclectic 2.0\" from the same company.\n\nThe 4-seater \"Stella\" was built for the 2013 World Solar Challenge by Eindhoven University. It was the winner in its class, and went on to win the crunchies award.\n\nThe 4-seater \"Stella Lux\" was built as successor of Stella for the 2015 World Solar Challenge by Eindhoven University. It was the winner in its class again. With a top speed of 125 km/h and a European range of 1000 kilometers it is a substantial achievement with respect to common electric vehicles.\n\nIn 2017 the Eindhoven team have built a new car for the World Solar Challenge, called Stella Vie, featuring five instead of four seats. By its introduction, this car was the most efficient family car ever built. More efficient than its predecessors due to its light weight and aerodynamic form and efficient enough to win the Cruiser Class of the World Solar Challenge for the third consecutive time with a big lead on the competition.\n\nThe Twente One is the second solar car developed by the University of Twente and Saxion University (at Enschede, Deventer and Apeldoorn) in 2007, and succeeds the SolUTra. It was designed to participate in the 2007 World Solar Challenge.\n\nIts main innovations were a pivoting solar screen that follows the angle of the sun, and a system of fresnel lenses that focus the sunlight onto a solar panel with a system to adjust the panel so that the maximum amount of light from the lenses shines on the individual solar cells.\n\nThe Twente One came in on the fifth place in the 2007 World solar Challenge.\n\nThe \"UltraCommuter\" from the University of Waikato was built for the 2013 World Solar Challenge.\n\nECO1, ECO1GL and ECO3GL are built by a technology enthusiast and entrepreneur Muhammad Aslam Azaad. Built in March 2014.\n\nA Mazda 5 was converted to an electric car named Metron 7 by Andrej Pecjak and his Metron team and in 2014 permitted for public use. Range with solar charging (160 Watt) is a few kilometers per day. Range with fully charged accumulator is 824 ! kilometers, proved from Berlin to Karlsruhe and through cities with average speed 72 km/h in June 2016.\n\nThe project \"Yuneec & Metron\" convert a Smart Roadster to an electric car with 80 Wp solar panel in 2011. Range with solar charging only is a few kilometers per day. Range with fully charged accumulator is 160 kilometers.\n\nThe project \"Icare\" build a Twike (Hybrid: muscular strength + electric engine) with an additional solar trailer (1800 Wp) and a small wind generator (diameter 1,8 m). With that vehicle they circumnavigate the world in 2010/11.\n\nThis vehicle circumnavigate the world 2007/08 with \"Lois Palmer\" as driver. Energy was delivered to 50% from the solar trailer (6 m²) and to 50% from the power grid (which was indirectly produced by a solar installation in Switzerland). Power consumption: 8 kWh / 100 km.\n\nThe \"Apollo Solar Car Team\" from National Kaohsiung University of Applied Sciences build a solar car for the 2013 World Solar Challenge.\n\n\"Daedalus\" of the University of Minnesota was built in 2013 for the World Solar Challenge. It is currently on display at the PTC world headquarters in Boston, MA.\n\n\"Eos\" of the University of Minnesota was built in 2015 for the World Solar Challenge and the 2016 American Solar Challenge. It is the only active multi-occupant solar car in the United States.\n"}
{"id": "34645512", "url": "https://en.wikipedia.org/wiki?curid=34645512", "title": "Mary Elizabeth Dawson", "text": "Mary Elizabeth Dawson\n\nMary Elizabeth Dawson (9 May 1833–22 February 1924) was a New Zealand servant, farmer, environmentalist and nurse. She was born in Mersham, Kent, England on 9 May 1833.\n"}
{"id": "23417661", "url": "https://en.wikipedia.org/wiki?curid=23417661", "title": "Motor fuel", "text": "Motor fuel\n\nA motor fuel is a fuel that is used to provide power to motor vehicles.\n\nCurrently, the majority of motor vehicles worldwide are powered by gasoline or diesel. Other energy sources include ethanol, biodiesel, propane, compressed natural gas (CNG), electric batteries charged from an external source, and hydrogen. The use of alternative fuels is increasing, especially in Europe. Before deciding on a particular fuel type, some factors should be considered:\n\n"}
{"id": "8840781", "url": "https://en.wikipedia.org/wiki?curid=8840781", "title": "Packhorse", "text": "Packhorse\n\nA packhorse or pack horse refers to a horse, mule, donkey, or pony used to carry goods on its back, usually in sidebags or panniers. Typically packhorses are used to cross difficult terrain, where the absence of roads prevents the use of wheeled vehicles. Use of packhorses dates from the neolithic period to the present day. Today, westernized nations primarily use packhorses for recreational pursuits, but they are still an important part of everyday transportation of goods throughout much of the third world and have some military uses in rugged regions.\n\nPackhorses have been used since the earliest period of domestication of the horse. They were invaluable throughout antiquity, through the Middle Ages, and into modern times where roads are nonexistent or poorly maintained.\n\nPackhorses were heavily used to transport goods and minerals in England from medieval times until the construction of the first turnpike roads and canals in the 18th century. Many routes crossed the Pennines between Lancashire and Yorkshire, enabling salt, limestone, coal, fleeces and cloth to be transported.\n\nSome had self-describing names, such as Limersgate and the Long Causeway; others were named after landmarks, such as the Reddyshore Scoutgate (\"gate\" is Old English for a road or way) and the Rapes Highway (after Rapes Hill). The medieval paths were marked by wayside crosses along their routes. Mount Cross, above the hamlet of Shore in the Cliviger Gorge, shows signs of Viking influence. As the Vikings moved eastwards from the Irish Sea in about 950 AD, it is likely that the pack horse routes were established from that time.\n\nMost packhorses were Galloways, small, stocky horses named after the Scottish district where they were first bred. Those employed in the lime-carriage trade were known as \"limegals\". Each pony could carry about in weight, spread between two panniers. Typically a train of ponies would number between 12 and 20, but sometimes up to 40. They averaged about a day. The train's leader commonly wore a bell to warn of its approach, since contemporary accounts emphasised the risk packhorse trains presented to others. They were particularly useful as roads were muddy and often impassable by wagon or cart, and there were no bridges over some major rivers in the north of England.\n\nAbout 1000 packhorses a day passed through Clitheroe before 1750, and \"commonly 200 to 300 laden horses every day over the River Calder (at a ford) called Fennysford in the King's Highway between Clitheroe and Whalley\" The importance of packhorse routes was reflected in jingles and rhymes, often \"aide-memoires\" of the routes.\n\nAs the need for cross-Pennine transportation increased, the main routes were improved, often by laying stone setts parallel to the horse track, at a distance of a cartwheel. They remained difficult in poor weather, the Reddyshore Scoutgate was \"notoriously difficult\", and became insufficient for a developing commercial and industrial economy. In the 18th century, canals started to be built in England and, following the Turnpike Act 1773, metalled roads. They made the ancient packhorse routes obsolete. Away from main routes, their use persisted into the 19th century leaving a legacy of paths across wilderness areas called \"packhorse routes, roads or trails\" and distinctive narrow, low sided stone arched packhorse bridges for example, at Marsden near Huddersfield. \"The Packhorse\" is a common public house name throughout England. During the 19th century, horses that transported officers' baggage during military campaigns were referred to as \"bathorses\" from the French \"bat\", meaning packsaddle.\n\nThe packhorse, mule or donkey was a critical tool in the development of the Americas. In colonial America, Spanish, French, Dutch and English traders made use of pack horses to carry goods to remote Native Americans and to carry hides back to colonial market centers. They had little choice, the America's had virtually no improved waterways before the 1820s and roads in times before the automobile were only improved locally around a municipality, and only rarely in between. This meant cities and towns were connected by roads which carts and wagons could navigate only with difficulty, for virtually every eastern hill or mountain with a shallow gradient was flanked by valleys with stream cut gullies and ravines in their bottoms, as well as Cut bank formations, including escarpments. Even a small stream would have steep banks in normal terrains.\n\nBy the 1790s the Lehigh Coal Mining Company was shipping Anthracite coal from Summit Hill, Pennsylvania to cargo boats on the Lehigh River using pack trains in what may be the earliest commercial mining company in North America. Afterwards in 1818−1827 its new management built first the Lehigh Canal, then the Mauch Chunk & Summit Hill Railroad, North America's second oldest which used mule trains to return the five ton coal cars the four hour climb the nine miles back to the upper terminus. Mules rode the roller-coaster precursor on the down trip to the docks, stables and paddocks below. The same company, as did its many competitors made extensive use of sure footed pack mules and donkeys in coal mines, including in some cases measures to stable the animals below ground. These were often managed by 'mule boys', a pay-grade up and a step above a breaker boy in the society of the times.\n\nAs the nation expanded west, packhorses, singly or in a pack train of several animals, were used by early surveyors and explorers, most notably by fur trappers, \"Mountain men\", and gold prospectors who covered great distances by themselves or in small groups. Packhorses were used by Native American people when traveling from place to place, and were also used by traders to carry goods to both Indian and White settlements. During a few decades of the 19th Century, enormous pack trains carried goods on the Old Spanish Trail from Santa Fe, New Mexico west to California.\n\nOn current United States Geological Survey maps, many such trails continue to be labeled \"pack trail\".\n\nPackhorses are used worldwide to convey many products. In feudal Japan riding in a saddle (kura) was reserved for the samurai class until the end of the samurai era (1868), lower classes would ride on a pack saddle (\"ni-gura or konida-gura\") or bareback. Pack horses (\"ni-uma or konida-uma\") carried a variety of merchandise and the baggage of travelers using a pack saddle that ranged from a basic wooden frame to the elaborate pack saddles used for the semi-annual processions (sankin kotai) of Daimyō. Pack horses also carried the equipment and food for samurai warriors during military campaigns.\n\nIn North America and Australia, in areas such the Bicentennial National Trail, the packhorse plays a major role in recreational pursuits, particularly to transport goods and supplies into wilderness areas and where motor vehicles are either prohibited or impracticable. They are used by mounted outfitters, hunters, campers, stockmen and cowboys to carry tools and equipment that cannot be carried with the rider. They are used by guest ranches to transport materials to remote locations to set up campsites for tourists and guests. They are used by the United States Forest Service and the National Park Service to carry in supplies to maintain trails, cabins and bring in commercial goods to backcountry tourist lodges and other remote, permanent residences.\nIn the third world, packhorses and donkeys to an even greater extent, still haul goods to market, carry supplies for workers, and many other of the same jobs that have been performed for millennia.\n\nIn modern warfare, pack mules are used to bring supplies to areas where roads are poor and fuel supply is uncertain. For example, they are a critical part of the supply chain for all sides of the conflict in remote parts of Afghanistan.\n\nFoundation training of the packhorse is similar to that of a riding horse. Many, though not all packhorses are also trained to be ridden. In addition, a packhorse is required to have additional skills that may not be required of a riding horse. A pack horse is required to be tolerant of close proximity to other animals in the packstring, both to the front and to the rear. The horse must also be tolerant of breeching, long ropes, noisy loads, and the shifting of the load during transit. Patience and tolerance is crucial; for example, while there are many ways that pack horses are put into a pack string, one method incorporates tying the halter lead of one animal to the tail of the animal in front of it, an act that often provokes kicking or bolting in untrained animals.\n\nLoading of a packhorse requires care. Weight carried is the first factor to consider. The average horse can carry up to approximately 30% of its body weight. Thus, a horse cannot carry more than . A load carried by a packhorse also has to be balanced, with weight even on both sides to the greatest degree possible.\n\n\n"}
{"id": "573343", "url": "https://en.wikipedia.org/wiki?curid=573343", "title": "Refractory metals", "text": "Refractory metals\n\nRefractory metals are a class of metals that are extraordinarily resistant to heat and wear. The expression is mostly used in the context of materials science, metallurgy and engineering. The definition of which elements belong to this group differs. The most common definition includes five elements: two of the fifth period (niobium and molybdenum) and three of the sixth period (tantalum, tungsten, and rhenium). They all share some properties, including a melting point above 2000 °C and high hardness at room temperature. They are chemically inert and have a relatively high density. Their high melting points make powder metallurgy the method of choice for fabricating components from these metals. Some of their applications include tools to work metals at high temperatures, wire filaments, casting molds, and chemical reaction vessels in corrosive environments. Partly due to the high melting point, refractory metals are stable against creep deformation to very high temperatures.\n\nMost definitions of the term 'refractory metals' list the extraordinarily high melting point as a key requirement for inclusion. By one definition, a melting point above is necessary to qualify. The five elements niobium, molybdenum, tantalum, tungsten and rhenium are included in all definitions, while the wider definition, including all elements with a melting point above , includes a varying number of nine additional elements: titanium, vanadium, chromium, zirconium, hafnium, ruthenium, rhodium, osmium and iridium. The artificial elements, being radioactive, are never considered to be part of the refractory metals, although technetium has a melting point of 2430 K or 2157 °C and rutherfordium is predicted to have melting point of 2400 K or 2100 °C.\n\nThe melting point of the refractory metals are the highest for all elements except carbon, osmium and iridium. This high melting point defines most of their applications. All the metals are body-centered cubic except rhenium which is hexagonal close-packed. Most physical properties of the elements in this group vary significantly because they are members of different groups.\n\nCreep resistance is a key property of the refractory metals. In metals, the starting of creep correlates with the melting point of the material; the creep in aluminium alloys starts at 200 °C, while for refractory metals temperatures above 1500 °C are necessary. This resistance against deformation at high temperatures makes the refractory metals suitable against strong forces at high temperature, for example in jet engines, or tools used during forging.\n\nThe refractory metals show a wide variety of chemical properties because they are members of three distinct groups in the periodic table. They are easily oxidized, but this reaction is slowed down in the bulk metal by the formation of stable oxide layers on the surface. Especially the oxide of rhenium is more volatile than the metal, and therefore at high temperature the stabilization against the attack of oxygen is lost, because the oxide layer evaporates. They all are relatively stable against acids.\n\nRefractory metals are used in lighting, tools, lubricants, nuclear reaction control rods, as catalysts, and for their chemical or electrical properties. Because of their high melting point, refractory metal components are never fabricated by casting. The process of powder metallurgy is used. Powders of the pure metal are compacted, heated using electric current, and further fabricated by cold working with annealing steps. Refractory metals can be worked into wire, ingots, rebars, sheets or foil.\n\nMolybdenum based alloys are widely used, because they are cheaper than superior tungsten alloys. The most widely used alloy of molybdenum is the Titanium-Zirconium-Molybdenum alloy TZM, composed of 0.5% titanium and 0.08% of zirconium (with molybdenum being the rest). The alloy exhibits a higher creep resistance and strength at high temperatures, making service temperatures of above 1060 °C possible for the material. The high resistivity of Mo-30W, an alloy of 70% molybdenum and 30% tungsten, against the attack of molten zinc makes it the ideal material for casting zinc. It is also used to construct valves for molten zinc.\n\nMolybdenum is used in mercury wetted reed relays, because molybdenum does not form amalgams and is therefore resistant to corrosion by liquid mercury.\n\nMolybdenum is the most commonly used of the refractory metals. Its most important use is as a strengthening alloy of steel. Structural tubing and piping often contains molybdenum, as do many stainless steels. Its strength at high temperatures, resistance to wear and low coefficient of friction are all properties which make it invaluable as an alloying compound. Its excellent anti-friction properties lead to its incorporation in greases and oils where reliability and performance are critical. Automotive constant-velocity joints use grease containing molybdenum. The compound sticks readily to metal and forms a very hard, friction resistant coating. Most of the world's molybdenum ore can be found in China, the USA, Chile and Canada.\n\nTungsten was discovered in 1781 by the Swedish chemist, Carl Wilhelm Scheele. Tungsten has the highest melting point of all metals, at .\n\nUp to 22% rhenium is alloyed with tungsten to improve its high temperature strength and corrosion resistance. Thorium as an alloying compound is used when electric arcs have to be established. The ignition is easier and the arc burns more stably than without the addition of thorium. For powder metallurgy applications, binders have to be used for the sintering process. For the production of the tungsten heavy alloy, binder mixtures of nickel and iron or nickel and copper are widely used. The tungsten content of the alloy is normally above 90%. The diffusion of the binder elements into the tungsten grains is low even at the sintering temperatures and therefore the interior of the grains is pure tungsten.\n\nTungsten and its alloys are often used in applications where high temperatures are present but still a high strength is necessary and the high density is not troublesome. Tungsten wire filaments provide the vast majority of household incandescent lighting, but are also common in industrial lighting as electrodes in arc lamps. Lamps get more efficient in the conversion of electric energy to light with higher temperatures and therefore a high melting point is essential for the application as filament in incandescent light. Gas tungsten arc welding (GTAW, also known as tungsten inert gas (TIG) welding) equipment uses a permanent, non-melting electrode. The high melting point and the wear resistance against the electric arc makes tungsten a suitable material for the electrode.\n\nTungsten's high density and strength is also a key property for its use in weapon projectiles, for example as an alternative to depleted Uranium for tank guns. Its high melting point makes tungsten a good material for applications like rocket nozzles, for example in the UGM-27 Polaris. Some of the applications of tungsten are not related to its refractory properties but simply to its density. For example, it is used in balance weights for planes and helicopters or for heads of golf clubs. In this applications similar dense materials like the more expensive osmium can also be used.\n\nThe most common use for tungsten is as the compound tungsten carbide in drill bits, machining and cutting tools. The largest reserves of tungsten are in China, with deposits in Korea, Bolivia, Australia, and other countries.\n\nIt also finds itself serving as a lubricant, antioxidant, in nozzles and bushings, as a protective coating and in many other ways. Tungsten can be found in printing inks, x-ray screens, photographic chemicals, in the processing of petroleum products, and flame proofing of textiles.\n\nNiobium is nearly always found together with tantalum, and was named after Niobe, the daughter of the mythical Greek king Tantalus for whom tantalum was named. Niobium has many uses, some of which it shares with other refractory metals. It is unique in that it can be worked through annealing to achieve a wide range of strength and elasticity, and is the least dense of the refractory metals. It can also be found in electrolytic capacitors and in the most practical superconducting alloys. Niobium can be found in aircraft gas turbines, vacuum tubes and nuclear reactors.\n\nAn alloy used for liquid rocket thruster nozzles, such as in the main engine of the Apollo Lunar Modules, is C103, which consists of 89% niobium, 10% hafnium and 1% titanium. Another niobium alloy was used for the nozzle of the Apollo Service Module. As niobium is oxidized at temperatures above 400 °C, a protective coating is necessary for these applications to prevent the alloy from becoming brittle.\n\nTantalum is one of the most corrosion resistant substances available.\n\nMany important uses have been found for tantalum owing to this property, particularly in the medical and surgical fields, and also in harsh acidic environments. It is also used to make superior electrolytic capacitors. Tantalum films provide the second most capacitance per volume of any substance after Aerogel, and allow miniaturization of electronic components and circuitry. Many cellular phones and computers contain tantalum capacitors.\n\nRhenium is the most recently discovered refractory metal. It is found in low concentrations with many other metals, in the ores of other refractory metals, platinum or copper ores. It is useful as an alloy to other refractory metals, where it adds ductility and tensile strength. Rhenium alloys are being used in electronic components, gyroscopes and nuclear reactors. Rhenium finds its most important use as a catalyst. It is used as a catalyst in reactions such as alkylation, dealkylation, hydrogenation and oxidation. However its rarity makes it the most expensive of the refractory metals.\n\nRefractory metals and alloys attract the attention of investigators because of their remarkable properties and promising practical usefulness.\n\nPhysical properties of refractory metals, such as molybdenum, tantalum and tungsten, their strength, and high-temperature stability make them suitable material for hot metalworking applications and for vacuum furnace technology. Many special applications exploit these properties: for example, tungsten lamp filaments operate at temperatures up to 3073 K, and molybdenum furnace windings withstand to 2273 K.\n\nHowever, poor low-temperature fabricability and extreme oxidability at high temperatures are shortcomings of most refractory metals. Interactions with the environment can significantly influence their high-temperature creep strength. Application of these metals requires a protective atmosphere or coating.\n\nThe refractory metal alloys of molybdenum, niobium, tantalum, and tungsten have been applied to space nuclear power systems. These systems were designed to operate at temperatures from 1350 K to approximately 1900 K. An environment must not interact with the material in question. Liquid alkali metals as the heat transfer fluids are used as well as the ultra-high vacuum.\n\nThe high-temperature creep strain of alloys must be limited for them to be used. The creep strain should not exceed 1–2%. An additional complication in studying creep behavior of the refractory metals is interactions with environment, which can significantly influence the creep behavior.\n\n\n"}
{"id": "1503750", "url": "https://en.wikipedia.org/wiki?curid=1503750", "title": "Rolling resistance", "text": "Rolling resistance\n\nRolling resistance, sometimes called rolling friction or rolling drag, is the force resisting the motion when a body (such as a ball, tire, or wheel) rolls on a surface. It is mainly caused by non-elastic effects; that is, not all the energy needed for deformation (or movement) of the wheel, roadbed, etc. is recovered when the pressure is removed. Two forms of this are hysteresis losses (see below), and permanent (plastic) deformation of the object or the surface (e.g. soil). Another cause of rolling resistance lies in the slippage between the wheel and the surface, which dissipates energy. Note that only the last of these effects involves friction, therefore the name \"rolling friction\" is to an extent a misnomer.\n\nIn analogy with sliding friction, rolling resistance is often expressed as a coefficient times the normal force. This coefficient of rolling resistance is generally much smaller than the coefficient of sliding friction.\n\nAny coasting wheeled vehicle will gradually slow down due to rolling resistance including that of the bearings, but a train car with steel wheels running on steel rails will roll farther than a bus of the same mass with rubber tires running on tarmac. Factors that contribute to rolling resistance are the (amount of) deformation of the wheels, the deformation of the roadbed surface, and movement below the surface. Additional contributing factors include wheel diameter, speed, load on wheel, surface adhesion, sliding, and relative micro-sliding between the surfaces of contact. The losses due to hysteresis also depend strongly on the material properties of the wheel or tire and the surface. For example, a rubber tire will have higher rolling resistance on a paved road than a steel railroad wheel on a steel rail. Also, sand on the ground will give more rolling resistance than concrete.\n\nThe primary cause of pneumatic tire rolling resistance is hysteresis:\n\nA characteristic of a deformable material such that the energy of deformation is greater than the energy of recovery. The rubber compound in a tire exhibits hysteresis. As the tire rotates under the weight of the vehicle, it experiences repeated cycles of deformation and recovery, and it dissipates the hysteresis energy loss as heat. Hysteresis is the main cause of energy loss associated with rolling resistance and is attributed to the viscoelastic characteristics of the rubber.\n\nThis main principle is illustrated in the figure of the rolling cylinders. If two equal cylinders are pressed together then the contact surface is flat. In the absence of surface friction, contact stresses are normal (i.e. perpendicular) to the contact surface. Consider a particle that enters the contact area at the right side, travels through the contact patch and leaves at the left side. Initially its vertical deformation is increasing, which is resisted by the hysteresis effect. Therefore, an additional pressure is generated to avoid interpenetration of the two surfaces. Later its vertical deformation is decreasing. This is again resisted by the hysteresis effect. In this case this decreases the pressure that is needed to keep the two bodies separate.\n\nThe resulting pressure distribution is asymmetrical and is shifted to the right. The line of action of the (aggregate) vertical force no longer passes through the centers of the cylinders. This means that a moment occurs that tends to retard the rolling motion.\n\nMaterials that have a large hysteresis effect, such as rubber, which bounce back slowly, exhibit more rolling resistance than materials with a small hysteresis effect that bounce back more quickly and more completely, such as steel or silica. Low rolling resistance tires typically incorporate silica in place of carbon black in their tread compounds to reduce low-frequency hysteresis without compromising traction. Note that railroads also have hysteresis in the roadbed structure.\n\nIn the broad sense, specific \"rolling resistance\" (for vehicles) is the force per unit vehicle weight required to move the vehicle on level ground at a constant slow speed where aerodynamic drag (air resistance) is insignificant and also where there are no traction (motor) forces or brakes applied. In other words, the vehicle would be coasting if it were not for the force to maintain constant speed. This broad sense includes wheel bearing resistance, the energy dissipated by vibration and oscillation of both the roadbed and the vehicle, and sliding of the wheel on the roadbed surface (pavement or a rail).\n\nBut there is an even broader sense that would include energy wasted by wheel slippage due to the torque applied from the engine. This includes the increased power required due to the increased velocity of the wheels where the tangential velocity of the driving wheel(s) becomes greater than the vehicle speed due to slippage. Since power is equal to force times velocity and the wheel velocity has increased, the power required has increased accordingly.\n\nThe pure \"rolling resistance\" for a train is that which happens due to deformation and possible minor sliding at the wheel-road contact. For a rubber tire, an analogous energy loss happens over the entire tire, but it is still called \"rolling resistance\". In the broad sense, \"rolling resistance\" includes wheel bearing resistance, energy loss by shaking both the roadbed (and the earth underneath) and the vehicle itself, and by sliding of the wheel, road/rail contact. Railroad textbooks seem to cover all these resistance forces but do not call their sum \"rolling resistance\" (broad sense) as is done in this article. They just sum up all the resistance forces (including aerodynamic drag) and call the sum basic train resistance (or the like).\n\nSince railroad rolling resistance in the broad sense may be a few times larger than just the pure rolling resistance reported values may be in serious conflict since they may be based on different definitions of \"rolling resistance\". The train's engines must, of course, provide the energy to overcome this broad-sense rolling resistance.\n\nFor tires, rolling resistance is defined as the energy consumed by a tire per unit distance covered. It is also called rolling friction or rolling drag. It is one of the forces that act to oppose the motion of a driver. The main reason for this is that when the tires are in motion and touch the surface, the surface changes shape and causes deformation of the tire.\n\nFor highway motor vehicles, there is obviously some energy dissipated in shaking the roadway (and the earth beneath it), the shaking of the vehicle itself, and the sliding of the tires. But, other than the additional power required due to torque and wheel bearing friction, non-pure rolling resistance doesn't seem to have been investigated, possibly because the \"pure\" rolling resistance of a rubber tire is several times higher than the neglected resistances.\n\nThe \"rolling resistance coefficient\" is defined by the following equation:\n\nformula_3 is the force needed to push (or tow) a wheeled vehicle forward (at constant speed on a level surface, or zero grade, with zero air resistance) per unit force of weight. It is assumed that all wheels are the same and bear identical weight. Thus: formula_6 means that it would only take 0.01 pounds to tow a vehicle weighing one pound. For a 1000-pound vehicle, it would take 100 times more tow force, i.e. 10 pounds. One could say that formula_3 is in lb(tow-force)/lb(vehicle weight). Since this lb/lb is force divided by force, formula_3 is dimensionless. Multiply it by 100 and you get the percent (%) of the weight of the vehicle required to maintain slow steady speed. formula_3 is often multiplied by 1000 to get the parts per thousand, which is the same as kilograms (kg force) per metric ton (tonne = 1000 kg ), which is the same as pounds of resistance per 1000 pounds\nof load or Newtons/kilo-Newton, etc. For the US railroads, lb/ton has been traditionally used; this is just formula_10. Thus, they are all just measures of resistance per unit vehicle weight. While they are all \"specific resistances\", sometimes they are just called \"resistance\" although they are really a coefficient (ratio)or a multiple thereof. If using pounds or kilograms as force units, mass is equal to weight (in earth's gravity a kilogram a mass weighs a kilogram and exerts a kilogram of force) so one could claim that formula_3 is also the force per unit mass in such units. The SI system would use N/tonne (N/T, N/t), which is formula_12 and is force per unit mass, where \"g\" is the acceleration of gravity in SI units (meters per second square).\n\nThe above shows resistance proportional to formula_13 but does not explicitly show any variation with speed, loads, torque, surface roughness, diameter, tire inflation/wear, etc. because formula_3 itself varies with those factors. It might seem from the above definition of formula_13 that the rolling resistance is directly proportional to vehicle weight but it is not.\n\nThere are at least two popular models for calculating rolling resistance.\n\n\nThe results of these tests can be hard for the general public to obtain as manufacturers prefer to publicize \"comfort\" and \"performance\".\n\nThe coefficient of rolling resistance for a slow rigid wheel on a perfectly elastic surface, not adjusted for velocity, can be calculated by \n\nEmpirical formula for formula_20 for cast iron mine car wheels on steel rails.\n\nAs an alternative to using formula_24 one can use formula_25, which is a different rolling resistance coefficient or coefficient of rolling friction with dimension of length. It is defined by the following formula:\n\nThe above equation, where resistance is inversely proportional to radius r. seems to be based on the discredited \"Coulomb's law\" (Neither Coulomb's inverse square law nor Coulomb's law of friction) (needs citation). See dependence on diameter. Equating this equation with the force per the rolling resistance coefficient, and solving for b, gives b = C·r. Therefore, if a source gives rolling resistance coefficient (C) as a dimensionless coefficient, it can be converted to b, having units of length, by multiplying C by wheel radius r.\n\nTable of rolling resistance coefficient examples: \n\nFor example, in earth gravity, a car of 1000 kg on asphalt will need a force of around 100 newtons for rolling (1000 kg × 9.81 m/s × 0.01 = 98.1 N).\n\nAccording to Dupuit (1837), rolling resistance (of wheeled carriages with wooden wheels with iron tires) is approximately inversely proportional to the square root of wheel diameter. This rule has been experimentally verified for cast iron wheels (8\" - 24\" diameter) on steel rail and for 19th century carriage wheels. But there are other tests on carriage wheels that do not agree. Theory of a cylinder rolling on an elastic roadway also gives this same rule These contradict earlier (1785) tests by Coulomb of rolling wooden cylinders where Coulomb reported that rolling resistance was inversely proportional to the diameter of the wheel (known as \"Coulomb's law\"). This disputed (or wrongly applied)\n-\"Coulomb's law\" is still found in handbooks, however.\n\nFor pneumatic tires on hard pavement, it is reported that the effect of diameter on rolling resistance is negligible (within a practical range of diameters).\n\nThe driving torque formula_31 to overcome rolling resistance formula_32 and maintain steady speed on level ground (with no air resistance) can be calculated by:\nIt is noteworthy that formula_36 is usually not equal to the radius of the rolling body.\n\n\"Applied torque\" may either be driving torque applied by a motor (often through a transmission) or a braking torque applied by brakes (including regenerative braking). Such torques results in energy dissipation (above that due to the basic rolling resistance of a freely rolling, non-driven, non-braked wheel). This additional loss is in part due to the fact that there is some slipping of the wheel, and for pneumatic tires, there is more flexing of the sidewalls due to the torque. Slip is defined such that a 2% slip means that the circumferential speed of the driving wheel exceeds the speed of the vehicle by 2%.\n\nA small percentage slip can result in a much larger percentage increase in rolling resistance. For example, for pneumatic tires, a 5% slip can translate into a 200% increase in rolling resistance. This is partly because the tractive force applied during this slip is many times greater than the rolling resistance force and thus much more power per unit velocity is being applied (recall power = force x velocity so that power per unit of velocity is just force). So just a small percentage increase in circumferential velocity due to slip can translate into a loss of traction power which may even exceed the power loss due to basic (ordinary) rolling resistance. For railroads, this effect may be even more pronounced due to the low rolling resistance of steel wheels.\n\nIn order to apply any traction to the wheels, some slippage of the wheel is required. For Russian trains climbing up a grade, this slip is normally 1.5% to 2.5%.\n\nSlip (also known as creep) is normally roughly directly proportional to tractive effort. An exception is if the tractive effort is so high that the wheel is close to substantial slipping (more than just a few percent as discussed above), then slip rapidly increases with tractive effort and is no longer linear. With a little higher applied tractive effort the wheel spins out of control and the adhesion drops resulting in the wheel spinning even faster. This is the type of slipping that is observable by eye—the slip of say 2% for traction is only observed by instruments. Such rapid slip may result in excessive wear or damage.\n\nRolling resistance greatly increases with applied torque. At high torques, which apply a tangential force to the road of about half the weight of the vehicle, the rolling resistance may triple (a 200% increase). This is in part due to a slip of about 5%. The rolling resistance increase with applied torque is not linear, but increases at a faster rate as the torque becomes higher.\n\nThe rolling resistance coefficient, Crr, significantly decreases as the weight of the rail car per wheel increases. For example, an empty Russian freight car had about twice the Crr as loaded car (Crr=0.002 vs. Crr=0.001). This same \"economy of scale\" shows up in testing of mine rail cars. The theoretical Crr for a rigid wheel rolling on an elastic roadbed shows Crr inversely proportional to the square root of the load.\n\nIf Crr is itself dependent on wheel load per an inverse square-root rule, then for an increase in load of 2% only a 1% increase in rolling resistance occurs.\n\nFor pneumatic tires, the direction of change in Crr (rolling resistance coefficient) depends on whether or not tire inflation is increased with increasing load. It is reported that, if inflation pressure is increased with load according to an (undefined) \"schedule\", then a 20% increase in load decreases Crr by 3%. But, if the inflation pressure is not changed, then a 20% increase in load results in a 4% increase in Crr. Of course, this will increase the rolling resistance by 20% due to the increase in load plus 1.2 x 4% due to the increase in Crr resulting in a 24.8% increase in rolling resistance.\n\nWhen a vehicle (motor vehicle or railroad train) goes around a curve, rolling resistance usually increases. If the curve is not banked so as to exactly counter the centrifugal force with an equal and opposing centripetal force due to the banking, then there will be a net unbalanced sideways force on the vehicle.\nThis will result in increased rolling resistance. Banking is also known as \"superelevation\" or \"cant\" (not to be confused with rail cant of a rail). For railroads, this is called curve resistance but for roads it has (at least once) been called rolling resistance due to cornering.\n\nRolling friction generates sound (vibrational) energy, as mechanical energy is converted to this form of energy due to the friction. One of the most common examples of rolling friction is the movement of motor vehicle tires on a roadway, a process which generates sound as a by-product. The sound generated by automobile and truck tires as they roll (especially noticeable at highway speeds) is mostly due to the percussion of the tire treads, and compression (and subsequent decompression) of air temporarily captured within the treads.\n\nSeveral factors affect the magnitude of rolling resistance a tire generates:\n\nIn a broad sense rolling resistance can be defined as the sum of components): \n\nWheel bearing torque losses can be measured as a rolling resistance at the wheel rim, Crr. Railroads normally use roller bearings which are either cylindrical (Russia) or tapered (United States). The specific rolling resistance in Russian bearings varies with both wheel loading and speed. Wheel bearing rolling resistance is lowest with high axle loads and intermediate speeds of 60–80 km/h with a Crr of 0.00013 (axle load of 21 tonnes). For empty freight cars with axle loads of 5.5 tonnes, Crr goes up to 0.00020 at 60 km/h but at a low speed of 20 km/h it increases to 0.00024 and at a high speed (for freight trains) of 120 km/h it is 0.00028. The Crr obtained above is added to the Crr of the other components to obtain the total Crr for the wheels.\n\nThe rolling resistance of steel wheels on steel rail of a train is far less than that of the rubber tires wheels of an automobile or truck. The weight of trains varies greatly; in some cases they may be much heavier per passenger or per net ton of freight than an automobile or truck, but in other cases they may be much lighter.\n\nAs an example of a very heavy passenger train, in 1975, Amtrak passenger trains weighed a little over 7 tonnes per passenger, which is much heavier than an average of a little over one ton per passenger for an automobile. This means that for an Amtrak passenger train in 1975, much of the energy savings of the lower rolling resistance was lost to its greater weight.\n\nAn example of a very light high-speed passenger train is the N700 Series Shinkansen, which weighs 715 tonnes and carries 1323 passengers, resulting in a per-passenger weight of about half a tonne. This lighter weight per passenger, combined with the lower rolling resistance of steel wheels on steel rail means that an N700 Shinkansen is much more energy efficient than a typical automobile.\n\nIn the case of freight, CSX ran an advertisement campaign in 2013 claiming that their freight trains move \"a ton of freight 436 miles on a gallon of fuel\", whereas some sources claim trucks move a ton of freight about 130 miles per gallon of fuel, indicating trains are more efficient overall.\n\n\n\n"}
{"id": "3645417", "url": "https://en.wikipedia.org/wiki?curid=3645417", "title": "Society for the Study of Evolution", "text": "Society for the Study of Evolution\n\nThe Society for the Study of Evolution is a professional organization of evolutionary biologists. It was formed in the United States in 1946 to promote evolution and the integration of various fields of science concerned with evolution and to organize the publication of a scientific journal to report on new research on evolution across a variety of fields.\n\nThe Society was established at a meeting in St. Louis on March 30, 1946. Fifty-seven scientists attended the meeting, which was chaired by Alfred E. Emerson. George Gaylord Simpson was elected as the Society's first President, with E. B. Babcock, Emerson, and J. T. Patterson as his Vice-presidents and Ernst Mayr as secretary. This society grew as an extension of the US National Research Council's Committee on Common Problems of Genetics and Paleontology (later renamed the Committee on Common Problems of Genetics, Paleontology and Systematics).\n\nThe first annual meeting of the society was held in Boston, December 28–31, 1946. A grant from the American Philosophical Society led to the publication of the journal \"Evolution\".\n\nCommonly known as 'evolution meeting,' the annual conference is often held together with the Society of Systematic Biologists and the American Society of Naturalists.\n\nThe society has an official journal \"Evolution\". It was started in 1947 and is published by John Wiley & Sons. In 2017, it launched a second journal \"Evolution Letters\".\n\n\n"}
{"id": "18420794", "url": "https://en.wikipedia.org/wiki?curid=18420794", "title": "South African Petroleum Industry Association", "text": "South African Petroleum Industry Association\n\nSAPIA (South African Petroleum Industry Association) was formed in July 1994 by six of South Africa's leading petroleum refining and marketing companies namely: \n\n\nThe stated goal of the organisation is to represent the interests of the petroleum refining and marketing industry in South Africa. The organisation's aim is to ensure that all interested parties clearly understand the contribution that this industry makes to the economic and social development of South Africa. Zenex Oil (PTY) LTD has subsequently become part of Engen Petroleum LTD. Sasol LTD and TEPCO Petroleum (PTY) LTD joined the organisation in 2000. Mossgas (PTY) LTD was replaced by PetroSA (PTY) LTD in 2002. \n\nSAPIA is managed by a board with representation from each of the member companies. The organisation's headquarters is situated in Johannesburg with Mr. A (Avhaphani) Tshifularo as its current executive director.\n\nThe organisation's website provides information as to how fuel prices in South Africa are determined (many liquid fuels in South Africa are subject to price controls). The site also has information on the demand for liquid fuel in South Africa.\n\nSAPIA was in the news early in 1995 during the arguments about the protection that Sasol (then not yet a member) received from government. In protest, SAPIA withdrew from the National Economic Development & Labour Council (NEDLAC) initiative's liquid fuels task team and was strongly criticized for this move, especially by Cosatu.\n\n"}
{"id": "644527", "url": "https://en.wikipedia.org/wiki?curid=644527", "title": "Tachyon condensation", "text": "Tachyon condensation\n\nTachyon condensation is a process in particle physics in which a system can lower its energy by spontaneously producing particles. The end result is a \"condensate\" of particles that fills the volume of the system. Tachyon condensation is closely related to second-order phase transitions.\n\nTachyon condensation is a process in which a tachyonic field—usually a scalar field—with a complex mass acquires a vacuum expectation value and reaches the minimum of the potential energy. While the field is tachyonic and unstable near the local maximum of the potential, the field gets a non-negative squared mass and becomes stable near the minimum.\n\nThe appearance of tachyons is a potentially serious problem for any theory; examples of tachyonic fields amenable to condensation are all cases of spontaneous symmetry breaking. In condensed matter physics a notable example is ferromagnetism; in particle physics the best known example is the Higgs mechanism in the standard model that breaks the electroweak symmetry.\n\nAlthough the notion of a tachyonic imaginary mass might seem troubling because there is no classical interpretation of an imaginary mass, the mass is not quantized. Rather, the scalar field is; even for tachyonic quantum fields, the field operators at spacelike separated points still commute (or anticommute), thus preserving causality. Therefore, information still does not propagate faster than light, and solutions grow exponentially, but not superluminally (there is no violation of causality).\n\nThe \"imaginary mass\" really means that the system becomes unstable. The zero value field is at a local maximum rather than a local minimum of its potential energy, much like a ball at the top of a hill. A very small impulse (which will always happen due to quantum fluctuations) will lead the field to roll down with exponentially increasing amplitudes toward the local minimum. In this way, tachyon condensation drives a physical system that has reached a local limit and might naively be expected to produce physical tachyons, to an alternate stable state where no physical tachyons exist. Once the tachyonic field reaches the minimum of the potential, its quanta are not tachyons any more but rather are ordinary particles with a positive mass-squared, such as the Higgs boson.\n\nIn the late 1990s, Ashoke Sen conjectured that the tachyons carried by open strings attached to D-branes in string theory reflect the instability of the D-branes with respect to their complete annihilation. The total energy carried by these tachyons has been calculated in string field theory; it agrees with the total energy of the D-branes, and all other tests have confirmed Sen's conjecture as well. Tachyons therefore became an active area of interest in the early 2000s.\n\nThe character of closed-string tachyon condensation is more subtle, though the first steps towards our understanding of their fate have been made by Adams, Polchinski, and Silverstein, in the case of twisted closed string tachyons, and by Simeon Hellerman and Ian Swanson, in a wider array of cases. The fate of the closed string tachyon in the 26-dimensional bosonic string theory remains unknown, though recent progress has revealed interesting new developments.\n\n\n"}
{"id": "52156276", "url": "https://en.wikipedia.org/wiki?curid=52156276", "title": "Tidal flooding", "text": "Tidal flooding\n\nTidal flooding, also known as sunny day flooding or nuisance flooding, is the temporary inundation of low-lying areas, especially streets, during exceptionally high tide events, such as at full and new moons. The highest tides of the year may be known as the king tide, with the month varying by location. In Florida, controversy was created when state-level government mandated that the term \"nuisance flooding\" and other terms be used in place of terms such as sea level rise, climate change and global warming, prompting allegations of climate change denial, specifically against Governor Rick Scott. This amid Florida, specifically South Florida and the Miami metropolitan area being one of the most at risk areas in the world for the potential effects of sea level rise, and where the frequency and severity of tidal flooding events increased in the 21st century. The issue is more bipartisan in South Florida, particularly in places like Miami Beach, where a several hundred million dollar project is underway to install more than 50 pumps and physically raise roads to combat the flooding, mainly along the west side of South Beach, formerly a mangrove wetland where the average elevation is less than . In the Miami area, where the vast majority of the land is below , even a one-foot increase over the average high tide can cause widespread flooding. The 2015 and 2016 king tide event levels reached about MLLW, above mean sea level, or about NAVD88, and nearly the same above MHHW. While the tide range is very small in Miami, averaging about , with the greatest range being less than , the area is very acute to minute differences down to single inches due to the vast area at low elevation. NOAA tide gauge data for most stations shows current water level graphs relative to a fixed datum, as well as mean sea level trends for some stations. During the king tides, the local Miami area tide gauge at Virginia Key shows levels running at times or more over datum.\n\nFort Lauderdale has installed over one hundred tidal valves since 2013 to combat flooding. Fort Lauderdale is nicknamed the \"Venice of America\" due to its roughly of canals.\n\nTidal flooding is capable of majorly inhibiting natural gravity-based drainage systems in low-lying areas when it reaches levels that are below visible inundation of the surface, but which are high enough to incapacitate the lower drainage or sewer system. Thus, even normal rainfall or storm surge events can cause greatly amplified flooding effects. One passive solution to intrusion through drainage systems are one way back-flow valves in drainage ways. However, while this may prevent a majority of the tidal intrusion, it also inhibits drainage during exceptionally high tides that shut the valves. In Miami Beach, where resilience work is underway, the pump systems replace insufficient gravity-based systems.\n\nA recent University of Florida study correlated the increased tidal flooding in south Florida, at least from 2011-2015 to episodic atmospheric conditions. The rate was about 3/4 of an inch (19 mm) per year, versus the global rate of just over a tenth of an inch (3 mm) per year.\n\nDue to changing geography such as subsidence, and poorly planned development, tidal flooding may exist separate from modern nuisance flooding associated with sea level rise and anthropocentric climate change. The widely publicized Holland Island in Maryland for example has disappeared over the years mainly due to subsidence and coastal erosion. In the New Orleans area on the Gulf Coast of Louisiana, land subsidence results in the Grand Isle tide gauge showing an extreme upward sea level trend.\n\nIn March 2018, South Florida experienced significant coastal erosion and flooding outside of the usual king tide or hurricane season during a Nor'easter event.\n\n"}
{"id": "3087602", "url": "https://en.wikipedia.org/wiki?curid=3087602", "title": "Topological order", "text": "Topological order\n\nIn physics, topological order is a kind of order in the zero-temperature phase of matter (also known as quantum matter). Macroscopically, topological order is defined and described by robust ground state degeneracy and quantized non-Abelian geometric phases of degenerate ground states. Microscopically,\ntopological order corresponds to patterns of long-range quantum entanglement. States with different topological orders (or different patterns of long range entanglements) cannot change into each other without a phase transition.\n\nTopologically ordered states have some interesting properties, such as (1) topological degeneracy and fractional statistics or non-abelian statistics that can be used to realize topological quantum computer; (2) perfect conducting edge states that may have important device applications; (3) emergent gauge field and Fermi statistics that suggest a quantum information origin of elementary particles; (4) topological entanglement entropy that reveals the entanglement origin of topological order, etc. \nTopological order is important in the study of several physical systems such as spin liquids \nand the quantum Hall effect,\nalong with potential applications to fault-tolerant quantum computation.\n\nTopological insulators \nand topological superconductors (beyond 1D) do not have topological order as defined above, their entanglements being only short-ranged.\n\nAlthough all matter is formed by atoms, matter can have different properties and appear in different forms, such as solid, liquid, superfluid, magnet, etc. These various forms of matter are often called states of matter or phases. According to condensed matter physics and the principle of emergence, the different properties of materials originate from the different ways in which the atoms are organized in the materials. Those different organizations of the atoms (or other particles) are formally called the orders in the materials.\n\nAtoms can organize in many ways which lead to many different orders and many different types of materials. Landau symmetry-breaking theory provides a general understanding of these different orders. It points out that different orders really correspond to different symmetries in the organizations of the constituent atoms. As a material changes from one order to another order (i.e., as the material undergoes a phase transition), what happens is that the symmetry of the organization of the atoms changes.\n\nFor example, atoms have a random distribution in a liquid, so a liquid remains the same as we displace atoms by an arbitrary distance. We say that a liquid has a \"continuous translation symmetry\". After a phase transition, a liquid can turn into a crystal. In a crystal, atoms organize into a regular array (a lattice). A lattice remains unchanged only when we displace it by a particular distance (integer times a lattice constant), so a crystal has only \"discrete translation symmetry\". The phase transition between a liquid and a crystal is a transition that reduces the continuous translation symmetry of the liquid to the discrete symmetry of the crystal. Such a change in symmetry is called \"symmetry breaking\". The essence of the difference between liquids and crystals is therefore that the organizations of atoms have different symmetries in the two phases.\n\nLandau symmetry-breaking theory has been a very successful theory. For a long time, physicists believed that Landau Theory described all possible orders in materials, and all possible (continuous) phase transitions.\n\nHowever, since the late 1980s, it has become gradually apparent that Landau symmetry-breaking theory may not describe all possible orders. In an attempt to explain high temperature superconductivity the chiral spin state was introduced. At first, physicists still wanted to use Landau symmetry-breaking theory to describe the chiral spin state. They identified the chiral spin state as a state that breaks the time reversal and parity symmetries, but not the spin rotation symmetry. This should be the end of the story according to Landau's symmetry breaking description of orders. However, it was quickly realized that there are many different chiral spin states that have exactly the same symmetry, so symmetry alone was not enough to characterize different chiral spin states. This means that the chiral spin states contain a new kind of order that is beyond the usual symmetry description. The proposed, new kind of order was named \"topological order\". (The name \"topological order\" is motivated by the low energy effective theory of the chiral spin states which is a topological quantum field theory (TQFT)). New quantum numbers, such as ground state degeneracy and the non-Abelian geometric phase of degenerate ground states, were introduced to characterize and define the different topological orders in chiral spin states. More recently, it was shown that topological orders can also be characterized by topological entropy.\n\nBut experiments soon indicated that chiral spin states do not describe high-temperature superconductors, and the theory of topological order became a theory with no experimental realization. However, the similarity between chiral spin states and quantum Hall states allows one to use the theory of topological order to describe different quantum Hall states. Just like chiral spin states, different quantum Hall states all have the same symmetry and are outside the Landau symmetry-breaking description. One finds that the different orders in different quantum Hall states can indeed be described by topological orders, so the topological order does have experimental realizations.\n\nThe fractional quantum Hall (FQH) state was discovered in 1982 before the introduction of the concept of topological order in 1989. But the FQH state is not the first experimentally discovered topologically ordered state. The superconductor, discovered in 1911, is the first experimentally discovered topologically ordered state; it has \"Z\" topological order.\n\nAlthough topologically ordered states usually appear in strongly interacting boson/fermion systems, a simple kind of topological order can also appear in free fermion systems. This kind of topological order corresponds to integral quantum Hall state, which can be characterized by the Chern number of the filled energy band if we consider integer quantum Hall state on a lattice. Theoretical calculations have proposed that such Chern numbers can be measured for a free fermion system experimentally.\n\nIt is also well known that such a Chern number can be measured (maybe indirectly) by edge states.\n\nA large class of 2+1D topological orders is realized through a mechanism called string-net condensation. This class of topological orders can have a gapped edge and are classified by unitary fusion category (or monoidal category) theory. One finds that string-net condensation can generate infinitely many different types of topological orders, which may indicate that there are many different new types of materials remaining to be discovered.\n\nThe collective motions of condensed strings give rise to excitations above the string-net condensed states. Those excitations turn out to be gauge bosons. The ends of strings are defects which correspond to another type of excitations. Those excitations are the gauge charges and can carry Fermi or fractional statistics.\n\nThe condensations of other extended objects such as \"membranes\", \"brane-nets\", and fractals also lead to topologically ordered phases and \"quantum glassiness\".\n\n\nWe know that group theory is the mathematical foundation of symmetry-breaking orders. What is the mathematical foundation of topological order? \nIt was found that a subclass of 2+1D topological orders—Abelian topological orders—can be classified by a K-matrix approach. The string-net condensation suggests that tensor category (such as fusion category or monoidal category) is part of the mathematical foundation of topological order in 2+1D. The more recent researches suggest that\n(up to invertible topological orders that have no fractionalized excitations):\n\n\nTopological order in higher dimensions may be related to n-Category theory. Quantum operator algebra is a very important mathematical tool in studying topological orders.\n\nSome also suggest that topological order is mathematically described by \"extended quantum symmetry\".\n\nThe materials described by Landau symmetry-breaking theory have had a substantial impact on technology. For example, ferromagnetic materials that break spin rotation symmetry can be used as the media of digital information storage. A hard drive made of ferromagnetic materials can store gigabytes of information. Liquid crystals that break the rotational symmetry of molecules find wide application in display technology. Crystals that break translation symmetry lead to well defined electronic bands which in turn allow us to make semiconducting devices such as transistors. Different types of topological orders are even richer than different types of symmetry-breaking orders. This suggests their potential for exciting, novel applications.\n\nOne theorized application would be to use topologically ordered states as media for quantum computing in a technique known as topological quantum computing. A topologically ordered state is a state with complicated non-local quantum entanglement. The non-locality means that the quantum entanglement in a topologically ordered state is distributed among many different particles. As a result, the pattern of quantum entanglements cannot be destroyed by local perturbations. This significantly reduces the effect of decoherence. This suggests that if we use different quantum entanglements in a topologically ordered state to encode quantum information, the information may last much longer. The quantum information encoded by the topological quantum entanglements can also be manipulated by dragging the topological defects around each other. This process may provide a physical apparatus for performing quantum computations. Therefore, topologically ordered states may provide natural media for both quantum memory and quantum computation. Such realizations of quantum memory and quantum computation may potentially be made fault tolerant.\n\nTopologically ordered states in general have a special property that they contain non-trivial boundary states. In many cases, those boundary states become perfect conducting channel that can conduct electricity without generating heat. This can be another potential application of topological order in electronic devices.\nSimilarly to topological order, topological insulators also have gapless boundary states. The boundary states of topological insulators play a key role in the detection and the application of topological insulators.\nThis observation naturally leads to a question:\nare topological insulators examples of topologically ordered states?\nIn fact topological insulators are different from topologically ordered states defined in this article.\nTopological insulators only have short-ranged entanglements and have no topological order, while the topological order defined in this article is a pattern of long-range entanglement. Topological order is robust against any perturbations. It has emergent gauge theory, emergent fractional charge and fractional statistics. In contrast, topological insulators are robust only against perturbations that respect time-reversal and U(1) symmetries. Their quasi-particle excitations have no fractional charge and fractional statistics. Strictly speaking, topological insulator is an example of SPT order, where the first example of SPT order is the Haldane phase of spin-1 chain.\n\nLandau symmetry-breaking theory is a cornerstone of condensed matter physics. It is used to define the territory of condensed matter research. The existence of topological order appears to indicate that nature is much richer than Landau symmetry-breaking theory has so far indicated. So topological order opens up a new direction in condensed matter physics—a new direction of highly entangled quantum matter.\nWe realize that quantum phases of matter (i.e. the zero-temperature phases of matter) can be divided into two classes:\nlong range entangled states and\nshort range entangled states.\nTopological order is the notion that describes the long range entangled states: topological order = pattern of\nlong range entanglements. Short range entangled states are trivial in the sense that they all belong to one phase.\nHowever, in the presence of symmetry, even short range entangled states are nontrivial and can belong to different phases.\nThose phases are said to contain SPT order. SPT order generalizes the notion\nof topological insulator to interacting systems.\n\nSome suggest that topological order (or more precisely, string-net condensation) in local bosonic (spin) models have the potential to provide a unified origin for photons, electrons and other elementary particles in our universe.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "51275269", "url": "https://en.wikipedia.org/wiki?curid=51275269", "title": "Toy forts and castles", "text": "Toy forts and castles\n\nA toy fort is a miniature fortress or castle that is used as a setting to stage battles using toy soldiers. Toy Forts come in many shapes and sizes; some are copies of existing historical structures, while others are imagined with specific elements to enable realistic play, such as moats, drawbridges, and battlements. Toy fort designs range from the châteaux of Europe to the stockade forts of the American wild west.\n\nToy forts and castles first appeared at the beginning of the nineteenth century in Germany, a country that dominated the world of toy manufacturing up until WW1. The earliest examples came as a set of generic wooden blocks which could be configured in many different ways. As time went on some of these sets were designed to portray specific structures associated with real battles of the time.\n\nAround 1850 dollhouse manufacturers started to apply their production methods and capabilities, to producing toy forts and castles. Sets would consist of wooden components, some blocks and some flat, painted to depict details such as stone, brick, windows, arches and vegetation. The parts would be shipped in a box which was designed to be inverted and then used as the base for the toy fort. This proved to be the standard design for toy forts and castles for the next 100 years.\n\nThe Germans dominated the toy fort market until about 1900 when other manufacturers from France, Denmark, Britain, and the USA started to appear on the scene. As the technology progressed new materials were used in the manufacture of toy forts including tin, zinc alloy, composition, cardboard, hardboard, MDF, and finally plastics.\n\nThe three best-known manufacturers of toy forts were Moritz Gottschalk (Germany), O. and M. Hausser (Germany), and Lines Bros. (Great Britain).\n\n\n\n\n\n"}
{"id": "1533268", "url": "https://en.wikipedia.org/wiki?curid=1533268", "title": "Waste hierarchy", "text": "Waste hierarchy\n\nWaste hierarchy is a tool used in the evaluation of processes that protect the environment alongside resource and energy consumption to most favourable to least favourable actions. The hierarchy establishes preferred program priorities based on sustainability. To be sustainable, waste management cannot be solved only with technical end-of-pipe solutions and an integrated approach is necessary.\n\nThe waste management hierarchy indicates an order of preference for action to reduce and manage waste, and is usually presented diagrammatically in the form of a pyramid. The hierarchy captures the progression of a material or product through successive stages of waste management, and represents the latter part of the life-cycle for each product.\n\nThe aim of the waste hierarchy is to extract the maximum practical benefits from products and to generate the minimum amount of waste. The proper application of the waste hierarchy can have several benefits. It can help prevent emissions of greenhouse gases, reduces pollutants, save energy, conserves resources, create jobs and stimulate the development of green technologies.\n\nAll products and services have environmental impacts, from the extraction of raw materials for production to manufacture, distribution, use and disposal. Following the waste hierarchy will generally lead to the most resource-efficient and environmentally sound choice but in some cases refining decisions within the hierarchy or departing from it can lead to better environmental outcomes.\n\nLife cycle thinking and assessment can be used to support decision-making in the area of waste management and to identify the best environmental options. It can help policy makers understand the benefits and trade-offs they have to face when making decisions on waste management strategies. Life-cycle assessment provides an approach to ensure that the best outcome for the environment can be identified and put in place. It involves looking at all stages of a product’s life to find where improvements can be made to reduce environmental impacts and improve the use or reuse of resources. A key goal is to avoid actions that shift negative impacts from one stage to another. Life cycle thinking can be applied to the five stages of the waste management hierarchy.\n\nFor example, life-cycle analysis has shown that it is often better for the environment to replace an old washing machine, despite the waste generated, than to continue to use an older machine which is less energy-efficient. This is because a washing machine’s greatest environmental impact is during its use phase. Buying an energy-efficient machine and using low- temperature detergent reduce environmental impacts.\n\nThe European Union Waste Framework Directive has introduced the concept of life-cycle thinking into waste policies. This duality approach gives a broader view of all environmental aspects and ensures any action has an overall benefit compared to other options. The actions to deal with waste along the hierarchy should be compatible with other environmental initiatives.\n\nIn 1975, The European Union’s Waste Framework Directive (1975/442/EEC) introduced for the first time the waste hierarchy concept into European waste policy. It emphasized the importance of waste minimization, and the protection of the environment and human health, as a priority. Following the 1975 Directive, European Union policy and legislation adapted to the principles of the waste hierarchy.\n\nIn 1989, it was formalized into a hierarchy of management options in the European Commission's Community Strategy for Waste Management and this waste strategy was further endorsed in the Commission's review in 1996.\n\nIn 2008, the European Union parliament introduced a new \"five-step waste hierarchy\" to its waste legislation, Directive 2008/98/EC, which member states must introduce into national waste management laws. Article 4 of the directive lays down a five-step hierarchy of waste management options which must be applied by Member States in this priority order. Waste prevention, as the preferred option, is followed by reuse, recycling, recovery including energy recovery and as a last option, safe disposal. Among engineers, a similar hierarchy of waste management has been known as ARRE strategy: avoid, reduce, recycle, eliminate.\n\nThe task of implementing the waste hierarchy in waste management practices within a country may be delegated to the different levels of government (national, regional, local) and to other possible factors including industry, private companies and households. Local and regional authorities can be particularly challenged by the following issues when applying the waste hierarchy approach. \n\nSource reduction involves efforts to reduce hazardous waste and other materials by modifying industrial production. Source reduction methods involve changes in manufacturing technology, raw material inputs, and product formulation. At times, the term \"pollution prevention\" may refer to source reduction.\n\nAnother method of source reduction is to increase incentives for recycling. Many communities in the United States are implementing variable-rate pricing for waste disposal (also known as Pay As You Throw - PAYT) which has been effective in reducing the size of the municipal waste stream.\n\nSource reduction is typically measured by efficiencies and cutbacks in waste. Toxics use reduction is a more controversial approach to source reduction that targets and measures reductions in the upstream use of toxic materials. Toxics use reduction emphasizes the more preventive aspects of source reduction but, due to its emphasis on toxic chemical inputs, has been opposed more vigorously by chemical manufacturers. Toxics use reduction programs have been set up by legislation in some states, e.g., Massachusetts, New Jersey, and Oregon. The 3 R's represent the 'Waste Hierarchy' which lists the best ways of managing waste from the most to the least desirable. Many of the things we currently throw away could be reused again with just a little thought and imagination.\n\n\n"}
{"id": "22639515", "url": "https://en.wikipedia.org/wiki?curid=22639515", "title": "Wetting transition", "text": "Wetting transition\n\nA wetting transition (Cassie–Wenzel transition) may occur during the process of wetting of a solid (or liquid) surface with a liquid. The transition corresponds to a certain change in contact angle, the macroscopic parameter characterizing wetting. Various contact angles can co-exist on the same solid substrate. Wetting transitions may occur in a different way depending on whether the surface is flat or rough.\n\nWhen a liquid drop is put onto a flat surface, two situations may result. If the contact angle is zero, the situation is referred to as complete wetting. If the contact angle is between 0 and 180°, the situation is called partial wetting. A wetting transition is a surface phase transition from partial wetting to complete wetting.\n\nThe situation on rough surfaces is much more complicated. The main characteristic of the wetting properties of rough surfaces is the so-called apparent contact angle (APCA). It is well known that the APCA usually measured are different from those predicted by the Young equation. Two main hypotheses were proposed in order to explain this discrepancy, namely the Wenzel and Cassie wetting models. According to the traditional Cassie model, air can remain trapped below the drop, forming \"air pockets\". Thus, the hydrophobicity of the surface is strengthened because the drop sits partially on air. On the other hand, according to the Wenzel model the roughness increases the area of a solid surface, which also geometrically modifies the wetting properties of this surface. Transition from Cassie to Wenzel regime is also called wetting transition. Under certain external stimuli, such as pressure or vibration, the Cassie air trapping wetting state could be converted into the Wenzel state. It is well accepted that the Cassie air trapping wetting regime corresponds to a higher energetic state, and the Cassie–Wenzel transition is irreversible. However, the mechanism of the transition remains unclear. It was suggested that the Cassie–Wenzel transition occurs via a nucleation mechanism starting from the drop center. On the other hand, recent experiments showed that the Cassie–Wenzel transition is more likely to be due to the displacement of a triple line under an external stimulus. The existence of so-called impregnating Cassie wetting state also has to be considered. Understanding wetting transitions is of a primary importance for design of superhydrophobic surfaces.\n\n"}
{"id": "187927", "url": "https://en.wikipedia.org/wiki?curid=187927", "title": "Xootr", "text": "Xootr\n\nXootr (pronounced \"zoo-ter\") is a manufacturer of folding kick scooters and was formerly a seller of folding bicycles. Xootr scooters are characterized by 180 mm polyurethane wheels with aluminium hubs, and a hand brake for the front wheel (for the rear on old models). Compared to the other folding scooters with smaller wheels, these larger wheels provide an easier ride, at the expense of a somewhat less easy portability. The hinge is distinguished from competitors in being locked by inserting a pin, rather than by a cam or other latch. The platform is large enough for adults, and available in plywood, magnesium, or aluminum at different prices.\n\nXootr scooters were originally introduced in 1999 by Nova Cruz Products. Since 2003, the Xootr scooter has been manufactured and sold by Xootr LLC.\n\nThe roots of Xootr go back to 1999, when Karl Ulrich, Nathan Ulrich, and Tom Miner (along with partners Lunar Design and Cheskin Research, and several other investors) founded Nova Cruz Products. Nova Cruz designed, manufactured and sold the Xootr kick scooter, the Xootr eX3 electric scooter, and the Voloci electric motorbike.\n\nThe members of the Xootr LLC team have created products for many companies including Apple Computer, Hewlett-Packard, and Motorola. They now focus their energies on designing and manufacturing products they consider to be fun, socially, and environmentally friendly.\n\nThe Swift Folder is a folding bicycle, designed by Peter Reich of Design Mobility Inc. of Brooklyn, New York, in collaboration with Jan VanderTuin of the Center for Appropriate Transport in Eugene, Oregon. The Swift Folder folds more quickly, though less compactly, than many other folding bicycles. From 2004 until about the end of 2016, the bicycle was sold under license by Xootr as the Xootr Swift - fully assembled to a standard specification, in a choice of single or 8-speed models\n\nThe Xootr CrossRack Bicycle Rack enables the mounting of a standard bicycle bag to almost any bicycle especially on bicycles with small wheels.\n\nXootr markets a line of folding hand trucks\n\nXootr markets its own line of baseball caps and tee-shirts.\n\nXootr wheels were originally sold in two styles: standard (translucent yellow) and ultra (black), with the ultra wheels having 10% lower rolling resistance. The wheels were very high in quality for the price and were used as a stock wheel for a number of gravity racing competitions.\n\nTeams using Xootr wheels with a custom rubber formulation swept the top three places at the All-American Soap Box Derby Ultimate Speed championships in July 2008.\n\nThe Xootr was featured in the film Little Manhattan, where a Xootr Street serves as the primary means of transportation for the main character, Gabe. In one scene, one of his friends has a Xootr Ultra Cruz while another friend makes do with a Razor.\n\n"}
