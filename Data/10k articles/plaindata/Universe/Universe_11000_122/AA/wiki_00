{"id": "38525554", "url": "https://en.wikipedia.org/wiki?curid=38525554", "title": "5052 aluminium alloy", "text": "5052 aluminium alloy\n\n5052 is an aluminium alloy, primarily alloyed with magnesium and chromium.\n\nThe alloy composition of 5052 is:\n\nThe similar alloy A5652 do exist differing only in impurities limits.\n\nTypical applications include marine, aircraft, architecture, general sheet metal work, heat exchangers, fuel lines and tanks, flooring panels, streetlights, appliances, rivets and wire.\n\nThe exceptional corrosion resistance of 5052 alloy against seawater and salt spray makes it a primary candidate for the failure-sensitive large marine structures, like tanks of liquefied natural gas tankers.\n\nWeldability – Gas: Good\n\nWeldability – Arc: Very Good\n\nWeldability – Resistance: Very Good\n\nBrazability: Acceptable\n\nSolderability: Not recommended\n"}
{"id": "8704921", "url": "https://en.wikipedia.org/wiki?curid=8704921", "title": "Adam Air Flight 574", "text": "Adam Air Flight 574\n\nAdam Air Flight 574 (KI574 or DHI574) was a scheduled domestic passenger flight operated by Adam Air between the Indonesian cities of Surabaya and Manado that broke up in mid-air and crashed into the Makassar Strait near Polewali in Sulawesi on 1 January 2007. All 102 people on board died, making it the deadliest aviation accident involving a Boeing 737-400. A national investigation was launched into the disaster. The final report, released on 25 March 2008, concluded that the pilots lost control of the aircraft after they became preoccupied with troubleshooting the inertial navigation system and inadvertently disconnected the autopilot. It was the only fatal accident for Adam Air in the company's 5-year history.\n\nThe crash is one of several transportation accidents, including the subsequent non-fatal crash of Flight 172, which have resulted in the United States downgrading its safety rating of Indonesian aviation, as well as subsequent and large-scale transportation safety reforms in Indonesia. All Indonesian airlines, at one time, had been banned from flying into the European Union several months after the crash. Adam Air was later subsequently banned from flying by the Indonesian government a few years later, and declared bankruptcy.\n\nThe aircraft involved, Boeing 737-4Q8 serial number 24070, registration PK-KKW, was manufactured in 1989. Prior to service with Adam Air, owned by ILFC the aircraft had been leased to seven airlines: Dan-Air, British Airways, GB Airways, Transaero, WFBN, Air One and Jat Airways. Provided with twin CFM56-3C1 powerplants, the plane had around 50,000 hours flying and was last evaluated and declared airworthy by the Indonesian transport ministry on 25 December 2005. It was due to be checked again in late January 2007. The Surabaya airport duty manager said that there were no technical problems with the aircraft prior to departure.\n\nThe pilot-in-command was 47-year-old Captain Refri Agustian Widodo from Sidoarjo, Indonesia, who joined Adam Air in 2006. The first officer was 36-year-old Yoga Susanto, an employee of Adam Air who also joined the company in 2006. Captain Widodo was a seasoned veteran, having logged more than 13,300 hours of flying time. As pilot-in-command of Boeing 737 aircraft, he had more than 3,800 hours of experience. First Officer Susanto had 4,200 total flying hours and he had almost 1,000 hours logged as a Boeing 737 first officer.\n\nOn 1 January 2007, at 12:59 local time (05:59 UTC), the plane departed from Juanda Airport, Surabaya, with 96 passengers (85 adults, 7 children and 4 infants) which represented a 56 percent load factor. and 6 crew on board. The passenger list was composed mainly of Indonesian nationals; the only foreigners were an American family of three. The two-hour flight, scheduled to arrive at Sam Ratulangi Airport, Manado, at 16:00 local time, was as expected until the plane disappeared from air traffic control radar screens at Makassar, South Sulawesi, with the last contact at 14:58 local time (06:58 UTC). The last known beacon position was detected by a Singaporean satellite. The altitude of the plane was shown as on the radar screen.\n\nWhile cruising at 35,000 feet (10,668 m), the pilots became preoccupied with troubleshooting the aircraft's two inertial reference systems (IRS), part of the navigation system. As they were correcting the problem, they accidentally disengaged the autopilot and failed to correct for a slow right roll even after a \"bank angle\" alarm sounded. By the time the pilots noticed the situation, the bank angle had reached 100° with almost 60° nose down attitude. Contrary to the correct recovery procedure, the pilots did not level the wings before trying to regain pitch control. The aircraft reached at the end of the recording, in excess of the aircraft's maximum rated speed for a dive (). The descent rate varied during the fatal dive, with a maximum recorded value of 53,760 feet per minute. The tailplane suffered a structural failure twenty seconds prior to the end of the recording, at which time the investigators concluded the aircraft was in a \"critically unrecoverable state\". Both flight recorders ceased to function when the 737 broke up in mid-air at 9,000 feet above sea level.\n\nWeather in the region was stormy; the Indonesian Bureau of Meteorology and Geophysics noted that the cloud thickness was up to in height and wind speed at an average of in the area. Although the Juanda Airport operator, PT Angkasa Pura I, had given warnings to the pilot concerning the weather condition, the plane had departed as scheduled. The plane ran into crosswinds of more than over the Makassar Strait, west of Sulawesi, where it changed course eastward toward land before losing contact.\n\nNo distress signals were sent by the aircraft.\n\n3,600 army and police personnel were mobilised in the search for the missing aircraft. One Boeing 737-200 Surveiller (a military surveillance plane), two infrared-equipped Fokker-50 aircraft from the Republic of Singapore Air Force, a Navy Nomad plane and six helicopters were dispatched to aid searching for the missing plane from the air. The Indonesian sonar-equipped ship \"Pulau Rengat\", capable of detecting underwater metallic objects later joined the team, equipped with a mini remote-controlled submarine. It searched the sea for five days between 3 and 8 January, without success.\n\nNaval ships combed the Makassar Strait while military personnel went through jungles and mountains of Sulawesi. In the face of heavy rain and strong winds in the area, the search efforts, coordinated from Makassar city, were focused in the area between the coastal town of Majene and the mountainous region of Toraja. The search in the two areas was due to twin signals, each carrying different emergency locator transmitter frequencies, received by the Singaporean satellite and an Indonesian military air base. The two separate locations produced on radar screens were a spot on the sea in Majene and on land in Rantepao, Tana Toraja. Searches were then expanded throughout the Island of Sulawesi; some were triggered by unknown distress signals received by a commercial Lion Air flight and an airport. A police officer at the Barru district police operational centre said that all the districts with stretches of coastline along the Makasser Strait had teams searching for the plane.\n\nThe head of the National Search and Rescue Agency told the Associated Press that he believed the aircraft was probably lost at sea. From 5 January 2007, the main focus of the search was relocated to areas south of Manado, after Manado's Sam Ratulangi Airport reported detecting a signal from the plane a day before. However, the rugged terrain coupled with thick and low hanging clouds continued to hamper the search efforts, and three relatives of missing passengers who overflew part of the area on a military reconnaissance plane admitted that the chances of finding the plane were slim. Officials said that it was unlikely any bodies had survived in one piece. On 14 January, at Indonesia's request, Singapore lent four towed underwater locator beacon detectors, sometimes called Towed Pinger Locators, and six consultants in their use to aid in the search. One week later, one of these Towed Pinger Locators, operated from the \"USNS Mary Sears\" successfully located the black boxes. On 24 January, the British ship MN \"Endeavour\" joined the search for wreckage. The ship is operated by local mining firm PT Gema Tera Mustikawati and is usually used by oil and gas drilling companies to map the seabed. By then, the Indonesian government had spent an average of Rp 1 billion (about US$110,000) a day on the search.\n\nOn 10 February, search operations were officially halted by the Search and Rescue Agency, according to Transportation Minister Hatta Rajasa, finalising the legal status of both the plane and its passengers and crew. This announcement allowed the families of the victims to start the insurance claims process.\n\nOn Monday, 8 January, three large metal objects, suspected to be wreckage, were detected by the Indonesian ship KRI \"Fatahillah\"'s sonar. First Admiral Gatot Subyanto of the Indonesian Navy indicated three locations, between apart, off Mamuju city on Sulawesi's western coast. Due to limitations of the navy's sonar equipment, it was not clear what the metal was, and Indonesia had no other equipment of its own. A United States Navy oceanographic survey ship, \"Mary Sears\", arrived in the area on 9 January with better equipment to help identify the objects, and on the same date a Canadian jet with five separate air crews, working in shifts, was sent to aid with aerial mapping of the suspected location. The Indonesian Marine and Fishery Department has since suggested that the metal objects could instead have been instruments deployed to study the underwater sea current. A total of twelve Indonesian Navy ships were deployed in the area, including the KRI \"Ajak\", KRI \"Leuser\" and KRI \"Nala\". Extra underwater equipment, including a metal detector and an undersea camera, was sent from the United States, and arrived aboard the USNS \"Mary Sears\" on 17 January. The flight recorders were subsequently located elsewhere, in the waters in an area known as Majene, and a wide, sweeping search of the area revealed high amounts of scattered debris there, too. This debris was analysed to confirm it belonged to the 737.\n\nThe aircraft's right horizontal stabiliser was found by a fisherman, south of Pare Pare, about off the beach on 11 January, although it was not originally handed in, as its discoverer thought it to be a piece of plywood, only later realising it was a piece of the tail. This was confirmed by the part number on the stabiliser, 65C25746-76, which matched that of components on the missing 737. The fisherman received a reward of 50 million rupiah (equivalent to about $5,500) for his discovery. Later, other parts of the aircraft, including passenger seats, life jackets, a food tray, part of an aircraft tire, eight pieces of aluminium and fibre, an ID card, a flare and a headrest were also recovered from the area. By 13 January, a piece of a wing was also recovered. It is unclear whether the long section was a section of the right wing or the left wing, although it was examined in an attempt to discover this. The total count of recovered objects associated with aircraft, as of 29 January, was 206, of which 194 were definitely from the 737. Pieces of clothing thought to belong to passengers were also recovered, and on 15 January, pieces of human hair and what is thought to be human scalp were recovered from a headrest that had been pulled from the sea. They were DNA tested to attempt to identify them; the results of this test are, however, unknown.\n\nOn 21 January 2007, the flight data recorder (FDR) and cockpit voice recorder (CVR), popularly known as black boxes, were located 42 nautical miles off the coast of West Sulawesi by the US vessel \"Mary Sears\". The flight data recorder was located at at a depth of , while the cockpit voice recorder was located at at a depth of , approximately apart. The Indonesian vessel \"Fatahillah\" travelled to the location. The \"Mary Sears\" used its side scan sonar (SSS) unit to map an area of approximately 10.3 km² (3 sq nmi) around the recorders in high resolution, an operation which required 18 passes across the area at approximately , taking six hours per pass including lining up for the next pass. It discovered a large amount of wreckage in the area, which was considered to be all that remained of the aircraft. A senior Indonesian marine official said on 24 January that he did not believe that the equipment required to retrieve the boxes from that depth was available in any Asian country. The black boxes had a battery life of just 30 days, and would subsequently be unable to emit locator signals.\n\nOn 3 February, Indonesian naval vessel KRI \"Tanjung Dalpele\" took affected families out to the crash site where a memorial service was held, which included throwing flowers into the sea.\n\nOn 26 January 2007, a dispute arose between Adam Air and the Indonesian government regarding the retrieval of the black boxes. Due to the depth involved, recovery required an underwater remotely operated vehicle, but Indonesia did not have such equipment. Vice-President of Indonesia Jusuf Kalla went as far as to question the need to retrieve the black boxes at all, although experts said in response that the accident was of international significance as it could indicate a fault with the aircraft. Adam Air said that in its opinion, the black boxes should be recovered, describing the accident as being relevant on both national and international levels, but refused to pay, saying that was the responsibility of the government. Indonesia did request technical assistance from the United States, Japan and France. Jim Hall, a former chairman of the US National Transportation Safety Board, said that it was essential the boxes be recovered quickly, as at that point their 30-day battery life was about to expire, which subsequently did happen. He cited problems such as poor visibility and strong currents making it difficult to recover the devices without the signal.\n\nOn 31 January it was reported that the US had to withdraw the vessel \"Mary Sears\" from the searches, the US military saying that the vessel had other duties. Further funding and help from the US would have to be approved by the United States Congress. At the same time, external companies were suggested as possible retrievers of the black boxes. Indonesia continued to seek help from other countries, such as France and Japan. Indonesia's National Transportation Safety Committee head Setio Rahardjo maintained that Adam Air should be charged with the retrieval costs.\n\nIt was originally confirmed that Indonesia would not pay for the salvage operation, and neither could they force Adam Air to do so. Nonetheless, Adam Air signed a contract with Phoenix International for the recovery operation. On 23 August, the \"Eas\" arrived in Sulawesi's Makassar port to begin salvage operations, which began with several days survey. The vessel was carrying a mini-submarine that could dive up to , and was equipped with sonar and deep sea cameras.\n\nA Phoenix International underwater robot scouring the sea off Majene for on Sulawesi finally retrieved the flight data recorder on 27 August and cockpit voice recorder on 28 August. The two devices were found at a depth of around and were apart. They had been moved from their original locations by powerful underwater currents. The black boxes were sent to Washington, D.C. for analysis. The final cost of the salvage operation to retrieve the black boxes was US$3 million, of which two million was contributed by the Indonesian government, with Adam Air paying the rest. Efforts continued with the hope of recovering various large pieces of wreckage from the seabed.\n\nPresident Susilo Bambang Yudhoyono ordered a full investigation to discover the cause of the aircraft's disappearance, including the cause of any accident it may have had, before the main debris field had even been found. The investigation also looked at the airworthiness of the plane and standard procedure on aircraft operations. A team from the United States with representatives from the National Transportation Safety Board, the Federal Aviation Administration, Boeing and General Electric were sent to Indonesia to assist the Indonesian National Committee for Transportation in the investigation. A wider investigation into Indonesia's transport system as a whole was planned. Eyewitnesses reported seeing a low-flying, unstable aircraft in the area from which the wreckage has been recovered but lost sight of it after hearing a loud bang. The chief of the Indonesian Plane Technicians group, Wahyu Supriantono, said that the plane was unlikely to have suffered an in-flight break up or explosion, as the debris field would have been larger, and as a result, wreckage would have been discovered earlier. The flight recorders were recovered in August 2007, without which it would not have been possible to discover the cause of the accident. The National Transportation Safety Committee (NTSC, or KNKT as per its Indonesian name) described the near eight-month wait for the recovery of the flight recorders as \"unacceptable\".\n\nOn 25 March 2008, the inquiry ruled that pilot error and a faulty navigation device were to blame.\n\nThe NTSC determined that:\n\nInvestigators quickly became concerned about apparent poor maintenance and believed it might have been an important factor in the accident.\n\nAdam Air's safety record, like that of a number of other Indonesian airlines, has been heavily criticized. Adam Air has reportedly bullied pilots to fly planes they knew were unsafe. Pilots have reported repeated and deliberate breaches of international safety regulations, and aircraft being flown in non-airworthy states (including one aircraft flying with a damaged door handle and another with a damaged window) for months. Other incidents include pilots being ordered to fly aircraft even after exceeding the take-off limit of five times per pilot per day, using spare parts from other planes to keep planes in the air, and ignoring requests not to take off due to unsafe aircraft. According to the Associated Press, one ex-Adam Air pilot stated that \"every time you flew, you had to fight with the ground staff and the management about all the regulations you had to violate.\" Pilots also claimed that if they confronted their seniors, they were grounded or docked pay.\n\nFounder Adam Adhitya Suherman had personally denied the accusations, stating that maintenance had made up \"40 percent of our total operational costs\".\n\nInvestigators discovered that the aircraft was the subject of a large number of complaints by pilots (called \"write-ups\" in the aviation industry). The highest number of complaints concerned the captain's side vertical speed indicator, which informs the air crew of the rate (in feet per minute) at which the aircraft is ascending or descending. In all, 48 complaints were made regarding the instrument in the three months before the crash. The aircraft's inertial navigation system, which informs pilots what direction the aircraft is turning in, was complained about a total of thirty times. The \"International Herald Tribune\" reported that this may be of particular significance. The third most-complained about instrument was a fuel differential light, which received fifteen write-ups. Numerous complaints were also received about inoperative cockpit instrument lights, as well as multiple other malfunctions. Several complaints were made that the flaps, which modify drag and lift during take-off and landing, were jamming at twenty-five degrees upon landing, and there were two complaints that the weather radar was faulty.\n\nAdam Air is being sued by Indonesian consumer and labour groups over the accident, for a total of one trillion rupiahs (US$100 million), to be paid to the families of the victims. According to a lawyer for the families, speaking in a press conference along with the secretary for the Adam Air KI-574 Passengers' Families Association (formed in the aftermath of the disaster), 30 of the victims' families intend to sue Boeing instead of Adam Air over the accident. However, this does not necessarily mean that all of the others will sue Adam Air, as they may not necessarily exercise their right to sue at all. Representatives of the families have explained that they believe the plane was brought down by a faulty rudder control valve, similar to the accidents involving United Airlines Flight 585 and USAir Flight 427, which went down in the early 1990s, although there is no evidence that proves this. They have explained that, as a result, they are suing Boeing and Parker Hannifin, the valve's manufacturer, although airlines using the 737-300, -400, and -500 have been warned about problems with the rudder control valves.\n\nVice-President Jusuf Kalla described the disappearance as an \"international issue.\" A few days after the disappearance, President Susilo Bambang Yudhoyono set up the National Team for Transportation Safety and Security, partially as a response to the high number of recent transportation accidents in Indonesia, and partially as a direct response to the event. The team was tasked to evaluate thoroughly the transport safety procedures and review the existing regulations on transportation. It was not, however, to investigate accidents; the entity deemed responsible for this was the Komisi Nasional Keselamatan Transportasi (KNKT), or in English the National Transportation Safety Commission (NTSC), which is part of Departemen Perhubungan (Ministry of Transportation).\n\nAdam Air has been accused by multiple organisations of poor maintenance, and of ordering pilots to fly in all weather and regardless of aircraft conditions. Adam Adhitya Suherman, founder of the family-run airline, has personally denied these accusations, and has said that maintenance consumes \"up 40 percent of our total operational costs\". \nAdam Air has compensated the families of deceased passengers Rp 500 million (equivalent to about US$55,000 or €42,000) per passenger. It also compensated families of the flight crew.\n\nThere has been some calls from relatives of the dead for Adam Air to build a memorial to the victims in Makassar, South Sulawesi. Adam Air said that if an agreement could be reached, then they would fulfill the request.\n\nShortly after the crash, Adam Air changed the number of the regular Surabaya–Manado flight from Flight 574 to Flight 582. This accident also weakened Adam Air's image, which at the time was already negative among the public because of their frequent breakdowns and delays. The crash also exacerbated financial difficulties at the airline, which ceased operations a few years later.\n\nThe Indonesian government announced plans immediately after the accident to ban jets over ten years old for any commercial purpose. Previously the age limit was 35 years or 70,000 landings. Although this was in response to a large number of aircraft accidents, it was mainly in response to this accident and the Flight 172 incident. Indonesia also announced that the Transportation Ministry would be reshuffled in response to this accident, Flight 172 and the loss of the ferries MV \"Senopati Nusantara\" and \"Levina 1\". Among those replaced were the directors of air and sea transports and the chairman of the National Committee for Transportation Safety. Indonesia also introduced a new system of ranking airlines according to their safety record, with a level one ranking meaning the airline has no serious issues, a level two ranking meaning the airline must fix problems, and a level three ranking which may force the airline to shut down.\n\nIn March 2007, the Indonesian government announced that Adam Air was one of fifteen airlines that would have their licences revoked within three months unless they could improve their safety standards. The other airlines included Batavia Air, Jatayu Airlines, Kartika Airlines, Manunggal Air Services, Transwisata Prima Aviation and Tri-MG Intra Asia Airlines. These airlines were all targeted as a direct result of the crash, as they were in the third level of the ranking system introduced as a result. All Indonesian airlines, including state-owned Garuda Indonesia, were told they would need to make some improvements, with none of them receiving a level one ranking.\n\nIt was reported on 28 June 2007, that Adam Air would escape closure and had been upgraded one rank in safety rating, to the middle tier. The airlines that lost their licences were Jatayu Gelang Sejahtera, Aviasi Upataraksa, Alfa Trans Dirgantara, and Prodexim. Additionally, Germania Trisila Air, Atlas Delta Setia, Survey Udara Penas, Kura-Kura Aviation and Sabang Merauke Raya Air Charter were grounded pending improvements and facing potential licence revocation.\n\nOn 16 April 2007, the American Federal Aviation Administration responded to the results of the new airline survey by downgrading Indonesia's air safety oversight category from a 1 to a 2 because of \"serious concerns\" over safety. This means it views Indonesia's civil aviation authority as failing to oversee air carriers in accordance with minimum international standards. As a direct result, the US Embassy in Jakarta issued a warning to all American citizens flying in or out of Indonesia to avoid using Indonesian airlines, and instead use international carriers with better safety reputations. This was followed on 28 June 2007 by the addition of all Indonesia's airlines, none of which flew to Europe at the time, to the List of air carriers banned in the EU; the ban was lifted for flag carrier Garuda Indonesia and three smaller airlines in 2009, and by 2018 all Indonesian airlines were once again permitted to fly to the EU. Budhi Mulyawan Suyitno, Director-general of civil aviation at the Indonesian transport ministry, responded by saying that he felt Indonesia had made the improvements required by the EU. A blanket ban on all Indonesian airlines flying to the United States was imposed in 2007, and lifted in 2016.\n\nOn 18 March 2008, just a day after an accident in Batam, the airline's Air Operator's Certificate was suspended by the Indonesian government. Three months later in June of the same year, the certificate was revoked, and the airline filed for bankruptcy.\n\nOn 21 February 2007, just 51 days after the loss of Flight 574, Flight 172, an Adam Air Boeing 737-300 aircraft (registration PK-KKV) flying from Jakarta to Surabaya had a hard landing at Juanda International Airport. The incident caused the fuselage of the plane to crack and bend in the middle, with the tail of the plane drooping towards the ground. There were no reports of serious injuries from the incident. As a result, six Adam Air 737s were grounded awaiting safety checks. Adam Air described this as \"harsh punishment\" for an accident it blamed on poor weather conditions, but Vice-President Kalla had said that all Boeing 737-300s should be checked.\n\nIn early August 2008, a five-minute-38-second digital recording allegedly retrieved from the plane's cockpit voice recorder was widely circulated on the Internet and transcribed by the media. The recording, which had been publicly distributed through chain e-mails, begins with what is believed by some to be a conversation between pilot Refi Agustian Widodo and copilot Yoga Susanto before the crash. Approximately two minutes before the end of the recording, the autopilot disconnect horn sounded, followed approximately a minute later by \"bank angle\" warnings from the GPWS and the altitude alerter. Immediately thereafter, as the aircraft began its final dive, the shotgun-like sounds of engine compressor surges and the overspeed \"clacker\" could be heard along with two background voices screaming in terror, and shouting out the name of God. Towards the end of the recording, there is a dramatic increase in windshield noise and two loud bangs (the second larger than the first), consistent with structural failure of the aircraft, followed 20 seconds later by an abrupt silence. It is likely that, when the pilots regained visual ground contact, they quickly pulled up, overloading the horizontal stabiliser downwards and a main wing spar upwards. It was dismissed by officials who said that it was not authentic and was not the original recording.\n\nThe crash was featured in Season 7 of the documentary series \"Mayday\", on the episode \"Flight 574: Lost\".\n\nThere were 99 Indonesian and three American citizens on board the flight. Two of the passengers were Portugal-Indonesian nationality (dual nationality).\n\nAmong the passengers were:\n\n\n"}
{"id": "40387505", "url": "https://en.wikipedia.org/wiki?curid=40387505", "title": "Adam Johann von Krusenstern", "text": "Adam Johann von Krusenstern\n\nAdam Johann Ritter (later Freiherr) von Krusenstern (, tr. ; 10 October 177012 August 1846) was a Russian admiral and explorer, who led the first Russian circumnavigation of the globe.\n\nKrusenstern was born in Hagudi, Harrien, Governorate of Estonia, Russian Empire into a Baltic German family descended from the Swedish aristocratic family von Krusenstjerna, who remained in the province after the country was ceded to Russia. In 1787, he joined the Russian Imperial Navy, and served in the war against Sweden. Subsequently, he served in the Royal Navy between 1793 and 1799, visiting America, India and China.\n\nAfter publishing a paper pointing out the advantages of direct communication by sea between Russia and China by passing Cape Horn at the southern tip of South America and the Cape of Good Hope at the tip of South Africa, he was appointed by Tsar Alexander I to make a voyage to the Far East coast of Asia to endeavour to carry out the project. Under the patronage of Alexander, Count Nikolay Petrovich Rumyantsev and the Russian-American Company, Krusenstern led the first Russian circumnavigation of the world. The chief object of this undertaking was the development of the fur trade with Russian America (Alaska). Other goals of the two-ship expedition were to establish trade with China and Japan, facilitate trade in South America, and examine the coast of California in western North America for a possible colony.\n\nThe two ships, \"Nadezhda\" (Hope, formerly the British merchant \"Leander\") under the command of Krusenstern, and \"Neva\" (formerly the British merchant \"Thames\") under the command of Captain-Lieutenant Yuri F. Lisianski, set sail from Kronstadt in August 1803, rounded Cape Horn of South America, reached the northern Pacific Ocean, and returned via the Cape of Good Hope at South Africa. Krusenstern arrived back at Kronstadt in August 1806. Both seafarers made maps and detailed recordings of their voyages.\n\nUpon his return, Krusenstern wrote a detailed report, \"Reise um die Welt in den Jahren 1803, 1804, 1805 und 1806 auf Befehl Seiner Kaiserlichen Majestät Alexanders des Ersten auf den Schiffen Nadeschda und Newa\" (\"Journey around the World in the Years 1803, 1804, 1805, and 1806 at the Command of his Imperial Majesty Alexander I in the Ships Nadezhda and Neva\") published in Saint Petersburg in 1810. It was published in 1811–1812 in Berlin; this was followed by an English translation, published in London in 1813 and subsequently by French, Dutch, Danish, Swedish, and Italian translations. His scientific work, which includes an atlas of the Pacific, was published in 1827 in Saint Petersburg.\n\nThe geographical discoveries of Krusenstern made his voyage very important for the progress of geographical science. His work won him an honorary membership in the Russian Academy of Sciences. In 1816, he was elected a foreign member of the Royal Swedish Academy of Sciences.\n\nAs director of the Russian naval school Krusenstern did much useful work. He was also a member of the scientific committee of the marine department, and his contrivance for counteracting the influence of the iron in vessels on the compass was adopted in the navy. Krusenstern became an admiral in 1841 and he was awarded the Pour le Mérite (civil class) in 1842. He died in 1846 in Kiltsi manor, an Estonian manor he had purchased in 1816, and was buried in the Tallinn Cathedral.\n\nThe Russian training tall ship \"Kruzenshtern\" is named after him. To commemorate the 200th anniversary of Krusenstern's circumnavigation, the ship retraced his route around the globe in 2005–2006.\n\nAnother ship named after him is the Russian icebreaker \"Ivan Kruzenshtern\". Also, an Aeroflot Airbus 320 VP-BKC is named after him.\n\nThe crater Krusenstern on the Moon is named after him. There is Krusenstern Island in the Bering Strait, as well as a small group of islands in the Kara Sea, southwest of the Nordenskiöld Archipelago, called Krusenstern Islands. Cape Krusenstern in Northwest Alaska is the site of Cape Krusenstern National Monument (1978), one of the most important archaeological sites in the state.\n\nIn Russia (as well as in other Russophone places), a fictional steamship \"Admiral Ivan Fyodorovich Kruzenshtern\" from the popular \"Prostokvashino\" animated film series is well-known, often as part of a catchphrase \"Admiral I.F. Kruzenshtern, a man and a steamship\", \"pirated\" from the title of a requiem poem by Vladimir Mayakovsky, \"To Comrade Nette, a Man and a Steamship\". As a third-level linguistic derivation, there is a Russophone Israel klezmer-rock band, Kruzenshtern & Parohod (\"Krusenstern and Steamship\").\n\nAnother legacy is that the Cook Islands in the South Pacific bear that name thanks to von Krusenstern. Previously known as the Hervey (or Harvey) Islands (or Group), he changed their name in 1835 to honour Captain Cook. More accurately, he changed the name of those which comprised the Southern Group and it was subsequently applied to all 15 islands when the New Zealand Parliament passed \"The Cook Islands and other Islands Government Act\" in 1901. He recorded the new name in his \"Atlas de l'Océan Pacifique\" published at St. Petersburg between 1824 and 1835.\n\n\n"}
{"id": "4624852", "url": "https://en.wikipedia.org/wiki?curid=4624852", "title": "Autogenic succession", "text": "Autogenic succession\n\nIn ecology, autogenic succession is succession driven by the biotic components of an ecosystem. In contrast, allogenic succession is driven by the abiotic components of the ecosystem.\n\nThe plants themselves (biotic components) cause succession to occur.\n\nThese aspects lead to a gradual ecological change in a particular spot of land, known as a progression of inhabiting species. Autogenic succession can be viewed as a secondary succession because of pre-existing plant life.\n\n\n"}
{"id": "1867633", "url": "https://en.wikipedia.org/wiki?curid=1867633", "title": "Blast shelter", "text": "Blast shelter\n\nA blast shelter is a place where people can go to protect themselves from blasts and explosions, like those from bombs, or in hazardous worksites, such as on oil and gas refineries or petrochemical facilities. It differs from a fallout shelter, in that its main purpose is to protect from shock waves and overpressure instead of from radioactive precipitation, as a fallout shelter does. It is also possible for a shelter to protect from both blasts and fallout.\n\nBlast shelters are a vital form of protection from nuclear attack and are employed in civil defense. There are above-ground, below-ground, dedicated, dual-purpose, and potential blast shelters. Dedicated blast shelters are built specifically for the purpose of blast protection (see bunker). Dual-purpose blast shelters are existing structures with blast-protective properties that have been modified to accommodate people seeking protection from blasts. Potential blast shelters are existing structures or geological features exhibiting blast-protective properties that have potential to be used for protection from blasts.\n\nBlast shelters deflect the blast wave from nearby explosions to prevent ear and internal injuries to people sheltering in the bunker. While frame buildings collapse from as little as 3 psi (20 kPa) of overpressure, blast shelters are regularly constructed to survive several hundred psi. This substantially decreases the likelihood that a bomb can harm the structure.\n\nThe basic plan is to provide a structure that is very strong in compression. The actual strength specification must be done individually, based on the nature and probability of the threat. A typical specification for heavy civil defence shelter in Europe during the Cold war was an overhead explosion of a 500 kiloton weapon at the height of 500 meters. Such a weapon would be used to attack soft targets (factories, administrative centres, communications) in the area.\n\nOnly the most heavy bedrock-shelters would stand a chance of surviving. However, in the countryside or in a suburb, the likely distance to the explosion is much larger, as it is improbable that anyone would waste an expensive nuclear device on such targets. The most common purpose-built structure is a steel-reinforced concrete vault or arch buried or located in the basement of a house.\n\nMost expedient blast shelters are civil engineering structures that contain large buried tubes or pipes such as sewage or rapid transit tunnels. Even these, nonetheless, require several additions to serve properly: blast doors, air-filtration and ventilation equipment, secondary exits, and air-proofing.\n\nImprovised purpose-built blast shelters normally use earthen arches or vaults. To form these, a narrow (1-2 metre-wide) flexible tent of thin wood is placed in a deep trench (usually the apex of the tent is below grade), and then covered with cloth or plastic, and then covered with 1–2 meters of tamped earth. Shelters of this type are approved field expedient blast shelters of both the U.S. and China. Entrances are constructed from thick wooden frames. Blast valves are to be constructed from tire-treads laid on thick wooden grids.\n\nNuclear bunkers must also cope with the underpressure that lasts for several seconds after the shock wave passes, and prompt radiation. The overburden and structure provide substantial radiation shielding, and the negative pressure is usually only 1/3 of the overpressure.\n\nThe doors must be at least as strong as the walls. The usual design is a trap-door, to minimize the size and expense. In dual-purpose shelters, which have a secondary peace time use, the door may be normal. To reduce the weight, the door is normally constructed of steel, with a fitted steel lintel and frame welded to the steel-reinforcement of the concrete. The shelter should be located so that there is no combustible material directly outside it.\n\nIf the door is on the surface and will be exposed to the blast wave, the edge of the door is normally counter-sunk in the frame so that the blast wave or a reflection cannot lift the edge. If possible, this should be avoided, and the door built so that it is sheltered from the blast wave by other structures. The most useful construction is to build the door behind a 90°-turn in a corridor that has an exit for the overpressure.\n\nA bunker commonly has two doors, one of which is convenient, and in peace time use, and the other is strong. Naturally, the shelter must always have a secondary exit which can be used if the primary door is blocked by debris. Door shafts may double as ventilation shafts to reduce the digging, although this is inadvisable.\n\nA large ground shock can move the walls of a bunker several centimeters in a few milliseconds. Bunkers designed for large ground shocks must have sprung internal buildings, hammocks, or bean-bag chairs to protect inhabitants from the walls and floors. However, most civilian-built improvised shelters do not need these as their structure cannot stand a shock large enough to seriously damage the occupants.\n\nEarth is an excellent insulator. In bunkers inhabited for prolonged periods, large amounts of ventilation or air-conditioning must be provided to prevent heat prostration. In bunkers designed for war-time use, manually operated ventilators must be provided because supplies of electricity or gas are unreliable. The simplest form of effective fan to cool a shelter is a wide, heavy frame with flaps that swings in the shelter's doorway and can be swung from hinges on the ceiling.\n\nThe flaps open in one direction and close in the other, pumping air. (This is a Kearny Air Pump, or KAP, named after the inventor Cresson Kearny.) Kearny asserts, based on field testing, that air filtration is not normally needed in a nuclear shelter. He asserts that fallout is either large enough to fall to the ground, or so fine that it will not settle and thus has little bulk to emit radiation. However, if possible, shelters should have air-filtration to stop chemical, biological and nuclear impurities which may abound after an explosion.\n\nVentilation openings in a bunker must be protected by blast valves. A blast valve is closed by a shock wave, but otherwise remains open. If the bunker is in a built-up area, it may include water-cooling or an immersion tube and breathing tubes to protect inhabitants from fire storms. In these cases, the secondary exit is also most useful.\n\nBunkers must also protect the inhabitants from normal weather, including rain, summer heat and winter cold. A normal form of rainproofing is to place plastic film on the bunker's main structure before burying it. Thick (5-mil or 125 µm), inexpensive polyethylene film serves quite well, because the overburden protects it from degradation by wind and sunlight. Naturally, a buried or basement-situated reinforced-concrete shelter usually has the normal appearance of a building.\n\nWhen a house is purpose-built with a blast shelter, the normal location is a reinforced below-grade bathroom with large cabinets. In apartment houses, the shelter may double as storage space, as long as it can be swiftly emptied for its primary use. A shelter can easily be added in a new basement construction by taking an existing corner and adding two poured walls and a ceiling.\n\nSome vendors provide true blast shelters engineered to provide good protection to individual families at modest cost. One common design approach uses fiber-reinforced plastic shells. Compressive protection may be provided by inexpensive earth arching. The overburden is designed to shield from radiation. To prevent the shelter from floating to the surface in high groundwater, some designs have a skirt held-down with the overburden. A properly designed, properly installed home shelter does not become a sinkhole in the lawn. In Switzerland, which requires shelters for private apartment blocks and large private houses, the lightest shelters are constructed of stainless steel.\n\nDuring World War II, people in London and Moscow survived German aerial bombing by taking refuge in the underground railway stations, e.g., the London Underground. In the second half of the 20th century, metro stations in eastern Europe and the USSR were constructed to serve as blast shelters.\n\nStations of the Pyongyang Metro in North Korea, constructed below ground in the 1960s and 1970s, are designed as nuclear blast shelters and each station entrance has thick steel blast doors.\n\n\n\n"}
{"id": "4474", "url": "https://en.wikipedia.org/wiki?curid=4474", "title": "Bose–Einstein condensate", "text": "Bose–Einstein condensate\n\nA Bose–Einstein condensate (BEC) is a state of matter of a dilute gas of bosons cooled to temperatures very close to absolute zero (-273.15°C). Under such conditions, a large fraction of bosons occupy the lowest quantum state, at which point microscopic quantum phenomena, particularly wavefunction interference, become apparent macroscopically. A BEC is formed by cooling a gas of extremely low density, about one-hundred-thousandth the density of normal air, to ultra-low temperatures.\n\nThis state was first predicted, generally, in 1924–1925 by Satyendra Nath Bose and Albert Einstein.\n\nSatyendra Nath Bose first sent a paper to Einstein on the quantum statistics of light quanta (now called photons), in which he derived Planck's quantum radiation law without any reference to classical physics. Einstein was impressed, translated the paper himself from English to German and submitted it for Bose to the \"Zeitschrift für Physik\", which published it in 1924. (The Einstein manuscript, once believed to be lost, was found in a library at Leiden University in 2005.). Einstein then extended Bose's ideas to matter in two other papers. The result of their efforts is the concept of a Bose gas, governed by Bose–Einstein statistics, which describes the statistical distribution of identical particles with integer spin, now called bosons. Bosons, which include the photon as well as atoms such as helium-4 (He), are allowed to share a quantum state. Einstein proposed that cooling bosonic atoms to a very low temperature would cause them to fall (or \"condense\") into the lowest accessible quantum state, resulting in a new form of matter.\n\nIn 1938 Fritz London proposed BEC as a mechanism for superfluidity in He and superconductivity.\n\nOn June 5, 1995 the first gaseous condensate was produced by Eric Cornell and Carl Wieman at the University of Colorado at Boulder NIST–JILA lab, in a gas of rubidium atoms cooled to 170 nanokelvins (nK). Shortly thereafter, Wolfgang Ketterle at MIT demonstrated important BEC properties. For their achievements Cornell, Wieman, and Ketterle received the 2001 Nobel Prize in Physics.\n\nMany isotopes were soon condensed, then molecules, quasi-particles, and photons in 2010.\n\nThis transition to BEC occurs below a critical temperature, which for a uniform three-dimensional gas consisting of non-interacting particles with no apparent internal degrees of freedom is given by:\n\nwhere:\n\nInteractions shift the value and the corrections can be calculated by mean-field theory.\nThis formula is derived from finding the gas degeneracy in the Bose gas using Bose–Einstein statistics.\n\nConsider a collection of \"N\" non-interacting particles, which can each be in one of two quantum states, formula_2 and formula_3. If the two states are equal in energy, each different configuration is equally likely.\n\nIf we can tell which particle is which, there are formula_4 different configurations, since each particle can be in formula_2 or formula_3 independently. In almost all of the configurations, about half the particles are in formula_2 and the other half in formula_3. The balance is a statistical effect: the number of configurations is largest when the particles are divided equally.\n\nIf the particles are indistinguishable, however, there are only \"N\"+1 different configurations. If there are \"K\" particles in state formula_3, there are particles in state formula_2. Whether any particular particle is in state formula_2 or in state formula_3 cannot be determined, so each value of \"K\" determines a unique quantum state for the whole system.\n\nSuppose now that the energy of state formula_3 is slightly greater than the energy of state formula_2 by an amount \"E\". At temperature \"T\", a particle will have a lesser probability to be in state formula_3 by formula_16. In the distinguishable case, the particle distribution will be biased slightly towards state formula_2. But in the indistinguishable case, since there is no statistical pressure toward equal numbers, the most-likely outcome is that most of the particles will collapse into state formula_2.\n\nIn the distinguishable case, for large \"N\", the fraction in state formula_2 can be computed. It is the same as flipping a coin with probability proportional to \"p\" = exp(−\"E\"/\"T\") to land tails.\n\nIn the indistinguishable case, each value of \"K\" is a single state, which has its own separate Boltzmann probability. So the probability distribution is exponential:\n\nFor large \"N\", the normalization constant \"C\" is . The expected total number of particles not in the lowest energy state, in the limit that formula_21, is equal to formula_22. It does not grow when \"N\" is large; it just approaches a constant. This will be a negligible fraction of the total number of particles. So a collection of enough Bose particles in thermal equilibrium will mostly be in the ground state, with only a few in any excited state, no matter how small the energy difference.\n\nConsider now a gas of particles, which can be in different momentum states labeled formula_23. If the number of particles is less than the number of thermally accessible states, for high temperatures and low densities, the particles will all be in different states. In this limit, the gas is classical. As the density increases or the temperature decreases, the number of accessible states per particle becomes smaller, and at some point, more particles will be forced into a single state than the maximum allowed for that state by statistical weighting. From this point on, any extra particle added will go into the ground state.\n\nTo calculate the transition temperature at any density, integrate, over all momentum states, the expression for maximum number of excited particles, :\n\nWhen the integral is evaluated with factors of \"k\" and ℏ restored by dimensional analysis, it gives the critical temperature formula of the preceding section. Therefore, this integral defines the critical temperature and particle number corresponding to the conditions of negligible chemical potential. In Bose–Einstein statistics distribution, \"μ\" is actually still nonzero for BECs; however, \"μ\" is less than the ground state energy. Except when specifically talking about the ground state, μ can be approximated for most energy or momentum states as \"μ\" ≈ 0.\n\nNikolay Bogoliubov considered perturbations on the limit of dilute gas, finding a finite pressure at zero temperature and positive chemical potential. This leads to corrections for the ground state. The Bogoliubov state has pressure (\"T\" = 0): formula_26.\n\nThe original interacting system can be converted to a system of non-interacting particles with a dispersion law.\n\nIn some simplest cases, the state of condensed particles can be described with a nonlinear Schrödinger equation, also known as Gross–Pitaevskii or Ginzburg–Landau equation. The validity of this approach is actually limited to the case of ultracold temperatures, which fits well for the most alkali atoms experiments.\n\nThis approach originates from the assumption that the state of the BEC can be described by the unique wavefunction of the condensate formula_27. For a system of this nature, formula_28 is interpreted as the particle density, so the total number of atoms is formula_29\n\nProvided essentially all atoms are in the condensate (that is, have condensed to the ground state), and treating the bosons using mean field theory, the energy (E) associated with the state formula_27 is:\n\nMinimizing this energy with respect to infinitesimal variations in formula_27, and holding the number of atoms constant, yields the Gross–Pitaevski equation (GPE) (also a non-linear Schrödinger equation):\n\nwhere:\n\nIn the case of zero external potential, the dispersion law of interacting Bose–Einstein-condensed particles is given by so-called Bogoliubov spectrum (for formula_34):\n\nThe Gross-Pitaevskii equation (GPE) provides a relatively good description of the behavior of atomic BEC's. However, GPE does not take into account the temperature dependence of dynamical variables, and is therefore valid only for formula_34.\nIt is not applicable, for example, for the condensates of excitons, magnons and photons, where the critical temperature is comparable to room temperature.\n\nThe Gross-Pitaevskii equation is a partial differential equation in space and time variables. Usually it does not have analytic solution and\ndifferent numerical methods, such as split-step \nCrank-Nicolson\nand Fourier spectral\n\nand long-range dipolar interaction\nThe Gross–Pitaevskii model of BEC is a physical approximation valid for certain classes of BECs. By construction, the GPE uses the following simplifications: it assumes that interactions between condensate particles are of the contact two-body type and also neglects anomalous contributions to self-energy. These assumptions are suitable mostly for the dilute three-dimensional condensates. If one relaxes any of these assumptions, the equation for the condensate wavefunction acquires the terms containing higher-order powers of the wavefunction. Moreover, for some physical systems the amount of such terms turns out to be infinite, therefore, the equation becomes essentially non-polynomial. The examples where this could happen are the Bose–Fermi composite condensates, effectively lower-dimensional condensates, and dense condensates and superfluid clusters and droplets.\n\nHowever, it is clear that in a general case the behaviour of Bose–Einstein condensate can be described by coupled evolution equations for condensate density, superfluid velocity and distribution function of elementary excitations. This problem was in 1977 by Peletminskii et al. in microscopical approach. The Peletminskii equations are valid for any finite temperatures below the critical point. Years after, in 1985, Kirkpatrick and Dorfman obtained similar equations using another microscopical approach. The Peletminskii equations also reproduce Khalatnikov hydrodynamical equations for superfluid as a limiting case.\n\nThe phenomena of superfluidity of a Bose gas and superconductivity of a strongly-correlated Fermi gas (a gas of Cooper pairs) are tightly connected to Bose–Einstein condensation. Under corresponding conditions, below the temperature of phase transition, these phenomena were observed in helium-4 and different classes of superconductors. In this sense, the superconductivity is often called the superfluidity of Fermi gas. In the simplest form, the origin of superfluidity can be seen from the weakly interacting bosons model.\n\nIn 1938, Pyotr Kapitsa, John Allen and Don Misener discovered that helium-4 became a new kind of fluid, now known as a superfluid, at temperatures less than 2.17 K (the lambda point). Superfluid helium has many unusual properties, including zero viscosity (the ability to flow without dissipating energy) and the existence of quantized vortices. It was quickly believed that the superfluidity was due to partial Bose–Einstein condensation of the liquid. In fact, many properties of superfluid helium also appear in gaseous condensates created by Cornell, Wieman and Ketterle (see below). Superfluid helium-4 is a liquid rather than a gas, which means that the interactions between the atoms are relatively strong; the original theory of Bose–Einstein condensation must be heavily modified in order to describe it. Bose–Einstein condensation remains, however, fundamental to the superfluid properties of helium-4. Note that helium-3, a fermion, also enters a superfluid phase (at a much lower temperature) which can be explained by the formation of bosonic Cooper pairs of two atoms (see also fermionic condensate).\n\nThe first \"pure\" Bose–Einstein condensate was created by Eric Cornell, Carl Wieman, and co-workers at JILA on 5 June 1995. They cooled a dilute vapor of approximately two thousand rubidium-87 atoms to below 170 nK using a combination of laser cooling (a technique that won its inventors Steven Chu, Claude Cohen-Tannoudji, and William D. Phillips the 1997 Nobel Prize in Physics) and magnetic evaporative cooling. About four months later, an independent effort led by Wolfgang Ketterle at MIT condensed sodium-23. Ketterle's condensate had a hundred times more atoms, allowing important results such as the observation of quantum mechanical interference between two different condensates. Cornell, Wieman and Ketterle won the 2001 Nobel Prize in Physics for their achievements.\n\nA group led by Randall Hulet at Rice University announced a condensate of lithium atoms only one month following the JILA work. Lithium has attractive interactions, causing the condensate to be unstable and collapse for all but a few atoms. Hulet's team subsequently showed the condensate could be stabilized by confinement quantum pressure for up to about 1000 atoms. Various isotopes have since been condensed.\n\nIn the image accompanying this article, the velocity-distribution data indicates the formation of a Bose–Einstein condensate out of a gas of rubidium atoms. The false colors indicate the number of atoms at each velocity, with red being the fewest and white being the most. The areas appearing white and light blue are at the lowest velocities. The peak is not infinitely narrow because of the Heisenberg uncertainty principle: spatially confined atoms have a minimum width velocity distribution. This width is given by the curvature of the magnetic potential in the given direction. More tightly confined directions have bigger widths in the ballistic velocity distribution. This anisotropy of the peak on the right is a purely quantum-mechanical effect and does not exist in the thermal distribution on the left. This graph served as the cover design for the 1999 textbook \"Thermal Physics\" by Ralph Baierlein.\n\nBose–Einstein condensation also applies to quasiparticles in solids. Magnons, Excitons, and Polaritons have integer spin which means they are bosons that can form condensates.\n\nMagnons, electron spin waves, can be controlled by a magnetic field. Densities from the limit of a dilute gas to a strongly interacting Bose liquid are possible. Magnetic ordering is the analog of superfluidity. In 1999 condensation was demonstrated in antiferromagnetic TlCuCl, at temperatures as large as 14 K. The high transition temperature (relative to atomic gases) is due to the magnons small mass (near an electron) and greater achievable density. In 2006, condensation in a ferromagnetic yttrium-iron-garnet thin film was seen even at room temperature, with optical pumping.\n\nExcitons, electron-hole pairs, were predicted to condense at low temperature and high density by Boer et al. in 1961. Bilayer system experiments first demonstrated condensation in 2003, by Hall voltage disappearance. Fast optical exciton creation was used to form condensates in sub-kelvin CuO in 2005 on.\n\nPolariton condensation was firstly detected for exciton-polaritons in a quantum well microcavity kept at 5 K.\n\nAs in many other systems, vortices can exist in BECs. These can be created, for example, by 'stirring' the condensate with lasers, or rotating the confining trap. The vortex created will be a quantum vortex. These phenomena are allowed for by the non-linear formula_28 term in the GPE. As the vortices must have quantized angular momentum the wavefunction may have the form formula_38 where formula_39 and formula_40 are as in the cylindrical coordinate system, and formula_41 is the angular number. This is particularly likely for an axially symmetric (for instance, harmonic) confining potential, which is commonly used. The notion is easily generalized. To determine formula_42, the energy of formula_27 must be minimized, according to the constraint formula_38. This is usually done computationally, however in a uniform medium the analytic form:\n\ndemonstrates the correct behavior, and is a good approximation.\n\nA singly charged vortex (formula_46) is in the ground state, with its energy formula_47 given by\n\nwhere formula_49 is the farthest distance from the vortices considered.(To obtain an energy which is well defined it is necessary to include this boundary formula_50.)\n\nFor multiply charged vortices (formula_51) the energy is approximated by\n\nwhich is greater than that of formula_41 singly charged vortices, indicating that these multiply charged vortices are unstable to decay. Research has, however, indicated they are metastable states, so may have relatively long lifetimes.\n\nClosely related to the creation of vortices in BECs is the generation of so-called dark solitons in one-dimensional BECs. These topological objects feature a phase gradient across their nodal plane, which stabilizes their shape even in propagation and interaction. Although solitons carry no charge and are thus prone to decay, relatively long-lived dark solitons have been produced and studied extensively.\n\nExperiments led by Randall Hulet at Rice University from 1995 through 2000 showed that lithium condensates with attractive interactions could stably exist up to a critical atom number. Quench cooling the gas, they observed the condensate to grow, then subsequently collapse as the attraction overwhelmed the zero-point energy of the confining potential, in a burst reminiscent of a supernova, with an explosion preceded by an implosion.\n\nFurther work on attractive condensates was performed in 2000 by the JILA team, of Cornell, Wieman and coworkers. Their instrumentation now had better control so they used naturally \"attracting\" atoms of rubidium-85 (having negative atom–atom scattering length). Through Feshbach resonance involving a sweep of the magnetic field causing spin flip collisions, they lowered the characteristic, discrete energies at which rubidium bonds, making their Rb-85 atoms repulsive and creating a stable condensate. The reversible flip from attraction to repulsion stems from quantum interference among wave-like condensate atoms.\n\nWhen the JILA team raised the magnetic field strength further, the condensate suddenly reverted to attraction, imploded and shrank beyond detection, then exploded, expelling about two-thirds of its 10,000 atoms. About half of the atoms in the condensate seemed to have disappeared from the experiment altogether, not seen in the cold remnant or expanding gas cloud. Carl Wieman explained that under current atomic theory this characteristic of Bose–Einstein condensate could not be explained because the energy state of an atom near absolute zero should not be enough to cause an implosion; however, subsequent mean field theories have been proposed to explain it. Most likely they formed molecules of two rubidium atoms; energy gained by this bond imparts velocity sufficient to leave the trap without being detected.\n\nThe process of creation of molecular Bose condensate during the sweep of the magnetic field throughout the Feshbach resonance, as well as the reverse process, are described by the exactly solvable model that can explain many experimental observations.\n\nCompared to more commonly encountered states of matter, Bose–Einstein condensates are extremely fragile. The slightest interaction with the external environment can be enough to warm them past the condensation threshold, eliminating their interesting properties and forming a normal gas.\n\nNevertheless, they have proven useful in exploring a wide range of questions in fundamental physics, and the years since the initial discoveries by the JILA and MIT groups have seen an increase in experimental and theoretical activity. Examples include experiments that have demonstrated interference between condensates due to wave–particle duality, the study of superfluidity and quantized vortices, the creation of bright matter wave solitons from Bose condensates confined to one dimension, and the slowing of light pulses to very low speeds using electromagnetically induced transparency. Vortices in Bose–Einstein condensates are also currently the subject of analogue gravity research, studying the possibility of modeling black holes and their related phenomena in such environments in the laboratory. Experimenters have also realized \"optical lattices\", where the interference pattern from overlapping lasers provides a periodic potential. These have been used to explore the transition between a superfluid and a Mott insulator, and may be useful in studying Bose–Einstein condensation in fewer than three dimensions, for example the Tonks–Girardeau gas.\n\nBose–Einstein condensates composed of a wide range of isotopes have been produced.\n\nCooling fermions to extremely low temperatures has created degenerate gases, subject to the Pauli exclusion principle. To exhibit Bose–Einstein condensation, the fermions must \"pair up\" to form bosonic compound particles (e.g. molecules or Cooper pairs). The first molecular condensates were created in November 2003 by the groups of Rudolf Grimm at the University of Innsbruck, Deborah S. Jin at the University of Colorado at Boulder and Wolfgang Ketterle at MIT. Jin quickly went on to create the first fermionic condensate composed of Cooper pairs.\n\nIn 1999, Danish physicist Lene Hau led a team from Harvard University which slowed a beam of light to about 17 meters per second, using a superfluid. Hau and her associates have since made a group of condensate atoms recoil from a light pulse such that they recorded the light's phase and amplitude, recovered by a second nearby condensate, in what they term \"slow-light-mediated atomic matter-wave amplification\" using Bose–Einstein condensates: details are discussed in \"Nature\".\n\nAnother current research interest is the creation of Bose–Einstein condensates in microgravity in order to use its properties for high precision atom interferometry. The first demonstration of a BEC in weightlessness was achieved in 2008 at a drop tower in Bremen, Germany by a consortium of researchers led by Ernst M. Rasel from Leibniz University of Hanover. The same team demonstrated in 2017 the first creation of a Bose–Einstein condensate in space and it is also the subject of two upcoming experiments on the International Space Station.\n\nResearchers in the new field of atomtronics use the properties of Bose–Einstein condensates when manipulating groups of identical cold atoms using lasers.\n\nIn 1970, BECs were proposed by Emmanuel David Tannenbaum for anti-stealth technology.\n\nP. Sikivie and Q. Yang showed that cold dark matter axions form a Bose-Einstein condensate by thermalisation because of gravitational self-interactions. Axions have not yet been confirmed to exist. However the important search for them has been greatly enhanced with the completion of upgrades to the Axion Dark Matter Experiment(ADMX) at the University of Washington in early 2018.\n\nThe effect has mainly been observed on alkaline atoms which have nuclear properties particularly suitable for working with traps. As of 2012, using ultra-low temperatures of or below, Bose–Einstein condensates had been obtained for a multitude of isotopes, mainly of alkali metal, alkaline earth metal,\nand lanthanide atoms (Li, Na, K, K, Rb, Rb, Cs, Cr, Ca, Sr, Sr, Sr, Yb, Dy, and Er). Research was finally successful in hydrogen with the aid of the newly developed method of 'evaporative cooling'. In contrast, the superfluid state of He below is not a good example, because the interaction between the atoms is too strong. Only 8% of atoms are in the ground state near absolute zero, rather than the 100% of a true condensate.\n\nThe bosonic behavior of some of these alkaline gases appears odd at first sight, because their nuclei have half-integer total spin. It arises from a subtle interplay of electronic and nuclear spins: at ultra-low temperatures and corresponding excitation energies, the half-integer total spin of the electronic shell and half-integer total spin of the nucleus are coupled by a very weak hyperfine interaction. The total spin of the atom, arising from this coupling, is an integer lower value. The chemistry of systems at room temperature is determined by the electronic properties, which is essentially fermionic, since room temperature thermal excitations have typical energies much higher than the hyperfine values.\n\n\n"}
{"id": "45186416", "url": "https://en.wikipedia.org/wiki?curid=45186416", "title": "Broken-Backed War Theory", "text": "Broken-Backed War Theory\n\nBroken-Backed War Theory is a form of conflict that could transpire after a massive nuclear exchange. \nAssuming that following a nuclear exchange all the participants have not been utterly annihilated, there may arise a scenario unique to military strategy and theory, one in which all or some of the parties involved strive to continue fighting until the other side is decisively defeated.\n\nBroken-Backed War Theory was first formally elaborated on in the 1952 British \"Defence White Paper\", to describe what would presumably happen after a major nuclear exchange. The American \"New Look Strategy of 1953/54\" utterly rejected the notion of Broken-Backed war. They dropped the term from the 1955 white paper, and the phrase has since faded from common usage.\n\nKlaus Knorr purported, that in a broken-backed war scenario, only military weapons and vehicles on hand prior to the sustained hostilities would be of use, as the economic potential of both sides would be, at least in theory, utterly shattered. \nHerman Kahn in his tome On Thermonuclear War, has posited that a broken-backed war is implausible, because one side would likely absorb vastly more damage than its opposition. As he was writing in the late 1950s, when the nuclear arsenals of the Soviet Union and the United States numbered in the tens of thousands, the validity of this statement in the modern war can be called into question.\n\nThe nuclear strategist Bernard Brodie argued that this form of conflict may be impractical simply because it is almost impossible to plan for. His writings on the subject came before the advent of Counter-force doctrine, and during a time of nuclear plenty, when it was safe to assume that a nuclear exchange would render a nation's industry useless.\n\nDuring the Cold War, Colonel Virgil Ney hypothesized that a nuclear exchange alone would not be enough to defeat the Soviet Union, and he argued for a modest construction of underground facilities and infrastructure.\n\nIn the novel, \"Final Blackout\" by L. Ron Hubbard, the conflict between the survivors of London and the United States has been characterized as a Broken-Backed War by some critics.\n\nThe table-top Role-playing game Twilight 2000 released by Game Designers' Workshop in 1984 entails a Broken-Backed War; in the aftermath of a nuclear exchange in 1997, by 2000, Warsaw Pact and NATO forces are still fighting for a decisive victory in Europe and elsewhere with dwindling conventional arms and munitions.\n"}
{"id": "2103899", "url": "https://en.wikipedia.org/wiki?curid=2103899", "title": "Chakib Khelil", "text": "Chakib Khelil\n\nChakib Khelil (born 8 August 1939) is an Algerian political figure who served in the government of Algeria as Minister of Energy and Mines from 1999 to 2010.\n\nKhelil was born on 8 August 1939 in Oujda, Morocco. He received a bachelor and master degrees in mining and petroleum engineering from Ohio State University in 1964 and 1965, respectively. He also obtained a doctorate in petroleum engineering from Texas A&M University in 1968.\n\nAfter working in the United States for a few years with Shell, Phillips Petroleum and D.R. McCord & Associates, Khelil joined Sonatrach in 1971 as head of its reservoir engineering department while being president of Alcore, a joint venture between Corelaboratories and Sonatrach. Then he became technical adviser in the presidency of Algeria from 1973 to 1975. He was appointed president of the Valhyd Group (with Valhyd standing for valorisation hydrocarbures in French) in 1975-1978. Next he served as coordinator in the hydrocarbon vice presidency of Sonatrach.\n\nIn 1980 he joined the World Bank, working on petroleum-related projects in Africa, Latin America and Asia; he retired from it as its petroleum adviser in 1999, only to join President Abdelaziz Bouteflika's cabinet on 1 November 1999, becoming Minister for Energy and Mines on 26 December 1999.\n\nIn 2001, while a minister, he was also named president of Sonatrach, and he nominated and was succeeded by Mohamed Meziane in 2003. He was also president of OPEC for the years 2001 and 2008, president of the African Energy Commission (AFREC) in 2001, president of the Association of African Oil Producing Countries (APPA) in 2004 and the Gas Exporting Countries Forum (GECF) in 2010.\n\n"}
{"id": "38562377", "url": "https://en.wikipedia.org/wiki?curid=38562377", "title": "Chelyabinsk meteorite", "text": "Chelyabinsk meteorite\n\nThe Chelyabinsk meteorite (Russian: Челябинск or Челябинский метеорит) is the fragmented remains of the large Chelyabinsk meteor of 15 February 2013 which reached the ground after the meteor's passage through the atmosphere. The descent of the meteor, visible as a brilliant superbolide in the morning sky, caused a series of shock waves that shattered windows, damaged approximately 7,200 buildings and left 1,500 people injured. The resulting fragments were scattered over a wide area.\n\nThe largest fragment had a total mass of and was raised from the bottom of Lake Chebarkul on 16 October 2013.\n\nThe meteor and meteorite are named after Chelyabinsk Oblast, over which the meteor exploded. An initial proposal was to name the meteorite after Lake Chebarkul, where one of its major fragments impacted and made a 6-metre-wide hole in the frozen lake surface.\n\nThe meteorite has been classified as an LL5 ordinary chondrite. First estimates of its composition indicate about 10% of meteoric iron, as well as olivine and sulfides.\n\nThe impacting asteroid started to brighten up in the general direction of the Pegasus constellation, close to the East horizon where the Sun was starting to rise. The impactor belonged to the Apollo group of near-Earth asteroids.\n\nThe asteroid had an approximate size of and a mass of about before it entered the denser parts of Earth's atmosphere and started to ablate. At an altitude of about 23.3 km (14.5 miles) the body exploded in an air burst. Meteorite fragments of the body landed on the ground.\n\nAnalysis of three fragments using optical microscopy, electron microscopy, Raman spectroscopy, and isotopic composition techniques used to date Solar System objects, showed the isotopic clocks in the asteroids (rubidium and strontium ratios, argon isotope ratios) appear to have partially or totally reset in past collisions. The isotopic clock resets may result from thermal effects changing isotopic ratios, and changes to cosmic radiation exposure. The asteroid appears to have had eight major collisions, around 4.53, 4.45, 3.73, 2.81, and 1.46 billion years ago, then at 852, 312, and 27 million years ago.\n\nScientists collected 53 samples nearby a 6-metre-wide hole in the ice of Lake Chebarkul, thought to be the result of a single meteorite fragment impact. The specimens are of various sizes, with the largest being , and initial laboratory analysis confirmed their meteoric origin.\n\nIn June 2013, Russian scientists reported that further investigation by magnetic imaging below the location of the ice hole in Lake Chebarkul has identified a meteorite buried in the mud at the bottom of the lake. An operation to recover it from the lake began in September 10, 2013, and concluded on 16 October 2013 with the raising of the rock with an estimated mass of . It was examined by scientists and then handed over to the local authorities. It was then put on display at the Chelyabinsk State Museum of Local Lore, causing protests from the followers of the recently set-up 'Church of Chelyabinsk Meteorite'.\n\nIn the aftermath of the superbolide air burst, a large number of small meteorite fragments fell on areas west of Chelyabinsk, including Deputatskoye, generally at terminal velocity, about the speed of a piece of gravel dropped from a skyscraper. Local residents and schoolchildren located and picked up some of the meteorites, many located in snowdrifts, by following a visible hole that had been left in the outer surface of the snow. Speculators have been active in the informal market for meteorite fragments that has rapidly emerged.\n\n\n\n"}
{"id": "53150446", "url": "https://en.wikipedia.org/wiki?curid=53150446", "title": "Chrysler minivans (RS)", "text": "Chrysler minivans (RS)\n\nThe RS-platform Chrysler minivans are a series of passenger minivans marketed by Chrysler from model years 2001 to 2007, the fourth in six generations of Chrysler minivans, which were heavily revised versions of the NS minivans. Depending on the market, these vans were known as the Dodge Caravan, Chrysler Town & Country and the Chrysler Voyager. With the discontinuation of the Plymouth brand, the Plymouth Voyager didn't return.\n\nIn development from February 1996 to December 1999, the Generation IV minivans were based on the Chrysler RS platform and featured a larger body frame with modified headlights and taillights. Design work by Brandon Faurote was approved in January 1997 and reached production approval in October 1997. Unveiled at the 2000 North American International Auto Show (NAIAS) on Monday, January 10, 2000, the RS minivans were released for sale in August 2000. The release was part of a promotional tie-in with Nabisco, which unveiled their new \"Mini Oreos\" inside the van during the unveiling. The first vans rolled off the line at the Windsor Assembly Plant on July 24.\nThe RS minivans were sold as the Dodge Caravan, Chrysler Voyager and Chrysler Town & Country, in 4 door body styles. The Caravan was available in a long wheelbase model, called the Grand Caravan, while the Chrysler Voyager was a short wheelbase model with base levels of equipment and the Town & Country was only available with a long wheelbase and was the highest end model. The U.S. and Canadian market Chrysler Voyager was originally intended to be a Plymouth, but became a Chrysler with the discontinuation of the Plymouth brand in 2000. This model was also exported, while the Town & Country was exported as a high-level model of the Voyager. Export models had the platform designation RG.\n\nTrim levels for the Caravan and Town & Country were carried over from the previous generation, while the Voyager was only offered in a base model. The Caravan was available in Base, SE, Sport and ES trims, and the Town & Country was available in LX, LXi and Limited. In addition to other detailed changes, remote operated sliding doors and rear hatch, which could be opened and closed at the push of a button, either inside the vehicle, or with the keyless entry fob, became options, as well as a new tri-zone climate control system and side seat mounted airbags. In 2002, the value-packed eL and eX models were added to the Town & Country. These models were both value-priced versions of the LX and LXi, respectively, with popular option packages. 2004 saw the addition of an unnamed base short-wheelbase model.\n\nIn 2003, the Chrysler Voyager was discontinued in the U.S., a short wheelbase Town & Country became available, and the Caravan C/V and Grand Caravan C/V returned after having been discontinued in 1995. The C/V featured the option of deleted side windows (replaced by composite panels), optional rear seats, a cargo floor made of plastic material similar to pickup truck bedliners, rubber flooring in lieu of carpeting and normal hatch at the rear. Minor changes were made to the Grand Caravan ES including many of the features included in Option Group 29S becoming standard, the 17 inch Titan Chrome wheels no longer being an option replaced with standard 16 inch chrome wheels, and the disappearance of the AutoStick Transmission option. This year also saw the appearance of an optional factory-installed rear seat DVD system with single disc player mounted below the HVAC controls, and the addition of a SXT model. 2004 offered an exclusive one year only \"Anniversary Edition\" package to mark Caravan's 20th year in production. This package was offered on higher level SXT models, and included chrome wheels, body color moldings, special interior accents and a unique fender badge.\n\nThe 2005 minivans received a mid-cycle refresh including revised front fascias and a mildly restyled interior. This model introduced a system of second- and third-row seating that folded completely into under-floor compartments. It was marketed as \"Stow 'N Go\" and was available exclusively on long-wheelbase models. In a development program costing $400 million, engineers initially used an Erector Set to visualize the complex interaction of the design and redesigned under-floor components. The system included the spare tire well, fuel tank, exhaust system, parking brake cables, rear climate control lines, and rear suspension but brought the elimination of all-wheel drive (AWD). The system, in turn, creates a combined volume of of under-floor storage when second-row seats are deployed. With both rows folded, the vans have a flat-load floor and a maximum cargo volume of . The Stow 'n Go system received the Popular Science Magazine's \"Best of What's New\" for 2005 award.\n\nTrim levels were again reshuffled on the Town & Country, being available in a short-wheelbase base model, and long-wheelbase LX, Touring and Limited models. As with the pre-refresh model, only the Touring and Limited were sold to consumers in Canada, the LX being restricted to fleets. A driver's side knee airbag was now standard on all models. The front seat-mounted side airbags of previous years were discontinued in favor of side-curtain airbags for all three rows. These were standard on Limited trim and optional on all other models, however could not be ordered with the moonroof option. Uconnect Bluetooth phone pairing was now available, as well as an overhead rail storage system with three moveable or removable compartments.\n\nTaiwanese-market Town & Country minivans were assembled in Yangmei, Taiwan under license by the China Motor Corporation, starting with the 2006 model year. They are more closely related to the RG Grand Voyager than the RS Town & Country sold in North America despite sharing a nameplate. Taiwanese models featured minor variations for the local market including LED taillights and mirror-mounted turn signals. In 2007, production ended and the production line was relocated to China where Soueast continued to assemble it under the Chrysler Grand Voyager and Dodge Grand Caravan nameplates from 2008 until late 2010.\n\nProduction of this generation continued in China under the Chrysler Grand Voyager and Dodge Grand Caravan nameplates from 2008, when the Taiwanese Town & Country production line was relocated to Soueast (a joint venture between CMC, Mitsubishi Motors and the Fujian Motors Group, until late 2010 when the facelifted fifth generation Chrysler Grand Voyager was introduced there. The Grand Caravan was replaced in this market by the JCUV. The Chinese Grand Voyager was identical to the Taiwanese Town & Country, while the Grand Caravan was not based on the RS Grand Caravan sold in the United States and Canada. Instead, it was a modified version of the Grand Voyager with a new grille, incandescent taillights instead of the Grand Voyager's LED units, fender-mounted turn signals instead of mirror-mounted units, and wheels from the RS Grand Caravan. Chinese models were equipped with Mitsubishi 6G72 engines.\n\nThe 2001 model of this version earned a \"Poor\" rating in the Insurance Institute for Highway Safety\n's 40 mph offset test. It did protect its occupants reasonably well, and the dummy movement was well controlled, however, a fuel leak occurred. Chrysler corrected this problem starting with the 2002 models, moving it up to an \"Acceptable\" rating.\n\nThe 2006 model year brought optional side curtain airbags and a stronger B-pillar, which was tested by the Insurance Institute for Highway Safety's side impact crash test. With the side airbags, it got an \"Acceptable\" rating. For the driver, there is a chance of serious neck injuries, rib fractures and/or internal organ injuries. The rear passengers, however, could leave this accident unharmed, as there is a low risk of significant injury in a crash of this severity for them.\n\nThe 4th generation Town & Country (Grand Voyager, as it is known in Europe))right hand drive (RHD) version performed very poorly in the Euro NCAP car safety tests and achieved the following ratings:\n\n<br>\nHowever, it was noted that \"The LHD car performed significantly better than the RHD car in the frontal impact, scoring 9 points, giving a potential four star adult occupant rating.\" \nThatcham's New Car Whiplash Ratings (NCWR) organization tested the 4th generation European Grand Voyager for its ability to protect occupants against whiplash injuries with the car achieving an 'Acceptable' rating overall.\n\nAs with the previous generation, the Caravan came standard with a 2.4 L \"EDZ\" I4 and a 3-speed TorqueFlite automatic transmission, with optional 3.3 L \"EGA\" and 3.8 L \"EGH\" V6 engines, which came with a 4-speed Ultradrive automatic transmission. The 3.3L V6 was the standard engine on the Voyager and Town & Country, as well as Canadian Caravans, where the 2.4L wasn't offered. Export models could be equipped with a 2.5 L Turbo Diesel \"R 425\" or a 2.8 L Turbo Diesel \"R 428\" from VM Motori, and were available with a 5-speed manual transmission. Chinese models got a 3.0 L \"6G72\".\n\n"}
{"id": "12646128", "url": "https://en.wikipedia.org/wiki?curid=12646128", "title": "Civaux Nuclear Power Plant", "text": "Civaux Nuclear Power Plant\n\nThe Civaux Nuclear Power Plant is located in the commune of Civaux (Vienne) at the edge of Vienne River between Confolens (60 km upstream) and Chauvigny (14 km downstream), and 44 km south-east of Poitiers.\n\nIt has two operating units that were the precursors to the European Pressurized Reactor, being the \"N4 stage\". Designed for a net power output of 1350 MWe per unit, power was uprated to 1360 MWe in 2010. The Civaux plant uses ambient air and water from the Vienne River for cooling.\n\nAs of 2014, 582 people work at the plant, with 2.9% women.\n\nThe cooling towers of Civaux Nuclear Power Plant are 178 metres in height, , which are the highest among those of EDF's nuclear power plants.\n\n\n"}
{"id": "39444110", "url": "https://en.wikipedia.org/wiki?curid=39444110", "title": "Coherent effects in semiconductor optics", "text": "Coherent effects in semiconductor optics\n\nThe interaction of matter with light, i.e., electromagnetic fields, is able to generate a coherent superposition of excited quantum states in the material.\n\"Coherent\" denotes the fact that the material excitations have a well defined phase relation which originates from the phase of the incident electromagnetic wave.\nMacroscopically, the superposition state of the material results in an optical polarization, i.e., a rapidly oscillating dipole density.\nThe optical polarization is a genuine non-equilibrium quantity that decays to zero when the excited system relaxes to its equilibrium state after the electromagnetic pulse is switched off.\nDue to this decay which is called \"dephasing\", coherent effects are observable only for a certain temporal duration after pulsed photoexcitation. Various materials such as atoms, molecules, metals, insulators, semiconductors are studied using coherent optical spectroscopy and such experiments and their theoretical analysis has revealed a wealth of insights on the involved matter states and their dynamical evolution.\n\nThis article focusses on coherent optical effects in semiconductors and semiconductor nanostructures.\nAfter an introduction into the basic principles, the semiconductor Bloch equations (abbreviated as SBEs) which are able to theoretically describe coherent semiconductor optics on the basis of a fully microscopic many-body quantum theory are introduced.\nThen, a few prominent examples for coherent effects in semiconductor optics are described all of which can be understood theoretically on the basis of the SBEs.\n\nMacroscopically, Maxwell's equations show that in the absence of free charges and currents an electromagnetic field interacts with matter via the optical polarization formula_1.\nThe wave equation for the electric field formula_2 reads formula_3 and shows that the second derivative with respect to time of formula_1, i.e., formula_5, appears as a source term in the wave equation for the electric field formula_2.\nThus, for optically thin samples and measurements performed in the far-field, i.e., at distances significantly exceedng the optical wavelength formula_7, the emitted electric field resulting from the polarization is proportional to its second time derivative, i.e., formula_8.\nTherefore, measuring the dynamics of the emitted field formula_9 provides direct information on the temporal evolution of the optical material polarization formula_10.\n\nMicroscopically, the optical polarization arises from quantum mechanical transitions between different states of the material system.\nFor the case of semiconductors, electromagnetic radiation with optical frequencies is able to move electrons from the valence (formula_11) to the conduction (formula_12) band.\nThe macroscopic polarization formula_1 is computed by summing over all microscopic transition dipoles formula_14 via formula_15, where formula_16 is the dipole matrix element which determines the strength of individual transitions between the states formula_11 and formula_12, formula_19 denotes the complex conjugate, and formula_20 is the appropriately chosen system's volume.\nIf formula_21 and formula_22 are the energies of the conduction and valence band states, their dynamic quantum mechanical evolution is according to the Schrödinger equation given by phase factors formula_23 and formula_24, respectively.\nThe superposition state described by formula_14 is evolving in time according to formula_26.\nAssuming that we start at formula_27 with formula_28, we have for the optical polarization\n\nformula_29.\n\nThus, formula_30 is given by a summation over the microscopic transition dipoles which all oscillate with frequencies corresponding to the energy differences between the involved quantum states.\nClearly, the optical polarization formula_30 is a coherent quantity which is characterized by an amplitude and a phase.\nDepending on the phase relationships of the microscopic transition dipoles, one may obtain constructive or destructive interference, in which the microscopic dipoles are in or out of phase, respectively, and temporal interference phenomena like quantum beats, in which the modulus of formula_30 varies as function of time.\n\nIgnoring many-body effects and the coupling to other quasi particles and to reservoirs, the dynamics of photoexcited two-level systems can be described by a set of two equations, the so-called optical Bloch equations.\nThese equations are named after Felix Bloch who formulated them in order to analyze the dynamics of spin systems in nuclear magnetic resonance.\nThe two-level Bloch equations read\n\nformula_33\n\nand\n\nformula_34\n\nHere, formula_35 denotes the energy difference between the two states and formula_36 is the inversion, i.e., the difference in the occupations of the upper and the lower states.\nThe electric field formula_2 couples the microscopic polarization formula_38 to the product of the Rabi energy formula_39 and the inversion formula_36.\nIn the absence of the driving electric field, i.e., for formula_41, the Bloch equation for formula_38 describes an oscillation, i.e., formula_43.\n\nThe optical Bloch equations enable a transparent analysis of several nonlinear optical experiments.\nThey are, however, only well suited for systems with optical transitions between isolated levels in which many-body interactions are of minor importance as is sometimes the case in atoms or small molecules.\nIn solid state systems, such as semiconductors and semiconductor nanostructures, an adequate description of the many-body Coulomb interaction and the coupling to additional degrees of freedom is essential and thus the optical Bloch equations are not applicable.\n\nFor a realistic description of optical processes in solid materials, it is essential to go beyond the simple picture of the optical Bloch equations and to treat many-body interactions that describe the coupling among the elementary material excitations by, e.g., the see article Coulomb interaction between the electrons and the coupling to other degrees of freedom, such as lattice vibrations, i.e., the electron-phonon coupling.\nWithin a semiclassical approach, where the light field is treated as a classical electromagnetic field and the material excitations are described quantum mechanically, all above mentioned effects can be treated microscopically on the basis of a many-body quantum theory.\nFor semiconductors the resulting system of equations are known as the semiconductor Bloch equations.\nFor the simplest case of a two-band model of a semiconductor, the SBEs can be written schematically as\n\nformula_44\n\nformula_45\n\nformula_46\n\nHere formula_47 is the microscopic polarization and formula_48 and formula_49 are the electron occupations in the conduction and valence bands (formula_12 and formula_11), respectively, and formula_52 denotes the crystal momentum.\nAs a result of the many-body Coulomb interaction and possibly further interaction processes, the transition energy formula_53 and the Rabi energy formula_54 both depend on the state of the excited system, i.e., they are functions of the time-dependent polarizations formula_55 and occupations formula_56 and formula_57, respectively, at all crystal momenta formula_58.\n\nDue to this coupling among the excitations for all values of the crystal momentum formula_52, the optical excitations in semiconductor cannot be described on the level of isolated optical transitions but have to be treated as an interacting many-body quantum system.\n\nA prominent and important result of the Coulomb interaction among the photoexcitations\nis the appearance of strongly absorbing discrete excitonic resonances which show up in the absorption spectra of semiconductors spectrally below the fundamental band gap frequency. \nSince an exciton consists of a negatively charged conduction band electron and a positively charged valence band hole (i.e., an electron missing in the valence band) which attract each other via the Coulomb interaction, excitons have a hydrogenic series of discrete absorption lines. \nDue to the optical selection rules of typical III-V semiconductors such as Galliumarsenide (GaAs) only the s-states, i.e., 1\"s\", 2\"s\", etc., can be optically excited and detected, see article on Wannier equation.\n\nThe many-body Coulomb interaction leads to significant complications since it results in an infinite hierarchy of dynamic equations for the microscopic correlation functions that describe the nonlinear optical response.\nThe terms given explicitly in the SBEs above arise from a treatment of the Coulomb interaction in the time-dependent Hartree–Fock approximation. \nWhereas this level is sufficient to describe excitonic resonances, there are several further effects, e.g., excitation-induced dephasing, contributions from higher-order correlations like excitonic populations and biexcitonic resonances, which require one to treat so-called many-body correlation effects that are by definition beyond the Hartree–Fock level.\nThese contributions are formally included in the SBEs given above in the terms denoted by formula_60.\n\nThe systematic truncation of the many-body hierarchy and the development and the analysis of controlled approximations schemes is an important topic in the microscopic theory of the optical processes in condensed matter systems.\nDepending on the particular system and the excitation conditions several approximations schemes have been developed and applied.\nFor highly excited systems, it is often sufficient to describe many-body Coulomb correlations using the second order Born approximation.\nSuch calculations were, in particular, able to successfully describe the spectra of semiconductor lasers, see article on semiconductor laser theory.\nIn the limit of weak light intensities, signature of exciton complexes, in particular, biexcitons, in the coherent nonlinear response have been analyzed using the dynamics controlled truncation scheme.\nThese two approaches and several other approximation schemes can be viewed as special cases of the so-called cluster expansion in which the nonlinear optical response is classified by correlation functions which explicitly take into account interactions between a certain maximum number of particles and factorize larger correlation functions into products of lower order ones.\n\nBy nonlinear optical spectroscopy using ultrafast laser pulses with durations on the order of ten to hundreds of femtoseconds, several coherent effects have been observed and interpreted.\nSuch studies and their proper theoretical analysis have revealed a wealth of information on the nature of the photoexcited quantum states, the coupling among them, and their dynamical evolution on ultrashort time scales. In the following, a few important effects are briefly described.\n\nQuantum beats are observable in systems in which the total optical polarization is due to a finite number of discrete transition frequencies which are quantum mechanically coupled, e.g., by common ground or excited states.\nAssuming for simplicity that all these transitions have the same dipole matrix element, after excitation with a short laser pulse at formula_27 the optical polarization formula_30 of the system evolves as\n\nformula_63,\n\nwhere the index formula_64 labels the participating transitions.\nA finite number of frequencies results in temporal modulations of the squared modulus of the polarization formula_65 and thus of the intensity of the emitted electromagnetic field formula_66 with time periods\n\nformula_67.\n\nFor the case of just two frequencies the squared modulus of the polarization is proportional to\n\nformula_68,\n\ni.e., due to the interference of two contributions with the same amplitude but different frequencies, the polarization varies between a maximum and zero.\n\nIn semiconductors and semiconductor heterostructures, such as quantum wells, nonlinear optical quantum-beat spectroscopy has been widely used to investigate the temporal dynamics of excitonic resonances.\nIn particular, the consequences of many-body effects which depending on the excitation conditions may lead to, e.g., a coupling among different excitonic resonances via biexcitons and other Coulomb correlation contributions and to a decay of the coherent dynamics by scattering and dephasing processes, has been explored in many pump-probe and four-wave-mixing measurements.\nThe theoretical analysis of such experiments in semiconductors requires a treatment on the basis of quantum mechanical many-body theory as is provided by the SBEs with many-body correlations incorporated on an adequate level.\n\nIn nonlinear optics it is possible to reverse the destructive interference of so-called inhomogeneously broadened systems which contain a distribution of uncoupled subsystems with different resonance frequencies.\nFor example, let us consider a four-wave-mixing experiment in which the first short laser pulse excites all transitions at formula_27.\nAs a result of the destructive interference between the different frequencies the overall polarization decays to zero.\nA second pulse arriving at formula_70 is able to conjugate the phases of the individual microscopic polarizations, i.e., formula_71, of the inhomogeneously broadened system.\nThe subsequent unperturbed dynamical evolution of the polarizations leads to rephasing such that all polarization are in phase at formula_72 which results in a measurable macroscopic signal.\nThus, this so-called photon echo occurs since all individual polarizations are in phase and add up constructively at formula_72.\nSince the rephasing is only possible if the polarizations remain coherent, the loss of coherence can be determined by measuring the decay of the photon echo amplitude with increasing time delay.\n\nWhen photon echo experiments are performed in semiconductors with exciton resonances, it is essential to include many-body effects in the theoretical analysis since they may qualitatively alter the dynamics. For example, numerical solutions of the SBEs have demonstrated that the dynamical reduction of the band gap which originates from the Coulomb interaction among the photoexcited electrons and holes is able to generate a photon echo even for resonant excitation of a single discrete exciton resonance with a pulse of sufficient intensity.\n\nBesides the rather simple effect of inhomogeneous broadening, spatial fluctuations of the energy, i.e., disorder, which in semiconductor nanostructure may, e.g., arise from imperfection of the interfaces between different materials, can also lead to a decay of the photon echo amplitude with increasing time delay. To consistently treat this phenomenon of disorder induced dephasing the SBEs need to be solved including biexciton correlations.\nAs shown in Ref. such a microscopic theoretical approach is able to describe disorder induced dephasing in good agreement with experimental results.\n\nIn a pump-probe experiment one excites the system with a pump pulse (formula_74) and probes its dynamics with a (weak) test pulse (formula_75).\nWith such experiments one can measure the so-called differential absorption formula_76 which is defined as the difference between the probe absorption in the presence of the pump formula_77 and the probe absorption without the pump formula_78.\n\nFor resonant pumping of an optical resonance and when the pump precedes the test, the absorption change formula_79 is usually negative in the vicinity of the resonance frequency.\nThis effect called bleaching arises from the fact that the excitation of the system with the pump pulse reduces the absorbance of the test pulse.\nThere may also be positive contributions to formula_79 spectrally near the original absorption line due to resonance broadening and at other spectral positions due to excited-state absorption, i.e., optical transitions to states such as biexcitons which are only possible if the system is in an excited state.\nThe bleaching and the positive contributions are generally present in both coherent and incoherent situations where the polarization vanishes but occupations in excited states are present.\n\nFor detuned pumping, i.e., when the frequency of the pump field is not identical with the frequency of the material transition, the resonance frequency shifts as a result of the light-matter coupling, an effect known as the optical Stark effect.\nThe optical Stark effect requires coherence i.e., a non vanishing optical polarization induced be the pump pulse, and thus decreases with increasing time delay between the pump and probe pulses and vanishes if the system has returned to its ground state.\n\nAs can be shown by solving the optical Bloch equations for a two-level system due to the optical Stark effect the resonance frequency should shift to higher values, if the pump frequency is smaller than the resonance frequency and vice versa.\nThis is also the typical result of experiments performed on excitons in semiconductors.\nThe fact that in certain situations such predictions which are based on simple models fail to even qualitatively describe experiments in semiconductors and semiconductor nanostructures has received significant attention.\nSuch deviations are because in semiconductors typically many-body effects dominate the optical response and therefore it is required to solve the SBEs instead of the optical Bloch equations to obtain an adequate understanding.\nAn important example was presented in Ref. where it was shown that many-body correlations arising from biexcitons are able to reverse the sign of the optical Stark effect. In contrast to the optical Bloch equations, the SBEs including coherent biexcitonic correlations were able to properly describe the experiments performed on semiconductor quantum wells.\n\nLet us consider formula_81 two-level systems at different positions in space.\nMaxwell's equations lead to a coupling among all the optical resonances since the field emitted from a specific resonance interferes with the emitted fields of all other resonances.\nAs a result, the system is characterized by formula_81 eigenmodes originating from the radiatively coupled optical resonances.\n\nA spectacular situation arises if formula_81 identical two-level systems are regularly arranged with distances that equals an integer multiple of formula_84, where formula_7 is the optical wavelength.\nIn this case, the emitted fields of all resonances interfere constructively and the system behaves effectively as a single system with a formula_81-times stronger optical polarization.\nSince the intensity of the emitted electromagnetic field is proportional to the squared modulus of the polarization, it scales initially as formula_87.\n\nDue to the cooperativity that originates from the coherent coupling of the subsystems, the radiative decay rate formula_88 is increased by formula_81, i.e., formula_90 where formula_91 is the radiative decay of a single two-level system.\nThus the coherent optical polarization decays formula_81-times faster proportional to formula_93 than that of an isolated system.\nAs a result the time integrated emitted field intensity scales as formula_81, since the initial formula_87 factor is multiplied by formula_96 which arises from the time integral over the enhanced radiative decay.\n\nThis effect of superradiance has been demonstrated by monitoring the decay of the exciton polarization in suitably arranged semiconductor multiple quantum wells.\nDue to superradiance introduced by the coherent radiative coupling among the quantum wells, the decay rate increases proportional to the number of quantum wells and is thus significantly more rapid than for a single quantum well.\nThe theoretical analysis of this phenomenon requires a consistent solution of Maxwell's equations together with the SBEs.\n\nThe few examples given above represent only a small subset of several further phenomena which demonstrate that the coherent optical response of semiconductors and semiconductor nanostructures is strongly influenced by many-body effects.\nOther interesting research directions which similarly require an adequate theoretical analysis including many-body interactions are, e.g., phototransport phenomena where optical fields generate and/or probe electronic currents, the combined spectroscopy with optical and Terahertz field, see article Terahertz spectroscopy and technology, and the rapidly developing area of semiconductor quantum optics, see article Semiconductor quantum optics with dots.\n\n\n"}
{"id": "47618045", "url": "https://en.wikipedia.org/wiki?curid=47618045", "title": "Copiapó Solar Project", "text": "Copiapó Solar Project\n\nThe Copiapó Solar Project is a 260 megawatt (MW) net solar thermal power project to be located near Copiapó, about 65 kilometers east of the coastal town of Caldera. The project is being developed by SolarReserve, and is scheduled to reach commercial operation in 2019.\n\nThe Copiapó project will comprise two 120 megawatt (MW, 130 MW gross) solar thermal towers with up to 14 hours thermal storage, combined with 150 MW of PV. The hybrid project will deliver over 1,700 gigawatt hours (GW·h) annually, as non-intermittent baseload power, 24 hours a day. The project uses heliostat mirrors that collect and focus the sun's thermal energy to heat molten salt flowing through a solar power tower. The molten salt circulates from the tower to a storage tank, where it is then used to produce steam and generate electricity. Excess thermal energy is stored in the molten salt and can be used to generate power for up to fourteen hours, including during the evening hours and when direct sunlight is not available. The project's solar tower component technology is based on the SolarReserve Crescent Dunes Solar Energy Project in the US.\n\nThe project is expected to cost $2 billion.\n\nThe Copiapó Solar project was submitted to a full environmental assessment under the Chilean system of Environmental Impact Assessment (SEIA) administered by the Department of Environmental Assessment (SEA), and received an Environmental Qualification Resolution (RCA), as it is called the Chilean environmental permit, on August 19, 2015.\n\nThe hybrid concept combines two or more energy conversion mechanisms, that when integrated, overcome limitations inherent in either. The purpose is to provide a high level of energy security and reliability through the integrated mix of complementary generation methods. Specifically, photovoltaics, to date, has a lower cost if one ignores the dispatchability question, that instead is solved by the solar thermal component.\n\nAt the 2017-01 auction, SolarReserve bid $63/MWh for 24-hour CSP power with no subsidies, competing with other types such as LNG gas turbines.\nAnd in the following auction they bid less than 5 ¢/kWh.\n"}
{"id": "17769306", "url": "https://en.wikipedia.org/wiki?curid=17769306", "title": "Cycle oil", "text": "Cycle oil\n\nCycle oil is a light lubricating oil suited for use on bicycles and similar devices. It is a liquid residue produced in the petroleum industry when catalytic cracking is employed to convert heavy hydrocarbon fractions remaining from earlier stages of crude oil refining into more valuable lighter products. \n\nCatalytic cracking produces petrol (gasoline), liquid petroleum gas (LPG), unsaturated olefin compounds, cracked gas oils, cycle oil, light gases and a solid coke residue. Cycle oil may be processed further to break it down into more useful products; in particular it may be mixed with heavier products and put through the refining process again (recycled).\n"}
{"id": "24891245", "url": "https://en.wikipedia.org/wiki?curid=24891245", "title": "Dental compomer", "text": "Dental compomer\n\nDental compomers are materials which are used in dentistry as restorative material. They were introduced in the early 1990s as a hybrid of two other dental materials: dental composites and glass ionomer cement. They are also known as polyacid-modified resin composites. They are used for restorations in low stress–bearing areas\n\nCompomers were introduced in the early 1990s. Previous available restorative materials included dental amalgam, glass ionomer cement, resin modified glass ionomer cement and dental composites.\n\nThe composition of compomers is similar to that of a dental composite however it has been modified, making it a polyacid-modified composite. This results in compomers still requiring a bonding system to bond to tooth tissue. Compomer contains poly acid–modified monomers and fluoride-releasing silicate glasses. An acid-base reaction occurs as the compomer absorbs water after contact with saliva, which facilitates cross-linking structure and fluoride release.\n\nCompomers release some fluoride ions, like a glass ionomer cement. The level of this fluoride release however is only around 10% of that of a glass ionomer, which makes it less useful for deciduous restorations. It has been shown to have no advantage over an amalgam restoration with a fluoride releasing bonding agent, which releases mercury and fluoride. Compomers also do not have the ability to 'recharge' their fluoride ion content with topically applied fluoride from toothpaste etc., like glass ionomer cements; this is another limit on their efficacy. Compomers are recommended for patients at medium risk of developing caries.\n\nHandling and ease of use of composites is generally seen as good by dental professionals. Compomers are available in both normal and flowable forms, with the manufacturers of the flowable compomers claiming that they have the ability to shape to the cavity without the need for hand instruments.\n\nCompomers are tooth coloured materials, and so their aesthetics can immediately be seen as better than that of dental amalgams. It has been shown that ratings in various aesthetic areas are better for compomers than resin modified glass ionomer cements. Compomers are also available in various non-natural colours from various dental companies for use in deciduous teeth.\n\n"}
{"id": "8630521", "url": "https://en.wikipedia.org/wiki?curid=8630521", "title": "Dilithium (Star Trek)", "text": "Dilithium (Star Trek)\n\nIn the \"Star Trek\" fictional universe, dilithium is an invented material which serves as a controlling agent in the faster-than-light warp drive. In , dilithium crystals were rare and could not be replicated, making the search for them a recurring plot element. According to a periodic table shown during a \"\" episode, it has the chemical symbol Dt and the atomic number 87, which in reality belongs to francium.\n\nIn the real world, dilithium (Li) is a molecule composed of two covalently bonded lithium atoms.\n\nDilithium is depicted as an extremely hard crystalline mineral that occurs naturally on some planets. It is believed that this material exists in more than three dimensions at the same time and this is somehow related to its properties. When placed in a high-frequency electromagnetic field, eddy currents are induced in its structure which keep charged particles away from the crystal lattice. This prevents it from reacting with antimatter when so energized, because the antimatter particles never actually touch it. Therefore, it is used to contain and regulate the annihilation reaction of matter and antimatter in a starship's warp core, which otherwise would explode from the uncontrolled annihilation reaction. Though low-quality artificial crystals can be grown or replicated, they are limited in the power of the reaction they can regulate without fragmenting, and are therefore largely unsuitable for warp drive applications. Due to the need for natural dilithium crystals for interstellar travel, deposits of this material are, much like real-world equivalents such as oil, a highly contested resource, and as such, dilithium crystals have led to more interstellar conflict than all other reasons combined.\n\nIn the original series, dilithium crystals were rare and could not be replicated. In \"\" Spock re-crystallized a Klingon Bird of Prey's decaying dilithium through exposure to high-energy photons as generated by 20th century fission reactors.\n\nAs shown on the series, the streams of matter (deuterium gas) and antimatter (anti-deuterium) directed into crystallized dilithium are unbalanced: there is usually much more matter in the stream than antimatter. The energy generated in the annihilation reaction heats up the excess deuterium gas, producing a plasma that is used to power the warp nacelles thus allowing a starship to travel faster than light. In addition, most starships use this plasma as a power source for the ship's systems; in the series \"\" (2001-2005), this was referred to as an \"electro-plasma system\", (a backronym of the term \"EPS\", which was used in all other series except ) to refer to a ship's or station's power system. The specific details of this reaction were officially established in the \"\" (1987-1994) series and technical manual; in earlier works it is not clearly defined.\n\nDilithium is a member of the fictional hypersonic series of elements, according to a periodic table graphic seen in episodes of \"Star Trek: The Next Generation\" and \"\" (1993-1999). The dilithium crystal structure is 2(5)6 dilithium 2(:)l diallosilicate 1:9:1 heptoferranide, according to the \"\" (1991).\n\n\n"}
{"id": "18007239", "url": "https://en.wikipedia.org/wiki?curid=18007239", "title": "EOL Săcele Wind Farm", "text": "EOL Săcele Wind Farm\n\nThe EOL Săcele Wind Farm is a proposed wind power project in Caraş-Severin County, Romania. It will have 25 individual wind turbines with a nominal output of around 2 MW which will deliver up to 50 MW of power, enough to power over 33,000 homes, with a capital investment required of approximately US$50 million.\n"}
{"id": "25904129", "url": "https://en.wikipedia.org/wiki?curid=25904129", "title": "Garuda Indonesia Flight 150", "text": "Garuda Indonesia Flight 150\n\nGaruda Indonesia Airways Flight 150 was a scheduled Indonesian domestic passenger flight from Kemayoran Airport, Jakarta to Sultan Mahmud Badaruddin II Airport, Palembang. On September 24, 1975, Flight 150 crashed on approach due to poor weather and fog just 2.5 miles (4 kilometers) from the town of Palembang. The accident killed 25 out of 61 passengers and crew on board and an additional person who was killed on the ground.\n\nThe Fokker F-28 Fellowship registration PK-GVC was built in 1971 and had done over 1000 hours of flying time before the fatal accident.\n\nGaruda Indonesia Airways Flight 150 took off from Kemayoran Airport on a short-haul flight to Sultan Mahmud Badaruddin II Airport with 61 passengers and crew on board. Less than 1 hour after take off, Flight 150 was cleared by the air traffic controllers at Sultan Mahmud Badaruddin II Airport to start their approach to land on runway 28. The flaps and landing gear were down as flight 150 was nearing the airport when fog started shadowing the town and the airport. Flight 150 entered the fog two minutes later. The tail end of the aircraft hit trees and crashed breaking it up into 2 parts. There was no fire when flight 150 crashed. The crash killed 25 people on board and 1 person on the ground. 36 passengers survived the crash and were taken to a local hospital.\n\nAn investigation into the crash found that the visual flight in weather conditions below minimums. Flight 150 was in a downwind leg as the aircraft was on approach to the airport in fog. It is unknown why the air traffic controller did not tell the pilots of flight 150 to execute a missed approach or why the pilots themselves didn't execute a missed approach.\n\n"}
{"id": "57939241", "url": "https://en.wikipedia.org/wiki?curid=57939241", "title": "Great Storm of 1854", "text": "Great Storm of 1854\n\nThe Great Storm of 1854 (also known as the Balaklava Storm) occurred in and around the Black Sea on 14 November 1854. It caused severe damage and caused major disruption to armed forces (and especially naval forces) in the region engaged in the Crimean War.\n\nAt the time of the storm, the British and allied supply fleet was in the Black Sea with all of the supplies for the winter campaign. A strong gale blew up and began battering the fleet. Eyewitness accounts record the flattening of tents and uprooting of trees, and at least 37 ships were either severely damaged or wrecked. Most of the winter supplies were lost, including food, fuel, and winter uniforms. As a result, many men died from hypothermia and disease.\n\nThe storm caused a series of scandals. Funds raised to help the troops disappeared, much of it into the pockets of officers. It was also discovered that the storm had been tracked across Europe prior to its arrival off Crimea, but no warning was sent. As a result of this latter occurrence, several countries quickly launched independent meteorological services. In France, Urbain Le Verrier, Director of the Observatoire de Paris, was commanded to set up a storm warning system; this later developed into an international meteorological service.\n\nThough it is impossible to precisely calculate the strength or speed of the wind, estimates have placed it around force 11 on the Beaufort Scale.\n"}
{"id": "397175", "url": "https://en.wikipedia.org/wiki?curid=397175", "title": "Hexamethylenetetramine", "text": "Hexamethylenetetramine\n\nHexamethylenetetramine or methenamine, also known as urotropin, is a heterocyclic organic compound with the formula (CH)N. This white crystalline compound is highly soluble in water and polar organic solvents. It has a cage-like structure similar to adamantane. It is useful in the synthesis of other chemical compounds, e.g., plastics, pharmaceuticals, rubber additives. It sublimes in vacuum at 280 °C.\n\nHexamethylenetetramine was discovered by Aleksandr Butlerov in 1859.\nIt is prepared industrially by combining formaldehyde and ammonia. The reaction can be conducted in gas phase and in solution.\n\nThe molecule has a symmetric tetrahedral cage-like structure, similar to adamantane, whose four \"corners\" are nitrogen atoms and \"edges\" are methylene bridges. Although the molecular shape defines a cage, no void space is available at the interior for binding other atoms or molecules, unlike crown ethers or larger cryptand structures.\n\nThe molecule behaves like an amine base, undergoing protonation and \"N\"-alkylation (e.g. Quaternium-15).\n\nThe dominant use of hexamethylenetetramine is in the production of powdery or liquid preparations of phenolic resins and phenolic resin moulding compounds, where it is added as a hardening component. These products are used as binders, e.g. in brake and clutch linings, abrasive products, non-woven textiles, formed parts produced by moulding processes, and fireproof materials.\n\nIt has been proposed that hexamethylenetetramine could work as a molecular building block for self-assembled molecular crystals.\n\nAs the mandelic acid salt (generic methenamine mandelate, USP) it is used for the treatment of urinary tract infection. It decomposes at an acidic pH to form formaldehyde and ammonia, and the formaldehyde is bactericidal; the mandelic acid adds to this effect. Urinary acidity is typically ensured by co-administering vitamin C (ascorbic acid) or ammonium chloride. Its use had temporarily been reduced in the late 1990s, due to adverse effects, particularly chemically-induced hemorrhagic cystitis in overdose, but its use has now been re-approved because of the prevalence of antibiotic resistance to more commonly used drugs. This drug is particularly suitable for long-term prophylactic treatment of urinary tract infection, because bacteria do not develop resistance to formaldehyde. It should not be used in the presence of renal insufficiency.\n\nMethenamine in the form of cream and spray is successfully used for treatment of excessive sweating and concomitant odor.\n\nMethenamine silver stains are used for staining in histology, including the following types:\n\nTogether with 1,3,5-trioxane, hexamethylenetetramine is a component of hexamine fuel tablets used by campers, hobbyists, the military and relief organizations for heating camping food or military rations. It burns smokelessly, has a high energy density of 30.0 megajoules per kilogram (MJ/kg), does not liquify while burning, and leaves no ashes, although its fumes are toxic.\n\nStandardized 0.149 g tablets of methenamine (hexamine) are used by fire-protection laboratories as a clean and reproducible fire source to test the flammability of carpets and rugs.\n\nHexamethylene tetramine or hexamine is also used as a food additive as a preservative (INS number 239). It is approved for usage for this purpose in the EU, where it is listed under E number E239, however it is not approved in the USA, Russia, Australia, or New Zealand.\n\nHexamethylenetetramine is a versatile reagent in organic synthesis. It is used in the Duff reaction (formylation of arenes), the Sommelet reaction (converting benzyl halides to aldehydes), and in the Delepine reaction (synthesis of amines from alkyl halides).\n\nHexamethylenetetramine is the base component to produce RDX and, consequently, C-4 as well as Octogen, hexamine dinitrate and HMTD.\n\nHexamethylenetetramine was first introduced into the medical setting in 1899 as a urinary antiseptic. However, it was only used in cases of acidic urine, whereas boric acid was used to treat urinary tract infections with alkaline urine. Scientist De Eds found that there was a direct correlation between the acidity of hexamethylenetetramine's environment and the rate of its decomposition. Therefore, its effectiveness as a drug depended greatly on the acidity of the urine rather than the amount of the drug administered. In an alkaline environment, hexamethylenetetramine was found to be almost completely inactive.\n\nHexamethylenetetramine was also used as a method of treatment for soldiers exposed to phosgene in World War I. Subsequent studies have shown that large doses of hexamethylenetetramine provide some protection if taken before phosgene exposure but none if taken afterwards.\n\nSince 1990 the number of European producers has been declining. The French SNPE factory closed in 1990; in 1993, the production of hexamethylenetetramine in Leuna, Germany ceased; in 1996, the Italian facility of Agrolinz closed down; in 2001, the UK producer Borden closed; in 2006, production at Chemko, Slovak Republic, was closed. Remaining producers include INEOS in Germany, Caldic in the Netherlands, and Hexion in Italy. In the US, Eli Lilly and Company stopped producing methenamine tablets in 2002. In Australia, Hexamine Tablets for fuel are made by Thales Australia Ltd. In México, Hexamine is produced by Abiya.\n"}
{"id": "9019997", "url": "https://en.wikipedia.org/wiki?curid=9019997", "title": "Indigenous (ecology)", "text": "Indigenous (ecology)\n\nIn biogeography, a species is indigenous to a given region or ecosystem if its presence in that region is the result of only natural processes, with no human intervention. The term is equivalent to \"native\" in less scientific usage. Every wild organism (as opposed to a domesticated organism) has its own natural range of distribution in which it is regarded as indigenous. Outside this native range, a species may be introduced by human activity, either intentionally or unintentionally; it is then referred to as an \"introduced species\" within the regions where it was anthropogenically introduced.\n\nThe notion of 'indigenous' is of necessity a blurred concept, and is clearly a function of both time and political boundaries. Seen over long periods of time, plants take part in the constant movement of tectonic plates - species appear and may flourish, endure or become extinct, but their distribution is never static or confined to a particular geographic location.\n\nAn indigenous species is not necessarily endemic. In biology and ecology, endemic means \"exclusively\" native to the biota of a specific place. An indigenous species may occur in areas other than the one under consideration.\n\nThe terms \"endemic\" and \"indigenous\" do not imply that an organism necessarily originated or evolved from where it is found.\n\n\n"}
{"id": "14175843", "url": "https://en.wikipedia.org/wiki?curid=14175843", "title": "Jadwiga Łopata", "text": "Jadwiga Łopata\n\nJadwiga Łopata, is an organic farmer living near Cracow, Poland. She was awarded the Goldman Environmental Prize in 2002, for her works on rural protection. She is co-founder and co-director of the \"International Coalition to Protect the Polish Countryside\" (ICPPC).\n\n"}
{"id": "4421561", "url": "https://en.wikipedia.org/wiki?curid=4421561", "title": "Linolenic acid", "text": "Linolenic acid\n\nLinolenic acid is a type of fatty acid. It can refer to either of two octadecatrienoic acids (i.e. with an 18-carbon chain and three double bonds, which are found in the \"cis\" configuration), or a mixture of the two. Linolenate (in the form of triglyceride esters of linolenic acid) is often found in vegetable oils; traditionally, such fatty acylates are reported as the fatty acids:\n\n"}
{"id": "3188110", "url": "https://en.wikipedia.org/wiki?curid=3188110", "title": "List of tornado-related deaths at schools", "text": "List of tornado-related deaths at schools\n\nThese are all known tornadoes resulting in student deaths at primary and secondary schools in the United States from 1865 to 2015. For the deadliest tornado incidents, only fires/explosions and bombings have killed more students.\n\n\nFrom 1884 to 2007, there were 46 tornadoes with school fatalities in the United States. These tornadoes killed 286 (not including the 9 from the probable downburst in New York state). Tornado warnings began being issued in 1950 (and tornado watches in late 1952); and there is a very sharp decrease in number of killer tornado events at schools after this time, as well as a large decrease in death tolls from tornadoes overall. There were 40 tornadoes with deaths at schools (234 deaths) before 1953 and 6 events (52 deaths) after that year (not including the probable downburst in New York). Two high fatality events after 1953 occurred in Mississippi (23 in 1955) and Illinois (13 in 1967); accounting for 82% of 1952-2006 deaths, both from violent class tornadoes.\n\nMore tornadoes with deaths in schools have occurred in the Southeastern United States—23 events or over half the national total—than any other region. Four of the top ten death toll events occurred in the Southeast. Relatively few school fatality tornado events have occurred in the area with the highest frequency of strong tornadoes, the Great Plains (Tornado Alley); only a single event occurred after warnings began being issued. This is probably chiefly due to three reasons: the low population density, greater tornado awareness (and better visibility affording more warning), and the time of year and of day that most tornadoes strike the Great Plains.\nThe state with the most tornado deaths throughout history is Illinois, with 90. The largest school death toll from a tornado was 69 during the Tri-State Tornado, which also struck Illinois and significantly raised that state's death toll. The greatest death toll at a single school also occurred during the Tri-State tornado, when it killed 33 at a school in De Soto, also in Illinois. This tornado also injured hundreds more at schools, and killed many students returning home from schools. Additionally, three of the top ten events by death toll, and four if separate schools of the same tornado are counted (33 in De Soto and 25 in Murphysboro again from the Tri-State Tornado), have occurred in Illinois.\n\nThe state with the highest number of tornadoes with deaths at schools is Alabama at 8 events. Illinois is second with 6 tornadoes. Missouri and Oklahoma are tied for third with 5 tornadoes. Fifth is Georgia with 3 tornadoes. Sixth are Texas, Tennessee, Indiana, Nebraska, Mississippi, and Arkansas, each with 2 events. One school fatality tornado event has occurred in Ohio, Louisiana, Iowa, Colorado, Kansas, South Carolina, Maryland, Virginia, North Carolina, Minnesota, and Florida (the probable downburst in New York is not included).\n\n\n\n"}
{"id": "2215849", "url": "https://en.wikipedia.org/wiki?curid=2215849", "title": "Magnetogravity wave", "text": "Magnetogravity wave\n\nA magnetogravity wave is a type of plasma wave. A magnetogravity wave is an acoustic gravity wave which is associated with fluctuations in the background magnetic field. In this context, gravity wave refers to a classical fluid wave, and is completely unrelated to the relativistic gravitational wave.\n\nMagnetogravity waves are found in the corona of the Sun.\n\n"}
{"id": "3178175", "url": "https://en.wikipedia.org/wiki?curid=3178175", "title": "Magnetotellurics", "text": "Magnetotellurics\n\nMagnetotellurics (MT) is an electromagnetic geophysical method for inferring the earth's subsurface electrical conductivity from measurements of natural geomagnetic and geoelectric field variation at the Earth's surface. Investigation depth ranges from 300 m below ground by recording higher frequencies down to 10,000 m or deeper with long-period soundings. Proposed in Japan in the 1940s, and France and the USSR during the early 1950s, MT is now an international academic discipline and is used in exploration surveys around the world. Commercial uses include hydrocarbon (oil and gas) exploration, geothermal exploration, carbon sequestration, mining exploration, as well as hydrocarbon and groundwater monitoring. Research applications include experimentation to further develop the MT technique, long-period deep crustal exploration, deep mantle probing, and earthquake precursor prediction research.\n\nThe magnetotelluric technique was introduced independently by Japanese scientists in the 1940s (Hirayama, Rikitake), and Russian geophysicist Andrey Nikolayevich Tikhonov in 1950 and the French geophysicist Louis Cagniard With advances in instrumentation, processing and modelling, MT has become one of the most important tools in deep Earth research.\n\nSince first being created in the 1950s, magnetotelluric sensors, receivers and data processing techniques have followed the general trends in electronics, becoming less expensive and more capable with each generation. Major advances in MT instrumentation and technique include the shift from analog to digital hardware, the advent of remote referencing, GPS time-based synchronization, and 3D data acquisition and processing.\n\nFor hydrocarbon exploration, MT is mainly used as a complement to the primary technique of reflection seismology exploration. While seismic imaging is able to image subsurface structure, it cannot detect the changes in resistivity associated with hydrocarbons and hydrocarbon-bearing formations. MT does detect resistivity variations in subsurface structures, which can differentiate between structures bearing hydrocarbons and those that do not.\n\nAt a basic level of interpretation, resistivity is correlated with different rock types. High-velocity layers are typically highly resistive, whereas sediments – porous and permeable – are typically much less resistive. While high-velocity layers are an acoustic barrier and make seismic ineffective, their electrical resistivity means the magnetic signal passes through almost unimpeded. This allows MT to see deep beneath these acoustic barrier layers, complementing the seismic data and assisting interpretation. 3-D MT survey results in Uzbekistan (32 x 32 grid of soundings) have guided further seismic mapping of a large known gas-bearing formation with complex subsurface geology.\n\nChina National Petroleum Corporation (CNPC) and Nord-West Ltd use onshore MT more than any other oil company in the world, conducting thousands of MT soundings for hydrocarbon exploration and mapping throughout the globe.\n\nMT is used for various base metals (nickel and precious metals exploration, as well as for kimberlite) mapping.\n\nINCO's 1991 proof-of-concept study in Sudbury, Ontario, Canada sensed a 1750-meter-deep nickel deposit. Falconbridge followed with a feasibility study in 1996 that accurately located two Ni-Cu mineralized zones at about 800 m and 1350 m depth. Since then, both major and junior mining companies are increasingly using MT and audio-magnetotellurics (AMT) for both brownfields and greenfields exploration. Significant MT mapping work has been done on areas of the Canadian Shield.\n\nDiamond exploration, by detecting kimberlites, is also a proven application.\n\nMT geothermal exploration measurements allow detection of resistivity anomalies associated with productive geothermal structures, including faults and the presence of a cap rock, and allow for estimation of geothermal reservoir temperatures at various depths. Dozens of MT geothermal exploration surveys have been completed in Japan and the Philippines since the early 1980s, helping to identify several hundred megawatts of renewable power at places such as the Hatchobaru plant on Kyushu and the Togonang plant on Leyte. Geothermal exploration with MT has also been done in the United States, Iceland, New Zealand, Hungary, China, Ethiopia, Indonesia, Peru, Australia, and India.\n\nMT is also used for groundwater exploration and mapping, hydrocarbon reservoir monitoring, deep investigation (100 km) of the electrical properties of the bedrock for high-voltage direct current (HVDC) transmission systems, carbon dioxide sequestration, and other environmental engineering applications (e.g. nuclear blast site monitoring and nuclear waste disposal site monitoring).\n\nMT has been used to investigate the distribution of silicate melts in the Earth's mantle and crust; large investigations have focused on the continental US (National Science Foundation EarthScope MT Program), the East Pacific Rise and the Tibetan Plateau. Other research work aims to better understand the plate-tectonic processes in the highly complex three-dimensional region formed by the collision of the African and European plates.\n\nFluctuations in the MT signal may be able to predict the onset of seismic events. Stationary MT monitoring systems have been installed in Japan since April 1996, providing a continuous recording of MT signals at the Wakuya Station (previously at the Mizusawa Geodetic Observatory) and the Esashi Station of the Geographical Survey Institute of Japan (GSIJ). These stations measure fluctuations in the Earth's electromagnetic field that correspond with seismic activity. The raw geophysical time-series data from these monitoring stations is freely available to the scientific community, enabling further study of the interaction between electromagnetic events and earthquake activity. The MT time series data from the GSIJ earthquake monitoring stations is available online at https://web.archive.org/web/20100225080738/http://vldb.gsi.go.jp/sokuchi/geomag/menu_03/mt_data-e.html\n\nAdditional MT earthquake precursor monitoring stations in Japan are located in Kagoshima, in Sawauchi, and on Shikoku. Similar stations are also deployed in Taiwan on Penghu Island, as well as in the Fushan Reserve on the island of Taiwan proper.\n\nPOLARIS is a Canadian research program investigating the structure and dynamics of the Earth's lithosphere and the prediction of earthquake ground motion.\n\nSolar energy and lightning cause natural variations in the earth's magnetic field, inducing electric currents (known as telluric currents) under the Earth's surface.\"' A subsurface resistivity model is then created using this tensor.\n\nDifferent rocks, sediments and geological structures have a wide range of different electrical conductivities. Measuring electrical resistivity allows different materials and structures to be distinguished from one another and can improve knowledge of tectonic processes and geologic structures.\n\nThe Earth's naturally varying electric and magnetic fields are measured over a wide range of magnetotelluric frequencies from 10,000 Hz to 0.0001 Hz (10,000 s). These fields are due to electric currents flowing in the Earth and the magnetic fields that induce these currents. The magnetic fields are produced mainly by the interaction between the solar wind and the magnetosphere. In addition, worldwide thunderstorm activity causes magnetic fields at frequencies above 1 Hz. Combined, these natural phenomena create strong MT source signals over the entire frequency spectrum.\n\nThe ratio of the electric field to magnetic field provides simple information about subsurface conductivity. Because the skin effect phenomenon affects the electromagnetic fields, the ratio at higher frequency ranges gives information on the shallow Earth, whereas deeper information is provided by the low-frequency range. The ratio is usually represented as both apparent resistivity as a function of frequency and phase as a function of frequency.\n\nMT measurements can investigate depths from about 300 m down to hundreds of kilometers, though investigations in the range of 500 m to 10,000 m are typical. Greater depth requires measuring lower frequencies, which in turn requires longer recording times. Very deep, very long-period measurements (mid-crust through upper mantle depths), may require recordings of several days to weeks or more to obtain satisfactory data quality.\n\nHorizontal resolution of MT mainly depends on the distance between sounding locations- closer sounding locations increase the horizontal resolution. Continuous profiling (known as Emap) has been used, with only meters between the edges of each telluric dipole.\n\nVertical resolution of MT mainly depends on the frequency being measured, as lower frequencies have greater depths of penetration. Accordingly, vertical resolution decreases as depth of investigation increases.\n\nMagnetic fields in the frequency range of 1 Hz to approximately 20 kHz are part of the audio-magnetotelluric (AMT) range. These are parallel to the Earth surface and move towards the Earth's centre. This large frequency band allows for a range of depth penetration from several metres to several kilometres below the Earth's surface. Due to the nature of magnetotelluric source, the waves generally fluctuate in amplitude height. Long recording times are needed to ascertain usable reading due to the fluctuations and the low signal strength. Generally, the signal is weak between 1 and 5 kHz, which is a crucial range in detecting the top 100 m of geology. The magnetotelluric method is also used in marine environments for hydrocarbon exploration and lithospheric studies. Due to the screening effect of the electrically conductive sea water, a usable upper limit of the spectrum is around 1 Hz.\n\nTwo-dimensional surveys consist of a longitudinal profile of MT soundings over the area of interest, providing two-dimensional \"slices\" of subsurface resistivity.\n\nThree-dimensional surveys consist of a loose grid pattern of MT soundings over the area of interest, providing a more sophisticated three-dimensional model of subsurface resistivity.\n\nAudio-magnetotellurics (AMT) is a higher-frequency magnetotelluric technique for shallower investigations. While AMT has less depth penetration than MT, AMT measurements often take only about one hour to perform (but deep AMT measurements during low-signal strength periods may take up to 24 hours) and use smaller and lighter magnetic sensors. Transient AMT is an AMT variant that records only temporarily during periods of more intense natural signal (transient impulses), improving signal-to-noise-ratio at the expense of strong linear polarization.\n\nCSEM Controlled source electro-magnetic is a deep-water offshore variant of controlled source audio magnetotellurics; CSEM is the name used in the offshore oil and gas industry.\n\nOnshore CSEM / CSAMT may be effective where electromagnetic cultural noise (e.g. power lines, electric fences) present interference problems for natural-source geophysical methods. An extensive grounded wire (2 km or more) has currents at a range of frequencies (0.1 Hz to 100 kHz) passed through it. The electric field parallel to the source and the magnetic field which is at right angles are measured. The resistivity is then calculated, and the lower the resistivity, the more likely there is a conductive target (graphite, nickel ore or iron ore). CSAMT is also known in the oil and gas industry as Onshore Controlled Source ElectroMagnetics (Onshore CSEM).\n\nAn offshore variant of MT, the marine magnetotelluric (MMT) method, uses instruments and sensors in pressure housings deployed by ship into shallow coastal areas where water is less than 300 m deep. A derivative of MMT is offshore single-channel measurement of the vertical magnetic field only (the Hz, or \"tipper\"), which eliminates the need for telluric measurements and horizontal magnetic measurements. While the theory is sound and many case histories exist, no commercial systems are available by KMS Technologies. Furthermore, any such system would require a solution providing for the precise orientation and stabilization of the magnetic sensor.\n\nMT exploration surveys are done to acquire resistivity data which can be interpreted to create a model of the subsurface. Data is acquired at each sounding location for a period of time (overnight soundings are common), with physical spacing between soundings dependent on the target size and geometry, local terrain constraints and financial cost. Reconnaissance surveys can have spacings of several kilometres, while more detailed work can have 200 m spacings, or even adjacent soundings (dipole-to-dipole). \n\nThe HSE impact of MT exploration is relatively low because of light-weight equipment, natural signal sources, and reduced hazards compared to other types of exploration (e.g. no drills, no explosives, and no high currents).\n\nRemote Reference is an MT technique used to account for cultural electrical noise by acquiring simultaneous data at more than one MT station. This greatly improves data quality, and may allow acquisition in areas where the natural MT signal is difficult to detect because of man-made EM interference.\n\nA typical full suite of MT equipment (for a \"five component\" sounding) consists of a receiver instrument with five sensors: three magnetic sensors (typically induction coil sensors), and two telluric (electric) sensors. For long-period MT (frequencies below approximately 1–10 Hz), the three discrete magnetic field sensors can typically be replaced with a single compact triaxial fluxgate magnetometer. In many situations, only the telluric sensors will be used, and magnetic data borrowed from other nearby soundings to reduce acquisition costs.\n\nA complete five-component set of MT equipment can be backpack-carried by a small field team (2 to 4 persons) or carried by a light helicopter (such as the MD Helicopters MD 500), allowing deployment in remote and rugged areas. Most MT equipment is capable of reliable operation over a wide range of environmental conditions, with ratings of typically −25 °C to +55 °C, from dry desert to high-humidity (condensing) and temporary full immersion.\n\nPost-acquisition processing is required to transform raw time-series data into frequency-based inversions. The resulting output of the processing program is used as the input for subsequent interpretation. Processing may include the use of remote reference data or local data only.\n\nProcessed MT data is modelled using various techniques to create a subsurface resistivity map, with lower frequencies generally corresponding to greater depth below ground. Anomalies such as faults, hydrocarbons, and conductive mineralization appear as areas of higher or lower resistivity from surrounding structures. For interpretation(inversion) of magnetotelluric data a number of software are used (WinGlink, Geotools MT, ZondMT2D).\n\nMT instrumentation design and construction is a specialized international activity, with only a small number of companies and scientific organizations having the necessary expertise and technology. Three companies supply most of the commercial-use world market: one in the United States, Zonge International, Inc.), one in Canada; Phoenix Geophysics, Ltd.), and one in Germany (Metronix Messgeraete und Elektronik GmbH). High power CSEM array systems above 100 KVA for hydrocarbon and geothermal expploration are manufactured by KMS Technologies, USA. Government agencies and smaller companies producing MT instrumentation for internal use include Vega Geophysics, Ltd. in Russia and the Russian Academy of Sciences (SPbF IZMIRAN); and the National Space Research Institute of Ukraine. Geometrics, Inc. of San Jose, California USA produces a high-frequency, hybrid-source AMT system combining both natural fields and a high-frequency (1 to 70 kHz) transmitter, as well as a multi-channel distributed network CSAMT instrument.\n\n\n"}
{"id": "3878707", "url": "https://en.wikipedia.org/wiki?curid=3878707", "title": "Maurice Bridgeman", "text": "Maurice Bridgeman\n\nHon. Sir Maurice Richard Bridgeman (26 January 1904 – 18 June 1980) was a British businessman and civil servant.\n\nBridgeman was the third son of the William Bridgeman, 1st Viscount Bridgeman and Caroline, Viscountess Bridgeman, DBE (née Parker), and younger brother of the 2nd Viscount. He was educated at Eton College, Berkshire and at Trinity College, Cambridge.\n\nIn 1939, Bridgeman was petroleum advisor to the Ministry of Economic Warfare, and, from 1944–46, as Principal Assistant Secretary to the Ministry of Fuel and Power. From 1960-69, he was chairman of British Petroleum. Bridgeman was honoured as a Commander of the Order of the British Empire in 1946 and later as a Knight Commander (KBE) in 1964. He was also a Knight of the Most Venerable Order of the Hospital of St John of Jerusalem.\n\nBridgeman married Diana Mary Erica Wilson, daughter of Humphrey Minto Wilson, on 23 February 1933. They had four daughters (Erica Jane Bridgeman, b. 20 April 1934; Teresa Anne Bridgeman, b. 25 October 1937; Elizabeth Caroline Bridgeman, b. 15 March 1944; Rachel Diana Bridgeman, b. 9 March 1947).\n\nSir Maurice Richard Bridgeman died on 18 June 1980, aged 76.\n"}
{"id": "43039", "url": "https://en.wikipedia.org/wiki?curid=43039", "title": "Omphalos", "text": "Omphalos\n\nAn omphalos is a religious stone artifact, or baetylus. In Ancient Greek, the word () means \"navel\". In Greek lore, Zeus sent two eagles across the world to meet at its center, the \"navel\" of the world. Among the Ancient Greeks, it was a widespread belief that Delphi was the center of the world. According to the myths regarding the founding of the Delphic Oracle, Zeus, in his attempt to locate the center of the earth, launched two eagles from the two ends of the world, and the eagles, starting simultaneously and flying at equal speed, crossed their paths above the area of Delphi, and so was the place where Zeus placed the stone. Omphalos is also the name of the stone given to Cronus. In the ancient world of the Mediterranean, it was a powerful religious symbol.\n\"Omphalos Syndrome\" refers to the belief that a place of geopolitical power and currency is the most important place in the world.\n\nThe omphalos was not only an object of Hellenic religious symbolism and world centrality; it was also considered an object of power. Its symbolic references included the uterus, the phallus, and a cup of red wine representing royal blood lines.\n\nMost accounts locate the Delphi omphalos in the adyton (sacred part of the temple) near the Pythia (oracle). The stone sculpture itself (which may be a copy), has a carving of a knotted net covering its surface, and a hollow center, widening towards the base. The omphalos represents the stone which Rhea wrapped in swaddling clothes, pretending it was Zeus, in order to deceive Cronus. (Cronus was the father who swallowed his children so as to prevent them from usurping him as he had deposed his own father, Uranus).\n\nOmphalos stones were believed to allow direct communication with the gods. Holland (1933) suggested that the stone was hollow to allow intoxicating vapours breathed by the Oracle to channel through it. Erwin Rohde wrote that the Python at Delphi was an earth spirit, who was conquered by Apollo and buried under the Omphalos. However, understanding of the use of the omphalos is uncertain due to destruction of the site by Theodosius I and Arcadius in the 4th century CE.\n\nThe omphalos at the Church of the Holy Sepulchre, Jerusalem, represents, in Christian mediaeval tradition, the \"navel of the world\" (the spiritual and cosmological centre of the world). Jewish tradition held that God revealed himself to His people through the Ark of the Covenant in the Temple in Jerusalem, which rested on the Foundation stone marking the centre of the world. This tradition may have stemmed from the similar one at Delphi. The omphalos has a collection box chained next to it. (See picture)\n\n\"Omphalos\" is a public art sculpture by Dimitri Hadzi formerly located in the Harvard Square, Cambridge, Massachusetts under the Arts on the Line program. , the sculpture has been deinstalled; it will be relocated to Rockport, Massachusetts.\n\nIn literature, the word \"omphalos\" has held various meanings but usually refers to the stone at Delphi. Authors who have used the term include: Homer, Pausanias, D.H. Lawrence, James Joyce, Jacques Derrida, and Sandy Hingston . For example, Joyce uses the term in the novel, \"Ulysses\":\n\n\n\n"}
{"id": "2722096", "url": "https://en.wikipedia.org/wiki?curid=2722096", "title": "Osmotrophy", "text": "Osmotrophy\n\nOsmotrophy is the uptake of dissolved organic compounds by osmosis for nutrition. Organisms that use osmotrophy are osmotrophs. Some mixotrophic microorganisms use osmotrophy to derive energy.\nThe organisms that used osmotrophy are known to be an osmotrophs which are usually found in protists and fungi although exclusively osmotrophic feeding in contemporary ecosystems are restricted to microscopic bacteria thereby used the process of osmosis for the movement of food although some macroscopic animals like molluscs, sponges, corals, brachiopods and echinoderms used osmotrophic feeding as a supplemental food source.\n\nOsmotrophy is a means of gathering nutrients in microscopic organisms that relies on cellular surface area to ensure that proper diffusion occurs throughout the cell. When organisms increase in size, the surface area per volume ratio drops and osmotrophy becomes insufficient to meet nutrient demands.\n\nIn stagnant waters photoautotrophs have a relative advantage over heterotrophic osmotrophs since the flux of photons as an energy source are not hindered at low temperatures, thus it depend on diffusion for mass acquisition through Brownian diffusion.\n\nFluid motion is important for osmotrophs because asymptotic reactions occurs in the absence of fluid motion. Movement brings cell that correspond to the highest gradients though diffusional core is safe on average concentrations.\n\nMany types of basal eukaryotes, particularly Fungi and Protista, are osmotrophic.\n\nExample of these types of organisms are tapeworms and \"Trypanasoma\".\n\n\n"}
{"id": "19480187", "url": "https://en.wikipedia.org/wiki?curid=19480187", "title": "PTFE structured packing", "text": "PTFE structured packing\n\nPTFE Structured Packing refers to a range of specially designed materials for use in absorption and distillation columns and chemical reactors. PTFE Structured packing typically consist of thin corrugated PTFE Sheets arranged in a way that they force fluids to take complicated paths through the column, thereby creating a large surface area for contact between different phases.\n"}
{"id": "36255668", "url": "https://en.wikipedia.org/wiki?curid=36255668", "title": "Panguite", "text": "Panguite\n\nPanguite is a type of titanium oxide mineral first discovered as an inclusion within the Allende meteorite, and first described in 2012.\n\nThe hitherto unknown meteorite mineral was named for the ancient Chinese god Pan Gu, the creator of the world through the separation of yin (earth) from yang (sky).\n\nThe mineral’s chemical formula is (Ti,Sc,Al,Mg,Zr,Ca)O. The elements found in it are titanium, scandium, aluminium, magnesium, zirconium, calcium, and oxygen. Samples from the meteorite include some which are zirconium rich. The mineral was found in conjunction with the already identified mineral davisite, within an olivine aggregate.\n\nPanguite is in a class of refractory minerals that formed under the high temperatures and extremely varied pressures present in the early solar system, up to 4.5 billion years ago. This makes panguite one of the oldest minerals in the Solar System. Zirconium is a key element in determining conditions prior to and during the Solar System’s formation.\n\nChi Ma, director of the Geological and Planetary Sciences division’s Analytical Facility at the California Institute of Technology was the lead author of its first peer-reviewed article, published in \"American Mineralogist\". Ma has been leading a nano mineralogy investigation, since 2007, of primitive meteorites, including the well studied Allende meteorite. The mineral was first described in a paper submitted to the 42nd annual Lunar and Planetary Science Conference in 2011.\n"}
{"id": "3508737", "url": "https://en.wikipedia.org/wiki?curid=3508737", "title": "Plutonium tetrafluoride", "text": "Plutonium tetrafluoride\n\nPlutonium(IV) fluoride is a chemical compound with the formula (PuF). Like all plutonium compounds, it is subject to control under the Nuclear Non-Proliferation Treaty.\n\nMetallic plutonium is produced by reacting plutonium tetrafluoride with barium, calcium, or lithium at 1200 °C:\n"}
{"id": "37273221", "url": "https://en.wikipedia.org/wiki?curid=37273221", "title": "PowerBuoy", "text": "PowerBuoy\n\nPowerBuoy is a power station for generating electrical energy from wave power. It is a point absorber or buoy, currently in-use or in-planning at 9 locations around the world, but primarily within Australia and the United States. \n\nThe PowerBuoy generates power using a hydroelectric turbine. PowerBuoys can be connected to the electrical grid by power transmission cables or can operate autonomously in a deep water environment. PowerBuoys are manufactured by Ocean Power Technologies (OPT) in Pennington, New Jersey.\n\nThe rising and falling of the waves offshore causes the buoy to move freely up and down. The resultant mechanical stroking drives an electrical generator. The generated wave power is transmitted ashore via an underwater power cable.\n\nAn OPT power station benefits from a deep water emplacement and has a very low \"surface profile\", meaning it is barely visible from shore. They also have a small horizontal footprint and have been designed to be scalable. As such, they are ideal for wave farms.\n\nOPT power stations have been designed for extreme wave conditions. Sensors on the PowerBuoy continuously monitor the performance of the various subsystems and surrounding ocean environment. Data is transmitted to shore in real time. In the event of very large oncoming waves, the system automatically locks up and ceases power production. When the wave heights return to normal, the system unlocks and recommences energy conversion and transmission of the electrical power ashore.\n\n"}
{"id": "56071575", "url": "https://en.wikipedia.org/wiki?curid=56071575", "title": "Program on Forests", "text": "Program on Forests\n\nThe Program on Forests (PROFOR) is multi-donor partnership on forests and poverty reduction led by the World Bank.\n\nPROFOR has released several studies on forests and livelihoods:\nThe program is funding the implementation of a project in Jamaica “assessing the economic valuation of coastal protection services provided by mangroves.”\n"}
{"id": "9529253", "url": "https://en.wikipedia.org/wiki?curid=9529253", "title": "Regenerative heat exchanger", "text": "Regenerative heat exchanger\n\nA regenerative heat exchanger, or more commonly a regenerator, is a type of heat exchanger where heat from the hot fluid is intermittently stored in a thermal storage medium before it is transferred to the cold fluid. To accomplish this the hot fluid is brought into contact with the heat storage medium, then the fluid is displaced with the cold fluid, which absorbs the heat.\n\nIn regenerative heat exchangers, the fluid on either side of the heat exchanger can be the same fluid. The fluid may go through an external processing step, and then it is flowed back through the heat exchanger in the opposite direction for further processing. Usually the application will use this process cyclically or repetitively.\n\nRegenerative heating was one of the most important technologies developed during the Industrial Revolution when it was used in the hot blast process on blast furnaces, It was later used in glass and steel making, to increase the efficiency of open hearth furnaces, and in high pressure boilers and chemical and other applications, where it continues to be important today.\n\nThe first regenerator was invented by Rev. Robert Stirling in 1816, and is commonly found as a component of his Stirling engine. The simplest Stirlings, and most models, use a less efficient but simpler to construct, \"displacer\" instead.\n\nLater applications included the blast furnace process known as hot blast and the Open hearth furnace also called Siemens regenerative furnace (which was used for making glass), where the hot exhaust gases from combustion are passed through firebrick regenerative chambers, which are thus heated. The flow is then reversed, so that the heated bricks preheat the fuel.\n\nEdward Alfred Cowper applied the regeneration principle to blast furnaces, in the form of the \"Cowper stove\", patented in 1857. This is almost invariably used with blast furnaces to this day.\n\nRegenerators exchange heat from one process fluid to an intermediate solid heat storage medium, then that medium exhanges heat with a second process fluid flow. The two flows are either separated in time, alternately circulating through the storage medium, or are separated in space and the heat storage medium is moved between the two flows.\n\nIn rotary regenerators the heat storage \"matrix\" in the form of a wheel or drum,that rotates continuously through two counter-flowing streams of fluid. In this way, the two streams are mostly separated. Only one stream flows through each section of the matrix at a time; however, over the course of a rotation, both streams eventually flow through all sections of the matrix in succession. The heat storage medium can be a relatively fine-grained set of metal plates or wire mesh, made of some resistant alloy or coated to resist chemical attack by the process fluids, or made of ceramics in high temperature applications. A large amount of heat transfer area can be provided in each unit volume of the rotary regenerator, compared to a shell-and-tube heat exchanger - up to 1000 square feet of surface can be contained in each cubic foot of regenerator matrix, compared to about 30 square feet in each cubic foot of a shell-and-tube exchanger.\n\nEach portion of the matrix will be nearly isothermal, since the rotation is perpendicular to both the temperature gradient and flow direction, and not through them. The two fluid streams flow counter-current. The fluid temperatures vary across the flow area; however the local stream temperatures are not a function of time. The seals between the two streams are not perfect, so some cross contamination will occur. The allowable pressure level of a roary regenerator is relatively low, compared to heat exchangers.\n\nIn a fixed matrix regenerator, a single fluid stream has cyclical, reversible flow; it is said to flow \"counter-current\". This regenerator may be part of a valveless system, such as a Stirling engine. In another configuration, the fluid is ducted through valves to different matrices in alternate operating periods resulting in outlet temperatures that vary with time. For example, a blast furnace may have several \"stoves\" or \"checkers\" full of refractory fire brick. The hot gas from the furnace is ducted through the brickwork for some interval, say one hour, until the brick reaches a high temperature. Valves then operate and switch the cold intake air through the brick, recovering the heat for use in the furnace. Practical installations will have multiple stoves and arrangements of valves to gradually transfer flow between a \"hot\" stove and an adjacent \"cold\" stove, so that the variations in the outlet air temperature are reduced.\n\nAnother type of regenerator is called a micro scale regenerative heat exchanger. It has a multilayer grating structure in which each layer is offset from the adjacent layer by half a cell which has an opening along both axes perpendicular to the flow axis. Each layer is a composite structure of two sublayers, one of a high thermal conductivity material and another of a low thermal conductivity material. When a hot fluid flows through the cell, heat from the fluid is transferred to the cell wells, and stored there. When the fluid flow reverses direction, heat is transferred from the cell walls back to the fluid.\n\nA third type of regenerator is called a \"Rothemuhle\" regenerator. This type has a fixed matrix in a disk shape, and streams of fluid are ducted through rotating hoods. The \"Rothemuhle\" regenerator is used as an air preheater in some power generating plants. The thermal design of this regenerator is the same as of other types of regenerators. \n\nWe use our nose and throat as a regenerative heat exchanger when we breathe. The cooler air coming in is warmed, so that it reaches the lungs as warm air. On the way back out, this warmed air deposits much of its heat back onto the sides of the nasal passages, so that these passages are then ready to warm the next batch of air coming in. Some animals, including humans, have curled sheets of bone inside the nose called nasal turbinates to increase the surface area for heat exchange.\n\nRegenerative heat exchangers are made up of materials with high volumetric heat capacity and low thermal conductivity in the longitudinal (flow) direction. At cryogenic (very low) temperatures around 20 K, the specific heat of metals is low, and so a regenerator must be larger for a given heat load.\n\nThe advantages of a regenerator over a recuperating (counter-flowing) heat exchanger is that it has a much higher surface area for a given volume, which provides a reduced exchanger volume for a given energy density, effectiveness and pressure drop. This makes a regenerator more economical in terms of materials and manufacturing, compared to an equivalent recuperator.\n\nThe design of inlet and outlet headers used to distribute hot and cold fluids in the matrix is much simpler in counter flow regenerators than recuperators. The reason behind this is that both streams flow in different sections for a rotary regenerator and one fluid enters and leaves one matrix at a time in a fixed-matrix regenerator. Furthermore, flow sectors for hot and cold fluids in rotary regenerators can be designed to optimize pressure drop in the fluids. The matrix surfaces of regenerators also have self-cleaning characteristics, reducing fluid-side fouling and corrosion. Finally properties such as small surface density and counter-flow arrangement of regenerators make it ideal for gas-gas heat exchange applications requiring effectiveness exceeding 85%. The heat transfer coefficient is much lower for gases than for liquids, thus the enormous surface area in a regenerator greatly increases heat transfer.\n\nThe major disadvantage of rotary and fixed-matrix regenerators is that there is always some mixing of the fluid streams, and they can not be completely separated. There is an unavoidable carryover of a small fraction of one fluid stream into the other. In the rotary regenerator, the carryover fluid is trapped inside the radial seal and in the matrix, and in a fixed-matrix regenerator, the carryover fluid is the fluid that remains in the void volume of the matrix. This small fraction will mix with the other stream in the following half-cycle. Therefore, rotary and fixed-matrix regenerators are only used when it is acceptable for the two fluid streams to be mixed. Mixed flow is common for gas-to-gas heat and/or energy transfer applications, and less common in liquid or phase-changing fluids since fluid contamination is often prohibited with liquid flows.\n\nThe constant heating and cooling that takes place in regenerative heat exchangers puts a lot of stress on the components of the heat exchanger, which can cause cracking or breakdown of materials.\n\n\n"}
{"id": "44789221", "url": "https://en.wikipedia.org/wiki?curid=44789221", "title": "Reventazón Dam", "text": "Reventazón Dam\n\nThe Reventazón Dam is a concrete-face rock-fill dam on the Reventazón River about southwest of Siquirres in Limón Province, Costa Rica. It was inaugurated on 16 September 2016, and its primary purpose is the production of hydroelectric power. The 1.4 billion USD project and largest power station in the country has an installed capacity of 305.5 MW and is expected to provide power for 525,000 homes. Construction on the dam began in 2009. At a height of and with a structural volume of , it is the largest dam in Central America. To produce electricity, water from the reservoir is diverted about to the northeast where it reaches the power station along the Reventazón River.\nDue to its environmental features, like offset habitats and migration corridors for jaguars and many other species, the project could be a model for other future hydroelectric power plants.\n\n"}
{"id": "559831", "url": "https://en.wikipedia.org/wiki?curid=559831", "title": "Rudnaya Pristan", "text": "Rudnaya Pristan\n\nRudnaya Pristan (, lit. \"Ore Wharf\") is a village (\"selo\") located at the mouth of the Rudnaya River, on the Pacific coast of Primorsky Krai. It is situated 35 km east of Dalnegorsk (also in Primorsky Krai) and approximately 514 km north of Vladivostok. Its population was 2,107 in 2010, 2,389 in 2002, and 2,947 in 1989.\n\nLead smelting has been the town's primary industry since a plant was built there in 1930. The plant has provided steady employment for most of the area's families since that time, but at enormous cost to both the health of the residents and the local environment. The residents suffer from many health problems, including an inordinately high rate of cancer, and the soil has become heavily contaminated with lead-related by-products. The Blacksmith Institute consequently declared Rudnaya Pristan, along with Dalnegorsk, one of the ten worst polluted places on earth, although Anatoly Lebedev, leader of the ecological NGO \"BROK\", disputes this inclusion.\n\nDespite its coastal location, Rudnaya Pristan's harbor has remained largely undeveloped, and its climate is harsh, dominated in winter by the vast Siberian high-pressure system and in summer by remnants of the East Asian monsoon. This combination results in very cold, dry winters with generally high winds, and muggy summers that provide ideal conditions for the breeding of mosquitoes.\n"}
{"id": "2642022", "url": "https://en.wikipedia.org/wiki?curid=2642022", "title": "Sharpening stone", "text": "Sharpening stone\n\nSharpening stones, water stones or whetstones are used to sharpen the edges of steel tools and implements through grinding and honing.\n\nExamples of items that can be sharpened with a sharpening stone include scissors, scythes, knives, razors, and tools such as chisels, hand scrapers, and plane blades.\n\nSharpening stones come in a wide range of shapes, sizes, and material compositions. Stones may be flat, for working flat edges, or shaped for more complex edges, such as those associated with some wood carving or woodturning tools. They may be composed of natural quarried material, or from man-made material.\n\nStones are usually available in various grades, which refer to the grit size of the abrasive particles in the stone. \nGrit size is given as a number, which indicates the spatial density of the particles. A higher number denotes a higher density and therefore smaller particles, which leads to a finer finish of the surface of the polished object.\n\nThough \"whetstone\" is often mistaken as a reference to the water sometimes used to lubricate such stones, the term is based on the word \"whet\", which means to sharpen a blade, not on the word \"wet\". The verb nowadays usually used to describe the process of using a sharpening stone is \"to sharpen\", but the older term \"to whet\" is still sometimes used. The term \"to stone\" is so rare in this sense that it is no longer mentioned in for example the Oxford Living Dictionaries. One of the most common mistaken idioms in English involves the phrase \"to whet your appetite\", too often mistakenly written as \"to wet\". But to \"whet\" quite appropriately means \"to sharpen\" one's appetite, not to douse it with water.\n\nThe Roman historian Pliny described use of several naturally occurring stones for sharpening in his \"Natural History\". He describes the use of both oil and water stones and gives the locations of several ancient sources for these stones.\n\nThe use of natural stone for sharpening has diminished with the widespread availability of high-quality, consistent particle size artificial stones.\n\nAs a result, the legendary Honyama mines in Kyoto, Japan, have been closed since 1967. Belgium currently has only a single mine that is still quarrying Coticules and their Belgian Blue Whetstone (BBW) counterparts.\n\nModern synthetic stones are generally of equal quality to natural stones, and are often considered superior in sharpening performance due to consistency of particle size and control over the properties of the stones. For example, the proportional content of abrasive particles as opposed to base or \"binder\" materials can be controlled to make the stone cut faster or slower, as desired. Natural stones are often prized for their natural beauty as stones and their rarity, adding value as collectors' items. Furthermore, each natural stone is different, and there are rare natural stones that contain abrasive particles in grit sizes finer than are currently available in artificial stones. \n\nOne of the most well-regarded natural whetstones is the yellow-gray \"Belgian Coticule\", which has been legendary for the edge it can give to blades since Roman times, and has been quarried for centuries from the Ardennes. The slightly coarser and more plentiful \"Belgian Blue\" whetstone is found naturally with the yellow coticule in adjacent strata; hence two-sided whetstones are available, with a naturally occurring seam between the yellow and blue layers. These are highly prized for their natural elegance and beauty, and for providing both a fast-cutting surface for establishing a bevel and a finer surface for refining it. This stone is considered one of the finest for sharpening straight razors. \n\nThe hard stone of Charnwood Forest in northwest Leicestershire, England, has been quarried for centuries, and was a source of whetstones and quern-stones.\n\nWhetstones may be natural or artificial stones. Artificial stones usually come in the form of a bonded abrasive composed of a ceramic such as silicon carbide (carborundum) or of aluminium oxide (corundum). Bonded abrasives provide a faster cutting action than natural stones. They are commonly available as a double-sided block with a coarse grit on one side and a fine grit on the other enabling one stone to satisfy the basic requirements of sharpening. Some shapes are designed for specific purposes such as sharpening scythes, drills or serrations.\n\nNatural stones are typically formed of quartz, such as novaculite. The Ouachita Mountains in Arkansas are noted as a source for these. Novaculite is also found in Syria and Lebanon, previously a part of the Ottoman (Turkish) empire, hence the use of the older name in America of \"Turkey stone\".\n\nWhen the block is intended for installation on a bench it is called a bench stone. Small, portable stones (commonly made of bonded abrasive) are called pocket stones. Being smaller, they are more portable than bench stones but present difficulty in maintaining a consistent angle and pressure when drawing the stone along larger blades. However, they still can form a good edge. Frequently, fine grained pocket stones are used for honing, especially \"in the field\". Despite being a homophone with \"wet\" in most dialects of modern English, whetstones do not need to be lubricated with oil or water, although it is very common to do so. Lubrication aids the cutting action and carries swarf away.\n\nThe Japanese traditionally used natural sharpening stones lubricated with water (using oil on a waterstone reduces its effectiveness.) They have been doing this for many hundreds of years. The geology of Japan provided a type of stone which consists of fine silicate particles in a clay matrix, somewhat softer than novaculite.\n\nJapanese stones are also sedimentary. The most famous are typically mined in the Narutaki District just north of Kyoto.\n\nHistorically, there are three broad grades of Japanese sharpening stones: the \"ara-to\", or \"rough stone\", the \"naka-to\" or \"middle/medium stone\" and the \"shiage-to\" or \"finishing stone\". There is a fourth type of stone, the \"nagura\", which is not used directly. Rather, it is used to form a cutting slurry on the \"shiage-to\", which is often too hard to create the necessary slurry. Converting these names to absolute grit size is difficult as the classes are broad and natural stones have no inherent \"grit number\". As an indication, \"ara-to\" is probably (using a non-Japanese system of grading grit size) 500–1000 grit. The \"naka-to\" is probably 3000–5000 grit and the \"shiage-to\" is likely 7000–10000 grit. Current synthetic grit values range from extremely coarse, such as 120 grit, through extremely fine, such as 30,000 grit (less than half a micrometer abrasive particle size).\n\nA diamond plate is a steel plate, sometimes mounted on a plastic or resin base, coated with diamond grit, an abrasive that will grind metal. When they are mounted they are sometimes known as diamond stones. The plate may have a series of holes cut in it that capture the swarf cast off as grinding takes place, and cuts costs by reducing the amount of abrasive surface area on each plate. Diamond plates can serve many purposes including sharpening steel tools, and for maintaining the flatness of man-made waterstones, which can become grooved or hollowed in use. Truing (flattening a stone whose shape has been changed as it wears away) is widely considered essential to the sharpening process but some hand sharpening techniques utilise the high points of a non-true stone. As the only part of a diamond plate to wear away is a very thin coating of grit and adhesive, and in a good diamond plate this wear is minimal due to diamond's hardness, a diamond plate retains its flatness. Rubbing the diamond plate on a whetstone to true (flatten) the whetstone is a modern alternative to more traditional truing methods.\n\nDiamond plates are available in various plate sizes (from credit card to bench plate size) and grades of grit. A coarser grit is used to remove larger amounts of metal more rapidly, such as when forming an edge or restoring a damaged edge. A finer grit is used to remove the scratches of larger grits and to refine an edge. There are two-sided plates with each side coated with a different grit.\n\nThe highest quality diamond sharpeners use monocrystalline diamonds, single structures which will not break, giving them an excellent lifespan. These diamonds are bonded onto a precision ground surface, set in nickel, and electroplated. This process locks the diamonds in place. \n"}
{"id": "4243182", "url": "https://en.wikipedia.org/wiki?curid=4243182", "title": "Silver telluride", "text": "Silver telluride\n\nSilver telluride (AgTe) is a chemical compound, a telluride of silver, also known as disilver telluride or silver(I) telluride. It forms a monoclinic crystal. In a wider sense, \"silver telluride\" can be used to denote AgTe (silver(II) telluride, a metastable compound) or AgTe.\n\nSilver(I) telluride occurs naturally as the mineral hessite, whereas silver(II) telluride is known as empressite.\n\nSilver telluride is a semiconductor which can be doped both n-type and p-type. Stoichiometric AgTe has n-type conductivity. On heating silver is lost from the material.\n\nNon-stoichiometric silver telluride has shown extraordinary magnetoresistance.\n\n\n\n"}
{"id": "33984856", "url": "https://en.wikipedia.org/wiki?curid=33984856", "title": "Size effect on structural strength", "text": "Size effect on structural strength\n\nAccording to the classical theories of elastic or plastic structures made from a material with non-random strength (\"f\"), the nominal strength (\"σ\") of a structure is independent of the structure size (\"D\") when geometrically similar structures are considered. Any deviation from this property is called the size effect. For example, conventional strength of materials predicts that a large beam and a tiny beam will fail at the same stress if they are made of the same material. In the real world, because of size effects, a larger beam will fail at a lower stress than a smaller beam.\n\nThe structural size effect concerns structures made of the same material, with the same microstructure. It must be distinguished from the size effect of material inhomogeneities, particularly the Hall-Petch effect, which describes how the material strength increases with decreasing grain size in polycrystalline metals.\n\nThe size effect can have two causes: \n\nThe limitations of elasticity theory are discussed in good textbooks on the topic. The same holds for plasticity theory. Modern computational models do not have these limitations and they predict structural strength correctly for any size. The scientists that develop new material models make sure that the results agree with the size effect laws. The engineers that design exceptionally large structures make sure that the calculations do not include a size effect mistake.\n\nThe statistical size effect occurs for a broad class of brittle structures that follow the weakest-link model. This model means that macro-fracture initiation from one material element, or more precisely one representative volume element (RVE), causes the whole structure to fail, like the failure of one link in a chain (Fig. 1a). Since the material strength is random, the strength of the weakest material element in the structure (Fig. 1a) is likely to decrease with increasing structure size formula_1 (as noted already by Mariotte in 1684).\n\nDenoting the failure probabilities of structure as formula_2 and of one RVE under stress formula_3 as formula_4, and noting that the survival probability of a chain is the joint probability of survival of all its formula_5 links, one readily concludes that\nThe key is the left tail of the distribution of formula_4. It was not successfully identified until Weibull in 1939 recognized that the tail is a power law. Denoting the tail exponent as formula_7, one can then show that, if the structure is sufficiently larger than one RVE (i.e., if ), the failure probability of a structure as a function of formula_8 is\n\nEq. 2 is the cumulative Weibull distribution with scale parameter formula_9 and shape parameter formula_7; formula_11 = constant factor depending on the structure geometry, formula_12 = structure volume; formula_13 = relative (size-independent) coordinate vectors, formula_14 = dimensionless stress field (dependent on geometry), scaled so that the maximum stress be 1; formula_15 = number of spatial dimensions (formula_15 = 1, 2 or 3); formula_17 = material characteristic length representing the effective size of the RVE (typically about 3 inhomogeneity sizes).\n\nThe RVE is here defined as the smallest material volume whose failure suffices to make the whole\nstructure fail. From experience, the structure is sufficiently larger than one RVE if the equivalent number formula_18 of RVEs in the structure is larger than about formula_19 ; formula_20 = number of RVEs giving the same formula_2 if the stress field is homogeneous (always formula_22, and usually formula_23). For most normal-scale applications to metals and fine-grained ceramics, except for micrometer scale devices, the size is large enough for the Weibull theory to apply (but not for coarse-grained materials such as concrete).\n\nFrom Eq. 2 one can show that the mean strength and the coefficient of variation of strength are obtained as follows:\n\n(where formula_24 is the gamma function) The first equation shows that the size effect on the mean nominal strength is\na power function of size formula_1, regardless of structure geometry.\n\nWeibull parameter formula_7 can be experimentally identified by two methods: 1) The values of formula_8 measured on many identical specimens are used to calculate the coefficient of variation of strength, and the value of formula_7 then follows by solving Eq. (4); or 2) the values of formula_29 are measured on geometrically similar specimens of several different sizes formula_1 and the slope of their linear regression in the plot of formula_31 versus formula_32 gives formula_33. Method 1 must give the same result for different sizes, and method 2 the same as method 1. If not, the size effect is partly or totally non-Weibullian. Omission of testing for different sizes has often led to incorrect conclusions. Another check is that the histogram of the strengths of many identical specimens must be a straight line when plotted in the Weibull scale. A deviation to the right at high strength range means that formula_18 is too small and the material quasibrittle.\n\nThe fact that the Weibull size effect is a power law means that it is self-similar, i.e., no characteristic structure size formula_35 exists, and formula_17 and material inhomogeneities are negligible compared to formula_1. This is the case for fatigue-embrittled metals or fine-grained ceramics except on the micrometer scale. The existence of a finite formula_35 is a salient feature of the energetic size effect, discovered in 1984. This kind of size effect represents a transition between two power laws and is observed in brittle heterogenous materials, termed quasibrittle. These materials include concrete, fiber composites, rocks, coarse-grained and toughened ceramics, rigid foams, sea ice, dental ceramics, dentine, bone, biological shells, many bio- and bio-inspired materials, masonry, mortar, stiff cohesive soils, grouted soils, consolidated snow, wood, paper, carton, coal, cemented sands, etc. On the micro- or nano scale, all the brittle materials become quasibrittle, and thus must exhibit the energetic size effect.\n\nA pronounced energetic size effect occurs in shear, torsional and punching failures of reinforced concrete, in pullout of anchors from concrete, in compression failure of slender reinforced concrete columns and prestressed concrete beams, in compression and tensile failures of fiber-polymer composites and sandwich structures, and in the failures of all the aforementioned quasibrittle materials. One may distinguish two basic types of this size effect.\n\nWhen the macro-crack initiates from one RVE whose size is not negligible compared to the structure size, the deterministic size effect dominates over the statistical size effect. What causes the size effect is a stress redistribution in the structure (Fig. 2c) due to damage in the initiating RVE, which is typically located at fracture surface.\n\nA simple intuitive justification of this size effect may be given by considering the ﬂexural failure of an unnotched simply supported beam under a concentrated load formula_39 at midspan (Fig. 2d). Due to material heterogeneity, what decides the maximum load formula_39 is not the elastically calculated stress formula_41 at the tensile face, where formula_42 = bending moment, formula_1 = beam depth, formula_44 and formula_45 = beam width. Rather, what decides is the stress value formula_46 roughly at distance formula_47 from the tensile face, which is at the middle of FPZ (2c). Noting that formula_46 = formula_49, where formula_50 = stress gradient = formula_51 and formula_52 = intrinsic tensile strength of the material, and considering the failure condition formula_46 = formula_54, one gets formula_55 = formula_56 where formula_57, which is a constant because for geometrically similar beams formula_58 = constant. This expression is valid only for small enough formula_59, and so (according to the first two terms of the binomial expansion) one may approximate it as\n\nwhich is the law of Type 1 deterministic size effect (Fig. 2a). The purpose of the approximation made is: (a) to prevent formula_8 from becoming negative for very small formula_1, for which the foregoing argument does not apply; and (b) to satisfy the asymptotic condition that the deterministic size effect must vanish for formula_62. Here formula_63 = positive empirical constant; the values formula_64 = or 2 have been used for concrete, while formula_65 is optimum according to the existing test data from the literature (Fig. 2d).\n\nA fundamental derivation of Eq. 5 for a general structural geometry has been given by\napplying dimensional analysis and asymptotic matching to the limit case of energy release when the initial macro-crack length tends to zero. For general structures, the following effective size may be substituted in Eq. (5):\n\nwhere formula_66 = strain gradient at the maximum strain point located at the surface, in the direction\nnormal to the surface.\n\nEq. 5 cannot apply for large sizes because it approaches for formula_67 a horizontal asymptote.\nFor large sizes, formula_8 must approach the Weibull statistical size effect, Eq. 3. This condition is satisfied by the generalized energetic-statistical size effect law:\n\nwhere formula_69 are empirical constants (formula_70). The deterministic formula (5) is recovered as the limit case for formula_71. (Fig. 2d) shows a comparison of the last formula with the test results for many different concretes, plotted as dimensionless strength formula_72 versus dimensionless structure size formula_73.\n\nThe probabilistic theory of Type 1 size effect can be derived from fracture nano-mechanics. Kramer’s\ntransition rate theory shows that, on the nano-scale, the far-left tail of the probability distribution of nano-scale strength formula_74 is a power law of the type formula_75. Analysis of the multiscale transition to the material macro-scale then shows that the RVE strength distribution is Gaussian but with a Weibull (or power-law) left tail whose exponent formula_7 is much larger than 2 and is grafted roughly at the probability of about 0.001.\n\nFor structures with formula_77, which are common for quasibrittle materials, the Weibull theory does not apply. But the underlying weakest-link model, expressed by Eq. (1) for formula_2, does, albeit with a finite formula_5, which is a crucial point. The finiteness of the weakest-link chain model causes major deviations from the Weibull distribution. As the structure size, measured by formula_18, increases, the grafting point of the Weibullian left part moves to the right until, at about formula_81, the entire distribution becomes Weibullian. The mean strength can be computed from this distribution and, as it turns out, its plot is identical with the plot of Eq. 5 seen in Fig. 2g. The point of deviation from the Weibull asymptote is determined by the location of the grafting point on the strength distribution of one RVE (Fig. 2g). Note that the finiteness of the chain in the weakest-link model captures the deterministic part of size effect.\n\nThis theory has also been extended to the size effect on the Evans and Paris' laws of crack growth in quasibrittle materials, and to the size effect on the static and fatigue lifetimes. It appeared that the size effect on the lifetime is much stronger than it is on the short-time strength (tail exponent formula_7 is an order-of-magnitude smaller).\n\nThe strongest possible size effect occurs for specimens with similar deep notches (Fig. 4b), or for structures in which a large crack, similar for different sizes, forms stably before the maximum load is reached. Because the location of fracture initiation is predetermined to occur at the crack tip and thus cannot sample the random strengths of different RVEs, the statistical contribution to the mean size effect is negligible. Such behavior is typical of reinforced concrete, damaged fiber-reinforced polymers and some compressed unreinforced structures.\n\nThe energetic size effect may be intuitively explained by considering the panel in Fig. 1c,d,\ninitially under a uniform stress equal to formula_8 . Introduction of a crack of length formula_84, with a damage zone\nof width formula_85 at the tip, relieves the stress, and thus also the strain energy, from the shaded undamaged triangles of slope formula_86 on the ﬂanks of the crack. Then, if formula_86 and formula_88 are approximately the same for different sizes, the energy released from the shaded triangles is proportional to formula_89, while the energy dissipated by the fracture process is proportional to formula_90; here formula_91 = fracture energy of the material, formula_92 = energy density before fracture, and formula_93 = Young’s elastic modulus. The discrepancy between formula_1 and formula_95 shows that a balance of energy release and dissipation rate can exist for every size formula_1 only if formula_8 decreases with increasing formula_1. If the energy dissipated within the damage zone of width formula_85 is added, one obtains the Bažant (1984) size effect law (Type 2):\n\n(Fig. 4c,d) where formula_100 = constants, where formula_54 = tensile strength of material, and formula_102 accounts for the structure geometry.\n\nFor more complex geometries such an intuitive derivation is not possible. However, dimensional\nanalysis coupled with asymptotic matching showed that Eq. 8 is applicable in general, and that the dependence of its parameters on the structure geometry has approximately the following form:\n\nwhere formula_103 half of the FPZ length, formula_104 = relative initial crack length (which is constant for geometrically similar scaling); formula_105 = dimensionless energy release function of linear elastic fracture mechanics (LEFM), which brings about the effect of structure geometry; formula_106, and formula_107 = stress intensity factor. Fitting Eq. 8 to formula_8 data from tests of geometrically similar notched specimens of very different sizes is a good way to identify the formula_91 and formula_110 of the material.\n\nNumerical simulations of failure by finite element codes can capture the energetic (or deterministic) size effect only if the material law relating the stress to deformation possesses a characteristic length. This was not the case for the classical finite element codes with a material characterized solely by stress-strain relations.\n\nOne simple enough computational method is the cohesive (or fictitious) crack model, in which it is assumed that the stress formula_111 transmitted across a partially opened crack is a decreasing function of the crack opening formula_112, i.e., formula_113. The area under this function is formula_91, and\n\nis the material characteristic length giving rise to the deterministic size effect. An even simpler method is the crack-band model, in which the cohesive crack is replaced in simulations by a crack band of width formula_85 equal to one finite element size and a stress-strain relation that is softening in the cross-band direction as formula_116 where formula_117 = average strain in that direction.\n\nWhen formula_85 needs to be adjusted, the softening stress strain relation is adjusted so as to maintain the correct energy dissipation formula_91. A more versatile method is the nonlocal damage model in which the stress at a continuum point is a function not of the strain at that point but of the average of the strain field within a certain neighborhood of size formula_85 centered at that point. Still another method is the gradient damage model in which the stress depends not only on the strain at that point but also on the gradient of strain. All these computational methods can ensure objectivity and proper convergence with respect to the refinement of the finite element mesh.\n\nThe fractal properties of material, including the fractal aspect of crack surface roughness and the lacunar fractal aspect of pore structure, may have a role in the size effect in concrete, and may affect the fracture energy of material. However, the fractal properties have yet not been experimentally documented for a broad enough scale and the problem has not yet been studied in depth comparable to the statistical and energetic size effects. The main obstacle to the practical consideration of a fractal inﬂuence on the size effect is that, if calibrated for one structure geometry, it is not clear how infer the size effect for another geometry. The pros and cons were discussed, e.g., by Carpinteri et al. (1994, 2001) and Bažant and Yavari (2005).\n\nTaking the size effect into account is essential for safe prediction of strength of large concrete bridges, nuclear containments, roof shells, tall buildings, tunnel linings, large load-bearing parts of aircraft, spacecraft and ships made of fiber-polymer composites, wind turbines, large geotechnical excavations, earth and rock slopes, ﬂoating sea ice carrying loads, oil platforms under ice forces, etc. Their design depends on the material properties measured on much smaller laboratory specimens. These properties must be extrapolated to sizes greater by one or two orders of magnitude. Even if an expensive full-scale failure test, for example a failure test of the rudder of a very large aircraft, can be carried out, it is financially prohibitive to repeat it thousand times to obtain the statistical distribution of load capacity. Such statistical information, underlying the safety factors, is obtainable only by proper extrapolation of laboratory tests.\n\nThe size effect is gaining in importance as larger and larger structures, of more and more slender forms, are being built. The safety factors, of course, give large safety margins—so large that even for the largest civil engineering structures the classical deterministic analysis based on the mean material properties normally yields failure loads smaller than the maximum design loads. For this reasons, the size effect on the strength in brittle failures of concrete structures and structural laminates has long been ignored. Then, however, the failure probability, which is required to be formula_121, and actually does have such values for normal-size structures, may become for very large structures as low as formula_122 per lifetime. Such high failure probability is intolerable as it adds significantly to the risks to which people are inevitably exposed. In fact, the historical experience shows that very large structures have been failing at a frequency several orders of magnitude higher than smaller ones. The reason it has not led to public outcry is that the large structures are few. But for the locals, who must use the structures daily, the risk is not acceptable.\n\nAnother application is the testing of the fracture energy and characteristic material length. For quasibrittle materials, measuring the size effect on the peak loads (and on the specimen softening after the peak load) is the simplest approach.\n\nKnowing the size effect is also important in the reverse sense—for micrometer scale devices if they\nare designed partly of fully on the basis of material properties measured more conveniently on the scale of 0.01m to 0.1m.\n\n\n\n"}
{"id": "24750731", "url": "https://en.wikipedia.org/wiki?curid=24750731", "title": "Solar simulator", "text": "Solar simulator\n\nA solar simulator (also artificial sun) is a device that provides illumination approximating natural sunlight. The purpose of the solar simulator is to provide a controllable indoor test facility under laboratory conditions, used for the testing of solar cells, sun screen, plastics, and other materials and devices.\n\nThe IEC 60904-9 Edition2 and ASTM E927-10 standards\n\nare a common specification for solar simulators used for photovoltaic testing. The light from a solar simulator is controlled in three dimensions:\nEach dimension is classified in one of three classes: A, B, or C. The specifications required for each class are defined in Table 1 below. A solar simulator meeting class A specifications in all three dimensions is referred to as a Class A solar simulator, or sometimes a Class AAA (referring to each of the dimensions in the order listed above).\n\nThe solar simulation spectrum is further specified via the integrated irradiance across several wavelength intervals. The percentage of total irradiance is shown below in Table 2 for the standard terrestrial spectra of AM1.5G and AM1.5D, and the extraterrestrial spectrum, AM0.\n\nThese specifications were primarily intended for silicon photovoltaics, and hence the spectral range over which the intervals were defined was limited mainly to the absorption region of silicon. While this definition is also adequate for several other photovoltaic technologies, including thin film solar cells constructed from CdTe or CIGS, it is not sufficient for the emerging sub-field of concentrated photovoltaics using high-efficiency III-V semiconductor multi-junction solar cells due to their wider absorption bandwidth of 300–1800 nm.\n\nSolar simulators can be divided into three broad categories: continuous, flashed, and pulsed. The first type is a familiar form of light source in which illumination is continuous in time. The specifications discussed in the previous section most directly relate to this type of solar simulator. This category is most often used for low intensity testing, from less than 1 sun up to several suns. In this context, 1 sun is typically defined as the nominal full sunlight intensity on a bright clear day on Earth, which measures 1000 W/m. Continuous light solar simulators may have several different lamp types combined (e.g. an arc source and one or more halogen lamps) to extend the spectrum far into the infrared. \n\nExamples of low-intensity and high-intensity continuous solar simulators are available from Solar Light Company, Inc. (inventor of the original solar simulator in 1967,) Atonometrics, Eternal Sun, TS-Space Systems,\nWACOM, \nNewport Oriel, \nSciencetech, Spectrolab, Photo Emission Tech,\nAbet Technologies, infinityPV \n\nThe second type of solar simulator is the flashed simulator which is qualitatively similar to flash photography and use flash tubes. With typical durations of several milliseconds, very high intensities of up to several thousand suns are possible. This type of equipment is often used to prevent unnecessary heat build-up in the device under test. However, due to the rapid heating and cooling of the lamp, the intensity and light spectrum are inherently transient, making repeated reliable testing more technically challenging. The temporal stability dimension of the standard does not directly apply to this category of solar simulators, although it can be replaced by an analogous shot-to-shot repeatability specification.\n\nThe third type of solar simulator is the pulsed simulator, which uses a shutter to quickly block or unblock the light from a continuous source. This category is a compromise between the continuous and flash, having the disadvantage of the high power usage and relatively low intensities of the continuous simulators, but advantage of stable output intensity and spectrum. The short illumination duration also provides the benefit of the low thermal loads of flashed simulators. Pulses are typically on the order of 100 milliseconds up to 800 milliseconds for special Xe Long Pulse Systems.\n\nSeveral types of lamps have been used as the light sources within solar simulators.\n\nXenon arc lamp: this is the most common type of lamp both for continuous and flashed solar simulators. These lamps offer high intensities and an unfiltered spectrum which matches reasonably well to sunlight. However, the Xe spectrum is also characterized by many undesirable sharp atomic transitional peaks, making the spectrum less desirable for some spectrally sensitive applications.\n\nMetal Halide arc lamp: Primarily developed for use in film and television lighting where a high temporal stability and daylight colour match are required, metal halide arc lamps are also used in solar simulation.\n\nQTH: quartz tungsten halogen lamps offer spectra which very closely match black body radiation, although typically with a lower color temperature than the sun.\n\nLED: light-emitting diodes have recently been used in research laboratories to construct solar simulators, and may offer promise in the future for energy-efficient production of spectrally tailored artificial sunlight.\n"}
{"id": "147853", "url": "https://en.wikipedia.org/wiki?curid=147853", "title": "Speed of sound", "text": "Speed of sound\n\nThe speed of sound is the distance travelled per unit time by a sound wave as it propagates through an elastic medium. At , the speed of sound in air is about 343 meters per second (1,234.8 km/h; 1,125 ft/s; 767 mph; 667 kn), or a kilometre in or a mile in . It depends strongly on temperature, but also varies by several meters per second due to which gases are present.\n\nThe speed of sound in an ideal gas depends only on its temperature and composition. The speed has a weak dependence on frequency and pressure in ordinary air, deviating slightly from ideal behavior.\n\nIn common everyday speech, \"speed of sound\" refers to the speed of sound waves in air. However, the speed of sound varies from substance to substance: sound travels most slowly in gases; it travels faster in liquids; and faster still in solids. For example, (as noted above), sound travels at in air; it travels at in water (4.3 times as fast as in air); and at in iron (about 15 times as fast as in air). In an exceptionally stiff material such as diamond, sound travels at 12,000 metres per second (27,000 mph); (about 35 times as fast as in air) which is around the maximum speed that sound will travel under normal conditions.\n\nSound waves in solids are composed of compression waves (just as in gases and liquids), and a different type of sound wave called a shear wave, which occurs only in solids. Shear waves in solids usually travel at different speeds, as exhibited in seismology. The speed of compression waves in solids is determined by the medium's compressibility, shear modulus and density. The speed of shear waves is determined only by the solid material's shear modulus and density.\n\nIn fluid dynamics, the speed of sound in a fluid medium (gas or liquid) is used as a relative measure for the speed of an object moving through the medium. The ratio of the speed of an object to the speed of sound in the fluid is called the object's Mach number. Objects moving at speeds greater than \"\" are said to be traveling at supersonic speeds.\n\nSir Isaac Newton computed the speed of sound in air as , which is too low by about 15%. Newton's analysis was good save for neglecting the (then unknown) effect of rapidly-fluctuating temperature in a sound wave (in modern terms, sound wave compression and expansion of air is an adiabatic process, not an isothermal process). This error was later rectified by Laplace.\n\nDuring the 17th century, there were several attempts to measure the speed of sound accurately, including attempts by Marin Mersenne in 1630 (1,380 Parisian feet per second), Pierre Gassendi in 1635 (1,473 Parisian feet per second) and Robert Boyle (1,125 Parisian feet per second).\n\nIn 1709, the Reverend William Derham, Rector of Upminster, published a more accurate measure of the speed of sound, at 1,072 Parisian feet per second. Derham used a telescope from the tower of the church of St Laurence, Upminster to observe the flash of a distant shotgun being fired, and then measured the time until he heard the gunshot with a half-second pendulum. Measurements were made of gunshots from a number of local landmarks, including North Ockendon church. The distance was known by triangulation, and thus the speed that the sound had travelled was calculated.\n\nThe transmission of sound can be illustrated by using a model consisting of an array of spherical objects interconnected by springs.\n\nIn real material terms, the spheres represent the material's molecules and the springs represent the bonds between them. Sound passes through the system by compressing and expanding the springs, transmitting the acoustic energy to neighboring spheres. This helps transmit the energy in-turn to the neighboring sphere's springs (bonds), and so on. \n\nThe speed of sound through the model depends on the stiffness/rigidity of the springs, and the mass of the spheres. As long as the spacing of the spheres remains constant, stiffer springs/bonds transmit energy quicker, while larger spheres transmit the energy slower. \n\nIn a real material, the stiffness of the springs is known as the \"elastic modulus\", and the mass corresponds to the material density. Given that all other things being equal (ceteris paribus), sound will travel slower in spongy materials, and faster in stiffer ones. Effects like dispersion and reflection can also be understood using this model. \n\nFor instance, sound will travel 1.59 times faster in nickel than in bronze, due to the greater stiffness of nickel at about the same density. Similarly, sound travels about 1.41 times faster in light hydrogen (protium) gas than in heavy hydrogen (deuterium) gas, since deuterium has similar properties but twice the density. At the same time, \"compression-type\" sound will travel faster in solids than in liquids, and faster in liquids than in gases, because the solids are more difficult to compress than liquids, while liquids in turn are more difficult to compress than gases.\n\nSome textbooks mistakenly state that the speed of sound increases with density. This notion is illustrated by presenting data for three materials, such as air, water and steel, which also have vastly different compressibility, more which making up for the density differences. An illustrative example of the two effects is that sound travels only 4.3 times faster in water than air, despite enormous differences in compressibility of the two media. The reason is that the larger density of water, which works to \"slow\" sound in water relative to air, nearly makes up for the compressibility differences in the two media.\n\nA practical example can be observed in Edinburgh when the \"One o' Clock Gun\" is fired at the eastern end of Edinburgh Castle. Standing at the base of the western end of the Castle Rock, the sound of the Gun can be heard through the rock, slightly before it arrives by the air route, partly delayed by the slightly longer route. It is particularly effective if a multi-gun salute such as for \"The Queen's Birthday\" is being fired.\n\nIn a gas or liquid, sound consists of compression waves. In solids, waves propagate as two different types. A longitudinal wave is associated with compression and decompression in the direction of travel, and is the same process in gases and liquids, with an analogous compression-type wave in solids. Only compression waves are supported in gases and liquids. An additional type of wave, the transverse wave, also called a shear wave, occurs only in solids because only solids support elastic deformations. It is due to elastic deformation of the medium perpendicular to the direction of wave travel; the direction of shear-deformation is called the \"polarization\" of this type of wave. In general, transverse waves occur as a pair of orthogonal polarizations.\n\nThese different waves (compression waves and the different polarizations of shear waves) may have different speeds at the same frequency. Therefore, they arrive at an observer at different times, an extreme example being an earthquake, where sharp compression waves arrive first and rocking transverse waves seconds later.\n\nThe speed of a compression wave in a fluid is determined by the medium's compressibility and density. In solids, the compression waves are analogous to those in fluids, depending on compressibility and density, but with the additional factor of shear modulus which affects compression waves due to off-axis elastic energies which are able to influence effective tension and relaxation in a compression. The speed of shear waves, which can occur only in solids, is determined simply by the solid material's shear modulus and density.\n\nThe speed of sound in mathematical notation is conventionally represented by \"c\", from the Latin \"celeritas\" meaning \"velocity\".\n\nFor fluids in general, the speed of sound \"c\" is given by the Newton–Laplace equation:\nwhere\n\nThus the speed of sound increases with the stiffness (the resistance of an elastic body to deformation by an applied force) of the material and decreases with an increase in density. For ideal gases, the bulk modulus \"K\" is simply the gas pressure multiplied by the dimensionless adiabatic index, which is about 1.4 for air under normal conditions of pressure and temperature.\n\nFor general equations of state, if classical mechanics is used, the speed of sound \"c\" is given by\nwhere\n\nIf relativistic effects are important, the speed of sound is calculated from the relativistic Euler equations.\n\nIn a non-dispersive medium, the speed of sound is independent of sound frequency, so the speeds of energy transport and sound propagation are the same for all frequencies. Air, a mixture of oxygen and nitrogen, constitutes a non-dispersive medium. However, air does contain a small amount of CO which \"is\" a dispersive medium, and causes dispersion to air at ultrasonic frequencies ().\n\nIn a dispersive medium, the speed of sound is a function of sound frequency, through the dispersion relation. Each frequency component propagates at its own speed, called the phase velocity, while the energy of the disturbance propagates at the group velocity. The same phenomenon occurs with light waves; see optical dispersion for a description.\n\nThe speed of sound is variable and depends on the properties of the substance through which the wave is travelling. In solids, the speed of transverse (or shear) waves depends on the shear deformation under shear stress (called the shear modulus), and the density of the medium. Longitudinal (or compression) waves in solids depend on the same two factors with the addition of a dependence on compressibility.\n\nIn fluids, only the medium's compressibility and density are the important factors, since fluids do not transmit shear stresses. In heterogeneous fluids, such as a liquid filled with gas bubbles, the density of the liquid and the compressibility of the gas affect the speed of sound in an additive manner, as demonstrated in the hot chocolate effect.\n\nIn gases, adiabatic compressibility is directly related to pressure through the heat capacity ratio (adiabatic index), while pressure and density are inversely related to the temperature and molecular weight, thus making only the completely independent properties of \"temperature and molecular structure\" important (heat capacity ratio may be determined by temperature and molecular structure, but simple molecular weight is not sufficient to determine it).\n\nIn low molecular weight gases such as helium, sound propagates faster as compared to heavier gases such as xenon. For monatomic gases, the speed of sound is about 75% of the mean speed that the atoms move in that gas.\n\nFor a given ideal gas the molecular composition is fixed, and thus the speed of sound depends only on its temperature. At a constant temperature, the gas pressure has no effect on the speed of sound, since the density will increase, and since pressure and density (also proportional to pressure) have equal but opposite effects on the speed of sound, and the two contributions cancel out exactly. In a similar way, compression waves in solids depend both on compressibility and density—just as in liquids—but in gases the density contributes to the compressibility in such a way that some part of each attribute factors out, leaving only a dependence on temperature, molecular weight, and heat capacity ratio which can be independently derived from temperature and molecular composition (see derivations below). Thus, for a single given gas (assuming the molecular weight does not change) and over a small temperature range (for which the heat capacity is relatively constant), the speed of sound becomes dependent on only the temperature of the gas.\n\nIn non-ideal gas behavior regimen, for which the van der Waals gas equation would be used, the proportionality is not exact, and there is a slight dependence of sound velocity on the gas pressure.\n\nHumidity has a small but measurable effect on the speed of sound (causing it to increase by about 0.1%–0.6%), because oxygen and nitrogen molecules of the air are replaced by lighter molecules of water. This is a simple mixing effect.\n\nIn the Earth's atmosphere, the chief factor affecting the speed of sound is the temperature. For a given ideal gas with constant heat capacity and composition, the speed of sound is dependent \"solely\" upon temperature; see Details below. In such an ideal case, the effects of decreased density and decreased pressure of altitude cancel each other out, save for the residual effect of temperature.\n\nSince temperature (and thus the speed of sound) decreases with increasing altitude up to , sound is refracted upward, away from listeners on the ground, creating an acoustic shadow at some distance from the source. The decrease of the speed of sound with height is referred to as a negative sound speed gradient.\n\nHowever, there are variations in this trend above . In particular, in the stratosphere above about , the speed of sound increases with height, due to an increase in temperature from heating within the ozone layer. This produces a positive speed of sound gradient in this region. Still another region of positive gradient occurs at very high altitudes, in the aptly-named thermosphere above .\n\nThe approximate speed of sound in dry (0% humidity) air, in meters per second, at temperatures near , can be calculated from\nwhere \"formula_4\" is the temperature in degrees Celsius (°C).\n\nThis equation is derived from the first two terms of the Taylor expansion of the following more accurate equation:\n\nDividing the first part, and multiplying the second part, on the right hand side, by gives the exactly equivalent form\n\nwhich can also be written as\n\nwhere T denotes the thermodynamic temperature.\n\nThe value of , which represents the speed at (or ), is based on theoretical (and some measured) values of the heat capacity ratio, \"γ\", as well as on the fact that at 1 atm real air is very well described by the ideal gas approximation. Commonly found values for the speed of sound at may vary from 331.2 to 331.6 due to the assumptions made when it is calculated. If ideal gas \"γ\" is assumed to be exactly, the speed is calculated (see section below) to be , the coefficient used above.\n\nThis equation is correct to a much wider temperature range, but still depends on the approximation of heat capacity ratio being independent of temperature, and for this reason will fail, particularly at higher temperatures. It gives good predictions in relatively dry, cold, low-pressure conditions, such as the Earth's stratosphere. The equation fails at extremely low pressures and short wavelengths, due to dependence on the assumption that the wavelength of the sound in the gas is much longer than the average mean free path between gas molecule collisions. A derivation of these equations will be given in the following section.\n\nA graph comparing results of the two equations is at right, using the slightly different value of for the speed of sound at .\n\nFor an ideal gas, \"K\" (the bulk modulus in equations above, equivalent to C, the coefficient of stiffness in solids) is given by\nthus, from the Newton–Laplace equation above, the speed of sound in an ideal gas is given by\nwhere\n\nUsing the ideal gas law to replace \"p\" with \"nRT\"/\"V\", and replacing \"ρ\" with \"nM\"/\"V\", the equation for an ideal gas becomes\nwhere\n\nThis equation applies only when the sound wave is a small perturbation on the ambient condition, and the certain other noted conditions are fulfilled, as noted below. Calculated values for \"c\" have been found to vary slightly from experimentally determined values.\n\nNewton famously considered the speed of sound before most of the development of thermodynamics and so incorrectly used isothermal calculations instead of adiabatic. His result was missing the factor of \"γ\" but was otherwise correct.\n\nNumerical substitution of the above values gives the ideal gas approximation of sound velocity for gases, which is accurate at relatively low gas pressures and densities (for air, this includes standard Earth sea-level conditions). Also, for diatomic gases the use of requires that the gas exists in a temperature range high enough that rotational heat capacity is fully excited (i.e., molecular rotation is fully used as a heat energy \"partition\" or reservoir); but at the same time the temperature must be low enough that molecular vibrational modes contribute no heat capacity (i.e., insignificant heat goes into vibration, as all vibrational quantum modes above the minimum-energy-mode, have energies too high to be populated by a significant number of molecules at this temperature). For air, these conditions are fulfilled at room temperature, and also temperatures considerably below room temperature (see tables below). See the section on gases in specific heat capacity for a more complete discussion of this phenomenon.\n\nFor air, we introduce the shorthand\nIn addition, we switch to the Celsius temperature , which is useful to calculate air speed in the region near \n0°C (about 273 kelvin). Then, for dry air,\n\nwhere \"formula_4\" (theta) is the temperature in degrees Celsius(°C).\n\nSubstituting numerical values \nfor the molar gas constant in J/mole/Kelvin, and\nfor the mean molar mass of air, in kg; and using the ideal diatomic gas value of , we have\n\nFinally, Taylor expansion of the remaining square root in formula_4 yields\n\nThe above derivation includes the first two equations given in the \"Practical formula for dry air\" section above.\n\nThe speed of sound varies with temperature. Since temperature and sound velocity normally decrease with increasing altitude, sound is refracted upward, away from listeners on the ground, creating an acoustic shadow at some distance from the source. Wind shear of 4 m/(s · km) can produce refraction equal to a typical temperature lapse rate of . Higher values of wind gradient will refract sound downward toward the surface in the downwind direction, eliminating the acoustic shadow on the downwind side. This will increase the audibility of sounds downwind. This downwind refraction effect occurs because there is a wind gradient; the sound is not being carried along by the wind.\n\nFor sound propagation, the exponential variation of wind speed with height can be defined as follows:\nwhere\n\nIn the 1862 American Civil War Battle of Iuka, an acoustic shadow, believed to have been enhanced by a northeast wind, kept two divisions of Union soldiers out of the battle, because they could not hear the sounds of battle only (six miles) downwind.\n\nIn the standard atmosphere:\n\nIn fact, assuming an ideal gas, the speed of sound \"c\" depends on temperature only, not on the pressure or density (since these change in lockstep for a given temperature and cancel out). Air is almost an ideal gas. The temperature of the air varies with altitude, giving the following variations in the speed of sound using the standard atmosphere—\"actual conditions may vary\".\n\nGiven normal atmospheric conditions, the temperature, and thus speed of sound, varies with altitude:\n\nThe medium in which a sound wave is travelling does not always respond adiabatically, and as a result, the speed of sound can vary with frequency.\n\nThe limitations of the concept of speed of sound due to extreme attenuation are also of concern. The attenuation which exists at sea level for high frequencies applies to successively lower frequencies as atmospheric pressure decreases, or as the mean free path increases. For this reason, the concept of speed of sound (except for frequencies approaching zero) progressively loses its range of applicability at high altitudes. The standard equations for the speed of sound apply with reasonable accuracy only to situations in which the wavelength of the soundwave is considerably longer than the mean free path of molecules in a gas.\n\nThe molecular composition of the gas contributes both as the mass (M) of the molecules, and their heat capacities, and so both have an influence on speed of sound. In general, at the same molecular mass, monatomic gases have slightly higher speed of sound (over 9% higher) because they have a higher \"γ\" (…) than diatomics do (). Thus, at the same molecular mass, the speed of sound of a monatomic gas goes up by a factor of\n\nThis gives the 9% difference, and would be a typical ratio for speeds of sound at room temperature in helium vs. deuterium, each with a molecular weight of 4. Sound travels faster in helium than deuterium because adiabatic compression heats helium more since the helium molecules can store heat energy from compression only in translation, but not rotation. Thus helium molecules (monatomic molecules) travel faster in a sound wave and transmit sound faster. (Sound travels at about 70% of the mean molecular speed in gases; the figure is 75% in monatomic gases and 68% in diatomic gases).\n\nNote that in this example we have assumed that temperature is low enough that heat capacities are not influenced by molecular vibration (see heat capacity). However, vibrational modes simply cause gammas which decrease toward 1, since vibration modes in a polyatomic gas give the gas additional ways to store heat which do not affect temperature, and thus do not affect molecular velocity and sound velocity. Thus, the effect of higher temperatures and vibrational heat capacity acts to increase the difference between the speed of sound in monatomic vs. polyatomic molecules, with the speed remaining greater in monatomics.\n\nBy far the most important factor influencing the speed of sound in air is temperature. The speed is proportional to the square root of the absolute temperature, giving an increase of about per degree Celsius. For this reason, the pitch of a musical wind instrument increases as its temperature increases.\n\nThe speed of sound is raised by humidity but decreased by carbon dioxide. The difference between 0% and 100% humidity is about at standard pressure and temperature, but the size of the humidity effect increases dramatically with temperature. The carbon dioxide content of air is not fixed, due to both carbon pollution and human breath (e.g., in the air blown through wind instruments).\n\nThe dependence on frequency and pressure are normally insignificant in practical applications. In dry air, the speed of sound increases by about as the frequency rises from to . For audible frequencies above it is relatively constant. Standard values of the speed of sound are quoted in the limit of low frequencies, where the wavelength is large compared to the mean free path.\n\nMach number, a useful quantity in aerodynamics, is the ratio of air speed to the local speed of sound. At altitude, for reasons explained, Mach number is a function of temperature.\nAircraft flight instruments, however, operate using pressure differential to compute Mach number, not temperature. The assumption is that a particular pressure represents a particular altitude and, therefore, a standard temperature. Aircraft flight instruments need to operate this way because the stagnation pressure sensed by a Pitot tube is dependent on altitude as well as speed.\n\nA range of different methods exist for the measurement of sound in air.\n\nThe earliest reasonably accurate estimate of the speed of sound in air was made by William Derham and acknowledged by Isaac Newton. Derham had a telescope at the top of the tower of the Church of St Laurence in Upminster, England. On a calm day, a synchronized pocket watch would be given to an assistant who would fire a shotgun at a pre-determined time from a conspicuous point some miles away, across the countryside. This could be confirmed by telescope. He then measured the interval between seeing gunsmoke and arrival of the sound using a half-second pendulum. The distance from where the gun was fired was found by triangulation, and simple division (distance/time) provided velocity. Lastly, by making many observations, using a range of different distances, the inaccuracy of the half-second pendulum could be averaged out, giving his final estimate of the speed of sound. Modern stopwatches enable this method to be used today over distances as short as 200–400 meters, and not needing something as loud as a shotgun.\n\nThe simplest concept is the measurement made using two microphones and a fast recording device such as a digital storage scope. This method uses the following idea.\n\nIf a sound source and two microphones are arranged in a straight line, with the sound source at one end, then the following can be measured:\n\nThen \"v\" = \"x\"/\"t\".\n\nIn these methods, the time measurement has been replaced by a measurement of the inverse of time (frequency).\n\nKundt's tube is an example of an experiment which can be used to measure the speed of sound in a small volume. It has the advantage of being able to measure the speed of sound in any gas. This method uses a powder to make the nodes and antinodes visible to the human eye. This is an example of a compact experimental setup.\n\nA tuning fork can be held near the mouth of a long pipe which is dipping into a barrel of water. In this system it is the case that the pipe can be brought to resonance if the length of the air column in the pipe is equal to \"(1 + 2\"n\")λ/4\" where \"n\" is an integer. As the antinodal point for the pipe at the open end is slightly outside the mouth of the pipe it is best to find two or more points of resonance and then measure half a wavelength between these.\n\nHere it is the case that \"v\" = \"fλ\".\n\nThe effect of impurities can be significant when making high-precision measurements. Chemical desiccants can be used to dry the air, but will, in turn, contaminate the sample. The air can be dried cryogenically, but this has the effect of removing the carbon dioxide as well; therefore many high-precision measurements are performed with air free of carbon dioxide rather than with natural air. A 2002 review found that a 1963 measurement by Smith and Harlow using a cylindrical resonator gave \"the most probable value of the standard speed of sound to date.\" The experiment was done with air from which the carbon dioxide had been removed, but the result was then corrected for this effect so as to be applicable to real air. The experiments were done at but corrected for temperature in order to report them at . The result was for dry air at STP, for frequencies from to .\n\nIn a solid, there is a non-zero stiffness both for volumetric deformations and shear deformations. Hence, it is possible to generate sound waves with different velocities dependent\non the deformation mode. Sound waves generating volumetric deformations (compression) and shear deformations (shearing) are called pressure waves (longitudinal waves) and shear waves (transverse waves), respectively. In earthquakes, the corresponding seismic waves are called P-waves (primary waves) and S-waves (secondary waves), respectively. The sound velocities of these two types of waves propagating in a homogeneous 3-dimensional solid are respectively given by\nwhere\n\nThe last quantity is not an independent one, as . Note that the speed of pressure waves depends both on the pressure and shear resistance properties of the material, while the speed of shear waves depends on the shear properties only.\n\nTypically, pressure waves travel faster in materials than do shear waves, and in earthquakes this is the reason that the onset of an earthquake is often preceded by a quick upward-downward shock, before arrival of waves that produce a side-to-side motion. For example, for a typical steel alloy, , and , yielding a compressional speed \"c\" of . This is in reasonable agreement with \"c\" measured experimentally at for a (possibly different) type of steel. The shear speed \"c\" is estimated at using the same numbers.\n\nThe speed of sound for pressure waves in stiff materials such as metals is sometimes given for \"long rods\" of the material in question, in which the speed is easier to measure. In rods where their diameter is shorter than a wavelength, the speed of pure pressure waves may be simplified and is given by:\nwhere \"E\" is Young's modulus. This is similar to the expression for shear waves, save that Young's modulus replaces the shear modulus. This speed of sound for pressure waves in long rods will always be slightly less than the same speed in homogeneous 3-dimensional solids, and the ratio of the speeds in the two different types of objects depends on Poisson's ratio for the material.\n\nIn a fluid, the only non-zero stiffness is to volumetric deformation (a fluid does not sustain shear forces).\n\nHence the speed of sound in a fluid is given by\nwhere \"K\" is the bulk modulus of the fluid.\n\nIn fresh water, sound travels at about at (see the External Links section below for online calculators). Applications of underwater sound can be found in sonar, acoustic communication and acoustical oceanography.\n\nIn salt water that is free of air bubbles or suspended sediment, sound travels at about ( at , 10°C and 3% salinity by one method). The speed of sound in seawater depends on pressure (hence depth), temperature (a change of ~ ), and salinity (a change of 1‰ ~ ), and empirical equations have been derived to accurately calculate the speed of sound from these variables. Other factors affecting the speed of sound are minor. Since in most ocean regions temperature decreases with depth, the profile of the speed of sound with depth decreases to a minimum at a depth of several hundred meters. Below the minimum, sound speed increases again, as the effect of increasing pressure overcomes the effect of decreasing temperature (right). For more information see Dushaw et al.\n\nA simple empirical equation for the speed of sound in sea water with reasonable accuracy for the world's oceans is due to Mackenzie:\nwhere\n\nThe constants \"a\", \"a\", …, \"a\" are\nwith check value for , , . This equation has a standard error of for salinity between 25 and 40 ppt. See Technical Guides. Speed of Sound in Sea-Water for an online calculator.\n\n(Note: The Sound Speed vs. Depth graph does \"not\" correlate directly to the MacKenzie formula.\nThis is due to the fact that the temperature and salinity varies at different depths.\nWhen T and S are held constant, the formula itself it always increasing.)\n\nOther equations for the speed of sound in sea water are accurate over a wide range of conditions, but are far more complicated, e.g., that by V. A. Del Grosso and the Chen-Millero-Li Equation.\n\nThe speed of sound in a plasma for the common case that the electrons are hotter than the ions (but not too much hotter) is given by the formula (see here)\nwhere\n\nIn contrast to a gas, the pressure and the density are provided by separate species, the pressure by the electrons and the density by the ions. The two are coupled through a fluctuating electric field.\n\nWhen sound spreads out evenly in all directions in three dimensions, the intensity drops in proportion to the inverse square of the distance. However, in the ocean, there is a layer called the 'deep sound channel' or SOFAR channel which can confine sound waves at a particular depth.\n\nIn the SOFAR channel, the speed of sound is lower than that in the layers above and below. Just as light waves will refract towards a region of higher index, sound waves will refract towards a region where their speed is reduced. The result is that sound gets confined in the layer, much the way light can be confined to a sheet of glass or optical fiber. Thus, the sound is confined in essentially two dimensions. In two dimensions the intensity drops in proportion to only the inverse of the distance. This allows waves to travel much further before being undetectably faint.\n\nA similar effect occurs in the atmosphere. Project Mogul successfully used this effect to detect a nuclear explosion at a considerable distance.\n\n\n"}
{"id": "27683520", "url": "https://en.wikipedia.org/wiki?curid=27683520", "title": "Sucker rod", "text": "Sucker rod\n\nA sucker rod is a steel rod, typically between 25 and 30 feet (7 to 9 meters) in length, and threaded at both ends, used in the oil industry to join together the surface and downhole components of a reciprocating piston pump installed in an oil well. The pumpjack is the visible above-ground drive for the well pump, and is connected to the downhole pump at the bottom of the well by a series of interconnected sucker rods. Sucker rods are also commonly available made of fiberglass in 37 1/2 foot lengths and diameters of 3/4, 7/8, 1, and 1 1/4 inch. These are terminated in metallic threaded ends, female at one end and male at the other.\n\nThe surface unit transfers energy for pumping the well from the prime-mover to the sucker rod string. In doing this, it must change the rotary motion of the prime-mover to reciprocating motion for the sucker rod. And it must reduce the speed of the prime-mover to a suitable pumping speed. Speed reduction is accomplished by using a gear reducer, and rotary motion of the crank shaft is converted to oscillatory motion by means of a walking beam. The crank arm is connected to the walking beam by means of a pitman arm. The walking beam is supported by a Samson post and saddle bearing. The horse head and bridle are used to ensure that the pull on the sucker rod string is vertical all times so that no bearing movement is applied to that part of sucker rod string above stuffing box. The polished rod and stuffing box combination is used to maintain a good liquid seal at the surface.\n\n\n\n"}
{"id": "158297", "url": "https://en.wikipedia.org/wiki?curid=158297", "title": "The Dragon in the Sea", "text": "The Dragon in the Sea\n\nThe Dragon in the Sea (1956), also known as Under Pressure from its serialization, is a novel by Frank Herbert. It was first serialized in \"Astounding\" magazine from 1955 to 1956, then reworked and published as a book in 1956. (A 1961 2nd printing of the Avon paperback, catalog # G-1092, was titled 21st Century Sub with the original title in parentheses.) It is usually classified as a psychological novel.\n\nIn a near-future earth, the West and the East have been at war for more than a decade, and resources are running thin. The West is stealing oil from the East with specialized nuclear submarines (\"subtugs\") that sneak into the underwater oil fields of the East to secretly pump out the oil and bring it back. Each manned by a crew of four, these submarines undertake the most hazardous, stressful mission conceivable, and of late, the missions have been failing, with the last twenty submarines simply disappearing.\n\nThe East has been very successful in planting sleepers in the West's military and command structures, and the suspicion is that sleepers are sabotaging the subs or revealing their positions once at sea. John Ramsey, a young psychologist from the Bureau of Psychology (BuPsych), is trained as an electronics operator and sent on the next mission, replacing the previous officer who went insane. His secret mission is to find the sleeper, or figure out why the crews are going insane.\n\nTypically for Herbert, psychology and religion (the title comes from a quote from the \"Book of Revelation\") play a large role in the narrative, as John Ramsey comes to understand the nature of the subtug crews and how they carry out their missions.\n\nThe technology of the submarines towing large bags filled with the surreptitiously pumped oil described in the books may have been an inspiration for the invention called the \"Dracone\" for which development started in the year following Herbert's serial.\n\n\"Galaxy\" reviewer Floyd C. Gale praised \"Dragon in the Sea\" as \"a dramatically fascinating story. . . . [a] tense and well-written novel.\" Algis Budrys described it as \"hypnotically fascinating,\" praising Herbert's \"intelligence, sophistication, [and] capacity for research\" as well as his \"ability to write clean prose as an unobtrusive but effective vehicle for a cleanly told story.\" Anthony Boucher found the novel \"as impressive in its cumulative depiction of a specialized scientific background as anything since Hal Clement's\" Mission of Gravity\".\" Spider Robinson, reviewing a mid-1970s reissue, faulted the novel's characterizations, saying \"there are no real people in it, only psychological types and syndromes walking around on legs.\"\n\nJ. Francis McComas praised the novel in \"The New York Times\", comparing it to Forester and Wouk and declaring, \"In this fine blend of speculation and action, Mr. Herbert has created a novel that ranks with the best of modern science fiction.\"\n\n\"The Dragon in the Sea\" tied for number thirty-four in the 1975 Locus All-Time Poll.\n\n"}
{"id": "42512625", "url": "https://en.wikipedia.org/wiki?curid=42512625", "title": "Toms River (book)", "text": "Toms River (book)\n\nToms River: A Story of Science and Salvation is a 2013 non-fiction book by the American author Dan Fagin. It is about the dumping of industrial pollution by chemical companies in Toms River, New Jersey beginning in 1952 through the 1980s, and the epidemiological investigations of a cancer cluster that subsequently emerged there. The book won the 2014 Pulitzer Prize for General Non-Fiction, the 2014 Helen Bernstein Book Award for Excellence in Journalism, and the 2014 National Academies Communication Award.\n\n\n"}
{"id": "3253842", "url": "https://en.wikipedia.org/wiki?curid=3253842", "title": "Transformity", "text": "Transformity\n\nThe concept of transformity was first introduced by David M. Scienceman in collaboration with Howard T. Odum. In 1987 Scienceman proposed that the phrases, \"energy quality\", \"energy quality factor\", and \"energy transformation ratio\", all used by H.T.Odum, be replaced by the word \"transformity\" (p. 261). This approach aims to solve a long standing issue about the relation of qualitative phenomena to quantitative phenomena often analysed in the physical sciences, which in turn is a synthesis of rationalism with phenomenology. That is to say that it aims to quantify quality.\n\nScienceman then defined transformity as,\n\n\"a quantitative variable describing the measurable property of a form of energy, its ability to amplify as feedback, relative to the source energy consumed in its formation, under maximum power conditions. As a quantitative variable analogous to thermodynamic temperature, transformity requires specification of units.\" (1987, p. 261. My emphasis).\n\nIn 1996 H.T.Odum defined transformity as,\n\n\"the emergy of one type required to make a unit of energy of another type. For example, since 3 coal emjoules (cej) of coal and 1 cej of services are required to generate 1 J of electricity, the coal transformity of electricity is 4 cej/J\"\n\nG.P.Genoni expanded on this definition and maintained that, \"the energy input of one kind required to sustain one unit of energy of another kind, is used to quantify hierarchical position\" (1997, p. 97). According to Scienceman, the concept of transformity introduces a new basic dimension into physics (1987, p. 261). However there is ambiguity in the dimensional analysis of transformity as Bastianoni et al. (2007) state that transformity is a dimensionless ratio.\n\nOne part of the rationalist viewpoint associated with modernity and science is to contrast qualitatively different phenomena under transformation through quantitative ratios, with the aim of uncovering any constancy amidst the transformation change. Like the efficiency ratio, transformity is quantitatively defined by a simple input-output ratio. However the transformity ratio is the inverse of efficiency and involves both indirect and direct energy flows rather than simply direct input-output energy ratio of energy efficiency. This is to say that it is defined as the ratio of emergy input to energy output.\n\nOriginal version:: formula_1\n\nHowever, it was realised that the term \"energy output\" refers to both the \"useful\" energy output and the \"non-useful\" energy output. (Note: that as given by P.K.Nag, an alternative name for 'useful energy' is 'availability' or exergy, and an alternative name for 'non-useful energy' is 'unavailability', or anergy (Nag 1984, p. 156)). But as E.Sciubba and S.Ulgiati observed, the notion of transformity meant to capture the emergy invested per unit product, or useful output. The concept of Transformity was therefore further specified as the ratio of \"input emergy dissipated (availability used up)\" to the \"unit output exergy\" (Sciubba and Ulgiati 2005, p. 1957). For Jørgensen (2000, p. 61) transformity is a strong indicator of the efficiency of the system.\n\nRevised version: formula_2 or formula_3\n(after Giannantoni 2002, p. 8).\n\nSubstituting in the mathematical definition of emergy given in that article.\n\nAlbertina Lourenci and João Antonio Zuffo from the Department of Electronic Systems Engineering at São Paulo have posited that there are two transformity values; formula_5 and formula_6 (Lourenci and Zuffo 2004, p. 411).\n\n\nUnder these definitions \"emergy\" can always be structured as follows:\n\nformula_9\n\n"}
{"id": "19795082", "url": "https://en.wikipedia.org/wiki?curid=19795082", "title": "Tree-free paper", "text": "Tree-free paper\n\nTree-free paper or tree-free newsprint describes an alternative to wood-pulp paper by its raw material composition. It is claimed to be more eco-friendly considering the product's entire life cycle.\n\nSources of fiber for tree-free paper include:\n\n\nNon-fibre sources include:\n\n\nPaper manufacturing is highly competitive, with historically tight margins and small operating profits. As a result, the raw materials used to make paper have to be very cost effective, using cheap, scalable renewable resources, coupled with relatively inexpensive ways to deliver large quantities to market. Until recently, commercial tree farming, has been shaped to account for these tight operating margins and supply cost limitations. Virtually all paper, however, requires massive cutting, replanting and re-cutting of wide swaths of forest. These limitations have made wood pulped farm grown supply stock the paper industry's overwhelming scalable raw material of choice.\n\nThe paper industry's answer to \"tree free\" paper has been focused on \"recycled waste paper\" as a tree free alternative even though the vast majority of \"recycled waste paper\" originally started its life cycle from tree grown pulp.\n\nFiber dense agricultural residues, have been known as a pulp substitute for years. Commercial low cost production technology coupled with limited resource abundancy plus low cost transportation to commercial business markets had created a barrier, virtually relegating true \"tree free\" paper from developing into anything more than small niche markets with even smaller niche market players. Furthermore, grasses and annual plants often have high silica contents. Silica is problematic as it consumes pulping chemicals and produces fly ash when burned.\n\n"}
{"id": "35332520", "url": "https://en.wikipedia.org/wiki?curid=35332520", "title": "Waag, Amsterdam", "text": "Waag, Amsterdam\n\nThe Waag (\"weigh house\") is a 15th-century building on Nieuwmarkt square in Amsterdam. It was originally a city gate and part of the walls of Amsterdam. The building has also served as a guildhall, museum, fire station and anatomical theatre, among other things.\n\nThe Waag is the oldest remaining non-religious building in Amsterdam. The building has held \"rijksmonument\" status since 1970.\n\nThe Waag is depicted in Rembrandt's 1632 painting \"The Anatomy Lesson of Dr. Nicolaes Tulp\". The surgeons' guild commissioned this painting for their guildhall in the Waag.\n\nOriginally, the building was one of the gates in the city wall, the Sint Antoniespoort (Saint Anthony's Gate). The gate was located at the end of the Zeedijk dike, which continued beyond the gate as the Sint Antoniesdijk. After the Lastage area was added to the city in the 16th century, the Sint Antoniesdijk became the Sint Antoniesbreestraat and a new Sint Antoniespoort city gate was built near the Hortus Botanicus.\n\nThe city gate was part of the medieval city walls along the moat formed by the current Singel canal and the canals of the Kloveniersburgwal and the Geldersekade. These walls were constructed during the period 1481–1494 and consisted of defensive towers and city gates connected by walls of brick with a natural stone pediment. All that remains of the walls is some sandstone in the Geldersekade canal wall. The only remains of the city gates are the Waag and part of the Regulierspoort gate, which is now the bottom half of the Munttoren tower. The Schreierstoren is the only remaining defensive tower.\n\nThe oldest gable stone in Amsterdam adorns the facade of the tower at the corner of Zeedijk and Geldersekade. It carries the inscription \"MCCCCLXXXVIII de XXVIII dach in April wart d'eerste steen van dese poert gheleit.\" (\"The first stone of this gate was laid on 28 April 1488\"). The year of construction 1488, as given on the gable stone and in many sources, may not actually be correct. There are clues that the gate may be of a much older date. For instance, there are several documents in the city archives of Amsterdam that pre-date 1488 and mention Saint Anthony's Gate. According to building archaeologist Jacqueline de Graauw, the building probably dates back to as early as 1425, because that is when the city was expanded and the Geldersekade and Kloveniersburgwal canals, at which the gate was placed, were dug.\n\nDe Graauw also found that Saint Anthony's Gate was originally much smaller, and was heightened at a later date. This is evident from the remains of merlons halfway up the towers of the Guild of Saint Eligius and the \"schuttersgilde\" (militia) - the two big towers on either side of the main gate. Also, the front gate, which differs from the main gate in several ways, was probably added to the main gate at a later date. These kinds of additions were commonplace at that time, as a protection against the increasing threat posed by canons. In Haarlem, for example, a front gate was added to the Amsterdam Gate in 1482 which is very similar to the front gate of Saint Anthony's Gate. Presumably the gable stone in Saint Anthony's Gate with the date 1488 refers simply to the addition of the front gate to the already existing main gate.\n\nFrom around the beginning of the 16th century, when Amsterdam had completed its surrounding stone city wall, Saint Anthony's Gate appeared as it is depicted in the wood carvings of Cornelis Anthonisz: a main gate with four towers on the inner (city) side — of which the masons' guild tower was still small — and a front gate with two towers on the outer (canal) side. Between the front gate and main gate, there was a small square covering a subterraneous sluice gate. The walls of the towers are almost 2 metres thick.\n\nWhen the city expanded beyond its walls the late 16th century, Saint Anthony's Gate lost its function as a city gate. Shortly thereafter, during the years 1603–1613, the walls were demolished. In 1614, the present Nieuwmarkt square was created by covering the canal on either side of the gate. In addition, the square was raised, causing part of the brickwork of the gate to disappear below ground. This makes the building appear shorter than it actually is.\n\nIn 1617, the former city gate was repurposed as a weigh house, a public building where various goods were weighed. This new weigh house was needed to relieve the \"Waag op de Dam\", the original weigh house on Dam square, which had become too small for the needs of the rapidly growing city.\n\nAn inner courtyard was added in 1617–1618 by covering the area between the front and main gates. A number of guilds were housed on the top floors of the building: the blacksmiths' guild, the painter's guild, the masons' guild and the surgeons' guild. Each guild had its own entrance gate. The guild emblems are still visible over these entrances. The gate of the masons' guild includes sculpture work by Hendrick de Keyser. Over the entrance for the surgeons' guild is the inscription \"Theatrum Anatomicum\".\n\nIn 1690–1691, a large dome-shaped hall was added, topped by a central octagonal tower. The interior also dates to this time period.\n\nThe Waag eventually lost its function as a weigh house. In 1819, a chest of indigo was the last item to be weighed there.\n\nAfter falling into disuse as a weigh house, the Waag served a range of different functions. In the 19th century it was used consecutively as a fencing hall, a furniture workshop, a workshop for oil lamps used for street lighting, a fire station, and as the city archives. In the first half of the 19th century, punishments were carried out in front of the building. There was even a guillotine.\n\nIn the 20th century, the building was used primarily as a museum. It was the original location of the Amsterdams Historisch Museum (now Amsterdam Museum) as well as the Joods Historisch Museum (Jewish Historical Museum). In the period 1989–1994, the building was not used and stood empty. Eventually the building was handed over to a foundation, Stichting Centrum De Waag, which commissioned Philippe Starck to design a glass extension that would have required part of the outer wall to be demolished. However, the foundation went bankrupt before these plans were carried out.\n\nOn 20 September 1991, local residents and preservationists opened the disused building to the press and the public. A general sense of dismay, which also resounded in the city council, led to the appointment of a commission of experts, which proposed to have the building restored under the guidance of an architect with expert knowledge of medieval construction and foundation. was appointed to lead the restoration.\n\nDuring restoration, the cellars (which had been filled in) were reopened, a wooden awning was added to the eastern facade. The paving around the building was changed so that de Waag again became the centre point on Nieuwmarkt square.\n\nFollowing the restoration, the building was rented out. Waag Society, a foundation that aims to foster experimentation with new technologies, art and culture, is housed on the upper floors. The ground floor is now a café and restaurant.\n\nThe building is slowly sinking due to the porous soil and the cement that was used for the restoration. In July 2009 it was falsely reported in the media that one of the towers was about to collapse. However, the foundation will need to be improved or replaced in order to remedy the situation. In February 2011, a frame was built around the tower to support it and prevent it from collapsing.\n\n\n\n"}
