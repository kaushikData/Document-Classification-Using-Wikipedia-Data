{"id": "48483903", "url": "https://en.wikipedia.org/wiki?curid=48483903", "title": "Asia Power Sapugaskanda Power Station", "text": "Asia Power Sapugaskanda Power Station\n\nThe Asia Power Sapugaskanda Power Station (also sometimes referred to as Asia Power Station) is a thermal power station in Sapugaskanda, Sri Lanka. Planning for the fuel oil-run power station dated back to 1994, when the Ceylon Electricity Board issued a tender for an IPP project for 50 megawatts. Construction of the plant began in 1996 and was commissioned in June 1998, with a PPA of 20-years. The power station utilizes eight generating units.\n"}
{"id": "35094904", "url": "https://en.wikipedia.org/wiki?curid=35094904", "title": "Austral Líneas Aéreas Flight 901", "text": "Austral Líneas Aéreas Flight 901\n\nAustral Líneas Aéreas Flight 901 crashed in a river near Buenos Aires after succumbing to a thunderstorm. All 31 people on the BAC 1-11 were killed in the accident.\n\nThe flight took off for Buenos Aires at 9:11 local time. The flight was uneventful until final approach. At 10:42, the flight was cleared to land at Runway 13 of Aeroparque Jorge Newbery. Visibility was reducing, and eventually the pilots lost sight of the runway and aborted the landing. The flight then conducted a go-around and started its second approach. When approaching the NDB, the flight was cleared to hold at 600 metres due to a cumulonimbus cloud ahead. They were then cleared for a straight-in approach to Runway 31. Shortly after, the crew lost control of the plane and it crashed into a river. All 26 passengers and five crew were killed in the disaster.\n\nInvestigations were hampered by the fact that only 55-65% of wreckage was recovered and after 42 days of searching, the cockpit voice recorder and the flight data recorder were not recovered. The final investigation report blamed the pilots for underestimating the intensity of the storm.\n"}
{"id": "1018637", "url": "https://en.wikipedia.org/wiki?curid=1018637", "title": "Beach nourishment", "text": "Beach nourishment\n\nBeach nourishment (also referred to as beach renourishment, beach replenishment, or sand replenishment) describes a process by which sediment, usually sand, lost through longshore drift or erosion is replaced from other sources. A wider beach can reduce storm damage to coastal structures by dissipating energy across the surf zone, protecting upland structures and infrastructure from storm surges, tsunamis and unusually high tides. Beach nourishment is typically part of a larger coastal defense scheme. Nourishment is typically a repetitive process since it does not remove the physical forces that cause erosion but simply mitigates their effects.\n\nThe first nourishment project in the United States was at Coney Island, New York in 1922 and 1923. It is now a common shore protection measure used by public and private entities.\nNourishment is one of three commonly accepted methods for protecting shorelines. The \"structural\" alternative involves constructing a seawall, revetment, groyne or breakwater. Alternatively, with \"managed retreat\" the shoreline is left to erode, while relocating buildings and infrastructure further inland. \"Nourishment\" gained popularity because it preserved beach resources and avoided the negative effects of hard structures. Instead, nourishment creates a “soft” (i.e., non-permanent) structure by creating a larger sand reservoir, pushing the shoreline seaward.\n\nBeaches can erode both naturally and due to human impacts (beach theft/sand mining).\n\nErosion is a natural response to storm activity. During storms, sand from the visible beach submerges to form sand bars that protect the beach. Submersion is only part of the cycle. During calm weather smaller waves return sand from bars to the visible beach surface in a process called accretion.\n\nSome beaches do not have enough sand available to coastal processes to respond naturally to storms. When not enough sand is available, the beach cannot recover following storms.\n\nMany areas of high erosion are due to human activities. Reasons can include: seawalls locking up sand dunes, coastal structures like ports and harbors that prevent longshore transport, dams and other river management structures. Continuous, long-term renourishment efforts, especially in cuspate-cape coastlines, can play a role in longshore transport inhibition and downdrift erosion. These activities interfere with the natural sediment flows either through dam construction (thereby reducing riverine sediment sources) or construction of littoral barriers such as jetties, or by deepening of inlets; thus preventing longshore transport of sediment.\n\nThe proportion of total sand in a beach that lies below the waterline (submersion fraction) critically impacts beach nourishment. Two beaches with the same amount of visible sand may be much different below the surface. An eroded beach with substantial submerged sand surrounding it may recover without nourishment. Nourishing a beach that has little submerged sand requires understanding of the reason that the submerged sand is missing. The same forces that stripped the submerged sand once are likely to do so again. The amount of submerged sand eroded is typically much greater than the amount of missing sand on shore. Replacing only the visible sand is insufficient unless the submerged sand is also replaced. Otherwise, the beach is unstable and the replenished sand quickly erodes. If human activity is a major cause of the erosion, mitigating that activity may be more cost effective over both short and long term periods than direct nourishment. \n\nSand fill must be compatible with native beach sand.\n\n\"Beach Profile Nourishment\" describes programs that nourish the full beach profile. In this instance, \"profile\" means the slope of the uneroded beach from above the water out to sea. The Gold Coast profile nourishment program placed 75% of its total sand volume below low water level. Some coastal authorities \"overnourish\" the below water beach (aka \"nearshore nourishment\") so that over time the natural beach increases in size. These approaches do not permanently protect beaches eroded by human activity, which requires that activity to be mitigated.\n\nThe selection of suitable material for a particular project depends upon the design needs, environmental factors and transport costs, considering both short and long-term implications.\n\nThe most important material characteristic is the sediment grain size, which must closely match the native material. Excess silt and clay fraction (mud) versus the natural turbidity in the nourishment area disqualifies some materials. Projects with unmatched grain sizes performed relatively poorly. Nourishment sand that is only slightly smaller than native sand can result in significantly narrower equilibrated dry beach widths compared to sand the same size as (or larger than) native sand. Evaluating material fit requires a sand survey that usually includes geophysical profiles and surface and core samples.\n\nSome beaches were nourished using a finer sand than the original. Thermoluminescence monitoring reveals that storms can erode such beaches far more quickly. This was observed at a Waikiki nourishment project in Hawaii.\n\nAdvantages:\n\n\nDisadvantages:\n\n\nBeach nourishment has significant impacts on local ecosystems. Nourishment may cause direct mortality to sessile organisms in the target area by burying them under the new sand. Seafloor habitat in both source and target areas are disrupted, e.g., when sand is deposited on coral reefs or when deposited sand hardens. Imported sand may differ in character (chemical makeup, grain size, non-native species) from that of the target environment. Light availability may be reduced, affecting nearby reefs and submerged aquatic vegetation. Imported sand may contain material toxic to local species. Removing material from near-shore environments may destabilize the shoreline, in part by steepening its submerged slope. Related attempts to reduce future erosion may provide a false sense of security that increases development pressure.\n\nNewly deposited sand can harden and complicate nest-digging for turtles. However, nourishment can provide more/better habitat for them, as well as for sea birds and beach flora. Florida addressed the concern that dredge pipes would suck turtles into the pumps by adding a special grill to the dredge pipes.\n\nNourishment is not the only technique used to address eroding beaches. Others can be used singly or in combination with nourishment, driven by economic, environmental and political considerations.\n\nHuman activities such as dam construction can interfere with natural sediment flows (thereby reducing riverine sediment sources.) Construction of littoral barriers such as jetties and deepening of inlets can prevent longshore sediment transport.\n\nThe structural approach attempts to prevent erosion. Armoring involves building revetments, seawalls, detached breakwaters, groins, etc. Structures that run parallel to the shore (seawalls or revetments) prevent erosion. While this protects structures, it doesn't protect the beach that is outside the wall. The beach generally disappears over a period that ranges from months to decades.\n\nGroynes and breakwaters that run perpendicular to the shore protect it from erosion. Filling a breakwater with imported sand can stop the breakwater from trapping sand from the littoral stream (the ocean running along the shore.) Otherwise the breakwater may deprive downstream beaches of sand and accelerate erosion there.\n\nArmoring may restrict beach/ocean access, enhance erosion of adjacent shorelines, and requires long-term maintenance.\n\nManaged retreat moves structures and other infrastructure inland as the shoreline erodes. Retreat is more often chosen in areas of rapid erosion and in the presence of little or obsolete development.\n\nAppropriately constructed and sited fences can capture blowing sand, building/restoring sand dunes, and progressively protecting the beach from the wind, and the shore from blowing sand.\n\nAll beaches grow and shrink depending on tides, precipitation, wind, waves and current. Wet beaches tend to lose sand. Waves infiltrate dry beaches easily and deposit sandy sediment. Generally a beach is wet during falling tide, because the sea sinks faster than the beach drains. As a result, most erosion happens during falling tide. Beach drainage (beach dewatering) using Pressure Equalizing Modules (PEMs) allow the beach to drain more effectively during falling tide. Fewer hours of wet beach translate to less erosion. Permeable PEM tubes inserted vertically into the foreshore connect the different layers of groundwater. The groundwater enters the PEM tube allowing gravity to conduct it to a coarser sand layer, where it can drain more quickly. The PEM modules are placed in a row from the dune to the mean low waterline. Distance between rows is typically but this is project-specific. PEM systems come in different sizes. Modules connect layers with varying hydraulic conductivity. Air/water can enter and equalize pressure.\n\nPEMs are minimially invasive, typically covering approximately 0.00005% of the beach. The tubes are below the beach surface, with no visible presence. PEM installations have been installed on beaches in Denmark, Sweden, Malaysia and Florida . The effectiveness of beach dewatering, however, is debatable and has not been proven convincingly on life-sized beaches.\n\nNourishment is typically a repetitive process, since nourishment mitigates the effects of erosion, but does not remove the causes. A benign environment increases the interval between nourishment projects, reducing costs. Conversely, high erosion rates may render nourishment financially impractical.\n\nIn many coastal areas, the economic impacts of a wide beach can be substantial. The –long shoreline fronting Miami Beach, Florida was replenished over the period 1976–1981. The project cost approximately $64,000,000 and revitalized the area's economy. Prior to nourishment, in many places the beach was too narrow to walk along, especially during high tide.\n\nThe first nourishment project in the U.S. was constructed at Coney Island, New York in 1922–1923.\n\nThe setting of a beach nourishment project is key to design and potential performance. Possible settings include a long straight beach, an inlet that may be either natural or modified and a pocket beach. Rocky or seawalled shorelines, that otherwise have no sediment, present unique problems.\n\nFederal and state governments in Mexico have invested about $71 million ($957 million pesos) throughout the state of Quintana Roo in restoring the beaches along Cancun, Playa del Carmen and Cozumel.\n\nHurricane Wilma hit the beaches of Cancun and the Riviera Maya in 2005. The initial nourishment project was unsuccessful, leading to a second round that began in September 2009 and was scheduled to complete in early 2010. The project designers and the government committed to invest in beach maintenance to address future erosion. Project designers considered factors such as the time of year and sand characteristics such as density. Restoration in Cancun was expected to deliver of sand to replenish of coastline.\n\nGold Coast beaches in Queensland, Australia have experienced periods of severe erosion. In 1967 a series of 11 cyclones removed most of the sand from Gold Coast beaches. The Government of Queensland engaged engineers from Delft University in the Netherlands to advise them. The 1971 Delft Report outlined a series of works for Gold Coast Beaches, including beach nourishment and an artificial reef. By 2005 most of the recommendations had been implemented.\n\nThe Northern Gold Coast Beach Protection Strategy (NGCBPS) was an A$10 million investment. NGCBPS was implemented between 1992 and 1999 and the works were completed between 1999 and 2003. The project included dredging of compatible sand from the Gold Coast Broadwater and delivering it through a pipeline to nourish of beach between Surfers Paradise and Main Beach. The new sand was stabilized by an artificial reef constructed at Narrowneck out of huge geotextile sand bags. The new reef was designed to improve wave conditions for surfing. A key monitoring program for the NGCBPS is the ARGUS coastal camera system.\n\nThe cost/benefit ratio for NGCBPS was conservatively estimated at 75:1 for a A$10 million investment into beach replenishment. The benefits were estimated from a model of lost visitor nights in hotels following previous erosion events. NGCBPS so improved beach health that recovery following minor and moderate storms occurred within weeks. Additional unquantified benefits included lifestyle benefits for residents, additional public open space and improved fishing, diving and surfing conditions.\n\nMore than one-quarter of the Netherlands is below sea level and about 81% of the coast consists of sand dune or beach. The shoreline is closely monitored by yearly recording of the cross section at points apart, to ensure adequate protection. Where long-term erosion is identified, beach nourishment using high-capacity suction dredgers is deployed.\n\nHawaii planned to replenish Waikiki beach in 2010. Budgeted at $2.5 million, the project covered in an attempt to return the beach to its 1985 width. Prior opponents supported this project, because the sand was to come from nearby shoals, reopening a blocked channel and leaving the overall local sand volume unchanged, while closely matching the \"new\" sand to existing materials. The project planned to apply up to of sand from deposits located offshore at a depth of . The project was larger than the prior recycling effort in 2006-07, which moved .\n\nMaui, Hawaii illustrated the complexities of even small-scale nourishment projects. A project at Sugar Cove transported upland sand to the beach. The sand allegedly was finer than the original sand and contained excess silt that enveloped coral, smothering it and killing the small animals that lived in and around it. As in other projects, on-shore sand availability was limited, forcing consideration of more expensive offshore sources.\n\nA second project, along Stable Road, that attempted to slow rather than halt erosion, was stopped halfway toward its goal of adding of sand. The beaches had been retreating at a \"comparatively fast rate\" for half a century. The restoration was complicated by the presence of old seawalls, groins, piles of rocks and other structures.\n\nThis project used sand-filled geotextile tube groins that were originally to remain in place for up to 3 years. A pipe was to transport sand from deeper water to the beach. The pipe was anchored by concrete blocks attached by fibre straps. A video showed the blocks bouncing off the coral in the current, killing whatever they touched. In places the straps broke, allowing the pipe to move across the reef, \"planing it down\". Bad weather exacerbated the damaging movement and killed the project. The smooth, cylindrical geotextile tubes could be difficult to climb over before they were covered by sand.\n\nSupporters claimed that 2010's seasonal summer erosion was less than in prior years, although the beach was narrower after the restoration ended than in 2008. Authorities were studying whether to require the project to remove the groins immediately. Potential alternatives to geotextile tubes for moving sand included floating dredges and/or trucking in sand dredged offshore.\n\nA final consideration was sea level rise and that Maui was sinking under its own weight. Both Maui and Hawaii Island surround massive mountains (Haleakala, Mauna Loa, and Mauna Kea) and were expanding a giant dimple in the ocean floor, some below the mountain summits.\n\n90 PEMs were Installed in February 2008 at Hillsboro Beach. After 18 months the beach had expanded significantly. Most of the PEMs were removed in 2011. Beach volume expanded by 38,500 cubic yards over 3 years compared to an average annual loss of 21,000.\n\nThe beach in Gold Coast was built as an artificial beach in the 1990s with HK$60m. Sands are supplied periodically, especially after typhoons, to keep the beach viable.\n\nNourishment projects usually involve physical, environmental and economic objectives.\n\nTypical physical measures include dry beach width/height, post-storm sand volume, post-storm damage avoidance assessments and aqueous sand volume.\n\nEnvironmental measures include marine life distribution, habitat and population counts.\n\nEconomic impacts include recreation, tourism, flood and \"disaster\" prevention.\n\nMany nourishment projects are advocated via economic impact studies that rely on additional tourist expenditure. This approach is however unsatisfactory. First, nothing proves that these expenditures are incremental (they could shift expenditures from other nearby areas). Second, economic impact does not account for costs and benefits for all economic agents, as cost benefit analysis does. Techniques for incorporating nourishment projects into flood insurance costs and disaster assistance remain controversial.\n\nThe performance of a beach nourishment project is most predictable for a long, straight shoreline without the complications of inlets or engineered structures. In addition, predictability is better for overall performance, e.g., average shoreline change, rather than shoreline change at a specific location.\n\nNourishment can affect eligibility in the U.S. National Flood Insurance Program and federal disaster assistance.\n\nNourishment may have the unintended consequence of promoting coastal development, which increases risk of other coastal hazards.\n\n\n\n"}
{"id": "40133589", "url": "https://en.wikipedia.org/wiki?curid=40133589", "title": "Beyyurdu Dam", "text": "Beyyurdu Dam\n\nThe Beyyurdu Dam is a gravity dam under construction on the Bembo River (an eventual tributary of the Great Zab) in Beyyurdu, Şemdinli district of Hakkâri Province, southeast Turkey. \n\nUnder contract from Turkey's State Hydraulic Works, Özdoğan Group began construction on the dam in 2008 and a completion date has not been announced. In July 2012 and May 2014 suspected Kurdistan Workers' Party (PKK) militants set fire to equipment at the construction sites of the Beyyurdu Dam and also the Aslandağ Dam which is located upstream.\n\nThe reported purpose of the dam is water storage and it can also support a hydroelectric power station in the future. Another purpose of the dam which has been widely reported in the Turkish press is to reduce the freedom of movement of PKK militants. Blocking and flooding valleys in close proximity to the Iraq–Turkey border is expected to help curb cross-border PKK smuggling and deny caves in which ammunition can be stored. A total of 11 dams along the border; seven in Şırnak Province and four in Hakkâri Province were implemented for this purpose. In Hakkâri are the Gölgeliyamaç (since cancelled) and Çocuktepe Dams on the Güzeldere River and the Aslandağ and Beyyurdu Dams on the Bembo River. In Şırnak there is the Silopi Dam on the Hezil River and the Şırnak, Uludere, Balli, Kavşaktepe, Musatepe and Çetintepe Dams on the Ortasu River.\n\n"}
{"id": "29459414", "url": "https://en.wikipedia.org/wiki?curid=29459414", "title": "Bukowsko–Nowotaniec wind farm", "text": "Bukowsko–Nowotaniec wind farm\n\nThe Bukowsko–Nowotaniec wind farm south of village Bukowsko, Poland, is the second wind-farm project planned by Martifer Renewables and REpower in Poland. \nThe project \"Bukowsko\" comprises eight REpower MM92 turbines, each with a rated output of 2 megawatts (MW). This is the first wind-farm project undertaken in Poland by Germany's third-largest wind-turbine manufacturer. The projects is located in the south-east of the Sanok county \n"}
{"id": "17680280", "url": "https://en.wikipedia.org/wiki?curid=17680280", "title": "CS Energy", "text": "CS Energy\n\nCS Energy is an Australia-based electricity generating company owned by the Government of Queensland with its head office located in Fortitude Valley, Brisbane. The company was established in 1997 and employs more than 400 staff.\n\nThe company's generation portfolio comprises coal-fired and pumped storage hydroelectric power stations. CS Energy has a trading portfolio of more than 4,035 megawatts in Australia's national electricity market. At present, the company owns and operates Kogan Creek Power Station, Wivenhoe Power Station, and Callide A and Callide B, and together with InterGen (in a joint venture, managed by Callide Power Management) 50% of Callide C Power Station.\n\nCS Energy was awarded A$32 million in 2010 to help construct Australia's first large-scale solar thermal project at Kogan Creek Power Station. Funding was provided by the Rudd Government under the Renewable Energy Demonstration Program. However, in March 2016, it was announced that CS Energy would 'pull the plug' on that project, at a loss of $40M to CS Energy and $6M to ARENA.\n\nIn 2007, the Chief Executive Officer, Mark Chatfield, was stood down by the board after allegations of inappropriate share dealings were raised.\n\n\n"}
{"id": "48517532", "url": "https://en.wikipedia.org/wiki?curid=48517532", "title": "Calorimetric Electron Telescope", "text": "Calorimetric Electron Telescope\n\nThe CALorimetric Electron Telescope (CALET) is a space telescope being mainly used to perform high precision observations of electrons and gamma rays. It tracks the trajectory of electrons, protons, nuclei, and gamma rays and measures their direction, charge and energy, which may help understand the nature of dark matter or nearby sources of high-energy particle acceleration.\n\nThe mission was developed and sponsored by the Japan Aerospace Exploration Agency (JAXA), involving teams from Japan, Italy, and the United States. CALET was launched aboard JAXA's H-II Transfer Vehicle Kounotori 5 (HTV-5) on August 19, 2015, and was placed on the International Space Station's Japanese Kibo module.\n\nCALET is an astrophysics mission that searches for signatures of dark matter and provides the highest energy direct measurements of the cosmic ray electron spectrum in order to observe discrete sources of high-energy particle acceleration in our local region of the galaxy. The mission was developed and sponsored by the Japan Aerospace Exploration Agency (JAXA), involving teams from Japan, Italy, and the United States. It seeks to understand the mechanisms of particle acceleration and propagation of cosmic rays in the Milky Way galaxy, to identify their sources of acceleration, their elemental composition as a function of energy, and possibly to unveil the nature of dark matter. Such sources seem to be able to accelerate particles to energies far higher than scientists can achieve on Earth using the largest accelerators. Understanding how nature does this is important to space travel and has possible applications here on Earth. The CALET Principal Investigator is Shoji Torii from the Waseda University, Japan; John Wefel is the co-principal investigator for the US team; Pier S. Marrocchesi, is the co-investigator from the Italy team.\n\nUnlike optical telescopes, CALET operates in a scanning mode. It records each cosmic ray event that enters its field of view and triggers its detectors to take measurements of the cosmic ray in the extremely high energy region of Tera electron volt (TeV, one trillion electron volts). These measurements are recorded on the space station and sent to a ground station at Louisiana State University for analyses. CALET may also yield evidence of rare interactions between matter and dark matter by working in synergy with the Alpha Magnetic Spectrometer (AMS) —also aboard the ISS— that is looking at positrons and antiprotons to identify dark matter. Observation will be carried out for 2–5 years.\n\nCALET contains a subpayload CIRC (Compact Infrared Camera) to observe Earth surface, to detect forest fires.\n\nThe objectives are to understand the following:\n\nAs a cosmic ray observatory, CALET aims to clarify high energy space phenomena and dark matter from two perspectives; one is particle creation and annihilation in the field of particle physics (or nuclear physics) and the other is particle acceleration and propagation in the field of space physics.\n\nCALET first published data on half a million electron and positron cosmic ray events in 2017, finding a spectral index of −3.152 ± 0.016 above 30 GeV.\n\n\n"}
{"id": "57352085", "url": "https://en.wikipedia.org/wiki?curid=57352085", "title": "Canada Tornado Alley", "text": "Canada Tornado Alley\n\nThe Canadian Tornado Alley(s) is a term describing a tornado-prone region in Canada. In Canada, the word is rarely used when compared to that of the United States where their \"Tornado Alley\" is largely a media-driven term.\n\nAlthough the official boundaries of the Canadian Tornado Alleys are not clearly defined, its core extends from Central Alberta through Saskatchewan, Southern Manitoba and Northwestern Ontario. A second tornado alley extends from Michigan to Central Ontario and from Central Ontario to Southwestern Quebec. The Canadian Prairies are often considered part of the United States tornado alley.\n\nOver the years, the location(s) of the Canadian Tornado Alley(s) have been better defined. No complete definition of Canadian Tornado Alley has ever been officially designated and announced by the Environment and Climate Change Canada (ECCC). \n\nAccording to the National Severe Storms Laboratory (NSSL) FAQ, \"Tornado Alley\" is a term used by the media as a reference to areas that have higher numbers of tornadoes. \n\nTechnically speaking, no province or territory is entirely free of tornadoes; however, they occur more frequently in the Central Canada and Southern Canada, between the Rocky Mountains and the Great Lakes and between the Great Lakes and the St. Lawrence River. \n\nOntario reports the most tornadoes of any province due to its large population, in addition to its proximity to Canadian Tornado Alley. Saskatchewan, Alberta and Manitoba rank in second place behind Ontario but rank before Quebec. \n\nCanadian Tornado Alley can also be defined as an area reaching from the Rocky Mountains to the St. Lawrence River, excepting Northeastern Ontario.\n\nIn the Canadian Tornado Alley in Central/Western Canada, warm, humid air from the United States meets cool, dry air from Northern Canada and the Rocky Mountains. This creates an ideal environment for severe thunderstorms which could produce tornadoes. In the Great Lakes region towards the St. Lawrence River, severe thunderstorms which could produce tornadoes forms from warm, humid air from the United States and cooler air across the Great Lakes, these are triggered alongside lake-breeze fronts. The amount of precipitation in storms around the Great Lakes is normally more than the prairies as the warm temperatures and Great Lakes fuel the storms with heavy precipitation right from the lakes. \n\n"}
{"id": "863472", "url": "https://en.wikipedia.org/wiki?curid=863472", "title": "Chrysler Voyager", "text": "Chrysler Voyager\n\nThe Chrysler Voyager or Chrysler Grand Voyager (since 2011 re-badged as Lancia Voyager in most of Europe) is a luxury minivan manufactured by Chrysler. For most of its existence, vehicles bearing the \"Chrysler Voyager\" nameplate have been sold exclusively outside the United States, primarily in Europe and Mexico.\n\nThe Voyager was introduced in Europe in 1988 as a rebadged version of the Dodge Caravan and Plymouth Voyager sold in the United States, and has evolved with the Caravan, Plymouth Voyager, and Chrysler Town & Country since. Vehicles bearing the Chrysler Voyager nameplate were marketed in the United States from 2001 to 2003 as a rebadged version of the short-wheelbase (SWB) variant of the Plymouth Voyager following the 2001 folding of the Plymouth division of DaimlerChrysler AG.\n\nTogether with its nameplate variants, the Chrysler minivans have ranked as the 13th bestselling automotive nameplate worldwide, with over 12 million sold.\n\nThe European Chrysler Voyager was first released in 1988, nearly identical to its American counterpart, the Plymouth Voyager; the only visual differences between the two were the head/taillights and grille. Besides the slightly different appearance, the European Voyagers were sold with different engines, including diesel engines, which are popular in Europe; and the trim was different. They were also available with manual transmission and a foot operated emergency brake.\n\nThe last European Chrysler Grand Voyagers are very similar to the 2008 and later Chrysler Town & Country cars, and were sold only in the long-wheelbase version (as in North America).\n\nAlthough now produced solely in Ontario, Canada, the Grand Voyagers were still available with diesel engines as standard. These diesel engines are based on a modern double overhead cam common rail design from VM Motori of Italy.\n\nFollowing the fifth generation, the Grand Voyager nameplate was discontinued in all markets with the exception of China, where it is used on a rebadged Chrysler Pacifica.\n\n1988–1990 models sold in Europe were Dodge Caravans rebranded as Chryslers. In America, the Caravan was sold alongside a similar Plymouth Voyager counterpart. Europe's Chrysler Voyager was nearly identical to the American Dodge Caravan except that a turbocharged four-cylinder engine and the 3.3 L Chrysler \"EGA\" V6 were never made available.\n\nIntroduced for the 1991 model year, the Chrysler Voyager in Europe continued to be identical to the Dodge Caravan in the United States except that the 3.8 L V6 was not available for the Chrysler Voyager. This would be the final generation available with a manual transmission. A 2.5 L turbo diesel four-cylinder engine produced by VM Motori was made available beginning in 1994. There were also military modifications available for the Voyager in South Africa, which included large fuel tanks available in 240 and 360 liter capacities.\n\nThe 1996–1999 models in Mexico are rebadged Dodge Caravans, although the Caravan was sold alongside the Voyager. For 2000, the Chrysler Voyager was identical to the Plymouth Voyager except that the 3.8 L V6 was not available. Base models of the Voyager were offered in most states with either a 2.4 L four-cylinder or a 3.0 L Mitsubishi V6 engine, except in California and several northeastern states, where the Mitsubishi V6 didn't meet emissions standards. In those locales, the 3.3 L engine was offered instead.\nFor the European market, Voyagers continued to be rebadged Caravans. Unique to this market were 2.0 L Straight-4 SOHC and DOHC engines and 2.5 L turbo diesel produced by VM Motori. European market vans also came with manual transmissions and in a six-passenger model with six captains chairs, not available elsewhere.\n\n\nAccording to EuroNCAP crash test results, the 1999 model Chrysler Voyager did so badly in the frontal impact that it earned no points, making it the worst of the group. The body structure became unstable and the steering column was driven back into the driver's chest and head'. The 2007 model Chrysler Voyager fared little better, achieving just 19% in the frontal impact test, with an overall score of 2 stars out of a possible 5. However, chest compression measurements on the test dummy 'indicated an unacceptably high risk of serious or fatal injury. As a result, the final star in the adult occupant rating is struck-through'.\n\nDespite the bad results in the Euro NCAP crash tests, statistics from the real world indicate that this is not the whole picture. Folksam is a Swedish insurance company that in May 2009 published a report on injuries and survivability of 172 car models. The 88–96 generation got a real world rating of \"Average\", and the 96-00 generation got a rating called \"Safest\" (at least 30% safer than the average car.)\n\nFrom 2001 to 2003, the Voyager was offered in the SWB model only, replacing the SWB Plymouth Voyager. It resembled the Town and Country more than the previous generation, the only major cosmetic difference besides the trim (where the Town and Country's is fancier) was the placement of the Chrysler emblem on the grille. After the 2003 model year, the Voyager was discontinued (United States market) and replaced by the Chrysler Town and Country, SWB model. The SWB Town & Country continued under the Voyager name in the Mexican market.\n\n\n\nIn Europe Chrysler began offering the Voyager with the first generation, followed by the second generation model in 2001, with a new engine range – including larger, more economical diesel engines (2.5 L and for 2005 – the 2.8 L 4 cylinder from VM Motori) and more fuel-efficient petrol engines (I4 and V6).\n\nThe fourth generation Grand Voyager continued production for the Chinese market alongside the Dodge Grand Caravan until late 2010. Both models were built by Soueast in China, using a Town & Country production line relocated from Taiwan, and were powered by Mitsubishi 6G72 engines.\nChrysler introduced the new Grand Voyager for 2008 and successfully positioned it in the automotive market as a luxury MPV suited for large families. The Grand Voyager is visually identical to the Chrysler Town & Country which is sold in the North American and South American markets. In similar fashion to the other large multi-purpose vehicles (MPVs) on the market the Grand Voyager is sold with a standard diesel engine in Europe.\n\nHowever, the seating is arranged in the 2–2–3 (front to rear) layout common in North America, rather than the 2–3–2 layout often seen in SUVs and MPVs in Europe. On right hand drive (RHD) models the gear shift lever is placed on a floor-mounted console between the seats, in contrast to the instrument panel positioning found on LHD models.\n\n\nThe 2009 Grand Voyager with diesel motor gets a combined fuel economy of .\n\nOptional engine on top of the range Limited models:\n\nAll engines are paired with Chrysler's 62TE 6-speed automatic transmission with variable line pressure (VLP) technology (See Ultradrive).\n\nAll Voyagers sold since October 2011 in continental Europe are sold under the Lancia brand.\n\nThe Chrysler-branded variant continues to be sold in the United Kingdom, Ireland, Russia, Australia, New Zealand, South Korea, Singapore, and China, as Lancia does not have sales operations in those markets. Voyager become the successor to previous unrelated series of minivans produced by Lancia, the last of which is the Phedra.\n\nHowever, the parent company Fiat Chrysler Automobiles under the leadership of CEO Sergio Marchionne has decided to shut down both Chrysler and Lancia brands out of its European market (with the exception of keeping one model of Lancia available for sale in Italy). Therefore, it appears Voyager too is on its way out of Europe.\n\nThe Lancia version is offered with engines compliant with Euro V emission standards.\n\nThe Grand Voyager nameplate continues to be used in China on a rebadged Chrysler Pacifica.\n\nThe Chrysler Voyager has incorporated various seating systems for their minivans to enhance interior flexibility.\n\nIn 1992, Chrysler introduced a second row bench seat integrating two child booster seats. These seats continued as an available option through fifth generation until they were discontinued in 2010.\n\nIn 1996, Chrysler had introduced a system of seats to simplify installation, removal, and re-positioning— marketed as \"Easy-Out Roller Seats\". The system remained in use throughout the life of the \"Chrysler Voyager\".\n\nWhen installed, the seats are latched to floor-mounted strikers. When unlatched, eight rollers lift each seat, allowing it to be rolled fore and aft. Tracks have locator depressions for rollers, thus enabling simple installation. Ergonomic levers at the seatbacks release the floor latches single-handedly without tools and raise the seats onto the rollers in a single motion. Additionally, seatbacks were designed to fold forward. Seat roller tracks are permanently attached to the floor and seat stanchions are aligned, facilitating the longitiudinal rolling of the seats. Bench seat stanchions were moved inboard to reduce bending stress in the seat frames, allowing them to be lighter.\n\nWhen configured as two- and three- person benches, the Easy Out Roller Seats could be unwieldy. Beginning in 2001, second and third row seats became available in a 'quad' configuration – bucket or captain chairs in the second row and a third row three-person 50/50 split \"bench\" — with each section weighing under .\n\nIn 2005, Chrysler introduced a system of second and third row seating that folded completely into under-floor compartments – marketed as \"Stow 'n Go\" seating and exclusively available on long-wheelbase models.\n\nIn a development program costing $400 million, engineers used an erector set to initially help visualize the complex interaction of the design and redesigned underfloor components to accommodate the system — including the spare tire well, fuel tank, exhaust system, parking brake cables, rear climate control lines, and the rear suspension. Even so, the new seating system precluded incorporation of an AWD system, effectively ending that option for the Chrysler minivans.\n\nThe system in turn creates a combined volume of of under floor storage when second row seats are deployed. With both row folded, the vans have a flat load floor and a maximum cargo volume of .\n\nThe Stow 'n Go system received the Popular Science Magazine's \"Best of What's New\" for 2005 award.\n\nThe Stow 'n Go system is not offered on the Volkswagen Routan, a rebadged nameplate variant of the Chrysler minivans.\n\nChrysler introduced a seating system in 2008, marketed as \"Swivel'n Go\". In the seating system, two full size second row seats swivel to face the third row. A detachable table can be placed between the second and third row seats. The Swivel'n Go seating system includes the 3rd row seating from the Stow'n Go system.\n\nThese Swivel 'n Go Seats are manufactured by Intier Corp. a division of Magna. The tracks, risers and swivel mechanisms are assembled by Camslide, a division of Intier. The swivel mechanism was designed by and is produced by Toyo Seat USA Corp.\n\nThe system is noted for its high strength. The entire load of the seat in the event of a crash is transferred through the swivel mechanism, which is almost twice as strong as the minimum government requirement.\n\nThe swivel mechanism includes bumpers that stabilize the seat while in the lock position. When rotated the seat comes off these bumpers to allow easy rotation.\n\nThe seat is not meant to be left in an unlocked position or swiveled with the occupant in it, although this will not damage the swivel mechanism.\n\nIn the early years of its marketing, the Voyagers were produced in North America and were exported to Europe (1988–1991).\n\nIn 1991, the first Voyagers were produced in Austria, at the Eurostar plant nearby Graz. Eurostar was a joint venture between Chrysler and the Austrian company Steyr-Daimler-Puch. It was later acquired by DaimlerChrysler and finally the plant was sold to Magna Steyr in 2002. The minivan production ended there at the end of 2007. Units produced in Austria were marketed in Europe, Asia and Africa. They were built with gasoline and diesel engines, with manual transmission version, in short-wheelbase (SWB) and long-wheelbase versions, and in right and left-hand drive versions (all sold as Chrysler Voyager).\n\nFrom 2008 to 2010, the fourth generation Grand Voyager was produced in China by Soueast using a relocated Taiwanese Town & Country assembly line.\n\nThe fifth generation Voyagers (2008–2011) have been exported to Europe from Windsor, Canada, where they are produced. Beginning in October 2011, they were exported and sold as the Lancia Voyager in most European markets, as Chrysler operations were merged with those of Lancia in many European countries. In the United Kingdom, only the Grand Voyager is marketed.\n\nSince 2011, the Voyager is sold under the Lancia badge in Europe to strengthen the Chrysler-Lancia integration, though it remains branded as the Chrysler Voyager in the United Kingdom and Ireland. In March 2015, Fiat Group announced that in 2017, Chrysler would be discontinued in the United Kingdom. It was removed from Chrysler UK's website in January 2016.\n\n\n"}
{"id": "5811763", "url": "https://en.wikipedia.org/wiki?curid=5811763", "title": "Cyclonic rotation", "text": "Cyclonic rotation\n\nCyclonic rotation or circulation is movement in the same direction as the Earth's rotation, as opposed to anticyclonic rotation. The Coriolis effect causes cyclonic rotation to be in a counterclockwise direction in the northern hemisphere, and clockwise in the southern hemisphere. A closed area of winds rotating cyclonically is known as a cyclone.\n\n"}
{"id": "29829474", "url": "https://en.wikipedia.org/wiki?curid=29829474", "title": "Digital Weight Indicator", "text": "Digital Weight Indicator\n\nDigital Weight Indicator also commonly called Weight Indicator is a system of indication or recording of the selector type or one that advances intermittently in which all values are presented digitally, or in numbers. In a digital indicating or recording element, or in digital representation, there are no graduations. The National Conference on Weights and Measures (NWCM) lists these device types as “Indicating Elements” in its certificate databases. \n\nThe National Conference on Weights and Measures have categorized Digital Weight Indicator as a Verified Conformity Assessment Program (VCAP). A program has been proposed by the National Conference on Weights and Measures to ensure compliance of certain device types with environmental requirements. \n\nOver the years, it gets more intelligent and complicated than conventional weighing scales. It can interact with different devices such as a load cell, printer, and computer via cables. \n\nLoad cell will produce a small electric current when weight is applied. The electric current is sent to Digital Weight Indicator via a normal cable. The Digital Weight Indicator will amplify the electric current and translate it to digital weight. Example: 15.7 kg\n\nWhen attached to a printer via a printer cable, it can print the weight on paper. Usually Weight Indicator come with a basic printing feature. It can be program in the firmware to print in few basic format. Usually there is a print button on the Digital Weight Indicator for user to press to print.\n\nA computer can be connected to it via a serial cable to RS-232 port on the computer. Information like weight, unit weight, status (stable, unstable, error) is send to the computer in a particular format. The format is not standard and differs with each brand. A computer interpret the information received and display them on screen and print it on printer. With user interaction, user can enters other information in relation to the weight received and save them to a database. By storing information in a database, many other process can be automated by software application like invoicing, quality control, batch control, warehousing, distribution, and more.\n"}
{"id": "8000781", "url": "https://en.wikipedia.org/wiki?curid=8000781", "title": "Displacement field (mechanics)", "text": "Displacement field (mechanics)\n\nA displacement field is an assignment of displacement vectors for all points in a region or body that is displaced from one state to another. A displacement vector specifies the position of a point or a particle in reference to an origin or to a previous position. For example, a displacement field may be used to describe the effects of deformation on a solid body.\n\nBefore considering displacement, the state before deformation must be defined. It is a state in which the coordinates of all points are known and described by the function:\n\nwhere\n\nMost often it is a state of the body in which no forces are applied.\n\nThen given any other state of this body in which coordinates of all its points are described as formula_5 the displacement field is the difference between two body states:\n\nwhere\n\n"}
{"id": "47215278", "url": "https://en.wikipedia.org/wiki?curid=47215278", "title": "Electronic entropy", "text": "Electronic entropy\n\nElectronic entropy is the entropy of a system attributable to electrons' probabilistic occupation of states. This entropy can take a number of forms. The first form can be termed a density of states based entropy. The Fermi–Dirac distribution implies that each eigenstate of a system, , is occupied with a certain probability, . As the entropy is given by a sum over the probabilities of occupation of those states, there is an entropy associated with the occupation of the various electronic states. In most molecular systems, the energy spacing between the highest occupied molecular orbital and the lowest unoccupied molecular orbital is usually large, and thus the probabilities associated with the occupation of the excited states are small. Therefore, the electronic entropy in molecular systems can safely be neglected. Electronic entropy is thus most relevant for the thermodynamics of condensed phases, where the density of states at the Fermi level can be quite large, and the electronic entropy can thus contribute substantially to thermodynamic behavior. A second form of electronic entropy can be attributed to the configurational entropy associated with localized electrons and holes. This entropy is similar in form to the configurational entropy associated with the mixing of atoms on a lattice.\n\nElectronic entropy can substantially modify phase behavior, as in lithium ion battery electrodes, high temperature superconductors, and some perovskites. It is also the driving force for the coupling of heat and charge transport in thermoelectric materials, via the Onsager reciprocal relations.\n\nThe entropy due to a set of states that can be either occupied with probability formula_1 or empty with probability formula_2 can be written as:\n\nwhere is Boltzmann constant.\n\nFor a continuously distributed set of states as a function of energy, such as the eigenstates in an electronic band structure, the above sum can be written as an integral over the possible energy values, rather than a sum. Switching from summing over individual states to integrating over energy levels, the entropy can be written as:\n\nwhere is the density of states of the solid. The probability of occupation of each eigenstate is given by the Fermi function, :\n\nwhere is the Fermi energy and is the absolute temperature. One can then re-write the entropy as:\nThis is the general formulation of the density-of-states based electronic entropy.\n\nIt is useful to recognize that the only states within ~ of the Fermi level contribute significantly to the entropy. Other states are either fully occupied, , or completely unoccupied, . In either case, these states do not contribute to the entropy. If one assumes that the density of states is constant within of the Fermi level, one can derive that the electronic entropy is equal to:\n\nwhere is the density of states at the Fermi level. Several other approximations can be made, but they all indicate that the electronic entropy should, to first order, be proportional to the temperature and the density of states at the Fermi level. As the density of states at the Fermi level varies widely between systems, this approximation is a reasonable heuristic for inferring when it may be necessary to include electronic entropy in the thermodynamic description of a system; only systems with large densities of states at the Fermi level should exhibit non-negligible electronic entropy (where large may be approximately defined as ).\n\nInsulators have zero density of states at the Fermi level due to their band gaps. Thus, the density of states-based electronic entropy is essentially zero in these systems.\n\nMetals have non-zero density of states at the Fermi level. Metals with free-electron-like band structures (e.g. alkali metals, alkaline earth metals, Cu, and Al) generally exhibit relatively low density of states at the Fermi level, and therefore exhibit fairly low electronic entropies. Transition metals, wherein the flat d-bands lie close to the Fermi level, generally exhibit much larger electronic entropies than the free-electron like metals.\n\nOxides have particularly flat band structures and thus can exhibit large , if the Fermi level intersects these bands. As most oxides are insulators, this is generally not the case. However, when oxides are metallic (i.e. the Fermi level lies within an unfilled, flat set of bands), oxides exhibit some of the largest electronic entropies of any material.\n\nThermoelectric materials are specifically engineered to have large electronic entropies. The thermoelectric effect relies on charge carriers exhibiting large entropies, as the driving force to establish a gradient in electrical potential is driven by the entropy associated with the charge carriers. In the thermoelectric literature, the term \"band structure engineering\" refers to the manipulation of material structure and chemistry to achieve a high density of states near the Fermi level. More specifically, thermoelectric materials are intentionally doped to exhibit only partially filled bands at the Fermi level, resulting in high electronic entropies. Instead of engineering band filling, one may also engineer the shape of the band structure itself via introduction of nanostructures or quantum wells to the materials.\n\nConfigurational electronic entropy is usually observed in mixed-valence transition metal oxides, as the charges in these systems are both localized (the system is ionic), and capable of changing (due to the mixed valency). To a first approximation (i.e. assuming that the charges are distributed randomly), the molar configurational electronic entropy is given by:\n\nwhere is the fraction of sites on which a localized electron/hole could reside (typically a transition metal site), and is the concentration of localized electrons/holes. Of course, the localized charges are not distributed randomly, as the charges will interact electrostatically with one another, and so the above formula should only be regarded as an approximation to the configurational atomic entropy. More sophisticated approximations have been made in the literature.\n"}
{"id": "2656271", "url": "https://en.wikipedia.org/wiki?curid=2656271", "title": "Evershed effect", "text": "Evershed effect\n\nThe Evershed effect, named after the British astronomer John Evershed, is the radial flow of gas across the photospheric surface of the penumbra of sunspots from the inner border with the umbra towards the outer edge.\n\nThe speed varies from around 1 km/s at the border between the umbra and the penumbra to a maximum of around double this in the middle of the penumbra and falls off to zero at the outer edge of the penumbra.\nEvershed first detected this phenomenon in January 1909, whilst working at the Kodaikanal Solar Observatory in India, when he found that the spectral lines of sunspots showed doppler shift.\n\nAfterwards, measurements of the spectral emission lines emitted in the ultraviolet wavelengths have shown a systematic red-shift. The Evershed effect is common to every spectral line formed at a temperature below 10 K; this fact would imply a constant downflow from the transition region towards the chromosphere. The observed velocity is about 5 km/s. Of course, this is impossible, since if it were true, the corona would disappear in a short time instead of being suspended over the Sun at temperatures of million degrees over distances much larger than a solar radius.\n\nMany theories have been proposed to explain this redshift in line profiles of the transition region, but the problem is still unsolved, since a coherent theory should take into account all the physical observations: UV line profiles are redshifted \"on average\", but they show back and forth velocity oscillations at the same time.\n\nIn synthesis, the proposed mechanisms are:\n\n\nThe effect was commemorated in a postage stamp issued in India on 2 December 2008.\n\n"}
{"id": "268420", "url": "https://en.wikipedia.org/wiki?curid=268420", "title": "Foam", "text": "Foam\n\nFoam is a object formed by trapping pockets of gas in a liquid or solid.\nA bath sponge and the head on a glass of beer are examples of foams. In most foams, the volume of gas is large, with thin films of liquid or solid separating the regions of gas. Soap foams are also known as suds.\n\nSolid foams can be closed-cell or open-cell. In closed-cell foam, the gas forms discrete pockets, each completely surrounded by the solid material. In open-cell foam, gas pockets connect to each other. A bath sponge is an example of an open-cell foam: water easily flows through the entire structure, displacing the air. A camping mat is an example of a closed-cell foam: gas pockets are sealed from each other so the mat cannot soak up water.\n\nFoams are examples of dispersed media. In general, gas is present, so it divides into gas bubbles of different sizes (i.e., the material is polydisperse)—separated by liquid regions that may form films, thinner and thinner when the liquid phase drains out of the system films. When the principal scale is small, i.e., for a very fine foam, this dispersed medium can be considered a type of colloid.\n\n\"Foam\" can also refer to something that is analogous to foam, such as quantum foam, polyurethane foam (foam rubber), XPS foam, polystyrene, phenolic, or many other manufactured foams.\n\nA foam is, in many cases, a multi-scale system.\nOne scale is the bubble: material foams are typically disordered and have a variety of bubble sizes. At larger sizes, the study of idealized foams is closely linked to the mathematical problems of minimal surfaces and three-dimensional tessellations, also called honeycombs. The Weaire–Phelan structure is considered the best possible (optimal) unit cell of a perfectly ordered foam, while Plateau's laws describe how soap-films form structures in foams.\n\nAt lower scale than the bubble is the thickness of the film for metastable foams, which can be considered a network of interconnected films called lamellae. Ideally, the lamellae connect in triads and radiate 120° outward from the connection points, known as Plateau borders.\n\nAn even lower scale is the liquid–air interface at the surface of the film. Most of the time this interface is stabilized by a layer of amphiphilic structure, often made of surfactants, particles (Pickering emulsion), or more complex associations.\n\nSeveral conditions are needed to produce foam: there must be mechanical work, surface active components (surfactants) that reduce the surface tension, and the formation of foam faster than its breakdown.\nTo create foam, work (W) is needed to increase the surface area (ΔA):\n\nwhere γ is the surface tension.\n\nOne of the ways foam is created is through dispersion, where a large amount of gas is mixed with a liquid. A more specific method of dispersion involves injecting a gas through a hole in a solid into a liquid. If this process is completed very slowly, then one bubble can be emitted from the orifice at a time as shown in the picture below.\n\nOne of the theories for determining the separation time is shown below; however, while this theory produces the theoretical data that matches with experimental data, detachment due to capillarity is accepted as a better explanation.\nThe buoyancy force acts to raise the bubble, which is\n\nwhere formula_3 is the volume of the bubble, formula_4 is the acceleration due to gravity, and ρ is the density of the gas ρ is the density of the liquid. The force working against the buoyancy force is the surface tension force, which is\n\nwhere γ is the surface tension, and formula_6 is the radius of the orifice.\nAs more air is pushed into the bubble, the buoyancy force grows quicker than the surface tension force. Thus, detachment occurs when the buoyancy force is large enough to overcome the surface tension force. \nIn addition, if the bubble is treated as a sphere with a radius of formula_8 and the volume formula_3 is substituted in to the equation above, separation occurs at the moment when\n\nExamining this phenomenon from a capillarity viewpoint for a bubble that is being formed very slowly, it can be assumed that the pressure formula_11 inside is constant everywhere. The hydrostatic pressure in the liquid is designated by formula_12. The change in pressure across the interface from gas to liquid is equal to the capillary pressure; hence,\n\nwhere R and R are the radii of curvature and are set as positive. At the stem of the bubble, R and R are the radii of curvature also treated as positive. Here the hydrostatic pressure in the liquid has to take in account z, the distance from the top to the stem of the bubble. The new hydrostatic pressure at the stem of the bubble is \"p\"(\"ρ\" − \"ρ\")\"z\". The hydrostatic pressure balances the capillary pressure, which is shown below:\n\nFinally, the difference in the top and bottom pressure equal the change in hydrostatic pressure:\n\nAt the stem of the bubble, the shape of the bubble is nearly cylindrical; consequently, either R or R is large while the other radius of curvature is small. As the stem of the bubble grows in length, it becomes more unstable as one of the radius grows and the other shrinks. At a certain point, the vertical length of the stem exceeds the circumference of the stem and due to the buoyancy forces the bubble separates and the process repeats.\n\nThe stabilization of a foam is caused by van der Waals forces between the molecules in the foam, electrical double layers created by dipolar surfactants, and the Marangoni effect, which acts as a restoring force to the lamellae.\n\nThe Marangoni effect depends on the liquid that is foaming being impure. Generally, surfactants in the solution decrease the surface tension. The surfactants also clump together on the surface and form a layer as shown below.\n\nFor the Marangoni effect to occur, the foam must be indented as shown in the first picture. This indentation increases local surface area. Surfactants have a larger diffusion time than the bulk of the solution—so the surfactants are less concentrated in the indentation.\n\nAlso, surface stretching makes the surface tension of the indented spot greater than the surrounding area. Consequentially—since diffusion time for the surfactants is large—the Marangoni effect has time to take place. The difference in surface tension creates a gradient, which instigates fluid flow from areas of lower surface tension to areas of higher surface tension. The second picture shows the film at equilibrium after the Marangoni effect has taken place.\n\nRybczynski and Hadamar developed an equation to calculate the velocity of bubbles that rise in foam with the assumption that the bubbles are spherical with a radius formula_6.\n\nwith velocity in units of centimeters per second. ρ and ρ is the density for a gas and liquid respectively in units of g/cm and ῃ and ῃ is the viscosity of the gas and liquid g/cm·s and g is the acceleration in units of cm/s.\n\nHowever, since the density and viscosity of a liquid is much greater than the gas, the density and viscosity of the gas can be neglected, which yields the new equation for velocity of bubbles rising as:\n\nHowever, through experiments it has been shown that a more accurate model for bubbles rising is:\n\nDeviations are due to the Marangoni effect and capillary pressure, which affect the assumption that the bubbles are spherical.\nFor laplace pressure of a curved gas liquid interface, the two principal radii of curvature at a point are R and R. With a curved interface, the pressure in one phase is greater than the pressure in another phase. The capillary pressure P is given by the equation of:\n\nwhere formula_21 is the surface tension. The bubble shown below is a gas (phase 1) in a liquid (phase 2) and point A designates the top of the bubble while point B designates the bottom of the bubble. \n\nAt the top of the bubble at point A, the pressure in the liquid is assumed to be p as well as in the gas. At the bottom of the bubble at point B, the hydrostatic pressure is:\nwhere ρ and ρ is the density for a gas and liquid respectively. The difference in hydrostatic pressure at the top of the bubble is 0, while the difference in hydrostatic pressure at the bottom of the bubble across the interface is \"gz\"(\"ρ\" − \"ρ\"). Assuming that the radii of curvature at point A are equal and denoted by R and that the radii of curvature at point B are equal and denoted by R, then the difference in capillary pressure between point A and point B is:\n\nAt equilibrium, the difference in capillary pressure must be balanced by the difference in hydrostatic pressure. Hence,\nSince, the density of the gas is less than the density of the liquid the left hand side of the equation is always positive. Therefore, the inverse of R must be larger than the R. Meaning that from the top of the bubble to the bottom of the bubble the radius of curvature increases. Therefore, without neglecting gravity the bubbles cannot be spherical. In addition, as z increases, this causes the difference in R and R too, which means the bubble deviates more from its shape the larger it grows.\n\nFoam destabilization occurs for several reasons. First, gravitation causes drainage of liquid to the foam base, which Rybczynski and Hadamar include in their theory; however, foam also destabilizes due to osmotic pressure causes drainage from the lamellas to the Plateau borders due to internal concentration differences in the foam, and Laplace pressure causes diffusion of gas from small to large bubbles due to pressure difference. In addition, films can break under disjoining pressure, These effects can lead to rearrangement of the foam structure at scales larger than the bubbles, which may be individual (T1 process) or collective (even of the \"avalanche\" type).\n\nBeing a multiscale system involving many phenomena, and a versatile medium, foam can be studied using many different techniques. Considering the different scales, experimental techniques are diffraction ones, mainly light scattering techniques (DWS, see below, static and dynamic light scattering, X rays and neutron scattering) at sub-micrometer scales, or microscopic ones. Considering the system as continuous, its \"bulk\" properties can be characterized by light transmittance but also conductimetry. The correlation between structure and bulk is evidenced more accurately by acoustics in particular. The organisation between bubbles has been studied numerically using sequential attempts of evolution of the minimum surface energy either at random (Pott's model) or deterministic way (surface evolver). The evolution with time (i.e., the dynamics) can be simulated using these models, or the \"bubble model\" (Durian), which considers the motion of individual bubbles.\n\nObservations of the small-scale structure can be made by shining the foam with laser light or x-ray beams and measuring the reflectivity of the films between bubbles. Observations of the global structure can be done using neutron scattering.\nA typical light scattering (or diffusion) optical technique, multiple light scattering coupled with vertical scanning, is the most widely used technique to monitor the dispersion state of a product, hence identifying and quantifying destabilization phenomena. It works on any concentrated dispersions without dilution, including foams. When light is sent through the sample, it is backscattered by the bubbles. The backscattering intensity is directly proportional to the size and volume fraction of the dispersed phase. Therefore, local changes in concentration (drainage, syneresis) and global changes in size (ripening, coalescence) are detected and monitored.\n\nLiquid foams can be used in fire retardant foam, such as those that are used in extinguishing fires, especially oil fires.\n\nIn some ways, leavened bread is a foam, as the yeast causes the bread to rise by producing tiny bubbles of gas in the dough. The dough has traditionally been understood as a closed-cell foam, in which the pores do not connect with each other. Cutting the dough releases the gas in the bubbles that are cut, but the gas in the rest of the dough cannot escape. When dough is allowed to rise too far, it becomes an open-cell foam, in which the gas pockets are connected. Now, if the dough is cut or the surface otherwise broken, a large volume of gas can escape, and the dough collapses. The open structure of an over-risen dough is easy to observe: instead of consisting of discrete gas bubbles, the dough consists of a gas space filled with threads of the flour-water paste. Recent research has indicated that the pore structure in bread is 99% interconnected into one large vacuole, thus the closed-cell foam of the moist dough is transformed into an open cell solid foam in the bread.\n\nThe unique property of gas-liquid foams having very high specific surface area is exploited in the chemical processes of froth flotation and foam fractionation.\n\nSolid foams are a class of lightweight cellular engineering materials. These foams are typically classified into two types based on their pore structure: open-cell-structured foams (also known as reticulated foams) and closed-cell foams. At high enough cell resolutions, any type can be treated as continuous or \"continuum\" materials and are referred to as cellular solids, with predictable mechanical properties.\n\nOpen-cell-structured foams contain pores that are connected to each other and form an interconnected network that is relatively soft. Open-cell foams fill with whatever gas surrounds them. If filled with air, a relatively good insulator results, but, if the open cells fill with water, insulation properties would be reduced. Recent studies have put the focus on studying the properties of open-cell foams as an insulator material. Wheat gluten/TEOS bio-foams have been produced, showing similar insulator properties as for those foams obtained from oil-based resources. Foam rubber is a type of open-cell foam.\n\nClosed-cell foams do not have interconnected pores. The closed-cell foams normally have higher compressive strength due to their structures. However, closed-cell foams are also, in general, a more dense foam, require more material, and as a consequence are more expensive to produce. The closed cells can be filled with a specialized gas to provide improved insulation. The closed-cell structure foams have higher dimensional stability, low moisture absorption coefficients, and higher strength compared to open-cell-structured foams. All types of foam are widely used as core material in sandwich-structured composite materials.\n\nThe earliest known engineering use of cellular solids is with wood, which in its dry form is a closed-cell foam composed of lignin, cellulose, and air. From the early 20th century, various types of specially manufactured solid foams came into use. The low density of these foams makes them excellent as thermal insulators and flotation devices and their lightness and compressibility make them ideal as packing materials and stuffings.\n\nAn example of the use of azodicarbonamide as a blowing agent is found in the manufacture of vinyl (PVC) and EVA-PE foams, where it plays a role in the formation of air bubbles by breaking down into gas at high temperature..\n\nThe random or \"stochastic\" geometry of these foams makes them good for energy absorption, as well. In the late 20th century to early 21st century, new manufacturing techniques have allowed for geometry that results in excellent strength and stiffness per weight. These new materials are typically referred to as engineered cellular solids.\n\nA special class of closed-cell foams, known as syntactic foam, contains hollow particles embedded in a matrix material. The spheres can be made from several materials, including glass, ceramic, and polymers. The advantage of syntactic foams is that they have a very high strength-to-weight ratio, making them ideal materials for many applications, including deep-sea and space applications. One particular syntactic foam employs shape memory polymer as its matrix, enabling the foam to take on the characteristics of shape memory resins and composite materials; i.e., it has the ability to be reshaped repeatedly when heated above a certain temperature and cooled. Shape memory foams have many possible applications, such as dynamic structural support, flexible foam core, and expandable foam fill.\n\n\"Integral skin foam\", also known as \"self-skin foam\", is a type of foam with a high-density skin and a low-density core. It can be formed in an \"open-mold process\" or a \"closed-mold process\". In the open-mold process, two reactive components are mixed and poured into an open mold. The mold is then closed and the mixture is allowed to expand and cure. Examples of items produced using this process include arm rests, baby seats, shoe soles, and mattresses. The closed-mold process, more commonly known as \"reaction injection molding\" (RIM), injects the mixed components into a closed mold under high pressures.\n\nFoam, in this case meaning \"bubbly liquid\", is also produced as an often-unwanted by-product in the manufacture of various substances. For example, foam is a serious problem in the chemical industry, especially for biochemical processes. Many biological substances, for example proteins, easily create foam on agitation or aeration. Foam is a problem because it alters the liquid flow and blocks oxygen transfer from air (thereby preventing microbial respiration in aerobic fermentation processes). For this reason, anti-foaming agents, like silicone oils, are added to prevent these problems. Chemical methods of foam control are not always desired with respect to the problems (i.e., contamination, reduction of mass transfer) they may cause especially in food and pharmaceutical industries, where the product quality is of great importance. Mechanical methods to prevent foam formation are more common than chemical ones.\n\nThe acoustical property of the speed of sound through a foam is of interest when analyzing failures of hydraulic components. The analysis involves calculating total hydraulic cycles to fatigue failure. The speed of sound in a foam is determined by the mechanical properties of the gas creating the foam: oxygen, nitrogen, or combinations.\n\nAssuming that the speed of sound is based on the liquid's fluid properties leads to errors in calculating fatigue cycles and failure of mechanical hydraulic components. Using acoustical transducers and related instrumentation that set low limits (0–50,000 Hz with roll-off) causes errors. The low roll-off during measurement of actual frequency of acoustic cycles results in miscalculation due to actual hydraulic cycles in the possible ranges of 1–1000 MHz or higher. Instrumentation systems are most revealing when cycle bandwidths exceed the actual measured cycles by a factor of 10 to 100. Associated instrumentation costs also increase by factors of 10 to 100.\n\nMost moving hydro-mechanical components cycle at 0–50 Hz, but entrained gas bubbles resulting in a foamy condition of the associated hydraulic fluid results in actual hydraulic cycles that can exceed 1000 MHz even if the moving mechanical components do not cycle at the higher cycle frequency.\n\n\n\n"}
{"id": "2071443", "url": "https://en.wikipedia.org/wiki?curid=2071443", "title": "Framing (construction)", "text": "Framing (construction)\n\nFraming, in construction, is the fitting together of pieces to give a structure support and shape. Framing materials are usually wood, engineered wood, or structural steel. The alternative to framed construction is generally called \"mass wall\" construction, where horizontal layers of stacked materials such as log building, masonry, rammed earth, adobe, etc. are used without framing.\n\nBuilding framing is divided into two broad categories, heavy-frame construction (heavy framing) if the vertical supports are few and heavy such as in timber framing, pole building framing, or steel framing; or light-frame construction (light-framing) if the supports are more numerous and smaller called light-frame construction, for example balloon, platform and light-steel framing. Light-frame construction using standardized dimensional lumber has become the dominant construction method in North America and Australia due to the economy of the method; use of minimal structural material allows builders to enclose a large area at minimal cost while achieving a wide variety of architectural styles.\n\nModern light-frame structures usually gain strength from rigid panels (plywood and other plywood-like composites such as oriented strand board (OSB) used to form all or part of wall sections), but until recently carpenters employed various forms of diagonal bracing to stabilize walls. Diagonal bracing remains a vital interior part of many roof systems, and in-wall wind braces are required by building codes in many municipalities or by individual state laws in the United States. Special framed shear walls are becoming more common to help buildings meet the requirements of earthquake engineering and wind engineering.\n\nHistorically, people fitted naturally shaped wooden poles together as framework and then began using joints to connect the timbers, a method today called \"traditional timber framing or log framing. In the United states, timber framing was superseded by balloon framing beginning in the 1830s. Balloon framing makes use of many lightweight wall members called studs rather than fewer, heavier supports called posts; balloon framing components are nailed together rather than fitted using joinery. The studs in a balloon frame extend two stories from sill to plate. Platform framing\"' superseded balloon framing and is the standard wooden framing method today. The name comes from each floor level being framed as a separate unit or platform.\n\nFramed construction was rarely used in Scandinavia before the 20th century because of the abundant availability of wood, an abundance of cheap labour, and the superiority of the thermal insulation of logs; hence timber framing took off there first for unheated buildings such as farm buildings, outbuldings and summer villas, and for houses only with the development of wall insulation.\n\nWall framing in house construction includes the vertical and horizontal members of exterior walls and interior partitions, both of bearing walls and non-bearing walls. These \"stick members\", referred to as studs, wall plates and lintels (sometimes called \"headers\"), serve as a nailing base for all covering material and support the upper floor platforms, which provide the lateral strength along a wall. The platforms may be the boxed structure of a ceiling and roof, or the ceiling and floor joists of the story above. In the building trades, the technique is variously referred to as \"stick and frame\", \"stick and platform\", or \"stick and box\", as the sticks (studs) give the structure its vertical support, and the box-shaped floor sections with joists contained within length-long post and lintels (more commonly called \"headers\"), support the weight of whatever is above, including the next wall up and the roof above the top story. The platform also provides the lateral support against wind and holds the stick walls true and square. Any lower platform supports the weight of the platforms and walls above the level of its component headers and joists.\n\nFraming lumber is subject to regulated standards that require a grade-stamp, and a moisture content not exceeding 19%.\n\nThere are three historically common methods of framing a house.\n\nWall sheathing, usually a plywood or other laminate, is usually applied to the framing prior to erection, thus eliminating the need to scaffold, and again increasing speed and cutting manpower needs and expenses. Some types of exterior sheathing, such as asphalt-impregnated fiberboard, plywood, oriented strand board and waferboard, will provide adequate bracing to resist lateral loads and keep the wall square. (Construction codes in most jurisdictions require a stiff plywood sheathing.) Others, such as rigid glass-fiber, asphalt-coated fiberboard, polystyrene or polyurethane board, will not. In this latter case, the wall should be reinforced with a diagonal wood or metal bracing inset into the studs. In jurisdictions subject to strong wind storms (hurricane countries, tornado alleys) local codes or state law will generally require both the diagonal wind braces and the stiff exterior sheathing regardless of the type and kind of outer weather resistant coverings.\n\nA multiple-stud post made up of at least three studs, or the equivalent, is generally used at exterior corners and intersections to secure a good tie between adjoining walls, and to provide nailing support for interior finishes and exterior sheathing. Corners and intersections, however, must be framed with at least two studs.\n\nNailing support for the edges of the ceiling is required at the junction of the wall and ceiling where partitions run parallel to the ceiling joists. This material is commonly referred to as \"dead wood\" or backing.\n\nWall framing in house construction includes the vertical and horizontal members of exterior walls and interior partitions. These members, referred to as studs, wall plates and lintels, serve as a nailing base for all covering material and support the upper floors, ceiling and roof.\n\nExterior wall studs are the vertical members to which the wall sheathing and cladding are attached. They are supported on a bottom plate or foundation sill and in turn support the top plate. Studs usually consist of or lumber and are commonly spaced at on center. This spacing may be changed to on center depending on the load and the limitations imposed by the type and thickness of the wall covering used. Wider studs may be used to provide space for more insulation. Insulation beyond that which can be accommodated within a stud space can also be provided by other means, such as rigid or semi-rigid insulation or batts between horizontal furring strips, or rigid or semi-rigid insulation sheathing to the outside of the studs. The studs are attached to horizontal top and bottom wall plates of lumber that are the same width as the studs.\n\nInterior partitions supporting floor, ceiling or roof loads are called loadbearing walls; others are called non-loadbearing or simply partitions. Interior loadbearing walls are framed in the same way as exterior walls. Studs are usually lumber spaced at on center. This spacing may be changed to depending on the loads supported and the type and thickness of the wall finish used.\n\nPartitions can be built with or studs spaced at on center depending on the type and thickness of the wall finish used. Where a partition does not contain a swinging door, studs at on center are sometimes used with the wide face of the stud parallel to the wall. This is usually done only for partitions enclosing clothes closets or cupboards to save space. Since there is no vertical load to be supported by partitions, single studs may be used at door openings. The top of the opening may be bridged with a single piece of lumber the same width as the studs. These members provide a nailing support for wall finish, door frames and trim.\n\nLintels (or, headers) are the horizontal members placed over window, door and other openings to carry loads to the adjoining studs. Lintels are usually constructed of two pieces of 2 in (nominal) (38 mm) lumber separated with spacers to the width of the studs and nailed together to form a single unit. The preferable spacer material is rigid insulation. The depth of a lintel is determined by the width of the opening and vertical loads supported.\n\nThe complete wall sections are then raised and put in place, temporary braces added and the bottom plates nailed through the subfloor to the floor framing members. The braces should have their larger dimension on the vertical and should permit adjustment of the vertical position of the wall.\n\nOnce the assembled sections are plumbed, they are nailed together at the corners and intersections. A strip of polyethylene is often placed between the interior walls and the exterior wall, and above the first top plate of interior walls before the second top plate is applied to attain continuity of the air barrier when polyethylene is serving this function.\n\nA second top plate, with joints offset at least one stud space away from the joints in the plate beneath, is then added. This second top plate usually laps the first plate at the corners and partition intersections and, when nailed in place, provides an additional tie to the framed walls. Where the second top plate does not lap the plate immediately underneath at corner and partition intersections, these may be tied with galvanized steel plates at least wide and long, nailed with at least three nails to each wall.\n\nBalloon framing is a method of wood construction also known as \"Chicago construction\" in the 19th century used primarily in areas rich in softwood forests: Scandinavia, Canada, the United States up until the mid-1950s, and around Thetford Forest in Norfolk, England. It uses long continuous framing members (studs) that run from the sill plate to the top plate, with intermediate floor structures let into and nailed to them. Here the heights of window sills, headers and next floor height would be marked out on the studs with a story pole. Once popular when long lumber was plentiful, balloon framing has been largely replaced by \"platform framing\".\n\nIt is not certain who introduced balloon framing in the United States. However, the first building using balloon framing was possibly a warehouse constructed in 1832 in Chicago, Illinois, by George Washington Snow. Architectural critic Sigfried Giedion cited Chicago architect John M. Van Osdel's 1880s attribution, as well as A. T. Andreas' 1885 \"History of Chicago,\" to credit Snow as 'inventor of the balloon frame method'. In 1833, Augustine Taylor (1796–1891) constructed St. Mary's Catholic Church in Chicago using the balloon framing method. \n\nIn the 1830s, Hoosier Solon Robinson published articles about a revolutionary new framing system, called \"balloon framing\" by later builders. Robinson's system called for standard 2x4 lumber, nailed together to form a sturdy, light skeleton. Builders were reluctant to adopt the new technology, however, by the 1880s, some form of 2x4 framing was standard.\n\nAlternatively, a precursor to the balloon frame may have been used by the French in Missouri as much as thirty-one years earlier.\n\nThe name comes from a French Missouri type of construction, \"maison en \", \"boulin\" being a French term for a horizontal scaffolding support. Historians have also fabricated the following story: As Taylor was constructing his first such building, St. Mary's Church, in 1833, skilled carpenters looked on at the comparatively thin framing members, all held together with nails, and declared this method of construction to be no more substantial than a balloon. It would surely blow over in the next wind! Though the criticism proved baseless, the name stuck.\n\nAlthough lumber was plentiful in 19th-century America, skilled labor was not. The advent of cheap machine-made nails, along with water-powered sawmills in the early 19th century made balloon framing highly attractive, because it did not require highly skilled carpenters, as did the dovetail joints, mortises and tenons required by post-and-beam construction. For the first time, any farmer could build his own buildings without a time-consuming learning curve.\n\nIt has been said that balloon framing populated the western United States and the western provinces of Canada. Without it, western boomtowns certainly could not have blossomed overnight. It is also likely that, by radically reducing construction costs, balloon framing improved the shelter options of poorer North Americans. For example, many 19th-century New England working neighborhoods consist of balloon-constructed three-story apartment buildings referred to as triple deckers. However, balloon framing did require very long studs and as tall trees were exhausted in the 1920s, platform framing became prevalent.\nThe main difference between platform and balloon framing is at the floor lines. The balloon wall studs extend from the sill of the first story all the way to the top plate or end rafter of the second story. The platform-framed wall, on the other hand, is independent for each floor.\n\nLight-frame materials are most often wood or rectangular steel, tubes or C-channels. Wood pieces are typically connected with nail fastener nails or screws; steel pieces are connected with nuts and bolts. Preferred species for linear structural members are softwoods such as spruce, pine and fir. Light frame material dimensions range from ; i.e., a Dimensional number two-by-four to 5 cm by 30 cm (two-by-twelve inches) at the cross-section, and lengths ranging from for walls to or more for joists and rafters. Recently, architects have begun experimenting with pre-cut modular aluminum framing to reduce on-site construction costs.\n\nWall panels built of studs are interrupted by sections that provide rough openings for doors and windows. Openings are typically spanned by a header or lintel that bears the weight of structure above the opening. Headers are usually built to rest on trimmers, also called jacks. Areas around windows are defined by a sill beneath the window, and cripples, which are shorter studs that span the area from the bottom plate to the sill and sometimes from the top of the window to a header, or from a header to a top plate. Diagonal bracings made of wood or steel provide shear (horizontal strength) as do panels of sheeting nailed to studs, sills and headers. \n\nWall sections usually include a bottom plate which is secured to the structure of a floor, and one, or more often two top plates that tie walls together and provide a bearing for structures above the wall. Wood or steel floor frames usually include a rim joist around the perimeter of a system of floor joists, and often include bridging material near the center of a span to prevent lateral buckling of the spanning members. In two-story construction, openings are left in the floor system for a stairwell, in which stair risers and treads are most often attached to squared faces cut into sloping stair stringers.\n\nInterior wall coverings in light-frame construction typically include wallboard, lath and plaster or decorative wood paneling.\n\nExterior finishes for walls and ceilings often include plywood or composite sheathing, brick or stone veneers, and various stucco finishes. Cavities between studs, usually placed apart, are usually filled with insulation materials, such as fiberglass batting, or cellulose filling sometimes made of recycled newsprint treated with boron additives for fire prevention and vermin control. \n\nIn natural building, straw bales, cob and adobe may be used for both exterior and interior walls.\n\nThe part of a structural building that goes diagonally across a wall is called a T-bar. It stops the walls from collapsing in gusty winds. \n\nRoofs are usually built to provide a sloping surface intended to shed rain or snow, with slopes ranging from 1:15 (less than an inch per linear foot of horizontal span), to steep slopes of more than 2:1. A light-frame structure built mostly inside sloping walls which also serve as a roof is called an A-frame.\n\nRoofs are most often covered with shingles made of asphalt, fiberglass and small gravel coating, but a wide range of materials are used. Molten tar is often used to waterproof flatter roofs, but newer materials include rubber and synthetic materials. Steel panels are popular roof coverings in some areas, preferred for their durability. Slate or tile roofs offer more historic coverings for light-frame roofs.\n\nLight-frame methods allow easy construction of unique roof designs; hip roofs, for example, slope toward walls on all sides and are joined at hip rafters that span from corners to a ridge. Valleys are formed when two sloping roof sections drain toward each other. Dormers are small areas in which vertical walls interrupt a roof line, and which are topped off by slopes at usually right angles to a main roof section. Gables are formed when a length-wise section of sloping roof ends to form a triangular wall section. Clerestories are formed by an interruption along the slope of a roof where a short vertical wall connects it to another roof section. Flat roofs, which usually include at least a nominal slope to shed water, are often surrounded by parapet walls with openings (called scuppers) to allow water to drain out. Sloping crickets are built into roofs to direct water away from areas of poor drainage, such as behind a chimney at the bottom of a sloping section.\n\nLight-frame buildings are often erected on monolithic concrete-slab foundations that serve both as a floor and as a support for the structure. Other light-frame buildings are built over a crawlspace or a basement, with wood or steel joists used to span between foundation walls, usually constructed of poured concrete or concrete blocks.\n\nEngineered components are commonly used to form floor, ceiling and roof structures in place of solid wood. I-joists (closed-web trusses) are often made from laminated woods, most often chipped poplar wood, in panels as thin as , glued between horizontally laminated members of less than 4 cm by 4 cm (\"two-by-twos\"), to span distances of as much as . Open web trussed joists and rafters are often formed of 4 cm by 9 cm (\"two-by-four\") wood members to provide support for floors, roofing systems and ceiling finishes.\n\nPlatform framing was traditionally limited to four floors but some jurisdictions have modified their building codes to allow up to six floors with added fire protection.\n\n\n\n"}
{"id": "12311724", "url": "https://en.wikipedia.org/wiki?curid=12311724", "title": "Fuse cutout", "text": "Fuse cutout\n\nIn electrical distribution, a fuse cutout or cut-out fuse is a combination of a fuse and a switch, used in primary overhead feeder lines and taps to protect distribution transformers from current surges and overloads. An overcurrent caused by a fault in the transformer or customer circuit will cause the fuse to melt, disconnecting the transformer from the line. It can also be opened manually by utility linemen standing on the ground and using a long insulating stick called a \"hot stick\".\n\nA cutout consists of three major components:\n\n\nThe fuse holder may be replaced by a solid blade, which would allow the fuse holder assembly to be used as a switch only.\n\nThe fuse elements used in most distribution cutouts are tin or silver alloy wires that melt when subjected to high enough current. Ampere ratings of fuse elements vary from 1 ampere to 200 amperes.\n\nCutouts are typically mounted about 20 degrees off vertical so that the center of gravity of the fuse holder is displaced and the fuse holder will rotate and fall open under its own weight when the fuse blows. Mechanical tension on the fuse link normally holds an ejector spring in a stable position. When the fuse blows, the released spring pulls the stub of the fuse link out of the fuse holder tube to reduce surge duration and damage to the transformer and fuse holder. This quenches any arc in the fuse holder.\n\nEach fuse holder typically has an attached pull ring that can be engaged by a hook at the end of a fiberglass hot stick operated by a lineworker standing on the ground or from a bucket truck, to manually open the switch. While often used for switching, the standard cutout shown is not designed to be manually opened under load. For applications where the switch is likely to be used to interrupt power manually, a \"load break\" version is available that has an attachment to quench the arc.\n\nUp until the mid-1990s each manufacturer used their own dimensional standards for cutout design; by the late 1990s most cutouts were of an \"interchangeable design\". This design allows for the interchangeable use of cutout bodies and fuse holders manufactured by different vendors.\n"}
{"id": "24542276", "url": "https://en.wikipedia.org/wiki?curid=24542276", "title": "Garuda Indonesia Flight 035", "text": "Garuda Indonesia Flight 035\n\nGaruda Indonesia Flight 035 was a domestic Garuda Indonesia flight that struck a pylon and crashed on approach to Medan-Polonia Airport on 4 April 1987. 23 of the 45 passengers and crew on board were killed in the accident. \n\nThe aircraft was on an Instrument Landing System approach to Medan Airport in a thunderstorm. The aircraft struck electrical power lines and crashed short of the runway. The aircraft broke up and the tail section separated and fire broke out.\n\nMost of the survivors escaped through breaks in the fuselage and 11 were flung free of the aircraft. Four of the eight crew died and 19 passengers suffered fatal injuries due to smoke inhalation and burns. Four crew and 18 passengers suffered serious injuries. All of the fatalities were a result of the fire and not due to the impact with the ground.\n\nThe flight was carried out by a 1976-built Douglas DC-9-32 registered \"PK-GNQ\". The aircraft was damaged beyond repair.\n"}
{"id": "14344033", "url": "https://en.wikipedia.org/wiki?curid=14344033", "title": "Greenpeace Foundation", "text": "Greenpeace Foundation\n\nGreenpeace Foundation is an environmental organization based in Hawaii. It was officially founded in 1976 as an independent offshoot of the Canadian Greenpeace Foundation and was the first Greenpeace in the United States. When the original Vancouver-based Greenpeace Foundation later evolved to Greenpeace International the Hawaii-based Greenpeace Foundation declined to join them, and remains an unaffiliated organisation.\n\nThe greenpeace movement grew out of the \"peace\" and \"environmental\" movements in the early 1970s. Back then, it looked likely that the planet was going to be subjected to a nuclear war. The Canadian \"Don't Make a Wave\" committee formed to protest US tectonic nuclear testing at Amchitka in the Aleutian Islands and later the French atmospheric testing at Mururoa in French Polynesia. The phrase \"green peace\" was used as a slogan to describe the ideals of those activists, who envisioned a healthy (green) and peaceful (peace) planet as a good thing. By about 1973, the phrase had been shortened to a word, Greenpeace, and an organization called \"Greenpeace Foundation\" was established in Vancouver, Canada. It was this ragtag group of idealists and visionaries who first did an at-sea protest of whaling in 1975 and conducted the high-profile campaign against the clubbing of baby harp seals off Newfoundland beginning in 1976.\n"}
{"id": "477513", "url": "https://en.wikipedia.org/wiki?curid=477513", "title": "Helmholtz's theorems", "text": "Helmholtz's theorems\n\nIn fluid mechanics, Helmholtz's theorems, named after Hermann von Helmholtz, describe the three-dimensional motion of fluid in the vicinity of vortex filaments. These theorems apply to inviscid flows and flows where the influence of viscous forces are small and can be ignored.\n\nHelmholtz’s three theorems are as follows:\nHelmholtz’s first theorem:\nHelmholtz’s second theorem:\nHelmholtz’s third theorem:\n\nHelmholtz’s theorems apply to inviscid flows. In observations of vortices in real fluids the strength of the vortices always decays gradually due to the dissipative effect of viscous forces.\n\nAlternative expressions of the three theorems are as follows:\n1. The strength of a vortex tube does not vary with time.<br>\n2. Fluid elements lying on a vortex line at some instant continue to lie on that vortex line. More simply, vortex lines move with the fluid. Also vortex lines and tubes must appear as a closed loop, extend to infinity or start/end at solid boundaries.\n3. Fluid elements initially free of vorticity remain free of vorticity.\n\nHelmholtz’s theorems have application in understanding:\n\nHelmholtz’s theorems are now generally proven with reference to Kelvin's circulation theorem. However the Helmholtz's theorems were published in 1858, nine years before the 1867 publication of Kelvin's theorem. There was much communication between the two men on the subject of vortex lines, with many references to the application of their theorems to the study of smoke rings.\n\n"}
{"id": "22233981", "url": "https://en.wikipedia.org/wiki?curid=22233981", "title": "IGNITOR", "text": "IGNITOR\n\nIGNITOR is the Italian name for a nuclear research project of magnetic confinement fusion, developed by ENEA Laboratories in Frascati. Construction (in Russia) is not complete.\n\nThe project theory is based on ignited plasma in tokamak. Started in 1977 by Prof. Bruno Coppi of MIT, IGNITOR is based on the 1970s Alcator machine at MIT which pioneered the high magnetic field approach to plasma magnetic confinement, continued with the Alcator C/C-Mod at MIT and the FT/FTU series of experiments. \n\nCompared to the ITER international project, IGNITOR would be much smaller and cheaper: the ITER reactor is 19,000 tons in weight while the IGNITOR is only 500 tons in weight. IGNITOR is designed to produce approximately 100 MW of fusion power (and ITER to produce ~500 MW fusion power).\n\nAt a meeting with the scientific attachés of the European embassies in Moscow in early February 2010 Mikhail Kovalchuk, Director of the Kurchatov Institute, announced that an initiative aimed at developing a fast paced joint research programme in nuclear fusion research was strongly supported by the Governments of Russia and Italy.\n\nThe original proposal had been initiated earlier by Evgeny Velikhov (President of the Kurchatov Institute) and Bruno Coppi (Head of the High Energy Plasmas Undertaking, MIT) during the early developments of the Alcator C-Mod programme at MIT, where well known scientists of the Kurchatov Institute made key contributions to experiments that identified the unique confinement and purity properties of the high density plasmas produced by the high field Alcator machine. In effects this investigated, for the first time, physical processes leading to attain self-sustained fusion burning plasmas.\n\nThe collaboration with the Kurchatov Institute is directed at the construction of the Ignitor machine, the first experiment proposed to achieve ignition conditions by nuclear fusion reactions on the basis of existing knowledge of plasma physics and available technologies. Ignitor is part of the line of research on high magnetic field, experiments producing high density plasmas that began with the Alcator and the Frascati Torus programs at MIT and in Italy, respectively. It remains, at the world level, the only experiment capable of reaching ignition by the magnetic field confinement approach. However, several fusion scientists have contested the claim made for IGNITOR that it is a bigger step towards fusion power than the international ITER project.\n\nAccording to existing plans, Ignitor will be installed at the Triniti site at Troitsk near Moscow that has facilities which can be upgraded to house and operate the machine. This site will become open and made to be easily accessible to scientists of all nations. The management of the relevant research programme will involve Italy and Russia only to facilitate the success of the enterprise. The proponents have suggested that the US become an Associate Member of this effort with a similar arrangement to that made with CERN for its participation in the LHC (Large Hadron Collider) Programme.\n\nThe goal to produce meaningful fusion reactors in a reasonable time leads to pursuing the achievement of ignition conditions in the near term in order to understand the plasma physical regimes needed for a net power producing reactor. In addition, an objective other than ignition that can be envisioned for the relatively near term is that of high flux neutron sources for material testing involving compact, high density fusion machines. This has been one of the incentives that have led the Ignitor Project to adopt magnesium diboride (MgB) superconducting cables in the machine design, a first in fusion research. Accordingly, the largest coils (about 5 m diameter) of the machine will be made entirely of MgB cables.\n\nIn the context of the Italy-Russia summit meeting held in Milan on 26 April 2010 the agreement to proceed with the proposed joint Ignitor program has been signed. The participants, from the Russian side, have included the Prime Minister Vladimir Putin, the Deputy Prime Minister Igor Sechin, the Energy Minister Sergei Shmatko, and the Vice Minister of Education and Research Sergey Mazurenko. Participants from the Italian side have included Prime Minister Silvio Berlusconi, the Foreign Affairs Advisor to the Prime Minister Valentino Valentini (who had a key role in forging the agreement on the Ignitor program), and the Minister of Education and Research Mariastella Gelmini who, together with Sergey Mazurenko, signed the agreement in the presence of the two Prime Ministers.\n\nSome components have been built in Italy.\n\n\n"}
{"id": "13576575", "url": "https://en.wikipedia.org/wiki?curid=13576575", "title": "Isoionic point", "text": "Isoionic point\n\nThe isoionic point is the pH value at which a zwitterion molecule has an equal number of positive and negative charges and no adherent ionic species. It was first defined by S.P.L. Sørensen, Kaj Ulrik Linderstrøm-Lang and Ellen Lund in 1926 and is mainly a term used in protein sciences.\n\nIt is different from the isoelectric point (p\"I\") in that p\"I\" is the pH value at which the net charge of the molecule, \"including\" bound ions is zero. Whereas the isoionic point is at net charge zero in a deionized solution. Thus, the isoelectric and isoionic points are equal when the concentration of charged species is zero.\n\nFor a diprotic acid, the hydrogen ion concentration can be found at the isoionic point using the following equation\n\nformula_1\n\n\nNote that if formula_7 then formula_8 and if formula_9 then formula_10. Therefore, under these conditions, the equation simplifies to\n\nformula_11\n\nThe equation can be further simplified to calculate the pH by taking the negative logarithm of both sides to yield\n\nformula_12\n\nwhich shows that under certain conditions, the isoionic and isoelectric point are similar.\n"}
{"id": "35125702", "url": "https://en.wikipedia.org/wiki?curid=35125702", "title": "January 1992 nor'easter", "text": "January 1992 nor'easter\n\nThe January 1992 nor'easter was the second in a series of nor'easters in a 14-month period that produced strong winds, high tides, and flooding along the East Coast of the United States. It was a small, short-lived storm that was poorly forecast, intensifying rapidly on January 4 before striking the Eastern Shore of Virginia. The strongest quadrant of the storm moved over Delaware, and winds in the state reached . The nor'easter weakened as it moved westward, and it dissipated over Virginia before the energy reformed and redeveloped offshore.\n\nIn North Carolina, the storm flooded the main highway connecting the Outer Banks. The nor'easter struck shortly during a new moon, producing high tides that resulted in significant beach erosion along the Delmarva Peninsula. The highest wind gust was , reported in Chincoteague, Virginia. The cost of the lost beach near Ocean City, Maryland was estimated at over $10 million (1992 USD). In the city, the storm destroyed the tidal gauge, although the storm surge was estimated at . The strongest quadrant of the storm moved over Delaware; in the state, strong easterly winds produced significant tidal flooding, and 500 houses were damaged. A high tide of at Dewey Beach was the second highest tide on record in the entire state. Flooding also affected South Jersey in many areas that experienced flooding from the 1991 Perfect Storm in the previous October. Damage was estimated at $45 million (1992 USD). Strong winds reached as far north as New York, where a fallen tree seriously injured a person driving a car. Freezing rain associated with the storm caused a traffic fatality in New York, as well as several accidents in Maine.\n\nOn January 2, a small low pressure area developed along the East Coast of the United States. Due to its small size, the storm was poorly forecast. A strong upper-level low moving through Georgia and South Carolina caused the low to move to the west, perpendicular to the Mid-Atlantic coast. As it did so, the combination of warm air from the south and strong upper-level winds caused the low to rapidly intensify. On January 4, a buoy reported a minimum barometric pressure of , and maximum sustained winds of about . The system developed tropical characteristics, including the development of an eye feature in the center of the storm. Between 0500 and 0700 UTC on January 4, the storm moved ashore along the Eastern Shore of Virginia, about 25 mi (40 km) south of Ocean City, Maryland. It quickly weakened while moving across the Delmarva Peninsula and it emerged into the Chesapeake Bay by 1000 UTC that day. Three hours later, the storm was stalled over northern Virginia. A secondary low pressure area developed within the overall system further out to sea later on January 4.\n\nThe nor'easter struck about two months after the destructive Perfect Storm, and preceded another damaging storm in December 1992. The storm produced strong winds and high surf during a new moon, which resulted in astronomically high tides. Such tides caused coastal flooding and damage to dune systems. Due to its small size, the storm's impact was highly localized and did not spread far to the north of the Delaware Bay.\n\nHigh waves flooded North Carolina Highway 12 in the Outer Banks up to deep, closing a portion between Rodanthe and Oregon Inlet. Portions of Kitty Hawk and Pea Island were also flooded. Offshore, the storm washed 500 drums of arsenic overboard a freighter, which prompted a Coast Guard search for the toxic supplies. In Virginia, the storm produced a peak wind gust of at Chincoteague. Along the eastern shore, high surf destroyed one house and damaged 110 camping trailers. Flooding and high winds damaged the Wallops Flight Facility. High winds downed trees and power lines, leaving 4,000 people without power. Property damage was estimated at $2.2 million (1992 USD).\n\nIn Maryland, the storm struck just before high tide. A station on Assateague Island reported wind gusts of over during the storm's closest approach. On the island, the storm killed 11 Assateague Ponies and 20 deer. The cost to repair damages at Assateague Island National Seashore was estimated at $2.5 million (1992 USD). High winds in eastern Maryland left 32,000 people without power. The storm surge at Ocean City was estimated at 6.6 ft (2 m), which washed over the city's dune system in several locations. However, high waves destroyed the pier that the tidal gauge was on in the city. The combined effects of the nor'easter and the Perfect Storm from the previous October caused significant beach erosion along the Maryland coastline. In Ocean City, some streets lost a significant volume of sand while others gained sand. The cost of the lost beach was estimated between $10–30 million (1992 USD). Low-lying sections and coastal hotels in Ocean City were flooded. West of the city, the storm damaged several summer homes, washing four houses off their foundations. Damage in Ocean City was estimated around $700,000 (1992 USD), much of which to a public golf course.\nThe stronger northeast quadrant moved directly over Delaware for one tidal cycle, producing strong east-northeasterly wind gusts that peaked at at a station along the Indian River. The highest tides and flooding along the Delaware coastline were between Bowers and Prime Hook National Wildlife Refuge, where high waves built momentum from the Atlantic Ocean through the Delaware Bay. In Bowers, the tide reached along the Murderkill River; the reading was only less than the record set during the Ash Wednesday Storm of 1962. The high tides flooded marsh vegetation in the town. In Lewes, the highest tide was , which was the second highest on record; only the 1962 storm was higher, which produced a tide of . The storm caused significant flooding in Dewey Beach, where a high tide of marked the second-highest tide in Delaware history. The saltwater contaminated the town's water supply. The ocean washed over dunes in several locations along Delaware Bay, including in Fowler Beach where flooding reached inland. Near Mispillion Light, the high tides damaged the marina and several nearby buildings. The high tides flooded marshes and low-lying areas throughout the state, which briefly closed portions of state highways 54, 26, and 1. High tides destroyed 11 blocks of the boardwalk in Rehoboth Beach and half of the promenade at Bethany Beach. At nearby South Bethany, high waves destroyed three houses. Along the coast, the storm destroyed 15 houses and damaged 500 others, amounting to over $5 million (1992 USD) in property damage. In addition to the strong winds and high tides, the nor'easter produced light rainfall that did not cause any flooding. The highest total in the state was in Georgetown. Due to the damage, Kent and Sussex counties were declared federal disaster areas on February 6; this allocated federal funding for repairing storm damage. Then-governor Michael Castle activated the state's National Guard to assist clearing highways and prevent civilian access to damaged areas. Due to storm damage, access to Dewey Beach and Rehoboth Beach were restricted to residents.\n\nFurther northeast, the nor'easter produced wind gusts of , along with high tides; Steel Pier in Atlantic City reported a peak tide of about . The ocean flooded under the boardwalk onto adjacent streets, and to the west of the city, flooding closed portions of the White Horse Pike. In Cape May Point, the storm breached dunes that withstood the Perfect Storm, which flooded half of the borough. High tides also caused flooding in nearby Cape May, leaving behind of sand in some locations. There was significant dune damage in Strathmere and Harvey Cedars. The storm damaged the boardwalk in Ocean City, and other coastal towns had damage to bulkheads and sea walls. About 3,000 people were left without power in Somers Point. The storm affected 4,000 houses, of which 76 had severe damage. Overall damage in the state was estimated at $45 million (1992 USD), of which around 30% was public parks and buildings. However, damage was less than that from the Perfect Storm in the previous October. About 80 people evacuated to emergency shelters in South Jersey, and 95 residents of a nursing home in Atlantic City evacuated inland. Due to the storm damage, then-governor Jim Florio declared a state of emergency in Cape May, Atlantic, Ocean, and Monmouth counties. Due to the successive impacts of nor'easters in October and January, the state government provided money to restore beaches.\n\nFurther north, LaGuardia Airport outside New York City reported a wind gust of . Strong winds on Long Island knocked a tree onto a car, seriously injuring the driver. Freezing rain from the storm caused traffic accidents, killing one person in White Creek. In Oyster Bay, the winds pushed an oil barge into a small boat, causing heavy damage to the smaller vessel. Moisture spread as far north as Maine, where the storm dropped freezing rain; this caused several car accidents that resulted in 11 injuries.\n\n"}
{"id": "26706201", "url": "https://en.wikipedia.org/wiki?curid=26706201", "title": "Kyoto Protocol and government action", "text": "Kyoto Protocol and government action\n\nThis article is about the Kyoto Protocol and government action in relation to that treaty.\n\nIn total, Annex I Parties managed a cut of 3.3% in greenhouse gas (GHG) emissions between 1990 and 2004 (UNFCCC, 2007, p. 11). In 2007, projections indicated rising emissions of 4.2% between 1990 and 2010. This projection assumed that no further mitigation action would be taken. The reduction in the 1990s was driven significantly by economic restructuring in the economies-in-transition (EITs. See the main Kyoto Protocol article for the list of EITs). Emission reductions in the EITs had little to do with climate change policy (Carbon Trust, 2009, p. 24). Some reductions in Annex I emissions have occurred due to policy measures, such as promoting energy efficiency (UNFCCC, 2007, p. 11).\n\nOn the change of government following the election in November 2007, Prime Minister Kevin Rudd signed the ratification immediately after assuming office on 3 December 2007, just before the meeting of the UN Framework Convention on Climate Change; it took effect in March 2008. Australia's target is to limit its emissions to 8% above their 1990 level over the 2008–2012 period, i.e., their average emissions over the 2008–2012 period should be kept below 108% of their 1990 level (IEA, 2005, p. 51). According to the Australian government, Australia should meet its Kyoto target (IEA, 2005, p. 56; DCCEE, 2010).\n\nWhen he was in the opposition, Rudd commissioned Ross Garnaut to report on the economic effects of reducing greenhouse gas emissions. The report was submitted to the Australian government on 30 September 2008.\n\nThe policy of the Rudd government contrasts with that of the former Australian government, which refused to ratify the agreement on the ground that following the protocol would be costly.\n\nAustralia's position, under Prime Minister John Howard, was that it did not intend to ratify the treaty (IEA, 2005, p. 51). The justification for this was that:\nThe Howard government did intend to meet its Kyoto target, but without ratification (IEA, 2005, p. 51).\n\nAs part of the 2004 budget, A$1.8 billion was committed towards its climate change strategy. A$700 million was directed towards low-emission technologies (IEA, 2005, p. 56). The Howard government, along with the United States, agreed to sign the Asia Pacific Partnership on Clean Development and Climate at the ASEAN regional forum on 28 July 2005. Furthermore, the state of New South Wales (NSW) commenced the NSW greenhouse gas abatement scheme. This mandatory scheme of greenhouse gas emissions trading commenced on 1 January 2003 and is currently in trial by the state government in NSW alone. Notably, this scheme allows accredited certificate providers to trade emissions from households in the state. As of 2006, the scheme is still in place despite the outgoing Prime Minister's clear dismissal of emissions trading as a credible solution to climate change.\n\nFollowing the example of NSW, the national emissions trading scheme (NETS) has been established as an initiative of state and territory governments of Australia, all of which have Labor Party governments, except Western Australia. The purpose of NETS is to establish an intra-Australian carbon trading scheme to coordinate policy among regions. As the Constitution of Australia does not refer specifically to environmental matters (apart from water), the allocation of responsibility is to be resolved at a political level. In the later years of the Howard administration (1996–2007), the states governed by the Labor took steps to establish a NETS (a) to take action in a field where there were few mandatory federal steps and (b) as a means of facilitating ratification of the Kyoto Protocol by the incoming Labor government.\n\nIn May 2009, Kevin Rudd delayed and changed the carbon pollution reduction scheme:\n\nGreenpeace\n\nGreenpeace has called clause 3.7 of the Kyoto Protocol the \"Australia clause\" on the ground that it unfairly made Australia a major beneficiary. The clause allows annex 1 countries with a high rate of land clearing in 1990 to set the level in that year as a base. Greenpeace argues that since Australia had an extremely high level of land clearing in 1990, Australia's \"baseline\" was unusually high compared to other countries.\n\nIn 2002, Australia represented about 1.5% of global greenhouse gas (GHG) emissions (IEA, 2005, p. 51). Over the 1990–2002 period, Australia's gross emissions rose by 22%, which was surpassed by only four other International Energy Agency (IEA) members (IEA, 2005, p. 54). This was in large part due to economic growth. Net emissions (including changes in land-use and forestry) increased by 1.3% over this period. In 2005, Australia's GHG emissions made up 1.2% of the global total (MNP, 2007).\n\nPer-capita emissions are a country's total emissions divided by its population (Banuri \"et al\"., 1996, p. 95). In 2005, per capita emissions in Australia were 26.3 tons per capita (MNP, 2007).\n\nOn 17 December 2002, Canada ratified the treaty that came into force in February 2005, requiring it to reduce emissions to 6% below 1990 levels during the 2008–2012 commitment period (IEA, 2004, p. 52). Under Canada's Kyoto Protocol Implementation Act (KPIA), the National Round Table on the Environment and the Economy (NRTEE) is required to respond to the government's climate change plans (Canadian Government, 2010). In the assessment of NRTEE (2008), \"Canada is not pursuing a policy objective of meeting the Kyoto Protocol emissions reductions targets. [...] [The] projected emissions profile described in the 2008 [government plan] would leave Canada in non-compliance with the Kyoto Protocol.\"\n\nOn 13 December 2011, a day after the end of the 2011 United Nations Climate Change Conference, Canada's environment minister, Peter Kent, announced that Canada would withdraw from the Kyoto Protocol.\n\nIn 2001, Canadian emissions had grown by more than 20% above their 1990 level (IEA, 2004, p. 49). High population and economic growth, added to the expansion of CO emissions-intensive sectors, such as oil sand production, were responsible for this growth in emissions. By 2004, CO emissions had risen to 27% above the level in 1990.\nIn 2006 they were down to 21.7% above 1990 levels.\n\nIn 2005, Canada's GHG emissions made up 2% of the global total (MNP, 2007). Per capita emissions in Canada were 23.2 tons per capita.\n\nProjections\n\nIn 2004, Canada's emission projections under a business-as-usual scenario (i.e., predicted emissions should policy not be changed) indicated a rise of 33% on the 1990 level by 2010 (IEA, 2004, p. 52). This is a gap of approximately 240 Mt between its target and projected emissions.\n\nWhen the treaty was ratified in 2002, numerous polls showed support for the Kyoto protocol at around 70%. Despite strong public support, there was still some opposition, particularly by the Canadian Alliance, a precursor to the governing Conservative Party, some business groups, and energy concerns, using arguments similar to those being voiced in the U.S. In particular, there was a fear that since U.S. companies would not be affected by the Kyoto Protocol, Canadian companies would be at a disadvantage. In 2005, a \"war of words\" was ongoing, primarily between Alberta, Canada's primary oil and gas producer, and the federal government.\n\nBetween 1998 and 2004, Canada committed $3.7 billion towards investment on climate change activities (IEA, 2004, p. 52). The Climate Change Plan for Canada, released in November 2002, described priority areas for climate change policy.\n\nIn January 2006, a Conservative minority government under Stephen Harper was elected, who previously has expressed opposition to Kyoto, and in particular to the international emission trading. Rona Ambrose, who replaced Stéphane Dion as the environment minister, has since endorsed and expressed interests in some types of emission trading. On 25 April 2006, Ambrose announced that Canada would have no chance of meeting its targets under Kyoto, but would look to participate in the Asia-Pacific Partnership on Clean Development and Climate sponsored by the U.S. \"We've been looking at the Asia-Pacific Partnership for a number of months now because the key principles around [it] are very much in line with where our government wants to go,\" Ambrose told reporters. On 2 May 2006, it was reported that the funding to meet the Kyoto standards had been cut, while the Harper government develops a new plan to take its place. As the co-chair of the UN Climate Change Conference in Nairobi in November 2006, the Canadian government received criticism from environmental groups and other governments for its position. On 4 January 2007, Rona Ambrose moved from the Ministry of the Environment to become Minister of Intergovernmental Affairs. The environment portfolio went to John Baird, the former President of the Treasury Board.\n\nThe federal government has introduced legislation to set mandatory emissions targets for industry, but they will not take effect until 2012, with a benchmark date of 2006 as opposed to Kyoto's 1990. The government has since begun working with opposition parties to modify the legislation.\n\nA private member's bill was put forth by Pablo Rodriguez, Liberal, to force the government to \"ensure that Canada meets its global climate change obligations under the Kyoto Protocol.\" With the support of the Liberals, the New Democratic Party and the Bloc Québécois, and with the current minority situation, the bill passed the House of Commons on 14 February 2007 with a vote of 161 to 113. The Senate passed the bill, and it received Royal Assent on 22 June 2007. However, the government, as promised, has largely ignored the bill, which was to force the government 60 days to form a detailed plan, citing economic reasons.\n\nIn May 2007, the Friends of the Earth sued the federal government for failing to meet the Kyoto Protocol obligations to cut greenhouse gas emissions. The obligations were based on a clause in the Canadian Environmental Protection Act that requires Ottawa to \"prevent air pollution that violates an international agreement binding on Canada\". Canada's obligation to the treaty began in 2008.\n\nRegardless of the federal policy, some provinces are pursuing policies to restrain emissions, including Quebec, Ontario, British Columbia and Manitoba as part of the Western Climate Initiative. Since 2003 Alberta operates a carbon offset program.\n\nEnvironmental groups\n\nEnvironmental groups in Canada are working together to demand that Canadian politicians take the threat of climate change seriously and make the necessary changes to ensure the safety and health of future generations. Participating groups have created a petition called KYOTOplus, on which signatories commit to the following acts:\n• set a national target to cut greenhouse gas emissions at least 25 per cent from 1990 levels by 2020;\n• implement an effective national plan to reach this target and help developing countries adapt and build low-carbon economies; and\n• adopt a strengthened second phase of the Kyoto Protocol at the United Nations climate change conference at Copenhagen, Denmark in December 2009.\n\nKYOTOplus is a national, non-partisan, petition-centered campaign for urgent federal government action on climate change. There are over fifty partner organizations, including: Climate Action Network Canada, Sierra Club Canada, Sierra Youth Coalition, Oxfam Canada, the Canadian Youth Climate Coalition, Greenpeace Canada, KAIROS: Canadian Ecumenical Justice Initiatives and the David Suzuki Foundation.\n\nOn 13 December 2011, Canada's environment minister, Peter Kent, announced that Canada would withdraw from the Kyoto Protocol. The announcement was a day after the end of the 2011 United Nations Climate Change Conference (the 17th Conference of the Parties, or \"COP 17\"). At COP 17, the representatives of the Canadian government gave their support to a new international climate change agreement that \"includes commitments from all major emitters.\" Canadian representatives also stated that \"the Kyoto Protocol is not where the solution lies – it is an agreement that covers fewer than 30 per cent of global emissions (...).\"\n\nThe Canadian government invoked Canada's legal right to formally withdraw from the Kyoto Protocol on 12 December 2011. Canada was committed to cutting its greenhouse emissions to 6% below 1990 levels by 2012, but in 2009 emissions were 17% higher than in 1990. Environment minister Peter Kent cited Canada's liability to \"enormous financial penalties\" under the treaty unless it withdrew. He also suggested that the recently signed Durban agreement may provide an alternative way forward.\n\nChristiana Figueres, Executive Secretary of the UNFCCC, said that she regretted Canada's decision to withdraw from the Kyoto treaty, and that \"[whether] or not Canada is a Party to the Kyoto Protocol, it has a legal obligation under the [UNFCCC] to reduce its emissions, and a moral obligation to itself and future generations to lead in the global effort.\"\n\nCanada's decision received a mostly negative response from representatives of other ratifying countries. A spokesman for France's foreign ministry called the move \"bad news for the fight against climate change.\" Japan's own environment minister, Goshi Hosono, urged Canada to stay in the protocol. Some countries, including India, were worried that Canada's decision might jeopardise future conferences.\n\nA spokesperson for the island nation of Tuvalu, significantly threatened by rising sea levels, accused Canada of an \"act of sabotage\" against his country. Australian government minister Greg Combet, however, defended the decision, saying that it did not mean Canada would not continue to \"play its part in global efforts to tackle climate change\". China called Canada's decision to withdraw from the Kyoto Protocol \"regrettable\" and said that it went against the efforts of the international community. Canada's move came days after climate-change negotiators met to hammer-out a global deal in Durban, South Africa.\n\nForeign Ministry spokesman Liu Weimin expressed China's dismay at the news that Canada had pulled out of the Kyoto Protocol. Noting that the timing was particularly bad, because negotiators at the just-concluded Durban conference made what he described as important progress on the issue of the Kyoto Protocol's second commitment period.\n\nThe UK's Guardian newspaper reported on Canada's decision to withdraw from the Kyoto treaty. According to the Guardian, \"Canada's inaction was blamed by some on its desire to protect the lucrative but highly polluting exploitation of tar sands, the second biggest oil reserve in the world.\"\n\nOn 31 May 2002, all fifteen then-members of the European Union deposited the relevant ratification paperwork at the UN. Under the Kyoto Protocol, the 15 member countries that were Member States of the EU when the Protocol was agreed (EU-15) are committed to reducing their collective GHG emissions in the period 2008–12 to 8% below levels in 1990 (EEA, 2009, p. 9). All but one EU Member State (Austria) anticipate that they will meet their commitments under the Kyoto Protocol (EEA, 2009, pp. 11–12).\n\nDenmark has committed itself to reducing its emissions by 21%. On 10 January 2007, the European Commission announced plans for a European Union energy policy that included a unilateral 20% reduction in GHG emissions by 2020.\n\nThe EU has consistently been one of the major nominal supporters of the Kyoto Protocol, negotiating hard to get wavering countries on board.\n\nIn December 2002, the EU created an emissions trading system (EU ETS) in an effort to meet these tough targets. Quotas were introduced in six key industries: energy, steel, cement, glass, brick making, and paper/cardboard. There are also fines for member nations that fail to meet their obligations, starting at €40/ton of carbon dioxide in 2005, and rising to €100/ton in 2008.\n\nThe position of the EU is not without controversy in Protocol negotiations, however. One criticism is that, rather than reducing 8%, all the EU member countries should cut 15% as the EU insisted a uniform target of 15% for other developed countries during the negotiation while allowing itself to share a big reduction in the former East Germany to meet the 15% goal for the entire EU. According to Aldy \"et al\". (2003, p. 7), the \"hot air\" in German and UK targets allows the EU to meet its Kyoto target at low cost.\n\nBoth the EU (as the European Community) and its member states are signatories to the Kyoto treaty. Greece, however was excluded from the Kyoto Protocol on Earth Day (22 April 2008) due to unfulfilled commitment of creating the adequate mechanisms of monitoring and reporting emissions, which is the minimum obligation, and delivering false reports by having no other data to report. A United Nations committee has decided to reinstate Greece in the emissions-trading system of the Kyoto Protocol after a seven-month suspension (on 15 November).\n\nEmissions\n\nIn 2005, the EU-27 made up 11% of total global GHG emissions (MNP, 2007). Per capita emissions were 10.6 tons per capita.\n\nTransport CO emissions in the EU grew by 32% between 1990 and 2004. The share of transport in CO emissions was 21% in 1990, but by 2004 this had grown to 28%. Current EU projections suggest that by 2008 the EU will be at 4.7% below 1990 levels.\n\nFrance's Kyoto commitment is to cap its emissions at their 1990 levels (Stern, 2007, p. 456). The country has a national objective to reduce emissions by 25% from their 1990 levels by 2020, and a long-term target to reduce emissions 75–80% by 2050.\n\nIn 2002, France's total GHG emissions were roughly equivalent to 1990 levels, and 6.4% below 1990 levels when accounting for sink enhancements, as allowed under the Protocol (IEA, 2004, p. 58). In 2001, France's per capita emissions were 6.32 tCO per capita. Only five other IEA countries had lower levels (p. 59). France's CO intensity of GDP (energy-related CO emissions per gross domestic production (GDP)) was the fifth-lowest among all IEA countries.\n\nIn 2004, France shut down its last coal mine, and now gets 80% of its electricity from nuclear power and therefore has relatively low CO emissions, except for its transport sector.\n\nGermany has taken on a target under the Kyoto Protocol to reduce its GHG emissions by 21% compared with the base year 1990 (and in some cases, 1995) (IEA, 2007, pp. 44–45). Through 2004, Germany reduced its total GHG emissions by 17.4% (p. 45). Including the effects of land-use change increases this to 18.5%. The two main approaches Germany has used to meet its Kyoto target are reductions from the EU ETS, and reductions from the transport, household, and small business sectors (p. 51).\n\nGermany's progress towards its Kyoto target benefits from its reunification in 1990 (Liverman, 2008, p. 12). This is because of the reduction in emissions of East Germany after the fall of the Berlin Wall. CO emissions in Germany fell 12% between 1990 and 1995 (Barrett, 1998, p. 34). Germany reduced gas emissions by 22.4% between 1990 and 2008.\n\nOn 28 June 2006, the German government announced that it would exempt its coal industry from requirements under the E.U. internal emission trading system. Claudia Kemfert, an energy professor at the German Institute for Economic Research in Berlin said, \"For all its support for a clean environment and the Kyoto Protocol, the cabinet decision is very disappointing. The energy lobbies have played a big role in this decision.\" However, Germany's voluntary commitment to reduce CO emissions by 21% from the level in 1990 has practically been met, because emission has already been reduced by 19%. Germany is thus contributing 75% of the 8% reduction promised by the E.U.\n\nAccording to the UK government, projections indicate that the UK's GHG emissions will fall about 23% below base year levels by 2010 (DECC, 2009, p. 3). The UK's Kyoto target of a 12.5% reduction in emissions on their 1990 level (Stern, 2007, p. 456) benefits from the country's relatively high emissions in that year (1990) (Liverman, 2008, p. 12). Compared to their 1990 level, UK emissions in 1995 were lower by 7%. This was despite the fact that the UK had not adopted a radical policy to reduce emissions (Barrett, 1998, p. 34).\n\nSince 1990, the UK has privatized its energy-consuming industries, which has helped to increase their energy efficiency (US Senate, 2005, p. 218). The UK has also liberalized its electricity and gas systems, resulting in a change from coal to gas (the \"dash for gas\"), which has lowered emissions. It is estimated that these changes have contributed about half of the total observed reductions in UK CO emissions.\n\nThe energy policy of the United Kingdom fully endorses goals for carbon dioxide emissions reduction and has committed to proportionate reduction in national emissions on a phased basis. The U.K. is a signatory to the Kyoto Protocol.\n\nOn 13 March 2007, a draft Climate Change Bill was published after cross-party pressure over several years, led by environmental groups. Informed by the Energy White Paper 2003, the bill aims to achieve a mandatory reduction of 60% in the carbon emission from the 1990 level by 2050, with an intermediate target of between 26% and 32% by 2020. On 26 November 2008, the Climate Change Act became law with a target of 80% reduction over 1990. The U.K. is the first country to ratify a law with such a long-range and significant carbon reduction target.\n\nThe U.K. currently appears on course to meet its Kyoto limitation for the basket of greenhouse gases, assuming the government is able to curb CO₂ emissions between 2007 and 2008 to 2012. Although the overall greenhouse gas emissions in the UK have fallen, annual net carbon dioxide emission has increased by about 2% since the Labour Party came to power in 1997. As a result, it now seems highly unlikely that the government will be able to honour its pledge to cut carbon dioxide emissions by 20% from the 1990 level by 2010, unless an immediate and drastic action is taken under after the ratification of the Climate Change Bill.\n\nNorway's commitment under the Kyoto Protocol is to restrict its increase of GHGs to 1% above the 1990 level by the commitment period 2008–2012 (IEA, 2005, p. 46). In 2003, total emissions were 9% above the 1990 level. 99% of Norway's electricity from CO-free hydropower. Oil and gas extraction activities contributed 74% to the total increase of CO in the period 1990–2003.\n\nThe Norwegian government (2009, p. 11) projected a rise in GHG emissions of 15% from 1990 to 2010. Measures and policies adopted after autumn 2008 are not included in the baseline scenario (i.e., the predicted emissions that would occur without additional policy measures) for this projection (p. 55).\n\nBetween 1990 and 2007, Norway's greenhouse gas emissions increased by 12%. As well as directly reducing their own greenhouse gas emissions, Norway's idea for carbon neutrality is to finance reforestation in China, a legal provision of the Kyoto protocol.\n\nJapan ratified the Kyoto Protocol in June 2002, and has committed to reducing its GHG emissions by 6% below their 1990 levels (IEA, 2008, p. 47). Estimates for 2005 showed that Japan's emissions were 7.8% higher than in the base year.\n\nTo meet its Kyoto target, the government aims for a 0.6% reduction in domestic GHG emissions compared with the base year. It also aims to meet part of its target through a forest sink of 13 million tonnes of carbon, which is equivalent to a 3.8% cut. Another reduction of 1.6% is aimed for using the Kyoto flexible mechanisms.\n\nAccording to IEA (2008, p. 45), Japan is a world leader in the field of sustainable energy policies. The legislation guiding Japan's efforts to reduce emissions is the Kyoto Protocol Target Achievement Plan, passed in 2005 and later amended (p. 47). This Plan includes about 60 policies and measures. Most of these policies and measures are related to improved energy efficiency.\n\nWhen measured using market exchange rates, Japan's energy intensity in terms of total primary energy supply per unit of GDP is the lowest among IEA countries (p. 53). Measured in terms of purchasing power parity, its energy intensity is one of the lowest.\n\nEmissions\n\nIn 2005, Japan's energy-related CO per capita emissions were 9.5 metric tons per head of population (World Bank, 2010, p. 362). Japan's total energy-related CO emissions made up 4.57% of global emissions in this year. Over the period 1850–2005, Japan's cumulative energy-related CO emissions were 46.1 billion metric tons.\n\nNew Zealand signed the Kyoto Protocol to the UNFCCC on 22 May 1998 and ratified it on 19 December 2002. New Zealand's target is to limit net greenhouse gas emissions for the five-year 2008–2012 commitment period to five times the 1990 gross volume of GHG emissions. New Zealand may meet this target by either reducing emissions or by obtaining carbon credits from the international market or from domestic carbon sinks. The credits may be any of the Kyoto units; Assigned amount units (AAU), removal units (RMU), Emission Reduction Units (ERU) and Certified Emission Reduction (CER) units. In April 2012, the projection of New Zealand's net Kyoto position was a surplus of 23.1 million emissions units valued at NZ$189 million, based on an international carbon price of 5.03 Euro per tonne. On 9 November 2012, the New Zealand Government announced it would make climate pledges for the period from 2013 to 2020 under the UNFCCC process instead of adopting a binding limit under a second commitment period of the Kyoto Protocol.\n\nAt the 2012 United Nations Climate Change Conference New Zealand was awarded two 'Fossil of the Day' awards for \"actively hampering international progress\". The New Zealand Youth Delegation heavily criticised the New Zealand government, saying New Zealand's decision not to sign up for a second commitment period under the Kyoto Protocol was \"embarrassing, short-sighted and irresponsible\".\n\nUnder the Kyoto Protocol, the Russian Federation committed itself to keeping its GHG emissions at the base year level during the first Kyoto commitment period from 2008–2012 (UNFCCC, 2009, p. 3). UNFCCC (2009, p. 11) reported that Russian GHG emissions were projected to decline by 28% relative to base year level by 2010.\n\nThe process of economic transition in the Russian Federation was accompanied by a sharp decline in its GDP in the 1990s (p. 4). Since 1998, the Russian Federation has experienced strong economic growth. In the period 1990–2006, emissions decreased by 33%. The difference between GDP and emissions was mainly driven by:\n\nRussia accounts for about two-thirds of the expected emission savings from Joint Implementation (JI) projects by 2012 (Carbon Trust, 2009, p. 21). These savings are projected to amount to 190 megatonnes of carbon dioxide equivalent (CO-eq) over the 2008–2012 period (p. 23).\n\nPolitics\n\nVladimir Putin approved the treaty on 4 November 2004, and Russia officially notified the United Nations of its ratification on 18 November 2004. The issue of Russian ratification was particularly closely watched in the international community, as the accord was brought into force 90 days after Russian ratification (16 February 2005).\n\nPresident Putin had earlier decided in favor of the protocol in September 2004, along with the Russian cabinet, against the opinion of the Russian Academy of Sciences, of the Ministry for Industry and Energy, and of the then-president's economic adviser, Andrey Illarionov, and in exchange for the EU's support for Russia's admission into the WTO. As anticipated, after this, ratification by the lower (22 October 2004) and upper house of parliament did not encounter any obstacles.\n\nThere is an ongoing scientific debate on whether Russia will actually gain from selling credits for unused AAUs.\n\nThe United States has not ratified the Kyoto Protocol (IEA, 2007, p. 90). Doing so would have committed it to reduce GHG emissions by 7% below 1990 levels by 2012. Emissions of GHGs in the US increased by 16% between 1990 and 2005 (IEA, 2007, p. 83). In this period, the most substantial increase in volume were emissions from energy use, followed by industrial processes.\n\nIn 2002, the US government set a goal to reduce the GHG emissions of the US economy per unit of economic output (the emissions intensity of the economy) (IEA, 2007, p. 87). The set goal is to reduce the GHG intensity of the US economy by 18% by 2012. To achieve this, policy has focused on supporting energy research and development, including support for carbon capture and storage (CCS), renewables, methane capture and use, and nuclear power. The America's Climate Security Act of 2007, also more commonly referred to in the U.S. as the \"Cap and trade Bill\", was proposed for greater U.S. alignment with the Kyoto standards and goals.\n\nBetween 2001–2007, growth in US CO emissions was only 3%, comparable with to that of IEA Europe, and lower than that of a number of other countries, some of which are parties to the Kyoto Protocol (IEA, 2007, p. 90). In 2005, the US made up 16% of global GHG emissions, and had per capita emissions of 24.1 tons of GHG per capita (MNP, 2007).\n\nThe United States (US), although a signatory to the Kyoto Protocol, has neither ratified nor withdrawn from the Protocol. The signature alone is merely symbolic, as the Kyoto Protocol is non-binding on the United States unless ratified.\n\nOn 25 July 1997, before the Kyoto Protocol was finalized (although it had been fully negotiated, and a penultimate draft was finished), the US Senate unanimously passed by a 95–0 vote the Byrd–Hagel Resolution (S. Res. 98), which stated the sense of the Senate was that the United States should not be a signatory to any protocol that did not include binding targets and timetables for developing nations as well as industrialized nations or \"would result in serious harm to the economy of the United States\". On 12 November 1998, Vice President Al Gore symbolically signed the protocol. Both Gore and Senator Joseph Lieberman indicated that the protocol would not be acted upon in the Senate until there was participation by the developing nations. The Clinton Administration never submitted the protocol to the Senate for ratification.\n\nThe Clinton Administration released an economic analysis in July 1998, prepared by the Council of Economic Advisors, which concluded that with emissions trading among the annex B/annex I countries, and participation of key developing countries in the \"Clean Development Mechanism\"—which grants the latter business-as-usual emissions rates through 2012—the costs of implementing the Kyoto Protocol could be reduced as much as 60% from many estimates. Estimates of the cost of achieving the Kyoto Protocol carbon reduction targets in the United States, as compared by the Energy Information Administration (EIA), predicted losses to GDP of between 1.0% and 4.2% by 2010, reducing to between 0.5% and 2.0% by 2020. Some of these estimates assumed that action had been taken by 1998, and would be increased by delays in starting action.\n\nUnder the Presidency of George W. Bush, the US government recognized climate change as a serious environmental challenge (IEA, 2007, p. 87). The policy of the Bush administration was to reduce the GHG emissions of the US economy per unit of economic output (the emissions intensity of the economy). This policy allowed for absolute increases in emissions. The Bush administration viewed a policy to reduce absolute emissions as incompatible with continued economic growth. A number of states set state-level GHG targets, despite the absence of a federal level target.\n\nPresident George W. Bush did not submit the treaty for Senate ratification based on the exemption granted to China (now the world's largest gross emitter of carbon dioxide, although emission is low per capita). Bush opposed the treaty because of the strain he believed the treaty would put on the economy; he emphasized the uncertainties that he believed were present in the scientific evidence. Furthermore, the U.S. was concerned with broader exemptions of the treaty. For example, the U.S. did not support the split between annex I countries and others.\n\nAt the G8 meeting in June 2005 administration officials expressed a desire for \"practical commitments industrialized countries can meet without damaging their economies\". According to those same officials, the United States is on track to fulfil its pledge to reduce its carbon intensity 18% by 2012. In 2002, the US National Environmental trust labelled carbon intensity, \"a bookkeeping trick which allows the administration to do nothing about global warming while unsafe levels of emissions continue to rise.\" The United States has signed the Asia Pacific Partnership on Clean Development and Climate, a pact that allows those countries to set their goals for reducing greenhouse gas emissions individually, but with no enforcement mechanism. Supporters of the pact see it as complementing the Kyoto Protocol while being more flexible.\n\nThe Administration's position was not uniformly accepted in the US For example, economist Paul Krugman noted that the target 18% reduction in carbon intensity is still actually an increase in overall emissions. The White House has also come under criticism for downplaying reports that link human activity and greenhouse gas emissions to climate change and that a White House official, former oil industry advocate and current Exxon Mobil officer, Philip Cooney, watered down descriptions of climate research that had already been approved by government scientists, charges the White House denies. Critics point to the Bush administration's close ties to the oil and gas industries. In June 2005, State Department papers showed the administration thanking Exxon executives for the company's \"active involvement\" in helping to determine climate change policy, including the US stance on Kyoto. Input from the business lobby group Global Climate Coalition was also a factor.\n\nIn 2002, Congressional researchers who examined the legal status of the Protocol advised that signature of the UNFCCC imposes an obligation to refrain from undermining the Protocol's object and purpose, and that while the President probably cannot implement the Protocol alone, Congress can create compatible laws on its own initiative.\n\nPresident Barack Obama did not take any action with the senate that would change the position of the United States towards this protocol. When Obama was in Turkey in April 2009, he said that \"it doesn't make sense for the United States to sign [the Kyoto Protocol] because [it] is about to end\". At this time, two years and eleven months remained from the four-year commitment period.\n\nThe Framework Convention on Climate Change is a treaty negotiated between countries at the UN; thus individual states are not free to participate independently within this Protocol to the treaty.\nNonetheless, several separate initiatives have started at the level of state or city.\nEight Northeastern US states created the Regional Greenhouse Gas Initiative (RGGI), a state level emissions capping and trading program, using their own independently-developed mechanisms. Their first allowances were auctioned in November 2008.\n\nOn 27 September 2006, California Governor Arnold Schwarzenegger signed into law the bill AB 32, also known as the Global Warming Solutions Act, establishing a timetable to reduce the state's greenhouse-gas emissions, which rank at 12th-largest in the world, by 25% by the year 2020. This law effectively puts California in line with the Kyoto limitations, but at a date later than the 2008–2012 Kyoto commitment period. Many of the features of the Californian system are similar to the Kyoto mechanisms, although the scope and targets are different. The parties in the Western Climate Initiative expect to be compatible with some or all of the Californian model.\n\nAs of 14 June 2009, 944 US cities in 50 states, the District of Columbia and Puerto Rico, representing over 80 million Americans support Kyoto after Mayor Greg Nickels of Seattle started a nationwide effort to get cities to agree to the protocol. On 29 October 2007, it was reported that Seattle met their target reduction in 2005, reducing their greenhouse gas emissions by 8 percent since 1990.\n\n\nUNFCCC (2005) compiled and synthesized information reported to it by non-Annex I Parties. Most reporting non-Annex I Parties belonged in the low-income group, with very few classified as middle-income (p. 4). Most Parties included information on policies relating to sustainable development. Sustainable development priorities mentioned by non-Annex I Parties included poverty alleviation and access to basic education and health care (p. 6). Many non-Annex I Parties are making efforts to amend and update their environmental legislation to include global concerns such as climate change (p. 7).\n\nA few Parties, e.g., South Africa and Iran, stated their concern over how efforts to reduce emissions could affect their economies. The economies of these countries are highly dependent on income generated from the production, processing, and export of fossil fuels.\n\nEmissions\n\nGHG emissions, excluding land use change and forestry (LUCF), reported by 122 non-Annex I Parties for the year 1994 or the closest year reported, totalled 11.7 billion tonnes (billion = 1,000,000,000) of CO-eq. CO was the largest proportion of emissions (63%), followed by methane (26%) and nitrous oxide (NO) (11%).\n\nThe energy sector was the largest source of emissions for 70 Parties, whereas for 45 Parties the agriculture sector was the largest. Per capita emissions (in tonnes of CO-eq, excluding LUCF) averaged 2.8 tonnes for the 122 non-Annex I Parties.\n\nParties reported a high level of uncertainty in LUCF emissions, but in aggregate, there appeared to only be a small difference of 1.7% with and without LUCF. With LUCF, emissions were 11.9 billion tonnes, without LUCF, total aggregate emissions were 11.7 billion tonnes.\n\nBrazil has a national objective to increase the share of alternative renewable energy sources (biomass, wind and small hydropower) to 10% by 2030. It also has programmes to protect public forests from deforestation (Stern, 2007, p. 456).\n\nChina has a number of domestic policy measures that affect its GHG emissions (Jones \"et al.\", 2008, p. 26). These include a target to reduce the energy intensity of their GDP by 20% during the 2005–10 period. China plans to expand renewable energy generation to 15% of total capacity by 2020 (Wang \"et al.\", p. 86). Other policies include (Jones \"et al.\", 2008, p. 26):\n\nFrom 1995–2004, China energy efficiency efforts reduced its energy intensity by 30% (Wang \"et al.\", 2010, p. 87). From 2006–09, China achieved a 14.4% reduction in energy intensity. Renewables account for 8% of China's energy and 17% of its electricity. In response to the financial crisis, China implemented one of the world's largest stimulus's in efficient and clean energy (p. 85).\n\nEmissions\n\nIn 2005, China made up 17% of global GHG emissions, with per capita emissions of 5.8 tons of GHG per head (MNP, 2007). Another way of measuring GHG emissions is to measure the cumulative emissions that a country has emitted over time (IEA, 2007b, p. 199). Over a long time period, cumulative emissions provide an indication of a country's total contribution to GHG concentrations in the atmosphere. Measured over the time period 1900–2005, China's cumulative energy-related CO emissions made up 8% of the global total (IEA, 2007b, p. 201).\n\nA report by the Carbon Trust (2009) assessed the use of CDM in China. The CDM has been used to finance projects in China for renewable energy and HFC-23 reductions (HFC's are powerful greenhouse gases). For renewables, the CDM was judged to have helped to stimulate wind and small hydro power projects. Critics have argued that these policies would generally have taken place without the CDM (Carbon Trust, 2009, p. 56).\n\nIndia signed and ratified the Protocol in August 2002. Since India is exempted from the framework of the treaty, it is expected to gain from the protocol in terms of transfer of technology and related foreign investments. At the G8 meeting in June 2005, Indian Prime Minister Manmohan Singh pointed out that the per-capita emission rates of the developing countries are a tiny fraction of those in the developed world. Following the principle of \"common but differentiated responsibility\", India maintains that the major responsibility of curbing emission rests with the developed countries, which have accumulated emissions over a long period of time. However, the U.S. and other Western nations assert that India, along with China, will account for most of the emissions in the coming decades, owing to their rapid industrialization and economic growth.\n\nPolicies in India related to greenhouse gas emissions have included (Stern, 2007, p. 456; Jones \"et al.\", 2008, p. 26):\n\nEmissions\n\nIn 2005, India accounted for 5% of global GHG emissions, with per capita emissions of 2.1 tons of GHG per head of population (MNP, 2007). Over the time period 1900–2005, India's contribution to the global total of cumulative energy-related CO emissions was 2% (IEA, 2007b, p. 201).\n\nAlthough the Minister of State for environment Malik Min Aslam was at first not very receptive, he subsequently convinced the Shoukat Aziz cabinet to ratify the Protocol. The decision was taken in 2001 but due to international circumstances, it was announced in Argentina in 2004 and accepted in 2005, opening the way for the creation of a policy framework. On 11 January 2005, Pakistan submitted its instruments of accession to the Kyoto Protocol. The Ministry of Environment assigned the task to work as designated national authority (DNA). According to a news story by Khan (2009), it was expected that the Protocol would help Pakistan lower dependence on fossil fuels through renewable energy projects.\n\nPakistan had a per capita income of US$492 in 2002–2003, and is a low-income country (Pakistan government, 2003, p. 15). The Pakistan government is concentrating on reducing the vulnerability of the country to current climatic events (p. 17). Though Pakistan is a developing country, the government is taking different steps to lower the pollution.\n\nCDM\n\nIn February 2006, the national CDM operational strategy was approved, and on 27 April 2006, the first CDM project was approved by DNA. It was reduction of large NO from nitric acid production (investor: Mitsubishi, Japan), with an estimated annual production of 1 million CERs. Finally, in November 2006, the first CDM project was registered with the United Nations Framework Convention on Climate Change (UNFCCC).\n\nPakistan has specified preferences for the CDM projects, including (Pakistan government, 2006, pp. 3–4):\nSo far, 23 CDM so far have been approved by the Pakistan government (n.d.).\n\nEmissions\n\nOver the period from July 1993 to June 1994, Pakistan's energy sector was by far the highest contributor to CO emissions, with a share of 81% of total CO emissions (Pakistan government, 2003, pp. 16). Pakistan's energy-related CO emissions rose by 94.1% between 1990 and 2005 (World Bank, 2010, p. 362).\n\nPakistan's per capita emissions in 2005 were 0.8 tCO per head (p. 362). In 2005, Pakistan contributed 0.45% of the global total in energy-related CO emissions. Pakistan's cumulative emissions over the period 1850–2005 was 2.4 billion metric tons. Cumulative emissions before 1971 are based on data for East and West Pakistan.\n\nThe Asia-Pacific Partnership for Clean Development and Climate (APP) is a US-led effort to accelerate the voluntary development and deployment of clean energy technologies (UNEP, 2007, p. 257). The purpose of the Partnership is to address the issues of energy security, air pollution, and climate change (IEA, 2007, p. 51). The partner countries are Australia, Canada, China, India, Japan, Korea, and the United States (APP, n.d., p. 1). According to the APP (n.d.), the APP contributes to Partners' efforts under the UNFCCC, while \"complementing\" the Kyoto Protocol.\n\n Estimates by MNP (2007) are for the GHG emissions from fossil fuel use and cement production. Calculations are for carbon dioxide (CO), methane (CH), nitrous oxide (NO) and fluor containing gases (the F-gases HFCs, PFCs and SF). These estimates are subject to large uncertainties regarding CO emissions from deforestation; and the per country emissions of other GHGs (e.g., methane). There are also other large uncertainties that mean that small differences between countries are not significant. CO emissions from the decay of remaining biomass after biomass burning/deforestation are not included.\n\n"}
{"id": "8796024", "url": "https://en.wikipedia.org/wiki?curid=8796024", "title": "Liquidus", "text": "Liquidus\n\nThe liquidus temperature, T or T specifies the temperature above which a material is completely liquid, and the maximum temperature at which crystals can co-exist with the melt in thermodynamic equilibrium. It is mostly used for impure substances (mixtures) such as glasses, alloys and rocks.\n\nAbove the liquidus temperature the material is homogeneous and liquid at equilibrium. Below the liquidus temperature, more and more crystals will form in the melt if one waits a sufficiently long time, depending on the material. Alternately, homogeneous glasses can be obtained through sufficiently fast cooling, i.e., through kinetic inhibition of the crystallization process.\n\nThe crystal phase that crystallizes first on cooling a substance to its liquidus temperature is termed primary crystalline phase or primary phase. The composition range within which the primary phase remains constant is known as primary crystalline phase field.\n\nThe liquidus temperature is important in the glass industry because crystallization can cause severe problems during the glass melting and forming processes, and it also may lead to product failure.\n\nThe liquidus temperature can be contrasted to the solidus temperature. The solidus temperature quantifies the point at which a material completely solidifies (crystallizes). The liquidus and solidus temperatures do not necessarily coincide; if a gap exists between the liquidus and solidus temperatures, then within that gap, the material consists of solid and liquid phases simultaneously (like a slurry).\n\nFor pure substances, e.g. pure metal, pure water, etc. the liquidus and solidus are at the same temperature, and the term \"melting point\" may be used. For impure substances, e.g. alloys, tap water, Coca-Cola, ice cream, etc. the melting point broadens into a melting interval instead. If the temperature is within the melting interval, one may see \"slurries\" at equilibrium, i.e. the slurry will neither fully solidify nor melt. This is why new snow of high purity either melts or stays solid, while dirty snow on the ground tends to become slushy at certain temperatures. Weld melt pools containing high levels of sulfur, either from melted impurities of the base metal or from the welding electrode, typically have very broad melting intervals, which leads to increased risk of hot cracking.\n\n"}
{"id": "3369754", "url": "https://en.wikipedia.org/wiki?curid=3369754", "title": "Materials informatics", "text": "Materials informatics\n\nMaterials informatics is a field of study that applies the principles of informatics to materials science and engineering to better understand the use, selection, development, and discovery of materials. This is an emerging field, with a goal to achieve high-speed and robust acquisition, management, analysis, and dissemination of diverse materials data with the goal of greatly reducing the time and risk required to develop, produce, and deploy a new materials, which generally takes longer than 20 years.\nThis field of endeavor is not limited to some traditional understandings of the relationship between materials and information. Some more narrow interpretations include combinatorial chemistry, Process Modeling, materials property databases, materials data management and product life cycle management. Materials informatics is at the convergence of these concepts, but also transcends them and has the potential to achieve greater insights and deeper understanding by applying lessons learned from data gathered on one type of material to others. By gathering appropriate meta data, the value of each individual data point can be greatly expanded.\n\nThe concept of materials informatics is addressed by the \"Materials Research Society\". For example, materials informatics is the theme of the December 2006 issue of the \"MRS Bulletin\". The issue was guest-edited by John Rodgers of Innovative Materials, Inc., and David Cebon of Cambridge University, who describe the \"high payoff for developing methodologies that will accelerate the insertion of materials, thereby saving millions of investment dollars.\"\n\nThe editors focus on a limited definition of materials informatics, \"the application of computational methodologies to processing and interpreting scientific and engineering data concerning materials.\" They state that \"specialized informatics tools for data capture, management, analysis, and dissemination\" and \"advances in computing power, coupled with computational modeling and simulation and materials properties databases\" will enable such accelerated insertion of materials.\n\nThis view is not universally held. A broader definition goes beyond the use of computational methods to carry out the same experimentation. An evolved view of informatics creates a framework in which a measurement or computation is not simply a data point but a step in an information-based learning process that uses the power of a collective to achieve greater efficiency in exploration. When properly organized, this framework crosses materials boundaries to uncover fundamental knowledge of the basis of physical, mechanical, and engineering properties.\n\nWhile there are many that believe in the future of informatics in the materials development and scaling process, many challenges remain. Hill, et. al., write that \"Today, the materials community faces serious challenges to bringing about this data-accelerated research paradigm, including diversity of research areas within materials, lack of data standards, and missing incentives for sharing, among others. Nonetheless, the landscape is rapidly changing in ways that should benefit the entire materials research enterprise.\"\nThis remaining tension between traditional materials development methodologies and the use of more computationally, machine learning, and analytics approaches will likely exist for some time as the materials industry overcomes some of the cultural barriers necessary to fully embrace such new ways of thinking.\n\nThe overarching goals of bioinformatics and systems biology may provide a useful analogy. Andrew Murray of Harvard University expresses the hope that such an approach \"will save us from the era of \"one graduate student, one gene, one PhD\". Similarly, the goal of materials informatics is to save us from one graduate student, one alloy, one PhD. Such goals will require more sophisticated strategies and research paradigms than applying computational methods to the same tasks set currently undertaken by students.\n\nOrganisations which are working on Materials informatics include the following:\n\n\n(March 2007 JOM-e issue on M.I.)\n\n"}
{"id": "37435596", "url": "https://en.wikipedia.org/wiki?curid=37435596", "title": "Metals in medicine", "text": "Metals in medicine\n\nMetals in medicine are used in organic systems for diagnostic and treatment purposes. Inorganic elements are also essential for organic life as cofactors in enzymes called metalloproteins. When metals are scarce or high quantities, equilibrium is set out of balance and must be returned to its natural state via interventional and natural methods.\n\nMetals can be toxic in high quantities. Either ingestion or faulty metabolic pathways can lead to metal poisoning. Sources of toxic metals include cadmium from tobacco, arsenic from agriculture and mercury from volcanoes and forest fires. Nature, in the form of trees and plants, is able to trap many toxins and can bring abnormally high levels back into equilibrium. Toxic metal poisoning is usually treated with some type of chelating agent. Heavy metal poisoning, e.g., Hg, Cd, Pb, are particularly pernicious.\n\nExamples of specific types of toxic metals include:\n\nHumans need a certain amount of certain metals to function normally. Most metals are used as cofactors or prosthetics in enzymes, catalyzing specific reactions and serving essential roles. The essential metals for humans are: Sodium, Potassium, Magnesium, Copper, Vanadium, Chromium, Manganese, Iron, Cobalt, Nickel, Zinc, Molybdenum, and Cadmium. Anemia symptoms are caused by lack of a certain essential metal. Anemia can be associated with malnourishment or faulty metabolic processes, usually caused by a genetic defect.\n\nExamples of specific types of metal anemia include:\n\nMetal ions are often used for diagnostic medical imaging. Metal complexes can be used either for radioisotope imaging (from their emitted radiation) or as contrast agents, for example, in magnetic resonance imaging (MRI). Such imaging can be enhanced by manipulation of the ligands in a complex to create specificity so that the complex will be taken up by a certain cell or organ type.\nExamples of metals used for diagnosis include:\n\nMetals have been used in treatments since ancient times. The Ebers Papyrus from 1500BC is the first written account of the use of metals for treatment and describes the use of Copper to reduce inflammation and the use of iron to treat anemia. Sodium vanadate has been used since the early 20th century to treat rheumatoid arthritis. Recently metals have been used to treat cancer, by specifically attacking cancer cells and interacting directly with DNA. The positive charge on most metals can interact with the negative charge of the phosphate backbone of DNA. Some drugs developed that include metals interact directly with other metals already present in protein active sites, while other drugs can use metals to interact with amino acids with the highest reduction potential.\nExamples of Metals used in treatment include:\n\n"}
{"id": "40133041", "url": "https://en.wikipedia.org/wiki?curid=40133041", "title": "Musatepe Dam", "text": "Musatepe Dam\n\nThe Musatepe Dam is a gravity dam under construction on the Ortasu River (a tributary of the Hezil River) in Uludere district of Şırnak Province, southeast Turkey. Under contract from Turkey's State Hydraulic Works, Ozerka Insaat began construction on the dam in 2008 and a completion date has not been announced.\n\nThe reported purpose of the dam is water storage and it can also support a 9 MW hydroelectric power station in the future. Another purpose of the dam which has been widely reported in the Turkish press is to reduce the freedom of movement of Kurdistan Workers' Party (PKK) militants. Blocking and flooding valleys in close proximity to the Iraq–Turkey border is expected to help curb cross-border PKK smuggling and deny caves in which ammunition can be stored. A total of 11 dams along the border; seven in Şırnak Province and four in Hakkâri Province were implemented for this purpose. In Şırnak they are the Silopi, Şırnak, Uludere, Balli and Kavşaktepe Dams downstream of the Musatepe Dam and the Çetintepe Dam upstream on the Ortasu River. In Hakkari are the Gölgeliyamaç (since cancelled) and Çocuktepe Dams on the Güzeldere River and the Aslandağ and Beyyurdu Dams on the Bembo River.\n\n"}
{"id": "57098134", "url": "https://en.wikipedia.org/wiki?curid=57098134", "title": "NIO ES8", "text": "NIO ES8\n\nThe NIO ES8 is an all-electric, full-size sport utility vehicle manufactured by NIO. The ES8 was put into production in June 2018 for the Chinese market. \n\nNIO had been testing the vehicle for thousands of miles in Yakeshi, Inner Mongolia, China to test the car's economy and performance.\n\nThe production and delivery started end of June 2018. The price starts at $65,000 USD. The car was developed with the help of Tata Technologies. Nio is cooperating with Bosch.\n\nThe ES8 is powered by a lithium-ion battery pack, a package that is also swappable.\n\nThe ES8 is a 7-seater full-size production car, with a wheelbase of 2,997 mm (118 in) and a body length of 4,978 mm (196 in). The body and chassis are going to be completely aluminum, and the drivetrain will be all-wheel drive standard, and also featuring active air suspension. The design will include the X-bar and NIO's signature \"Spark Beat\" taillights. The range of the car with one battery charge is 500 km (311 miles).\n"}
{"id": "11323758", "url": "https://en.wikipedia.org/wiki?curid=11323758", "title": "Natig Aliyev", "text": "Natig Aliyev\n\nNatiq Aghaami oglu Aliyev (; November 23, 1947 – June 9, 2017) was an Azerbaijani politician. He served as the president of the State Oil Company of Azerbaijan Republic and was the Minister of Industry and Energy of Azerbaijan Republic at the time of his death.\n\nAliyev was born on November 23, 1947 in Baku, Azerbaijan. He graduated from secondary school in 1965. In 1970, he graduated from Azerbaijan State Oil Academy. He has a PhD in Geology-Mineralogy Sciences. He started working in the field once he was employed by \"Xəzərdənizneft\" state concern in 1970. Starting from 1984, Aliyev was appointed the Chief Instructor of Department at the administration of Central Committee of Azerbaijan Communist Party. In 1989-1991, he worked as Director of the Economic-Social Issues Department. In 1992, he appointed the Director of Ipesko representative office in Baku. At the same time, he worked as a consultant at SOCAR conducting research work for geophysical and geological projects, consulted on exploration and development, transportation and refining of oil, etc.\n\nIn 1993, he was appointed the Chairman of Board of Director and President of State Oil Company of Azerbaijan Republic by President Heydar Aliyev. He retained the post of the President until December 6, 2004 when he was appointed the Minister of Industry and Energy of Azerbaijan Republic. Rovnag Abdullayev replaced Aliyev as the head of SOCAR.\n\nAliyev died on June 9, 2017 at Florence Nightingale Hospital in Istanbul from a heart ailment after suffering a heart attack the week before in Baku.\n\nAliyev was the Chairman of the State Committee for Development of Azeri-Chirag-Guneshli fields and is the Chairman of the Board of Directors of Baku-Tbilisi-Ceyhan (BTC). He authored more than 100 scientific publications, articles and books. In 2008, he was elected a member of the International Engineering Academy in Moscow. Aliyev has been awarded with Shohrat Order due to service in development of oil industry in Azerbaijan Republic. He was also awarded with Order of the Glory of Georgia, Legion d’Honneur of France and Meritted Engineer Medal of Azerbaijan Republic.\n\n"}
{"id": "21586", "url": "https://en.wikipedia.org/wiki?curid=21586", "title": "Nereid", "text": "Nereid\n\nIn Greek mythology, the Nereids ( ; \"Nereides\", sg. \"Nereis\") are sea nymphs (female spirits of sea waters), the 50 daughters of Nereus and Doris, sisters to Nerites. They often accompany Poseidon, the god of the sea, and can be friendly and helpful to sailors, like the Argonauts in their search for the Golden Fleece.\n\nNereids are particularly associated with the Aegean Sea, where they dwelt with their father Nereus in the depths within a golden palace. The most notable of them are Thetis, wife of Peleus and mother of Achilles; Amphitrite, wife of Poseidon; and Galatea, the vain love interest of the Cyclops Polyphemus.They symbolized everything that is beautiful and kind about the sea. Their melodious voices sang as they danced around their father. They are represented as very beautiful girls, crowned with branches of red coral and dressed in white silk robes trimmed with gold, but who went barefoot. They were part of Poseidon's entourage and carried his trident.\n\nIn Homer's \"Iliad\" XVIII, when Thetis cries out in sympathy for the grief of Achilles for the slain Patroclus, her sisters appear. The Nereid Opis is mentioned in Virgil's \"Aeneid\". She is called by the goddess Diana to avenge the death of the Amazon-like female warrior Camilla. Diana gives Opis magical weapons for revenge on Camilla's killer, the Etruscan Arruns. Opis sees and laments Camilla's death and shoots Arruns in revenge as directed by Diana.\n\nIn modern Greek folklore, the term \"nereid\" (, \"neráida\") has come to be used for all nymphs, fairies, or mermaids, not merely nymphs of the sea.\n\nNereid, a moon of the planet Neptune, is named after the Nereids.\n\n\"This list is correlated from four sources: Homer's \"Iliad\", Hesiod's \"Theogony\", the \"Bibliotheca\", and Hyginus\"\". Because of this, the total number of names goes beyond fifty.\"\n"}
{"id": "13986656", "url": "https://en.wikipedia.org/wiki?curid=13986656", "title": "Neuroprotectin", "text": "Neuroprotectin\n\nNeuroprotectin D1 (NPD1) (10\"R\",17\"S\"-dihydroxy-4\"Z\",7\"Z\",11\"E\",13\"E\",15\"Z\",19\"Z\"-docosahexaenoic acid) also known as Protectin D1 (PD1) is a docosanoid derived from the polyunsaturated fatty acid (PUFA) docosahexaenoic acid (DHA), which is a component of fish oil and the most important omega-3 PUFA. Like other members of the specialized proresolving mediators class of PUFA metabolites, NPD1 exerts potent anti-inflammatory and anti-apoptotic/neuroprotective bioactivity. Other neuroprotectins with similar activity include: PDX (10\"R\",17\"S\"-dihydroxy-4\"Z\",7\"Z\",11\"E\",13\"Z\",15\"E\",19\"Z\"-docosahexaenoic acid); 20-hydroxy-PD1 (10\"R\",17\"S\",20-trihydroxy-4\"Z\",7\"Z\",11\"E\",13\"E\",15\"Z\",19\"Z\"-docosahexaenoic acid); and 10-epi-PD1 (10\"R\",17\"S\"-Dihydroxy-4\"Z\",7\"Z\",11\"E\",13\"E\",15\"Z\",19\"Z\"-docosahexaenoic acid). The activity of neuroprotectin-like metabolite, 17-epi-PD1 (10\"R\",17\"R\"-dihydroxy-4\"Z\",7\"Z\",11\"E\",13\"E\",15\"Z\",19\"Z\"-docosahexaenoic acid), has not yet been reported.\n\nNeuroprotectins A and B, which are bicyclohexapeptides, are to be distinguished structurally and mechanistically from the neuroprotectin D's.\n\n"}
{"id": "49071845", "url": "https://en.wikipedia.org/wiki?curid=49071845", "title": "Nordsee One offshore wind farm", "text": "Nordsee One offshore wind farm\n\nNordsee One is an offshore wind farm in the German part of the North Sea. \nIt has a nameplate capacity of 332 MW and was commissioned in 2017. \nIt uses 54 Senvion 6.2M126 wind turbines that are expected to produce 1200 GWh of electricity annually. The wind farm is owned by Northland Power (85%) and Innogy (15%). \nOffshore construction began in December 2015.\n\n"}
{"id": "14656113", "url": "https://en.wikipedia.org/wiki?curid=14656113", "title": "Oil megaprojects (2004)", "text": "Oil megaprojects (2004)\n\nThis page summarizes projects that brought more than of new liquid fuel capacity to market with the first production of fuel beginning in 2004. This is part of the Wikipedia summary of Oil Megaprojects—see that page for further details. 2004 saw 24 projects come on stream with an aggregate capacity of when full production was reached (which may not have been in 2004).\n\nSee also the 2004 world oil market chronology\n\nThis table is available in csv format here (updated daily).\n"}
{"id": "14507786", "url": "https://en.wikipedia.org/wiki?curid=14507786", "title": "Petrochemical industry in Romania", "text": "Petrochemical industry in Romania\n\nRomania was the largest European producer of oil in World War II. The oil extracted from Romania was essential for Axis military operations. The petrochemical industry near Ploieşti was bombed heavily by American bombers (see Operation Tidal Wave). After the war, a heavy reconstruction and expansion was done under the communist regime. Since then, most of the industry has been privatized.\n\nPossessing substantial oil refining capacities, Romania is particularly interested in the Central Asia-Europe pipelines and seeks to strengthen its relations with some Arab States of the Persian Gulf. With 10 refineries and an overall refining capacity of approximately , Romania has the largest refining industry in the region. Romania's refining capacity far exceeds domestic demand for refined petroleum products, allowing the country to export a wide range of oil products and petrochemicals — such as lubricants, bitumen, and fertilizers — throughout the region.\n\nThis is an incomplete list of oil refineries in Romania:\n\n\n1857 - First drilling of oil wells at Bend, northeast of Bucharest, on the Romanian side of the Carpathians; this year is registered as a beginning of Romania’s oil production. In 1846, in Baku (settlement Bibi-Heybat) the first ever well drilled with percussion tools to a depth of 21 meters for oil exploration, based on data of Nicolay Voskoboynikov; it was 11 years before the Romanian's well was drilled in Bend.\n\n\n4. www.geohelp.net/world.html; \n5. Mirbabayev Miryusif. \"Brief history of the first oil drilling wells in Baku region\" - \"Noema\" (Romania), 2018, v.XVII, p.175-185\n6. \n"}
{"id": "21919642", "url": "https://en.wikipedia.org/wiki?curid=21919642", "title": "Porta-Color", "text": "Porta-Color\n\nGeneral Electric's Porta-Color was the first \"portable\" color television introduced in the United States in 1966.\n\nThe Porta-Color set introduced a new variation of the shadow mask display tube. It had the electron guns arranged in an in-line configuration, rather than RCA's delta arrangement. The main benefit of the in-line gun arrangement is that it simplified the convergence process, and did not become easily misaligned when moved, thus making true portability possible. There were many variations of this set produced from its introduction in 1966 until 1978, all using GE's Compactron vacuum tubes (valves).\n\nThe name has been variously written, even in GE's literature, as \"Porta Color\", \"Porta-Color\" and \"Porta-color\". The name may also refer to the specific television model, or less commonly, the style of television tube it used.\n\nA conventional black and white television (B&W) uses a tube that is uniformly coated with a phosphor on the inside face. When excited by high-speed electrons, the phosphor gives off light, typically white but other colors are also used in certain circumstances. An electron gun at the back of the tube provides a beam of high-speed electrons, and a set of electromagnets arranged near the gun allow the beam to be moved around the display. Time base generators are used to produce a scanning motion. The television signal is sent as a series of stripes, each one of which is displayed as a separate line on the display. The strength of the signal increases or decreases the current in the beam, producing bright or dark points on the display as the beam sweeps across the tube.\n\nIn a color display the uniform coating of white phosphor is replaced by dots or lines of three colored phosphors, producing red, green or blue light (RGB color model) when excited. When excited in the same fashion as a B&W tube, the three phosphors produce different amount of these primary colors, which mix in the human eye to produce an apparent color. To produce the same resolution as the B&W display, a color screen must have three times the resolution. This presents a problem for conventional electron guns, which cannot be focused or positioned accurately enough to hit these much smaller individual patterns.\n\nThe conventional solution to this problem was introduced by RCA in 1950, with their shadow mask system. The shadow mask is a thin steel sheet with small round holes cut into it, positioned so the holes lie directly above one triplet of colored phosphor dots. Three separate electron guns are individually focussed on the mask, sweeping the screen as normal. When the beams pass over one of the holes, they travel through it, and since the guns are separated by a small distance from each other at the back of the tube, each beam has a slight angle as it travels through the hole. The phosphor dots are arranged on the screen such that the beams hit only their correct phosphor.\n\nThe primary problem with the shadow mask system is that the vast majority of the beam energy, typically 85%, is lost 'lighting up' the mask as the beam passes over the opaque sections between the holes. This means that the beams must be greatly increased in power to produce acceptable brightness when they do pass through the holes.\n\nPaul Pelczynski was the project leader in the conception and production of the General Electric Porta Color in 1966. \nGeneral Electric (GE) had been working on a variety of systems that would allow them to introduce color sets that did not rely on the shadow mask patents. Through the 1950s they had put considerable effort into the Penetron concept, but were never able to make it work as a basic color television, and started looking for alternate arrangements. GE eventually improved on the basic shadow mask system with a simple change to layout.\n\nInstead of arranging the guns, and phosphors, in a triangle, their system arranged them side-by-side. This meant that the phosphors did not have to be displaced from each other in two directions, only one, which allowed much-simplified convergence adjustments of the three beams, compared to the conventional delta shadow mask tube. This differed sufficiently from RCA's design to allow GE to circumvent the patents. It is important to realize that the GE 11\" tube still had round mask holes and phosphor dots, not rectangular ones as in the later slot-mask tubes. The innovation here was with the in-line guns as opposed to the triad arrangement.\n\nThis change, which allowed vastly simpler convergence measures, together with the use of GE's own Compactron multi-function vacuum tubes led to reductions in size of the entire chassis. GE used the small size of their system as the primary selling feature. The original 28 pound set used a 11\" tube and sold for $249, which was very inexpensive for a color set at that time. Introduced in 1966, the Porta-Color was extremely successful and led to a rush by other companies to introduce similar systems. GE continued refining this system, up until 1978, which marked the end of production of vacuum tube type television receivers.\n\nGE produced the basic Porta-Color design well into the 1970s, even after most companies had moved to solid state designs when transistors with the required power capabilities were introduced. The Porta Color II was their attempt at a solid state version, but did not see widespread sales. The basic technology, however, was copied into GE's entire lineup as product refresh cycles allowed. By the early 1970s most companies had introduced the \"slot-mask\" designs, including RCA.\n\nIn a conventional shadow mask television design the electron guns at the back of the tube are arranged in a triangle. They are individually focused, with some difficulty, so that the three beams meet at a spot when they reach the shadow mask. The mask cuts off any unfocussed portions of the beams, which then continue through the holes towards the screen. Since the beams approach the mask at an angle, they separate again on the far side of the mask. This allows the beams to address the individual phosphor spots on the back of the screen.\n\nGE's design modified this layout by arranging the electron guns in a side-by-side line (the \"in-line gun\") instead of a triangle (the \"delta gun\"). This meant that after they passed through the mask they separated horizontally only, hitting phosphors that were also arranged side-by-side. Otherwise, the GE design retained the round dot structure. \n\nLater, Sony changed the whole game, replacing the shadow mask with an aperture grille and the phosphor dots with vertical phosphor stripes. They cleverly implemented a single electron gun with three independent cathodes, later branded Trinitron, all of which greatly simplified convergence.\n\nToshiba then countered this with their slot-mask system, which was somewhat in-between the Trinitron and the original delta-mask systems.\nSony attempted to stop Toshiba from producing their in-line gun system, citing patent violations, but Toshiba won this battle, and the Toshiba tube eventually became the standard in most domestic television receivers.\n\nOver the years, the Porta Color has attracted interest as an old (dead) technology to be rescued. Once considered throw-away items, the Porta Color has become a collectible item, being the last all-vacuum tube color television made in the US.\n\n\n\n"}
{"id": "30103934", "url": "https://en.wikipedia.org/wiki?curid=30103934", "title": "Puntukas", "text": "Puntukas\n\nPuntukas is the second-largest boulder in Lithuania. It is situated some south of Anykščiai on the left bank of the Šventoji River. It was believed to be the largest stone in Lithuania until the discovery of Barstyčiai stone in the Skuodas district in 1957.\n\nPuntukas is a glacial erratic—it was brought by glaciers during the last glacial period (18th–12th millennium BC) probably from Finland. It measures in length, in width, and in depth (including underground). It weighs about 265 tons. It is made of Rapakivi granite. Its reddish mass includes large crystals of potassium feldspar surrounded by green rings of oligoclase.\n\nIn 1943, sculptor Bronius Pundzius engraved portraits and quotes from last wills of Lithuanian pilots Steponas Darius and Stasys Girėnas for the 10th anniversary of their deaths during the transatlantic flight with \"Lituanica\". A local legend has it that velnias (a devil in the Lithuanian mythology) carried the stone to destroy the Anykščiai Church, however a rooster crowed. The devil disappeared back into the underworld, leaving only Puntukas behind. The legend was featured in the famous poem \"Anykščių šilelis\" by Antanas Baranauskas. According to another story, a brave Lithuanian warrior Puntukas was killed and was burned (a usual pagan custom) on the stone; since then it is known as Puntukas stone. Other legends claim that the stone was a pagan shrine and that oaks growing around are relics of the sacred groves.\n"}
{"id": "621749", "url": "https://en.wikipedia.org/wiki?curid=621749", "title": "Radioactive contamination", "text": "Radioactive contamination\n\nThe sources of radioactive pollution can be classified into two groups: natural and man made.\n\nRadioactive contamination, also called radiological contamination, is the deposition of, or presence of radioactive substances on surfaces or within solids, liquids or gases (including the human body), where their presence is unintended or undesirable (from the International Atomic Energy Agency - IAEA - definition).\n\nSuch contamination presents a hazard because of the radioactive decay of the contaminants, which emit harmful ionising radiation such as alpha particles or beta particles, gamma rays or neutrons. The degree of hazard is determined by the concentration of the contaminants, the energy of the radiation being emitted, the type of radiation, and the proximity of the contamination to organs of the body. It is important to be clear that the contamination gives rise to the radiation hazard, and the terms \"radiation\" and \"contamination\" are not interchangeable.\n\nContamination may affect a person, a place, an animal, or an object such as clothing. Following an atmospheric nuclear weapon discharge or a nuclear reactor containment breach, the air, soil, people, plants, and animals in the vicinity will become contaminated by nuclear fuel and fission products. A spilled vial of radioactive material like uranyl nitrate may contaminate the floor and any rags used to wipe up the spill. Cases of widespread radioactive contamination include the Bikini Atoll, the Rocky Flats Plant in Colorado, the Fukushima Daiichi nuclear disaster, the Chernobyl disaster, and the area around the Mayak facility in Russia.\n\nRadioactive contamination can be due to a variety of causes. It may occur due to release of radioactive gases, liquids or particles. For example, if a radionuclide used in nuclear medicine is spilled (accidentally or, as in the case of the Goiânia accident, through ignorance), the material could be spread by people as they walk around. \n\nRadioactive contamination may also be an inevitable result of certain processes, such as the release of radioactive xenon in nuclear fuel reprocessing. In cases that radioactive material cannot be contained, it may be diluted to safe concentrations. For a discussion of environmental contamination by alpha emitters please see actinides in the environment.\n\nNuclear fallout is the distribution of radioactive contamination by the 520 atmospheric nuclear explosions that took place from the 1950s to the 1980s. \n\nIn nuclear accidents, a measure of the type and amount of radioactivity released,such as from a reactor containment failure, is known as the source term. The United States Nuclear Regulatory Commission defines this as \"Types and amounts of radioactive or hazardous material released to the environment following an accident.\"\n\nContamination does not include residual radioactive material remaining at a site after the completion of decommissioning. Therefore, radioactive material in sealed and designated containers is not properly referred to as contamination, although the units of measurement might be the same.\n\nContainment is the primary way of preventing contamination being released into the environment or coming into contact or being ingested by humans.\n\nBeing within the intended Containment differentiates radioactive \"material\" from radioactive \"contamination\". When radioactive materials are concentrated to a detectable level outside a containment, the area affected is generally referred to as \"contaminated\".\n\nThere are a large number of techniques for containing radioactive materials so that it does not spread beyond the containment and become contamination. In the case of liquids this is by the use of high integrity tanks or containers, usually with a sump system so that leakage can be detected by radiometric or conventional instrumentation.\n\nWhere material is likely to become airborne, then extensive use is made of the glovebox, which is a common technique in hazardous laboratory and process operations in many industries. The gloveboxes are kept under a slight negative pressure and the vent gas is filtered in high efficiency filters, which are monitored by radiological instrumentation to ensure they are functioning correctly.\n\nA variety of radionuclides occur naturally in the environment. Elements like uranium and thorium, and their decay products, are present in rock and soil. Potassium-40, a primordial nuclide, makes up a small percentage of all potassium and is present in the human body. Other nuclides, like carbon-14, which is present in all living organisms, are continuously created by cosmic rays.\n\nThese levels of radioactivity pose little danger but can confuse measurement. A particular problem is encountered with naturally generated radon gas which can affect instruments which are set to detect contamination close to normal background levels and can cause false alarms. Because of this skill is required by the operator of radiological survey equipment to differentiate between background radiation and the radiation which emanates from contamination.\n\nNaturally occurring radioactive materials (NORM) can be brought to the surface or concentrated by human activities like mining, oil and gas extraction and coal consumption.\n\nRadioactive contamination may exist on surfaces or in volumes of material or air, and specialist techniques are used to measure the levels of contamination by detection of the emitted radiation.\n\nContamination monitoring depends entirely upon the correct and appropriate deployment and utilisation of radiation monitoring instruments.\n\nSurface contamination may either be fixed or \"free\". In the case of fixed contamination, the radioactive material cannot by definition be spread, but its radiation is still measurable. In the case of free contamination there is the hazard of contamination spread to other surfaces such as skin or clothing, or entrainment in the air. A concrete surface contaminated by radioactivity can be shaved to a specific depth, removing the contaminated material for disposal.\n\nFor occupational workers controlled areas are established where there may be a contamination hazard. Access to such areas is controlled by a variety of barrier techniques, sometimes involving changes of clothing and foot wear as required. The contamination within a controlled area is normally regularly monitored. Radiological protection instrumentation (RPI) plays a key role in monitoring and detecting any potential contamination spread, and combinations of hand held survey instruments and permanently installed area monitors such as Airborne particulate monitors and area gamma monitors are often installed.\nDetection and measurement of surface contamination of personnel and plant is normally by Geiger counter, scintillation counter or proportional counter. Proportional counters and dual phosphor scintillation counters can discriminate between alpha and beta contamination, but the Geiger counter cannot. Scintillation detectors are generally preferred for hand held monitoring instruments, and are designed with a large detection window to make monitoring of large areas faster. Geiger detectors tend to have small windows, which are more suited to small areas of contamination.\n\nThe spread of contamination by personnel exiting controlled areas in which nuclear material is used or processed is monitored by specialised installed exit control instruments such as frisk probes, hand contamination monitors and whole body exit monitors. These are used to check that persons exiting controlled areas do not carry contamination on their body or clothes.\n\nIn the United Kingdom the HSE has issued a user guidance note on selecting the correct portable radiation measurement instrument for the application concerned. This covers all radiation instrument technologies, and is a useful comparative guide for selecting the correct technology for the contamination type.\n\nThe UK NPL publishes a guide on the alarm levels to be used with instruments for checking personnel exiting controlled areas in which contamination may be encountered.\nSurface contamination is usually expressed in units of radioactivity per unit of area for alpha or beta emitters. For SI, this is becquerels per square meter (or Bq/m). Other units such as picoCuries per 100 cm or disintegrations per minute per square centimeter (1 dpm/cm = 167 Bq/m) may be used.\n\nThe air can be contaminated with radioactive isotopes in particulate form, which poses a particular inhalation hazard. Respirators with suitable air filters, or completely self-contained suits with their own air supply can mitigate these dangers.\n\nAirborne contamination is measured by specialist radiological instruments that continuously pump the sampled air through a filter. Airborne particles accumulate on the filter and can be measured in a number of ways:\n\n\nCommonly a semiconductor radiation detection sensor is used that can also provide spectrographic information on the contamination being collected.\n\nA particular problem with airborne contamination monitors designed to detect alpha particles is that naturally occurring radon can be quite prevalent and may appear as contamination when low contamination levels are being sought. Modern instruments consequently have \"radon compensation\" to overcome this effect.\n\nSee the article on Airborne particulate radioactivity monitoring for more information.\n\nRadioactive contamination can enter the body through ingestion, inhalation, absorption, or injection. This will result in a committed dose of radiation.\n\nFor this reason, it is important to use personal protective equipment when working with radioactive materials. Radioactive contamination may also be ingested as the result of eating contaminated plants and animals or drinking contaminated water or milk from exposed animals. Following a major contamination incident, all potential pathways of internal exposure should be considered.\n\nSuccessfully used on Harold McCluskey, chelation therapy and other treatments exist for internal radionuclide contamination.\n\nCleaning up contamination results in radioactive waste unless the radioactive material can be returned to commercial use by reprocessing. In some cases of large areas of contamination, the contamination may be mitigated by burying and covering the contaminated substances with concrete, soil, or rock to prevent further spread of the contamination to the environment. If a person's body is contaminated by ingestion or by injury and standard cleaning cannot reduce the contamination further, then the person may be permanently contaminated.\n\nContamination control products have been used by the U.S. Department of Energy (DOE) and the commercial nuclear industry for decades to minimize contamination on radioactive equipment and surfaces and fix contamination in place. “Contamination control products” is a broad term that includes fixatives, strippable coatings, and decontamination gels. A \"fixative\" product functions as a permanent coating to stabilize residual loose/transferable radioactive contamination by fixing it in place; this aids in preventing the spread of contamination and reduces the possibility of the contamination becoming airborne, reducing workforce exposure and facilitating future deactivation and decommissioning (D&D) activities. \"Strippable coating\" products are loosely adhered paint-like films and are used for their decontamination abilities. They are applied to surfaces with loose/transferable radioactive contamination and then, once dried, are peeled off, which removes the loose/transferable contamination along with the product. The residual radioactive contamination on the surface is significantly reduced once the strippable coating is removed. Modern strippable coatings show high decontamination efficiency and can rival traditional mechanical and chemical decontamination methods. \"Decontamination gels\" work in much the same way as other strippable coatings. The results obtained through the use of contamination control products is variable and depends on the type of substrate, the selected contamination control product, the contaminants, and the environmental conditions (e.g., temperature, humidity, etc.).\n\nSome of the largest areas committed to be decontaminated are in the Fukushima Prefecture, Japan. The national government is under pressure to clean up radioactivity due to the Fukushima nuclear accident of March 2011 from as much land as possible so that some of the 110,000 displaced people can return. Stripping out the key radioisotope threatening health (caesium-137) from low level waste could also dramatically decrease the volume of waste requiring special disposal. A goal is to find techniques that might be able to strip out 80 to 95% of the caesium from contaminated soil and other materials, efficiently and without destroying the organic content in the soil. One being investigated is termed hydrothermal blasting. The caesium is broken away from soil particles and then precipitated with ferric ferricyanide (Prussian blue). It would be the only component of the waste requiring special burial sites. The aim is to get annual exposure from the contaminated environment down to one millisievert (mSv) above background. The most contaminated area where radiation doses are greater than 50 mSv/year must remain off limits, but some areas that are currently less than 5 mSv/year may be decontaminated allowing 22,000 residents to return.\n\nTo help with protection of people living in geographical areas which have been radioactively contaminated the International Commission on Radiological Protection has published a guide: \"Publication 111 - Application of the Commission’s Recommendations to the Protection of People Living in Long-term Contaminated Areas after a Nuclear Accident or a Radiation Emergency\".\n\nThe hazards to people and the environment from radioactive contamination depend on the nature of the radioactive contaminant, the level of contamination, and the extent of the spread of contamination. Low levels of radioactive contamination pose little risk, but can still be detected by radiation instrumentation. If a survey or map is made of a contaminated area, random sampling locations may be labeled with their activity in becquerels or curies on contact. Low levels may be reported in counts per minute using a scintillation counter.\n\nIn the case of low-level contamination by isotopes with a short half-life, the best course of action may be to simply allow the material to naturally decay. Longer-lived isotopes should be cleaned up and properly disposed of, because even a very low level of radiation can be life-threatening when in long exposure to it.\n\nFacilities and physical locations that are deemed to be contaminated may be cordoned off by a health physicist and labeled \"Contaminated area.\" Persons coming near such an area would typically require anti-contamination clothing (\"anti-Cs\").\n\nHigh levels of contamination may pose major risks to people and the environment. People can be exposed to potentially lethal radiation levels, both externally and internally, from the spread of contamination following an accident (or a deliberate initiation) involving large quantities of radioactive material. The biological effects of external exposure to radioactive contamination are generally the same as those from an external radiation source not involving radioactive materials, such as x-ray machines, and are dependent on the absorbed dose.\n\nWhen radioactive contamination is being measured or mapped \"in situ\", any location that appears to be a point source of radiation is likely to be heavily contaminated. A highly contaminated location is colloquially referred to as a \"hot spot.\" On a map of a contaminated place, hot spots may be labeled with their \"on contact\" dose rate in mSv/h. In a contaminated facility, hot spots may be marked with a sign, shielded with bags of lead shot, or cordoned off with warning tape containing the radioactive trefoil symbol.\n\nThe hazard from contamination is the emission of ionising radiation. The principal radiations which will be encountered are alpha, beta and gamma, but these have quite different characteristics. They have widely differing penetrating powers and radiation effect, and the accompanying diagram shows the penetration of these radiations in simple terms. For an understanding of the different ionising effects of these radiations and the weighting factors applied, see the article on absorbed dose.\n\nRadiation monitoring involves the measurement of radiation dose or radionuclide contamination for reasons related to the assessment or control of exposure to radiation or radioactive substances, and the interpretation of the results. The methodological and technical details of the design and operation of environmental radiation monitoring programmes and systems for different radionuclides, environmental media and types of facility are given in IAEA Safety Standards Series No. RS–G-1.8 and in IAEA Safety Reports Series No. 64.\n\nRadioactive contamination by definition emits ionizing radiation, which can irradiate the human body from an external or internal origin.\n\nThis is due to radiation from contamination located outside the human body. The source can be in the vicinity of the body or can be on the skin surface. The level of health risk is dependent on duration and the type and strength of irradiation. Penetrating radiation such as gamma rays, X-rays, neutrons or beta particles pose the greatest risk from an external source. Low penetrating radiation such as alpha particles have a low external risk due to the shielding effect of the top layers of skin. See the article on sievert for more information on how this is calculated.\n\nRadioactive contamination can be ingested into the human body if it is airborne or is taken in as contamination of food or drink, and will irradiate the body internally. The art and science of assessing internally generated radiation dose is Internal dosimetry.\n\nThe biological effects of ingested radionuclides depend greatly on the activity, the biodistribution, and the removal rates of the radionuclide, which in turn depends on its chemical form, the particle size, and route of entry. Effects may also depend on the chemical toxicity of the deposited material, independent of its radioactivity. Some radionuclides may be generally distributed throughout the body and rapidly removed, as is the case with tritiated water.\n\nSome organs concentrate certain elements and hence radionuclide variants of those elements. This action may lead to much lower removal rates. For instance, the thyroid gland takes up a large percentage of any iodine that enters the body. Large quantities of inhaled or ingested radioactive iodine may impair or destroy the thyroid, while other tissues are affected to a lesser extent. Radioactive iodine-131 is a common fission product; it was a major component of the radioactivity released from the Chernobyl disaster, leading to nine fatal cases of pediatric thyroid cancer and hypothyroidism. On the other hand, radioactive iodine is used in the diagnosis and treatment of many diseases of the thyroid precisely because of the thyroid's selective uptake of iodine.\n\nThe radiation risk proposed by the International Commission on Radiological Protection (ICRP) predicts that an effective dose of one sievert (100 rem) carries a 5.5% chance of developing cancer. Such a risk is the sum of both internal and external radiation dose.\n\nThe ICRP states \"Radionuclides incorporated in the human body irradiate the tissues over time periods determined by their physical half-life and their biological retention within the body. Thus they may give rise to doses to body tissues for many months or years after the intake. The need to regulate exposures to radionuclides and the accumulation of radiation dose over extended periods of time has led to the definition of committed dose quantities\".\nThe ICRP further states \"For internal exposure, committed effective doses are generally determined from an assessment of the intakes of radionuclides from bioassay measurements or other quantities (e.g., activity retained in the body or in daily excreta). The radiation dose is determined from the intake using recommended dose coefficients\".\n\nThe ICRP defines two dose quantities for individual committed dose:\n\nCommitted equivalent dose, H (\"t\") is the time integral of the equivalent dose rate in a particular tissue or organ that will be received by an individual following intake of radioactive material into the body by a Reference Person, where \"t\" is the integration time in years.\nThis refers specifically to the dose in a specific tissue or organ, in a similar way to external equivalent dose.\n\nCommitted effective dose, E(\"t\") is the sum of the products of the committed organ or tissue equivalent doses and the appropriate tissue weighting factors \"W\", where \"t\" is the integration time in years following the intake. The commitment period is taken to be 50 years for adults, and to age 70 years for children. This refers specifically to the dose to the whole body, in a similar way to external effective dose.\n\nA 2015 report in \"Lancet\" explained that serious impacts of nuclear accidents were often not directly attributable to radiation exposure, but rather social and psychological effects. The consequences of low-level radiation are often more psychological than radiological. Because damage from very-low-level radiation cannot be detected, people exposed to it are left in anguished uncertainty about what will happen to them. Many believe they have been fundamentally contaminated for life and may refuse to have children for fear of birth defects. They may be shunned by others in their community who fear a sort of mysterious contagion.\n\nForced evacuation from a radiological or nuclear accident may lead to social isolation, anxiety, depression, psychosomatic medical problems, reckless behavior, even suicide. Such was the outcome of the 1986 Chernobyl nuclear disaster in Ukraine. A comprehensive 2005 study concluded that \"the mental health impact of Chernobyl is the largest public health problem unleashed by the accident to date\". Frank N. von Hippel, a U.S. scientist, commented on 2011 Fukushima nuclear disaster, saying that \"fear of ionizing radiation could have long-term psychological effects on a large portion of the population in the contaminated areas\". Evacuation and long-term displacement of affected populations create problems for many people, especially the elderly and hospital patients.\n\nSuch great psychological danger does not accompany other materials that put people at risk of cancer and other deadly illness. Visceral fear is not widely aroused by, for example, the daily emissions from coal burning, although, as a National Academy of Sciences study found, this causes 10,000 premature deaths a year in the US population of 317,413,000. Medical errors leading to death in U.S. hospitals are estimated to be between 44,000 and 98,000. It is \"only nuclear radiation that bears a huge psychological burden — for it carries a unique historical legacy\".\n\n\n\n"}
{"id": "21802123", "url": "https://en.wikipedia.org/wiki?curid=21802123", "title": "Sabloal Valea Dacilor Wind Farm", "text": "Sabloal Valea Dacilor Wind Farm\n\nThe Sabloal Valea Dacilor Wind Farm is an under construction wind power project in Constanţa County, Romania. It will consist of 49 individual wind turbines with a nominal output of around 3 MW which will deliver up to 147 MW of power, enough to power over 96,285 homes, with a capital investment required of approximately US$200 million.\n"}
{"id": "23615251", "url": "https://en.wikipedia.org/wiki?curid=23615251", "title": "Salvage sales", "text": "Salvage sales\n\nIn the United States, salvage sales are timber sales from national forests primarily to remove dead, infested, damaged, or down trees and associated trees for stand improvement. They are controversial partly because there are no standards for the number or proportion of trees that must be dead, infested, damaged, or down and partly because the U.S. Forest Service may retain the revenues to prepare and administer future salvage sales.\n"}
{"id": "1924150", "url": "https://en.wikipedia.org/wiki?curid=1924150", "title": "Slugcatcher", "text": "Slugcatcher\n\nSlug Catcher is the name of a unit in the gas refinery or petroleum industry in which slugs at the outlet of pipelines are collected or caught. A slug is a large quantity of gas or liquid that exits in the pipeline.\n\nPipelines that transport both gas and liquids together, known as two-phase flow, can operate in a flow regime known as slugging flow or slug flow. Under the influence of gravity, liquids will tend to settle on the bottom of the pipeline, while the gases occupy the top section of the pipeline. Under certain operating conditions gas and liquid are not evenly distributed throughout the pipeline, but travel as large plugs with mostly liquids or mostly gases through the pipeline. These large plugs are called slugs.\n\nSlugs exiting the pipeline can overload the gas/liquid handling capacity of the plant at the pipeline outlet, as they are often produced at a much larger rate than the equipment is designed for.\n\nSlugs can be generated by different mechanisms in a pipeline:\n\n\nSlugs formed by terrain slugging, hydrodynamic slugging or riser-based slugging are periodical in nature. Whether a slug is able to reach the outlet of the pipeline depends on the rate at which liquids are added to the slug at the front (i.e. in the direction of flow) and the rate at which liquids leave the slug at the back. Some slugs will grow as they travel the pipeline, while others are damped and disappear before reaching the outlet of the pipeline.\n\nA slug catcher is a vessel with sufficient buffer volume to store the largest slugs expected from the upstream system. The slug catcher is located between the outlet of the pipeline and the processing equipment. The buffered liquids can be drained to the processing equipment at a much slower rate to prevent overloading the system. As slugs are a periodical phenomenon, the slug catcher should be emptied before the next slug arrives\n\nSlug catchers can be used continuously or on-demand. A slug catcher permanently connected to the pipeline will buffer all production, including the slugs, before it is sent to the gas and liquid handling facilities. This is used for difficult to predict slugging behaviour found in terrain slugging, hydrodynamic slugging or riser-based slugging. Alternatively, the slug catcher can be bypassed in normal operation and be brought online when a slug is expected, usually during pigging operations. An advantage of this set-up is that inspection and maintenance on the slug catcher can be done without interrupting the normal operation.\n\nSlug catchers are designed in different forms,\n\nA basic slug catcher design contains the buffer volume for gas and liquid. A control system is used for controlled outflow of gas and liquid to the downstream processing facilities. The inlet section is designed to promote the separation of gas and liquids.\n\nhttp://www.glossary.oilfield.slb.com/en/Terms/s/slug_flow.aspx\n"}
{"id": "12297965", "url": "https://en.wikipedia.org/wiki?curid=12297965", "title": "Solar Splash", "text": "Solar Splash\n\nSolar Splash is an intercollegiate solar/electric boat competition dedicated to showing the feasibility of solar energy.\n\nAn annual collegiate solar boating competition started in 1994. The 2007 contest was hosted by the City of Fayetteville, Arkansas, and the University of Arkansas College of Engineering. It took place June 13–17.\n\n"}
{"id": "14846664", "url": "https://en.wikipedia.org/wiki?curid=14846664", "title": "Solar car racing", "text": "Solar car racing\n\nSolar car racing refers to competitive races of electric vehicles which are powered by solar energy obtained from solar panels on the surface of the car (solar cars). The first solar car race was the Tour de Sol in 1985 which led to several similar races in Europe, US and Australia. Such challenges are often entered by universities to develop their students' engineering and technological skills, but many business corporations have entered competitions in the past. A small number of high school teams participate in solar car races designed exclusively for high school students.\n\nThe two most notable solar car distance (overland) races are the World Solar Challenge and the American Solar Challenge. They are contested by a variety of university and corporate teams. Corporate teams participate in the races to give their design teams experience of working with both alternative energy sources and advanced materials. University teams participate in order to give their students experience in designing high technology cars and working with environmental and advanced materials technology. These races are often sponsored by government or educational agencies, and businesses such as Toyota keen to promote renewable energy sources.\n\nThe cars require intensive support teams similar in size to professional motor racing teams. This is especially the case with the World Solar Challenge where sections of the race run through very remote country. The solar car will travel escorted by a small caravan of support cars. In a long distance race each solar car will be preceded by a lead car that can identify problems or obstacles ahead of the race car. Behind the solar car there will be a mission control vehicle from which the race pace is controlled. Here tactical decisions are made based on information from the solar car and environmental information about the weather and terrain. Behind the mission control there might be one or more other vehicles carrying replacement drivers and maintenance support as well as supplies and camping equipment for the entire team.\n\nThis race features a field of competitors from around the world who race to cross the Australian continent. The 30th Anniversary race of the World Solar Challenge will be held in October 2017. Major regulation changes were released in June 2006 for this race to increase safety, to build a new generation of solar car, which with little modification could be the basis for a practical proposition for sustainable transport and intended to slow down cars in the main event, which could easily exceed the speed limit (110 km/h) in previous years.\n\nIn 2013 the organisers of the event introduced the Cruiser Class to the World Solar Challenge, designed to encourage contestants to design a \"practical\" solar powered vehicle. This race requires that vehicles have four wheels and upright seating for passengers, and is judged on a number of factors including time, payload, passenger miles, and external energy use. The Dutch TU Eindhoven solar racing team were the inaugural Cruiser Class winner with their vehicle \"Stella\".\n\nThe American Solar Challenge, previously known as the 'North American Solar Challenge' and 'Sunrayce', features mostly collegiate teams racing in timed intervals in the United States and Canada. The annual Formula Sun Grand Prix track race is used as a qualifier for ASC.\n\nThe American Solar Challenge was sponsored in part by several small sponsors. However, funding was cut near the end of 2005, and the NASC 2007 was cancelled. The North American solar racing community worked to find a solution, bringing in Toyota as a primary sponsor for a 2008 race. Toyota has since dropped the sponsorship. The last North American Solar Challenge was run 2016, from Brecksville, OH to Hot Springs, SD. The race was won by the University of Michigan. Michigan has won the race the last 6 times it has been held.\n\nThe Dell-Winston School Solar Car Challenge is an annual solar-powered car race for high school students. The event attracts teams from around the world, but mostly from American high schools. The race was first held in 1995. Each event is the end product of a two-year education cycle launched by the Winston Solar Car Team. On odd-numbered years, the race is a road course that starts at the Dell Diamond in Round Rock, Texas; the end of the course varies from year to year. On even-numbered years, the race is a track race around the Texas Motor Speedway. Dell has sponsored the event since 2002.[1]\n\nThe South African Solar Challenge is a biennial, two-week solar-powered car race through the length and breadth of South Africa. The first challenge in 2008 proved that this event can attract the interest of the public, and that it has the necessary international backing from the FIA. Late in September, all entrants will take off from Pretoria and make their way to Cape Town, then drive along the coast to Durban, before climbing the escarpment on their way back to the finish line in Pretoria 11 days later. The event has (in both 2008 and 2010) been endorsed by International Solarcar Federation (ISF), Fédération Internationale de l'Automobile (FIA), World Wildlife Fund (WWF) making it the first Solar Race to receive endorsement from these 3 organizations. The last race took place in 2016. Sasol confirmed their support of the South Africa Solar Challenge, by taking naming rights to the event, so that for the duration of their sponsorship, the event was known as the Sasol Solar Challenge, South Africa.\n\nThe Carrera Solar Atacama is the first solar-powered car race of its kind in Latin America; the race covers from Santiago to Arica in the north of Chile. The race's founder, La Ruta Solar, claims it is the most extreme of the vehicular races due to the high levels of solar radiation, up to 8.5 kWh/m/day, encountered while traversing the Atacama Desert, as well as challenging participating teams to climb above sea level. The race, which made its debut in 2009 with just a handful of local teams, is set for its fifth version in late October 2018, welcoming international teams in all categories and is for the first time in both English and Spanish.\n\n\nSolar drag races are another form of solar racing. Unlike long distance solar races, solar dragsters do not use any batteries or pre-charged energy storage devices. Racers go head-to-head over a straight quarter kilometer distance. Currently, a solar drag race is held each year on the Saturday closest to the summer solstice in Wenatchee, Washington, USA. The world record for this event is 29.5 seconds set by the South Whidbey High School team on June 23, 2007.\n\nThe FIA recognise a land speed record for vehicles powered only by solar panels. The current record was set by the Raedthuys Solar Team, of the University of Twente with their car SolUTra. The record of 37.757 km/h was set in 2005. The record takes place over a flying 1000m run, and is the average speed of 2 runs in opposite directions.\n\nIn July, 2014, a group of Australian students from the UNSW Sunswift solar racing team at the University of New South Wales broke a world record in their solar car, for the fastest electric car weighing less than and capable of travelling on a single battery charge. This particular record was overseen by the Confederation of Australian Motorsport on behalf of the FIA and is not exclusive to solar powered cars but to any electric car, and so during the attempt the solar panels were disconnected from the electrical systems. The previous record of - which had been set in 1988 - was broken by the team with an average speed of over the distance.\n\nGuinness World Records recognize a land speed record for vehicles powered only by solar panels. This record is currently held by the University of New South Wales with the car Sunswift IV. Its battery was removed so the vehicle was powered only by its solar panels. The record of was set on 7 January 2011 at the naval air base in Nowra, breaking the record previously held by the General Motors car Sunraycer of . The record takes place over a flying stretch, and is the average of two runs in opposite directions.\n\nThe Perth to Sydney Transcontinental record has held a certain allure in Solar Car Racing. Hans Tholstrup (the founder of the World Solar Challenge) first completed this journey in \"The Quiet Achiever\" in under 20 days in 1983. This vehicle is in the collection of the National Museum of Australia in Canberra.\n\nThe record was beaten by Dick Smith and the Aurora Solar Vehicle Association racing in the \"Aurora Q1\"\n\nThe current record was set in 2007 by the UNSW Solar Racing Team with their car \"Sunswift III mk2\"\n\nSolar cars combine technology used in the aerospace, bicycle, alternative energy and automotive industries. Unlike most race cars, solar cars are designed with severe energy constraints imposed by the race regulations. These rules limit the energy used to only that collected from solar radiation, albeit starting with a fully charged battery pack. Some vehicle classes also allow human power input. As a result, optimizing the design to account for aerodynamic drag, vehicle weight, rolling resistance and electrical efficiency are paramount.\n\nA usual design for today's successful vehicles is a small canopy in the middle of a curved wing-like array, entirely covered in cells, with 3 wheels. Before, the cockroach style with a smooth nose fairing into the panel was more successful. At lower speeds, with less powerful arrays, other configurations are viable and easier to construct, e.g. covering available surfaces of existing electric vehicles with solar cells or fastening solar canopies above them.\n\nThe electrical system controls all of the power entering and leaving the system. The battery pack stores surplus solar energy produced when the vehicle is stationary or travelling slowly or downhill. Solar cars use a range of batteries including lead-acid batteries, nickel-metal hydride batteries (NiMH), nickel-cadmium batteries (NiCd), lithium ion batteries and lithium polymer batteries.\n\nPower electronics may be used to optimize the electrical system. The maximum power tracker adjusts the operating point of the solar array to that voltage which produces the most power for the given conditions, e.g. temperature. The battery manager protects the batteries from overcharging. The motor controller controls the desired motor power. Many controllers allow regenerative braking, i.e. power is fed back into the battery during deceleration.\n\nSome solar cars have complex data acquisition systems that monitor the whole electrical system, while basic cars show battery voltage and motor current. In order to judge the range available with varying solar production and motive consumption, an ampere-hour meter multiplies battery current and rate, thus providing the remaining vehicle range at each moment in the given conditions.\n\nA wide variety of motor types have been used. The most efficient motors exceed 98% efficiency. These are brushless three-\"phase\" DC, electronically commutated, wheel motors, with a Halbach array configuration for the neodymium-iron-boron magnets, and Litz wire for the windings. Cheaper alternatives are asynchronous AC or brushed DC motors.\n\nThe mechanical systems are designed to keep friction and weight to a minimum while maintaining strength and stiffness. Designers normally use aluminium, titanium and composites to provide a structure that meets strength and stiffness requirements whilst being fairly light. Steel is used for some suspension parts on many cars.\n\nSolar cars usually have three wheels, but some have four. Three-wheelers usually have two front wheels and one rear wheel: the front wheels steer and the rear wheel follows. Four-wheel vehicles are set up like normal cars or similarly to three-wheeled vehicles with the two rear wheels close together.\n\nSolar cars have a wide range of suspensions because of varying bodies and chassis. The most common front suspension is the double wishbone suspension. The rear suspension is often a trailing-arm suspension as found in motor cycles.\n\nSolar cars are required to meet rigorous standards for brakes. Disc brakes are the most commonly used due to their good braking ability and ability to adjust. Mechanical and hydraulic brakes are both widely used. The brake pads or shoes are typically designed to retract to minimize brake drag, on leading cars.\n\nSteering systems for solar cars also vary. The major design factors for steering systems are efficiency, reliability and precision alignment to minimize tire wear and power loss. The popularity of solar car racing has led to some tire manufacturers designing tires for solar vehicles. This has increased overall safety and performance.\n\nAll the top teams now use wheel motors, eliminating belt or chain drives.\n\nTesting is essential to demonstrating vehicle reliability prior to a race. It is easy to spend a hundred thousand dollars to gain a two-hour advantage, and equally easy to lose two hours due to reliability issues.\n\nThe solar array consists of hundreds (or thousands) of photovoltaic solar cells converting sunlight into electricity. Cars can use a variety of solar cell technologies; most often polycrystalline silicon, monocrystalline silicon, or gallium arsenide. The cells are wired together into strings while strings are often wired together to form a panel. Panels normally have voltages close to the nominal battery voltage. The main aim is to get as much cell area in as small a space as possible. Designers encapsulate the cells to protect them from the weather and breakage.\n\nDesigning a solar array is more than just stringing a bunch of cells together. A solar array acts like many very small batteries all hooked together in series. The total voltage produced is the sum of all cell voltages. The problem is that if a single cell is in shadow it acts like a diode, blocking the current for the entire string of cells. To design against this, array designers use by-pass diodes in parallel with smaller segments of the string of cells, allowing current around the non-functioning cell(s). Another consideration is that the battery itself can force current backwards through the array unless there are blocking diodes put at the end of each panel.\n\nThe power produced by the solar array depends on the weather conditions, the position of the sun and the capacity of the array. At noon on a bright day, a good array can produce over 2 kilowatts (2.6 hp). A 6 m array of 20% cells will produce roughly 6 kW·h (22 kJ) of energy during a typical day on the WSC.\n\nSome cars have employed free-standing or integrated sails to harness wind energy. Races including the WSC and ASC, consider wind energy to be solar energy, so their race regulations allow this practice.\n\nAerodynamic drag is the main source of losses on a solar race car. The aerodynamic drag of a vehicle is the product of the frontal area and its \"C\". For most solar cars the frontal area is 0.75 to 1.3 m. While \"C\" as low as 0.10 have been reported, 0.13 is more typical. This needs a great deal of attention to detail.\n\nThe vehicle's mass is also a significant factor. A light vehicle generates less rolling resistance and will need smaller lighter brakes and other suspension components. This is the virtuous circle when designing lightweight vehicles.\n\nRolling resistance can be minimized by using the right tires, inflated to the right pressure, correctly aligned, and by minimizing the weight of the vehicle.\n\nThe design of a solar car is governed by the following work equation:\n\nwhich can be usefully simplified to the performance equation\n\nfor long distance races, and values seen in practice.\n\nBriefly, the left hand side represents the energy input into the car (batteries and power from the sun) and the right hand side is the energy needed to drive the car along the race route (overcoming rolling resistance, aerodynamic drag, going uphill and accelerating). Everything in this equation can be estimated except \"v\". The parameters include:\n\nNote 1 For the WSC the average panel power can be approximated as (7/9)×nominal power.\n\nSolving the long form of the equation for velocity results in a large equation (approximately 100 terms). Using the power equation as the arbiter, vehicle designers can compare various car designs and evaluate the comparative performance over a given route. Combined with CAE and systems modeling, the power equation can be a useful tool in solar car design.\n\nThe directional orientation of a solar car race route affects the apparent position of the sun in the sky during a race day, which in turn affects the energy input to the vehicle.\n\nThis is significant to designers, who seek to maximize energy input to a panel of solar cells (often called an \"array\" of cells) by designing the array to point directly toward the sun for as long as possible during the race day. Thus, a south-north race car designer might increase the car's total energy input by using solar cells on the sides of the vehicle where the sun will strike them (or by creating a convex array coaxial with the vehicle's movement). In contrast, an east-west race alignment might reduce the benefit from having cells on the side of the vehicle, and thus might encourage design of a flat array.\n\nBecause solar cars are often purpose-built, and because arrays do not usually move in relation to the rest of the vehicle (with notable exceptions), this race-route-driven, flat-panel versus convex design compromise is one of the most significant decisions that a solar car designer must make.\n\nFor example, the 1990 and 1993 Sunrayce USA events were won by vehicles with significantly convex arrays, corresponding to the south-north race alignments; by 1997, however, most cars in that event had flat arrays to match the change to an east-west route.\n\nOptimizing energy consumption is of prime importance in a solar car race. Therefore, it is useful to be able to continually monitor and optimize the vehicle's energy parameters. Given the variable conditions, most teams have race speed optimization programs that continuously update the team on how fast the vehicle should be traveling. Some teams employ telemetry that relays vehicle performance data to a following support vehicle, which can provide the vehicle's driver with an optimum strategy.\n\nThe race route itself will affect strategy, because the apparent position of the sun in the sky will vary depending various factors which are specific to the vehicle's orientation (see \"Race Route Considerations,\" above).\n\nIn addition, elevation changes over a race route can dramatically change the amount of power needed to travel the route. For example, the 2001 and 2003 North American Solar Challenge route crossed the Rocky Mountains (see graph at right).\n\nA successful solar car racing team will need to have access to reliable weather forecasts in order to predict the power input to the vehicle from the sun during each race day.\n\n\n"}
{"id": "38180478", "url": "https://en.wikipedia.org/wiki?curid=38180478", "title": "Solar particle event", "text": "Solar particle event\n\nA solar proton event (SPE), or \"proton storm\", occurs when particles (mostly protons) emitted by the Sun become accelerated either close to the Sun during a flare or in interplanetary space by CME shocks. The events can include other nuclei such as helium ions and HZE ions. These particles cause multiple effects. They can penetrate the Earth's magnetic field and cause ionization in the ionosphere. The effect is similar to auroral events, except that protons rather than electrons are involved. Energetic protons are a significant radiation hazard to spacecraft and astronauts.\n\nSolar protons normally have insufficient energy to penetrate the Earth's magnetic field. However, during unusually strong flares, protons can be produced with sufficient energies to reach the Earth's magnetosphere and ionosphere around the north pole and south pole.\n\nProtons are charged particles and are therefore influenced by magnetic fields. When the energetic protons leave the Sun, they preferentially follow (or are guided by) the Sun's powerful magnetic field. When solar protons enter the Earth's magnetosphere where the magnetic fields are stronger than solar magnetic fields, they are guided by the Earth's magnetic field into the polar regions where the majority of the Earth's magnetic field lines enter and exit.\n\nEnergetic protons that are guided into the polar regions collide with atmospheric constituents and release their energy through the process of ionization. The majority of the energy is extinguished in the extreme lower region of the ionosphere (around 50–80 km in altitude). This area is particularly important to ionospheric radio communications because this is the area where most of the absorption of radio signal energy occurs. The enhanced ionization produced by incoming energetic protons increases the absorption levels in the lower ionosphere and can have the effect of completely blocking all ionospheric radio communications through the polar regions. Such events are known as Polar Cap Absorption events (or PCAs). These events commence and last as long as the energy of incoming protons at approximately greater than 10 MeV (million electron volts) exceeds roughly 10 pfu (particle flux units or ) at geosynchronous satellite altitudes.\n\nThe more severe proton events can be associated with geomagnetic storms that can cause widespread disruption to electrical grids. However, proton events themselves are not responsible for producing anomalies in power grids, nor are they responsible for producing geomagnetic storms. Power grids are only sensitive to fluctuations in the Earth's magnetic field.\n\nExtremely intense solar proton flares capable of producing energetic protons with energies in excess of 100 MeV can increase neutron count rates at ground levels through secondary radiation effects. These rare events are known as Ground Level Enhancements(or GLEs). Some events produce large amounts of HZE ions, although their contribution to the total radiation is small compared to the level of protons.<ref name=\"NASA/TP-1999-209320\">Contribution of High Charge and Energy (HZE) Ions During Solar-Particle Event of September 29, 1989 Kim, Myung-Hee Y.; Wilson, John W.; Cucinotta, Francis A.; Simonsen, Lisa C.; Atwell, William; Badavi, Francis F.; Miller, Jack, NASA Johnson Space Center; Langley Research Center, May 1999.</ref>\n\nThere is no substantive scientific evidence to suggest that energetic proton events are harmful to human health at ground levels, particularly at latitudes where most of the Earth's population resides. The Earth's magnetic field is exceptionally good at preventing the radiative effects of energetic particles from reaching ground levels. High altitude commercial transpolar aircraft flights have measured increases in radiation during energetic proton events, but a warning system is in place that limits these effects by alerting pilots to lower their cruising altitudes. Aircraft flights away from the polar regions are far less likely to see an impact from solar proton events.\n\nSignificant proton radiation exposure can be experienced by astronauts who are outside of the protective shield of the Earth's magnetosphere, such as an astronaut in-transit to, or located on the Moon. However, the effects can be minimized if astronauts are in a low-Earth orbit and remain confined to the most heavily shielded regions of their spacecraft. Proton radiation levels in low earth orbit increase with orbital inclination. Therefore, the closer a spacecraft approaches the polar regions, the greater the exposure to energetic proton radiation will be.\n\nAstronauts have reported seeing flashes or streaks of light as energetic protons interact with their optic tissues. Similar flashes and streaks of light occur when energetic protons strike the sensitive optical electronics in spacecraft (such as star trackers and other cameras). The effect can be so pronounced that during extreme events, it is not possible to obtain quality images of the Sun or stars. This can cause spacecraft to lose their orientation, which is critical if ground controllers are to maintain control.\n\nEnergetic proton storms can also electrically charge spacecraft to levels that can damage electronic components. They can also cause electronic components to behave erratically. For example, solid state memory on spacecraft can be altered, which may cause data or software contamination and result in unexpected (phantom) spacecraft commands being executed. Energetic proton storms also destroy the efficiency of the solar panels that are designed to collect and convert sunlight to electricity. During years of exposure to energetic proton activity from the Sun, spacecraft can lose a substantial amount of electrical power that may require important instruments to be turned off.\n\n\n"}
{"id": "960519", "url": "https://en.wikipedia.org/wiki?curid=960519", "title": "Square Leg", "text": "Square Leg\n\nSquare Leg was a 1980 British government home defence Command Post and field exercise, which tested the Transition to War and Home Defence roles of the Ministry of Defence and British government. Part of the exercise involved a mock nuclear attack on Britain. It was assumed that 131 nuclear weapons would fall on Britain with a total yield of 205 megatons (69 ground burst; 62 air burst) with yields of 500 KT to 3 MT That was felt to be a reasonably realistic scenario, but the report stated that a total strike in excess of 1,000 megatons would be likely.\n\nMortality was estimated at 29 million (53 percent of the population); serious injuries at 7 million (12 percent); short-term survivors at 19 million (35 percent).\n\nSquare Leg was criticised for a number of reasons: the weapons used were exclusively in the high-yield megaton range, with an average of 1.5 megatons per bomb, but a realistic attack based on known Soviet capabilities would have seen mixed weapons yields, including many missile-based warheads in the low hundred kiloton range. Also, no targets in Inner London are attacked (for example, Whitehall, the centre of British government); towns such as Eastbourne are hit for no obvious reason.\n\nOperation Square Leg was one of the exercises used to estimate the destructiveness of a Soviet nuclear attack in the 1984 BBC production \"Threads\".\n\n\n"}
{"id": "48221698", "url": "https://en.wikipedia.org/wiki?curid=48221698", "title": "Stefan Oschmann", "text": "Stefan Oschmann\n\nStefan Oschmann (born 25 July 1957) is a German businessman, who has served as the chief executive officer (CEO) and chairman of the executive board of the Germany-based Merck Group since April 2016. Before joining the Merck Group, Oschmann worked for Merck & Co., in a range of senior executive positions in Europe and the developing world, from 1989 to 2011. \n\nOschmann was born on 25 July 1957 in Würzburg, Germany. He holds a doctorate in veterinary medicine from the Ludwig Maximilian University of Munich.\n\nOschmann started his career at an agency of the International Atomic Energy Agency. In 1987 he started working for the German Animal Health Federation (\"Bundesverband für Tiergesundheit\"), the trade association for German manufacturers of veterinary drugs and feed additives.\n\nFrom 1989 to 2011, Oschmann worked for Merck Sharp & Dohme (MSD), the European operations of Merck & Co. He was managing director of MSD Austria from 1994, vice president for Central and Eastern Europe from 1998, vice president of MSD Europe and managing director of MSD Germany from 1999, senior vice president for worldwide human health marketing from 2005, president for Europe, the Middle East, Africa, and Canada from 2006 and finally MSD's president of emerging markets from 2009 to 2011.\n\nOschmann joined the Merck Group, the original parent company of MSD which is now a separate company, as a member of the executive board in 2011, and was responsible for the healthcare business sector until the end of 2014. He became vice chairman of the executive board and deputy CEO of the Merck Group in 2015. Oschmann became the chairman of the executive board and CEO of the Merck Group in April 2016, succeeding Karl-Ludwig Kley.\n\n\nOschmann is married and has two children.\n"}
{"id": "70671", "url": "https://en.wikipedia.org/wiki?curid=70671", "title": "Stress–energy tensor", "text": "Stress–energy tensor\n\nThe stress–energy tensor, sometimes stress–energy–momentum tensor or energy–momentum tensor, is a tensor quantity in physics that describes the density and flux of energy and momentum in spacetime, generalizing the stress tensor of Newtonian physics. It is an attribute of matter, radiation, and non-gravitational force fields. The stress–energy tensor is the source of the gravitational field in the Einstein field equations of general relativity, just as mass density is the source of such a field in Newtonian gravity.\n\nThe stress–energy tensor involves the use of superscripted variables (\"not\" exponents; see tensor index notation and Einstein summation notation). If Cartesian coordinates in SI units are used, then the components of the position four-vector are given by: , , , and , where \"t\" is time in seconds, and \"x\", \"y\", and \"z\" are distances in meters.\n\nThe stress–energy tensor is defined as the tensor \"T\" of order two that gives the flux of the \"α\"th component of the momentum vector across a surface with constant \"x\" coordinate. In the theory of relativity, this momentum vector is taken as the four-momentum. In general relativity, the stress–energy tensor is symmetric,\n\nIn some alternative theories like Einstein–Cartan theory, the stress–energy tensor may not be perfectly symmetric because of a nonzero spin tensor, which geometrically corresponds to a nonzero torsion tensor.\n\nBecause the stress–energy tensor is of order two, its components can be displayed in 4 × 4 matrix form:\n\nIn the following, \"i\" and \"k\" range from 1 through 3.\n\nThe time–time component is the density of relativistic mass, i.e. the energy density divided by the speed of light squared. Its components have a direct physical interpretation. In the case of a perfect fluid this component is\n\nwhere formula_4 is the relativistic mass per unit volume, and for an electromagnetic field in otherwise empty space this component is\n\nwhere \"E\" and \"B\" are the electric and magnetic fields, respectively.\n\nThe flux of relativistic mass across the \"x\" surface is equivalent to the density of the \"i\"th component of linear momentum,\n\nThe components\nrepresent flux of \"i\"th component of linear momentum across the \"x\" surface. In particular,\n(not summed) represents normal stress, which is called pressure when it is independent of direction. The remaining components\nrepresent shear stress (compare with the stress tensor).\n\nIn solid state physics and fluid mechanics, the stress tensor is defined to be the spatial components of the stress–energy tensor in the proper frame of reference. In other words, the stress energy tensor in engineering \"differs\" from the stress–energy tensor here by a momentum convective term.\n\nIn most of this article we work with the contravariant form, \"T\" of the stress–energy tensor. However, it is often necessary to work with the covariant form,\n\nor the mixed form,\n\nor as a mixed tensor density\n\nIn this article we use the spacelike sign convention (−+++) for the metric signature.\n\nThe stress–energy tensor is the conserved Noether current associated with spacetime translations.\n\nThe divergence of the non-gravitational stress–energy is zero. In other words, non-gravitational energy and momentum are conserved,\nWhen gravity is negligible and using a Cartesian coordinate system for spacetime, this may be expressed in terms of partial derivatives as \n\nThe integral form of this is\n\nwhere \"N\" is any compact four-dimensional region of spacetime; formula_16 is its boundary, a three-dimensional hypersurface; and formula_17 is an element of the boundary regarded as the outward pointing normal.\n\nIn flat spacetime and using Cartesian coordinates, if one combines this with the symmetry of the stress–energy tensor, one can show that angular momentum is also conserved:\n\nWhen gravity is non-negligible or when using arbitrary coordinate systems, the divergence of the stress–energy still vanishes. But in this case, a coordinate free definition of the divergence is used which incorporates the covariant derivative\n\nwhere formula_20 is the Christoffel symbol which is the gravitational force field.\n\nConsequently, if formula_21 is any Killing vector field, then the conservation law associated with the symmetry generated by the Killing vector field may be expressed as\n\nThe integral form of this is\n\nIn general relativity, the symmetric stress–energy tensor acts as the source of spacetime curvature, and is the current density associated with gauge transformations of gravity which are general curvilinear coordinate transformations. (If there is torsion, then the tensor is no longer symmetric. This corresponds to the case with a nonzero spin tensor in Einstein–Cartan gravity theory.)\n\nIn general relativity, the partial derivatives used in special relativity are replaced by covariant derivatives. What this means is that the continuity equation no longer implies that the non-gravitational energy and momentum expressed by the tensor are absolutely conserved, i.e. the gravitational field can do work on matter and vice versa. In the classical limit of Newtonian gravity, this has a simple interpretation: energy is being exchanged with gravitational potential energy, which is not included in the tensor, and momentum is being transferred through the field to other bodies. In general relativity the Landau–Lifshitz pseudotensor is a unique way to define the \"gravitational\" field energy and momentum densities. Any such stress–energy pseudotensor can be made to vanish locally by a coordinate transformation.\n\nIn curved spacetime, the spacelike integral now depends on the spacelike slice, in general. There is in fact no way to define a global energy–momentum vector in a general curved spacetime.\n\nIn general relativity, the stress tensor is studied in the context of the Einstein field equations which are often written as\n\nwhere formula_25 is the Ricci tensor, formula_26 is the Ricci scalar (the tensor contraction of the Ricci tensor), formula_27 is the metric tensor, is the cosmological constant (negligible at the scale of a galaxy or smaller), and formula_28 is the universal gravitational constant.\n\nIn special relativity, the stress–energy of a non-interacting particle with mass \"m\" and trajectory formula_29 is:\n\nwhere formula_31 is the velocity vector (which should not be confused with four-velocity, since it is missing a formula_32)\nδ is the Dirac delta function and formula_34 is the energy of the particle.\n\nFor a perfect fluid in thermodynamic equilibrium, the stress–energy tensor takes on a particularly simple form\n\nwhere formula_4 is the mass–energy density (kilograms per cubic meter), formula_37 is the hydrostatic pressure (pascals), formula_38 is the fluid's four velocity, and formula_39 is the reciprocal of the metric tensor. Therefore, the trace is given by\n\nThe four velocity satisfies\n\nIn an inertial frame of reference comoving with the fluid, better known as the fluid's proper frame of reference, the four velocity is \n\nthe reciprocal of the metric tensor is simply\n\nand the stress–energy tensor is a diagonal matrix\n\nThe Hilbert stress–energy tensor of a source-free electromagnetic field is\n\nwhere formula_46 is the electromagnetic field tensor.\n\nThe stress–energy tensor for a complex scalar field formula_47 which satisfies the Klein–Gordon equation is\nand when the metric is flat (Minkowski) its components work out to be:\n\nThere are a number of inequivalent definitions of non-gravitational stress–energy:\n\nThe Hilbert stress–energy tensor is defined as the functional derivative\n\nwhere formula_51 is the nongravitational part of the Lagrangian density of the action. This is symmetric and gauge-invariant. See Einstein–Hilbert action for more information.\n\nNoether's theorem implies that there is a conserved current associated with translations through space and time. This is called the canonical stress–energy tensor. Generally, this is not symmetric and if we have some gauge theory, it may not be gauge invariant because space-dependent gauge transformations do not commute with spatial translations.\n\nIn general relativity, the translations are with respect to the coordinate system and as such, do not transform covariantly. See the section below on the gravitational stress–energy pseudo-tensor.\n\nIn the presence of spin or other intrinsic angular momentum, the canonical Noether stress energy tensor fails to be symmetric. The Belinfante–Rosenfeld stress energy tensor is constructed from the canonical stress–energy tensor and the spin current in such a way as to be symmetric and still conserved. In general relativity, this modified tensor agrees with the Hilbert stress–energy tensor.\n\nBy the equivalence principle gravitational stress–energy will always vanish locally at any chosen point in some chosen frame, therefore gravitational stress–energy cannot be expressed as a non-zero tensor; instead we have to use a pseudotensor.\n\nIn general relativity, there are many possible distinct definitions of the gravitational stress–energy–momentum pseudotensor. These include the Einstein pseudotensor and the Landau–Lifshitz pseudotensor. The Landau–Lifshitz pseudotensor can be reduced to zero at any event in spacetime by choosing an appropriate coordinate system.\n\n\n"}
{"id": "537975", "url": "https://en.wikipedia.org/wiki?curid=537975", "title": "Thermoluminescence", "text": "Thermoluminescence\n\nThermoluminescence is a form of luminescence that is exhibited by certain crystalline materials, such as some minerals, when previously absorbed energy from electromagnetic radiation or other ionizing radiation is re-emitted as light upon heating of the material. The phenomenon is distinct from that of black-body radiation.\n\nHigh energy radiation creates electronic excited states in crystalline materials. In some materials, these states are \"trapped\", or \"arrested\", for extended periods of time by localized defects, or imperfections, in the lattice interrupting the normal intermolecular or inter-atomic interactions in the crystal lattice. Quantum-mechanically, these states are stationary states which have no formal time dependence; however, they are not stable energetically. Heating the material enables the trapped states to interact with phonons, i.e. lattice vibrations, to rapidly decay into lower-energy states, causing the emission of photons in the process.\n\nThe amount of luminescence is proportional to the original dose of radiation received. In thermoluminescence dating, this can be used to date buried objects that have been heated in the past, since the ionizing dose received from radioactive elements in the soil or from cosmic rays is proportional to age. This phenomenon has been applied in the thermoluminescent dosimeter, a device to measure the radiation dose received by a chip of suitable material that is carried by a person or placed with an object.\n\nThermoluminescence is a common geochronology tool for dating pottery or other fired archeological materials, as heat empties or resets the thermoluminescent signature of the material (Figure 1). Subsequent recharging of this material from ambient radiation can then be empirically dated by the equation:\n\nAge = (subsequently accumulated dose of ambient radiation) / (dose accumulated per year)\n\nThis technique was modified for use as a \"passive sand migration analysis tool\".(Figure 2). The research shows direct consequences resulting from the improper replenishment of starving beaches using fine sands. Beach nourishment is a problem worldwide and receives large amounts of attention due to the millions of dollars spent yearly in order to keep beaches beautified for tourists, e.g., in Waikiki, Hawaii. Sands with sizes 90-150 μm (very fine sand) were found to migrate from the swash zone 67% faster than sand grains of 150-212 μm (fine sand; Figure 3). Furthermore, the technique was shown to provide a passive method of policing sand replenishment and a passive method of observing riverine or other sand inputs along shorelines (Figure 4).\n\n"}
{"id": "2716153", "url": "https://en.wikipedia.org/wiki?curid=2716153", "title": "Wood grain", "text": "Wood grain\n\nWood grain is the longitudinal arrangement of wood fibers or the pattern resulting from this.\n\nR. Bruce Hoadley wrote that \"grain\" is a \"confusingly versatile term\" with numerous different uses, including the direction of the wood cells (e.g., \"straight grain\", \"spiral grain\"), surface appearance or figure, growth-ring placement (e.g., \"vertical grain\"), plane of the cut (e.g., \"end grain\"), rate of growth (e.g., \"narrow grain\"), and relative cell size (e.g., \"open grain\").\n\nPerhaps most important physical aspect of wood grain in woodworking is the grain direction or slope (e.g. against the grain). The two basic categories of grain are straight and cross grain. Straight grain runs parallel to the longitudinal axis of the piece. Cross grain deviates from the longitudinal axis in two ways, spiral grain or diagonal grain. The amount of deviation is called the slope of the grain.\n\nIn describing the application of a woodworking technique to a given piece of wood, the direction of the technique may be:\n\nGrain alignment must be considered when joining pieces of wood, or designing wooden structures. For example, a stressed span is less likely to fail if tension is applied along the grain, rather than across the grain. Grain direction will also affect the type of warping seen in the finished item.\n\nIn describing the alignment of the wood in the tree a distinction may be made. Basic grain descriptions and types include:\n\nIn its simplest aesthetic meaning, wood grain is the alternating regions of relatively darker and lighter wood resulting from the differing growth parameters occurring in different seasons (i.e., growth rings) on a cut or split piece of wood.\n\nCauses including fungus, burls, stress, knots, special grain alignments, and others produce figure in wood. Their rarity often promotes the value of both the raw material, and the finished work it becomes a part of. These include:\n\n\nThe way a given piece of wood has been sawn affects both its appearance and physical properties:\n\nStrictly speaking, grain is not always the same as the \"figure\" of wood.\n\nThere is irregular grain in burr wood or burl wood, but this is result of very many knots.\n\n"}
