{"id": "3497227", "url": "https://en.wikipedia.org/wiki?curid=3497227", "title": "1937 Sabena Junkers Ju 52 Ostend crash", "text": "1937 Sabena Junkers Ju 52 Ostend crash\n\nOn 16 November 1937 a Junkers Ju 52/3m owned by Belgian airline SABENA, operating as a scheduled international passenger flight from Cologne, Germany, to London, England, crashed near Ostend, Belgium. The aircraft hit a tall factory chimney while attempting to land at Steene aerodrome near Ostend, Belgium. The accident killed prominent members of the Hesse royal family on the way to London for the wedding of Louis, Prince of Hesse and by Rhine.\n\nThe flight from Frankfurt to London was scheduled to stop at Ostend Airport but diverted to Steene Aerodrome due to the bad weather. The aircraft hit the chimney of a brick factory and crashed, bursting into flames.\n\nAll eleven passengers and crew who boarded the aircraft died. The remains of Grand Duchess Cecilie's newborn child were found among the wreckage; a Belgian official enquiry into the crash indicated that the birth was the reason the pilot was attempting to land despite the poor weather conditions.\n\nThe aircraft had a crew of four: pilot, wireless operator and mechanic as well as a German engineer.\n\nThe members of royalty among the passengers were: \n\nThe aircraft was a three-engined Junkers Ju 52/3m airliner operated by SABENA and registered in Belgium as \"OO-AUB\".\n\nThe wedding of Louis with Margaret Campbell-Geddes, daughter of Sir Auckland Geddes was brought forward to the morning after the accident. Baron Riedesel would have been Louis' best man; their friend Oliver Chesterton stood in as best man; the ceremony was small and solemn with the guests in mourning clothes.\n\nImmediately following the wedding, Prince Louis and his wife Margaret travelled to Belgium and visited a hospital where the victims' bodies had been laid out.\n\nThe Hereditary Grand Duke and Duchess' fourteen-month-old daughter, Johanna, was the only one of the family who was not on board the aircraft. She was adopted by her uncle Louis in early 1938. Johanna died of meningitis in 1939.\n\nWith the death of the childless Prince Louis in 1968, the male line of the Hesse and by Rhine became extinct.\n\n\n\n"}
{"id": "30159163", "url": "https://en.wikipedia.org/wiki?curid=30159163", "title": "2010 Puebla oil pipeline explosion", "text": "2010 Puebla oil pipeline explosion\n\nThe 2010 Puebla oil pipeline explosion was a large oil pipeline explosion that occurred at 5:50 am CST on December 19, 2010, in the city of San Martín Texmelucan de Labastida, Puebla, Mexico. The pipeline, running from Tabasco to Hidalgo, was owned by the Pemex petroleum company, and exploded after thieves from the Los Zetas drug cartel attempted to siphon off the oil. The gas explosion and resulting oil fire killed 29 people, including thirteen children, and injured 52. Some of the flames in the fire became ten metres high, and the smoke towered over the city. The blast also damaged 115 homes, completely destroying 32 of them, and prompted the evacuation of 5,000 residents. Firefighters eventually controlled the blaze, but electricity and water remained cut following the explosions, and the military was deployed to the site. Mexican President Felipe Calderón visited the explosion site on the day of the incident to offer condolences to the victims' families. The fire was one of the deadliest in Mexican history, largely destroying an area of five-kilometre radius, and some oil may have polluted the Atoyac River.\n\nThe Los Zetas gang, one of the most powerful drug cartels and paramilitary groups involved in the ongoing Mexican Drug War, was blamed for the explosion. Throughout 2010, drug-related conflicts had killed 12,456 people. In 2008, Pemex reported 9.3 billion pesos ($750 million USD) of oil lost to thieves. Previously, close to sixty illegal tapping incidents occurred near the explosion site due to thieves stealing the oil. Much of the stolen oil is often trafficked to the United States.\n\nFelipe Calderón ordered an official investigation into the incident. A hole was found at the pipeline, and several bodies lay near the initial site of the explosion. The cause of the spark that led to the explosion is still unknown. The investigation is to include an assessment of the environmental impact of the explosion, including the pollution of downstream reservoirs. Mechanical failure was not ruled out as a possible cause of the oil leak despite evidence of theft and tampering of the pipeline. The pipeline was re-opened on December 22.\n"}
{"id": "3689043", "url": "https://en.wikipedia.org/wiki?curid=3689043", "title": "Airborne Laser", "text": "Airborne Laser\n\nAn airborne laser (ABL) is a laser system operated from a flying platform, as in the:\n\nDevelopment in the United States has seen the system tested in 1981, when researchers had mounted the system to a KC-135 Stratotanker and successfully destroyed 5 AIM-9 Sidewinder missiles and a simulated cruise missile. The laser used in these tests was the \"Chemical Oxygen Iodine Laser\" (COIL) and was developed in 1977, and used $1,000 of chemicals each time the laser fired and emitted in the Infrared frequency.\n\nAirborne lasers when fired, have enough energy to vaporize the metal of the missile that it is currently targeting. \"It deposits enough heat to laze a hole through it. It's like taking a magnifying glass and burning a hole through a piece of paper, but we do it through metal,\" said Dr. Keith Truesdell, Phillips Lab chief of the applied laser technology branch. To help with firing, newer ABL systems utilize tracking lasers which have been tested recently in 2007 when the US Missile Defense Agency tested the track illuminator laser (TILL) on the Boeing NC-135. The TILL is a solid state laser and is a key part of the fire control of the ABL system. \n\nAs stated before, operational costs of an airborne laser are quite high. And the projected cost of the program was listed as 5.1 million USD in 2009 according to the US Department of Defense. Along with the issues of price, there is also the issue of sensitivity of the laser itself. \"Jitter\" is a result of the chemicals in the laser burning off and creating turbulence under the plane, which will result in targeting issues. \n"}
{"id": "27619217", "url": "https://en.wikipedia.org/wiki?curid=27619217", "title": "CAREM", "text": "CAREM\n\nCAREM () is a small modular reactor for electrical power generation currently under construction near the city of Zárate, in the northern part of Buenos Aires province beside the Atucha I Nuclear Power Plant.\n\nThe reactor was integrally designed by CNEA (National Atomic Energy Commission), being the first power reactor designed by the country. It is basically a simplified pressurized water reactor (PWR) designed to have an electrical output of 25MW for the first prototype, 100MW in the following one. It is an integral reactor – the coolant system is inside the reactor vessel – so that the entire plant operates at the same pressure. This design minimizes the risk of loss-of-coolant accidents (LOCA). Its fuel is uranium oxide with a enrichment of 3.4% that needs to be replaced annually. The primary coolant system uses natural circulation, so there are no pumps required, which provides inherent safety against core meltdown, even in accident situations.\n\nIn 1984 it was presented publicly for the first time during an IAEA conference in Peru. For political reasons the project was halted but was relaunched by the 2006 Argentine nuclear reactivation plan.\n\nThe 25 MWe prototype version of CAREM currently being built will be followed by a second one of 100-200 MWe to be installed in Formosa Province. As of 2013, the first prototype was planned to receive its first fuel load in 2017. As of 2016, the completion of the project was scheduled for the end of 2018 \n\n\n"}
{"id": "54561770", "url": "https://en.wikipedia.org/wiki?curid=54561770", "title": "COGIX", "text": "COGIX\n\nThe so-called COGIX (COGeneration IndeX) is an indicator for the operating efficiency of CHP-plants, that receive revenues at the wholesale electricity market. It is similar to the spark spread resp. green spark spread including emission costs for greenhouse gases, which is a contribution margin calculated from fuel costs, emission costs and revenues from electricity sales. In the COGIX framework, also the revenues from heat sales of the CHP-plant are considered.\n\nThe COGIX evaluates generalised assumptions on cogeneration plants and the current reference prices for input and output streams, such as fuel, greenhouse gases, electrical and thermal energy. The index is for the assessment of the economic situation of the whole cogen-industry, not for single CHP projects, thus the values for technical parameters (efficiency, power-to-heat ratio) should be chosen according to BAT or industry average. Prices refer to delivery at the system boundary of the CHP-plant, e.g. the price for the fuel includes a lump-sum for transportation costs and the estimated heat price has to consider distribution expenses.\n\nWith\n\nCOGIX : contribution margin for cogenerated electricity in €/MWh <br>\np : electricity price in €/MWh <br>\np : heat price in €/MWh <br>\np : fuel price in €/MWh <br>\np : CO price in €/t <br>\ne : fuel specific CO emissions in t/MWh <br>\nη : electrical efficiency of the CHP-plant <br>\nσ : energy-based power-to-heat ratio of the CHP-plant\n\nThe COGIX has been developed in 2011 to analyse the situation of gas-fired cogeneration in the German energy market and is used regularly for an update. A practical figure for the value of heat turned out to be 1,3 times the price for natural gas. This estimate fits both for large industrial heat user and smaller heat customers in extensive district heating networks. As the power-to-heat ratio does affect the COGIX only insignificantly, a ratio of σ=1 is usually chosen normatively, as shown by small CCGT plants and large combustion engines in practical life. The standard assumption for the overall efficiency is 85%, resulting in thermal and electrical efficiencies of η = η = 42,5 %. The emission factor for national gas is 0,2 t per MWh fuel.\n\nGenerating capacity is usually sold on a long-term basis at the future market, therefore COGIX's variable reference prices (for baseload electricity, natural gas, CO2-certificates) are also taken from the futures market, specifically the Y+1 segment with delivery in the following calendar year. Considering the wholesale gas price, an overall surcharge of 4 €/MWh (based on the superior heating value H) has been added for transportation and structuring.\n\nRoughly estimated, the COGIX is for current CHP technology the electricity price minus the natural gas price minus half the CO price.\n\n\"Note:\" There is also the option to calculate a COGIX for coal-fired CHP plants with adjusted emission factors, efficiencies and fuel prices. The emission factor of hard coal is 0,34 t per MWh fuel. The coal-fired cogeneration plant has an assumed power-to-heat ratio of 0,7, this results in η = 35% and η = 50%. The estimation for the coal transportation costs is 2 €/MWh.\n"}
{"id": "12374303", "url": "https://en.wikipedia.org/wiki?curid=12374303", "title": "Catlettsburg Refinery", "text": "Catlettsburg Refinery\n\nThe Catlettsburg Refinery is an American oil refinery. It is located in northeastern Kentucky, at the intersection of Interstate 64 and U.S. Route 23 in Catlettsburg, Kentucky near the cities of Ashland, Kentucky and Huntington, West Virginia. The facility was built in 1916 by the Great Eastern Refining Company and purchased in 1924 by the Ashland Refining Company. The refinery now occupies a plus site, producing more than , and employing around 1,600 employees and contractors. Its location on the west banks of the Big Sandy River and only two miles south of the Ohio River, allows it to ship products by barge as well as pipeline. It is owned and operated by Marathon Petroleum Corporation.\n\nDue to the coal mining industry and the large quantity of petroleum products shipped from the refinery, the Port of Huntington Tri-State is the largest inland port in the United States, with 76.5 million tons shipped in 2007.\n\n"}
{"id": "8656111", "url": "https://en.wikipedia.org/wiki?curid=8656111", "title": "China Shipbuilding Industry Corporation", "text": "China Shipbuilding Industry Corporation\n\nThe China Shipbuilding Industry Corporation (CSIC) is one of the two largest shipbuilding conglomerates in China, the other being the China State Shipbuilding Corporation (CSSC). It was formed by the Government of the People's Republic of China on 1 July 1999 from companies spun off from CSSC, and is 100% owned by SASAC. Headquartered in Beijing, the CSIC handles shipbuilding activities in the north and the west of China, while the China State Shipbuilding Corporation (CSSC) deals with those in the east and the south of the country.\n\nCSIC's subsidiary, China Shipbuilding Industry Company Limited (CSICL), was listed on the Shanghai Stock Exchange in 2008. Its trade arm is China Shipbuilding & Offshore International Co. Ltd.\n\nCSIC has developed 10 main product sections: shipbuilding, marine engineering, diesel engines, storage batteries, large steel structure fabrications, port machinery, turbochargers, tobacco machinery, gas meters and automation distribution systems.\n\nThe main business scope of CSIC includes management of all the state owned assets of the corporation and its subsidiaries, domestic and overseas investment and financing, undertaking scientific research and production of military products, mainly of warships, design, production and repair of domestic and overseas civil vessels, marine equipment and other non-ship products, various forms of economic and technological co-operation, overseas turnkey project contracting, labour export, projects of production with foreign materials, engineering project contracting, engineering construction, building construction and installation, and other business authorized.\n\nCSIC consists of 96 enterprises located in northern China, and employs over 300,000 people. Assets include shipbuilding and industrial enterprises in Dalian (Dalian Shipbuilding Industry Company), Tianjin, Qingdao, Wuhan, Xi'an, Chongqing and Kunming, as well as 30 research institutes and ten laboratories developing naval and civil vessels and related equipment.\n\nChina State Shipbuilding Corporation (CSSC) carried out fundamental institutional restructuring. Ship building and repair enterprises and related equipment manufacturers formerly owned by CSSC in areas of Dalian, Tianjin, Wuhan, Kunming and Xi’an, together with majority of the institutes under China Ship Research & Development Academy, formed China Shipbuilding Industry Corporation (CSIC), which was founded on 1 July 1999 in Beijing. This was part of the overall State Council initiative of 1 July 1999, under which the Chinese government split the top five Defense and Technology Corporations into ten new enterprises. These corporations are all large State-owned enterprises (SOEs) under direct supervision of the State Council. These SOEs include the China State Shipbuilding Corporation (CSSC) and the China Shipbuilding Industry Corporation (CSIC).\n\nCSIC is the largest group in China in the field of design, manufacture, and trade of military and civil ships, marine engineering and marine equipment. CSIC is a very large state owned enterprise (SOE) supervised by The State Council, a state authorised institution of investment, to combine investment with the industry, industry with trade, research and production, undertaking scientific and technological research and design on ships, civil vessels, marine equipment, production and sales of civil electro- mechanical products.\n\nConsisting of some of the enterprises and institutes of former China State Shipbuilding Corporation (old CSSC), it has a total of 97 member units, assets worth of RMB 34 billion, annual turnover of RMB11 billion, 1.1 million DWT shipbuilding output per annum. There are 48 industrial enterprises, 28 scientific & technological research institutes, 15 share holding companies in CSIC, spread in more than 20 provinces and cities in China. Among its subsidiaries, CSIC has the world-renowned Dalian Shipyard, Dalian New Shipyard, Bohai Shipyard, Wuchang Shipyard, Shanhaiguan Shipyard, China Ship Research & Development Academy, China Shipbuilding Trading Company Ltd., China National Shipbuilding Equipment & Material Corporation, China Offshore Industrial Corporation, HZ Windpower (Haizhuang Wind Power Equipment) etc.\n\nCSIC possesses the largest shipbuilding and ship repair bases, capable of undertaking research, design, manufacture and repair of various civil vessels up to the capacity of 388,000 DWT, marine engineering, surface and submersible combat vessels, naval supplementary boats, under water weapons and other related equipment. The hundreds of civil vessels, marine engineering projects and other machinery and electronics designed and built by CSIC have been exported to dozens of countries and regions. By utilizing shipbuilding and military technologies, CSIC has developed hundreds of electro-mechanic products which have been used in more than 20 domestic industries and fields such as aerospace, metallurgical, hydroelectricity, light industries, etc. The world-renowned Dalian shipyard, Dalian New Shipyard, Bohai Shipyard, Shan Hai Guan Shipyard, Wuchang Shipyard, Dalian Marine Diesel Engine Works, Shaanxi Diesel Engine Factory, China Ship Research & Development Academy, China Ship Scientific Research Center, China Shipbuilding Trading Co Ltd, China United Shipbuilding Co Ltd (Hong Kong), China National Shipbuilding Equipment & Materials Corp and China Offshore Industrial Corp are all members of CSIC.\n\nCSIC is the largest shipbuilding and ship repair corporation in the country capable of undertaking new building and ship repair of various ships including VLCCs, surface and submersible naval vessels and naval supplementary boats up to a capacity of 300 000 dwt class. Ships of this description have been exported worldwide over 50 countries and regions. CSIC also consists of the country's main force on research and design of military ships. It has 28 scientific research institutes with over 360 major specialties involved, with about 30 000 technicians, six state-level laboratory centers, 150 large-scale laboratories and three state-level technological centers. CSIC is capable of developing and inventing new products with its strong scientific back up. CSIC is also the largest marine equipment manufacturer in the country. It has 38 plants producing marine diesel engines, marine equipment and auxiliary products including ship accessories for both military and civilian use. Some of these products are either produced under production licenses or technological transfer. By using shipbuilding and military technologies, CSIC has strength in developing and manufacturing large turnkey equipment. It has developed and produced hundreds of nonship products that have been used in more than 20 industries and fields such as aerospace, metallurgy, hydro power, petrochemicals, tobacco, railways, coal, light industries and city construction. The nonship products are also exported worldwide.\n\nThe market task of CSIC is to strengthen its market position with core businesses on military products and shipbuilding in which the military products are one priority. It also diverts into other industries while concentrating on technological innovation and cost controlling. The main business scope of CSIC includes: managing the group and its subsidiaries assets; domestic and overseas investing and financing; research and development of military products with more focus on warships; designing, producing and repairing civilian ships, marine equipment for both domestic and overseas markets; various forms of economic and technological cooperation; and overseas turnkey project contracting, labour exporting, construction and installation. CISC is also authorized to carry out other businesses.\n\nIn 2004, CSOS started the company Haizhuang Wind Power Equipment. In 2008 the first prototype of a 2MW wind turbine (designed by aerodyn Energiesysteme GmbH) was installed.\n\nHaizhuang Wind Power currently produces three types of wind turbines:\n\n\n"}
{"id": "4661493", "url": "https://en.wikipedia.org/wiki?curid=4661493", "title": "Chromium(IV) oxide", "text": "Chromium(IV) oxide\n\nChromium dioxide or chromium(IV) oxide is an inorganic compound with the formula CrO. It is a black synthetic magnetic solid. It once was widely used in magnetic tape emulsion. With the increasing popularity of CDs and DVDs, the use of chromium(IV) oxide has declined. However, it is still used in data tape applications for enterprise-class storage systems. It is still considered by many oxide and tape manufacturers to have been one of the best magnetic recording particulates ever invented.\n\nCrO was first prepared by Friedrich Wöhler by decomposition of chromyl chloride. Acicular chromium dioxide was first synthesized in 1956 by Norman L. Cox, a chemist at E.I. DuPont, by decomposing chromium trioxide in the presence of water at a temperature of 800 K and a pressure of 200 MPa. The balanced equation for the hydrothermal synthesis is:\n\nThe magnetic crystal that forms is a long, slender glass-like rod — perfect as a magnetic pigment for recording tape. When commercialized in the late 1960s as a recording medium, DuPont assigned it the tradename of \"Magtrieve\".\n\nCrO adopts the rutile structure (as do many metal dioxides). As such each Cr(IV) center has octahedral coordination geometry and each oxide is trigonal planar.\n\nThe crystal’s magnetic properties, derived from its ideal shape such as anisotropy which imparted high coercivity and remanent magnetization intensities, resulted in exceptional stability and efficiency for short wavelengths, and it almost immediately appeared in high performance audio tape used in audio cassette for which treble response and hiss were always problems. Unlike the spongy looking ferric oxides used in common tape, the chromium dioxide crystals were perfectly formed and could be evenly and densely dispersed in a magnetic coating; and that led to unparalleled low noise in audio tapes. Chrome tapes did, however, require a new generation of audiocassette recorders equipped with a higher bias current capability (roughly 50% greater) than that used by iron oxide to properly magnetize the tape particles. Also introduced was a new equalization (70 µs) that traded some of the extended high-frequency response for lower noise resulting in a 5–6 dB improvement in signal-to-noise ratio over ferric-oxide audio tapes. These bias and EQ settings were later carried over to “chrome-equivalent” cobalt-modified tapes introduced in the mid 1970s by TDK, Maxell, and others. Later research significantly increased the coercivity of the particle by doping or adsorbing rare elements such as iridium onto the crystal matrix or by improving the axial length-to-deprecated ratios. The resulting product was potentially a competitor to metallic iron pigments but apparently achieved little market penetration.\n\nUntil manufacturers developed new ways to mill the oxide, the crystals could easily be broken in the manufacturing process, and this led to excessive print-through (echo). Output from a tape could drop about 1 dB or so in a year's time. Although the decrease was uniform across the frequency range and noise also dropped the same amount, preserving the dynamic range, the decrease misaligned Dolby noise reduction decoders that were sensitive to level settings. The chrome coating was harder than competitive coatings, and that led to accusations of excessive head wear. Although the tape wore hard ferrite heads faster than oxide based tapes, it actually wore softer permalloy heads at a slower rate; and head wear was more a problem for permalloy heads than for ferrite heads. The head wear scare and licensing issues with DuPont kept blank consumer chrome tapes at a great disadvantage versus the eventually more popular Type II tapes that used cobalt-modified iron oxide, but chrome was the tape of choice for the music industry's cassette releases. Because of its low Curie temperature (approximately ), chrome tape lent itself to high-speed thermomagnetic duplication of audio and video cassettes for pre-recorded product sales to the consumer and industrial markets.\n\nDuPont licensed the product to Sony in Japan and BASF in Germany in the early 1970s for regional production and distribution. Japanese competitors developed cobalt-adsorbed (TDK: \"Avilyn\") and cobalt ferrite (Maxell: \"Epitaxial\") \"chrome equivalent\" Type II audio cassettes and various videotape formats as substitutes. Added to that was the problem that the production of CrO yielded toxic by-products of which Japanese manufacturers had great difficulty properly disposing. BASF eventually became the largest producer of both the chromium dioxide pigment and chrome tapes, basing its VHS & S-VHS video tape, audio cassettes, and 3480 data cartridges on this formulation. Dupont and BASF had also introduced chrome-cobalt \"blended\" oxide pigments which combined about 70% cobalt-modified iron oxide with 30% chrome oxide into a single coating, presumably to offer improved performance at lower costs than pure chrome. Many high grade VHS tapes also used much smaller amounts of chrome in their formulations because its magnetic properties combined with its cleaning effects on heads made it a better choice than aluminium oxide or other non-magnetic materials added to VHS tape to keep heads clean. Dupont discontinued its production of chromium dioxide particles in the 1990s. In addition to BASF, which no longer owns a tape manufacturing division, Bayer AG of Germany, Toda Kogyo and Sakai Chemical of Japan also do or can produce the magnetic particles for commercial applications.\n\n"}
{"id": "21117851", "url": "https://en.wikipedia.org/wiki?curid=21117851", "title": "Composite laminate", "text": "Composite laminate\n\nIn materials science, a composite laminate is an assembly of layers of fibrous composite materials which can be joined to provide required engineering properties, including in-plane stiffness, bending stiffness, strength, and coefficient of thermal expansion.\n\nThe individual layers consist of high-modulus, high-strength fibers in a polymeric, metallic, or ceramic matrix material. Typical fibers used include cellulose, graphite, glass, boron, and silicon carbide, and some matrix materials are epoxies, polyimides, aluminium, titanium, and alumina.\n\nLayers of different materials may be used, resulting in a hybrid laminate. The individual layers generally are orthotropic (that is, with principal properties in orthogonal directions) or transversely isotropic (with isotropic properties in the transverse plane) with the laminate then exhibiting anisotropic (with variable direction of principal properties), orthotropic, or quasi-isotropic properties. Quasi-isotropic laminates exhibit isotropic (that is, independent of direction) inplane response but are not restricted to isotropic out-of-plane (bending) response. Depending upon the stacking sequence of the individual layers, the laminate may exhibit coupling between inplane and out-of-plane response. An example of bending-stretching coupling is the presence of curvature developing as a result of in-plane loading.\n\nComposite laminates may be regarded as a type of plate or thin-shell structure, and as such their stiffness properties may be found by integration of in-plane stress in the direction normal to the laminates surface. The broad majority of ply or lamina materials obey Hooke's law and hence all of their stresses and strains may be related by a system of linear equations. Laminates are assumed to deform by developing three strains of the mid-plane/surface and three changes in curvature\n\nformula_1\nand\nformula_2\n\nwhere formula_3 and formula_4 define the co-ordinate system at the laminate level. Individual plies have local co-ordinate axes which are aligned with the materials characteristic directions; such as the principal directions of its elasticity tensor. Uni-directional ply's for example always have their first axis aligned with the direction of the reinforcement. A laminate is a stack of individual plies having a set of ply orientations\n\nformula_5\n\nwhich have a strong influence on both the stiffness and strength of the laminate as a whole. Rotating an anisotropic material results in a variation of its elasticity tensor. If in its local co-ordinates a ply is assumed to behave according to the stress-strain law\n\nformula_6\n\nthen under a rotation transformation (see transformation matrix) it has the modified elasticity terms\n\nformula_7\n\nformula_8\n\nformula_9\n\nformula_10\n\nformula_11\n\nformula_12\n\nHence\n\nformula_13\n\nAn important assumption in the theory of classical laminate analysis is that the strains resulting from curvature vary linearly in the thickness direction, and that the total in-plane strains are a sum of those derived from membrane loads and bending loads. Hence\n\nformula_14\n\nFurthermore, a three-dimensional stress field is replaced by six stress resultants; three membrane forces (forces per unit length) and bending moments per unit length. It is assumed that if these three quantities are known at any location (x,y) then the stresses may be computed from them. Once part of a laminate the transformed elasticity is treated as a piecewise function of the thickness direction, hence the integration operation may be treated as the sum of a finite series, giving\n\nformula_15\n\nwhere\n\n\n"}
{"id": "21378374", "url": "https://en.wikipedia.org/wiki?curid=21378374", "title": "DS 5", "text": "DS 5\n\nThe DS 5 is a compact executive car which was designed and developed by the French automaker Citroën, and launched in the market in Europe in November 2011. It was the third model in the premium sub brand DS. Released as the Citroën DS5, the car was relaunched as the DS 5 in 2015, following Citroen's decision to rebadge its DS models and market them under the brand DS.\n\nThe DS5 was revealed at the 2011 Shanghai Auto Show in April 2011. Although Peugeot and Citroën products have shared platforms and principal components since the closing decades of the twentieth century, the DS5 became in 2011 the first Citroën branded car to be assembled in at Sochaux.\n\nThe DS5 mixes hatchback and estate styling, resembling a shooting-brake. It is long and wide, dimensions that are similar to those of the Lancia Delta. This is hardly a coincidence: the DS5 is based on the PF2 platform as 3008 is too, not on the C5 as its name could imply. \n\nThe DS5 fills the spot where the first generation C5 hatchback left off, as the current C5 no longer have hatchback versions. Like the original concept car, its interior is heavily aviation inspired and available with two centre consoles, one of which is located on the roof directly above the other. \n\nWith a head up display in front of the driver, the cabin is designed to resemble a jet aeroplane. Emphasising its links to aviation, and the original Citroën DS model, the carmaker recreated an old photoshoot of the car with Concorde.\n\nBuyers can choose between a turbocharged petrol, two diesel engines and PSA's diesel-electric Hybrid4. It marries a 163 hp 2.0 HDi diesel engine with a electric motor mounted on the rear axle and sends the power to all four wheels as it is needed. \n\nDepending on trim level, this powertrain emits 99g/km or 107g/km , and the car can drive on electricity alone if the battery is sufficiently charged. It is also worth noting that the DS5 is the first Citroën with a hybrid drivetrain, and the first production car with a diesel electric hybrid drivetrain.\n\nA hybrid convertible Citroën DS5 was chosen by François Hollande, for his investiture parade as President of France in May 2012.\n\nThe Citroën DS5 was relaunched as the DS 5, without Citroën badging, in 2015.\n\nThe Citroën DS5 was prefigured by the Citroën C-SportLounge, a concept car presented by Citroën in September 2005 at the Frankfurt Motor Show, and designed under Citroën design chief Jean-Pierre Ploué. The Citroën C-SportLounge inspired the DS 5 in 2011, and it has rear suicide doors, while the production car has the lack of the suicide doors.\n\nThe C-SportLounge is a front-wheel-drive concept car that includes engine, with six speed automatic transmission and twenty inch alloy wheels, with 255/40 tires. Its body has drag coefficient of 0.26 and features an interior design inspired by aeroplane cockpits.\n\n"}
{"id": "34482599", "url": "https://en.wikipedia.org/wiki?curid=34482599", "title": "Electrically conductive adhesive", "text": "Electrically conductive adhesive\n\nAn electrically conductive adhesive is a glue that is primarily used for electronics.\n\nThe electric conductivity is caused by a component that makes ca. 80% of the total mass of an electrically conductive adhesive. This conductive component is suspended in a sticky component that holds the electrically conductive adhesive together. The particles of the conductive component are in contact to each other and in this way make electric current possible.\n\nThe conductive component can be silver, nickel, copper or graphite. Other conductive materials are possible but unusual.\nThe adhesive component can be a varnish, synthetic resin, or Silicone. Variations in conductive component's type and concentration change the resistivity of the adhesive. \n\n• One could fix a defective conductor on a printed circuit board using an electrically conductive adhesive. In the same way, one could fix a defective rear windscreen heater on a car using an electrically conductive adhesive. \n\n• If just a small current is needed, a temperature-sensitive electronic element can be electrically connected to a circuit using an electrically conductive adhesive instead of soldering.\n\n• Electrically conductive adhesives can be used to paint the inner surface of plastic boxes containing electronic devices. This makes a Faraday Cage saving the internal components from electromagnetic radiation. \n\n• Electrically conductive adhesives are used in scanning electron microscopy (SEM) to fix and ground the sample to avoid electrostatic charging of the surface.\n\n"}
{"id": "31517617", "url": "https://en.wikipedia.org/wiki?curid=31517617", "title": "Electrification of the New York, New Haven, and Hartford Railroad", "text": "Electrification of the New York, New Haven, and Hartford Railroad\n\nThe New York, New Haven and Hartford Railroad pioneered electrification of main line railroads using high-voltage, alternating current, single-phase overhead catenary. It electrified its mainline between Stamford, Connecticut, and Woodlawn, New York, in 1907, and extended the electrification to New Haven, Connecticut, in 1914. While single-phase AC railroad electrification has become commonplace, the New Haven's system was unprecedented at the time of construction. The significance of this electrification was recognized in 1982 by its designation as a National Historic Engineering Landmark by the American Society of Mechanical Engineers (ASME).\n\nThe New Haven tried several experiments with low-voltage DC electrification in the decade preceding their main line overhead electrification. These included:\n\nThe third rail system resulted, not surprisingly, in a number of accidents. It also resulted in a decree from the Connecticut Supreme Court on June 13, 1906 forbidding the use of third rail electrification within the state. The New Haven was forced by this decision to design their main line electrification system using overhead catenary.\n\nSeveral different systems combinations of voltage and frequency were considered in the initial design. Due to the relatively large distances involved, transmission at high voltages using alternate current was recognized as being unavoidable. An architecture similar to commercial DC utilities and urban railroads was considered using high voltage transmission lines, rotary converters, and overhead DC catenary. The studies of the time assumed an electrical efficiency of only 75 percent for this architecture.\n\nThe highest voltage for which generators could be reliably designed at this time was about 22 kV. An intermediate design was considered using 22 kV transmission lines, substations to reduce catenary voltage to between 3 and 6 kV, and transformers on the engines to the 560 V required by the traction motors. The railroad realized that it could save significant capital cost if the intermediate substitution were omitted and locomotives received line voltage at around 11 kV.\n\nThe New Haven's electrification was the first of its kind; no previous railroad had practical experience operating a high voltage distribution system above a steam railroad. Many of the system's ultimate specifications were the result of educated design decisions based on the state of the electrical technology in 1907.\n\nProposals were obtained from General Electric (GE) and Westinghouse. Both companies submitted a variety of AC and DC schemes, though GE favoured DC electrification. But New Haven chose single-phase AC at 11 kV, 25 Hz. as proposed by Westinghouse, who had been researching AC electrification of railroads since 1895 and in association with Baldwin supplied Baldwin-Westinghouse locomotives. Later GE also supplied some locomotives.\n\nThe designers considered several voltages for the transmission segment of the system including 3-6 kV, 11 kV, and 22 kV. Ultimately, the transmission and catenary systems were combined into a transformerless system, that utilized the same voltage from output of generator to catenary to locomotive pantograph. As 11 kV was the highest voltage that could be obtained directly from the output of the generators of 1907, 11 kV was selected as the transmission and catenary voltage of the system.\n\nThe New Haven considered two different operating frequencies for use in their electrification: 15 Hz and 25 Hz. The lower frequency of 15 Hz afforded reduced motor size, lower inductive losses, and a higher motor power factor. 25 Hz had by 1907 already become a commercial standard, and the railroad already operated a number of trolley power houses at 25 Hz and had equipped many of its shops with 25 Hz motors. Selection of 15 Hz viewed by the railroad as a 'break in gage' which would have limited the commercial value of the system. Thus the railroad selected the 25 Hz standard, even though it might have been more desirable from an engineering perspective. Note that many European railroads standardized on a 16.7 Hz traction power frequency.\n\nThe New Haven had no precedent to follow when designing its catenary system. Overhead catenary had previously been the domain of trolleys, except for a few three-phase railways in Europe. No prior experience existed with operating high-speed railways with an overhead contact system. The catenary designed by the New Haven was a unique, relatively rigid triangular cross-section.\n\nThe triangular cross-section of catenary used in the original electrification was only repeated by one other railway. The London, Brighton and South Coast Railway used a similar triangular catenary from 1909 until 1929. The New Haven's 1914 extensions dispensed with the triangular catenary design.\n\nCatenary support spacing was set at . This was based on keeping the straight line deviation from center of track to within with a curve radius of 3 degree, which was the tightest curve between the original system's termini at Woodlawn and Stamford.\n\nThe generators at the Cos Cob Power Station were designed to supply single-phase power directly to the catenary. They were also required to supply three-phase power both to the New Haven itself for use along the lines, and to the New York Central's (NYC) Port Morris generating station to compensate the NYC for the power consumed by New Haven trains on the NYC's third-rail supplied line to Grand Central Terminal. The Cos Cob generators were three-phase machines, but wired to supply both three phase and single phase power simultaneously.\n\nAlthough the railroad considered the 1907 electrification highly successful, two problems required an ultimate redesign of the transmission system. The first was electromagnetic interference in adjacent, parallel telegraph and telephone wires caused by the high currents in the traction power system.\n\nThe second was that the system's geographic growth and the evolving state of electrical technology created a need for higher transmission voltages. The railroad could have simply raised the operating voltage of the entire system, however this would have required all the catenary insulators to be upgraded to withstand a higher potential, and replacement of all the locomotive high voltage equipment. And while higher transmission voltages had become common in the seven years since the initial electrification, generators were still limited by economics to a maximum output voltage of around 11 kV.\n\nThe solution decided upon by the railroad, after several years of study, was a balanced autotransformer system.\n\nRemarkably, the railroad changed transmission system architectures within four hours, although preliminary work had taken the preceding 18 months. On Sunday, January 25, 1914, the railroad shut down the entire power system at 2 am. Gangs of workers throughout the system reconfigured the transmission lines over the next 70 minutes. System startup was commenced and by 5:30 am, electric trains were running over the new, autotransformer supplied system.\n\nThe New Haven's system was extended across the Hell Gate Bridge to the New York Connecting Railroad upon the line's construction. The system of electrification was an extension of the New Haven's revised 11/22 kV autotransformer architecture. The original electrification extended from the New Haven's main line, across the Hell Gate Bridge, to the Bay Ridge yard. The line south of Bowery Bay Junction was de-electrified in the 1950s. The line between New Rochelle and the Harold Interlocking was transferred to Amtrak in 1976 upon dissolution of Penn Central. The electrification system continued to be controlled as a portion of the ex-New Haven system until the 1987 conversion to 60 Hz operation.\n\nWhen the New Haven main line was converted by Metro-North to 60 Hz operation, the Amtrak Hell Gate line was also converted, but as an isolated system powered from the Van Nest substation. Control of the catenary system was transferred from Cos Cob to the Load Dispatcher at New York Penn Station. Although conversion occurred subsequent to the PRR-era electrification, Amtrak substation numbers 45-47 were assigned for consistency with the rest of the PRR numbering scheme.\n\nOrdered by date of publication.\n\n"}
{"id": "43409632", "url": "https://en.wikipedia.org/wiki?curid=43409632", "title": "Energy Department (Punjab, Pakistan)", "text": "Energy Department (Punjab, Pakistan)\n\nEnergy Department is a department of Government of Punjab, Pakistan. The department is responsible for regulation and policy formulation regarding power sector. After passing of 18th Amendment, provinces are fully powered to develop power projects through public or private sector.\n\nThe department is headed by Minister in charge democratically and a secretary at bureaucratic level, who is assisted by an additional secretary and four deputy secretaries.\n\nAs of May 2012, there are more than 52 projects under development of around 1400 MW.\n\nPunjab Power Development Board (PPDB) provides One-Window facility to promote private sector participation in power generation. \n\nCompany to develop power projects in Public-Private Partnership.\n\nPunjab Power Management Unit (PPMU) has been established to plan, procure and implement the ADB funded Renewable Energy Projects.\n\n\n"}
{"id": "276504", "url": "https://en.wikipedia.org/wiki?curid=276504", "title": "Engineered wood", "text": "Engineered wood\n\nEngineered wood, also called composite wood, man-made wood, or manufactured board, includes a range of derivative wood products which are manufactured by binding or fixing the strands, particles, fibres, or veneers or boards of wood, together with adhesives, or other methods of fixation to form composite materials. These products are engineered to precise design specifications which are tested to meet national or international standards. Engineered wood products are used in a variety of applications, from home construction to commercial buildings to industrial products. The products can be used for joists and beams that replace steel in many building projects.\n\nTypically, engineered wood products are made from the same hardwoods and softwoods used to manufacture lumber. Sawmill scraps and other wood waste can be used for engineered wood composed of wood particles or fibers, but whole logs are usually used for veneers, such as plywood, MDF or particle board. Some engineered wood products, like oriented strand board (OSB), can use trees from the poplar family, a common but non-structural species.\n\nAlternatively, it is also possible to manufacture similar engineered bamboo from bamboo; and similar engineered cellulosic products from other lignin-containing materials such as rye straw, wheat straw, rice straw, hemp stalks, kenaf stalks, or sugar cane residue, in which case they contain no actual wood but rather vegetable fibers.\n\nFlat pack furniture is typically made out of man-made wood due to its low manufacturing costs and its low weight.\n\nPlywood, a wood structural panel, is sometimes called the original engineered wood product. Plywood is manufactured from sheets of cross-laminated veneer and bonded under heat and pressure with durable, moisture-resistant adhesives. By alternating the grain direction of the veneers from layer to layer, or “cross-orienting”, panel strength and stiffness in both directions are maximized. Other structural wood panels include oriented strand board and structural composite panels.\n\nDensified wood is made by using a mechanical hot press to compress wood fibers and increase the density by a factor of three. This increase in density is expected to enhance the strength and stiffness of the wood by a proportional amount. Early studies confirmed this result with a reported increase in mechanical strength by a factor of three. \n\nMore recent studies have combined chemical process with traditional mechanical hot press methods to increase density and thus mechanical properties of the wood. In these methods, chemical processes break down lignin and hemicellulose that is found naturally in wood. Following dissolution, the cellulose strands that remain are mechanically hot compressed. Compared to the three-fold increase in strength observed from hot pressing alone, chemically processed wood has been shown to yield an 11-fold improvement. This extra strength comes from hydrogen bonds formed between the aligned cellulose nanofibers. \n\nThe densified wood possessed mechanical strength properties on par with steel used in building construction, opening the door for applications of densified wood in situations where regular strength wood would fail. Environmentally, wood requires significantly less carbon dioxide to produce than steel and acts as a source for carbon sequestration.\n\nMedium-density fibreboard, is made by breaking down hardwood or softwood residuals into wood fibres, combining it with wax and a resin binder, and forming panels by applying high temperature and pressure. \n\nParticle board is manufactured from wood chips, sawmil shavings, or even sawdust, and a synthetic resin or other suitable binder, which is pressed and extruded. Oriented strand board, also known as flakeboard, waferboard, or chipboard, is similar but uses machined wood flakes offering more strength. Particle board is cheaper, denser and more uniform than conventional wood and plywood and is substituted for them when cost is more important than strength and appearance. A major disadvantage of particleboard is that it is very prone to expansion and discoloration due to moisture, particularly when it is not covered with paint or another sealer.\n\nOriented strand board (OSB) is a wood structural panel manufactured from rectangular-shaped strands of wood that are oriented lengthwise and then arranged in layers, laid up into mats, and bonded together with moisture-resistant, heat-cured adhesives. The individual layers can be cross-oriented to provide strength and stiffness to the panel. However, most OSB boards are delivered with more strength in one direction. The wood strands in the outmost layer on each side of the board are normally aligned into the strongest direction of the board. Arrows on the product will often identify the strongest direction of the board (When bought in most cases the height (The longest dimension) of the board). Produced in huge, continuous mats, OSB is a solid panel product of consistent quality with no laps, gaps or voids.\n\nOSB is delivered in various dimensions, strengths and levels of water resistance.\n\nGlued laminated timber (glulam) is composed of several layers of dimensional timber glued together with moisture-resistant adhesives, creating a large, strong, structural member that can be used as vertical columns or horizontal beams. Glulam can also be produced in curved shapes, offering extensive design flexibility.\n\nLaminated veneer lumber (LVL) is produced by bonding thin wood veneers together in a large billet. The grain of all veneers in the LVL billet is parallel to the long direction. The resulting product features enhanced mechanical properties and dimensional stability that offer a broader range in product width, depth and length than conventional lumber. LVL is a member of the structural composite lumber (SCL) family of engineered wood products that are commonly used in the same structural applications as conventional sawn lumber and timber, including rafters, headers, beams, joists, rim boards, studs and columns.\n\nCross-Laminated Timber (CLT) is a versatile multi-layered panel made of lumber. Each layer of boards is placed cross-wise to adjacent layers for increased rigidity and strength. CLT can be used for long spans and all assemblies, e.g. floors, walls or roofs. CLT has the advantage of faster construction times as the panels are manufactured and finished off site and supplied ready to fit and screw together as a flat pack assembly project.\n\nParallel strand lumber (PSL) consists of long veneer strands laid in parallel formation and bonded together with an adhesive to form the finished structural section. A strong, consistent material, it has a high load carrying ability and is resistant to seasoning stresses so it is well suited for use as beams and columns for post and beam construction, and for beams, headers, and lintels for light framing construction. PSL is a member of the structural composite lumber (SCL) family of engineered wood products.\n\nLaminated strand lumber (LSL) and oriented strand lumber (OSL) are manufactured from flaked wood strands that have a high length-to-thickness ratio. Combined with an adhesive, the strands are oriented and formed into a large mat or billet and pressed. LSL and OSL offer good fastener-holding strength and mechanical connector performance and are commonly used in a variety of applications, such as beams, headers, studs, rim boards, and millwork components. These products are members of the structural composite lumber (SCL) family of engineered wood products. LSL is manufactured from relatively short strands—typically about 1 foot long—compared to the 2 foot to 8 foot long strands used in PSL.\n\nThe finger joint is made up of short pieces of wood combined to form longer lengths and is used in doorjambs, mouldings and studs. It is also produced in long lengths and wide dimensions for floors.\n\nI-joists and wood I-beams are \"\"-shaped structural members designed for use in floor and roof construction. An I-joist consists of top and bottom flanges of various widths united with webs of various depths. The flanges resist common bending stresses, and the web provides shear performance. I-joists are designed to carry heavy loads over long distances while using less lumber than a dimensional solid wood joist of a size necessary to do the same task [1]. As of 2005, approximately half of all wood light framed floors were framed using I-joists [2].\n\nRoof trusses and floor trusses are structural frames relying on a triangular arrangement of webs and chords to transfer loads to reaction points. For a given load, long wood trusses built from smaller pieces of lumber require less raw material and make it easier for AC contractors, plumbers, and electricians to do their work, compared to the long 2x10s and 2x12s traditionally used as rafters and floor joists.\n\nTransparent wood composites are new composites made at the laboratory scale that combine transparency and stiffness. They are not available yet on the market.\n\nEngineered wood products are used in a variety of ways, often in applications similar to solid wood products. Engineered wood products may be preferred over solid wood in some applications due to certain comparative advantages:\n\n\nPlywood and OSB typically have a density of 550 - 650 kg/m (35 to 40 pounds per cubic foot). For example, 1 cm (3/8\") plywood sheathing or OSB sheathing typically has a weight of 1 - 1.2 kg/m (1.0 to 1.2 pounds per square foot.). Many other engineered woods have densities much higher than OSB.\n\nThe lamella is the face layer of the wood that is visible when installed. Typically, it is a sawn piece of timber. The timber can be cut in three different styles: flat-sawn, quarter-sawn, and rift-sawn. Keep in mind that each cut will give the board a different final appearance.\n\n\nEngineered wood flooring is mainly industrially fabricated in the form of straight edged boards, with milled jointing profiles to provide for interconnecting of the boards. Such manufacturing is most cost efficient but leaves an industrial looking surface. In nature, no straight lines exist; therefore there is a rising trend to modify the visual appearance to imitate it. In recent years, numerous producers have been taking on the challenge of adding more natural aesthetics.\n\nThe types of adhesives used in engineered wood include:\n\nA more inclusive term is \"structural composites\". For example, fiber cement siding is made of cement and wood fiber, while cement board is a low-density cement panel, often with added resin, faced with fiberglass mesh.\n\nWhile formaldehyde is an essential ingredient of cellular metabolism in mammals, studies have linked prolonged inhalation of formaldehyde gases to cancer. Engineered wood composites have been found to emit potentially harmful amounts of formaldehyde gas in two ways: unreacted free formaldehyde and chemical decomposition of resin adhesives. When exorbitant amounts of formaldehyde are added to a process, the excess will not have any additive to bond with and may seep from the wood product over time. Cheap urea-formaldehyde (UF) adhesives are largely responsible for degraded resin emissions. Moisture degrades the weak UF molecules, resulting in potentially harmful formaldehyde emissions. McLube offers release agents and platen sealers designed for those manufacturers who use reduced-formaldehyde UF and melamine-formaldehyde adhesives. Many oriented strand board (SB) and plywood manufacturers use phenol-formaldehyde (PF) because phenol is a much more effective additive. Phenol forms a water-resistant bond with formaldehyde that will not degrade in moist environments. PF resins have not been found to pose significant health risks due to formaldehyde emissions. While PF is an excellent adhesive, the engineered wood industry has started to shift toward polyurethane binders like pMDI to achieve even greater water-resistance, strength, and process efficiency. pMDIs are also used extensively in the production of rigid polyurethane foams and insulators for refrigeration. pMDIs outperform other resin adhesives, but they are notoriously difficult to release and cause buildup on tooling surfaces.\n\nSome engineered products such as CLT Cross Laminated Timber can be assembled without the use of adhesives using mechanical fixing. These can range from profiled interlocking jointed boards, proprietary metal fixings, nails or timber dowels (Brettstapel - single layer or CLT).\n\nThe following standards are related to engineered wood products:\n\n\n"}
{"id": "233668", "url": "https://en.wikipedia.org/wiki?curid=233668", "title": "Figure of the Earth", "text": "Figure of the Earth\n\nThe figure of the Earth is the size and shape of the Earth in geodesy. Its specific meaning depends on the way it is used and the precision with which the Earth's size and shape is to be defined. While the sphere is a close approximation of the true figure of the Earth and satisfactory for many purposes, geodesists have developed several models that more closely approximate the shape of the Earth so that coordinate systems can serve the precise needs of navigation, surveying, cadastre, land use, and various other concerns.\n\nEarth's topographic surface is apparent with its variety of land forms and water areas. This topographic surface is generally the concern of topographers, hydrographers, and geophysicists. While it is the surface on which Earth measurements are made, mathematically modeling it while taking the irregularities into account would be extremely complicated.\n\nThe Pythagorean concept of a spherical Earth offers a simple surface that is easy to deal with mathematically. Many astronomical and navigational computations use a sphere to model the Earth as a close approximation. However, a more accurate figure is needed for measuring distances and areas on the scale beyond the purely local. Better approximations can be had by modeling the entire surface as an oblate spheroid, using spherical harmonics to approximate the geoid, or modeling a region with a best-fit reference ellipsoids.\n\nFor surveys of small areas, a planar (flat) model of Earth's surface suffices because the local topography overwhelms the curvature. Plane-table surveys are made for relatively small areas without considering the size and shape of the entire Earth. A survey of a city, for example, might be conducted this way.\n\nBy the late 1600s, serious effort was devoted to modeling the earth as an ellipsoid, beginning with Jean Picard's measurement of a degree of arc along the Paris meridian. Improved maps and better measurement of distances and areas of national territories motivated these early attempts. Surveying instrumentation and techniques improved over the ensuing centuries. Models for the figure of the earth improved in step.\n\nIn the mid- to late 20th century, research across the geosciences contributed to drastic improvements in the accuracy of the figure of the Earth. The primary utility of this improved accuracy was to provide geographical and gravitational data for the inertial guidance systems of ballistic missiles. This funding also drove the expansion of geoscientific disciplines, fostering the creation and growth of various geoscience departments at many universities. These develops benefited many civilian pursuits as well, such as weather and communication satellite control and GPS location-finding, which would be impossible without highly accurate models for the figure of the Earth.\n\nThe models for the figure of the Earth vary in the way they are used, in their complexity, and in the accuracy with which they represent the size and shape of the Earth.\n\nThe simplest model for the shape of the entire Earth is a sphere. The Earth's radius is the distance from Earth's center to its surface, about . While \"radius\" normally is a characteristic of perfect spheres, the Earth deviates from spherical by only a third of a percent, sufficiently close to treat it as a sphere in many contexts and justifying the term \"the radius of the Earth\".\n\nThe concept of a spherical Earth dates back to around the 6th century BC, but remained a matter of philosophical speculation until the 3rd century BC. The first scientific estimation of the radius of the earth was given by Eratosthenes about 240 BC, with estimates of the accuracy of Eratosthenes’s measurement ranging from 2% to 15%.\n\nThe Earth is only approximately spherical, so no single value serves as its natural radius. Distances from points on the surface to the center range from 6,353 km to 6,384 km (3,947 – 3,968 mi). Several different ways of modeling the Earth as a sphere each yield a mean radius of . Regardless of the model, any radius falls between the polar minimum of about 6,357 km and the equatorial maximum of about 6,378 km (3,950 – 3,963 mi). The difference correspond to the polar radius being approximately 0.3% shorter than the equator radius.\n\nSince the Earth is flattened at the poles and bulges at the Equator, geodesy represents the figure of the Earth as an oblate spheroid. The oblate spheroid, or oblate ellipsoid, is an ellipsoid of revolution obtained by rotating an ellipse about its shorter axis. It is the regular geometric shape that most nearly approximates the shape of the Earth. A spheroid describing the figure of the Earth or other celestial body is called a reference ellipsoid. The reference ellipsoid for Earth is called an Earth ellipsoid.\n\nAn ellipsoid of revolution is uniquely defined by two quantities. Several conventions for expressing the two quantities are used in geodesy, but they are all equivalent to and convertible with each other:\nEccentricity and flattening are different ways of expressing how squashed the ellipsoid is. When flattening appears as one of the defining quantities in geodesy, generally it is expressed by its reciprocal. For example, in the WGS 84 spheroid used by today's GPS systems, the reciprocal of the flattening formula_7 is set to be exactly .\n\nThe difference between a sphere and a reference ellipsoid for Earth is small, only about one part in 300. Historically, flattening was computed from grade measurements. Nowadays, geodetic networks and satellite geodesy are used. In practice, many reference ellipsoids have been developed over the centuries from different surveys. The flattening value varies slightly from one reference ellipsoid to another, reflecting local conditions and whether the reference ellipsoid is intended to model the entire Earth or only some portion of it.\n\nA sphere has a single radius of curvature, which is simply the radius of the sphere. More complex surfaces have radii of curvature that vary over the surface. The radius of curvature describes the radius of the sphere that best approximates the surface at that point. Oblate ellipsoids have constant radius of curvature east to west along parallels, if a graticule is drawn on the surface, but varying curvature in any other direction. For an oblate ellipsoid, the polar radius of curvature formula_8 is larger than the equatorial\n\nbecause the pole is flattened: the flatter the surface, the larger the sphere must be to approximate it. Conversely, the ellipsoid's north-south radius of curvature at the equator formula_10 is smaller than the polar\n\nwhere formula_1 is the distance from the center of the ellipsoid to the equator (semi-major axis), and formula_2 is the distance from the center to the pole. (semi-minor axis)\n\nThe possibility that the Earth's equator is better characterized as an ellipse rather than a circle and therefore that the ellipsoid is triaxial has been a matter of scientific controversy for many years. Modern technological developments have furnished new and rapid methods for data collection and, since the launch of \"Sputnik 1\", orbital data have been used to investigate the theory of ellipticity.\n\nA second theory, more complicated than triaxiality, proposed that observed long periodic orbital variations of the first Earth satellites indicate an additional depression at the south pole accompanied by a bulge of the same degree at the north pole. It is also contended that the northern middle latitudes were slightly flattened and the southern middle latitudes bulged in a similar amount. This concept suggested a slightly pear-shaped Earth and was the subject of much public discussion. Modern geodesy tends to retain the ellipsoid of revolution as a reference ellipsoid and treat triaxiality and pear shape as a part of the geoid figure: they are represented by the spherical harmonic coefficients formula_14 and formula_15, respectively, corresponding to degree and order numbers 2.2 for the triaxiality and 3.0 for the pear shape.\n\nIt was stated earlier that measurements are made on the apparent or topographic surface of the Earth and it has just been explained that computations are performed on an ellipsoid. One other surface is involved in geodetic measurement: the geoid. In geodetic surveying, the computation of the geodetic coordinates of points is commonly performed on a reference ellipsoid closely approximating the size and shape of the Earth in the area of the survey. The actual measurements made on the surface of the Earth with certain instruments are however referred to the geoid. The ellipsoid is a mathematically defined regular surface with specific dimensions. The geoid, on the other hand, coincides with that surface to which the oceans would conform over the entire Earth if free to adjust to the combined effect of the Earth's mass attraction (gravitation) and the centrifugal force of the Earth's rotation. As a result of the uneven distribution of the Earth's mass, the geoidal surface is irregular and, since the ellipsoid is a regular surface, the separations between the two, referred to as geoid undulations, geoid heights, or geoid separations, will be irregular as well.\n\nThe geoid is a surface along which the gravity potential is everywhere equal and to which the direction of gravity is always perpendicular (see equipotential surface). The latter is particularly important because optical instruments containing gravity-reference leveling devices are commonly used to make geodetic measurements. When properly adjusted, the vertical axis of the instrument coincides with the direction of gravity and is, therefore, perpendicular to the geoid. The angle between the plumb line which is perpendicular to the geoid (sometimes called \"the vertical\") and the perpendicular to the ellipsoid (sometimes called \"the ellipsoidal normal\") is defined as the deflection of the vertical. It has two components: an east-west and a north-south component.\n\nDetermining the exact figure of the Earth is not only a geodetic operation or a task of geometry, but is also related to geophysics. Without any idea of the Earth's interior, we can state a \"constant density\" of 5.515 g/cm and, according to theoretical arguments (see Leonhard Euler, Albert Wangerin, etc.), such a body rotating like the Earth would have a flattening of 1:230.\n\nIn fact, the measured flattening is 1:298.25, which is closer to a sphere and a strong argument that the Earth's core is \"very compact\". Therefore, the density must be a function of the depth, ranging from 2.6 g/cm at the surface (rock density of granite, etc.), up to 13 g/cm within the inner core, see Structure of the Earth.\n\nAlso with implications for the physical exploration of the Earth's interior is the gravitational field, which can be measured very accurately at the surface and remotely by satellites. True vertical generally does not correspond to theoretical vertical (deflection ranges up to 50\") because topography and all \"geological masses\" disturb the gravitational field. Therefore, the gross structure of the earth's crust and mantle can be determined by geodetic-geophysical models of the subsurface.\n\nThe volume of the reference ellipsoid is , where a and b are its semimajor and semiminor axes. Using the parameters from WGS84 ellipsoid of revolution, and , .\n\n\n\n"}
{"id": "44796138", "url": "https://en.wikipedia.org/wiki?curid=44796138", "title": "Flask (unit)", "text": "Flask (unit)\n\nFlask is a British unit of weight in the avoirdupois system, used to measure mercury. It is defined as 76 lb.\n\n1 flask (mercury) ≡ 76 lb\n\n1 flask (mercury) ≡ 34.47302012 kg\n"}
{"id": "47897218", "url": "https://en.wikipedia.org/wiki?curid=47897218", "title": "Foam glass", "text": "Foam glass\n\nFoam glass is a porous glass foam material. Its advantages as a building material include its light weight, high strength and its thermal and acoustic insulating properties. It is made by heating a mixture of crushed or granulated glass and a blowing agent (chemical foaming agent) such as carbon or limestone. Near the melting point of the glass, the blowing agent releases a gas, producing a foaming effect in the glass. After cooling the mixture hardens into a rigid material with gas-filled closed-cell pores comprising a large portion of its volume.\n\nIn the 1930s, Saint-Gobain of France first developed foam glass with calcium carbonate as a foaming agent. In 1935, it applied for the first patent. Subsequently, in 1939, the Soviet Union experimentally produced foam glass at the intermediate pilot plant of the Mendeleev Institute of Chemical Technology.\n\nFoam Glass as Floor Insulation Foamed glass insulation aggregate is used in the same way as coated clay aggregate, but is capable of being used as a load bearing hardcore. It also offers better insulation (lambda/k value = 0.08 – approximately 20% lower thermal conductivity than lightweight expanded clay aggregate). It therefore needs less depth for similar thermal performance.\n\nFoam glass was invented by Corning, Pittsburgh, USA. It is made of cullet, foaming agent, modified additive and foaming accelerator. After fine pulverization and uniform mixing, it is then melted at high temperature, foamed and annealed. Inorganic non-metallic glass material. It consists of a large number of uniform bubble structures with a diameter of 1 to 2 mm. Sound absorbing foam glass is more than 50% open cell bubbles, and heat insulating foam glass is more than 75% closed cell air bubbles, which can be adjusted according to the requirements of use, through changes in production technical parameters.\n\nBecause this new material is moisture proof, fireproof and anti-corrosion, and the glass material has the advantages of long-term use performance, it is in harsh environments such as in heat insulation, deep cooling, underground, open air, flammable, damp and chemical attack. It is favored by users. It is widely used in wall insulation, petroleum, chemical industry, machine room noise reduction, highway sound absorption barrier, electric power, military products, etc., and is called green environmental protection insulation material by users.\n\nDepending on the properties of the foam glass, it can be used as insulation material in various sectors of construction engineering, as well as in shipbuilding, chemical, cryogenic and high temperature technologies. White and stained glass are also used as sound absorbing and decorative materials. Waste in production – foamed glass powder and scrap can also be used as fillers for decorative light concrete and other applications. Depending on the application, foam glass products produced by the corresponding processes can be divided into four categories, namely, insulating foam glass, sound absorbing decorative foam glass, facing foam glass and granular foam glass.\n\n= Property =\nFoam glass is a kind of lightweight, high-strength building materials and decorative materials with excellent performance (insulation), sound absorption, moisture proof and fireproof. The temperature range is from 196 degrees to 450 degrees. The wet coefficient is almost zero. Although other new insulation materials emerge in an endless stream, foam glass occupies an increasingly important position in the fields of low thermal insulation, moisture proof engineering, and sound absorption due to its permanentness, safety and high reliability. Its production is the recycling of waste solid materials, which is an example of protecting the environment and obtaining substantial economic benefits.\n\nLava foam glass: natural lava such as obsidian and industrial waste slag is used as the base material, and a certain amount of glass powder can also be added to reduce the foaming temperature and foam glass made of or the like as a foaming agent. Generally used as insulation materials and wall materials for construction and industrial equipment.\n\n= Application =\nFoam glass is a porous inorganic non-metallic material which is made of waste flat glass and bottle glass and is foamed at high temperature. It is fireproof, waterproof, non-toxic, corrosion-resistant, anti-mite, non-aging, non-radioactive and insulating. Anti-magnetic wave, anti-static, high mechanical strength, good adhesion to various types of mud. It is a stable building exterior wall and roof insulation, sound insulation and waterproof material.\n\nAccording to reports, foam glass can also be used in the heat preservation project of flue, kiln and cold storage, heat insulation, waterproof and fireproof engineering of various gas, liquid and oil transmission pipelines, subway, library, office building, opera house, cinema, etc. Various places that require sound insulation and heat insulation equipment, isolation and sound insulation engineering for infrastructure construction, leak prevention and flood control works for river canals, guardrails, dams, etc. It even has functions for home cleaning and health care. The use of foam glass to protect the heating duct reduces heat loss by approximately 25% compared to conventional protective materials.\n\nFoam glass, also known as porous glass, is filled with numerous open or closed small pores. The area of ​​the pores is 80%~90% of the total volume, and the pore size is 0.5~5mm, and some are as small as a few microns. \n\n= See also =\n\n\nhttps://www.britannica.com/science/foam-glass\nhttp://www.glazette.com/Glass-Knowledge-Bank-96/Foam-Glass.html\n"}
{"id": "23714631", "url": "https://en.wikipedia.org/wiki?curid=23714631", "title": "Folding boxboard", "text": "Folding boxboard\n\nFolding boxboard, also referred to as FBB or by the DIN Standard 19303 codes of GC or UC, is a paperboard grade made up of multiple layers of chemical and mechanical pulp. This grade is made up of mechanical pulp in between two layers of chemical pulp.The top layer is of bleached chemical pulp with an optional pigment coating. This is a low-density material with high stiffness and has a slightly yellow colour, mainly on the inside. The major end uses of folding boxboard are health and beauty products, frozen, chilled and other foods, confectionaries, pharmaceuticals, graphical uses and cigarettes.\n\n"}
{"id": "55257056", "url": "https://en.wikipedia.org/wiki?curid=55257056", "title": "Grunewaldsee", "text": "Grunewaldsee\n\nLake Grunewaldsee is located in the western Bezirk Charlottenburg-Wilmersdorf in Berlin within the Grunewald. It has a surface of c. 175,000 m.\n\nBathing is prohibited for humans since 2004.\n\n\n"}
{"id": "28581970", "url": "https://en.wikipedia.org/wiki?curid=28581970", "title": "Iași II Power Station", "text": "Iași II Power Station\n\nThe Iași II Power Station is a large thermal power plant located in Iași, having 2 generation groups of 50 MW each having a total electricity generation capacity of 100 MW.\n"}
{"id": "6797196", "url": "https://en.wikipedia.org/wiki?curid=6797196", "title": "Institute for Energy and Environmental Research", "text": "Institute for Energy and Environmental Research\n\nThe Institute for Energy and Environmental Research (IEER) focuses on the environmental safety of nuclear weapons production, ozone layer depletion, and other issues relating to energy. IEER publishes a variety of books on energy-related issues, conducts workshops for activists on nuclear issues, and sponsors international symposia and educational outreach projects. IEER was established in 1987 and is based in Takoma Park, Maryland.\n\nArjun Makhijani is President of the Institute for Energy and Environmental Research.\n\n\n"}
{"id": "42566", "url": "https://en.wikipedia.org/wiki?curid=42566", "title": "Ionic crystal", "text": "Ionic crystal\n\nAn ionic crystal is a crystal consisting of ions bound together by their electrostatic attraction. Examples of such crystals are the alkali halides, including potassium fluoride, potassium chloride, potassium bromide, potassium iodide, sodium fluoride, and other combinations of sodium, caesium, rubidium, or lithium ions with fluoride, bromide, chloride or iodide ions.\nNaCl has a 6:6 co-ordination.\nThe properties of NaCl reflect the strong interactions that exist between the ions. It is a good conductor of electricity when molten, but very poor in the solid state. When fused the mobile ions carry charge through the liquid.\nThey are characterized by strong absorption of infrared radiation and have planes along which they cleave easily.\nThe exact arrangement of ions in an ionic lattice varies according to the size of the ions in the solid.\n\n"}
{"id": "19319149", "url": "https://en.wikipedia.org/wiki?curid=19319149", "title": "Kheti Virasat Mission", "text": "Kheti Virasat Mission\n\nKheti Virasat Mission (KVM) is a non-profit, civil society action group founded in 2005. It works in the field of sustainable agriculture, conservation of natural resources, environmental health and eco-sustainable technologies.\n\nKVM is registered as a trust, with head office at Jaitu, Faridkot, Punjab.\n"}
{"id": "37754790", "url": "https://en.wikipedia.org/wiki?curid=37754790", "title": "Land grant to Ḫunnubat-Nanaya kudurru", "text": "Land grant to Ḫunnubat-Nanaya kudurru\n\nThe Land grant to Ḫunnubat-Nanaya kudurru is an ancient Mesopotamian entitlement \"narû\" recording the gift of forty (around a thousand acres) of uncultivated land and control over three settlements by Kassite king Meli-Šipak to his daughter and the provision of exemptions from service and taxation to villages in the region guaranteed with a sealed tablet given to her, presumably to make the land transfer more palatable to the local population. It was excavated by a French archaeological team under the auspices of Jacques de Morgan at the turn of the twentieth century at Susa where it (excavation reference Sb 23) was found with a duplicate (reference Sb 24). It had been taken as booty by Elamite king Šutruk-Naḫḫunte after his 1158 BC campaign that brought about the demise of the regime of Babylonian king Zababa-šuma-iddina, the penultimate monarch of the Kassite dynasty. It is significant in that it shows the king making a second bequest with land \"he purchased\" to provide for his beneficiary, contradicting the earlier view of Kassite feudalism, where all land belonged to the monarch.\n\nThe rounded top, shaped kudurru is covered on three sides by an inscription of Meli–Šipak. On the fourth side a rectangular recessed scene shows the king dressed in a long robe with his right hand raised in a gesture of greeting. With his left hand he grasps the wrist of his daughter. The princess carries in her left hand a nine-stringed harp. Both face an enthroned goddess (Nanaya, a deity worshipped especially at Uruk) who is dressed in a flounced or segmented garment and donning a feathered mitre and sits on the far side of a cultic censer (inscribed: ; Akkadian: \"nignakku\") on a stand. Apart from the area carved in relief, this side of the stele has been entirely defaced, possibly by an Elamite king intending to have his own inscription engraved. Only three divine icons remain, that of a star of Ištar, the crescent moon of Sîn and the sun-disc of Šamaš.\n\nThe surviving parts of the inscription describe an additional three-way real-estate transaction concerning a small orchard (of three ) in the Sealand where the vendor sells his property to the governor of the Sealand, presumably his overlord, who in turn passes it on to the king, thereby relinquishing all claim over its jurisdiction. The main bequest was located in the province of Malgû, on the Tigris, south of its confluence with the Diyala. A later literary work, known as the \"Berlin letter\", provides a historical background where an Elamite king, who may be Šutruk-Naḫḫunte, claimed he married the eldest daughter of Meli–Šipak and this may be the purpose of this legal text, to arrange a substantial dowry for a diplomatic marriage, legitimized with the intercession of the goddess Nanaya. The subject matter of the second and third columns dwells on the provision of a prebend and ritual arrangements for the cult of deity, suggesting an alternative purpose for the bequest, that of elevating Ḫunnubat-Nanaya to a senior priestess position through the largesse of her father.\n\n"}
{"id": "34637866", "url": "https://en.wikipedia.org/wiki?curid=34637866", "title": "Leitwind", "text": "Leitwind\n\nLeitwind (stylized as LEITWIND) is a brand of the worldwide company Leitner AG. With this brand the company produce wind turbines in the megawatt class. The company's headquarter is located in Sterzing (Vipiteno), Italy and is part of the Corporate group HTI (High Technology Innovation).\n\nThe Leitwind story starts with ropeways, a core expertise of Leitwind’s parent company HTI. One of the interesting features of ropeways is the electric direct drive. Its construction has proven its persuasive, safety and reliability even in difficult conditions.\nWith the application of this technology in a wind turbine Leitwind’s first wind power system was built.\nThe company combined the direct drive with a synchronous generator and the first prototype came out, which was installed in 2003 in Malles, South Tyrol (IT). In subsequent test runs the concept proved to be successful respect from simplified assembly and maintenance to operational reliability and profitability.\n\nIn 2007 Leitwind started serial production of onshore wind turbines. By now there are plants on three different continents. Backed by intra-group synergies the company can now offer not only turbines, but also complete wind farm construction as well as operation, maintenance and service packages. Producing in Italy, Austria and India, Leitwind has installed more than 340 wind turbines.\n\n\nWith nominal capacities ranging from 0.8 MW to 3.0 MW and a wide range of rotor diameters and generators Leitwind consults its clients when choosing the best turbine for the climate, wind and terrain conditions of their particular project.\nTurbine types\n\n"}
{"id": "29062023", "url": "https://en.wikipedia.org/wiki?curid=29062023", "title": "List of ancient woods in England", "text": "List of ancient woods in England\n\nThis list of ancient woods in England contains areas of ancient woodland in England larger than ten hectares. The list is arranged alphabetically by ceremonial county.\n\nThe woodlands of Bedfordshire cover 6.2% of the county. Some two thirds of this (4,990ha) is broadleaved woodland, principally oak and ash. A woodland Trust estimate of all ancient woodland in Bedfordshire (dating back to at least the year 1600), including woods of 0.1ha and upward suggests an area of 1468ha. This list of Bedfordshire's ancient woodland shows only those woods of over 10ha, all of which have SSSI status, and cover a total of 628ha. Of the eight woods shown, five fall roughly on the line of heavily wooded sandstone that runs diagonally across the county south of Bedford.\n\nBerkshire has woodland covering 18,304ha, which is 14.5% of its land area. The woodlands listed below are all ancient woods of 10ha or more, and these cover some 2,403ha. A major proportion of the area is the area of woodland along the Surrey and Buckinghamshire borders. This is Windsor Great Park and Forest, and as well as the woodland area listed here, it has vast tracts of heath and parkland. Also in the east of the county are woodlands on the southern end of the Chiltern Hills. The great majority of the woods listed are in West Berkshire and follow the line of the chalk hills across the county.\n\nThere is only one sizeable area of Ancient Woodland within Bristol. The Avon Gorge SSSI is partly within the city boundary, but the woodland is mainly in Somerset, so is covered under that county.\n9.4% of the land area of Buckinghamshire is Woodland.\n\nThe ancient woods listed here are those over 10 ha. With one exception, these are all SSSIs. The woods are distributed very unevenly. Large areas of the fenland in the north-eastern side of the county have none. There are significant numbers in the south, toward Suffolk. More of the woods are found in the western half of the county, with three near Peterborough.\n\nCheshire has some 4% of its area under woodland - around half the national average. Since 1994 the Mersey Community Forest has been promoting new woodland planting within the Merseyside and Cheshire region to alleviate this deficit, and also better manage the existing woodland to secure its future. Cheshire has less ancient woodland, and in smaller units than most counties. Many of the ancient woodlands survive in steep valleys or \"cloughs\", of small extent. Taylor's Rough, Wellmeadow Wood, Warburton's Wood And Well Wood are examples of clough woodland too small for inclusion in this list. Most of the ancient woodland in the county is in units smaller than 10 ha and 65% of the area is in woods smaller than 5 ha. The list below is of ancient woodland larger than 10 ha.\nNo Ancient Woodland remains in the City of London although the City of London Corporation are directly responsible for large areas of woodland elsewhere, notably Epping Forest (Essex), Highgate Wood (Greater London) and Burnham Beeches (Bucks)\n\nThe county of Cornwall has woodland representing 7.5% of the Land Area.\n\n9.5% of the land area of Cumbria is woodland.\n\n\n\n\nBurton Bushes Beverley Westwood\nhttps://www.woodlandtrust.org.uk/visiting-woods/wood/41169/burton-bushes/\n\n16.7% of the land area of East Sussex is woodland.\n\n\n11.2% of the land area of Gloucestershire is woodland.\n\n\n17.7% of the Land Area of Hampshire is woodland.\n\n\n9.5% of Hertfordshire's land area is woodland.\nBirchanger Wood, near Bishop's Stortford\nWhippendell Wood, 160 acres, Watford\n\nIn 2012 the Isle of Wight Biodiversity Partnership commissioned a revised Ancient Woodland Inventory for the island, and this was completed in 2014. This has a list of all identified ancient woodland sites on the Isle of Wight. \n\n10.6% of Kent's land area is wooded.\n\n\nIt is estimated that 2% of Leicestershire's land area is ancient woodland, of which half has been replaced by new plantings in recent times. There are over 100 woods in Leicestershire believed to be ancient. The sites listed below are those over 10ha in size, and with one exception, all have SSSI status. With one group of woods near Hinckley, in the south-west, the remainder fall into three broad areas. In East Leicestershire, close to the border with Rutland, are the woods near Leighfield Forest, an extensive Royal Forest which straddled the two counties. North west of Leicester are the woods of Charnwood Forest. Further west are the woods of the coal measures toward the border with Derbyshire. \n\n\n\n\n\nThe ancient woods of Northants are concentrated towards the south and west of the county, to that region bordering Bucks, Oxford and Beds. Many are managed by the Forestry Commission, although others are in private hands. They tend to occur on limestone soils in elevated country, and exhibit a diversity of habitats.\n\nWhittle Dene\n\n\nThe ancient woods of Oxfordshire are concentrated in three distinct areas. In the south are woods of the Chiltern Hills. A second cluster lies to the east of Oxford. The Cotswolds woods on the western side of the county include those in the Royal Forest of Wychwood. Oxfordshire has nearly 18,000 ha of woodland in total (6.9% of its area), two-thirds of which are in woods of over 10 ha. 1,839 ha of woodland is represented in the 17 ancient woods listed below. Some 6,000 ha of woodland is split among the 3,390 woods smaller than 10 ha. Many of these smaller woods may be ancient, but are not covered by this list. The list here covers woods of over 10 ha with SSSI status.\n\n\n\nSomerset is a rural county of rolling hills such as the Blackdown Hills, Mendip Hills, Quantock Hills and Exmoor National Park, and large flat expanses of land including the Somerset Levels. Many of the woodland areas have been designated as SSSIs with some being managed by the Avon Wildlife Trust or Somerset Wildlife Trust. Woodland covers seven per cent of the land area of the county.\n\n\n\n22.4% of the Land Area of Surrey is woodland this makes it the most wooded county in England.\n\n\n\n\n18.9% of West Sussex's land area is woodland.\n\n\n\n\n\n\n"}
{"id": "867108", "url": "https://en.wikipedia.org/wiki?curid=867108", "title": "Low power", "text": "Low power\n\nIn electronics, low power may mean:\n"}
{"id": "41311936", "url": "https://en.wikipedia.org/wiki?curid=41311936", "title": "Maritime Central Airways Flight 315", "text": "Maritime Central Airways Flight 315\n\nMaritime Central Airways Flight 315 was an international charter flight from London, England to Toronto, Ontario with refueling stops in Reykjavík, Iceland and Goose Bay, Newfoundland. On August 11, 1957, the aircraft operating this flight, a Douglas DC-4, crashed in bad weather near Issoudun, Quebec, killing all 79 people on board. At the time, it was the deadliest aviation accident in Canadian history, and is currently the fifth-deadliest as of 2016.\n\nFlight 315 departed London Heathrow International Airport for Reykjavík at 21:48 GMT. Then, after stopping in Reykjavík for 66 minutes to refuel, it proceeded on the next leg of its route to Canada. After entering Canadian airspace, the flight crew radioed that they wished to bypass Goose Bay and proceed to Montreal instead. At 18:10, Quebec Radio Range Station relayed a message to the aircraft, requesting it to contact Montreal Range while approaching Rougemont for clearance. This was the final communication with the aircraft prior to the accident.\n\nWhile flying in the vicinity of Quebec City at an altitude of approx. 6,000 feet, Flight 315 flew into a cumulonimbus cloud. Encountering severe turbulence, the aircraft somehow lost control and went into a near-vertical dive from which it could not recover. At 18:15 UTC, at a speed of over 200 knots, the aircraft slammed into the ground near Issoudun in a 70-degree nose-down position and a slight left bank angle. All 73 passengers and six crew were killed on impact.\n"}
{"id": "7781606", "url": "https://en.wikipedia.org/wiki?curid=7781606", "title": "Plasma receiver", "text": "Plasma receiver\n\nA plasma receiver is an instrument capable of detecting the vibrations in outer space plasma.\n\nIt may have been Donald Gurnett, University of Iowa Professor of Physics, who invented the Plasma receiver. Gurnett has been intimately involved in the key space probes sent since 1962 (Ijun III, Voyager I and II, Galileo and Cassini–Huygens amongst others).\n\nVibrations in the audible frequency range are perceived by humans when air vibrates against their eardrum. Air, or some other vibrating medium such as water, is indispensable in the perception of sound by the human ear. Without it acting as a transmitter, the sound produced by the source will not be perceived by a human.\nThere is no air in outer space, nor there is any other type of medium capable of transmitting any vibration from a source to a human ear. However, there are sources in outer space that do vibrate at frequencies that would be audible by a human, if only there were some sort of transmitting media to carry those vibrations from the source to a human eardrum.\n\nOne such source, capable of vibrating at audible frequencies (45 to 20,000 vibrations per second) is plasma. Plasma is a collection of charged particles, such as free electrons or ionized gas atoms. Examples of plasma are solar flares, solar wind, neon signs and fluorescent lamps. Plasma interacts with electrical and magnetic fields in ways that can result in vibrations in many frequencies, including the audible range.\n\nIt appears that Gurnett designed the first plasma receiver, an instrument capable of detecting the vibrations in outer space plasma. These interplanetary plasma vibrations can be transformed into sound waves or air vibrations audible to a human ear. NASA provided recordings of these interplanetary and outer space plasma vibrations to composer Terry Riley and Kronos quartet founder David Harrington, which inspired the composition of \"Sun Rings\", a multimedia 85-minute piece for string quartet and choir. \"Sun Rings\" was performed November 3, 2006, at the Veteran's Auditorium, in Providence, Rhode Island.\n\n\n"}
{"id": "1135929", "url": "https://en.wikipedia.org/wiki?curid=1135929", "title": "Pood", "text": "Pood\n\nPood (), is a unit of mass equal to 40 funt (фунт, Russian pound). Plural: pudi or pudy. Since 1899 It is approximately set to 16.38 kilograms (36.11 pounds). It was used in Russia, Belarus, and Ukraine. Pood was first mentioned in a number of 12th-century documents. Unlike \"funt\", which came at least in the 14th century from , \"pud\" (the earlier unattested form *пѫдъ \"pǫdŭ\") is a much older borrowing from which in turn came through the mediation of from \"weight\".\n\nTogether with other units of weight of the Imperial Russian weight measurement system, the USSR officially abolished the pood in 1924. But the term remained in widespread use at least until the 1940s. In his 1953 short story \"Matryona's Place\", Aleksandr Solzhenitsyn presents the pood as still in use amongst the Khrushchev-era Soviet peasants. \n\nIts usage is preserved in modern Russian in certain specific cases, e.g., in reference to sports weights, such as traditional Russian kettlebells, cast in multiples and fractions of 16 kg (which is pood rounded to metric units). For example, a 24 kg kettlebell is commonly referred to as \"one-and-half pood kettlebell\" (\"polutorapudovaya girya\"). It is also sometimes used when reporting the amounts of bulk agricultural production, such as grains or potatoes.\n\nAn old Russian proverb reads, \"You know a man when you have eaten a pood of salt with him.\" () \n\nThe expression \"Сто пудов\" - \"Hundred poods\" means \"very large amount\". In modern colloquial Russian it is used in a generic meanings of \"very much\" and \"very\", as well as \"most surely\"; The adjective 'stopudovy' and the adverb 'stopudovo' are also used in the latter meaning.\n\nAlso used in Polish as idiomatic/proverb (commonly forgotten old original/strict meaning): \"nudy na pudy\" (Polish for: \"unsupportable boredoms\", originally: \"boredoms [that could be measured] in poods\")\n"}
{"id": "4994694", "url": "https://en.wikipedia.org/wiki?curid=4994694", "title": "Predator trap", "text": "Predator trap\n\nA \"predator trap\" is a natural hazard such as a tar pit. Predators are attracted to struggling animals that have become entrapped in viscous or glutinous material, such as a heavy sedimentary deposit or tar and, in the process, become entrapped themselves. More predators, scavengers, insects and birds become attracted to this mounting accumulation of carrion, until a wide variety of animals are caught and ultimately killed by the hazard. This may happen many times over. Typically, the number of lured predators will greatly outnumber the prey, thus providing the name.\n\nA famous example is the La Brea Tar Pits site. The Cleveland-Lloyd Dinosaur Quarry may yet prove to be a further example, although still nowadays it is debated if it's a predator trap or not. The Gobi Desert dinosaur death traps may have been sauropod footprints that have been filled with a mixture of thick mud & sandstone in the former wetland.\n\n"}
{"id": "33336970", "url": "https://en.wikipedia.org/wiki?curid=33336970", "title": "RITM-200", "text": "RITM-200\n\nThe RITM-200 is an integrated pressurized water reactor being developed by OKBM Afrikantov and is designed to produce 55 MWe. It would use up to 20% enriched uranium-235 and will be refueled every 7 years for a 40 year planned lifespan.\n\nThe RITM-200 has a compact integrated layout placing equipment within the steam generator casing, halving system weight compared to earlier designs and improving ability to operate in rolling and pitching seas.\n\nIt powers LK-60Ya-class icebreakers, the first of which is expected to enter service in 2019.\n\n"}
{"id": "28074705", "url": "https://en.wikipedia.org/wiki?curid=28074705", "title": "Ribbon theory", "text": "Ribbon theory\n\nRibbon theory is a strand of mathematics within topology that has seen particular application as regards DNA.\n\n\nWork by Călugăreanu, White and Brock Fuller led to the Călugăreanu–White–Fuller theorem that Link = Writhe + Twist.\n\n\n"}
{"id": "27233172", "url": "https://en.wikipedia.org/wiki?curid=27233172", "title": "Rudas", "text": "Rudas\n\nRudas is a Hungarian word and surname, literally meaning \"of \"rúd\", pole-horse with carriage, wagon\" ( \"pole, rod, beam, (wagon with) drawbar\"):\n\n\n"}
{"id": "25927", "url": "https://en.wikipedia.org/wiki?curid=25927", "title": "Rutherfordium", "text": "Rutherfordium\n\nRutherfordium is a synthetic chemical element with symbol Rf and atomic number 104, named after physicist Ernest Rutherford. As a synthetic element, it is not found in nature and can only be created in a laboratory. It is radioactive; the most stable known isotope, Rf, has a half-life of approximately 1.3 hours.\n\nIn the periodic table of the elements, it is a d-block element and the second of the fourth-row transition elements. It is a member of the 7th period and belongs to the group 4 elements. Chemistry experiments have confirmed that rutherfordium behaves as the heavier homologue to hafnium in group 4. The chemical properties of rutherfordium are characterized only partly. They compare well with the chemistry of the other group 4 elements, even though some calculations had indicated that the element might show significantly different properties due to relativistic effects.\n\nIn the 1960s, small amounts of rutherfordium were produced in the Joint Institute for Nuclear Research in the former Soviet Union and at Lawrence Berkeley National Laboratory in California. The priority of the discovery and therefore the naming of the element was disputed between Soviet and American scientists, and it was not until 1997 that International Union of Pure and Applied Chemistry (IUPAC) established rutherfordium as the official name for the element.\n\nRutherfordium was reportedly first detected in 1964 at the Joint Institute of Nuclear Research at Dubna (then in the Soviet Union). Researchers there bombarded a plutonium-242 target with neon-22 ions and separated the reaction products by gradient thermochromatography after conversion to chlorides by interaction with ZrCl. The team identified spontaneous fission activity contained within a volatile chloride portraying eka-hafnium properties. Although a half-life was not accurately determined, later calculations indicated that the product was most likely rutherfordium-259 (abbreviated as Rf in standard notation):\n\nIn 1969, researchers at the University of California, Berkeley conclusively synthesized the element by bombarding a californium-249 target with carbon-12 ions and measured the alpha decay of Rf, correlated with the daughter decay of nobelium-253:\n\nThe American synthesis was independently confirmed in 1973 and secured the identification of rutherfordium as the parent by the observation of K-alpha X-rays in the elemental signature of the Rf decay product, nobelium-253.\n\nThe Russian scientists proposed the name kurchatovium and the American scientists suggested the name \"rutherfordium\" for the new element. In 1992, the IUPAC/IUPAP Transfermium Working Group (TWG) assessed the claims of discovery and concluded that both teams provided contemporaneous evidence to the synthesis of element 104 and that credit should be shared between the two groups.\n\nThe American group wrote a scathing response to the findings of the TWG, stating that they had given too much emphasis on the results from the Dubna group. In particular they pointed out that the Russian group had altered the details of their claims several times over a period of 20 years, a fact that the Russian team does not deny. They also stressed that the TWG had given too much credence to the chemistry experiments performed by the Russians and accused the TWG of not having appropriately qualified personnel on the committee. The TWG responded by saying that this was not the case and having assessed each point raised by the American group said that they found no reason to alter their conclusion regarding priority of discovery. The IUPAC finally used the name suggested by the American team (\"rutherfordium\") which may in some way reflect a change of opinion.\n\nAs a consequence of the initial competing claims of discovery, an element naming controversy arose. Since the Soviets claimed to have first detected the new element they suggested the name \"kurchatovium\" (Ku) in honor of Igor Kurchatov (1903–1960), former head of Soviet nuclear research. This name had been used in books of the Soviet Bloc as the official name of the element. The Americans, however, proposed \"rutherfordium\" (Rf) for the new element to honor Ernest Rutherford, who is known as the \"father\" of nuclear physics. The International Union of Pure and Applied Chemistry (IUPAC) adopted \"unnilquadium\" (Unq) as a temporary, systematic element name, derived from the Latin names for digits 1, 0, and 4. In 1994, IUPAC suggested the name \"dubnium\" (Db) to be used since \"rutherfordium\" was suggested for element 106 and IUPAC felt that the Dubna team should be rightly recognized for their contributions. However, there was still a dispute over the names of elements 104–107. In 1997 the teams involved resolved the dispute and adopted the current name \"rutherfordium\". The name \"dubnium\" was given to element 105 at the same time.\nRutherfordium has no stable or naturally occurring isotopes. Several radioactive isotopes have been synthesized in the laboratory, either by fusing two atoms or by observing the decay of heavier elements. Sixteen different isotopes have been reported with atomic masses from 253 to 270 (with the exceptions of 264 and 269). Most of these decay predominantly through spontaneous fission pathways.\n\nOut of isotopes whose half-lives are known, the lighter isotopes usually have shorter half-lives; half-lives of under 50 μs for Rf and Rf were observed. Rf, Rf, Rf are more stable at around 10 ms, Rf, Rf, Rf, and Rf live between 1 and 5 seconds, and Rf, Rf, and Rf are more stable, at around 1, 1.5, and 10 min respectively. The heaviest isotopes are the most stable, with Rf having a measured half-life of about 1.3 h. Half-lives for Rf, Rf, and higher are not known and have not yet been predicted.\n\nThe lightest isotopes were synthesized by direct fusion between two lighter nuclei and as decay products. The heaviest isotope produced by direct fusion is Rf; heavier isotopes have only been observed as decay products of elements with larger atomic numbers, of which only Rf has been confirmed. The heavy isotopes Rf and Rf have also been observed as electron capture daughters of the dubnium isotopes Db and Db, but have short half-lives to spontaneous fission: it seems likely that the same is true of Rf, a likely daughter of Db. While the isotope Rf has yet to be observed, it is predicted to have a short half-life of 5 s.\n\nIn 1999, American scientists at the University of California, Berkeley, announced that they had succeeded in synthesizing three atoms of Og. These parent nuclei were reported to have successively emitted seven alpha particles to form Rf nuclei, but their claim was retracted in 2001.\n\nRutherfordium is the first transactinide element and the second member of the 6d series of transition metals. Calculations on its ionization potentials, atomic radius, as well as radii, orbital energies, and ground levels of its ionized states are similar to that of hafnium and very different from that of lead. Therefore, it was concluded that rutherfordium's basic properties will resemble those of other group 4 elements, below titanium, zirconium, and hafnium. Some of its properties were determined by gas-phase experiments and aqueous chemistry. The oxidation state +4 is the only stable state for the latter two elements and therefore rutherfordium should also exhibit a stable +4 state. In addition, rutherfordium is also expected to be able to form a less stable +3 state. The standard reduction potential of the Rf/Rf couple is predicted to be higher than −1.7 V.\n\nInitial predictions of the chemical properties of rutherfordium were based on calculations which indicated that the relativistic effects on the electron shell might be strong enough that the 7p orbitals would have a lower energy level than the 6d orbitals, giving it a valence electron configuration of 6d 7s 7p or even 7s 7p, therefore making the element behave more like lead than hafnium. With better calculation methods and experimental studies of the chemical properties of rutherfordium compounds it could be shown that this does not happen and that rutherfordium instead behaves like the rest of the group 4 elements.\n\nIn an analogous manner to zirconium and hafnium, rutherfordium is projected to form a very stable, refractory oxide, RfO. It reacts with halogens to form tetrahalides, RfX, which hydrolyze on contact with water to form oxyhalides RfOX. The tetrahalides are volatile solids existing as monomeric tetrahedral molecules in the vapor phase.\n\nIn the aqueous phase, the Rf ion hydrolyzes less than titanium(IV) and to a similar extent as zirconium and hafnium, thus resulting in the RfO ion. Treatment of the halides with halide ions promotes the formation of complex ions. The use of chloride and bromide ions produces the hexahalide complexes and . For the fluoride complexes, zirconium and hafnium tend to form hepta- and octa- complexes. Thus, for the larger rutherfordium ion, the complexes , and are possible.\n\nRutherfordium is expected to be a solid under normal conditions and assume a hexagonal close-packed crystal structure (/ = 1.61), similar to its lighter congener hafnium. It should be a very heavy metal with a density of around 23.2 g/cm; in comparison, the densest known element that has had its density measured, osmium, has a density of 22.61 g/cm. This results from rutherfordium's high atomic weight, the lanthanide and actinide contractions, and relativistic effects, although production of enough rutherfordium to measure this quantity would be impractical, and the sample would quickly decay. The atomic radius for rutherfordium is expected to be around 150 pm. Due to the relativistic stabilization of the 7s orbital and destabilization of the 6d orbital, the Rf and Rf ions are predicted to give up 6d electrons instead of 7s electrons, which is the opposite of the behavior of its lighter homologues.\n\nEarly work on the study of the chemistry of rutherfordium focused on gas thermochromatography and measurement of relative deposition temperature adsorption curves. The initial work was carried out at Dubna in an attempt to reaffirm their discovery of the element. Recent work is more reliable regarding the identification of the parent rutherfordium radioisotopes. The isotope Rf has been used for these studies, though the long-lived isotope Rf (produced in the decay chains of Lv, Fl, and Cn) may be advantageous for future experiments. The experiments relied on the expectation that rutherfordium would begin the new 6d series of elements and should therefore form a volatile tetrachloride due to the tetrahedral nature of the molecule. Rutherfordium(IV) chloride is more volatile than its lighter homologue hafnium(IV) chloride (HfCl) because its bonds are more covalent.\n\nA series of experiments confirmed that rutherfordium behaves as a typical member of group 4, forming a tetravalent chloride (RfCl) and bromide (RfBr) as well as an oxychloride (RfOCl). A decreased volatility was observed for when potassium chloride is provided as the solid phase instead of gas, highly indicative of the formation of nonvolatile mixed salt.\n\nRutherfordium is expected to have the electron configuration [Rn]5f 6d 7s and therefore behave as the heavier homologue of hafnium in group 4 of the periodic table. It should therefore readily form a hydrated Rf ion in strong acid solution and should readily form complexes in hydrochloric acid, hydrobromic or hydrofluoric acid solutions.\n\nThe most conclusive aqueous chemistry studies of rutherfordium have been performed by the Japanese team at Japan Atomic Energy Research Institute using the isotope Rf. Extraction experiments from hydrochloric acid solutions using isotopes of rutherfordium, hafnium, zirconium, as well as the pseudo-group 4 element thorium have proved a non-actinide behavior for rutherfordium. A comparison with its lighter homologues placed rutherfordium firmly in group 4 and indicated the formation of a hexachlororutherfordate complex in chloride solutions, in a manner similar to hafnium and zirconium.\n\nVery similar results were observed in hydrofluoric acid solutions. Differences in the extraction curves were interpreted as a weaker affinity for fluoride ion and the formation of the hexafluororutherfordate ion, whereas hafnium and zirconium ions complex seven or eight fluoride ions at the concentrations used:\n\n"}
{"id": "31286905", "url": "https://en.wikipedia.org/wiki?curid=31286905", "title": "Sawdust brandy", "text": "Sawdust brandy\n\nSawdust brandy (from the German \"Holzbranntwein\") is a neutral spirit produced through the distillation of wood products.\n\nTo produce sawdust brandy, the wood is cooked with a diluted sulfuric acid, which causes the cellulose to be broken down via acid hydrolysis into dextrose and other simple sugars, while the lignin remains. The acidic, sugar-containing liquid is neutralised and allowed to ferment, producing the ethanol that will later be purified through distillation.\n\nIn the German Democratic Republic, the lye produced from the wood pulp was employed in the production of the \"Holzschnaps\" (\"wood schnapps\"). Brandy for human consumption (spirits diluted to drinking strength) produced in this method has no additional risks compared to brandy produced from grain; it actually contains fewer fusel oils than traditional distilling methods involving spirits from grain or fruit. In spite of this, German law governing spirit production (overseen by the \") forbids the use of sawdust brandy in the production of alcoholic beverages. This ban stems from the purpose of the law as an agricultural subsidy, rather than out of public health concerns.\n\nPrior to modern, synthetic production methods, wood was also once commonly destructively distilled or dry distilled for the production of methanol (wood alcohol).\n"}
{"id": "36764088", "url": "https://en.wikipedia.org/wiki?curid=36764088", "title": "Shamoon", "text": "Shamoon\n\nShamoon, also known as W32.DisTrack, is a modular computer virus discovered by Seculert in 2012, targeting recent 32-bit NT kernel versions of Microsoft Windows. The virus has been noted to have behavior differing from other malware attacks, due to the destructive nature and the cost of the attack and recovery. Known years later as the \"biggest hack in history,\" the virus was apparently intended for cyber warfare. Shamoon can spread from an infected machine to other computers on the network. Once a system is infected, the virus continues to compile a list of files from specific locations on the system, upload them to the attacker, and erase them. Finally the virus overwrites the master boot record of the infected computer, making it unusable. The virus has been used for cyber warfare against the national oil companies of Saudi Arabia's Saudi Aramco and Qatar's RasGas. Its discovery was announced on 16 August 2012 by Symantec, Kaspersky Lab, and Seculert. Similarities have been highlighted by Kaspersky Lab and Seculert between Shamoon and the Flame malware.\n\nThe malware was unique, used to target the Saudi government by causing destruction to the state owned national oil company Saudi Aramco. The attackers posted a pastie on PasteBin.com hours prior to the wiper logic bomb occurring, citing oppression and the Al-Saud regime as a reason behind the attack. The attack was well-staged, according to Christina Kubecka, a former security advisor to Saudi Aramco after the attack and group leader of security for Aramco Overseas. An unnamed Saudi Aramco employee on the Information Technology team opened a malicious phishing email, allowing initial entry into the computer network around mid-2012.\n\nKubecka also detailed in her Black Hat USA talk Saudi Aramco placed the majority of their security budget on the ICS control network, leaving the business network at risk for a major incident. \"When you realize most of your security budget was spent on ICS & IT gets Pwnd\".\n\n\"We, behalf of an anti-oppression hacker group that have been fed up of crimes and atrocities taking place in various countries around the world, especially in the neighboring countries such as Syria, Bahrain, Yemen, Lebanon, Egypt and ..., and also of dual approach of the world community to these nations, want to hit the main supporters of these disasters by this action. One of the main supporters of this disasters is Al-Saud corrupt regime that sponsors such oppressive measures by using Muslims oil resources. Al-Saud is a partner in committing these crimes. It's hands are infected with the blood of innocent children and people. In the first step, an action was performed against Aramco company, as the largest financial source for Al-Saud regime. In this step, we penetrated a system of Aramco company by using the hacked systems in several countries and then sended a malicious virus to destroy thirty thousand computers networked in this company. The destruction operations began on Wednesday, Aug 15, 2012 at 11:08 AM (Local time in Saudi Arabia) and will be completed within a few hours.\"\nThe Shamoon attack was designed to do two things, replace the data on hard drives with an image of a burning American Flag and report the addresses of infected computers back to the computer inside the company’s network. The virus consisted of three components, the Dropper, the Wiper and the Reporter. The Dropper is the main component and the source of the infection. This component drops the Wiper and the Reporter onto the infected computer, copies itself to network shares, executes itself and creates a service to start itself whenever Windows starts. The Wiper was the destructive component. This component gathers all the files from locations of the infected computers and erases them. It then sends information about the files to the attacker and the erased files are overwritten with corrupted files so they cannot be recovered. The Reporter component sends the infected information back to the attacker.\n\nOn 15 August 2012 at 11:08 am local time, over 30,000 Windows based systems began to be overwritten. Symantec found some of the affected systems had the image of an American flag whilst data was being deleted and overwritten. Saudi Aramco announced the attack on their Facebook page and went offline again until a company statement was issued on 25 August 2012. The statement falsely reported normal business was resumed on 25 August 2012. However a Middle Eastern journalist leaked photographs taken on 1 September 2012 showing kilometers of petrol trucks unable to be loaded due to backed business systems still inoperable. \n\"Saudi Aramco has restored all its main internal network services that were impacted on August 15, 2012, by a malicious virus that originated from external sources and affected about 30,000 workstations. The workstations have since been cleaned and restored to service. As a precaution, remote Internet access to online resources was restricted. Saudi Aramco employees returned to work August 25, 2012, following the Eid holidays, resuming normal business. The company confirmed that its primary enterprise systems of hydrocarbon exploration and production were unaffected as they operate on isolated network systems. Production plants were also fully operational as these control systems are also isolated.\"\n\nOn August 29, 2012 the same attackers behind Shamoon posted another pastie on PasteBin.com, taunting Saudi Aramco with proof they still retained access to the company network. The post contained the username and password on security and network equipment and the new password for Aramco CEO Khalid Al-Falih The attackers also referenced a portion of the Shamoon malware as further proof in the pastie:\n\n- internet service routers are three and their info as follows:\nCore router: SA-AR-CO-1# password (telnet): c1sc0p@ss-ar-cr-tl / (enable): c1sc0p@ss-ar-cr-bl\nBackup router: SA-AR-CO-3# password (telnet): c1sc0p@ss-ar-bk-tl / (enable): c1sc0p@ss-ar-bk-bl\nMiddle router: SA-AR-CO-2# password (telnet): c1sc0p@ss-ar-st-tl / (enable): c1sc0p@ss-ar-st-bl\n- Khalid A. Al-Falih, CEO, email info as follows:\nKhalid.falih@aramco.com password:kal@ram@sa1960\n- security appliances used:\n\nAccording to Kubecka, in order to restore operations, Saudi Aramco used its large private fleet of aircraft and available funds to purchase much of the world's hard drives, driving the price up. New hard drives were required as quickly as possible so oil prices were not affected by speculation. By September 1, 2012 gasoline resources were dwindling for the public of Saudi Arabia 17 days after the August 15th attack. RasGas was also affected by a different variant, crippling them in a similar manner.\n\nIt is unclear why the attacker may have an interest in actually destroying the infected PC. Kaspersky Labs hinted that the 900 KB malware could be related to Wiper, that was used in a cyber attack on Iran in April. After a 2-day analysis, the company erroneously concluded that the malware is more likely to come from \"scriptkiddies\" who were inspired by Wiper. Later, in a blog post, Eugene Kaspersky clarified the use of Shamoon categorizing as cyber warfare.\n\nThe virus has hit companies within the oil and energy sectors. A group named \"Cutting Sword of Justice\" claimed responsibility for an attack on 35,000 Saudi Aramco workstations, causing the company to spend a week restoring their services. The group later indicated that the Shamoon virus had been used in the attack. Computer systems at RasGas were also knocked offline by an unidentified computer virus, with some security experts attributing the damage to Shamoon.\n\nShamoon made a surprise comeback in November 2016 according to Symantec, and it was involved in a new attack on 23 January 2017.\n\nThe malware had a logic bomb which triggered the master boot record and data wiping payload at 11:08am local time on Wednesday, August, 15. The attack occurred during the month of Ramadan in 2012. It would appear that the attack was timed to occur after most staff had gone on holiday reducing the chance of discovery before maximum damage could be caused, hampering recovery.\n\nShamoon uses a number of components to infect computers. The first component is a dropper, which creates a service with the name ‘NtsSrv’ to remain persistent on the infected computer. It spreads across a local network by copying itself on to other computers and will drop additional components to infected computers. The dropper comes in 32-bit and 64-bit versions. If the 32-bit dropper detects a 64-bit architecture, it will drop the 64-bit version. The malware also contains a disk wiping component, which utilizes an Eldos-produced driver known as RawDisk to achieve direct user-mode access to a hard drive without using Windows APIs. The component overwrites files with portions of an image; the 2012 attack used an image of a burning U.S. flag, while the 2016 attack used a photo of the body of Alan Kurdi.\n"}
{"id": "4232526", "url": "https://en.wikipedia.org/wiki?curid=4232526", "title": "Solar tracker", "text": "Solar tracker\n\nA solar tracker is a device that orients a payload toward the Sun. Payloads are usually solar panels, parabolic troughs, fresnel reflectors, lenses or the mirrors of a heliostat.\n\nFor flat-panel photovoltaic systems, trackers are used to minimize the angle of incidence between the incoming sunlight and a photovoltaic panel. This increases the amount of energy produced from a fixed amount of installed power generating capacity. In standard photovoltaic applications, it was predicted in 2008-2009 that trackers could be used in at least 85% of commercial installations greater than one megawatt from 2009 to 2012. However, as of April 2014, there is not any data to support these predictions.\n\nIn concentrator photovoltaics (CPV) and concentrated solar power (CSP) applications, trackers are used to enable the optical components in the CPV and CSP systems. The optics in concentrated solar applications accept the direct component of sunlight light and therefore must be oriented appropriately to collect energy. Tracking systems are found in all concentrator applications because such systems collect the sun's energy with maximum efficiency when the optical axis is aligned with incident solar radiation.\n\nSunlight has two components, the \"direct beam\" that carries about 90% of the solar energy, and the \"diffuse sunlight\" that carries the remainder – the diffuse portion is the blue sky on a clear day, and is a larger proportion of the total on cloudy days. As the majority of the energy is in the direct beam, maximizing collection requires the Sun to be visible to the panels for as long as possible. However, please note that in more cloudy areas the ratio of direct vs. difuse light can be as low as 60%:40% or even lower. \n\nThe energy contributed by the direct beam drops off with the cosine of the angle between the incoming light and the panel. In addition, the reflectance (averaged across all polarizations) is approximately constant for angles of incidence up to around 50°, beyond which reflectance degrades rapidly.\n\nFor example, trackers that have accuracies of ± 5° can deliver greater than 99.6% of the energy delivered by the direct beam plus 100% of the diffuse light. As a result, high accuracy tracking is not typically used in non-concentrating PV applications.\n\nThe purpose of a tracking mechanism is to follow the Sun as it moves across the sky. In the following sections, in which each of the main factors are described in a little more detail, the complex path of the Sun is simplified by considering its daily east-west motion separately from its yearly north-south variation with the seasons of the year.\n\nThe amount of solar energy available for collection from the direct beam is the amount of light intercepted by the panel. This is given by the area of the panel multiplied by the cosine of the angle of incidence of the direct beam (see illustration above). Or put another way, the energy intercepted is equivalent to the area of the shadow cast by the panel onto a surface perpendicular to the direct beam.\n\nThis cosine relationship is very closely related to the observation formalized in 1760 by Lambert's cosine law. This describes that the observed brightness of an object is proportional to the cosine of the angle of incidence of the light illuminating it.\n\nNot all of the light intercepted is transmitted into the panel - a little is reflected at its surface. The amount reflected is influenced by both the refractive index of the surface material and the angle of incidence of the incoming light. The amount reflected also differs depending on the polarization of the incoming light. Incoming sunlight is a mixture of all polarizations. Averaged over all polarizations, the reflective losses are approximately constant up to angles of incidence up to around 50° beyond which it degrades rapidly. See for example the left graph.\n\nThe Sun travels through 360 degrees east to west per day, but from the perspective of any fixed location the visible portion is 180 degrees during an average 1/2 day period (more in spring and summer; less, in fall and winter). Local horizon effects reduce this somewhat, making the effective motion about 150 degrees. A solar panel in a fixed orientation between the dawn and sunset extremes will see a motion of 75 degrees to either side, and thus, according to the table above, will lose over 75% of the energy in the morning and evening. Rotating the panels to the east and west can help recapture those losses. A tracker that only attempts to compensate for the east-west movement of the Sun is known as a single-axis tracker.\n\nDue to the tilt of the Earth's axis, the Sun also moves through 46 degrees north and south during a year. The same set of panels set at the midpoint between the two local extremes will thus see the Sun move 23 degrees on either side. Thus according to the above table, an optimally aligned single-axis tracker (see polar aligned tracker below) will only lose 8.3% at the summer and winter seasonal extremes, or around 5% averaged over a year. Conversely a vertically or horizontally aligned single-axis tracker will lose considerably more as a result of these seasonal variations in the Sun's path. For example, a vertical tracker at a site at 60° latitude will lose up to 40% of the available energy in summer, while a horizontal tracker located at 25° latitude will lose up to 33% in winter.\n\nA tracker that accounts for both the daily and seasonal motions is known as a dual-axis tracker. Generally speaking, the losses due to seasonal angle changes is complicated by changes in the length of the day, increasing collection in the summer in northern or southern latitudes. This biases collection toward the summer, so if the panels are tilted closer to the average summer angles, the total yearly losses are reduced compared to a system tilted at the spring/fall solstice angle (which is the same as the site's latitude).\n\nThere is considerable argument within the industry whether the small difference in yearly collection between single and dual-axis trackers makes the added complexity of a two-axis tracker worthwhile. A recent review of actual production statistics from southern Ontario suggested the difference was about 4% in total, which was far less than the added costs of the dual-axis systems. This compares unfavourably with the 24-32% improvement between a fixed-array and single-axis tracker.\n\nThe above models assume uniform likelihood of cloud cover at different times of day or year. In different climate zones cloud cover can vary with seasons, affecting the averaged performance figures described above. Alternatively, for example in an area where cloud cover on average builds up during the day, there can be particular benefits in collecting morning sun.\n\nThe distance that sunlight has to travel through the atmosphere increases as the sun approaches the horizon, as the sunlight has to travel diagonally through the atmosphere. As the path length through the atmosphere increases, the solar intensity reaching the collector decreases. This increasing path length is referred to as the air mass (AM) or air mass coefficient, where AM0 is at the top of the atmosphere, AM1 refers to the direct vertical path down to sea-level with Sun overhead, and AM greater than 1 refers to diagonal paths as the Sun approaches the horizon.\n\nEven though the sun may not feel particularly hot in the early mornings or during the winter months, the diagonal path through the atmosphere has a less than expected impact on the solar intensity. Even when the sun is only 15° above the horizon the solar intensity can be around 60% of its maximum value, around 50% at 10° and 25% at only 5° above the horizon. Therefore, trackers can deliver benefit by collecting the significant energy available when the Sun is close to the horizontal is this is possible.\n\nOf course the underlying power conversion efficiency of a photovoltaic cell has a major influence on the end result, regardless of whether tracking is employed or not. Of particular relevance to the benefits of tracking are the following:\n\nMuch research is aimed at developing surface materials to guide the maximum amount of energy down into the cell and minimize reflective losses.\n\nPhotovoltaic solar cell efficiency decreases with increasing temperature, at the rate of about 0.4%/°C. For example, 20% higher efficiency at 10 °C in early morning or winter as compared with 60 °C in the heat of the day or summer. Therefore, trackers can deliver additional benefit by collecting early morning and winter energy when the cells are operating at their highest efficiency.\n\nTrackers for concentrating collectors must employ high accuracy tracking so as to keep the collector at the focus point.\n\nTrackers for non-concentrating flat-panel do not need high accuracy tracking:\n\nThe benefits of tracking non-concentrating flat-panel collectors flow from the following:\nthis may used to produced a huge amount of solar excution over the following same\n\nSolar collectors may be:\n\nSolar collector mounting systems may be fixed (manually aligned) or tracking. Different types of solar collector and their location (latitude) require different types of tracking mechanism. Tracking systems may be configured as:\n\nResidential and small-capacity commercial or industrial rooftop solar panels and solar water heater panels are usually fixed, often flush-mounted on an appropriately facing pitched roof. Advantages of fixed mounts over trackers include the following:\n\nFixed mounts are usually used in conjunction with non-concentrating systems, however an important class of non-tracking concentrating collectors, of particular value in the 3rd world, are portable solar cookers. These utilize relatively low levels of concentration, typically around 2 to 8 Suns and are manually aligned.\n\nEven though a fixed flat-panel can be set to collect a high proportion of available noon-time energy, significant power is also available in the early mornings and late afternoons when the misalignment with a fixed panel becomes excessive to collect a reasonable proportion of the available energy. For example, even when the Sun is only 10° above the horizon the available energy can be around half the noon-time energy levels (or even greater depending on latitude, season, and atmospheric conditions).\n\nThus the primary benefit of a tracking system is to collect solar energy for the longest period of the day, and with the most accurate alignment as the Sun's position shifts with the seasons.\n\nIn addition, the greater the level of concentration employed, the more important accurate tracking becomes, because the proportion of energy derived from direct radiation is higher, and the region where that concentrated energy is focused becomes smaller.\n\nMany collectors cannot be moved, for example high-temperature collectors where the energy is recovered as hot liquid or gas (e.g. steam). Other examples include direct heating and lighting of buildings and fixed in-built solar cookers, such as Scheffler reflectors. In such cases it is necessary to employ a moving mirror so that, regardless of where the Sun is positioned in the sky, the Sun's rays are redirected onto the collector.\n\nDue to the complicated motion of the Sun across the sky, and the level of precision required to correctly aim the Sun's rays onto the target, a heliostat mirror generally employs a dual axis tracking system, with at least one axis mechanized. In different applications, mirrors may be flat or concave.\n\nTrackers can be grouped into classes by the number and orientation of the tracker's axes. Compared to a fixed mount, a single axis tracker increases annual output by approximately 30%, and a dual axis tracker an additional 10-20%.\n\nPhotovoltaic trackers can be classified into two types: standard photovoltaic (PV) trackers and concentrated photovoltaic (CPV) trackers. Each of these tracker types can be further categorized by the number and orientation of their axes, their actuation architecture and drive type, their intended applications, their vertical supports and foundation.\n\nSolar trackers can be built using a “floating” foundation, which sits on the ground without the need for invasive concrete foundations. Instead of placing the tracker on concrete foundations, the tracker is placed on a gravel pan that can be filled with a variety of materials, such as sand or gravel, to secure the tracker to the ground. These “floating” trackers can sustain the same wind load as a traditional fixed mounted tracker. The use of floating trackers increases the number of potential sites for commercial solar projects since they can be placed on top of capped landfills or in areas where excavated foundations are not feasible.\n\nPhotovoltaic panels accept both direct and diffuse light from the sky. The panels on standard photovoltaic trackers gather both the available direct and diffuse light. The tracking functionality in standard photovoltaic trackers is used to minimize the angle of incidence between incoming light and the photovoltaic panel. This increases the amount of energy gathered from the direct component of the incoming sunlight.\n\nThe physics behind standard photovoltaic (PV) trackers works with all standard photovoltaic module technologies. These include all types of crystalline silicon panels (either mono-Si, or multi-Si) and all types of thin film panels (amorphous silicon, CdTe, CIGS, microcrystalline).\n\nThe optics in CPV modules accept the direct component of the incoming light and therefore must be oriented appropriately to maximize the energy collected. In low concentration applications a portion of the diffuse light from the sky can also be captured. The tracking functionality in CPV modules is used to orient the optics such that the incoming light is focused to a photovoltaic collector.\n\nCPV modules that concentrate in one dimension must be tracked normal to the Sun in one axis. CPV modules that concentrate in two dimensions must be tracked normal to the Sun in two axes.\n\n\nThe physics behind CPV optics requires that tracking accuracy increase as the systems concentration ratio increases. However, for a given concentration, nonimaging optics provide the widest possible acceptance angles, which may be used to reduce tracking accuracy.\n\nIn typical high concentration systems tracking accuracy must be in the ± 0.1° range to deliver approximately 90% of the rated power output. In low concentration systems, tracking accuracy must be in the ± 2.0° range to deliver 90% of the rated power output. As a result, high accuracy tracking systems are typical.\n\n\nConcentrated photovoltaic trackers are used with refractive and reflective based concentrator systems. There are a range of emerging photovoltaic cell technologies used in these systems. These range from conventional, crystalline silicon-based photovoltaic receivers to germanium-based triple junction receivers.\n\nSingle axis trackers have one degree of freedom that acts as an axis of rotation. The axis of rotation of single axis trackers is typically aligned along a true North meridian. It is possible to align them in any cardinal direction with advanced tracking algorithms. There are several common implementations of single axis trackers. These include horizontal single axis trackers (HSAT), horizontal single axis tracker with tilted modules (HTSAT), vertical single axis trackers (VSAT), tilted single axis trackers (TSAT) and polar aligned single axis trackers (PSAT). The orientation of the module with respect to the tracker axis is important when modeling performance.\n\n\nThe axis of rotation for horizontal single axis tracker is horizontal with respect to the ground.\nThe posts at either end of the axis of rotation of a horizontal single axis tracker can be shared between trackers to lower the installation cost. This type of solar tracker is most appropriate for low latitude regions. Field layouts with horizontal single axis trackers are very flexible. The simple geometry means that keeping all of the axes of rotation parallel to one another is all that is required for appropriately positioning the trackers with respect to one another. Appropriate spacing can maximize the ratio of energy production to cost, this being dependent upon local terrain and shading conditions and the time-of-day value of the energy produced. Backtracking is one means of computing the disposition of panels. Horizontal trackers typically have the face of the module oriented parallel to the axis of rotation. As a module tracks, it sweeps a cylinder that is rotationally symmetric around the axis of rotation. In single axis horizontal trackers, a long horizontal tube is supported on bearings mounted upon pylons or frames. The axis of the tube is on a north–south line. Panels are mounted upon the tube, and the tube will rotate on its axis to track the apparent motion of the Sun through the day.\n\n\nIn HSAT, the modules are mounted flat at 0 degrees, while in HTSAT, the modules are installed at a certain tilt. It works on same principle as HSAT, keeping the axis of tube horizontal in north–south line and rotates the solar modules east to west throughout the day. These trackers are usually suitable in high latitude locations but does not take as much land space as consumed by Vertical single axis tracker (VSAT). Therefore, it brings the advantages of VSAT in a horizontal tracker and minimizes the overall cost of solar project.\n\n\nThe axis of rotation for vertical single axis trackers is vertical with respect to the ground. These trackers rotate from East to West over the course of the day. Such trackers are more effective at high latitudes than are horizontal axis trackers. Field layouts must consider shading to avoid unnecessary energy losses and to optimize land utilization. Also optimization for dense packing is limited due to the nature of the shading over the course of a year. Vertical single axis trackers typically have the face of the module oriented at an angle with respect to the axis of rotation. As a module tracks, it sweeps a cone that is rotationally symmetric around the axis of rotation.\n\nAll trackers with axes of rotation between horizontal and vertical are considered tilted single axis trackers. Tracker tilt angles are often limited to reduce the wind profile and decrease the elevated end height. With backtracking, they can be packed without shading perpendicular to their axis of rotation at any density. However, the packing parallel to their axes of rotation is limited by the tilt angle and the latitude. Tilted single axis trackers typically have the face of the module oriented parallel to the axis of rotation. As a module tracks, it sweeps a cylinder that is rotationally symmetric around the axis of rotation.\n\nDual axis trackers have two degrees of freedom that act as axes of rotation. These axes are typically normal to one another. The axis that is fixed with respect to the ground can be considered a primary axis. The axis that is referenced to the primary axis can be considered a secondary axis. There are several common implementations of dual axis trackers. They are classified by the orientation of their primary axes with respect to the ground. Two common implementations are tip-tilt dual axis trackers (TTDAT) and azimuth-altitude dual axis trackers (AADAT). The orientation of the module with respect to the tracker axis is important when modeling performance. Dual axis trackers typically have modules oriented parallel to the secondary axis of rotation. Dual axis trackers allow for optimum solar energy levels due to their ability to follow the Sun vertically and horizontally. No matter where the Sun is in the sky, dual axis trackers are able to angle themselves to be in direct contact with the Sun.\n\nA tip–tilt dual axis tracker (TTDAT) is so-named because the panel array is mounted on the top of a pole. Normally the east–west movement is driven by rotating the array around the top of the pole. On top of the rotating bearing is a T- or H-shaped mechanism that provides vertical rotation of the panels and provides the main mounting points for the array. The posts at either end of the primary axis of rotation of a tip–tilt dual axis tracker can be shared between trackers to lower installation costs.\n\nOther such TTDAT trackers have a horizontal primary axis and a dependent orthogonal axis. The vertical azimuthal axis is fixed. This allows for great flexibility of the payload connection to the ground mounted equipment because there is no twisting of the cabling around the pole.\n\nField layouts with tip–tilt dual axis trackers are very flexible. The simple geometry means that keeping the axes of rotation parallel to one another is all that is required for appropriately positioning the trackers with respect to one another. Normally the trackers would have to be positioned at fairly low density in order to avoid one tracker casting a shadow on others when the Sun is low in the sky. Tip-tilt trackers can make up for this by tilting closer to horizontal to minimize up-Sun shading and therefore maximize the total power being collected.\n\nThe axes of rotation of many tip–tilt dual axis trackers are typically aligned either along a true north meridian or an east–west line of latitude.\n\nGiven the unique capabilities of the Tip-Tilt configuration and the appropriate controller, totally automatic tracking is possible for use on portable platforms. The orientation of the tracker is of no importance and can be placed as needed.\n\nAn azimuth–altitude (or alt-azimuth) dual axis tracker (AADAT) has its primary axis (the azimuth axis) vertical to the ground. The secondary axis, often called elevation axis, is then typically normal to the primary axis. They are similar to tip-tilt systems in operation, but they differ in the way the array is rotated for daily tracking. Instead of rotating the array around the top of the pole, AADAT systems can use a large ring mounted on the ground with the array mounted on a series of rollers. The main advantage of this arrangement is the weight of the array is distributed over a portion of the ring, as opposed to the single loading point of the pole in the TTDAT. This allows AADAT to support much larger arrays. Unlike the TTDAT, however, the AADAT system cannot be placed closer together than the diameter of the ring, which may reduce the system density, especially considering inter-tracker shading.\n\nAs described later, the economical balance between cost of panel and tracker is not trivial. The steep drop in cost for solar panels in the early 2010s made it more challenging to find a sensible solution. As can be seen in the attached media files, most constructions use industrial and/or heavy materials unsuitable for small or craft workshops. Even commercial offers like \"Complete-Kit-1KW-Single-Axis-Solar-Panel-Tracking-System-Linear-Actuator-Electric-Controller-For-Sunlight-Solar/1279440_2037007138\" have rather unsuitable solutions (a big rock) for stabilisation. For a small(amateur/enthusiast) construction following criteria have to be met: economy, stability of endproduct against elemental hazards, ease of handling materials and joinery.\n\nThe selection of tracker type is on many factors including installation size, electric rates, government incentives, land constraints, latitude, and local weather.\n\nHorizontal single axis trackers are typically used for large distributed generation projects and utility scale projects. The combination of energy improvement and lower product cost and lower installation complexity results in compelling economics in large deployments. In addition the strong afternoon performance is particularly desirable for large grid-tied photovoltaic systems so that production will match the peak demand time. Horizontal single axis trackers also add a substantial amount of productivity during the spring and summer seasons when the Sun is high in the sky. The inherent robustness of their supporting structure and the simplicity of the mechanism also result in high reliability which keeps maintenance costs low. Since the panels are horizontal, they can be compactly placed on the axle tube without danger of self-shading and are also readily accessible for cleaning.\n\nA vertical axis tracker pivots only about a vertical axle, with the panels either vertical, at a fixed, adjustable, or tracked elevation angle. Such trackers with fixed or (seasonally) adjustable angles are suitable for high latitudes, where the apparent solar path is not especially high, but which leads to long days in summer, with the Sun traveling through a long arc.\n\nDual axis trackers are typically used in smaller residential installations and locations with very high government feed in tariffs.\n\nThis device uses multiple mirrors in a horizontal plane to reflect sunlight upward to a high temperature photovoltaic or other system requiring concentrated solar power. Structural problems and expense are greatly reduced since the mirrors are not significantly exposed to wind loads. Through the employment of a patented mechanism, only two drive systems are required for each device. Because of the configuration of the device it is especially suited for use on flat roofs and at lower latitudes. The units illustrated each produce approximately 200 peak DC watts.\n\nA multiple mirror reflective system combined with a central power tower is employed at the Sierra SunTower, located in Lancaster, California. This generation plant operated by eSolar is scheduled to begin operations on August 5, 2009. This system, which uses multiple heliostats in a north–south alignment, uses pre-fabricated parts and construction as a way of decreasing startup and operating costs.\n\nActive trackers use motors and gear trains to perform solar tracking. They can use microprocessors and sensors, date and time-based algorithms, or a combination of both to detect the position of the sun. In order to control and manage the movement of these massive structures special slewing drives are designed and rigorously tested. The technologies used to direct the tracker are constantly evolving and recent developments at Google and Eternegy have included the use of wire-ropes and winches to replace some of the more costly and more fragile components.\n\nCounter rotating slewing drives sandwiching a fixed angle support can be applied to create a \"multi-axis\" tracking method which eliminates rotation relative to longitudinal alignment. This method if placed on a column or pillar will generate more electricity than fixed PV and its PV array will never rotate into a parking lot drive lane. It will also allow for maximum solar generation in virtually any parking lot lane/row orientation, including circular or curvilinear.\n\nActive two-axis trackers are also used to orient heliostats - movable mirrors that reflect sunlight toward the absorber of a central power station. As each mirror in a large field will have an individual orientation these are controlled programmatically through a central computer system, which also allows the system to be shut down when necessary.\n\nLight-sensing trackers typically have two or more photosensors, such as photodiodes, configured differentially so that they output a null when receiving the same light flux. Mechanically, they should be omnidirectional (i.e. flat) and are aimed 90 degrees apart. This will cause the steepest part of their cosine transfer functions to balance at the steepest part, which translates into maximum sensitivity. For more information about controllers see active daylighting.\n\nSince the motors consume energy, one wants to use them only as necessary. So instead of a continuous motion, the heliostat is moved in discrete steps. Also, if the light is below some threshold there would not be enough power generated to warrant reorientation. This is also true when there is not enough difference in light level from one direction to another, such as when clouds are passing overhead. Consideration must be made to keep the tracker from wasting energy during cloudy periods.\n\nThe most common Passive trackers use a low boiling point compressed gas fluid that is driven to one side or the other (by solar heat creating gas pressure) to cause the tracker to move in response to an imbalance. As this is a non-precision orientation it is unsuitable for certain types of concentrating photovoltaic collectors but works fine for common PV panel types. These will have viscous dampers to prevent excessive motion in response to wind gusts. Shader/reflectors are used to reflect early morning sunlight to \"wake up\" the panel and tilt it toward the Sun, which can take nearly an hour. The time to do this can be greatly reduced by adding a self-releasing tiedown that positions the panel slightly past the zenith (so that the fluid does not have to overcome gravity) and using the tiedown in the evening. (A slack-pulling spring will prevent release in windy overnight conditions.)\n\nA newly emerging type of passive tracker for photovoltaic solar panels uses a hologram behind stripes of photovoltaic cells so that sunlight passes through the transparent part of the module and reflects on the hologram. This allows sunlight to hit the cell from behind, thereby increasing the module's efficiency. Also, the panel does not have to move since the hologram always reflects sunlight from the correct angle towards the cells.\n\nIn some developing nations, drives have been replaced by operators who adjust the trackers. This has the benefits of robustness, having staff available for maintenance and creating employment for the population in the vicinity of the site.\n\nIn Freiburg im Breisgau, Germany, Rolf Disch built the Heliotrop in 1996, a residential building that is rotating with the sun and has an additional dual axis photovoltaic sail on the roof. It's producing four times the amount of energy the building consumes.\n\nThe Gemini House is a unique example of a vertical axis tracker. This cylindrical house in Austria (latitude above 45 degrees north) rotates in its entirety to track the Sun, with vertical solar panels mounted on one side of the building, rotating independently, allowing control of the natural heating from the Sun.\n\n\"ReVolt House\" is a rotating, floating house designed by TU Delft students for the Solar Decathlon Europe competition in Madrid. The house would be realized in September 2012. A closed façade turns itself towards the Sun in summer to prevent the interior space from direct heat gains. In winter, the glass façade faces the Sun to get direct sunlight in the house.\n\nTrackers add cost and maintenance to the system - if they add 25% to the cost, and improve the output by 25%, the same performance can be obtained by making the system 25% larger, eliminating the additional maintenance. Tracking was very cost effective in the past when photovoltaic modules were expensive compared to today. Because they were expensive, it was important to use tracking to minimize the number of panels used in a system with a given power output. But as panels get cheaper, the cost effectiveness of tracking vs using a greater number of panels decreases.\n\nTracking is also not suitable for typical residential rooftop photovoltaic installations. Since tracking requires that panels tilt or otherwise move, provisions must be made to allow this. This requires that panels be offset a significant distance from the roof, which requires expensive racking and increases wind load. Also, such a setup would not make for a very aesthetically pleasing install on residential rooftops. Because of this (and the high cost of such a system), tracking is not used on residential rooftop installations, and is unlikely to ever be used in such installations. This is especially true as the cost of photovoltaic modules continues to decrease, which makes increasing the number of modules for more power the more cost-effective option. Tracking can (and sometimes is) used for residential ground mount installations, where greater freedom of movement is possible.\n\nTracking can also cause shading problems. As the panels move during the course of the day, it is possible that, if the panels are located too close to one another, they may shade one another due to profile angle effects. As an example, if you have several panels in a row from east to west, there will be no shading during solar noon. But in the afternoon, panels could be shaded by their west neighboring panel if they are sufficiently close. This means that panels must be spaced sufficiently far to prevent shading in systems with tracking, which can reduce the available power from a given area during the peak Sun hours. This is not a big problem if there is sufficient land area to widely space the panels. But it will reduce output during certain hours of the day (i.e. around solar noon) compared to a fixed array.\n\nFurther, single-axis tracking systems are prone to become unstable already at relatively modest wind speeds (galloping). This is due to torsional instability of single-axis solar tracking systems. Anti-galloping measures such as automatic stowing and external dampers must be implemented. For more see this paper.\n\n\nOptimum Trackers builds innovative solar trackers that increases solar plant production up to 27% compared with a fixed-tilt installation :\n"}
{"id": "313591", "url": "https://en.wikipedia.org/wiki?curid=313591", "title": "Spermaceti", "text": "Spermaceti\n\nSpermaceti (from Greek \"sperma\" meaning \"seed\", and \"ceti\", the genitive form of \"whale\") is a waxy substance found in the head cavities of the sperm whale (and, in smaller quantities, in the oils of other whales). Spermaceti is created in the spermaceti organ inside the whale's head. This organ may contain as much as of spermaceti.\n\nTwo theories for the spermaceti organ's biological function suggest it either controls buoyancy, or acts as a focusing apparatus for the whale's sense of echolocation. There has been concrete evidence to support both theories. The buoyancy theory holds that the sperm whale is capable of heating the spermaceti, lowering its density and thus allowing the whale to float; in order for the whale to sink again, it must take water into its blowhole which cools the spermaceti into a denser solid. This claim has been called into question by recent research which indicates a lack of biological structures to support this heat exchange, as well as the fact that the change in density is too small to be meaningful until the organ grows to huge size.\n\nThe proportion of wax esters in the spermaceti organ increases with the age of the whale: 38–51% in calves, 58–87% in adult females, and 71–94% in adult males. Spermaceti wax is extracted from sperm oil by crystallisation at , when treated by pressure and a chemical solution of caustic alkali. Spermaceti forms brilliant white crystals that are hard but oily to the touch, and are devoid of taste or smell, making it very useful as an ingredient in cosmetics, leatherworking, and lubricants. The substance was also used in making candles of a standard photometric value, in the dressing of fabrics, and as a pharmaceutical excipient, especially in cerates and ointments. Candlepower, a photometric unit defined in the United Kingdom Act of Parliament Metropolitan Gas Act 1860 and adopted at the International Electrotechnical Conference of 1883, was based on the light produced by a pure spermaceti candle.\n\nRaw spermaceti is liquid within the head of the sperm whale, and is said to have a smell similar to raw milk. It is composed mostly of wax esters (chiefly cetyl palmitate) and a smaller proportion of triglycerides. Unlike other toothed whales, most of the carbon chains in the wax esters are relatively long (C–C). The proportion of wax esters in the spermaceti organ increases with the age of the whale: 38–51% in calves, 58–87% in adult females, and 71–94% in adult males. The blubber oil of the whale is about 66% wax. When it cools to 30 °C or below, the waxes begin to solidify. The speed of sound in spermaceti is 2,684 m/s (at 40 kHz, 36 °C), making it nearly twice as good a conductor of sounds as the oil in a dolphin's melon.\n\nSpermaceti is insoluble in water, very slightly soluble in cold ethanol, but easily dissolved in ether, chloroform, carbon disulfide, and boiling ethanol. Spermaceti consists principally of cetyl palmitate (the ester of cetyl alcohol and palmitic acid), CHCOO-CH.\n\nA botanical alternative to spermaceti is a derivative of jojoba oil, jojoba esters, CHCOO-CH, a solid wax which is chemically and physically very similar to spermaceti and may be used in many of the same applications.\n\nCurrently there is disagreement on what biological purpose or purposes spermaceti serves. It might be used as a means of altering the whale's buoyancy, since the density of the spermaceti alters with its phase. Another hypothesis has been that it is used as a cushion to protect the sperm whale's delicate snout while diving.\n\nThe most likely primary function of the spermaceti organ is to add internal echo or resonator clicks to the sonar echo location clicks emitted by the respiratory organs. This makes it possible for the whale to sense the motion of its prey as well as its position. The changing distance to the prey affects the time interval between the returning clicks reflected by the prey (doppler effect). This would explain the low density and high compressibility of the spermaceti, which enhance the resonance by the contrast of the acoustic properties of the sea water and of the hard tissue surrounding the spermaceti.\n\nAfter killing a sperm whale, the whalers would pull the carcass alongside the ship, cut off the head and pull it on deck, whereupon they would cut a hole in it and bail out the matter inside with a bucket. The harvested matter, raw spermaceti, was stored in casks to be processed back on land. A large whale could yield as much as 500 gallons. The spermaceti was boiled and strained of impurities to prevent it from going rancid. On land, the casks were allowed to chill during the winter, causing the spermaceti to congeal into a spongy and viscous mass. The congealed matter was then loaded into wool sacks and placed in a press to squeeze out the liquid. This liquid was bottled and sold as \"winter-strained sperm oil\". This was the most valuable product: an oil that remained liquid in freezing winter temperatures.\n\nLater, during the warmer seasons, the leftover solid was allowed to partially melt, and the liquid was strained off to leave a fully solid wax. This wax, brown in color, was then bleached and sold as \"spermaceti wax\". Spermaceti wax is white and translucent. It melts at about and congeals at .\n\n\n"}
{"id": "39620647", "url": "https://en.wikipedia.org/wiki?curid=39620647", "title": "Surface chemistry of paper", "text": "Surface chemistry of paper\n\nThe surface chemistry of paper is responsible for many important paper properties, such as gloss, waterproofing, and printability. Many components are used in the paper-making process that affect the surface.\n\nCoating components are subject to particle-particle, particle-solvent, and particle-polymer interactions. Van der Waals forces, electrostatic repulsions, and steric stabilization are the reasons for these interactions. Importantly, the characteristics of adhesion and cohesion between the components form the base coating structure. Calcium carbonate and kaolin are commonly used pigments. Pigments support a structure of fine porosity and form a light scattering surface. The surface charge of the pigment plays an important role in dispersion consistency. The surface charge of calcium carbonate is negative and not dependent on pH, however it can decompose under acidic conditions. Kaolin has negatively charged faces while the charge of its laterals depend on pH, being positive in acidic conditions and negative in basic conditions with an isoelectric point at 7.5. The equation for determining the isoelectric point is as follows:\nwhere formula_2 is the interfacial tension between the solid and the liquid, formula_3 is the interfacial tension between the liquid and the vapor, and formula_4 is the interfacial tension between the solid and the vapor.\n\nAn ideal oleophilic surface would have a contact angle of 0° with oil, therefore allowing the ink to transfer to the paper and be absorbed. The hydrocarbon plasma coating provides an oleophilic surface to the paper by lowering the contact angle of the paper with the oil in the ink. \nThe hydrocarbon plasma coating increases the non-polar interactions while decreasing polar interactions which allow paper to absorb ink while preventing dampening water absorption.\n\nPrinting quality is highly influenced by the various treatments and methods used in creating paper and enhancing the paper surface. Consumers are most concerned with the paper-ink interactions which vary for certain types of paper due to different chemical properties of the surface. Inkjet paper is the most commercially used type of paper. Filter paper is another key type of paper whose surface chemistry affects its various forms and uses. The ability of adhesives to bond to a paper surface is also affected by the surface chemistry.\n\nCo-styrene-maleic anhydride and co-styrene acrylate are common binders associated with a cationic starch pigment in Inkjet printing paper. Table 1 shows their surface tension under given conditions.\n\nThere have been several studies that have focused on how the paper printing quality is dependent on the concentration of these binders and ink pigment. Data from the experiments are congruent and stated in Table 2 as the corrected contact angle of water, the corrected contact angle of black ink, and the total surface energy.\nThe contact angle measurement has proven to be a very useful tool to evaluate the influence of the sizing formulation on the printing properties. Surface free energy has also shown to be very valuable in explaining the differences in sample behavior.\n\nVarious composite coatings were analyzed on filter paper in an experiment done by Wang et al. The ability to separate homogenous liquid solutions based on varying surface tensions has great practical use. Creating superhydrophobic and superoleophilic filter paper was achieved by treating the surface of commercially available filter paper with hydrophobic silica nanoparticles and polystyrene solution in toluene. Oil and water were successfully separated through the use of the filter paper created with an efficiency greater than 96%. In a homogenous solution the filter paper was also successful in separating the liquids through differentiating for surface tensions. Although with a lower efficiency, aqueous ethanol was also extraced from the solution when tested on the filter paper.\n"}
{"id": "42798343", "url": "https://en.wikipedia.org/wiki?curid=42798343", "title": "Sustainable Energy Utility", "text": "Sustainable Energy Utility\n\nSustainable Energy Utility (SEU) is a community-based model of development based on energy conservation and the use of renewables, seeking to permanently decrease the use of source materials, water, and energy. The model prescribes the creation of independent and financially self-sufficient non-profit entities for energy sustainability through conservation, efficiency, and end-user based decentralized renewable energy in an effort to address concerns about climate change, rising energy prices, inequity of energy availability, and a lack of community governance of energy development. The SEU model was developed by Dr. J. Byrne at the Center for Energy and Environmental Policy, University of Delaware. The Foundation for Renewable Energy and Environment (FREE) is implementing versions of the model.\n\nIn the U.S., the SEU model was first implemented by the State of Delaware, followed by the District of Columbia, Sonoma County in California, the California Statewide Communities Development Authority (CSCDA), and the Pennsylvania Sustainable Energy Finance program. which is a partnership initiative between Pennsylvania Treasury and FREE. The SEU model is recognized by the U.S. White House, the Asian Development Bank, and the International Energy Agency as a viable platform to spur sustainable energy investment while driving local economic development. \n\nThe SEU model was a result of efforts by Dr. J. Byrne to realize a model of energy-environment-society relations which could reflect Amory Lovins’ promise of the negawatt and the philosophical tenets of Amulya K. N. Reddy’s DEFENDUS. While working as an author with the Intergovernmental Panel on Climate Change, Byrne was influenced by the philosophical framework of the discourse on political ecology and environmental justice. He and his team worked on calculating the amount of greenhouse gas emissions which eliminate climate change, leading to an energy-sustainable future. Their study concluded that a target of 3.3 tons of emissions per capita per year would be sustainable emissions. In response to this finding, Byrne’s team worked on designing an energy system model that would use a commonwealth economy and community trusts to achieve this goal; the result was the SEU. \n\nThe SEU aims to achieve a four-fold paradigm shift: \n\n\nTo achieve this paradigm shift, the model is guided by three main principles: establishing civil society based energy governance, increasing reliance on savings and environmental benefits of SEU investments to build out a sustainable energy future, and continued evaluation of performance determined by environmental factors, affordability, and local economic impact.\n\nAn SEU departs from the traditional model of energy supply and expansion, as well as the traditional efficiency models for power plants and utilities. An SEU is focused on permanently lowering overall energy use and limiting supply to renewable energy sources. Rather than slowing the rate of energy market expansion or improving the efficiency of energy services, the SEU cuts energy requirements based on sustainability defined constraints; notably, the need to adhere to an annual 3.3 ton per capita emission budget for greenhouse gases released, expressed in a CO2 equivalent.\n\nAn SEU functions as a 'community utility' directly accountable to the local community it serves as it seeks to deliver sustainable energy services. SEUs do not report to stockholders or utility regulators. SEUs can be organized by communities of almost any scale (towns, cities, or regions) seeking to gain independence and agency in their energy development pathway. As a community utility, the success of the SEU rests on the participation of local stakeholders, namely individuals, businesses, farms, localities, etc., and is directly answerable to these entities. The SEU itself remains independent.\n\nThe Sustainable Energy Bond (SEB) Program was pioneered by Dr. Byrne to build a clean energy infrastructure from guaranteed savings earned by participants. The SEU uses bonds at a scale which allows a city or a region to treat conserved and renewable energy as primary sources rather than the current situation in which fossil and nuclear energy sources dominate. Historically, tax-exempt bonds were used to underwrite investments in public goods and services and, for this reason, SEBs are seen as a critical tool for SEUs to serve their communities. An SEU can be given bond issuing capacity which allows it to sell tax-exempt bonds in order to treat sustainable energy as an infrastructure scale investment.\n\nIn 2011, the Delaware SEU issued a statewide tax exempt bond for SEUs, the first of its kind in the U.S., acquiring $72.5 million for capital investments in sustainable energy measures. Targeting about 4% of Delaware's total state owned or managed building stock, the SEU bond issue included contractual guarantees of $148 million in savings which cut energy use in participating buildings by more than 25% for 20 years. The 2011 bond issue average payback period was almost 14 years and the longest maturity was 20 years, while average performance guarantees were greater than 20 years. \n\nSEUs present a different paradigm in energy governance, and they can present unique challenges. One of the greatest challenges SEUs face is empowering communities to break away from the existing paradigm of top-down energy supply. Sometimes local initiatives have wide support at the outset, but as time passes, active participation is limited to a smaller core group. Adding to this situation is the problem of limited resources at the local level, making it challenging for the SEU movement to institutionalize and maintain momentum.\n\nAnother area of concern is solvency: financial sustainability of the initiative is vital to its long term success, and the appropriation and allocation of funding can determine the longevity of the SEU itself. Depending on its organizational structure and funding source, the SEU can encounter problems when some of its dedicated funds are re-allocated to fill general obligation gaps in, for instance, the state budget. An example of a sustainable energy focused organization encountering such a difficulty is the New Jersey Clean Energy Program, a third-party demand side management administrator operating in New Jersey. Sustaining a long-term program that seeks to implement transformative change further can encounter difficulties when operated in parallel with shorter-term projects. For example, shorter-term projects can capitalize on \"low-hanging fruit\" with quick returns, limiting funding allocation to multi-year projects. Support from local government and the possibility of collaborating with other available government programs can substantially increase success.\n\nPractical operation of SEU models encountered some of these challenges. For instance, in 2016, after an investigation into one of the Delaware SEU projects, the Delaware State Auditor issued a report indicating several problems with the Delaware SEU 2011 bond program. A former Delaware State Senator published an opinion piece on Delaware Online in support of the State Auditor report. A presiding Senator, a member of the Delaware SEU's oversight board, disagreed with the findings of the Auditor, calling them \"mystifying\". Delaware SEU Executive Director defended the program by arguing that the State Auditor did not counsel with experts in the field of energy engineering and, as such, misrepresented the actual workings of the program. . The Delaware Office of Management and Budget similarly questioned the validity of the Auditor's report. In 2018, the Delaware SEU was named Environmental Protection Agency (EPA) 2018 Energy STAR Partner of the Year and received the EPA Energy STAR Excellence Award. The Delaware SEU has \"Standards for Excellence\" accreditation by the Standards for Excellence Institute for its ethics, accountability, and transparency. Early evaluation of the 2011 bond program shows first-year savings exceeded the 25% savings guarantee by 3%. The Delaware SEU is registered as a non-profit, tax-exempt 501 (c)(3) entity. The Delaware SEU is operating and planning new rounds of the bond program.\n\nSEUs currently in practice in the U.S. are the Delaware SEU., the Washington D.C. SEU, the SCEF Program by the Sonoma County Water Agency (SCWA) in California, California’s Sustainable Energy Bond Program by CSCDA and FREE and the Pennsylvania Sustainable Energy Finance program (PennSEF) by the Pennsylvania Treasury and FREE. International application of the SEU model is being investigated by the City of Seoul (South Korea) and the City of Thane (India).\n\nDevelopment of the Delaware SEU began in 2006. The Delaware General Assembly convened a bipartisan task force to research and recommend a course for sustainable energy in Delaware. The 2007 report published by the task force (\"The Sustainable Energy Utility: a Delaware First\") introduced the SEU as an approach that would move away from utility administered efficiency and renewables to an independent management system. Passage of the State Senate Bill 18 in 2007 created the Delaware SEU. \n\nIn 2007, the Center for Energy and Environmental Policy (University of Delaware) contributed to the design of the DC SEU by providing \"technical support in analyzing options for effectively administering and implementing energy efficiency/conservation programs in the District of Columbia\" and publishing a report detailing the findings The Clean and Affordable Energy Act of 2008 (D.C. Code §8-1773.01 et seq.) created the DC SEU as a private entity tasked to \"administer sustainable energy programs in the District, including the development, coordination, and provision of programs for the purpose of promoting the sustainable use of energy in the District\". The DC SEU management contract was awarded by the DC Council to Vermont Energy Investment Corporation (VEIC) in March of 2011. \n\nIn 2012, the Sonoma County Water Agency launched the Sonoma County Efficiency Financing (SCEF) program following the SEU model. The SCEF program offered participating organizations a contractual dollar savings guarantee. The SCEF program intends to use tax-exempt bonds to finance the projects. In 2013, no-cost preliminary audits were conducted for nine organizations that expressed interest. To date, no efficiency retrofit project contracts have been executed under this program. Applied Solutions, a partner organization of the program, notes that this is due \"to participants' hesitancy to move forward based on unfamiliarity with the program approach and competing funding mechanisms that offer grant (not just loan) funding such as Proposition 39 - the California Clean Energy Jobs Act.\" \n\nIn 2014, the California Statewide Communities Development Authority (CSCDA) and FREE partnered together to provide public agencies throughout California with access to tax exempt financing for sustainable energy investments. The project was recommended to the members of the California League of Cities and the California State Association of Counties. \n\nCreated in 2014, the Pennsylvania Sustainable Energy Finance Program (PennSEF) is a partnership between the Pennsylvania Treasury Department and FREE with financial start-up support from the West Penn Power Sustainable Energy Fund. In 2017, PennSEF organized its first financing pool, called the Regional Streetlight Procurement Project (RSLPP) and brought to PennSEF by the Delaware Valley Regional Planning Commission (DVRPC), the pool brings together 35 municipalities around the city of Philadelphia in order to replace and retrofit 28,000 exterior lights, street lights, and traffic signals. The project guarantees gross energy savings of $30.6 million and, after deduction of all costs, deliver $15.6 million in net savings. \n\nThe mayor of the Seoul, South Korea, Park Won-soon, launched a citizens’ campaign in April 2012 to reduce city greenhouse emissions 25% by 2020 and 40% by 2030. The city has embarked on a One Less Nuclear Power Plant (OLNPP) strategy to realize these goals. The city cut its emissions by 11.9% in two years, and was designated as the Global Earth Hour Capital 2015. The Seoul Metropolitan Government signed a memorandum of understanding with FREE on June 16, 2015, where both parties pledged to cooperate to design climate-sensitive, sustainable, and equitable energy policies for Seoul using the SEU model. Following the advice of a Seoul International Energy Advisory Committee (SIEAC), Seoul established a Seoul Energy Corporation in December 2016. \n"}
{"id": "49773846", "url": "https://en.wikipedia.org/wiki?curid=49773846", "title": "Sydney tornado", "text": "Sydney tornado\n\nThe 1795 Sydney Tornado was the first tornado ever recorded in Australia. The tornado caused damage to crops and land in the early settlement. Little is known about the event as people didn't have many supplies during early colonization nor significant technology to record data.\n"}
{"id": "325060", "url": "https://en.wikipedia.org/wiki?curid=325060", "title": "Tidal power", "text": "Tidal power\n\nTidal power or tidal energy is a form of hydropower that converts the energy obtained from tides into useful forms of power, mainly electricity.\nAlthough not yet widely used, tidal energy has potential for future electricity generation. Tides are more predictable than the wind and the sun. Among sources of renewable energy, tidal energy has traditionally suffered from relatively high cost and limited availability of sites with sufficiently high tidal ranges or flow velocities, thus constricting its total availability. However, many recent technological developments and improvements, both in design (e.g. dynamic tidal power, tidal lagoons) and turbine technology (e.g. new axial turbines, cross flow turbines), indicate that the total availability of tidal power may be much higher than previously assumed, and that economic and environmental costs may be brought down to competitive levels.\n\nHistorically, tide mills have been used both in Europe and on the Atlantic coast of North America. The incoming water was contained in large storage ponds, and as the tide went out, it turned waterwheels that used the mechanical power it produced to mill grain. The earliest occurrences date from the Middle Ages, or even from Roman times. The process of using falling water and spinning turbines to create electricity was introduced in the U.S. and Europe in the 19th century.\n\nThe world's first large-scale tidal power plant was the Rance Tidal Power Station in France, which became operational in 1966. It was the largest tidal power station in terms of output until Sihwa Lake Tidal Power Station opened in South Korea in August 2011. The Sihwa station uses sea wall defense barriers complete with 10 turbines generating 254 MW.\n\nTidal power is taken from the Earth's oceanic tides. Tidal forces are periodic variations in gravitational attraction exerted by celestial bodies. These forces create corresponding motions or currents in the world's oceans. Due to the strong attraction to the oceans, a bulge in the water level is created, causing a temporary increase in sea level. As the Earth rotates, this \"bulge\" of ocean water meets the shallow water adjacent to the shoreline and creates a tide. This occurrence takes place in an unfailing manner, due to the consistent pattern of the moon's orbit around the earth. The magnitude and character of this motion reflects the changing positions of the Moon and Sun relative to the Earth, the effects of Earth's rotation, and local geography of the sea floor and coastlines.\n\nTidal power is the only technology that draws on energy inherent in the orbital characteristics of the Earth–Moon system, and to a lesser extent in the Earth–Sun system. Other natural energies exploited by human technology originate directly or indirectly with the Sun, including fossil fuel, conventional hydroelectric, wind, biofuel, wave and solar energy. Nuclear energy makes use of Earth's mineral deposits of fissionable elements, while geothermal power taps the Earth's internal heat, which comes from a combination of residual heat from planetary accretion (about 20%) and heat produced through radioactive decay (80%).\n\nA tidal generator converts the energy of tidal flows into electricity. Greater tidal variation and higher tidal current velocities can dramatically increase the potential of a site for tidal electricity generation.\n\nBecause the Earth's tides are ultimately due to gravitational interaction with the Moon and Sun and the Earth's rotation, tidal power is practically inexhaustible and classified as a renewable energy resource. Movement of tides causes a loss of mechanical energy in the Earth–Moon system: this is a result of pumping of water through natural restrictions around coastlines and consequent viscous dissipation at the seabed and in turbulence. This loss of energy has caused the rotation of the Earth to slow in the 4.5 billion years since its formation. During the last 620 million years the period of rotation of the earth (length of a day) has increased from 21.9 hours to 24 hours; in this period the Earth has lost 17% of its rotational energy. While tidal power will take additional energy from the system, the effect is negligible and would only be noticed over millions of years.\n\nTidal power can be classified into four generating methods:\n\nTidal stream generators make use of the kinetic energy of moving water to power turbines, in a similar way to wind turbines that use wind to power turbines. Some tidal generators can be built into the structures of existing bridges or are entirely submersed, thus avoiding concerns over impact on the natural landscape. Land constrictions such as straits or inlets can create high velocities at specific sites, which can be captured with the use of turbines. These turbines can be horizontal, vertical, open, or ducted.\n\nStream energy can be used at a much higher rate than wind turbines due to water being more dense than air. Using similar technology to wind turbines converting energy in tidal energy is much more efficient. Close to 10 mph (about 8.6 knots in nautical terms) ocean tidal current would have an energy output equal or greater than a 90 mph wind speed for the same size of turbine system.\n\nTidal barrages make use of the potential energy in the difference in height (or hydraulic head) between high and low tides. When using tidal barrages to generate power, the potential energy from a tide is seized through strategic placement of specialized dams. When the sea level rises and the tide begins to come in, the temporary increase in tidal power is channeled into a large basin behind the dam, holding a large amount of potential energy. With the receding tide, this energy is then converted into mechanical energy as the water is released through large turbines that create electrical power through the use of generators. Barrages are essentially dams across the full width of a tidal estuary.\n\nDynamic tidal power (or DTP) is an untried but promising technology that would exploit an interaction between potential and kinetic energies in tidal flows. It proposes that very long dams (for example: 30–50 km length) be built from coasts straight out into the sea or ocean, without enclosing an area. Tidal phase differences are introduced across the dam, leading to a significant water-level differential in shallow coastal seas – featuring strong coast-parallel oscillating tidal currents such as found in the UK, China, and Korea.\n\nA new tidal energy design option is to construct circular retaining walls embedded with turbines that can capture the potential energy of tides. The created reservoirs are similar to those of tidal barrages, except that the location is artificial and does not contain a pre-existing ecosystem.\nThe lagoons can also be in double (or triple) format without pumping or with pumping that will flatten out the power output. The pumping power could be provided by excess to grid demand renewable energy from for example wind turbines or solar photovoltaic arrays. Excess renewable energy rather than being curtailed could be used and stored for a later period of time. Geographically dispersed tidal lagoons with a time delay between peak production would also flatten out peak production providing near base load production though at a higher cost than some other alternatives such as district heating renewable energy storage. The cancelled Tidal Lagoon Swansea Bay in Wales, United Kingdom would have been the first tidal power station of this type once built.\n\nThe first study of large scale tidal power plants was by the US Federal Power Commission in 1924 which if built would have been located in the northern border area of the US state of Maine and the south eastern border area of the Canadian province of New Brunswick, with various dams, powerhouses, and ship locks enclosing the Bay of Fundy and Passamaquoddy Bay (note: see map in reference). Nothing came of the study and it is unknown whether Canada had been approached about the study by the US Federal Power Commission.\n\nIn 1956, utility Nova Scotia Light and Power of Halifax commissioned a pair of studies into the feasibility of commercial tidal power development on the Nova Scotia side of the Bay of Fundy. The two studies, by Stone & Webster of Boston and by Montreal Engineering Company of Montreal independently concluded that millions of horsepower could be harnessed from Fundy but that development costs would be commercially prohibitive at that time.\n\nThere was also a report on the international commission in April 1961 entitled \"Investigation of the International Passamaquoddy Tidal Power Project\" produced by both the US and Canadian Federal Governments. According to benefit to costs ratios, the project was beneficial to the US but not to Canada. A highway system along the top of the dams was envisioned as well.\n\nA study was commissioned by the Canadian, Nova Scotian and New Brunswick governments (Reassessment of Fundy Tidal Power) to determine the potential for tidal barrages at Chignecto Bay and Minas Basin – at the end of the Fundy Bay estuary. There were three sites determined to be financially feasible: Shepody Bay (1550 MW), Cumberline Basin (1085 MW), and Cobequid Bay (3800 MW). These were never built despite their apparent feasibility in 1977.\n\nThe Snohomish PUD, a public utility district located primarily in Snohomish county, Washington State, began a tidal energy project in 2007; in April 2009 the PUD selected OpenHydro, a company based in Ireland, to develop turbines and equipment for eventual installation. The project as initially designed was to place generation equipment in areas of high tidal flow and operate that equipment for four to five years. After the trial period the equipment would be removed. The project was initially budgeted at a total cost of $10 million, with half of that funding provided by the PUD out of utility reserve funds, and half from grants, primarily from the US federal government. The PUD paid for a portion of this project with reserves and received a $900,000 grant in 2009 and a $3.5 million grant in 2010 in addition to using reserves to pay an estimated $4 million of costs. In 2010 the budget estimate was increased to $20 million, half to be paid by the utility, half by the federal government. The Utility was unable to control costs on this project, and by Oct of 2014 the costs had ballooned to an estimated $38 million and were projected to continue to increase. The PUD proposed that the federal government provide an additional $10 million towards this increased cost citing a \"gentlemans agreement\". When the federal government refused to provide the additional funding the project was cancelled by the PUD after spending nearly $10 million in reserves and grants. The PUD abandoned all tidal energy exploration after this project was cancelled and does not own or operate any tidal energy sources.\n\nThe world's first marine energy test facility was established in 2003 to start the development of the wave and tidal energy industry in the UK. Based in Orkney, Scotland, the European Marine Energy Centre (EMEC) has supported the deployment of more wave and tidal energy devices than at any other single site in the world. EMEC provides a variety of test sites in real sea conditions. Its grid connected tidal test site is located at the Fall of Warness, off the island of Eday, in a narrow channel which concentrates the tide as it flows between the Atlantic Ocean and North Sea. This area has a very strong tidal current, which can travel up to 4 m/s (8 knots) in spring tides. Tidal energy developers that have tested at the site include: Alstom (formerly Tidal Generation Ltd); ANDRITZ HYDRO Hammerfest; Atlantis Resources Corporation; Nautricity; OpenHydro; Scotrenewables Tidal Power; Voith. The resource could be 4 TJ per year. Elsewhere in the UK, annual energy of 50 TWh can be extracted if 25 GW capacity is installed with pivotable blades.\n\n\nTidal power can have effects on marine life. The turbines can accidentally kill swimming sea life with the rotating blades, although projects such as the one in Strangford feature a safety mechanism that turns off the turbine when marine animals approach. Some fish may no longer utilize the area if threatened with a constant rotating or noise-making object. Marine life is a huge factor when placing tidal power energy generators in the water and precautions are made to ensure that as many marine animals as possible will not be affected by it. The Tethys database provides access to scientific literature and general information on the potential environmental effects of tidal energy.\n\nThe main environmental concern with tidal energy is associated with blade strike and entanglement of marine organisms as high speed water increases the risk of organisms being pushed near or through these devices. As with all offshore renewable energies, there is also a concern about how the creation of EMF and acoustic outputs may affect marine organisms. Because these devices are in the water, the acoustic output can be greater than those created with offshore wind energy. Depending on the frequency and amplitude of sound generated by the tidal energy devices, this acoustic output can have varying effects on marine mammals (particularly those who echolocate to communicate and navigate in the marine environment, such as dolphins and whales). Tidal energy removal can also cause environmental concerns such as degrading farfield water quality and disrupting sediment processes. Depending on the size of the project, these effects can range from small traces of sediment building up near the tidal device to severely affecting nearshore ecosystems and processes.\n\nInstalling a barrage may change the shoreline within the bay or estuary, affecting a large ecosystem that depends on tidal flats. Inhibiting the flow of water in and out of the bay, there may also be less flushing of the bay or estuary, causing additional turbidity (suspended solids) and less saltwater, which may result in the death of fish that act as a vital food source to birds and mammals. Migrating fish may also be unable to access breeding streams, and may attempt to pass through the turbines. The same acoustic concerns apply to tidal barrages. Decreasing shipping accessibility can become a socio-economic issue, though locks can be added to allow slow passage. However, the barrage may improve the local economy by increasing land access as a bridge. Calmer waters may also allow better recreation in the bay or estuary. In August 2004, a humpback whale swam through the open sluice gate of the Annapolis Royal Generating Station at slack tide, ending up trapped for several days before eventually finding its way out to the Annapolis Basin.\n\nEnvironmentally, the main concerns are blade strike on fish attempting to enter the lagoon, acoustic output from turbines, and changes in sedimentation processes. However, all these effects are localized and do not affect the entire estuary or bay.\n\nSalt water causes corrosion in metal parts. It can be difficult to maintain tidal stream generators due to their size and depth in the water. The use of corrosion-resistant materials such as stainless steels, high-nickel alloys, copper-nickel alloys, nickel-copper alloys and titanium can greatly reduce, or eliminate, corrosion damage.\n\nMechanical fluids, such as lubricants, can leak out, which may be harmful to the marine life nearby. Proper maintenance can minimize the amount of harmful chemicals that may enter the environment.\n\nThe biological events that happen when placing any structure in an area of high tidal currents and high biological productivity in the ocean will ensure that the structure becomes an ideal substrate for the growth of marine organisms. In the references of the Tidal Current Project at Race Rocks in British Columbia this is documented.\nAlso see this page and\nSeveral structural materials and coatings were tested by the Lester Pearson College divers to assist Clean Current in reducing fouling on the turbine and other underwater infrastructure.\n\nTidal Energy has an expensive initial cost which may be one of the reasons tidal energy not a popular source of renewable energy. It is important to realize that the methods for generating electricity from tidal energy is a relatively new technology. It is projected that tidal power will be commercially profitable within 2020 with better technology and larger scales. Tidal Energy is however still very early in the research process and the ability to reduce the price of tidal energy can be an option. The cost effectiveness depends on each site tidal generators are being placed. To figure out the cost effectiveness they use the Gilbert ratio, which is the length of the barrage in metres to the annual energy production in kilowatt hours (1 kilowatt hour = 1 KWH = 1000 watts used for 1 hour).\n\nDue to tidal energy reliability the expensive upfront cost of these generators will slowly be paid off. Due to the success of a greatly simplified design, the orthogonal turbine offers considerable cost savings. As a result, the production period of each generating unit is reduced, lower metal consumption is needed and technical efficiency is greater. Scientific research has the capability to have a renewable resource like tidal energy that is affordable as well as profitable.\n\nThe high load factors resulting from the fact that water is 800 times denser than air and the predictable and reliable nature of tides compared with the wind makes tidal energy particularly attractive for electric power generation. Condition monitoring is the key for exploiting it cost-efficiently.\n\n\n"}
{"id": "1477117", "url": "https://en.wikipedia.org/wiki?curid=1477117", "title": "Tornado myths", "text": "Tornado myths\n\nTornado myths are incorrect beliefs about tornadoes, which can be attributed to many factors, including stories and news reports told by people unfamiliar with tornadoes, sensationalism by news media, and the presentation of incorrect information in popular entertainment. Common myths cover various aspects of the tornado, and include ideas about tornado safety, the minimization of tornado damage, and false assumptions about the size, shape, power, and path of the tornado itself.\n\nSome people incorrectly believe that opening windows ahead of a tornado will reduce the damage from the storm, but this is not true. Some people also believe that escaping in a vehicle is the safest method of avoiding a tornado, but this could increase the danger in some situations. Other myths are that tornadoes can skip houses, always travel in a predictable direction, always extend visibly from the ground to the cloud, and increase in intensity with increasing width. Finally, some people believe that tornadoes only occur in North America, do not occur in winter, are attracted to trailer park homes, or that some areas are protected from tornadoes by rivers, mountains, valleys, tall buildings or other geographical or man-made features; the truth is that tornadoes can occur almost anywhere at any time if the conditions are right. Some geographic areas are simply more prone to these conditions than others.\n\nSome tornado myths are remaining bits of folklore which are passed down by word of mouth. The idea that the southwest corner of a structure is the safest place in a tornado was first published in the 1800s and persisted until the 1990s despite being thoroughly debunked in the 1960s and 70s. One notable instance of mass media spreading a tornado myth was after the 1999 Oklahoma tornado outbreak, where TIME magazine ran a caption on a picture suggesting that highway overpasses were safer tornado shelters than houses. The spread of some myths can be attributed to popular tornado-themed movies such as \"The Wizard of Oz\" and \"Twister\".\n\nIn 1887, the first book on tornadoes was written by John Park Finley, a pioneer in the field of tornado research. While it was a revolutionary book containing many breakthrough ideas, it contained a few ideas which have since been proven false. One of these was the idea that the northeast or east part of a structure was the least safe, and should be avoided when seeking shelter from a tornado.\n\nThis myth was derived from two misconceptions: First, that tornadoes always travel in a northeasterly direction, and second, that debris from a structure will be carried away in the direction of the tornado's propagation, leaving anyone taking shelter on the side of the structure facing the tornado's approach unharmed. The seriousness of these misconceptions began to be revealed in the 1960s and 1970s, when surveys of major tornado damage in residential areas showed that the section of a house in the direction of the tornado's approach is actually the safe. Additionally, many tornadoes have traveled in directions other than northeasterly, including the Jarrell Tornado (F5 on the Fujita scale), which moved south-southwesterly. Because determining a tornado's direction of approach can take time away from seeking shelter, official advice is to seek shelter in an interior room on the lowest floor of a building, under a staircase, I-beam, or sturdy piece of furniture if possible.\n\nOne of the oldest pieces of tornado folklore is the idea that tornadoes do most of their damage due to the lower atmospheric pressure at the center of a tornado, which causes the house to explode outward. As the theory goes, opening windows helps to equalize the pressure.\n\nThe source of this myth is from the appearance of some destroyed structures after violent tornadoes. When one wall receives the extreme pressure of tornado winds, it will likely collapse . This then leads to a considerable pressure on the three remaining walls, which fall outwards as the roof falls down, creating the impression of a house which has exploded. Indeed, damage surveys of \"exploded\" houses usually show at least one wall which has blown inward. Additionally, if the roof is lifted before any walls fall, the walls can fall in any direction. If they fall outward, this structure can also appear to have exploded.\n\nIn even the most violent tornadoes, there is only a pressure drop of about 10%, which is about . Not only can this difference be equalized in most structures in approximately three seconds, but if a significant pressure differential manages to form, the windows will break first, equalizing the pressure. Additionally, as the windows are the most fragile parts of a house, in a significant tornado flying debris will likely break enough windows to equalize any pressure difference fairly quickly. Regardless of any pressure drop, the direct effects of a tornado's winds are enough to cause damage to a house in all but the weakest tornadoes.\n\nCurrent advice is that opening windows in advance of a tornado wastes time that could be spent seeking shelter. Also, being near windows is very dangerous during a severe weather event, possibly exposing people to flying glass.\n\nThere are several documented cases of people surviving under highway overpasses, but scientists and meteorologists warn against using them for protection. From scientific lessons learned, meteorologists insist that overpasses are insufficient shelter from tornado winds and debris, and may be to be during a violent tornado. The embankment under an overpass is higher than the surrounding terrain, and the wind speed increases with height. Additionally, the overpass design may create a \"wind-tunnel\" effect under the span, further increasing the wind speed. Many overpasses are completely exposed underneath and most lack hanging girders or a crawlspace-like area to provide sufficient protection from debris, which can travel at high speeds even in weak tornadoes. ( a highway underpass is close at hand, and it has such deep crawlspaces behind sheltering girders, and no better shelter is available in the face of imminent danger from a tornado, then this could be the best and most survivable option. Otherwise, an underpass is no shelter at all.) People stopping underneath overpasses block the flow of traffic, putting others in danger.\n\nOften people try to avoid or outrun a tornado in a vehicle. Although cars can travel faster than the average tornado, the directive from the National Weather Service is for house-dwellers in the path of a tornado to take shelter at home rather than risk an escape by vehicle. This is a result of several factors and statistics. An interior room inside a well-built frame house (especially one with a basement) provides a reasonable degree of protection from all but the most violent tornadoes. Underground or above-ground tornado shelters, as well as extremely strong structures such as bank vaults, offer almost complete protection. Cars, on the other hand, can be heavily damaged by even weak tornadoes, and in violent tornadoes they can be thrown large distances, even into buildings. High-profile vehicles such as buses and tractor trailers are even more vulnerable to high winds.\n\nThere are many reasons to avoid cars when a tornado is imminent. Severe thunderstorms which produce tornadoes can produce flooding rains, hail, and strong winds far from the tornado-producing area, all of which can make driving difficult or even impossible. Any of these situations can leave drivers stranded in the path of the tornado far from substantial shelter. When coupled with driver panic, they may also result in dangerous but preventable accidents. This situation would be magnified greatly if all the residents of a warned area left in their vehicles, which would cause traffic jams and accidents as the tornado approached. Numerous victims of the deadly Wichita Falls, Texas tornado on April 10, 1979 died in their vehicles in such a situation.\n\nIf a person spots a nearby tornado while driving, the official National Weather Service directive has been for the individual to abandon the car and seek shelter in a ditch or culvert, or substantial shelter if nearby. Far-away, highly visible tornadoes, however, can be successfully fled from at right angles (90-degrees) from its direction of apparent movement. Despite dangers inherent with operating a vehicle during a tornado, given sufficient advance warning, mobile home residents have been instructed by the National Weather Service to drive to the nearest secure shelter during a warning.\n\nSeveral different phenomena have lent credence to the idea that tornadoes \"skip\" houses, like a person jumping over hurdles. Tornadoes vary in intensity along their path, sometimes drastically over a short period and distance. If a tornado was causing damage, then weakened to the point where it could cause no damage, followed by a re-intensification, it would appear as if it skipped a section. Occasionally with violent tornadoes, a smaller subvortex within a tornado will completely destroy a structure next to another building which appears almost unscathed and thus apparently skipped over.\n\nIt is true that a house that is between two destroyed homes can be undamaged, but this is not the result of a tornado skipping, as some previously thought. After the 1974 Super Outbreak, Dr. Ted Fujita studied many films of tornadoes from that day. Included in his review was damage and tornado film footage of F4 and F5 tornadoes. Fujita concluded that multiple vortices, highly volatile tornadic satellites transiting within a parent tornado at high speeds, are responsible for making tornadoes appear to skip houses. The phenomenon of satellite tornadoes, where a smaller tornado orbits a larger companion tornado, can also lead to gaps in damage between the two tornadoes.\n\nWeaker tornadoes, and at times even stronger tornadoes, can occasionally lift, meaning their circulation ceases to affect the ground. The result is an erratic and discontinuous linear damage path, leading to the term skipping tornado. These discontinuities tend to occur over areas larger than the small neighborhoods where the house-skipping effect is observed, except possibly at the time of the birth and organization of the tornado. This situation is not commonly observed and the term is now rarely applied. Typically, when one tornado weakens and another forms, the process of successive parent mesocyclones forming and decaying is known as cyclic tornadogenesis, thus leading to a series of tornadoes spawned by the same supercell. This series of tornadoes is known as a tornado family.\n\nSome people have been led to assume that small, skinny tornadoes are always weaker than large, wedge-shaped tornadoes. There is an observed trend of wider tornadoes causing worse damage. It is unknown whether this is due to an actual tendency of tornado dynamics or an ability for the tornado to affect a larger area. However, this is not a reliable indicator of an individual tornado's intensity. Some small, rope-like tornadoes, traditionally thought of as weak, have been among the strongest in history. Since 1950, more than 100 violent tornadoes (F4/EF4 or higher) had a maximum width of . Also, tornadoes typically change shape during the course of their lifespan, further complicating any attempt to classify how dangerous a tornado is as it is occurring.\n\nIt is commonly and mistakenly thought that if the condensation funnel of a tornado does not reach the ground, then the tornado cannot cause substantial damage. This is another deadly myth. A tornado appears to be on the ground only when its condensation funnel descends to the surface, but this is misleading. The circular, violent surface winds, not the condensation funnel, are what both define the tornado and cause the tornado's damage. Spotters should keep sight of swirling debris directly under any visible funnel or rotating wall cloud, even if such structures appear to not descend entirely to the ground. Additionally, tornadoes can be wrapped in rain and thus may not be visible at all.\n\nIt has been thought in the past that tornadoes moved almost exclusively in a northeasterly direction. This is false, and a potentially deadly myth which can lead to a false sense of security, especially for unaware spotters or chasers. Although the majority of tornadoes move northeast, this is normally due to the motion of the storm, and tornadoes can arrive from any direction. The expectation of northeasterly travel may be accurate in many cases, but is no more than a statistical observation about tornadoes in general that any particular tornado may defy at any time. A deadly F5 tornado that hit the city of Jarrell, Texas in 1997 moved to the southwest - directly opposite of commonly expected storm motion. Additionally, tornadoes can shift without notice due to storm motion changes or effects on the tornado itself from factors such as its rear flank downdraft. This change of direction proved deadly in the 2013 El Reno tornado in which a 2.6 mile wide tornado shifted from an east direction to a northeast direction killing at least 4 storm chasers.\n\nIt is often thought that tornadoes only occur in North America. The majority of tornadoes do occur in the United States; however, tornadoes have been observed on every continent except Antarctica.\n\nBesides North America, Argentina, Europe, Australia, the United Kingdom, western Russia, Bangladesh and the Philippines also experience tornadoes on a regular basis. \n\nThere are many misconceptions involving the effect of terrain features—bodies of water, mountains, valleys, and others—on tornado formation and behavior. While most modes of tornadogenesis are poorly understood, no terrain feature can prevent the occurrence of a tornado.\n\nSmall bodies of water such as lakes and rivers are insignificant obstacles to tornadoes. Violent tornadoes have formed over rivers and lakes—including the 1878 Wallingford tornado and the 1899 New Richmond tornado—as well as crossing over them after forming elsewhere. More than a dozen tornadoes are reported to have crossed the Mississippi River. Strong tornadoes have also been known to cross the Detroit River and St. Claire River separating southeast Michigan and southwest Ontario.\nRegarding mountains, tornadoes have been observed on terrain as high as above sea level, and have been known to pass up a ridge unaffected.\n\nThese myths have been debunked. The devastating Tri-State Tornado crossed two major rivers along a record or longer path. In 1944, a violent tornado cut a continuous path at least through heavily forested and mountainous territory in West Virginia, killing at least 100 people. A hill known as Burnett's Mound on the southwest end of Topeka, Kansas was purported to protect the city from tornadoes, according to an old legend. However, in 1966, an F5 tornado passed directly over the hill through downtown, killing 18 people and causing $100 million (1966 USD) in damage. Downtown Memphis, Tennessee was believed by residents to be protected from tornadoes and other severe weather by the Chickasaw Bluff along the Mississippi River. During the 1974 Super Outbreak, violent tornadoes crossed dozens of rivers, including the Ohio, Detroit River as well as crossing over mountains and ridges hundreds of feet high. Another example of tornadoes hitting mountainous regions of the United States is the 2011 Super Outbreak, which hit mountainous parts of East Tennessee, Northeast Alabama, Southwest Virginia and North Georgia, killing many people, including an entire family of 4 in Ringgold, Georgia.\n\nThe idea that manufactured housing units, or mobile homes, attract tornadoes has been around for decades. This may appear to be true at first from looking at tornado fatality statistics: from 2000 to 2008, 539 people were killed by tornadoes in the US, with more than half (282) of those deaths in mobile homes. Only around 6.8% of homes in the US are \"manufactured/mobile homes\".\n\nHowever, it is highly unlikely that single-story structures such as mobile homes can have a substantial effect on tornado development or evolution. More people are killed in trailer parks because mobile homes are less able to withstand high winds than permanent structures. Winds which can demolish or roll a mobile home may only cause roof damage to a typical one- or two-family permanent residence. Another likely contributing factor to the continued propagation of this myth is confirmation bias: whenever a new instance of a tornado hitting a mobile home park occurs, media outlets report on it more extensively, ignoring damage to the surrounding area which may not have produced as many casualties.\n\nSome people believe that, for various reasons, large cities cannot be struck by tornadoes. More than 100 tornadoes have been reported to strike downtown areas of large cities. Many cities have been struck twice or more, and a few—including Lubbock, Texas; St. Louis, Missouri; Topeka, Kansas; and London, England—have been struck by violent tornadoes (F4 or stronger).\n\nTornadoes may seem rare in downtown areas because downtown areas cover such a small geographical area. Considering the size of a central business district is very small compared to the city limits, tornadoes will strike outside of the downtown area more often.\n\nThe misconception, like most, has a small basis in truth. Research has been done in a few metropolitan areas suggesting that the urban heat island effect may discourage the formation of weak tornadoes in city centers, due to turbulent warm air disrupting their formation. This does not apply to significant tornadoes, however, and it is possible that the presence of tall buildings may actually storms which move into downtown areas.\n\nBecause they generally require warm weather to form, tornadoes are uncommon in winter in the mid-latitudes. However, they can form, and tornadoes have even been known to travel over snow-covered surfaces. Deadly tornadoes are no exception: from 2000 to 2008, 135 of the 539 US tornado deaths occurred during meteorological winter (December through February). Tornadoes in winter may be more dangerous, since they tend to move faster than tornadoes at other times of the year.\n\n\n"}
{"id": "17387012", "url": "https://en.wikipedia.org/wiki?curid=17387012", "title": "WAsP", "text": "WAsP\n\nWAsP (Wind Atlas Analysis and Application Program) is a Windows program for predicting wind climates, wind resources, and energy yields from wind turbines and wind farms. The predictions are based on wind data measured at meteorological stations in the same region, or on generalized wind climates derived from mesoscale model results. The program includes a complex terrain flow model, a roughness change model, a model for sheltering obstacles, a wind turbine wake model and a model for the average atmospheric stability conditions at the site. The software package further contains a Climate Analyst for creating the wind-climatological inputs, a Map Editor for creating and editing the topographical inputs, and a Turbine Editor for creating the wind turbine inputs to WAsP. The fundamentals of WAsP and the wind atlas methodology are described in the European Wind Atlas. WAsP is developed and distributed by the Department of Wind Energy at the Technical University of Denmark, Denmark. Current version is WAsP 12.\n\nWAsP is used for:\n\n"}
{"id": "57930685", "url": "https://en.wikipedia.org/wiki?curid=57930685", "title": "Wu number", "text": "Wu number\n\nThe Wu number (formula_1) is a nondimensional quantity in fluid mechanics describing the effects of rotating axisymmetric bodies on wall-bounded sheer flows. The parameter is named after the Chinese scientist Yongxiang Wu during his work on boundary layer instabilities.\n\nThe Wu number is defined as.\n\nformula_2,\n\nwhere formula_3 is the circular frequency, formula_4 the diameter of the axisymmetric body (like for instance a cylinder) and formula_5 the local flow velocity at the top of the body. It is therefore the ratio of the velocities of the body edge and the surrounding flow.\n\nRotating bodies on boundary layers are a means of active flow control on wings, empanage or stabilizers on airplanes. For large Wu numbers (formula_6), the instability mechanisms are governed by the rotation of the body. For small Wu numbers (formula_7) the stability-related effects of the body itself may outweigh its rotatory effects\n\n"}
