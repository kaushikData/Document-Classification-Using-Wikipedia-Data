{"id": "14861167", "url": "https://en.wikipedia.org/wiki?curid=14861167", "title": "Aapua Wind Farm", "text": "Aapua Wind Farm\n\nThe Aapua wind park () is a wind park on Eta, southwest of Aapua, Övertorneå municipality in Norrbotten County, Sweden. , it is the northern-most wind park in Sweden. The park is owned by Aapua Vind AB, representing 14 private investors, and operates on land owned by the state-owned Sveaskog, Sweden's largest forest owner. The plant came online in 2005\n\nThe park consists of seven 1.5 MW Vestas V82-1.5 MW Arctic wind turbines with a total stated power of 9.9 MW (10.5 MW according to supplier Vestas). Each unit has a nacelle height of 78 m and a rotor diameter of 82 m.\n\n"}
{"id": "856432", "url": "https://en.wikipedia.org/wiki?curid=856432", "title": "Appliance classes", "text": "Appliance classes\n\nIn the electrical appliance manufacturing industry, the following IEC protection classes are defined in IEC 61140 and used to differentiate between the protective-earth connection requirements of devices. These appliances must have their chassis connected to electrical earth.\n\nThese appliances have no protective-earth connection and feature only a single level of insulation between live parts and exposed metalwork. If permitted at all, Class 0 items are intended for use in dry areas only. A single fault could cause an electric shock or other dangerous occurrence, without triggering the automatic operation of any fuse or circuit breaker. Sales of such items have been prohibited in much of the world for safety reasons, for example in the UK by Section 8 of The Low Voltage Electrical Equipment (Safety) Regulations 1989 and New Zealand by the Electricity Act. However, equipment of this class is common in some 110 V countries, and in much of the 220 V developing world, whether permitted officially or not. These appliances do not have their chassis connected to electrical earth.\n\nThese appliances must have their chassis connected to electrical earth (US: ground) by a separate earth conductor (coloured green/yellow in most countries, green in the US, Canada and Japan).\nThe earth connection is achieved with a 3-conductor mains cable, typically ending with 3-prong AC connector which plugs into a corresponding AC outlet. The basic requirement is that no single failure can result in dangerous voltage becoming exposed so that it might cause an electric shock and that if a fault occurs the supply will be removed automatically (this is sometimes referred to as ADS = Automatic Disconnection of Supply).\n\nA fault in the appliance which causes a live conductor to contact the casing will cause a current to flow in the earth conductor. If large enough, this current will trip an over-current device (fuse or circuit breaker (CB)) and disconnect the supply. The disconnection time has to be fast enough not to allow fibrillation to start if a person is in contact with the casing at the time. This time and the current rating in turn sets a maximum earth resistance permissible. \nTo provide supplementary protection against high-impedance faults it is common to recommend a residual-current device (RCD) also known as a residual current circuit breaker (RCCB), ground fault circuit interrupter (GFCI), or residual current operated circuit-breaker with integral over-current protection (RCBO), which will cut off the supply of electricity to the appliance if the currents in the two poles of the supply are not equal and opposite.\n\nElectrical installations where the chassis is connected to earth with a separate terminal, instead of via the mains cable. In effect this provides the same automatic disconnection as Class I, for equipment that otherwise would be Class 0\n\nA Class II or double insulated electrical appliance is one which has been designed in such a way that it does not require a safety connection to electrical earth (ground).\n\nThe basic requirement is that no single failure can result in dangerous voltage becoming exposed so that it might cause an electric shock and that this is achieved without relying on an earthed metal casing. This is usually achieved at least in part by having at least two layers of insulating material between live parts and the user, or by using reinforced insulation.\n\nIn Europe, a double insulated appliance must be labelled \"Class II\" or \"double insulated\" or bear the double insulation symbol (a square inside another square).\n\nInsulated AC/DC power supplies (such as cell-phone chargers) are typically designated as Class II, meaning that the DC output wires are isolated from the AC input. The designation \"Class II\" should not be confused with the designation \"Class 2\", as the latter is unrelated to insulation (it originates from standard UL 1310, setting limits on maximum output voltage/current/power).\n\nA Class III appliance is designed to be supplied from a separated/safety extra-low voltage (SELV) power source. The voltage from a SELV supply is low enough that under normal conditions a person can safely come into contact with it without risk of electrical shock. The extra safety features built into Class I and Class II appliances are therefore not required. For medical devices, compliance with Class III is \"not\" considered sufficient protection, and further more-stringent regulations apply to such equipment.\n\n\n"}
{"id": "23613008", "url": "https://en.wikipedia.org/wiki?curid=23613008", "title": "Average rectified value", "text": "Average rectified value\n\nIn electrical engineering, the average rectified value (ARV) of the quantity is the average of its absolute value.\n\nThe average of a symmetric alternating value is zero and it is therefore not useful to characterize it. Thus the easiest way to determine a quantitative measurement size is to use the average rectified value. The average rectified value is mainly used to characterize alternating voltage and alternating current. It can be computed by averaging the absolute value of a waveform over one full period of the waveform.\n\nWhile conceptually similar to the root mean square (RMS), ARV will differ from it whenever a function's absolute value varies locally, as the former then increases disproportionately. The difference is expressed by the form factor\n\n"}
{"id": "460997", "url": "https://en.wikipedia.org/wiki?curid=460997", "title": "Backdraft", "text": "Backdraft\n\nA backdraft is a dramatic event caused through rapid re-introduction of oxygen to combustion into an oxygen-depleted environment in a fire; for example, the breaking of a window or opening of a door to an enclosed space. Backdrafts present a serious threat to firefighters. There is some debate concerning whether backdrafts should be considered a type of flashover (see below).\n\nA backdraft can occur when a compartment fire has little or no ventilation, leading to slowing of gas-phase combustion (due to the lack of oxygen); however, the combustible fuel gases (unburnt fuel vapor and gas-phase combustion intermediates such as hydrocarbons and carbon monoxide) and smoke (primarily particulate matter) remain at a temperature hotter than the auto-ignition temperature of the fuel mixture. If oxygen is then re-introduced to the compartment, e.g. by opening a door or window to a closed room, combustion will restart, often rapidly, as the gases are heated by the combustion and expand rapidly because of the rapidly increasing temperature.\n\nCharacteristic signs of a backdraft situation include yellow or brown smoke, smoke which exits small holes in puffs (a sort of breathing effect) and is often found around the edges of doors and windows, and windows which appear brown or black when viewed from the exterior. These darker colors are caused by the presence of large amounts of particulate matter suspended in the air inside the room due to incomplete combustion; it is an indication that the room lacks enough oxygen to permit oxidation of the soot particles. Firefighters often look to see if there is soot on the inside of windows and in any cracks in the window (caused e.g. by the heat). The windows may also have a slight vibration due to varying pressure within the compartment due to intermittent combustion.\n\nIf firefighters discover a room pulling air into itself, for example through a crack, they generally evacuate immediately, because this is a strong indication that a backdraft is imminent. Due to pressure differences, puffs of smoke are sometimes drawn back into the enclosed space from which they emanated, which is how the term \"backdraft\" originated.\n\nBackdrafts are very dangerous, often surprising even experienced firefighters. The most common tactic used by firefighters to defuse a potential backdraft is to ventilate a room from its highest point, allowing the heat and smoke to escape without igniting.\n\nCommon signs of backdraft include a sudden inrush of air upon an opening into a compartment being created, lack of visible signs of flame (fire above its upper flammability limit), \"pulsing\" smoke plumes from openings and auto-ignition of hot gases at openings where they mix with oxygen in the surrounding air. \n\nAlthough ISO 13943 defines flashover as \"transition to a state of total surface involvement in a fire of combustible materials within an enclosure,\" a broad definition that embraces several different scenarios, including backdrafts, there is nevertheless considerable disagreement regarding whether or not backdrafts should be properly considered flashovers. The most common use of the term flashover is to describe the near-simultaneous ignition of material caused by heat attaining the autoignition temperature of the combustible material and gases in an enclosure. Flashovers of this type are not backdrafts as they are caused by thermal change. Backdrafts are caused by the introduction of oxygen into an enclosed space with conditions already suitable for ignition, and are thus caused by chemical change.\n\nBackdrafts were publicized by the 1991 movie \"Backdraft\", in which a serial arsonist in Chicago was using them as a means of assassinating co-conspirators in a scam.\n\nIn the film adaptation of Stephen King's \"1408\", the protagonist Mike Enslin induces one as a last-ditch effort to kill the room.\n\nThe term is also used and is the title of a scene in the 2012 video game \"\".\n\n\n"}
{"id": "6248691", "url": "https://en.wikipedia.org/wiki?curid=6248691", "title": "Bake-out", "text": "Bake-out\n\nBake-out, in several areas of technology and fabrication, and in building construction, refers to the process of using high heat temperature (heat), and possibly vacuum, to remove volatile compounds from materials and objects before placing them into situations where the slow release of the same volatile compounds would contaminate the contents of a container or vessel, spoil a vacuum, or cause discomfort (odor or irritation) or illness. Bake-out is an artificial acceleration of the process of outgassing.\n\nElectrical insulation paper is often baked dry, without vacuum, before being placed into insulating oil, because even small amounts of water degrade the insulating performance of oil.\n\nIn various physics and vacuum device engineering, such as particle accelerators, semiconductor fabrication, and vacuum tubes, bake-out is a manufacturing process, the period of time when a part or device is placed in a vacuum chamber (or its operating vacuum state, for devices which operate in a vacuum) and heated, usually by built-in heaters. This drives off gases, which are removed by continued operation of the vacuum pump.\n\nIn building construction, bake-out is the use of heat to remove volatile organic compounds such as solvents remaining in paint, carpets, and other building materials from a building after its construction, to reduce annoying odors or improve indoor air quality. The building interior is heated to a much higher temperature than normal and kept at that temperature for an extended period of time, to encourage such compounds to vaporize into the air, which is vented (released to the atmosphere).\n\n"}
{"id": "7404011", "url": "https://en.wikipedia.org/wiki?curid=7404011", "title": "Bead (woodworking)", "text": "Bead (woodworking)\n\nA bead is a woodworking decorative treatment applied to various elements of wooden furniture, boxes and other items.\n\nA bead is typically a rounded shape cut into a square edge to soften the edge and provide some protection against splitting. Beads can be simple round shapes, or more complex patterns.\n\nA bead may be created with an electric router, a special moulding handplane or a scratch stock. Beads are usually cut directly into the edge of the item to which the bead is being applied. However, beads applied across the grain are usually cut into a separate piece which is then fixed in position.\n\nA bead is also an important design element in wood turning, a ring-shape or convex curve incised into a piece by the use of a chisel or skew.\n"}
{"id": "15785261", "url": "https://en.wikipedia.org/wiki?curid=15785261", "title": "Belarusian nuclear power plant", "text": "Belarusian nuclear power plant\n\nThe Belarusian nuclear power plant is a multi-reactor nuclear power plant project in Belarus. Initial plans were announced in the 1980s, but were suspended after the 1986 Chernobyl disaster. The drive for the current project was fueled by the Russia-Belarus energy dispute in 2007. The project foresees construction of two nuclear reactors between 2016 and 2020, and probably two more reactors by 2025. The reactors would be supplied by Atomstroyexport and the plant would be located in the Astravyets District, Grodno Region.\n\nIn the 1980s there were plans to build a nuclear heating and power plant in Rudensk, some south of the vicinity of Minsk. Following the Chernobyl nuclear disaster, these plans were halted. The plant was to comprise two VVER-1000 nuclear reactors, designed to provide both electricity and heat for the city of Minsk. The reactors would each have had a power rating of 900 MW net and 940 MW gross capacity.\n\nThe nuclear initiative was revitalized after Belarus gained independence from the Soviet Union. On 22 December 1992, Belarus announced its intention to build nuclear power plants and started a program to examine 15 possible sites. It was foreseen that the first unit of 500-600 MW would be commissioned by 2005, and additional units with a combined capacity of 1,000 MW by 2005 and 2010. However, no decision concerning site or reactor type was made. In 1999, the Government of Belarus adopted nuclear moratorium.\n\nOn 2 May 2002, Belarusian President Alexander Lukashenko stated that Belarus would not construct a nuclear power plant on its territory, but was interested in purchasing nuclear power from Russia, and in the possibility of constructing a Belarus-owned reactor at the Smolensk nuclear power plant in Russia. However, in mid 2006, the Government of Belarus approved a plan for the construction of an initial 2000 MWe nuclear power plant in the Mahilyow Voblast using pressurized water reactors technology.\n\nAfter the Russia-Belarus energy dispute in 2007, Lukashenko re-declared that to ensure national energy security, Belarus needed to build its own nuclear power plant. The Belarusian Security Council made the decision to construct a nuclear power plant on 15 January 2008. According to the presidential decree signed in January 2008, the first reactor of the nuclear power plant should be operational by 2016, and the second one by 2018. The Nuclear Power Act, covering the design and construction of nuclear facilities, the security, safety, and physical protection of such facilities, and their regulation (and also prohibiting the production of nuclear weapons and other nuclear explosives), was adopted by the House of Representatives of the National Assembly of Belarus on 25 June 2008.\n\nIn June 2007, Russia offered a US$2 billion credit line for the purchasing of equipment from Russia's Power Machines Company. In January 2009, it was decided that the nuclear power plant will be built by Atomstroyexpoert and the Russian loan was agreed in February 2009.\n\nOn 27 February 2008, Iran announced that it is ready to assist Belarus with the construction and operation of a nuclear power plant in areas such as funding, personnel training, the sharing of experience, the installation of equipment, and training in operating such equipment.\n\nOn 1 July 2009, a Ukrainian NGO sent a complaint to the Implementation Committee of the Espoo Convention alleging numerous violations of the Espoo Convention. In particular, the complaint argues that Belarus is in violation of the requirements of the Convention by pre-defining two key alternatives of the nuclear power plant construction – location and no-action alternative, as well as by not establishing an environmental impact assessment procedure that permits public participation. In December 2009 European ECO Forum Legal Focal Points submitted a complaint to the Compliance Committee of the Aarhus Convention challenging the legality of NPP construction due to violation of public participation rights provided by the Aarhus Convention.\n\nOn 22 January 2011 the news was released that the contract for the Belarusian NPP will be signed in the first quarter of 2011 with Rosatom.\n\nOn June 2012 the construction of the foundation pit for the nuclear power plant started near the small village of Shulniki in Astravets District, Hrodna Region, some from the Lithuanian border.\n\nBoth in March and April 2013 journalists were not permitted to visit the construction site. In March 2013 Radio Svaboda’s correspondent Mikhail Karnevich received official permission to make a report about the construction of the power plant. But when he came to Astravets, he found out that he would not be able to visit the construction site. In April 2013 journalists Ales Barazenka and Nastaśsia Jaūmien were detained in Astravets where they were filming the nuclear power plant construction and were asked an \"intelligible explanation to the fact of filming the construction works\".\n\nThe first nuclear concrete for Unit 1 was poured on 6 November 2013.\nThe construction of the second unit began 8 months later. Construction of each unit is expected to take about five years.\n\nOn February 2016, the 330-tonne, 13-meter high, 4.5 meters diameter, reactor vessel (which was the first reactor produced by Atommash after a 29-year hiatus) was delivered to the site. According to press reports, it took Atommash 840 days (2 years and 4 months) to build the reactor; it was shipped from the plant on October 14, 2015. After being transported by barge over the Tsimlyansk Reservoir, the Volga-Don Canal, the Volga–Baltic Waterway, and the Volkhov River to Novgorod, the reactor was then shipped by a special rail car to the Astravyets railway station near the plant.\n\nIn November 2015, Lithuania refused to allow Kruonis Pumped Storage Plant to be used, aside from emergency case, as reserve for the Belarusian nuclear power plant.\n\nThe location of the construction site was chosen on 20 December 2008. The nuclear power plant is being built some 18km away from the town of Astravets in Hrodna Voblast, from Vilnius, Lithuania. Alternative sites were Chyrvonaya Palyana near Bykhaw in Mahilyow Voblast, and Kukshynava between Horki and Shkloŭ in Mahilyow Voblast.\n\nRussian Atomstroyexport is the contractor to build the nuclear power plant. Atomstroyexport has also supplied the III generation VVER-1200 type reactors (AES-2006 model). \nThe first reactor is scheduled for commissioning in December 2019, with the second one to go online in July 2020. The first two reactors will have the combined capacity of around 2400 MW. It is possible that two additional reactors will be built by 2025.\n\nThe nuclear power plant is expected to cost up to US$11 billion. In addition, there are investments to upgrade the national power grid for power transmission from the nuclear power plant, and the construction of an urban settlement for the power plant's workers.\n\nOn 12 November 2007, a decree defining the organizations responsible for preparing the construction of the nuclear power plant was signed. In accordance with the decree, a Directorate for the Construction of a Nuclear Power Plant was established under the Ministry of Energy. This directorate oversees the preparation, design and exploration works. The Nuclear and Radiation Safety Department, part of the Emergencies Ministry, is acting as the state nuclear regulator and licensing authority.\n\nScientific support for the project is provided by the United Power & Nuclear Research Institute Sosny of the National Academy of Sciences of Belarus. The state-owned power engineering industry research and design institute Belnipienergoprom is the general designer of the plant and operates as the project management company, negotiating and signing contracts with suppliers, carrying out feasibility studies and preparing tender documents. Yelena Mironova is the Head of the project management service.\n\nThe nuclear power plant plans have raised several concerns. Civil society groups have campaigned and collected signatures against the construction of a nuclear power plant in Belarus. Young members of the Belarusian People's Front have campaigned against possible Russian involvement in the construction of the plant and urged the Belarusian government to award the contract to build the nuclear power plant to a company based in a country other than Russia. A group of Belarusian scientists founded a movement for a nuclear-free Belarus, claiming that the Belarusian government started preparations for the construction of the nuclear power plant before a moratorium adopted in 1999 was expired. The moratorium expired on 14 January 2009.\n\nLithuania is a critic of the power plant and intends to boycott it. \n"}
{"id": "3758", "url": "https://en.wikipedia.org/wiki?curid=3758", "title": "Berkelium", "text": "Berkelium\n\nBerkelium is a transuranic radioactive chemical element with symbol Bk and atomic number 97. It is a member of the actinide and transuranium element series. It is named after the city of Berkeley, California, the location of the Lawrence Berkeley National Laboratory (then the University of California Radiation Laboratory) where it was discovered in December 1949. Berkelium was the fifth transuranium element discovered after neptunium, plutonium, curium and americium.\n\nThe major isotope of berkelium, Bk, is synthesized in minute quantities in dedicated high-flux nuclear reactors, mainly at the Oak Ridge National Laboratory in Tennessee, USA, and at the Research Institute of Atomic Reactors in Dimitrovgrad, Russia. The production of the second-most important isotope Bk involves the irradiation of the rare isotope Cm with high-energy alpha particles.\n\nJust over one gram of berkelium has been produced in the United States since 1967. There is no practical application of berkelium outside scientific research which is mostly directed at the synthesis of heavier transuranic elements and transactinides. A 22 milligram batch of berkelium-249 was prepared during a 250-day irradiation period and then purified for a further 90 days at Oak Ridge in 2009. This sample was used to synthesize the new element tennessine for the first time in 2009 at the Joint Institute for Nuclear Research, Russia, after it was bombarded with calcium-48 ions for 150 days. This was the culmination of the Russia–US collaboration on the synthesis of the heaviest elements on the periodic table.\n\nBerkelium is a soft, silvery-white, radioactive metal. The berkelium-249 isotope emits low-energy electrons and thus is relatively safe to handle. It decays with a half-life of 330 days to californium-249, which is a strong emitter of ionizing alpha particles. This gradual transformation is an important consideration when studying the properties of elemental berkelium and its chemical compounds, since the formation of californium brings not only chemical contamination, but also free-radical effects and self-heating from the emitted alpha particles.\n\nBerkelium is a soft, silvery-white, radioactive actinide metal. In the periodic table, it is located to the right of the actinide curium, to the left of the actinide californium and below the lanthanide terbium with which it shares many similarities in physical and chemical properties. Its density of 14.78 g/cm lies between those of curium (13.52 g/cm) and californium (15.1 g/cm), as does its melting point of 986 °C, below that of curium (1340 °C) but higher than that of californium (900 °C). Berkelium is relatively soft and has one of the lowest bulk moduli among the actinides, at about 20 GPa (2 Pa).\n\nBerkelium(III) ions shows two sharp fluorescence peaks at 652 nanometers (red light) and 742 nanometers (deep red – near infrared) due to internal transitions at the f-electron shell. The relative intensity of these peaks depends on the excitation power and temperature of the sample. This emission can be observed, for example, after dispersing berkelium ions in a silicate glass, by melting the glass in presence of berkelium oxide or halide.\n\nBetween 70 K and room temperature, berkelium behaves as a Curie–Weiss paramagnetic material with an effective magnetic moment of 9.69 Bohr magnetons (µ) and a Curie temperature of 101 K. This magnetic moment is almost equal to the theoretical value of 9.72 µ calculated within the simple atomic L-S coupling model. Upon cooling to about 34 K, berkelium undergoes a transition to an antiferromagnetic state. Enthalpy of dissolution in hydrochloric acid at standard conditions is −600 kJ/mol, from which the standard enthalpy change of formation (Δ\"H\"°) of aqueous Bk ions is obtained as −601 kJ/mol. The standard potential Bk/Bk is −2.01 V. The ionization potential of a neutral berkelium atom is 6.23 eV.\n\nAt ambient conditions, berkelium assumes its most stable α form which has a hexagonal symmetry, space group \"P6/mmc\", lattice parameters of 341 pm and 1107 pm. The crystal has a double-hexagonal close packing structure with the layer sequence ABAC and so is isotypic (having a similar structure) with α-lanthanum and α-forms of actinides beyond curium. This crystal structure changes with pressure and temperature. When compressed at room temperature to 7 GPa, α-berkelium transforms to the beta modification, which has a face-centered cubic (\"fcc\") symmetry and space group \"Fmm\". This transition occurs without change in volume, but the enthalpy increases by 3.66 kJ/mol. Upon further compression to 25 GPa, berkelium transforms to an orthorhombic γ-berkelium structure similar to that of α-uranium. This transition is accompanied by a 12% volume decrease and delocalization of the electrons at the 5f electron shell. No further phase transitions are observed up to 57 GPa.\n\nUpon heating, α-berkelium transforms into another phase with an \"fcc\" lattice (but slightly different from β-berkelium), space group \"Fmm\" and the lattice constant of 500 pm; this \"fcc\" structure is equivalent to the closest packing with the sequence ABC. This phase is metastable and will gradually revert to the original α-berkelium phase at room temperature. The temperature of the phase transition is believed to be quite close to the melting point.\n\nLike all actinides, berkelium dissolves in various aqueous inorganic acids, liberating gaseous hydrogen and converting into the berkelium(III) state. This trivalent oxidation state (+3) is the most stable, especially in aqueous solutions, but tetravalent (+4) and possibly divalent (+2) berkelium compounds are also known. The existence of divalent berkelium salts is uncertain and has only been reported in mixed lanthanum chloride-strontium chloride melts. A similar behavior is observed for the lanthanide analogue of berkelium, terbium. Aqueous solutions of Bk ions are green in most acids. The color of Bk ions is yellow in hydrochloric acid and orange-yellow in sulfuric acid. Berkelium does not react rapidly with oxygen at room temperature, possibly due to the formation of a protective oxide layer surface. However, it reacts with molten metals, hydrogen, halogens, chalcogens and pnictogens to form various binary compounds.\n\nAbout twenty isotopes and six nuclear isomers (excited states of an isotope) of berkelium have been characterized with the mass numbers ranging from 235 to 254. All of them are radioactive. The longest half-lives are observed for Bk (1,380 years), Bk (over 300 years) and Bk (330 days); the half-lives of the other isotopes range from microseconds to several days. The isotope which is the easiest to synthesize is berkelium-249. This emits mostly soft β-particles which are inconvenient for detection. Its alpha radiation is rather weak – 1.45% with respect to the β-radiation – but is sometimes used to detect this isotope. The second important berkelium isotope, berkelium-247, is an alpha-emitter, as are most actinide isotopes.\n\nAll berkelium isotopes have a half-life far too short to be primordial. Therefore, any primordial berkelium, that is, berkelium present on the Earth during its formation, has decayed by now.\n\nOn Earth, berkelium is mostly concentrated in certain areas, which were used for the atmospheric nuclear weapons tests between 1945 and 1980, as well as at the sites of nuclear incidents, such as the Chernobyl disaster, Three Mile Island accident and 1968 Thule Air Base B-52 crash. Analysis of the debris at the testing site of the first U.S. hydrogen bomb, Ivy Mike, (1 November 1952, Enewetak Atoll), revealed high concentrations of various actinides, including berkelium. For reasons of military secrecy, this result was published only in 1956.\n\nNuclear reactors produce mostly, among the berkelium isotopes, berkelium-249. During the storage and before the fuel disposal, most of it beta decays to californium-249. The latter has a half-life of 351 years, which is relatively long when compared to the other isotopes produced in the reactor, and is therefore undesirable in the disposal products.\n\nThe transuranic elements from americium to fermium, including berkelium, occurred naturally in the natural nuclear fission reactor at Oklo, but no longer do so.\n\nAlthough very small amounts of berkelium were possibly produced in previous nuclear experiments, it was first intentionally synthesized, isolated and identified in December 1949 by Glenn T. Seaborg, Albert Ghiorso, Stanley G. Thompson, and Kenneth Street, Jr.. They used the 60-inch cyclotron at the University of California, Berkeley. Similar to the nearly simultaneous discovery of americium (element 95) and curium (element 96) in 1944, the new elements berkelium and californium (element 98) were both produced in 1949–1950.\n\nThe name choice for element 97 followed the previous tradition of the Californian group to draw an analogy between the newly discovered actinide and the lanthanide element positioned above it in the periodic table. Previously, americium was named after a continent as its analogue europium, and curium honored scientists Marie and Pierre Curie as the lanthanide above it, gadolinium, was named after the explorer of the rare earth elements Johan Gadolin. Thus the discovery report by the Berkeley group reads: \"It is suggested that element 97 be given the name berkelium (symbol Bk) after the city of Berkeley in a manner similar to that used in naming its chemical homologue terbium (atomic number 65) whose name was derived from the town of Ytterby, Sweden, where the rare earth minerals were first found.\" This tradition ended on berkelium, though, as the naming of the next discovered actinide, californium, was not related to its lanthanide analogue dysprosium, but after the discovery place.\n\nThe most difficult steps in the synthesis of berkelium were its separation from the final products and the production of sufficient quantities of americium for the target material. First, americium (Am) nitrate solution was coated on a platinum foil, the solution was evaporated and the residue converted by annealing to americium dioxide (AmO). This target was irradiated with 35 MeV alpha particles for 6 hours in the 60-inch cyclotron at the Lawrence Radiation Laboratory, University of California, Berkeley. The (α,2n) reaction induced by the irradiation yielded the Bk isotope and two free neutrons:\n\nAfter the irradiation, the coating was dissolved with nitric acid and then precipitated as the hydroxide using concentrated aqueous ammonia solution. The product was centrifugated and re-dissolved in nitric acid. To separate berkelium from the unreacted americium, this solution was added to a mixture of ammonium and ammonium sulfate and heated to convert all the dissolved americium into the oxidation state +6. Unoxidized residual americium was precipitated by the addition of hydrofluoric acid as americium(III) fluoride (). This step yielded a mixture of the accompanying product curium and the expected element 97 in form of trifluorides. The mixture was converted to the corresponding hydroxides by treating it with potassium hydroxide, and after centrifugation, was dissolved in perchloric acid.\nFurther separation was carried out in the presence of a citric acid/ammonium buffer solution in a weakly acidic medium (pH≈3.5), using ion exchange at elevated temperature. The chromatographic separation behavior was unknown for the element 97 at the time, but was anticipated by analogy with terbium. The first results were disappointing because no alpha-particle emission signature could be detected from the elution product. With further analysis, searching for characteristic X-rays and conversion electron signals, a berkelium isotope was eventually detected. Its mass number was uncertain between 243 and 244 in the initial report, but was later established as 243.\n\nBerkelium is produced by bombarding lighter actinides uranium (U) or plutonium (Pu) with neutrons in a nuclear reactor. In a more common case of uranium fuel, plutonium is produced first by neutron capture (the so-called (n,γ) reaction or neutron fusion) followed by beta-decay:\n\nPlutonium-239 is further irradiated by a source that has a high neutron flux, several times higher than a conventional nuclear reactor, such as the 85-megawatt High Flux Isotope Reactor (HFIR) at the Oak Ridge National Laboratory in Tennessee, USA. The higher flux promotes fusion reactions involving not one but several neutrons, converting Pu to Cm and then to Cm:\n\nCurium-249 has a short half-life of 64 minutes, and thus its further conversion to Cm has a low probability. Instead, it transforms by beta-decay into Bk:\n\nThe thus-produced Bk has a long half-life of 330 days and thus can capture another neutron. However, the product, Bk, again has a relatively short half-life of 3.212 hours and thus, does not yield any heavier berkelium isotopes. Instead decays to the californium isotope Cf:\n\nAlthough Bk is the most stable isotope of berkelium, its production in nuclear reactors is very inefficient due to the long half-life of its potential progenitor curium-247, which does not allow it sufficient time to beta decay before capturing another neutron. Thus, Bk is the most accessible isotope of berkelium, which still, is available only in small quantities (only 0.66 grams have been produced in the US over the period 1967–1983) at a high price of the order 185 USD per microgram.\n\nThe isotope Bk was first obtained in 1956 by bombarding a mixture of curium isotopes with 25 MeV α-particles. Although its direct detection was hindered by strong signal interference with Bk, the existence of a new isotope was proven by the growth of the decay product Cf which had been previously characterized. The half-life of Bk was estimated as hours, though later 1965 work gave a half-life in excess of 300 years (which may be due to an isomeric state). Berkelium-247 was produced during the same year by irradiating Cm with alpha-particles:\n\nBerkelium-242 was synthesized in 1979 by bombarding U with B, U with B, Th with N or Th with N. It converts by electron capture to Cm with a half-life of minutes. A search for an initially suspected isotope Bk was then unsuccessful; Bk has since been synthesized.\n\nThe fact that berkelium readily assumes oxidation state +4 in solids, and is relatively stable in this state in liquids greatly assists separation of berkelium away from many other actinides. These are inevitably produced in relatively large amounts during the nuclear synthesis and often favor the +3 state. This fact was not yet known in the initial experiments, which used a more complex separation procedure. Various inorganic oxidation agents can be applied to the berkelium(III) solutions to convert it to the +4 state, such as bromates (), bismuthates (), chromates ( and CrO), silver(I) thiolate (), lead(IV) oxide (), ozone (), or photochemical oxidation procedures. More recently, it has been discovered that some organic and bio-inspired molecules, such as the chelator called 3,4,3-LI(1,2-HOPO), can also oxidize Bk(III) and stabilize Bk(IV) under mild conditions. Berkelium(IV) is then extracted with ion exchange, extraction chromatography or liquid-liquid extraction using HDEHP (bis-(2-ethylhexyl) phosphoric scid), amines, tributyl phosphate or various other reagents. These procedures separate berkelium from most trivalent actinides and lanthanides, except for the lanthanide cerium (lanthanides are absent in the irradiation target but are created in various nuclear fission decay chains).\n\nA more detailed procedure adopted at the Oak Ridge National Laboratory was as follows: the initial mixture of actinides is processed with ion exchange using lithium chloride reagent, then precipitated as hydroxides, filtered and dissolved in nitric acid. It is then treated with high-pressure elution from cation exchange resins, and the berkelium phase is oxidized and extracted using one of the procedures described above. Reduction of the thus-obtained berkelium(IV) to the +3 oxidation state yields a solution, which is nearly free from other actinides (but contains cerium). Berkelium and cerium are then separated with another round of ion-exchange treatment.\n\nIn order to characterize chemical and physical properties of solid berkelium and its compounds, a program was initiated in 1952 at the Material Testing Reactor, Arco, Idaho, US. It resulted in preparation of an eight-gram plutonium-239 target and in the first production of macroscopic quantities (0.6 micrograms) of berkelium by Burris B. Cunningham and Stanley G. Thompson in 1958, after a continuous reactor irradiation of this target for six years. This irradiation method was and still is the only way of producing weighable amounts of the element, and most solid-state studies of berkelium have been conducted on microgram or submicrogram-sized samples.\n\nThe world's major irradiation sources are the 85-megawatt High Flux Isotope Reactor at the Oak Ridge National Laboratory in Tennessee, USA, and the SM-2 loop reactor at the Research Institute of Atomic Reactors (NIIAR) in Dimitrovgrad, Russia, which are both dedicated to the production of transcurium elements (atomic number greater than 96). These facilities have similar power and flux levels, and are expected to have comparable production capacities for transcurium elements, although the quantities produced at NIIAR are not publicly reported. In a \"typical processing campaign\" at Oak Ridge, tens of grams of curium are irradiated to produce decigram quantities of californium, milligram quantities of berkelium-249 and einsteinium, and picogram quantities of fermium. In total, just over one gram of berkelium-249 has been produced at Oak Ridge since 1967.\n\nThe first berkelium metal sample weighing 1.7 micrograms was prepared in 1971 by the reduction of berkelium(III) fluoride with lithium vapor at 1000 °C; the fluoride was suspended on a tungsten wire above a tantalum crucible containing molten lithium. Later, metal samples weighing up to 0.5 milligrams were obtained with this method.\n\nSimilar results are obtained with berkelium(IV) fluoride. Berkelium metal can also be produced by the reduction of berkelium(IV) oxide with thorium or lanthanum.\n\nTwo oxides of berkelium are known, with the berkelium oxidation state of +3 (BkO) and +4 (BkO). Berkelium(IV) oxide is a brown solid, while berkelium(III) oxide is a yellow-green solid with a melting point of 1920 °C and is formed from BkO by reduction with molecular hydrogen:\n\nUpon heating to 1200 °C, the oxide BkO undergoes a phase change; it undergoes another phase change at 1750 °C. Such three-phase behavior is typical for the actinide sesquioxides. Berkelium(II) oxide, BkO, has been reported as a brittle gray solid but its exact chemical composition remains uncertain.\n\nIn halides, berkelium assumes the oxidation states +3 and +4. The +3 state is the most stable, especially in solutions, while the tetravalent halides BkF and CsBkCl are only known in the solid phase. The coordination of berkelium atom in its trivalent fluoride and chloride is tricapped trigonal prismatic, with the coordination number of 9. In trivalent bromide, it is bicapped trigonal prismatic (coordination 8) or octahedral (coordination 6), and in the iodide it is octahedral.\n\nBerkelium(IV) fluoride (BkF) is a yellow-green ionic solid and is isotypic with uranium tetrafluoride or zirconium(IV) fluoride. Berkelium(III) fluoride (BkF) is also a yellow-green solid, but it has two crystalline structures. The most stable phase at low temperatures is isotypic with yttrium(III) fluoride, while upon heating to between 350 and 600 °C, it transforms to the structure found in lanthanum(III) fluoride.\n\nVisible amounts of berkelium(III) chloride (BkCl) were first isolated and characterized in 1962, and weighed only 3 billionths of a gram. It can be prepared by introducing hydrogen chloride vapors into an evacuated quartz tube containing berkelium oxide at a temperature about 500 °C. This green solid has a melting point of 600 °C, and is isotypic with uranium(III) chloride. Upon heating to nearly melting point, BkCl converts into an orthorhombic phase.\n\nTwo forms of berkelium(III) bromide are known: one with berkelium having coordination 6, and one with coordination 8. The latter is less stable and transforms to the former phase upon heating to about 350 °C. An important phenomenon for radioactive solids has been studied on these two crystal forms: the structure of fresh and aged BkBr samples was probed by X-ray diffraction over a period longer than 3 years, so that various fractions of berkelium-249 had beta decayed to californium-249. No change in structure was observed upon the BkBr—CfBr transformation. However, other differences were noted for BkBr and CfBr. For example, the latter could be reduced with hydrogen to CfBr, but the former could not – this result was reproduced on individual BkBr and CfBr samples, as well on the samples containing both bromides. The intergrowth of californium in berkelium occurs at a rate of 0.22% per day and is an intrinsic obstacle in studying berkelium properties. Beside a chemical contamination, Cf, being an alpha emitter, brings undesirable self-damage of the crystal lattice and the resulting self-heating. The chemical effect however can be avoided by performing measurements as a function of time and extrapolating the obtained results.\n\nThe pnictides of berkelium-249 of the type BkX are known for the elements nitrogen, phosphorus, arsenic and antimony. They crystallize in the rock-salt structure and are prepared by the reaction of either berkelium(III) hydride (BkH) or metallic berkelium with these elements at elevated temperature (about 600 °C) under high vacuum.\n\nBerkelium(III) sulfide, BkS, is prepared by either treating berkelium oxide with a mixture of hydrogen sulfide and carbon disulfide vapors at 1130 °C, or by directly reacting metallic berkelium with elemental sulfur. These procedures yield brownish-black crystals.\n\nBerkelium(III) and berkelium(IV) hydroxides are both stable in 1 molar solutions of sodium hydroxide. Berkelium(III) phosphate (BkPO) has been prepared as a solid, which shows strong fluorescence under excitation with a green light. Berkelium hydrides are produced by reacting metal with hydrogen gas at temperatures about 250 °C. They are non-stoichiometric with the nominal formula BkH (0 < x < 1). Several other salts of berkelium are known, including an oxysulfide (BkOS), and hydrated nitrate (), chloride (), sulfate () and oxalate (). Thermal decomposition at about 600 °C in an argon atmosphere (to avoid oxidation to ) of yields the crystals of berkelium(III) oxysulfate (). This compound is thermally stable to at least 1000 °C in inert atmosphere.\n\nBerkelium forms a trigonal (η–CH)Bk metallocene complex with three cyclopentadienyl rings, which can be synthesized by reacting berkelium(III) chloride with the molten beryllocene (Be(CH)) at about 70 °C. It has an amber color and a density of 2.47 g/cm. The complex is stable to heating to at least 250 °C, and sublimates without melting at about 350 °C. The high radioactivity of berkelium gradually destroys the compound (within a period of weeks). One cyclopentadienyl ring in (η–CH)Bk can be substituted by chlorine to yield [Bk(CH)Cl]. The optical absorption spectra of this compound are very similar to those of (η–CH)Bk.\n\nThere is currently no use for any isotope of berkelium outside basic scientific research. Berkelium-249 is a common target nuclide to prepare still heavier transuranic elements and transactinides, such as lawrencium, rutherfordium and bohrium. It is also useful as a source of the isotope californium-249, which is used for studies on the chemistry of californium in preference to the more radioactive californium-252 that is produced in neutron bombardment facilities such as the HFIR.\n\nA 22 milligram batch of berkelium-249 was prepared in a 250-day irradiation and then purified for 90 days at Oak Ridge in 2009. This target yielded the first 6 atoms of tennessine at the Joint Institute for Nuclear Research (JINR), Dubna, Russia, after bombarding it with calcium ions in the U400 cyclotron for 150 days. This synthesis was a culmination of the Russia—US collaboration between JINR and Lawrence Livermore National Laboratory on the synthesis of elements 113 to 118 which was initiated in 1989.\n\nThe nuclear fission properties of berkelium are different from those of the neighboring actinides curium and californium, and they suggest berkelium to perform poorly as a fuel in a nuclear reactor. Specifically, berkelium-249 has a moderately large neutron capture cross section of 710 barns for thermal neutrons, 1200 barns resonance integral, but very low fission cross section for thermal neutrons. In a thermal reactor, much of it will therefore be converted to berkelium-250 which quickly decays to californium-250. In principle, berkelium-249 can sustain a nuclear chain reaction in a fast breeder reactor. Its critical mass is relatively high at 192 kg; it can be reduced with a water or steel reflector but would still exceed the world production of this isotope.\n\nBerkelium-247 can maintain chain reaction both in a thermal-neutron and in a fast-neutron reactor, however, its production is rather complex and thus the availability is much lower than its critical mass, which is about 75.7 kg for a bare sphere, 41.2 kg with a water reflector and 35.2 kg with a steel reflector (30 cm thickness).\n\nLittle is known about the effects of berkelium on human body, and analogies with other elements may not be drawn because of different radiation products (electrons for berkelium and alpha particles, neutrons, or both for most other actinides). The low energy of electrons emitted from berkelium-249 (less than 126 keV) hinders its detection, due to signal interference with other decay processes, but also makes this isotope relatively harmless to humans as compared to other actinides. However, berkelium-249 transforms with a half-life of only 330 days to the strong alpha-emitter californium-249, which is rather dangerous and has to be handled in a glove box in a dedicated laboratory.\n\nMost available berkelium toxicity data originate from research on animals. Upon ingestion by rats, only about 0.01% berkelium ends in the blood stream. From there, about 65% goes to the bones, where it remains for about 50 years, 25% to the lungs (biological half-life about 20 years), 0.035% to the testicles or 0.01% to the ovaries where berkelium stays indefinitely. The balance of about 10% is excreted. In all these organs berkelium might promote cancer, and in the skeletal system its radiation can damage red blood cells. The maximum permissible amount of berkelium-249 in the human skeleton is 0.4 nanograms.\n\n\n"}
{"id": "22650244", "url": "https://en.wikipedia.org/wiki?curid=22650244", "title": "Boron monoxide", "text": "Boron monoxide\n\nBoron monoxide (BO) is a chemical compound of boron and oxygen. Two experimental studies have proposed existence of diamond-like and graphite-like BO, as for boron nitride and carbon solids. However, a later, systematic, experimental study of boron oxide phase diagram suggests that BO is unstable. The instability of the graphite-like BO phase was also predicted theoretically.\n\n"}
{"id": "4263491", "url": "https://en.wikipedia.org/wiki?curid=4263491", "title": "Cavity method", "text": "Cavity method\n\nThe cavity method is a mathematical method presented by M. Mezard, Giorgio Parisi and Miguel Angel Virasoro in 1985 to solve some mean field type models in statistical physics, specially adapted to disordered systems. The method has been used to compute properties of ground states in many condensed matter and optimization problems. \n\nInitially invented to deal with the Sherrington Kirkpatrick model of spin glasses, the cavity method has shown wider applicability. It can be regarded as a generalization of the Bethe Peierls iterative method in tree-like graphs, to the case of a graph with loops that are not too short. The different approximations that can be done with the cavity method are usually named after their equivalent with the different steps of the replica method which is mathematically more subtle and less intuitive than the cavity approach.\n\nThe cavity method has proved useful in the solution of optimization problems such as k-satisfiability and graph coloring. It has yielded not only ground states energy predictions in the average case, but also has inspired algorithmic methods.\n\nThe cavity method originated in the context of statistical physics, but is also closely related to methods from other areas such as belief propagation.\n\n"}
{"id": "403602", "url": "https://en.wikipedia.org/wiki?curid=403602", "title": "Ceiling (aeronautics)", "text": "Ceiling (aeronautics)\n\nWith respect to aircraft performance, a ceiling is the maximum density altitude an aircraft can reach under a set of conditions, as determined by its flight envelope.\n\nService ceiling is where the rate of climb drops below a prescribed value.\n\nThe service ceiling is the maximum usable altitude of an aircraft. Specifically, it is the density altitude at which flying in a clean configuration, at the best rate of climb airspeed for that altitude and with all engines operating and producing maximum continuous power, will produce a given rate of climb (a typical value might be 100 feet per minute climb or 30 metres per minute, or on the order of 500 feet per minute climb for jet aircraft). Margin to stall at service ceiling is 1.5 g.\n\nThe one engine inoperative (OEI) service ceiling of a twin-engine, fixed-wing aircraft is the density altitude at which flying in a clean configuration, at the best rate of climb airspeed for that altitude with one engine producing maximum continuous power and the other engine shut down and feathered, will produce a given rate of climb (usually 50 feet per minute).\n\nHowever some performance charts will define the service ceiling as the pressure altitude at which the aircraft will have the capability of climbing at 50 ft/min with one propeller feathered.\n\nThe absolute ceiling is the highest altitude at which an aircraft can sustain level flight. Due to the thin air at higher altitudes, a much higher true airspeed is required to generate sufficient lift on the wings. The absolute ceiling is therefore the altitude at which the engines are operating at maximum thrust, yet only generate enough lift to match the weight of the aircraft. Hence, the aircraft will not have any excess capacity to climb further. At absolute ceiling, the aircraft can no longer accelerate, since any acceleration will lead to higher airspeed and therefore excess lift. Stated technically, it is the altitude where the maximum sustained (with no decreasing airspeed) rate of climb is zero.\n\nMost commercial jetliners have a service (or certificated) ceiling of about and some business jets about . Before its retirement, the Concorde Supersonic transport (SST) routinely flew at . While the absolute ceiling of these aircraft is much higher than for standard operational purposes—in Concorde's case, it was tested to —it is impossible to reach for most (because of the vertical speed asymptotically approaching zero) without afterburners or other devices temporarily increasing thrust. Another factor that makes it impossible for some aircraft to reach their absolute ceiling, even with temporary increases in thrust, is the aircraft reaching the \"coffin corner.\" Flight at the absolute ceiling is also not economically advantageous due to the low indicated airspeed which can be sustained: although the true airspeed (TAS) at an altitude is typically greater than indicated airspeed (IAS), the difference is not enough to compensate for the fact that IAS at which minimum drag is achieved is usually low, so a flight at an absolute ceiling altitude results in a low TAS as well, and therefore in a high fuel burn rate per distance traveled. The absolute ceiling varies with the air temperature and, overall, the aircraft weight (usually calculated at MTOW).\n\n"}
{"id": "1997989", "url": "https://en.wikipedia.org/wiki?curid=1997989", "title": "Crystal twinning", "text": "Crystal twinning\n\nCrystal twinning occurs when two separate crystals share some of the same crystal lattice points in a symmetrical manner. The result is an intergrowth of two separate crystals in a variety of specific configurations. The surface along which the lattice points are shared in twinned crystals is called a composition surface or twin plane. \n\nCrystallographers classify twinned crystals by a number of twin laws. These twin laws are specific to the crystal system. The type of twinning can be a diagnostic tool in mineral identification. Twinning is an important mechanism for permanent shape changes in a crystal.\n\nTwinning can often be a problem in X-ray crystallography, as a twinned crystal does not produce a simple diffraction pattern.\n\nTwin laws are either defined by their twin planes (i.e. {hkl}) or the direction of the twin axes (i.e. [hkl]). If the twin law can be defined by a simple planar composition surface, the twin plane is always parallel to a possible crystal face and never parallel to an existing plane of symmetry (remember that twinning adds symmetry).\n\nIf the twin law is a rotation axis, the composition surface will be irregular, the twin axis will be perpendicular to a lattice plane, but will never be an even-fold rotation axis of the existing symmetry. For example twinning cannot occur on a new 2 fold axis that is parallel to an existing 4-fold axis.\n\nIn the isometric system, the most common types of twins are the Spinel Law (twin plane, parallel to an octahedron), [111] where the twin axis is perpendicular to an octahedral face, and the Iron Cross [001] which is the interpenetration of two pyritohedrons a subtype of dodecahedron.\n\nIn the hexagonal system, calcite shows the contact twin laws {0001} and {0112}. Quartz shows the Brazil Law {1120}, and Dauphiné Law [0001] which are penetration twins caused by transformation and Japanese Law {1122} which is often caused by accidents during growth.\n\nIn the tetragonal system, cyclical contact twins are the most commonly observed type of twin, such as in rutile titanium dioxide and cassiterite tin oxide. \n\nIn the orthorhombic system, crystals usually twin on planes parallel to the prism face, where the most common is a {110} twin which produces cyclical twins, such as in aragonite, chrysoberyl, and cerussite.\n\nIn the monoclinic system, twin occur most often on the planes {100} and {001} by the Manebach Law {001}, Carlsbad Law [001], Braveno Law {021} in orthoclase, and the Swallow Tail Twins {001} in gypsum.\n\nIn the triclinic system, the most commonly twinned crystals are the feldspar minerals plagioclase and microcline. These minerals show the Albite and Pericline Laws.\n\nSimple twinned crystals may be contact twins or penetration twins. \"Contact twins\" share a single composition surface often appearing as mirror images across the boundary. Plagioclase, quartz, gypsum, and spinel often exhibit contact twinning. \"Merohedral twinning\" occurs when the lattices of the contact twins superimpose in three dimensions, such as by relative rotation of one twin from the other. An example is metazeunerite. In \"penetration twins\" the individual crystals have the appearance of \"passing through\" each other in a symmetrical manner. Orthoclase, staurolite, pyrite, and fluorite often show penetration twinning.\nIf several twin crystal parts are aligned by the same twin law they are referred to as \"multiple\" or \"repeated twins\". If these multiple twins are aligned in parallel they are called \"polysynthetic twins\". When the multiple twins are not parallel they are \"cyclic twins\". Albite, calcite, and pyrite often show polysynthetic twinning. Closely spaced polysynthetic twinning is often observed as striations or fine parallel lines on the crystal face. Rutile, aragonite, cerussite, and chrysoberyl often exhibit cyclic twinning, typically in a radiating pattern.\nBut in general, based on the relationship between the twin axis and twin plane, there are 3 types of twinning:\n\nThere are three modes of formation of twinned crystals. \"Growth twins\" are the result of an interruption or change in the lattice during formation or growth due to a possible deformation from a larger substituting ion. \"Annealing\" or \"transformation twins\" are the result of a change in crystal system during cooling as one \"form\" becomes unstable and the crystal structure must re-organize or \"transform\" into another more stable form. \"Deformation\" or \"gliding twins\" are the result of stress on the crystal after the crystal has formed. If an \"FCC\" metal like aluminium experiences extreme stresses, it will experience twinning, as seen in the case of explosions. Deformation twinning is a common result of regional metamorphism. Crystal twinning is also used as an indicator of force direction in mountain building processes in orogeny research.\n\nCrystals that grow adjacent to each other may be aligned to resemble twinning. This \"parallel growth\" simply reduces system energy and is not twinning.\n\nTwin boundaries can be constructed from both twist and tilt boundaries.\n\nTwinning can occur cooperative displacement of atoms along the face of the twin boundary. This displacement of a large quantity of atoms simultaneously requires significant energy to perform. Therefore, the theoretical stress required to form a twin is quite high. It is believed that twinning is associated with dislocation motion on a coordinated scale, in contrast to slip, which is caused by independent glide at several locations in the crystal.\n\nTwinning and slip are competitive mechanisms for crystal deformation. Each mechanism is dominant in certain crystal systems and under certain conditions. In FCC metals, slip is almost always dominant because the stress required is far less than twinning stress.\n\nCompared to slip, twinning produces a deformation pattern that is more heterogeneous in nature. This deformation produces a local gradient across the material and near intersections between twins and grain boundaries. The deformation gradient can lead to fracture along the boundaries, particularly in BCC transition metals at low temperatures.\n\nThe conditions of crystal formation in solution have an effect on the type and density of dislocations in the crystal. It frequently happens that the crystal is oriented so that there will a more rapid deposition of material on one part than on another; for instance, if the crystal be attached to some other solid it cannot grow in that direction. If the crystal is freely suspended in the solution and material for growth is supplied at the same rate on all sides does an equably developed form result.\n\nTwin boundaries occur when two crystals of the same type intergrow so that only a slight misorientation exists between them. It is a highly symmetrical interface, often with one crystal the mirror image of the other; also, atoms are shared by the two crystals at regular intervals. This is also a much lower-energy interface than the grain boundaries that form when crystals of arbitrary orientation grow together. Twin boundaries may also display a higher degree of symmetry than the single crystal. These twins are called \"mimetic\" or \"pseudo-symmetric\" twins.\n\nTwin boundaries are partly responsible for shock hardening and for many of the changes that occur in cold work of metals with limited slip systems or at very low temperatures. They also occur due to martensitic transformations: the motion of twin boundaries is responsible for the pseudoelastic and shape-memory behavior of nitinol, and their presence is partly responsible for the hardness due to quenching of steel. In certain types of high strength steels, very fine deformation twins act as primary obstacles against dislocation motion. These steels are referred to as 'TWIP' steels, where TWIP stands for \"Twinning Induced Plasticity\".\n\nOf the three common crystalline structures BCC, FCC, and HCP, the HCP structure is the most likely to form deformation twins when strained, because they rarely have a sufficient number of slip systems for an arbitrary shape change. High strain rates, low stacking-fault energy and low temperatures facilitate deformation twinning.\n\n\n\n"}
{"id": "26381612", "url": "https://en.wikipedia.org/wiki?curid=26381612", "title": "Cu-Pt type ordering in III-V semiconductor", "text": "Cu-Pt type ordering in III-V semiconductor\n\nOne of the most studied atomic ordering is CuPt type ordering in III-V semiconductor alloy. It occurs in III-V alloy when the cation planes take an alternate sequence of A-rich and B-rich plane following ABC. The resulting structure is usually called as superlattice-like structure along [1-11] or [-111] – called (111) B plane, or along [-1-1-1] or [11-1] – called (111) A plane. The two ordering direction is thus called CuPt type A and B ordering.\n\nThe same type ordering is also observed in II-VI alloy such as CdZnTe alloy. The ordering sequence viewed on atomic planes along [-111] and [1-11] reveals Cd and Zn alternating planes. Figure 1 below shows schematically the ordering mentioned:\n\nFigure 1. Example of Cu-Pt type ordering in CdZnTe alloy\n\nThis ordering reduces the lattice symmetry ordering to L11 and leads to change of its optical and electrical properties. The apparent effects are: band gap reduction, valence band splitting, and optical anisotropy. For instance, in CuPt type B ordering, the L point of the Brillouin zone fold back to the Γ-point, and the folded band “repels” the original conduction band minimum and bring down the bottom of conduction band.\n\n"}
{"id": "55616265", "url": "https://en.wikipedia.org/wiki?curid=55616265", "title": "Darrynane Beg Ogham Stone", "text": "Darrynane Beg Ogham Stone\n\nThe Darrynane Beg Ogham Stone is an ogham stone (CIIC 220) and a National Monument located in County Kerry, Ireland.\n\nThe stone originally lay recumbent on Derrynane strand. The Office of Public Works erected it by the roadside in the 1940s.\n\nThis stone was erected as a grave marker, with inscription in Primitive Irish, some time in the early 6th century AD.\n\nThe stone is sandstone grit, 211 × 51 × 30 cm. The inscription, heavily weathered, reads (\"name of Llatigni, son of Minerc, of the tribe of Q...ci\") \"Llatigni\" contains the diminutive particle -\"gno\"-, suggesting the name \"Láithbe\" or \"Láithech\". \"Min\"- is also a diminutive particle, suggesting \"Little Erc\" (\"Erc Becc\").\n"}
{"id": "13679083", "url": "https://en.wikipedia.org/wiki?curid=13679083", "title": "Domestic energy consumption", "text": "Domestic energy consumption\n\nDomestic energy consumption is the total amount of energy used in a house for household work. The amount of energy used per household varies widely depending on the standard of living of the country, the climate, and the age and type of residence.\n\nIn the United States as of 2008, in an average household in a temperate climate, the yearly use of household energy can be composed as follows:\n\nThis equates to an average instantaneous power consumption of 2 kW at any given time.\n\nHouseholds in different parts of the world will have differing levels of consumption, based on latitude and technology.\n\nAs of 2015, the average annual household electricity consumption in the US is 10,766 kWh.\n\n"}
{"id": "517512", "url": "https://en.wikipedia.org/wiki?curid=517512", "title": "Draft horse", "text": "Draft horse\n\nA draft horse (US), draught horse (UK, Ireland and Commonwealth) or dray horse (from the Old English \"dragan\" meaning \"to draw or haul\"; compare Dutch \"dragen\" and German \"tragen\" meaning \"to carry\" and Danish \"drage\" meaning \"to draw\" or \"to fare\"), less often called a carthorse, work horse or heavy horse, is a large horse bred to be a working animal doing hard tasks such as plowing and other farm labor. There are a number of breeds, with varying characteristics, but all share common traits of strength, patience, and a docile temperament which made them indispensable to generations of pre-industrial farmers.\n\nDraft horses and draft crossbreds are versatile breeds used today for a multitude of purposes, including farming, draft horse showing, logging, recreation, and other uses. They are also commonly used for crossbreeding, especially to light riding breeds such as the Thoroughbred, for the purpose of creating sport horses of warmblood type. While most draft horses are used for driving, they can be ridden and some of the lighter draft breeds are capable performers under saddle.\n\nDraft horses are recognizable by their tall stature and extremely muscular build. In general, they tend to have a more upright shoulder, producing more upright movement and conformation that is well-suited for pulling. They tend to have broad, short backs with powerful hindquarters, again best suited for the purpose of pulling. Additionally, the draft breeds usually have heavy bone, and a good deal of feathering on their lower legs. Many have a straight profile or \"Roman nose\" (a convex profile). Draft breeds range from approximately high and from .\n\nDraft horses crossbred on light riding horses adds height and weight to the ensuing offspring, and may increase the power and \"scope\" of the animal's movement.\n\nThe largest horse in recorded history was probably a Shire named Sampson (later Mammoth), who was born in 1846. He stood high, and his peak weight was estimated at . At over , a Shire gelding named Goliath was the Guinness Book of World Records record holder for the world's tallest horse until his death in 2001.\n\nHumans domesticated horses and needed them to perform a variety of duties. One type of horse-powered work was the hauling of heavy loads, plowing fields, and other tasks that required pulling ability. A heavy, calm, patient, well-muscled animal was desired for this work. Conversely, a light, more energetic horse was needed for riding and rapid transport. Thus, to the extent possible, a certain amount of selective breeding was used to develop different types of horse for different types of work.\nIt is a common misunderstanding that the Destrier that carried the armoured knight of the Middle Ages had the size and conformation of a modern draft horse, and some of these Medieval war horses may have provided some bloodlines for some of the modern draft breeds. The reality was that the high-spirited, quick-moving Destrier was closer to the size, build, and temperament of a modern Andalusian or Friesian. There also were working farm horses of more phlegmatic temperaments used for pulling military wagons or performing ordinary farm work which provided bloodlines of the modern draft horse. Records indicate that even medieval drafts were not as large as those today. Of the modern draft breeds, the Percheron probably has the closest ties to the medieval war horse.\nBy the 19th century, horses weighing more than that also moved at a quick pace were in demand. Tall stature, muscular backs, and powerful hindquarters made the draft horse a source of “horsepower” for farming, hauling freight and moving passengers. The railroads increased demand for working horses, as a growing economy still needed transport over the 'last mile' between the goods yard or station and the final customer. Even in the 20th century, draft horses were used for practical work, including over half a million used during World War I to support the military effort, until motor vehicles became an affordable and reliable substitute.\n\nIn the late 19th century and early 20th century, thousands of draft horses were imported from Western Europe into the United States. Percherons came from France, Belgians from Belgium, Shires from England, Clydesdales from Scotland. Many American draft registries were founded in the late 19th century. The Percheron, with 40,000 broodmares registered as of 1915, was America’s most numerous draft breed at the turn of the 20th century. A breed developed exclusively in the U.S. was the American Cream Draft, which had a stud book established by the 1930s.\n\nBeginning in the late 19th century, and with increasing mechanization in the 20th century, especially following World War I in the USA and after World War II in Europe, the popularity of the internal combustion engine, and particularly the tractor, reduced the need for the draft horse. Many were sold to slaughter for horsemeat and a number of breeds went into significant decline.\n\nToday draft horses are most often seen at shows, pulling competition and entered in competitions called \"heavy horse\" trials, or as exhibition animals pulling large wagons. However, they are still seen on some smaller farms in the USA and Europe. They are particularly popular with groups such as Amish and Mennonite farmers, as well as those individuals who wish to farm with a renewable source of power. They are also sometimes used during forestry management to remove logs from dense woodland where there is insufficient space for mechanized techniques. Crossbred draft horses also played a significant role in the development of a number of warmblood breeds, popular today in international FEI competition up to the Olympic Equestrian level.\n\nSmall areas still exist where draft horses are widely used as transportation, due to legislation preventing automotive traffic, such as on Mackinac Island in the United States.\n\nFeeding, caring for and shoeing a one-ton draft horse is costly. Although many draft horses can work without a need for shoes, if they are required, farriers may charge twice the price to shoe a draft horse as a light riding horse because of the extra labor and specialized equipment required. Historically, draft horses were shod with horseshoes that were significantly wider and heavier than those for other types of horses, custom-made, often with caulkins.\n\nThe draft horse’s metabolism is a bit slower than riding horse breeds, more akin to that of ponies, requiring less feed per pound of body weight. This is possibly due to their calmer nature. Nonetheless, because of their sheer size, most require a significant amount of fodder per day. Generally a supplement to balance nutrients is preferred over a large quantity of grain. They consume hay or other forage from 1.5% to 3% of their body weight per day, depending on work level. They also can drink up to of water a day. Overfeeding can lead to obesity, and risk of laminitis can be a concern.\n\nThe Shire horse holds the record for the world's biggest horse; Sampson, foaled in 1846 in Bedfordshire, England, stood at his withers, and weighed approx .\n\nA number of horse breeds are used as draft horses, with the popularity of a given breed often closely linked to geographic location. In North America, there were five draft horse breeds on the classic list: Belgian, Clydesdale, Percheron, Shire, and Suffolk. \n\nThe Draft Cross Breeders and Owners Association recognizes the following breeds as draft horses: \nAdditional breeds may be classified as draft horses by other organizations.\n\nThe terms harness horse and light harness horse refer to horses of a lighter build, such as traditional carriage horses and show horses, and are not terms generally used to denote \"heavy\" or draft horses. Harness horse breeds include heavy warmblood breeds such as the Oldenburg and Cleveland Bay, as well as lighter breeds such as the Hackney, and in some disciplines, such as combined driving, light riding breeds such as the Thoroughbred or Morgan may be seen.\n\n\n"}
{"id": "946919", "url": "https://en.wikipedia.org/wiki?curid=946919", "title": "Edmund Germer", "text": "Edmund Germer\n\nEdmund Germer (August 24, 1901 in Berlin – August 10, 1987) was a German inventor recognized as the father of the fluorescent lamp. He applied for a patent with Friedrich Meyer and Hans J. Spanner on December 10, 1926, which led to . The patent was later purchased by the General Electric Company, which also licensed his patent on the high-pressure mercury-vapor lamp.\n\nThe idea of coating the tube of an arc lamp emitting in the ultraviolet with fluorescing powder to transform UV into visible light led to the realization of arc discharge emitters with spectral quality competing with incandescent emitters. Prior to that, the unpleasant color of the emitted light made these lamps unfit for daily use, despite their much higher efficiency.\n\nHe and Hans Spanner were each awarded the Franklin Institute's Frank P. Brown Medal in 1954.\n\n"}
{"id": "53066523", "url": "https://en.wikipedia.org/wiki?curid=53066523", "title": "Energy Research &amp; Social Science", "text": "Energy Research &amp; Social Science\n\nEnergy Research & Social Science is a monthly peer-reviewed academic journal covering social science research on energy systems and energy and society, including anthropology, economics, geography, psychology, political science, social policy, sociology, science and technology studies and legal studies. It was established in 2014 and is published by Elsevier. The editor-in-chief is Benjamin K. Sovacool (Aarhus University and University of Sussex).\n\nThe journal is abstracted and indexed in:\n\n"}
{"id": "15655623", "url": "https://en.wikipedia.org/wiki?curid=15655623", "title": "Energy Watch Group", "text": "Energy Watch Group\n\nThe Energy Watch Group (EWG) is an international network of scientists and parliamentarians. The EWG conducts research and publishes studies on global energy developments concerning both fossil fuels and renewables. The organization states that it seeks to provide energy policy with objective information.\n\nThe EWG was founded in 2006 by the former German parliamentarian & The Order of Merit of the Federal Republic of Germany awardee Hans-Josef Fell and further parliamentarians from other countries to provide both experts and political decision makers as well as the public with information on energy issues.\n\nThe EWG conducts research on energy issues, including natural gas, crude oil, coal, renewables and uranium. In particular, they focus on three interrelated topics:\n\nThe EWG studies examines ecological, technological and economic connections in the energy sector to estimate developments in the availability and supply of different energy sources and production techniques.\nThe results of the EWG studies are to be presented not only to experts but also to the politically interested public. All EWG studies are open access and are available on the website.\n\nNext to topics covering the energy transition towards a system based on 100% renewables, the EWG is also monitoring and regularly informing about the latest developments in the global divestment movement on their website.\n\nStudies of the EWG by and large come to the conclusion that the planet will run out of fossil fuels earlier than previously thought. The global supply of fossil fuels is therefore extremely strained. An early study of the EWG estimates that there is far less coal available than what is commonly expected. Moreover, coal is distributed very unevenly across countries. 85% of global coal reserves are situated in six countries: USA, Russia, India, China and South Africa. The report suggests that a global peak of coal production will occur in 2025 the latest. The situation for crude oil is even more critical. Global oil production is said to collapse to 40% in 2030 compared to production in 2012. According to calculations by the EWG, peak-oil has already been reached in 2006 with a global oil production maximum of 81 million barrels per day and is now on a steep decline.\n\nThe EWG further maintains that neither new production techniques such as fracking, nor nuclear power nor a diversification of the fossil fuel portfolio can reverse the trend of a collapsing conventional energy system. A recent EWG report warns that fracking not only has catastrophic consequences for the environmental and detrimental health impacts, but is also economically unviable, particularly in Europe. The US is heading straight to a peak in shale gas extraction after which production will plummet within this decade. Another study claims that a diversification of natural gas imports to decrease the EU's dependence on Russia is not an option. Neither Russia nor any other producer of natural gas can be a reliable energy supplier for the European Union. As Russian gas supply is declining, there is increasing demand in other countries, including China and Japan, competing for gas resources. Moreover, liquid natural gas (LNG) cannot contribute to security of supply. Although the EU has the capacity to import 200 billion m³/year, it only imported 45 billion m³/year. According to the EWG, this is a clear indication that the producing countries lack export capacities. Substitution of declining fossil fuel reserves with nuclear energy is also doomed to fail due to two factors. First, with the proven reserves, the stocks will be exhausted in 30 years if demand remains constant. Second, as only 3–4 reactors per year are currently being completed, the competition of 15-20 would be required to maintain present reactor capacity.\n\nSeveral other studies argue that a global shortage of fossil energy supply can only can be intercepted by a rigorous extension of the renewable energy system. The potential for this endeavour is greater than previously thought, according to the EWG. An EWG study posits that such arguments against wind power as fluctuations of wind, lack of grid connections and lack of reserve capacities do not hold due to improvements in planning, growing price incentives and technical improvements. In 2008, the EWG estimated that 17–29% of global energy demand can be covered by renewable energy depending on the willingness to invest. The report maintains that political will is the most crucial obstacle to a global energy transition.\n\nIn 2016/17, the main focus of EWG's studies based on model based approaches concerning how a transformation towards a 100% RE system can be achieved sustainably, e.g. for India, Iran and Nigeria. Also storage technologies are an important factor of a successful energy transition, thus this topic is covered in a paper about the transition of the energy system in Ukraine.\n\nA collaborative study with the Lappeenranta University of Technology is currently in progress (April 2017). It will tackle the question how a global solution towards an energy system based on 100% renewables can be achieved in a feasible and low cost way and across four major sectors: electricity, heating, industrial demand and transport.\n\nSeveral statements made by the EWG are in stark contrast with those of the International Energy Agency (IEA) and other organizations in the field. The EWG even claims that the IEA is institutionally biased towards conventional energy sources and follows a 'hidden agenda' to keep up the belief of an abundant supply of fossil energy sources while downplaying the potential for renewable energy and publishing misleading data.\n\nThe Energy Watch Group criticism of the IEA credibility has attracted a lot of attention in the international media. The EWG achieved a partial victory when the IEA confirmed the EWG's warnings of a shrinking global supply of fossil fuels in 2010.\n\nSince then, the EWG has published a series of studies examining the biased and misleading scenarios of the IEA, including the study on the World Energy Outlook Reports from 1994 to 2004, the analysis of the IEA's Medium Term Renewable Energy Market Report 2016 and the analysis of the World Energy Outlook 2015 and the World Energy Outlook 2016. The EWG findings were also revisited by specialist media.\n\nThe Energy Watch Group has also criticized the German government in recent years for their controversial policies in regard to climate change and Energiewende. The targets set by the German Federal Government are insufficient to meet the climate targets of the international Paris Agreement and also slow down the installation of renewable energy sources, according to EWG.\n\nForthcoming \n2017\n2016\n2015\n\n\n2014\n\n\n2013\n\n\n2010\n\n\n2009\n\n\n2008\n\n\n2007\n\n\n2006\n\n\n"}
{"id": "39211211", "url": "https://en.wikipedia.org/wiki?curid=39211211", "title": "Freeze-casting", "text": "Freeze-casting\n\nFreeze-casting is a technique that exploits the highly anisotropic solidification behavior of a solvent (generally water) in a well-dispersed slurry to template controllably a directionally porous ceramic. By subjecting an aqueous slurry to a directional temperature gradient, ice crystals will nucleate on one side of the slurry and grow along the temperature gradient. The ice crystals will redistribute the suspended ceramic particles as they grow within the slurry, effectively templating the ceramic.\n\nOnce solidification has ended, the frozen, templated ceramic is placed into a freeze-dryer to remove the ice crystals. The resulting green body contains anisotropic macropores in a replica of the sublimated ice crystals and micropores found between the ceramic particles in the walls. This structure is often sintered to consolidate the particulate walls and provide strength to the porous material. The porosity left by the sublimation of solvent crystals is typically between 2–200 μm.\n\nThe first observation of cellular structures resulting from the freezing of water goes back over a century, but the first reported instance of freeze-casting, in the modern sense, was in 1954 when Maxwell et al. attempted to fabricate turbosupercharger blades out of refractory powders. They froze extremely thick slips of titanium carbide, producing near-net-shape castings that were easy to sinter and machine. The goal of this work, however, was to make dense ceramics. It was not until 2001, when Fukasawa et al. created directionally porous alumina castings, that the idea of using freeze-casting as a means of creating novel porous structures really took hold. Since that time, research has grown considerably with hundreds of papers coming out within the last decade.\n\nBecause freeze-casting is a physical process, the techniques developed for one material system can be applied to a wide range of materials. Additionally, due to the inordinate amount of control and broad range of possible porous microstructures that freeze-casting can produce, the technique has found its niche in a number of disparate fields such as tissue scaffolds, photonics, metal-matrix composites, dentistry, materials science, and even food science \n\nThere are three possible end results to uni-directionally freezing a suspension of particles. First, the ice-growth proceeds as a planar front, pushing particles in front like a bulldozer pushes a pile of rocks. This scenario usually occurs at very low solidification velocities (< 1 μm s) or with extremely fine particles because they can move by Brownian motion away from the front. The resultant structure contains no macroporosity. If one were to increase the solidification speed, the size of the particles or solid loading moderately, the particles begin to interact in a meaningful way with the approaching ice front. The result is typically a lamellar or cellular templated structure whose exact morphology depends on the particular conditions of the system. It is this type of solidification that is targeted for porous materials made by freeze-casting. The third possibility for a freeze-cast structure occurs when particles are given insufficient time to segregate from the suspension, resulting in complete encapsulation of the particles within the ice front. This occurs when the freezing rates are rapid, particle size becomes sufficiently large, or when the solids loading is high enough to hinder particle motion.\nTo ensure templating, the particles must be ejected from the oncoming front. Energetically speaking, this will occur if there is an overall increase in free energy if the particle were to be engulfed \"(Δσ > 0)\".\n\nformula_1\n\nwhere \"Δσ\" is the change in free energy of the particle, σ is the surface potential between the particle and interface, \"σ\" is the potential between the particle and the liquid phase and \"σ\" is the surface potential between the solid and liquid phases. This expression is valid at low solidification velocities, when the system is shifted only slightly from equilibrium. At high solidification velocities, kinetics must also be taken into consideration. There will be a liquid film between the front and particle to maintain constant transport of the molecules which are incorporated into the growing crystal. When the front velocity increases, this film thickness \"(d)\" will decrease due to increasing drag forces. A critical velocity \"(v)\" occurs when the film is no longer thick enough to supply the needed molecular supply. At this speed the particle will be engulfed. Most authors express v as a function of particle size where formula_2. The transition from a porous R (lamellar) morphology to one where the majority of particles are entrapped occurs at \"v\", which was defined by Deville et al. to be:\n\nformula_3\n\nwhere \"a\" is the average intermolecular distance of the molecule that is freezing within the liquid, \"d\" is the overall thickness of the liquid film, \"η\" is the solution viscosity, \"R\" is the particle radius and \"z\" is an exponent that can vary from 1 to 5. As expected, we see that \"v\" decreases as particle radius \"R\" goes up.\n\nWaschkies et al. studied the structure of dilute to concentrated freeze-casts from low (< 1 μm s) to extremely high (> 700 μm s) solidification velocities. From this study, they were able to generate morphological maps for freeze-cast structures made under various conditions. Maps such as these are excellent for showing general trends, but they are quite specific to the materials system from which they were derived. For most applications where freeze-casts will be used after freezing, binders are needed to supply strength in the green state. The addition of binder can significantly alter the chemistry within the frozen environment, depressing the freezing point and hampering particle motion leading to particle entrapment at speeds far below the predicted \"v\". Assuming, however, that we are operating at speeds below v and above those which produce a planar front, we will achieve some cellular structure with both ice-crystals and walls composed of packed ceramic particles. The morphology of this structure is tied to some variables, but the most influential is the temperature gradient as a function of time and distance along the freezing direction.\n\nFreeze-casts have at least three apparent morphological regions. At the side where freezing initiates is a nearly isotropic region with no visible macropores dubbed the Initial Zone (IZ). Directly after the IZ is the Transition Zone (TZ), where macropores begin to form and align with one another. The pores in this region may appear randomly oriented. The third zone is called the Steady-State Zone (SSZ), macropores in this region are aligned with one another and grow in a regular fashion. Within the SSZ, the structure is defined by a value λ that is the average thickness of a ceramic wall and its adjacent macropore.\n\nAlthough the ability of ice to exclude suspended particles has long been known, the mechanism is still being debated. It was believed initially that during the moments immediately following the nucleation of the ice crystals, particles were ejected from the growing planar ice front, leading to the formation of a constitutionally super-cooled zone directly ahead of the growing ice. This unstable region eventually resulted in perturbations, breaking the planar front into a columnar ice front, a phenomenon better known as a Mullins-Serkerka instability. After the breakdown, the ice crystals grow along the temperature gradient, pushing ceramic particles from the liquid phase aside so that they accumulate between the growing ice crystals. However, recent in-situ X-ray radiography of directionally frozen alumina suspensions reveal a different mechanism.\n\nIn-situ testing reveals that freeze-casting is an aggressive growth process. In the moments immediately before nucleation, the suspension is in an unstable super-cooled state. Homogeneous (spatially speaking) nucleation of ice crystals occurs followed by explosive crystal growth in every spatial and crystallographic direction. The initial nucleation and growth steps are so rapid (approaching 800 mm s) that all suspended particles are completely engulfed by the oncoming ice front because not enough time is given for particle redistribution, resulting in a structure with anisotropic particle distribution. This step is what provides the initial zone structure.\n\nAs solidification slows and growth kinetics become rate-limiting, the ice crystals begin to exclude the particles, redistributing them within the suspension. A competitive growth process develops between two crystal populations, those with their basal planes aligned with the thermal gradient (z-crystals) and those that are randomly oriented (r-crystals) giving rise to the start of the TZ.\n\nThere are colonies of similarly aligned ice crystals growing throughout the suspension. There are fine lamellae of aligned z-crystals growing with their basal planes aligned with the thermal gradient. The r-crystals appear in this cross-section as platelets but in actuality, they are most similar to columnar dendritic crystals cut along a bias. Within the transition zone, the r-crystals either stop growing or turn into z-crystals that eventually become the predominant orientation, and lead to steady-state growth. There are some reasons why this occurs. For one, during freezing, the growing crystals tend to align with the temperature gradient, as this is the lowest energy configuration and thermodynamically preferential. Aligned growth, however, can mean two different things. Assuming the temperature gradient is vertical, the growing crystal will either be parallel (z-crystal) or perpendicular (r-crystal) to this gradient. A crystal that lays horizontally can still grow in line with the temperature gradient, but it will mean growing on its face rather than its edge. Since the thermal conductivity of ice is so small (1.6 - 2.4 W mK) compared with most every other ceramic (ex. AlO= 40 W mK), the growing ice will have a significant insulative effect on the localized thermal conditions within the slurry. This can be illustrated using simple resistor elements.\n\nWhen ice crystals are aligned with their basal planes parallel to the temperature gradient (z-crystals), they can be represented as two resistors in parallel. The thermal resistance of the ceramic is significantly smaller than that of the ice however, so the apparent resistance can be expressed as the lower R. If the ice crystals are aligned perpendicular to the temperature gradient (r-crystals), they can be approximated as two resistor elements in series. For this case, the R is limiting and will dictate the localized thermal conditions. The lower thermal resistance for the z-crystal case leads to lower temperatures and greater heat flux at the growing crystals tips, driving further growth in this direction while, at the same time, the large R value hinders the growth of the r-crystals. Each ice crystal growing within the slurry will be some combination of these two scenarios. Thermodynamics dictate that all crystals will tend to align with the preferential temperature gradient causing r-crystals to eventually give way to z-crystals, which can be seen from the following radiographs taken within the TZ.\n\nWhen z-crystals become the only significant crystal orientation present, the ice-front grows in a steady-state manner except there are no significant changes to the system conditions. It was observed in 2012 that, in the initial moments of freezing, there are dendritic r-crystals that grow 5 - 15 times faster than the solidifying front. These shoot up into the suspension ahead of the main ice front and partially melt back. These crystals stop growing at the point where the TZ will eventually fully transition to the SSZ. Researchers determined that this particular point marks the position where the suspension is in an equilibrium state (i.e. freezing temperature and suspension temperature are equal). We can say then that the size of the initial and transition zones are controlled by the extent of supercooling beyond the already low freezing temperature. If the freeze-casting setup is controlled so that nucleation is favored at only small supercooling, then the TZ will give way to the SSZ sooner.\n\nThe structure in this final region contains long, aligned lamellae that alternate between ice crystals and ceramic walls. The faster a sample is frozen, the finer its solvent crystals (and its eventual macroporosity) will be. Within the SSZ, the normal speeds which are usable for colloidal templating are 10 – 100 mm s leading to solvent crystals typically between 2 mm and 200 mm. Subsequent sublimation of the ice within the SSZ yields a green ceramic preform with porosity in a nearly exact replica of these ice crystals. The microstructure of a freeze-cast within the SSZ is defined by its wavelength \"(λ)\" which is the average thickness of a single ceramic wall plus its adjacent macropore. Several publications have reported the effects of solidification kinetics on the microstructures of freeze-cast materials. It has been shown that \"λ\" follows an empirical power-law relationship with solidification velocity \"(υ)\" (Eq. 2.14):\n\nformula_4\n\nBoth \"A\" and \"υ\" are used as fitting parameters as currently there is no way of calculating them from first principles, although it is generally believed that \"A\" is related to slurry parameters like viscosity and solid loading while \"n\" is influenced by particle characteristics.\n\nThere are two general categories of tools for architecture a freeze-cast:\n\nInitially, the materials system is chosen based on what sort of final structure is needed. This review has focused on water as the vehicle for freezing, but there are some other solvents that may be used. Notably, camphene, which is an organic solvent that is waxy at room temperature. Freezing of this solution produces highly branched dendritic crystals. Once the materials system is settled on however, the majority of microstructural control comes from external operational conditions such as mold material and temperature gradient.\n\nThe microstructural wavelength (average pore + wall thickness) can be described as a function of the solidification velocity v (λ= Av) where \"A\" is dependent on solids loading. There are two ways then that the pore size can be controlled. The first is to change the solidification speed that then alters the microstructural wavelength, or the solids loading can be changed. In doing so, the ratio of pore size to wall size is changed.\nIt is often more prudent to alter the solidification velocity seeing as a minimum solid loading is usually desired. Since microstructural size \"(λ)\" is inversely related to the velocity of the freezing front, faster speeds lead to finer structures, while slower speeds produce a coarse microstructure. Controlling the solidification velocity is, therefore, crucial to being able to control the microstructure.\n\nAdditives can prove highly useful and versatile in changing the morphology of pores. These work by affecting the growth kinetics and microstructure of the ice in addition to the topology of the ice-water interface. Some additives work by altering the phase diagram of the solvent. For example, water and NaCl have a eutectic phase diagram. When NaCl is added into a freeze-casting suspension, the solid ice phase and liquid regions are separated by a zone where both solids and liquids can coexist. This briny region is removed during sublimation, but its existence has a strong effect on the microstructure of the porous ceramic. Other additives work by either altering the interfacial surface energies between the solid/liquid and particle/liquid, changing the viscosity of the suspension, or the degree of undercooling in the system. Studies have been done with glycerol, sucrose, ethanol, Coca-Cola, acetic acid and more.\n\nIf a freeze casting setup with a constant temperature on either side of the freezing system is used, (static freeze-casting) the front solidification velocity in the SSZ will decrease over time due to the increasing thermal buffer caused by the growing ice front. When this occurs, more time is given for the anisotropic ice crystals to grow perpendicularly to the freezing direction (c-axis) resulting in a structure with ice lamellae that increase in thickness along the length of the sample.\n\nTo ensure highly anisotropic, yet predictable solidification behavior within the SSZ, dynamic freezing patterns are preferred. Using dynamic freezing, the velocity of the solidification front, and, therefore, the ice crystal size, can be controlled with a changing temperature gradient. The increasing thermal gradient counters the effect of the growing thermal buffer imposed by the growing ice front. It has been shown that a linearly decreasing temperature on one side of a freeze-cast will result in near-constant solidification velocity, yielding ice crystals with an almost constant thickness along the SSZ of an entire sample. However, as pointed out by Waschkies et al. even with constant solidification velocity, the thickness of the ice crystals does increase slightly over the course of freezing.\n\nEven if the temperature gradient within the slurry is perfectly vertical, it is common to see tilting or curvature of the lamellae as they grow through the suspension. To explain this, it is possible to define two distinct growth directions for each ice crystal. There is the direction determined by the temperature gradient, and the one defined by the preferred growth direction crystallographically speaking. These angles are often at odds with one another, and their balance will describe the tilt of the crystal.\n\nThe non-overlapping growth directions also help to explain why dendritic textures are often seen in freeze-casts. This texturing is usually found only on the side of each lamella; the direction of the imposed temperature gradient. The ceramic structure left behind shows the negative image of these dendrites. In 2013, Deville et al. made the observation that the periodicity of these dendrites (tip-to-tip distance) actually seems to be related to the primary crystal thickness.\n\nUp until now, the focus has been mostly on the structure of the ice itself; the particles are almost an afterthought to the templating process but in fact, the particles can and do play a significant role during freeze-casting. It turns out that particle arrangement also changes as a function of the freezing conditions. For example, researchers have shown that freezing velocity has a marked effect on wall roughness. Faster freezing rates produce rougher walls since particles are given insufficient time to rearrange. This could be of use when developing permeable gas transfer membranes where tortuosity and roughness could impede gas flow. It also turns out that z- and r-crystals do not interact with ceramic particles in the same way. The z-crystals pack particles in the x-y plane while r-crystals pack particles primarily in the z-direction. R-crystals actually pack particles more efficiently than z-crystals and because of this, the area fraction of the particle-rich phase (1 - area fraction of ice crystals) changes as the crystal population shifts from a mixture of z- and r-crystals to only z-crystals. Starting from where ice crystals first begin to exclude particles, marking the beginning of the transition zone, we have a majority of r-crystals and a high value for the particle-rich phase fraction. We can assume that because the solidification speed is still rapid that the particles will not be packed efficiently. As the solidification rate slows down, however, the area fraction of the particle-rich phase drops indicating an increase in packing efficiency. At the same time, the competitive growth process is taking place, replacing r-crystals with z-crystals. At a certain point nearing the end of the transition zone, the particle-rich phase fraction rises sharply since z-crystals are less efficient at packing particles than r-crystals. The apex of this curve marks the point where only z-crystals are present (SSZ). During steady-state growth, after the maximum particle-rich phase fraction is reached, the efficiency of packing increases as steady-state is achieved. In 2011, researchers at Yale University set out to probe the actual spatial packing of particles within the walls. Using small-angle X-ray scattering (SAXS) they characterized the particle size, shape and interparticle spacing of nominally 32 nm silica suspensions that had been freeze-cast at different speeds. Computer simulations indicated that for this system, the particles within the walls should not be touching but rather separated from one another by thin films of ice. Testing, however, revealed that the particles were, in fact, touching and more than that, they attained a packed morphology that cannot be explained by typical equilibrium densification processes.\n\nIn an ideal world, the spatial concentration of particles within the SSZ would remain constant throughout solidification. As it happens, though, the concentration of particles does change during compression, and this process is highly sensitive to solidification speed. At low freezing rates, Brownian motion takes place, allowing particles to move easily away from the solid-liquid interface and maintain a homogeneous suspension. In this situation, the suspension is always warmer than the solidified portion. At fast solidification speeds, approaching VC, the concentration, and concentration gradient at the solid-liquid interface increases because particles cannot redistribute soon enough. When it has built up enough, the freezing point of the suspension is below the temperature gradient in the solution and morphological instabilities can occur. For situations where the particle concentration bleeds into the diffusion layer, both the actual and freezing temperature dip below the equilibrium freezing temperature creating an unstable system. Often, these situations lead to the formation of what are known as ice lenses.\n\nThese morphological instabilities can trap particles, preventing full redistribution and resulting in inhomogeneous distribution of solids along the freezing direction as well as discontinuities in the ceramic walls, creating voids larger than intrinsic pores within the walls of the porous ceramic.\n\nFreeze-casting can be applied to numerous materials systems including ceramics, polymers, and metals. As long as there are particles that may be excluded when the solvent changes phase, a templated structure is possible. Using various novel processing techniques, some authors have demonstrated even greater levels of control made available with freeze-casting. Munch et al. showed that it is possible to control the long-range arrangement and orientation of crystals normal to the growth direction by templating the nucleation surface. This technique works by providing lower energy nucleation sites to control the initial crystal growth and arrangement. The orientation of ice crystals can also be affected by applied electromagnetic fields as was demonstrated in 2010 by Tang et al. Using specialized setups, researchers have been able to create radially aligned freeze-casts tailored for filtration or gas separation applications. Inspired by Nature, scientists have also been able to use coordinating chemicals and cryopreserved to create remarkably distinctive microstructural architectures \n\n\n\n"}
{"id": "9882858", "url": "https://en.wikipedia.org/wiki?curid=9882858", "title": "Gas spring", "text": "Gas spring\n\nA gas spring is a type of spring that, unlike a typical mechanical spring that relies on elastic deformation, uses compressed gas contained within an enclosed cylinder sealed by a sliding piston to pneumatically store potential energy and withstand external force applied parallel to the direction of the piston shaft.\n\nCommon applications include automobiles (where they are incorporated into the design of struts that support the weight of the hatchback tailgate while they are open) and office chairs. They are also used in furniture, medical and aerospace applications. Much larger gas springs are found in machines that are used in industrial manufacturing (the press tooling industry), where the forces they are required to exert often range from 2500N to 400,000N (forty tonnes).\n\nGas springs are manufactured in many varieties including:\n\nIf the internal plunger has a diaphragm which extends to the side of the gas tube, it will cease to move once an applied force is constant and will support a weight, like a normal spring. If a fine hole exists in the plunger, however, it is termed a \"slow-dampened spring\" and can be used on heavy doors and windows. A gas spring designed for fast operation is termed a \"quick gas spring\" and is used in the manufacture of air guns and recoil buffers.\n\nReducing the gas volume and hence increasing its internal pressure by means of a movable end stop, or allowing one tube to slide over another, can allow the characteristics of a gas spring to be adjusted during operation. The rod may be hollow by use of clever seals or may consist of multiple small-diameter rods. A small amount of oil is normally present.\n\nThe gas may be introduced by a Schrader-type valve, using a lip seal around the rod and forcing it to allow gas in by external overpressure or a shuttling O-ring system. Gas springs of high pressure contain a very large amount of energy and can be used as a power pack. In emergency use the gas may be introduced via a gas generator cell, similar to those used in airbags. Passive heave compensators features large gas springs. The stroke lengths can exceed 6 meter.\n\nA variety of features are available from various manufacturers:\n\n\n"}
{"id": "52102716", "url": "https://en.wikipedia.org/wiki?curid=52102716", "title": "Growth Energy", "text": "Growth Energy\n\nGrowth Energy is an American trade association that represents ethanol producers.\n\nGrowth Energy describes its mission as representing \"the producers and supporters of ethanol, who feed the world and fuel America in ways that achieve energy independence, improve economic well-being, and create a healthier environment for all Americans now.\"\n\nGrowth Energy strongly supports the Renewable Fuel Standard. In a written statement, Tom Buis, co-chairman of the group, said, \"“The RFS is the only meaningful policy to help break Big Oil’s stranglehold on the liquid fuels marketplace. This is an energy policy that is working. It is doing exactly what it was intended to do, with great success. It is irresponsible to rely solely on fossil fuels, and we should not put all our eggs in one basket when it comes to our national and energy security. The bottom line is that ten years after the RFS, Americans across the country are celebrating and recognizing a decade of job creation, rural economic revitalization, clean air, innovation, and increased energy independence and consumer choice.”\n\nSpeaking of comments submitted by Growth Energy on a Technical Assessment Report on Corporate Average Fuel Economy/Greenhouse Gas (CAFE/GHG) standards, a spokesman said, “Our comments highlight the wealth of available research that outlines the vital role that affordable, higher blends of ethanol can play in helping automakers achieve increasing future GHG and CAFÉ standards. Furthermore, we encourage the agencies involved in this review to not only acknowledge the important role higher blends can play, but ensure they are part of the larger goal in achieving greater efficiency and a reduction in harmful emissions.\"\n\nIncreasing consumer access to ethanol is a key issue for the organization. One of the ways Growth Energy does this is by increasing access to ethanol-gasoline blends with higher amounts of ethanol than the standard E10. The organization's \"Prime the Pump\" program encourages retailers to sell E15, containing about 15% ethanol. Their efforts face a number of challenges. First, many states, such as California, prohibit the sale of E15. Second, regulations on Reid vapor pressure often prohibit the sale of E15 to non-flexfuel cars during the summer.\n\nEmily Skor became Growth Energy's CEO in 2016. Before joining Growth Energy, Skor was vice president for communications at vice president of communications at the Consumer Healthcare Products Association (CHPA). She also served as the CHPA Educational Foundation's executive director. Skor was born in Minnesota, where ethanol is an important industry. Skor replaced Tom Buis as CEO.\n\nGrowth Energy presents an annual \"Fueling Growth\" award to members of the United States Congress. The group calls this award “highest honor given to congressional leaders who advocate for renewable fuels like ethanol and consumer choice at the pump.” The winner for 2016 was Senator Charles Grassley of Iowa.\n"}
{"id": "20425286", "url": "https://en.wikipedia.org/wiki?curid=20425286", "title": "How to Live a Low-Carbon Life", "text": "How to Live a Low-Carbon Life\n\nHow to Live a Low-Carbon Life: The Individual's Guide to Stopping Climate Change is a 2007 book by Chris Goodall, published by Earthscan/Routledge.\n\nAccording to \"New Scientist\", this book provides \"the definitive guide to reducing your carbon footprint\". Goodall explains how consumers can cut carbon usage by 75 percent without making drastic lifestyle changes.\n\n\"How to Live a Low-Carbon Life\" has been reviewed in the \"Journal of Environmental Health Research\", \"The Guardian\", and \"The Times\".\n\n\"How to Live a Low-carbon Life\" won the 2007 Clarion award for non-fiction.\n\nA second edition was published in 2010.\n\n\n"}
{"id": "23453755", "url": "https://en.wikipedia.org/wiki?curid=23453755", "title": "International Symposium on Alcohol Fuels", "text": "International Symposium on Alcohol Fuels\n\nThe International Symposium on Alcohol Fuels (ISAF) is a non-profit international organization which gathers together specialists, technologists, executives and technical experts from alcohol, alcohol fuels, methanol, ethers and bio-fuel industries. ISAF came into being in 1976. The 2011 meeting (ISAF-XIX) will be held in Verona, Italy.\n\nISAF brings together technologists, technical experts, technology providers and executives in fields pertaining to the alcohol fuel industry, who share their ideas and consider solutions to the challenges ahead. ISAF discusses substitute energy sources like alcohol fuels and other alternative fuels like methanol, ethers, etc. ISAF furthers the cause of research, development and utilization of alcohol fuels to accelerate the exploitation of clean alternate fuels for vehicles and to reduce environmental pollution both from industrial and motor sources.\n\nReports and handbooks of the ISAF are frequently cited in scholarly articles and treatises about methanol and similar fuels.\n\nISAF symposia have been held in all the continents:\n\n\nISAF XVII was held in Taiyuan China in 2008. The ISAF Symposium is held every two years.\n\nThe theme of ISAF XVIII 2010 was \"Innovation for local and global sustainability of alcohol fuels.\" Delegates discussed methodologies and technologies for production of alcohol fuels, and control of environmental pollution.\n\n"}
{"id": "69453", "url": "https://en.wikipedia.org/wiki?curid=69453", "title": "Interstellar medium", "text": "Interstellar medium\n\nIn astronomy, the interstellar medium (ISM) is the matter and radiation that exists in the space between the star systems in a galaxy. This matter includes gas in ionic, atomic, and molecular form, as well as dust and cosmic rays. It fills interstellar space and blends smoothly into the surrounding intergalactic space. The energy that occupies the same volume, in the form of electromagnetic radiation, is the interstellar radiation field.\n\nThe interstellar medium is composed of multiple phases, distinguished by whether matter is ionic, atomic, or molecular, and the temperature and density of the matter. The interstellar medium is composed primarily of hydrogen followed by helium with trace amounts of carbon, oxygen, and nitrogen comparatively to hydrogen. The thermal pressures of these phases are in rough equilibrium with one another. Magnetic fields and turbulent motions also provide pressure in the ISM, and are typically more important dynamically than the thermal pressure is.\n\nIn all phases, the interstellar medium is extremely tenuous by terrestrial standards. In cool, dense regions of the ISM, matter is primarily in molecular form, and reaches number densities of 10 molecules per cm (1 million molecules per cm). In hot, diffuse regions of the ISM, matter is primarily ionized, and the density may be as low as 10 ions per cm. Compare this with a number density of roughly 10 molecules per cm for air at sea level, and 10 molecules per cm (10 billion molecules per cm) for a laboratory high-vacuum chamber. By mass, 99% of the ISM is gas in any form, and 1% is dust. Of the gas in the ISM, by number 91% of atoms are hydrogen and 8.9% are helium, with 0.1% being atoms of elements heavier than hydrogen or helium, known as \"metals\" in astronomical parlance. By mass this amounts to 70% hydrogen, 28% helium, and 1.5% heavier elements. The hydrogen and helium are primarily a result of primordial nucleosynthesis, while the heavier elements in the ISM are mostly a result of enrichment in the process of stellar evolution.\n\nThe ISM plays a crucial role in astrophysics precisely because of its intermediate role between stellar and galactic scales. Stars form within the densest regions of the ISM, which ultimately contributes to molecular clouds and replenishes the ISM with matter and energy through planetary nebulae, stellar winds, and supernovae. This interplay between stars and the ISM helps determine the rate at which a galaxy depletes its gaseous content, and therefore its lifespan of active star formation.\n\n\"Voyager 1\" reached the ISM on August 25, 2012, making it the first artificial object from Earth to do so. Interstellar plasma and dust will be studied until the mission's end in 2025.\n\nTable 1 shows a breakdown of the properties of the components of the ISM of the Milky Way.\n\n put forward the static two \"phase\" equilibrium model to explain the observed properties of the ISM. Their modeled ISM consisted of a cold dense phase (\"T\" < 300 K), consisting of clouds of neutral and molecular hydrogen, and a warm intercloud phase (\"T\" ~ 10 K), consisting of rarefied neutral and ionized gas. added a dynamic third phase that represented the very hot (\"T\" ~ 10 K) gas which had been shock heated by supernovae and constituted most of the volume of the ISM.\nThese phases are the temperatures where heating and cooling can reach a stable equilibrium. Their paper formed the basis for further study over the past three decades. However, the relative proportions of the phases and their subdivisions are still not well known.\n\nThis model takes into account only atomic hydrogen : Temperature larger than 3000 K breaks molecules, lower than 50 000 K leaves atoms in their ground state. It is assumed that influence of other atoms (He ...) is negligible. Pressure is assumed very low, so that durations of free paths of atoms are larger than ~ 1 nanosecond duration of light pulses which make ordinary, temporally incoherent light .\n\nIn this collisionless gas, Einstein’s theory of coherent light-matter interactions applies, all gas-light interactions are spatially coherent. \nSuppose that a monochromatic light is pulsed, then scattered by molecules having a quadrupole (Raman) resonance frequency. If “length of light pulses is shorter than all involved time constants” (Lamb (1971)), an “impulsive stimulated Raman scattering (ISRS) ” (Yan, Gamble & Nelson (1985)) works: While light generated by incoherent Raman at a shifted frequency has a phase independent on phase of exciting light, thus generates a new spectral line, coherence between incident and scattered light allows their interference into a single frequency, thus shifts incident frequency.\nAssume that a star radiates a continuous light spectrum up to X rays. Lyman frequencies are absorbed in this light and pump atoms mainly to first excited state. In this state, hyperfine periods are longer than 1 ns, so that an ISRS “may” redshift light frequency, populating high hyperfine levels. An other ISRS “may” transfer energy from hyperfine levels to thermal electromagnetic waves, so that redshift is permanent. Temperature of a light beam is defined from frequency and spectral radiance by Planck’s formula. As entropy must increase, “may” becomes “does”. \nHowever, where a previously absorbed line (first Lyman beta, ...) reaches Lyman alpha frequency, redshifting process stops and all hydrogen lines are strongly absorbed. But the stop is not perfect if there is energy at frequency shifted to Lyman beta frequency, which produces a slow redshift. Successive redshifts separated by Lyman absorptions generate many absorption lines, frequencies of which, deduced from absorption process, obey a law more dependable than Karlsson’s formula.\n\nThe previous process excites more and more atoms because a de-excitation obeys Einstein’s law of coherent interactions: Variation dI of radiance I of a light beam along a path dx is dI=BIdx, where B is Einstein amplification coefficient which depends on medium. I is the modulus of Poynting vector of field, absorption occurs for an opposed vector, which corresponds to a change of sign of B. Factor I in this formula shows that intense rays are more amplified than weak ones (competition of modes). Emission of a flare requires a sufficient radiance I provided by random zero point field. After emission of a flare, weak B increases by pumping while I remains close to zero: De-excitation by a coherent emission involves stochastic parameters of zero point field, as observed close to quasars (and in polar auroras).\n\nThe ISM is turbulent and therefore full of structure on all spatial scales. Stars are born deep inside large complexes of molecular clouds, typically a few parsecs in size. During their lives and deaths, stars interact physically with the ISM.\n\nStellar winds from young clusters of stars (often with giant or supergiant HII regions surrounding them) and shock waves created by supernovae inject enormous amounts of energy into their surroundings, which leads to hypersonic turbulence. The resultant structures – of varying sizes – can be observed, such as stellar wind bubbles and superbubbles of hot gas, seen by X-ray satellite telescopes or turbulent flows observed in radio telescope maps.\n\nThe Sun is currently traveling through the Local Interstellar Cloud, a denser region in the low-density Local Bubble.\n\nThe interstellar medium begins where the interplanetary medium of the Solar System ends. The solar wind slows to subsonic velocities at the termination shock, 90—100 astronomical units from the Sun. In the region beyond the termination shock, called the heliosheath, interstellar matter interacts with the solar wind. Voyager 1, the farthest human-made object from the Earth (after 1998), crossed the termination shock December 16, 2004 and later entered interstellar space when it crossed the heliopause on August 25, 2012, providing the first direct probe of conditions in the ISM .\n\nThe ISM is also responsible for extinction and reddening, the decreasing light intensity and shift in the dominant observable wavelengths of light from a star. These effects are caused by scattering and absorption of photons and allow the ISM to be observed with the naked eye in a dark sky. The apparent rifts that can be seen in the band of the Milky Way – a uniform disk of stars – are caused by absorption of background starlight by molecular clouds within a few thousand light years from Earth.\n\nFar ultraviolet light is absorbed effectively by the neutral components of the ISM. For example, a typical absorption wavelength of atomic hydrogen lies at about 121.5 nanometers, the Lyman-alpha transition. Therefore, it is nearly impossible to see light emitted at that wavelength from a star farther than a few hundred light years from Earth, because most of it is absorbed during the trip to Earth by intervening neutral hydrogen.\n\nThe ISM is usually far from thermodynamic equilibrium. Collisions establish a Maxwell–Boltzmann distribution of velocities, and the 'temperature' normally used to describe interstellar gas is the 'kinetic temperature', which describes the temperature at which the particles would have the observed Maxwell–Boltzmann velocity distribution in thermodynamic equilibrium. However, the interstellar radiation field is typically much weaker than a medium in thermodynamic equilibrium; it is most often roughly that of an A star (surface temperature of ~10,000 K) highly diluted. Therefore, bound levels within an atom or molecule in the ISM are rarely populated according to the Boltzmann formula .\n\nDepending on the temperature, density, and ionization state of a portion of the ISM, different heating and cooling mechanisms determine the temperature of the gas.\n\n\n\n\n\n\n\nGrain heating by thermal exchange is very important in supernova remnants where densities and temperatures are very high.\n\nGas heating via grain-gas collisions is dominant deep in giant molecular clouds (especially at high densities). Far infrared radiation penetrates deeply due to the low optical depth. Dust grains are heated via this radiation and can transfer thermal energy during collisions with the gas. A measure of efficiency in the heating is given by the accommodation coefficient:\nwhere \"T\" is the gas temperature, \"T\" the dust temperature, and \"T\" the post-collision temperature of the gas atom or molecule. This coefficient was measured by as \"α\" = 0.35.\n\n\n\n\nRadio waves from ≈10 kHz (very low frequency) to ≈300 GHz (extremely high frequency) propagate differently in interstellar space than on the Earth's surface. There are many sources of interference and signal distortion that do not exist on Earth. A great deal of radio astronomy depends on compensating for the different propagation effects to uncover the desired signal.\n\nThe nature of the interstellar medium has received the attention of astronomers and scientists over the centuries, and understanding of the ISM has developed. However, they first had to acknowledge the basic concept of \"interstellar\" space. The term appears to have been first used in print by : \"The Interstellar Skie.. hath .. so much Affinity with the Starre, that there is a Rotation of that, as well as of the Starre.\" Later, natural philosopher discussed \"The inter-stellar part of heaven, which several of the modern Epicureans would have to be empty.\"\n\nBefore modern electromagnetic theory, early physicists postulated that an invisible luminiferous aether existed as a medium to carry lightwaves. It was assumed that this aether extended into interstellar space, as wrote, \"this efflux occasions a thrill, or vibratory motion, in the ether which fills the interstellar spaces.\"\n\nThe advent of deep photographic imaging allowed Edward Barnard to produce the first images of dark nebulae silhouetted against the background star field of the galaxy, while the first actual detection of cold diffuse matter in interstellar space was made by Johannes Hartmann in 1904 through the use of absorption line spectroscopy. In his historic study of the spectrum and orbit of Delta Orionis, Hartmann observed the light coming from this star and realized that some of this light was being absorbed before it reached the Earth. Hartmann reported that absorption from the \"K\" line of calcium appeared \"extraordinarily weak, but almost perfectly sharp\" and also reported the \"quite surprising result that the calcium line at 393.4 nanometres does not share in the periodic displacements of the lines caused by the orbital motion of the spectroscopic binary star\". The stationary nature of the line led Hartmann to conclude that the gas responsible for the absorption was not present in the atmosphere of Delta Orionis, but was instead located within an isolated cloud of matter residing somewhere along the line-of-sight to this star. This discovery launched the study of the Interstellar Medium.\n\nIn the series of investigations, Viktor Ambartsumian introduced the now commonly accepted notion that interstellar matter occurs in the form of clouds.\n\nFollowing Hartmann's identification of interstellar calcium absorption, interstellar sodium was detected by through the observation of stationary absorption from the atom's \"D\" lines at 589.0 and 589.6 nanometres towards Delta Orionis and Beta Scorpii.\n\nSubsequent observations of the \"H\" and \"K\" lines of calcium by revealed double and asymmetric profiles in the spectra of Epsilon and Zeta Orionis. These were the first steps in the study of the very complex interstellar sightline towards Orion. Asymmetric absorption line profiles are the result of the superposition of multiple absorption lines, each corresponding to the same atomic transition (for example the \"K\" line of calcium), but occurring in interstellar clouds with different radial velocities. Because each cloud has a different velocity (either towards or away from the observer/Earth) the absorption lines occurring within each cloud are either Blue-shifted or Red-shifted (respectively) from the lines' rest wavelength, through the Doppler Effect. These observations confirming that matter is not distributed homogeneously were the first evidence of multiple discrete clouds within the ISM.\n\nThe growing evidence for interstellar material led to comment that \"While the interstellar absorbing medium may be simply the ether, yet the character of its selective absorption, as indicated by Kapteyn, is characteristic of a gas, and free gaseous molecules are certainly there, since they are probably constantly being expelled by the Sun and stars.\"\n\nThe same year Victor Hess's discovery of cosmic rays, highly energetic charged particles that rain onto the Earth from space, led others to speculate whether they also pervaded interstellar space. The following year the Norwegian explorer and physicist Kristian Birkeland wrote: \"It seems to be a natural consequence of our points of view to assume that the whole of space is filled with electrons and flying electric ions of all kinds. We have assumed that each stellar system in evolutions throws off electric corpuscles into space. It does not seem unreasonable therefore to think that the greater part of the material masses in the universe is found, not in the solar systems or nebulae, but in 'empty' space\" .\n\nIn September 2012, NASA scientists reported that polycyclic aromatic hydrocarbons (PAHs), subjected to \"interstellar medium (ISM)\" conditions, are transformed, through hydrogenation, oxygenation and hydroxylation, to more complex organics – \"a step along the path toward amino acids and nucleotides, the raw materials of proteins and DNA, respectively\". Further, as a result of these transformations, the PAHs lose their spectroscopic signature which could be one of the reasons \"for the lack of PAH detection in interstellar ice grains, particularly the outer regions of cold, dense clouds or the upper molecular layers of protoplanetary disks.\"\n\nIn February 2014, NASA announced a greatly upgraded database for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. According to scientists, more than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.\n\n\n"}
{"id": "49405717", "url": "https://en.wikipedia.org/wiki?curid=49405717", "title": "James Douglas Hamilton Dickson", "text": "James Douglas Hamilton Dickson\n\nJames Douglas Hamilton Dickson FRSE MRI (1849–1931) was a Scottish mathematician and expert in electricity. He was a Senior Fellow at Peterhouse, Cambridge. Glasgow University elected him an Eglinton Fellow. He was the elder brother of Charles Dickson, Lord Dickson.\n\nHe had in-depth knowledge in fields of electricity and electrostatics and also a great interest in low temperature physics.\n\nHe was born in Glasgow on 1 May 1849 the son of Dr John Robert Dickson of Edinburgh.\n\nHe attended both Glasgow and Cambridge Universities, graduating MA. From 1867 to 1869 he was assistant to William Thomson, Lord Kelvin, being the joint-builder of the technical equipment which Kelvin used to measure electrostatic energy. In 1869 he also assisted Kelvin in the laying of the first Transatlantic communication cables. The French company overseeing the work were impressed by Dickson and kept him in their employ as Electrician-in-Charge, based in Brest until 1870.\n\nHe then returned to Cambridge to collaborate with W H King and Theophillus Varley in creating more of Lord Kelvin’s machines, including the siphon recorder.\nIn 1877 he became a Maths Tutor at Peterhouse, his alma mater. In 1907 the college made him a Senior Fellow. He was later made a Governor of the college. He was also a Governor of Huntingdon Grammar School.\n\nHe was elected a Fellow of the Royal Society of Edinburgh in 1876. His proposers were Sir James Dewar (his brother-in-law), Peter Guthrie Tait, Alexander Crum Brown, and William Turner.\n\nIn the First World War, at which point he was officially retired, he was asked to fill in for absent masters teaching Maths at both Fettes College and Edinburgh Academy.\n\nHe died on 6 February 1931.\n\nOver and above multiple papers on mathematics and physics, Dickson enjoyed biographical work. Three entries in the Dictionary of National Biography are under his authorship:\n\nDickson could speak Japanese and was very keen on Japanese culture. He had a large collection of swords and tsuba (the ornamental hand guards). On his death these were gifted to the Fitzwilliam Museum in Cambridge and to the Royal Scottish Museum in Edinburgh.\n\nHe married Isobel Catherine Banks, sister of his sister-in-law Hestor Bagot Banks (i.e. the two brothers married two sisters). A third sister, Helen Rose Banks, married Sir James Dewar, connecting all three figures.\n"}
{"id": "30917587", "url": "https://en.wikipedia.org/wiki?curid=30917587", "title": "Karl-Edwin Manshaus", "text": "Karl-Edwin Manshaus\n\nKarl-Edwin Manshaus (born 19 February 1940) is a Norwegian businessperson and civil servant.\n\nHe was born in Bergen, and holds the cand.jur. degree. He was hired in the Norwegian Ministry of Industry in 1970, then in the Norwegian Ministry of Petroleum and Energy in 1978 where he became deputy under-secretary of state. From 1984 to 1987 he served as counsellor in energy affairs at the Norwegian embassy in the United States. From 1988 to 2004 he was the permanent under-secretary of state in the Ministry of Petroleum and Energy, the highest-ranking bureaucratic position. Because of a reorganization, his ministry was merged to form the Ministry of Industry and Energy in 1993, but a Ministry of Petroleum and Energy was re-formed in 1997.\n\nIn 2004 he was hired in ConocoPhillips. He is also the chair of Manshaus Consulting. He was decorated as a Knight, First Class of the Royal Norwegian Order of St. Olav and a Chevalier of the Legion of Honour.\n"}
{"id": "2558074", "url": "https://en.wikipedia.org/wiki?curid=2558074", "title": "Lee Valley Tools", "text": "Lee Valley Tools\n\nLee Valley Tools Ltd. is a Canadian business specializing in tools and gifts for woodworking and gardening.\n\nThe company is family-owned. The founder, Leonard Lee, was a recipient of the Order of Canada. He founded the company in 1978, in Ottawa, Ontario. Over the next ten years, the company opened several more stores (Toronto, Vancouver) and started manufacturing its own line of tools (starting with the Veritas Dovetail Marker in 1982). Since then, it has continued opening stores, manufacturing more diverse tools, and selling through mail order and the Internet. In 1998, Canica Design was launched. Canica is a medical design company associated with Lee Valley Tools which arose out of consultations between Leonard Lee and surgeon Michael Bell after Lee found that Bell was using Lee Valley tools in his plastic surgery practice.\n\nThe primary business is mail-order and retail, purveying mainly woodworking and gardening tools and equipment, as well as woodworking hardware and gifts. The consumer part of the business runs under the main company name, Lee Valley Tools.\n\nLee Valley also has a manufacturing arm, called Veritas Tools. Veritas makes many woodworking hand-tools, including hand planes, marking gauges and other measuring tools, router tables, sharpening systems, and numerous other gadgets. Veritas does research and development activities for the factory line, and has developed and patented many innovative designs.\n\n"}
{"id": "34593676", "url": "https://en.wikipedia.org/wiki?curid=34593676", "title": "Lithium Tokamak Experiment", "text": "Lithium Tokamak Experiment\n\nThe Lithium Tokamak Experiment (LTX), and its predecessor, the Current Drive Experiment-Upgrade (CDX-U), are devices dedicated to the study of liquid lithium as a plasma-facing component (PFC) at Princeton Plasma Physics Laboratory.\n\nOne of the attractive features of a liquid lithium PFC is that it virtually eliminates recycling, or the re-introduction of cold gas back into the plasma from the vacuum chamber walls. This is because lithium has a high chemical reactivity with atomic hydrogen, which is then retained in the PFC. Flowing liquid lithium can also potentially handle the high power densities expected on fusion reactor walls. In addition, lithium has a low atomic number, Z. This gives the lowest possible energy loss by radiation from PFC material that may end up in the plasma, because radiation increases strongly with increasing Z.\n\nAll major tokamaks have obtained their best performance under low recycling conditions. If a fully non-recycling wall can be achieved, theory predicts that the basic nature of magnetic confinement will be changed. The temperature and density profiles, and plasma current distributions, would potentially eliminate deleterious plasma instabilities. Furthermore, the transport mechanisms causing the loss of energy and particles would be reduced, and plasmas with higher energy confinement could result.\n\nOperated at PPPL from 2000 to 2005.\n\nAs the first test of large area liquid lithium PFC, CDX-U had a toroidal tray on the bottom of the vacuum chamber to contain the lithium. Even with this partial non-recycling PFC, major improvements in plasma performance were obtained. Impurities were reduced, and a dramatic improvement (x6) in energy confinement was observed in 2005.\n\nTo achieve more a complete non-recycling PFC, the CDX-U vacuum vessel was disassembled for the installation of a heated inner shell inside it. This was a major step for the conversion of CDX-U to LTX.\n\nLTX had its first plasma in 2008, and first run with lithium wall coatings in 2010.\n\nThe shell was fabricated out of 3/8” copper sheets, which had a stainless steel liner explosively bonded to it. The stainless steel plasma-facing surface of the inner LTX shell will be coated with lithium between shots, using an electron beam evaporator. By keeping the shell temperature above the melting point of lithium, 90% of the LTX PFC area (~5 m2) will consist of non-recycling liquid lithium.\n\n"}
{"id": "1925023", "url": "https://en.wikipedia.org/wiki?curid=1925023", "title": "Marking out", "text": "Marking out\n\nMarking out or layout means the process of transferring a design or pattern to a workpiece, as the first step in the manufacturing process. It is performed in many industries or hobbies although in the repetition industries the machine's initial setup is designed to remove the need to mark out every individual piece.\n\nMarking out consists of transferring the dimensions from the plan to the workpiece in preparation for the next step, machining or manufacture.\n\nTypical tools include:\n\nAs welding does not always require the use of fine tolerances, marking out is usually performed by using centre punches, hammers, tape measures and chalk.\n\nThe \"chalk\" is actually a small pre-cut block of talc (soapstone). These talc blocks can be sharpened to a stronger point than the softer blackboard chalk. The color of the chalk provides good contrast against the dark color of the hot rolled steel that is generally used.\n\nIn carpentry and joinery practice a pencil is used for marking while in cabinetmaking a marking knife provides for greater accuracy. A storey pole is used to lay out repeated measurements such as the location of joints in timber framing, courses of siding such as wood shingles and clapboards, the heights of doorjambs and the courses of bricks in masonry. Carpenters typically mark out framing members \"on-center\", the measurements are to the centers of each member.\n"}
{"id": "12630251", "url": "https://en.wikipedia.org/wiki?curid=12630251", "title": "Mercury vapour turbine", "text": "Mercury vapour turbine\n\nA mercury vapour turbine is a form of heat engine that uses mercury to drive the thermal cycle. A mercury vapour turbine has been used in conjunction with a steam turbine for generating electricity. This example of combined cycle generation does not seem to have been widely adopted, probably because of high capital cost and the obvious toxic hazard if the mercury leaked into the environment.\n\nThe mercury cycle offers an efficiency increase compared to a steam-only cycle because energy can be injected into the Rankine cycle at higher temperature. Metallurgical developments have allowed steam-only plants to increase in efficiency over time, making the mercury vapour turbine obsolete. Modern Combined Cycle Combustion Turbine Generating Stations operate at 61% efficiency, and with none of the safety issues indigenous to a binary mercury Rankine cycle steam power plant\n\nThe Electrical Year Book, 1937, contained the following description of a mercury vapour turbine operating in commercial use:\n\nThe advantage of operating a mercury-vapour turbine in conjunction with a steam power plant lies in the fact that the complete cycle can be worked over a very wide range of temperature without employing any abnormal pressure. The exhaust from the mercury turbine is used to raise steam for the steam turbine. The Hartford Electric Light Co. (U.S.A.) has a 10,000kW turbo-generator driven by mercury vapour, which reaches the turbine at 70 lb. per sq. in. (gauge), 880°F. The mercury vapour is condensed at 445°F and raises 129,000 lb. steam per hr. at 280 lb. per sq. in. pressure. The latter is superheated to 735°F and passed to the steam turbines. During 4 months continuous operation, this plant averaged about 0.715 lb. of coal per kWh of net output, about 43% of the output being from the mercury turbine generator and 57% from the steam plant. On maintained full-load the heat output averages 9800 BTU per net kWh [34.8% efficiency]. It is believed that maintenance costs will be lower than in ordinary steam plant. The back-pressure on the mercury turbine is fixed by the steam boiler pressure; only a small vacuum pump is needed, as there is no air or other gas in the mercury system.\nPower plants designed by William Le Roy Emmet were constructed by General Electric and operated between 1923 and 1950. Large plants included:\n\n"}
{"id": "24452145", "url": "https://en.wikipedia.org/wiki?curid=24452145", "title": "Motorized wheelchair", "text": "Motorized wheelchair\n\nA motorized wheelchair, powerchair, electric wheelchair or electric-powered wheelchair (EPW) is a wheelchair that is propelled by means of an electric motor rather than manual power. Motorized wheelchairs are useful for those unable to propel a manual wheelchair or who may need to use a wheelchair for distances or over terrain which would be fatiguing in a manual wheelchair. They may also be used not just by people with 'traditional' mobility impairments, but also by people with cardiovascular and fatigue-based conditions.\n\nAn electrically propelled tricycle was developed by the R.A. Harding company in England in the 1930s. The electric-powered wheelchair was invented by George Klein who worked for the National Research Council of Canada, to assist injured veterans after World War II.\n\nPowerchair design may be categorized by drive system/chassis, battery, controller, seat, and use.\n\nPowerchairs are generally four-wheeled or six-wheeled and non-folding, however some folding designs exist and other designs may have some ability to partially dismantle for transit.\n\nFour general styles of powerchair drive systems exist: front, centre or rear wheel drive and all-wheel drive. Powered wheels are typically somewhat larger than the trailing/castoring wheels, while castoring wheels are typically larger than the castors on a manual chair. Centre wheel drive powerchairs have castors at both front and rear for a six-wheel layout.\nPowerchair chassis may also mount a kerb-climber, a powered device to lift the front wheels over a kerb of 10 cm or less.\n\nSome manual wheelchairs may also be fitted with an auxiliary electric power system. This can take one of three forms: integrated with the hub of hand-propelled wheels, so that any force on the pushrims is magnified by the drive system, or mounted under the wheelchair and controlled as for a powerchair, but with the motive force either transmitted to the main wheels via a friction drive system, or delivered directly through an auxiliary drive wheel.\n\nSome experimental all-terrain powerchair designs have been produced with tracks rather than wheels, but these are not in common use.\n\nOther experimental designs have incorporated stair-climbing abilities and Dean Kamen's iBOT design featured both stair climbing and the ability to 'stand' on its upended chassis via the use of advanced gyroscopic sensors. The iBOT was at one time a production model, but is no longer marketed.\n\nThe electric motors of powerchairs are usually powered by 12 to 80 ampere-hour rechargeable deep-cycle batteries, the smaller batteries are used in pairs to give the chair enough power to last at least one day between charges. These are available in wet or dry options. As wet-cell batteries may not legally be carried on an aircraft without removing them from the wheelchair and securing them in a shipping container, dry-cell batteries are preferred for powerchair use. Many powerchairs carry an on-board charger which can be plugged into a standard wall outlet; older or more portable models may have a separate charger unit.\n\nControllers are most commonly an arm-rest mounted joystick which may have additional controls to allow the user to tailor sensitivity or access multiple control modes. The controller may be swing-away to aid in side-transfers. For users who are unable to use a hand controller various alternatives are available such as sip-and-puff controllers, worked by blowing into a sensor. In some cases the controller may be mounted for use by an aide walking behind the chair rather than by the user. Capabilities include turning one drivewheel forward while the other goes backward, thus turning the wheelchair within its own length.\n\n'Thought-control' of powerchairs, actually working by the detection of brainwaves or nerve signals via sensors on the scalp or elsewhere, has been demonstrated in the laboratory environment.\n\nThe seating on a powerchair can vary in design. Starting with a basic sling seat and backrest made of vinyl or nylon, some chairs have an optional padding, some have more comfortable cushion and backrest options which may include a head rest. There are companies which can fit their own backrests and seat cushions for people with increased need for stability in the trunk, or at increased risk of pressure sores from sitting out. Finally, specialist seating solutions are available for users who need individually tailored support. Leg rests may be integrated into the seating design and may have powered adjustment for those users who need to vary their leg position. Powerchairs may also have a tilt-in-space, or reclining facility for users who are unable to maintain an upright seating position indefinitely. This function can also help with comfort by shifting pressure to different areas for a while, or with positioning in a wheelchair when a user needs to be hoisted in.\n\nCertain high-end powerchairs feature a 'standing' capability in which either the entire seat elevates to bring the user to standing height or the seat-base, seat-back and leg rests move in conjunction to bring the user into an upright position. The powerchair may or may not be able to move while in the elevated position.\n\nPowerchairs may be designed for indoor, outdoor or indoor/outdoor use. A typical indoor powerchair will be narrow and short, to enable better manoeuvring around tight environments. Controls are usually simple, and due to the smaller design, the chair would be less stable outdoors. Tyres are often smoother (once called carpet tyres) to look after the flooring in a home. Indoor/outdoor powerchairs again will be as small in design as possible, but with a reasonable range in the batteries, some grippy tyres (but not big knobbly off-road tyres), these often include a kerb-climber to assist with manoeuvres where there are no drop kerbs. Intended for pavement use only. Outdoor powerchairs have a considerable range, a large wheelbase to help with stability, and large tyres which improve the comfort and handling of the chair. These can sometimes be driven indoors in adapted environments, but not around a typical home.\n\nSome very large outdoor powerchairs have been designed with cross-country mobility in mind and show design convergence with other types of cross-country vehicle.\n\nMost wheelchairs are crash tested to standards 7176, and ISO 10542. These standards mean that a wheelchair can be used facing forward in a vehicle if the vehicle has been fitted with an approved tiedown or docking system for securing the wheelchair, and a method of securing the occupant of the wheelchair.\n\nPowerchairs are generally prescribed for use by users who are unable to use a manual wheelchair. However, in both the US (Medicare and some private insurers) and the UK (National Health Service) powerchairs are generally not prescribed to users who have any ability to walk within the home, even if that ability is so functionally constrained as to be practically useless and where the user is unable to use a manual wheelchair independently. Disability rights activists are campaigning for prescription procedures to focus on an individual needs based assessment rather than on inflexible application of prescription rules. The restricted prescribing leads to many users being forced to procure a solution privately, in some cases settling for a powerchair or a mobility scooter that is less than ideal to their needs but which falls within their budget.\n\nThe use of powerchairs is not restricted solely to users unable to use manual chairs. Any disabled person with a mobility, fatigue or pain-based impairment or cardio-vascular issues may find a powerchair advantageous in some circumstances; however, existing prescription practices generally mean that powerchairs for such use must be privately procured or hired for the occasion.\n\nAccess adaptations such as wheelchair spaces on public transport and wheelchair lifts are frequently designed around a typical manual wheelchair (in the UK referred to as a 'reference wheelchair'). Powerchairs, however, frequently exceed the size and weight limits of manual wheelchairs as they are not constrained by the ability of the user to self-propel. Some designs are too large or heavy for certain wheelchair spaces and lifts. However, there are new designs and innovations seeking to overcome these issues.\n\n\n"}
{"id": "4600283", "url": "https://en.wikipedia.org/wiki?curid=4600283", "title": "National Wind Institute", "text": "National Wind Institute\n\nThe National Wind Institute (NWI) at Texas Tech University (TTU) was established in December 2012, and is intended to serve as Texas Tech University’s intellectual hub for interdisciplinary and transdisciplinary research, commercialization and education related to wind science, wind energy, wind engineering and wind hazard mitigation and serves faculty affiliates, students, and external partners.\n\nIn 2003, with support from the National Science Foundation, the first interdisciplinary Ph.D. program dedicated to wind science and engineering was developed. Later, the Texas Wind Energy Institute (TWEI) was established, and is a partnership between TTU and Texas State Technical College designed to develop education and career pathways to meet workforce and educational needs of the expanding wind energy industry. Partly funded by the Texas Workforce Commission. \nIn an effort to streamline and to promote synergy, both WiSE and TWEI have now integrated to form the National Wind Institute.\n\nNWI organizes and administers large multi-dimensional TTU wind-related research projects and serves as the contact point for major project sponsors and other external partners.\n\nThe Wind Science and Engineering (WiSE) Research Center was established in 1970 as the Institute for Disaster Research, following the F5 Lubbock tornado that caused 26 fatalities and over $100 million in damage. Following the aftermath of the tornado, the WISE center developed the first comprehensive wind engineering report of its kind. In 2006, the Enhanced Fujita scale was developed at TTU to update the original Fujita scale that was first introduced in 1971. In 2003, with support from the National Science Foundation, the first interdisciplinary Ph.D. program dedicated to wind science and engineering was developed. Later, the Texas Wind Energy Institute (TWEI) was established as a partnership between TTU and Texas State Technical College designed to develop education and career pathways to meet workforce and educational needs of the expanding wind energy industry. A Bachelor's degree program in Wind Energy was opened in Spring 2012 and now has more than 100 students in the process of completing the degree requirements.\n\nBoth WiSE and TWEI have now integrated to form the National Wind Institute (NWI).\n\nThe Texas Tech campus hosts the NWI's administrative offices and the Boundary Layer Wind Tunnel and Wind Library. The Boundary Layer Wind Tunnel is a closed wind tunnel offering a by test section that is capable of producing up winds speeds of up to . The facility also hosts VorTECH, a tornado vortex simulator and a Pulsed Jet Wind tunnel, which is used to simulate thunderstorm downbursts. The NWI's Wind Library hosts one of the largest collections of wind related material in the world. The collection includes Ted Fujita's papers, reports and photographs, which were donated by the Fujita family and the University of Chicago. The library also includes documentation of more than 100 wind storms.\n\nThe National Wind Institute occupies of indoor laboratory space and has a large 67-acre field test site at the Reese Technology Center. Some of the facilities housed at the Reese Center include a 200-meter data acquisition tower, used to measure and record atmospheric conditions at ten levels, the Debris Impact Facility, the Scaled Wind Farm Technology SWiFT facility and a weather balloon facility. The Reese Center is also home to several radar systems including a SODAR, Low Level Profiler, and the SMART-R Mobile Radar.\n\nSome of the National Wind Institute's wind energy research goals are the assessment of the risk and effects on wind turbine exposure to extreme wind events, the improvement of wind turbine design codes with emphasis on extreme wind events, and the analysis and testing of utility-scale wind turbines for use in less-energetic wind conditions. The NWI is also focused on the identification of advanced wind-driven water treatment and desalination systems for municipal and other applications, as well as the full-scale testing of wind-driven water desalination systems and the development of modeling codes for integrated wind-water desalination systems.\n\nThe NWI Debris Impact Facility performs tests on storm shelters and their various components to see if they meet established Federal Emergency Management Agency (FEMA)l and International Code Council guidelines. The Debris Impact Facility houses a high-powered air cannon that shoots wooden two-by-fours at shelter walls and doors to simulate flying debris. The cannon is capable of producing simulated wind speeds of more than 250 mph and provides valuable impact resistance data. Such data is used to develop standards for safe above-ground and below-ground shelters, and continues to be in demand for testing new shelter materials and constructions.\n\n\n"}
{"id": "3157738", "url": "https://en.wikipedia.org/wiki?curid=3157738", "title": "Naval stores", "text": "Naval stores\n\nNaval stores are all products derived from pine sap, which are used to manufacture soap, paint, varnish, shoe polish, lubricants, linoleum, and roofing materials.\n\nThe term \"naval stores\" originally applied to the resin-based components used in building and maintaining wooden sailing ships, a category which includes cordage, mask, turpentine, rosin, pitch and tar.\n\nShips made of wood required a flexible material, insoluble in water, to seal the spaces between planks. Pine pitch was often mixed with fibers like hemp to caulk spaces which might otherwise leak. Crude gum or oleoresin can be collected from the wounds of living pine trees.\n\nThe Royal Navy relied heavily upon naval stores from American colonies, and naval stores were an essential part of the colonial economy. Masts came from the large white pines of New England, while pitch came from the longleaf pine forests of Carolina, which also produced sawn lumber, shake shingles, and staves.\n\nNaval stores played a role during the American Revolutionary War. As Britain attempted to cripple French and Spanish capacities through blockade, they declared naval stores to be contraband. At the time Russia was Europe's chief producer of naval stores, leading to the seizure of 'neutral' Russian vessels. In 1780 Catherine the Great announced that her navy would be used against anyone interfering with neutral trade, and she gathered together European neutrals in the League of Armed Neutrality. These actions were beneficial for the struggling colonists as the British were forced to act with greater caution.\n\nThe major producers of naval stores in the 19th and 20th century were the United States of America, and France, where Napoleon encouraged planting of pines in areas of sand dunes. In the 1920s the United States exported eleven million gallons of spirits. By 1927, France exported about 20 percent of the world's resin.\n\nNaval stores are recovered from the tall oil byproduct stream of Kraft process pulping of pines in the United States. Tapping of living pines remains common in other parts of the world. Turpentine and pine oil may be recovered by steam distillation of oleoresin or by destructive distillation of pine wood; solvent extraction of shredded stumps and roots has become more common with the availability of inexpensive naphtha. Rosin remains in the still after turpentine and water have boiled off.\n\n\n"}
{"id": "21679808", "url": "https://en.wikipedia.org/wiki?curid=21679808", "title": "Neo-Neon Holdings", "text": "Neo-Neon Holdings\n\nNeo-Neon Holdings Limited () is a decorative lighting company stationed in Hong Kong. It engages in the research, development, manufacturing and distribution of lighting products including incandescent, LED, decorative and entertainment lighting products.\n\n\n"}
{"id": "53948587", "url": "https://en.wikipedia.org/wiki?curid=53948587", "title": "North American Board of Certified Energy Practitioners", "text": "North American Board of Certified Energy Practitioners\n\nThe North American Board of Certified Energy Practitioners (NABCEP) is a nonprofit professional certification and accreditation organization that offers both individual and company accreditation programs for photovoltaic system installers, solar heat installers, technical sales, and other renewable energy professionals throughout North America. NABCEP was officially incorporated in 2002 and its mission is to raise standards while promoting consumer and other stakeholders' confidence within the renewable energy industry.\n\nNABCEP is a nationally recognized credentialing body formed to set competency standards for professional practitioners in the fields of renewable energy and energy efficiency. NABCEP PV Installation Professional Certification (formerly NABCEP Solar PV Installer Certification) has been accredited to the international ANSI/ISO/IEC 17024 standard for personnel certification bodies since 2007, and the NABCEP Solar Heating Installer Certification became accredited in 2013.\n\nNABCEP has become the primary organization for solar energy professional certification in the United States and Canada. NABCEP designed an Associate Program for individuals who are interested in learning about and finding jobs within the solar field; the NABCEP PV Associate Exam allows candidates to demonstrate a basic knowledge of the fundamental principles related to PV systems and designs. NABCEP also offers certifications in: \nAs of February 1, 2018, only ten companies had earned NABCEP Company Accerditation. They include:\nNABCEP Certification is either preferred or mandatory for solar system installations to be eligible for incentive programs in several states. In order to be eligible for state rebate funds in Minnesota, Maine, and Wisconsin, PV solar systems must be installed by a NABCEP-certified professional. California, Massachusetts, and Delaware's solar rebate programs prefer or recommend NABCEP-certified professionals. NABCEP-certification is a prerequisite for qualifying for a state solar contractor license in Utah. Specific policies and incentives can be found on DSIRE's website.\n\nNABCEP partners with Castle Worldwide to offer its associate and certification exams at multiple sites in the United States and Canada.\n"}
{"id": "1340719", "url": "https://en.wikipedia.org/wiki?curid=1340719", "title": "Particle board", "text": "Particle board\n\nParticle board – also known as particleboard, low-density fibreboard (LDF), and chipboard – is an engineered wood product manufactured from wood chips, sawmill shavings, or even sawdust, and a synthetic resin or other suitable binder, which is pressed and extruded. Oriented strand board, also known as flakeboard, waferboard, or chipboard, is similar but uses machined wood flakes offering more strength. All of these are composite materials that belong to the spectrum of fiberboard products.\n\nParticle board is cheaper, denser and more uniform than conventional wood and plywood and is substituted for them when cost is more important than strength and appearance. Particleboard can be made more appealing by painting or the use of wood veneers on visible surfaces. Though it is more dense than conventional wood, it is the lightest and weakest type of fiberboard, except for insulation board. Medium-density fibreboard and hardboard, also called high-density fiberboard, are stronger and denser than particleboard. Different grades of particleboard have different densities, with higher density connoting greater strength and greater resistance to failure of screw fasteners.\n\nA significant disadvantage of particleboard is its susceptibility to expansion and discoloration from moisture absorption, particularly when it is not covered with paint or another sealer. Therefore, it is rarely used outdoors or in places where there are high levels of moisture, except in bathrooms, kitchens and laundries, where it is commonly used as an underlayment shielded beneath a moisture resistant continuous sheet of vinyl flooring.\n\nIn dry environments, veneered particleboard is preferred over veneered plywood because of its stability, lower cost, and convenience.\n\nParticleboard originated in Germany. It was first produced in 1887, when Hubbard made so-called \"artificial wood\" from wood flour and an adhesive based on albumin, which was consolidated under high temperature and pressure. \n\nAlthough the use of two or three layers of wood veneer is ancient, modern 4' x 8' sheets of plywood with 5-11 core layers of veneer were invented in the early 20th century, and began to become common by the Second World War. During the war, phenolic resin was more readily accessible than top grade wood veneer in Germany, and Luftwaffe pilot and inventor Max Himmelheber played a role in making the first sheets of particleboard, which were little more than pourings of floor sweepings, wood chips, and ground up off-cuts and glue. The first commercial piece was produced during World War II at a factory in Bremen, Germany. For its production, waste material was used - such as planer shavings, offcuts or sawdust - hammer-milled into chips and bound together with a phenolic resin. Hammer-milling involves smashing material into smaller and smaller pieces until they can pass through a screen. Most other early particleboard manufacturers used similar processes, though often with slightly different resins.\n\nIt was found that better strength, appearance and resin economy could be achieved by using more uniform, manufactured chips. Producers began processing solid birch, beech, alder, pine and spruce into consistent chips and flakes; these finer layers were then placed on the outside of the board, with its core composed of coarser, cheaper chips. This type of board is known as three-layer particleboard.\n\nMore recently, graded-density particleboard has also evolved. It contains particles that gradually become smaller as they get closer to the surface\n\nParticleboard or chipboard is manufactured by mixing wood particles or flakes together with a resin and forming the mixture into a sheet. The raw material is fed into a disc chipper with between four and sixteen radially arranged blades. The chips from disk chippers are more uniform in shape and size than from other types of wood chippers. The particles are then dried, and any oversized or undersized particles are screened out.\n\nResin is then sprayed as a fine mist onto the particles. Several types of resins are used. Amino-formaldehyde based resins are the best performing based on cost and ease of use. Urea Melamine resins offer water resistance with more Melamine offering higher resistance. It is typically used in external applications, with the coloured resin darkening the panel. To further enhance the panel properties, resorcinol resins can be mixed with phenolic resins, but that is more often used with marine plywood applications.\n\nPanel production involves other chemicals including wax, dyes, wetting agents and release agents, to aid processing or make the final product water resistant, fireproof, or insect proof.\n\nAfter the particles pass through a mist of resin sufficient to coat all surfaces, they are layered into a continuous carpet. This 'carpet' is then separated into discrete, rectangular 'blankets' which will be compacted in a cold press. A scale weighs the flakes, and they are distributed by rotating rakes. In graded-density particleboard, the flakes are spread by an air jet that throws finer particles further than coarse ones. Two such jets, reversed, allow the particles to build up from fine to coarse and back to fine.\n\nThe formed sheets are cold-compressed to reduce thickness and make them easier to transport. Later, they are compressed again, under pressures between and temperatures between to set and harden the glue. The entire process is controlled to ensure the correct size, density and consistency of the board.\n\nThe boards are then cooled, trimmed and sanded. They can then be sold as raw board or surface improved through the addition of a wood veneer or laminate surface.\n\nParticle board has had an enormous influence on furniture design. In the early 1950s, particle board kitchens started to come into use in furniture construction but, in many cases, it remained more expensive than solid wood. A particle board kitchen was only available to the very wealthy. Once the technology was more developed, particle board became cheaper.\n\nSome large companies base their strategies around providing furniture at a low price. To do this, they use the least expensive materials possible. In almost all cases, this means particle board or MDF or similar. However, manufacturers, in order to maintain a reputation for quality at low cost, may use higher grades of particle board, e.g., higher density particle board, thicker particle board, or particle board using higher-quality resins. One may note the amount of sag in a shelf of a given width in order to draw the distinction. \n\nIn general the much lower cost of sheet goods (particle board, medium density fiberboard, and other engineered wood products) has helped to displace solid wood from many cabinetry applications.\n\nSafety concerns exist for both manufacturing and use. Fine dust and chemicals are released when particleboard is machined (e.g., sawing or routing). Occupational exposure limits exist in many countries recognizing the hazard of wood dusts. Cutting particle board can release formaldehyde, carbon monoxide, hydrogen cyanide in the case of amino resins, and phenol in the case of phenol-formaldehyde resins. \n\nThe other safety concern is the slow release of formaldehyde over time. In 1984 concerns about the high indoor levels of formaldehyde in new manufactured homes led the United States Department of Housing and Urban Development to set construction standards. Particleboard (PB), medium density fibreboard (MDF), oriented strand board (OSB), and laminated flooring have been major sources of formaldehyde emissions. In response to consumer and woodworker pressure on the industry, PB and MDF became available in \"no added formaldehyde\" (NAF) versions, but were not common use . Many other building materials such as furniture finish, carpeting and caulking give off formaldehyde, as well as urea-formaldehyde foam insulation, which is banned in Canada for installation in a residential closed cavity wall. Formaldehyde is classified by the WHO as a known human carcinogen.\n\n\n"}
{"id": "16228719", "url": "https://en.wikipedia.org/wiki?curid=16228719", "title": "Saint John Energy", "text": "Saint John Energy\n\nSaint John Energy, formerly known as Power Commission of the City of Saint John and Civic Hydro, is the electrical utility reseller of power purchased from NB Power in Saint John, New Brunswick. It was founded in 1922 and now serves over 36,000 customers.\n\nThe utility sells 950GWh of electricity annually, however the utility has no electrical generating capacity of its own.\n\n"}
{"id": "36376979", "url": "https://en.wikipedia.org/wiki?curid=36376979", "title": "Solar power in Brazil", "text": "Solar power in Brazil\n\nThe total installed solar power in Brazil was estimated to be about 69 MWp at the end of 2015, generating less than 0.01% of the country's electricity demand. \nChanges to net metering rules for small-scale solar were announced in November 2015 although there were only 1,300 grid-connected systems at that time. \nBrazil expects to have 1.2 million systems in the year 2024. \n\nSolar energy has great potential in Brazil, with the country having one of the highest levels of insolation in the world at 4.25 to 6.5 sun hours/day. \n\nSource:\n\nIn 2016, a factory capable of producing 400 MW of solar panels a year opened in Sorocaba in Sao Paulo, owned by Canadian Solar. A plan to build a solar panel factory in Rio Grande do Norte was announced by the Chinese manufacturer Chint in 2017.\n\nThe opening of three major solar farms in Brazil in 2017 altered the solar situation: the 292 MW Nova Olinda Solar Farm in Ribeira do Piauí, Piauí, the 254 MW Ituverava Solar Farm in Tabocas do Brejo Velho, Bahia and the 158 MW Parque Solar Lapa in Bom Jesus da Lapa, Bahia ranked among the largest installations in the world. The total capacity of these three plants was more than ten times the installed total in the entire country in 2015. A total of 1000 MW is expected to be installed in 2017 with an additional 2000 MW assigned by auctions to be completed in future years.\n\n\n4. http://www.seia.org/research-resources/solar-energy-support-germany-closer-look\n\n5. http://repository.unm.edu/bitstream/handle/1928/15053/Brazil.Solar.Power10.11.pdf?sequence=1&isAllowed=y\n"}
{"id": "29560064", "url": "https://en.wikipedia.org/wiki?curid=29560064", "title": "Solarlite", "text": "Solarlite\n\nSolarlite CSP Technology GmbH, located in Mecklenburg-Pomerania, Germany, develops and builds decentralized solar-thermal parabolic trough power plants (CSP – Concentrated Solar Power) and process heat plants. For the first time worldwide, Solarlite is using direct steam generation commercially in a power plant. In 2012, Solarlite declared insolvency. The company was reincorporated the next year by Joachim Krüger (CEO).\n\nSolar thermal facilities offer one of the most sustainable forms of energy recovery in terms of the environment, resources and availability. The technology has the advantage that as direct solar radiation increases, so does the efficiency of the facilities. The facilities are also characterized by a high degree of flexibility. They can be combined with all other fossil and renewable energy sources and are thus base-load capable. Another plus is the option of producing electricity and process heat at the same time or independently of one another. In addition, the residual heat can be used for further industrial applications for example desalination and cooling.\n\nSolarlite has tested the DSG concept successfully in three pilot projects in Thailand and Germany. The DSG concept is environment friendly and allows significant reductions of the total investment costs and levelized electricity cost. With DSG a higher operating temperature will be achievable.\n\nThe Solarlite SL 4600 parabolic trough is a highly efficient product that can generate temperatures of up to 400 °C. Each panel has an aperture width of 4,6 m and is made of composite materials combined with an efficient thinglass mirror. This mirror reflects up to 95% of the sun's radiation onto the absorber pipe positioned at the ideal focus of the parabolic mirror.\n\nIn 2012, Solarlite was forced to declare insolvency. According to BonVenture, the reason for this \"...was the unexpected market shift towards photovoltaics because of the Chinese government´s subsidies policy and the non-fulfilment of contractual obligations and payments by two customers\". The company then reincorporated on January 1, 2013 as Solarlite CSP Technology GmbH with the same CEO, Joachim Krüger.\n\n\nSolarlite has tested the DSG concept successfully in three pilot projects in Thailand and Germany. Solarlite’s adaptation of DSG concept is based on the following advantages: \n\nFlow path concept, development and testing - In a joint research project known as “Duke” that was supported by the German Federal Ministry for the Environment, Nature Conservation and Nuclear Safety, Solarlite CSP Technology GmbH and the German Aerospace Centre tested a new version of direct steam generation.\nProject aim\n\nThe Solarlite 4600 parabolic trough is a newly developed highly efficient product that can generate temperatures of up to 400 °C. Each panel has an aperture width of 4.6 m and is made of composite materials combined with an efficient thinglass mirror. This mirror reflects up to 95% of the sun’s radiation onto the receiver pipe positioned at the ideal focus of the parabolic mirror. Water passing through the receiver pipe is heated up by the concentrated reflected sun radiation and is converted into steam within a controlled process. A turbine generator produces electricity. Residual heat can be used for other applications such as seawater desalination or absorption cooling.\nThe basic element of the Solarlite 4600 collector is the Solarlite composite panel which has a dimension of 2.3 m width and 1 m length. These panels are combined together to form 1 segment with a dimension 4.6 m aperture width and 12 m length. It is possible to connect 10 of these segments to form one collector thus reaching up to 120 m. These collectors are combined together to form rows (collectors aligned in 1 axis in the North-South direction) and loops (collectors connected in series where the cold fluid enters in one end and the hot fluids leaves in the other end). The collector is moved from east to west to track the sun by means of a hydraulic drive system. The modular concept of Solarlite allows choosing the optimal length of the collector based on the specific locations’ wind data. The combination of the light weight composite and slender steel structure allows having a reduced specific weight compared to the competitors.\n\n\n\n"}
{"id": "196788", "url": "https://en.wikipedia.org/wiki?curid=196788", "title": "Steam locomotive", "text": "Steam locomotive\n\nA steam locomotive is a type of railway locomotive that produces its pulling power through a steam engine. These locomotives are fueled by burning combustible material – usually coal, wood, or oil – to produce steam in a boiler. The steam moves reciprocating pistons which are mechanically connected to the locomotive's main wheels (drivers). Both fuel and water supplies are carried with the locomotive, either on the locomotive itself or in wagons (tenders) pulled behind.\n\nSteam locomotives were first developed in Great Britain during the early 19th century and used for railway transport until the middle of the 20th century. The first steam locomotive, made by Richard Trevithick, first operated on 21 February 1804, three years after the road locomotive he made in 1801. The first commercially successful steam locomotive was created in 1812–13 by John Blenkinsop. Built by George Stephenson and his son Robert's company Robert Stephenson and Company, the \"Locomotion\" No. 1 is the first steam locomotive to carry passengers on a public rail line, the Stockton and Darlington Railway in 1825. George also built the first public inter-city railway line in the world to use locomotives, the Liverpool and Manchester Railway, which opened in 1830. Stephenson established his company as the pre-eminent builder of steam locomotives for railways in the United Kingdom, the United States, and much of Europe.\n\nIn the 20th century, Chief Mechanical Engineer of the London and North Eastern Railway (LNER) Nigel Gresley designed some of the most famous locomotives, including the \"Flying Scotsman\", the first steam locomotive officially recorded over 100 mph in passenger service, and a LNER Class A4, 4468 \"Mallard\", which still holds the record for being the fastest steam locomotive in the world (126 mph).\n\nFrom the early 1900s steam locomotives were gradually superseded by electric and diesel locomotives, with railways fully converting to electric and diesel power beginning in the late 1930s. The majority of steam locomotives were retired from regular service by the 1980s, though several continue to run on tourist and heritage lines.\n\nThe earliest railways employed horses to draw carts along rail tracks. In 1784, William Murdoch, a Scottish inventor, built a small-scale prototype of a steam road locomotive in Birmingham. A full-scale rail steam locomotive was proposed by William Reynolds around 1787. An early working model of a steam rail locomotive was designed and constructed by steamboat pioneer John Fitch in the US during 1794. His steam locomotive used interior bladed wheels guided by rails or tracks. The model still exists at the Ohio Historical Society Museum in Columbus. The authenticity and date of this locomotive is disputed by some experts and a workable steam train would have to await the invention of the high-pressure steam engine by Richard Trevithick, who pioneered the use of steam locomotives.\nThe first full-scale working railway steam locomotive, was the gauge \"Coalbrookdale Locomotive\", built by Trevithick in 1802. It was constructed for the Coalbrookdale ironworks in Shropshire in the United Kingdom though no record of it working there has survived. On 21 February 1804, the first recorded steam-hauled railway journey took place as another of Trevithick's locomotives hauled a train along the tramway from the Pen-y-darren ironworks, near Merthyr Tydfil, to Abercynon in South Wales. Accompanied by Andrew Vivian, it ran with mixed success. The design incorporated a number of important innovations that included using high-pressure steam which reduced the weight of the engine and increased its efficiency.\n\nTrevithick visited the Newcastle area in 1804 and had a ready audience of colliery (coal mine) owners and engineers. The visit was so successful that the colliery railways in north-east England became the leading centre for experimentation and development of the steam locomotive. Trevithick continued his own steam propulsion experiments through another trio of locomotives, concluding with the \"Catch Me Who Can\" in 1808.\n\nIn 1812, Matthew Murray's successful twin-cylinder rack locomotive \"Salamanca\" first ran on the edge-railed rack-and-pinion Middleton Railway. Another well-known early locomotive was \"Puffing Billy\", built 1813–14 by engineer William Hedley. It was intended to work on the Wylam Colliery near Newcastle upon Tyne. This locomotive is the oldest preserved, and is on static display in the Science Museum, London. In 1825 George Stephenson built \"Locomotion No. 1\" for the Stockton and Darlington Railway, north-east England, which was the first public steam railway in the world. In 1829, his son Robert built in Newcastle \"The Rocket\" which was entered in and won the Rainhill Trials. This success led to the company emerging as the pre-eminent builder of steam locomotives used on railways in the UK, US and much of Europe. The Liverpool and Manchester Railway opened a year later making exclusive use of steam power for passenger and goods trains.\n\nMany of the earliest locomotives for American railroads were imported from Great Britain, including first the \"Stourbridge Lion\" and later the \"John Bull\" (still the oldest operable engine-powered vehicle in the United States of any kind, as of 1981) however a domestic locomotive-manufacturing industry was quickly established. The Baltimore and Ohio Railroad's \"Tom Thumb\" in 1830, designed and built by Peter Cooper, was the first US-built locomotive to run in America, although it was intended as a demonstration of the potential of steam traction, rather than as a revenue-earning locomotive. The \"DeWitt Clinton\" was also built in the 1830s.\n\nThe first railway service outside the United Kingdom and North America was opened in 1829 in France between Saint-Etienne and Lyon. Then on 5 May 1835 the first line in Belgium linked Mechelen and Brussels. The locomotive was named \"The Elephant\".\nIn Germany, the first working steam locomotive was a rack-and-pinion engine, similar to the \"Salamanca\", designed by the British locomotive pioneer John Blenkinsop. Built in June 1816 by Johann Friedrich Krigar in the Royal Berlin Iron Foundry (\"Königliche Eisengießerei\" zu Berlin), the locomotive ran on a circular track in the factory yard. It was the first locomotive to be built on the European mainland and the first steam-powered passenger service; curious onlookers could ride in the attached coaches for a fee. It is portrayed on a New Year's badge for the Royal Foundry dated 1816. Another locomotive was built using the same system in 1817. They were to be used on pit railways in Königshütte and in Luisenthal on the Saar (today part of Völklingen), but neither could be returned to working order after being dismantled, moved and reassembled. On 7 December 1835 the \"Adler\" ran for the first time between Nuremberg and Fürth on the Bavarian Ludwig Railway. It was the 118th engine from the locomotive works of Robert Stephenson and stood under patent protection.\n\nIn 1837, the first steam railway started in Austria on the Emperor Ferdinand Northern Railway between Vienna-Floridsdorf and Deutsch-Wagram. The oldest continually working steam engine in the world also runs in Austria: the GKB 671 built in 1860, has never been taken out of service, and is still used for special excursions.\n\nIn 1838, the third steam locomotive to be built in Germany, the \"Saxonia\", was manufactured by the \"Maschinenbaufirma Übigau\" near Dresden, built by Prof. Johann Andreas Schubert. The first independently designed locomotive in Germany was the \"Beuth\", built by August Borsig in 1841. The first locomotive produced by Henschel-Werke in Kassel, the \"Drache\", was delivered in 1848.\n\nThe first steam locomotives operating in Italy were the \"Bayard\" and the \"Vesuvio\", running on the Napoli-Portici line, in the Kingdom of the Two Sicilies.\n\nThe first railway line over Swiss territory was the Strasbourg–Basle line opened in 1844. Three years later, in 1847, the first fully Swiss railway line, the \"Spanisch Brötli Bahn\", from Zürich to Baden was opened.\n\nThe fire-tube boiler was standard practice for steam locomotives and although other types of boiler were evaluated they were not widely used except for 1000 locomotives in Hungary which used the water-tube Brotan boiler.\nA boiler consists of a firebox where the fuel is burned, a barrel where water is turned into steam and a smokebox which is kept at a slightly lower pressure than outside the firebox.\n\nSolid fuel, such as wood, coal or coke, is thrown into the firebox through a door by a fireman, onto a set of grates which hold the fuel in a bed as it burns. Ash falls through the grate into an ashpan. If oil is used as the fuel, a door is needed for adjusting the air flow, maintaining the firebox, and cleaning the oil jets.\n\nThe fire-tube boiler has internal tubes connecting the firebox to the smokebox through which the combustion gases flow transferring heat to the water. All the tubes together provide a large contact area, called the tube heating surface, between the gas and water in the boiler. Boiler water surrounds the firebox to stop the metal from becoming too hot. This is another area where the gas transfers heat to the water and is called the firebox heating surface. Ash and char collect in the smokebox as the gas gets drawn up the chimney (\"stack\" or \"smokestack\" in the US) by the exhaust steam from the cylinders.\nSurrounding the boiler are layers of insulation or lagging to reduce heat loss.\n\nThe pressure in the boiler has to be monitored using a gauge mounted in the cab. Steam pressure can be released manually by the driver or fireman. If the pressure reaches the boiler's design working limit, a safety valve opens automatically to reduce the pressure and avoid a catastrophic accident.\nThe exhaust steam from the engine cylinders shoots out of a nozzle pointing up the chimney in the smokebox. The steam entrains or drags the smokebox gases with it which maintains a lower pressure in the smokebox than that under the firebox grate. This pressure difference causes air to flow up through the coal bed and keeps the fire burning.\n\nThe search for thermal efficiency greater than that of a typical fire-tube boiler led engineers, such as Nigel Gresley, to consider the water-tube boiler. Although he tested the concept on the LNER Class W1, the difficulties during development exceeded the will to increase efficiency by that route.\n\nThe steam generated in the boiler not only moves the locomotive, but is also used to operate other devices such as the whistle, the air compressor for the brakes, the pump for replenishing the water in the boiler and the passenger car heating system. The constant demand for steam requires a periodic replacement of water in the boiler. The water is kept in a tank in the locomotive tender or wrapped around the boiler in the case of a tank locomotive. Periodic stops are required to refill the tanks; an alternative was a scoop installed under the tender that collected water as the train passed over a track pan located between the rails.\n\nWhile the locomotive is producing steam, the amount of water in the boiler is constantly monitored by looking at the water level in a transparent tube, or sight glass. Efficient and safe operation of the boiler requires keeping the level in between lines marked on the sight glass. If the water level is too high, steam production falls, efficiency is lost and water is carried out with the steam into the cylinders, possibly causing mechanical damage. More seriously, if the water level gets too low, the crown(top)sheet of the firebox becomes exposed. Without water on top of the sheet to transfer away the heat of combustion, it softens and fails, letting high-pressure steam into the firebox and the cab. The development of the fusible plug, a temperature-sensitive device, ensured a controlled venting of steam into the firebox to warn the fireman to add water.\n\nScale builds up in the boiler and prevents adequate heat transfer, and corrosion eventually degrades the boiler materials to the point where it needs to be rebuilt or replaced. Start-up on a large engine may take hours of preliminary heating of the boiler water before sufficient steam is available.\n\nAlthough the boiler is typically placed horizontally, for locomotives designed to work in locations with steep slopes it may be more appropriate to consider a vertical boiler or one mounted such that the boiler remains horizontal but the wheels are inclined to suit the slope of the rails.\n\nThe steam generated in the boiler fills the space above the water in the partially filled boiler. Its maximum working pressure is limited by spring-loaded safety valves. It is then collected either in a perforated tube fitted above the water level or by a dome that often houses the regulator valve, or throttle, the purpose of which is to control the amount of steam leaving the boiler. The steam then either travels directly along and down a steam pipe to the engine unit or may first pass into the wet header of a superheater, the role of the latter being to improve thermal efficiency and eliminate water droplets suspended in the \"saturated steam\", the state in which it leaves the boiler. On leaving the superheater, the steam exits the dry header of the superheater and passes down a steam pipe, entering the steam chests adjacent to the cylinders of a reciprocating engine. Inside each steam chest is a sliding valve that distributes the steam via ports that connect the steam chest to the ends of the cylinder space. The role of the valves is twofold: admission of each fresh dose of steam, and exhaust of the used steam once it has done its work.\n\nThe cylinders are double-acting, with steam admitted to each side of the piston in turn. In a two-cylinder locomotive, one cylinder is located on each side of the vehicle. The cranks are set 90° out of phase. During a full rotation of the driving wheel, steam provides four power strokes; each cylinder receives two injections of steam per revolution. The first stroke is to the front of the piston and the second stroke to the rear of the piston; hence two working strokes. Consequently, two deliveries of steam onto each piston face in the two cylinders generates a full revolution of the driving wheel. Each piston is attached to the driving axle on each side by a connecting rod, and the driving wheels are connected together by coupling rods to transmit power from the main driver to the other wheels. Note that at the two \"dead centres\", when the connecting rod is on the same axis as the crankpin on the driving wheel, the connecting rod applies no torque to the wheel. Therefore, if both cranksets could be at \"dead centre\" at the same time, and the wheels should happen to stop in this position, the locomotive could not start moving. Therefore, the crankpins are attached to the wheels at a 90° angle to each other, so only one side can be at dead centre at a time.\n\nEach piston transmits power through a crosshead, connecting rod (\"Main rod\" in the US) and a crankpin on the driving wheel (\"Main driver\" in the US) or to a crank on a driving axle. The movement of the valves in the steam chest is controlled through a set of rods and linkages called the valve gear, actuated from the driving axle or from the crankpin; the valve gear includes devices that allow reversing the engine, adjusting valve travel and the timing of the admission and exhaust events. The cut-off point determines the moment when the valve blocks a steam port, \"cutting off\" admission steam and thus determining the proportion of the stroke during which steam is admitted into the cylinder; for example a 50% cut-off admits steam for half the stroke of the piston. The remainder of the stroke is driven by the expansive force of the steam. Careful use of cut-off provides economical use of steam and in turn reduces fuel and water consumption. The reversing lever (\"Johnson bar\" in the US), or screw-reverser (if so equipped), that controls the cut-off therefore performs a similar function to a gearshift in an automobile – maximum cut-off, providing maximum tractive effort at the expense of efficiency, is used to pull away from a standing start, whilst a cut-off as low as 10% is used when cruising, providing reduced tractive effort, and therefore lower fuel/water consumption.\n\nExhaust steam is directed upwards out of the locomotive through the chimney, by way of a nozzle called a blastpipe, creating the familiar \"chuffing\" sound of the steam locomotive. The blastpipe is placed at a strategic point inside the smokebox that is at the same time traversed by the combustion gases drawn through the boiler and grate by the action of the steam blast. The combining of the two streams, steam and exhaust gases, is crucial to the efficiency of any steam locomotive, and the internal profiles of the chimney (or, strictly speaking, the \"ejector\") require careful design and adjustment. This has been the object of intensive studies by a number of engineers (and often ignored by others, sometimes with catastrophic consequences). The fact that the draught depends on the exhaust pressure means that power delivery and power generation are automatically self-adjusting. Among other things, a balance has to be struck between obtaining sufficient draught for combustion whilst giving the exhaust gases and particles sufficient time to be consumed. In the past, a strong draught could lift the fire off the grate, or cause the ejection of unburnt particles of fuel, dirt and pollution for which steam locomotives had an unenviable reputation. Moreover, the pumping action of the exhaust has the counter-effect of exerting \"back pressure\" on the side of the piston receiving steam, thus slightly reducing cylinder power. Designing the exhaust ejector became a specific science, with engineers such as Chapelon, Giesl and Porta making large improvements in thermal efficiency and a significant reduction in maintenance time and pollution. A similar system was used by some early gasoline/kerosene tractor manufacturers (Advance-Rumely/Hart-Parr) – the exhaust gas volume was vented through a cooling tower, allowing the steam exhaust to draw more air past the radiator.\n\nRunning gear includes the brake gear, wheel sets, axleboxes, springing and the motion that includes connecting rods and valve gear. The transmission of the power from the pistons to the rails and the behaviour of the locomotive as a vehicle, being able to negotiate curves, points and irregularities in the track, is of paramount importance. Because reciprocating power has to be directly applied to the rail from 0 rpm upwards, this creates the problem of adhesion of the driving wheels to the smooth rail surface. Adhesive weight is the portion of the locomotive's weight bearing on the driving wheels. This is made more effective if a pair of driving wheels is able to make the most of its axle load, i.e. its individual share of the adhesive weight. Equalising beams connecting the ends of leaf springs have often been deemed a complication in Britain, however locomotives fitted with the beams have usually been less prone to loss of traction due to wheel-slip. Suspension using equalizing levers between driving axles, and between driving axles and trucks, was standard practice on North American locomotives to maintain even wheel loads when operating on uneven track.\n\nLocomotives with total adhesion, where all of the wheels are coupled together, generally lack stability at speed. To counter this, locomotives often fit unpowered carrying wheels mounted on two-wheeled trucks or four-wheeled bogies centred by springs/inverted rockers/geared rollers that help to guide the locomotive through curves. These usually take on weight – of the cylinders at the front or the firebox at the rear — when the width exceeds that of the mainframes. Locomotives with multiple coupled-wheels on a rigid chassis would have unacceptable flange forces on tight curves giving excessive flange and rail wear, track spreading and wheel climb derailments. One solution was to remove or thin the flanges on an axle. More common was to give axles end-play and use lateral motion control with spring or inclined-plane gravity devices.\n\nRailroads generally preferred locomotives with fewer axles, to reduce maintenance costs. The number of axles required was dictated by the maximum axle loading of the railroad in question. A builder would typically add axles until the maximum weight on any one axle was acceptable to the railroad's maximum axle loading. A locomotive with a wheel arrangement of two lead axles, two drive axles, and one trailing axle was a high-speed machine. Two lead axles were necessary to have good tracking at high speeds. Two drive axles had a lower reciprocating mass than three, four, five or six coupled axles. They were thus able to turn at very high speeds due to the lower reciprocating mass. A trailing axle was able to support a huge firebox, hence most locomotives with the wheel arrangement of 4-4-2 (American Type Atlantic) were called free steamers and were able to maintain steam pressure regardless of throttle setting.\n\nThe chassis, or locomotive frame, is the principal structure onto which the boiler is mounted and which incorporates the various elements of the running gear. The boiler is rigidly mounted on a \"saddle\" beneath the smokebox and in front of the boiler barrel, but the firebox at the rear is allowed to slide forward and backwards, to allow for expansion when hot.\n\nEuropean locomotives usually use \"plate frames\", where two vertical flat plates form the main chassis, with a variety of spacers and a buffer beam at each end to keep them apart. When inside cylinders are mounted between the frames, the plate frames are a single large casting that forms a major support. The axleboxes slide up and down to give some sprung suspension, against thickened webs attached to the frame, called \"hornblocks\".\n\nAmerican practice for many years was to use built-up bar frames, with the smokebox saddle/cylinder structure and drag beam integrated therein. In the 1920s, with the introduction of \"superpower\", the cast-steel locomotive bed became the norm, incorporating frames, spring hangers, motion brackets, smokebox saddle and cylinder blocks into a single complex, sturdy but heavy casting. An S.N.C.F design study using welded tubular frames\ngave a rigid frame with a 30% weight reduction.\n\nGenerally, the largest locomotives are permanently coupled to a tender that carries the water and fuel. Often, locomotives working shorter distances do not have a tender and carry the fuel in a bunker, with the water carried in tanks placed next to the boiler either in two tanks alongside (side tank and pannier tank), one on top (saddle tank) or one underneath (well tank); these are called tank engines and usually have a 'T' suffix added to the Whyte notation, e.g. 0-6-0T.\n\nThe fuel used depended on what was economically available to the railway. In the UK and other parts of Europe, plentiful supplies of coal made this the obvious choice from the earliest days of the steam engine. Until 1870, the majority of locomotives in the United States burned wood, but as the Eastern forests were cleared, coal gradually became more widely used. Thereafter, coal became and remained the dominant fuel worldwide until the end of general use of steam locomotives. Railways serving sugar cane farming operations burned bagasse, a byproduct of sugar refining. In the US, the ready availability of oil made it a popular steam locomotive fuel after 1900 for the southwestern railroads, particularly the Southern Pacific. In the Australian state of Victoria after World War II, many steam locomotives were converted to heavy oil firing. German, Russian, Australian and British railways experimented with using coal dust to fire locomotives.\n\nDuring World War II, a number of Swiss steam shunting locomotives were modified to use electrically heated boilers, consuming around 480 kW of power collected from an overhead line with a pantograph. These locomotives were significantly less efficient than electric ones; they were used because Switzerland had access to plentiful hydroelectricity, and suffered from a shortage of coal because of the war.\n\nA number of tourist lines and heritage locomotives in Switzerland, Argentina and Australia have used light diesel-type oil.\n\nWater was supplied at stopping places and locomotive depots from a dedicated water tower connected to water cranes or gantries. In the UK, the US and France, water troughs (\"track pans\" in the US) were provided on some main lines to allow locomotives to replenish their water supply without stopping, from rainwater or snowmelt that filled the trough due to inclement weather. This was achieved by using a deployable \"water scoop\" fitted under the tender or the rear water tank in the case of a large tank engine; the fireman remotely lowered the scoop into the trough, the speed of the engine forced the water up into the tank, and the scoop was raised again once it was full.\n\nWater is an essential element in the operation of a steam locomotive. As Swengel argued:\n\nSwengel went on to note that \"at low temperature and relatively low boiler outputs\", good water and regular boiler washout was an acceptable practice, even though such maintenance was high. As steam pressures increased, however, a problem of \"foaming\" or \"priming\" developed in the boiler, wherein dissolved solids in the water formed \"tough-skinned bubbles\" inside the boiler, which in turn were carried into the steam pipes and could blow off the cylinder heads. To overcome the problem, hot mineral-concentrated water was deliberately wasted (blown down) from the boiler periodically. Higher steam pressures required more blowing-down of water out of the boiler. Oxygen generated by boiling water attacks the boiler, and with increased steam pressure the rate of rust (iron oxide) generated inside the boiler increases. One way to help overcome the problem was water treatment. Swengel suggested that these problems contributed to the interest in electrification of railways.\n\nIn the 1970s, L.D. Porta developed a sophisticated system of heavy-duty chemical water treatment (Porta Treatment) that not only keeps the inside of the boiler clean and prevents corrosion, but modifies the foam in such a way as to form a compact \"blanket\" on the water surface that filters the steam as it is produced, keeping it pure and preventing carry-over into the cylinders of water and suspended abrasive matter.\n\nA steam locomotive is normally controlled from the boiler's backhead, and the crew is usually protected from the elements by a cab. A crew of at least two people is normally required to operate a steam locomotive. One, the train driver or engineer (North America), is responsible for controlling the locomotive's starting, stopping and speed, and the fireman is responsible for maintaining the fire, regulating steam pressure and monitoring boiler and tender water levels. Due to the historical loss of operational infrastructure and staffing, preserved steam locomotives operating on the mainline will often have a support crew travelling with the train.\n\nAll locomotives are fitted with a variety of appliances. Some of these relate directly to the operation of the steam engine; while others are for signalling, train control or other purposes. In the United States, the Federal Railroad Administration mandated the use of certain appliances over the years in response to safety concerns. The most typical appliances are as follows:\nWater (feedwater) must be delivered to the boiler to replace that which is exhausted as steam after delivering a working stroke to the pistons. As the boiler is under pressure during operation, feedwater must be forced into the boiler at a pressure that is greater than the steam pressure, necessitating the use of some sort of pump. Hand-operated pumps sufficed for the very earliest locomotives. Later engines used pumps driven by the motion of the pistons (axle pumps), which were simple to operate, reliable and could handle large quantities of water but only operated when the locomotive was moving and could overload the valve gear and piston rods at high speeds. Steam injectors later replaced the pump, while some engines transitioned to turbopumps. Standard practice evolved to use two independent systems for feeding water to the boiler; either two steam injectors or, on more conservative designs, axle pumps when running at service speed and a steam injector for filling the boiler when stationary or at low speeds. By the 20th century virtually all new-built locomotives used only steam injectors – often one injector was supplied with \"live\" steam straight from the boiler itself and the other used exhaust steam from the locomotive's cylinders, which was more efficient (since it made use of otherwise wasted steam) but could only be used when the locomotive was in motion and the regulator was open. Injectors became unreliable if the feedwater was at a high temperature, so locomotives with feedwater heaters, tank locomotives with the tanks in contact with the boiler and condensing locomotives sometimes used reciprocating steam pumps or turbopumps. \n\nVertical glass tubes, known as water gauges or water glasses, show the level of water in the boiler and are carefully monitored at all times while the boiler is being fired. Before the 1870s it was more common to have a series of try-cocks fitted to the boiler within reach of the crew; each try cock (at least two and usually three were fitted) was mounted at a different level. By opening each try-cock and seeing if steam or water vented through it, the level of water in the boiler could be estimated with limited accuracy. As boiler pressures increased the use of try-cocks became increasingly dangerous and the valves were prone to blockage with scale or sediment, giving false readings. This led to their replacement with the sight glass. As with the injectors, two glasses with separate fittings were usually installed to provide independent readings.\n\nThe term for pipe and boiler insulation is \"lagging\" which derives from the cooper's term for a wooden barrel stave. Two of the earliest steam locomotives used wooden lagging to insulate their boilers: the Salamanca, the first commercially successful steam locomotive, built in 1812, and the Locomotion No. 1, the first steam locomotive to carry passengers on a public rail line. Large amounts of heat are wasted if a boiler is not insulated. Early locomotives used lags, shaped wooden staves, fitted lengthways along the boiler barrel, and held in place by hoops, metal bands, the terms and methods are from cooperage.\nImproved insulating methods included applying a thick paste containing a porous mineral such as kieselgur, or attaching shaped blocks of insulating compound such as magnesia blocks. In the latter days of steam, \"mattresses\" of stitched asbestos cloth stuffed with asbestos fibre were fixed to the boiler, on separators so as not quite to touch the boiler. However, asbestos is currently banned in most countries for health reasons. The most common modern day material is glass wool, or wrappings of aluminium foil.\n\nThe lagging is protected by a close-fitted sheet-metal casing known as boiler clothing or cleading.\n\nEffective lagging is particularly important for fireless locomotives; however in recent times under the influence of L.D. Porta, \"exaggerated\" insulation has been practised for all types of locomotive on all surfaces liable to dissipate heat, such as cylinder ends and facings between the cylinders and the mainframes. This considerably reduces engine warmup time with marked increase in overall efficiency.\n\nEarly locomotives were fitted with a valve controlled by a weight suspended from the end of a lever, with the steam outlet being stopped by a cone-shaped valve. As there was nothing to prevent the weighted lever from bouncing when the locomotive ran over irregularities in the track, thus wasting steam, the weight was later replaced by a more stable spring-loaded column, often supplied by Salter, a well-known spring scale manufacturer. The danger of these devices was that the driving crew could be tempted to add weight to the arm to increase pressure. Most early boilers were fitted with a tamper-proof \"lockup\" direct-loaded ball valve protected by a cowl. In the late 1850s, John Ramsbottom introduced a safety valve that became popular in Britain during the latter part of the 19th century. Not only was this valve tamper-proof, but tampering by the driver could only have the effect of easing pressure. George Richardson's safety valve was an American invention introduced in 1875, and was designed to release the steam only at the moment when the pressure attained the maximum permitted. This type of valve is in almost universal use at present. Britain's Great Western Railway was a notable exception to this rule, retaining the direct-loaded type until the end of its separate existence, because it was considered that such a valve lost less pressure between opening and closing.\n\nThe earliest locomotives did not show the pressure of steam in the boiler, but it was possible to estimate this by the position of the safety valve arm which often extended onto the firebox back plate; gradations marked on the spring column gave a rough indication of the actual pressure. The promoters of the Rainhill trials urged that each contender have a proper mechanism for reading the boiler pressure, and Stephenson devised a nine-foot vertical tube of mercury with a sight-glass at the top, mounted alongside the chimney, for his \"Rocket\". The Bourdon tube gauge, in which the pressure straightens an oval-section coiled tube of brass or bronze connected to a pointer, was introduced in 1849 and quickly gained acceptance, and is still used today. Some locomotives have an additional pressure gauge in the steam chest. This helps the driver avoid wheel-slip at startup, by warning if the regulator opening is too great.\n\nWood-burners emit large quantities of flying sparks which necessitate an efficient spark-arresting device generally housed in the smokestack. Many different types were fitted, the most common early type being the Bonnet stack that incorporated a cone-shaped deflector placed before the mouth of the chimney pipe, and a wire screen covering the wide stack exit. A more-efficient design was the Radley and Hunter centrifugal stack patented in 1850 (commonly known as the diamond stack), incorporating baffles so oriented as to induce a swirl effect in the chamber that encouraged the embers to burn out and fall to the bottom as ash. In the self-cleaning smokebox the opposite effect was achieved: by allowing the flue gasses to strike a series of deflector plates, angled in such a way that the blast was not impaired, the larger particles were broken into small pieces that would be ejected with the blast, rather than settle in the bottom of the smokebox to be removed by hand at the end of the run. As with the arrestor, a screen was incorporated to retain any large embers.\n\nLocomotives of the British Railways standard classes fitted with self-cleaning smokeboxes were identified by a small cast oval plate marked \"S.C.\", fitted at the bottom of the smokebox door. These engines required different disposal procedures and the plate highlighted this need to depot staff.\n\nA factor that limits locomotive performance is the rate at which fuel is fed into the fire. In the early 20th century some locomotives became so large that the fireman could not shovel coal fast enough. In the United States, various steam-powered mechanical stokers became standard equipment and were adopted and used elsewhere including Australia and South Africa.\n\nIntroducing cold water into a boiler reduces power, and from the 1920s a variety of heaters were incorporated. The most common type for locomotives was the exhaust steam feedwater heater that piped some of the exhaust through small tanks mounted on top of the boiler or smokebox or into the tender tank; the warm water then had to be delivered to the boiler by a small auxiliary steam pump. The rare economiser type differed in that it extracted residual heat from the exhaust gases. An example of this is the pre-heater drum(s) found on the Franco-Crosti boiler.\n\nThe use of live steam and exhaust steam injectors also assists in the pre-heating of boiler feedwater to a small degree, though there is no efficiency advantage to live steam injectors. Such pre-heating also reduces the thermal shock that a boiler might experience when cold water is introduced directly. This is further helped by the top feed, where water is introduced to the highest part of the boiler and made to trickle over a series of trays. G.J. Churchward fitted this arrangement to the high end of his domeless coned boilers. Other British lines such as the LBSCR fitted some locomotives with the top feed inside a separate dome forward of the main one.\n\nSteam locomotives consume vast quantities of water because they operate on an open cycle, expelling their steam immediately after a single use rather than recycling it in a closed loop as stationary and marine steam engines do. Water was a constant logistical problem, and condensing engines were devised for use in desert areas. These engines had huge radiators in their tenders and instead of exhausting steam out of the funnel it was captured, passed back to the tender and condensed. The cylinder lubricating oil was removed from the exhausted steam to avoid a phenomenon known as priming, a condition caused by foaming in the boiler which would allow water to be carried into the cylinders causing damage because of its incompressibility. The most notable engines employing condensers (Class 25, the \"puffers which never puff\") worked across the Karoo desert of South Africa from the 1950s until the 1980s.\n\nSome British and American locomotives were equipped with scoops which collected water from \"water troughs\" (\"track pans\" in the US) while in motion, thus avoiding stops for water. In the US, small communities often did not have refilling facilities. During the early days of railroading, the crew simply stopped next to a stream and filled the tender using leather buckets. This was known as \"jerking water\" and led to the term \"jerkwater towns\" (meaning a small town, a term which today is considered derisive). In Australia and South Africa, locomotives in drier regions operated with large oversized tenders and some even had an additional water wagon, sometimes called a \"canteen\" or in Australia (particularly in New South Wales) a \"water gin\".\n\nSteam locomotives working on underground railways (such as London's Metropolitan Railway) were fitted with condensing apparatus to prevent steam from escaping into the railway tunnels. These were still being used between King's Cross and Moorgate into the early 1960s.\n\nLocomotives have their own braking system, independent from the rest of the train. Locomotive brakes employ large shoes which press against the driving wheel treads. With the advent of compressed air brakes, a separate system allowed the driver to control the brakes on all cars. A single-stage, steam-driven, air compressor was mounted on the side of the boiler. Long freight trains needed more air and a two-stage compressor with LP and HP cylinders, driven by cross-compound HP and LP steam cylinders, was introduced. It had three and a half times the capacity of the single stage. Most were made by Westinghouse. Two were fitted in front of the smokebox on big articulated locomotives. Westinghouse systems were used in the United States, Canada, Australia and New Zealand.\n\nAn alternative to the air brake is the vacuum brake, in which a steam-operated ejector is mounted on the engine instead of the air pump, to create a vacuum and release the brakes. A secondary ejector or crosshead vacuum pump is used to maintain the vacuum in the system against the small leaks in the pipe connections between carriages and wagons. Vacuum systems existed on British, Indian, West Australian and South African railway networks.\n\nSteam locomotives are fitted with sandboxes from which sand can be deposited on top of the rail to improve traction and braking in wet or icy weather. On American locomotives the sandboxes, or sand domes, are usually mounted on top of the boiler. In Britain, the limited loading gauge precludes this, so the sandboxes are mounted just above, or just below, the running plate.\n\nThe pistons and valves on the earliest locomotives were lubricated by the enginemen dropping a lump of tallow down the blast pipe.\n\nAs speeds and distances increased, mechanisms were developed that injected thick mineral oil into the steam supply. The first, a displacement lubricator, mounted in the cab, uses a controlled stream of steam condensing into a sealed container of oil. Water from the condensed steam displaces the oil into pipes. The apparatus is usually fitted with sight-glasses to confirm the rate of supply. A later method uses a mechanical pump worked from one of the crossheads. In both cases, the supply of oil is proportional to the speed of the locomotive.\n\nLubricating the frame components (axle bearings, horn blocks and bogie pivots) depends on capillary action: trimmings of worsted yarn are trailed from oil reservoirs into pipes leading to the respective component. The rate of oil supplied is controlled by the size of the bundle of yarn and not the speed of the locomotive, so it is necessary to remove the trimmings (which are mounted on wire) when stationary. However, at regular stops (such as a terminating station platform), oil finding its way onto the track can still be a problem.\n\nCrank pin and crosshead bearings carry small cup-shaped reservoirs for oil. These have feed pipes to the bearing surface that start above the normal fill level, or are kept closed by a loose-fitting pin, so that only when the locomotive is in motion does oil enter. In United Kingdom practice the cups are closed with simple corks, but these have a piece of porous cane pushed through them to admit air. It is customary for a small capsule of pungent oil (aniseed or garlic) to be incorporated in the bearing metal to warn if the lubrication fails and excess heating or wear occurs.\n\nWhen the locomotive is running under power, a draught on the fire is created by the exhaust steam directed up the chimney by the blastpipe. Without draught, the fire will quickly die down and steam pressure will fall. When the locomotive is stopped, or coasting with the regulator closed, there is no exhaust steam to create a draught, so the draught is maintained by means of a blower. This is a ring placed either around the base of the chimney, or around the blast pipe orifice, containing several small steam nozzles directed up the chimney. These nozzles are fed with steam directly from the boiler, controlled by the blower valve. When the regulator is open, the blower valve is closed; when the driver intends to close the regulator, he will first open the blower valve. It is important that the blower be opened before the regulator is closed, since without draught on the fire, there may be backdraught – where atmospheric air blows down the chimney, causing the flow of hot gases through the boiler tubes to be reversed, with the fire itself being blown through the firehole onto the footplate, with serious consequences for the crew. The risk of backdraught is higher when the locomotive enters a tunnel because of the pressure shock. The blower is also used to create draught when steam is being raised at the start of the locomotive's duty, at any time when the driver needs to increase the draught on the fire, and to clear smoke from the driver's line of vision.\n\nBlowbacks were fairly common. In a 1955 report on an accident near Dunstable, the Inspector wrote, \"In 1953 twenty-three cases, which were not caused by an engine defect, were reported and they resulted in 26 enginemen receiving injuries. In 1954 the number of occurrences and of injuries were the same and there was also one fatal casualty.\" They remain a problem, as evidenced by the 2012 incident with BR standard class 7 70013 Oliver Cromwell.\n\nIn British and European (except former Soviet Union countries) practice, locomotives usually have buffers at each end to absorb compressive loads (\"buffets\"). The tensional load of drawing the train (draft force) is carried by the coupling system. Together these control slack between the locomotive and train, absorb minor impacts and provide a bearing point for pushing movements.\n\nIn Canadian and American practice all of the forces between the locomotive and cars are handled through the coupler – particularly the Janney coupler, long standard on American railroad rolling stock – and its associated draft gear, which allows some limited slack movement. Small dimples called \"poling pockets\" at the front and rear corners of the locomotive allowed cars to be pushed onto an adjacent track using a pole braced between the locomotive and the cars. In Britain and Europe, North American style \"buckeye\" and other couplers that handle forces between items of rolling stock have become increasingly popular.\n\nA pilot was usually fixed to the front end of locomotives, although in European and a few other railway systems including New South Wales, they were considered unnecessary. Plough-shaped, sometimes called \"cow catchers\", they were quite large and were designed to remove obstacles from the track such as cattle, bison, other animals or tree limbs. Though unable to \"catch\" stray cattle, these distinctive items remained on locomotives until the end of steam. Switching engines usually replaced the pilot with small steps, known as \"footboards\". Many systems used the pilot and other design features to produce a distinctive appearance.\n\nWhen night operations began, railway companies in some countries equipped their locomotives with lights to allow the driver to see what lay ahead of the train, or to enable others to see the locomotive. Headlights were originally oil or acetylene lamps, but when electric arc lamps became available in the late 1880s, they quickly replaced the older types.\n\nBritain did not adopt bright headlights as they would affect night vision and so could mask the low-intensity oil lamps used in the semaphore signals and at each end of trains, increasing the danger of missing signals, especially on busy tracks. Locomotive stopping distances were also normally much greater than the range of headlights, and the railways were well-signalled and fully fenced to prevent livestock and people from straying onto them, largely negating the need for bright lamps. Thus low-intensity oil lamps continued to be used, positioned on the front of locomotives to indicate the class of each train. Four \"lamp irons\" (brackets on which to place the lamps) were provided: one below the chimney and three evenly spaced across the top of the buffer beam. The exception to this was the Southern Railway and its constituents, who added an extra lamp iron each side of the smokebox, and the arrangement of lamps (or in daylight, white circular plates) told railway staff the origin and destination of the train. On all vehicles, equivalent lamp irons were also provided on the rear of the locomotive or tender for when the locomotive was running tender- or bunker-first.\n\nIn some countries, heritage steam operation continues on the national network. Some railway authorities have mandated powerful headlights on at all times, including during daylight. This was to further inform the public or track workers of any active trains.\n\nLocomotives used bells and steam whistles from earliest days of steam locomotion. In the United States, India and Canada, bells warned of a train in motion. In Britain, where all lines are by law fenced throughout, bells were only a requirement on railways running on a road (i.e. not fenced off), for example a tramway along the side of the road or in a dockyard. Consequently, only a minority of locomotives in the UK carried bells. Whistles are used to signal personnel and give warnings. Depending on the terrain the locomotive was being used in, the whistle could be designed for long-distance warning of impending arrival, or for more localised use.\n\nEarly bells and whistles were sounded through pull-string cords and levers. Automatic bell ringers came into widespread use in the US after 1910.\n\nFrom the early 20th century operating companies in such countries as Germany and Britain began to fit locomotives with Automatic Warning System (AWS) in-cab signalling, which automatically applied the brakes when a signal was passed at \"caution\". In Britain, these became mandatory in 1956. In the United States, the Pennsylvania Railroad also fitted their locomotives with such devices.\n\nThe booster engine was an auxiliary steam engine which provided extra tractive effort for starting. It was a low speed device, usually mounted on the trailing truck. It was dis-engaged via an idler gear at a low speed, eg 30 km/hr. Boosters were widely used in the US and tried experimentally in Britain and France. On the narrow-gauged New Zealand railway system, six Kb 4-8-4 locomotives were fitted with boosters, the only engines in the world to have such equipment.\n\nBooster engines were also fitted to tender trucks in the US and known as auxiliary locomotives. Two and even three truck axles were connected together using side rods which limited them to slow-speed service.\n\nThe firedoor is used to cover the firehole when coal is not being added. It serves two purposes, first it prevents air being drawn over the top of the fire, rather forcing it to be drawn through it. The second purpose is to safeguard the train crew against blowbacks. It does however have a means to allow some air to pass over the top of the fire (referred to as \"secondary air\") to complete the combustion of gases produced by the fire.\n\nFiredoors come in multiple designs, the most basic of which is a single piece which is hinged on one side and can swing open onto the footplate. This design has two issues, first it takes up lots of room on the footplate the second is the draught will tend to pull it completely shut, thus cutting off any secondary air. To compensate for this some locomotives are fitted with a latch that prevents the firedoor from closing completely whereas others have a small vent on the door that may be opened to allow secondary air to flow through. Though it was considered to design a firedoor that opens inwards into the firebox thus preventing the inconvience caused on the footplate, such a door would be exposed to the full heat of the fire and would likely deform, thus becoming useless.\n\nA more popular type of firedoor consists of a two piece sliding door operated by a single lever. There are tracks above and below the firedoor which the door runs along. These tracks are prone to becoming jammed by debris and the doors required more effort to open than the aforementioned swinging door. In order to address this some firedoors use powered operation which utilized a steam or air cylinder to open the door. Among these are the butterfly doors which pivot at the upper corner, the pivoting action offers low resistance to the cylinder that opens the door.\n\nNumerous variations on the basic locomotive occurred as railways attempted to improve efficiency and performance.\n\nEarly steam locomotives had two cylinders, one either side, and this practice persisted as the simplest arrangement. The cylinders could be mounted between the main frames (known as \"inside\" cylinders), or mounted outside the frames and driving wheels (\"outside\" cylinders). Inside cylinders are driven by cranks built into the driving axle; outside cylinders are driven by cranks on extensions to the driving axles.\n\nLater designs employed three or four cylinders, mounted both inside and outside the frames, for a more even power cycle and greater power output. This was at the expense of more complicated valve gear and increased maintenance requirements. In some cases the third cylinder was added inside simply to allow for smaller diameter outside cylinders, and hence reduce the width of the locomotive for use on lines with a restricted loading gauge, for example the SR K1 and U1 classes.\n\nMost British express-passenger locomotives built between 1930 and 1950 were 4-6-0 or 4-6-2 types with three or four cylinders (e.g. GWR 6000 Class, LMS Coronation Class, SR Merchant Navy Class, LNER Gresley Class A3). From 1951, all but one of the 999 new British Rail standard class steam locomotives across all types used 2-cylinder configurations for easier maintenance.\n\nEarly locomotives used a simple valve gear that gave full power in either forward or reverse. Soon the Stephenson valve gear allowed the driver to control cut-off; this was largely superseded by Walschaerts valve gear and similar patterns. Early locomotive designs using slide valves and outside admission were relatively easy to construct, but inefficient and prone to wear. Eventually, slide valves were superseded by inside admission piston valves, though there were attempts to apply poppet valves (commonly used in stationary engines) in the 20th century. Stephenson valve gear was generally placed within the frame and was difficult to access for maintenance; later patterns applied outside the frame were more readily visible and maintained.\n\nCompound locomotives were used from 1876, expanding the steam twice or more through separate cylinders – reducing thermal losses caused by cylinder cooling. Compound locomotives were especially useful in trains where long periods of continuous efforts were needed. Compounding contributed to the dramatic increase in power achieved by André Chapelon's rebuilds from 1929. A common application was in articulated locomotives, the most common being that designed by Anatole Mallet, in which the high-pressure stage was attached directly to the boiler frame; in front of this was pivoted a low-pressure engine on its own frame, which takes the exhaust from the rear engine.\n\nMore-powerful locomotives tend to be longer, but long rigid-framed designs are impractical for the tight curves frequently found on narrow-gauge railways. Various designs for articulated locomotives were developed to overcome this problem. The Mallet and the Garratt were the two most popular, both using a single boiler and two engines (sets of cylinders and driving wheels). The Garratt has two power bogies, whereas the Mallet has one. There were also a few examples of triplex locomotives that had a third engine under the tender. Both the front and tender engines were low-pressure compounded, though they could be operated simple (high-pressure) for starting off. Other less common variations included the Fairlie locomotive, which had two boilers back-to-back on a common frame, with two separate power bogies.\n\nDuplex locomotives, containing two engines in one rigid frame, were also tried, but were not notably successful. For example, the 4-4-4-4 Pennsylvania Railroad's T1 class, designed for very fast running, suffered recurring and ultimately unfixable slippage problems throughout their careers.\n\nFor locomotives where a high starting torque and low speed were required, the conventional direct drive approach was inadequate. \"Geared\" steam locomotives, such as the Shay, the Climax and the Heisler, were developed to meet this need on industrial, logging, mine and quarry railways. The common feature of these three types was the provision of reduction gearing and a drive shaft between the crankshaft and the driving axles. This arrangement allowed the engine to run at a much higher speed than the driving wheels compared to the conventional design, where the ratio is 1:1.\n\nIn the United States on the Southern Pacific Railroad, a series of cab forward locomotives were produced with the cab and the firebox at the front of the locomotive and the tender behind the smokebox, so that the engine appeared to run backwards. This was only possible by using oil-firing. Southern Pacific selected this design to provide air free of smoke for the engine driver to breathe as the locomotive passed through mountain tunnels and snow sheds. Another variation was the Camelback locomotive, with the cab situated halfway along the boiler. In England, Oliver Bulleid developed the SR Leader class locomotive during the nationalisation process in the late 1940s. The locomotive was heavily tested but several design faults (such as coal firing and sleeve valves) meant that this locomotive and the other part-built locomotives were scrapped. The cab-forward design was taken by Bulleid to Ireland, where he moved after nationalisation, where he developed the \"turfburner\". This locomotive was more successful, but was scrapped due to the dieselisation of the Irish railways.\n\nThe only preserved cab forward locomotive is Southern Pacific 4294 in Sacramento, California.\n\nIn France, the three Heilmann locomotives were built with a cab forward design.\n\nSteam turbines were created as an attempt to improve the operation and efficiency of steam locomotives. Experiments with steam turbines using direct-drive and electrical transmissions in various countries proved mostly unsuccessful. The London, Midland and Scottish Railway built the Turbomotive, a largely successful attempt to prove the efficiency of steam turbines. Had it not been for the outbreak of World War II, more may have been built. The Turbomotive ran from 1935 to 1949, when it was rebuilt into a conventional locomotive because many parts required replacement, an uneconomical proposition for a \"one-off\" locomotive. In the United States, Union Pacific, Chesapeake and Ohio and Norfolk & Western (N&W) railways all built turbine-electric locomotives. The Pennsylvania Railroad (PRR) also built turbine locomotives, but with a direct-drive gearbox. However, all designs failed due to dust, vibration, design flaws or inefficiency at lower speeds. The final one remaining in service was the N&W's, retired in January 1958. The only truly successful design was the TGOJ MT3, used for hauling iron ore from Grängesberg in Sweden to the ports of Oxelösund. Despite functioning correctly, only three were built. Two of them are preserved in working order in museums in Sweden.\n\nIn a fireless locomotive the boiler is replaced by a steam accumulator, which is charged with steam (actually water at a temperature well above boiling point, 212 °F/100 °C) from a stationary boiler. Fireless locomotives were used where there was a high fire risk (e.g. oil refineries), where cleanliness was important (e.g. food-production plants) or where steam is readily available (e.g. paper mills and power stations where steam is either a by-product or is cheaply available). The water vessel (\"boiler\") is heavily insulated the same as with a fired locomotive. Until all the water has boiled away, the steam pressure does not drop except as the temperature drops.\n\nAnother class of fireless locomotive is a compressed-air locomotive.\n\nMixed power locomotives, utilising both steam and diesel propulsion, have been produced in Russia, Britain and Italy.\n\nUnder unusual conditions (lack of coal, abundant hydroelectricity) some locomotives in Switzerland were modified to use electricity to heat the boiler, making them electric-steam locomotives.\n\nA steam-electric locomotive is similar in concept to a diesel-electric locomotive, except that a steam engine instead of a diesel engine is used to drive a generator. Three such locomotives were built by the French engineer in the 1890s.\n\nSteam locomotives are categorised by their wheel arrangement. The two dominant systems for this are the Whyte notation and UIC classification.\n\nThe Whyte notation, used in most English-speaking and Commonwealth countries, represents each set of wheels with a number. These numbers typically represented the number of un-powered leading wheels, followed by the number of driving wheels (sometimes in several groups), followed by the number of un-powered trailing wheels. For example, a yard engine with only 4 drive wheels would be categorised as a \"0-4-0\" wheel arrangement. A locomotive with a 4-wheel leading truck, followed by 6 drive wheels, and a 2-wheel trailing truck, would be classed as a \"4-6-2\". Different arrangements were given names which usually reflect the first usage of the arrangement; for instance the \"Santa Fe\" type (2-10-2) is so called because the first examples were built for the Atchison, Topeka and Santa Fe Railway. These names were informally given and varied according to region and even politics.\n\nThe UIC classification is used mostly in European countries apart from the United Kingdom. It designates consecutive pairs of wheels (informally \"axles\") with a number for non-driving wheels and a capital letter for driving wheels (A=1, B=2, etc.) So a Whyte 4-6-2 designation would be an equivalent to a 2-C-1 UIC designation.\n\nOn many railroads, locomotives were organised into classes. These broadly represented locomotives which could be substituted for each other in service, but most commonly a class represented a single design. As a rule classes were assigned some sort of code, generally based on the wheel arrangement. Classes also commonly acquired nicknames, such as \"Pugs\", representing notable (and sometimes uncomplimentary) features of the locomotives.\n\nIn the steam locomotive era, two measures of locomotive performance were generally applied. At first, locomotives were rated by tractive effort, defined as the average force developed during one revolution of the driving wheels at the rail head. This can be roughly calculated by multiplying the total piston area by 85% of the boiler pressure (a rule of thumb reflecting the slightly lower pressure in the steam chest above the cylinder), and dividing by the ratio of the driver diameter over the piston stroke. However, the precise formula is:\n\nwhere is the bore of the cylinder (diameter) in inches,\nand is a factor that depends on the effective cut-off. In the US, is usually set at 0.85, but lower on engines that have maximum cutoff limited to 50–75%.\n\nThe tractive effort is only the \"average\" force, as not all effort is constant during the one revolution of the drivers. At some points of the cycle only one piston is exerting turning moment and at other points both pistons are working. Not all boilers deliver full power at starting, and the tractive effort also decreases as the rotating speed increases.\n\nTractive effort is a measure of the heaviest load a locomotive can start or haul at very low speed over the ruling grade in a given territory. However, as the pressure grew to run faster goods and heavier passenger trains, tractive effort was seen to be an inadequate measure of performance because it did not take into account speed. Therefore, in the 20th century, locomotives began to be rated by power output. A variety of calculations and formulas were applied, but in general railways used dynamometer cars to measure tractive force at speed in actual road testing.\n\nBritish railway companies have been reluctant to disclose figures for drawbar horsepower and have usually relied on continuous tractive effort instead.\n\nWhyte classification is indirectly connected to locomotive performance. Given adequate proportions of the rest of the locomotive, power output is determined by the size of the fire, and for a bituminous coal-fuelled locomotive, this is determined by the grate area. Modern non-compound locomotives are typically able to produce about 40 drawbar horsepower per square foot of grate. Tractive force, as noted earlier, is largely determined by the boiler pressure, the cylinder proportions and the size of the driving wheels. However, it is also limited by the weight on the driving wheels (termed \"adhesive weight\"), which needs to be at least four times the tractive effort.\n\nThe weight of the locomotive is roughly proportional to the power output; the number of axles required is determined by this weight divided by the axleload limit for the trackage where the locomotive is to be used. The number of driving wheels is derived from the adhesive weight in the same manner, leaving the remaining axles to be accounted for by the leading and trailing bogies. Passenger locomotives conventionally had two-axle leading bogies for better guidance at speed; on the other hand, the vast increase in the size of the grate and firebox in the 20th century meant that a trailing bogie was called upon to provide support. In Europe, some use was made of several variants of the \"Bissel bogie\" in which the swivelling movement of a single axle truck controls the lateral displacement of the front driving axle (and in one case the second axle too). This was mostly applied to 8-coupled express and mixed traffic locomotives, and considerably improved their ability to negotiate curves whilst restricting overall locomotive wheelbase and maximising adhesion weight.\n\nAs a rule, \"shunting engines\" (US: \"switching engines\") omitted leading and trailing bogies, both to maximise tractive effort available and to reduce wheelbase. Speed was unimportant; making the smallest engine (and therefore smallest fuel consumption) for the tractive effort was paramount. Driving wheels were small and usually supported the firebox as well as the main section of the boiler. Banking engines (US: \"helper engines\") tended to follow the principles of shunting engines, except that the wheelbase limitation did not apply, so banking engines tended to have more driving wheels. In the US, this process eventually resulted in the Mallet type engine with its many driven wheels, and these tended to acquire leading and then trailing bogies as guidance of the engine became more of an issue.\n\nAs locomotive types began to diverge in the late 19th century, freight engine designs at first emphasised tractive effort, whereas those for passenger engines emphasised speed. Over time, freight locomotive size increased, and the overall number of axles increased accordingly; the leading bogie was usually a single axle, but a trailing truck was added to larger locomotives to support a larger firebox that could no longer fit between or above the driving wheels. Passenger locomotives had leading bogies with two axles, fewer driving axles, and very large driving wheels in order to limit the speed at which the reciprocating parts had to move.\n\nIn the 1920s, the focus in the United States turned to horsepower, epitomised by the \"super power\" concept promoted by the Lima Locomotive Works, although tractive effort was still the prime consideration after World War I to the end of steam. Goods trains were designed to run faster, while passenger locomotives needed to pull heavier loads at speed. This was achieved by increasing the size of grate and firebox without changes to the rest of the locomotive, requiring the addition of a second axle to the trailing truck. Freight 2-8-2s became 2-8-4s while 2-10-2s became 2-10-4s. Similarly, passenger 4-6-2s became 4-6-4s. In the United States this led to a convergence on the dual-purpose 4-8-4 and the 4-6-6-4 articulated configuration, which was used for both freight and passenger service. Mallet locomotives went through a similar transformation, evolving from bank engines into huge mainline locomotives with much larger fireboxes; their driving wheels were also increased in size in order to allow faster running.\n\nThe most-manufactured single class of steam locomotive in the world is the 0-10-0 Russian locomotive class E steam locomotive with around 11,000 produced both in Russia and other countries such as Czechoslovakia, Germany, Sweden, Hungary and Poland. The Russian locomotive class O numbered 9,129 locomotives, built between 1890 and 1928. Around 7,000 units were produced of the German DRB Class 52 2-10-0 Kriegslok.\nThe British GWR 5700 class numbered about 863 units. The DX class of the London and North Western Railway numbered 943 units, including 86 engines built for the Lancashire and Yorkshire Railway.\n\nBefore the 1923 Grouping Act, production in the UK was mixed. The larger railway companies built locomotives in their own workshops, with the smaller ones and industrial concerns ordering them from outside builders. A large market for outside builders existed due to the home-build policy exercised by the main railway companies. An example of a pre-grouping works was the one at Melton Constable, which maintained and built some of the locomotives for the Midland and Great Northern Joint Railway. Other works included one at Boston (an early GNR building) and Horwich works.\n\nBetween 1923 and 1947, the \"Big Four\" railway companies (the Great Western Railway, the London, Midland and Scottish Railway, the London and North Eastern Railway and the Southern Railway) all built most of their own locomotives, only buying locomotives from outside builders when their own works were fully occupied (or as a result of government-mandated standardisation during wartime).\n\nFrom 1948, British Railways allowed the former \"Big Four\" companies (now designated as \"Regions\") to continue to produce their own designs, but also created a range of standard locomotives which supposedly combined the best features from each region. Although a policy of \"dieselisation\" was adopted in 1955, BR continued to build new steam locomotives until 1960, with the final engine being named \"Evening Star\".\n\nSome independent manufacturers produced steam locomotives for a few more years, with the last British-built industrial steam locomotive being constructed by Hunslet in 1971. Since then, a few specialised manufacturers have continued to produce small locomotives for narrow gauge and miniature railways, but as the prime market for these is the tourist and heritage railway sector, the demand for such locomotives is limited. In November 2008, a new build main line steam locomotive, 60163 \"Tornado\", was tested on UK mainlines for eventual charter and tour use.\n\nIn the 19th and early 20th centuries, most Swedish steam locomotives were manufactured in Britain. Later, however, most steam locomotives were built by local factories including NOHAB in Trollhättan and ASJ in Falun. One of the most successful types was the class \"B\" (4-6-0), inspired by the Prussian class P8. Many of the Swedish steam locomotives were preserved during the Cold War in case of war. During the 1990s, these steam locomotives were sold to non-profit associations or abroad, which is why the Swedish class B, class S (2-6-4) and class E2 (2-8-0) locomotives can now be seen in Britain, the Netherlands, Germany and Canada.\n\nLocomotives for American railroads were nearly always built in the United States with very few imports, except in the earliest days of steam engines. This was due to the basic differences of markets in the United States which initially had many small markets located large distances apart, in contrast to Europe's higher density of markets. Locomotives that were cheap and rugged and could go large distances over cheaply built and maintained tracks were required. Once the manufacture of engines was established on a wide scale there was very little advantage to buying an engine from overseas that would have to be customised to fit the local requirements and track conditions. Improvements in engine design of both European and US origin were incorporated by manufacturers when they could be justified in a generally very conservative and slow-changing market. With the notable exception of the USRA standard locomotives built during World War I, in the United States, steam locomotive manufacture was always semi-customised. Railroads ordered locomotives tailored to their specific requirements, though some basic design features were always present. Railroads developed some specific characteristics; for example, the Pennsylvania Railroad and the Great Northern Railway had a preference for the Belpaire firebox. In the United States, large-scale manufacturers constructed locomotives for nearly all rail companies, although nearly all major railroads had shops capable of heavy repairs and some railroads (for example, the Norfolk and Western Railway and the Pennsylvania Railroad, which had two erecting shops) constructed locomotives entirely in their own shops. Companies manufacturing locomotives in the US included Baldwin Locomotive Works, American Locomotive Company (Alco), and Lima Locomotive Works. Altogether, between 1830 and 1950, over 160,000 steam locomotives were built in the United States, with Baldwin alone accounting for a majority, nearly 70,000.\n\nSteam locomotives required regular and, compared to a diesel-electric engine, frequent service and overhaul (often at government-regulated intervals in Europe and the US). Alterations and upgrades regularly occurred during overhauls. New appliances were added, unsatisfactory features removed, cylinders improved or replaced. Almost any part of the locomotive, including boilers, was replaced or upgraded. When service or upgrades got too expensive the locomotive was traded off or retired. On the Baltimore and Ohio Railroad two 2-10-2 locomotives were dismantled; the boilers were placed onto two new Class T 4-8-2 locomotives and the residual wheel machinery made into a pair of Class U 0-10-0 switchers with new boilers. Union Pacific's fleet of 3-cylinder 4-10-2 engines were converted into two-cylinder engines in 1942, because of high maintenance problems.\n\nIn Sydney, Clyde Engineering and the workshops in Eveleigh both built steam locomotives for the New South Wales Government Railways. These include the C38 class 4-6-2; the first five were built at Clyde with streamlining, the other 25 locomotives were built at Eveleigh (13) and Cardiff Workshops (12) near Newcastle. In Queensland, steam locomotives were locally constructed by Walkers. Similarly the South Australian state government railways also manufactured steam locomotives locally at Islington Railway Workshops in Adelaide. Victorian Railways constructed most of their locomotives at their Newport Workshops and in Bendigo, while in the early days locomotives were built at the Phoenix Foundry in Ballarat. Locomotives constructed at the Newport shops ranged from the nA class 2-6-2T built for the narrow gauge, up to the H class 4-8-4 – the largest conventional locomotive ever to operate in Australia, weighing 260 tons. However, the title of largest locomotive ever used in Australia goes to the 263-ton NSWGR AD60 class 4-8-4+4-8-4 Garratt, built by Beyer-Peacock in the United Kingdom. Most steam locomotives used in Western Australia were built in the United Kingdom, though some examples were designed and built locally at the Western Australian Government Railways' Midland Railway Workshops. The 10 WAGR S class locomotives (introduced in 1943) were the only class of steam locomotive to be wholly conceived, designed and built in Western Australia, while the Midland workshops notably participated in the Australia-wide construction program of Australian Standard Garratts – these wartime locomotives were built at Midland in Western Australia, Clyde Engineering in New South Wales, Newport in Victoria and Islington in South Australia and saw varying degrees of service in all Australian states.\nThe introduction of electric locomotives around the turn of the 20th century and later diesel-electric locomotives spelled the beginning of a decline in the use of steam locomotives, although it was some time before they were phased out of general use. As diesel power (especially with electric transmission) became more reliable in the 1930s, it gained a foothold in North America. The full transition away from steam power in North America took place during the 1950s. In continental Europe, large-scale electrification had replaced steam power by the 1970s. Steam was a familiar technology, adapted well to local facilities, and also consumed a wide variety of fuels; this led to its continued use in many countries until the end of the 20th century.\n\nSteam engines have considerably less thermal efficiency than modern diesels, requiring constant maintenance and labour to keep them operational. Water is required at many points throughout a rail network, making it a major problem in desert areas, as are found in some regions of the United States, Australia and South Africa. In places where water is available, it may be hard, which can cause \"scale\" to form, composed mainly of calcium carbonate, magnesium hydroxide and calcium sulfate. Calcium and magnesium carbonates tend to be deposited as off-white solids on the inside the surfaces of pipes and heat exchangers. This precipitation is principally caused by thermal decomposition of bicarbonate ions but also happens in cases where the carbonate ion is at saturation concentration. The resulting build-up of scale restricts the flow of water in pipes. In boilers, the deposits impair the flow of heat into the water, reducing the heating efficiency and allowing the metal boiler components to overheat.\n\nThe reciprocating mechanism on the driving wheels of a two-cylinder single expansion steam locomotive tended to pound the rails (see hammer blow), thus requiring more maintenance. Raising steam from coal took a matter of hours, and created serious pollution problems. Coal-burning locomotives required fire cleaning and ash removal between turns of duty. Diesel or electric locomotives, by comparison, drew benefit from new custom-built servicing facilities. The smoke from steam locomotives was also deemed objectionable; the first electric and diesel locomotives were developed in response to smoke abatement requirements, although this did not take into account the high level of less-visible pollution in diesel exhaust smoke, especially when idling. In some countries, however, power for electric locomotives is derived from steam generated in power stations, which are often run by coal.\n\nThe first diesel locomotive appeared on the Central Railroad of New Jersey in 1925 and on the New York Central in 1927. Since then, diesel locomotives began to appear in mainline service in the United States in the mid-1930s. The diesel engines reduced maintenance costs dramatically, while increasing locomotive availability. On the Chicago, Rock Island and Pacific Railroad the new units delivered over a year, compared with about for a mainline steam locomotive. World War II delayed dieselisation in the US. In 1949 the Gulf, Mobile and Ohio Railroad became the first large mainline railroad to convert completely to diesel locomotives, and \"Life Magazine\" ran an article on 5 December 1949 titled \"The GM&O puts all its steam engines to torch, becomes first major US railroad to dieselize 100%\". The Susquehanna was one of the earliest railroads in America to fully dieselize by 1947 and retiring their steam locomotives by 1949. The final 2-8-4 Berkshire built was Nickle Plate Road's 779 built in 1949. The last steam locomotive manufactured for general service was a Norfolk and Western 0-8-0, built in its Roanoke shops in December, 1953. In Spring of 1960, Norfolk and Western Y6b 2190 and S1290 doused their fires for the last time in a Williamson, West Virginia roundhouse. 1960 is normally considered the final year of regular Class 1 main line standard gauge steam operation in the United States, with operations on the Grand Trunk Western, Illinois Central, Norfolk and Western and Duluth Missabe and Iron Range Railroads, as well as Canadian Pacific operations in Maine.\n\nHowever, the Grand Trunk Western used some steam power for regular passenger trains until 1961, the last instance of this occurring unannounced on trains 56 and 21 in the Detroit area on 20 September 1961 with 4-8-4 6323, one day before its flue time expired. The last steam-powered standard-gauge regular freight service by a class 1 railroad was on the isolated Leadville branch of the Colorado and Southern (Burlington Lines) 11 October 1962 with 2-8-0 641. Narrow-gauge steam was used for freight service by the Denver and Rio Grande Western on the run from Alamosa, Colorado to Farmington, New Mexico via Durango until service ceased on 6 December 1968. The Union Pacific is the only Class I railroad in the US to have never completely dieselised, at least nominally. It has always had at least one operational steam locomotive, Union Pacific 844, on its roster. Some US shortlines continued steam operations into the 1960s, and the Northwestern Steel and Wire mill in Sterling, Illinois, continued to operate steam locomotives until December 1980. Two surviving sections of the Denver and Rio Grande Western's Alamosa to Durango narrow-gauge line mentioned above, now operating separately as the Cumbres and Toltec Scenic Railroad and the Durango and Silverton Narrow Gauge Railroad, continue to use steam locomotives and operate as tourist railroads.\n\nBy the end of the 20th century, around 1,800 of the over 160,000 steam locomotives built in the United States between 1830 and 1950 still existed, but with only a few still in operating condition.\n\nTrials of diesel locomotives and railcars began in Britain in the 1930s but made only limited progress. One problem was that British diesel locomotives were often seriously under-powered compared with the steam locomotives against which they were competing. Moreover, labour and coal were relatively cheap.\n\nAfter 1945, problems associated with post-war reconstruction and the availability of cheap domestic-produced coal kept steam in widespread use throughout the two following decades. However the ready availability of cheap oil led to new dieselisation programmes from 1955, and these began to take full effect from around 1962. Towards the end of the steam era, steam motive power fell into a state of disrepair. The last steam locomotive built for mainline British Railways was BR Standard Class 9F 92220 Evening Star, which was completed in March 1960. The last steam-hauled service trains on the British Railways network ran in 1968, but the use of steam locomotives in British industry continued into the 1980s. In June 1975, there were still 41 locations where steam was in regular use, and many more where engines were maintained in reserve in case of diesel failures. Gradually, the decline of the ironstone quarries, steel, coal mining and shipbuilding industries – and the plentiful supply of redundant British Rail diesel shunters as replacements – led to the end of steam power for commercial uses.\n\nSeveral hundred rebuilt and preserved steam locomotives are still used on preserved volunteer-run 'heritage' railway lines in the UK. A proportion of the locomotives are regularly used on the national rail network by private operators where they run special excursions and touring trains. A new steam locomotive, the LNER Peppercorn Class A1 60163 Tornado has been built (began service in 2009), and more are in the planning stage.\n\nAfter the Second World War, Germany was divided into the Federal Republic of Germany, with the Deutsche Bundesbahn (founded in 1949) as the new state-owned railway, and the German Democratic Republic (GDR), where railway service continued under the old pre-war name Deutsche Reichsbahn.\n\nFor a short period after the war, both Bundesbahn (DB) and Reichsbahn (DR) still placed orders for new steam locomotives. They needed to renew the rolling stock, mostly with steam locomotives designed for accelerated passenger trains. Many of the existing predecessors of those types of steam locomotives in Germany had been lost in the battles or simply reached the end of their lifetime, such as the famous Prussian P 8. There was no need for new freight train engines, however, because thousands of the Classes 50 and 52 had been built during the Second World War.\nBecause the concept of the so-called \"Einheitslokomotiven\", the standard locomotives built in the 1920s and 1930s, and still in wide use, was already outdated in the pre-war era, a whole new design for the new steam locomotives was developed by DB and DR, called \"Neubaudampflokomotiven\" (new-build steam locomotives). The steam locomotives made by the DB in West Germany, under the guidance of Friedrich Witte, represented the latest evolution in steam locomotive construction including fully welded frames, high-performance boilers and roller bearings on all moving parts. Although these new DB classes (10, 23, 65, 66 and 82) were said to be among the finest and best-performing German steam locomotives ever built, none of them exceeded 25 years in service. The last one, 23 105 (still preserved), went into service in 1959.\n\nThe Democratic Republic in East Germany began a similar procurement plan, including engines for a narrow gauge. The DR-Neubaudampflokomotiven were the classes 23.10, 25.10, 50.40, 65.10, 83.10, 99.23-24 and 99.77-79. The purchase of new-build steam locomotives by the DR ended in 1960 with 50 4088, the last standard-gauge steam locomotive built in Germany. No locomotive of the classes 25.10 and 83.10 was in service for more than 17 years. The last engines of the classes 23.10, 65.10 and 50.40 were retired in the late 1970s, with some units older than 25 years. Some of the narrow-gauge locomotives are still in service for tourism purposes. Later, during the early 1960s, the DR developed a way to reconstruct older locomotives to conform with contemporary requirements. The high-speed locomotive 18 201 and the class 01.5 are examples of designs from that programme.\n\nAround 1960, the Bundesbahn in West Germany began to phase out all steam-hauled trains over a period of ten years, but still had about 5,000 of them in running condition. Even though DB were very assertive in continuing the electrification on the main lines – in 1963 they reached of electrified routes – and dieselisation with new developed stock, they had not completely removed steam locomotives within the ten-year goal. In 1972, the Hamburg and Frankfurt departments of the DB rail networks became the first to no longer operate steam locomotives in their areas. The remaining steam locomotives began to gather in rail yards in Rheine, Tübingen, Hof, Saarbrücken, Gelsenkirchen-Bismarck and others, which soon became popular with rail enthusiasts.\n\nIn 1975, DB's last steam express train made its final run on the Emsland-Line from Rheine to Norddeich in the upper north of Germany. Two years later, on 26 October 1977, the heavy freight engine 44 903 (computer-based new number 043 903-4) made her final run at the same railway yard. After this date, no regular steam service took place on the network of the DB until their privatisation in 1994.\nIn the GDR, the Reichsbahn continued steam operation until 1988 on standard gauge tracks for economic and political reasons, despite strong efforts to phase out steam being made since the 1970s. The last locomotives in service where of the classes 50.35 and 52.80, which hauled goods trains on rural main and branch lines. Unlike the DB, there was never a large concentration of steam locomotives in just a few yards in the East, because throughout the DR network the infrastructure for steam locomotives remained intact until the end of the GDR in 1990. This was also the reason that there was never a strict \"final cut\" at steam operations, with the DR continuing to use steam locomotives from time to time until they merged with the DB in 1994.\n\nOn their narrow-gauge lines, however, steam locomotives continued to be used on a daily year-round basis, mainly for tourist reasons. The largest of these is the \"Harzer Schmalspurbahn\" (Harz Narrow Gauge Railways) network in the Harz Mountains, but the lines in Saxony and on the coast of the Baltic Sea are also notable. Even though all former DR narrow-gauge railways have undergone privatisation, steam operations are still commonplace there.\n\n In the USSR, although the first mainline diesel-electric locomotive was built in USSR in 1924, the last steam locomotive (, serial number 251) was built in 1956; it is now in the Museum of Railway Machinery at the former Warsaw Rail Terminal, Saint Petersburg. In the European part of the USSR, almost all steam locomotives were replaced by diesel and electric locomotives in the 1960s; in Siberia and Central Asia, state records verify that L-class 2-10-0s and LV-class 2-10-2s were not retired until 1985. Until 1994, Russia had at least 1,000 steam locomotives stored in operable condition in case of \"national emergencies\".\n\nChina continued to build mainline steam locomotives until the late 20th century, even building a few examples for American tourist operations. China was the last main-line user of steam locomotives, with use ending officially on the Ji-Tong line at the end of 2005. Some steam locomotives are still in use in industrial operations in China. Some coal and other mineral operations maintain an active roster of China Railways JS (建设, \"Jiànshè\") or China Railways SY (上游, \"Shàngyóu\") steam locomotives bought secondhand from China Railway. The last steam locomotive built in China was 2-8-2 SY 1772, finished in 1999. at least six Chinese steam locomotives exist in the United States – 3 QJs bought by the Rail Development Corporation (Nos. 6988 and 7081 for IAIS and No. 7040 for R.J. Corman), a JS bought by the Boone and Scenic Valley Railroad, and two SYs. No. 142 (formerly No. 1647) is owned by the NYSW for tourist operations, re-painted and modified to represent a 1920s-era US locomotive; No. 58 is operated by the Valley Railroad and has been modified to represent New Haven Railroad number 3025.\n\nOwing to the destruction of most of the nation's infrastructure during the Second World War, and the cost of electrification and dieselisation, new steam locomotives were built in Japan until 1960. The number of Japanese steam locomotives reached a peak of 5,958 in 1946.\n\nWith the booming post-war Japanese economy, steam locomotives were gradually withdrawn from main line service beginning in the early 1960s, and were replaced with diesel and electric locomotives. They were relegated to branch line and sub-main line services for several more years until the late 1960s, when electrification and dieselisation began to increase. From 1970 onwards, steam locomotion was gradually abolished on the JNR:\n\nThe last steam passenger train, pulled by a C57-class locomotive built in 1940, departed from Muroran railway station to Iwamizawa on 14 December 1975. It was then officially retired from service, dismantled and sent to the Tokyo Transportation Museum, where it was inaugurated as an exhibit on 14 May 1976. It was moved to the Saitama Railway Museum in early 2007. The last Japanese main line steam train, D51-241, a D51-class locomotive built in 1939, left Yubari railway station on 24 December 1975. That same day, all steam main line service ended. D51-241 was retired on 10 March 1976, and destroyed in a depot fire a month later, though some parts were preserved.\n\nOn 2 March 1976, the only steam locomotive still operating on the JNR, 9600-39679, a 9600-class locomotive built in 1920, made its final journey from Oiwake railway station, ending 104 years of steam locomotion in Japan.\n\nThe first steam locomotive in South Korea (Korea at the time) was the Moga (Mogul) 2-6-0, which first ran on 9 September 1899 on the Gyeong-In Line. Other South Korean steam locomotive classes include the Sata, Pureo, Ame, Sig, Mika (USRA Heavy Mikado), Pasi (USRA Light Pacific), Hyeogi (Narrow gauge), Class 901, Mateo, Sori and Tou. Used until 1967, the Moga is now in the Railroad Museum.\n\nNew steam locomotives were built in India well into the early 1970s; the last broad-gauge steam locomotive to be manufactured, \"Last Star\", a WG-class locomotive (No. 10560) was built in June 1970, followed by the last meter-gauge locomotive in February 1972. Steam locomotion continued to predominate on Indian Railways through the early 1980s; in fiscal year 1980–81, there were 7,469 steam locomotives in regular service, compared to 2,403 diesels and 1,036 electrics. Subsequently, steam locomotion was gradually phased out from regular service, beginning with the Southern Railway Zone in 1985; the number of diesel and electric locomotives in regular service surpassed the number of steam locomotives in service in 1987–88. All regular broad-gauge steam service in India ended in 1995, with the final run made from Jalandhar to Ferozpur on 6 December. The last meter-gauge and narrow-gauge steam locomotives in regular service were retired in 2000. After being withdrawn from service, most steam locomotives were scrapped, though some have been preserved in various railway museums. The only steam locomotives remaining in regular service are on India's heritage lines.\n\nIn South Africa, the last new steam locomotives purchased were 2-6-2+2-6-2 Garratts from Hunslet Taylor for the gauge lines in 1968.\nAnother class 25NC locomotive, No. 3450, nicknamed the \"Red Devil\" because of its colour scheme, received modifications including a prominent set of double side-by-side exhaust stacks. In southern Natal, two former South African Railway gauge NGG16 Garratts operating on the privatised Port Shepstone and Alfred County Railway (ACR) received some L.D. Porta modifications in 1990, becoming a new NGG16A class.\n\nBy 1994 almost all commercial steam locomotives were put out of service, although many of them are preserved in museums or at railway stations for public viewing. Today only a few privately owned steam locomotives are still operating in South Africa, including the ones being used by the 5-star luxury train Rovos Rail, and the tourist trains \"Outeniqua Tjoe Choo\", \"Apple Express\" and (until 2008) \"Banana Express\".\n\nIn other countries, the dates for conversion from steam to diesel and electric power varied.\n\nOn the contiguous North American standard gauge network across Canada, Mexico and the United States, the use of standard gauge main line steam locomotion using 4-8-4s built in 1946 for handling freight between Mexico City and Irapuato lasted until 1968. The Mexican Pacific line, a standard gauge short line in the state of Sinaloa, was reported in August 1987 to still be using steam, with a roster of one 4-6-0, two 2-6-2s and one 2-8-2.\n\nBy March 1973 in Australia, steam was no longer used for industrial purposes. Diesel locomotives were more efficient and the demand for manual labour for service and repairs was less than for steam. Cheap oil also had cost advantages over coal. Regular scheduled steam services operated from 1998 until 2004 on the West Coast Railway.\n\nIn New Zealand's North Island, steam traction ended in 1968 when A 832 (now stored at the Glenbrook Vintage Railway, Auckland, but owned by MOTAT) hauled a Farmers Trading Company \"Santa Special\" from Frankton Junction to Claudelands. In the South Island, due to the inability of the new D class diesel locomotives to provide in-train steam heating, steam operations continued using the J and J class 4-8-2 tender locomotives on the overnight Christchurch-Invercargill expresses, Trains 189/190, until 1971. By this time sufficient F steam-heating vans were available, thus allowing the last steam locomotives to be withdrawn. Two A class 4-6-2 tender locomotives, A 778 and A 795, were retained at Lyttelton to steam-heat the coaches for the Boat Trains between Christchurch and Lyttelton, until they were restored for the Kingston Flyer tourist train in 1972.\n\nIn Finland, the first diesels were introduced in the mid-1950s, superseding steam locomotives by the early 1960s. State railways (VR) operated steam locomotives until 1975.\n\nIn the Netherlands, the first electric trains appeared in 1908, making the trip from Rotterdam to The Hague. The first diesels were introduced in 1934. As electric and diesel trains performed so well, the decline of steam started just after World War II, with steam traction ending in 1958.\n\nIn Poland, on non-electrified tracks, steam locomotives were superseded almost entirely by diesels by the 1990s. A few steam locomotives, however, operate in the regular scheduled service from Wolsztyn. After ceasing on 31 March 2014, regular service resumed out of Wolsztyn on 15 May 2017 with weekday runs to Leszno. This operation is maintained as a means of preserving railway heritage and as a tourist attraction. Apart from that, numerous railway museums and heritage railways (mostly narrow gauge) own steam locomotives in working condition.\n\nIn France, steam locomotives have not been used for commercial services since 24 September 1975.\n\nIn Spain, the first electric trains were introduced en 1911, and the first diesels in 1935, just one year before the Spanish Civil War. National railway company (Renfe) operated steam locomotives until 9 June 1975.\n\nIn Bosnia and Herzegovina, some steam locomotives are still used for industrial purposes, for example at the coal mine in Banovići and ArcelorMittal factory in Zenica.\n\nIn Paraguay, wood-burning steam locomotives operated until 1999.\n\nIn Thailand, all steam locomotives were withdrawn from service between the late 1960s and early 1970s. Most were scrapped in 1980. However, there are about 20 to 30 locomotives preserved for exhibit in important or end-of-the-line stations throughout the country. During the late 1980s, six locomotives were restored to running condition. Most are JNR-built 4-6-2 steam locomotives with the exception of a single 2-8-2.\nIndonesia has also used steam locomotives since 1876. The last batch of E10 0-10-0 rack tank locomotives were purchased in 1967 (Kautzor, 2010) from Nippon Sharyo. The last locomotives – the D 52 class, manufactured by the German firm Krupp in 1954 – operated until 1994, when they were replaced by diesel locomotives. Indonesia also purchased the last batch of mallet locomotives from Nippon Sharyo, to be used on the Aceh Railway. In Sumatra Barat (West Sumatra) and Ambarawa some rack railways (with a maximum gradient of 6% in mountainous areas) are now operated for tourism only. There are two rail museums in Indonesia, Taman Mini and Ambarawa (Ambarawa Railway Museum).\n\nPakistan Railways still has a regular steam locomotive service; a line operates in the North-West Frontier Province and in Sindh. It has been preserved as a \"nostalgia\" service for tourism in exotic locales, and is specifically advertised as being for \"steam buffs\".\n\nIn Sri Lanka, one steam locomotive is maintained for private service to power the \"Viceroy Special\".\n\nDramatic increases in the cost of diesel fuel prompted several initiatives to revive steam power. However none of these has progressed to the point of production and, as of the early 21st century, steam locomotives operate only in a few isolated regions of the world and in tourist operations.\n\nSeveral heritage railways in the UK have built new steam locomotives in the 1990s and early 21st century. These include the narrow-gauge Ffestiniog and Corris railways in Wales. The Hunslet Engine Company was revived in 2005, and began building steam locomotives on a commercial basis. A standard-gauge LNER Peppercorn Pacific \"Tornado\" was completed at Hopetown Works, Darlington, and made its first run on 1 August 2008. It entered main line service later in 2008, to great public acclaim. Demonstration trips in France and Germany have been planned. over half-a-dozen projects to build working replicas of extinct steam engines are going ahead, in many cases using existing parts from other types to build them. Examples include BR Class 6MT \"Hengist\", BR Class 3MT No. 82045, BR Class 2MT No. 84030, Brighton Atlantic \"Beachy Head\", the LMS \"Patriot 45551 The Unknown Warrior\" project, GWR \"47xx 4709, BR\" Class 6 72010 Hengist, GWR \"Saint\" 2999 Lady of Legend, 1014 \"County\" of Glamorgan and 6880 Betton \"Grange\" projects. These United Kingdom based new build projects are further complimented by the new build Pennsylvania Railroad T1 class No. 5550 project in the United States, which will attempt to surpass the speed record held by the LNER Class A4 4468 Mallard when completed.\n\nIn 1980, American financier Ross Rowland established American Coal Enterprises to develop a modernised coal-fired steam locomotive. His ACE 3000 concept attracted considerable attention, but was never built.\n\nIn 1998, in his book \"The Red Devil and Other Tales from the Age of Steam\", David Wardale put forward the concept of a high-speed high-efficiency \"Super Class 5 4-6-0\" locomotive for future steam haulage of tour trains on British main lines. The idea was formalised in 2001 by the formation of 5AT Project dedicated to developing and building the 5AT Advanced Technology Steam Locomotive, but it never received any major railway backing.\n\nLocations where new builds are taking place include:\n\nIn 2012, the Coalition for Sustainable Rail project was started in the US with the goal of creating a modern higher-speed steam locomotive, incorporating the improvements proposed by Livio Dante Porta and others, and using torrefied biomass as solid fuel. The fuel has been recently developed by the University of Minnesota in a collaboration between the university's Institute on the Environment (IonE) and Sustainable Rail International (SRI), an organisation set up to explore the use of steam traction in a modern railway setup. The group have received the last surviving (but non-running) ATSF 3460 class steam locomotive (No. 3463) via donation from its previous owner in Kansas, the Great Overland Station Museum. They hope to use it as a platform for developing \"the world's cleanest, most powerful passenger locomotive\", capable of speeds up to . Named \"Project 130\", it aims to break the world steam-train speed record set by LNER Class A4 4468 Mallard in the UK at . However, any demonstration of the project's claims is yet to be seen.\n\nIn Germany, a small number of fireless steam locomotives are still working in industrial service, e.g. at power stations, where an on-site supply of steam is readily available.\n\nThe Swiss company Dampflokomotiv- und Maschinenfabrik DLM AG delivered eight steam locomotives to rack railways in Switzerland and Austria between 1992 and 1996. Four of them are now the main traction on the Brienz Rothorn Bahn; the four others were built for the Schafbergbahn in Austria, where they run 90% of the trains.\n\nThe same company also rebuilt a German 2-10-0 locomotive to new standards with modifications such as roller bearings, light oil firing and boiler insulation.\n\nSteam locomotives have been present in popular culture since the 19th century. Folk songs from that period including \"I've Been Working on the Railroad\" and the \"Ballad of John Henry\" are a mainstay of American music and culture.\n\nMany steam locomotive toys have been made, and railway modelling is a popular hobby.\n\nSteam locomotives are often portrayed in fictional works, notably \"The Railway Series\" by the Rev W. V. Awdry, \"The Little Engine That Could\" by Watty Piper, \"The Polar Express\" by Chris Van Allsburg, and the Hogwarts Express from J.K. Rowling's Harry Potter series. They have also been featured in many children's television shows, such as \"Thomas the Tank Engine and Friends\", based on characters from the books by Awdry, and \"Ivor the Engine\" created by Oliver Postgate.\n\nThe Hogwarts Express also appears in the Harry Potter series of films, portrayed by GWR 4900 Class 5972 Olton Hall in a special Hogwarts livery. The Polar Express appears in the animated movie of the same name.\n\nAn elaborate, themed funicular Hogwarts Express ride is featured in the Universal Orlando Resort in Florida, connecting the Harry Potter section of Universal Studios with the Islands of Adventure theme park.\n\nThe Polar Express is recreated on many heritage railroads in the United States, including the North Pole Express pulled by the Pere Marquette 1225 locomotive, which is operated by the Steam Railroading Institute in Owosso, Michigan. According to author Van Allsburg, this locomotive was the inspiration for the story and it was used in the production of the movie.\n\nA number of computer and video games feature steam locomotives. \"Railroad Tycoon\" produced in 1990, was as \"one of the best computer games of the year\".\n\nThere are two notable examples of steam locomotives used as charges on heraldic coats of arms. One is that of Darlington, which displays \"Locomotion No. 1\". The other is the original coat of arms of Swindon, not currently in use, which displays a basic steam locomotive.\n\nSteam locomotives are a popular topic for coin collectors. The 1950 Silver 5 Peso coin of Mexico has a steam locomotive on its reverse as the prominent feature.\n\nThe 20 euro Biedermeier Period coin, minted 11 June 2003, shows on the obverse an early model steam locomotive (the \"Ajax\") on Austria's first railway line, the Kaiser Ferdinands-Nordbahn. The \"Ajax\" can still be seen today in the Technisches Museum Wien.\n\nAs part of the 50 State Quarters program, the quarter representing the US state of Utah depicts the ceremony where the two halves of the First Transcontinental Railroad met at Promontory Summit in 1869. The coin recreates a popular image from the ceremony with steam locomotives from each company facing each other while the golden spike is being driven.\n\n\n"}
{"id": "1432487", "url": "https://en.wikipedia.org/wiki?curid=1432487", "title": "Sunburst (finish)", "text": "Sunburst (finish)\n\nSunburst is a style of finishing for musical instruments such as electric and acoustic guitars and electric basses. At the center of a sunburst-finished surface is an area of lighter color (often showing the wood grain underneath) that darkens gradually towards the edges before hitting a dark rim. Among the best known examples of a sunburst finish are the Gibson Les Paul guitars and the Fender Stratocaster. It was originally intended to imitate an aged French polish finish, as applied to classical string instruments such as violins, as well as to enable the use of wood with less attractive edge grain on high-end instruments.\n\nSome vintage mandolins made by Gibson actually had a burst style finish achieved with stain that was wiped on to the top of the instrument and sometimes the back as well but sprayed tinted nitrocelulose lacquer later proved to be a faster way to achieve a burst finish.\n\nThere are various types of sunburst finishes. Some common types include \"vintage sunburst\", which is golden yellow in the very center and black around the edges, \"cherry sunburst\" - sometimes disparagingly called \"clownburst\", which is a golden yellow at the very center and cherry red towards the edges, \"tobacco sunburst\", which is golden yellow in the very center and brown around the edges, and \"three-color sunburst,\" which fades from golden yellow at the center through a layer of red and finally to black around the edges. The finish is often transparent in order to show, and accentuate, attractively-patterned wood or wood veneers such as flame maple, but may be opaque where the wood is not strongly figured, such as basswood or alder.\n\nOther sunburst varieties over the years included \"Sienna Sunburst\" and \"Blue Burst\", first introduced by Harmon in the late '70s and mid '80s. The American Series Strat HSS and the Strat Plus guitars are examples. The British Burns company use greenburst finishes on a number of their guitars, such as the Steer and Scorpion.\n\nThere are also aged variants of sunburst finishes which are usually found on high-end boutique instruments from Fender, Suhr, Tom Anderson, Melancon, Don Grosh and James Tyler, such as Aged Cherry Burst, Fireburst, Lightburst and Antique Tobacco Sunburst. These aged sunburst finishes are mostly suited for quilted, spalted and flamed maple tops, as well with other highly figured woods such as swamp ash and koa.\n"}
{"id": "39870400", "url": "https://en.wikipedia.org/wiki?curid=39870400", "title": "Transmission Service Request", "text": "Transmission Service Request\n\nIn the electric power industry, a Transmission Service Request is an application requesting a transmission-owning utility to allocate physical capacity in the form of Transmission Service Rights (TSRs) for the transmission of electric power. Every TSR includes at least three components: 1) a Point of Delivery (POD), where the power is injected into the utility's transmission network; 2) a Point of Receipt (POR), where the power is withdrawn from the utility's transmission network; 3) the amount of power, or capacity, in megawatts to be reserved. In order to receive TSR, a Transmission Customer (TC) must complete a transmission service request and go through a system impact study to determine if the existing transmission facilities can reliably accommodate the physical transfer of the electric power. If the capacity is not available, the transmission customer may decide to pay to upgrade the transmission network.\n\nOpen access to transmission facilities arose from the Federal Energy Regulatory Commission (FERC) Order 888, issued April 24, 1996. This order required each utility which transmitted electric energy in interstate commerce to file an Open Access Transmission Tariff (OATT) to stipulate the process for non-discriminatory access to its transmission facilities. This access includes Generation Interconnection and Transmission Service. Generation Interconnection allows for a power generator to connect to the transmission system while Transmission Service allows for the delivery of electricity through the transmission system. The goal of open access to electric power transmission facilities is to reduce electricity prices by promoting wholesale competition. In exchange for transmission service, transmitting utilities are allowed to charge the customer costs associated with the access to their transmission network. The cost for TSR is determined by a formula contained in the OATT and is often in the units of dollars per megawatt per year. TSRs are often renewed on an annual basis.\n\nTransmission Service Rights can be used for multiple purposes. Often, the TSR is used to facilitate two parties to arrange a power purchase agreement by transferring power from a generator to a load through the utility's transmission facilities. A few of the many potential scenarios where TSRs would be necessary:\nTSR is often pursued when it is either the lowest cost option or is necessary to meet a renewable portfolio standard.\n"}
{"id": "3299423", "url": "https://en.wikipedia.org/wiki?curid=3299423", "title": "Transversality (mathematics)", "text": "Transversality (mathematics)\n\nIn mathematics, transversality is a notion that describes how spaces can intersect; transversality can be seen as the \"opposite\" of tangency, and plays a role in general position. It formalizes the idea of a generic intersection in differential topology. It is defined by considering the linearizations of the intersecting spaces at the points of intersection.\n\nTwo submanifolds of a given finite-dimensional smooth manifold are said to intersect transversally if at every point of intersection, their separate tangent spaces at that point together generate the tangent space of the ambient manifold at that point. Manifolds that do not intersect are vacuously transverse. If the manifolds are of complementary dimension (i.e., their dimensions add up to the dimension of the ambient space), the condition means that the tangent space to the ambient manifold is the direct sum of the two smaller tangent spaces. If an intersection is transverse, then the intersection will be a submanifold whose codimension is equal to the sums of the codimensions of the two manifolds. In the absence of the transversality condition the intersection may fail to be a submanifold, having some sort of singular point.\n\nIn particular, this means that transverse submanifolds of complementary dimension intersect in isolated points (i.e., a 0-manifold). If both submanifolds and the ambient manifold are oriented, their intersection is oriented. When the intersection is zero-dimensional, the orientation is simply a plus or minus for each point.\n\nOne notation for the transverse intersection of two submanifolds \"L\" and \"L\" of a given manifold \"M\" is formula_1. This notation can be read in two ways: either as “\"L\" and \"L\" intersect transversally” or as an alternative notation for the set-theoretic intersection \"L\" ∩ \"L\" of \"L\" and \"L\" when that intersection is transverse. In this notation, the definition of transversality reads\n\nThe notion of transversality of a pair of submanifolds is easily extended to transversality of a submanifold and a map to the ambient manifold, or to a pair of maps to the ambient manifold, by asking whether the pushforwards of the tangent spaces along the preimage of points of intersection of the images generate the entire tangent space of the ambient manifold. If the maps are embeddings, this is equivalent to transversality of submanifolds.\n\nSuppose we have transverse maps formula_3 and formula_4 where formula_5 and formula_6 are manifolds with dimensions formula_7 and formula_8 respectively.\n\nThe meaning of transversality differs a lot depending on the relative dimensions of formula_9 and formula_10. The relationship between transversality and tangency is clearest when formula_11.\n\nWe can consider three separate cases:\n\nGiven any two smooth submanifolds, it is possible to perturb either of them by an arbitrarily small amount such that the resulting submanifold intersects transversally with the fixed submanifold. Such perturbations do not affect the homology class of the manifolds or of their intersections. For example, if manifolds of complementary dimension intersect transversally, the signed sum of the number of their intersection points does not change even if we isotope the manifolds to another transverse intersection. (The intersection points can be counted modulo 2, ignoring the signs, to obtain a coarser invariant.) This descends to a bilinear intersection product on homology classes of any dimension, which is Poincaré dual to the cup product on cohomology. Like the cup product, the intersection product is graded-commutative.\n\nThe simplest non-trivial example of transversality is of arcs in a surface. An intersection point between two arcs is transverse if and only if it is not a tangency, i.e., their tangent lines inside the tangent plane to the surface are distinct.\n\nIn a three-dimensional space, transverse curves do not intersect. Curves transverse to surfaces intersect in points, and surfaces transverse to each other intersect in curves. Curves that are tangent to a surface at a point (for instance, curves lying on a surface) do not intersect the surface transversally.\n\nHere is a more specialised example: suppose that formula_26 is a simple Lie group and formula_27 is its Lie algebra. By the Jacobson–Morozov theorem every nilpotent element formula_28 can be included into an formula_29-triple formula_30. The representation theory of formula_31 tells us that formula_32. The space formula_33 is the tangent space at formula_34 to the adjoint orbit formula_35 and so the affine space formula_36 intersects the orbit of formula_34 transversally. The space formula_36 is known as the \"Slodowy slice\" after Peter Slodowy.\n\nIn fields utilizing the calculus of variations or the related Pontryagin maximum principle, the transversality condition is frequently used to control the types of solutions found in optimization problems. For example, it is a necessary condition for solution curves to problems of the form:\n\nIn many of these problems, the solution satisfies the condition that the solution curve should cross transversally the nullcline or some other curve describing terminal conditions.\n\nUsing Sard's theorem, whose hypothesis is a special case of the transversality of maps, it can be shown that transverse intersections between submanifolds of a space of complementary dimensions or between submanifolds and maps to a space are themselves smooth submanifolds. For instance, if a smooth section of an oriented manifold's tangent bundle—i.e. a vector field—is viewed as a map from the base to the total space, and intersects the zero-section (viewed either as a map or as a submanifold) transversely, then the zero set of the section—i.e. the singularities of the vector field—forms a smooth 0-dimensional submanifold of the base, i.e. a set of signed points. The signs agree with the indices of the vector field, and thus the sum of the signs—i.e. the fundamental class of the zero set—is equal to the Euler characteristic of the manifold. More generally, for a vector bundle over an oriented smooth closed finite-dimensional manifold, the zero set of a section transverse to the zero section will be a submanifold of the base of codimension equal to the rank of the vector bundle, and its homology class will be Poincaré dual to the Euler class of the bundle.\n\nAn extremely special case of this is the following: if a differentiable function from reals to the reals has nonzero derivative at a zero of the function, then the zero is simple, i.e. it the graph is transverse to the \"x\"-axis at that zero; a zero derivative would mean a horizontal tangent to the curve, which would agree with the tangent space to the \"x\"-axis.\n\nFor an infinite-dimensional example, the d-bar operator is a section of a certain Banach space bundle over the space of maps from a Riemann surface into an almost-complex manifold. The zero set of this section consists of holomorphic maps. If the d-bar operator can be shown to be transverse to the zero-section, this moduli space will be a smooth manifold. These considerations play a fundamental role in the theory of pseudoholomorphic curves and Gromov–Witten theory. (Note that for this example, the definition of transversality has to be refined in order to deal with Banach spaces!)\n\n\"Transversal\" is a noun; the adjective is \"transverse.\"\nquote from J.H.C. Whitehead, 1959\n\n\n"}
{"id": "35283476", "url": "https://en.wikipedia.org/wiki?curid=35283476", "title": "Uganda Airlines Flight 775", "text": "Uganda Airlines Flight 775\n\nUganda Airlines Flight 775 was a Boeing 707-338C, registration 5X-UBC, that crashed while attempting to land at Rome-Fiumicino Airport in Rome, Italy on 17 October 1988. Thirty-three of the 52 occupants on board were killed.\n\nFlight 775 took off from London-Gatwick bound for Entebbe with an intermediate stop in Rome. While descending into Rome, the flight crew was given clearance for an ILS approach to runway 16L. Due to poor visibility, a missed approach was carried out. A second approach was attempted to runway 25. This too had to be abandoned due to weather conditions. The crew requested radar vectoring to runway 34L. The aircraft was established on the localiser but descended through minimum safe altitude. The aircraft impacted with some trees, then crashed, broke into pieces, and burst into flames approximately short of the runway.\n\nOne of the 19 survivors was John Harigye, a businessman and former Ugandan ambassador to the Vatican.\n\nThe probable cause of the crash was determined to be \"The crew's lack of adequate preparation in the procedure for a Non Precision Approach on runway 34L at Fiumicino Airport, especially in the matter of crew coordination and altitude callouts and their continued descent beyond MDA without having located the runway visual markings.\"\n\nThe following factors may have contributed to the cause of the accident:\n\nPart of the Board of Inquiry as well as the representative of the Ugandan CA, disassociated themselves from the majority, during the phase of identifying the factors that may have contributed to causing the accident.\n\n"}
{"id": "47948101", "url": "https://en.wikipedia.org/wiki?curid=47948101", "title": "Wells in Flames", "text": "Wells in Flames\n\nWells in Flames (French: Puits en flammes) is a 1937 German adventure film directed by Viktor Tourjansky and starring Josseline Gaël, George Rigaud and Suzy Vernon. It is the French-language version of \"City of Anatol\" (1936)\n\nThe film's sets were designed by the art director Otto Hunte.\n\n\n"}
{"id": "5883740", "url": "https://en.wikipedia.org/wiki?curid=5883740", "title": "Wheeling (electric power transmission)", "text": "Wheeling (electric power transmission)\n\nIn electric power transmission, wheeling is the transportation of electric energy (megawatt-hours) from within an electrical grid to an electrical load outside the grid boundaries. Two types of wheeling are 1) a wheel-through, where the electrical power generation and the load are both outside the boundaries of the transmission system and 2) a wheel-out, where the generation resource is inside the boundaries of the transmission system but the load is outside. Wheeling often refers to the scheduling of the energy transfer from one Balancing Authority to another. Since the wheeling of electric energy requires use of a transmission system, there is often an associated fee which goes to the transmission owners. In a simpler sense, it refers to the process of transmission of electricity through the transmission lines. \n\nUnder deregulation, many vertically integrated utilities were separated into generation owners, transmission and distribution owners, and retail providers. In order to recover capital costs, operating costs, and earn a return on investment, a transmission revenue requirement (TRR) is established and approved by a national agency (such as the Federal Energy Regulatory Commission in the United States) for each transmission owner. The TRR is paid through transmission access charges (TACs), load-weighted fees charged to internal load and energy exports for use of the transmission facilities. The energy export fee is often referred to as a wheeling charge. When wheeling-through, the transmission access charge only applies to the exported amount.\n\nA wheeling charge is a currency per megawatt-hour amount that a transmission owner receives for the use of its system to export energy. The total amount due in TAC fees is determined by the equation\nTotal wheeling fee = Wc ($/MWh) * Pw (MW) * t (h)\nThe fee associated with wheeling is referred to as a \"wheeling charge.\" This is an amount in $/MWh which transmission owner recovers for the use of its system. If the resource entity must go through multiple [transmission owner]s, it may be charged a wheeling charge for each one. The reasons for a wheeling charge are manifold. It may be simply to recover some costs of transmission facilities or congestion. However, another motivation would be to keep prices low. For instance, if the electricity prices in Arizona are 30 $/MWh and prices in California are 50 $/MWh, resources in Arizona would want to sell to the California market to make more money. The utilities in Arizona would then be forced to pay 50 $/MWh if they needed these resources. If Arizona charged a wheeling charge of 10 $/MWh, Arizona would only have to pay 40 $ /MWh to compete with California. However, Arizona would not want to charge too much, as this could impact advantages of trading electric energy between systems. In this way, it works similarly to [tariff]s.\n\n"}
{"id": "16624581", "url": "https://en.wikipedia.org/wiki?curid=16624581", "title": "Winter of Terror", "text": "Winter of Terror\n\nThe Winter of Terror was the three-month period during the winter of 1950–1951\nwhen an unprecedented number of avalanches took place in the Alps,\nAustria–Switzerland border.\nThe series of 649 avalanches killed over 265 people and\ncaused large amounts of damage to residential and\nother human-made structures.\n\nAustria suffered the most damage and loss of human life with 135 killed and many villages destroyed. Thousands of acres of economically valuable forest in both Austria and Switzerland, were also damaged during the period.\n\nThe Valais canton of Switzerland suffered 92 human deaths, approximately 500 cattle deaths, and destruction of 900 human-made structures. As in Austria, economically important forests were also damaged during the period.\n\nThe Swiss town of Andermatt in the Adula Alps was hit by six avalanches within a 60-minute period, resulting in 13 human deaths.\n\nThis period is thought to be a result of atypical weather conditions in the Alps: high precipitation due to the meeting of an Atlantic warm front with a polar cold front resulted in 3–4.5 metres of snow being deposited in a two- to three-day period. More than 600 buildings were destroyed and over 40,000 people were buried under snow.\n"}
